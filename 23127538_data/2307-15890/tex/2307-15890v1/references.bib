%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Saved with string encoding Unicode (UTF-8)


@article{li2023accelerated,
  title={Accelerated and Instance-Optimal Policy Evaluation with Linear Function Approximation},
  author={Li, Tianjiao and Lan, Guanghui and Pananjady, Ashwin},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={5},
  number={1},
  pages={174--200},
  year={2023},
  publisher={SIAM}
}

@article{lan2022policy,
  title={Policy optimization over general state and action spaces},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:2211.16715},
  year={2022}
}

@article{nesterov2009primal,
  title={Primal-dual subgradient methods for convex problems},
  author={Nesterov, Yurii},
  journal={Mathematical programming},
  volume={120},
  number={1},
  pages={221--259},
  year={2009},
  publisher={Springer}
}

@article{kotsalis2020simple,
  title={Simple and optimal methods for stochastic variational inequalities, ii: Markovian noise and policy evaluation in reinforcement learning},
  author={Kotsalis, Georgios and Lan, Guanghui and Li, Tianjiao},
  journal={arXiv preprint arXiv:2011.08434},
  year={2020}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}


@article{tao2015random,
  title={Random matrices: universality of local spectral statistics of non-Hermitian matrices},
  author={Tao, Terence and Vu, Van},
  year={2015}
}


@inproceedings{sutton2009fast,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={993--1000},
  year={2009}
}


@article{sutton2008convergent,
  title={A convergent O (n) algorithm for off-policy temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Szepesv{\'a}ri, Csaba and Maei, Hamid Reza},
  journal={Advances in neural information processing systems},
  volume={21},
  number={21},
  pages={1609--1616},
  year={2008},
  publisher={MIT Press}
}

@inproceedings{nair2018overcoming,
  title={Overcoming exploration in reinforcement learning with demonstrations},
  author={Nair, Ashvin and McGrew, Bob and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={6292--6299},
  year={2018},
  organization={IEEE}
}

@inproceedings{Danskin1967TheTO,
  title={The Theory of Max-Min and its Application to Weapons Allocation Problems},
  author={John M. Danskin},
  year={1967}
}


@article{gower2015randomized,
  title={Randomized iterative methods for linear systems},
  author={Gower, Robert M and Richt{\'a}rik, Peter},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={36},
  number={4},
  pages={1660--1690},
  year={2015},
  publisher={SIAM}
}

@article{strohmer2009randomized,
  title={A randomized Kaczmarz algorithm with exponential convergence},
  author={Strohmer, Thomas and Vershynin, Roman},
  journal={Journal of Fourier Analysis and Applications},
  volume={15},
  number={2},
  pages={262},
  year={2009},
  publisher={Springer}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}


@article{goyal2023robust,
  title={Robust Markov decision processes: Beyond rectangularity},
  author={Goyal, Vineet and Grand-Clement, Julien},
  journal={Mathematics of Operations Research},
  volume={48},
  number={1},
  pages={203--226},
  year={2023},
  publisher={INFORMS}
}

@article{mannor2016robust,
  title={Robust MDPs with k-rectangular uncertainty},
  author={Mannor, Shie and Mebel, Ofir and Xu, Huan},
  journal={Mathematics of Operations Research},
  volume={41},
  number={4},
  pages={1484--1509},
  year={2016},
  publisher={INFORMS}
}

@article{karczmarz1937angenaherte,
  title={Angenaherte auflosung von systemen linearer glei-chungen},
  author={Karczmarz, Stefan},
  journal={Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat.},
  pages={355--357},
  year={1937}
}

@book{rockafellar1970convex,
  title={Convex analysis},
  author={Rockafellar, R Tyrrell},
  volume={18},
  year={1970},
  publisher={Princeton university press}
}

@article{kruger2003frechet,
  title={On fr{\'e}chet subdifferentials},
  author={Kruger, A Ya},
  journal={Journal of Mathematical Sciences},
  volume={116},
  number={3},
  pages={3325--3358},
  year={2003},
  publisher={Springer}
}


@article{li2023policy,
  title={Policy Mirror Descent Inherently Explores Action Space},
  author={Li, Yan and Lan, Guanghui},
  journal={arXiv preprint arXiv:2303.04386},
  year={2023}
}

@article{simon2014introduction,
  title={Introduction to geometric measure theory},
  author={Simon, Leon},
  journal={Tsinghua Lectures},
  volume={2},
  number={2},
  pages={3--1},
  year={2014}
}


@article{li2022first,
  title={First-order policy optimization for robust Markov decision process},
  author={Li, Yan and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2209.10579},
  year={2022}
}

@book{lecture_gmt,
	author = {Simon, Leon},
	title = {Lectures on Geometric Measure Theory},
	publisher = {The Australian National University, Mathematical Sciences Institute, Centre for Mathematics and its Applications},
	year = {1983},
	volume = {3},
	month = {1}
}

@article{yang2022rorl,
  title={RORL: Robust Offline Reinforcement Learning via Conservative Smoothing},
  author={Yang, Rui and Bai, Chenjia and Ma, Xiaoteng and Wang, Zhaoran and Zhang, Chongjie and Han, Lei},
  journal={arXiv preprint arXiv:2206.02829},
  year={2022}
}

@inproceedings{shen2020deep,
  title={Deep reinforcement learning with robust and smooth policy},
  author={Shen, Qianli and Li, Yan and Jiang, Haoming and Wang, Zhaoran and Zhao, Tuo},
  booktitle={International Conference on Machine Learning},
  pages={8707--8718},
  year={2020},
  organization={PMLR}
}

@inproceedings{
Li2020Implicit,
title={Implicit Bias of Gradient Descent based Adversarial Training on Separable Data},
author={Yan Li and Ethan X.Fang and Huan Xu and Tuo Zhao},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkgTTh4FDH}
}


@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Soc.}
}



@article{chen2021lyapunov,
  title={A Lyapunov theory for finite-sample guarantees of asynchronous Q-learning and TD-learning variants},
  author={Chen, Zaiwei and Maguluri, Siva Theja and Shakkottai, Sanjay and Shanmugam, Karthikeyan},
  journal={arXiv preprint arXiv:2102.01567},
  year={2021}
}

@article{kumar2022efficient,
  title={Efficient Policy Iteration for Robust Markov Decision Processes via Regularization},
  author={Kumar, Navdeep and Levy, Kfir and Wang, Kaixin and Mannor, Shie},
  journal={arXiv preprint arXiv:2205.14327},
  year={2022}
}


@article{ma2022distributionally,
  title={Distributionally robust offline reinforcement learning with linear function approximation},
  author={Ma, Xiaoteng and Liang, Zhipeng and Xia, Li and Zhang, Jiheng and Blanchet, Jose and Liu, Mingwen and Zhao, Qianchuan and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2209.06620},
  year={2022}
}


@inproceedings{zhou2021finite,
  title={Finite-sample regret bound for distributionally robust offline tabular reinforcement learning},
  author={Zhou, Zhengqing and Zhou, Zhengyuan and Bai, Qinxun and Qiu, Linhai and Blanchet, Jose and Glynn, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3331--3339},
  year={2021},
  organization={PMLR}
}


@article{huber1992robust,
  title={Robust estimation of a location parameter},
  author={Huber, Peter J},
  journal={Breakthroughs in statistics: Methodology and distribution},
  pages={492--518},
  year={1992},
  publisher={Springer}
}

@inproceedings{liu2022distributionally,
  title={Distributionally Robust $ Q $-Learning},
  author={Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan},
  booktitle={International Conference on Machine Learning},
  pages={13623--13643},
  year={2022},
  organization={PMLR}
}

@article{derman2021twice,
  title={Twice regularized MDPs and the equivalence between robustness and regularization},
  author={Derman, Esther and Geist, Matthieu and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  year={2020},
  publisher={Springer}
}

@article{wang2022policy,
  title={Policy Gradient Method For Robust Reinforcement Learning},
  author={Wang, Yue and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2205.07344},
  year={2022}
}

@article{grand2020scalable,
  title={Scalable first-order methods for robust MDPs},
  author={Grand-Cl{\'e}ment, Julien and Kroer, Christian},
  journal={arXiv preprint arXiv:2005.05434},
  year={2020}
}

@article{goyal2022first,
  title={A first-order approach to accelerated value iteration},
  author={Goyal, Vineet and Grand-Clement, Julien},
  journal={Operations Research},
  year={2022},
  publisher={INFORMS}
}

@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5668--5675},
  year={2020}
}

@article{xu2020improving,
  title={Improving sample complexity bounds for actor-critic algorithms},
  author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
  journal={arXiv preprint arXiv:2004.12956},
  year={2020}
}


@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}


@article{cen2021fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  year={2021},
  publisher={INFORMS}
}

@article{li2022homotopic,
  title={Homotopic Policy Mirror Descent: Policy Convergence, Implicit Regularization, and Improved Sample Complexity},
  author={Li, Yan and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2201.09457},
  year={2022}
}

@article{xiao2022convergence,
  title={On the Convergence Rates of Policy Gradient Methods},
  author={Xiao, Lin},
  journal={arXiv preprint arXiv:2201.07443},
  year={2022}
}

@article{lan2021policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:2102.00135},
  year={2021}
}

@article{khodadadian2021linear,
  title={On the linear convergence of natural policy gradient algorithm},
  author={Khodadadian, Sajad and Jhunjhunwala, Prakirt Raj and Varma, Sushil Mahavir and Maguluri, Siva Theja},
  journal={arXiv preprint arXiv:2105.01424},
  year={2021}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021},
  publisher={Microtome Publishing}
}

@article{ho2021partial,
  title={Partial policy iteration for L1-robust Markov decision processes},
  author={Ho, Chin Pang and Petrik, Marek and Wiesemann, Wolfram},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={275},
  pages={1--46},
  year={2021}
}

@inproceedings{panaganti2022sample,
  title={Sample Complexity of Robust Reinforcement Learning with a Generative Model},
  author={Panaganti, Kishan and Kalathil, Dileep},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={9582--9602},
  year={2022},
  organization={PMLR}
}

@article{goyal2022robust,
  title={Robust Markov Decision Processes: Beyond Rectangularity},
  author={Goyal, Vineet and Grand-Clement, Julien},
  journal={Mathematics of Operations Research},
  year={2022},
  publisher={INFORMS}
}

@article{wiesemann2013robust,
  title={Robust Markov decision processes},
  author={Wiesemann, Wolfram and Kuhn, Daniel and Rustem, Ber{\c{c}}},
  journal={Mathematics of Operations Research},
  volume={38},
  number={1},
  pages={153--183},
  year={2013},
  publisher={INFORMS}
}

@article{kaufman2013robust,
  title={Robust modified policy iteration},
  author={Kaufman, David L and Schaefer, Andrew J},
  journal={INFORMS Journal on Computing},
  volume={25},
  number={3},
  pages={396--410},
  year={2013},
  publisher={INFORMS}
}


@ARTICLE{580874,
  author={Tsitsiklis, J.N. and Van Roy, B.},
  journal={IEEE Transactions on Automatic Control}, 
  title={An analysis of temporal-difference learning with function approximation}, 
  year={1997},
  volume={42},
  number={5},
  pages={674-690},
  doi={10.1109/9.580874}}


@article{kose2020risk,
  title={Risk-averse learning by temporal difference methods},
  author={Kose, Umit and Ruszczynski, Andrzej},
  journal={arXiv preprint arXiv:2003.00780},
  year={2020}
}

@article{wang2021online,
  title={Online Robust Reinforcement Learning with Model Uncertainty},
  author={Wang, Yue and Zou, Shaofeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{roy2017reinforcement,
  title={Reinforcement learning under model mismatch},
  author={Roy, Aurko and Xu, Huan and Pokutta, Sebastian},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
} 

@book{powell2007approximate,
  title={Approximate Dynamic Programming: Solving the curses of dimensionality},
  author={Powell, Warren B},
  volume={703},
  year={2007},
  publisher={John Wiley \& Sons}
}

@inproceedings{tamar2014scaling,
  title={Scaling up robust MDPs using function approximation},
  author={Tamar, Aviv and Mannor, Shie and Xu, Huan},
  booktitle={International conference on machine learning},
  pages={181--189},
  year={2014},
  organization={PMLR}
}

@inproceedings{badrinath2021robust,
  title={Robust reinforcement learning using least squares policy iteration with provable performance guarantees},
  author={Badrinath, Kishan Panaganti and Kalathil, Dileep},
  booktitle={International Conference on Machine Learning},
  pages={511--520},
  year={2021},
  organization={PMLR}
}

@article{ruszczynski2010risk,
  title={Risk-averse dynamic programming for Markov decision processes},
  author={Ruszczy{\'n}ski, Andrzej},
  journal={Mathematical programming},
  volume={125},
  number={2},
  pages={235--261},
  year={2010},
  publisher={Springer}
}


@article{li2023policy,
  title={Policy Mirror Descent Inherently Explores Action Space},
  author={Li, Yan and Lan, Guanghui},
  journal={arXiv preprint arXiv:2303.04386},
  year={2023}
}

@article{vial1982strong,
  title={Strong convexity of sets and functions},
  author={Vial, Jean-Philippe},
  journal={Journal of Mathematical Economics},
  volume={9},
  number={1-2},
  pages={187--205},
  year={1982},
  publisher={Elsevier}
}



@article{roy2017reinforcement,
  title={Reinforcement learning under model mismatch},
  author={Roy, Aurko and Xu, Huan and Pokutta, Sebastian},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{wang2022policy,
  title={Policy Gradient Method For Robust Reinforcement Learning},
  author={Wang, Yue and Zou, Shaofeng},
  journal={arXiv preprint arXiv:2205.07344},
  year={2022}
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}


@inproceedings{wang2022policy,
  title={Policy gradient method for robust reinforcement learning},
  author={Wang, Yue and Zou, Shaofeng},
  booktitle={International Conference on Machine Learning},
  pages={23484--23526},
  year={2022},
  organization={PMLR}
}

@inproceedings{liu2022distributionally,
  title={Distributionally Robust $ Q $-Learning},
  author={Liu, Zijian and Bai, Qinxun and Blanchet, Jose and Dong, Perry and Xu, Wei and Zhou, Zhengqing and Zhou, Zhengyuan},
  booktitle={International Conference on Machine Learning},
  pages={13623--13643},
  year={2022},
  organization={PMLR}
}



@article{li2022homotopic,
  title={Homotopic Policy Mirror Descent: Policy Convergence, Implicit Regularization, and Improved Sample Complexity},
  author={Li, Yan and Zhao, Tuo and Lan, Guanghui},
  journal={arXiv preprint arXiv:2201.09457},
  year={2022}
}


@article{shapiro2021distributionally,
  title={Distributionally robust optimal control and MDP modeling},
  author={Shapiro, Alexander},
  journal={Operations Research Letters},
  volume={49},
  number={5},
  pages={809--814},
  year={2021},
  publisher={Elsevier}
}


@article{ruszczynski2010risk,
  title={Risk-averse dynamic programming for Markov decision processes},
  author={Ruszczy{\'n}ski, Andrzej},
  journal={Mathematical programming},
  volume={125},
  pages={235--261},
  year={2010},
  publisher={Springer}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021},
  publisher={Microtome Publishing}
}


@article{xiao2022convergence,
  title={On the Convergence Rates of Policy Gradient Methods},
  author={Xiao, Lin},
  journal={arXiv preprint arXiv:2201.07443},
  year={2022}
}


@inproceedings{badrinath2021robust,
  title={Robust reinforcement learning using least squares policy iteration with provable performance guarantees},
  author={Badrinath, Kishan Panaganti and Kalathil, Dileep},
  booktitle={International Conference on Machine Learning},
  pages={511--520},
  year={2021},
  organization={PMLR}
}




@article{xu2008robust,
  title={Robust regression and lasso},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}


@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}


@article{nilim2005robust,
  title={Robust control of Markov decision processes with uncertain transition matrices},
  author={Nilim, Arnab and El Ghaoui, Laurent},
  journal={Operations Research},
  volume={53},
  number={5},
  pages={780--798},
  year={2005},
  publisher={INFORMS}
}


@article{iyengar2005robust,
  title={Robust dynamic programming},
  author={Iyengar, Garud N},
  journal={Mathematics of Operations Research},
  volume={30},
  number={2},
  pages={257--280},
  year={2005},
  publisher={INFORMS}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}


@article{lan2021policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={arXiv preprint arXiv:2102.00135},
  year={2021}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.12152},
  year={2018}
}


@article{xu2009robustness,
  title={Robustness and Regularization of Support Vector Machines.},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={Journal of machine learning research},
  volume={10},
  number={7},
  year={2009}
}