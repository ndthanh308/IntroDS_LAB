%!TEX root = ./robust_pe.tex

\section{Stochastic Robust Policy Evaluation}\label{sec_stochastic}

Compared to the deterministic setting, in the stochastic setting we do not have exact information on $\overline{\PP}$. Instead only transition samples with distribution governed by $\overline{\PP}$ are available.
As in Section \ref{sec_deterministic}, we initiate our discussion with tabular setting.
%, where the size of the state space is relatively small compared to available computational resource. 
Then we proceed to discuss the convergence of the proposed method with linear function approximation to handle large state spaces.\footnote{
The method to be developed can indeed be combined with general function approximation schemes, provided certain off-policy evaluation problem can be solved to a properly pre-specified accuracy. We defer related discussions to Section~\ref{sec_conclusion}. 
}
In both cases we present sample complexities that appears to be completely new for stochastic robust policy evaluation.








We start by introducing the framework of stochastic first-order robust policy evaluation (SFRPE).
Compared to Section \ref{sec_deterministic}, 
the distance generating function $w_s(\cdot)$ in the stochastic setting can potentially depend on $s$ through $\vartheta(\cdot|s)$. 
We assume in addition that the Bregman divergence induced by $w_s(\cdot)$ satisfies 
\begin{align}\label{sc_in_group_norm}
\cB_s(\DD, \DD') \coloneqq w_s(\DD') - w_s(\DD) - \inner{\nabla  w_s(\DD)}{\DD' - \DD}
 \geq  \frac{\mu_w}{2} \tsum_{a \in \cA} \vartheta(a|s) \norm{\DD_a - \DD'_a}_1^2
\end{align}
for some $\mu_w > 0$.
Many of our ensuing discussions consider the case where   
\begin{align}\label{dgf_negative_entropy}
w_s(\DD) =  \tsum_{a \in \cA} \vartheta(a|s) \tsum_{s' \in \cS} \DD_{a}(s') \log \DD_{a}(s') + \log \abs{\cS}.
\end{align}
In this case, it can be readily verified that $\cB_s(\DD, \DD')  =  \tsum_{a \in \cA}\vartheta(a|s) \tsum_{s' \in \cS}  \DD'_a(s') \log \rbr{ 
{\DD'_a(s')}/{\DD_a(s')}
}$,
and 
\begin{align}
0  \leq   w_s(\DD)  & \leq \log \abs{\cS}, \label{bregman_divergence_negative_entropy_lb_ub} \\
\cB_s(\DD, \DD')  
%=  \tsum_{a \in \cA}\vartheta(a|s) \tsum_{s' \in \cS}  \DD_a(s') \log \rbr{ 
%\frac{\DD_a(s')}{\DD'_a(s')}
%} 
&  \geq \frac{1}{2} \tsum_{a \in \cA}  \vartheta(a|s) \norm{\DD_a - \DD'_a}_1^2, \label{bregman_divergence_negative_entropy_sc}
%~ \forall \DD, \DD' \in \Delta_{\cS}^{\abs{\cA}}.
\end{align}
where \eqref{bregman_divergence_negative_entropy_sc} follows from the Pinsker's inequality.
Similar to the deterministic setting, we require $\overline{w} = \sup_{s \in \cS} \sup_{\DD \in \Delta_{\cS}^{\abs{\cA}}} w_s(\DD) < \infty$.
In view of \eqref{bregman_divergence_negative_entropy_lb_ub} and \eqref{bregman_divergence_negative_entropy_sc}, for $w_s(\cdot)$ defined in \eqref{dgf_negative_entropy} one can take $\overline{w} =\log \abs{\cA}$ and $\mu_w = 1$.




\begin{algorithm}[t]
  \caption{Stochastic First-order Robust Policy Evaluation (SFRPE)}
  \begin{algorithmic}
%    \REQUIRE Input
%    \ENSURE Output
    \STATE {\bf Input:} $\cbr{(\beta_k, \lambda_k)}$.
    \STATE {\bf Initialize:} arbitrary initial policy $\pi_0 \in \Pi$.
    \FOR{$k = 0, 1, \ldots$}
 	\STATE  Run stochastic evaluation operator to obtain $\hat{\cV}^{\pi_k} $.
%	form $\hat{\cV}^{\pi_n}_{\vartheta, s}$ for every $s \in \cS$, where 
%	\begin{align*}
%	\hat{\cV}^{\pi_n}_{\vartheta, s} \coloneqq \vartheta(\cdot|s) \otimes \hat{\cV}^{\pi_n} .
%	\end{align*}
	\STATE  Update:
	\begin{align}\label{raw_update_stoch_rpe}
	\textstyle
	\pi_{k+1}(s) = \gamma \zeta \argmin_{\DD \in \cD_s}  \tsum_{t=0}^{k}  \beta_t \inner{ \DD}{\hat{\cV}^{\pi_t}_{\vartheta, s}} + \lambda_k w_s(\DD), ~ \forall s \in \cS,
	\end{align}
	where 
	$
		\hat{\cV}^{\pi_t}_{\vartheta, s} \coloneqq \vartheta(\cdot|s) \otimes \hat{\cV}^{\pi_t} .
	$
    \ENDFOR
    \RETURN $\tsum_{t=1}^k \theta_t \hat{\cV}^{\pi_t}$, where 
    \begin{align}\label{def_theta}
	\theta_t = \beta_t / (\tsum_{t=1}^k \beta_t) .
	\end{align}
     \end{algorithmic}
\end{algorithm}


\begin{remark}
%It turns out that divergence \eqref{dgf_negative_entropy} enjoys at least two apparent advantages. 
%In purely online robust MDP problems (cf. Example \ref{online_robust_mdp}) where $\cD_s = \Delta_{\cS}^{\abs{\cA}}$, it can readily seen that update of SFRPE \eqref{raw_update_stoch_rpe} has closed-form solution
%$
%\DD^{\pi_{n+1}(s)}_a \propto \exp (
%- \gamma \zeta \tsum_{t=0}^n \hat{\cV}^{\pi_t} / \lambda_n
%)
%$
%for any $a \in \cA$.
The weighted construction of divergence \eqref{dgf_negative_entropy}  appears to be essential. In particular, setting equal weights in \eqref{dgf_negative_entropy} would result in a sample complexity that linearly depends on the action space $\abs{\cA}$ despite we are evaluating the robust value function.
%
%\yan{Need to mention on the motivation of state-dependent divergence -- remove the dependence on the action space}
%\yan{need to mention how the update can be solved efficiently for large state space, or has explicit solution}
%\yan{both are related to the choice of divergence}
\end{remark}

It is clear that update \eqref{raw_update_stoch_rpe} is equivalent to the following update
\begin{align}
\pi_{k+1}(s) & = \argmin_{\DD \in \cD_s} \tsum_{t= 0}^k \beta_t \hat{\phi}_t(s, \DD)  + \lambda_k w_s(\DD) \nonumber \\
& = \argmin_{\DD \in \cD_s}  \hat{\Phi}_k(s, \DD)  + \lambda_k w_s(\DD) , \label{pda_tabular_stoch_update}
\end{align}
where  
$\hat{\phi}_t(s, \DD)  = \gamma \zeta \inner{\DD - \pi_t(s)}{\hat{\cV}^{\pi_t}_{\vartheta, s}}$ and $ \hat{\Phi}_k  = \tsum_{t=0}^k \beta_t \hat{\phi}_t$.
Let us also define 
\begin{align}\label{def_noise_in_phi}
\delta_t(s, \DD)  \coloneqq \hat{\phi}_t(s, \DD)  - \phi_t (s, \DD) = \gamma \zeta \inner{\DD - \pi_t(s)}{\hat{\cV}^{\pi_t}_{\vartheta, s} - {\cV}^{\pi_t}_{\vartheta, s}},
\end{align}
where the last equality follows from the definition of $\hat{\phi}_t$ and $\phi_t$.

The following lemma follows the exact same argument as in Lemma \ref{lemma_pda_determinsitic_step_characterization}.

\begin{lemma}\label{lemma_pda_stochastic_step_characterization}
Define $\hat{\Phi}_{-1} \equiv 0$ and $\lambda_{-1} = 0$, and let $\lambda_k \geq \lambda_{k-1}$ for every $k \geq 1$.
Then for any $k \geq 0$, we have 
\begin{align}
& \hat{\Phi}_k(s, \pi_{k+1}(s)) + \lambda_k \cB_s(\pi_{k+1}(s), \DD) \leq \hat{\Phi}_k(s, \DD), ~ \forall \DD \in \cD_s, \label{pda_nhree_point_stoch} \\
& \beta_k \hat{\phi}_k(s, \pi_{k+1}(s)) 
 \leq \hat{\Phi}_k(s, \pi_{k+1}(s))  - \hat{\Phi}_{k-1}(s, \pi_k(s)) - \lambda_{k-1} \cB_s( \pi_k(s), \pi_{k+1}(s))  \label{raw_progress_ineq_pda_stoch} .
\end{align}
\end{lemma}

We next establish some generic convergence properties of SFRPE. 


\begin{lemma}\label{lemma_generic_prop_stoch}
Let  $\lambda_t \geq \lambda_{t-1}$ for every $n \geq 1$,  $\beta_0 = 0$.
%For any $k \geq 1$, define 
%\begin{align}\label{def_theta}
%\theta_t = \beta_t / (\tsum_{t=1}^k \beta_t) .
%\end{align}
Then
%Suppose $\EE_{|k} \sbr{\norm{\hat{\cV}^{\pi_k}}_\infty^2} \leq M$ for any $k \geq 0$,  then for any $s \in \cS$, 
\begin{align}
 \tsum_{t=1}^k \theta_t \rbr{\hat{\cV}^{\pi_t}(s) - \cV^{\pi_t}(s)}
&  \leq \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s) - \cV^*(s) \nonumber \\
& \leq \rbr{\tsum_{t=1}^k \beta_t}^{-1}  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2}{2 \mu_w \lambda_{t-1} (1-\gamma)}
+  \rbr{\tsum_{t=1}^k \beta_t}^{-1} \frac{\lambda_k \overline{w}}{1-\gamma} \nonumber \\
& ~~~ + \tsum_{t=1}^k \frac{\theta_t}{1-\gamma}  \EE_{s' \sim d_{s}^{\pi^*}} \sbr{ \delta_t(s', \pi^*(s'))} 
+ \tsum_{t=1}^k \theta_t \rbr{\hat{\cV}^{\pi_t}(s) - \cV^{\pi_t}(s)}. \label{eq_generic_prop_stoch}
\end{align}
\end{lemma}

\begin{proof}
%Let us define $\DD^{\pi(s)}  = \pi(s)$ for any policy $\pi$.
%For notational clarity we will sometimes use these two quantities interchangeably. 
Taking the telescopic sum of \eqref{raw_progress_ineq_pda_stoch}  yields
% \yan{change the annotation to the following, and modify the ensuing results on dependence of $\mu_w$ accordingly}
\begin{align*}
\hat{\Phi}_0 (s, \pi_1(s)) & \leq 
\hat{\Phi}_k(s, \pi_{k+1}(s)) 
- \tsum_{t=1}^k \lambda_{t-1} \cB_s( \pi_t(s), \pi_{t+1}(s)) 
- \tsum_{t=1}^k \beta_t \hat{\phi}_t (s, \pi_{t+1}(s)) \\
& \overset{(a)}{\leq} 
 \hat{\Phi}_k(s, \pi_{k+1}(s)) 
-   \tsum_{t=1}^k \tsum_{a \in \cA} \frac{\lambda_{t-1} \mu_w \vartheta(a|s)}{2} \norm{\DD^{\pi_{t+1}(s)}_a - \DD^{\pi_t(s)}_a }_1^2  \\
& ~~~ - \tsum_{t=1}^k \beta_t  \gamma \zeta \inner{\pi_{t+1}(s) - \pi_t(s)}{\hat{\cV}^{t}_{\vartheta, s}} \\
& \overset{(b)}{=} 
 \hat{\Phi}_k(s, \pi_{k+1}(s)) 
-  \tsum_{t=1}^k \tsum_{a \in \cA}  \frac{\lambda_{t-1} \mu_w \vartheta(a|s)}{2} \norm{\DD^{\pi_{t+1}(s)}_a - \DD^{\pi_t(s)}_a }_1^2  \\
& ~~~  -  \tsum_{t=1}^k \beta_t  \tsum_{a \in \cA}  \gamma \zeta  \vartheta(a|s) \inner{\DD^{\pi_{t+1}(s)}_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t}} \\
& \overset{(c)}{\leq} 
 \hat{\Phi}_k(s, \pi_{k+1}(s)) 
-  \tsum_{t=1}^k \tsum_{a \in \cA}  \frac{\lambda_{t-1} \mu_w \vartheta(a|s)}{2}  \norm{\DD^{\pi_{t+1}(s)}_a - \DD^{\pi_t(s)}_a }_1^2  \\
& ~~~  -   \tsum_{t=1}^k \tsum_{a \in \cA} \beta_t  \gamma \zeta  \vartheta(a|s)   \norm{\hat{\cV}^{\pi_t}}_\infty \norm{\DD^{\pi_{t+1}(s)}_a - \DD^{\pi_t(s)}_a}_1 \\
& \overset{(d)}{\leq} 
 \hat{\Phi}_k(s, \pi_{k+1}(s)) 
+   \tsum_{t=1}^k \tsum_{a \in \cA} \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2 \vartheta(a|s)}{2 \mu_w \lambda_{t-1}} \\
& =
 \hat{\Phi}_k(s, \pi_{k+1}(s)) 
+   \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2 }{2 \mu_w \lambda_{t-1}} \\
&\overset{(e)}{\leq} \hat{\Phi}_k(s, \DD)  + \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2}{2 \mu_w \lambda_{t-1}}, ~ \forall \DD \in \cD_s,
\end{align*}
where $(a)$ follows from the definition of $\hat{\phi}_t$ and \eqref{sc_in_group_norm};
$(b)$ follows from the definition of $\hat{\cV}^t_{\vartheta, s}$;
$(c)$ follows from H\"{o}lder's inequality and the definition of $M$;
$(d)$ follows from Young's inequality; 
and $(e)$ applies \eqref{pda_nhree_point_stoch}.
Since $\beta_0 = 0$ and $w_s(\cdot) \geq 0$, the above inequality implies
\begin{align*}
0 \leq \tsum_{t=1}^k \beta_t {\phi}_t(s, \DD) +  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2}{2 \mu_w \lambda_{t-1}} + \lambda_k w_s(\DD)
+ \tsum_{t=1}^k \beta_t  \delta_t(s,\DD),
\end{align*}
%where $\delta_t \coloneqq \hat{\phi}_n - \phi_t$.
Now consider  aggregating the above inequalities by weights $d_{s}^{\pi^*}$ after taking $\DD=\pi^*(s)$ therein.
From Lemma \ref{lemma_perf_diff} we then obtain  
\begin{align*}
0 \leq \tsum_{t=1}^k \beta_t (1-\gamma) \rbr{\cV^*(s) - \cV^{\pi_t}(s)} 
+  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2}{2 \mu_w \lambda_{t-1}} + \lambda_k \overline{w}
+ \tsum_{t=1}^k \beta_t \EE_{s' \sim d_{s}^{\pi^*}} \sbr{ \delta_t(s', \pi^*(s'))},
\end{align*}
Simple rearrangement of the above inequality yields the desired claim.
%\begin{align*}
%\tsum_{s \in \cS} \rho(s) \tsum_{t=1}^k \theta_t \sbr{
%\cV^{\pi_t}(s) - \cV^*(s)
% }
%& \leq \rbr{\tsum_{t=1}^k \beta_t}^{-1}  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2 M^2}{2 \mu_w \lambda_{t-1}}
%+  \rbr{\tsum_{t=1}^k \beta_t}^{-1} \lambda_k \overline{w} \\
%& ~~~ +  \rbr{\tsum_{t=1}^k \beta_t}^{-1}  \rbr{ \tsum_{t=1}^k \beta_t \EE_{s \sim d_{\rho}^{\pi^*}} \sbr{ \delta_t(s, \pi^*(s))} }.
%\end{align*}
\end{proof}


We make the following terminology for accuracy certificate.

\begin{definition}[$\epsilon$-estimator]\label{def_acc_certificate}
For any $\epsilon \geq 0$, we say that a randomized quantity $\hat{\cV}$ is an $\epsilon$-estimator of the robust value function $\cV^*$, defined in \eqref{nature_opt_as_robust_value}, in expectation (resp. in high probability) if 
$-\epsilon \leq \EE \sbr{\hat{\cV}(s)} - \cV^*(s) \leq \epsilon$ (resp. $-\epsilon \leq  {\hat{\cV}}(s) - \cV^*(s) \leq \epsilon$ with high probability) for every $s \in \cS$.
\end{definition}

With Lemma \ref{lemma_generic_prop_stoch} in place, we proceed to establish the convergence of  SFRPE in expectation. 


\begin{proposition}\label{thrm_stoch_generic_convergence_expectation}
Fix $\lambda > 0$ and set  
\begin{align}\label{param_choice_stoch_general_expectation}
\beta_k = k^{1/2},   ~ \lambda_k = (k+1)\lambda, ~\forall k \geq 0.
\end{align}
Suppose 
\begin{align}\label{stoch_expecation_conv_bias_condition}
 \norm{\EE_{|k}\hat{\cV}^{\pi_k} - \cV^{\pi_k}}_\infty \leq \varepsilon, ~ \EE_{|k} \sbr{\norm{\hat{\cV}^{\pi_k}}_\infty^2} \leq M, ~ \forall k \geq 1.
\end{align}
Then for any $k \geq 1$, 
\begin{align}\label{ineq_expecatation_sfrpe_opt_gap}
- \varepsilon 
\leq 
\EE \sbr{ \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s)} - \cV^*(s)  
& \leq  \frac{\gamma^2 \zeta^2 M^2}{\mu_w \lambda \sqrt{k} (1-\gamma)}
+  \frac{4 \lambda  \overline{w}}{\sqrt{k} (1-\gamma)}  + (\frac{2 \gamma \zeta}{1-\gamma} + 1) \varepsilon, ~ \forall s \in \cS.
\end{align}
In particular, taking $\lambda = \frac{\gamma \zeta M}{2 \sqrt{\mu_w \overline{w}}}$ yields 
%\yan{discuss on mis-specify $M$ in choosing $\lambda$ affects the convergence -- looks like linear over-estimate of $M$ leads to linear factor of the upper bound, point out to section on linear function approximation}
\begin{align*}
- \varepsilon 
\leq 
\EE \sbr{ \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s)} - \cV^*(s)  
& \leq  \frac{4 \gamma \zeta M \sqrt{\overline{w}}}{(1-\gamma) \sqrt{\mu_w k}}  + \frac{3 \varepsilon}{1-\gamma}, ~ \forall s \in \cS.
\end{align*}
%\begin{align*}
%- \varepsilon 
%\leq 
%\EE \sbr{ \tsum_{n=1}^k \theta_n 
%\hat{\cV}^{\pi_n}(s)} - \cV^*(s)  
%& \leq \rbr{\tsum_{n=1}^k \beta_n}^{-1}  \tsum_{n=1}^k \frac{\beta_n^2 \gamma^2 \zeta^2 M^2}{2 \mu_w \lambda_{n-1}}
%+  \rbr{\tsum_{n=1}^k \beta_n}^{-1} \lambda_k \overline{w}  + (2 \gamma \zeta + 1) \varepsilon.
%\end{align*} 
\end{proposition}

\begin{remark}
%In view of the choice of $\lambda$ in Proposition \ref{thrm_stoch_generic_convergence_expectation}, 
When $M$ is unknown, one can instead use an estimate $\hat{M}$ and accordingly choose $\lambda = \frac{\gamma \zeta \hat{M}}{2 \sqrt{\mu_w \overline{w}}}$.
The price for using such an estimate is an $\max \cbr{\hat{M}/M, M/\hat{M}}$ factor increase for the right-hand side of \eqref{ineq_expecatation_sfrpe_opt_gap}.
This would be particularly helpful for our discussion in Section \ref{sec_stoch_linear_approx}, when exact information of $M$ can be difficult to obtain.
\end{remark}


\begin{proof}
Given the definition of $\delta_t(s, \DD)$ in \eqref{def_noise_in_phi}, we obtain 
\begin{align*}
\abs{\EE\sbr{\delta_t(s, \DD)}} 
=\gamma \zeta  \abs{\EE\sbr{ \inner{\DD - \pi_t(s)}{\EE_{|t}\hat{\cV}^{\pi_t}_{\vartheta, s} - {\cV}^{\pi_t}_{\vartheta, s}} }}
\overset{(a)}{\leq} 2 \gamma \zeta \norm{\EE_{|t}\hat{\cV}^{\pi_t} - \cV^{\pi_t}}_\infty
= 2 \gamma \zeta \varepsilon,  ~ \forall s \in \cS, \DD \in \cD_s,
\end{align*}
where $(a)$ follows from a direct application of H\"{o}lder's inequality combined with the definition of $\hat{V}^{\pi}_{\vartheta,s}$ and ${V}^{\pi}_{\vartheta,s}$.
Taking expectation of \eqref{eq_generic_prop_stoch} in Lemma \ref{lemma_generic_prop_stoch} and applying the above inequality yields
\begin{align*}
- \varepsilon 
\leq 
\EE \sbr{ \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s)} - \cV^*(s)  
& \leq \rbr{\tsum_{t=1}^k \beta_t}^{-1}  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2 M^2}{2 \mu_w \lambda_{t-1}(1-\gamma)}
+  \rbr{\tsum_{t=1}^k \beta_t}^{-1} \frac{\lambda_k \overline{w}}{1-\gamma}  + (\frac{2 \gamma \zeta}{1-\gamma} + 1) \varepsilon.
\end{align*} 
The rest of the claims then follow from direct computations after plugging \eqref{param_choice_stoch_general_expectation} into the above inequality. 
%In particular, taking
%\begin{align*}
%\beta_0 = 0, ~ \beta_n = n^{1/2}, ~ \forall n \geq 1; ~ \lambda_n = (n+1)\lambda, ~\forall n \geq 0, 
%\end{align*}
%we obtain 
%\begin{align*}
%- \varepsilon 
%\leq 
%\EE \sbr{ \tsum_{n=1}^k \theta_n 
%\hat{\cV}^{\pi_n}(s)} - \cV^*(s)  
%& \leq  \frac{\gamma^2 \zeta^2 M^2}{\mu_w \lambda \sqrt{k}}
%+  \frac{4 \lambda  \overline{w}}{\sqrt{k}}  + (2 \gamma \zeta + 1) \varepsilon.
%\end{align*}
%Taking $\lambda = \frac{\gamma \zeta M}{2 \sqrt{\mu_w}}$, and using the fact that $\gamma, \zeta \in [0,1]$ yields 
%\begin{align*}
%- \varepsilon 
%\leq 
%\EE \sbr{ \tsum_{n=1}^k \theta_n 
%\hat{\cV}^{\pi_n}(s)} - \cV^*(s)  
%& \leq  \frac{4 \gamma \zeta M}{\sqrt{\mu_w k}}  + 3 \varepsilon.
%\end{align*}
\end{proof}


%Clearly, the above convergence guarantees hinge upon the existence of a policy evaluation operator that certifies condition \eqref{stoch_expecation_conv_bias_condition}.
%We next discuss two policy evaluation operators with this capability,
%and consequently determine the sample complexities of SFRPE when instantiated with these operators. 

Up to now, our discussion is based on the existence of stochastic evaluation operators that can certify noise condition \eqref{stoch_expecation_conv_bias_condition}.
For the remainder of our discussions we proceed to construct such evaluation operators for both tabular setting and with linear function approximation. 
Consequently one can invoke Proposition \ref{thrm_stoch_generic_convergence_expectation} to establish the output of SFRPE being an $\epsilon$-estimator of the robust value function in expectation with $\cO(\abs{\cS} /\epsilon^2)$ sample complexity.
It turns out that one can also obtain a much stronger result with essentially the same number of samples.
In particular, we show  later in this section that the output of SFRPE is an $\epsilon$-estimator of the robust value function with high probability.
This improvement seems to be important for applications of SFRPE in stochastic policy optimization of robust MDPs \cite{li2022first}.


\subsection{Tabular Setting} 

%We then proceed to introduce two policy evaluation operators that can certify condition \eqref{stoch_expecation_conv_bias_condition}.
%Consequently one can invoke Proposition \ref{thrm_stoch_generic_convergence_expectation} to obtain the sample complexity of SFRPE instantiated with these evaluation operators. 
%More importantly, we will also establish direct control over  $ { \tsum_{n=1}^k \theta_n 
%\hat{\cV}^{\pi_n}(s)} - \cV^*(s)  $ with high probability as opposed to the expectation bound in Proposition \ref{thrm_stoch_generic_convergence_expectation}.


%{\bf Simulator-based Evaluation (SE).}
The first evaluation operator presented in Algorithm \ref{alg_se}, named simulator-based evaluation (SE), 
 assumes the access to a so-called simulator of MDP $\cM_{\overline{\PP}}$, and performs an $l$-step process for estimating the value function $\cV^\pi$.
 At each step, SE generates a transition pair $(s,s')$ for each state, where $s'$ denotes the random next state upon committing to an action $a \sim \vartheta(\cdot|s)$ at the state $s$. 
 This transition pairs are then used  to construct auxiliary matrix estimates and update the estimator $\hat{\cV}^\pi$ in an incremental fashion.

\begin{algorithm}[t!]
  \caption{Simulator-based Evaluation (SE)}
  \begin{algorithmic}\label{alg_se}
    \STATE {\bf Initialize:} $R_{-1} = I$ and $\hat{\cV}^{\pi}_{-1} = 0$.
    \STATE Construct $\DD^{\pi, \vartheta} \in \RR^{\abs{\cS} \times \abs{\cS}}$ as
	$
	\DD^{\pi, \vartheta}(s, s') = \tsum_{a \in \cA} \vartheta(a|s) \DD^{\pi(s)}_a (s')    , ~ \forall (s, s') \in \cS \times \cS.
    	$
%	\yan{probably replace this also by sample}
        \FOR{ $i = 0, 1, ... l-1$}
    \STATE Set $\cD = \emptyset$. For each $s \in \cS$, commit action $a \sim \vartheta(\cdot|s)$, and collect $s'$. Save $(s, s')$ into  $\cD$.
    \STATE Construct $\hat{\PP}_i \in \RR^{\abs{\cS} \times \abs{\cS}}$ such that 
    \begin{align*}
     \hat{\PP}_i (s, s') =
    \begin{cases}
     1, ~ (s,s') \in \cD; \\
     0, ~ (s, s') \notin \cD.
     \end{cases}
     \end{align*}
    \STATE Update $R_i = R_{i-1} ((1-\zeta) \hat{\PP}_i + \zeta \DD^{\pi, \vartheta})$, and
    $
    \hat{\cV}^{\pi}_i= \hat{\cV}^{\pi}_{i-1} + \gamma^i R_i \mathfrak{C}.
   $
     \ENDFOR
%    \RETURN $\tsum_{t=1}^k \theta_t \hat{\cV}^{\pi_t}$, where $\theta_t = \beta_t / (\tsum_{t=1}^k \beta_t) $.
\RETURN $\hat{\cV}^{\pi} = \hat{\cV}^{\pi}_l$.
     \end{algorithmic}
\end{algorithm}



%The SE estimator is defined as 
%\begin{align*}
%\hat{\cV}^{\pi}(s) = - \hat{V}^{\vartheta}_{\PP^{\pi}}(s), ~ \text{where} ~ \hat{V}^{\vartheta}_{\PP^{\pi}}(s) = \tsum_{t=0}^{l} \gamma^t c(S_t, A_t).
%\end{align*}
%We make the following immediate observations.

\begin{proposition}\label{prop_se_properties}
For any fixed policy $\pi$, let $\xi$ denote the sample used by the SE operator, then  
\begin{align*}
\norm{\EE_{\xi} \hat{\cV}^{\pi} - \cV^{\pi}}_\infty \leq \frac{\gamma^l}{1-\gamma},~
\norm{\hat{\cV}^{\pi}}_\infty \leq \frac{1}{1-\gamma}.
\end{align*}
\end{proposition}

\begin{proof}
It should be clear that $\EE_{|i} \sbr{ (1-\zeta) \hat{\PP}_i + \zeta \DD^{\pi, \vartheta}} = \mathtt{P}^\pi$,
where $\mathtt{P}^{\pi}$ denotes the transition matrix of the state chain $\cbr{S_t}$ induced by $\vartheta$ within $\cM_{\PP^\pi}$, with $\PP^\pi$ defined as in \eqref{kernel_defined_by_nature_policy}.
Consequently 
from the definition of $R_i$, we obtain
\begin{align*}
\EE_\xi \sbr{R_i}  =  \rbr{ \mathtt{P}^{\pi}}^i  ~ ; ~ \norm{R_i}_\infty  \leq 1, ~ \forall 0 \leq i \leq l,
\end{align*} 
where $\norm{R_i}_\infty$ denotes the matrix $\norm{\cdot}_\infty$ norm.
In addition, one also has 
\begin{align*}
\cV^\pi = (I - \gamma \mathtt{P}^\pi)^{-1}  \mathfrak{C} =  \tsum_{i=0}^\infty \gamma^i \rbr{\mathtt{P}^{\pi}}^i \mathfrak{C}.
\end{align*}
The desired claim then follows immediately from the above two observations and $\norm{\mathfrak{C}}_\infty \leq 1$.
%Consequently, it holds that 
%\begin{align*}
%\norm{\EE_\xi \sbr{\hat{\cV}^{\pi}} - \cV^\pi}_\infty \leq \frac{\gamma^l}{1-\gamma},~
%\norm{\hat{\cV}^{\pi}} \leq \frac{1}{1-\gamma}. 
%\end{align*}
%The rest
%\begin{align*}
%\abs{\EE_\xi \hat{V}^{\vartheta}_{\PP^{\pi}}(s) - V^{\vartheta}_{\PP^{\pi}}(s)} \leq \frac{\gamma^l}{1-\gamma},
%~ \norm{\hat{V}^{\vartheta}_{\PP^{\pi}}}_\infty \leq \frac{1}{1-\gamma}.
%\end{align*}
%The rest of the claims then follows from the above relation.
\end{proof}

\begin{remark}
It should be noted that one can also avoid the construction of $\DD^{\pi, \vartheta}$ via sampling. 
Namely, in addition to the sampled $(s, a, s')$, one also samples $s'' \sim \DD^{\pi(s)}_a(\cdot)$. 
Then $\hat{\DD}^{\pi, \vartheta}_i$ can be constructed in the same way as $\PP_i$ using transition pair $(s, s'')$. 
Accordingly matrix $R_i$ is updated by $R_i = R_{i-1} ((1-\zeta) \hat{\PP}_i + \zeta \hat{\DD}_i^{\pi, \vartheta})$.
\end{remark}


%{\bf Independent Trajectories (IT).}
%IT assumes the access to a so-called simulator. 
%Specifically, for any to be evaluated policy $\pi$, IT generates a trajectory $\xi^{\pi}_{\vartheta}(s)$ of length $l$ as follows: 
%\begin{align*}
%\xi^{\pi}(s) = (S_0=s, A_0, ,  S_1, A_1, , \ldots, S_{l}, A_l ), ~ \text{where} ~ A_t \sim \vartheta(\cdot| S_t), S_{t+1} \sim \PP^{\pi}_{S_t, A_t},
%\end{align*} 
%where $\PP^{\pi}$ is defined as in \eqref{kernel_defined_by_nature_policy}.
%Let us denote $\xi = \cbr{\xi^{\pi}_{\vartheta}(s)}_{s \in \cS}$, then 
%the IT estimator is defined as 
%\begin{align*}
%\hat{\cV}^{\pi}(s) = - \hat{V}^{\vartheta}_{\PP^{\pi}}(s), ~ \text{where} ~ \hat{V}^{\vartheta}_{\PP^{\pi}}(s) = \tsum_{t=0}^{l} \gamma^t c(S_t, A_t).
%\end{align*}
%We make the following immediate observations.
%
%\begin{proposition}\label{prop_se_properties}
%For any fixed policy $\pi$, SE operator yields 
%\begin{align*}
%\norm{\EE_{\xi} \hat{\cV}^{\pi} - \cV^{\pi}}_\infty \leq \frac{\gamma^l}{1-\gamma},~
%\norm{\hat{\cV}^{\pi}}_\infty \leq \frac{1}{1-\gamma}.
%\end{align*}
%\end{proposition}
%
%\begin{proof}
%It is clear that from the definition of $\PP^{\pi}$ and $\xi$, we obtain 
%\begin{align*}
%\abs{\EE_\xi \hat{V}^{\vartheta}_{\PP^{\pi}}(s) - V^{\vartheta}_{\PP^{\pi}}(s)} \leq \frac{\gamma^l}{1-\gamma},
%~ \norm{\hat{V}^{\vartheta}_{\PP^{\pi}}}_\infty \leq \frac{1}{1-\gamma}.
%\end{align*}
%The rest of the claims follow trivially from \eqref{eq_nature_value_as_player_value}.
%\end{proof}


We are now ready to establish the sample complexity of SFRPE with the SE operator. 
As our first result, we establish an $\cO(\abs{\cS}/\epsilon^2)$ sample complexity for SFRPE to output an $\epsilon$-estimator (in expectation) of the robust value function.


\begin{theorem}\label{thrm_sample_se_expectation}
Suppose SFRPE is instantiated with the SE operator, and 
\begin{align*}
\beta_k = k^{1/2},   ~ \lambda_k =  \frac{ (k+1) \gamma \zeta }{2 (1-\gamma) \sqrt{\mu_w \overline{w}}}, ~\forall k \geq 0.
\end{align*}
For any $\epsilon > 0$, to find an approximate robust value such that 
\begin{align*}
-\epsilon \leq \EE \sbr{\tsum_{t=1}^k \theta_t \hat{\cV}^{\pi_t}}(s) - \cV^{*}(s) \leq \epsilon,  ~ s \in \cS,
\end{align*}
SFRPE needs at most $k = 1 + \frac{64 \gamma^2 \zeta^2  \overline{w}}{\mu_w (1-\gamma)^4 \epsilon^2}$ iterations.
The total number of samples can be bounded by 
\begin{align}\label{eq_num_samples_se_expectation}
{\cO} \rbr{ \frac{\gamma^2 \zeta^2 \abs{\cS} \overline{w} \log(1/\epsilon) }{\mu_w \epsilon^2 (1-\gamma)^5 } + \frac{\abs{\cS}}{1-\gamma} \log \rbr{\frac{1}{\epsilon}} }.
\end{align}
In particular, when the distance generating function $w_s(\cdot)$ is set as in \eqref{dgf_negative_entropy}, the number of samples required is bounded by 
\begin{align*}
{\cO} \rbr{ \frac{\gamma^2 \zeta^2 \abs{\cS}  \log \abs{\cS} \log(1/\epsilon) }{ \epsilon^2 (1-\gamma)^5 } + \frac{\abs{\cS}}{1-\gamma} \log \rbr{\frac{1}{\epsilon}} }.
\end{align*}
\end{theorem}


\begin{proof}
Given Lemma \ref{lemma_value_correspondence} and  Proposition \ref{thrm_stoch_generic_convergence_expectation}, it suffices to make sure 
\begin{align*}
\varepsilon \leq \frac{\epsilon(1-\gamma)}{6}, ~ \frac{4 \gamma \zeta M \sqrt{\overline{w}}}{\sqrt{\mu_w k}} \leq \frac{\epsilon(1-\gamma)}{2},
\end{align*}
which in view of Proposition \ref{prop_se_properties}, can be readily satisfied by 
\begin{align*}
l = \frac{1}{1-\gamma} \log \rbr{\frac{6(1-\gamma)^2}{\epsilon}}, ~ k = 1+ \frac{64 \gamma^2 \zeta^2 M^2 \overline{w}}{\mu_w (1-\gamma)^2 \epsilon^2} = 1 + \frac{64 \gamma^2 \zeta^2  \overline{w}}{\mu_w (1-\gamma)^4 \epsilon^2}.
\end{align*}
Consequently, the total number of samples required is bounded by 
\begin{align*}
\abs{\cS} \cdot l \cdot k = {\cO} \rbr{ \frac{\gamma^2 \zeta^2 \overline{w} \abs{\cS} \log(1/\epsilon)}{\mu_w \epsilon^2 (1-\gamma)^5 } + \frac{\abs{\cS}}{1-\gamma} \log \rbr{\frac{1}{\epsilon}} }.
\end{align*}
The rest of the claim follows from \eqref{bregman_divergence_negative_entropy_lb_ub} and \eqref{bregman_divergence_negative_entropy_sc}, from which we conclude the proof.
\end{proof}

%\yan{need some remark on how the sample complexity scales with $\zeta$, and how finite samples are needed despite have continuous action space}
In view of Theorem \ref{thrm_sample_se_expectation}, it is worth noting here that despite SFRPE being applied to solve the MDP $\mathfrak{M}$ of nature with continuous action space,
its sample complexity is independent of the action space.
 This is in sharp contrast when solving general MDPs, where linear dependence on the size of the action space is necessary.
The obtained sample complexity decomposes into two terms that clearly delineates the role of robustness in affecting the sample complexity.
In particular, the first term corresponds to the price we pay for robustness,
and the second term corresponds to the number of samples needed for estimating non-robust value function that is an $\epsilon$-estimator in expectation. 

We next establish the convergence of SFRPE instantiated with the SE operator in high probability. 

\begin{theorem}\label{thrm_stoch_se_high_prob}
Suppose SFRPE is instantiated with the SE operator, and 
\begin{align}\label{se_high_prob_param_choice}
\beta_k = k^{1/2},   ~ \lambda_k =  \frac{ (k+1) \gamma \zeta }{2 (1-\gamma) \sqrt{\mu_w \overline{w}}}, ~\forall k \geq 0.
\end{align}
Then for any $k \geq 0$ and any $\delta \in (0,1)$, with probability at least $1-\delta$ we have 
\begin{align}
& -\sbr{ \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \log \rbr{\frac{4 \abs{\cS} }{\delta}}}  \nonumber \\
 \leq &  \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s) - \cV^*(s) \nonumber \\
 \leq  & \frac{4 \gamma \zeta  \sqrt{\overline{w}}}{(1-\gamma)^2 \sqrt{\mu_w k}}
+  \frac{\gamma \zeta}{1-\gamma} 
\rbr{
\frac{2 \gamma^l}{1-\gamma} 
+ \frac{8}{(1-\gamma) \sqrt{k}} \sqrt{  \log(\frac{2 \abs{\cS} }{\delta})}
}
+ 
 \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \sqrt{ \log \rbr{\frac{4 \abs{\cS} }{\delta}}}. \label{high_prob_err_bound_se}
\end{align}
Moreover, the total number of samples required by SFRPE to output $-\epsilon \leq \tsum_{t=1}^k \theta_t 
\hat{\cV}^{\pi_t}(s) - \cV^*(s) \leq \epsilon$ with at least probability $1-\delta$  is bounded by 
\begin{align}\label{se_sample_high_prob}
{\cO} \rbr{
\frac{\gamma^2 \zeta^2 \overline{w} \abs{\cS} \log(1/\epsilon) }{(1-\gamma)^5 \mu_w \epsilon^2} \log \rbr{\frac{\abs{\cS}}{\delta}} 
+ \frac{\abs{\cS}  \log(1/\epsilon)}{(1-\gamma)^3 \epsilon^2} \log \rbr{\frac{\abs{\cS}}{\delta}} 
}.
\end{align}
In particular, when the distance generating function $w_s(\cdot)$ is set as in \eqref{dgf_negative_entropy},  the total number of samples required can be bounded by 
\begin{align*}
{\cO} \rbr{
\frac{\gamma^2 \zeta^2  \abs{\cS} \log \abs{\cS}  \log(1/\epsilon)}{(1-\gamma)^5  \epsilon^2} \log \rbr{\frac{\abs{\cS}}{\delta}} 
+ \frac{\abs{\cS}  \log(1/\epsilon)}{(1-\gamma)^3 \epsilon^2} \log \rbr{\frac{\abs{\cS}}{\delta}} 
}.
\end{align*}
\end{theorem}

\begin{proof}
%Let us first consider bounding 
%$\tsum_{t=1}^k \theta_t \rbr{\hat{\cV}^{\pi_t}(s) - \cV^{\pi_t}(s)}$.
%It can be directly verified that 
%\begin{align}\label{eq_opt_error_accumulation}
%\rbr{\tsum_{t=1}^k \beta_n}^{-1}  \tsum_{t=1}^k \frac{\beta_n^2 \gamma^2 \zeta^2  \norm{\hat{\cV}^{\pi_t}}_\infty^2}{2 \mu_w \lambda_{n-1} (1-\gamma)} \leq  \frac{4 \gamma \zeta M \sqrt{\overline{w}}}{(1-\gamma) \sqrt{\mu_w k}}.
%\end{align}
Fixing $s \in \cS$, for any $\delta > 0$, from Proposition \ref{prop_se_properties}, applying Azuma–Hoeffding inequality yields 
%\yan{expand this, skipped definition of $\theta_t$ in the computation, need to follow the second part}
\begin{align*}
\abs{ \tsum_{t=1}^k \theta_t \rbr{\hat{\cV}^{\pi_t}(s) - \cV^{\pi_t}(s)}  } 
 \leq 
\tsum_{t=1}^k  \frac{\theta_t \gamma^l}{1-\gamma} 
+ \frac{1}{1-\gamma} \sqrt{2 \tsum_{t=1}^k \theta_t^2 \log(\frac{4}{\delta})}
 \overset{(a)}{\leq} \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \sqrt{\log \rbr{\frac{4}{\delta}}}, 
\end{align*}
with probability $1 - \delta/ 2$,
where $(a)$ follows from the definition of $\theta_t$ in \eqref{def_theta} together with $\beta_n = n^{1/2}$.
Applying union bound  yields 
\begin{align}\label{se_high_prob_accum_noise_1}
\abs{ \tsum_{t=1}^k \theta_t \rbr{\hat{\cV}^{\pi_t}(s) - \cV^{\pi_t}(s)} }\leq \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \log \rbr{\frac{4 \abs{\cS} }{\delta}}, ~ \forall s \in \cS, 
\end{align}
with probability $1 - \delta/ 2$.
%We proceed to bound $\tsum_{t=1}^k \frac{\theta_t}{1-\gamma}  \EE_{s' \sim d_{s}^{\pi^*}} \sbr{ \delta_t(s', \pi^*(s'))} $.
Fixing $\DD \in \cD_s$, by definition, we have 
\begin{align*}
\delta_t(s,\DD) = \gamma \zeta \tsum_{a \in \cA}  \vartheta(a|s) \inner{\DD_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t} - \cV^{\pi_t}}.
\end{align*}
From Proposition \ref{prop_se_properties}, it is immediate that 
\begin{align*}
\abs{\EE_{|t}  \tsum_{a \in \cA}  \vartheta(a|s)   \inner{\DD_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t} - \cV^{\pi_t}}} \leq \frac{2 \gamma^l}{1-\gamma} ,~
 \abs{  \tsum_{a \in \cA}  \vartheta(a|s) \inner{\DD_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t} - \cV^{\pi_t}}} \leq \frac{2}{1-\gamma}.
\end{align*}
Consequently, applying Azuma–Hoeffding inequality yields
\begin{align*}
\tsum_{t=1}^k   \theta_t   \tsum_{a \in \cA}  \vartheta(a|s) \inner{\DD_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t} - \cV^{\pi_t}}
\leq \frac{2 \gamma^l}{1-\gamma} 
+ \frac{2}{1-\gamma} \sqrt{2 \tsum_{t=1}^k \theta_t^2 \log(\frac{2}{\delta})} 
\leq 
\frac{2 \gamma^l}{1-\gamma} 
+ \frac{8}{1-\gamma} \sqrt{\log(\frac{2}{\delta})} ,
\end{align*}
with probability $1- {\delta}/{2}$, for any $s \in \cS$.
Applying union bound again, we obtain 
\begin{align*}
\tsum_{t=1}^k  \theta_t   \tsum_{a \in \cA}  \vartheta(a|s) \inner{\DD_a - \DD^{\pi_t(s)}_a}{\hat{\cV}^{\pi_t} - \cV^{\pi_t}}
\leq \frac{2 \gamma^l}{1-\gamma} 
+  \frac{8}{1-\gamma} \sqrt{\log(\frac{2 \abs{\cS}}{\delta})} , ~\forall s \in \cS , 
%+ \frac{2}{1-\gamma} \sqrt{2 \tsum_{t=1}^k \theta_t^2 \log(\frac{2 \abs{\cS} \abs{\cA}}{\delta})}, ~ \forall (s,a) \in \cS \times\cA, 
\end{align*}
with probability $1- {\delta}/{2}$.
Setting $\DD = \pi^*(s)$ in the above inequality yields  
\begin{align}
\tsum_{t=1}^k \frac{\theta_t}{1-\gamma}  \EE_{s' \sim d_{s}^{\pi^*}} \sbr{ \delta_t(s', \pi^*(s'))}
%&\leq 
%\frac{\gamma \zeta}{1-\gamma} 
%\rbr{
%\frac{2 \gamma^l}{1-\gamma} 
%+ \frac{2}{1-\gamma} \sqrt{2 \tsum_{t=1}^k \theta_t^2 \log(\frac{2 \abs{\cS} \abs{\cA}}{\delta})}
%}
% \nonumber  \\
 & \leq 
 \frac{\gamma \zeta}{1-\gamma} 
\rbr{
\frac{2 \gamma^l}{1-\gamma} 
+ \frac{8}{(1-\gamma) \sqrt{k}} \sqrt{  \log(\frac{2 \abs{\cS} }{\delta})}
}. \label{se_high_prob_accum_noise_2}
\end{align}
%where the last inequality follows again from $\beta_n = n^{1/2}$ and the definition of $\theta_t$.
Substituting \eqref{se_high_prob_param_choice}, \eqref{se_high_prob_accum_noise_1}, and \eqref{se_high_prob_accum_noise_2}
into \eqref{eq_generic_prop_stoch}, and noting that $\norm{\hat{\cV}^{\pi_t}}_\infty \leq \frac{1}{1-\gamma}$ therein yields \eqref{high_prob_err_bound_se}.
Finally, \eqref{se_sample_high_prob} follows from a similar argument as in Theorem \ref{thrm_sample_se_expectation} and applying \eqref{high_prob_err_bound_se} and Proposition~\ref{prop_se_properties}.
%and using $\beta_n = n^{1/2}$, $\lambda_n = (n+1) \lambda$ with $\lambda = \frac{\gamma \zeta M}{2 \sqrt{\mu_w \overline{w}}}$ and $M = \frac{1}{1-\gamma}$,
%we obtain
%\begin{align*}
%& -\sbr{ \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \log \rbr{\frac{4 \abs{\cS} }{\delta}}} \\
% \leq &  \tsum_{t=1}^k \theta_t 
%\hat{\cV}^{\pi_t}(s) - \cV^*(s) \nonumber \\
% \leq  & \frac{4 \gamma \zeta  \sqrt{\overline{w}}}{(1-\gamma)^2 \sqrt{\mu_w k}}
%+  \frac{\gamma \zeta}{1-\gamma} 
%\rbr{
%\frac{2 \gamma^l}{1-\gamma} 
%+ \frac{8}{(1-\gamma) \sqrt{k}} \sqrt{  \log(\frac{2 \abs{\cS} \abs{\cA}}{\delta})}
%}
%+ 
% \frac{\gamma^l}{1-\gamma} + \frac{4}{(1-\gamma)\sqrt{k}} \log \rbr{\frac{4 \abs{\cS} }{\delta}}.
%\end{align*}
%The proof is then completed.
\end{proof}

A few remarks are in order for interpreting Theorem \ref{thrm_stoch_se_high_prob}.
First, it is clear that the sample complexity in \eqref{se_sample_high_prob} is comparable to that of \eqref{eq_num_samples_se_expectation}, while the accuracy certificate is now stated with high probability instead of expectation. 
Second, similar to Theorem \ref{thrm_sample_se_expectation}, the sample complexity in \eqref{se_sample_high_prob} consists of two terms of different roots.
The first term corresponds to the price of robustness,
and the second term corresponds to the number of samples required for estimating the standard value function up to $\epsilon$-accuracy with high probability. 
Clearly, when $\zeta =0$ the established sample complexities are tight for evaluating the standard value function in both expectation and in high probability.
It is also important to note that 
in view of \eqref{se_sample_high_prob},  when $\zeta = \cO (1-\gamma)$,  the robust value function can be evaluated with the same number of samples as for evaluating standard value function. 
Consequently there is no additional price of robustness for robust policy evaluation with small-sized ambiguity sets.


The development of SFRPE in this section appears to be new in several aspects. 
The convergence of SFRPE  is established in both expectation and high probability, while existing development of stochastic robust policy evaluation only establish convergence in expectation for mean-squared error \cite{li2022first}. 
Additionally, the impact of the ambiguity set size on the sample complexity has not been previously reported.
%In addition, the size of the ambiguity set in affecting the sample complexity has not been reported before.  
Though these results already hint upon potential benefits of the SFRPE  framework, in the next section we proceed to demonstrate its true advantage of scaling robust policy evaluation to large-scale problems, 
a scenario that appears yet to have an algorithmic solution.
%where no previous algorithmic solution exists.

%
%\yan{need some remark on how the sample complexity scales with $\zeta$,
%\begin{itemize}
%\item the first term of the sample complexity corresponds to the price we pay for robustness -- notably when $\zeta = \cO(1-\gamma)$ there is no price to pay for robustness!
%\item the second term corresponds to the one for standard evaluation 
%\end{itemize}
%}
%
%
%\yan{comparison to prior work: this is the first work where linear dependence on state space size can be obtained. previous approach, aside from making strong assumptions, is inherently subject to the exploration issue over state space}
%
%
%


































