%!TEX root = ./robust_pe.tex


\section{Concluding Remarks}\label{sec_conclusion}

We introduce a first-order method, named FRPE, applied to robust policy evaluation problem by taking a policy optimization viewpoint.
For offline robust MDPs, FRPE attains linear convergence, and is compatible with function approximation for large-scale MDPs. 
For online robust MDPs, we further develop a stochastic variant that attains optimal $\cO(1/\epsilon^2)$ sample complexity.
The obtained sample complexities also delineate the clear role of robustness when performing policy evaluation. 
Notably these sample complexities are established for both tabular setting and for large-scale problems where linear approximation is applied. 
We conclude by discussing the application of FRPE in the context of stochastic policy optimization applied to robust MDPs. 
It appears to us that the FRPE framework overcomes long-standing challenges of robust policy evaluation and opens up a new spectrum of potential directions. 
We now discuss a few of them as follows. 

First, though we mainly presents linear function approximation for stochastic robust policy evaluation,  it should be noted stochastic FRPE can be combined with generic function approximation scheme as long as the stochastic evaluation of $\cV^\pi$ can be performed efficiently.
As an example one can consider employing over-parameterized neural networks in SLPE. With recent advances in showing their global convergence, it appears that a polynomial sample complexity can be attained. 

Second, the proposed LSPE for estimating $\cV^\pi$ requires the simulator access. This seems to be natural for online robust MDPs motivated by sim-to-real problems (cf. Example \ref{online_robust_mdp}), where a simulator is inherently available. 
As the estimation of $\cV^\pi$ can be essentially viewed as an off-policy evaluation problem (cf., \eqref{eq_nature_value_as_player_value}), one can also apply existing off-policy evaluation methods that can be implemented without simulator access while being compatible with linear function approximation \cite{sutton2008convergent, sutton2009fast}. 
To attain an $\tilde{\cO}(1/\epsilon^2)$ sample complexity for stochastic FRPE, it is important to develop two properties of the method. 
Namely the fast bias reduction (Lemma \ref{lemma_bias_linear}) and the high probability control on the boundedness of the iterate (Lemma \ref{lemma_norm_bound_high_prob}). 
%For this purpose we believe a single time-scale modification is required before adopting the aforementioned methods that are two-timescale in nature.


Third, while  $\cO(1/\epsilon^2)$ samples suffices for stochastic FRPE  to output an $\epsilon$-estimator of the robust value function in expectation, it remains to be seen whether this sample complexity is tight. 
In particular, when $\zeta = 0$, and the robust policy evaluation reduces to standard policy evaluation,  it can be seen from Theorem \ref{thrm_sample_se_expectation} and \ref{thrm_lspe_expectation} that only $\cO(\log(1/\epsilon))$ samples are needed. 
Whether the phase transition between robust and standard policy evaluation exists could be impactful for potentially improving the sample complexity of stochastic policy optimization applied to robust MDPs.
We refer interested readers to \cite{li2022first} for related~discussions. 



Finally, it is also highly rewarding to extend FRPE to a broader class of ambiguity sets  beyond $\mathrm{s}$- and $(\mathrm{s}, \mathrm{a})$-rectangularity \cite{mannor2016robust, goyal2023robust},
and to settings where ambiguity sets admit more complex structure than \eqref{def_ambiguity_set_structure}. 
%and discuss their applications in the context of policy optimization.

%\yan{
%Potential directions:
%\begin{itemize}
%\item the framework allows using more advanced function approximation -- neural networks (Zhaoran's paper)
%\item purely online evaluation method for LSPE, for example, GTD 
%\item whether bias can be shown to converge faster 
%\item application of the results in policy optimization methods
%\item other ambiguity sets
%\end{itemize}
%
%}