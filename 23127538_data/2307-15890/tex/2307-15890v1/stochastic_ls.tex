%!TEX root = ./robust_pe.tex


\subsection{SFRPE with Linear Function Approximation}\label{sec_stoch_linear_approx}





Unless stated otherwise, going forward we let $\norm{\cdot} = \norm{\cdot}_2$.
For robust MDP with large state space, exact policy evaluation becomes prohibitive. 
In this case one can instead seek to learn a linearly parameterized $\cV^{\pi}_{\theta}(\cdot) \coloneqq   \psi(\cdot)^\top \theta$ that approximate $\cV^{\pi}(\cdot)$ well, 
where $\psi: \cS \to \RR^d$ is the so-called feature map, and we assume without loss of generality that $\norm{\psi(\cdot)}\leq 1$.
In view of Lemma \ref{lemma_value_correspondence}, this is equivalent to approximation of $V^{\vartheta}_{\PP^\pi}$ by $- \psi(\cdot)^\top \theta$. 
As $V^{\vartheta}_{\PP^\pi}$ itself is the value function of $\vartheta$ within MDP $\cM_{\PP^\pi}$, we consider solving\footnote{
Interested readers might suggest directly learning the robust value function of $V^{\vartheta}_r$ -- the ultimate goal of this manuscript -- by simply formulating a least-squares objective that fits the mean-squared robust  Bellman error. 
Though intuitively appealing, this perspective can be computationally intractable. Namely, one can easily construct a $\mathrm{s}$-rectangular robust MDP instance where the resulting  least-squares objective of robust mean-squared Bellman error  is non-convex even in the tabular setting.
Accordingly one can only seek to find the stationary point of the least-square objective  \cite{roy2017reinforcement}.
}: 
\begin{align}\label{ls_objective}
\textstyle
\min_{\theta \in \RR^d} \cbr{g(\theta) \coloneqq  \norm{\Psi \theta - \gamma \mathtt{P}^\pi \Psi \theta - \mathfrak{C}}_{\nu}^2 },
\end{align}
which corresponds to the mean-squared Bellman error of $\vartheta$ within MDP $\cM_{\PP^\pi}$.
Here $\Psi \in \RR^{\abs{\cS} \times d}$ denotes the feature matrix induced by $\psi$ applied to every state, and $\mathtt{P}^{\pi}$ denotes the  transition matrix of the state chain $\cbr{S_t}$ induced by $\vartheta$ within $\cM_{\PP^\pi}$.
It is important to note that here we do not know $\mathtt{P}^\pi$.
Instead, we only assume the access to sample from $\cM_{\overline{\PP}}$.
We introduce the following standard assumption on the $\nu$ and $\psi$. 




\begin{algorithm}[t]
  \caption{Stochastic Least-squares Policy Evaluation (SLPE)}
  \begin{algorithmic}\label{alg_lspe}
    \STATE {\bf Input:} Stepsizes $\cbr{\eta_t}$.
    \STATE {\bf Initialize:} $\theta_0 \in \RR^d$.
        \FOR{ $t = 0, 1, ... T-1$}
    \STATE Sample $s_t \sim \nu$, commit action $a_t \sim \vartheta(\cdot |s_t)$. Sample independent $x_t, x_t' \sim \overline{\PP}_{s_t, a_t}$
    and $y_t, y_t' \sim \DD^{\pi(s_t)}_{a_t}$. 
    \STATE Update:
    \begin{align*}
    \theta_{t+1} = \theta_t - \eta_t \sbr{
    \psi(s_t)^\top \theta_t - \gamma \rbr{(1-\zeta) \psi(x_t') + \zeta \psi(y_t') }^\top \theta_t - \mathfrak{C}(s_t)
    }
    \sbr{
    \psi(s_t) - \gamma \rbr{ (1-\zeta) \psi(x_t) +  \zeta \psi(y_t) }
    }
    \end{align*}
     \ENDFOR
%    \RETURN $\tsum_{t=1}^k \theta_t \cV^{\pi_t}$, where $\theta_t = \beta_t / (\tsum_{t=1}^k \beta_t) $.
\RETURN $\theta_T$, and $\hat{\cV}^{\pi}(\cdot) = \cV^{\pi}_{\theta_T}(\cdot) \coloneqq  \psi(\cdot)^\top \theta_T$.
     \end{algorithmic}
\end{algorithm}



%\yan{should mention that directly using robust bellman operator to form least square objective leads to non-convex objective}



\begin{assumption}\label{assump_sampling_and_feature}
The distribution $\nu$ has full support and the feature matrix $\Psi \in \RR^{\abs{\cS} \times d}$ is non-singular.
That is, $\diag(\nu) \succ 0$ and $\sigma_{\min}(\Psi) > 0$.
\end{assumption}

Following Assumption \ref{assump_sampling_and_feature},
it holds that $\mu = \lambda_{\min} ( \Psi^\top (I - \gamma \mathtt{P}^{\pi})^\top \diag(\nu) (I - \gamma \mathtt{P}^{\pi}) \Psi) ) > 0$.
We also denote $L = \lambda_{\max} \rbr{ \Psi^\top (I - \gamma \mathtt{P}^{\pi})^\top \diag(\nu) (I - \gamma \mathtt{P}^{\pi}) \Psi}$.
Let us denote $\theta^\pi$ as the unique solution of \eqref{ls_objective}, and use 
\begin{align*}
\textstyle
\varepsilon_{\mathrm{approx}} \coloneqq \sup_{\pi \in \Pi} \norm{ \cV^{\pi}_{\theta^\pi} - \cV^{\pi} }_\infty
%= \sup_{\pi \in \Pi} \norm{ V^{\vartheta}_{\PP^\pi} - \Psi \theta^{\pi} }_\infty
\end{align*}
 to characterize the function approximation error of $\cV^{\pi}$. 
Clearly, when $\Psi = I_{\abs{\cS}}$, we have $\varepsilon_{\mathrm{approx}} = 0$.
Our ensuing discussions will often make use of the following quantities to simplify presentation:
\begin{align*}
\textstyle
%c_1 = 4 \norm{\theta^{\pi}} + 2, 
r_{\Theta} = \max_{\pi \in \Pi} \norm{\theta^\pi} , 
~
c_1 = 4 r_{\Theta} + 2 .
\end{align*}
With Assumption \ref{assump_sampling_and_feature} it holds that $r_{\Theta} < \infty$ as $\theta^{\pi}$ is a continuous mapping from $\Pi$ to $\RR^d$.


The stochastic least-squares policy evaluation (SLPE) method (Algorithm \ref{alg_lspe}) can be viewed as solving \eqref{ls_objective} by stochastic gradient descent. 
It can be seen that SLPE requires a simulator of $\cM_{\PP^\pi}$ to draw sample $s_t$, a condition that mainly serves to keep the technical discussion concise. With a slightly more complex analysis  one can also sample $s_t$ by following the trajectory of $\vartheta$ within $\cM_{\PP^\pi}$  \cite{kotsalis2020simple}. 


\begin{lemma}
Define operator $F: \RR^d \to \RR^d$ as 
\begin{align*}
F(\theta) = \rbr{(I - \gamma \mathtt{P}^{\pi}) \Psi}^\top \diag(\nu) (\Psi \theta - \gamma \mathtt{P}^{\pi} \Psi \theta - \mathfrak{C}).
\end{align*}
%Suppose $\diag(\nu) \succ 0$ and $\sigma_{\min}(\Psi) > 0$, then  
Then   $F(\theta^{\pi}) = 0$, and
\begin{align}
\inner{F(\theta)}{\theta - \theta^{\pi}}   \geq  \mu \norm{\theta - \theta^{\pi}}^2. \label{ctd_monotone}
\end{align}
%where  $F(\theta^{\pi}) = 0$.
%Given Assumption \ref{assump_ergodic} we also have $\mu > 0$, $M \succ \mathbf{0}$ and $\theta^{\pi}$ being unique.
\end{lemma}
\begin{proof}
The first part of the claim directly follows from the optimality condition of \eqref{ls_objective}.
In addition, 
\begin{align}
\inner{F(\theta)}{\theta - \theta^{\pi}}
& = \inner{F(\theta) - F(\theta^{\pi})}{\theta - \theta^{\pi}} \nonumber \\
& = \inner{\Psi^\top (I - \gamma \mathtt{P}^{\pi})^\top \diag(\nu)(I-\gamma \mathtt{P}^\pi) \Psi (\theta - \theta^{\pi})}{\theta - \theta^{\pi}} \nonumber \\
& \geq  \mu \norm{\theta - \theta^{\pi}}^2, \nonumber
\end{align}
where the last inequality follows from the definition of $\mu$.
\end{proof}


We proceed by characterizing each step of SLPE and establish the boundedness of iterates in expectation. 

\begin{lemma}\label{lemma_ctd_recursion}
%Define $c_1 = 4 \norm{\theta^{\pi}} + 2$, then
We have
%\begin{align*}
%L= \sigma_{\max}\rbr{\Psi^\top (I - \gamma \mathtt{P}^\pi) \Psi} , ~ & \kappa_2  = \sigma_{\max} \rbr{\Psi^\top (I-\gamma \mathtt{P}^\pi)},  
%\kappa_3 =  \lambda_{\max} \rbr{\Psi^\top \Psi} + 1, \\
%c_1 = 2 \norm{\theta^{\pi}} + 1, 
%~ & \varepsilon_{\mathrm{approx}} = \sup_{\pi \in \Pi} \norm{\Psi \theta^{\pi} - V^{\vartheta}_{\PP^{\pi}}},
%\end{align*}
%where $V^{\vartheta}_{{\PP}^\pi}$ denotes the value function of $\vartheta$ within $\cM_{\mathtt{P}^\pi}$.
\begin{align}
 \EE \sbr{\norm{\theta_{t+1} - \theta^{\pi}}^2 }\leq 
 \rbr{
1 - 2 \eta_t \mu + 32 \eta_t^2  
}
\EE \sbr{\norm{ \theta_t - \theta^{\pi}}^2} + 2 \eta_t^2 c_1^2 .
\label{convergence_mse}
\end{align}
In particular, setting $\eta_t = \eta \leq \frac{\mu}{32}$ yields
%\begin{align}\label{param_for_norm_bound}
%\eta_t = \eta \leq \frac{\mu}{32}.
%%~ (L + \kappa_2) m \rho^\tau \leq \frac{\mu}{4}.
%%T \geq  \frac{1}{\eta \mu} \log 2.
%\end{align}
%Then we have 
\begin{align}\label{mse_bound_each_epoch_iter}
\EE \sbr{\norm{{\theta}_t - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
c_1^2.
%+ \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
\end{align}
 
\end{lemma}

\begin{proof}
Clearly,  each update in SLPE (Algorithm \ref{alg_lspe}) is equivalent to the following
\begin{align}\label{ctd_update_equiv_form}
\textstyle
\theta_{t+1} = \argmin_{\theta \in \RR^d} \eta_t \inner{\hat{F}_t(\theta_t)}{\theta} - \frac{1}{2} \norm{\theta - \theta_t}^2,
\end{align}
where 
$
\hat{F}_t (\theta) =  \sbr{
    \psi(s_t)^\top \theta - \gamma \rbr{(1-\zeta) \psi(x_t') + \zeta \psi(y_t') }^\top \theta - \mathfrak{C}(s_t)
    }
    \sbr{
    \psi(s_t) - \gamma \rbr{ (1-\zeta) \psi(x_t) +  \zeta \psi(y_t) }
    }
$.
From the optimality condition of \eqref{ctd_update_equiv_form}, we obtain 
\begin{align}\label{ctd_three_point}
\eta_t \inner{\hat{F}_t(\theta_t)}{ \theta_t - \theta} + \eta_t \inner{\hat{F}_t (\theta_t)}{\theta_{t+1} - \theta_t}
+ \frac{1}{2} \norm{\theta_{t+1} - \theta_t}^2 \leq \frac{1}{2} \norm{\theta - \theta_t}^2 - \frac{1}{2} \norm{\theta - \theta_{t+1}}^2.
\end{align}
We now make the following two observations. First,  from the definition of $(s_t, x_t, x_t', y_t, y_t')$ it is clear that 
\begin{align*}
 \EE_{|t} \sbr{ \hat{F}_t(\theta) }
 = \EE_{|s_t, t} \big[ \sbr{\psi(s_t)^\top \theta - \gamma  \mathtt{P}^\pi_{s_t, \cdot} \Psi \theta - \mathfrak{C}(s_t)}
 \sbr{\psi(s_t) - \gamma \mathtt{P}^\pi_{s_t, \cdot} \Psi } \big]
 = F(\theta), ~ \forall \theta \in \RR^d,
\end{align*}
where $\EE_{|t} \sbr{\cdot}$ denotes the conditional expectation with respect to the $\sigma$-algebra up to (excluding) iteration $t$.
Consequently, by denoting $\delta_t = \hat{F}_t(\theta_t) - F(\theta_t)$, then
\begin{align}
\EE_{|t} \sbr{\delta_t} = 0.
 \label{bias_conditional_expectation}
\end{align}
%where $\EE_{|t} \sbr{\cdot}$ denotes the conditional expectation with respect to the $\sigma$-algebra up to (excluding) iteration $t$ of epoch $e$.
%where 
%\begin{align*}
%L= \sigma_{\max}\rbr{\Psi^\top (I - \gamma \mathtt{P}^\pi) \Psi} , ~ \kappa_2 = \sigma_{\max} \rbr{\Psi^\top (I-\gamma \mathtt{P}^\pi)},  ~ \varepsilon_{\mathrm{approx}} = \norm{\Psi \theta^{\pi} - V^{\vartheta}_{{\PP}^\pi}}.
%\end{align*}
Second, we have 
$ \hat{F}_t(\theta_t)  = \hat{F}_t(\theta_t) - \hat{F}_t(\theta^{\pi}) + \hat{F}_t(\theta^{\pi})  $,
and 
\begin{align}
\norm{ \hat{F}_t(\theta_t) - \hat{F}_t(\theta^{\pi})}
& = \norm{
\sbr{\psi(s_t) -  \gamma \rbr{(1-\zeta) \psi(x_t') + \zeta\psi(y_t')}}^\top  (\theta_t - \theta^\pi)  \sbr{
    \psi(s_t) - \gamma \rbr{ (1-\zeta) \psi(x_t) +  \zeta \psi(y_t) }
    }
} \nonumber  \\
& \leq 4 \norm{\theta_t - \theta^\pi},  \label{norm_bound_diff_stoch_op} \\
\norm{\hat{F}_t(\theta^{\pi})}
& = \norm{
\sbr{
    \psi(s_t)^\top \theta^\pi - \gamma \rbr{(1-\zeta) \psi(x_t') + \zeta \psi(y_t') }^\top \theta^\pi - \mathfrak{C}(s_t)
    }
    \sbr{
    \psi(s_t) - \gamma \rbr{ (1-\zeta) \psi(x_t) +  \zeta \psi(y_t) }
    }
} \nonumber \\
& \leq 4 \norm{\theta^\pi} + 2. \label{norm_bound_stoch_op}
\end{align}
Hence it follows that 
\begin{align*}
\inner{\hat{F}_t(\theta_t)}{\theta_{t+1} - \theta_t} 
& = \inner{  \hat{F}_t(\theta_t) - \hat{F}_t(\theta^{\pi}) + \hat{F}_t(\theta^{\pi})}{\theta_{t+1} - \theta_t} \nonumber \\
& \geq 
- 4 \norm{\theta_t - \theta^{\pi}} \norm{\theta_{t+1} - \theta_t} 
- \norm{\hat{F}_t(\theta^{\pi})} \norm{\theta_{t+1} - \theta_t} \nonumber  \\
& \geq 
-( 4 \norm{\theta_t - \theta^{\pi}} + c_1) \norm{\theta_{t+1} - \theta_t} . 
%\label{ctd_subgrad_inner}
\end{align*}
%where 
%\begin{align*}
%\kappa_3 =  \lambda_{\max} \rbr{\Psi^\top \Psi} + 1,~
%c_1 = 2 \norm{\theta^{\pi}} + 1.
%\end{align*}
Substituting  the above relation into \eqref{ctd_three_point} yields 
\begin{align*}
\eta_t \inner{\hat{F}_t(\theta_t)}{ \theta_t - \theta} - \eta_t ( 4 \norm{\theta_t - \theta^{\pi}} + c_1) \norm{\theta_{t+1} - \theta_t}
+  \frac{1}{2} \norm{\theta_{t+1} - \theta_t}^2 \leq \frac{1}{2} \norm{\theta - \theta_t}^2 - \frac{1}{2} \norm{\theta - \theta_{t+1}}^2,
\end{align*}
which after applying Cauchy-Schwarz inequality and the definition of $\delta_t$, gives 
\begin{align*}
\eta_t \inner{{F}(\theta_t)}{ \theta_t - \theta} - 16 \eta_t^2  \norm{\theta_t - \theta^{\pi}}^2
- \eta_t^2 c_1^2
 \leq \frac{1}{2} \norm{\theta - \theta_t}^2 - \frac{1}{2} \norm{\theta - \theta_{t+1}}^2
 - \eta_t \inner{\delta_t}{\theta_t - \theta}.
\end{align*}
 Setting $\theta = \theta^{\pi}$, and further plugging \eqref{ctd_monotone} into the above relation yields 
\begin{align}\label{norm_recursion_with_noise}
\frac{1}{2} \norm{\theta_{t+1} - \theta^{\pi}}^2 
\leq (\frac{1}{2} - \eta_t \mu + 16 \eta_t^2 ) \norm{\theta_t - \theta^{\pi}}^2
+ \eta_t^2 c_1^2 +  \eta_t \inner{\delta_t}{\theta_t - \theta^{\pi}}.
\end{align}
Taking expectation on both sides and applying  \eqref{bias_conditional_expectation}, we obtain \eqref{ctd_update_equiv_form}.
 Finally, recursive application of \eqref{convergence_mse}  with $\eta_t = \eta \leq \frac{\mu}{32}$ yields 
\begin{align*}
%\label{convergence_mse_clean}
\EE \sbr{\norm{\theta_t - \theta^{\pi}}^2 }
\leq (1-\eta \mu)^t  \EE \sbr{ \norm{\theta_{0} - \theta^{\pi}}^2 } + \frac{2 \eta c_1^2}{\mu} ,
%+ \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}},
\end{align*}
from which the desired claim follows.
%\begin{align*}
%\frac{1}{2} \EE \sbr{\norm{\theta_{t+1} - \theta^{\pi}}^2 }\leq 
% \rbr{
%\frac{1}{2} - \eta_t \mu + 16 \eta_t^2   
%}
%\EE \sbr{\norm{ \theta_t - \theta^{\pi}}^2} + \eta_t^2 c_1^2.
%%\label{convergence_mse}
%\end{align*}
%Further dividing both sides by $\Gamma_{t+1}$, where 
%\begin{align*}
%\Gamma_t = \begin{cases}
%1, ~ & t = 0; \\
%\Gamma_{t-1}  \rbr{1 - 2 \eta_t \mu + 2 \eta_t^2 \kappa_3^2 + 2 \eta_t m (L + \kappa_2) \rho^\tau }, & t \geq 1,
%\end{cases}
%\end{align*}
%and taking the telescopic sum of the resulting inequality, we obtain 
%\begin{align*}
%\frac{1}{2 \Gamma_k} \EE \sbr{ \norm{\theta_k - \theta^{\pi}}^2 } \leq \frac{1}{2} \norm{\theta_0 - \theta^{\pi}}^2 + \tsum_{t=0}^{k-1} \frac{\eta_t^2}{\Gamma_{t+1}} c_1^2
%+ \tsum_{t=0}^{k-1} \frac{\eta_t}{\Gamma_{t+1}} \kappa_2 \rho^\tau \varepsilon^2_{\mathrm{approx}}.
%\end{align*}
%Hence we conclude that 
%\begin{align*}
%\EE \sbr{ \norm{\theta_k - \theta^{\pi}}^2 }
% \leq \Gamma_k  \norm{\theta_0 - \theta^{\pi}}^2
% + 2 \Gamma_k c_1^2 \tsum_{t=0}^{k-1} \frac{\eta_t^2}{\Gamma_{t+1}} 
% + 2 \Gamma_k  \kappa_2 m \rho^\tau \varepsilon^2_{\mathrm{approx}} \tsum_{t=0}^{k-1} \frac{\eta_t}{\Gamma_{t+1}} .
%\end{align*}
\end{proof}

%The next lemma establishes the boundedness of $\norm{ \theta_t - \theta^{\pi}}^2$ in expectation.


%We proceed to show that $ \EE \sbr{\norm{\theta_t - \theta^{\pi}}^2}$ is indeed bounded with proper parameter specifications.
%
%\begin{lemma}\label{lemma_norm_bound_in_expectation}
%Set 
%\begin{align}\label{param_for_norm_bound}
%\eta_t = \eta \leq \frac{\mu}{32}.
%%~ (L + \kappa_2) m \rho^\tau \leq \frac{\mu}{4}.
%%T \geq  \frac{1}{\eta \mu} \log 2.
%\end{align}
%Then we have 
%\begin{align}\label{mse_bound_each_epoch_iter}
%\EE \sbr{\norm{{\theta}_t - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
%\frac{2 \eta c_1^2}{\mu} .
%%+ \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%\end{align}
%\end{lemma}
%
%\begin{proof}
%%We proceed to show that $ \EE \sbr{\norm{\theta_t - \theta^{\pi}}^2}$ is indeed bounded with proper parameter specifications.
%%Let 
%%\begin{align}\label{param_for_norm_bound}
%%\eta_t = \eta \leq \frac{\mu}{4 \kappa_3^2}, ~ (L + \kappa_2) m \rho^\tau \leq \frac{\mu}{4},
%%\end{align}
% Recursive application of \eqref{convergence_mse} combined with \eqref{param_for_norm_bound} yields 
%\begin{align}\label{convergence_mse_clean}
%\EE \sbr{\norm{\theta_t - \theta^{\pi}}^2 }
%\leq (1-\eta \mu)^t  \EE \sbr{ \norm{\theta_{0} - \theta^{\pi}}^2 } + \frac{2 \eta c_1^2}{\mu} 
%%+ \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}},
%\end{align}
%from which the desired claim follows.
%%Consequently, with the choice of $T$ 
%%%\begin{align}\label{ctd_expectation_bd_choice_of_t}
%%%T \geq  \frac{1}{\eta \mu} \log 2,
%%%%T = \ceil{ \frac{1}{\eta \mu} \log 2},
%%%\end{align} 
%%and the definition of $\cbr{{\theta}}$, we obtain from \eqref{convergence_mse_clean} that 
%%\begin{align}\label{mse_bound_each_epoch}
%%\EE \sbr{\norm{{\theta} - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
%%\frac{4 \eta c_1^2}{\mu} 
%%+ \frac{4m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%%\end{align}
%%Applying \eqref{convergence_mse_clean} again with \eqref{mse_bound_each_epoch} then yields the desired claim. 
%%\begin{align}\label{mse_bound_each_epoch_iter}
%%\EE \sbr{\norm{{\theta}_t - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
%%\frac{6 \eta c_1^2}{\mu} 
%%+ \frac{6m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%%\end{align}
%\end{proof}


The next lemma establishes the fast bias reduction  of the estimated value function.

\begin{lemma}\label{lemma_bias_linear}
%Let $L = \lambda_{\max} \rbr{ \Psi^\top (I - \gamma \mathtt{P}^{\pi})^\top \diag(\nu) (I - \gamma \mathtt{P}^{\pi}) \Psi}$, and
Denote $\overline{\theta}_t = \EE \sbr{\theta_t}$, and let
\begin{align}\label{param_choice_bias_reduction}
\eta_t = \eta \leq \frac{\mu}{ L^2}.
%T \geq  \frac{1}{\eta \mu} \log 2.
\end{align}
Then we have 
\begin{align}\label{bound_on_parameter_bias}
\norm{\overline{\theta}_t - \theta^{\pi}}^2
\leq 
(1 - \eta \mu)^t \norm{\theta_0 - \theta^{\pi}}^2.
\end{align}
Consequently, 
%by denoting $\cV^{\pi}_{\theta} = - \Psi \theta$, 
we obtain 
\begin{align}\label{bound_on_value_bias}
\norm{\EE \sbr{ \cV^{\pi}_{\theta_t}} - \cV^{\pi}}_\infty \leq  (1 - \eta \mu)^{t/2} \norm{\theta_0 - \theta^{\pi}}
+  \varepsilon_{\mathrm{approx}} .
\end{align}
%\yan{need a statement on the err of the estimated value}
\end{lemma}

\begin{proof}
%Define $\overline{\theta}_t = \EE\sbr{\theta_t}$ and $\overline{\theta} = \EE\sbr{\theta}$. 
Since we have $\theta_{t+1} = \theta_t - \eta_t \hat{F}_t(\theta_t)$, taking expectation on both sides yields 
\begin{align*}
\overline{\theta}_{t+1} = \EE\sbr{\theta_{t+1}}
= \EE\sbr{\theta_t} - \eta_t \EE [\hat{F}_t(\theta_t)] 
& = \EE\sbr{\theta_t}
-  \eta_t \EE [ \EE_{|t} [\hat{F}_t(\theta_t)] ] \\
& = 
 \EE\sbr{\theta_t}
- \eta_t \EE [ F(\theta_t) + \EE_{|t} [\delta_t] ]  \\
& = 
\overline{\theta}_t 
- \eta_t  F(\overline{\theta}_t)
\end{align*}
where the last equality follows from the linearity of $F(\cdot)$ and \eqref{bias_conditional_expectation}.
Consequently, we have 
\begin{align}
\norm{\overline{\theta}_{t+1} - \theta^{\pi}} 
& = \norm{ \overline{\theta}_t 
-  \eta_t  F(\overline{\theta}_t)
- \theta^{\pi}
}  \nonumber \\
& \leq
\norm{\overline{\theta}_t - \theta^{\pi}}^2 - 2 \eta_t \inner{F(\overline{\theta}_t) }{\overline{\theta}_t - \theta^{\pi}}
+   \eta_t^2 \norm{F(\overline{\theta}_t)}^2 . 
\label{bias_progress_raw}
\end{align}
%We proceed to bound $\norm{\EE\sbr{\delta_t}}$ and $\norm{F(\overline{\theta}_t)}$ separately. 
%For $\norm{\EE\sbr{\delta_t}}$, we have 
%\begin{align}
%\norm{\EE[\delta_t]}
%= \norm{\EE[ \EE_{|t}[\delta_t] ]}
%&\overset{(a)}{ =} \lVert
%\EE \big[
% \Psi^\top (\EE_{|t}[\hat{M}] - M)(I -\gamma \mathtt{P}^\pi) \Psi (\theta_t  -  \theta^{\pi} )
% \nonumber \\
%& ~~~~~~ + \Psi^\top (\EE_{|t}[\hat{M}] - M) (I -\gamma \mathtt{P}^\pi)  [\Psi \theta^{\pi} - V^{\vartheta}_{{\PP}^\pi}] 
%\big]
%\rVert \nonumber \\
%& \leq 
%m \rho^\tau \sbr{ L \EE \sbr{\norm{\theta_t - \theta^{\pi}}}
%+  \kappa_2 \varepsilon_{\mathrm{approx}} }, \label{norm_bound_on_bias},
%\end{align}
%where $(a)$ follows from \eqref{bias_conditional_expectation}.
In addition,  it holds that 
\begin{align}
\norm{F(\overline{\theta}_t)}
& = \norm{F(\overline{\theta}_t) - F(\theta^\pi)}  = \norm{\Psi^\top (I - \gamma \mathtt{P}^{\pi})^\top \diag(\nu) (I - \gamma \mathtt{P}^{\pi}) \Psi (\overline{\theta}_t - \theta^{\pi}}  \leq L  \norm{\overline{\theta}_t - \theta^{\pi}}. 
\label{bound_on_op_norm}
\end{align}
Hence by plugging \eqref{bound_on_op_norm} into \eqref{bias_progress_raw}, and applying \eqref{ctd_monotone}, we obtain 
\begin{align}
\norm{\overline{\theta}_{t+1} - \theta^{\pi}}^2 
& \leq (1- 2\eta_t \mu + L^2 \eta_t^2) \norm{\overline{\theta}_t - \theta^{\pi}}^2 
\label{convergence_bias}
\end{align}
%
%We proceed to show that $ \EE \sbr{\norm{\theta_t - \theta^{\pi}}^2}$ is indeed bounded with proper parameter specifications.
%Let 
%\begin{align}\label{param_for_norm_bound}
%\eta_t = \eta \leq \frac{\mu}{4 \kappa_3^2}, ~ (L + \kappa_2) m \rho^\tau \leq \frac{\mu}{4},
%\end{align}
%then recursive application of \eqref{convergence_mse} yields 
%\begin{align}\label{convergence_mse_clean}
%\EE \sbr{\norm{\theta_t - \theta^{\pi}}^2 }
%\leq (1-\eta \mu)^t  \EE \sbr{ \norm{\theta_{0} - \theta^{\pi}}^2 } + \frac{2 \eta c_1^2}{\mu} 
%+ \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%\end{align}
%Consequently, with 
%\begin{align}\label{ctd_expectation_bd_choice_of_t}
%T \geq  \frac{1}{\eta \mu} \log 2,
%%T = \ceil{ \frac{1}{\eta \mu} \log 2},
%\end{align} 
%and the definition of $\cbr{{\theta}}$, we obtain from \eqref{convergence_mse_clean} that 
%\begin{align}\label{mse_bound_each_epoch}
%\EE \sbr{\norm{{\theta} - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
%\frac{4 \eta c_1^2}{\mu} 
%+ \frac{4m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%\end{align}
%Applying \eqref{convergence_mse_clean} again with \eqref{mse_bound_each_epoch} then shows 
%\begin{align}\label{mse_bound_each_epoch_iter}
%\EE \sbr{\norm{{\theta}_t - \theta^{\pi}}^2} \leq \norm{\theta_0 - \theta^{\pi}}^2 + 
%\frac{6 \eta c_1^2}{\mu} 
%+ \frac{6m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}.
%\end{align}
%Clearly, \eqref{param_for_norm_bound}  holds given the parameter choice specified in \eqref{param_choice_bias_reduction}, and consequently Lemma \ref{lemma_norm_bound_in_expectation} applies.
%On the other hand, suppose further that 
%\begin{align}\label{param_for_bias_bound_1}
%\eta \leq \frac{\mu}{8 L^2}, ~
%m \rho^{\tau} \max \cbr{ L, \kappa_2} \leq \frac{\mu}{4},
%~
%m \rho^\tau  \leq \eta \kappa_2, ~ \eta^2 m^2 \rho^{2 \tau} \leq \frac{1}{2},
%\end{align}
%then recursive application of \eqref{convergence_bias}, combined with  \eqref{mse_bound_each_epoch_iter} yields 
%\begin{align}\label{convergence_bias_cleaner}
%& \norm{\overline{\theta}_t - \theta^{\pi}}^2 \nonumber \\
%\leq &  (1-\eta \mu)^{t} \norm{\theta_0 - \theta^{\pi}}^2 
%+ \frac{8 \eta \kappa_2^2}{\mu}  \varepsilon^2_{\mathrm{approx}} 
%+ \frac{m \rho^\tau}{\mu} (L + 4 \eta m \rho^\tau L^2)
%(
%\norm{\theta_0 - \theta^{\pi}}^2 + \frac{2 \eta c_1^2 }{\mu} +  \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}
%).
%\end{align}
%Further requiring that 
%\begin{align}\label{param_for_bias_bound_2}
%\eta \leq \frac{\mu }{16 \kappa_2^2},
%~
% \frac{m \rho^\tau}{\mu} (L + 4 \eta m \rho^\tau L^2)
%(
%\norm{\theta_0 - \theta^{\pi}}^2 + \frac{2 \eta c_1^2 }{\mu} +  \frac{2m \kappa_2 \rho^\tau}{\mu} \varepsilon^2_{\mathrm{approx}}
%) \leq \frac{ \varepsilon^2_{\mathrm{approx}}}{2},
%\end{align}
The above relation simplifies to \eqref{bound_on_parameter_bias} with choice of $\cbr{\eta_t}$ in \eqref{param_choice_bias_reduction}.
%\begin{align*}
%\norm{\overline{\theta}_t - \theta^{\pi}}^2
%\leq 
%(1 - \eta \mu)^t \norm{\theta_0 - \theta^{\pi}}^2
%+  \varepsilon^2_{\mathrm{approx}} .
%\end{align*}
%Consequently, with the definition of $\overline{\theta}$ and the choice of $T$, we obtain   
%\begin{align*}
%\norm{\overline{\theta}^{(e+1)} - \theta^{\pi}}^2 \leq \frac{1}{2} \norm{\overline{\theta} - \theta^{\pi}}^2 
%+ \varepsilon^2_{\mathrm{approx}} ,
%\end{align*}
%from which \eqref{bound_on_parameter_bias} follows.
%It remains to note that \eqref{param_for_bias_bound_1} and \eqref{param_for_bias_bound_2} can be satisfied by the parameter choice specified in \eqref{param_choice_bias_reduction}.
%, from which we obtain \eqref{bound_on_parameter_bias}.
Finally, \eqref{bound_on_value_bias} follows from the direct application of \eqref{bound_on_parameter_bias},
by noting that 
\begin{align*}
\abs{\EE \sbr{ \cV^{\pi}_{\theta_t}}(s) - \cV^{\pi}(s)}
& \leq 
 \abs{\EE \sbr{ \cV^{\pi}_{\theta_t}}(s) - \cV^\pi_{\theta^\pi}(s)} +  \abs{ \cV^\pi_{\theta^\pi}(s)- \cV^{\pi}(s)}
\\ & \leq 
 \abs{ \rbr{\overline{\theta}_t - \theta^\pi}^\top \psi(s)} +  \abs{ \cV^\pi_{\theta^\pi}(s)- \cV^{\pi}(s)} \\
 & \leq 
  (1 - \eta \mu)^{t/2} \norm{\theta_0 - \theta^{\pi}} +  \varepsilon_{\mathrm{approx}} .
\end{align*}
The proof is then completed.
\end{proof}






With Lemma \ref{lemma_ctd_recursion} and \ref{lemma_bias_linear} in place, we are ready to establish the sample complexity of SFRPE with the SLPE operator, 
in order to output an $\epsilon$-estimator of the robust value function in expectation.
%For notational simplicity, going forward we denote 
%$r_{\Theta} = \max_{\pi \in \Pi} \norm{\theta^\pi} < \infty$.





\begin{theorem}\label{thrm_lspe_expectation}
For any $\epsilon \geq \frac{8 \varepsilon_{\mathrm{approx}} }{(1-\gamma)}$,
let SFRPE be instantiated with SLPE operator with  evaluation parameters
\begin{align*}
\eta \leq \frac{\mu}{L^2 +  32} , ~ \theta_0 = 0, ~ T = \cO \rbr{ \frac{1}{\eta \mu} \log \rbr{\frac{r_\Theta }{ \epsilon}} }, 
\end{align*}
 and  optimization parameters
\begin{align*}
\beta_k = k^{1/2},   ~ \lambda_k = \frac{ (k+1) M \gamma \zeta }{2  \sqrt{\mu \overline{w}}}, ~\forall k \geq 0, 
\end{align*}
where $M^2 \geq 4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2}$.
To find an approximate robust value such that 
\begin{align*}
-\epsilon \leq \EE \sbr{\tsum_{t=1}^k \theta_t \cV^{\pi_t}}(s) - \cV^*(s) \leq \epsilon,  ~ s \in \cS,
\end{align*}
SFRPE needs at most $k = 1 + \frac{256 \gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu \epsilon^2}$ iterations.
%where $M^2 = 4 \rbr{r_\Theta^2 + 1  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2}$.
The total number of samples can be bounded by 
\begin{align}\label{eq_sample_lspe_expectation}
\cO \rbr{
\frac{1}{\eta \mu} 
\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
\log \rbr{\frac{r_\Theta }{ \epsilon}}
} .
% \cO \rbr{
%\frac{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}{ \mu^2} 
%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2 \overline{w}}{(1-\gamma)^2 \mu \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%} .
\end{align}
In particular, when the distance generating function $w(\cdot)$ is set as in \eqref{dgf_negative_entropy}, the number of samples required is bounded by 
\begin{align*}
\cO \rbr{
\frac{1}{\eta \mu} 
\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \log \abs{\cS} }{(1-\gamma)^2  \epsilon^2}}
\log \rbr{\frac{r_\Theta }{\epsilon}}
} .
% \cO \rbr{
%\rbr{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}
%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2   \log \abs{\cS} }{(1-\gamma)^2  \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%} .
\end{align*}
\end{theorem}

\begin{proof}
With a slight overload of notation, let us denote $\theta_k$ as the parameters output by SLPE at the $k$-th iteration of SFRPE. 
Then given the choice of parameters, one can apply Lemma \ref{lemma_ctd_recursion} and obtain  
%$
%\EE \sbr{ \norm{\theta_k - \theta^{\pi_k}}^2} \leq r_\Theta^2 + c_1^2
%$,
%and hence 
%\yan{ambiguity in notation $\theta_k$ here}
\begin{align*}
%\label{ctd_estimate_expectation_bound_on_korm}
\EE \sbr{ \norm{\cV^{\pi_k}_{\theta_k} - \cV^{\pi_k}}_\infty^2} 
\leq 2 \rbr{ \EE \norm{\theta_k - \theta^{\pi_k}}^2   + \varepsilon^2_{\mathrm{approx}}} 
\leq 2 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}}.
\end{align*}
Consequently, 
\begin{align*}
\EE  \sbr{ \norm{\cV^{\pi_k}_{\theta_k}}_\infty^2} \leq 2 \rbr{\EE \sbr{ \norm{\cV^{\pi_k}_{\theta_k} - \cV^{\pi_k}}_\infty^2} + \norm{\cV^{\pi_k}}_\infty^2 } \leq  4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2} .
\end{align*}
In addition, from Lemma \ref{lemma_bias_linear}, 
for any $\epsilon \geq \frac{8 \varepsilon_{\mathrm{approx}}}{1-\gamma} $, 
taking $T = \cO\rbr{ \frac{1}{\eta \mu} \log \rbr{\frac{r_\Theta }{ \epsilon}}}$ further yields that 
\eqref{stoch_expecation_conv_bias_condition} is satisfied with 
\begin{align*}
\frac{3\varepsilon}{1-\gamma} = \frac{3 \epsilon}{4}, ~ M^2 = 4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2} .
\end{align*}
Combining the above relation and Proposition \ref{thrm_stoch_generic_convergence_expectation}, the total number of iterations required by SFRPE is bounded by 
$
k = 1 + \frac{256 \gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}.
$
The total number of samples can be bounded by 
\begin{align*}
T \cdot k  & = \cO \rbr{
\frac{1}{\eta \mu} 
\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
\log \rbr{\frac{r_\Theta }{\epsilon}}
} .
% \\
%& = \cO \rbr{
%\frac{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}{ \mu^2} 
%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%} .
\end{align*}
The proof is then completed.
%We conclude the proof by noting  taking $\eta = \frac{\mu}{16(L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2)}$ suffices to satisfy the condition \eqref{xx} of Lemma \ref{lemma_bias_linear}
\end{proof}

In view of Theorem \ref{thrm_lspe_expectation}, SFRPE yields an $\tilde{\cO}({\zeta^2}/ \epsilon^2 + \log(1/\epsilon))$ sample complexity with linear function approximation.
Notably, this appears to be the first method of stochastic robust policy evaluation beyond tabular settings with non-asymptotic convergence, and does so without restrictive assumptions on the transition kernel and discount factor \cite{tamar2014scaling, roy2017reinforcement}. 
Similar to Theorem \ref{thrm_sample_se_expectation} and \ref{thrm_stoch_se_high_prob}, the obtained sample complexity in \eqref{eq_sample_lspe_expectation} admits a natural decomposition, with the first term corresponding to learning the standard value function up to $\epsilon$-accuracy in bias, and the second term corresponding to the price of robustness. 


With the same spirit as Theorem \ref{thrm_stoch_se_high_prob}, we proceed to show that with the same number of samples, SFRPE with SLPE operator can indeed output an $\epsilon$-estimator of the robust value function in high probability.
To this end, we first establish the following high probability bound on the iterate produced by SLPE operator. 
The challenge for such a statement primarily comes from that the noise in SLPE itself depends on the boundedness of the iterate, thus preventing a direct application of standard concentration argument. 
The following lemma constructs an approximate martingale sequence that upper bounds the noise of SLPE in high probability, with increment of the former sequence  bounded with proper choice of stepsize. 
Similar argument can also be found in \cite{li2023policy} for bounded increments but potentially non-zero conditional expectation.
%Such an argument can be viewed as a generalization of \cite{li2023policy} to unbounded sequence. 
%In view of this we proceed to show that the iterate of SLPE is indeed bounded in high probability. 
%In view of this observation, we adopt a similar technical idea that can be found in \cite{li2023policy}.
%, originally designed for establishing inherent exploration properties  of stochastic policy optimization.

\begin{lemma}\label{lemma_norm_bound_high_prob}
Fix total iterations $T > 0$.
For any $\delta \in (0,1)$, let $\eta_t = \eta \coloneqq \alpha / \sqrt{T}$ with 
\begin{align}
\alpha & \leq 
\min \big\{
%\frac{\mu \sqrt{T}}{\kappa_3^2}, ~
\frac{\mu }{32}, ~
\frac{1}{{192 \sbr{L^2 + c_1^2 + 16} }\log(2T / \delta)},~
\frac{1}{2 c_1}, ~
\frac{1}{4 G}
\big\},  \label{param_choice_bounded_norm_high_prob} \\
G & = 4 \sqrt{\log(2T /\delta)} \big[(L + 4) \norm{\theta_0 - \theta^{\pi}}^2 + c_1 \norm{\theta_0 - \theta^{\pi}}\big] + c_1. \label{G_bounded_norm_high_prob}
\end{align}
%where $G \geq 4 \sqrt{\log(2T /\delta)} \big[(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}\big] + c_1$.
Then 
with probability at least $1-\delta$,
\begin{align*}
\norm{\theta_t - \theta^{\pi}}^2
\leq  \norm{\theta_0 - \theta^{\pi}}^2 + 1, ~ \forall t \leq T.
\end{align*}
\end{lemma}
%Going forward let us fix the epoch length $T > 0$ and the number of epochs $E > 0$.
%and assume $\eta_t = \eta \coloneqq \alpha / \sqrt{T}$ for some $\alpha > 0$ to be determined later. 
\begin{proof}
We begin by noting that  
\begin{align*}
\delta_t  = \hat{F}_t(\theta_t) - F(\theta_t)  =  \hat{F}_t(\theta_t)  - \hat{F}_t(\theta^\pi) -  F(\theta_t)  +  \hat{F}_t(\theta^\pi).
\end{align*}
Consequently, it is clear that  
\begin{align}\label{eq_norm_bound_with_noise_version}
\inner{\delta_t}{\theta_t - \theta^{\pi}}
& = 
 \inner{ \hat{F}_t(\theta_t)  - \hat{F}_t(\theta^\pi)}{\theta_t - \theta^\pi}
- \inner{F(\theta_t) }{\theta_t - \theta^\pi}
+ \inner{\hat{F}_t(\theta^\pi)}{\theta_t - \theta^\pi} \nonumber 
\\
& \leq 
(L + 4) \norm{\theta_t - \theta^\pi}^2  + 
 c_1 \norm{\theta_t - \theta^\pi},
\end{align}
where the last inequality applies \eqref{norm_bound_diff_stoch_op}, \eqref{norm_bound_stoch_op}, and \eqref{bound_on_op_norm}.
In addition, note that \eqref{norm_recursion_with_noise}  still holds.  Hence with 
$
 \eta \leq \frac{\mu}{32}, 
$
%or equivalently, $\alpha \leq \frac{\mu \sqrt{T}}{\kappa_3^2}$, 
or equivalently, $\alpha \leq \frac{\mu }{32}$, 
we obtain 
\begin{align}\label{recursion_on_norm_const_stepsize}
\norm{\theta_{t+1} - \theta^{\pi}}^2 
\leq  \norm{\theta_t - \theta^{\pi}}^2
+ 2\eta^2 c_1^2 +  2 \eta \inner{\delta_t}{\theta_t - \theta^{\pi}}.
\end{align}
Let us define random sequences $\cbr{X_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}}}$, $\cbr{\tilde{X}_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}}$,
where  $\cG_t = \cbr{Y_t \leq G \sqrt{t}}$, and
\begin{align*}
 Y_0 \equiv \tilde{Y}_0 \equiv 0,  ~
Y_t  = Y_{t-1} + X_{t-1}, ~
\tilde{Y}_t  = \tilde{Y}_{t-1} + \tilde{X}_{t-1}.
\end{align*}
%Now consider event $\cG_t = \cbr{Y_t \leq G \sqrt{te}}$ for some $G > 0$.
%Now consider event 
%$\cG_t = \cbr{\tsum_{i=0}^{t-1} \inner{\delta_i}{\theta_i - \theta^{\pi}} \leq G \sqrt{t}}$ for some $G > 0$.
Then over $ \cG_t$, 
taking the telescopic sum of \eqref{recursion_on_norm_const_stepsize}  yields   
\begin{align}
\norm{\theta_t - \theta^{\pi}}^2 & \leq \norm{\theta_0 - \theta^{\pi}}^2 + 2 t \eta^2 c_1^2 + 2 \eta G \sqrt{t} \nonumber \\
& \leq  \norm{\theta_0 - \theta^{\pi}}^2 + 2 \alpha^2 c_1^2  + 2 \alpha G  \coloneqq M_{(\alpha, G)}.
\label{norm_bound_on_good_event_generic}
\end{align}
We proceed to establish that \eqref{norm_bound_on_good_event_generic} indeed holds with probability at least $1-\delta$ given proper choice of $(\alpha, G)$.

%Let us define random sequences $\cbr{X_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}}}$, $\cbr{\tilde{X}_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}}$,
%together with 
%\begin{align*}
%Y_t = \tsum_{i=0}^{t-1} X_i, ~ \tilde{Y}_t = \tsum_{i=0}^{t-1} X_i', ~ Y_0 \equiv Y_0' \equiv 0.
%\end{align*}
%Then by definition we have $\cG_t = \cbr{Y_t \leq G \sqrt{t}}$.
First, it is  clear that 
\begin{align}\label{aux_sequence_norm_bound}
\abs{\tilde{X}_t} = \abs{\inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}} \leq (L + 4) M_{(\alpha, G)} + c_1 \sqrt{M_{(\alpha, G)}},
\end{align}
where the last inequality follows from \eqref{eq_norm_bound_with_noise_version} and \eqref{norm_bound_on_good_event_generic}.
Moreover, we also have 
\begin{align}
\abs{\EE_{|t}\sbr{\tilde{X}_t}}
& =  \abs{\EE_{|t}\sbr{\inner{\delta_t}{\theta_t - \theta^{\pi}}}  \mathbbm{1}_{\cG_t}}  = 0,\label{aux_sequence_conditional_expectation}
\end{align}
where the last equality follows from \eqref{bias_conditional_expectation}.
In view of \eqref{aux_sequence_norm_bound} and \eqref{aux_sequence_conditional_expectation}, we can now apply Azumaâ€“Hoeffding inequality to $\cbr{\tilde{Y}_t}$ and obtain 
\begin{align}
\abs{
\tilde{Y}_t
} \leq 2 \sqrt{t}  \big[ (L + 4) M_{(\alpha, G)} + c_1 \sqrt{M_{(\alpha, G)}} \big]\sqrt{\log(2T/ \delta)}, 
~ \forall t \leq T,
\label{aux_sequence_accumulation_raw}
\end{align}
with probability $1-\delta$.
Through direct computation, one can verify that \eqref{aux_sequence_accumulation_raw} and the $(\alpha, G)$ specified in \eqref{param_choice_bounded_norm_high_prob}, \eqref{G_bounded_norm_high_prob}, together with the definition of $M_{(\alpha, G)}$, implies 
%\begin{align}
%& \sbr{ c_1 + \kappa_2  \varepsilon_{\mathrm{approx}} + L + \kappa_3} \sqrt{\alpha} \leq \frac{1}{8 \sqrt{\log(2T/\delta)}}, 
%~ \tau = \tilde{\cO} (t_{\mathrm{mix}}), \label{param_alpha_tau} \\
%& G \geq 4 \sqrt{\log(2T /\delta)} \big[(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}\big] + c_1 , \label{param_choice_G}
%\end{align}
\begin{align}\label{aux_seq_accumulation_bounded}
\abs{\tilde{Y}_t} \leq G \sqrt{t }, ~ \forall t \leq T,
\end{align}
with probability $1-\delta$.
%and  $G = 4 \sqrt{\log(2k /\delta)} \sbr{(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}} + c_1 + 1$.
Let us denote the event corresponding to \eqref{aux_seq_accumulation_bounded} by $\cG$.
%Clearly, from the definition of $\cbr{\cG_t}$ and \eqref{aux_seq_accumulation_bounded} we obtain $\cap_{t \leq T} \cG_t \subseteq \cG$.
Our next goal is to show that $Y_t = \tilde{Y}_t$ over $\cG$ for every $t \leq T$, and consequently 
\begin{align}\label{noise_accumulation_bound}
\abs{Y_t} \leq G \sqrt{t}, ~ \forall t \leq T, 
\end{align}
with probability $1-\delta$.
We proceed with an inductive argument. 
The claim trivially holds at $t = 0$.
Suppose the claim holds at iteration $t \geq 0$, then for any $\omega \in \cG$, 
\begin{align}
Y_{t+1}(\omega) & = Y_t(\omega) + X_t(\omega) \nonumber \\
& = Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cG}(\omega) \nonumber \\
& \overset{(a)}{=} Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cbr{\tilde{Y}_t \leq G \sqrt{t}}}(\omega) \nonumber \\
& \overset{(b)}{=} Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cbr{Y_t \leq G \sqrt{t}}}(\omega) \nonumber \\
& \overset{(c)}{=} Y_t(\omega) + \tilde{X}_t(\omega) \nonumber \\
& = \tilde{Y}_{t+1}(\omega), \label{equiv_induction_step_1}
\end{align}
where $(a)$ follows from the definition of $\cG$ and \eqref{aux_seq_accumulation_bounded}, 
 $(b)$ follows from the induction hypothesis that $Y_t (\omega) = \tilde{Y}_t(\omega)$ for $\omega \in \cG$,
and $(c)$ applies the definition of $\tilde{X}_t$.
Hence \eqref{equiv_induction_step_1} completes the induction step.
%On the other hand, suppose the claim holds for all $t \leq T$ of epoch $e$, then for any $\omega \in \cG$,
%\begin{align*}
%Y_{0}^{(e+1)}(\omega) & = Y_{T}(\omega) = \tilde{Y}_{T}(\omega)  = \tilde{Y}^{(e+1)}_{0}(\omega).
%\end{align*}
%\begin{align*}
%Y_{0}^{(e+1)}(\omega) & = Y_{T}(\omega) \\
%& = Y_{T-1}(\omega) + X_{T-1}(\omega) \mathbbm{1}_{\cG}(\omega) \\
%& = Y_{T-1}(\omega) + X_{T-1}(\omega) \mathbbm{1}_{\cbr{\tilde{Y}_{T-1} \leq G \sqrt{T-1 + e T}}}(\omega) \\
%& =Y_{T-1}(\omega) + X_{T-1}(\omega) \mathbbm{1}_{\cbr{Y_{T-1} \leq G \sqrt{T-1 + e T}}}(\omega) \\
%& = Y_{T-1}(\omega) + \tilde{X}_{T-1}(\omega) \\
%& = \tilde{Y}^{(e+1)}_{0}(\omega),
%\end{align*}
%Combining \eqref{equiv_induction_step_1} and the above relation completes the induction step.

In view of \eqref{norm_bound_on_good_event_generic} and \eqref{noise_accumulation_bound}, we conclude that
% for $(\alpha, \tau, G)$ specified  in  \eqref{param_alpha_tau} and \eqref{param_choice_G}, 
\begin{align*}
\norm{\theta_t - \theta^{\pi}}^2
\leq \norm{\theta_0 - \theta^{\pi}}^2 + 2 \alpha^2 c_1^2  + 2 \alpha G,
\end{align*}
with probability at least $1-\delta$.
The desired claim follows immediately by noting that $\alpha \leq \min\cbr{\frac{1}{2 c_1}, ~
\frac{1}{4 G} }$.
\end{proof}
%With \eqref{xx}, \eqref{xx} and \eqref{xx} in place, we are now ready to complete the proof.
%Clearly, with an inductive argument, one can readily show that with 
%\begin{align*}
%& \sbr{ c_1 + \kappa_2  \varepsilon_{\mathrm{approx}} + L + \kappa_3} \sqrt{\alpha} \leq \frac{1}{8 \sqrt{\log(2T/\delta)}}, ~
%\alpha c_1 \leq 1, 
%~ \tau = \cO(t_{\mathrm{mix}}),  \\
%& G \geq 4 \sqrt{\log(2k /\delta)} \big[(L + \kappa_3) (e + 1) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \sqrt{e + 1} \norm{\theta_0 - \theta^{\pi}}\big] + c_1 + 1, \\
%& 
% 2 \alpha^2 c_1^2  + 2 \alpha G \leq \norm{\theta_0 - \theta^{\pi}}^2.
%\end{align*}






By combining Lemma \ref{lemma_bias_linear} and \ref{lemma_norm_bound_high_prob}, we proceed to establish that SLPE with proper parameter specification yields fast bias reduction while controlling boundedness of the estimated value function $\hat{\cV}^\pi$. 


\begin{lemma}\label{lemma_high_prob_norm_and_bias}
Fix total iterations $T > 0$ a priori in SLPE.
For any $\delta \in (0,1)$ and any $\varepsilon \geq 2 \varepsilon_{\mathrm{approx}}$, let the parameters in SLPE be chosen as 
\begin{align*}
\eta_t = \alpha /\sqrt{T}, 
%~ T = \frac{2 \log^2 \rbr{4{\norm{\theta_0 -\theta^{\pi}}}/{\varepsilon}}}{\alpha^2 \mu^2}, 
\end{align*} 
with 
$
\alpha  \leq 
\min \big\{
\frac{\mu }{L^2 + 32}, 
%\frac{\mu \sqrt{T}}{\kappa_3^2}, ~
\frac{1}{192 \sbr{L^2 + c_1^2 + 16} \log(2T / \delta)},
\frac{1}{2 c_1}, 
\frac{1}{4 G}
\big\},
%\\
%G & \geq 4 \sqrt{\log(2T /\delta)} \big[(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}\big] + c_1. 
$
and $G$ defined in \eqref{G_bounded_norm_high_prob}.
Then the number of iterations required by SLPE to output 
\begin{align}\label{ctd_bias_bd_with_norm_bd}
\norm{\EE \sbr{ \cV^{\pi}_{\theta_t}} - \cV^{\pi} }_\infty \leq  \varepsilon
\end{align}
  is bounded by
\begin{align}\label{ctd_num_iter_bias_and_norm}
T = \cO \rbr{
 \frac{\log^2 \rbr{{ \norm{\theta_0 -\theta^{\pi}}}/{\varepsilon}}}{\alpha^2 \mu^2} 
 }.
%\end{align}
%The number of samples can be bounded by 
%\begin{align}
%\label{ctd_num_samples_bias_and_norm}
% \cO \rbr{
%\frac{t_{\mathrm{mix}}}{\alpha^2 \mu^2} \log^2 \rbr{\frac{\norm{\theta_0 -\theta^{\pi}}}{ \varepsilon}}
%}.
\end{align}
In addition, we have 
\begin{align}\label{ctd_norm_bound_bias_and_norm}
\norm{\cV^{\pi}_{\theta_t} -  \cV^{\pi} }_\infty
\leq  \norm{\theta_0 - \theta^{\pi}} + 1 + \varepsilon_{\mathrm{approx}}, ~ \forall t \leq T ,
\end{align}
with probability at least $1-\delta$.
\end{lemma}

\begin{proof}
Clearly,  $\eta = \alpha/\sqrt{T}$ with the choice of specified $\alpha$ satisfy 
 \eqref{param_choice_bias_reduction}.
 Consequently, one can  apply \eqref{bound_on_value_bias} in Lemma \ref{lemma_bias_linear}, and obtain that for any $\varepsilon \geq 2 \varepsilon_{\mathrm{approx}}$, 
SLPE outputs 
$
\norm{\EE \sbr{ \cV^{\pi}_{\theta_t}} - \cV^{\pi}}_\infty \leq \varepsilon
$
in 
\begin{align*}
T =\cO \rbr{\frac{1}{\eta \mu} \log \rbr{\frac{\norm{\theta_0 - \theta^*}}{\varepsilon}}}
\end{align*} 
steps. Combining the above relation with the definition of $\eta = \alpha /\sqrt{T}$ implies \eqref{ctd_num_iter_bias_and_norm}.
%\eqref{ctd_num_samples_bias_and_norm} then follows from \eqref{ctd_num_iter_bias_and_norm} and the choice of $\tau = \tilde{\cO}(t_{\mathrm{mix}})$. 
%The total number of samples consumed by SLPE is bounded by 
%\begin{align*}
%T \cdot \tau 
%= \cO \rbr{
%\frac{t_{\mathrm{mix}}}{\alpha^2 \mu^2} \log^2 \rbr{\frac{\norm{\theta_0 -\theta^{\pi}}}{\varepsilon}}
%}.
%\end{align*}
Finally, \eqref{ctd_norm_bound_bias_and_norm} follows  from 
 \begin{align*}
  |\cV^{\pi}_{\theta_t} (s) - \cV^{\pi}  |_\infty \leq \norm{\theta_t - \theta^*} +  \varepsilon_{\mathrm{approx}} \leq  \norm{\theta_0 - \theta^{\pi}} + 1 + \varepsilon_{\mathrm{approx}}, ~ \forall s \in \cS,
 \end{align*}
 where the last inequality applies Lemma \ref{lemma_norm_bound_high_prob}.
 The proof is then completed.
\end{proof}

%
%\begin{remark}
%One can also readily employ Lemma \ref{lemma_ctd_recursion} and \ref{lemma_norm_bound_high_prob} to establish the convergence of $\EE \sbr{\norm{\overline{\theta}_t -\theta^{\pi}}^2} $ with high probability, for proper defined ergodic iterate $\overline{\theta}_t$.
%We omit its explicit discussion to keep the scope of the manuscript concise. 
%\yan{remark on extension to least square to improve the dependence on visitation measure, with simplified analysis. also this can exploits the generator}
%\end{remark}

With Lemma \ref{lemma_bias_linear} and \ref{lemma_norm_bound_high_prob} in place, we can now establish the sample complexity of SFRPE with the SLPE operator 
that outputs an $\epsilon$-estimator of the robust value function
with high probability.




\begin{theorem}\label{thrm_sample_slpe_high_prob}
Fix total iterations $k > 0$ in SFRPE and $\delta \in (0,1)$.
For any $\epsilon \geq \frac{8 \varepsilon_{\mathrm{approx}} }{ (1-\gamma)}$, 
 let SFRPE be instantiated with the SLPE operator with evaluation parameters 
\begin{align*}
\eta_t = \alpha /\sqrt{T},  ~
\theta_0 = 0, 
~ T = \cO \rbr{
 \frac{\log^2 \rbr{{ r_\Theta}/{\epsilon}}}{\alpha^2 \mu^2} 
 }, 
\end{align*} 
where 
\begin{align*}
\alpha  & \leq 
\min \big\{
\frac{\mu }{L^2 + 32}, ~
%\frac{\mu \sqrt{T}}{\kappa_3^2}, ~
\frac{1}{192 \sbr{L^2 + c_1^2 + 16} \log(12 Tk/ \delta)},~
\frac{1}{2 c_1}, ~
\frac{1}{4 G}
\big\}
\\
G&   = 4 \sqrt{\log(12 Tk/\delta)} \big[(L + 4) r_{\Theta}^2 + c_1   r_\Theta \big] + c_1,
\end{align*}
and optimization parameters specified as
\begin{align*}
\beta_t = t^{1/2},   ~ \lambda_t =  \frac{ (t+1) M \gamma \zeta }{2  \sqrt{\mu \overline{w}}}, ~\forall t \leq k,
\end{align*}
where $M \geq r_\Theta + 1 + \varepsilon_{\mathrm{approx}} + \frac{1}{1-\gamma}$.
Then with probability at least $1-\delta$ we have 
\begin{align}
& -\frac{\epsilon}{4} -  \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{6 (k+1)\abs{\cS}}{\delta})} \\
  \leq &  \tsum_{t=1}^k \theta_t 
\cV^{\pi_t}(s) - \cV^*(s) \nonumber \\
 \leq & \frac{4 \gamma \zeta M \sqrt{\overline{w}}}{(1-\gamma) \sqrt{\mu k}}  + \frac{3 \epsilon }{4}
+ \frac{8 \gamma \zeta M}{(1-\gamma) \sqrt{k}} \sqrt{\log (\frac{6 (k+1)\abs{\cS}}{\delta})}
+ \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{6(k+1) \abs{\cS}}{\delta})}, ~ \forall s \in \cS,
 \label{high_prob_err_bound_ctd}
\end{align}
The total number of samples required by SFRPE to output $-  \epsilon \leq \tsum_{t=1}^k \theta_t
\cV^{\pi_t}(s) - \cV^{\pi^*}(s) \leq  \epsilon$ for all $s \in \cS$ with at least probability $1-\delta$  is bounded by 
\begin{align}\label{ctd_sample_high_prob}
\tilde{\cO} \rbr{
\frac{1}{\alpha^2 \mu^2}
\rbr{
\frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}
+ \frac{M^2}{\epsilon^2}
}
 \log^2 \rbr{\frac{r_\Theta}{\epsilon}}
}.
\end{align}
In particular, when the distance generating function $w_s(\cdot)$ is set as in \eqref{dgf_negative_entropy}, the total number of samples required can be bounded by 
\begin{align*}
\tilde{\cO} \rbr{
\frac{1}{\alpha^2 \mu^2}
\rbr{
\frac{\gamma^2 \zeta^2 M^2 \log \abs{\cS}}{(1-\gamma)^2  \epsilon^2}
+ \frac{M^2}{\epsilon^2}
}
 \log^2 \rbr{\frac{r_\Theta}{\epsilon}}
}.
\end{align*}
\end{theorem}

\begin{proof}
The essential argument is similar to that of Theorem \ref{thrm_stoch_se_high_prob}, but we will use Lemma \ref{lemma_high_prob_norm_and_bias} instead of Proposition \ref{thrm_stoch_generic_convergence_expectation}.
%Denote $M = r_\Theta + 1 + \varepsilon_{\mathrm{approx}}$.
Clearly the choice of parameters satisfies conditions of Lemma \ref{lemma_high_prob_norm_and_bias}, and hence
\begin{align}\label{norm_bound_every_iter_whp}
\norm{\cV^{\pi_t}_{\theta_t}  }_\infty \leq M, ~
\norm{\EE \sbr{ \cV^{\pi_t}_{\theta_t}} - \cV^{\pi_t}}_\infty \leq   \frac{(1-\gamma) \epsilon}{4},  ~ \forall t \leq k, 
\end{align}
 with probability $1 -  \delta / 3$.
Combining the above relation with Proposition 34 of \cite{tao2015random} yields
\begin{align}\label{ctd_noise_accumulation_1}
\abs{ \tsum_{t=1}^k \theta_t \rbr{\cV^{\pi_t}(s) - \cV^{\pi_t}(s)}  } 
  \leq 
\frac{(1-\gamma) \epsilon}{4} 
+  M \sqrt{2 \tsum_{t=1}^k \theta_t^2 \log(\frac{2}{\delta})}  \leq \frac{(1-\gamma) \epsilon}{4} + \frac{4 M }{\sqrt{k}} \sqrt{\log(\frac{2 }{\delta})}, 
\end{align}
with probability at least $1 - \delta / 6$, for every $s\in \cS$. Further applying union bound yields 
\begin{align*}
\abs{ \tsum_{t=1}^k \theta_t \rbr{\cV^{\pi_t}(s) - \cV^{\pi_t}(s)}  } 
  \leq \frac{(1-\gamma) \epsilon}{4} + \frac{4 M }{\sqrt{k}} \sqrt{\log(\frac{2 \abs{\cS} }{\delta})}, ~ \forall s \in \cS, 
\end{align*}
with probability at least $1 -  \delta/3$.
Applying the same treatment, one can also show that 
\begin{align}\label{ctd_noise_accumulation_2}
\tsum_{t=1}^k \frac{\theta_t}{1-\gamma}  \EE_{s' \sim d_{s}^{\pi^*}} \sbr{ \delta_t(s', \pi^*(s'))}
 & \leq 
 \frac{\gamma \zeta}{1-\gamma} 
\rbr{
 \frac{(1-\gamma) \epsilon}{2}
+ \frac{8 M }{ \sqrt{k}} \sqrt{  \log(\frac{2 \abs{\cS} }{\delta})}
}, ~ \forall s \in \cS,
\end{align}
with probability at least $1 - \delta/3$. 
By plugging \eqref{norm_bound_every_iter_whp},  \eqref{ctd_noise_accumulation_1} and \eqref{ctd_noise_accumulation_2} into Lemma \ref{lemma_generic_prop_stoch}, we obtain
\begin{align*}
-\frac{\epsilon}{4} -  \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{\abs{2 \cS}}{\delta})}
&  \leq \tsum_{t=1}^k \theta_t 
\cV^{\pi_t}(s) - \cV^{\pi^*}(s) \nonumber \\
& \leq \rbr{\tsum_{t=1}^k \beta_t}^{-1}  \tsum_{t=1}^k \frac{\beta_t^2 \gamma^2 \zeta^2  M^2}{2 \mu \lambda_{t-1} (1-\gamma)}
+  \rbr{\tsum_{t=1}^k \beta_t}^{-1} \frac{\lambda_k \overline{w}}{1-\gamma} \nonumber \\
& ~~~ +  \frac{3 \epsilon }{4}
+ \frac{8 \gamma \zeta M}{(1-\gamma) \sqrt{k}} \sqrt{\log (\frac{2 \abs{\cS}}{\delta})}
+ \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{2 \abs{\cS}}{\delta})}, ~ \forall s \in \cS,
\end{align*}
with probability $1-  \delta $. 
Plugging the choice of $\cbr{(\beta_t, \lambda_t)}$ into the above relation yields \eqref{high_prob_err_bound_ctd}.
%\begin{align*}
%& -\epsilon -  \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{6 (k+1)\abs{\cS}}{\delta})} \\
%  \leq &  \tsum_{t=1}^k \theta_t 
%\cV^{\pi_t}(s) - \cV^{\pi^*}(s) \nonumber \\
% \leq & \frac{4 \gamma \zeta M \sqrt{\overline{w}}}{(1-\gamma) \sqrt{\mu k}}  + \rbr{1 + \frac{2}{1-\gamma}} \epsilon 
%+ \frac{8 \gamma \zeta M}{(1-\gamma) \sqrt{k}} \sqrt{\log (\frac{6 (k+1)\abs{\cS}}{\delta})}
%+ \frac{4M}{\sqrt{k}} \sqrt{\log (\frac{6(k+1) \abs{\cS}}{\delta})}, 
%\end{align*}
%for any $s \in \cS$, with probability at least $1-\delta$.
Given \eqref{high_prob_err_bound_ctd}, the total number of iterations by SFRPE  to output $-  \epsilon \leq \tsum_{t=1}^k \theta_t 
\cV^{\pi_t}(s) - \cV^{\pi^*}(s) \leq  \epsilon$ can be bounded by 
\begin{align*}
k = \tilde{\cO} \rbr{
\frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}
+ \frac{M^2}{\epsilon^2}
}.
\end{align*}
We conclude the proof by noting that the total number of samples required is $k T$.
%The total number of samples is bounded by 
%\begin{align*}
%\cO \rbr{
%\frac{t_{\mathrm{mix}}}{\alpha^2 \mu^2}
%\rbr{
%\frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}
%+ \frac{M^2}{\epsilon^2}
%}
% \log^2 \rbr{\frac{r_\Theta}{\epsilon}}
%}
%\end{align*}
%Finally, \eqref{ctd_sample_high_prob} follows from the above relation and \eqref{ctd_num_samples_bias_and_norm}.
\end{proof}

In view of Theorem \ref{thrm_sample_slpe_high_prob}, SFRPE method instantiated with LSPE operator attains an $\tilde{\cO}(\zeta^2/\epsilon^2 + 1/\epsilon^2)$ sample complexity to output an $\epsilon$-estimator of the robust value function in high probability. 
Clearly the accuracy certificate of Theorem \ref{thrm_sample_slpe_high_prob} is stated in a stronger sense compared to the expectation statement in Theorem \ref{thrm_lspe_expectation}.
As before, the sample complexity possesses two terms that can be attributed to the price of robustness and standard value function estimation, respectively. 


\vspace{6pt}
%Before we conclude our discussion in this section.
%\begin{remark}[Applications to Stochastic Policy Optimization for Robust MDPs]
{\bf  Applications to Stochastic Policy Optimization for Robust MDPs.}
We conclude this section by briefly demonstrating the application of the developed results in the context of  stochastic policy optimization for robust MDPs. 
Consider solving large-scale robust MDPs with $(\mathrm{s}, \mathrm{a})$-rectangular sets using the stochastic robust policy mirror descent (SRPMD) method in \cite{li2022first}.
If log-linear policy class is employed, and one applies the same feature map for the policy class and the robust state-action value function, then applying SFRPE for learning the robust state-action value function immediately implies an $\tilde{\cO}(1/\epsilon^2)$ sample complexity of SRPMD for finding an $\epsilon$-optimal robust policy (cf. Proposition 5.1, \cite{li2022first}).
In particular, this simple application already yields the first sample complexity of policy gradient methods applied to robust MDPs beyond tabular~settings. 

% Theorem \ref{xx} can be particularly useful when combined with existing first-order stochastic robust policy optimization method \cite{xx}.
%Namely, when log-linear policy class is employed in the outer policy optimization method, Theorem \ref{xx} can be readily incorporated 

%
%
%\begin{theorem}
%For any $\epsilon \geq \frac{8 \varepsilon_{\mathrm{approx}} }{(1-\gamma)}$,
%let SFRPE be instantiated with SLPE operator with  evaluation parameters
%\begin{align*}
%\eta \leq \frac{\mu}{L^2 +  32} , ~ \theta_0 = 0, ~ T = \cO \rbr{ \frac{1}{\eta \mu} \log \rbr{\frac{r_\Theta }{ \epsilon}} }, 
%\end{align*}
% and  optimization parameters
%\begin{align*}
%\beta_n = n^{1/2},   ~ \lambda_n = \frac{ (n+1) M \gamma \zeta }{2  \sqrt{\mu \overline{w}}}, ~\forall n \geq 0, 
%\end{align*}
%where $M^2 = 4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2}$.
%To find an approximate robust value such that 
%\begin{align*}
%-\epsilon \leq \EE \sbr{\tsum_{n=1}^k \theta_n \cV^{\pi_n}}(s) + V^{\vartheta}_r(s) \leq \epsilon,  ~ s \in \cS,
%\end{align*}
%SFRPE needs at most $k = 1 + \frac{256 \gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu \epsilon^2}$ iterations.
%%where $M^2 = 4 \rbr{r_\Theta^2 + 1  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2}$.
%The total number of samples can be bounded by 
%\begin{align*}
%\cO \rbr{
%\frac{1}{\eta \mu} 
%\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{ \epsilon}}
%} .
%% \cO \rbr{
%%\frac{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}{ \mu^2} 
%%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2 \overline{w}}{(1-\gamma)^2 \mu \epsilon^2}}
%%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%%} .
%\end{align*}
%In particular, when the distance generating function $w(\cdot)$ is set as in \eqref{dgf_negative_entropy}, the number of samples required is bounded by 
%\begin{align*}
%\cO \rbr{
%\frac{1}{\eta \mu} 
%\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \log \abs{\cS} }{(1-\gamma)^2  \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{\epsilon}}
%} .
%% \cO \rbr{
%%\rbr{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}
%%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2   \log \abs{\cS} }{(1-\gamma)^2  \epsilon^2}}
%%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%%} .
%\end{align*}
%\end{theorem}
%
%\begin{proof}
%With the choice of parameters one can apply Lemma \ref{lemma_ctd_recursion} and obtain  
%$
%\EE \sbr{ \norm{\theta_n - \theta^\pi}^2} \leq \norm{\theta_0 - \theta^\pi}^2 + c_1^2 \leq r_\Theta^2 + c_1^2
%$,
%and hence 
%\begin{align*}
%%\label{ctd_estimate_expectation_bound_on_norm}
%\EE \sbr{ \norm{\cV^{\pi_n}_{\theta_n} - \cV^{\pi_n}}_\infty^2} 
%\leq 2 \rbr{ \EE \norm{\theta_n - \theta^*}^2   + \varepsilon^2_{\mathrm{approx}}} 
%\leq 2 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}}.
%\end{align*}
%Consequently, 
%\begin{align*}
%\EE  \sbr{ \norm{\cV^{\pi_n}_{\theta_n}}_\infty^2} \leq 2 \rbr{\EE \sbr{ \norm{\cV^{\pi_n}_{\theta_n} - \cV^{\pi_n}}_\infty^2} + \norm{\cV^\pi}_\infty^2 } \leq  4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2} .
%\end{align*}
%In addition, from Lemma \ref{lemma_bias_linear}, 
%for any $\epsilon \geq \frac{8 \varepsilon_{\mathrm{approx}}}{1-\gamma} $, 
%taking $T = \cO\rbr{ \frac{1}{\eta \mu} \log \rbr{\frac{r_\Theta }{ \epsilon}}}$ further yields that 
%\eqref{stoch_expecation_conv_bias_condition} is satisfied with 
%\begin{align*}
%\frac{3\varepsilon}{1-\gamma} = \frac{3 \epsilon}{4}, ~ M^2 = 4 \rbr{r_\Theta^2 + c_1^2  + \varepsilon^2_{\mathrm{approx}}} + {2}/{(1-\gamma)^2} .
%\end{align*}
%Combining the above relation and Proposition \ref{thrm_stoch_generic_convergence_expectation}, the total number of iterations required by SFRPE is bounded by 
%$
%k = 1 + \frac{256 \gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}.
%$
%The total number of samples can be bounded by 
%\begin{align*}
%T \cdot k  & = \cO \rbr{
%\frac{1}{\eta \mu} 
%\rbr{1 + \frac{\gamma^2 \zeta^2 M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
%\log \rbr{\frac{r_\Theta }{\epsilon}}
%} .
%% \\
%%& = \cO \rbr{
%%\frac{ L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2}{ \mu^2} 
%%\rbr{1 + \frac{\gamma^2 \zeta^2  M^2 \overline{w}}{(1-\gamma)^2 \mu_w \epsilon^2}}
%%\log \rbr{\frac{r_\Theta }{(1-\gamma) \epsilon}}
%%} .
%\end{align*}
%The proof is then completed.
%%We conclude the proof by noting  taking $\eta = \frac{\mu}{16(L^2 + \kappa_2^2 + \kappa_3^2 + c_1^2)}$ suffices to satisfy the condition \eqref{xx} of Lemma \ref{lemma_bias_linear}
%\end{proof}
%







%
%\newpage 
%
%
%\begin{lemma}
%sdfs
%\end{lemma}
%
%For any $0 \leq e \leq E$, we will denote $\cF$ the $\sigma$-algebra up to (excluding) epoch $e$.
%Going forward let us consider any fixed epoch $0 \leq e \leq E$.
%
%Going forward let us fix the epoch length $T > 0$,
%and assume $\eta_t = \eta \coloneqq \alpha / \sqrt{T}$ for some $\alpha > 0$ to be determined later. 
%We begin by noting that 
%\begin{align*}
%\delta_t &= \Psi^\top \hat{M} (\Psi\theta_t - \gamma \hat{\mathtt{P}}^\pi \theta_t - \mathfrak{C}) - 
%\Psi^\top {M} (\Psi\theta_t - \gamma {\mathtt{P}}^\pi \theta_t - \mathfrak{C}) \\
%& = \Psi^\top \hat{M} (I - \gamma \hat{\mathtt{P}}^\pi) \Psi (\theta_t - \theta^{\pi})
%- \Psi^\top {M} (I - \gamma {\mathtt{P}}^\pi) \Psi (\theta_t - \theta^{\pi}) \\
%& ~~~~~~ +  \Psi^\top \hat{M} (\Psi\theta^{\pi} - \gamma \hat{\mathtt{P}}^\pi \theta^{\pi} - \mathfrak{C}) 
%- \Psi^\top M (\Psi \theta^{\pi} - \gamma {\mathtt{P}}^\pi \Psi \theta^{\pi} - \mathfrak{C}) \\
%& = \Psi^\top \hat{M} (I - \gamma \hat{\mathtt{P}}^\pi) \Psi (\theta_t - \theta^{\pi})
%- \Psi^\top {M} (I - \gamma {\mathtt{P}}^\pi) \Psi (\theta_t - \theta^{\pi}) \\
%& ~~~~~~ +  \Psi^\top \hat{M} (\Psi\theta^{\pi} - \gamma \hat{\mathtt{P}}^\pi \theta^{\pi} - \mathfrak{C}) 
%- \Psi^\top M (I - \gamma {\mathtt{P}}^\pi) (\Psi \theta^{\pi} - \cV^{\pi}).
%\end{align*}
%Consequently, it is clear that  
%\begin{align}\label{eq_norm_bound_with_noise_version}
%\inner{\delta_t}{\theta_t - \theta^{\pi}}
%\leq (L + \kappa_3) \norm{\theta_t - \theta^{\pi}}^2 + (c_1 + \kappa_2 \varepsilon_{\mathrm{approx}}) \norm{\theta_t - \theta^{\pi}}.
%\end{align}
%In addition, note that \eqref{norm_recursion_with_noise}  still holds,  hence with 
%\begin{align*}
% \eta \leq \frac{\mu}{\kappa_3^2}, 
%\end{align*}
%we obtain 
%\begin{align}\label{recursion_on_norm_const_stepsize}
%\norm{\theta_{t+1} - \theta^{\pi}}^2 
%\leq  \norm{\theta_t - \theta^{\pi}}^2
%+ 2\eta^2 c_1^2 +  2 \eta \inner{\delta_t}{\theta_t - \theta^{\pi}}.
%\end{align}
%
%Let us define random sequences $\cbr{X_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}}}$, $\cbr{X_t' \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}}$,
%together with 
%\begin{align*}
%Y_t = \tsum_{i=0}^{t-1} X_i, ~ Y_t' = \tsum_{i=0}^{t-1} X_i', ~ Y_0 \equiv Y_0' \equiv 0.
%\end{align*}
%Now consider event $\cG_t = \cbr{Y_t \leq G \sqrt{t}}$ for some $G > 0$.
%%Now consider event 
%%$\cG_t = \cbr{\tsum_{i=0}^{t-1} \inner{\delta_i}{\theta_i - \theta^{\pi}} \leq G \sqrt{t}}$ for some $G > 0$.
%Then over $\cG_t$, 
%taking the telescopic sum of \eqref{recursion_on_norm_const_stepsize}  yields that for any $t \leq T$, 
%\begin{align}
%\norm{\theta_t - \theta^{\pi}}^2 & \leq \norm{\theta_0 - \theta^{\pi}}^2 + 2 t \eta^2 c_1^2 + 2 \eta G \sqrt{t} \nonumber \\
%& \leq  \norm{\theta_0 - \theta^{\pi}}^2 + 2 \alpha^2 c_1^2  + 2 \alpha G  \coloneqq M_{(\alpha, G)}.
%\label{norm_bound_on_good_event_generic}
%\end{align}
%We proceed to establish that \eqref{norm_bound_on_good_event_generic} indeed holds with high probability given proper choice of $(\alpha, \tau, G)$.
%
%%Let us define random sequences $\cbr{X_t \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}}}$, $\cbr{X_t' \coloneqq \inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}}$,
%%together with 
%%\begin{align*}
%%Y_t = \tsum_{i=0}^{t-1} X_i, ~ Y_t' = \tsum_{i=0}^{t-1} X_i', ~ Y_0 \equiv Y_0' \equiv 0.
%%\end{align*}
%%Then by definition we have $\cG_t = \cbr{Y_t \leq G \sqrt{t}}$.
%First, it is  clear that 
%\begin{align}\label{aux_sequence_norm_bound}
%\abs{X_t'} = \abs{\inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}} \leq (L + \kappa_3) M_{(\alpha, G)} + (c_1 + \kappa_2 \varepsilon_{\mathrm{approx}}) \sqrt{M_{(\alpha, G)}},
%\end{align}
%where the last inequality follows from \eqref{eq_norm_bound_with_noise_version} and \eqref{norm_bound_on_good_event_generic}.
%Moreover, we also have 
%\begin{align}
%\abs{\EE_{|t}\sbr{\inner{\delta_t}{\theta_t - \theta^{\pi}} \mathbbm{1}_{\cG_t}}}
%& =  \abs{\EE_{|t}\sbr{\inner{\delta_t}{\theta_t - \theta^{\pi}}}  \mathbbm{1}_{\cG_t}}  \nonumber \\
%& \overset{(a)}{\leq} m \sbr{ (L + \kappa_2) \rho^\tau \norm{\theta_t - \theta^{\pi}}^2 + \kappa_2 \rho^\tau  \varepsilon^2_{\mathrm{approx}}} \mathbbm{1}_{\cG_t} \nonumber \\
%& \overset{(b)}{\leq} m \sbr{ (L + \kappa_2) \rho^\tau M_{(\alpha, G)} + \kappa_2 \rho^\tau  \varepsilon^2_{\mathrm{approx}}} \label{aux_sequence_conditional_expectation}
%\end{align}
%where $(a)$ follows from \eqref{ctd_bias_expectation}, and $(b)$ follows from \eqref{norm_bound_on_good_event_generic}.
%In view of \eqref{aux_sequence_norm_bound} and \eqref{aux_sequence_conditional_expectation}, we can now apply Azumaâ€“Hoeffding inequality to $\cbr{Y_t'}$ and obtain that conditioned on $\cF$, 
%\begin{align}
%\abs{
%Y_t'
%}
%& \leq t m \sbr{ (L + \kappa_2) \rho^\tau M_{(\alpha, G)} + \kappa_2 \rho^\tau  \varepsilon^2_{\mathrm{approx}}} \nonumber \\
%& ~~~~~~ + \sqrt{t}  \big[ (L + \kappa_3) M_{(\alpha, G)} + (c_1 + \kappa_2 \varepsilon_{\mathrm{approx}}) \sqrt{M_{(\alpha, G)}} \big]\sqrt{\log(2T / \delta)}, 
%~ \forall t \leq T.
%\label{aux_sequence_accumulation_raw}
%\end{align}
%with probability $1-\delta$.
%Through direct computation, one can verify that with
%\begin{align}
%& \sbr{ c_1 + \kappa_2  \varepsilon_{\mathrm{approx}} + L + \kappa_3} \sqrt{\alpha} \leq \frac{1}{8 \sqrt{\log(2T/\delta)}}, ~
%\alpha c_1 \leq 1, 
%~ \tau = \cO(t_{\mathrm{mix}}), \label{param_alpha_tau} \\
%& G \geq 4 \sqrt{\log(2T /\delta)} \sbr{(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}} + c_1 + 1, \label{param_choice_G}
%\end{align}
% \eqref{aux_sequence_accumulation_raw}, combined with the definition of $M_{(\alpha, G)}$ implies that conditioned on $\cF$,
%\begin{align}\label{aux_seq_accumulation_bounded}
%\abs{Y_t'} \leq G \sqrt{t}, ~ \forall t \leq T,
%\end{align}
%with probability $1-\delta$.
%%and  $G = 4 \sqrt{\log(2k /\delta)} \sbr{(L + \kappa_3) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \norm{\theta_0 - \theta^{\pi}}} + c_1 + 1$.
%Let us denote the event corresponding to \eqref{aux_seq_accumulation_bounded} by $\cG$.
%%Clearly, from the definition of $\cbr{\cG_t}$ and \eqref{aux_seq_accumulation_bounded} we obtain $\cap_{t \leq T} \cG_t \subseteq \cG$.
%Our next goal is to show that $Y_t = Y_t'$ over $\cG$ for every $t \leq T$, and consequently 
%\begin{align}\label{noise_accumulation_bound}
%\abs{Y_t} \leq G \sqrt{t}, ~ \forall t \leq T,
%\end{align}
%with probability $1-\delta$, conditioned on $\cF$.
%We proceed with an inductive argument. 
%The claim trivially holds at $t = 0$.
%Suppose the claim holds at $t$, then for any $\omega \in \cG$, 
%\begin{align*}
%Y_{t+1}(\omega) & = Y_t(\omega) + X_t(\omega) \\
%& = Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cG}(\omega) \\
%& \overset{(c)}{=} Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cbr{Y_t' \leq G \sqrt{t}}}(\omega) \\
%& \overset{(d)}{=} Y_t(\omega) + X_t(\omega) \mathbbm{1}_{\cbr{Y_t \leq G \sqrt{t}}}(\omega) \\
%& = Y_t(\omega) + X_t'(\omega) \\
%& = Y_{t+1}'(\omega),
%\end{align*}
%where $(c)$ follows from the definition of $\cG$ and \eqref{aux_seq_accumulation_bounded}, 
%and $(d)$ follows from the induction hypothesis that $Y_t (\omega) = Y_t'(\omega)$ for $\omega \in \cG$.
%In view of \eqref{norm_bound_on_good_event_generic}, \eqref{noise_accumulation_bound} and the definition of $\cbr{Y_t}$, we conclude that
%conditioned on $\cF$,
%% for $(\alpha, \tau, G)$ specified  in  \eqref{param_alpha_tau} and \eqref{param_choice_G}, 
%\begin{align*}
%\norm{\theta_t - \theta^{\pi}}^2
%\leq \norm{\theta_0 - \theta^{\pi}}^2 + 2 \alpha^2 c_1^2  + 2 \alpha G,
%\end{align*}
%with probability at least $1-\delta$.
%
%With \eqref{xx}, \eqref{xx} and \eqref{xx} in place, we are now ready to complete the proof.
%Clearly, with an inductive argument, one can readily show that with 
%\begin{align*}
%& \sbr{ c_1 + \kappa_2  \varepsilon_{\mathrm{approx}} + L + \kappa_3} \sqrt{\alpha} \leq \frac{1}{8 \sqrt{\log(2T/\delta)}}, ~
%\alpha c_1 \leq 1, 
%~ \tau = \cO(t_{\mathrm{mix}}),  \\
%& G \geq 4 \sqrt{\log(2k /\delta)} \big[(L + \kappa_3) (e + 1) \norm{\theta_0 - \theta^{\pi}}^2 + (c_1 + \kappa_2  \varepsilon_{\mathrm{approx}}) \sqrt{e + 1} \norm{\theta_0 - \theta^{\pi}}\big] + c_1 + 1, \\
%& 
% 2 \alpha^2 c_1^2  + 2 \alpha G \leq \norm{\theta_0 - \theta^{\pi}}^2.
%\end{align*}
%
%











