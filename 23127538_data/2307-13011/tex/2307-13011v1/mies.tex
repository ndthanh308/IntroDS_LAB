Most of pooling methods are based on a projection score for each vertex. This strategy is based on the assumption that we can learn  features characterizing relevant vertices for a given classification task. However, even if this hypothesis holds, two adjacent vertices may have  similar scores and the choice of the survivor is in this case arbitrary. An alternative strategy consists in merging similar nodes. Given a GNN with  hierarchical pooling, the graph sequence corresponds to an increasing abstraction from the initial graphs. Consequently,  vertices encoded at each layer of the GNN encode different types of information. Based on this observation, we decided to learn a similarity measure between adjacent vertices at each layer. Inspired by~\cite{verma2018feastnet}, we define the similarity at layer $l$ between two adjacent vertices $u$ and $v$ as $s^{(l)}_{uv} = exp(-\|\matrice{W}{l}.(x_u - x_v)\|)$ where $x_u$ and $x_v$ are the features of vertices $u$ and $v$, $\matrice{W}{l}$ is a learnable matrix and $\|.\|$ is the $L_2$ norm. 

        Given the maximal weighted matching $\mathcal{J}^{(l)}$ defined at level $l$, each vertex of $\graph{l}$ is incident to at most one edge of $\mathcal{J}^{(l)}$. If $u\in \mathcal{V}^{(l)}$ is not incident to $\mathcal{J}^{(l)}$  its features are just duplicated at the next layer. Otherwise, $u$ is incident to one edge $e_{uv}\in \mathcal{J}^{(l)}$ and both $u$ and $v$ are contracted at the next layer. Since $u$ and $v$ are supposed to be similar the attributes of the vertex encoding the contraction of $u$ and $v$ at the next layer must be symmetrical according to $u$ and $v$. To do so, we first define the attribute of $e_{uv}$ as 
        \begin{equation}
            x_{uv}=\frac{1}{2}(x_u^{(l)}+x_v^{(l)})
            \label{eq:edge_attribute}
        \end{equation} 
        where $x_u$ and $x_v$ are the features of vertices $u$ and $v$. The attribute of the merged vertex is then defined as $s_{uv}x_{uv}$.

        An equivalent update of the attributes of the reduced graph may be obtained by computing the matrix $\matrice{S}{l}$ encoding the transformation from $\graph{l}$ to $\graph{l+1}$. This matrix  can  be defined as $\matrice{S}{l}_{ii}=1$ if $i$ is not incident to $\mathcal{J}^{(l)}$, and by selecting arbitrary one survivor among $\{u,v\}$ if $e_{uv}\in \mathcal{J}^{(l)}$. If $u$ is selected we set $\matrice{S}{l}_{uu}=\matrice{S}{l}_{vu}=\frac{1}{2}s_{uv}$. All remaining entries of $\matrice{S}{l}$ are set to $0$.
        Matrix $\matrice{X}{l+1}$ can then be obtained using equation~\ref{eq:attributes_reduction}. We call this method MIESPool and the main steps are presented in Figures~\ref{fig:MIS_a} to~\ref{fig:MIS_c}.
