\section{Conclusions}\label{sec:conclusions}

In this work, we have presented a framework to train adversarially robust models on tabular data with focus on the fraud detection use case. We defined a perturbation space with a norm, as well as efficient methods to propagate perturbations to the feature engineered space of the model. Then we studied several attack search methods on such space, benchmarking them, and included the best method in the adversarial training loop to successfully developed models able to withstand strong attacks. This included studies of the best adversarial hyperparameters and, finally, an evaluation on test data to understand the trade-offs between model performance on clean data and attacked data for the adversarially trained model and the baseline. Our main conclusions are:
\begin{itemize}
    \item 
    Regarding the attack search method, we found greedy search to be the most effective.     
    \item Adversarial training prevents large performance drops (of about 30\%) against moderately large attacks and is essential against very aggressive  attacks (total loss in performance).
    \item Even for adversarially trained models with very strong attacks, their drop in performance on clean test data (with no attacks) is not larger than 7\%.
\end{itemize}
Interesting directions for future work are as follows. Our exploration of the hyperparameters was also mostly focused on parameters related to the adversarial components. Thus, it would be interesting to perform a more extensive search study of all the hyperparameters. Regarding comparisons with other attack search settings, we could also compare our results against a white box adversarial training setting and explore further attack search algorithms. As for the data, it would be interesting to study further datasets and use cases. It would also be interesting to be able to adversarially train a model
using a dataset that has surely undergone an adversarial change to test the robustness of a model adversarially trained before such a change happened.