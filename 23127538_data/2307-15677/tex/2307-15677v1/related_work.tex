\section{Related Work}
\label{sec:related-work}
\if false
\textbf{Target structure:} focus on the formulation that matters for the methods developed in the thesis
\begin{itemize}
    \item Brief overview of the history of the adversarial attacks and training literature. This may also include mentions to various formulations in a very summarized way.
    \item Maybe focused subsection detailing the formulation that matters for the paper (i.e., attacks that aim at switching the prediction of the model, and adversarial training). Other formulations are mentioned briefly before. (NOTE: we may also want to move details on the formulation to the methods section and keep this ultra short!?).
    \item Subsection on the literature on attacking strategies that are particularly relevant for this work, including the definition of norms (or maybe a different subsection for this point).
    \item Subsection on the literature on adversarial training strategies that are relevant to our method. Here we can also mention other strategies and why they are not suitable for our problem while the one we use is.
\end{itemize}
\fi

Adversarial robustness has recently gained a lot of interest with many studies addressing new strategies to design attacks \cite{goodfellow2015explaining, carlini2017evaluating, cheng2018queryefficient, cartella2021adversarial, hsja, ballet2019imperceptible, permuteattack, https://doi.org/10.48550/arxiv.2203.06414}. 
%
%Most literature focuses on image classification, with relatively little research tackling this concern in the tabular domain. One common approach is to get the adversarial sample closer to the original sample through the usage of customized distance metrics \cite{cartella2021adversarial, hsja}. Another approach is to compute and update perturbations based on the gradient of the model \cite{ballet2019imperceptible}, and to later enforce each feature's domain through additional operations (e.g., rounding to the nearest integer). Additionally, there are also genetic algorithms that introduce random noise (within a ball), sampled from each features acceptable set of values \cite{permuteattack}.
%
%\subsection{Problem Statement}
The problem is typically stated as follows. Consider the loss function $J(\theta,x,y)$, which corresponds to the cost of classifying an example $x$ of label $y$ with a model parameterized by $\theta$. 
%This cost function is what we (as the model developer) want to minimize when optimizing for $\theta$. 
The adversary's goal will often be to attack the model by adjusting the input, $\tilde{x}$, that is passed to the model 
%(for example, an image in an image classification scenario) 
so that, instead, it maximizes the loss function:
\begin{equation}
\max_{\tilde{x}\in x + \Delta} J(\theta,\tilde{x},y)
\label{eq:goal}
\end{equation}
Typically, these perturbations to $x$ cannot be arbitrary. In most scenarios, the generated example still needs to be close \enquote{enough} to the original input. In some contexts, certain features of $x$ cannot be edited or have to conform with the original input domain. We denote this allowable set of perturbations by $\Delta$. 

\subsection{Adversarial Attacks}
Typical classes of attacks in the literature are as follows. A white box attack assumes an attacker with full access to the model that can perform infinite queries, thus being able to devise strategies with high success rates \cite{carlini2017evaluating}. It has been argued, \cite{cheng2018queryefficient}, that such approaches are somewhat detached from reality: in practice, attackers have no access to the real model being attacked, and more importantly, there is a limited number of queries an attacker can perform before being flagged as suspicious and denied any further attempts. This latter approach has been coined as a \textit{\enquote{black-box}} attack, and has been studied in contexts that often resemble real world scenarios; i.e., when the attacker only has access to the model decision~\cite{cheng2018queryefficient}.  

% attack strategies depend on the model being attacked. FGSM and intuituion on the generation of attacks

%Suggestion:
%- attacks are just trying to solve the maximization problem.
%- Early proposed methods (FGSM) do this by solving the constrained problem replacing J with a linear approximation around x. (Essentially equivalent to a single step of projected gradient descent)
%- Later other more elaborate methods prorposed (e.g., PGD which is multiple steps)
%- Then short summary for tabular/trees! One sentence per paper at most!!

Adversarial attacks aim to solve the maximization problem posed in Equation~\eqref{eq:goal}: given a set of allowed perturbations, find the adversarial sample that will maximize the loss of the target model. One of the first proposed methods to generate adversarial attacks was the Fast Gradient Sign Method (FGSM) \cite{goodfellow2015explaining}. FGSM solves the constrained problem by replacing the cost function $J$ with a linear approximation around the victim sample $x$. This solution can be interpreted as being a one-step scheme for solving Equation~\eqref{eq:goal} \cite{madry2017towards}. Later approaches proposed to  develop a stronger attack by iteratively perfecting the adversarial perturbation by performing projected gradient descent (PGD) upon the negative loss function. Considering FGSM as a single gradient step, PGD tries to solve Equation~\eqref{eq:goal} with multiple projected gradient steps, where the result is projected onto the set of valid adversarial examples \cite{madry2017towards}. 
For attacks on non-differentiable models or black-box settings, an approach is to compute zeroth order gradients to generate attacks~\cite{cheng2018queryefficient}.

% Attacks reliant on the model gradients are naturally only applicable to differentiable models that the attacker has access to. This excludes tree-based models and adversarial attacks developed in a black box setting. A common approach in both of these settings is to compute the zeroth order gradients to generate attacks~\cite{cheng2018queryefficient}.

\subsection{Adversarial Training}
In order to increase the robustness of classifiers against adversarial examples, Szegedy et al. \cite{szegedy2014intriguing} propose \textit{adversarial training}. 
It consists of a form of data augmentation where the goal is to expose flaws in how the classifier models its decision function by generating examples that target these specific sub-spaces of the input space \cite{goodfellow2015explaining}.
When training the model, adversarial examples are generated iteratively. The primary reason for this to be done at train time is that the concept of an adversarial example depends on the parameters of the model. An adversarial example generated prior to training is, most likely, very different to one generated post training. The authors show that by training the model using a mixture between clean and adversarial samples the model can thus be regularized~\cite{szegedy2014intriguing,goodfellow2015explaining}.



