\section{Introduction}\label{sec:introduction}

Machine Learning (ML) models are vital in many security-sensitive applications \cite{papernot2016distillation}. In the financial services industry they are used to classify, for example, credit card transactions as legitimate or fraudulent~\cite{cartella2021adversarial}. In this naturally adversarial setting, fraudsters continuously adapt their techniques to bypass the system, while the system maintainers try to stop them. Thus, being able to train a stable model that withstands such attacks is valuable to avoid frequent and expensive model retraining operations with fresher data.

%Developing adversarial training for tabular data poses challenges. 
In tabular data domains, the raw features space containing some of the features available for the attackers to manipulate directly is often enriched, for model training, into an enlarged space via complex temporal and entity based aggregations~\cite{rnns_feedzai_10.1145/3394486.3403361}. This poses the additional challenge of incorporating the propagation of perturbations between two spaces in the adversarial training loop. Furthermore, designing appropriate attacks and attack search methods is essential to obtain models that are robust. Thus we propose an extension of adversarial training with all these ingredients
%in the adversarial training loop   and efficientcontinuously applying such transformations . 
% we propose an extension of adversarial training with suitable attacks and search methods, as well as attack propagation  for tabular data, as well as 
% that includes efficient approxi
%that includes, in the training loop, attack searches an injection on the raw data space and attack propagation to the enlarged feature engineered space 
and perform an empirical study on a credit card fraud detection dataset. Methods that tackle a similar setting do not address the propagation of perturbations and focus only on attacks~\cite{cartella2021adversarial}. 
%Thus, the perturbations space available for fraudsters to attack the model is not the same space available to train the model, which requires a more complex setup to propagate perturbations from original features to engineered features.

Our main contributions are as follows:
\begin{itemize}
    \item We parameterize adversarial perturbations and develop methods to search for strong attacks on tabular data (Sect.~\ref{subsec:adversarial-perturbations},~\ref{sub:pert_norm} and~\ref{subsec:perturbation-search-strategies}). 
    \item We formulate different approximation methods to efficiently update complex temporal aggregations as a result of adversarial perturbations of raw features (Sect.~\ref{subsubsec:perturbed-features-update-methods}).
    \item We introduce an adversarial training framework that increase the robustness of a classifier against a broad range of attacks (Sect.~\ref{subsec:adversarial-training}). Particularly, in our empirical study (Sect.~\ref{sec:experiments}) we develop a model that is well protected against multiple attack strategies generalizing well under new attacks in test. This is achieved at a cost in performance on test data without attacks between $4\%$ and $7\%$, (the adversarial robustness trade-off~\cite{tsipras2019robustness,zhang2019theoretically}), while avoiding disastrous losses if the developed attacks arise (from $30\%$ loss to random guessing performance, depending on the perturbations).
\end{itemize}
The paper is structured starting with a description of the methods (Sect.~\ref{sec:methods}) followed by the empirical study (Sect.~\ref{sec:experiments}), and a brief summary of related work (Sect.~\ref{sec:related-work}) before the conclusions (Sect.~\ref{sec:conclusions}).