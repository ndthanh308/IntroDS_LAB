\section{Experiments}\label{sec:experiments}
In this section we present our empirical study of the adversarial training method. In Section~\ref{subsec:data-preparation} we provide an overview of the datasets and its processing. 
%describe the dataset and how it was processed. 
Next, we describe the training of the input models, namely the baseline classifier (Section~\ref{subsec:baseline-classifier}) and the profile estimation models (Section~\ref{subsec:profile-estimation-models}). Finally, we discuss the results of the experiments conducted using adversarial methods, specifically addressing the benchmarking of attacks (Section~\ref{subsec:adversarial-attacks-benchmarking}) and the adversarial training experiments (Section~\ref{subsec:adversarial-training-experiments}).

\subsection{Data Preparation}
\label{subsec:data-preparation}

We use a proprietary dataset of a payment processing network for the task of credit card fraud detection. 
The dataset was sampled by card entity (the main grouping entity for profiles) including 100\% of the cards with fraudulent transaction and 5\% of the cards with exclusively legitimate transactions. After sampling, the resulting dataset contains $\sim$180 million transactions and a fraud rate of 0.29\%.

About 250 of the engineered features were designed by domain experts for a production system, including profiles by card and merchant, row mappings and higher order transformations (see Section~\ref{subsubsec-input-data}). One raw feature flags each transaction as Card Present (CP) or Card Not Present (CNP). We focus on CNP transactions since online transactions are more prone to fraud. After computing the engineered features, we only kept the CNP transactions ($\sim$34 million) with a fraud rate of 1.2\%. The data was split, with the first 10 weeks for training, followed by 4 weeks for validation and 6 weeks for testing.

\subsection{Baseline Classifier}
\label{subsec:baseline-classifier}
Tree-based models, such as gradient boosted decision trees, have been shown to perform better on tabular data in comparison to deep learning approaches \cite{https://doi.org/10.48550/arxiv.2207.08815}.
A state of the art implementation is given by the LightGBM library~\cite{ke2017lightgbm}, which we use to hyperparameter tune and train the baseline classifier on the clean train and validation sets without adversarial attacks. To assess its performance we compute the normalized partial AUC up to an FPR of 1\% and the recall at a fixed FPR of 1\% obtaining, respectively, 0.42 and 0.58.

\subsection{Profile Estimation Models}
\label{subsec:profile-estimation-models}

For the profile estimation models, we employ LightGBMRegressor models using the train and validation sets and evaluate them on the test set. For simplicity, hyperparameter tuning is done for a single profile regression model and then the hyperparameters are fixed to train the remaining models.

We evaluated the regression models using the $R^2$ metric. 
We compared the results with a baseline that always keeps the profiles unchanged under the perturbation and obtained large values for $R^2$ but also low ones. 
%Looking at the residuals 
%We observe large values for residuals when attempting to predict certain profiles. 
To identify under-performers, in Figure~\ref{fig:residual_x_r2} we show the absolute difference between the maximum and the minimum residuals for each profile estimator (normalized by the profiles's standard deviation). We highlight the region of interest in darker red (larger $R^2$ and smaller residuals). 
%
% Figure environment removed
%
We checked that by removing  the poorly estimated profile features (outside this region) from the training of the baseline classifier, there is no significant drop in the baseline classifier performance. Thus we discard these under-performing profiles in the remainder.

\subsection{Adversarial attacks benchmarking}
\label{subsec:adversarial-attacks-benchmarking}
In this section we compare the various attack strategies by analyzing their success rate \textit{versus} norm constraint trade-offs.
% Figure environment removed

\subsubsection{Random search baseline}
In the left plot of Figure~\ref{fig:success_rate_plots} we show the success rate versus the norm constraint for random search. In this benchmark, we ran 500 iterations of Random Search for each case. We display two curves to show the difference in success rates with and without temporal perturbations.
% % Figure environment removed
As expected, loosening the norm constraint produces higher success rates, i.e., more successful adversarial attacks. This figure also reveals two very effective discrete perturbations: resetting a card and switching a card (which also demands resetting it), indicated in the figure by the vertical dashed dotted lines. 
%We point out that, given the attack evaluation procedure, this analysis is unfavourable against this strategy, as higher norm constraints will have a larger number of attacks being considered since in these large spaces it is harder for the random search to saturate the bound. Nonetheless, 
Overall, the designed perturbation space can be well explored with a simple random search, finding attacks from as little as 20\% to 80\% success rate.  
%When looking at the figure 
The strategy reaches its peak well before it hits the maximum norm constraint (100), suggesting that smarter attack strategies may have space to find successful attacks at large norm constraints.

Before discussing other strategies, we note that the execution times observed for runs with temporal perturbations is four times larger than runs without them. Thus, we opt to ignore them in the remaining attack benchmarks (i.e., the norm will be capped at the corresponding maximum value of 82). In the adversarial training experiments we will, however, re-introduce them.

\subsubsection{Comparison of Black Box Attacks Under Partial Observability}
Regarding the results obtained using the remaining strategies,
%
% % Figure environment removed
% %
we show their performance in the right plot of Figure~\ref{fig:success_rate_plots}. The cost-efficient approach produces more efficient attacks than its greedy counterpart for most of the norm constraints. However, both still do not beat the random search baseline except for the largest norm constraint.
The greedy approach, on the other hand, is the best since it beats the benchmark for nearly all norm constraints. 
%Moreover, it even seems that if we allowed perturbations to be bigger we could hit higher success rates. 

\subsection{Adversarial training experiments}
\label{subsec:adversarial-training-experiments}
In this section we finally present the results for the full adversarial training experiments. For simplicity, we choose to fix the hyperparameters of the LightGBM algorithm to those found for the baseline classifier. 
%We argue that the results should not vary too much since a large portion of the data is still clean. 
Otherwise, we vary parameters related to the adversarial components, namely: the norm constraint used to generate attacks, the fraction of positives to attack in the training set and the frequency of attacks during the model training.
%<We start by exploring a few possible combinations of frequency and adversarial fractions for two norm constraints. Then we fix them to their best configuration and sweep over the norm constraint space.

%We expect the adversarial fraction to play an important role in balancing the performance in clean and adversarial settings - namely, as we increase the adversarial prevalence in the training set, we expect the model to yield a better adversarial performance at the expense of a worse performance on clean data. 
%We try adversarial fractions of 0.05 and 0.25.

% It is worth highlighting that we start this introductory exploration with a norm constraint of 30, meaning that no \enquote{strong} perturbations (like card resets) are present, and a norm constraint of 65, where all the perturbations are available. 

% After fixing the frequency and victim rate, we generate adversarial models using four distinct norm constraints. Once we obtain these adversarially robust models, we can assess their adversarial robustness. We attack each of these models with the same four norm constraints with no temporal perturbations and then, for fewer norm constraints, we conduct the same analysis including timestamp perturbations. We then score all models on a clean dataset and asses their performance over historical test data. % We perform this experiment first in a setting where there are no temporal perturbations allowed, and afterwards we perform a few final experiments also including them.

\subsubsection{Parameter Tuning}
We start by analyzing the results obtained in validation in four adversarial training configurations and norm constraints of 30 and 65 (respectively not allowing and allowing card perturbations - see Table~\ref{tab:costs}). 
One of the key metrics to indicate if the models are getting more robust is the adversarial pAUC, which we analyze, in Figure~\ref{fig:adv_auc_val}. We display two options for attacking the training set and updating the model: i) attack after a fixed number of gradient boosting model update rounds (i.e., periodically) and ii) attack after full convergence of the gradient boosting model update (evaluated by the pAUC on the validation set).
%
% Figure environment removed
%
The results show that larger adversarial fractions in the training set yield faster improvements in adversarial performance in early iterations, though both converge to similar values. Similar conclusions hold for a norm constraint of 65, which we do not display for simplicity. % In the right-most plot however, this is no longer the case. In either case, with enough iterations, there seems to be no benefit in including a larger adversarial fraction as the training until convergence with the smaller fraction always achieves a result as good or better than with the larger fraction.
%
% Conversely, we observe a similar phenomenon when measuring success rate: as we perform more rounds of adversarial training, our strategies find it increasingly harder to generate successful attacks, up until stabilization. Also observe that despite the different norm constraints, each with their own initial success rates, the best models converge toward similar values, showing that the model is able to protect itself against different attacks. 

It is also important to evaluate the performance of the adversarially trained model on clean data. In the right panel of Figure~\ref{fig:adv_auc_val} we visualize the trade-off between clean (horizontal axis) and adversarial (vertical axis) performance at each iteration, for each experiment (the last iteration of each configuration is indicated with a star symbol). 
We observe that clean performance is lower when including a larger adversarial fraction in the training set, as expected. If we consider the best configuration the one with the highest adversarial pAUC, we observe that we only incur a small clean pAUC drop (of about 0.005 pAUC points, i.e., $1.0\%$). Thus, we select the configuration with an adversarial fraction of 0.05 and train until convergence for the remaining experiments. 
%
% % Figure environment removed

% We notice that for a norm constraint of 30, the best iteration is the same across both values of $\alpha$ in most experiments. Within this norm constraint of 30, we notice that clean pAUC consistently assumes values of 0.40, making the experiment that offers the largest adversarial performance a prime candidate: training a model until convergence with an adversarial fraction of 0.05. On the right hand side of the table, $\alpha$ plays a larger role in determining which iteration is best. However, in this case the choice is more obvious, as the \enquote{0.05 until convergence} experiment seems to offer a much better adversarial performance with minimal clean pAUC costs for both $\alpha$ values. Given these results, we opted to use an adversarial fraction of 0.05 and to attack the model, on each iteration, only after it has converged. As previously described, we now fix these two parameters and vary the norm constraint used to generate the attacks during adversarial training. As a final note, we notice that, in general, both $\alpha$ values point us towards similar combinations of scores. For this reason, we will select the best iteration based solely on adversarial pAUC, for the sake of simplicity - that is $\alpha = 1$.

\subsubsection{Test Set Results}

In this section, we finally train models with various norm constraints and evaluate them on the test set.
Robustness should improve when stronger attacks are used to train the model, since a model robust against a strong attack is expected to also be robust against a weak one. We test this hypothesis by evaluating the pAUC on an attacked test set for each model.
%
% Figure environment removed
In the left plot of Figure \ref{fig:adv_pauc_norm_a} we show the adversarial pAUC for models trained with various norm constraints, evaluated on the test set attacked with various norm constraints without temporal perturbations (x-axis).  
We observe that the performance of the baseline model drops fast with increasingly larger attacks, which shows that it is not robust under such attacks.
As for the adversarially trained models, they are significantly more robust. We also see that training with a larger norm constraint does not necessarily produce a more robust model. For example, a model trained with a norm constraint of 65 achieves robustness similar to one with 82. This happens, in part, because both constraints allow the same types of perturbations (see Table~\ref{tab:costs}). 
Also, observe that a model trained with amount and categorical perturbations (norm constraint of 30) is more robust at small norm constraints than models trained with a less constrained norm. This is expected, since a model trained with a more constrained perturbation space should be more optimized to attacks on that space. %Thus, overall the trained models are significantly more robust to the perturbations used to train them.

% is still more robust than the baseline when it comes to defending against larger spaces with card resets - even though it was not exposed to this type of perturbations during training. Our reasoning is that this difference in success rate between the baseline and the model with norm constraint of 30 comes from the fact that our model should now be more robust against amount and some categorical perturbations. 
%All in all, we observe that all the trained models have become significantly more robust to the perturbations used to train them. 
%In other words, we observe that when we cross the norm constraint threshold to train a model with attacks that capture the entirety of the perturbation space (larger than 49), there is a limited gain in including larger attacks during adversarial training. 

%
% % Figure environment removed
In the right plot of Figure~\ref{fig:adv_pauc_norm_a} we show results for experiments with temporal perturbations. Since they are computationally demanding, we only explore two norm constraints, i) only allowing small perturbations up to 30 (i.e., without card resets/switches); and allowing all perturbations (100). 
The overall conclusions are qualitatively similar: as we include larger perturbations we get a model that is more robust to a broader spectrum of perturbations.

% % Figure environment removed
Finally, focusing on the points with a norm constraint of zero in the plots of Fig.~\ref{fig:adv_pauc_norm_a} we observe that, 
%to understand the looses in clean pAUC we note that...... 
%show it in Figure~\ref{fig:score_over_norm_a_ts} for the various models (with standard deviations estimated by bootstrapping the test set). The black line corresponds to the baseline model performance. 
as expected, we sacrifice some performance on clean data, since the blue point for the baseline evaluated on clean data, is above the adversarially robust models evaluated also on clean data. The observed drop ranges between $4\%$ and $7\%$ (the highest for the models trained with the most aggressive attacks injected).
%Also note that a model trained with timestamp perturbations and a norm constraint of 30 performs similarly to the corresponding model without %timestamp perturbations. 
%The same seems holds between a model trained with a norm constraint of 100 and a model with no temporal perturbations trained on the largest norm constraint. 


