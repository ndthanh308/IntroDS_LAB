\section{Preliminaries}

Throughout this work, we use the notation defined in Table \ref{tab:causal:prelim}.
\begin{table}[]
    \centering
    \begin{tabular}{|c|c|}
        \hline
         Symbol & Meaning \\
         \hline
         $\mathbf u_i$ & a random variable \\
         $u_i$ & a realization of a random variable $\mathbf u_i$ \\
         $\mathcal U$ & a space defining the values taken by realizations; e.g., $u_i \in \mathcal U$ \\
         $[N]$ & shorthand for the set $\{1, \hdots, N\}$ \\
         $\mathbf y_i$ & a random variable representing an outcome \\
         $\mathbf x_i$ & a random variable representing a covariate \\
         $\mathbf t_i$ & a random variable denoting group/treatment \\
         $F_{\mathbf u_i, \mathbf w_i}$ & joint distribution of $(\mathbf u_i, \mathbf w_i)$ \\
         $f_{\mathbf u_i, \mathbf w_i}(u, w)$ & joint density evaluated at $(u, v)$ \\
         $F_{\mathbf u_i | \mathbf w_i}$ & distribution of $\mathbf u_i$ conditional on $\mathbf w_i$ \\
         $F_{\mathbf u_i | w}$ & distribution of $\mathbf u_i$ conditional on the event $\mathbf w_i = w$ \\
         $\prob{\mathbf u_i = u}$ & probability mass for the event where $\mathbf u_i = u$ \\
         $\mathbf y_i(k)$ & potential outcome for an individual $i$ in group $k$ \\
         \hline
    \end{tabular}
    \caption{The notation table used within this work.}
    \label{tab:causal:prelim}
\end{table}
%We use the notation $\overset{\mathcal D}{\neq}$ to denote inequality in distribution, and $\overset{\mathcal D}{=}$ to denote equality in distribution. 


\subsection*{From \anova~to conditional $K$-sample testing}

Some of the earliest works that yielded the development of modern statistical inference addressed the $2$-sample testing problem. In its simplest form, the outcome $\mathbf y_i$ takes values $y \in \mathbb R$ and the group $\mathbf t_i$ takes values $t \in [2]$. The $2$-sample testing problem is defined as a test of:
\begin{align*}
    H_0 : F_{\mathbf y_i | 1} = F_{\mathbf y_i | 2}\text{ against }H_A : F_{\mathbf y_i | 1} \neq F_{\mathbf y_i | 2}.
\end{align*}
In the case where $F_{\mathbf y_i | t}$ are $\Norm{\mu_t, \sigma^2}$ and each $\mathbf y_i$ are independent samples, this problem is known as the one-way \anova \cite{Fisher1935}, and can be addressed via the $F$ test \cite{Agresti2015Feb,McCullagh1989Aug}. 

This was later relaxed via the development of the development of the conditional \anova, wherein the test can be sufficiently generalized to $K$-samples, and can incorporate other covariates via a likelihood ratio test \cite{Agresti2015Feb,McCullagh1989Aug}. In this case, we are interested in the $K$-sample conditional discrepancy, defined in Definition \ref{def:cond_kst}.
\begin{definition}[$K$-sample conditional discrepancy]
For outcomes $\mathbf y_i$ taking values $y \in \mathcal Y$, a grouping variable $\mathbf t_i$ taking values $t \in [K]$, and a set of covariates $\mathbf x_i$ taking values $x \in \mathcal X$, a $K$-sample conditional discrepancy exists if for some $k, l \in [K]$ and $x \in \mathcal X$, then:
\begin{align*}
    F_{\mathbf y_i | k, x} \neq F_{\mathbf y_i | l, x}.
\end{align*}
\label{def:cond_kst}
\end{definition}
Intuitively, a $K$-sample conditional discrepancy exists if, conditional on the covariates $x$, there is a difference in the outcome distributions for some pair of groups $k$ and $l$ conditional on the covariates $x$. A natural test can be developed with:
\begin{align*}
    H_0 &: \text{a conditional discrepancy does not exist for any $k, l, x$} \\
    H_A &: \text{a conditional discrepancy exists}.
    \numberthis\label{eqn:cond_kst_discrep}
\end{align*}
The $K$-sample conditional discrepancy can be intuited via linear regression, where assuming that $\pmb \epsilon_i  \distas{iid} \Norm{0, \sigma^2}$:
\begin{align*}
    \mathbf y_i = \mu_{\mathbf t_i}(\mathbf x_i) + \pmb \epsilon_{i},
\end{align*}
In the case where $\mu_{\mathbf t_i}(\mathbf x_i) = \mu_{\mathbf t_i} + f(\mathbf x_i) + g(\mathbf x_i, \mathbf t_i)$ (a model with a group-specific offset, a covariate-specific term delineated by $f$, and an interaction term delineated by $g$) and the functions $f$ and $g$ are assumed to be known, this can be tested directly via the likelihood ratio test \cite{Agresti2015Feb}. With this linear regression intuition in mind, the interpretation of such a test is very similar to that of the one-way \anova, and the hypothesis simplifies to a test of whether the group means $\mu_{t}$ and the interactions are equal for all groups against the alternative that for some pair of groups, they are unequal.

This approach was further relaxed by the conditional \manova, wherein we instead suppose that $\vec{\pmb \epsilon}_i \distas{iid} \Norm{\vec 0, \Sigma}$, and we allow $\mathbf y_i$ to take values $\vec y \in \mathbb R^{P}$, where for each dimension $p$, we model:
\begin{align*}
    \mathbf y_{ip} = \mu_{\mathbf t_i p} + f_p(\mathbf x_i) + g_p(\mathbf x_i, \mathbf t_i) + \pmb \epsilon_{ip}.
\end{align*}
A suitable test can be developed using this strategy via \cmanova \cite{Pillai1976,Pillai1977,Rao1951,Jobson1992}, detailed in Appendix \ref{app:methods:cmanova}, when the functions $f$ and $g$ are known (optionally, additional parametric assumptions may be placed on $\Sigma$ to yield an identifiable solution to the linear model \cite{McCullagh1989Aug,Jobson1992}). Conclusions do not generally apply without Gaussian assumptions, and the technique cannot be applied to high-dimensional datasets without additional parametric assumptions or regularization \cite{Cai2014Oct}.

A closely related problem to the $K$-sample conditional discrepancy problem is the conditional independence testing problem. It is framed as follows: we observe samples $(\vec y_i, \vec v_i, \vec x_i) \in \mathbb R^p \times \mathbb R^q \times \mathbb R^r$ for $i \in [n]$. We suppose the existence of three random variables $\vec{\mathbf y}_i$, $\vec{\mathbf w}_i$, and $\vec{\mathbf y}_i$, where $(\vec{\mathbf y}_i, \vec{\mathbf v}_i, \vec{\mathbf x}_i)$ are sampled independently and identically from $F_{\mathbf y_i,\mathbf v_i,\mathbf x_i}$. The two random variables $\vec{\mathbf y}_i$ and $\vec{\mathbf v}_i$ are independent conditionally on $\vec{\mathbf x}_i$ if and only if $F_{\mathbf y_i, \mathbf v_i | \mathbf x_i} = F_{\mathbf y_i | \mathbf x_i}F_{\mathbf v_i|\mathbf x_i}$. So, the conditional independence testing problem can be stated as:
\begin{align}
    H_0 : F_{\mathbf y_i, \mathbf v_i|\mathbf x_i} = F_{\mathbf y_i|\mathbf x_i}F_{\mathbf v_i|\mathbf x_i} \text{ against }H_A : F_{\mathbf y_i, \mathbf v_i|\mathbf x_i} \neq F_{\mathbf y_i|\mathbf x_i}F_{\mathbf v_i|\mathbf x_i}.
    \label{eqn:hypo_cond_ind}
\end{align}

Under general assumptions, consistent conditional independence tests of Equation \eqref{eqn:hypo_cond_ind} are consistent $k$-sample conditional discrepancy tests from Equation \eqref{eqn:cond_kst_discrep}, as explained in Remark \ref{rem:ind_test_kst}.

\begin{remark}[Consistent independence testing and consistent $k$-sample conditional discrepancy testing]
Suppose the setup described in \ref{setup}, and let $\mathbf v_i$ be a random $K$-dimensional vector, where for each $t \in [K]$:
\begin{align*}
    \mathbf v_{it} = g(\mathbf t_i) = \begin{cases}
        1 & \mathbf t_i = t \\
        0 & \mathbf t_i \neq t
    \end{cases}.
\end{align*}
Then for any $k, l \in [K]$ and $x \in \mathcal X$, $F_{\mathbf y_i | k, x} \neq F_{\mathbf y_i | l, x}$ if and only if $F_{\mathbf y_i, \mathbf v_i | \mathbf x_i = x} \neq F_{\mathbf y_i | \mathbf x_i = x}F_{\mathbf v_i | \mathbf x_i = x}$.
\label{rem:ind_test_kst}
\end{remark}

This trivial result proven explicitly by direct application of the main result from \citet{Panda2019Oct} allows us to tie together $k$-sample conditional discrepancy testing with conditional independence testing, and can therefore be used to construct a relaxation of the assumptions inherent in the \cmanova~framework (Gaussianity, and choice of the functions $f_p$ and $g_p$ for all $p$). The Generalized Covariance Measure (\gcm) \cite{Shah2018Apr} addresses this problem using a regression of $\mathbf y_i$ onto $\mathbf v_i$ conditional on $\mathbf x_i$. This strategy instead investigates vanishing correlation, a normalized covariance between the scaled residuals and $(\mathbf v_i, \mathbf x_i)$ \cite{Li2019Dec}. Further, \gcm~flexibly extends the intuition of \cmanova~to higher-dimensional settings and achieves consistency outside of gaussian contexts \cite{Shah2018Apr}.

Derivatives of the Hilbert-Schmidt Information Criterion (\hsic) leverage normalized conditional cross-covariance operators (such as \kcd) on reproducing kernel Hilbert spaces (RKHSs) \cite{Park2021Jul}, but are limited to $2$-class settings. Other generalizations leveraging RKHSs have been proposed, such as the Randomized Conditional Independence Test (\rcit) and the Randomized Correlation Test (\rcot) \cite{Strobl2019Mar}, but the finite-sample performance of these techniques in $k$-sample regimes and when the positivity criterion is not ensured \textit{a priori} are unknown. Further, two generalizations of Energy statistics, the conditional distance correlation (\cdcorr) and the partial distance correlation (\pdcorr) have been developed to subvert these limitations under the growing distance correlation framework \cite{Szekely2007Dec}. \pdcorr~provides numerous intuitive and computational advantages similar to \Dcorr \cite{Szekely2007Dec}, but unfortunately is not a dependence measure \cite{Szekely2014Dec}. \cdcorr~provides a test which has shown high testing power under a range of dependence structures, particularly when the relationship between the two random variables given the third is non-monotonic or non-linear \cite{Wang2015}. 

\subsection*{From average treatment effects to causal conditional discrepancies}

Juxtaposed by the developments in the conditional testing literature, the average treatment effect \cite{Rubin1974,Athey2017May} has long formed the backbone of many investigations in causal inference. We obtain the observed data $(y_i, t_i,  x_i)$, where $y_i \in \mathbb R$ is the outcome, $t_i \in \{1, 2\}$ is the (binary) treatment/intervention of interest (either treated or untreated), and $ x_i$ are the vector of baseline covariates (potential confounders), for individuals $i \in [n]$. To investigate this problem, we assume the existence of three random variables, $(\mathbf y_i, \mathbf t_i, {\mathbf x}_i)$, which are sampled independently and identically from some unknown data-generating distribution $F_{\mathbf y_i, \mathbf t_i, \mathbf x_i}$. Additionally, we assume the existence of two counterfactual random variables, $\mathbf y_i(1)$ and $\mathbf y_i(2)$, which represent the \textit{potential} outcomes under the two possible treatments. Conceptually, the theory of causal inference rests on the \textbf{consistency} assumption, which asserts that there is a single \textit{version} of each treatment level; e.g., if $\mathbf t_i = t$, then $\mathbf y_i = \mathbf y_i(t)$ \cite{Cole2009Jan}. Under this framework, the observed outcome is:
\begin{align*}
    \mathbf y_i &= \mathbf y_i(1) \indicator{\mathbf t_i = 1} + \mathbf y_i(2) \indicator{\mathbf t_i = 2},
\end{align*}
where only one of the potential outcomes will actually be realized in the observed data. The average treatment effect ($ATE$) is given by:
\begin{align}
    \gamma \triangleq ATE = \expect{\mathbf y_i(2) - \mathbf y_i(1)} = \expect{\mathbf y_i(2)} - \expect{\mathbf y_i(1)}
\end{align}
To test whether there is an $ATE$ implies the following hypothesis test:
\begin{align}
    H_0 : \gamma = 0 \text{ against }H_A: \gamma \neq 0.
\end{align}

The most obvious issue regarding the ATE is that, in practice, we observe realizations of $\mathbf y_i$ (the observed data), and \textit{not} $\mathbf y_i(t)$ (the counterfactual data); this is known as the ``fundamental problem of causal inference.'' When is $\expect{\mathbf y_i(t)}$ identifiable from the observed data, and how do we identify it?

The identifiability of $\expect{\mathbf y_i(t)}$ is ensured by the \textit{ignorability}, \textit{consistency}, \textit{positivity}, and the \textit{no interference} constraints. \textbf{Ignorability} is said to hold provided that $(\mathbf y_i(2), \mathbf y_i(1)) \indep \mathbf t_i \cond {\mathbf x}_i$; that is, the treatment is independent with respect to the observed baseline covariates. This condition will hold if the study executes it by design (such as in a perfect randomized trial) or all confounders have been recorded with the baseline covariates \cite{Holland1986}. The \textit{consistency} assumption is described above. \textbf{Positivity} holds if, for each possible treatment $t$, $Pr(\mathbf t_i = t \cond  {\mathbf x}_i =  x) > 0$ for any $ x$ in the support of ${\mathbf x}_i$ \cite{Rosenbaum1985,Rosenbaum2010}. Conceptually, any possible individual with a given covariate level \textit{could} have been observed in either the treated or untreated group. \textbf{No interference} asserts that there is no impact between the treatment assignments of \textit{other} participants on the potential outcomes of a given participant; this allows $\mathbf y_i(k)$ to be well-defined without reference to other individuals' treatment assignments \cite{Hernán2006}. When the consistency and no interference assumptions hold, these two assumptions are collectively referred to as the \textbf{Stable-Unit Treatment Value Assumption} (SUTVA) \cite{Hernán2006,Imbens2015,Rubin1980}. 

Under the $G$-computation formula \cite{Robins1986Jan}, if these assumptions hold, then:
\begin{align*}
    \expect{\mathbf y_i(t)} &= \expect{\expect{\mathbf y_i \cond \mathbf t_i = t,  {\mathbf x}_i}},
\end{align*}
and the ATE can be expressed as:
\begin{align}
    \gamma &= \expect{\expect{\mathbf y_i \cond \mathbf t_i = 2,  {\mathbf x}_i}} - \expect{ \expect{\mathbf y_i \cond \mathbf t_i = 1, {\mathbf x}_i}}.
    \label{eqn:g_comp}
\end{align}

As a general measure of treatment effects, the average treatment effect, as defined above, is rather limiting. First, treatment effects aren't necessarily constant across all individuals. One could conceptualize an intervention which has a strongly positive effect on younger people, but has a negative effect on older people. The average treatment effect ends up "averaging away" the heterogeneous effect of treatment on younger and older people, with the average treatment effect ending up being zero. This has been overcome by study of conditional (on baseline covariates, such as age) average treatment effects (the $CATE$) \cite{hahn1998role}: 
\begin{align*}
    \gamma_x \triangleq CATE(x) = \expect{\mathbf y_i(2) - \mathbf y_i(1) \cond \mathbf x_i = x} = \expect{\mathbf y_i(2) \cond \mathbf x_i = x} - \expect{\mathbf y_i(1) \cond \mathbf x_i = x}
\end{align*}
and a relevant test is whether, for each possible $x$:
\begin{align}
    H_0 : \gamma_x = 0 \text{ against }H_A: \gamma_x \neq 0,
    \label{eqn:hypo_cate}
\end{align}
but these too are not without limitations. These limiting definitions of treatment effects are well-defined for multivariate data, in that two multivariate random variables can differ in expectation (or not). However, treatments may impact outcomes beyond simple differences in expectation, such as differences in higher order moments. While one could augment the outcome of interest to be other useful functions such as the square of the outcome, it is unclear in practice how to search over this space of measurable functions to better characterize treatment effects. The setup that we follow for the remainder of this work is noted in Setup \ref{setup}.

\begin{setup}[Causal]
We obtain the observed data $(y_i, t_i, x_i)$, where $y_i \in \mathcal Y$ is the outcome, $t_i \in [T]$ is the nominal treatment, and $x_i \in \mathcal X$ are a collection of baseline covariates (potential confounders), for $i \in [n]$ individuals. We assume that the tuple $(y_i, t_i, x_i)$ is a realization of the random tuple $(\mathbf y_i, \mathbf t_i, \mathbf x_i)$, where:
\begin{enumerate}
    \item $\mathbf y_i$ is the $\mathcal Y$-valued random outcome, where $(\mathcal Y, \delta_y)$ is a metric space, %  $\mathcal F_y \triangleq \sigma(\mathbf y_i)$, and $F_{\mathbf y_i}$ is a probability measure induced by $\mathbf y_i$,
    \item $\mathbf t_i$ is the nominal treatment which is a $[T]$-valued random variable, %$\mathcal F_t \triangleq \sigma(\mathbf t_i)$, and $F_{\mathbf t_i}$ is a probability measure induced by $\mathbf t_i$,
    \item $\mathbf x_i$ are the $\mathcal X$-valued random baseline covariates, where for all $x \in \mathcal X$, $f_{\mathbf x_i}(x) > 0$,% where $(\mathcal X, \delta_x)$ are a metric space, $\mathcal F_x \triangleq \sigma(\mathbf x_i)$, and $F_{\mathbf x_i}$ is a probability measure induced by $\mathbf x_i$,
    \item For each individual, the treatment assignment mechanism is \textbf{consistent}, where the counterfactual random variables $\mathbf y_i(k)$ represent the potential outcome under treatment $k$, with distribution $F_{\mathbf y_i(k)}$.
    % is a probability measure induced by $\mathbf y_i(k)$, 
    The outcome is:
    \begin{align*}
        \mathbf y_i &= \sum_{k = 1}^K \mathbf y_i(k) \indicator{\mathbf t_i = k}.
    \end{align*}
\end{enumerate}
We assume that the tuples $(\mathbf y_i, \mathbf t_i, \mathbf x_i)$ are sampled independently and identically from some unknown data-generating distribution $F_{\mathbf y_i, \mathbf t_i, \mathbf x_i}$. Note that the potential outcome $\mathbf y_i(k)$ is \textit{not} a function of the treatment assignments for any other individuals $j \neq i$, implying that the \textbf{no interference} criterion is satisfied.
\label{setup}
\end{setup}

A natural generalization of the hypothesis given in Equation \eqref{eqn:hypo_cate} for the case where $K=2$ to data of arbitrary distribution is the causal conditional discrepancy test, first explored explicitly by \citet{Park2021Jul}:
\begin{align}
    H_0 : F_{\mathbf y_i(1)|\mathbf x_i = x} = F_{\mathbf y_i(2)|\mathbf x_i = x} \text{for all $x \in \mathcal X$    against    }H_A :  F_{\mathbf y_i(1)|\mathbf x_i = x} \neq F_{\mathbf y_i(2)|\mathbf x_i = x}\text{ for some $x \in \mathcal X$}.
    \label{eqn:hypo_causal_2s}
\end{align}
That this hypothesis can be relaxed to arbitrary $K$-sample tests, and can be generalized and tested via augmentations of any conditional independence test, serves as the motivation for this work.
