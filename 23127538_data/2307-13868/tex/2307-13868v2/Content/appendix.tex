\section{Theoretical Results}

Under the restriction that $\mathbf y_i(2)$ and $\mathbf y_i(1)$ differ at most by an offset, the hypothesis in Equation \eqref{eqn:hypo_causal_2s} is exactly equivalent to a test of whether there is a CATE:

\begin{remark}
Suppose that if $\mathbf y_i(1) | \mathbf x_i = x \distas{d} F_{\mathbf y_i(1) | \mathbf x_i = x}$, then there exists a constant $\epsilon_x$ s.t. if $\mathbf x_i = x$, then $\mathbf y_i(2) = \mathbf y_i(1) + \epsilon_x$. Then $\gamma_x \neq 0$ if and only if $F_{\mathbf y_i(1) | \mathbf x_i = x} \neq F_{\mathbf y_i(2) | \mathbf x_i = x}$.
\end{remark}
\begin{proof}

$\Rightarrow$) Suppose that $\gamma_x \neq 0$.

Then $F_{\mathbf y_i(1) | \mathbf x_i = x} \neq F_{\mathbf y_i(2) | \mathbf x_i = x}$, since two random variables differing in expectation implies a difference in distribution.

$\Leftarrow$) Suppose that $F_{\mathbf y_i(1) | \mathbf x_i = x} \neq F_{\mathbf y_i(2) | \mathbf x_i = x}$, and further for any $\mathbf y_i(1) | \mathbf x_i = x \distas{d} F_{\mathbf y_i(1) | \mathbf x_i = x}$, that if given $\mathbf x_i = x$, $\mathbf y_i(2) = \mathbf y_i(1) + \epsilon_x$ for some $\epsilon_x$.

Since $F_{\mathbf y_i(1) | \mathbf x_i = x} \neq F_{\mathbf y_i(2) | \mathbf x_i = x}$, then $\epsilon \neq 0$, as otherwise $F_{\mathbf y_i(2) | \mathbf x_i = x} = F_{\mathbf y_i(1) | \mathbf x_i = x}$.

Then:
\begin{align*}
    \gamma_x &= \expect{\mathbf y_i(2) - \mathbf y_i(1) \cond \mathbf x_i = x} \\
    &= \expect{\epsilon_x} = \epsilon_x \neq 0,
\end{align*}
as desired.
\end{proof}
Note that the forward direction of this proof does not require any of the assumptions that $\mathbf y_i(2)$ is an offset of $\mathbf y_i(1)$, which demonstrates that the $2$-sample test shown in Equation \eqref{eqn:hypo_causal_2s} is a less-restrictive characterization than a CATE in \eqref{eqn:hypo_cate}.

\printProofs


\section{Methods}
\label{app:methods}
\subsection{\Dcorr} The distance correlation \cite{Szekely2007Dec} is implemented using the \sct{hyppo}~package \cite{Panda2019Jul}. $p$-values are estimated using \cite{Shen2022Jan}.

\subsection{\cmanova}
\label{app:methods:cmanova} With the hypothesis in Definition \ref{def:hypo_causal_kst} in mind, we  investigate whether an ``alternative model'' including, for each dimension in the dataset, an intercept, a slope associate with the covariates, a group-specific intercept, and an interaction term between the group and the covariate is significant against a ``null model'' which includes only an intercept and a slope associated with the covariates. Conceptually, this corresponds to investigating whether the two groups differ (either conditionally or unconditionally on the covariates). 

With $\Sigma_{A,m}$ and $\Sigma_{0,m}$ the product of model variance for alternative and the null models respectively, under standard assumptions of multivariate least-squares regression (which are violated by the below-described simulations), the test in Definition \ref{def:hypo_causal_kst} is equivalent to testing:
\begin{align*}
    H_0 : \Sigma_{A,m} = \Sigma_{0,m}\text{ against }H_A : \Sigma_{A,m} \neq \Sigma_{A,m}. \numberthis \label{eqn:manova_hypo}
\end{align*}
Denote $\Sigma_{A, E}$ and $\Sigma_{0,E}$ to be the error of the variance matrix under the alternative model and the null model respectively, and let $B_j = \frac{\Sigma_{j,m}}{\Sigma_{j,E}}$. Note that if $D$ is the number of dimensions of the outcome, the error of the variance matrix is a $D \times D$ matrix, and will be non-invertible if $D > n - m_j$, where $n$ is the number of samples and $m_j$ is the number of parameters in model $j$. In the simulations described, this occurs for the high dimensional case (where $D = 100 = n$), so \cmanova~cannot be used. Under this framework, given that the models are nested, the Pillai-Bartlett trace \cite{Pillai1959,Pillai1976,Pillai1977} is defined:
\begin{align*}
    \Lambda_{Pillai}^{A,0} &= \Lambda_{Pillai}^A - \Lambda_{Pillai}^0 \\
    &= \trace{B_A\parens*{I_D + B_A}^{-1}} - \trace{B_0\parens*{I_D + B_0}^{-1}}
\end{align*}
The statistic $\Lambda_{Pillai}$ can be used to test the hypothesis in Equation \eqref{eqn:manova_hypo}
The statistic $\Lambda_{Pillai}$ can be used to test the hypothesis in Equation \eqref{eqn:manova_hypo} by rescaling to a statistic which is approximately $F$-distributed \cite{Pillai1959,Pillai1976,Pillai1977}. \cmanova~in this manuscript is performed using the \sct{anova.mlmlist} (analysis of variance for nested multivariate linear models) function in \sct{R}, provided by the \sct{stats} package \cite{Fox2018Oct,stats}.

\subsection{\rcit~and \rcot} \rcit~and \rcot~are implemented using the \sct{RCIT}~package \cite{Strobl2019Mar} in \sct{R}. $p$-values for both techniques are estimated using a permutation test, with the number of null replicates $N=1000$. All other settings leverage the default values.

\subsection{\gcm} \gcm~is implemented using the \sct{GeneralisedCovarianceMeasure}~package \cite{Shah2018Apr,Peters2022}. The regression method leveraged is ``xgboost'' \cite{xgboost}, and $p$-values are estimated using a permutation test, with the number of null replicates $N=1000$. All other settings leverage the default values.

\subsection{\kcd} \kcd~\cite{Park2021Jul} is implemented using the \sct{doDiscover}~package \cite{dodisc}. $p$-values are estimated using a permutation test, with the number of null replicates $N=1000$. All other settings leverage the default values.

\subsection{\cdcorr} \cdcorr~\cite{Wang2015} is implemented using the \sct{hyppo}~package \cite{Panda2019Jul}. The distance metric used is the Euclidean distance, and $p$-values are estimated using a permutation test, with the number of null replicates $N=1000$. All other settings leverage the default values.

\section{Unconditional Discrepancies}
\label{app:udice}
For the unconditional case, we assume the setup described in \ref{setup}.

\begin{definition}[Unconditional $k$-sample causal discrepancy]
Suppose the setup described in \ref{setup}. An unconditional causal discrepancy exists if for any $k, l \in [K]$:
\begin{align*}
    F_{\mathbf y_i(k)} \neq F_{\mathbf y_i(l)}.
\end{align*}
\label{def:udice}
\end{definition}
and is hereafter referred to as a $k$-sample causal discrepancy (with no qualifiers about conditionality). 

This implies a natural hypothesis test for a causal discrepancy of:
\begin{align}
    H_0 : F_{\mathbf y_i(k)} = F_{\mathbf y_i(l)} \text{ for all $k, l$}\text{   against   }H_A : F_{\mathbf y_i(k)} \neq F_{\mathbf y_i(l)}\text{ for some }k, l\label{eqn:hypo_udice}
\end{align}

Consistent tests of Equation \eqref{eqn:hypo_udice} and Definition \ref{def:hypo_cond_kst} are equivalent if further causal assumptions are satisfied:
\begin{lemma}[Consistent $k$-sample conditional discrepancy tests and $k$-sample causal discrepancies]
\label{lem:udice}
Assume the setup described in \ref{setup}. Further, suppose that:
\begin{enumerate}
    \item The treatment assignment is ignorable: $\parens*{\mathbf y_i(1), ..., \mathbf y_i(K)} \indep \mathbf t_i \cond \mathbf x_i$,
    \item The treatment assignments are positive for all levels of the covariates: $\prob{\mathbf t_i = k \cond \mathbf x_i = x} > 0$ for any $x \in \mathcal X$,
    \item The conditional effect of treatment is equivalent in \textit{direction} across all covariate levels; e.g., for a given $y \in \mathcal Y$, for all $x \in \mathcal X$:
    \begin{align*}
        f_{\mathbf y_i | k, x}(y) - f_{\mathbf y_i | l, x}(y) \geq 0 \text{   or   }f_{\mathbf y_i|l, x}(y) - f_{\mathbf y_i | k, x}(y) \geq 0,
    \end{align*}
    and the inequality is strict for some $x \in \mathcal X$ where $f_{\mathbf x_i}(x) > 0$.
\end{enumerate}
Then a consistent test of Equation \eqref{eqn:cond_kst_discrep} for a conditional discrepancy is equivalent to a consistent test of Equation \eqref{eqn:hypo_udice} for an unconditional causal discrepancy.
\end{lemma}
\begin{proof}
By definition, $f_{\mathbf y_i(k)}(y)$ can be expressed as a marginalization over the joint density $f_{\mathbf y_i(k), \mathbf x_i, \mathbf t_i}(y,t,x)$ with respect to $\mathbf x_i$ and $\mathbf t_i$:
\begin{align*}
    f_{\mathbf y_i(k)}(y) &= \int_{\mathcal X \times \mathcal T}
    f_{\mathbf y_i(k) | t, x}(y)f_{\mathbf x_i, \mathbf t_i}(x, t)\,\text d (x,t),
\end{align*}

Using the definition of conditional probability gives:
\begin{align*}
    f_{\mathbf y_i(k)}(y) &= \int_{\mathcal X \times \mathcal T}
    f_{\mathbf y_i(k) | t, x}(y)\prob{\mathbf t_i = t | \mathbf x_i = x}f_{\mathbf x_i}(x)\,\text d (x,t).
\end{align*}
That the treatment levels are positive for all levels of the covariates gives that this quantity is well-defined. By Fubini's theorem, and using that $\mathcal T = [K]$ is discrete:
\begin{align*}
    f_{\mathbf y_i(k)}(y) &= \int_{\mathcal X}\bracks*{\sum_{t \in [K]}
    f_{\mathbf y_i(k) | t, x}(y)\prob{\mathbf t_i = t | \mathbf x_i = x}} f_{\mathbf x_i}(x)\,\text d x.
\end{align*}

By ignorability and consistency, $f_{\mathbf y_i(k) | t, x}(y) = f_{\mathbf y_i | k, x}(y)$, so:
\begin{align*}
    f_{\mathbf y_i(k)}(y) &= \int_{\mathcal X}\bracks*{\sum_{t \in [K]}
    f_{\mathbf y_i | k, x}(y)\prob{\mathbf t_i = t | \mathbf x_i = x}} f_{\mathbf x_i}(x)\,\text d x.
\end{align*}
Finally, since $f_{\mathbf y_i | k, x}(y)$ is constant with respect to $t$:
\begin{align*}
    f_{\mathbf y_i(k)}(y) &= \int_{\mathcal X}
    f_{\mathbf y_i | k, x}(y)\bracks*{\sum_{t \in [K]}\prob{\mathbf t_i = t | \mathbf x_i = x}} f_{\mathbf x_i}(x)\,\text d x \\
    &= \int_{\mathcal X}
    f_{\mathbf y_i | k, x}(y) f_{\mathbf x_i}(x)\,\text d x, \,\,\,\, \sum_{t \in [K]}\prob{\mathbf t_i = t | \mathbf x_i = x} = 1.
\end{align*}
Then:
\begin{align*}
    f_{\mathbf y_i(k)}(y) - 
    f_{\mathbf y_i(l)}(y) &= \int_{\mathcal X}
    \bracks*{f_{\mathbf y_i | k, x}(y) - f_{\mathbf y_i | l, x}(y) }f_{\mathbf x_i}(x)\,\text d x\numberthis \label{eqn:udice_helper}.
\end{align*}
By assumption 3., $f_{\mathbf y_i(k)} \neq f_{\mathbf y_i(l)}$ precisely when $f_{\mathbf y_i | k, x} \neq f_{\mathbf y_i | l, x}$ for some $x$ where $f_{\mathbf x_i}(x) > 0$.
\end{proof}
This implies the following corollary:
\begin{corollary}[Equivalence of causal conditional discrepancies and unconditional causal discrepancies]
Suppose the setup described in Setup \ref{setup}, and further make the assumptions of Lemma \ref{lem:udice}. Then a causal conditional discrepancy in equivalent to an unconditional causal discrepancy.
\end{corollary}
\begin{proof}
Follows by direct application of Lemma \ref{lem:udice} and Lemma \ref{thm:CoDiCE}, by transitivity.
\end{proof}

In this sense, we can conceptualize a 
causal discrepancy as \textit{smoothing} a causal conditional discrepancy (based on the relative contributions of a given $x \in \mathcal X$ to the weighted average, weighted by way of $f_{\mathbf x_i}(x)$). When all of the conditional discrepancies in the distributions of groups $k$ and $l$ are equal in sign (across covariate levels), a test of an unconditional causal discrepancy is equivalent to a causal conditional discrepancy.

The $k$-sample testing problem \cite{Panda2019Oct} is given by:
\begin{align}
    H_0 : F_{\mathbf y_i|k} = F_{\mathbf y_i | l} \text{ for all $k, l$}\text{   against   }H_A : F_{\mathbf y_i | k} \neq F_{\mathbf y_i | l}\text{ for some }k, l
    \label{eqn:hypo_kst}
\end{align}

The results of Lemma \ref{lem:udice} coupled with Equation \eqref{eqn:hypo_kst} suggest the following corollary, which generalizes the concept of an average treatment effect (ATE) to arbitrary $\mathcal Y$ when the effect is further \textit{identical} across covariate levels:
\begin{corollary}[Consistent $k$-sample discrepancy testing and $k$-sample causal discrepancy testing]
\label{cor:cons_discrep_causal}
Make the assumptions of Lemma \ref{lem:udice}, and further suppose that for all $x \in \mathcal X$, that:
\begin{align*}
    f_{\mathbf y_i|k}(y) - f_{\mathbf y_i|l}(y) = f_{\mathbf y_i | k, x}(y) - f_{\mathbf y_i | l, x}(y),
\end{align*}
Then a consistent test of Equation \eqref{eqn:hypo_kst} is equivalent to a test of Equation \eqref{eqn:hypo_udice}.
\end{corollary}
\begin{proof}
Starting at Equation \eqref{eqn:udice_helper} and using the assumption:
\begin{align*}f_{\mathbf y_i(k)}(y) - 
    f_{\mathbf y_i(l)}(y) &= \int_{\mathcal X}
    \bracks*{f_{\mathbf y_i | k, x}(y) - f_{\mathbf y_i | l, x}(y) }f_{\mathbf x_i}(x)\,\text d x \\
    &= f_{\mathbf y_i|k}(y) - f_{\mathbf y_i | l}(y),\,\,\,\,\int_{\mathcal X}f_{\mathbf x_i}(x)\,\text d x = 1.
\end{align*}
Therefore, $f_{\mathbf y_i(k)} \neq f_{\mathbf y_i(l)}$ precisely when $f_{\mathbf y_i | k} \neq f_{\mathbf y_i | l}$.
\end{proof}
Conceptually, this result indicates that, so long as the impact of treatment (on the outcome distribution) is identical (in magnitude) across covariate levels, then a $k$-sample discrepancy test is a consistent test for a $k$-sample causal discrepancy. \citet{Panda2019Oct} gives that a consistent test of Equation \eqref{eqn:hypo_kst} can be practically achieved via independence testing (such as through \Dcorr) assuming that further assumptions are satisfied about $F_{\mathbf y_i|k}$ and $F_{\mathbf y_i|l}$ (such as finite first and second moments).

This result is noteworthy in that it would be simple to use the intuition of Lemma \ref{thm:CoDiCE} to conclude that under causal assumptions $k$-sample discrepancy tests would be equivalent to $k$-sample causal discrepancy tests intuitively. However, the fine details of the implications of the ignorability condition reveal this to be incorrect, and we need an additional (stronger) condition (such as the one given in Corollary \ref{cor:cons_discrep_causal}) for this to be the case. Conceptually, under the condition given, the ``smoothing'' that we noted in Equation \eqref{eqn:udice_helper} need not be relevant (because the effects are all the same) to attain the proper marginal distributions for $k$-sample discrepancy testing. 

A weaker, albeit less intuitive, condition that would also give the desired result would be that $\int_{\mathcal X}f_{\mathbf y_i | k, x}(y)f_{\mathbf x_i}(x)\,\text d x = f_{\mathbf y_i | k}$. A potentially stronger condition would be unconditional ignorability $(\mathbf y_i(1), \hdots, \mathbf y_i(K)) \indep \mathbf t_i$, which coupled with positivity, can be practically achieved via randomization.

\section{Simulations}
\label{app:sims}

\subsection{Setup}

$n=100$ samples are generated when balance $\pi_b$ is high or low ($\pi_b = 0.8$ or $\pi_b = 0.4$) and dimensionality is high or low ($D = 10$ or $D=101$).

\paragraph{Covariate sampling, $2$-group}

The treatment group $\mathbf t_i \distas{iid} \Bern{\pi}$ for $i \in [n]$ with $\pi = 0.5$.

The balance of a sample $\mathbf b_i \distas{iid} \Bern{\pi_b}$ for $i \in [n]$.

The values $\mathbf z_i$ are sampled independently as:
\begin{align*}
    \mathbf z_i | \mathbf b_i = b, \mathbf t_i = t \distas{d} \begin{cases}
        \Beta{10, 10} & b = 1 \\
        \Beta{2, 8} & b = 0, t = 0 \\
        \Beta{8, 2} & b = 0, t = 1
    \end{cases},
\end{align*}
and the covariate $\mathbf x_i = 2\mathbf z_i - 1$. Conceptually, $\pi_b$ of the points (ignoring the group assignments) have the same covariate distribution given by $2\Beta{10, 10} - 1$, and $1 - \pi_b$ of the points are in the right- and left-skewed distributions given by $2\Beta{2, 8}-1$ if the point is in group $0$ and $2\Beta{8, 2}-1$ if the point is in group $1$ respectively. Note further that by construction, the covariate distributions are symmetric about $0$; e.g., $f(x|0) = f(-x | 1)$ for all $x \in \mathcal X = [-1, 1]$. Figure \ref{fig:vm}(A) details the covariate generation procedure.

\paragraph{Covariate sampling, $K$-group}

The treatment probability vector $\vec \pi$ is:

\begin{align*}
    \pi_k = \begin{cases}
        \pi & k = 1 \\
        \frac{1 - \pi}{K - 1} & k > 1
    \end{cases}.
\end{align*}
The treatment group $\mathbf t_i \distas{iid} \Categorical{\vec \pi}$, where $\Categorical{\vec \pi}$ is the categorical distribution (e.g., $\prob{\mathbf t_i = k} = \pi_k$ for $k \in [K]$ when $\vec \pi$ is in the $K$-probability simplex). Conceptually, $\pi$ is the fraction of points in the first group, and $1 - \pi$ is the fraction of points that are evenly distributed amongst the remaining $K-1$ groups.

The balance of a sample $\mathbf b_i \distas{iid} \Bern{\pi_b}$ for $i \in [n]$.

The values $\mathbf z_i$ are sampled independently as:
\begin{align*}
    \mathbf z_i | \mathbf b_i = b, \mathbf t_i = t \distas{d} \begin{cases}
        \Beta{10, 10} & b = 1 \\
        \Beta{2, 8} & b = 0, t = 1 \\
        \Beta{8, 2} & b = 0, t \neq 1
    \end{cases},
\end{align*}
and the covariate $\mathbf x_i = 2\mathbf z_i - 1$. Conceptually, $\pi_b$ of the points have the same distribution given by $2\Beta{10, 10} - 1$, and $1 - \pi_b$ of the points assigned to group $1$ are in the right-skewed distribution given by $2\Beta{2, 8} - 1$, with $1 - \pi_b$ of the points in the groups $\{2, ..., K\}$ in the left-skewed distribution given by $2\Beta{8,2}-1$ (the first group is right-skewed, and the remaining groups have the same left-skewed distribution). Note that by construction, for all $t' \in \{2, ..., K\}$, that $f(x | 1) = f(-x | t')$, by a similar argument to above.

% Figure environment removed

\paragraph{Common characteristics and the signal to noise ratio} Several of the below simulations use the non-linear (but monotonic) sigmoid function, which is defined for $x \in \mathbb R$ as:
\begin{align*}
    \sigmoid(x) &= \frac{1}{1 + \exp(-x)} = \frac{\exp(x)}{\exp(x) + 1} \in [0, 1]
\end{align*}

Further, all of the simulations will control the signal to noise ratio in successive dimensions. Intuitively, if the level of signal is not decreased (per dimension) as the dimensionality increases, the signal to noise ratio of the simulation will increase to infinity. The vector $\vec \beta \in \mathbb R^{D}$ for a given dimensionality $D \in \natn$ is given by:
\begin{align*}
    \beta_p = \frac{2}{p^{q}}
\end{align*}
where $q > 1$. Conceptually, $\beta_p$ controls the amount of signal for the outcome in a given dimension $p \in [D]$, and $\beta_{p + 1} < \beta_p$ for all $p \in [D]$ (higher dimensions have \textit{less} signal). 

The noise in a given simulation is $\vec{\pmb \epsilon}_{i} \distas{iid} \Norm[D]{\vec 0, \frac{1}{4}I_D}$ unless otherwise indicated. Since the noise $\vec{\mathbf \epsilon}_{i}$ has variance $1$ for all dimensions $p \in [D]$, note that the SNR for all contexts except for \textit{Heteroskedastic} is, where the signal $\mathbf s_i$:
\begin{align*}
    SNR &= \frac{\expect{||\mathbf s_i||^2_2}}{\expect{||\pmb \epsilon_i||_2^2}} \\
    &\leq \frac{\norm{\vec\beta}_2^2}{\norm{\vec{\pmb \epsilon}_{i}}_2^2},\,\,\,\,\Delta \leq 1 \\
    &= \frac{2}{D}\sum_{p = 1}^D \frac{1}{p^x}.
\end{align*}
The right-most quantity is a $p$-series, and therefore converges for $x > 1$. Therefore, the SNR is finite for large $D$, and converges to $0$ as $D \rightarrow \infty$. That the SNR is finite for the heteroskedastic simulation is discussed in its respective paragraph.

\paragraph{Simulations are rotated to ensure tests are incorporating information across dimensions} To ensure the flexibility of the described techniques to high dimensional investigations so that no simulations are benefiting from only looking at the first dimension (which contains a much higher quantity of signal than successive dimensions), the outcomes are rotated. For all simulations, the outcome is $\mathbf R\vec{ \mathbf y_i}$, where $\mathbf R \distas{d} \text{Haar}_D$ is a Haar random orthogonal matrix. Conceptually, realizations $R$ of $\mathbf R$ are $D \times D$ rotation matrices drawn from the Haar distribution, which is the uniform distribution on special orthogonal matrices of dimension $D$ (SO$(D)$) \cite{scipy,haar}. 

\subsection{Simulation Settings}

\paragraph{Sigmoidal} Let $q = 1.5$. Conceptually, consider a plot of the outcome (per dimension) relative the covariate value, as in Figure \ref{fig:sigmoidal}. the outcome distribution (for a given dimension) is \textit{rotated} by $\Delta\pi$ radians (the \textit{effect size}, where $\Delta$ ranges from a minimum of $0$ to a maximum of $1$) for one group relative the other about the horizontal line with an intercept at $\frac{5}{p^{1.5}}$. This corresponds to the value attained by the sigmoid at the \textit{midpoint} $x = 0$ in the covariate distributions. That the covariate distributions are effectively \textit{reflections} of one another about the point $x = 0$ gives that there is no unconditional causal discrepancy when the effect size $\Delta = 1$.

The rotation factor $r = \cos(\Delta\pi)$ rotates the outcomes by $\Delta\pi$ radians about the horizontal line at $y = 0$. The outcomes are:
\begin{align*}
    \vec{\mathbf y}_i(t)| x &= \begin{cases}
        \left(r\left(5\sigmoid(8x) -  \frac{5}{2}\right) + \frac{5}{2}\right)\vec \beta + \vec{\pmb\epsilon}_i ,& t = 0 \\
        5\sigmoid(8x)\vec \beta + \vec {\pmb\epsilon}_i ,& t = 1
    \end{cases}
\end{align*}
By construction, note that $\vec{\mathbf y}_i(0)| x \eqdist \vec{\mathbf y}_i(1)| -x$ when $\Delta = 1$, and further, that $f(x | 1) = f(-x | 0)$. Therefore, $F_{\mathbf y_i(1)} = F_{\mathbf y_i(0)}$ unconditionally when $\Delta = 1$, and no unconditional causal discrepancy from Definition \ref{def:udice} exists. Figure \ref{fig:sigmoidal} illustrates the sigmoidal simulations from $\Delta = 0.0$ to $\Delta = 1.0$.


% Figure environment removed

\paragraph{Non-monotone} Let $q = 1.5$. Conceptually, There is only a covariate-specific effect for points when $\mathbf x_i \in [-.3, .3]$. This effect is $\Delta\vec\beta$ for points that are in group $1$, and $-\Delta \vec\beta$ for points in group $2$ ($\Delta$ is the \textit{effect size} ranging from $0$ to $1$). The outcomes are:
\begin{align*}
    \vec{\mathbf y}_i(t)| x &= \begin{cases}
        \vec{\pmb \epsilon}_i ,& x \not\in [-0.3, 0.3] \\
        \Delta \vec \beta + \vec{\pmb \epsilon}_i ,& t = 1, x \in [-0.3, 0.3] \\
        -\Delta \vec \beta + \vec{\pmb \epsilon}_i ,& t = 0, x \in [-0.3, 0.3]
    \end{cases}.
\end{align*}
For a given $x \in \mathcal X = [-1, 1]$, the covariate-specific effect is:
\begin{align*}
    \mathbf y_i(1) - \mathbf y_i(0)|x &= \begin{cases}
        0 ,& x \not\in [-0.3, 0.3] \\
        2\Delta \vec\beta ,& x \in [-0.3, 0.3]
    \end{cases},
\end{align*}
So a causal conditional discrepancy exists when $\Delta > 0$. Note that by using ignorability and positivity (which are true, by construction) and that the distributions are equal for $x \not\in [-0.3, 0.3]$:
\begin{align*}
    \expect{\mathbf y_i(1) - \mathbf y_i(0)} &= \Delta \vec\beta \bracks*{\int_{-0.3}^{0.3}f(x|0)\text d x + \int_{-0.3}^{0.3}f(x | 1)\text dx} \\
    &= 2\Delta \vec \beta \int_{-0.3}^{0.3}f(x | 0)\text dx \\
    &\begin{cases}
        > 0, & \Delta > 0 \\
        = 0, & \Delta = 0
    \end{cases},
\end{align*}
where the second to last line follows because $f(x|0) = f(-x|1)$ by construction, and the intervals are symmetric of the form $[-u, u]$ (for instance, using the substitution $u=-x$ for the right-most quantity gives the desired result). The last line follows because the interval $[-0.3, 0.3]$ is in the support of $\mathbf x_i$ conditional on the group $t$. Since the expectations are unequal, the distributions $F_{\mathbf y_i(1)} \neq F_{\mathbf y_i(0)}$. This shows that an unconditional causal discrepancy from Definition \ref{def:udice} also exists when $\Delta > 0$.  Figure \ref{fig:nm} illustrates the non-monotonic simulations from $\Delta = 0.0$ to $\Delta = 1.0$.

% Figure environment removed

\paragraph{$K$-group} Let $q = 1.1$. This simulation is conceptually identical to the Sigmoidal simulation. With $r = \cos(\Delta \pi)$ the rotation factor, the outcomes are:
\begin{align*}
    \vec{\mathbf y}_i | \mathbf t_i = t, \mathbf x_i = x &= \begin{cases}
        \left(r\left(5\sigmoid(8\mathbf x_i) -  \frac{5}{2}\right) + \frac{5}{2}\right)\vec \beta + \vec \epsilon_i ,& \mathbf t_i = 1 \\
        5\sigmoid(8\mathbf x_i)\vec \beta + \vec \epsilon_i ,& \mathbf t_i > 1
    \end{cases}.
\end{align*}
Again, when the effect size is at a maximum of $\Delta = 1$, the covariate distribution for group $1$ is a \textit{reflection} of the covariate distribution for groups $\{2, ..., K\}$ about the point $x=0$. Therefore there is no unconditional causal discrepancy. Figure \ref{fig:kclass} illustrates the $K$-group simulations from $\Delta = 0.0$ to $\Delta = 1.0$.


% Figure environment removed

% add why the SNR is finite for a given D
\paragraph{Heteroskedastic} Let $q = 1.5$. Conceptually, an unconditional causal discrepancy is present, in that the difference between $F_{y_i(1) | x} = F_{y_i(2)|x}$ can be characterized by $\mathbf y_i(1)$ having a larger covariance than $\mathbf y_i(2)$ (and the difference does not depend on $x$). The outcome model is:
\begin{align*}
    \vec{\mathbf y}_i \cond \mathbf t_i = t, \mathbf x_i = x &= \begin{cases}
        5\sigmoid\parens*{8\mathbf x_i}\vec \beta + \sqrt{1 + \Delta}\pmb{\epsilon}_i & \mathbf t_i = 0 \\
        5\sigmoid\parens*{8\mathbf x_i}\vec \beta + \pmb{\epsilon}_i & \mathbf t_i = 1
    \end{cases}
\end{align*}
for all $x \in \mathcal X$, indicating that an \textit{unconditional} causal discrepancy exists when $\Delta > 0$. Figure \ref{fig:hetero} illustrates the heteroskedastic simulations from $\Delta = 0.0$ to $\Delta = 1.0$. By construction, for any $D$, the SNR is $\Delta\sum_{p = 1}^D \frac{2}{p^q}$, so the SNR is finite for any $D$ for $\Delta \leq 1$.


% Figure environment removed

\section{Multinomial Regression Model} For our proposed inferential procedure, we use the baseline-category logit model for vector matching (\vm) \cite{Lopez2014}. With $\vec x_i$ the vector of covariates and group $k$ is arbitrarily the baseline, the model is:
\begin{align*}
    \log\parens*{\frac{r(l, x_i)}{r(k, x_i)}} &= \vec \beta_l^\top \vec x_i,
\end{align*}
where $r(l, x_i)$ is the generalized propensity score of item $i$ in group $l$. The generalized propensity scores are:
\begin{align*}
    r(l, x_i) &= \begin{cases}
    \frac{\exp\parens*{\vec \beta_l^\top \vec x_i}}{1 + \sum_{t \neq k}\exp\parens*{\vec \beta_t^\top\vec x_i}},& l \neq k \\
    \frac{1}{1 + \sum_{t \neq k}\exp\parens*{\vec \beta_t^\top\vec x_i}},& l = k 
    \end{cases} \numberthis \label{eqn:multinom}
\end{align*}
 The model is fit using the \sct{statsmodels} package \cite{seabold2010statsmodels} in the \sct{python} programming language to obtain estimates of the regression coefficients $\hat {\vec\beta}_l$ for all $l \neq k$. Estimated propensity scores $\hat r(l, x_i)$ are obtained for all samples $i$ and for all groups $l \in [K]$ by plugging in the estimated regression coefficients $\hat {\vec\beta}_l$ for $l \neq k$ to the Equations given in \eqref{eqn:multinom}. In the univariate regime, \vm~corresponds to identifying the highest/lowest propensity samples within a given treatment group for all treatment groups, finding the smallest/largest across all treatment groups for a given treatment group, and finally filtering points using the identified cutoffs, as illustrated in Figure \ref{fig:vm}(B) and according to Equation \eqref{eqn:vm}.