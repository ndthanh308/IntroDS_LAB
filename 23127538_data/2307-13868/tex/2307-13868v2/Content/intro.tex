%
% Betsy comments
% more information on the existing methods
% ground the definitions in the techniques used
% make clear straight up that the crossover effect is best
% 
\section{Introduction}

% purpose: introduce anova and anova-esque techniques
Many of the earliest developments in statistics have focused on differentiating sources of variability in scientific data. In one of the most impactful works of statistics, \citet{Fisher1935} pioneered the concept of the \anova, a test for determining whether two groups of outcomes fundamentally differ from one another (the \textit{two-sample test}). This early work was made possible for univariate outcomes through assumptions about how the outcomes could differ (that is, each of the two samples of outcomes were $iid$ within-group) and strong distributional assumptions. To overcome the former limitation, early efforts focused on the development of regression techniques to account for factors which influenced the distribution of each group of outcomes (so called `covariates'), loosening $iid$ assumptions to independence given the covariate \cite{Agresti2015Feb,McCullagh1989Aug}. Similarly, these regression techniques allowed further generalizations of \anova~ to the case where the outcomes have more than two groups, facilitating a so-called conditional \anova \cite{McCullagh1989Aug,Agresti2015Feb}.

% purpose: introduce generalizations with nonpar strategies
More recently, techniques have been developed to address other limitations of \anova. \manova, or multivariate \anova, extends the idea of determining differences in distribution from univariate to multivariate data via leveraging parametric Gaussian assumptions \cite{Pillai1976,Pillai1977,Rao1951}. The assumption of multivariate Gaussianity have been further loosened via independence tests \cite{Szekely2007Dec}, which can be combined with $iid$ assumptions to produce asymptotically consistent tests on arbitrary metric spaces. It has been illustrated that these tests can be augmented \cite{Panda2019Oct} to facilitate non-parametric $K$-sample testing. Correspondingly, numerous conditional independence tests have been proposed \cite{Strobl2019Mar,Wang2015,Shah2018Apr} which therefore enable non-parametric conditional $K$-sample testing. Unfortunately, these more complicated strategies are not without limitations. \citet{Shah2018Apr} demonstrate that there is no conditional independence test which can achieve both sensitivity and specificity without additional assumptions placed on the joint distribution of the outcomes (for each group) and the covariates. 
% Shah and peters:
% when (X, Y, Z) is continuously distributed, they proved that there is no conditional independence test that both:
% (a) controls type I error over any null distribution (i.e. any distribution of (X, Y, Z) with X⫫Y|Z) and
% (b) has better than random power against even one alternative hypothesis.
% informally, the test is either specific or sensitive to arbitrary hypotheses, but not both, without additional assumptions.

% purpose: causal stuff was developed concurrently, which elucidates such assumptions which overcome Shah and Peters
These developments in the statistical hypothesis testing literature have been juxtaposed by efforts to determine whether a predictor actually `causes' an effect on an outcome in the presence of other covariates \cite{Rubin1974,Athey2017May}. In the simplest possible case where the potential predictor is binary (it is either present, or it is not present), the predictor is often known as a `treatment,' and the causal estimand is known as an `average treatment effect' \cite{Rosenbaum1983Apr,Rosenbaum1984Mar} or a `conditional average treatment effect'~\cite{Robins1986Jan} (the effect of the treatment on the outcome, depending on the value taken by the covariates). These methods can be understood to address the limitations posed by \citet{Shah2018Apr} by placing additional assumptions on the joint distribution via a `potential outcome' or `counterfactual' framework, where the causal effect is measured in terms of differences (usually in the expectation) of the potential outcomes \cite{Hernán2006,Pearl2009Jan,Pearl2010Jul}. 

% purpose: Park work brings this to the conditional independence testing literature by stating said assumptions along with a conditional 2-sample test, but is not clear about how restrictive these assumptions are, or how robust the techniques are to when those assumptions fail (e.g., confounding)
An effort to harmonize the conditional independence testing and causal literature has been addressed by \textit{multivariate causal discrepancy testing}, which focuses on the question of determining whether two potential outcome distributions differ \cite{Park2021Jul}. It is unclear the extent to which these techniques generalize when the samples do not overlap in terms of upstream covariates (more specifically, the \textit{confounders}; so called ``covariate imbalance''), which is an  common data presentation in observational settings. Informally, it is unclear how robust these techniques are to the situation when the underlying causal assumptions fail, or are poorly reflected in the data sample. Moreover, there is little insight into formally characterizing causal discrepancy testing for more than two groups, which leaves a logical disconnect for many settings. Finally, it is unclear how sensitive and specific these tests are under non-linearities and non-monotonicities, which presents a substantial hurdle to their utility in high-dimensional settings.

% concept of k-sample causal testing and multivariate generalizations
Inspired by \citet{Park2021Jul} and \citet{Bridgeford2022Oct}, we introduce practical estimands which generalize the concepts of causal effects to data of arbitrary distribution (so long as realizations are measurable) and any number of treatment groups. 
First, we prove that, under general assumptions, any conditional independence test can be used to produce a $K$-sample causal conditional discrepancy test in both randomized clinical studies and observational studies. This result directly unifies the rich field of independence testing with causal discrepancy testing, and provides explicit theoretical motivation for the use of independence tests in \textit{causal structure learning} \cite{Robins2000Sep} with nominal treatment variables. Second, we illustrate additional assumptions needed to generalize these results to unconditional potential outcomes, drawing a parallel between causal conditional discrepancy testing with causal unconditional discrepancy testing in the same manner as the parallel between the average treatment effect and the conditional average treatment effect. Third, we propose simple and practical procedures for augmenting existing approaches for conditional independence testing to better suit the conditional discrepancy testing regime. 

Our proposed strategy, \ccdcorr, achieves substantial improvements in finite-sample validity and finite-sample power over existing approaches that are typically used for conditional discrepancy testing and conditional independence testing in the case of nominally-grouped data across regimes with both low and high degrees of covariate imbalance. We explore the values of this perspective across a range of simulations in both low and high-dimensional regimes, and show that our augmentations to existing conditional independence tests facilitate principled causal inference across a variety of multivariate contexts in which other techniques do not generalize, including non-linearities and non-monotonicities, higher moment differences across groups, and multi-group settings. Together, we believe that these results suggest the value of harmonizing causal perspectives with cutting-edge developments in non-parametric statistics. 
% Finally, we demonstrate the application of this framework to neuroimaging and genomics studies, indicating the practical value of our proposed procedures. 

