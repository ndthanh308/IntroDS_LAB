\section{Theory}

\subsection{From conditional $K$-sample tests to causal conditional discrepancy tests}

We can further relax this to the case where we have arbitrarily many treatment levels, by a modification of the definition presented by \citet{Park2021Jul}:
\begin{definition}[$K$-sample causal conditional discrepancy]
Suppose the setup described in Setup \ref{setup}. A $K$-sample causal conditional discrepancy exists if for any $k, l \in [K]$ and for any $x \in \mathcal X$:
\begin{align*}
    F_{\mathbf y_i(k)|\mathbf x_i = x} \neq F_{\mathbf y_i(l) | \mathbf x_i = x}.
\end{align*}
\label{def:CoDiCE}
\end{definition}
We explicitly incorporate the language \textit{causal} conditional discrepancy to emphasize explicitly that this effect represents a conditional (on the covariates) discrepancy in the \textit{potential} outcome distributions under nominal treatments. In this case for $K=2$, this definition is equivalent to the definition of a Conditional Distributional Treatment Effect (CoDiTE) given by \citet{Park2021Jul}. A suitable hypothesis test for a $K$-sample causal conditional discrepancy is a $K$-sample causal discrepancy test.
\begin{definition}[$K$-sample causal conditional discrepancy test]
    \label{def:hypo_causal_kst}
Suppose the setup in Setup \ref{setup}, where the nominal treatment variable $\mathbf t_i$ is a $[K]$-valued random variable where $K \geq 2$. A $K$-sample causal discrepancy test is:
\begin{align*}
    H_0 : F_{\mathbf y_i(k)|\mathbf x_i = x} = F_{\mathbf y_i(l)|\mathbf x_i = x}\text{ for all $k, l, x$   against     }H_A : F_{\mathbf y_i(k)|\mathbf x_i = x} \neq F_{\mathbf y_i(l)|\mathbf x_i = x}\text{ for some $k, l, x$.}
\end{align*}
\end{definition}
As it was for the CATE, we do not observe realizations of $\mathbf y_i(k)$ in practice, as these outcomes are only \textit{potential} outcomes under the treatment. This means that the hypothesis test in Definition \ref{def:hypo_causal_kst} is not directly testable in practice without additional assumptions.

Unlike the $K$-sample causal discrepancy test, which tests for discrepancies in the conditional (on only the covariates) distributions of the \textit{potential} outcomes, the $K$-sample conditional discrepancy test implied by Definition \ref{def:cond_kst} in Equation \eqref{eqn:cond_kst_discrep} for discrepancies in the conditional (on \textit{both} the covariates and the nominal treatment assignment $\mathbf t_i$) distributions of the \textit{realized} outcomes $\mathbf y_i$. Under causal assumptions, the $K$-sample conditional discrepancy test is a $K$-sample causal discrepancy test.

\begin{lemmaE}[$K$-sample conditional and causal discrepancy testing equivalence]
Assume the setup described in \ref{setup}. Further, suppose that:
\begin{enumerate}
    \item The treatment assignment is ignorable: $\parens*{\mathbf y_i(1), ..., \mathbf y_i(K)} \indep \mathbf t_i \cond \mathbf x_i$, and
    \item The treatment assignments are positive for all levels of the covariates: $\prob{\mathbf t_i = k \cond \mathbf x_i = x} > 0$ for any $x \in \mathcal X$.
\end{enumerate}
Then a consistent test of Equation \eqref{eqn:cond_kst_discrep} for a $K$-sample conditional discrepancy is equivalent to a $K$-sample causal conditional discrepancy test in Definition \ref{def:hypo_causal_kst}.
\label{thm:CoDiCE}
\end{lemmaE}
\begin{proofE}
Recall that for Equation \eqref{eqn:cond_kst_discrep} and Definition \ref{def:hypo_causal_kst}, that since the density fully determines a distribution, that a difference in distribution exists $\iff$ a difference in the densities exists.

For any $k, l \in [K]$ and $x \in \mathcal X$, note that Equation \ref{def:hypo_causal_kst} can be expressed in terms of the relevant densities from Equation \eqref{eqn:cond_kst_discrep}:
\begin{align*}
    f_{\mathbf y_i(k)|\mathbf x_i = x}(y) - f_{\mathbf y_i(l)|\mathbf x_i = x}(y) &= \sum_{t \in \mathcal [K]}\parens*{f_{\mathbf y_i(k)| t, x}(y) - f_{\mathbf y_i(l)| t, x}(y)}\prob{\mathbf t_i = t | \mathbf x_i = x}.
\end{align*}
By ignorability and consistency, $f_{\mathbf y_i(k) | t, x} = f_{\mathbf y_i | k, x}$ for all $t \in [K]$, so:
\begin{align*}
    f_{\mathbf y_i(k)|\mathbf x_i = x}(y) - f_{\mathbf y_i(l)|\mathbf x_i = x}(y) &= \sum_{t \in \mathcal [K]}\parens*{f_{\mathbf y_i| k, x}(y) - f_{\mathbf y_i| l, x}(y)}\prob{\mathbf t_i = t | \mathbf x_i = x}.
\end{align*}
Positivity gives that this quantity is well-defined for any $x \in \mathcal X$. Removing constants:
\begin{align*}
    f_{\mathbf y_i(k)|\mathbf x_i = x}(y) - f_{\mathbf y_i(l)|\mathbf x_i = x}(y) &= \parens*{f_{\mathbf y_i| k, x}(y) - f_{\mathbf y_i| l, x}(y)}\sum_{t \in \mathcal [K]}\prob{\mathbf t_i = t | \mathbf x_i = x} \\
    &= f_{\mathbf y_i| k, x}(y) - f_{\mathbf y_i| l, x}.(y),\,\,\,\,\sum_{t \in \mathcal [K]}\prob{\mathbf t_i = t | \mathbf x_i = x} = 1
\end{align*}
Therefore, $f_{\mathbf y_i(k)|\mathbf x_i = x}(y) \neq f_{\mathbf y_i(l)|\mathbf x_i = x}(y)$ for some $k, l \in [K]$ if and only if there exists some $x \in \mathcal X$ s.t. $f_{\mathbf y_i | k, x}(y) \neq f_{\mathbf y_i | l, x}(y)$.
\end{proofE}

\subsection{Hypothesis Testing}

By Lemma \ref{thm:CoDiCE}, under causal assumptions, conditional $K$-sample tests can be used for inference about potential outcomes. We can tie this into ith $K$-sample causal discrepancy testing, with \textit{any} consistent conditional independence test via the following corollary:
\begin{corollaryE}[Consistent Conditional Independence Tests and Consistent $K$-sample causal discrepancy tests]
Assume the setup described in Setup \ref{setup}. Further, suppose that:
\begin{enumerate}
    \item The treatment assignment is ignorable: $\parens*{\mathbf y_i(1), ..., \mathbf y_i(K)} \indep \mathbf t_i \cond \mathbf x_i$, and
    \item The treatment assignments are positive for all levels of the covariates: $\prob{\mathbf t_i = k \cond \mathbf x_i = x} > 0$ for any $x \in \mathcal X$.
\end{enumerate}
Then a consistent conditional independence test of Equation \eqref{eqn:hypo_cond_ind} is equivalent to a consistent $K$-sample causal conditional discrepancy test in Definition \ref{def:hypo_causal_kst}.
\label{cor:cond_ind}
\end{corollaryE}
\begin{proofE}
Note that with $\vec{\mathbf v}_i = g(\mathbf t_i)$ as-defined in Remark \ref{rem:ind_test_kst}, then direct application of Theorem \ref{thm:CoDiCE} followed by Remark \ref{rem:ind_test_kst} give that for some $k, l \in [K]$ and $x \in \mathcal X$, then $F_{\mathbf y_i(k)|\mathbf x_i = x} \neq F_{\mathbf y_i(l)|\mathbf x_i = x} \iff F_{\mathbf y_i | k, x} \neq F_{\mathbf y_i | l, x} \iff F_{\mathbf y_i, \mathbf v_i | \mathbf x_i} \neq F_{\mathbf y_i | \mathbf x_i} F_{\mathbf v_i | \mathbf x_i}$, as-desired.
\end{proofE}

Briefly, as long as we can identify a consistent conditional independence test for a related problem of identifying the independence of $\mathbf y_i$ and $\mathbf v_i$ conditional on the covariates $\mathbf x_i$, and the causal assumptions of ignorability and positivity are applicable, then we can obtain evidence for a $K$-sample causal conditional discrepancy on the basis of a conditional independence test. Note that this theorem provides guarantees for the hypotheses to be equivalent; it does \textit{not} make guarantees about a particular conditional independence test being consistent in a given setting. For instance, we may need further qualifications for a conditional independence test to be a consistent test, such as finite first and second moments of the outcomes and the baseline covariates.

\subsection{$K$-sample Causal Unconditional Discrepencies}
\label{sec:causal_uncond_discrep}
The above approaches readily generalize to discrepencies, in which $F_{\mathbf y_i(k)} \neq F_{\mathbf y_i(l)}$ for some $k, l$ with additional assumptions (called an unconditional causal discrepancy). Appendix \ref{app:udice} derives the above results and sufficient additional assumptions for the approaches described herein to the unconditional case. Intuitively, our main result is that tests for $K$-sample conditional discrepancy tests are equivalent to $K$-sample (unconditional) causal discrepency tests if differences in the covariate distributions (across groups) can be characterized by shifts in the density of the outcome (conditional on the covariates) that are in the same ``direction'' across all levels of the covariate(s) $\mathbf x_i$. 

Somewhat contrary to intuition, under standard causal assumptions, unconditional conditional discrepancy tests are \textit{not} equivalent to unconditional causal discrepancy tests without the restrictive assumption that these ``shifts'' are constant across all covariates. Under this more restrictive framework, a similar approach to corollary \ref{cor:cond_ind} gives that unconditional discrepancy tests can be used to test for unconditional causal discrepancies. In the more general case, as long as the discrepancy is in the same direction (and may be of different magnitudes), conditional discrepancy tests can be used for unconditional causal discrepancy tests. Together, these results give us the ability to characterize unconditional causal discrepancies to causal conditional discrepancies in much the same way as characterizations of the ATE to CATEs.

\section{Numerical Experiments}
\subsection{Conditional independence tests}
\label{sec:statistics}

In this paper, we consider the effectiveness of \cmanova, \kcd, \rcit, \rcot, \gcm, and \cdcorr~for $K$-sample causal conditional discrepancy testing. We also benchmark these strategies against the distance correlation, \Dcorr, which facilitates a strategy for unconditional causal discrepancy testing (see Appendix \ref{app:udice}). The reason that we compare to \Dcorr~is that it is possible that by ignoring covariates entirely, we may see increased finite-sample testing power due to computational efficiency under certain contexts, despite a loss of testing validity or power under others. See Appendix \ref{app:methods} for details on the methods and statistical tests employed.

\subsection{Causal Assumptions in Observational Studies}
\label{sec:matching}

The results discussed in Lemma \ref{thm:CoDiCE} and Corollary \ref{cor:cond_ind}, which allow us to make causal conclusions on the basis of the outcome of a $K$-sample conditional discrepancy test or conditional independence test, apply only in the event that the causal assumptions hold: \textit{ignorability}, \textit{positivity}, and \textit{no interference}. Other than the no interference criterion, both the ignorability and positivity assumptions can be readily reasoned in randomized trials: if we randomly assign people to a treatment or control group (conditionally or unconditionally on baseline covariates, as long as we \textit{measure} the covariates we use to randomly assign treatment or controls), both the ignorability and positivity assumptions can be satisfied \cite{Oakes2013}. The \textit{no interference} criterion can be reasoned through via domain expertise by noting whether or not the treatment group of individuals impacts the outcomes of other individuals \cite{Rosenbaum2007Mar}. Consider, for instance, a case where we want to measure the impact of a vaccine, and we treat individuals with a vaccine who will only be exposed to other vaccinated individuals. The no interference assumption could be violated since the treatments of the vaccinated individuals could impact the potential outcomes of the other individuals.

However, what happens if our data is not randomized, but is observational, in that the researcher does not have explicit control over ensuring treatment assignments in a randomized fashion? Intuitively, ignorability can be conceptualized as \textit{no unmeasured confounding}, in that any confounding variables must be collected in the observed data \cite{Greenland2009}. This criterion can be limiting in that while strong assumptions can be made about unobserved variables, it is unverifiable in the obtained data sample. Causal inference in these observational settings is therefore limited by the sensitivity of the ignorability assumption to these unmeasured variables \cite{Rosenbaum1984Mar}. The no interference criterion is similar in interpretation as it is for the randomized trial case. Ignorability aside, the positivity assumption can prove difficult to reason through. Conceptually, positivity asserts that for all levels of the covariate under study, there is a non-zero probability of a sample obtaining the treatment \textit{or} the control. This assumption is impossible to verify in an observational study, since it is statistically a \textit{pre-hoc} criterion. However, we can take principled approaches to identify subsamples of the data upon which this might be the case. Attempts to rectify this assumption are practically addressed via matching \cite{Petersen2012Feb,Stuart2010Feb}.

Through matching, we attempt to identify a subset of the samples from an observational study upon which positivity \textit{might} hold by trimming subjects whose treatment group assignment is deterministic in the empirical sample based on their observed covariates \cite{Kang2016Jan}. A number of techniques have been proposed to ascertain positivity from observational studies \cite{Stuart2010Feb}. In this investigation, we choose to address this via vector matching (\vm) as proposed by \citet{Lopez2014}, which is a strategy for matching subjects across multiple treatments and is closely related to propensity score matching for more than two treatments. In the case where the treatment is nominal, the generalized propensity score $r(t, x)$ is the probability $Pr(\mathbf t = t | \mathbf x = x)$ of being assigned to treatment group $t$ given the baseline covariates $x$. For a given individual with baseline covariates $x_i$, a set of generalized propensity scores $\mathcal R(x_i) = \left\{\hat r(t, x_i)\right\}_{t \in [K]}$ are estimated using a multinomial regression model. For each treatment $t \in [K]$, we compute the following quantities:
\begin{align*}
    l(t) &\triangleq \max_{t' \in [K]}\parens*{\min_{i \in [n] : t_i = t'}\set*{\hat r(t, x_i)}},\,\,
    h(t) \triangleq \min_{t' \in [K]}\parens*{\max_{i \in [n] : t_i = t'}\set*{\hat r(t, x_i)}} \numberthis \label{eqn:vm}
\end{align*}
Individuals $i$ with $\hat r(t, x_i) \not \in \left(l(t), h(t)\right)$ for any $t \in [K]$ are discarded from successive hypothesis testing. Intuitively, \vm~ensures that no retained individuals have covariates which occur with extremely low probability (or extremely high probability) for any particular group. Strategies which first pre-process observational data using \vm~are henceforth referred to as ``causal'', and we use this strategy for \ccdcorr. The model employed for multinomial regression is given in Appendix \ref{app:sims}.

\subsection{Simulations}
\label{sec:sims}

We empirically investigate the flexibility, validity, and accuracy of $K$-sample causal conditional discrepancy testing using simulations that extend beyond our theoretical claims and mirror observational frameworks. In Figure \ref{fig:simsetup}, we illustrate the simulations under which our proposed techniques are evaluated. $n=100$ samples are collected from the indicated statistical model, with variable ``dimensionality'' and ``balance''. The ``dimensionality'' indicates the number of dimensions of the outcome, and the ``balance'' indicates the level of similarity for the covariate distributions of the different treatment groups. The $y$-axis indicates the outcome in the first dimension, and the $x$-axis indicates the covariate associated with a single sample point. The solid line indicates the average outcome for a given group at a particular covariate level. The causal conditional discrepancy that we wish to detect is illustrated by the difference between these two lines for a given covariate level (the \textit{signal}).

% Figure environment removed

For successive dimensions, the relationship between the outcome and the covariate for a given group remains the same; however, the signal to noise ratio (per-dimension) decreases as the dimensionality increases. We run each simulation in a low dimensionality ($D = 10$) and high dimensionality ($D = 101$) regime. The dimensionality for high dimensional simulations was chosen such that the simulation exhibits the high-dimensionality, low sample size phenomenon (HDLSS) \cite{Hall2005}, which presents a challenge for parametric techniques as the number of dimensions exceeds the number of samples. Hence, \cmanova-like strategies cannot be employed without restrictive assumptions on the true underlying model \cite{Chi2013}, which may not be practical for many high-dimensional datasets (such as connectomics or genomics datasets) or new datasets which are not yet fully understood. 

Figure \ref{fig:simsetup}\textbf{.I.} \textit{Sigmoidal} indicates a simulation with a sigmoidal relationship between the outcome at a given dimension and the covariate. The effect size measures the degree to which the second group is \textit{rotated} about the other group (effect size of $0$ corresponds to the two distributions being identical, and effect size of $1$ corresponds to the second group being rotated $180$ degrees). This simulation was designed to test the sensitivity of the included tests to a causal conditional discrepancy when there is no unconditional causal discrepancy, as effect sizes of both $0$ and $1$ correspond to the unconditional outcome distributions being identical across both groups across all dimensions. This is due to the fact that the effect that is introduced is a rotation of the outcomes about a particular covariate level. Figure \ref{fig:simsetup}\textbf{.II.} \textit{Non-Monotone} shows a simulation with a non-monotone relationship between the outcome at a given dimension and the covariate. In this case, an unconditional causal discrepancy exists. We would expect that high performing techniques will perform as well as or better than unconditional causal discrepancy tests in this context. Figure \ref{fig:simsetup}\textbf{.III.} \textit{K-Group} indicates a simulation with three groups, wherein the first group has a different covariate distribution from the second two groups. The effect size again measures the degree to which the first group is rotated about the other two groups (as in \ref{fig:simsetup}\textbf{.I.} Sigmoidal). Figure \ref{fig:simsetup}\textbf{.IV.} \textit{Heteroskedastic} indicates a simulation where the covariance of the outcome for one group exceeds that of the other group for each possible value of the covariate, despite the fact that the average outcome (conditional on the covariate value) is identical across both groups. The effect size measures how many times larger the covariance is for the first group than the second group. 

These simulations were conducted under various balance contexts, where the balance indicates the fraction of samples which have the same covariate distribution. The remaining samples are collected from asymmetric covariate distributions. When balance is low (shown in Figure \ref{fig:simsetup}(A), with a balance of $40\%$) this induces group-specific imbalance in which the treatment groups being compared do not have common support. For \ccdcorr, the samples are first filtered using \vm, as described in Methods \ref{sec:matching}. The effects of this pre-processing step are indicated in Figure \ref{fig:simsetup}(B). Note that \vm~pre-processes the samples so that both groups have common covariate support.

The outcomes are finally randomly rotated in $D$-dimensional space to ensure that the techniques are incorporating information across dimensions (as the top dimension contains more signal than successive dimensions) \cite{haar}. Appendix \ref{app:sims} provides technical details for each simulation employed, and illustrates similar plots across a range of effect sizes for a given setting.
