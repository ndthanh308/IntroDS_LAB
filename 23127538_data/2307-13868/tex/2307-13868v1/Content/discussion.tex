\section{Discussion}

In this work, we introduce a conceptual framework for defining and understanding causal sources of heterogeneity in outcomes across nominal treatment regimes. This work builds upon the efforts of \citet{Park2021Jul}, and provides a theoretical framework in which conditional independence tests produce $k$-sample causal conditional discrepancy tests. We give sufficient conditions for $k$-sample (unconditional) causal discrepancy tests to be equivalent to $k$-sample causal conditional discrepancy tests, which generalizes the relationship between the ATE and the CATE to multivariate frameworks.  While the theoretical framework which we elucidated is almost certainly violated in observational investigations, we demonstrate how augmentation of conditional independence tests via \vm~yields tests which achieve both high power and empirical validity in finite sample contexts. These features are often missed entirely by other methods; in particular, in a number of contexts power decreases as the effect size increases, indicating that they are extremely subject to biases due to confounding. In this sense, while they may be theoretically powerful when $n$ is arbitrarily large, they are extremely subject to finite-sample issues when confounding is present. Together, we believe that this work demonstrates the utility of devising assumption-driven statistical tests for conditional independence testing on the basis of causality.

\paragraph*{Connections to causal structure learning and causal discovery}

The framework that we have introduced features direct applications in the field of constraint-based causal discovery. Typically, constraint-based causal discovery is implemented via incorporation of conditional independence tests with strategies such as the PC or the FCI algorithms \cite{Spirtes,Spirtes2001Jan}. 

These procedures typically decompose the sets of variables under study into nodes in a graph, and proceed to iteratively build a graph of causal dependencies (a directed acyclic graph, or \texttt{DAG} \cite{Pearl2009Jan,Pearl2010Jul}) through progressively more restrictive conditional independence tests (e.g., iteratively ``building'' a proper conditioning set $\mathbf x_i$ from the observed data through sequences of conditional independence tests). In the case where sets of variables that we wish to test are nominal, the conditional independence tests used are conditional discrepancy tests, of which we have directly compared our techniques to, such as \kcd~\cite{Park2021Jul}. Through elucidating explicit statistical assumptions necessary for conditional independence tests to be causal conditional discrepancy tests, we were able to yield a new test, \ccdcorr, by attempting to faithfully modify the observed data to better reflect the underlying statistical assumptions. \ccdcorr~showed substantial improvements in both finite sample sensitivity and specificity (it shows higher validity in null experiments, and power generally increases with effect size), and generalizability (it applies readily to greater than $2$-level contexts, where we have more than binary treatment levels), while maintaining empirical validity. 

Inherently, the success or failure at recovering the underlying causal structure is directly a function of the sensitivity and specificity of the conditional independence tests leveraged to true underlying dependencies in the data. We believe that this demonstrates that our assumption-driven statistical framework may yield fruitful improvements to causal structure learning as it relates to nominal variables, and will allow our methodologies to inform new developments in this exciting avenue of work.

\paragraph*{Limitations and future work}

A chief limitation of this work is that we largely focus on the nominal treatment case; e.g., where $\mathbf t_i$ is a $[T]$-valued random variable. Indeed, it is often the case where one may wish to investigate continuous or multivariate treatments through more general conditional independence tests. In the case of continuous or multivariate $\mathbf t_i$, the positivity assumption becomes one of densities rather than probabilities. With $(\mathcal T, \delta_t)$ a metric space and $\mathbf t_i$ a $\mathcal T$-valued random variable, the positivity assumption states $f_{\mathbf t_i | x}(t) > 0$ for all $t \in \mathcal T$ and for all $x \in \mathcal X$. In light of Lemma \ref{thm:CoDiCE}, our sums become integrals, and the results that we have developed herein remain otherwise identical. 

Due to the flexibility of \cdcorr, the approach upon which \ccdcorr~was based, we believe that this limitation can be overcome with similar pre-processing and analytical strategies to how we worked towards better reflecting the positivity assumption in observational data in the nominal case. Many strategies exist which attempt to bin continuous or multivariate treatments into discrete bins (either through stratification or binning of the treatment variable in the continuous or multivariate case), which would directly extend under the framework which we have proposed. However, the utility of these strategies are dubious due to their tendency to impart strong biases and a resulting loss of statistical power \cite{Royston2006Jan}. Both parametric and non-parametric strategies exist which broadly attempt to ``re-weight'' the observed samples in observational studies which can be empirically conceptualized as assumption-driven approaches working towards the positivity assumption from observational data. These similarly leverage the generalized propensity score, where $r(t, x) = f_{\mathbf t_i | x}(t)$ \cite{Robins1986Jan,Imai2004Sep,Hirano2004Jul} is a density instead of a probability. These techniques include entropy balancing \cite{Vegetabile2021Mar,Tubbicke2020Oct} and more classical weight-stabilization approaches \cite{Robins2000Sep}. Natively, the \cdcorr~procedure \cite{Wang2015} directly incorporates weight to reflect conditional structures in the model. The non-parametric models used to infer weights could be directly altered to reflect better finite-sample characteristics, and thereby directly tune for positivity, using the generalized propensity score while maintaining the asymptotic theoretical guarantees afforded by the traditional \cdcorr~approach. 

\paragraph{Acknowledgements}


\paragraph{Code and Data Availability Statement}

All figures within this manuscript can be reproduced via the github repository at \href{https://github.com/ebridge2/cdcorr/}{github.com/ebridge2/cdcorr/}. %The approach which we propose for causal conditional discrepancy testing, \ccdcorr, is made publicly available via the python package \texttt{doWhy}.

