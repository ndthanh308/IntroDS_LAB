
@inproceedings{jaber_causal_2022,
	title = {Causal {Identification} under {Markov} equivalence: {Calculus}, {Algorithm}, and {Completeness}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jaber, Amin and Ribeiro, Adèle H and Zhang, Jiji and Bareinboim, Elias},
	year = {2022},
}

@misc{noauthor_notitle_nodate,
	url = {https://causalai.net/r42.pdf},
	urldate = {2023-02-01},
}

@article{lin_random_2006,
	title = {Random {Forests} and {Adaptive} {Nearest} {Neighbors}},
	volume = {101},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/27590719},
	abstract = {In this article we study random forests through their connection with a new framework of adaptive nearest-neighbor methods. We introduce a concept of potential nearest neighbors (k-PNNs) and show that random forests can be viewed as adaptively weighted k-PNN methods. Various aspects of random forests can be studied from this perspective. We study the effect of terminal node sizes on the prediction accuracy of random forests. We further show that random forests with adaptive splitting schemes assign weights to k-PNNs in a desirable way: for the estimation at a given target point, these random forests assign voting weights to the k-PNNs of the target point according to the local importance of different input variables. We propose a new simple splitting scheme that achieves desirable adaptivity in a straightforward fashion. This simple scheme can be combined with existing algorithms. The resulting algorithm is computationally faster and gives comparable results. Other possible aspects of random forests, such as using linear combinations in splitting, are also discussed. Simulations and real datasets are used to illustrate the results.},
	number = {474},
	urldate = {2023-01-29},
	journal = {Journal of the American Statistical Association},
	author = {Lin, Yi and Jeon, Yongho},
	year = {2006},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {578--590},
}

@article{leonenko_sample_1987,
	title = {Sample estimate of the entropy of a random vector},
	volume = {23},
	number = {2},
	journal = {Problemy Peredachi Informatsii},
	author = {Leonenko, Nikolai N},
	year = {1987},
	note = {Publisher: Russian Academy of Sciences, Branch of Informatics, Computer Equipment and …},
	pages = {9--16},
}

@inproceedings{madhyastha_geodesic_2020,
	address = {New York, NY, USA},
	series = {{KDD} '20},
	title = {Geodesic {Forests}},
	isbn = {978-1-4503-7998-4},
	url = {https://doi.org/10.1145/3394486.3403094},
	doi = {10.1145/3394486.3403094},
	abstract = {Together with the curse of dimensionality, nonlinear dependencies in large data sets persist as major challenges in data mining tasks. A reliable way to accurately preserve nonlinear structure is to compute geodesic distances between data points. Manifold learning methods, such as Isomap, aim to preserve geodesic distances in a Riemannian manifold. However, as manifold learning algorithms operate on the ambient dimensionality of the data, the essential step of geodesic distance computation is sensitive to high-dimensional noise. Therefore, a direct application of these algorithms to high-dimensional, noisy data often yields unsatisfactory results and does not accurately capture nonlinear structure. We propose an unsupervised random forest approach called geodesic forests (GF) to geodesic distance estimation in linear and nonlinear manifolds with noise. GF operates on low-dimensional sparse linear combinations of features, rather than the full observed dimensionality. To choose the optimal split in a computationally efficient fashion, we developed Fast-BIC, a fast Bayesian Information Criterion statistic for Gaussian mixture models. We additionally propose geodesic precision and geodesic recall as novel evaluation metrics that quantify how well the geodesic distances of a latent manifold are preserved. Empirical results on simulated and real data demonstrate that GF is robust to high-dimensional noise, whereas other methods, such as Isomap, UMAP, and FLANN, quickly deteriorate in such settings. Notably, GF is able to estimate geodesic distances better than other approaches on a real connectome dataset.},
	urldate = {2023-01-29},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Madhyastha, Meghana and Li, Gongkai and Strnadová-Neeley, Veronika and Browne, James and Vogelstein, Joshua T. and Burns, Randal and Priebe, Carey E.},
	month = aug,
	year = {2020},
	keywords = {manifold learning, noisy data, random forest, unsupervised},
	pages = {513--523},
}

@article{huang_latent_2022,
	title = {Latent hierarchical causal structure discovery with rank constraints},
	journal = {arXiv preprint arXiv:2210.01798},
	author = {Huang, Biwei and Low, Charles Jia Han and Xie, Feng and Glymour, Clark and Zhang, Kun},
	year = {2022},
}

@misc{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	doi = {10.48550/arXiv.2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv:2102.11107 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{gao_demystifying_2016,
	title = {Demystifying {Fixed} k-{Nearest} {Neighbor} {Information} {Estimators}},
	url = {http://arxiv.org/abs/1604.03006},
	abstract = {Estimating mutual information from i.i.d. samples drawn from an unknown joint density function is a basic statistical problem of broad interest with multitudinous applications. The most popular estimator is one proposed by Kraskov and St{\textbackslash}"ogbauer and Grassberger (KSG) in 2004, and is nonparametric and based on the distances of each sample to its \$k{\textasciicircum}\{{\textbackslash}rm th\}\$ nearest neighboring sample, where \$k\$ is a fixed small integer. Despite its widespread use (part of scientific software packages), theoretical properties of this estimator have been largely unexplored. In this paper we demonstrate that the estimator is consistent and also identify an upper bound on the rate of convergence of the bias as a function of number of samples. We argue that the superior performance benefits of the KSG estimator stems from a curious "correlation boosting" effect and build on this intuition to modify the KSG estimator in novel ways to construct a superior estimator. As a byproduct of our investigations, we obtain nearly tight rates of convergence of the \${\textbackslash}ell\_2\$ error of the well known fixed \$k\$ nearest neighbor estimator of differential entropy by Kozachenko and Leonenko.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Gao, Weihao and Oh, Sewoong and Viswanath, Pramod},
	month = aug,
	year = {2016},
	note = {arXiv:1604.03006 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{gao_efficient_2015,
	title = {Efficient {Estimation} of {Mutual} {Information} for {Strongly} {Dependent} {Variables}},
	url = {http://arxiv.org/abs/1411.2003},
	abstract = {We demonstrate that a popular class of nonparametric mutual information (MI) estimators based on k-nearest-neighbor graphs requires number of samples that scales exponentially with the true MI. Consequently, accurate estimation of MI between two strongly dependent variables is possible only for prohibitively large sample size. This important yet overlooked shortcoming of the existing estimators is due to their implicit reliance on local uniformity of the underlying joint distribution. We introduce a new estimator that is robust to local non-uniformity, works well with limited data, and is able to capture relationship strengths over many orders of magnitude. We demonstrate the superior performance of the proposed estimator on both synthetic and real-world data.},
	urldate = {2023-01-27},
	publisher = {arXiv},
	author = {Gao, Shuyang and Steeg, Greg Ver and Galstyan, Aram},
	month = mar,
	year = {2015},
	note = {arXiv:1411.2003 [physics, stat]},
	keywords = {Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@article{kraskov_estimating_2004,
	title = {Estimating mutual information},
	volume = {69},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066138},
	doi = {10.1103/PhysRevE.69.066138},
	abstract = {We present two classes of improved estimators for mutual information M(X,Y), from samples of random points distributed according to some joint probability density μ(x,y). In contrast to conventional estimators based on binnings, they are based on entropy estimates from k-nearest neighbor distances. This means that they are data efficient (with k=1 we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to nonuniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of k∕N for N points. Numerically, we find that both families become exact for independent distributions, i.e. the estimator ˆM(X,Y) vanishes (up to statistical fluctuations) if μ(x,y)=μ(x)μ(y). This holds for all tested marginal distributions and for all dimensions of x and y. In addition, we give estimators for redundancies between more than two random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation., This article appears in the following collections:},
	number = {6},
	urldate = {2023-01-27},
	journal = {Physical Review E},
	author = {Kraskov, Alexander and Stögbauer, Harald and Grassberger, Peter},
	month = jun,
	year = {2004},
	note = {Publisher: American Physical Society},
	pages = {066138},
}

@article{frenzel_partial_2007,
	title = {Partial {Mutual} {Information} for {Coupling} {Analysis} of {Multivariate} {Time} {Series}},
	volume = {99},
	doi = {10.1103/PhysRevLett.99.204101},
	abstract = {We propose a method to discover couplings in multivariate time series, based on partial mutual information, an information-theoretic generalization of partial correlation. It represents the part of mutual information of two random quantities that is not contained in a third one. By suitable choice of the latter, we can differentiate between direct and indirect interactions and derive an appropriate graphical model. An efficient estimator for partial mutual information is presented as well.},
	journal = {Physical review letters},
	author = {Frenzel, Stefan and Pompe, Bernd},
	month = dec,
	year = {2007},
	pages = {204101},
}

@article{ramsey_adjacency-faithfulness_nodate,
	title = {Adjacency-{Faithfulness} and {Conservative} {Causal} {Inference}},
	abstract = {Most causal discovery algorithms in the literature exploit an assumption usually referred to as the Causal Faithfulness or Stability Condition. In this paper, we highlight two components of the condition used in constraint-based algorithms, which we call "Adjacency-Faithfulness" and "Orientation-Faithfulness." We point out that assuming Adjacency-Faithfulness is true, it is possible to test the validity of Orientation-Faithfulness. Motivated by this observation , we explore the consequence of making only the Adjacency-Faithfulness assumption. We show that the familiar PC algorithm has to be modified to be correct under the weaker, Adjacency-Faithfulness assumption. The modified algorithm, called Conservative PC (CPC), checks whether Orientation-Faithfulness holds in the orientation phase, and if not, avoids drawing certain causal conclusions the PC algorithm would draw. However , if the stronger, standard causal Faith-fulness condition actually obtains, the CPC algorithm outputs the same pattern as the PC algorithm does in the large sample limit. We also present a simulation study showing that the CPC algorithm runs almost as fast as the PC algorithm, and outputs significantly fewer false causal arrowheads than the PC algorithm does on realistic sample sizes.},
	author = {Ramsey, Joseph and Spirtes, Peter and Zhang, Jiji},
}

@article{zhang_completeness_2008,
	title = {On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias},
	volume = {172},
	issn = {0004-3702},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370208001008},
	doi = {10.1016/j.artint.2008.08.001},
	abstract = {Causal discovery becomes especially challenging when the possibility of latent confounding and/or selection bias is not assumed away. For this task, ancestral graph models are particularly useful in that they can represent the presence of latent confounding and selection effect, without explicitly invoking unobserved variables. Based on the machinery of ancestral graphs, there is a provably sound causal discovery algorithm, known as the FCI algorithm, that allows the possibility of latent confounders and selection bias. However, the orientation rules used in the algorithm are not complete. In this paper, we provide additional orientation rules, augmented by which the FCI algorithm is shown to be complete, in the sense that it can, under standard assumptions, discover all aspects of the causal structure that are uniquely determined by facts of probabilistic dependence and independence. The result is useful for developing any causal discovery and reasoning system based on ancestral graph models.},
	language = {en},
	number = {16},
	urldate = {2022-09-12},
	journal = {Artificial Intelligence},
	author = {Zhang, Jiji},
	month = nov,
	year = {2008},
	keywords = {Ancestral graphs, Automated causal discovery, Bayesian networks, Causal models, Latent variables, Markov equivalence},
	pages = {1873--1896},
}

@misc{zhang_kernel-based_2012,
	title = {Kernel-based {Conditional} {Independence} {Test} and {Application} in {Causal} {Discovery}},
	url = {http://arxiv.org/abs/1202.3775},
	abstract = {Conditional independence testing is an important problem, especially in Bayesian network learning and causal discovery. Due to the curse of dimensionality, testing for conditional independence of continuous variables is particularly challenging. We propose a Kernel-based Conditional Independence test (KCI-test), by constructing an appropriate test statistic and deriving its asymptotic distribution under the null hypothesis of conditional independence. The proposed method is computationally efficient and easy to implement. Experimental results show that it outperforms other methods, especially when the conditioning set is large or the sample size is not very large, in which case other methods encounter difficulties.},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Zhang, Kun and Peters, Jonas and Janzing, Dominik and Schoelkopf, Bernhard},
	month = feb,
	year = {2012},
	note = {arXiv:1202.3775 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lopes_more_2015,
	title = {A {More} {Powerful} {Two}-{Sample} {Test} in {High} {Dimensions} using {Random} {Projection}},
	url = {http://arxiv.org/abs/1108.2401},
	abstract = {We consider the hypothesis testing problem of detecting a shift between the means of two multivariate normal distributions in the high-dimensional setting, allowing for the data dimension p to exceed the sample size n. Specifically, we propose a new test statistic for the two-sample test of means that integrates a random projection with the classical Hotelling T{\textasciicircum}2 statistic. Working under a high-dimensional framework with (p,n) tending to infinity, we first derive an asymptotic power function for our test, and then provide sufficient conditions for it to achieve greater power than other state-of-the-art tests. Using ROC curves generated from synthetic data, we demonstrate superior performance against competing tests in the parameter regimes anticipated by our theoretical results. Lastly, we illustrate an advantage of our procedure's false positive rate with comparisons on high-dimensional gene expression data involving the discrimination of different types of cancer.},
	urldate = {2022-09-09},
	publisher = {arXiv},
	author = {Lopes, Miles E. and Jacob, Laurent J. and Wainwright, Martin J.},
	month = sep,
	year = {2015},
	note = {arXiv:1108.2401 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{mukherjee_ccmi_2019,
	title = {{CCMI} : {Classifier} based {Conditional} {Mutual} {Information} {Estimation}},
	shorttitle = {{CCMI}},
	url = {http://arxiv.org/abs/1906.01824},
	abstract = {Conditional Mutual Information (CMI) is a measure of conditional dependence between random variables X and Y, given another random variable Z. It can be used to quantify conditional dependence among variables in many data-driven inference problems such as graphical models, causal learning, feature selection and time-series analysis. While k-nearest neighbor (kNN) based estimators as well as kernel-based methods have been widely used for CMI estimation, they suffer severely from the curse of dimensionality. In this paper, we leverage advances in classifiers and generative models to design methods for CMI estimation. Specifically, we introduce an estimator for KL-Divergence based on the likelihood ratio by training a classifier to distinguish the observed joint distribution from the product distribution. We then show how to construct several CMI estimators using this basic divergence estimator by drawing ideas from conditional generative models. We demonstrate that the estimates from our proposed approaches do not degrade in performance with increasing dimension and obtain significant improvement over the widely used KSG estimator. Finally, as an application of accurate CMI estimation, we use our best estimator for conditional independence testing and achieve superior performance than the state-of-the-art tester on both simulated and real data-sets.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Mukherjee, Sudipto and Asnani, Himanshu and Kannan, Sreeram},
	month = jun,
	year = {2019},
	note = {arXiv:1906.01824 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{belghazi_mine_2021,
	title = {{MINE}: {Mutual} {Information} {Neural} {Estimation}},
	shorttitle = {{MINE}},
	url = {http://arxiv.org/abs/1801.04062},
	abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
	urldate = {2022-08-31},
	publisher = {arXiv},
	author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
	month = aug,
	year = {2021},
	note = {arXiv:1801.04062 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{sen_model-powered_2017,
	title = {Model-{Powered} {Conditional} {Independence} {Test}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/02f039058bd48307e6f653a2005c9dd2-Abstract.html},
	urldate = {2022-07-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sen, Rajat and Suresh, Ananda Theertha and Shanmugam, Karthikeyan and Dimakis, Alexandros G and Shakkottai, Sanjay},
	year = {2017},
}

@article{zhang_feature--feature_nodate,
	title = {Feature-to-{Feature} {Regression} for a {Two}-{Step} {Conditional} {Independence} {Test}},
	abstract = {The algorithms for causal discovery and more broadly for learning the structure of graphical models require well calibrated and consistent conditional independence (CI) tests. We revisit the CI tests which are based on two-step procedures and involve regression with subsequent (unconditional) independence test (RESIT) on regression residuals and investigate the assumptions under which these tests operate. In particular, we demonstrate that when going beyond simple functional relationships with additive noise, such tests can lead to an inﬂated number of false discoveries. We study the relationship of these tests with those based on dependence measures using reproducing kernel Hilbert spaces (RKHS) and propose an extension of RESIT which uses RKHS-valued regression. The resulting test inherits the simple two-step testing procedure of RESIT, while giving correct Type I control and competitive power. When used as a component of the PC algorithm, the proposed test is more robust to the case where hidden variables induce a switching behaviour in the associations present in the data.},
	language = {en},
	author = {Zhang, Qinyi and Filippi, Sarah and Flaxman, Seth and Sejdinovic, Dino},
	pages = {12},
}

@article{uhler_geometry_2013,
	title = {Geometry of the faithfulness assumption in causal inference},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1207.0547},
	doi = {10.1214/12-AOS1080},
	abstract = {Many algorithms for inferring causality rely heavily on the faithfulness assumption. The main justification for imposing this assumption is that the set of unfaithful distributions has Lebesgue measure zero, since it can be seen as a collection of hypersurfaces in a hypercube. However, due to sampling error the faithfulness condition alone is not sufficient for statistical estimation, and strong-faithfulness has been proposed and assumed to achieve uniform or high-dimensional consistency. In contrast to the plain faithfulness assumption, the set of distributions that is not strong-faithful has nonzero Lebesgue measure and in fact, can be surprisingly large as we show in this paper. We study the strong-faithfulness condition from a geometric and combinatorial point of view and give upper and lower bounds on the Lebesgue measure of strong-faithful distributions for various classes of directed acyclic graphs. Our results imply fundamental limitations for the PC-algorithm and potentially also for other algorithms based on partial correlation testing in the Gaussian case.},
	number = {2},
	urldate = {2022-10-27},
	journal = {The Annals of Statistics},
	author = {Uhler, Caroline and Raskutti, Garvesh and Bühlmann, Peter and Yu, Bin},
	month = apr,
	year = {2013},
	note = {arXiv:1207.0547 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@unpublished{alexis_bellot_scores_2022,
	address = {Columbia CausalAI Laboratory},
	title = {Scores for {Learning} {Discrete} {Causal} {Graphs} with {Unobserved} {Confounders}},
	url = {https://causalai.net/r83.pdf},
	urldate = {2023-01-10},
	author = {Alexis Bellot and {Zhang Junzhe} and {Bareinboim Elias}},
	month = may,
	year = {2022},
}

@misc{smucler_efficient_2020,
	title = {Efficient adjustment sets in causal graphical models with hidden variables},
	url = {http://arxiv.org/abs/2004.10521},
	abstract = {We study the selection of covariate adjustment sets for estimating the value of point exposure dynamic policies, also known as dynamic treatment regimes, assuming a non-parametric causal graphical model with hidden variables, in which at least one adjustment set is fully observable. We show that recently developed criteria, for graphs without hidden variables, to compare the asymptotic variance of non-parametric estimators of static policy values that control for certain adjustment sets, are also valid under dynamic policies and graphs with hidden variables. We show that there exist adjustment sets that are optimal minimal (minimum), in the sense of yielding estimators with the smallest variance among those that control for adjustment sets that are minimal (of minimum cardinality). Moreover, we show that if either no variables are hidden or if all the observable variables are ancestors of either treatment, outcome, or the variables that are used to decide treatment, a globally optimal adjustment set exists. We provide polynomial time algorithms to compute the globally optimal (when it exists), optimal minimal, and optimal minimum adjustment sets. Our results are based on the construction of an undirected graph in which vertex cuts between the treatment and outcome variables correspond to adjustment sets. In this undirected graph, a partial order between minimal vertex cuts can be defined that makes the set of minimal cuts a lattice. This partial order corresponds directly to the ordering of the asymptotic variances of the corresponding non-parametrically adjusted estimators.},
	urldate = {2023-01-23},
	publisher = {arXiv},
	author = {Smucler, Ezequiel and Sapienza, Facundo and Rotnitzky, Andrea},
	month = may,
	year = {2020},
	note = {arXiv:2004.10521 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@book{cover_elements_2012,
	title = {Elements of {Information} {Theory}},
	isbn = {978-1-118-58577-1},
	abstract = {The latest edition of this classic is updated with new problem sets and material   The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	month = nov,
	year = {2012},
	note = {Google-Books-ID: VWq5GG6ycxMC},
	keywords = {Computers / Computer Science, Computers / General, Computers / Information Technology},
}

@article{maathuis_generalized_2015,
	title = {A generalized back-door criterion},
	volume = {43},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-43/issue-3/A-generalized-back-door-criterion/10.1214/14-AOS1295.full},
	doi = {10.1214/14-AOS1295},
	abstract = {We generalize Pearl’s back-door criterion for directed acyclic graphs (DAGs) to more general types of graphs that describe Markov equivalence classes of DAGs and/or allow for arbitrarily many hidden variables. We also give easily checkable necessary and sufficient graphical criteria for the existence of a set of variables that satisfies our generalized back-door criterion, when considering a single intervention and a single outcome variable. Moreover, if such a set exists, we provide an explicit set that fulfills the criterion. We illustrate the results in several examples. R-code is available in the R-package pcalg.},
	number = {3},
	urldate = {2023-01-10},
	journal = {The Annals of Statistics},
	author = {Maathuis, Marloes H. and Colombo, Diego},
	month = jun,
	year = {2015},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62H99, CPDAG, Causal inference, DAG, MAG, PAG, covariate adjustment, hidden confounders},
	pages = {1060--1088},
}

@incollection{bareinboim_pearls_2022,
	address = {New York, NY, USA},
	edition = {1},
	title = {On {Pearl}’s {Hierarchy} and the {Foundations} of {Causal} {Inference}},
	volume = {36},
	isbn = {978-1-4503-9586-1},
	url = {https://doi.org/10.1145/3501714.3501743},
	urldate = {2023-01-10},
	booktitle = {Probabilistic and {Causal} {Inference}: {The} {Works} of {Judea} {Pearl}},
	publisher = {Association for Computing Machinery},
	author = {Bareinboim, Elias and Correa, Juan D. and Ibeling, Duligur and Icard, Thomas},
	month = mar,
	year = {2022},
	pages = {507--556},
}

@inproceedings{hyttinen_constraint-based_2014,
	address = {Arlington, Virginia, USA},
	series = {{UAI}'14},
	title = {Constraint-based causal discovery: conflict resolution with answer set programming},
	isbn = {978-0-9749039-1-0},
	shorttitle = {Constraint-based causal discovery},
	abstract = {Recent approaches to causal discovery based on Boolean satisfiability solvers have opened new opportunities to consider search spaces for causal models with both feedback cycles and unmeasured confounders. However, the available methods have so far not been able to provide a principled account of how to handle conflicting constraints that arise from statistical variability. Here we present a new approach that preserves the versatility of Boolean constraint solving and attains a high accuracy despite the presence of statistical errors. We develop a new logical encoding of (in)dependence constraints that is both well suited for the domain and allows for faster solving. We represent this encoding in Answer Set Programming (ASP), and apply a state-of-the-art ASP solver for the optimization task. Based on different theoretical motivations, we explore a variety of methods to handle statistical errors. Our approach currently scales to cyclic latent variable models with up to seven observed variables and outperforms the available constraint-based methods in accuracy.},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the {Thirtieth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {AUAI Press},
	author = {Hyttinen, Antti and Eberhardt, Frederick and Järvisalo, Matti},
	month = jul,
	year = {2014},
	pages = {340--349},
}

@misc{noauthor_notitle_nodate-1,
	url = {https://auai.org/uai2014/proceedings/individuals/87.pdf},
	urldate = {2023-01-10},
}

@misc{noauthor_notitle_nodate-2,
	url = {https://www.its.caltech.edu/~fehardt/papers/HEJ_UAI2014.pdf},
	urldate = {2023-01-10},
}

@article{shen_challenges_2020,
	title = {Challenges and {Opportunities} with {Causal} {Discovery} {Algorithms}: {Application} to {Alzheimer}’s {Pathophysiology}},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Challenges and {Opportunities} with {Causal} {Discovery} {Algorithms}},
	url = {https://www.nature.com/articles/s41598-020-59669-x},
	doi = {10.1038/s41598-020-59669-x},
	abstract = {Causal Structure Discovery (CSD) is the problem of identifying causal relationships from large quantities of data through computational methods. With the limited ability of traditional association-based computational methods to discover causal relationships, CSD methodologies are gaining popularity. The goal of the study was to systematically examine whether (i) CSD methods can discover the known causal relationships from observational clinical data and (ii) to offer guidance to accurately discover known causal relationships. We used Alzheimer’s disease (AD), a complex progressive disease, as a model because the well-established evidence provides a “gold-standard” causal graph for evaluation. We evaluated two CSD methods, Fast Causal Inference (FCI) and Fast Greedy Equivalence Search (FGES) in their ability to discover this structure from data collected by the Alzheimer’s Disease Neuroimaging Initiative (ADNI). We used structural equation models (which is not designed for CSD) as control. We applied these methods under three scenarios defined by increasing amounts of background knowledge provided to the methods. The methods were evaluated by comparing the resulting causal relationships with the “gold standard” graph that was constructed from literature. Dedicated CSD methods managed to discover graphs that nearly coincided with the gold standard. For best results, CSD algorithms should be used with longitudinal data providing as much prior knowledge as possible.},
	language = {en},
	number = {1},
	urldate = {2023-01-10},
	journal = {Scientific Reports},
	author = {Shen, Xinpeng and Ma, Sisi and Vemuri, Prashanthi and Simon, Gyorgy},
	month = feb,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Alzheimer's disease, Cognitive ageing},
	pages = {2975},
}

@article{glymour_review_2019,
	title = {Review of {Causal} {Discovery} {Methods} {Based} on {Graphical} {Models}},
	volume = {10},
	issn = {1664-8021},
	url = {https://www.frontiersin.org/articles/10.3389/fgene.2019.00524},
	abstract = {A fundamental task in various disciplines of science, including biology, is to find underlying causal relations and make use of them. Causal relations can be seen if interventions are properly applied; however, in many cases they are difficult or even impossible to conduct. It is then necessary to discover causal relations by analyzing statistical properties of purely observational data, which is known as causal discovery or causal structure search. This paper aims to give a introduction to and a brief review of the computational methods for causal discovery that were developed in the past three decades, including constraint-based and score-based methods and those based on functional causal models, supplemented by some illustrations and applications.},
	urldate = {2023-01-10},
	journal = {Frontiers in Genetics},
	author = {Glymour, Clark and Zhang, Kun and Spirtes, Peter},
	year = {2019},
}

@article{shimizu_linear_2006,
	title = {A {Linear} {Non}-{Gaussian} {Acyclic} {Model} for {Causal} {Discovery}},
	volume = {7},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v7/shimizu06a.html},
	abstract = {In recent years, several methods have been proposed for the discovery of causal structure from non-experimental data. Such methods make various assumptions on the data generating process to facilitate its identification from purely observational data. Continuing this line of research, we show how to discover the complete causal structure of continuous-valued data, under the assumptions that (a) the data generating process is linear, (b) there are no unobserved confounders, and (c) disturbance variables have non-Gaussian distributions of non-zero variances. The solution relies on the use of the statistical method known as independent component analysis, and does not require any pre-specified time-ordering of the variables. We provide a complete Matlab package for performing this LiNGAM analysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the effectiveness of the method using artificially generated data and real-world data.},
	number = {72},
	urldate = {2023-01-10},
	journal = {Journal of Machine Learning Research},
	author = {Shimizu, Shohei and Hoyer, Patrik O. and Hyv\&\#228, Aapo and rinen and Kerminen, Antti},
	year = {2006},
	pages = {2003--2030},
}

@misc{noauthor_notitle_nodate-3,
	url = {https://www.jmlr.org/papers/volume7/shimizu06a/shimizu06a.pdf},
	urldate = {2023-01-10},
}

@misc{nowzohour_distributional_2017,
	title = {Distributional {Equivalence} and {Structure} {Learning} for {Bow}-free {Acyclic} {Path} {Diagrams}},
	url = {http://arxiv.org/abs/1508.01717},
	doi = {10.48550/arXiv.1508.01717},
	abstract = {We consider the problem of structure learning for bow-free acyclic path diagrams (BAPs). BAPs can be viewed as a generalization of linear Gaussian DAG models that allow for certain hidden variables. We present a first method for this problem using a greedy score-based search algorithm. We also prove some necessary and some sufficient conditions for distributional equivalence of BAPs which are used in an algorithmic ap- proach to compute (nearly) equivalent model structures. This allows us to infer lower bounds of causal effects. We also present applications to real and simulated datasets using our publicly available R-package.},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Nowzohour, Christopher and Maathuis, Marloes H. and Evans, Robin J. and Bühlmann, Peter},
	month = dec,
	year = {2017},
	note = {arXiv:1508.01717 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{chickering_optimal_2002,
	title = {Optimal {Structure} {Identification} {With} {Greedy} {Search}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {https://jmlr.org/papers/v3/chickering02b.html},
	abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
	number = {Nov},
	urldate = {2023-01-10},
	journal = {Journal of Machine Learning Research},
	author = {Chickering, David Maxwell},
	year = {2002},
	pages = {507--554},
}

@misc{meek_causal_2013,
	title = {Causal {Inference} and {Causal} {Explanation} with {Background} {Knowledge}},
	url = {http://arxiv.org/abs/1302.4972},
	abstract = {This paper presents correct algorithms for answering the following two questions; (i) Does there exist a causal explanation consistent with a set of background knowledge which explains all of the observed independence facts in a sample? (ii) Given that there is such a causal explanation what are the causal relationships common to every such causal explanation?},
	urldate = {2023-01-10},
	publisher = {arXiv},
	author = {Meek, Christopher},
	month = feb,
	year = {2013},
	note = {arXiv:1302.4972 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{yao_learning_2022,
	title = {{LEARNING} {TEMPORALLY} {CAUSAL} {LATENT} {PRO}- {CESSES} {FROM} {GENERAL} {TEMPORAL} {DATA}},
	abstract = {Our goal is to recover time-delayed latent causal variables and identify their relations from measured temporal data. Estimating causally-related latent variables from observations is particularly challenging as the latent variables are not uniquely recoverable in the most general case. In this work, we consider both a nonparametric, nonstationary setting and a parametric setting for the latent processes and propose two provable conditions under which temporally causal latent processes can be identified from their nonlinear mixtures. We propose LEAP, a theoretically-grounded framework that extends Variational AutoEncoders (VAEs) by enforcing our conditions through proper constraints in causal process prior. Experimental results on various datasets demonstrate that temporally causal latent processes are reliably identified from observed variables under different dependency structures and that our approach considerably outperforms baselines that do not properly leverage history or nonstationarity information. This demonstrates that using temporal information to learn latent processes from their invertible nonlinear mixtures in an unsupervised manner, for which we believe our work is one of the first, seems promising even without sparsity or minimality assumptions.},
	language = {en},
	author = {Yao, Weiran and Sun, Yuewen and Ho, Alex and Sun, Changyin and Zhang, Kun},
	year = {2022},
	pages = {31},
}

@techreport{tu_causal_2020,
	title = {Causal {Discovery} in the {Presence} of {Missing} {Data}},
	url = {http://arxiv.org/abs/1807.04010},
	abstract = {Missing data are ubiquitous in many domains including healthcare. When these data entries are not missing completely at random, the (conditional) independence relations in the observed data may be different from those in the complete data generated by the underlying causal process. Consequently, simply applying existing causal discovery methods to the observed data may lead to wrong conclusions. In this paper, we aim at developing a causal discovery method to recover the underlying causal structure from observed data that follow different missingness mechanisms, including missing completely at random (MCAR), missing at random (MAR), and missing not at random (MNAR). With missingness mechanisms represented by missingness graphs, we analyse conditions under which additional correction is needed to derive conditional independence/dependence relations in the complete data. Based on our analysis, we propose the Missing Value PC (MVPC) algorithm for both continuous and binary variables, which extends the PC algorithm to incorporate additional corrections. Our proposed MVPC is shown in theory to give asymptotically correct results even on data that are MAR or MNAR. Experimental results on synthetic data show that the proposed algorithm is able to find correct causal relations even in the general case of MNAR. Moreover, we create a neuropathic pain diagnostic simulator for evaluating causal discovery methods. Evaluated on such simulated neuropathic pain diagnosis records and the other two real world applications, MVPC outperforms the other benchmark methods.},
	number = {arXiv:1807.04010},
	urldate = {2022-08-11},
	institution = {arXiv},
	author = {Tu, Ruibo and Zhang, Kun and Ackermann, Paul and Bertilson, Bo Christer and Glymour, Clark and Kjellström, Hedvig and Zhang, Cheng},
	month = jul,
	year = {2020},
	note = {arXiv:1807.04010 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{spirtes_anytime_2001,
	title = {An {Anytime} {Algorithm} for {Causal} {Inference}},
	url = {https://proceedings.mlr.press/r3/spirtes01a.html},
	abstract = {The Fast Casual Inference (FCI) algorithm searches for features common to observationally equivalent sets of causal directed acyclic graphs. It is correct in the large sample limit with probability...},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {International {Workshop} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Spirtes, Peter},
	month = jan,
	year = {2001},
	note = {ISSN: 2640-3498},
	pages = {278--285},
}

@techreport{runge_conditional_2017,
	title = {Conditional independence testing based on a nearest-neighbor estimator of conditional mutual information},
	url = {http://arxiv.org/abs/1709.01447},
	abstract = {Conditional independence testing is a fundamental problem underlying causal discovery and a particularly challenging task in the presence of nonlinear and high-dimensional dependencies. Here a fully non-parametric test for continuous data based on conditional mutual information combined with a local permutation scheme is presented. Through a nearest neighbor approach, the test efficiently adapts also to non-smooth distributions due to strongly nonlinear dependencies. Numerical experiments demonstrate that the test reliably simulates the null distribution even for small sample sizes and with high-dimensional conditioning sets. The test is better calibrated than kernel-based tests utilizing an analytical approximation of the null distribution, especially for non-smooth densities, and reaches the same or higher power levels. Combining the local permutation scheme with the kernel tests leads to better calibration, but suffers in power. For smaller sample sizes and lower dimensions, the test is faster than random fourier feature-based kernel tests if the permutation scheme is (embarrassingly) parallelized, but the runtime increases more sharply with sample size and dimensionality. Thus, more theoretical research to analytically approximate the null distribution and speed up the estimation for larger sample sizes is desirable.},
	number = {arXiv:1709.01447},
	urldate = {2022-08-11},
	institution = {arXiv},
	author = {Runge, Jakob},
	month = sep,
	year = {2017},
	note = {arXiv:1709.01447 [cs, math, stat]
type: article},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{rohekar_iterative_2022,
	title = {Iterative {Causal} {Discovery} in the {Possible} {Presence} of {Latent} {Confounders} and {Selection} {Bias}},
	url = {http://arxiv.org/abs/2111.04095},
	doi = {10.48550/arXiv.2111.04095},
	abstract = {We present a sound and complete algorithm, called iterative causal discovery (ICD), for recovering causal graphs in the presence of latent confounders and selection bias. ICD relies on the causal Markov and faithfulness assumptions and recovers the equivalence class of the underlying causal graph. It starts with a complete graph, and consists of a single iterative stage that gradually refines this graph by identifying conditional independence (CI) between connected nodes. Independence and causal relations entailed after any iteration are correct, rendering ICD anytime. Essentially, we tie the size of the CI conditioning set to its distance on the graph from the tested nodes, and increase this value in the successive iteration. Thus, each iteration refines a graph that was recovered by previous iterations having smaller conditioning sets -- a higher statistical power -- which contributes to stability. We demonstrate empirically that ICD requires significantly fewer CI tests and learns more accurate causal graphs compared to FCI, FCI+, and RFCI algorithms (code is available at https://github.com/IntelLabs/causality-lab).},
	urldate = {2022-10-20},
	publisher = {arXiv},
	author = {Rohekar, Raanan Y. and Nisimov, Shami and Gurwicz, Yaniv and Novik, Gal},
	month = jan,
	year = {2022},
	note = {arXiv:2111.04095 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{mooij_constraint-based_2020,
	title = {Constraint-{Based} {Causal} {Discovery} using {Partial} {Ancestral} {Graphs} in the presence of {Cycles}},
	url = {https://proceedings.mlr.press/v124/m-mooij20a.html},
	abstract = {While feedback loops are known to play important roles in many complex systems, their existence is ignored in a large part of the causal discovery literature, as systems are typically assumed to be acyclic from the outset. When applying causal discovery algorithms designed for the acyclic setting on data generated by a system that involves feedback, one would not expect to obtain correct results. In this work, we show that—surprisingly—the output of the Fast Causal Inference (FCI) algorithm is correct if it is applied to observational data generated by a system that involves feedback. More specifically, we prove that for observational data generated by a simple and sigma-faithful Structural Causal Model (SCM), FCI is sound and complete, and can be used to consistently estimate (i) the presence and absence of causal relations, (ii) the presence and absence of direct causal relations, (iii) the absence of confounders, and (iv) the absence of specific cycles in the causal graph of the SCM. We extend these results to constraint-based causal discovery algorithms that exploit certain forms of background knowledge, including the causally sufficient setting (e.g., the PC algorithm) and the Joint Causal Inference setting (e.g., the FCI-JCI algorithm).},
	language = {en},
	urldate = {2022-10-10},
	booktitle = {Proceedings of the 36th {Conference} on {Uncertainty} in {Artificial} {Intelligence} ({UAI})},
	publisher = {PMLR},
	author = {Mooij, Joris M. and Claassen, Tom},
	month = aug,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1159--1168},
}

@misc{gerhardus_characterization_2021,
	title = {Characterization of causal ancestral graphs for time series with latent confounders},
	url = {http://arxiv.org/abs/2112.08417},
	abstract = {Generalizing directed maximal ancestral graphs, we introduce a class of graphical models for representing time lag specific causal relationships and independencies among finitely many regularly sampled and regularly subsampled time steps of multivariate time series with unobserved variables. We completely characterize these graphs and show that they entail constraints beyond those that have previously been considered in the literature. This allows for stronger causal inferences without having imposed additional assumptions. In generalization of directed partial ancestral graphs we further introduce a graphical representation of Markov equivalence classes of the novel type of graphs and show that these are more informative than what current state-of-the-art causal discovery algorithms learn. We also analyze the additional information gained by increasing the number of observed time steps.},
	urldate = {2022-10-10},
	publisher = {arXiv},
	author = {Gerhardus, Andreas},
	month = dec,
	year = {2021},
	note = {arXiv:2112.08417 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{colombo_learning_2012,
	title = {Learning high-dimensional directed acyclic graphs with latent and selection variables},
	volume = {40},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1104.5617},
	doi = {10.1214/11-AOS940},
	abstract = {We consider the problem of learning causal information between random variables in directed acyclic graphs (DAGs) when allowing arbitrarily many latent and selection variables. The FCI (Fast Causal Inference) algorithm has been explicitly designed to infer conditional independence and causal information in such settings. However, FCI is computationally infeasible for large graphs. We therefore propose the new RFCI algorithm, which is much faster than FCI. In some situations the output of RFCI is slightly less informative, in particular with respect to conditional independence information. However, we prove that any causal information in the output of RFCI is correct in the asymptotic limit. We also define a class of graphs on which the outputs of FCI and RFCI are identical. We prove consistency of FCI and RFCI in sparse high-dimensional settings, and demonstrate in simulations that the estimation performances of the algorithms are very similar. All software is implemented in the R-package pcalg.},
	number = {1},
	urldate = {2022-10-10},
	journal = {The Annals of Statistics},
	author = {Colombo, Diego and Maathuis, Marloes H. and Kalisch, Markus and Richardson, Thomas S.},
	month = feb,
	year = {2012},
	note = {arXiv:1104.5617 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Methodology},
}

@misc{entner_causal_nodate,
	title = {On {Causal} {Discovery} from {Time} {Series} {Data} using {FCI}},
	abstract = {We adapt the Fast Causal Inference (FCI) algorithm of Spirtes et al. (2000) to the problem of inferring causal relationships from time series data and evaluate our adaptation and the original FCI algorithm, comparing them to other methods including Granger causality. One advantage of FCI based approaches is the possibility of taking latent confounding variables into account, as opposed to methods based on Granger causality. From simulations we see, however, that while the FCI based approaches are in principle quite powerful for finding causal relationships in time series data, such methods are not very reliable for most practical sample sizes. We further apply the framework to microeconomic data on the dynamics of firm growth. By releasing the full computer code for the method we hope to facilitate the application of the procedure to other domains. 1},
	author = {Entner, Doris and Hoyer, Patrik O.},
}

@inproceedings{andrews_completeness_2020,
	title = {On the {Completeness} of {Causal} {Discovery} in the {Presence} of {Latent} {Confounding} with {Tiered} {Background} {Knowledge}},
	url = {https://proceedings.mlr.press/v108/andrews20a.html},
	abstract = {The discovery of causal relationships is a core part of scientific research. Accordingly, over the past several decades, algorithms have been developed to discover the causal structure for a system of variables from observational data. Learning ancestral graphs is of particular interest due to their ability to represent latent confounding implicitly with bi-directed edges. The well-known FCI algorithm provably recovers an ancestral graph for a system of variables encoding the sound and complete set of causal relationships identifiable from observational data. Additional causal relationships become identifiable with the incorporation of background knowledge; however, it is not known for what types of knowledge FCI remains complete. In this paper, we define tiered background knowledge and show that FCI is sound and complete with the incorporation of this knowledge.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Andrews, Bryan and Spirtes, Peter and Cooper, Gregory F.},
	month = jun,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4002--4011},
}

@inproceedings{jung_estimating_2021,
	title = {Estimating {Identifiable} {Causal} {Effects} on {Markov} {Equivalence} {Class} through {Double} {Machine} {Learning}},
	url = {https://proceedings.mlr.press/v139/jung21b.html},
	abstract = {General methods have been developed for estimating causal effects from observational data under causal assumptions encoded in the form of a causal graph. Most of this literature assumes that the underlying causal graph is completely specified. However, only observational data is available in most practical settings, which means that one can learn at most a Markov equivalence class (MEC) of the underlying causal graph. In this paper, we study the problem of causal estimation from a MEC represented by a partial ancestral graph (PAG), which is learnable from observational data. We develop a general estimator for any identifiable causal effects in a PAG. The result fills a gap for an end-to-end solution to causal inference from observational data to effects estimation. Specifically, we develop a complete identification algorithm that derives an influence function for any identifiable causal effects from PAGs. We then construct a double/debiased machine learning (DML) estimator that is robust to model misspecification and biases in nuisance function estimation, permitting the use of modern machine learning techniques. Simulation results corroborate with the theory.},
	language = {en},
	urldate = {2022-10-20},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Jung, Yonghan and Tian, Jin and Bareinboim, Elias},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5168--5179},
}

@article{smucler_efficient_2022,
	title = {Efficient adjustment sets in causal graphical models with hidden variables},
	volume = {109},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article/109/1/49/6174686},
	doi = {10.1093/biomet/asab018},
	abstract = {We study the selection of adjustment sets for estimating the interventional mean under a point exposure dynamic treatment regime, that is, a treatment rule that depends on the subject’s covariates. We assume a nonparametric causal graphical model with, possibly, hidden variables and at least one adjustment set comprised of observable variables. We provide the deﬁnition of a valid adjustment set for a point exposure dynamic treatment regime, which generalizes the existing deﬁnition for a static intervention. We show that there exists an adjustment set, referred to as optimal minimal, that yields the nonparametric estimator of the interventional mean with the smallest asymptotic variance among those that are based on observable minimal adjustment sets. An observable minimal adjustment set is a valid adjustment set such that all its variables are observable and the removal of any of its variables destroys its validity. We provide similar optimality results for the class of observable minimum adjustment sets, that is, valid observable adjustment sets of minimum cardinality among the observable adjustment sets. Moreover, we show that if either no variables are hidden or if all the observable variables are ancestors of either treatment, outcome or the variables that are used to decide treatment, a globally optimal adjustment set exists. We provide polynomial-time algorithms to compute the globally optimal, optimal minimal and optimal minimum adjustment sets. Because static interventions can be viewed as a special case of dynamic regimes, all our results also apply for static interventions.},
	language = {en},
	number = {1},
	urldate = {2022-07-28},
	journal = {Biometrika},
	author = {Smucler, E and Sapienza, F and Rotnitzky, A},
	month = feb,
	year = {2022},
	pages = {49--65},
}

@misc{perkovic_complete_2015,
	title = {A {Complete} {Generalized} {Adjustment} {Criterion}},
	url = {http://arxiv.org/abs/1507.01524},
	abstract = {Covariate adjustment is a widely used approach to estimate total causal effects from observational data. Several graphical criteria have been developed in recent years to identify valid covariates for adjustment from graphical causal models. These criteria can handle multiple causes, latent confounding, or partial knowledge of the causal structure; however, their diversity is confusing and some of them are only sufficient, but not necessary. In this paper, we present a criterion that is necessary and sufficient for four different classes of graphical causal models: directed acyclic graphs (DAGs), maximum ancestral graphs (MAGs), completed partially directed acyclic graphs (CPDAGs), and partial ancestral graphs (PAGs). Our criterion subsumes the existing ones and in this way unifies adjustment set construction for a large set of graph classes.},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Perković, Emilija and Textor, Johannes and Kalisch, Markus and Maathuis, Marloes H.},
	month = jul,
	year = {2015},
	note = {arXiv:1507.01524 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@article{wang_conditional_2015,
	title = {Conditional {Distance} {Correlation}},
	volume = {110},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2014.993081},
	doi = {10.1080/01621459.2014.993081},
	language = {en},
	number = {512},
	urldate = {2022-07-27},
	journal = {Journal of the American Statistical Association},
	author = {Wang, Xueqin and Pan, Wenliang and Hu, Wenhao and Tian, Yuan and Zhang, Heping},
	month = oct,
	year = {2015},
	pages = {1726--1734},
}

@article{anand_effect_nodate,
	title = {Effect {Identification} in {Cluster} {Causal} {Diagrams}},
	abstract = {One pervasive task found throughout the empirical sciences is to determine the effect of interventions from non-experimental data. It is well-understood that assumptions are necessary to perform causal inferences, which are commonly articulated through causal diagrams (Pearl, 2000). Despite the power of this approach, there are settings where the knowledge necessary to specify a causal diagram over all observed variables may not be available, particularly in complex, high-dimensional domains. In this paper, we introduce a new type of graphical model called cluster causal diagrams (for short, C-DAGs) that allows for the partial specification of relationships among variables based on limited prior knowledge, alleviating the stringent requirement of specifying a full causal diagram. A C-DAG specifies relationships between clusters of variables, while the relationships between the variables within a cluster are left unspecified. We develop the foundations and machinery for valid causal inferences over C-DAGs. In particular, we first define a new version of the d-separation criterion and prove its sound-ness and completeness. Secondly, we extend these new separation rules and prove the validity of the corresponding do-calculus. Lastly, we show that a standard identification algorithm is sound and complete to systematically compute causal effects from observational data given a C-DAG.},
	author = {Anand, Tara V and Ribeiro, Adèle H and Tian, Jin and Bareinboim, Elias},
}

@article{zheng_dags_nodate,
	title = {{DAGs} with {NO} {TEARS}: {Continuous} {Optimization} for {Structure} {Learning}},
	url = {https://github.com/xunzheng/},
	abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P},
}

@article{spirtes_causation_1993,
	title = {Causation, {Prediction}, and {Search}},
	volume = {81},
	issn = {978-1-4612-7650-0},
	url = {http://link.springer.com/10.1007/978-1-4612-2748-9},
	doi = {10.1007/978-1-4612-2748-9},
	author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
	year = {1993},
	note = {Place: New York, NY
Publisher: Springer New York},
}

@inproceedings{niculescu-mizil_predicting_2005,
	address = {New York, New York, USA},
	title = {Predicting good probabilities with supervised learning},
	isbn = {1-59593-180-5},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102430},
	doi = {10.1145/1102351.1102430},
	abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
	publisher = {ACM Press},
	author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
	year = {2005},
	pages = {625--632},
}

@article{oord_wavenet_2016,
	title = {{WaveNet}: {A} {Generative} {Model} for {Raw} {Audio}},
	issn = {9783901882760},
	url = {http://arxiv.org/abs/1609.03499},
	doi = {10.1109/ICASSP.2009.4960364},
	abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	year = {2016},
	pages = {1--15},
}

@inproceedings{meyerson_pseudo-task_2018,
	title = {Pseudo-task augmentation: {From} deep multitask learning to intratask sharing-and back},
	volume = {8},
	isbn = {978-1-5108-6796-3},
	url = {https://arxiv.org/pdf/1803.04062.pdf},
	abstract = {Deep multitask learning boosts performance by sharing learned structure across related tasks. This paper adapts ideas from deep multitask learning to the setting where only a single task is available. The method is formalized as pseudo-task augmentation, in which models are trained with multiple decoders for each task. Pseudo-tasks simulate the effect of training towards closely- related tasks drawn from the same universe. In a suite of experiments, pseudo-task augmentation improves performance on single-task learning problems. When combined with multitask learning, further improvements are achieved, including state-of-the-art performance on the CelebA dataset, showing that pseudo-task augmentation and multitask learning have complementary value. All in all, pseudo-task augmentation is a broadly applicable and efficient way to boost performance in deep learning systems.},
	author = {Meyerson, Elliot and Miikkulainen, Risto},
	year = {2018},
	pages = {5664--5673},
}

@article{bengio_representation_2012,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	url = {http://arxiv.org/abs/1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	journal = {Arxiv},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = jun,
	year = {2012},
}

@article{lecun_lenet_1998,
	title = {{LeNet}},
	issn = {0018-9219},
	url = {http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf},
	doi = {10.1109/5.726791},
	abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
	number = {November},
	journal = {Proceedings of the IEEE},
	author = {Lecun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	year = {1998},
	keywords = {convo-, document recogni-, fi-, gradient-based learning, graph transformer networks, lutional neural networks, machine learning, neural networks, nite state transducers, ocr, tion},
	pages = {1--46},
}

@article{mcnemar_note_1947,
	title = {Note on the sampling error of the difference between correlated proportions or percentages},
	volume = {12},
	url = {https://link.springer.com/article/10.1007/BF02295996},
	doi = {10.1007/BF02295996},
	abstract = {Two formulas are presented for judging the significance of the difference between correlated proportions. The chi square equivalent of one of the developed formulas is pointed out. © 1947 Psychometric Society.},
	number = {2},
	journal = {Psychometrika},
	author = {McNemar, Quinn},
	month = jun,
	year = {1947},
	note = {Publisher: Springer-Verlag},
	keywords = {Assessment, Humanities, Law, Psychometrics, Statistical Theory and Methods, Statistics for Social Sciences, Testing and Evaluation},
	pages = {153--157},
}

@article{tsividis_human_2017,
	title = {Human {Learning} in {Atari}},
	issn = {9781577357797},
	url = {http://gershmanlab.webfactional.com/pubs/Tsividis17.pdf},
	abstract = {Atari games are an excellent testbed for studying intelligent behavior, as they offer a range of tasks that differ widely in their visual representation, game dynamics, and goals pre-sented to an agent. The last two years have seen a spate of research into artificial agents that use a single algorithm to learn to play these games. The best of these artificial agents perform at better-than-human levels on most games, but re-quire hundreds of hours of game-play experience to produce such behavior. Humans, on the other hand, can learn to per-form well on these tasks in a matter of minutes. In this pa-per we present data on human learning trajectories for several Atari games, and test several hypotheses about the mecha-nisms that lead to such rapid learning.},
	journal = {\{AAAI Spring Symposium on Science of Intelligence: Computational Principles of Natural and Artificial Intelligence\}},
	author = {Tsividis, Pedro A and Pouncy, Thomas and Xu, Jacqueline L and Tenenbaum, Joshua B and Gershman, Samuel J},
	year = {2017},
}

@article{stuart_active_1994,
	title = {Active propagation of somatic action potentials into neocortical pyramidal cell dendrites},
	volume = {367},
	url = {http://www.nature.com/articles/367069a0},
	doi = {10.1038/367069a0},
	abstract = {THE dendrites of neurons in the mammalian central nervous system have been considered as electrically passive structures which funnel synaptic potentials to the soma and axon initial segment, the site of action potential initiation1,2. More recent studies, however, have shown that the dendrites of many neurons are not passive, but contain active conductances 3,4. The role of these dendritic voltage-activated channels in the initiation of action potentials in neurons is largely unknown. To assess this directly, patch-clamp recordings were made from the dendrites of neocortical pyramidal cells in brain slices. Voltage-activated sodium currents were observed in dendritic outside-out patches, while action potentials could be evoked by depolarizing current pulses or by synaptic stimulation during dendritic whole-cell recordings. To determine the site of initiation of these action potentials, simultaneous whole-cell recordings were made from the soma and the apical dendrite or axon of the same cell. These experiments showed that action potentials are initiated first in the axon and then actively propagate back into the dendritic tree. © 1994 Nature Publishing Group.},
	number = {6458},
	journal = {Nature},
	author = {Stuart, Greg J. and Sakmann, Bert},
	month = jan,
	year = {1994},
	pages = {69--72},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0262661160},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	year = {1986},
	pages = {533--536},
}

@inproceedings{fabbri_statistical_2017,
	title = {A {Statistical} {Distance} {Derived} {From} {The} {Kolmogorov}-{Smirnov} {Test}: specification, reference measures (benchmarks) and example uses},
	url = {https://arxiv.org/pdf/1711.00761.pdf},
	abstract = {Statistical distances quantifies the difference between two statistical constructs. In this article, we describe reference values for a distance between samples derived from the Kolmogorov-Smirnov statistic \$D\_\{F,F'\}\$. Each measure of the \$D\_\{F,F'\}\$ is a measure of difference between two samples. This distance is normalized by the number of observations in each sample to yield the \$c'=D\_\{F,F'\}{\textbackslash}sqrt\{{\textbackslash}frac\{n n'\}\{n+n'\}\}\$ statistic, for which high levels favor the rejection of the null hypothesis (that the samples are drawn from the same distribution). One great feature of \$c'\$ is that it inherits the robustness of \$D\_\{F,F'\}\$ and is thus suitable for use in settings where the underlying distributions are not known. Benchmarks are obtained by comparing samples derived from standard distributions. The supplied example applications of the \$c'\$ statistic for the distinction of samples in real data enables further insights about the robustness and power of such statistical distance.},
	author = {Fabbri, Renato and De León, Fernando Gularte},
	year = {2017},
	note = {Issue: 4},
	keywords = {benchmark, kolmogorov-smirnov test, statistical distance, statistical test, statistics},
}

@article{yao_deep_2017,
	title = {Deep {Learning} from {Noisy} {Image} {Labels} with {Quality} {Embedding}},
	volume = {14},
	url = {http://arxiv.org/abs/1711.00583},
	abstract = {There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among the datasets severely degenerates the {\textbackslash}mbox\{performance of deep\} learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a quality embedding model, which explicitly introduces a quality variable to represent the trustworthiness of noisy labels. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the noise effect. At the same time, the high-quality labels is still able to be applied for training. To instantiate the model, we further propose a Contrastive-Additive Noise network (CAN), which consists of two important layers: (1) the contrastive layer estimates the quality variable in the embedding space to reduce noise effect; and (2) the additive layer aggregates the prior predictions and noisy labels as the posterior to train the classifier. Moreover, to tackle the optimization difficulty, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We conduct the experimental evaluation of the proposed method over a range of noisy image datasets. Comprehensive results have demonstrated CAN outperforms the state-of-the-art deep learning approaches.},
	number = {8},
	author = {Yao, Jiangchao and Wang, Jiajie and Tsang, Ivor and Zhang, Ya and Sun, Jun and Zhang, Chengqi and Zhang, Rui},
	year = {2017},
	pages = {1--12},
}

@article{dodhia_review_2005,
	title = {A {Review} of {Applied} {Multiple} {Regression}/{Correlation} {Analysis} for the {Behavioral} {Sciences}},
	volume = {30},
	url = {https://www.jstor.org/stable/pdf/3701351.pdf},
	abstract = {Applied M ple Regression/Correlation Analysis for the Behavioral Sciences (3rd ed.). Mah NJ: Erlbaum, 2003, 728 pp., \$65. The previous editions of this book, written by the first two authors, Jacob Patricia Cohen, became essential reference tools for many social scientists needed a solid grounding in statistical methodology. It was first published in with an intention to provide a foundation in the regression framework for the tical researcher. The second edition (1983) focused on causality and analysis variance and covariance. The third edition is even more extensive, containing topics that have evolved over the last 20 years, and has extensions of earlier ters. It remains as vital as ever. The expansion of the book was greatly aided by Leona Aiken and Stephen West, whose expertise on interactions and longitudinal studies helped build some of the new chapters. Jacob Cohen died in 1998, but his contributions and direction of focus for this edition still show the indispensability of his thinking for social scientists. Some readers may be disappointed that the focus is almost completely on regres-sion analyses, with barely a mention of analysis of variance. However, Jacob Cohen was a leading proponent of bringing separate data analysis frameworks within more unified, general frameworks. He also pushed for de-emphasizing classical hypo-thesis testing with significance values, but many of the examples carried over from the previous edition still retain this format. These are examples of shifts that are indicative of the direction statistical analysis has taken in the last few years. From their choice of topics to include and to discontinue, the authors' assessment of cur-rent trends in statistical analysis seems to have been on the mark. Although Jacob Cohen's thinking and style will be missed, his coauthors will surely carry on this enterprise. Their success in this edition has us looking forward to the fourth edition. The changes from the previous edition are numerous, resulting in a version that, although bearing a great resemblance to its predecessor, is richer with greater breadth and elaborated issues. Many topics have been updated to reflect recent developments in applied statistical analysis and are usually explained in detail. Nevertheless, readers will find that this still leaves quite a few topics undeveloped. Hence, two levels of exposition become apparent. There are detailed accounts where the theory is supplemented by examples and methodological instruction. 227 This content downloaded from 162.129.251.102 on Wed, 26 Apr 2017 13:33:17 UTC All use subject to http://about.jstor.org/terms},
	number = {2},
	journal = {Journal of Educational and Behavioral Statistics},
	author = {Dodhia, Rahul M},
	year = {2005},
	pages = {227--229},
}

@article{kelly_key_2019,
	title = {Key challenges for delivering clinical impact with artificial intelligence},
	volume = {17},
	url = {https://bmcmedicine.biomedcentral.com/articles/10.1186/s12916-019-1426-2},
	doi = {10.1186/s12916-019-1426-2},
	abstract = {Background: Artificial intelligence (AI) research in healthcare is accelerating rapidly, with potential applications being demonstrated across various domains of medicine. However, there are currently limited examples of such techniques being successfully deployed into clinical practice. This article explores the main challenges and limitations of AI in healthcare, and considers the steps required to translate these potentially transformative technologies from research to clinical practice. Main body: Key challenges for the translation of AI systems in healthcare include those intrinsic to the science of machine learning, logistical difficulties in implementation, and consideration of the barriers to adoption as well as of the necessary sociocultural or pathway changes. Robust peer-reviewed clinical evaluation as part of randomised controlled trials should be viewed as the gold standard for evidence generation, but conducting these in practice may not always be appropriate or feasible. Performance metrics should aim to capture real clinical applicability and be understandable to intended users. Regulation that balances the pace of innovation with the potential for harm, alongside thoughtful post-market surveillance, is required to ensure that patients are not exposed to dangerous interventions nor deprived of access to beneficial innovations. Mechanisms to enable direct comparisons of AI systems must be developed, including the use of independent, local and representative test sets. Developers of AI algorithms must be vigilant to potential dangers, including dataset shift, accidental fitting of confounders, unintended discriminatory bias, the challenges of generalisation to new populations, and the unintended negative consequences of new algorithms on health outcomes. Conclusion: The safe and timely translation of AI research into clinically validated and appropriately regulated systems that can benefit everyone is challenging. Robust clinical evaluation, using metrics that are intuitive to clinicians and ideally go beyond measures of technical accuracy to include quality of care and patient outcomes, is essential. Further work is required (1) to identify themes of algorithmic bias and unfairness while developing mitigations to address these, (2) to reduce brittleness and improve generalisability, and (3) to develop methods for improved interpretability of machine learning predictions. If these goals can be achieved, the benefits for patients are likely to be transformational.},
	number = {1},
	journal = {BMC Medicine},
	author = {Kelly, Christopher J. and Karthikesalingam, Alan and Suleyman, Mustafa and Corrado, Greg and King, Dominic},
	month = dec,
	year = {2019},
	keywords = {Algorithms, Artificial intelligence, Evaluation, Machine learning, Regulation, Translation},
	pages = {195--195},
}

@article{arora_provable_2013,
	title = {Provable {Bounds} for {Learning} {Some} {Deep} {Representations}},
	issn = {9781634393973},
	url = {http://arxiv.org/abs/1310.6343},
	abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an \$n\$ node multilayer neural net that has degree at most \$n{\textasciicircum}\{{\textbackslash}gamma\}\$ for some \${\textbackslash}gamma {\textless}1\$ and each edge has a random edge weight in \$[-1,1]\$. Our algorithm learns \{{\textbackslash}em almost all\} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
	author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
	year = {2013},
}

@article{welch_use_1967,
	title = {The {Use} of {Fast} {Fourier} {Transform} for the {Estimation} of {Power} {Spectra}: {A} {Method} {Based} on {Time} {Averaging} {Over} {Short}, {Modified} {Periodograms}},
	volume = {15},
	url = {http://ieeexplore.ieee.org/document/1161901/},
	doi = {10.1109/TAU.1967.1161901},
	abstract = {The use of the fast Fourier transform in power spectrum analysis is described. Principal advantages of this method are a reduction in the number of computations and in required core storage, and convenient application in nonstationarity tests. The method involves sectioning the record and averaging modified periodograms of the sections.},
	number = {2},
	journal = {IEEE Transactions on Audio and Electroacoustics},
	author = {Welch, Peter D.},
	month = jun,
	year = {1967},
	pages = {70--73},
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	number = {Nips},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	year = {2017},
}

@article{dunson_response_1992,
	title = {Response models for mixed binary and quantitative variables},
	volume = {79},
	url = {https://academic.oup.com/biomet/advance-article/doi/10.1093/biomet/asz066/5686745?guestAccessKey=2cd7946a-ed54-4b15-be48-f8f12965b545},
	doi = {10.1093/BIOMET},
	abstract = {Cox, D. R., \& Wermuth, N. (1992). Response models for mixed binary and quantitative variables. Biometrika, 79(3), 441-461.},
	number = {3},
	journal = {Biometrika},
	author = {Dunson, D B and Johndrow, J E},
	month = dec,
	year = {1992},
	note = {Publisher: Oxford Academic},
	keywords = {Direct adjustment, Discriminant matching, Matched sampling, Same hey words: Covariance adjustment, Standardization, Stratification, Subclassification, Xonrandomized study},
	pages = {441--461--441--461},
}

@article{murdoch_p-values_2008,
	title = {P-values are random variables},
	volume = {62},
	issn = {0003-1305},
	doi = {10.1198/000313008X332421},
	abstract = {P-values are taught in introductory statistics classes in a way that confuses many of the students, leading to common misconceptions about their meaning. In this article, we argue that p-values should be taught through simulation, emphasizing that p-values are random variables. By means of elementary examples we illustrate how to teach students valid interpretations of p-values and give them a deeper understanding of hypothesis testing.},
	number = {3},
	journal = {American Statistician},
	author = {Murdoch, Duncan J. and Tsai, Yu Ling and Adcock, James},
	year = {2008},
	keywords = {Empirical cumulative distribution function (ECDF), Histograms, Hypothesis testing, Teaching statistics},
	pages = {242--245},
}

@article{jaber_causal_2019,
	title = {Causal {Identification} under {Markov} {Equivalence}: {Completeness} {Results}},
	url = {https://proceedings.mlr.press/v97/jaber19a.html},
	abstract = {Causal effect identification is the task of determining whether a causal distribution is computable from the combination of an observational distribution and substantive knowledge about the domain under investigation. One of the most studied versions of this problem assumes that knowledge is articulated in the form of a fully known causal diagram, which is arguably a strong assumption in many settings. In this paper, we relax this requirement and consider that the knowledge is articulated in the form of an equivalence class of causal diagrams, in particular, a partial ancestral graph (PAG). This is attractive because a PAG can be learned directly from data, and the scientist does not need to commit to a particular, unique diagram. There are different sufficient conditions for identification in PAGs, but none is complete. We derive a complete algorithm for identification given a PAG. This implies that whenever the causal effect is identifiable, the algorithm returns a valid identification expression; alternatively, it will throw a failure condition, which means that the effect is provably not identifiable. We further provide a graphical characterization of non-identifiability of causal effects in PAGs.},
	author = {Jaber, Amin and Zhang, Jiji and Bareinboim, Elias},
	month = may,
	year = {2019},
	note = {Publisher: PMLR},
	pages = {2981--2989},
}

@article{spirtes_causal_2013,
	title = {Causal {Inference} in the {Presence} of {Latent} {Variables} and {Selection} {Bias}},
	url = {https://arxiv.org/abs/1302.4983v1},
	abstract = {We show that there is a general, informative and reliable procedure for
discovering causal relations when, for all the investigator knows, both latent
variables and selection bias may be at work. Given information about
conditional independence and dependence relations between measured variables,
even when latent variables and selection bias may be present, there are
sufficient conditions for reliably concluding that there is a causal path from
one variable to another, and sufficient conditions for reliably concluding when
no such causal path exists.},
	author = {Spirtes, Peter L. and Meek, Christopher and Richardson, Thomas S.},
	month = feb,
	year = {2013},
}

@article{spirtes_polynomial_1997,
	title = {A {Polynomial} {Time} {Algorithm} for {Determining} {DAG} {Equivalence} in the {Presence} of {Latent} {Variables} and {Selection} {Bias}},
	url = {https://proceedings.mlr.press/r1/spirtes97b.html},
	abstract = {Following the terminology of Lauritzen et. al. (1990) say that a probability measure over a set of variables V satisfies the local directed Markov property for a directed acyclic graph (DAG) G with vertices V if and only if for every Win V, Wis independent of the set of all its non-descendants conditional on the set of its parents. One natural question that arises with respect to DAGs is when two DAGs are "statistically equivalent". One interesting sense of "statistical equivalence" is "conditional independence equivalence" which holds when two DAGs entail the same set of conditional independence relations. In the case of DAGs, conditional independence equivalence also corresponds to a variety of other natural senses of statistical equivalence (such as representing the same set of distributions). Theorems characterizing conditional independence equivalence for directed acyclic graphs and that can be used as the basis for polynomial time algorithms for checking conditional independence equivalence were provided by Verma and Pearl (1990), and Frydenberg (1990). The question we will examine is how to extend these results to cases where a DAG may have latent (unmeasured) variables or selection bias (i.e. some of the variables in the DAG have been conditioned on.) Conditional independence equivalence is of interest in part because there are algorithms for constructing DAGs with latent variables and selection bias that are based on observed conditional independence relations. For this class of algorithms, it is impossible to determine which of two conditional independence equivalent causal structures generated a given probability distribution, given only the set of conditional independence and dependence relations true of the observed distribution. We will describe a polynomial (in the number of vertices) time algorithm for determining when two DAGs which may have latent variables or selection bias are conditional independence equivalent. A DAG G entails a conditional independence relation R if and only if R is true in every probability measure satisfying the local directed Markov property for G. (We place definitions and sets of variables in boldface.) Pearl, Geiger, and Verma (Pearl 1988) have shown that there is a graphical relation, ct-separation, that holds among three disjoint sets of variable A, and B, and C in DAG G if and only if G entails that A is independent of B given C. A vertex Y is a collider on an undirected path U if U contains a subpath X ➔ Y {\textasciitilde} Z. Say that a vertex V on an undirected path U between X and Y is active on U given Z (Z not containing X and Y) if and only if either V is not a collider on U and not in Z, or V is a collider on U and is an ancestor of Z. For three disjoint sets of variables A, B, and C, A is d-connected to B given C in graph G, if and only if there is an undirected path from some member of A to a member of B such that every vertex on U is active given C; for three disjoint sets of variables A, B, and C, A is d-separated from B given C in graph G, if and only A is not cl-connected to B given C. Two DAGs are conditional independence equivalent if and only if they have the same vertices and entail the same set of conditional independence relations. If two DAGs G 1 and G 2 are conditional independence equivalent, the set of distributions that satisfy the local directed Markov property for G 1 equals the set of distribution that satisfy the local directed Markov property for G 2 • Theorems that provide the basis for polynomial time algorithms for testing conditional independence equivalence for DAGs were given in Verma and Pearl (1990), for cyclic directed graphs in Richardson (1994), and for directed acyclic graphs with latent variables in Spirtes and Verma (1992). DAGs are also used to represent causal processes. Under this interpretation, a directed edge from A to B means that A is a direct cause of B relative to the variables in the DAG. Suppose a causal process represented by DAG G generates some population with a given distribution P(V) that satisfies the local directed Markov property for G. If some of the variables in V are unmeasured, and some have been conditioned on (due to those variables being causally related to the sampling mechanism) then the set of conditional independence relations entailed for the subset of measured variables in the subpopulation from which the sample is drawn is not necessarily equal to the set of conditional independence relations entailed by any DAG (without latent variables or selection bias). Assume then that the variables in V can be partitioned into O (observed), L (latent), and S (selected, or conditioned on.) In that case instead of observing P(V), we may be able to observe only P(OIS), that is the marginal distribution over the observed 489},
	author = {Spirtes, Peter and Richardson, Thomas S.},
	month = jan,
	year = {1997},
	note = {Publisher: PMLR},
	pages = {489--500},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	url = {https://www.pnas.org/content/113/15/3932},
	doi = {10.1073/PNAS.1517384113/-/DCSUPPLEMENTAL},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan and Bialek, William},
	month = apr,
	year = {2016},
	note = {Publisher: National Academy of Sciences},
	keywords = {Dynamical systems, Machine learning, Optimization, Sparse regression, System identification},
	pages = {3932--3937},
}

@article{correa_general_nodate,
	title = {General {Transportability} of {Soft} {Interventions}: {Completeness} {Results}},
	abstract = {The challenge of generalizing causal knowledge across different environments is pervasive in scientific explorations, including in AI, ML, and Data Science. Experiments are usually performed in one environment (e.g., in a lab, on Earth) with the intent, almost invariably, of being used elsewhere (e.g., outside the lab, on Mars), where the conditions are likely to be different. In the causal inference literature, this generalization task has been formalized under the rubric of transportability (Pearl and Bareinboim, 2011), where a number of criteria and algorithms have been developed for various settings. Despite the generality of such results, trans-portability theory has been confined to atomic, do()-interventions. In practice, many real-world applications require more complex, stochastic interventions; for instance, in reinforcement learning, agents need to continuously adapt to the changing conditions of an uncertain and unknown environment. In this paper, we extend transportability theory to encompass these more complex types of interventions, which are known as "soft," both relative to the input as well as the target distribution of the analysis. Specifically, we develop a graphical condition that is both necessary and sufficient for deciding soft-transportability. Second, we develop an algorithm to determine whether a non-atomic intervention is computable from a combination of the distributions available across domains. As a corollary, we show that the σ-calculus is complete for the task of soft-transportability.},
	author = {Correa, Juan D and Bareinboim, Elias},
}

@article{jaber_graphical_2018,
	title = {A graphical criterion for effect identification in equivalence classes of causal diagrams},
	volume = {2018-July},
	issn = {9780999241127},
	doi = {10.24963/IJCAI.2018/697},
	abstract = {Computing the effects of interventions from observational data is an important task encountered in many data-driven sciences. The problem is addressed by identifying the post-interventional distribution with an expression that involves only quantities estimable from the pre-interventional distribution over observed variables, given some knowledge about the causal structure. In this work, we relax the requirement of having a fully specified causal structure and study the identifiability of effects with a singleton intervention (X), supposing that the structure is known only up to an equivalence class of causal diagrams, which is the output of standard structural learning algorithms (e.g., FCI). We derive a necessary and sufficient graphical criterion for the identifiability of the effect of X on all observed variables. We further establish a sufficient graphical criterion to identify the effect of X on a subset of the observed variables, and prove that it is strictly more powerful than the current state-of-the-art result on this problem.},
	journal = {IJCAI International Joint Conference on Artificial Intelligence},
	author = {Jaber, Amin and Zhang, Jiji and Bareinboim, Elias},
	year = {2018},
	note = {Publisher: International Joint Conferences on Artificial Intelligence},
	pages = {5024--5030},
}

@article{simchowitz_learning_2018,
	title = {Learning {Without} {Mixing}: {Towards} {A} {Sharp} {Analysis} of {Linear} {System} {Identification}},
	url = {http://arxiv.org/abs/1802.08334},
	abstract = {We prove that the ordinary least-squares (OLS) estimator attains nearly minimax optimal performance for the identification of linear dynamical systems from a single observed trajectory. Our upper bound relies on a generalization of Mendelson's small-ball method to dependent data, eschewing the use of standard mixing-time arguments. Our lower bounds reveal that these upper bounds match up to logarithmic factors. In particular, we capture the correct signal-to-noise behavior of the problem, showing that more unstable linear systems are easier to estimate. This behavior is qualitatively different from arguments which rely on mixing-time calculations that suggest that unstable systems are more difficult to estimate. We generalize our technique to provide bounds for a more general class of linear response time-series.},
	journal = {arXiv},
	author = {Simchowitz, Max and Mania, Horia and Tu, Stephen and Jordan, Michael I. and Recht, Benjamin},
	month = feb,
	year = {2018},
	note = {Publisher: arXiv},
}

@article{kato_perturbation_1995,
	title = {Perturbation {Theory} for {Linear} {Operators}},
	volume = {132},
	issn = {978-3-540-58661-6},
	url = {http://link.springer.com/10.1007/978-3-642-66282-9},
	doi = {10.1007/978-3-642-66282-9},
	author = {Kato, Tosio},
	year = {1995},
	note = {Place: Berlin, Heidelberg
Publisher: Springer Berlin Heidelberg},
}

@article{holm_simple_1979,
	title = {A {Simple} {Sequentially} {Rejective} {Multiple} {Test} {Procedure} {A} {Simple} {Sequentially} {Rejective} {Multiple} {Test} {Procedure}},
	volume = {6},
	issn = {201308:15:53},
	abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Wiley and Board of the Foundation of the Scandinavian Journal of Statistics are collaborating with JSTOR to digitize, preserve and extend access to Scandinavian Journal of Statistics. ABSTRACT. This paper presents a simple and widely applicable multiple test procedure of the sequentially rejective type, i.e. hypotheses are rejected one at a tine until no further rejections can be done. It is shown that the test has a prescribed level of significance protection against error of the first kind for any combination of true hypotheses. The power properties of the test and a number of possible applications are also discussed.},
	number = {2},
	journal = {Source: Scandinavian Journal of Statistics},
	author = {Holm, Sture},
	year = {1979},
	keywords = {multiple test, simultaneous test},
	pages = {65--70},
}

@inproceedings{gilpin_explaining_2019,
	title = {Explaining explanations: {An} overview of interpretability of machine learning},
	isbn = {978-1-5386-5090-5},
	url = {https://arxiv.org/pdf/1806.00069.pdf},
	doi = {10.1109/DSAA.2018.00018},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	year = {2019},
	keywords = {Deep learning and deep analytics, Fairness and transparency in data science, Machine learning theories, Models and systems},
	pages = {80--89},
}

@article{zhang_partial_2021,
	title = {Partial {Counterfactual} {Identification} from {Observational} and {Experimental} {Data}},
	url = {https://arxiv.org/abs/2110.05690v1},
	abstract = {This paper investigates the problem of bounding counterfactual queries from
an arbitrary collection of observational and experimental distributions and
qualitative knowledge about the underlying data-generating model represented in
the form of a causal diagram. We show that all counterfactual distributions in
an arbitrary structural causal model (SCM) could be generated by a canonical
family of SCMs with the same causal diagram where unobserved (exogenous)
variables are discrete with a finite domain. Utilizing the canonical SCMs, we
translate the problem of bounding counterfactuals into that of polynomial
programming whose solution provides optimal bounds for the counterfactual
query. Solving such polynomial programs is in general computationally
expensive. We therefore develop effective Monte Carlo algorithms to approximate
the optimal bounds from an arbitrary combination of observational and
experimental data. Our algorithms are validated extensively on synthetic and
real-world datasets.},
	author = {Zhang, Junzhe and Tian, Jin and Bareinboim, Elias},
	month = oct,
	year = {2021},
}

@article{xia_causal-neural_2021,
	title = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}},
	url = {https://arxiv.org/abs/2107.00793v2},
	abstract = {One of the central elements of any causal inference is an object called
structural causal model (SCM), which represents a collection of mechanisms and
exogenous sources of random variation of the system under investigation (Pearl,
2000). An important property of many kinds of neural networks is universal
approximability: the ability to approximate any function to arbitrary
precision. Given this property, one may be tempted to surmise that a collection
of neural nets is capable of learning any SCM by training on data generated by
that SCM. In this paper, we show this is not the case by disentangling the
notions of expressivity and learnability. Specifically, we show that the causal
hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits
of what can be learned from data, still holds for neural models. For instance,
an arbitrarily complex and expressive neural net is unable to predict the
effects of interventions given observational data alone. Given this result, we
introduce a special type of SCM called a neural causal model (NCM), and
formalize a new type of inductive bias to encode structural constraints
necessary for performing causal inferences. Building on this new class of
models, we focus on solving two canonical tasks found in the literature known
as causal identification and estimation. Leveraging the neural toolbox, we
develop an algorithm that is both sufficient and necessary to determine whether
a causal effect can be learned from data (i.e., causal identifiability); it
then estimates the effect whenever identifiability holds (causal estimation).
Simulations corroborate the proposed approach.},
	author = {Xia, Kevin and Lee Bloomberg, Kai-Zhan LP and Bengio, Yoshua and Bareinboim, Elias},
	month = jul,
	year = {2021},
}

@article{bareinboim_transportability_2012,
	title = {Transportability of {Causal} {Effects}: {Completeness} {Results}},
	volume = {26},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/8232},
	abstract = {The study of transportability aims to identify conditions under which causal information learned from experiments can be reused in a different environment where only passive observations can be collected. The theory introduced in [Pearl and Bareinboim, 2011] (henceforth [PB, 2011]) defines formal conditions for such transfer but falls short of providing an effective procedure for deciding, given assumptions about differences between the source and target domains, whether transportability is feasible. This paper provides such procedure. It establishes a necessary and sufficient condition for deciding when causal effects in the target domain are estimable from both the statistical information available and the causal information transferred from the experiments. The paper further provides a complete algorithm for computing the transport formula, that is, a way of fusing experimental and observational information to synthesize an estimate of the desired causal relation.},
	number = {1},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Bareinboim, Elias and Pearl, Judea},
	year = {2012},
	keywords = {Knowledge Representation and Reasoning (Main Track)},
	pages = {698--704},
}

@article{zhang_causal_2008,
	title = {Causal {Reasoning} with {Ancestral} {Graphs}},
	volume = {9},
	abstract = {Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The first result extends Pearl (1995)'s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl's calculus-the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the first result. The second result also improves the earlier, similar results due to Spirtes et al. (1993).},
	journal = {Journal of Machine Learning Research},
	author = {Zhang, Jiji and Cooper, Gregory F},
	year = {2008},
	keywords = {ancestral graphs, causal Bayesian network, do-calculus, intervention},
	pages = {1437--1474},
}

@article{kocaoglu_experimental_2017,
	title = {Experimental {Design} for {Learning} {Causal} {Graphs} with {Latent} {Variables}},
	volume = {30},
	abstract = {We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(d log 2 n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d 2 log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n 2) interventions, our combined algorithm can learn the causal graph with latents using O(d log 2 n + d 2 log (n)) interventions.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},
	year = {2017},
}

@book{pearl_book_2019,
	title = {The book of why : the new science of cause and effect},
	isbn = {978-0-465-09760-9},
	abstract = {First edition. "Everyone has heard the claim, "Correlation does not imply causation." What might sound like a reasonable dictum metastasized in the twentieth century into one of science's biggest obstacles, as a legion of researchers became unwilling to make the claim that one thing could cause another. Even two decades ago, asking a statistician a question like "Was it the aspirin that stopped my headache?" would have been like asking if he believed in voodoo, or at best a topic for conversation at a cocktail party rather than a legitimate target of scientific inquiry. Scientists were allowed to posit only that the probability that one thing was associated with another. This all changed with Judea Pearl, whose work on causality was not just a victory for common sense, but a revolution in the study of the world" Introduction : Mind over data -- The ladder of causation -- From buccaneers to guinea pigs the genesis of causal inference -- From evidence to causes: Reverend Bayes meets Mr. Holmes -- Confounding and deconfounding: or, slaying the lurking variable -- The smoke-filled debate: clearing the air -- Paradoxes galore! -- Beyond adjustment: the conquest of Mount Intervention -- Counterfactuals: mining worlds that could have been -- Mediation: the search for a mechanism -- Big data, artificial intelligence, and the big questions.},
	author = {Pearl, Judea and Mackenzie, Dana},
	year = {2019},
	note = {Pages: 418},
}

@article{balke_probabilistic_1994,
	title = {Probabilistic {Evaluation} of {Counterfactual} {Queries}},
	url = {www.aaai.org},
	abstract = {Evaluation of counterfactual queries (e.g., "If A were true, would C have been true?") is important to fault diagnosis, planning, and determination of liability. We present a formalism that uses probabilistic causal networks to evaluate one's belief that the counterfactual consequent, C, would have been true if the antecedent, A, were true. The antecedent of the query is interpreted as an external action that forces the proposition A to be true, which is consistent with Lewis' Miraculous Analysis. This formalism offers a concrete embodiment of the "closest world" approach which (I) properly reflects common understanding of causal influences , (2) deals with the uncertainties inherent in the world, and (3) is amenable to machine representation .},
	journal = {AAAI-Proceedings},
	author = {Balke, , Alexander and Pearl, , Judea},
	year = {1994},
}

@article{galles_axiomatic_1998,
	title = {An {Axiomatic} {Characterization} of {Causal} {Counterfactuals}},
	volume = {3},
	url = {https://link.springer.com/article/10.1023/A:1009602825894},
	doi = {10.1023/A:1009602825894},
	abstract = {This paper studies the causal interpretation of counterfactual sentences using a modifiable structural equation model. It is shown that two properties of counterfactuals, namely, composition and effectiveness, are sound and complete relative to this interpretation, when recursive (i.e., feedback-less) models are considered. Composition and effectiveness also hold in Lewis's closest-world semantics, which implies that for recursive models the causal interpretation imposes no restrictions beyond those embodied in Lewis's framework. A third property, called reversibility, holds in nonrecursive causal models but not in Lewis's closest-world semantics, which implies that Lewis's axioms do not capture some properties of systems with feedback. Causal inferences based on counterfactual analysis are exemplified and compared to those based on graphical models.},
	number = {1},
	journal = {Foundations of Science 1998 3:1},
	author = {Galles, David and Pearl, Judea},
	year = {1998},
	note = {Publisher: Springer},
	keywords = {Interventions, Mathematical Logic and Foundations, Methodology of the Social Sciences, Philosophy of Science, Policy analysis, Structural equations},
	pages = {151--182},
}

@article{forney_counterfactual_2019,
	title = {Counterfactual {Randomization}: {Rescuing} {Experimental} {Studies} from {Obscured} {Confounding}},
	volume = {33},
	issn = {9781577358091},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4090},
	doi = {10.1609/AAAI.V33I01.33012454},
	abstract = {Randomized clinical trials (RCTs) like those conducted by the FDA provide medical practitioners with average effects of treatments, and are generally more desirable than observational studies due to their control of unobserved confounders (UCs), viz., latent factors that influence both treatment and recovery. However, recent results from causal inference have shown that randomization results in a subsequent loss of information about the UCs, which may impede treatment efficacy if left uncontrolled in practice (Bareinboim, Forney, and Pearl 2015). Our paper presents a novel experimental design that can be noninvasively layered atop past and future RCTs to not only expose the presence of UCs in a system, but also reveal patient- and practitioner-specific treatment effects in order to improve decision-making. Applications are given to personalized medicine, second opinions in diagnosis, and employing offline results in online recommender systems.},
	number = {01},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Forney, Andrew and Bareinboim, Elias},
	month = jul,
	year = {2019},
	note = {Publisher: AAAI Press},
	pages = {2454--2461},
}

@article{bareinboim_bandits_nodate,
	title = {Bandits with {Unobserved} {Confounders}: {A} {Causal} {Approach}},
	abstract = {The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmea-sured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employ-ing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.},
	author = {Bareinboim, Elias and Forney, Andrew and Pearl, Judea},
}

@book{judea_pearl_causality_2009,
	edition = {2nd},
	title = {{CAUSALITY}: {Models}, reasoning, and inference.},
	url = {http://bayes.cs.ucla.edu/BOOK-2K/},
	publisher = {Cambridge University Press},
	author = {{Judea Pearl}},
	year = {2009},
}

@article{shpitser_complete_2008,
	title = {Complete {Identification} {Methods} for the {Causal} {Hierarchy}},
	volume = {9},
	abstract = {We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables ; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple "parallel worlds" and resulting from simultaneous, possibly conflicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Specifically, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy.},
	journal = {Journal of Machine Learning Research},
	author = {Shpitser, Ilya and Pearl, Judea},
	year = {2008},
	keywords = {causality, graphical causal models, identification},
	pages = {1941--1979},
}

@article{bareinboim_causal_2012,
	title = {Causal {Inference} by {Surrogate} {Experiments}: z-{Identifiability}},
	issn = {9780974903989},
	url = {https://arxiv.org/abs/1210.4842v1},
	abstract = {We address the problem of estimating the effect of intervening on a set of
variables X from experiments on a different set, Z, that is more accessible to
manipulation. This problem, which we call z-identifiability, reduces to
ordinary identifiability when Z = empty and, like the latter, can be given
syntactic characterization using the do-calculus [Pearl, 1995; 2000]. We
provide a graphical necessary and sufficient condition for z-identifiability
for arbitrary sets X,Z, and Y (the outcomes). We further develop a complete
algorithm for computing the causal effect of X on Y using information provided
by experiments on Z. Finally, we use our results to prove completeness of
do-calculus relative to z-identifiability, a result that does not follow from
completeness relative to ordinary identifiability.},
	journal = {Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI 2012},
	author = {Bareinboim, Elias and Pearl, Judea},
	month = oct,
	year = {2012},
	pages = {113--120},
}

@article{ke_learning_nodate,
	title = {Learning {Neural} {Causal} {Models} from {Unknown} {Interventions}},
	abstract = {Promising results have driven a recent surge of interest in continuous optimization methods for Bayesian network structure learning from observational data. However, there are theoretical limitations on the identifiability of underlying structures obtained from observational data alone. Interventional data provides much richer information about the underlying data-generating process. However, the extension and application of methods designed for observational data to include interventions is not straightforward and remains an open problem. In this paper we provide a general framework based on continuous optimization and neural networks to create models for the combination of observational and interventional data. The proposed method is even applicable in the challenging and realistic case that the identity of the intervened upon variable is unknown. We examine the proposed method in the setting of graph recovery both de novo and from a partially-known edge set. We establish strong benchmark results on several structure learning tasks, including structure recovery of both synthetic graphs as well as standard graphs from the Bayesian Network Repository.},
	author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Schölkopf, Bernhard and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
}

@article{kocaoglu_characterization_2019,
	title = {Characterization and {Learning} of {Causal} {Graphs} with {Latent} {Variables} from {Soft} {Interventions}},
	volume = {32},
	abstract = {The challenge of learning the causal structure underlying a certain phenomenon is undertaken by connecting the set of conditional independences (CIs) readable from the observational data, on the one side, with the set of corresponding constraints implied over the graphical structure, on the other, which are tied through a graphical criterion known as d-separation (Pearl, 1988). In this paper, we investigate the more general setting where multiple observational and experimental distributions are available. We start with the simple observation that the invariances given by CIs/d-separation are just one special type of a broader set of constraints, which follow from the careful comparison of the different distributions available. Remarkably, these new constraints are intrinsically connected with do-calculus (Pearl, 1995) in the context of soft-interventions. We then introduce a novel notion of interventional equivalence class of causal graphs with latent variables based on these invariances, which associates each graphical structure with a set of interventional distributions that respect the do-calculus rules. Given a collection of distributions, two causal graphs are called interventionally equivalent if they are associated with the same family of interventional distributions, where the elements of the family are indistinguishable using the invariances obtained from a direct application of the calculus rules. We introduce a graphical representation that can be used to determine if two causal graphs are interventionally equivalent. We provide a formal graphical characterization of this equivalence. Finally, we extend the FCI algorithm, which was originally designed to operate based on CIs, to combine observational and interventional datasets, including new orientation rules particular to this setting.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Kocaoglu, Murat and Jaber, Amin and Shanmugam, Karthikeyan and Bareinboim, Elias},
	year = {2019},
}

@article{pearl_transportability_2018,
	title = {Transportability across studies: {A} formal approach},
	abstract = {We provide a formal definition of the notion of "transportability," or "external validity," which we view as a license to transfer causal information learned in experimental studies to a different environment, in which only observational studies can be conducted. We introduce a formal representation called "selection diagrams" for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we derive procedures for deciding whether causal effects in the target environment can be inferred from experimental findings in a different environment. When the answer is affirmative, the procedures identify the set of experimental and observational studies that need be conducted to license the transport. We further demonstrate how transportability analysis can guide the transfer of knowledge among non-experimental studies to minimize re-measurement cost and improve prediction power. We further provide a causally principled definition of "surrogate endpoint" and show that the theory of transportability can assist the identification of valid surrogates in a complex network of cause-effect relationships.},
	author = {Pearl, Judea and Bareinboim, Elias},
	year = {2018},
}

@article{correa_statistical_nodate,
	title = {From {Statistical} {Transportability} to {Estimating} the {Effect} of {Stochastic} {Interventions}},
	abstract = {Learning systems often face a critical challenge when applied to settings that differ from those under which they were initially trained. In particular , the assumption that both the source/training and the target/deployment domains follow the same causal mechanisms and observed distributions is commonly violated. This implies that the robust-ness and convergence guarantees usually expected from these methods are no longer attainable. In this paper, we study these violations through causal lens using the formalism of statistical transporta-bility [Pearl and Bareinboim, 2011] (PB, for short). We start by proving sufficient and necessary graph-ical conditions under which a probability distribution observed in the source domain can be extrapolated to the target one, where strictly less data is available. We develop the first sound and complete procedure for statistical transportability, which formally closes the problem introduced by PB. Further , we tackle the general challenge of identification of stochastic interventions from observational data [Sec. 4.4, Pearl, 2000]. This problem has been solved in the context of atomic interventions using Pearl's do-calculus, which lacks complete treatment in the stochastic case. We prove completeness of stochastic identification by constructing a reduction of any instance of this problem to an instance of statistical transportability, closing the problem.},
	author = {Correa, Juan D and Bareinboim, Elias},
}

@article{criminisi_decision_2011,
	title = {Decision forests: {A} unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning},
	volume = {7},
	url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CriminisiForests_FoundTrends_2011.pdf},
	doi = {10.1561/0600000035},
	abstract = {This review presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision, and medical image analysis tasks. Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semisupervised learning, and active learning under the same decision forest framework. This gives us the opportunity to write and optimize the core implementation only once, with application to many diverse tasks. The proposed model may be used both in a discriminative or generative way and may be applied to discrete or continuous, labeled or unlabeled data. The main contributions of this review are: (1) Proposing a unified, probabilistic and efficient model for a variety of learning tasks; (2) Demonstrating margin-maximizing properties of classification forests; (3) Discussing probabilistic regression forests in comparison with other nonlinear regression algorithms; (4) Introducing density forests for estimating probability density functions; (5) Proposing an efficient algorithm for sampling from a density forest; (6) Introducing manifold forests for nonlinear dimensionality reduction; (7) Proposing new algorithms for transductive learning and active learning. Finally, we discuss how alternatives such as random ferns and extremely randomized trees stem from our more general forest model. This document is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in the new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state-of-the-art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed. The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility. © 2012 A. Criminisi, J. Shotton and E. Konukoglu.},
	number = {2-3},
	journal = {Foundations and Trends in Computer Graphics and Vision},
	author = {Criminisi, Antonio and Shotton, Jamie and Konukoglu, Ender},
	year = {2011},
	pages = {81--227},
}

@article{bareinboim_causal_2016,
	title = {Causal inference and the data-fusion problem},
	volume = {113},
	url = {https://www.pnas.org/content/113/27/7345},
	doi = {10.1073/PNAS.1510507113},
	abstract = {We review concepts, principles, and tools that unify current approaches to causal analysis and attend to new challenges presented by big data. In particular, we address the problem of data fusion—piecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts, because the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, nonparametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data fusion in causal inference tasks.},
	number = {27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bareinboim, Elias and Pearl, Judea},
	month = jul,
	year = {2016},
	note = {Publisher: National Academy of Sciences},
	keywords = {Causal inference, Counterfactuals, External validity, Selection bias, Transportability},
	pages = {7345--7352},
}

@article{wager_estimation_2018,
	title = {Estimation and {Inference} of {Heterogeneous} {Treatment} {Effects} using {Random} {Forests}},
	volume = {113},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2017.1319839},
	doi = {10.1080/01621459.2017.1319839},
	abstract = {Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we...},
	number = {523},
	journal = {Journal of the American Statistical Association},
	author = {Wager, Stefan and Athey, Susan},
	month = jul,
	year = {2018},
	note = {Publisher: Taylor \& Francis},
	keywords = {Adaptive nearest neighbors matching, Asymptotic normality, Potential outcomes, Unconfoundedness},
	pages = {1228--1242},
}

@article{tian_general_2002,
	title = {A {General} {Identification} {Condition} for {Causal} {Effects}},
	url = {www.aaai.org},
	abstract = {This paper concerns the assessment of the effects of actions or policy interventions from a combination of: (i) nonexperi-mental data, and (ii) substantive assumptions. The assumptions are encoded in the form of a directed acyclic graph, also called "causal graph", in which some variables are presumed to be unobserved. The paper establishes a necessary and sufficient criterion for the identifiability of the causal effects of a singleton variable on all other variables in the model, and a powerful sufficient criterion for the effects of a singleton variable on any set of variables.},
	journal = {AAAI},
	author = {Tian, Jin and Pearl, Judea},
	year = {2002},
}

@inproceedings{lee_general_2019,
	title = {General identifiability with arbitrary surrogate experiments},
	abstract = {We study the problem of causal identification from an arbitrary collection of observational and experimental distributions, and substantive knowledge about the phenomenon under investigation, which usually comes in the form of a causal graph. We call this problem g-identifiability, or gID for short. The gID setting encompasses two well-known problems in causal inference, namely, identifiability [Pearl, 1995] and z-identifiability [Bareinboim and Pearl, 2012] — the former assumes that an observational distribution is necessarily available, and no experiments can be performed, conditions that are both relaxed in the gID setting; the latter assumes that all combinations of experiments are available, i.e., the power set of the experimental set Z, which gID does not require a priori. In this paper, we introduce a general strategy to prove non-gID based on hedgelets and thickets, which leads to a necessary and sufficient graphical condition for the corresponding decision problem. We further develop a procedure for systematically computing the target effect, and prove that it is sound and complete for gID instances. In other words, failure of the algorithm in returning an expression implies that the target effect is not computable from the available distributions. Finally, as a corollary of these results, we show that do-calculus is complete for the task of g-identifiability.},
	author = {Lee, Sanghack and Correa, Juan D. and Bareinboim, Elias},
	year = {2019},
}

@article{shpitser_identification_2006,
	title = {Identification of {Joint} {Interventional} {Distributions} in {Recursive} {Semi}-{Markovian} {Causal} {Models}},
	abstract = {This paper is concerned with estimating the effects of actions from causal assumptions, represented concisely as a directed graph, and statistical knowledge, given as a probability distribution. We provide a necessary and sufficient graphical condition for the cases when the causal effect of an arbitrary set of variables on another arbitrary set can be determined uniquely from the available information, as well as an algorithm which computes the effect whenever this condition holds. Furthermore, we use our results to prove completeness of do-calculus [Pearl, 1995], and a version of an identification algorithm in [Tian, 2002] for the same identification problem.},
	journal = {AAAI-Proceedings},
	author = {Shpitser, Ilya and Pearl, Judea},
	year = {2006},
	pages = {1219--1226},
}

@article{zhang_generalized_nodate,
	title = {Generalized {Do}-{Calculus} with {Testable} {Causal} {Assumptions}},
	abstract = {A primary object of causal reasoning concerns what would happen to a system under certain interventions. Specifically, we are often interested in estimating the probability distribution of some random variables that would result from forcing some other variables to take certain values. The renowned do-calculus (Pearl 1995) gives a set of rules that govern the identification of such post-intervention probabilities in terms of (estimable) pre-intervention probabilities , assuming available a directed acyclic graph (DAG) that represents the underlying causal structure. However, a DAG causal structure is seldom fully testable given pre-intervention, observational data, since many competing DAG structures are equally compatible with the data. In this paper we extend the do-calculus to cover cases where the available causal information is summarized in a so-called partial ancestral graph (PAG) that represents an equivalence class of DAG structures. The causal assumptions encoded by a PAG are significantly weaker than those encoded by a full-blown DAG causal structure , and are in principle fully testable by observed conditional independence relations.},
	author = {Zhang, Jiji},
}

@article{entner_causal_nodate,
	title = {On {Causal} {Discovery} from {Time} {Series} {Data} using {FCI}},
	abstract = {We adapt the Fast Causal Inference (FCI) algorithm of Spirtes et al. (2000) to the problem of inferring causal relationships from time series data and evaluate our adaptation and the original FCI algorithm, comparing them to other methods including Granger causality. One advantage of FCI based approaches is the possibility of taking latent confounding variables into account, as opposed to methods based on Granger causality. From simulations we see, however, that while the FCI based approaches are in principle quite powerful for finding causal relationships in time series data, such methods are not very reliable for most practical sample sizes. We further apply the framework to microeconomic data on the dynamics of firm growth. By releasing the full computer code for the method we hope to facilitate the application of the procedure to other domains.},
	author = {Entner, Doris and Hoyer, Patrik O},
}

@article{runge_detecting_2019,
	title = {Detecting and quantifying causal associations in large nonlinear time series datasets},
	volume = {5},
	url = {https://www.science.org/doi/10.1126/sciadv.aau4996},
	doi = {10.1126/SCIADV.AAU4996/SUPPL_FILE/AAU4996_SM.PDF},
	abstract = {Identifying causal relationships and quantifying their strength from observational time series data are key problems in disciplines dealing with complex dynamical systems such as the Earth system or the human body. Data-driven causal inference in such systems is challenging since datasets are often high dimensional and nonlinear with limited sample sizes. Here, we introduce a novel method that flexibly combines linear or nonlinear conditional independence tests with a causal discovery algorithm to estimate causal networks from large-scale time series datasets. We validate the method on time series of well-understood physical mechanisms in the climate system and the human heart and using large-scale synthetic datasets mimicking the typical properties of real-world data. The experiments demonstrate that our method outperforms state-of-the-art techniques in detection power, which opens up entirely new possibilities to discover and quantify causal networks from time series across a range of research fields.},
	number = {11},
	journal = {Science Advances},
	author = {Runge, Jakob and Nowack, Peer and Kretschmer, Marlene and Flaxman, Seth and Sejdinovic, Dino},
	month = nov,
	year = {2019},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {4996--5023},
}

@article{runge_discovering_nodate,
	title = {Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets},
	abstract = {The paper introduces a novel conditional independence (CI) based method for linear and nonlinear, lagged and contemporaneous causal discovery from observational time series in the causally sufficient case. Existing CI-based methods such as the PC algorithm and also common methods from other frameworks suffer from low recall and partially inflated false positives for strong autocorrelation which is an ubiquitous challenge in time series. The novel method, PCMCI + , extends PCMCI [Runge et al., 2019b] to include discovery of contempo-raneous links. PCMCI + improves the reliability of CI tests by optimizing the choice of conditioning sets and even benefits from auto-correlation. The method is order-independent and consistent in the oracle case. A broad range of numerical experiments demonstrates that PCMCI + has higher adjacency detection power and especially more contempo-raneous orientation recall compared to other methods while better controlling false posi-tives. Optimized conditioning sets also lead to much shorter runtimes than the PC algorithm. PCMCI + can be of considerable use in many real world application scenarios where often time resolutions are too coarse to resolve time delays and strong autocorrelation is present.},
	author = {Runge, Jakob},
}

@article{lakens_calculating_2013,
	title = {Calculating and reporting effect sizes to facilitate cumulative science: {A} practical primer for t-tests and {ANOVAs}},
	volume = {4},
	doi = {10.3389/fpsyg.2013.00863},
	abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
	number = {NOV},
	journal = {Frontiers in Psychology},
	author = {Lakens, Daniël},
	year = {2013},
	keywords = {Cohen's d, eta-squared, Effect sizes, Power analysis, Sample size planning},
}

@article{kalman_new_1960,
	title = {A new approach to linear filtering and prediction problems},
	volume = {82},
	url = {https://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf},
	doi = {10.1115/1.3662552},
	abstract = {The classical filtering and prediction problem is re-examined using the Bode-Sliannon representation of random processes and the “state-transition” method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinitememory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the coefficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix. © 1960 by ASME.},
	number = {1},
	journal = {Journal of Fluids Engineering, Transactions of the ASME},
	author = {Kalman, R E},
	year = {1960},
	pages = {35--45},
}

@article{wassertheil_statistical_1970,
	title = {Statistical {Power} {Analysis} for the {Behavioral} {Sciences}},
	volume = {26},
	url = {https://books.google.com/books?id=2v9zDAsLvA0C&pg=PP1#v=onepage&q&f=false},
	doi = {10.2307/2529115},
	abstract = {Statistical Power Analysisis a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes: a chapter covering power analysis in set correlation and multivariate methods; a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; expanded power and sample size tables for multiple regression/correlation.},
	number = {3},
	journal = {Biometrics},
	author = {Wassertheil, Sylvia and Cohen, Jacob},
	year = {1970},
	pages = {588--588},
}

@article{qin_robust_2017,
	title = {Robust hypothesis testing via {Lq}-likelihood},
	volume = {27},
	issn = {202015.0441},
	url = {https://doi.org/10.5705/ss.202015.0441},
	doi = {10.5705/ss.202015.0441},
	abstract = {This article introduces a robust hypothesis testing procedure: the Lq-likelihood-ratio-type test (LqRT). By deriving the asymptotic distribution of the test statistic, we demonstrate its robustness analytically and numerically, and investigate the properties of its influence function and breakdown point. A proposed method to select the tuning parameter q offers a good efficiency/robustness tradeoff compared with the traditional likelihood ratio test (LRT) and other robust tests. Simulation and a real data analysis provide further evidence of the advantages of the proposed LqRT method. In particular, for the special case of testing the location parameter in the presence of gross error contamination, the LqRT dominates the Wilcoxon-Mann-Whitney test and the sign test at various levels of contamination.},
	number = {4},
	journal = {Statistica Sinica},
	author = {Qin, Yichen and Priebe, Carey E},
	year = {2017},
	keywords = {Gross error model, Relative efficiency, Robustness},
	pages = {1793--1813},
}

@incollection{ramsay_functional_2015,
	title = {Functional {Data} {Analysis}},
	isbn = {978-0-08-097087-5},
	abstract = {Most statistical analyses involve one or more observations taken on each of a number of individuals in a sample, with the aim of making inferences about the general population from which the sample is drawn. In an increasing number of fields, these observations are curves or images. Curves and images are examples of functions, since an observed intensity is available at each point on a line segment, a portion of a plane, or a volume. For this reason, we call observed curves and images 'functional data,' and statistical methods for analyzing such data are described by the term 'functional data analysis.' It is the smoothness of the processes generating functional data that differentiates this type of data from more classical multivariate observations. This smoothness means that we can work with the information in the derivatives of functions or images. This article includes several illustrative examples.},
	booktitle = {International {Encyclopedia} of the {Social} \& {Behavioral} {Sciences}: {Second} {Edition}},
	publisher = {Springer},
	author = {Ramsay, James O. and Silverman, Bernard W.},
	year = {2015},
	doi = {10.1016/B978-0-08-097086-8.42046-5},
	keywords = {Curves, Data analyses, Functional data analysis},
	pages = {514--518},
}

@article{cochran_comparison_1950,
	title = {The comparison of percentages in matched samples.},
	volume = {37},
	url = {https://academic.oup.com/biomet/article-abstract/37/3-4/256/176193},
	doi = {10.1093/biomet/37.3-4.256},
	number = {3-4},
	journal = {Biometrika},
	author = {COCHRAN, W. G.},
	year = {1950},
	pages = {256--266},
}

@article{mann_test_1947,
	title = {On a {Test} of {Whether} one of {Two} {Random} {Variables} is {Stochastically} {Larger} than the {Other}},
	volume = {18},
	doi = {10.1214/aoms/1177730491},
	abstract = {The object of this note is to point out and discuss a simple transformation of an absolutely continuous k-variate distribution in the uniform distribution on the k-dimensional hypercube.},
	number = {1},
	journal = {The Annals of Mathematical Statistics},
	author = {Mann, H. B. and Whitney, D. R.},
	month = mar,
	year = {1947},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {50--60},
}

@article{alyakin_lqrt_2019,
	title = {{LqRT}: {Robust} {Hypothesis} {Testing} of {Location} {Parameters} using {Lq}-{Likelihood}-{Ratio}-{Type} {Test} in {Python}},
	url = {http://arxiv.org/abs/1911.11922},
	abstract = {A t-test is considered a standard procedure for inference on population means and is widely used in scientific discovery. However, as a special case of a likelihood-ratio test, t-test often shows drastic performance degradation due to the deviations from its hard-to-verify distributional assumptions. Alternatively, in this article, we propose a new two-sample Lq-likelihood-ratio-type test (LqRT) along with an easy-to-use Python package for implementation. LqRT preserves high power when the distributional assumption is violated, and maintains the satisfactory performance when the assumption is valid. As numerical studies suggest, LqRT dominates many other robust tests in power, such as Wilcoxon test and sign test, while maintaining a valid size. To the extent that the robustness of the Wilcoxon test (minimum asymptotic relative efficiency (ARE) of the Wilcoxon test vs the t-test is 0.864) suggests that the Wilcoxon test should be the default test of choice (rather than "use Wilcoxon if there is evidence of non-normality", the default position should be "use Wilcoxon unless there is good reason to believe the normality assumption"), the results in this article suggest that the LqRT is potentially the new default go-to test for practitioners.},
	journal = {Arxiv},
	author = {Alyakin, Anton and Qin, Yichen and Priebe, Carey E.},
	month = nov,
	year = {2019},
}

@article{priebe_statistical_2015,
	title = {Statistical {Inference} on {Errorfully} {Observed} {Graphs}},
	volume = {24},
	url = {https://www.tandfonline.com/doi/abs/10.1080/10618600.2014.951049},
	doi = {10.1080/10618600.2014.951049},
	abstract = {Statistical inference on graphs is a burgeoning field in the applied and theoretical statistics communities, as well as throughout the wider world of science, engineering, business, etc. In many applications, we are faced with the reality of errorfully observed graphs. That is, the existence of an edge between two vertices is based on some imperfect assessment. In this article, we consider a graph G = (V, E). We wish to perform an inference task—the inference task considered here is “vertex classification,” that is, given a vertex v with unknown label Y(v), we want to infer the label for v based on the graph G and the given labels for some set of vertices in G not containing v. However, we do not observe G; rather, for each potential edge (Formula presented.) we observe an “edge feature” that we use to classify uv as edge/not-edge. Thus, we errorfully observe G when we observe the graph (Formula presented.) as the edges in (Formula presented.) arise from the classifications of the “edge features,” and are expected to be errorful. Moreover, we face a quantity/quality trade-off regarding the edge features we observe—more informative edge features are more expensive, and hence the number of potential edges that can be assessed decreases with the quality of the edge features. We studied this problem by formulating a quantity/quality trade-off for a simple class of random graphs model, namely, the stochastic blockmodel. We then consider a simple but optimal vertex classifier for classifying v and we derive the optimal quantity/quality operating point for subsequent graph inference in the face of this trade-off. The optimal operating points for the quantity/quality trade-off are surprising and illustrate the issue that methods for intermediate tasks should be chosen to maximize performance for the ultimate inference task. Finally, we investigate the quantity/quality tradeoff for errorful observations of the C. elegans connectome graph.},
	number = {4},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Priebe, Carey E. and Sussman, Daniel L. and Tang, Minh and Vogelstein, Joshua T.},
	month = oct,
	year = {2015},
	note = {Publisher: American Statistical Association},
	keywords = {Classification, Connectomics, Error, Graph, Social networks},
	pages = {930--953},
}

@inproceedings{li_fragility_2017,
	title = {Fragility in {Epileptic} {Networks} : the {Epileptogenic} {Zone}},
	isbn = {978-1-5090-5992-8},
	doi = {10.23919/ACC.2017.7963378},
	author = {Li, Adam and Inati, Sara and Zaghloul, Kareem and Sarma, Sridevi},
	year = {2017},
	pages = {1--8},
}

@article{whitley_sign_2002,
	title = {The sign test},
	volume = {6},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/12493072},
	doi = {10.1186/cc1820},
	abstract = {509 ICU = intensive care unit; SvO 2 = central venous oxygen saturation. Available online http://ccforum.com/content/6/6/509 Many statistical methods require assumptions to be made about the format of the data to be analysed. For example, the paired t-test introduced in Statistics review 5 requires that the distribu-tion of the differences be approximately Normal, while the unpaired t-test requires an assumption of Normality to hold sep-arately for both sets of observations. Fortunately, these assump-tions are often valid in clinical data, and where they are not true of the raw data it is often possible to apply a suitable transforma-tion. There are situations in which even transformed data may not satisfy the assumptions, however, and in these cases it may be inappropriate to use traditional (parametric) methods of analysis. (Methods such as the t-test are known as 'parametric' because they require estimation of the parameters that define the underlying distribution of the data; in the case of the t-test, for instance, these parameters are the mean and standard devi-ation that define the Normal distribution.) Nonparametric methods provide an alternative series of statis-tical methods that require no or very limited assumptions to be made about the data. There is a wide range of methods that can be used in different circumstances, but some of the more commonly used are the nonparametric alternatives to the t-tests, and it is these that are covered in the present review.},
	number = {6},
	journal = {Critical care (London, England)},
	author = {Whitley, Elise and Ball, Jonathan},
	month = dec,
	year = {2002},
	note = {Publisher: BioMed Central},
	pages = {509--13},
}

@article{prieto_smart_2017,
	title = {Smart school multimodal dataset and challenges},
	volume = {1828},
	issn = {9781450321389},
	doi = {10.1145/1235},
	abstract = {The pervasive presence of interconnected objects enables new communication paradigms where devices can easily reach each other while interacting within their environment. The so-called Internet of Things (IoT) represents the integration of several computing and communications systems aiming at facilitating the interaction between these devices. Arduino is one of the most popular platforms used to prototype new IoT devices due to its open, flexible and easy-to-use archi- tecture. Ardunio Yun is a dual board microcontroller that supports a Linux distribution and it is currently one of the most versatile and powerful Arduino systems. This feature positions Arduino Yun as a popular platform for developers, but it also introduces unique infection vectors from the secu- rity viewpoint. In this work, we present a security analysis of Arduino Yun. We show that Arduino Yun is vulnerable to a number of attacks and we implement a proof of concept capable of exploiting some of them.},
	journal = {CEUR Workshop Proceedings},
	author = {Prieto, Luis P. and Rodríguez-Triana, María Jesús and Kusmin, Marge and Laanpere, Mart},
	year = {2017},
	keywords = {Multimodal learning analytics, Multimodal teaching analytics, STEM education, Sensors, Smart classroom, Smart school},
	pages = {53--59},
}

@article{jovanovic_sparsity-promoting_2014,
	title = {Sparsity-promoting dynamic mode decomposition},
	volume = {26},
	url = {http://dx.doi.org/10.1063/1.4863670]},
	doi = {10.1063/1.4863670},
	abstract = {Dynamic mode decomposition (DMD) represents an effective means for capturing the essential features of numerically or experimentally generated flow fields. In order to achieve a desirable tradeoff between the quality of approximation and the number of modes that are used to approximate the given fields, we develop a sparsity-promoting variant of the standard DMD algorithm. Sparsity is induced by regularizing the least-squares deviation between the matrix of snapshots and the linear combination of DMD modes with an additional term that penalizes the l1-norm of the vector of DMD amplitudes. The globally optimal solution of the resulting regularized convex optimization problem is computed using the alternating direction method of multipliers, an algorithm well-suited for large problems. Several examples of flow fields resulting from numerical simulations and physical experiments are used to illustrate the effectiveness of the developed method. © 2014 AIP Publishing LLC.},
	number = {2},
	journal = {Physics of Fluids},
	author = {Jovanović, Mihailo R. and Schmid, Peter J and Nichols, Joseph W},
	year = {2014},
	pages = {24103--24103},
}

@article{chollet_xception_2017,
	title = {Xception: {Deep} learning with depthwise separable convolutions},
	volume = {2017-Janua},
	issn = {9781538604571},
	doi = {10.1109/CVPR.2017.195},
	abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
	journal = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	author = {Chollet, François},
	year = {2017},
	pages = {1800--1807},
}

@article{saunders_solution_1995,
	title = {Solution of sparse rectangular systems using {LSQR} and {CRAIG}},
	volume = {35},
	url = {http://link.springer.com/10.1007/BF01739829},
	doi = {10.1007/BF01739829},
	number = {4},
	journal = {BIT Numerical Mathematics},
	author = {Saunders, Michael A.},
	month = dec,
	year = {1995},
	note = {Publisher: Kluwer Academic Publishers},
	pages = {588--604},
}

@article{van_der_zander_finding_nodate,
	title = {Finding minimal d-separators in linear time and applications *},
	abstract = {The study of graphical causal models is fundamentally the study of separations and conditional independences. We provide linear-time algorithms for two graphical primitives: to test, if a given set is a minimal d-separator, and to find a minimal d-separator in directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs) and restricted chain graphs (RCGs) as well as minimal m-separators in ancestral graphs (AGs). These algorithms improve the runtime of the best previously known algorithms for minimal separa-tors that are based on moralization and thus require quadratic time to construct and handle the moral graph. (Minimal) separating sets have important applications like finding (min-imal) covariate adjustment sets or conditional instrumental variables.},
	author = {Van Der Zander, Benito and Li´skiewiczli´skiewicz, Maciej},
}

@article{colombo_order-independent_2014,
	title = {Order-{Independent} {Constraint}-{Based} {Causal} {Structure} {Learning}},
	volume = {15},
	abstract = {We consider constraint-based methods for causal structure learning, such as the PC-, FCI-, RFCI-and CCD-algorithms (Spirtes et al., 1993, 2000; Richardson, 1996; Colombo et al., 2012; Claassen et al., 2013). The first step of all these algorithms consists of the adja-cency search of the PC-algorithm. The PC-algorithm is known to be order-dependent, in the sense that the output can depend on the order in which the variables are given. This order-dependence is a minor issue in low-dimensional settings. We show, however, that it can be very pronounced in high-dimensional settings, where it can lead to highly variable results. We propose several modifications of the PC-algorithm (and hence also of the other algorithms) that remove part or all of this order-dependence. All proposed modifications are consistent in high-dimensional settings under the same conditions as their original counterparts. We compare the PC-, FCI-, and RFCI-algorithms and their modifications in simulation studies and on a yeast gene expression data set. We show that our modifications yield similar performance in low-dimensional settings and improved performance in high-dimensional settings. All software is implemented in the R-package pcalg.},
	journal = {Journal of Machine Learning Research},
	author = {Colombo, Diego and Maathuis, Marloes H},
	year = {2014},
	keywords = {CCD-algorithm, FCI-algorithm, PC-algorithm, consistency, directed acyclic graph, high-dimensional data, order-dependence},
	pages = {3921--3962},
}

@article{tian_finding_1998,
	title = {Finding {Minimal} {D}-separators},
	abstract = {We address the problem of nding a minimal separator in a directed acyclic graph (DAG), namely, nding a set Z of nodes that d-separates a given pair of nodes, such that no proper subset of Z d-separates that pair. We analyze several versions of this problem and ooer polynomial algorithms for each. These include: nding a minimal separator from a restricted set of nodes, ,nding a minimum-cost separator, and testing whether a given separator is minimal. We connrm the intuition that any separator which cannot be reduced by a s i n g l e n o d e m ust be minimal.},
	author = {Tian, Jin and Paz, Azaria and Pearl, Judea},
	year = {1998},
}

@article{tomita_sparse_2020,
	title = {Sparse projection oblique randomer forests},
	volume = {21},
	url = {https://cran.r-project.},
	abstract = {Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called “Sparse Projection Oblique Randomer Forests” (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with {\textgreater} 100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. Very sparse random projections can be incorporated into gradient boosted trees to obtain potentially similar gains.},
	journal = {Journal of Machine Learning Research},
	author = {Tomita, Tyler M and Browne, James and Shen, Cencheng and Chung, Jaewon and Patsolic, Jesse L and Falk, Benjamin and Priebe, Carey E and Yim, Jason and Burns, Randal and Maggioni, Mauro and Vogelstein, Joshua T},
	year = {2020},
	keywords = {Classification, Decision Trees, Ensemble Learning, Feature Extraction, Random Forests, Random Projections, Regression, Sparse Learning},
}

@article{couronne_random_2018,
	title = {Random forest versus logistic regression: {A} large-scale benchmark experiment},
	volume = {19},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2264-5},
	doi = {10.1186/s12859-018-2264-5},
	abstract = {Background and goal: The Random Forest (RF) algorithm for regression and classification has considerably gained popularity since its introduction in 2001. Meanwhile, it has grown to a standard classification approach competing with logistic regression in many innovation-friendly scientific fields. Results: In this context, we present a large scale benchmarking experiment based on 243 real datasets comparing the prediction performance of the original version of RF with default parameters and LR as binary classification tools. Most importantly, the design of our benchmark experiment is inspired from clinical trial methodology, thus avoiding common pitfalls and major sources of biases. Conclusion: RF performed better than LR according to the considered accuracy measured in approximately 69\% of the datasets. The mean difference between RF and LR was 0.029 (95\%-CI =[0.022,0.038]) for the accuracy, 0.041 (95\%-CI =[0.031,0.053]) for the Area Under the Curve, and - 0.027 (95\%-CI =[-0.034,-0.021]) for the Brier score, all measures thus suggesting a significantly better performance of RF. As a side-result of our benchmarking experiment, we observed that the results were noticeably dependent on the inclusion criteria used to select the example datasets, thus emphasizing the importance of clear statements regarding this dataset selection process. We also stress that neutral studies similar to ours, based on a high number of datasets and carefully designed, will be necessary in the future to evaluate further variants, implementations or parameters of random forests which may yield improved accuracy compared to the original version with default values.},
	number = {1},
	journal = {BMC Bioinformatics},
	author = {Couronné, Raphael and Probst, Philipp and Boulesteix, Anne Laure},
	month = jul,
	year = {2018},
	note = {Publisher: BioMed Central Ltd.},
	keywords = {Classification, Comparison study, Logistic regression, Prediction},
	pages = {270--270},
}

@article{perry_manifold_2019,
	title = {Manifold {Forests}: {Closing} the {Gap} on {Neural} {Networks}},
	url = {http://arxiv.org/abs/1909.11799},
	abstract = {Decision forests (DFs), in particular random forests and gradient boosting trees, have demonstrated state-of-the-art accuracy compared to other methods in many supervised learning scenarios. In particular, DFs dominate other methods in tabular data, that is, when the feature space is unstructured, so that the signal is invariant to permuting feature indices. However, in structured data lying on a manifold---such as images, text, and speech---deep networks (DNs), specifically convolutional deep networks (ConvNets), tend to outperform DFs. We conjecture that at least part of the reason for this is that the input to DNs is not simply the feature magnitudes, but also their indices (for example, the convolution operation uses feature locality). In contrast, naive DF implementations fail to explicitly consider feature indices. A recently proposed DF approach demonstrates that DFs, for each node, implicitly sample a random matrix from some specific distribution. These DFs, like some classes of DNs, learn by partitioning the feature space into convex polytopes corresponding to linear functions. We build on that approach and show that one can choose distributions in a manifold-aware fashion to incorporate feature locality. We demonstrate the empirical performance on data whose features live on three different manifolds: a torus, images, and time-series. In all simulations, our Manifold Oblique Random Forest (MORF) algorithm empirically dominates other state-of-the-art approaches that ignore feature space structure and challenges the performance of ConvNets. Moreover, MORF runs significantly faster than ConvNets and maintains interpretability and theoretical justification. This approach, therefore, has promise to enable DFs and other machine learning methods to close the gap to deep networks on manifold-valued data.},
	journal = {Arxiv},
	author = {Perry, Ronan and Tomita, Tyler M. and Mehta, Ronak and Arroyo, Jesus and Patsolic, Jesse and Falk, Benjamin and Vogelstein, Joshua T.},
	month = sep,
	year = {2019},
}

@article{zhang_simultaneous_2020,
	title = {A {Simultaneous} {Discover}-{Identify} {Approach} to {Causal} {Inference} in {Linear} {Models}},
	volume = {34},
	issn = {9781577358350},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6595},
	doi = {10.1609/AAAI.V34I06.6595},
	abstract = {Modern causal analysis involves two major tasks, discovery and identification. The first aims to learn a causal structure compatible with the available data, the second leverages that structure to estimate causal effects. Rather than performing the two tasks in tandem, as is usually done in the literature, we propose a symbiotic approach in which the two are performed simultaneously for mutual benefit; information gained through identification helps causal discovery and vice versa. This approach enables the usage of Verma constraints, which remain dormant in constraint-based methods of discovery, and permit us to learn more complete structures, hence identify a larger set of causal effects than previously achievable with standard methods.},
	number = {06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Chi and Chen, Bryant and Pearl, Judea},
	month = apr,
	year = {2020},
	note = {Publisher: AAAI press},
	keywords = {Reasoning under Uncertainty},
	pages = {10318--10325},
}

@article{henckel_graphical_2020,
	title = {Graphical criteria for efficient total effect estimation via adjustment in causal linear models},
	issn = {1907.02435v2},
	url = {https://github.com/henckell/CodeEfficientVAS.},
	abstract = {Covariate adjustment is a commonly used method for total causal effect estimation. In recent years, graphical criteria have been developed to identify all valid adjustment sets, that is, all covariate sets that can be used for this purpose. Different valid adjustment sets typically provide total effect estimates of varying accuracies. Restricting ourselves to causal linear models, we introduce a graphical criterion to compare the asymptotic variances provided by certain valid adjustment sets. We employ this result to develop two further graphical tools. First, we introduce a simple variance reducing pruning procedure for any given valid adjustment set. Second , we give a graphical characterization of a valid adjustment set that provides the optimal asymptotic variance among all valid adjustment sets. Our results depend only on the graphical structure and not on the specific error variances or edge coefficients of the underlying causal linear model. They can be applied to directed acyclic graphs (DAGs), completed partially directed acyclic graphs (CPDAGs) and maximally oriented partially directed acyclic graphs (maximal PDAGs). We present simulations and a real data example to support our results and show their practical applicability.},
	author = {Henckel, Leonard and Emilija, Zurich and Perkovi´c, Perkovi´c and Maathuis, Marloes H and Zurich, Eth},
	year = {2020},
}

@article{witte_efficient_2020,
	title = {On {Efficient} {Adjustment} in {Causal} {Graphs}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/20-175.html.},
	abstract = {We consider estimation of a total causal effect from observational data via covariate adjustment. Ideally, adjustment sets are selected based on a given causal graph, reflecting knowledge of the underlying causal structure. Valid adjustment sets are, however, not unique. Recent research has introduced a graphical criterion for an 'optimal' valid adjustment set (O-set). For a given graph, adjustment by the O-set yields the smallest asymptotic variance compared to other adjustment sets in certain parametric and non-parametric models. In this paper, we provide three new results on the O-set. First, we give a novel, more intuitive graphical characterisation: We show that the O-set is the parent set of the outcome node(s) in a suitable latent projection graph, which we call the forbidden projection. An important property is that the forbidden projection preserves all information relevant to total causal effect estimation via covariate adjustment, making it a useful methodological tool in its own right. Second, we extend the existing IDA algorithm to use the O-set, and argue that the algorithm remains semi-local. This is implemented in the R-package pcalg. Third, we present assumptions under which the O-set can be viewed as the target set of popular non-graphical variable selection algorithms such as stepwise backward selection.},
	journal = {Journal of Machine Learning Research},
	author = {Witte, Janine and Henckel, Leonard and Maathuis, Marloes H and Didelez, Vanessa},
	year = {2020},
	keywords = {IDA algorithm, causal discovery, causal inference, confounder selection, confounding, effi-ciency, graphical models, model selection, sufficient adjustment set},
	pages = {1--45},
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	url = {https://link.springer.com/content/pdf/10.1023%2FA%3A1010933404324.pdf},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, * * * , 148-156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	year = {2001},
	keywords = {classification, ensemble, regression},
	pages = {5--32},
}

@article{correa_calculus_2020,
	title = {A {Calculus} for {Stochastic} {Interventions}:{Causal} {Effect} {Identification} and {Surrogate} {Experiments}},
	volume = {34},
	issn = {9781577358350},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6567},
	doi = {10.1609/AAAI.V34I06.6567},
	abstract = {Some of the most prominent results in causal inference have been developed in the context of atomic interventions, following the semantics of the do-operator and the inferential power of the do-calculus. In practice, many real-world settings require more complex types of interventions that cannot be represented by a simple atomic intervention. In this paper, we investigate a general class of interventions that covers some non-trivial types of policies (conditional and stochastic), which goes beyond the atomic class. Our goal is to develop general understanding and formal machinery to be able to reason about the effects of those policies, similar to the robust treatment developed to handle the atomic case. Specifically, in this paper, we introduce a new set of inference rules (akin to do-calculus) that can be used to derive claims about general interventions, which we call σ-calculus. We develop a systematic and efficient procedure for finding estimands of the effect of general policies as a function of the available observational and experimental distributions. We then prove that our algorithm and σ-calculus are both sound for the tasks of identification (Pearl, 1995) and z-identification (Bareinboim and Pearl, 2012) under this class of interventions.},
	number = {06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Correa, Juan D. and Bareinboim, Elias},
	month = apr,
	year = {2020},
	note = {Publisher: AAAI press},
	keywords = {Reasoning under Uncertainty},
	pages = {10093--10100},
}

@article{pearl_invited_2011,
	title = {Invited {Commentary}: {Understanding} {Bias} {Amplification}},
	volume = {174},
	url = {https://academic.oup.com/aje/article/174/11/1223/111637},
	doi = {10.1093/AJE/KWR352},
	abstract = {In choosing covariates for adjustment or inclusion in propensity score analysis, researchers must weigh the benefit of reducing confounding bias carried by those covariates against the risk of amplifying residual bias carried by unmeasured confounders. The latter is characteristic of covariates that act like instrumental variables-that is, variables that are more strongly associated with the exposure than with the outcome. In this issue of the Journal (Am J Epidemiol. 2011;174(11):1213-1222), Myers et al. compare the bias amplification of a near-instrumental variable with its bias-reducing potential and suggest that, in practice, the latter outweighs the former. The author of this commentary sheds broader light on this comparison by considering the cumulative effects of conditioning on multiple covariates and showing that bias amplification may build up at a faster rate than bias reduction. The author further derives a partial order on sets of covariates which reveals preference for conditioning on outcome-related, rather than exposure-related, confounders. © The Author 2011.},
	number = {11},
	journal = {American Journal of Epidemiology},
	author = {Pearl, Judea},
	month = dec,
	year = {2011},
	note = {Publisher: Oxford Academic},
	keywords = {Confounding factors (epidemiology), Epidemiologic methods, Instrumental variable, Precision, Simulation, Variable selection, amplification},
	pages = {1223--1227},
}

@article{runge_necessary_2021,
	title = {Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables},
	volume = {19},
	issn = {9781713845393},
	url = {https://arxiv.org/abs/2102.10324v3},
	doi = {10.48550/arxiv.2102.10324},
	abstract = {The problem of selecting optimal backdoor adjustment sets to estimate causal
effects in graphical models with hidden and conditioned variables is addressed.
Previous work has defined optimality as achieving the smallest asymptotic
estimation variance and derived an optimal set for the case without hidden
variables. For the case with hidden variables there can be settings where no
optimal set exists and currently only a sufficient graphical optimality
criterion of limited applicability has been derived. In the present work
optimality is characterized as maximizing a certain adjustment information
which allows to derive a necessary and sufficient graphical criterion for the
existence of an optimal adjustment set and a definition and algorithm to
construct it. Further, the optimal set is valid if and only if a valid
adjustment set exists and has higher (or equal) adjustment information than the
Adjust-set proposed in Perkovi\{{\textbackslash}'c\} et al. [Journal of Machine Learning
Research, 18: 1--62, 2018] for any graph. The results translate to minimal
asymptotic estimation variance for a class of estimators whose asymptotic
variance follows a certain information-theoretic relation. Numerical
experiments indicate that the asymptotic results also hold for relatively small
sample sizes and that the optimal adjustment set or minimized variants thereof
often yield better variance also beyond that estimator class. Surprisingly,
among the randomly created setups more than 90{\textbackslash}\% fulfill the optimality
conditions indicating that also in many real-world scenarios graphical
optimality may hold. Code is available as part of the python package
{\textbackslash}url\{https://github.com/jakobrunge/tigramite\}.},
	journal = {Advances in Neural Information Processing Systems},
	author = {Runge, Jakob},
	month = feb,
	year = {2021},
	note = {Publisher: Neural information processing systems foundation},
	keywords = {Causal inference ·, Graphical models ·, Information theory},
	pages = {15762--15773},
}

@article{austin_introduction_2011,
	title = {An introduction to propensity score methods for reducing the effects of confounding in observational studies},
	volume = {46},
	doi = {10.1080/00273171.2011.568786},
	abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propen-sity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses. © Taylor \& Francis Group, LLC.},
	number = {3},
	journal = {Multivariate Behavioral Research},
	author = {Austin, Peter C},
	year = {2011},
	pages = {399--424},
}

@article{rotnitzky_efficient_2020,
	title = {Efficient {Adjustment} {Sets} for {Population} {Average} {Causal} {Treatment} {Effect} {Estimation} in {Graphical} {Models}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-1026.html.},
	abstract = {The method of covariate adjustment is often used for estimation of total treatment effects from observational studies. Restricting attention to causal linear models, a recent article (Henckel et al., 2019) derived two novel graphical criteria: one to compare the asymptotic variance of linear regression treatment effect estimators that control for certain distinct adjustment sets and another to identify the optimal adjustment set that yields the least squares estimator with the smallest asymptotic variance. In this paper we show that the same graphical criteria can be used in non-parametric causal graphical models when treatment effects are estimated using non-parametrically adjusted estimators of the inter-ventional means. We also provide a new graphical criterion for determining the optimal adjustment set among the minimal adjustment sets and another novel graphical criterion for comparing time dependent adjustment sets. We show that uniformly optimal time dependent adjustment sets do not always exist. For point interventions, we provide a sound and complete graphical criterion for determining when a non-parametric optimally adjusted estimator of an interventional mean, or of a contrast of interventional means, is semipara-metric efficient under the non-parametric causal graphical model. In addition, when the criterion is not met, we provide a sound algorithm that checks for possible simplifications of the efficient influence function of the parameter. Finally, we find an interesting connection between identification and efficient covariate adjustment estimation. Specifically, we show that if there exists an identifying formula for an interventional mean that depends only on treatment, outcome and mediators, then the non-parametric optimally adjusted estimator can never be globally efficient under the causal graphical model.},
	journal = {Journal of Machine Learning Research},
	author = {Rotnitzky, Andrea and Smucler, Ezequiel},
	year = {2020},
	keywords = {Bayesian networks, adjustment sets, back-door formula, causal inference, semiparametric inference},
	pages = {1--86},
}

@article{pearl_class_2012,
	title = {On a {Class} of {Bias}-{Amplifying} {Variables} that {Endanger} {Effect} {Estimates}},
	issn = {9780974903965},
	url = {https://arxiv.org/abs/1203.3503v1},
	doi = {10.48550/arxiv.1203.3503},
	abstract = {This note deals with a class of variables that, if conditioned on, tends to
amplify confounding bias in the analysis of causal effects. This class,
independently discovered by Bhattacharya and Vogt (2007) and Wooldridge (2009),
includes instrumental variables and variables that have greater influence on
treatment selection than on the outcome. We offer a simple derivation and an
intuitive explanation of this phenomenon and then extend the analysis to non
linear models. We show that: 1. the bias-amplifying potential of instrumental
variables extends over to non-linear models, though not as sweepingly as in
linear models; 2. in non-linear models, conditioning on instrumental variables
may introduce new bias where none existed before; 3. in both linear and
non-linear models, instrumental variables have no effect on selection-induced
bias.},
	journal = {Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
	author = {Pearl, Judea},
	month = mar,
	year = {2012},
	note = {Publisher: AUAI Press},
	pages = {417--424},
}

@article{vanderweele_new_2011,
	title = {A new criterion for confounder selection},
	volume = {67},
	url = {/pmc/articles/PMC3166439/},
	doi = {10.1111/J.1541-0420.2011.01619.X},
	abstract = {We propose a new criterion for confounder selection when the underlying causal structure is unknown and only limited knowledge is available. We assume all covariates being considered are pretreatment variables and that for each covariate it is known (i) whether the covariate is a cause of treatment, and (ii) whether the covariate is a cause of the outcome. The causal relationships the covariates have with one another is assumed unknown. We propose that control be made for any covariate that is either a cause of treatment or of the outcome or both. We show that irrespective of the actual underlying causal structure, if any subset of the observed covariates suffices to control for confounding then the set of covariates chosen by our criterion will also suffice. We show that other, commonly used, criteria for confounding control do not have this property. We use formal theory concerning causal diagrams to prove our result but the application of the result does not rely on familiarity with causal diagrams. An investigator simply need ask, "Is the covariate a cause of the treatment?" and "Is the covariate a cause of the outcome?" If the answer to either question is "yes" then the covariate is included for confounder control. We discuss some additional covariate selection results that preserve unconfoundedness and that may be of interest when used with our criterion. © 2011, The International Biometric Society.},
	number = {4},
	journal = {Biometrics},
	author = {Vanderweele, Tyler J. and Shpitser, Ilya},
	month = dec,
	year = {2011},
	note = {Publisher: NIH Public Access},
	keywords = {Causal inference, Confounding, Covariate selection, Directed acyclic graphs},
	pages = {1406--1406},
}

@article{mehta_independence_nodate,
	title = {Independence {Testing} for {Multivariate} {Time} {Series}},
	abstract = {Complex data structures such as time series are increasingly present in modern data science problems. A fundamental question is whether two such time-series are statistically dependent. Many current approaches make parametric assumptions on the random processes, only detect linear association, require multiple tests, or forfeit power in high-dimensional, nonlinear settings. Estimating the distribution of any test statistic under the null is non-trivial, as the permutation test is invalid. This work juxtaposes distance correlation (Dcorr) and multiscale graph correlation (MGC) from independence testing literature and block permutation from time series analysis to address these challenges. The proposed nonparametric procedure is valid and consistent, building upon prior work by characterizing the geometry of the relationship, estimating the time lag at which dependence is maximized, avoiding the need for multiple testing, and exhibiting superior power in high-dimensional, low sample size, nonlinear settings. Neural connectivity is analyzed via fMRI data, revealing linear dependence of signals within the visual network and default mode network, and nonlinear relationships in other networks. This work uncovers a first-resort data analysis tool with open-source code available, directly impacting a wide range of scientific disciplines.},
	author = {Mehta, Ronak and Chung, Jaewon and Shen, Cencheng and Xu, Ting and Vogelstein, Joshua T},
	keywords = {Block Permutation Testing, Distance Correlation, Distance Covariance, Independence Testing, Time Series},
}

@article{jaber_causal_nodate,
	title = {Causal {Discovery} from {Soft} {Interventions} with {Unknown} {Targets}: {Characterization} and {Learning}},
	abstract = {One fundamental problem in the empirical sciences is of reconstructing the causal structure that underlies a phenomenon of interest through observation and experimentation. While there exists a plethora of methods capable of learning the equivalence class of causal structures that are compatible with observations, it is less well-understood how to systematically combine observations and experiments to reconstruct the underlying structure. In this paper, we investigate the task of structural learning in non-Markovian systems (i.e., when latent variables a↵ect more than one observable) from a combination of observational and soft experimental data when the interventional targets are unknown. Using causal invariances found across the collection of observational and interventional distributions (not only conditional independences), we define a property called-Markov that connects these distributions to a pair consisting of (1) a causal graph D and (2) a set of interventional targets I. Building on this property, our main contributions are twofold: First, we provide a graphical characterization that allows one to test whether two causal graphs with possibly di↵erent sets of interventional targets belong to the same-Markov equivalence class. Second, we develop an algorithm capable of harnessing the collection of data to learn the corresponding equivalence class. We then prove that this algorithm is sound and complete, in the sense that it is the most informative in the sample limit, i.e., it discovers as many tails and arrowheads as can be oriented within a-Markov equivalence class.},
	author = {Jaber, Amin and Kocaoglu, Murat and Shanmugam, Karthikeyan and Bareinboim, Elias},
}

@article{entner_data-driven_2013,
	title = {Data-driven covariate selection for nonparametric estimation of causal effects},
	abstract = {The estimation of causal effects from non-experimental data is a fundamental problem in many fields of science. One of the main obstacles concerns confounding by observed or latent covariates, an issue which is typically tackled by adjusting for some set of observed covariates. In this contribution, we analyze the problem of inferring whether a given variable has a causal effect on another and, if it does, inferring an adjustment set of covari-ates that yields a consistent and unbiased estimator of this effect, based on the (con-ditional) independence and dependence relationships among the observed variables. We provide two elementary rules that we show to be both sound and complete for this task, and compare the performance of a straightforward application of these rules with standard alternative procedures for selecting adjustment sets.},
	journal = {Proceedings of Machine Learning Research},
	author = {Entner, Doris and Hoyer, Patrik O and Spirtes, Peter},
	year = {2013},
}

@article{colombo_learning_2011,
	title = {Learning high-dimensional directed acyclic graphs with latent and selection variables},
	volume = {40},
	url = {http://arxiv.org/abs/1104.5617},
	doi = {10.1214/11-AOS940},
	abstract = {We consider the problem of learning causal information between random variables in directed acyclic graphs (DAGs) when allowing arbitrarily many latent and selection variables. The FCI (Fast Causal Inference) algorithm has been explicitly designed to infer conditional independence and causal information in such settings. However, FCI is computationally infeasible for large graphs. We therefore propose the new RFCI algorithm, which is much faster than FCI. In some situations the output of RFCI is slightly less informative, in particular with respect to conditional independence information. However, we prove that any causal information in the output of RFCI is correct in the asymptotic limit. We also define a class of graphs on which the outputs of FCI and RFCI are identical. We prove consistency of FCI and RFCI in sparse high-dimensional settings, and demonstrate in simulations that the estimation performances of the algorithms are very similar. All software is implemented in the R-package pcalg.},
	number = {1},
	journal = {Annals of Statistics},
	author = {Colombo, Diego and Maathuis, Marloes H. and Kalisch, Markus and Richardson, Thomas S.},
	month = apr,
	year = {2011},
	keywords = {Causal structure learning, Consistency, FCI algorithm, High-dimensionality, Maximal ancestral graphs (MAGs), Partial ancestral graphs (PAGs), RFCI algorithm, Sparsity, high-dimensionality, maximal ancestral graphs (MAGs), partial ancestral graphs (PAGs), sparsity},
	pages = {294--321},
}

@article{gerhardus_high-recall_nodate,
	title = {High-recall causal discovery for autocorrelated time series with latent confounders},
	abstract = {We present a new method for linear and nonlinear, lagged and contemporaneous constraint-based causal discovery from observational time series in the presence of latent confounders. We show that existing causal discovery methods such as FCI and variants suffer from low recall in the autocorrelated time series case and identify low effect size of conditional independence tests as the main reason. Information-theoretical arguments show that effect size can often be increased if causal parents are included in the conditioning sets. To identify parents early on, we suggest an iterative procedure that utilizes novel orientation rules to determine ancestral relationships already during the edge removal phase. We prove that the method is order-independent, and sound and complete in the oracle case. Extensive simulation studies for different numbers of variables, time lags, sample sizes, and further cases demonstrate that our method indeed achieves much higher recall than existing methods for the case of autocorrelated continuous variables while keeping false positives at the desired level. This performance gain grows with stronger autocorrelation. At github.com/jakobrunge/tigramite we provide Python code for all methods involved in the simulation studies.},
	author = {Gerhardus, Andreas and Runge, Jakob},
}

@article{zhang_completeness_2008,
	title = {On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias},
	volume = {172},
	doi = {10.1016/J.ARTINT.2008.08.001},
	abstract = {Causal discovery becomes especially challenging when the possibility of latent confounding and/or selection bias is not assumed away. For this task, ancestral graph models are particularly useful in that they can represent the presence of latent confounding and selection effect, without explicitly invoking unobserved variables. Based on the machinery of ancestral graphs, there is a provably sound causal discovery algorithm, known as the FCI algorithm, that allows the possibility of latent confounders and selection bias. However, the orientation rules used in the algorithm are not complete. In this paper, we provide additional orientation rules, augmented by which the FCI algorithm is shown to be complete, in the sense that it can, under standard assumptions, discover all aspects of the causal structure that are uniquely determined by facts of probabilistic dependence and independence. The result is useful for developing any causal discovery and reasoning system based on ancestral graph models. © 2008 Elsevier B.V. All rights reserved.},
	number = {16-17},
	journal = {Artificial Intelligence},
	author = {Zhang, Jiji},
	month = nov,
	year = {2008},
	note = {Publisher: Elsevier},
	keywords = {Ancestral graphs, Automated causal discovery, Bayesian networks, Causal models, Latent variables, Markov equivalence},
	pages = {1873--1896},
}

@article{van_der_zander_constructing_nodate,
	title = {Constructing {Separators} and {Adjustment} {Sets} in {Ancestral} {Graphs}},
	abstract = {Ancestral graphs (AGs) are graphical causal models that can represent uncertainty about the presence of latent confounders, and can be inferred from data. Here, we present an algo-rithmic framework for efficiently testing, constructing , and enumerating m-separators in AGs. Moreover, we present a new constructive criterion for covariate adjustment in directed acyclic graphs (DAGs) and maximal ancestral graphs (MAGs) that characterizes adjustment sets as m-separators in a subgraph. Jointly, these results allow to find all adjustment sets that can identify a desired causal effect with multivariate exposures and outcomes in the presence of latent confounding. Our results generalize and improve upon several existing solutions for special cases of these problems.},
	author = {Van Der Zander, Benito and Li´skiewiczli´skiewicz, Maciej and Textor, Johannes},
}

@article{kaiser_unsuitability_2022,
	title = {Unsuitability of {NOTEARS} for {Causal} {Graph} {Discovery} when {Dealing} with {Dimensional} {Quantities}},
	volume = {54},
	url = {https://link.springer.com/article/10.1007/s11063-021-10694-5},
	doi = {10.1007/S11063-021-10694-5/FIGURES/4},
	abstract = {Causal discovery methods aim to identify a DAG structure that represents causal relationships from observational data. In this article, we stress that it is important to test such methods for robustness in practical settings. As our main example, we analyze the NOTEARS method, for which we demonstrate a lack of scale-invariance. We show that NOTEARS is a method that aims to identify a parsimonious DAG from the data that explains the residual variance. We conclude that NOTEARS is not suitable for identifying truly causal relationships from the data for dimensional quantities.},
	number = {3},
	journal = {Neural Processing Letters},
	author = {Kaiser, Marcus and Sipos, Maksim},
	month = jun,
	year = {2022},
	note = {Publisher: Springer},
	keywords = {Bayesian networks, Causal discovery, DAGs, Structure learning},
	pages = {1587--1595},
}

@article{alba_discrimination_2017,
	title = {Discrimination and calibration of clinical prediction models: {Users}’ guides to the medical literature},
	volume = {318},
	url = {https://jamanetwork.com/},
	doi = {10.1001/jama.2017.12126},
	abstract = {Accurate information regarding prognosis is fundamental to optimal clinical care. The best approach to assess patient prognosis relies on prediction models that simultaneously consider a number of prognostic factors and provide an estimate of patients’ absolute risk of an event. Such prediction models should be characterized by adequately discriminating between patients who will have an event and those who will not and by adequate calibration ensuring accurate prediction of absolute risk. This Users’ Guide will help clinicians understand the available metrics for assessing discrimination, calibration, and the relative performance of different prediction models. This article complements existing Users’ Guides that address the development and validation of prediction models. Together, these guides will help clinicians to make optimal use of existing prediction models.},
	number = {14},
	journal = {JAMA - Journal of the American Medical Association},
	author = {Alba, Ana Carolina and Agoritsas, Thomas and Walsh, Michael and Hanna, Steven and Iorio, Alfonso and Devereaux, P. J. and McGinn, Thomas and Guyatt, Gordon},
	month = oct,
	year = {2017},
	note = {Publisher: American Medical Association},
	keywords = {The JAMA Network},
	pages = {1377--1384},
}

@article{tikka_clustering_2021,
	title = {Clustering and {Structural} {Robustness} in {Causal} {Diagrams}},
	abstract = {Graphs are commonly used to represent and visualize causal relations. For a small number of variables, this approach provides a succinct and clear view of the scenario at hand. As the number of variables under study increases, the graphi-cal approach may become impractical, and the clarity of the representation is lost. Clustering of variables is a natural way to reduce the size of the causal diagram but it may erroneously change the essential properties of the causal relations if implemented arbitrarily. We define a specific type of cluster, called transit cluster, that is guaranteed to preserve the identifiability properties of causal effects under certain conditions. We provide a sound and complete algorithm for finding all transit clusters in a given graph and demonstrate how clustering can simplify the identification of causal effects. We also study the inverse problem, where one starts with a clustered graph and looks for extended graphs where the identifiability properties of causal effects remain unchanged. We show that this kind of structural robustness is closely related to transit clusters.},
	author = {Tikka, Santtu and Helske, Jouni and Karvanen, Juha},
	year = {2021},
}

@article{malinsky_causal_nodate,
	title = {Causal {Structure} {Learning} from {Time} {Series} {Causal} {Structure} {Learning} from {Multivariate} {Time} {Series} in {Settings} with {Unmeasured} {Confounding}},
	abstract = {We present constraint-based and (hybrid) score-based algorithms for causal structure learning that estimate dynamic graphical models from multivariate time series data. In contrast to previous work, our methods allow for both "contemporaneous" causal relations and arbitrary unmeasured ("latent") processes influencing observed variables. The performance of our algorithms is investigated with simulation experiments and we briefly illustrate the proposed approach on some real data from international political economy.},
	author = {Malinsky, Daniel and Spirtes, Peter and Thuc, Editor : and Le, Duy and Zhang, Kun and Kıcıman, Emre and Hyvärinen, Aapo and Liu, Lin},
	keywords = {Causal Discovery, Graphical Models, Structure Learning, Time Series},
}

@inproceedings{subbaswamy_counterfactual_2018,
	title = {Counterfactual normalization: {Proactively} addressing dataset shift using causal mechanisms},
	volume = {2},
	isbn = {978-1-5108-7160-1},
	url = {http://arxiv.org/abs/1808.03253},
	abstract = {Predictive models can fail to generalize from training to deployment environments because of dataset shift, posing a threat to model reliability in practice. As opposed to previous methods which use samples from the target distribution to reactively correct dataset shift, we propose using graphical knowledge of the causal mechanisms relating variables in a prediction problem to proactively remove variables that participate in spurious associations with the prediction target, allowing models to generalize across datasets. To accomplish this, we augment the causal graph with latent counter-factual variables that account for the underlying causal mechanisms, and show how we can estimate these variables. In our experiments we demonstrate that models using good estimates of the latent variables instead of the observed variables transfer better from training to target domains with minimal accuracy loss in the training domain.},
	author = {Subbaswamy, Adarsh and Saria, Suchi},
	month = aug,
	year = {2018},
	pages = {947--957},
}

@article{pearl_theoretical_2018,
	title = {Theoretical {Impediments} to {Machine} {Learning} {With} {Seven} {Sparks} from the {Causal} {Revolution}},
	issn = {9781450355810},
	url = {http://arxiv.org/abs/1801.04016},
	doi = {10.1145/3159652.3176182},
	abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or model-free mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference tasks. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal modeling.},
	author = {Pearl, Judea},
	year = {2018},
	pages = {1--8},
}

@inproceedings{pearl_confounding_2010,
	title = {Confounding equivalence in causal inference},
	isbn = {978-0-9749039-6-5},
	doi = {10.1515/jci-2013-0020},
	abstract = {The paper provides a simple test for deciding, from a given causal diagram, whether two sets of variables have the same bias-reducing potential under adjustment. The test requires that one of the following two conditions holds: either (1) both sets are admissible (i.e., satisfy the back-door criterion) or (2) the Markov boundaries surrounding the manipulated variable(s) are identical in both sets. Applications to covariate selection and model testing are discussed.},
	author = {Pearl, Judea and Paz, Azaria},
	year = {2010},
	pages = {433--441},
}

@article{skocik_i_2016,
	title = {I {Tried} a {Bunch} of {Things}: {The} {Dangers} of {Unexpected} {Overfitting} in {Classification}},
	url = {https://www.biorxiv.org/content/10.1101/078816v1.full},
	doi = {10.1101/078816},
	abstract = {Machine learning is a powerful set of techniques that has enhanced the abilities of neuroscientists to interpret information collected through EEG, fMRI, MEG, and PET data. With these new techniques come new dangers of overfitting that are not well understood by the neuroscience community. In this article, we use Support Vector Machine (SVM) classifiers, and genetic algorithms to demonstrate the ease by which overfitting can occur, despite the use of cross validation. We demonstrate that comparable and non-generalizable results can be obtained on informative and non-informative (i.e. random) data by iteratively modifying hyperparameters in seemingly innocuous ways. We recommend a number of techniques for limiting overfitting, such as lock boxes, blind analyses, and pre-registrations. These techniques, although uncommon in neuroscience applications, are common in many other fields that use machine learning, including computer science and physics. Adopting similar safeguards is critical for ensuring the robustness of machine-learning techniques.},
	journal = {bioRxiv},
	author = {Skocik, Michael and Collins, John and Callahan-Flintoft, Chloe and Bowman, Howard and Wyble, Brad},
	month = feb,
	year = {2016},
	note = {Publisher: Cold Spring Harbor Laboratory},
	pages = {078816--078816},
}

@article{bickel_mathematical_1979,
	title = {Mathematical {Statistics}: {Basic} {Ideas} and {Selected} {Topics}.},
	volume = {74},
	url = {https://www.pearson.com/us/higher-education/product/Bickel-Mathematical-Statistics-Basic-Ideas-and-Selected-Topics-Vol-I-2nd-Edition/9780138503635.html},
	doi = {10.2307/2286373},
	abstract = {This classic, time-honored introduction to the theory and practice of statistics modeling and inference reflects the changing focus of contemporary Statistics. Coverage begins with the more general nonparametric point of view and then looks at parametric models as submodels of the nonparametric ones which can be described smoothly by Euclidean parameters. Although some computational issues are discussed, this is very much a book on theory. It relates theory to conceptual and technical issues encountered in practice, viewing theory as suggestive for practice, not prescriptive. It shows readers how assumptions which lead to neat theory may be unrealistic in practice. Statistical Models, Goals, and Performance Criteria. Methods of Estimation. Measures of Performance, Notions of Optimality, and Construction of Optimal Procedures in Simple Situations. Testing Statistical Hypotheses: Basic Theory. Asymptotic Approximations. Multiparameter Estimation, Testing and Confidence Regions. A Review of Basic Probability Theory. More Advanced Topics in Analysis and Probability. Matrix Algebra. For anyone interested in mathematical statistics working in statistics, bio-statistics, economics, computer science, and mathematics.},
	number = {366},
	journal = {Journal of the American Statistical Association},
	author = {Bickel, Peter J. and Doksum, Kjell A.},
	year = {1979},
	pages = {507--507},
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	issn = {1406.2572},
	url = {http://arxiv.org/abs/1406.2572},
	abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
	author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	year = {2014},
	pages = {1--14},
}

@article{Matsumoto2013,
	title = {Pathological and physiological high-frequency oscillations in focal human epilepsy},
	volume = {110},
	issn = {0022-3077},
	url = {http://jn.physiology.org/cgi/doi/10.1152/jn.00341.2013},
	doi = {10.1152/jn.00341.2013},
	abstract = {High-frequency oscillations (HFO; gamma: 40-100 Hz, ripples: 100-200 Hz, and fast ripples: 250-500 Hz) have been widely studied in health and disease. These phenomena may serve as biomarkers for epileptic brain; however, a means of differentiating between pathological and normal physiological HFO is essential. We categorized task-induced physiological HFO during periods of HFO induced by a visual or motor task by measuring frequency, duration, and spectral amplitude of each event in single trial time-frequency spectra and compared them to pathological HFO similarly measured. Pathological HFO had higher mean spectral amplitude, longer mean duration, and lower mean frequency than physiological-induced HFO. In individual patients, support vector machine analysis correctly classified pathological HFO with sensitivities ranging from 70-98\% and specificities {\textgreater}90\% in all but one patient. In this patient, infrequent high-amplitude HFO were observed in the motor cortex just before movement onset in the motor task. This finding raises the possibility that in epileptic brain physiological-induced gamma can assume higher spectral amplitudes similar to those seen in pathologic HFO. This method if automated and validated could provide a step towards differentiating physiological HFO from pathological HFO and improving localization of epileptogenic brain.},
	number = {8},
	urldate = {2017-06-28},
	journal = {Journal of Neurophysiology},
	author = {Matsumoto, Andrew and Brinkmann, Benjamin H. and Stead, S. Matthew and Matsumoto, Joseph and Kucewicz, Michal T. and Marsh, W. Richard and Meyer, Frederic and Worrell, Gregory and Matthew Stead, S. and Matsumoto, Joseph and Kucewicz, Michal T. and Marsh, W. Richard and Meyer, Frederic and Worrell, Gregory},
	month = oct,
	year = {2013},
	pmid = {23926038},
	keywords = {Epilepsy, Gamma oscillations, High-frequency oscillations},
	pages = {1958--1964},
}

@article{DeFelipe1999,
	title = {Chandelier cells and epilepsy},
	volume = {122},
	issn = {00068950},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/10506085},
	doi = {10.1093/brain/122.10.1807},
	abstract = {The main goal of this article is to review certain aspects of the circuitry of the human cerebral cortex that may be particularly relevant for the development, maintenance or spread of seizures. There are a number of different structural abnormalities that are commonly found in the cortex of epileptic patients, but these abnormalities do not appear to be intrinsically epileptogenic, since some patients displaying them are epileptic (after variable delays) whereas others are not. Therefore, cortical circuits in an affected brain may undergo a series of changes that finally cause epilepsy. In this article, it is proposed that the chandelier cell, which is considered to be the most powerful cortical GABAergic inhibitory interneuron, is probably a key component of cortical circuits in the establishment of human intractable temporal lobe epilepsy. These cells (among other types) have been found to be lost or reduced at epileptic foci in both experimental animals and epileptic patients. A hypothesis is presented by which the normal variability in the number of interneurons might explain the predisposition of some individuals to develop epilepsy more than others as a result of a lesion or other precipitating factors that lead to loss of neurons. The sources of GABAergic input on dendrites and somata of cortical pyramidal cells originate from many and diverse types of interneurons but, at the level of the axon initial segment of these cells, all synapses come from a few chandelier cells (five or less). Loss of one class of interneurons ending on soma and dendrites might have relatively little impact on the inhibitory control of the pyramidal cell. However, if chandelier cells were affected, it would have serious consequences for the inhibitory control of the pyramidal cells. Evidence suggests that the loss of chandelier cells may be nonspecific and that when this occurs epilepsy may develop. Therefore, these cells might represent a key component in the aetiology of human temporal lobe epilepsy.},
	number = {10},
	urldate = {2018-07-16},
	journal = {Brain},
	author = {DeFelipe, Javier},
	month = oct,
	year = {1999},
	pmid = {10506085},
	keywords = {Cortical circuitry, Human, Interneurons, Neuronal loss, Parvalbumin},
	pages = {1807--1822},
}

@article{Ehrens2015a,
	title = {Closed-loop control of a fragile network: {Application} to seizure-like dynamics of an epilepsy model},
	volume = {9},
	issn = {1662453X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/25784851},
	doi = {10.3389/fnins.2015.00058},
	abstract = {It has recently been proposed that the epileptic cortex is fragile in the sense that seizures manifest through small perturbations in the synaptic connections that render the entire cortical network unstable. Closed-loop therapy could therefore entail detecting when the network goes unstable, and then stimulating with an exogenous current to stabilize the network. In this study, a non-linear stochastic model of a neuronal network was used to simulate both seizure and non-seizure activity. In particular, synaptic weights between neurons were chosen such that the network's fixed point is stable during non-seizure periods, and a subset of these connections (the most fragile) were perturbed to make the same fixed point unstable to model seizure events; and, the model randomly transitions between these two modes. The goal of this study was to measure spike train observations from this epileptic network and then apply a feedback controller that (i) detects when the network goes unstable, and then (ii) applies a state-feedback gain control input to the network to stabilize it. The stability detector is based on a 2-state (stable, unstable) hidden Markov model (HMM) of the network, and detects the transition from the stable mode to the unstable mode from using the firing rate of the most fragile node in the network (which is the output of the HMM). When the unstable mode is detected, a state-feedback gain is applied to generate a control input to the fragile node bringing the network back to the stable mode. Finally, when the network is detected as stable again, the feedback control input is switched off. High performance was achieved for the stability detector, and feedback control suppressed seizures within 2 s after onset.},
	number = {MAR},
	urldate = {2018-11-29},
	journal = {Frontiers in Neuroscience},
	author = {Ehrens, Daniel and Sritharan, Duluxan and Sarma, Sridevi V.},
	month = mar,
	year = {2015},
	pmid = {25784851},
	note = {Publisher: Frontiers Media SA},
	keywords = {Closed-loop stimulation, Epilepsy model, Hidden markov models, Network fragility, Neuronal network model, closed-loop stimulation, epilepsy model, hidden Markov models, network fragility, neuronal network model},
	pages = {58},
}

@article{MARGERISON1966,
	title = {Epilepsy and the temporal lobes: {A} clinical, electroencephalographic and neuropathological study of the brain in epilepsy, with particular reference to the temporal lobes},
	volume = {89},
	issn = {00068950},
	url = {https://academic.oup.com/brain/article-lookup/doi/10.1093/brain/89.3.499},
	doi = {10.1093/brain/89.3.499},
	abstract = {Epilepsy is a common outcome of traumatic brain injury (TBI), but the mechanisms of posttraumatic epileptogenesis are poorly understood. One clue is the occurrence of selective hippocampal cell death after fluid-percussion TBI in rats, consistent with the reported reduction of hippocampal volume bilaterally in humans after TBI and resembling hippocampal sclerosis, a hallmark of temporal-lobe epilepsy. Other features of temporal-lobe epilepsy, such as long-term seizure susceptibility, persistent hyperexcitability in the dentate gyrus (DG), and mossy fiber synaptic reorganization, however, have not been examined after TBI. To determine whether TBI induces these changes, we used a well studied model of TBI by weight drop on somatosensory cortex in adult rats. First, we confirmed an early and selective cell loss in the hilus of the DG and area CA3 of hippocampus, ipsilateral to the impact. Second, we found persistently enhanced susceptibility to pentylenetetrazole-induced convulsions 15 weeks after TBI. Third, by applying GABA(A) antagonists during field-potential and optical recordings in hippocampal slices 3 and 15 weeks after TBI, we unmasked a persistent, abnormal APV-sensitive hyperexcitability that was bilateral and localized to the granule cell and molecular layers of the DG. Finally, using Timm histochemistry, we detected progressive sprouting of mossy fibers into the inner molecular layers of the DG bilaterally 2-27 weeks after TBI. These findings are consistent with the development of posttraumatic epilepsy in an animal model of impact head injury, showing a striking similarity to the enduring behavioral, functional, and structural alterations associated with temporal-lobe epilepsy.},
	number = {3},
	urldate = {2017-06-03},
	journal = {Brain},
	author = {Margerison, J. H. and Corsellis, J. A.N.},
	year = {1966},
	pmid = {5922048},
	note = {Publisher: Oxford University Press
ISBN: 0006-8950},
	keywords = {brain, electroencephalography, epilepsy, temporal lobe},
	pages = {499--530},
}
