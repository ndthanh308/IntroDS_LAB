In this section we provide the following empirical analyses. First, we show that the CMA-ESwM is not efficient for the integer-valued optimization problem we consider. Then we compare the performances of \ooea with the $\pm 1$ operator, \ooea with the heavy-tailed operator and the RLS with the self-adjusting operator. 

Note that although the CMA-ESwM is designed to optimize mixed integer valued problems, we restrict ourselves to all-integer inputs. This allows us to make comparisons with the \ooea and RLS with different mutation operators. We use the code from GitHub\footnote{\url{https://github.com/EvoConJP/CMA-ES_with_Margin}} provided by the authors of the paper where the CMA-ESwM is proposed \cite{CMAESwMargin}.

% \Acomment{Sherry, "therefore we let the continuous part of the algorithm be $0$ so that we can compare run time of different algorithms used for discrete optimization problems", we removed this statement because we are not getting into the details of the implementation of the algorithm and "let the continuous part of the algorithm be $0$" is not very clear. Is this okay?} 

All our theoretical analyses are concerned with the unbounded integer search space, whereas, for practical reasons, we have to restrict our search space to be bounded. We bound the maximum value of the considered integer strings by a value $r$.

% We are interested in the run time of finding the optimum of four algorithms, namely \ooea with $\pm 1$ operator,\ooea with the heavy-tailed operator,  \rlswself with the self-adjusting operator and CMA-ESwM in the sense of different sizes of search space $r$ experimentally. \Scomment{Aishwarya point out for discrete algorithms there aren't any bounds so she points out need to mention we need a bound for discrete algorithm. I think 'in the sense of different size of search space $r$ experimentally' give an idea of r.}

We set the optimum as the all-$r$ integer string and let the algorithm run until it finds the optimum and record the run time (number of function evaluations). For \ooea with different mutation operators and \rlswself, we start with the all-$0$ integer string. As for CMA-ESwM, we consider the same set up proposed in \cite{CMAESwMargin}. 

We choose the step size of $r$ as follows: 
% \setlist{nolistsep}
\begin{enumerate}
    \item \text{\ooea} with $\pm 1$ operator: $r$ from 10 till 150 with a step size of 10. Then we consider $r \in \{ 10^{3}, 10^{4}, 10^{5} \}$ to analyze the run time of the algorithm for exponentially increasing values of~$r$. 
    \item  \rlswself with the self-adjusting operator and  \ooea with the heavy-tailed operator: we consider $r = 10^{k}$, for $k \in \{1, \ldots, 12\}$.
    % $r$ from $10$ till $10^{12}$ with an exponential increase step size, which means 
    For RLS with the self-adjusting operator we set the parameter $\alpha = 2.0$ and $\beta = 0.5$ and for the \ooea with the heavy-tailed operator we set $\epsilon = 0.001$.
    \item CMA-ESwM: we consider the same values of $r$ as for the RLS since there are no specific restrictions mentioned in~\cite{CMAESwMargin}. 
\end{enumerate}


 

\subsection{Success rate}
During initial exploration we noticed that the CMA-ESwM does not always find the optimum before it reaches the termination condition (minimum eigenvalue of the covariance matrix in the update step of the CMA-ESwM algorithm is less than $10^{-30}$). To further analyze this, we consider the following different values of $r$, $\{10, 100, 1000\}$ and different $n$ (length of the integer string) from $10$ to $100$ in steps of $10$. In Figure~\ref{fig:failedRateCMAESwM} we see the proportion of failed runs over total of $100$ runs named failure rate. Note that for $n < 40$ the failure rate is zero so in Figure \ref{fig:failedRateCMAESwM}, $n \geq 40$. 

% Figure environment removed

We can see from Figure ~\ref{fig:failedRateCMAESwM} that the
failure rate increases as $n$ increases. Also, the failure rate is significant despite $r$ being quite small. This shows the limitations of CMA-ESwM in optimizing a complete integer-valued problem.


\subsection{Comparison between different algorithms}
We present experimental results of the \ooea with the $\pm 1$ operator and the heavy-tailed operator in Figure~\ref{fig:allCom}. 
All results are averaged over $20$ independent runs. We attach the results of the statistical test for $n = 100$ in the appendix. % and presented in plots generated using the Matplotlib library \cite{matplotlib}.

% % Figure environment removed



% % Figure environment removed

% In Figure~\ref{fig:heavyTailSelfAdjustCom} we notice for $r$ up to $10^4$, the run time of the \rlswself with \emph{heavy-tailed operator} and \ooea with \emph{heavy-tailed operator} does not show an obvious difference. And until $r= 10^9$, \ooea with \emph{heavy-tailed operator} does not show the strength in short run time \Acomment{I think it is not the runtime but small value of r. And also it is the opposite, heavy-tailed operator is supposed to take the more time than RLS and it cannot be seen for small values of r}. However, after $r$ increase significantly large, the \ooea with \emph{heavy-tailed operator} increase \Acomment{What increases? I think it is important to mention that the run time increases}noticeably while \rlswself with \emph{heavy-tailed operator} \Acomment{We don't use RLS with heavy-tailed operator} maintains a steady increases as $r$. 

% Figure environment removed

% For both x-axis and y-axis, we apply $log_{10}$ for $r$ and $time$ normalized by individual $n$ for a better view

In Figure~\ref{fig:allCom} we can see that the scaling behavior with respect to $r$ is independent of the value of $n$.

Asymptotically, the results are as suggested by the theoretical results given in the prior sections. However, for small values of $r$, the scaling behavior is not yet the deciding factor. In particular, the $\pm 1$ operator is competitive as long as the optimum is not much more than $r = 10^2 = 100$ away in each component. For higher values of $r$, the constantly small step size is very much detrimental to efficient search.

An interesting finding is that the heavy-tailed operator can outperform the self-adjusting RLS, in spite of what the asymptotic bounds given in this paper suggest. For small values of $r$, a lot of time is wasted on attempting larger jumps, but for middle-ranged $r$ these jumps start to pay off. In contrast, the self-adaptive RLS needs a warm-up phase adjusting its velocity value, meanwhile the heavy-tailed operator can make progress starting in the first iteration.

% % As for comparing the run time for \ooea with heavy-tailed operator to \rlswself to self-adjusting operator, when $r$ is in the middle range, experimental result are different from \ref{thm:heavy_tailed_operator} and \ref{thm:self_adjusting}, which asymptotically \rlswself with self-adjusting operator takes shorter time to find the optimum. 





% From Figure~\ref{fig:simpleOperatorRuntimeOverR} we see a linear increase of run time as the linear increase of $r$ value, which as \ref{thm:better_lowerbound_pm_operator} and \ref{thm:better_upperbound_pm_operator} shows.