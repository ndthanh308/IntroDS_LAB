%% bare_adv.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See: 
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the advanced use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE Computer
%% Society journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


% IEEEtran V1.7 and later provides for these CLASSINPUT macros to allow the
% user to reprogram some IEEEtran.cls defaults if needed. These settings
% override the internal defaults of IEEEtran.cls regardless of which class
% options are used. Do not use these unless you have good reason to do so as
% they can result in nonIEEE compliant documents. User beware. ;)
%
%\newcommand{\CLASSINPUTbaselinestretch}{1.0} % baselinestretch
%\newcommand{\CLASSINPUTinnersidemargin}{1in} % inner side margin
%\newcommand{\CLASSINPUToutersidemargin}{1in} % outer side margin
%\newcommand{\CLASSINPUTtoptextmargin}{1in}   % top text margin
%\newcommand{\CLASSINPUTbottomtextmargin}{1in}% bottom text margin


%
\documentclass[10pt,journal,compsoc]{IEEEtran}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[10pt,journal,compsoc]{../sty/IEEEtran}


% For Computer Society journals, IEEEtran defaults to the use of 
% Palatino/Palladio as is done in IEEE Computer Society journals.
% To go back to Times Roman, you can use this code:
%\renewcommand{\rmdefault}{ptm}\selectfont



\usepackage{arydshln}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
% \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{bm}

\usepackage{xcolor}
% \usepackage{arydshln}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage[outdir=./]{epstopdf}


\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother





% \usepackage{caption}
% \usepackage{subcaption}



% \usepackage{arydshln}





\usepackage{colortbl}

\definecolor{tablegray}{gray}{.9}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\newcommand{\etal}{\textit{et al}.}
\newcommand{\ie}{\textit{i}.\textit{e}.}
\newcommand{\eg}{\textit{e}.\textit{g}.}

\newenvironment{tightcenter}{%
  \setlength\topsep{3pt}
  \setlength\parskip{3pt}
  \begin{center}
  \begin{minipage}{.42\textwidth}
}{%
  \end{minipage}
  \end{center}
}
% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % The IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.





% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%\usepackage{acronym}
% acronym.sty was written by Tobias Oetiker. This package provides tools for
% managing documents with large numbers of acronyms. (You don't *have* to
% use this package - unless you have a lot of acronyms, you may feel that
% such package management of them is bit of an overkill.)
% Do note that the acronym environment (which lists acronyms) will have a
% problem when used under IEEEtran.cls because acronym.sty relies on the
% description list environment - which IEEEtran.cls has customized for
% producing IEEE style lists. A workaround is to declared the longest
% label width via the IEEEtran.cls \IEEEiedlistdecl global control:
%
% \renewcommand{\IEEEiedlistdecl}{\IEEEsetlabelwidth{SONET}}
% \begin{acronym}
%
% \end{acronym}
% \renewcommand{\IEEEiedlistdecl}{\relax}% remember to reset \IEEEiedlistdecl
%
% instead of using the acronym environment's optional argument.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/acronym


%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/pkg/mdwtools


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/pkg/eqparbox




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat[]), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.
% Be aware that for subfig.sty to generate the (a), (b), etc., subfigure
% labels, the optional argument to \subfloat must be present. If a
% subcaption is not desired, just leave its contents blank,
% e.g., \subfloat[].


% An example of a floating table. Note that, for IEEE style tables, the
% \caption command should come BEFORE the table and, given that table
% captions serve much like titles, are usually capitalized except for words
% such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to
% and up, which are usually not capitalized unless they are the first or
% last word of the caption. Table text will default to \footnotesize as
% the IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that the IEEE does not put floats in the very first column
% - or typically anywhere on the first page for that matter. Also,
% in-text middle ("here") positioning is typically not used, but it
% is allowed and encouraged for Computer Society conferences (but
% not Computer Society journals). Most IEEE journals/conferences use
% top floats exclusively. 
% Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the
% \fnbelowfloat command of the stfloats package.

% \IEEEPARstart{I}{n} this section, we introduce the current research status, problems, and challenges of BVQA methods.




\IEEEPARstart{W}{e} are undoubtedly living in an era, where we are exposed to a great volume of video data through various video on demand and live streaming media applications. No matter at what stages along the video delivery chain from video creation to consumption, the perceptual video quality is of central concern \cite{juluri2015measurement}.

Video quality assessment (VQA) plays a fundamental role in a wide range of video processing systems, such as filtering videos of extremely low visual quality from the content producer side, guiding video compression and transmission algorithms to achieve the perceptually optimal rate-distortion trade-off, and ensuring smooth and faithful rendering on various displays of different spatial resolutions, dynamic ranges, and color gamuts. How to perform reliable VQA subjectively~\cite{seshadrinathan2010study, nuutinen2016cvd2014, ghadiyaram2017capture, hosu2017konstanz, sinno2018large, wang2019youtube, chen2019qoe, ying2021patch, yu2022subjective} and objectively~\cite{saad2014blind, mittal2015completely, liu2018end, korhonen2019two, li2019quality, korhonen2020blind, tu2021ugc, ying2021patch, yu2021predicting, yi2021attention, wang2021rich, li2022blindly, sun2022deep, wu2022fast, wu2022disentangling} are two practically demanding problems that have been extensively studied in the interdisciplinary fields of signal and image processing, psychophysics, computational neuroscience, and machine learning.


Subjective VQA involves two key steps: \textit{sample selection} and \textit{subjective testing}, which outputs a video dataset with perceptual quality annotations in the form of mean opinion scores (MOSs). During sample selection, the visual distortion types and levels are largely determined by the video applications of interest. Early subjective VQA~\cite{seshadrinathan2010study, li2019avc, mackin2015study, lee2021subjective} focuses on synthetic visual distortions arising from different video processing stages, including spatiotemporal downsampling, compression, and transmission. Recent subjective VQA~\cite{nuutinen2016cvd2014, ghadiyaram2017capture, hosu2017konstanz, sinno2018large, wang2019youtube, chen2019qoe, ying2021patch, yu2022subjective} shifts the attention to realistic visual distortions arising during video capture (and subsequent realistic video processing over the Internet). To encourage wide content coverage, Vonikakis~\etal~\cite{vonikakis2016shaping} cast sample selection as a mixed integer linear program to enforce specific marginal distributions over different video attributes (\eg, spatial and temporal information). This dataset shaping technique has inspired later researchers to give a careful treatment of sample selection during dataset construction~\cite{hosu2017konstanz,wang2019youtube,ying2021patch}.
With respect to subjective testing methodologies, International Telecommunication Union (ITU) has made several recommendations~\cite{recommendation910subjective,union2016methods,series2012methodology} regarding the experimental environment, the video clip length, the number of subjects, and the rating type.


Objective VQA aims to build computational models to accurately predict the human perception of video quality. When the reference video  is assumed, full-reference VQA models~\cite{wang2004image,moorthy2010efficient,zhang2018unreasonable,ding2020image,li2018vmaf} can be constructed to compute the perceptual quality of a test video by comparing it to the corresponding reference. A more practical yet challenging scenario is to design VQA models~\cite{saad2014blind, korhonen2019two, li2019quality, wu2022fast} with reliance only on the test  video itself, hence the name no-reference VQA or blind VQA (BVQA). In most Internet-based video applications, the reference video is not practically accessible (even if available), which makes BVQA the focus in this paper. Compared to blind image quality assessment (BIQA), BVQA is more difficult due to the added temporal dimension, which complicates the computational modeling of visual distortions possibly localized in space and time. 



As an experimental field, the improvements of BVQA models have been evaluated routinely on a few human-rated VQA datasets. To probe how reliable current progress in BVQA is, we need to gain a better understanding of these VQA datasets. 
Towards this goal, we conduct one of the first computational analyses of existing VQA datasets~\cite{nuutinen2016cvd2014,ghadiyaram2017capture,hosu2017konstanz,sinno2018large,wang2019youtube,chen2019qoe,ying2021patch,yu2022subjective}, with emphasis on identifying the \textit{easy dataset} problem~\cite{cao2022image}. We say that a VQA dataset suffers from the easy dataset problem if it
\begin{itemize}
    \item admits close-to-BIQA solutions,
    \item or poses little challenge to current BVQA models.
\end{itemize}
The first criterion indicates that the temporal distortions (if any) in
 the VQA dataset are dominated by or strongly correlated with the spatial distortions. As a result, a BIQA model that properly penalizes spatial distortions tends to perform well. This goes against the original intent of constructing the VQA dataset.
 The second criterion suggests that the VQA dataset exposes few failure cases of current BVQA models, leading to a significant waste of the human labeling budget that could have been reallocated to more difficult videos. 





Our main computational analysis of VQA datasets examines the first criterion of the easy dataset problem. Specifically, we design a family of minimalistic BVQA models, which consist of four basic building blocks: a video preprocessor, a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor. The video preprocessor is designed to aggressively downsample the input test video in both spatial and temporal dimensions, giving rise to a few key video frames. The spatial quality analyzer acts as a BIQA model to compute a spatial quality representation for each key frame. The temporal quality analyzer is optional, extracting a temporal quality representation from a video chunk centered at each key frame. The quality regressor is responsible for computing and pooling local quality scores from the concatenated spatial and temporal features to obtain a global quality score for the test video. The four building blocks can be instantiated with different computational structures, equipped with different quality-aware initializations, and end-to-end optimized.



 
 We train ten BVQA model variants on eight VQA datasets~\cite{nuutinen2016cvd2014,ghadiyaram2017capture,hosu2017konstanz,sinno2018large,sinno2018large,wang2019youtube,chen2019qoe,ying2021patch,yu2022subjective} by minimizing the Pearson linear correlation coefficient (PLCC) between the MOSs and the model predictions. Provided sufficient optimization, we attribute the performance variations to the implementation differences of the basic building blocks, especially the spatial and temporal quality analyzers. The primary observation from our computational analysis is that nearly all datasets suffer from the easy dataset problem by satisfying the first criterion to varying degrees. 
To further support our claims, we examine the second criterion of the easy dataset problem by testing the generalization of our models trained on the largest VQA dataset~\cite{ying2021patch} to the other seven datasets. We additionally ablate 
a dizzying set of BVQA design choices related to the basic building blocks.

In summary, our contributions include
\begin{itemize}
\item a family of minimalistic BVQA models, serving as strong baselines and laying the foundation for analyzing VQA datasets,
\item a computational analysis of eight VQA datasets, which we are able to empirically rank according to the severity of the easy dataset problem,
\item a proposal to construct more useful VQA datasets from the sample selection and subjective testing perspectives. 
\end{itemize}

\section{Related Work}
\label{review}
In this section, we provide an overview of eight VQA datasets with realistic visual distortions 
to address the practical BVQA problem. We then review and characterize existing BVQA methods, putting our minimalistic BVQA models in proper context.

\begin{table*}
	\centering
	\renewcommand{\arraystretch}{1.25}
	\caption{Summary of eight VQA datasets.  Duration is in seconds}
	\label{overview_vqa_dataset}
 
    % \resizebox{1\textwidth}{!}{
	\begin{tabular}{lcccccccc}
	\toprule[.15em]
% 		\hline
% 		\hline
		\multirow{1}{*}{Dataset} & \multirow{1}{*}{Year} &\multirow{1}{*}{\# of Videos} & \multirow{1}{*}{\# of Scenes} & \multirow{1}{*}{Resolution} & Duration& \multirow{1}{*}{Frame Rate} &  \multirow{1}{*}{Type} & \multirow{1}{*}{Environment} \\
		\hline
  	CVD2014	\cite{nuutinen2016cvd2014} &2014& 234 & 5& 480p, 720p & 10-25& 10-32 &  In-capture &  In-laboratory \\
        LIVE-Qualcomm \cite{ghadiyaram2017capture} &2017& 208 &54 & 1080p & 15&  30&  In-capture & In-laboratory \\
		KoNViD-1k \cite{hosu2017konstanz} &2017& 1,200 & 1,200 & 540p & 8 & 24, 25, 30 &  Internet-collected & Crowdsourcing \\
  	LIVE-VQC \cite{sinno2018large} &2018& 585 & 585 & 240p-1080p & 10& 30 &  In-capture & Crowdsourcing \\
		YouTube-UGC \cite{wang2019youtube} &2019& 1,500 & 1,500 & 360p-4K & 20 & 30 &  Internet-collected & Crowdsourcing \\
		LBVD \cite{chen2019qoe} &2019& 1,013 & 1,013 & 240p-540p & 10 & $<$ 30 &  Internet-collected &  In-laboratory \\
		LSVQ \cite{ying2021patch} &2021& 38,811 & 38,811 & 99p-4K & 5-12 & $<$ 60 &  Internet-collected & Crowdsourcing \\
		LIVE-YT-Gaming \cite{yu2022subjective} &2022& 600& 600& 360p-1080p& 8-9& 30, 60&  Internet-collected & Crowdsourcing \\
    \bottomrule[.15em]
	\end{tabular}
	% }
\end{table*}

\subsection{VQA Datasets}
\label{vqa_dataset}
It is common to divide the videos with realistic distortions in existing VQA datasets into two categories: \textit{in-captured videos}, which are directly captured by various cameras with minimal subsequent processing (\eg, transcoding and transmission) and \textit{Internet-collected videos}, which suffer from a greater mixture of video distortions along the video delivery chain.
\subsubsection{In-Captured VQA Datasets}
\textbf{CVD2014} \cite{nuutinen2016cvd2014} presents the first subjective quality study of in-captured videos, containing $234$ videos recorded by $78$ cameras with varying compression formats, spatial resolutions, and frame rates. In addition to MOSs, detailed video attributes such as sharpness, saturation, color pleasantness, lightness, motion fluency, and sound quality were also rated. Nevertheless, CVD2014 only includes five hand-picked scenes captured by nearly still cameras, making it little representative of 
hard natural videos encountered in the real world. 





To increase the scene diversity, \textbf{LIVE-Qualcomm} \cite{ghadiyaram2017capture} was created to comprise $208$ videos using $8$ different smartphones under $54$ unique scenes\footnote{Compared to CVD2014, LIVE-Qualcomm decreases the number of captured devices while increases the number of scenes.}. The dataset is further divided into six subsets, each corresponding to one dominant distortion type, characterized by spatial artifacts (\eg, noise and blockiness), color, exposure, autofocus, sharpness, and stabilization.  
The subjective experiments of CVD2014 and LIVE-Qualcomm were both conducted under well-controlled laboratory environments, which limits the total number of subjects to be recruited and videos to be rated. 




\textbf{LIVE-VQC} \cite{sinno2018large} consists of $585$ in-capture videos by $80$ mobile cameras, each  corresponding to a unique scene. LIVE-VQC spans a wide range of natural scenes under different lighting conditions as well as diverse levels of motion. There are $18$ video resolutions, ranging from 240p to 1080p.  The subjective user study was conducted on the Amazon Mechanical Turk, in which $4,776$ subjects took part. Compared with a standard laboratory environment in which the display has a fixed and high resolution, the viewing conditions (such as the effective viewing distance and the ambient luminance) of crowdsourcing platforms vary greatly. For example, the test video, regardless of its native resolution, is adapted to the resolution of the screen in the crowdsourcing experiment (which is typically larger than $1,280\times720$). As a consequence, the collected MOSs may not account for the influence of spatial resolutions.


\subsubsection{Internet Collected VQA Datasets}
\label{internet_collected_video}

\textbf{KoNViD-1k} \cite{hosu2017konstanz} is the first dataset to study the visual quality of Internet videos. A density-based fair sampling strategy was used to sample $1,200$ videos with the resolution of $960\times 540$ from YFCC100M~\cite{thomee2016yfcc100m} based on six attributes: blur, colorfulness, contrast, spatial information, temporal information, and predicted video quality by V-NIQE~\cite{mittal2012making}. 
The MOSs were also crowdsourced. Since the videos originated from YFCC100M, a Flickr dataset containing videos uploaded from 2004 to 2014, most videos in KoNViD-1k are dominated by spatial distortions like noise, blur, low visibility, and compression artifacts. 


To enrich video content, \textbf{YouTube-UGC} \cite{wang2019youtube} selects an initial set of $1.5$ million videos belonging to $15$ overlapping categories\footnote{These are Animation, Cover Song, Gaming, HDR, How-To, Lecture, Live Music, Lyric Video, Music Video, News Clip, Sports, Television Clip, Vertical Video, Vlog, and VR.} from YouTube with resolutions spanning from  360p to 4K. Similar to KoNViD-1k, YouTube-UGC relies on a (quantized) density-based sampling strategy to select $1,500$ videos\footnote{Up to now, only 1,228 videos are publicly available.} with respect to spatial, color, temporal, and chunk variation attributes (measured by compression bitrates). Compared with other datasets, a portion of  YouTube-UGC videos are of higher quality. Meanwhile, the time duration of each video is of 20-second long, which often contains scene changes.

\textbf{LBVD} \cite{chen2019qoe} gathers  $1,013$ live broadcasting videos based on HTTP adaptive streaming. The main sources of distortions include unstable network conditions (\eg, rebuffering and stalling), camera shake (\eg, jittering and motion blur), and drastic dynamic range changes. The videos are mostly human-centric under different lighting conditions. The predominant resolutions are 240p, 360p, and 540p, which are significantly lower compared to other datasets. The subjective experiment was conducted in a well-controlled laboratory environment.




Till then, the number of included videos is rather limited, which is in the order of hundreds. To overcome this shortcoming, \textbf{LSVQ} \cite{ying2021patch} was constructed with $38,811$ videos sampled from the Internet Archive and YFCC100M by matching six video feature distributions\footnote{These are luminance, colorfulness, luminance contrast, number of detected faces, spatial Gaussian derivative filter responses, and temporal Gaussian derivatives.} \cite{vonikakis2016shaping}. LSVQ covers a broad range of resolutions and frame rates. Because of the large number of videos (including three variants of video patches) that need to be rated, the subjective experiment was practically held online. LSVQ enables training BVQA models for local video quality prediction, but it is not the focus of the current paper, and is thus not investigated.  


\textbf{LIVE-YT-Gaming} \cite{yu2022subjective} is a VQA dataset targeting streamed gaming videos.  A total of $600$ gaming videos belonging to $59$ games are obtained either from the Internet Archive or by direct recording using professional software. Since gaming videos are synthetically rendered, the distortions pertain to 
the graphic configuration settings of the operating system, the settings built in each game, and the grade of the recording software. Hence, both the content statistics, as well as the distortion appearances, are significantly different from those of natural videos captured by optical cameras. For example, gaming videos are generally composed of sharper edges with minimal noise, but may exhibit loss of details in grass and water regions, unnaturalness of shadows, and aliasing. 

Table~\ref{overview_vqa_dataset} gives a summary of the eight VQA datasets. From the \textit{sample selection} perspective, many prefer to incorporate diverse videos with respect to several video attributes. Nevertheless, diversity is just one piece of the story; the included videos shall also be \textit{difficult} to challenge BVQA model design, which is unfortunately overlooked during sample selection, causing the easy dataset problem. From the \textit{subjective testing} perspective, crowdsourcing has been proved to collect reliable MOSs provided that the psychophysical experimental procedure is designed appropriately. And thus we assume the reliability of MOSs in all datasets without discussing the label noise and uncertainty in this paper.









\begin{table*}
	\centering
	\renewcommand{\arraystretch}{1.25}
	\caption{Categorization of DNN-based BVQA models based on the four fundamental blocks and the optimization strategies. ``None'' in Video Preprocessor column means that no such preprocessing, while ``---'' indicates no such building block is incorporated into the corresponding BVQA model. PaQ-2-PiQ~\cite{ying2020patches} and UNIQUE~\cite{zhang2021uncertainty} are two BIQA models that pre-trained ResNet-18~\cite{he2016deep} and ResNet-50~\cite{he2016deep} on the LIVE-FB~\cite{ying2020patches} dataset and four merged IQA datasets, respectively. EfficientNet-B0~\cite{tan2019efficientnet} in CoINVQ was pre-trained on tasks involving video classification and distortion type classification, whereas D3D~\cite{stroud2020d3d} in CoINVQ was pre-trained for video compression classification. ConvNeXt-T~\cite{liu2022convnet} in DOVER was initially pre-trained for image aesthetic assessment with the cross-scale feature consistency constraint (not shown in the Loss Function Column due to the space limit). 
 FAST-VQA and DOVER sample a fixed number of frames (\ie, 32) regardless of the video duration in the VQA datasets, and we convert it to frames per second (fps)}
	\label{tab:bvqa_models}

    \resizebox{1\textwidth}{!}{
	\begin{tabular}{lccccccc}
	\toprule[.15em]
% 		\hline
% 		\hline
		\multirow{2}{*}{Model} & \multicolumn{2}{c}{Video Preprocessor} & Spatial Quality & Temporal Quality & Quality & \multirow{2}{*}{Optimization Strategy}& \multirow{2}{*}{Loss Function} \\
		 & Spatial Branch & Temporal Branch & Analyzer (SA)  & Analyzer (TA) &  Regressor (QR) & &  \\
		\hline
  	VSFA \cite{li2019quality} &  None & --- & ResNet-50 & --- & GRU & Fix SA, Train QR &  L1 \\
    Yi21 \cite{yi2021attention} &  $448 \times 448$, $1$ fps& ---& VGG16 & --- & GRU & End-to-End Training &  L1 \\
    PatchVQ \cite{ying2021patch} &  None &None& PaQ-2-PiQ &  3D ResNet-18 & InceptionTime & Fix SA\&TA, Train QR &  L1 \\
    CoINVQ \cite{wang2021rich} & $360\times 640$, $1$ fps & $180\times320$, $5$ fps& EfficientNet-B0 &  D3D & MLP & Fix SA\&TA, Train QR &  L1 \\
    Li22 \cite{li2022blindly} &  None &None& UNIQUE &  SlowFast & GRU & Fix SA\&TA, Train QR &  PLCC\&SRCC  \\
  	SimpleVQA \cite{sun2022deep} &  $448\times448$, $1$ fps &$224\times224$, None& ResNet-50 & SlowFast & MLP & Fix TA, Train SA\&QR &  L1\&Ranking \\
  	FAST-VQA \cite{wu2022fast} &  ---& $224\times224$, $1.6$-$6.4$ fps& --- & Video Swin-T & MLP & End-to-End Training &  PLCC\&Ranking  \\
  	DOVER \cite{wu2022disentangling} &  $224\times224$, $1.6$-$6.4$ fps& $224\times224$, $1.6$-$6.4$ fps& ConvNeXt-T & Video Swin-T & MLP & End-to-End Training &  PLCC\&Ranking  \\
   \hline
  	Ours (Model (\rom{1})) &  $448\times448$, $1$ fps &$224\times224$, $16$ fps& ResNet-50 & SlowFast & MLP & Fix TA, Train SA\&QR &  PLCC \\
    \bottomrule[.15em]
	\end{tabular}
	}
\end{table*}



\subsection{BVQA Models}
\label{sec:bvqa_models}
BVQA models can be loosely categorized into knowledge-driven and data-driven methods.


\noindent\textbf{Knowledge-driven BVQA models} rely heavily on handcrafted features such as natural scene statistics (NSS). A na\"{i}ve BVQA method computes the quality of each frame via popular NSS-based BIQA methods (\eg, NIQE \cite{mittal2012making} and BRISQUE \cite{mittal2012no}), followed by temporal quality pooling \cite{tu2020comparative}. Since motion analysis is regarded crucial in BVQA to quantify the (spatio)temporal distortions, many methods  incorporate  temporal features into BIQA solutions. V-BLIINDS \cite{saad2014blind} examines spatial naturalness (by NIQE), spatiotemporal statistical regularity (by 2D DCT analysis over frame differences),  and motion coherency (through computed motion vectors). Mittal \textit{et al.} \cite{mittal2015completely} proposed a training-free BVQA model named VIIDEO that exploits intrinsic statistical regularities of natural videos. Korhonen \cite{korhonen2019two} extracted a larger set of spatiotemporal features related to motion regularities, flickering, jerkiness, blur, blockiness, noise, and color, at two scales. Tu~\etal~\cite{tu2021ugc} combined the features from multiple BIQA and BVQA methods, and trained a support vector regressor (SVR) for quality assessment. 


It is believed that understanding semantic video information is beneficial for BVQA. Hence, some studies \cite{tu2021rapique, korhonen2020blind} tried to combine low-level handcrafted features with high-level deep features to boost the BVQA performance. 
For example, CNN-TLVQM \cite{korhonen2020blind} combines the handcrafted temporal features from TLVQM~\cite{korhonen2019two} and spatial features extracted by the pre-trained ResNet-50 for BIQA. 
RAPIQUE \cite{tu2021rapique} utilizes the handcrafted  NSS and deep features by ResNet-50 pre-trained for object recognition. Despite the long pursuit of
handcrafted NSS for BVQA, they exhibit subpar performance, some of which induce high computational complexity.



\noindent\textbf{Data-driven BVQA models} differ mainly in the instantiations of the four building blocks introduced in Section~\ref{sec:introduction} and the optimization strategies. For the \textit{video preprocessor},  a considerable body of work chooses to perform spatiotemporal downsampling to reduce the computational burdens, whose settings may be different if there are separate spatial and temporal feature extraction branches. For the \textit{spatial quality analyzer}, the prevailing model architectures are  ResNets~\cite{he2016deep} and VGG networks~\cite{simonyan2014very} pre-trained for object recognition~\cite{deng2009imagenet} or  IQA~\cite{ying2020patches,zhang2021uncertainty}. Other advanced architectures such as EfficientNets~\cite{tan2019efficientnet}, ConvNeXts~\cite{liu2022convnet}, vision Transformers (ViTs)~\cite{dosovitskiy2020image} have also been explored~\cite{wang2021rich,wu2022fast,wu2022disentangling}. For the \textit{temporal quality analyzer}, the dominant model architectures are heavily influenced by those in the action recognition field, including 3D ResNets and D3D~\cite{stroud2020d3d}, SlowFast networks~\cite{feichtenhofer2019slowfast}, and video Swin Transformers~\cite{liu2022video}. For the \textit{quality regressor}, priority is given to long-term dependency modeling in the temporal domain. Representative regressors include the simple average pooling, the multilayer perceptron (MLP), the gated recurrent unit (GRU), and deep models for time series (\eg, InceptionTime~\cite{ismail2020inceptiontime}).

With respect to optimization strategies, early methods prefer to fix spatial and temporal quality analyzers, and optimize the quality regressor solely, such as VSFA \cite{li2019quality}, PatchVQ~\cite{ying2021patch}, CoINVQ~\cite{wang2021rich}, and Li22~\cite{li2022blindly}. Recently,  visual-language pre-training is exploited to enable zero-shot BVQA \cite{wu2023towards,wu2023exploring}. BVQA methods based on pre-trained spatial and temporal quality analyzers benefit from other visual tasks, and mitigate the reliance on human-rated video data for end-to-end training. However,  domain shifts commonly exist between pretext and BVQA tasks, which are bound to give sub-optimal solutions.



End-to-end optimization of BVQA models is no easy task due to the huge demand for human-rated data, the mass computer memory consumption, and the long training time to convergence. However, if enabled, it gives BVQA models the opportunity to learn quality-aware features from raw video pixels for improved performance. Liu \textit{et al.} \cite{liu2018end} proposed a lightweight BVQA model, V-MEON, by jointly optimizing a 3D-DNN for quality assessment and compression distortion classification. Yi \textit{et al.} \cite{yi2021attention} relied only on the spatial quality analyzer (\ie, a non-local VGG network for attention modeling), leaving temporal modeling to the quality regressor. Wu \textit{et al.} \cite{wu2022fast} extracted fragments (\ie, the spatially spliced and temporally aligned patches)  as input to a video Swin Transformer~\cite{liu2022video}. They \cite{wu2022disentangling} later used the same trick to enable end-to-end optimization, and further captured video distortions from both signal-level and aesthetics-level cues. As the preliminary work~\cite{sun2022deep}, we  developed a simple BVQA method consisting of an end-to-end trainable multi-scale spatial feature extractor and a pre-trained fixed motion extractor.

To facilitate the comparison of DNN-based BVQA models, we summarize their computational structures as well as the optimization strategies in Table \ref{tab:bvqa_models}. It is clear that all BVQA models can be decomposed into the four basic building blocks. Here we limit our BVQA model capacity with minimalistic instantiations of these building blocks to probe the easy dataset problem that may occur in existing VQA datasets.







\section{Design of Minimalistic BVQA Models}
\label{proposed_model}

In this section, we design a family of minimalistic BVQA models with the goal of computationally analyzing  existing VQA datasets. Our models consist of four basic building blocks: a video preprocessor, a spatial quality analyzer, an optional temporal quality analyzer, and a quality regressor.

\subsection{Formulation of the BVQA Problem}
\label{model_design}
We first formulate the problem of  BVQA. Assume a video $\bm{x}= \{\bm x_i\}_{i=0}^{N-1}$, where $\bm x_i\in\mathbb{R}^{H\times W \times 3}$ denotes the $i$-th frame. $H$ and $W$ are  the height and the width of each frame, and $N$ is the total number of frames. The objective of a BVQA model $q_{\bm{w}}(\cdot):\mathbb{R}^{H\times W \times 3\times N} \mapsto \mathbb{R}$, parameterized by a vector $\bm w$, is to compute a scalar as an approximation to the true perceptual quality, $q(\bm x)$:
\begin{align}
    \hat{q} = q_{\bm w}(\bm x).
\end{align}
We may conveniently break down $q_{\bm w}(\cdot)$ into four basic building blocks. The first is a video preprocessor $\bm{p} (\cdot)$ to spatiotemporally downsample $\bm x$, leading to a set of $K$ key frames $\bm z = [\bm z_0, \bm z_1, \ldots, \bm z_{K-1}]$.  Each key frame $\bm z_i$ may be optionally accompanied by a video chunk $\bm v^{(i)}$ for temporal quality analysis:
\begin{align}
\bm{z}, \mathcal{V} = \bm{p}(\bm{x}),
\end{align}
where $\mathcal{V} = \{\bm v^{(i)}\}_{i=0}^{K-1}$.
The second is a spatial quality analyzer $\bm{s}(\cdot)$ that takes a key frame as input and extracts quality-aware spatial features:
\begin{align}
    \bm{s}_i = \bm{s}(\bm z_i), \quad \mathrm{for}\quad i = 0, 1,\ldots K-1.
\end{align}
The third is an optional temporal quality analyzer $\bm{t}(\cdot)$ to extract temporal features from $\bm v^{(i)}$:
\begin{align}
    \bm{t}_i = \bm{t}(\bm v^{(i)}), \quad \mathrm{for}\quad i = 0, 1,\ldots K-1.
\end{align}
The last is a quality regressor $a(\cdot)$ that maps the spatial and temporal quality features/scores into a global quality score:
\begin{align}
\hat{q}  = a(\bm s,\bm t),
\end{align}
where $\bm s = [\bm s_0, \bm s_1, \ldots, \bm s_{K-1}]$ and $\bm t = [\bm t_0, \bm t_1, \ldots, \bm t_{K-1}]$.
\subsection{Video Preprocessor}
As it is convenient to record videos in 4K resolution and $60$ fps, inputting such a raw video to the BVQA model would induce extremely high computation \cite{li2019quality,li2022blindly}. It also poses a grand challenge to  end-to-end optimization of BVQA models. To enable end-to-end training and be minimalistic, we perform aggressive spatiotemporal downsampling as video preprocessing. We first temporally downsample the raw video $\bm x=[\bm x_0,\bm x_1,
\ldots,\bm x_{N-1}]$ from $R$ to $R_a$ fps:
% \begin{align}
% \bm y_{i} = \bm{x}_{\lfloor R/2 \rfloor + R \times i},
% \end{align}
\begin{align}
\bm y_{i} = \bm{x}_{\lfloor R/R_a\times (0.5 + i) \rfloor},
\end{align}
where $\bm{y}_i$ is the $i$-th key frame subject to spatial downsampling, $\lfloor\cdot\rfloor$ is the floor function, and $K= \lfloor NR_a/R\rfloor$. We then spatially downsample each key frame $\bm y_i$ to $\bm z_i$ using the bilinear kernel such that the shorter side has a length of $L_s$, while keeping the aspect ratio.


Optionally, for each key frame $\bm y_i$, we sample a video chunk of $T+1$ frames $\bm{y}^{(i)}= [\bm y_0^{(i)},\bm y_2^{(i)},\ldots,\bm y_{T}^{(i)}]$
 from $\bm x$ at a frame interval $\tau$ for temporal quality analysis:
\begin{align}\label{eq:vc}
\bm{y}_j^{(i)} = \bm{x}_{\lfloor R/R_a(0.5+i) \rfloor +\tau(j-\lfloor T/2\rfloor)},
\end{align}
where $\bm y_i = \bm y^{(i)}_{\lfloor T/2\rfloor }$ is centered at the video chunk. 
% We set $T=32$ to be compatible with pre-trained DNNs for video processing~\cite{feichtenhofer2019slowfast}. 
As  temporal features are relative stable under different spatial resolutions~\cite{feichtenhofer2019slowfast}, we resize $\bm y^{(i)}$ to $\bm{v}^{(i)}$ with resolution of $L_t\times L_t$, where  $L_t< L_s$. To downgrade the importance of the temporal features, we may further temporally downsample the key frames $\bm y=[\bm y_0,\bm y_1,
\ldots,\bm y_{K-1}]$ from $R_a$ to $R_b$ fps before the video chunk extraction by Eq.~\eqref{eq:vc}. In this case, the missing video chunks for decimated key frames can be na\"{i}vely broadcast from the computed video chunks, as shown in Fig.~\ref{fig:video_preprocessor}.
 

% Figure environment removed



\subsection{Spatial Quality Analyzer}
The spatial quality analyzer $\bm{s}(\cdot)$ takes one key frame as input, and extracts quality-aware spatial features. It is computationally close to BIQA models, and many previous BVQA models \cite{ying2021patch,li2022blindly} directly adopt BIQA models for spatial quality analysis. Nevertheless, the impact of spatial distortions in videos may differ from that in images. For example, a fast-moving object may appear blurred in a single frame but it looks good when viewed in the video. In this case, a BIQA model may over-penalize the spatial distortions, causing inaccurate estimation of perceptual video quality.  We look into these subtle differences by training DNN-based BIQA models with different quality-aware initializations as the spatial quality analyzers. Specifically, we choose two popular backbones,  ResNet-50 \cite{he2016deep} and Swin Transformer-B \cite{liu2021swin}, and strip their classification heads. Being minimalistic,  we do not employ advanced feature extraction strategies, such as multi-stage feature fusion \cite{sun2022deep} and global average and standard deviation pooling  \cite{li2019quality,li2022blindly,sun2022deep}, and only apply global average pooling to the last-stage feature maps. 

As stated in Section \ref{sec:bvqa_models}, leveraging quality-aware initializations has been a common practice for designing DNN-based BVQA models. For the spatial quality analyzer, we explore initializations 1) pre-trained on ImageNet, 2) pre-trained on IQA datasets, and 3) pre-trained on  VQA datasets (\eg, pre-trained on LSVQ and tested on LIVE-VQC).
Other initializations (\eg, resulting from unsupervised learning~\cite{he2020momentum,he2022masked,radford2021learning}) will be ablated in Section \ref{subsubsec:oi}. 

\subsection{Temporal Quality Analyzer} 
We allow an optional temporal quality analyzer $\bm t(\cdot)$, and try to make its design as simple as possible. Specifically, $\bm t(\cdot)$ independently extracts the temporal features from the $i$-th video chunk $\bm v^{(i)}$ centered at the $i$-th key frame $\bm z_i$ as a form of short-term memory. Being minimalistic, temporal dependencies across different video chunks as the long-term memory is not modeled. We also choose to freeze the temporal quality analyzer,   making it more of a local motion analyzer. 
We instantiate the temporal quality analyzer by the temporal pathway of the SlowFast network~\cite{feichtenhofer2019slowfast}, pre-trained on the Kinetics-400 dataset~\cite{carreira2017quo}. Similar to the spatial feature extraction, we throw away the original classification head, and compute the temporal features by global average pooling the last-stage feature maps. 


\subsection{Quality Regressor}
The last step is to map the extracted spatial (and temporal) features to a single scalar as the global quality score, achieved by the quality regressor. In previous studies, sequential models such as GRU \cite{li2019quality} and InceptionTime \cite{ying2021patch} are adopted to maintain useful long-term dependencies among key frames (and the associated video chunks). Being minimalistic, we intentionally opt for a much simpler fully-connected (FC) layer (as a special case of the MLP), regressing the $i$-th local quality score $\hat{q}_i$ based on the concatenated spatial and temporal features:
\begin{align}
\hat{q}_i = [\bm{s}_i{^\intercal}, \bm{t}_i{^\intercal}] \bm{\beta}, \quad \mathrm{for}\quad i = 0, 1, \ldots, K-1,
\end{align}
where $\bm{\beta}$ denotes the learnable linear projection vector.
Finally, we aggregate frame-level (or chunk-level) quality estimates into the video-level quality score by simple average pooling:
\begin{align}
\hat{q} = \frac{1}{K}\sum_{i=0}^{K-1} \hat{q}_i.
\end{align}







% \subsection{Training techniques}
\subsection{Loss function}
\label{loss_function}
The remaining is to specify a proper loss function for end-to-end BVQA model optimization. Conventionally, BVQA is regarded as a regression task, which can be supervised by 
the mean absolute error (MAE)  and the mean squared error (MSE) \cite{li2019quality, ying2021patch, korhonen2020blind, liu2018end, wang2021rich, yi2021attention} to predict \textit{absolute} quality (\ie, MOSs). Alternatively, we may look at \textit{relative} quality prediction by formulating BVQA as a learning-to-rank problem. Thus, pairwise ranking losses such as the fidelity loss under the Thurstone-Mosteller models~\cite{thurstone1927law} (or the Bradley-Terry models~\cite{bradley1952rank}) and listwise ranking losses such as the cross entropy loss under the permutation probabilities, 
differentiable Spearman's rank correlation coefficient (SRCC) \cite{li2022blindly, wu2022disentangling}, and PLCC \cite{li2022blindly, wu2022fast,wu2022disentangling} are viable choices.
Here, we adopt PLCC as the sole loss for optimization of our BVQA models.

\section{Computational Analysis of VQA Datasets}
\label{experimental_results}

In this section, we first present in detail the experimental settings for comparison of our minimalistic  BVQA models against three representative methods with different design philosophies as reference. Based on the experimental results, we then conduct a computational analysis of eight VQA datasets, pointing out the overlooked easy dataset problem.





\subsection{Experimental Settings}
\noindent\textbf{VQA Datasets}.
We evaluate  BVQA models on eight  VQA datasets introduced in Section \ref{vqa_dataset} and summarized in Table \ref{overview_vqa_dataset}, including CVD2014 \cite{nuutinen2016cvd2014}, LIVE-Qualcomm \cite{ghadiyaram2017capture}, KoNViD-1k \cite{hosu2017konstanz}, LIVE-VQC \cite{sinno2018large}, YouTube-UGC \cite{wang2019youtube}, LIVE-YT-Gaming \cite{yu2022subjective}, LBVD \cite{chen2019qoe}, and LSVQ \cite{ying2021patch}. For the LSVQ dataset, we follow the original dataset splitting \cite{ying2021patch}, in which the training set consists of $28,013$ videos, and the default and 1080p test sets contain $7,220$ and $3,561$ videos, respectively. We randomly sample $20\%$ videos from the training set for validation, and report the results on the two test sets. For the remaining seven datasets, we randomly split each into the training, validation, and test sets with a ratio of $6:2:2$  for $10$ times, and report the median results.


\noindent\textbf{Competing Methods.} We select three representative BVQA models as the competitors: VSFA~\cite{li2019quality}, Li22~\cite{li2022blindly}, and FAST-VQA~\cite{wu2022fast}. VSFA and Li22 rely on pre-trained models to extract spatial and temporal features, and only train the quality regressor. FAST-VQA is a record-breaking BVQA model that end-to-end optimizes a variant of video Swin Transformer~\cite{liu2022video}. For a fair comparison, we retrain the three competing models on seven small-scale VQA datasets based on our dataset splitting strategy and their original optimization procedures. For the large-scale LSVQ with held-out test sets, we copy the results from the respective papers.

We instantiate ten models from our minimalistic BVQA model family: \textbf{(\rom{1})} ResNet-50 pre-trained on ImageNet-1K as the spatial quality analyzer with no temporal quality analyzer (as the baseline model); \textbf{(\rom{2})} pre-training (\rom{1}) on the combination of four IQA datasets\footnote{These are BID \cite{ciancio2011no}, LIVE Challenge \cite{ghadiyaram2016massive}, KonIQ-10k \cite{hosu2020koniq}, and SPAQ \cite{fang2020perceptual}. The enabling training strategy was proposed in~\cite{zhang2021uncertainty}.}; \textbf{(\rom{3})} pre-training (\rom{1}) on the training set of LSVQ\footnote{This variant shall be tested on the other seven VQA datasets.}; \textbf{(\rom{4})} adding the temporal analyzer to (\rom{1}); \textbf{(\rom{5})} adding the temporal analyzer to (\rom{2}); \textbf{(\rom{6})} adding the temporal analyzer to (\rom{3}); \textbf{(\rom{7})} to \textbf{(\rom{10})}: replacing ResNet-50 with Swin Transformer-B as the spatial quality analyzer in (\rom{1}), (\rom{3}), (\rom{4}), and (\rom{6}), respectively.

\noindent\textbf{Evaluation Criteria.}
We adopt two criteria to evaluate the performance of VQA models:  SRCC and PLCC, which indicate the prediction monotonicity and prediction linearity, respectively. Before calculating PLCC, we follow the procedure in \cite{antkowiak2000final} to map the model predictions to the MOSs using a four-parameter logistic function to compensate for the prediction nonlinearity.

\noindent\textbf{Implementation Details}.
For the video preprocessor, the shorter side of the key frames, $L_s$, is set to $448$ for ResNet-50 and $384$ for Swin Transformer-B, while the spatial resolution of the video chunks, $L_t$, is set to $224$. The temporal sampling rates of the key frames, $R_a$, and the video chunks, $R_b$, are set to $1$ and $0.5$ fps, respectively. We sample $T=32$ frames for each video chunk at a frame interval $\tau = 1$. We choose two backbone network: ResNet-50 \cite{he2016deep} and Swin Transformer-B \cite{liu2021swin} pre-trained on ImageNet-1k \cite{deng2009imagenet} as the spatial quality analyzer, and use the fast pathway of SlowFast Network (ResNet-50)\footnote{The hyperparameters $T$ and $\tau$ of SlowFast used in this paper are both $8$.}~\cite{feichtenhofer2019slowfast} pre-trained on Kinetics-400~\cite{carreira2017quo} as the temporal quality analyzer. 


Training is carried out by optimizing PLCC using the Adam method~\cite{kingma2014adam} with an initial learning rate of $10^{-5}$ and a minibatch size of $8$. In the training stage, the input frames are randomly cropped to $448\times448$ for ResNet-50 and $384\times384$ for Swin Transformer-B. In the validation and test stages, the input frames are center-cropped with the same spatial resolution as in the training stage. We decay the learning rate by a factor of $10$ after $2$ epochs for LSVQ, and after $10$ epochs for other datasets. Depending on the dataset size, the maximum number of epochs is set to $10$ for LSVQ, $30$ for KoNViD-1k, YouTube-UGC and LBVD, and $50$ for CVD2014, LIVE-Qualcomm, LIVE-VQC and LIVE-YT-Gaming, respectively.


\begin{table*}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance comparison in terms of SRCC and PLCC of the proposed minimalistic BVQA models against three competing methods on eight VQA datasets. Li22 relies on model parameters pre-trained on the combination of four IQA datasets (as in UNIQUE~\cite{zhang2021uncertainty}), while the results of FAST-VQA on seven small-scale VQA datasets (except for LSVQ) are based on the model parameters pre-trained on LSVQ. W.A. means the weighted average result, with the weightings proportional to the number of videos in each VQA dataset. The top-2 results on each dataset are highlighted in bold}
\label{performance_proposed_method}
\begin{tabular}{ll|cc|cc|cc|cc|cc}
\toprule[.15em]
 \multirow{2}{*}{Method} & \multirow{2}{*}{}  & \multicolumn{2}{c|}{CVD2014} & \multicolumn{2}{c|}{LIVE-Qualcomm} & \multicolumn{2}{c|}{KoNViD-1k} & \multicolumn{2}{c|}{LIVE-VQC}& \multicolumn{2}{c}{YouTube-UGC}\\
  &  & SRCC  & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC\\


\hline
\rowcolor{tablegray}
\multicolumn{12}{l}{\textit{Competing Methods as Reference}} \\
VSFA \cite{li2019quality} &  & 0.850 &  0.869  & 0.708  & 0.774 &  0.794 & 0.799 &  0.718 & 0.771 & 0.787  & 0.789 \\
Li22 \cite{li2022blindly} &  & 0.863&  0.883 &  \textbf{0.833} & \textbf{0.837} & 0.839  & 0.830 & 0.841  & 0.839&   0.825 &  0.818 \\
FAST-VQA \cite{wu2022fast} &  &  \textbf{0.883} &  \textbf{0.901}  &  0.807  &   0.814 &   \textbf{0.893} &  \textbf{0.887} &    \textbf{0.853} &   \textbf{0.873} &  0.863 & 0.859 \\
% FAST-VQA \cite{wu2022fast} & 224 & - & -  & -  & - &  - & - & -   & -  & - & - \\
% Faster-VQA & 224 & 0.873&  0.874 &  0.772 & 0.811 & 0.877  & 0.882 & 0.855  & 0.859& 0.852  & 0.869 \\
\hline
\rowcolor{tablegray}
\multicolumn{12}{l}{\textit{Proposed Methods}} \\
\multicolumn{2}{l|}{(\rom{1}): ResNet-50 as baseline} &0.867 &   0.889 & 0.651  & 0.718 & 0.764   & 0.781  &  0.653 &  0.719&  0.822 & 0.826 \\ 
\cdashline{1-12}
\multicolumn{2}{l|}{(\rom{2}): \textit{(\rom{1}) + IQA pre-trained}}& 0.866&   0.882 &  0.762 &  0.805&  0.822 & 0.830 & 0.736  & 0.788 &  0.847 &  0.843 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{-0.1\%} &  \textcolor{gray}{-0.8\%} &  \textcolor{gray}{+17.1\%} & \textcolor{gray}{+12.1\%} &  \textcolor{gray}{+7.6\%} & \textcolor{gray}{+6.3\%} & \textcolor{gray}{+12.7\%}  & \textcolor{gray}{+9.6\%}& \textcolor{gray}{+3.0\%}  & \textcolor{gray}{+2.1\%} \\
\multicolumn{2}{l|}{(\rom{3}): \textit{(\rom{1}) + VQA pre-trained}}&0.860 & 0.889  &  0.774 & 0.800 &  0.861 &  0.861&  0.722 &0.761 &  0.834 & 0.843 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{-0.8\%} &  \textcolor{gray}{0.0\%} &  \textcolor{gray}{+18.9\%} & \textcolor{gray}{+11.4\%} &  \textcolor{gray}{+12.7\%} & \textcolor{gray}{+10.2\%} & \textcolor{gray}{+10.6\%}  & \textcolor{gray}{+5.8\%}& \textcolor{gray}{+1.5\%}  & \textcolor{gray}{+2.1\%} \\
% \cdashline{1-12}
\multicolumn{2}{l|}{(\rom{4}): \textit{(\rom{1}) + motion features}} &0.852 &   0.871 &  0.693 &  0.732 &  0.797 & 0.803 & 0.752   &  0.798 & 0.810   &  0.809  \\
% \rowcolor{tablegray}
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{-1.7\%} &  \textcolor{gray}{-2.0\%} &  \textcolor{gray}{+6.5\%} & \textcolor{gray}{+1.9\%} &  \textcolor{gray}{+4.4\%} & \textcolor{gray}{+2.8\%} & \textcolor{gray}{+15.2\%}  & \textcolor{gray}{+11.0\%}& \textcolor{gray}{-1.5\%}  & \textcolor{gray}{-2.1\%} \\
\multicolumn{2}{l|}{(\rom{5}): \textit{(\rom{2}) + motion features}}& \textbf{0.876}&  \textbf{0.899} & 0.749  & 0.805 &  0.830 & 0.835 &  0.795 & 0.819&  0.830 & 0.835 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+1.0\%} &  \textcolor{gray}{+1.1\%} &  \textcolor{gray}{+15.1\%} & \textcolor{gray}{+12.1\%} &  \textcolor{gray}{+8.7\%} & \textcolor{gray}{+6.9\%} & \textcolor{gray}{+21.7\%}  & \textcolor{gray}{+13.9\%}& \textcolor{gray}{+1.0\%}  & \textcolor{gray}{+1.1\%} \\
\multicolumn{2}{l|}{(\rom{6}): \textit{(\rom{3}) + motion features}}&\textbf{0.876} & \textbf{0.899}  &  0.753 &  0.800 &  0.859 & 0.864 & 0.813  & 0.843& 0.845  &  0.853 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+1.0\%} &  \textcolor{gray}{+1.1\%} &  \textcolor{gray}{+15.7\%} & \textcolor{gray}{+11.4\%} &  \textcolor{gray}{+12.5\%} & \textcolor{gray}{+10.6\%} & \textcolor{gray}{+24.5\%}  & \textcolor{gray}{+17.2\%}& \textcolor{gray}{+2.8\%}  & \textcolor{gray}{+3.3\%} \\
\hline


(\rom{7}): Swin Transformer-B as baseline &  & 0.833&  0.860  &  0.710 &  0.711 & 0.823   & 0.830  &  0.683 &  0.735 &  0.832 & 0.837 \\
\cdashline{1-12}
\multicolumn{2}{l|}{(\rom{8}): \textit{(\rom{7}) + VQA pre-trained}}& 0.862 &  0.888 &  0.768 & 0.808 & 0.850  & 0.857 &  0.765 & 0.821& \textbf{0.877}  &  \textbf{0.873} \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+3.5\%} &  \textcolor{gray}{+3.3\%} &  \textcolor{gray}{+8.2\%} & \textcolor{gray}{+13.6\%} &  \textcolor{gray}{+3.3\%} & \textcolor{gray}{+3.2\%} & \textcolor{gray}{+12.0\%}  & \textcolor{gray}{+11.7\%}& \textcolor{gray}{+5.4\%}  & \textcolor{gray}{+4.3\%} \\
% \cdashline{1-12}

\multicolumn{2}{l|}{(\rom{9}): \textit{(\rom{7}) + motion features}}&0.845 &   0.869 & 0.728  & 0.748 &  0.851 & 0.854 &  0.772 & 0.809& 0.845  &  0.870  \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+1.4\%} &  \textcolor{gray}{+1.0\%} &  \textcolor{gray}{+2.5\%} & \textcolor{gray}{+5.2\%} &  \textcolor{gray}{+3.4\%} & \textcolor{gray}{+2.9\%} & \textcolor{gray}{+13.0\%}  & \textcolor{gray}{+10.1\%}& \textcolor{gray}{+1.6\%}  & \textcolor{gray}{+3.9\%} \\

\multicolumn{2}{l|}{(\rom{10}): \textit{(\rom{8}) + motion features}}& 0.869 &   0.897 &  \textbf{0.835} & \textbf{0.837} & \textbf{0.889}  & \textbf{0.890} &  \textbf{0.842} & \textbf{0.854}&  \textbf{0.890} &  \textbf{0.891} \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+4.3\%} &  \textcolor{gray}{+4.3\%} &  \textcolor{gray}{+17.6\%} & \textcolor{gray}{+17.7\%} &  \textcolor{gray}{+8.0\%} & \textcolor{gray}{+7.2\%} & \textcolor{gray}{+23.3\%}  & \textcolor{gray}{+16.2\%}& \textcolor{gray}{+7.0\%}  & \textcolor{gray}{+6.5\%} \\



\bottomrule[.15em]
 \multirow{2}{*}{Method} & \multirow{2}{*}{}  & \multicolumn{2}{c|}{LBVD} & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c|}{LSVQ-1080p} & \multicolumn{2}{c|}{LIVE-YT-Gaming}& \multicolumn{2}{c}{W.A.}\\
  &  & SRCC  & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC & SRCC & PLCC\\


\hline
\rowcolor{tablegray}
\multicolumn{12}{l}{\textit{Competing Methods as Reference}} \\
VSFA \cite{li2019quality} &  & 0.834 &   0.830 &  0.801 & 0.796 &  0.675 &  0.704 &  0.784 &  0.819 & 0.769&0.778\\
Li22 \cite{li2022blindly} &  &\textbf{0.891} &  \textbf{0.887} & 0.852 & 0.854 &  0.772 & 0.788 &   0.852 & 0.868 & 0.833&0.837\\
FAST-VQA \cite{wu2022fast} &  & 0.804 &  0.809  &  \textbf{0.876} & \textbf{0.877} & \textbf{0.779}  & \textbf{0.814} & \textbf{0.869}  &  \textbf{0.880}  &\textbf{0.848} &\textbf{0.857}\\

\hline
\rowcolor{tablegray}
\multicolumn{12}{l}{\textit{Porposed  Methods}} \\
\multicolumn{2}{l|}{(\rom{1}): ResNet-50 as baseline}  & 0.704&   0.706 &  0.833 & 0.833 &  0.711 & 0.761&  0.800 &   0.853& 0.784&0.802\\
\cdashline{1-12}
\multicolumn{2}{l|}{(\rom{2}): \textit{(\rom{1}) + IQA pre-trained}}&0.716 &  0.718  & 0.834  &  0.833&  0.713 &  0.765 &  0.835 & 0.877 &  0.795 &  0.811\\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+1.7\%} &  \textcolor{gray}{+1.7\%} &  \textcolor{gray}{+0.1\%} & \textcolor{gray}{+0.0\%} &  \textcolor{gray}{+0.3\%} & \textcolor{gray}{+0.5\%} & \textcolor{gray}{+4.4\%}  & \textcolor{gray}{+2.8\%}& \textcolor{gray}{+1.4\%}  & \textcolor{gray}{+1.1\%} \\
\multicolumn{2}{l|}{(\rom{3}): \textit{(\rom{1}) + VQA pre-trained}}&0.705 & 0.705  & 0.833  &  0.833 & 0.711  &  0.761 & 0.822  & 0.868&  0.795 &  0.810\\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+0.1\%} &  \textcolor{gray}{-0.1\%} &  \textcolor{gray}{+0.0\%} & \textcolor{gray}{+0.0\%} &  \textcolor{gray}{+0.0\%} & \textcolor{gray}{+0.0\%} & \textcolor{gray}{+2.7\%}  & \textcolor{gray}{+1.8\%}& \textcolor{gray}{+1.4\%}  & \textcolor{gray}{+1.0\%} \\
% \cdashline{1-12}

\multicolumn{2}{l|}{(\rom{4}): \textit{(\rom{1}) + motion features}}& 0.878 &  0.876   &  0.864  &  0.862  & 0.748   & 0.789  &  0.841   & 0.880    & 0.822 &0.834 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+25.3\%} &  \textcolor{gray}{+24.8\%} &  \textcolor{gray}{+3.7\%} & \textcolor{gray}{+3.5\%} &  \textcolor{gray}{+5.2\%} & \textcolor{gray}{+3.7\%} & \textcolor{gray}{+3.2\%}  & \textcolor{gray}{+0.5\%}& \textcolor{gray}{+4.8\%}  & \textcolor{gray}{+4.0\%} \\

\multicolumn{2}{l|}{(\rom{5}): \textit{(\rom{2}) + motion features}}& \textbf{0.884}&   0.877 & 0.864    &  0.863   & 0.756    &  0.792   & 0.839   &   0.883 & 0.825 &0.836 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+25.6\%} &  \textcolor{gray}{+24.2\%} &  \textcolor{gray}{+3.7\%} & \textcolor{gray}{+3.6\%} &  \textcolor{gray}{+6.3\%} & \textcolor{gray}{+4.1\%} & \textcolor{gray}{+4.9\%}  & \textcolor{gray}{+3.5\%}& \textcolor{gray}{+5.2\%}  & \textcolor{gray}{+4.2\%} \\

\multicolumn{2}{l|}{(\rom{6}): \textit{(\rom{3}) + motion features}}& 0.881&  \textbf{0.885} & 0.863  &  0.861 &  0.752 &  0.793 &  0.836 &   0.871 & 0.833 &0.845 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+24.7\%} &  \textcolor{gray}{+24.1\%} &  \textcolor{gray}{+3.7\%} & \textcolor{gray}{+3.5\%} &  \textcolor{gray}{+5.2\%} & \textcolor{gray}{+3.7\%} & \textcolor{gray}{+5.1\%}  & \textcolor{gray}{+3.2\%}& \textcolor{gray}{+6.2\%}  & \textcolor{gray}{+5.4\%} \\

\hline


(\rom{7}): Swin Transformer-B as baseline &  & 0.692&   0.698 & 0.860  &  0.859 &  0.758 &  0.802 &  0.810 & 0.853 & 0.809&0.824\\
\cdashline{1-12}
\multicolumn{2}{l|}{(\rom{8}): \textit{(\rom{7}) + VQA pre-trained}}& 0.694 &  0.695 &  0.855 & 0.853 &  0.737 & 0.789 &  0.847 & 0.875&  0.821 &  0.835 \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+0.3\%}  &  \textcolor{gray}{-0.4\%}  &  \textcolor{gray}{+0.0\%}  & \textcolor{gray}{+0.0\%}  &  \textcolor{gray}{+0.0\%}  & \textcolor{gray}{+0.0\%}  & \textcolor{gray}{+4.6\%}   & \textcolor{gray}{+2.6\%} & \textcolor{gray}{+1.5\%}  & \textcolor{gray}{+1.3\%} \\
% \cdashline{1-12}

\multicolumn{2}{l|}{(\rom{9}): \textit{(\rom{7}) + motion features}}&0.880  &  0.884   &  \textbf{0.881 } & \textbf{0.879 } &  \textbf{0.781 } & \textbf{0.820 } &  0.845  &  0.870  & 0.845 &\textbf{0.857} \\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+27.2\%} &  \textcolor{gray}{+26.6\%} &  \textcolor{gray}{+2.4\%} & \textcolor{gray}{+2.3\%} &  \textcolor{gray}{+3.0\%} & \textcolor{gray}{+2.2\%} & \textcolor{gray}{+4.3\%}  & \textcolor{gray}{+2.0\%}& \textcolor{gray}{+4.4\%}  & \textcolor{gray}{+4.0\%} \\

\multicolumn{2}{l|}{(\rom{10}): \textit{(\rom{8}) + motion features}}& 0.882 &  0.882  & \textbf{0.881 } &  \textbf{0.879 }&  \textbf{0.781 } & \textbf{0.820 } &  \textbf{0.857}  & \textbf{0.888 }  & \textbf{0.857 }&\textbf{0.867 }\\
\multicolumn{2}{l|}{\textit{\textcolor{gray}{Improvement over baseline}}} &\textcolor{gray}{+27.5\%} &  \textcolor{gray}{+26.4\%} &  \textcolor{gray}{+2.4\%} & \textcolor{gray}{+2.3\%} &  \textcolor{gray}{+3.0\%} & \textcolor{gray}{+2.2\%} & \textcolor{gray}{+5.8\%}  & \textcolor{gray}{+4.1\%}& \textcolor{gray}{+5.9\%}  & \textcolor{gray}{+5.2\%} \\



\bottomrule[.15em]
\end{tabular}
\end{table*}

\subsection{Main Computational Analysis}
\label{computational_analysis}
\label{sec:performance_analysis}Table~\ref{performance_proposed_method} shows the SRCC and PLCC results of our minimalistic BVQA models with emphasis on relative improvements, based on which we perform a computational analysis of each dataset in chronological order. The three competing methods serve as the reference of state-of-the-art performance.


\noindent\textbf{CVD2014}. Model (\rom{1}), closest to a BIQA solution with ResNet-50 as the spatial quality analyzer, no temporal quality analysis, and no temporal feature aggregation, is able to achieve highly nontrivial results, outperforming VSFA~\cite{li2019quality} and Li22~\cite{wu2022fast}. We consider this as a strong indication that CVD2014 suffers from the easy dataset problem due to a poor sample selection strategy. In fact, the videos in CVD2014 were captured using nearly still cameras, with minimal temporal distortions. Meanwhile, the scene diversity was manually optimized, which is bound to be sub-optimal. Our claim is further supported by the results of Models (\rom{4}) to (\rom{6}), in which the incorporation of motion features by the temporal quality analyzer provides little to no improvement (by around $1\%$). In addition, due to the limited number of videos in CVD2014, increasing the capacity of the spatial quality analyzer from ResNet-50 to Swin Transformer-B leads to overfitting. Models (\rom{8}) and (\rom{9}) that fine-tune Swin Transformer-B from the IQA or VQA pre-trained initialization restore some performance, but are still no better than Model (\rom{1}).




\noindent\textbf{LIVE-Qualcomm}. Although Model (\rom{1}) lags behind the three competing methods, it quickly catches up when starting from IQA and VQA pre-trained initializations. This suggests that despite its simplicity, Model (\rom{1}) has the capacity to handle LIVE-Qualcomm, provided that the adopted optimization technique converges to good local optima. Moreover, by comparing Model (\rom{4}) to Models (\rom{2}) and (\rom{3}) (and Model (\rom{9}) to Model (\rom{8})), it is clear that a better initialization (as part of the optimization technique) for the spatial quality analyzer is more important than the integration of the temporal quality analyzer. This is somewhat surprising because LIVE-Qualcomm includes videos with stabilization issues as an explicit form of temporal distortions. A plausible explanation may be that the temporal distortions in LIVE-Qualcomm are strongly correlated with the spatial distortions. We treat these results as signals of the easy dataset problem, although less severe than CVD2014.

Unlike the results on CVD2014, Swin Transformer-B  performs better than ResNet-50 as the spatial quality analyzer with no signal of overfitting. This is attributed to the increased number of unique scenes relative to CVD2014 (from $5$ to $54$), showing that  maximizing content and distortion diversity during sample selection is necessary.  
By including all components, Model (\rom{10}) as the most ``sophisticated'' variant in our minimalistic BVQA model family matches the best performance by Li22 with the long-term dependency modeling. 

\noindent \textbf{KoNViD-1k}. Models (\rom{3}) and (\rom{8}) present clear improvements over Models (\rom{1}) and (\rom{7}), respectively, and surpass VSFA and Li22 by a clear margin. This verifies the importance of quality-aware initializations for the spatial quality analyzer. Moreover, KoNViD-1k adopts a density-based fair sampling strategy to encourage content and distortion diversity, which allows the spatial quality analyzer with increased capacity to be trained without overfitting. However, the temporal information (\ie, the mean of frame-wise standard deviations
of pixel-wise frame differences) used in KoNViD-1k for sample selection appears poorly correlated with temporal content and distortion complexity, and plays little role in discouraging BIQA solutions. Moreover, the original video pool YFCC100M, from which KoNViD-1k is sampled, contains mostly legacy videos dominated by spatial distortions. These explain the marginal performance gains by adding the motion features.  Last, similar to LIVE-Qualcomm, Model (\rom{10}) attains the contemporary performance set by FAST-VQA.



\noindent\textbf{LIVE-VQC}. In contrast to the results on CVD2014, LIVE-Qualcomm, and KoNViD-1k, the temporal quality analyzer brings noticeable improvements (more than $10\%$ by comparing Models (\rom{4}) and (\rom{9}) to Models (\rom {1}) and (\rom{7}), respectively). This is due mainly to the manual selection of videos with diverse levels of motion, contributing to complex composite spatiotemporal distortions such as jittering (due to camera shake), flickering\footnote{Flickering is typically used to describe frequent and annoying luminance or chrominance changes along the temporal dimension~\cite{zeng2014characterizing}.}, and jerkiness\footnote{Jerkiness is due to temporal aliasing, in which the temporal sampling rate is low relative to the object speed, making the object appear to behave peculiarly~\cite{zeng2014characterizing}.}. Combined with quality-aware initializations, Models (\rom{6}) and (\rom{10}) experience a significant performance boost compared to their respective baselines.  



\noindent\textbf{YouTube-UGC}. The model behaviors on YouTube-UGC resemble those on CVD2014. In particular, the two baselines, Models (\rom{1}) and (\rom{6}), solely perform satisfactorily. The added gains by the temporal quality analyzer are very limited (and sometimes negative),  compared to quality-aware initializations. The increased content diversity (by a density-based sampling strategy) and complexity (often due to scene changes) can be handled by better spatial quality analyzers (\ie, Swin Transformer-B over ResNet-50). This suggests that YouTube-UGC is dominated by the spatial distortions, and exhibits the easy dataset problem.  
In fact, two temporal features (\ie, the number of bits used to encode a P-frame and the standard deviation of compressed bitrates of all one-second video chunks) are used for sampling selection, which turn out to be inadequate in identifying challenging videos characterized by temporal distortions. 


\noindent\textbf{LBVD}. The most important observation is that the temporal quality analyzer plays an important role on LBVD, whose effect is even more pronounced than that on LIVE-VQC. Specifically, in the absence of the temporal quality analyzer, regardless of the variations of the spatial quality analyzers (\ie, whether to adopt a more sophisticated architecture and whether to start from quality-aware initializations), the SRCC results are roughly around $0.7$. When the temporal quality analyzer is incorporated, the performance is significantly boosted to about $0.88$, uplifting by $25.7\%$. This arises because  LBVD consists of live broadcasting videos based on HTTP adaptive streaming, which are characterized by \textit{localized} temporal distortions like rebuffering, stalling, and jittering. Additionally, LBVD mainly contains low-resolution front-face videos with different backgrounds, whose spatial information is far simpler than that in other datasets. Hence, a simple spatial quality analyzer (\eg, ResNet-50) suffices to analyze spatial quality.

\noindent\textbf{LSVQ}. As the largest VQA dataset so far, LSVQ neutralizes the performance gains by the IQA pre-trained initialization. Meanwhile, LSVQ generally provides more effective initializations to improve  BVQA models on other small-scale VQA datasets. 
Nevertheless, like KoNViD-1k and YouTube-UGC, LSVQ is dominated by the spatial distortions, on which Models (\rom{1}) and (\rom{7}) achieve competitive results, weakening its role of benchmarking BVQA models. 

LSVQ includes a second test set of 1080p videos, with a resolution higher than that in the training and default test sets. We find that the performance on LSVQ-1080p degrades noticeably for two main reasons. The first is aggressive spatial downsampling, which results in a loss of fine details relevant to BVQA. As evidence, Li22 that employs the same (but fixed) spatial and temporal quality analyzers as in Models (\rom{4}) to (\rom{6}) achieves better performance on LSVQ-1080p by operating on the original spatial resolution.
The second is that the 1080p videos commonly lie in the high-quality regime, making it difficult for BVQA models to discriminate their fine-grained relative quality.

\noindent\textbf{LIVE-YT-Gaming}. The trend on LIVE-YT-Gaming is similar to that on KoNViD-1k, YouTube-UGC, and LSVQ. First, a more powerful spatial quality analyzer in terms of both initializations and network architectures is more beneficial. It is noteworthy that the videos in LIVE-YT-Gaming are rendered by computer graphics techniques. The performance gains by IQA and VQA pre-trained initializations indicate that the domain gap between gaming videos and natural videos is relatively small at least for the quality assessment task. This also elucidates previous practices of employing NSS \cite{yu2022subjective} in gaming VQA. Second,  the temporal quality analyzer is secondary, despite the fact that the objects in gaming videos often move fast, and scene transition is also frequent (especially for first-person video games).



Putting the computational analysis results together, we make the following conclusive statements. \underline{First}, we empirically rank the eight VQA datasets according to the severity of the easy dataset problem (from least to most): 
\begin{itemize}
    \item LBVD, that relies heavily on temporal quality analysis by including live broadcasting videos with limited spatial information;
    \item LIVE-VQC, that carefully selects videos of complex temporal distortions;
    \item LIVE-Qualcomm, that contains certain temporal distortions but strongly correlated with the spatial distortions (see also Section~\ref{subsec:cde});
    \item KoNViD-1k, YoutubeUGC, LSVQ, and LIVE-YT-Gaming, that are dominated by the spatial distortions with some content diversity constraints;
    \item CVD2014, that is dominated by spatial distortions with limited content diversity.
\end{itemize} 
\underline{Second}, although the temporal quality analyzer is indispensable in LBVD and LIVE-VQC, our instantiation is embarrassingly simple. We subsample a set of video chunks (relative to the key frames), which are subject to independent local temporal analysis by a pre-trained network for action recognition. Neither end-to-end fine-tuning nor long-term dependency is modeled. This suggests that there is still ample room for constructing better VQA datasets that desire a complete treatment of temporal quality analysis.
\underline{Third}, encouraging sample diversity either manually~\cite{nuutinen2016cvd2014,ghadiyaram2017capture,sinno2018large,chen2019qoe,yu2022subjective} or automatically~\cite{hosu2017konstanz,wang2019youtube,ying2021patch}  is necessary to promote generalizable BVQA models, as evidenced by the results of switching ResNet-50 to Swin Transformer-B as the spatial quality analyzer. The missing piece during dataset construction is to simultaneously encourage sample difficulty in a way to favor more appropriate designs of the (spatio)temporal quality analyzers, and ultimately to prevent close-to-BIQA solutions.
\underline{Fourth}, albeit chasing the top performance on existing VQA datasets is not our goal, Model (\rom{10}) obtains the best weighted average results, where the weighting is proportional to the number of videos in each VQA dataset. This justifies the rationality of the decomposition and implementation of our minimalistic BVQA model family used for computational analysis.




\begin{table*}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Cross-dataset evaluation as a way of examining the second criterion of the easy dataset problem. The models are trained on LSVQ and tested on the other seven VQA datasets without fine-tuning. The best results are highlighted in bold}
\label{performance_cross_dataset_evaluation}
\resizebox{1\textwidth}{!}{
\begin{tabular}{l|c|c|c|c|c|c|c}
\toprule[.15em]
 \multirow{2}{*}{Method} & CVD2014  & LIVE-Qualcomm & KoNViD-1k & LIVE-VQC & YouTube-UGC& LBVD& LIVE-YT-Gaming\\
  & SRCC/PLCC & SRCC/PLCC  & SRCC/PLCC & SRCC/PLCC & SRCC/PLCC & SRCC/PLCC & SRCC/PLCC  \\
\hline
\rowcolor{tablegray}
\multicolumn{8}{l}{\textit{Competing Methods}} \\
VSFA  & 0.756/0.760 & 0.604/0.633 &  0.810/0.811 & 0.753/0.795  & 0.718/0.721 & 0.713/0.678  & 0.669/0.698   \\
Li22  & 0.817/0.811 &  0.732/0.744& 0.843/0.835  & 0.793/0.811  & 0.802/0.792 & \textbf{0.788}/\textbf{0.769}  & 0.690/0.740  \\
FAST-VQA  & 0.805/0.814 & \textbf{0.737}/\textbf{0.749} & 0.859/0.856  & \textbf{0.822}/\textbf{0.845}  &  0.730/0.747& 0.734/0.715 & 0.620/0.666   \\
\hline
\rowcolor{tablegray}
\multicolumn{8}{l}{\textit{Proposed Methods}} \\
Model (\rom{4})  & 0.780/0.802 & 0.533/0.590 & 0.826/0.820  & 0.749/0.789  & 0.802/0.806 &  0.674/0.661 & 0.656/0.717   \\
Model (\rom{9})  & \textbf{0.858}/\textbf{0.867} & 0.635/0.699 & \textbf{0.889}/\textbf{0.887}  &  0.798/0.837 & \textbf{0.826}/\textbf{0.833} & 0.670/0.655  & \textbf{0.711}/\textbf{0.755}  \\
\bottomrule[.15em]
\end{tabular}
}
\end{table*}

\subsection{Auxiliary Computational Analysis}\label{subsec:cde}
To support our analysis results, we continue to examine the second criterion that defines the easy dataset problem by testing the generalization of BVQA models trained on LSVQ to the other seven datasets. The competing models include VSFA, Li22, FAST-VQA, Model (\rom{4}) and Model (\rom{9}). 
The experimental results are listed in Table \ref{performance_cross_dataset_evaluation}, from which we make similar observations. First, trained on the large-scale LSVQ dominated by the spatial distortions, Models (\rom{4}), and (\rom{9}), even equipped with the temporal quality analyzer, generalize poorly to LBVD and LIVE-Qualcomm, characterized by the temporal distortions. Combining with the intra-dataset
results, we reinforce the claim that the spatial and temporal distortions in LIVE-Qualcomm are highly correlated.
In the intra-dataset setting, such correlation can be effectively exploited. But this is not the case in the cross-dataset setting; the models may have to capture the temporal distortions to enable generalization on LIVE-Qualcomm, and this capability is less likely to be learned on LSVQ.
Second, for CVD2014, KoNViD-1k, and YouTube-UGC, both Model (\rom{4}) and Model (\rom{9}) generalize reasonably, and a more powerful spatial quality analyzer gives better performance, pointing out the severity of the easy dataset problem. 
Third, for LIVE-VQC whose problem is more severe than LBVD but less severe than KoNViD-1k, Model (\rom{4}) performs marginally, and Model (\rom{9}) performs moderately, aligning with the intra-dataset results.
Fourth, although LIVE-YT-Gaming is analyzed to be spatial-distortion-dominant, the model generalizability is weaker compared to that of CVD2014, KoNViD-1k, and Youtube-UGC. We attribute this to the domain gap between gaming and natural videos, which can not be ignored in the cross-dataset setting. 

\section{Further Ablation Studies}
In the previous section, we have identified the easy dataset problem occurred in existing VQA datasets. In this section, we accompany our main computational analysis with comprehensive ablation experiments, investigating additional BVQA design choices pertaining to the basic building blocks. These include 1) the spatial resolution and temporal downsampling rate in the video preprocessor, 2) the backbone, initialization, and feature representation in the spatial quality analyzer, and 3) the feature aggregation and score pooling in the quality regressor. Meanwhile, we ablate the loss function as the key part of the optimization technique.  We report the results of Model (\rom{1}) on LSVQ unless otherwise specified.

% Figure environment removed


% Figure environment removed

\subsection{Video Preprocessor}

\noindent\textbf{Spatial Resolution for Key Frames}. 
The spatial resolution, $L_s$, in our video preprocessor determines how aggressively we choose to downsample each key frame. In our default settings, $L_s = 448$ for the ResNet-50, and $L_s = 384$ for the Swin Transformer-B. Here we make other choices: $\{224,384, 448,512\}$, whose results are shown in Fig. \ref{fig:resolution_lsvq_test} and \ref{fig:resolution_lsvq_test_1080p}.  We find that the performance is fairly stable for different spatial resolutions,  suggesting that the spatial distortions presented in current VQA datasets are not sensitive to spatial downsampling. Although operating at full resolution may be advantageous (by comparing Li22~\cite{li2022blindly} to Models (\rom{4}) to (\rom{6})), it is computationally prohibitive in the case of end-to-end training.


\noindent\textbf{Temporal Sampling Rate for Key Frames}.
Natural videos are highly redundant in the temporal dimension. We reduce the temporal sampling rate to $R_a=1$ fps, from which we extract quality-aware spatial features. Here, we explore a set of alternative choices: $\{0.5,1,2\}$, and illustrate the results in Fig. \ref{fig:sampling_lsvq_test} and \ref{fig:sampling_lsvq_test_1080p}. When the temporal sampling rate is extremely low (\eg, $0.5$ fps), the performance drops considerably. Increasing the temporal sampling rate from $1$ to $2$ fps has little impact on quality prediction, but doubles the computation. Overall, $R_a = 1$ is a decent choice for  BVQA models to work well on existing VQA datasets.

\noindent\textbf{Temporal Sampling Rate for Video Chunks}.
We explore the impact of the temporal sampling rate, $R_b$, for video chunks. Specifically, we sample $R_b$ from $\{1/3, 1/2, 1\}$ under Model (\rom{4}), whose results are listed in Fig. \ref{fig:samplint_rate_video_chunk}. We observe that setting $R_b$ to slightly lower values induces little performance variations. Altogether, $R_a$ and $R_b$ give a rough but convenient characterization of the temporal content and distortion complexity. A good VQA dataset shall not permit the setting of too low $R_a$ and $R_b$ (\eg, that used in our minimalistic BVQA model family), which corresponds to very sparsely implemented temporal analysis.



\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying of different backbones of the spatial quality analyzer under Model (\rom{1})}
\label{performance_backbone_lsvq}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{cc|cc|cc}
\toprule[.15em]
 \multirow{2}{*}{Backbone}& \multirow{2}{*}{Size} & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
  & & SRCC  & PLCC & SRCC & PLCC  \\
\hline
\multirow{3}{*}{ResNet}& 50&  0.819         & 0.821  & 0.679  &  0.734      \\
& 200&   0.816   &  0.817 &  0.662 &   0.727 \\
& \textit{Gain}&   \textcolor{gray}{-0.4\%}   &  \textcolor{gray}{-0.5\%} &  \textcolor{gray}{-2.5\%} &   \textcolor{gray}{-1.0\%} \\
\hline
\multirow{3}{*}{ViT}&Tiny &    0.815      & 0.819 &  0.685 &   0.744      \\
& Base&     0.848     & 0.849 & 0.718  &   0.769     \\
&\textit{Gain} &    \textcolor{gray}{+4.0\%}      & \textcolor{gray}{+3.7\%} &  \textcolor{gray}{+4.8\%} &   \textcolor{gray}{+3.5\%}     \\
\hline
\multirow{3}{*}{ConvNeXt}&Tiny &    0.833      & 0.835 & 0.712  &    0.764    \\
&Base &     0.843     & 0.844 & 0.720  &   0.769     \\
&\textit{Gain} &     \textcolor{gray}{+1.2\%}     & \textcolor{gray}{+1.1\%} & \textcolor{gray}{+1.1\%}  &    \textcolor{gray}{+0.7\%}    \\
\hline
\multirow{3}{*}{\makecell[c]{Swin \\Transformer}}& Tiny&   \textbf{0.842}       & \textbf{0.844} & \textbf{0.718}  &   \textbf{0.766}     \\
&Base &      \textbf{0.852}    & \textbf{0.854} & \textbf{0.736}  &   \textbf{0.780}     \\
& \textit{Gain}&       \textcolor{gray}{+1.2\%}   & \textcolor{gray}{+1.2\%} & \textcolor{gray}{+2.5\%}  &    \textcolor{gray}{+1.8\%}    \\
\bottomrule[.15em]
\end{tabular}
% }
\end{table}

\subsection{Spatial Quality Analyzer}

\noindent\textbf{Backbone}.
We test four backbone networks, each with two sizes, to probe the impact of different network architectures and model sizes on  BVQA. 
The first are ResNet-50~\cite{he2016deep}, ViT-Tiny~\cite{dosovitskiy2020image}, ConvNeXt-tiny~\cite{liu2022convnet}, and Swin Transformer-Tiny~\cite{liu2021swin} with FLOPS\footnote{The FLOPS of backbones are calculated using a single frame with the spatial resolution of $224\times224$.} around $4.5\times10^9$. The second are ResNet-200, ViT-Base, ConvNeXt-Base, and Swin Transformer-Base with FLOPS around $15.0 \times 10^9$. The spatial resolution for key frames is set to $L_s = 224$ for all backbones, and other configurations are the same as in Model (\rom{1}).

The results are listed in Table \ref{performance_backbone_lsvq}, from which we make two useful observations. First, except for ResNets, the performance improves as the FLOPS of the backbones increase, by an average of $2.2\%$. ResNet-200 gives slightly worse results than ResNet-50, showing that scaling up ResNets for the BVQA task is more challenging. 
Second, the performance of more advanced backbones, such as Swin Transformers, is better than standard DNNs with convolution (\eg, ConvNeXt). 


\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying different initializations under models without the temporal quality analyzer}
\label{performance_initilization_lsvq}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{cc|cc|cc}
\toprule[.15em]
 \multirow{2}{*}{Backbone}&\multirow{2}{*}{Initialization} & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
  & & SRCC  & PLCC & SRCC & PLCC  \\
\hline
\multirow{5}{*}{ResNet-50}&ImageNet-1k&0.819&0.821&0.679&0.734\\
&MoCoV2&0.825&0.828&0.697&0.758\\
&\textit{Gain}&\textcolor{gray}{+0.7\%}&\textcolor{gray}{+0.9\%}&\textcolor{gray}{+2.7\%}&\textcolor{gray}{+3.3\%}\\
&CLIP&0.842&0.846&0.731&0.769\\
&\textit{Gain}&\textcolor{gray}{2.8\%}&\textcolor{gray}{+3.0\%}&\textcolor{gray}{+7.7\%}&\textcolor{gray}{+4.8\%}\\
\hline
\multirow{5}{*}{ViT-B}&ImageNet-1k&0.848&0.849&0.718&0.770\\
&MAE&0.846&0.849&0.730&0.770\\
&\textit{Gain}&\textcolor{gray}{-0.2\%}&\textcolor{gray}{+0\%}&\textcolor{gray}{+1.7\%}&\textcolor{gray}{+0\%}\\
&CLIP&0.863&0.865&0.751&0.802\\
&\textit{Gain}&\textcolor{gray}{+1.8\%}&\textcolor{gray}{+1.9\%}&\textcolor{gray}{+4.6\%}&\textcolor{gray}{+4.2\%}\\
\hline
\multirow{3}{*}{\makecell[c]{Swin \\Transformer-B}}&ImageNet-1k&0.852&0.854&0.736&0.780\\
&ImageNet-22k&0.857&0.860&0.741&0.793\\
&\textit{Gain}&\textcolor{gray}{+0.6\%}&\textcolor{gray}{+0.7\%}&\textcolor{gray}{+0.7\%}&\textcolor{gray}{+1.7\%}\\
\bottomrule[.15em]
\end{tabular}
}
\end{table}




\noindent\textbf{Initialization}.\label{subsubsec:oi}
We have investigated the effectiveness of quality-aware initializations for  BVQA. Here, we further explore other possibilities: 1) initializations from pre-training on larger computer vision datasets (\eg, ImageNet-22k) and 2) initializations from unsupervised pre-training, including MoCo~\cite{he2020momentum}, MAE \cite{he2022masked}, and CLIP \cite{radford2021learning}. Specifically, we initialize ResNet-50 with weights supplied by MoCoV2 and CLIP, ViT-B by MAE and CLIP, and Swin Transformer-B by ImageNet-22k. The spatial resolution is set to $224$, while keeping other configurations the same as in Model (\rom{1}). 

The results are given in Table \ref{performance_initilization_lsvq}, from which we have several interesting observations. First, switching from  ImageNet-1k to ImageNet-22k leads to about $1\%$ improvement on LSVQ in terms of SRCC. Second,
 we find surprisingly that MoCoV2 as a kind of contrastive learning method improves BVQA compared with its counterpart pre-trained on ImageNet-1K. MoCoV2 tries to identify different positive pairs derived from one image by a bag of data augmentation methods, some of which are quality non-preserving (\eg, color jittering and color-to-gray conversion).  We conjecture that these augmentations  as visual distortions are not common in LSVQ, and thus may not hurt the quality prediction performance as measured on LSVQ.  MAE attempts to reconstruct the masked patches of an image as a way of encouraging semantic-related distortion-unaware features, hence not beneficial to the BVQA task.
CLIP, as a visual-language pre-training method, consistently improves  BVQA, which well aligns with the recent studies\cite{wang2023exploring,wu2023exploring} that exploit CLIP features for BVQA  without any fine-tuning.



\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying different feature extraction strategies under Model (\rom{1}). AVG and STD stand for global average and standard deviation pooling, respectively. ``+'' means feature concatenation. Single-stage and multi-stage indicate that the feature maps are extracted from the last stage and the last three stages of ResNet-50, respectively}
\label{performance_statistics_features_resnet50_lsvq}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{cc|cc|cc}
\toprule[.15em]
\multicolumn{2}{c|}{\multirow{2}{*}{Feature Extraction}} & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
 & & SRCC  & PLCC & SRCC & PLCC  \\
\hline
\multirow{2}{*}{Single-stage}& AVG &   0.833  & 0.833  & 0.711  & 0.761   \\
% &STD &    0.833  & 0.833  & 0.703  & 0.755 \\
&AVG+STD &   0.831   & 0.834  & 0.693  & 0.752  \\
\hline
\multirow{2}{*}{Multi-stage}&AVG&0.838&0.840&0.712&0.761\\
&AVG+STD&\textbf{0.839}&\textbf{0.841}&\textbf{0.720}&\textbf{0.766}\\
\bottomrule[.15em]
\end{tabular}
% }
\end{table}

\noindent\textbf{Feature Extraction Strategy}.
The global average pooling of the last-stage feature maps is the most common in BVQA. Nevertheless, other options exist, such as 1) the global average and standard deviation pooling \cite{li2019quality} to deal with  non-uniformity of  spatial distortions and 2) multi-stage feature  fusion to enrich the feature expressiveness. Here, we further explore these two more involved feature extraction strategies, and show the results in Table \ref{performance_statistics_features_resnet50_lsvq}. Unlike previous studies~\cite{li2019quality,li2022blindly}, we do not observe significant improvements compared to the  global average pooling of the last-stage feature maps. We believe this arises because of the end-to-end training, which encourages better quality-aware features to be learned without additional feature engineering.

\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying different feature aggregation methods under Model (\rom{1})}
\label{performance_regressor_resnet50_lsvq}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule[.15em]
\multirow{2}{*}{Regressor}  & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
   & SRCC  & PLCC & SRCC & PLCC  \\
\hline
MLP &      0.833  & 0.833 &  0.711 & 0.761    \\
GRU &  0.831    & 0.829 &  0.687 & 0.746  \\
Transformer &   \textbf{0.841}   & \textbf{0.841} & \textbf{0.717}  & \textbf{0.765}  \\
\bottomrule[.15em]
\end{tabular}
% }
\end{table}

\subsection{Quality Regressor}
\noindent\textbf{Temporal Feature Aggregation}.
In our minimalistic BVQA model family, we adopt an FC layer to regress local quality scores without temporal feature aggregation. We may as well employ sequential models such as GRU \cite{cho2014properties}, and the Transformer \cite{vaswani2017attention} to capture long-term dependencies among key frames or video chunks under Model (\rom{1}). The results are listed in Table \ref{performance_regressor_resnet50_lsvq}, from which we observe that FC outperforms GRU. The Transformer achieves the best performance, but the improvement relative to FC is marginal, indicating that long-term dependencies are relatively weak in LSVQ.


\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying different temporal pooling strategies under Model (\rom{1})}
\label{performance_quality_pooling_resnet50_lsvq}
% \resizebox{0.48\textwidth}{!}{
\begin{tabular}{c|cc|cc}
\toprule[.15em]
\multirow{2}{*}{Pooling Strategy}  & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
  & SRCC  & PLCC & SRCC & PLCC  \\
\hline
Simple average &         0.833  & 0.833  &  0.711 & \textbf{0.761}       \\
Temporal hysteresis &   0.826   & 0.825  &  0.689 & 0.738   \\
Learning-based &   \textbf{0.837}   & \textbf{0.835}  &  \textbf{0.715} & 0.757   \\
\bottomrule[.15em]
\end{tabular}
% }
\end{table}

\noindent\textbf{Temporal Pooling}.
The temporal pooling merges frame-level or chunk-level quality scores into a video-level quality score. Representative strategies include simple average pooling, worst-quality-based pooling \cite{park2012video}, temporal variation pooling \cite{ninassi2009considering}, and temporal hysteresis pooling \cite{seshadrinathan2011temporal,li2019quality}. Considering that only differentiable pooling methods can be integrated for end-to-end optimization, we employ three kinds of pooling strategies under Model (\rom{1}), which are the simple average pooling, the temporal hysteresis pooling introduced in \cite{li2019quality}, and the learning-based pooling using a 1D convolution layer (with a kernel size of $5$). The results are listed in Table \ref{performance_quality_pooling_resnet50_lsvq}. We find that the temporal hysteresis pooling and the learning-based pooling are no better than the simple average pooling. This shows that the videos included in LSVQ are not capable of giving credit to BVQA methods with explicit temporal modeling.





\begin{table}
\centering
\renewcommand{\arraystretch}{1.25}
\caption{Performance on the LSVQ test sets by varying different loss functions under Model (\rom{1})}
\label{performance_loss_function_resnet50_lsvq}
\begin{tabular}{c|cc|cc}
\toprule[.15em]
\multirow{2}{*}{Loss} & \multicolumn{2}{c|}{LSVQ} & \multicolumn{2}{c}{LSVQ-1080p}  \\
  & SRCC  & PLCC & SRCC & PLCC  \\
\hline
L1 &   0.825   & 0.825  &  0.693 & 0.746   \\
L2 &    0.822  & 0.823 &  0.686 & 0.741 \\
PLCC &   \textbf{0.833}  & \textbf{0.833}  &  \textbf{0.711} & \textbf{0.761} \\
SRCC &   0.833  & 0.825  &  0.697 & 0.749  \\
\bottomrule[.15em]
\end{tabular}
\end{table}




\subsection{Loss Function}
BVQA is commonly formulated as a regression problem, optimized for MAE or MSE as the loss function. Here we compare MAE and MSE with the default PLCC. We also include a differentiable version of SRCC~\cite{li2022blindly} into comparison in Table \ref{performance_loss_function_resnet50_lsvq}. It is clear that PLCC performs the best, especially on LSVQ-1080p, which justifies its use in optimizing our minimalistic BVQA model family. Nevertheless, BVQA models optimized by PLCC suffer from scale ambiguity, meaning that the results stay unchanged if we scale the model prediction $\hat{q}$ by a positive scalar $\alpha$ to $\alpha\hat{q}$. Thus, same as using PLCC for evaluation~\cite{antkowiak2000final}, to obtain more interpretable quality predictions, a  monotonic logistic function that maps model predictions to MOSs can be fitted.

In summary, by ablating more BVQA design choices on LSVQ, we arrive at similar observations in support of the main computational analysis: the BVQA performance benefits from more advanced spatial quality analysis, but not from temporal quality analysis.

\section{Discussion}
To reliably measure the progress of BVQA and to develop better BVQA methods, it is crucial to construct next-generation VQA datasets that overcome the easy dataset problem. Here we propose to go back to the two key steps in subjective VQA: sample selection and subjective testing.

From the sample selection perspective, the selected videos should be \textit{diverse} in terms of content and distortion types and \textit{difficult} in terms of preventing simple close-to-BIQA solutions and falsifying existing BVQA models. To promote diversity, Vonikakis \etal~\cite{vonikakis2016shaping} described an elegant computational procedure that is mathematically grounded. The tricky part lies in the specification of video attributes, which should be efficiently computed and truly reflect how the human visual system perceives these attributes (especially those related to time-varying video quality). To promote difficulty, Wang \etal~\cite{wang2021troubleshooting,wang2021active} proposed to leverage the group maximum differentiation (gMAD) competition~\cite{ma2018group} to let a group of BIQA methods compete with one another on a large-scale unlabeled image dataset. In a similar spirit, we may select a group of BVQA models (including those from our minimalistic BVQA model family) to participate in the gMAD competition on a large-scale unlabeled video dataset. The resulting video subset has great potential to falsify the competing BVQA model. Alternatively, we may borrow ideas from the active learning literature, and co-train an auxiliary failure prediction module~\cite{cao2022image} (along with the main BVQA model) to assess the sample difficulty on the unlabeled video dataset. In general, sample selection is an NP-hard problem~\cite{davis1997adaptive} unless the diversity and difficulty measures have special structures to be
exploited. Popular approximate solutions
include greedy algorithms and convex relaxation methods.


From the subjective testing perspective, instead of relying on the absolute category rating that gives a single scalar to indicate the perceptual quality of the test video, we may resort to single stimulus continuous quality rating~\cite{series2012methodology}, in which the test video is rated continuously over time, giving rise to a quality curve over time. Such quality annotations force BVQA models to analyze temporal quality as a direct means of preventing close-to-BIQA solutions. However, the continuous quality rating has notorious drawbacks regarding rating efficiency and reliability. Following this line of thought,  we may formulate different variants of BVQA problems that emphasize temporal quality assessment: 1) localization of worst and best quality regions in space and time, and  2) two-alternative forced choice between two non-overlapping video chunks with better perceived quality.



\section{Conclusion}
We have conducted a computational analysis of eight VQA datasets by designing minimalistic BVQA models, which perform aggressive spatiotemporal downsampling as preprocessing and rely on as little temporal quality analysis and aggregation as possible. Despite being minimalistic,  our models achieve very competitive quality prediction performance compared to existing BVQA methods. We view the results not as the advancement of BVQA, but as the underlying problem of existing VQA datasets---the easy dataset problem. We empirically rank the eight VQA datasets according to the severity of the easy dataset problem, which is supported by the cross-dataset evaluation results and ablation experiments. We hope our computational studies shed light on how to construct next-generation VQA datasets and models in a closed loop.





% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% % you can choose not to have a title for an appendix
% % if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% % use section* for acknowledgment
% \ifCLASSOPTIONcompsoc
%   % The Computer Society usually uses the plural form
%   \section*{Acknowledgments}
% \else
%   % regular IEEE prefers the singular form
%   \section*{Acknowledgment}
% \fi


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}

% \bibitem{IEEEhowto:kopka}
% H.~Kopka and P.~W. Daly, \emph{A Guide to {\LaTeX}}, 3rd~ed.\hskip 1em plus
%   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}


\bibliographystyle{IEEEtran}
\bibliography{bare_adv}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{% Figure removed}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% % if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% % insert where needed to balance the two columns on the last page with
% % biographies
% %\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


