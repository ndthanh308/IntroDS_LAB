\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[a4paper, total={7in, 9.5in}]{geometry}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{balance}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).


\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref} %% for review


% \usepackage[breaklinks=true,bookmarks=false]{hyperref} %% for final


\newcommand{\addDDC}[1]{\textcolor{red}{#1}}
% put all the external documents here!


\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\pagestyle{empty}
\date{08 March 2023}
\begin{document}

%%%%%%%%% TITLE
\title{\Huge Implicit neural representation for change detection}

\author{Peter Naylor\\
RIKEN AIP\\
Kyoto, Japan\\
{\tt\small peter.naylor@riken.jp}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Diego Di Carlo\\
RIKEN AIP\\
Kyoto, Japan\\
{\tt\small diego.dicarlo@riken.jp}
\and
Arianna Traviglia\\
Istituto Italiano di Tecnologia\\
Venice, Italy\\
{\tt\small arianna.traviglia@iit.it}
\and
Makoto Yamada\\
OIST\\
Okinawa, Japan\\
{\tt\small makoto.yamada@oist.jp}
\and
Marco Fiorucci\\
Istituto Italiano di Tecnologia\\
Venice, Italy\\
{\tt\small marco.fiorucci@iit.it}
}

\maketitle
% Remove page # from the first page of camera-ready.
\thispagestyle{empty}



%%%%%%%%% ABSTRACT
% new method for change detection
% Lidar point clouds
% Unsupervised method
% Use of recent trends with PINNS
% NeRF
% Scales to million of points
% 
% Beat SOTA by a large margin
% 

\newcommand{\NFacr}{NF}
\newcommand{\NFfull}{Neural Field}

\begin{abstract}
Detecting changes that occurred in a pair of 3D airborne LiDAR point clouds, acquired at two different times over the same geographical area, is a challenging task because of unmatching spatial supports and acquisition system noise. 
Most recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data unavailable in real-world applications. 
To address these issues, we propose an unsupervised approach that comprises two components: \NFfull{} (\NFacr) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes.
\NFacr{} offer a grid-agnostic representation to encode bi-temporal point clouds with unmatched spatial support that can be regularised to increase high-frequency details and reduce noise. 
The reconstructions at each timestamp are compared at arbitrary spatial scales, leading to a significant increase in detection capabilities.
We apply our method to a benchmark dataset of simulated LiDAR point clouds for urban sprawling.
The dataset offers different challenging scenarios with different resolutions, input modalities and noise levels, allowing a multi-scenario comparison of our method with the current state-of-the-art.
We boast the previous methods on this dataset by a $10\%$ margin in intersection over union metric.
In addition, we apply our methods to a real-world scenario to identify illegal excavation (looting) of archaeological sites and confirm that they match findings from field experts.

% We propose an unsupervised approach for point cloud data that is scalable, customisable and effective.
% The method comprises two components: \NFfull{} (\NFacr) for continuous shape reconstruction and a Gaussian Mixture Model for categorising changes.
% We equip the \NFac with feature mapping methods to capture high-frequency signals and regularisation to unify the bi-temporal cloud points for unsupervised change detection, increasing the detection capabilities.
% To better understand, we perform many ablation studies to investigate the contribution of the feature mapping and the regularisations.
 % We present a new method for change detection in 3D LiDAR cloud point data.
%    We use recent advances in deep neural networks, from Implicit Function Learning and PINNS, for surface estimation to tackle unsupervised change detection in urban data.
   
% Our best-performing method, IMPlicit nEural RepreSentatiON chAnge deTEction (IMPERSONATE),  uses a single network, random Fourier features, discrete-time difference regularisation and total variational norm over the spatial coordinates.
   % We boast the previous state of the art by a $10\%$ margin in intersection over union metric.
   
% Moreover, our proposed method scales to millions of points and enables precise detection of added and removed elements in contrast to the previous methods.
   % In addition, we apply our methods to identify the illegal excavation (looting) of cultural heritage sites and check that they match findings from field experts.

%Detecting changes that occurred in a pair of 3D LiDAR point clouds, acquired at two different times (bi-temporal) over the same geographical area, is a challenging task because of unmatching supports. Most of the recent attempts to detect changes on point clouds are based on supervised methods, which require large labelled data that are often not available in real-world applications. In this paper, we propose  

\end{abstract}
 
%%%%%%%%% BODY TEXT
\section{Introduction}

% Figure environment removed

% Abundance of data in EO
Nowadays, we observe the Earth through various sensors at unprecedented spatial and temporal resolutions. 
Thanks to these measurements named Earth Observation data, we can monitor and expand our current knowledge about our planet \cite{salcedo2020machine}.
EO data are mainly captured through remote sensing systems mounted on satellites and flying devices, and terabytes of data are acquired daily.
LiDAR data has risen in popularity with applications in autonomous driving \cite{Li2020}, robotics \cite{nubert2022}, urban sprawling \cite{de2021change}, and cultural heritage \cite{verschoof_2022_thesis}. 
This increase is threefold.
Firstly, LiDAR data are highly precise (the spatial resolution is under 1m) in recording a 3D environment.
Secondly, LiDAR acquisition systems are insensitive to lighting conditions.
Finally, LiDAR can map terrain and uncover structures hidden by canopies. 
LiDAR data presents itself as point cloud data, shown in Fig.~\ref{fig:lidar}, and can contain millions of points depending on the sensor resolution and scanned area.
% LiDAR has instead become the mainstay of remote sensing archaeological reconnaissance \cite{Chase2017_LiDAR} because LiDAR acquisition systems are insensitive to lighting conditions, can map terrain, and uncover structures hidden by canopies  \cite{hanssen2021utilizing}.

In this context, Change Detection (CD) aims to identify relevant differences in altitude on a pair of LiDAR point clouds at two different times (bi-temporal) over the same geographical area, shown in Fig.~\ref{fig:lidar}.
CD is an active field of research because it plays an important role in monitoring global world changes and human activities, such as disasters, deforestation, urban sprawling, and human pillaging \cite{Asokan2019ESI, Shi2020_RS}.
This paper tackles CD for monitoring urban sprawling and identifying cultural heritage looting. 
Monitoring archaeological looting is paramount as looting leads to damage, misplacement or complete loss of valuable archaeological artefacts.
While airborne LiDAR has become the mainstay of remote sensing archaeological reconnaissance \cite{Chase2017_LiDAR, fiorucci_2022, verschoof_2022_thesis}, to the best of our knowledge, this is the first time it is applied to looting detection.
Urban sprawling has been monitored by detecting both newly built and demolished buildings occurring in multi-temporal LiDAR point clouds 
which is essential to help landscape and city managers to promote sustainable development.
This task has been widely explored in remote sensing and computer vision communities in the last decades \cite{de2021change}. 



%LiDAR acquisition systems have not yet been used to identify illegal excavation of archaeological sites to collect historical material culture (looting).
% To the best author knowledge, this is the first time that
% LiDAR acquisition systems is used to identify illegal excavation of archaeological sites to collect historical material culture (looting)
%it has not been applied yet to looting detection.

%LiDAR has instead become the mainstay of remote sensing archaeological reconnaissance \cite{Chase2017_LiDAR} because it is insensitive to lighting conditions, and vegetation \cite{hanssenAS2021utilizing}.
%Their usage is also paired with advances in machine learning methods, that allow to automatise certains task such as the identification of archaeological structure \cite{verschoof_2022_thesis}. 
%An increasing interest in the automatic recognition of hidden (buried or covered by vegetation) archaeological structures has led to widespread use of the Digital Elevation Model (DEM) \cite{banasiak_2022, fiorucci_2022, guyot_2021, somrak_2020}.
%DEM transform the raw LiDARS point clouds to a regular grid of interpolated height measurements \cite{Stular_2012}.

Most existing methods process data defined on discrete regular grids and require supervised training. 
Feature extraction methods and Deep Neural Networks (DNN) have been both applied to CD for urban sprawling, \cite{Zhang2018ChangeDB, Kharroubi2022, de2021benchmarking, Zhang2019}, achieving state-of-the-art performances.
However, these methods require a preprocessing step to project the raw 3D LiDAR point cloud onto a 2D regular grid. 
The most widespread projection methods are the Digital Elevation Model (DEM) \cite{OKYAY2019}, and Digital Surface Model (DSM) \cite{Erdogan2019, Lyu2020, Zhang2019}.
These models are therefore limited by the correctness of the projection and memory complexity that scale with the desired resolution, making these approaches inefficient for large point clouds.
In contrast, we investigate the application of CD directly to the off-grid large raw 3D LiDAR point clouds in an unsupervised manner which is particularly important for real-world applications.
% larger memory induced by the mapping onto a regular grid.

% Analysing and processing large amounts of off-grid data for unsupervised CD is more complex and we address this particular challenge in this work.

% Our proposed approach for CD consists in the two following steps, by recasting the problem into shape reconstuction and ...
% At first, we propose a new method in the field on \NFacr, also known as Implicit Neural Representation~\cite{xie2022neural}, complemented by results from physical-informed neural networks~\cite{raissi2019physics} for surface reconstruction. 
% Moreover, we add a CD reguraliser over time in the DNN shape reconstruction loss to improve its detection capabilities.


We develop a novel grid-agnostic unsupervised method in a two-step approach for CD based on recent advances in DNN.
In the first step, we propose to use \NFacr, also known as Implicit Neural Representation \cite{xie2022neural} for surface reconstruction. 
We propose to model the changes by assuming a sparse modification of the learned field over time. 
The total variation loss \cite{bleakley2011group} regularises and improves detection capabilities in any scenario.
In the second step, we then categorise the change in altitude given by the reconstruction of the surfaces at both timestamps.
We apply our method to two EO datasets.
The first is an open simulated airborne LiDAR dataset consisting of 15 unique points clouds, where the goal is to identify newly built and demolished buildings.
The underlying task of this objective is a tool to understand the landscape better and help city managers promote sustainable development.
The second dataset is a pair point cloud over Kulen, an area of Cambodia with several archaeological structures. 
We aim to detect precise locations of illegal excavation sites to collect historical material culture, i.e. looting. 

% , driven by their outstanding results for image interpolation~\cite{tancik2020fourier,sitzmann2019siren}, surface reconstruction~\cite{huang2022surface}, view interpolation~\cite{mildenhall2020nerf}, and physical-informed neural networks~\cite{raissi2019physics} for surface reconstruction.


% In this work, we focus on directly processing 3D Lidar point clouds to detect changes in bi-temporal pairs of point clouds acquired at two consecutive times over the same geographical area. 

% DEM suffers from drawbacks, notably an information loss related to the height of the objects.
% The same trend is observed in the building change detection literature, where most of the works are tailored for 2D images obtained by interpolating 3D point clouds to obtain a Digital Surface Model (DSM) \cite{Erdogan2019, Lyu2020, Zhang2019} 
% The projection of 3D point clouds is particularly harmful when the number of data points is low. 
% In addition, projecting the data onto a regular grid implies increasing the data memory size by an important factor, especially for 3D images that scale cubically.



%, it has not been applied yet to looting detection. The identification of looting activities is mainly performed on Earth Observation (EO) data, like synthetic aperture radar3, and in any case, their application is rather limited. Notwithstanding EO data can be effectively used for monitoring looting in critical areas such as conflict zones and deserts3, they are not suited to identify looting in areas with a thick vegetation coverage. Airborne LiDAR is instead the best single remote sensing method for penetrating landscapes covered by continuous vegetation due to its unique ability of recording 3D topography with a high degree of accuracy, typically 10–15cm in both plan and height, proving the opportunity to assess looters’ damages by detecting the depth of looting pits.

% processing the data, different types, why we should work on point cloud
%Analysing and processing such large amounts of data is not straightforward compared to data lying on a grid.
%Previous methods transform the point cloud data to fit existing tools. 
%The recent advances and achievements of the Deep Neural Network (DNN) for images pushed many works to project the point cloud onto 2D or 3D grids \textcolor{red}{CITATIONS}.
%However, with these projections, precision is lost, and this is particularly harmful when the number of data points is low.
%In addition, projecting the data onto a regular grid implies increasing the data memory size by an important factor, especially for 3D images that scale cubically.
%There has been a high demand for neural networks applied directly to point clouds for many tasks such as classification, segmentation, 3D surface reconstruction and change detection.


%In the present work, we focus on processing point cloud data for urban planning and cultural heritage applications.
The contributions of our work are the following:
\begin{itemize}
   \itemsep0em 
   \item Applying DNN for unsupervised CD from 3D LiDAR;
   \item Using unsupervised sparse regularisation over time for CD in the shape reconstruction loss;
   % \item Unsupervised benchmark for change detection on an open simulated airborne LiDAR dataset;
   \item Outperforming state-of-the-art by more than $10\%$;
   \item Applying CD for looting monitoring activities with field experts' feedback.
\end{itemize}

\section{Related works}
% Different data modalities -> point cloud
%The acquisition method imposes a number of natural difficulties in detecting change over time.
%Voxels, meshes, and points-clouds are common storing structures to render 3D information. 
%Due to the grid's regularity, ordering and consistency, Voxels are ideally suited for convolutional operations.
% Hence, 2D DNN can be applied without loss of generality to voxel data \textcolor{red}{CITATION}.
% 3D DNNs suffer from two drawbacks that limit their use in practice.
% The induced memory footprint of 3D data scales cubically and slows training and inference.
% The resolution is fixed and is given before training, compared to meshes and cloud points.
% Meshes have been widely used in the community as they are fast to render, adaptive, efficient to texture and intuitively deformable for modelling, editing reconstruction and shape analysis.
% Mesh data is light and does not, a priori, define a resolution.
% However, meshes suffer from irregularity, lack of ordering and orientation, and grid inconsistency, leading to challenging models \textcolor{red}{CITATION}.
% Point cloud analysis is the most basic format and the lightest data structure.
% Because of those advantages, many works have efficiently processed cloud points to translate them to meshes or voxels. 
% They have also solved more demanding tasks such as reconstruction, generation, segmentation and classification \textcolor{red}{CITATION}.
%Point clouds suffer from similar drawbacks to meshes.

The off-grid and noisy nature of point clouds collected by a LiDAR system detrimentally impacts the performances of change detection methods.
Moreover, weather conditions and remote sensor trajectory may differ for two measurements at different timestamps, leading to spatial unmatching support.
Despite this, supervised methods for CD can achieve great performances even in challenging noisy data. 
However, their usage is still limited to a few real-world applications due to the prohibitive cost of collecting and labelling training datasets.

For these reasons, unsupervised methods represent an attractive alternative. 
Such methods can be broadly classified into two categories~\cite{de2021change}: those based on distance computation \cite{girardeau2005change,shirowzhan2019comparative}, and those based on optimal transport \cite{courty2016optimal, fiorucci2023optimal}.
Distance-based methods, such as C2C \cite{girardeau2005change}, and M3C2 \cite{lague2013accurate,shirowzhan2019comparative} divide the point cloud via octrees, estimate the surface normal and orientation to calculate pair-wise euclidean distances.
Alternatively, optimal transport-based methods estimate a distance based on the projection matrix of the first cloud onto the second.
In both cases, the actual changes are then classified via empirical thresholding, or the OTSU method \cite{otsu1979threshold}.
These methods have been developed and applied to the only available airborne LiDAR dataset for building change detection \cite{de2021change}. 
However, datasets usually contain millions of points because they are acquired with high-spatial resolution acquisition systems \cite{Schwarz_2010}. 
The previous methods do not scale well with the data size and have to subdivide the point cloud for analysis. 


Other methods for unsupervised CD do not use raw 3D LiDAR data directly.
Instead, they used 2D images obtained by projecting the 3D point clouds on a 2D regular grid, e.g., for DEM and DSM \cite{Stular_2012}. 
Due to the grid's regularity, ordering and consistency, these projected 2D images are ideally suited for convolutional operations. 
Hence, convolutional neural network-based architectures can be applied to the 2D digital models for building CD \cite{Zhang2018ChangeDB, Zhang2019, Shi2020_RS}.
However, these projections lead to precision loss of the LiDAR data as the height measurements are interpolated to output the DSM or the DEM \cite{de2021change}. 
To the best of our knowledge, the work we presented here is the first study investigating the use of DNNs for unsupervised CD from raw point cloud data.

% ML applied to LIDAR cloud point analysis and reconstruction
Novel DNN models, called \NFacr s~\cite{xie2022neural}, have tackled off-grid point cloud analysis \cite{qi2017pointnet, Kharroubi2022} and 3D surface reconstruction~\cite{tancik2020fourier,sitzmann2019siren,huang2022surface} in both supervided and unsupervide way.
The \NFacr s are \textit{coordinate}-based deep learning models that map coordinates to descriptive a field (e.g. occupancy, density or colour). 
The resulting function is called the Implicit Neural Representation of the field, which can be estimated by fitting observation and inducing desired properties by application-based regularisation terms.
% In contrast to classic DNNs models, the dataset comprises a single scene or object when applied to surface reconstruction.

% Two sets of methods exist and derive from the field of implicit function learning (IFL) \cite{xie2022neural}.
% Explicit functions aim to learn, for example, a reconstruction explicitly.
% Specifically, we learn a function $F$ such that $F(x,y) = z$ where $(x,y,z)\in\mathbb{R}^3$ are spatial coordinates.
% In contrast, implicit function aims at predicting a different task, which through ad hoc methods, can construct the surface from the initially learned task.
% For example, you can use $F$ to learn the occupancy of any given point in a shape, $F(x,y,z)=1$ if the spatial point is within the object and $0$ otherwise \cite{mescheder2019occupancy}.
% From $F$, you can then extract the surface by looking at the decision boundaries, i.e. the set $\mathcal{S}=\{p\in\mathbb{R}^3|\mathbb{P}(p) = 0.5\}$.
% \textcolor{red}{input to the \NFacr{} $F$ 
% In particualat,  occupancy-based network condition the input to a latent codes representing diffrence objects to enable the model to learn from multiple objects and derive object shape priors.}
% Recent studies push further in this direction by investigating a better shape encoder and decoder \cite{chibane2020implicit}.
% Recent studies push further in this direction by investigating how to reconstruct multiple object using the same \NFacr~\cite{chibane2020implicit}.
% These methods have worked efficiently in standard 2D image analysis, where multiple-resolution views of the processed data interact for the final prediction.


% Other considerable advances were laid by finding better projections for the input data. 
A native limitation of \NFacr{} is their poor capability of capturing high-frequency details of the surface, referred to as spectral bias~\cite{tancik2020fourier}.
To address this, positional encoding and Random Fourier Features (RFF)~\cite{rahimi2007random} of the input coordinates have become standard practice.
This allowed to apply \NFacr{} to a new plethora of applications and methodologies, such as 3D shape reconstruction from sparse images, animation of human bodies and faces, as well as video coding~\cite{xie2022neural}.
Moreover, RFF have improved Physics-Informerd Neural Networks (PINNs)~\cite{raissi2019physics}, which are similar coordinate-based DNNs trained to fit observations while solving partial differential equation evaluated with network's backpropagation algorithm~\cite{karniadakis2021physics,di2022post}. 


An alternative approach to surpass the frequency bias is SIREN~\cite{sitzmann2019siren}, where the standard nonlinearities are replaced with periodic sine functions. 
Such architectures show a very low reconstruction error, both in fitting the target field and its gradient with respect to the input. 
SIRENs have become standard backbone networks for both \NFacr{} and physics-informed neural networks~\cite{xie2022neural}.
Despite their success, SIRENs suffer from difficulties in training as they are prone to overfit or being stuck in local minima, for which careful parameter tuning is required.
These issues have been addressed in subsequent works~\cite{fathony2021multiplicative,lindell2022bacon}, where novel architectures have been proposed claiming easier training and faster convergence. 
Nevertheless, in practice, SIREN's performance depends on the applications. 
As reported in this work, SIREN does not consistently outperform RFF-based architecture.


Most of the \NFacr-based models for 3D reconstruction recover the object shape by minimising the signed distance between any given point and the closest surface~\cite{park2019deepsdf}.
Then, the associated \NFacr{} is a function of the three spatial coordinates.
A positive sign implies the point lies outside the object and vice versa.
The shape contours are then found by checking the $0$ iso-line of the learned function.
Neural Unsigned Distance Fields \cite{chibane2020neural} remove the sign from the distance function and encode continuous locations for a stronger regularisation, leading to better reconstruction.
However, these methods are inappropriate for CD with LiDAR data as they require normals vectors to reconstruct the 3D shape~\cite{huang2022surface}, which are unavailable in LiDAR data. Moreover, as we will discuss in Section~\ref{sec:method}, urban sprawling can be reasonably approximated by a continuous function over a 2D space.


\section{Method} \label{sec:method}
% This could go in the related section
We want to detect changes in two LiDAR point clouds with unsupervised methods.
A cloud point at time $t$ will be denoted by $\mathcal{X}_t \subset \mathbb{R}^3$ where each element is a 3D coordinate, i.e. $(x, y, z) \in \mathcal{X}_t$.
We will denote by $t_0=0$ and $t_1=1$ the two timestamps for which we wish to detect change.
If the support of $\mathcal{X}_0$ and $\mathcal{X}_1$ match, we naturally define the addition of an element by a positive difference, i.e. the 2D point $(x, y)$ is of the label "Addition" if the associated altitudes: $z_1 - z_0 > \alpha$, where $z_t$ corresponds to the altitude at time $t$ and with $\alpha$ a fixed scalar.
Similarly, we can define the "Deletion" class by a negative difference.
However, this is not directly applicable to LiDAR point clouds as the supports do not match because of different acquisition conditions.
To fix the support, some have used projection methods \cite{de2021change} and optimal transport \cite{fiorucci2023optimal}.
In the current paper, we fix the support matching by estimating a surface from the cloud point, allowing us to interpolate and query any spatial point.
In other words, we reconstruct the surface at a given time point.
In particular, we can then compute the difference to find additions or deletions.
It is important to note that we are estimating a function that maps a position to an altitude, which is different from the actual cloud point. 
Some points will share the same $(x,y)$ but have different altitudes due to the inclination of the LiDAR emitter and the verticality of elements in the maps, especially if these are buildings. 

\subsection{Regression model}
We denote by $f_{\theta}$ an MLP model with learnable parameters $\theta$.
Given an input vector $\mathbf{v}$, we estimate the density:
\begin{align*}
   f_\theta:V & \rightarrow \mathbb{R} \\
   \mathbf{v} & \mapsto  z
\end{align*}
where $V=\mathbb{R}^2$ in our first formulation.
Naturally, we can independently reconstruct the first and second cloud.
We sum the baseline method in the following detection formula:
\begin{equation}
 \Delta z(x, y) = (f_{\theta_1} - f_{\theta_0})(x,y).
\label{eq:loss2}
\end{equation}
Here $\theta_t$ corresponds to the set of DNN parameters to reconstruct the surface at time $t$ that are optimised by minimising the mean squared error (MSE) between the estimated and observed altitude. 

A more compact and efficient representation is possible where we modify the input $\mathbf{v}\in\mathbb{R}^3$ of the network to incorporate time. 
In particular, we learn a single model with parameters $\theta$, and the detection formula is modified to:
\begin{equation}
\Delta z(x, y) = f_{\theta}(x,y,t_1) - f_{\theta}(x,y, t_0) \label{eq:loss1}
\end{equation}
\textit{A priori}, it is not evident to know in advance which method is better suited for a given dataset.
In some configurations of the simulated datasets, $\mathcal{X}_0$ and $\mathcal{X}_1$ are not drawn from the same distribution, which could potentially be harmful to the single model.
Therefore we investigate the benefits of a single model as opposed to two.
In the following sections, we will describe in more detail the different regularisations and model specificities we apply to improve reconstruction.

\subsection{Random Fourier Features}

We map the input $\mathbf{v}$ to a higher dimensional space with RFF to help capture higher frequencies in the surface.
It has been shown that this projection is crucial for estimating high frequencies for a better reconstruction and that the gaussian mapping performs best \cite{tancik2020fourier}.
For a fixed model, RFF are defined as follows:
\begin{align*}
& B \in \mathbb{R}^{M\times 3}, \ \forall (i,j), B_{ij} \sim \mathcal{N}(0, \sigma)   \\
& \gamma_B(\mathbf{v}) = [\cos(2\pi B\mathbf{v}),~\sin(2\pi B\mathbf{v})] 
\end{align*}
The size of $B$ depends on the size of $\mathbf{v}$, and here we supposed $\mathbf{v} \in \mathbb{R}^3$.
The mapping size $M$ and the scale $\sigma$ are two hyper-parameters that need tuning.

\subsection{Network architecture}

% Figure environment removed

% Figure environment removed

We propose several network architectures comprising MLP layers, different activations, and skip layers in Table~\ref{tab:model-recap} of the Appendix. 
We show in Fig.~\ref{fig:model} a particular model that we name `skip-ten-only'.
In particular, the network size will depend on the data complexity.
We use skip layers allowing a better gradient flow \cite{he2016deep}.
The activation functions will be ReLu or hyperbolic tangent (tanh), as tanh allows the resulting  $f_{\theta}$ to be $\mathcal{C}^\infty$, we do not use any activation layers for the last layer.

We use a similar architecture to the one presented in Fig.~\ref{fig:model} for the SIREN methods. 
We use skip layers and tanh activation, including the final layer. 
We allow the final model to choose the architecture for both methods by making it a hyperparameter.



\subsection{Total variation norm}
The total variation norm, defined as $\sum |u_{i+1} - u_i|$ is a standard regularisation scheme for sequential like data \cite{bleakley2011group}.
It is also helpful to smooth spatial patterns, as a sudden altitude change should be penalised.
Such a scheme has been generalised to a continuous version by enforcing that the resulting gradient of the function $f_{\theta}$ be sparse over the spatial coordinates ~\cite{raissi2019physics}.
It is possible to add to the loss function the regularisation term $\mathcal{R}_{TV} = \lvert \frac{\partial f_{\theta}}{\partial x} \rvert + \lvert \frac{\partial f_{\theta}}{\partial y} \rvert$.

\subsection{Time difference}
Similarly to the discrete total variation norm, we can enforce that the change over time be sparse, which indicates to the DNN that most points do not change over time, but we will allow some to change. 
To enforce such a constraint, we add the following regularisation to the loss function: $\mathcal{R}_{TD} = \lvert f_{\theta}(x, y, t_1) - f_{\theta}(x, y, t_0) \rvert$.
This regularisation is only possible when the input $\mathbf{v}$ contains time, and therefore it cannot be applied where we reconstruct the surface for two models (\ref{eq:loss2}). 
This regularisation over time is very similar to the total variation norm over the temporal domain.
We stress that shape reconstruction does not need this regularisation term, and this is only a CD regulariser.
In particular, adding this term to the loss allows us to fuse the information from both cloud points more efficiently and enables the point clouds to benefit from each other mutually.

\subsection{SIREN}

From an architectural point of view, the SIREN network~\cite{sitzmann2019siren} is a simple modification of an MLP where standard activation functions, e.g. ReLU and tanh, are continuous sine functions.
This substitution enables the modelling of a continuous complicated signal without the need for explicit upsampling in various domains. The SIREN network can then be described by
\begin{equation}
\begin{split}
    f({\bf x}) &= \mathbf{W}_n (g_{n-1} \circ g_{n-2} \circ \ldots \circ g_0{\mathbf{x}}) + \mathbf{h}_n, \\
    & \text{where} \quad g_i(\mathbf{h}_i) = \sin(\mathbf{W}_i \mathbf{h}_i + \mathbf{h}_i).
\end{split}
\end{equation}
SIREN operates as a composition of sinusoidal transformations recalling the principle behind RFF. 
In fact, it has been shown that positional encoding with RFF is equivalent to periodic nonlinearities with one hidden layer as the first DNN  layer~\cite{benbarka2022seeing}.
Good initialisation of the weights is critical for their successful training.
To avoid saturation of the sine activations, a scalar hyperparameter $\sigma$ is introduced to scale the layers' weights, like for RFF.


\subsection{Unsupervised labelling of $\Delta z$}

Once the surface reconstruction is performed, we have access to $\Delta z$ given by equation (\ref{eq:loss2}) or (\ref{eq:loss1}).
The OTSU threshold method has been used to separate binary sources \cite{otsu1979threshold, naylor2018segmentation} for change detection.
However, in our case, we have three sources to distinguish and use a Gaussian Mixture Model (GMM) with three components \cite{mclachlan1988mixture}. 
We show in Fig.~\ref{fig:diffz} the application of GMM to $\Delta z$ for a small clipped sample.
This step can be tricky depending on the distribution of $\Delta z$ as the GMM will divide the distribution into three, regardless of the shape of the distribution.
It could potentially lead to a random score.



\section{Experimental setting}

\subsection{Simulated dataset}

We use the publicly simulated airborne LiDAR dataset for CD \cite{de2021change}.
Even if this dataset is simulated, it mimics true data with different noise levels and sensors used in practice.
Five simulation configurations are given: 1 - Low resolution -- low noise, 2 - high resolution -- low noise, 3 - low resolution -- high noise, 4 - photogrammetry and 5 - multi-sensor.
The photogrammetry setting is low resolution, high noise and tight scan angle for each timestamp. They mimic satellite acquisition.
The multi-sensor simulation is characterised by $pc_0$ and $pc_1$ having different resolutions and noise levels. 
$pc_0$ is low resolution and high noise, whereas $pc_1$ is high resolution and low noise. 
In this situation $\# pc_1 >> \# pc_0$ which is different to other subsets where $\# pc_1 \approx \# pc_0$.
$\#$ denotes the cardinality of the set.
Each configuration is divided into training and testing datasets.
We will only apply the method to the testing sets as the methods used are unsupervised.
For each testing configuration, three simulated datasets exist where the ground truth is different for each.
The testing set comprises three different geographical areas of the city of Lyon in France \cite{de2021change}.
Only the second cloud point $pc_1$ is annotated with additions and deletion changes and will be used to evaluate the methods.
In Fig. \ref{fig:pc1change}, we overlay $pc_1$ with the annotation.
%\subsection{Archeological data}
% The two-fold purpose of using LiDAR point clouds to identify looting activities is to validate INCANT on real (non-simulated) bi-temporal pairs of LiDAR point clouds and assess its capability to undermine looting, which is a pressing problem on a global scale. The dataset comprises two bi-temporal pairs of Lidar points clouds acquired over the Phnom Kulen region (Cambodia), where there are temples and ancient dams of Angkor-era largely obscured by thick and closed canopies, but the LiDAR deals well with this kind of environment thanks to    
% For each pair of point clouds ($pc_0$, $pc_1$), the first cloud was acqired in the $2012$ and the second cloud in the $2015$ using \textcolor{red}{to finish} 
 
\subsection{Metrics}
The minimised MSE used to cross-validate training will not be used for evaluation.
Due to the noise level, a good performance on this metric will not imply a good reconstruction.
Indeed an MSE of $ 0 $ implies that the model perfectly reconstructs the data and the noise.

\textbf{Intersection over union} uses predicted and true labels: $IoU(P, G) = \frac{P \cap G}{P \cup G}$.
This metric is very sensitive to small changes, especially when the ground truth is small, like in our situation. 
The $IoU$ will be measured after applying the GMM to $\Delta z$.
In particular, a low score could mean that the GMM is unfit for converting the differences into labels or that the surface reconstruction failed.

\textbf{Average AUC} will be computed to highlight good-performing methods irrespective of the GMM results.
We compute the standard AUC over three settings: addition vs no addition, deletion vs no deletion and change vs no change.
In the last setting, we use $\lvert \Delta z \rvert$.

% \paragraph{MSE} will be used to evaluate the efficiency of the reconstruction.
% This MSE metric will be computed between the reconstructed surface and the high resolution low noise dataset average at both time stamps, which is the data that is closest to the ground truth.
% During training and validation we minimise the MSE on the point cloud, however 
% When the level of noise is high, a very low MSE would imply that we overfit on the noise structure of the dataset which is not a good thing.
\subsection{Training procedure}
In detail, we will describe how we train our network $f_\theta$ given a point cloud.
When a single network is used, cloud points $pc_0$ and $pc_1$ are concatenated, and the input dimension is three.
With no loss of generality, we will consider that we have a single point cloud $pc$ of dimension two or three.
We normalise the point cloud to be in $[-1;1]$ on each axis.
We randomly split the dataset in two where $80\%$ is retained for training and the other $20\%$ is used for validation.
We minimise and backpropagate through the training loss and evaluate the MSE performance on the validation.
We use Optuna \cite{akiba2019optuna} to find the best set of hyperparameters via bayesian optimisation that minimises the validation MSE.
The tuned hyper-parameters are the model architecture, the learning rate, the batch size, the scale of the gaussian mapping, the scalars associated with the regularisation terms $\lambda_{TD}$, and $\lambda_{TV}$.
When we use the SIREN model, we also optimise the scale size by multiplying the signal in the sinusoidal activation function, the number of layers, and the number of hidden units for each layer.
We use the optimisation method Adam \cite{kingma2014adam} wrapped with the Layer-wise Adaptive Rate Scaling (LARS) \cite{torchlars} that enables the use of an enormous batch size that for us is essential to carry out our experiments in a reasonable time.
To speed up computation, we set the number of epochs to 50, use learning rate decay and early stopping.
To compute the TV norm, we sample random elements from $pc$ that we corrupt with noise and backpropagate their prediction to the input \cite{raissi2019physics}.

% Figure environment removed 

% Figure environment removed

\section{Results  and discussion}
\subsection{Feature mapping} \label{sec:featuremap}
% Introduction of the study
In Fig. \ref{fig:results_feature_mapping}, we show the \textit{IoU} results between the different mapping methods: no feature mapping, RFF and SIREN.
In Fig. \ref{fig:auc} of the Appendix, we show the results for the AUC metric.
In Fig. \ref{fig:crop_visualisation}, we show some resulting crops trained with no feature mapping, RFF and SIREN.
We show additional crops as well as the whole map for data (3) in Fig.\ref{fig:crop2_visualisation} and \ref{fig:whole_visualisation} of the Appendix.
% Observations of the shown results
For both metrics, \textit{IoU} and AUC, RFF outperforms SIREN and the default configuration by a fair margin. 
In terms of average performance with standard deviation, RFF reaches an \textit{IoU} of $0.53 \pm 0.12$  and an AUC of $0.976 \pm 7\times10^{-3}$ and, SIREN $0.39 \pm 0.10$ and  $0.959 \pm 2.8\times10^{-2}$, and not using any mapping reaches $0.30 \pm 0.15$  and $0.959 \pm 3.8\times 10^{-2}$.
Please check Table~\ref{fig:full-results} in the Appendix for the deconvolution of the values reported.
From the visualisation in Fig. \ref{fig:crop_visualisation}, no feature mapping leads to a reconstruction where the building delimitation is unclear and fuzzy.
SIREN gives the sharpest reconstruction with close to no noise between the buildings.
Conversely, SIREN's projection onto the support of the second timestamp, given in the final column, is subject to many false positives along building boundaries.
The RFF method produces distinctive buildings, like SIREN, but with a noisier output.
However, the number of false positives in the final row is smaller.
% Discussion
This ablation study shows the necessity of feature mapping to achieve good \textit{IoU} and, therefore, a good reconstruction.
Capturing high-frequencies is essential to our current problem due to the verticality of buildings.


% Figure environment removed

 
\subsection{Hyper-parameter influence}
%Introduction of the study
In Fig. \ref{fig:hyper}, we show a study on the regularisation parameter $\lambda_{TD}$ and $\lambda_\text{TVN}$ for both RFF, in darker and SIREN, in lighter colours.
We measure the \textit{IoU} and RMSE for both and compare them to the setting without penalty.
% Observation of the graphs and crops
Naturally, a too-strong penalty damages the performance, and a too-low value leads will render the penalisation negligible.
Only for the method using RFF and TD regularisation do we see a $6\%$ improvement in \textit{IoU} compared to the baseline. 
This optimal $\lambda_{TD}$ does not necessarily correspond to a minimal RMSE, metric used for the validation scheme.
Similarly to Section \ref{sec:featuremap}, RFF features obtain better performance on both metrics and a more stable reconstruction noticeable by a lower RMSE and smaller confidence intervals.
% Discussion

The literature reports better reconstruction for SIREN over the RFF methods.
However, in our situation, SIREN gives lower performances in terms of \textit{IoU} and MSE.
SIREN suffers more from the mathematical formulation given in Section \ref{sec:method}, which is ill-posed because of many points on the sides of the buildings.
As SIREN induces qualitatively a better reconstruction, i.e. sharper edges and less noise. 
Having samples from the point cloud sharing similar geographical coordinates but radically different altitudes (along the verticality of the building) leads SIREN to 
slighly misplace the bundaries of the buildings, leading to many false positives along their edges, and hence a lower \textit{IoU}.
% wrongly estimate the correct boundaries of the building as shown by false positive along the boundaries of buildings, and hence a lower \textit{IoU}.
%Similarly, SIREN should have a lower MSE value.
SIREN's MSE is higher, as the MSE penalises false positives more strongly as they correspond to larger differences between the ground truth and the prediction.
The sharper edges of SIREN, compared to RFF, are harmful in terms of \textit{IoU} and MSE.
In other words, SIREN's reconstruction is sharper and, due to noise, wrongly estimates the building size. 
In contrast, the RFF method has softer edges, i.e. less overfitting induced by capturing fewer high-frequencies. This leads to a better mean error along the building edges and, therefore, to a lower minimisation of the MSE and fewer false positives.


\subsection{Comparison to state of the art}
% introdcution
In Table \ref{tab:results}, we benchmark methods on the simulated dataset and compare the previous state-of-the-art in unsupervised detection, M3C2 \cite{lague2013accurate}, OT \cite{fiorucci2023optimal} and SIREN to our method. 
% Blunt observation
RFF outperforms the other methods by a large margin, about $13\%$ and $8\%$  in \textit{IoU} over the previous state of the art.
In addition, the previous state-of-the-art maximised the \textit{IoU} with respect to a threshold, whereas our method is completely unsupervised.
For example, SIREN is still an improvement over the previous state-of-the-art even though the results do not show this because the metric report for SIREN is fairer than those reported.

The experiments favour the use of one single function for the reconstruction of the surface for both timestamps.
We notice a difference of $2\%$ in \textit{IoU} between using a single model and two models when no regularisation is applied. 

The regularisation $\mathcal{R}_{TD}$ shows the most improvement in terms of \textit{IoU} compared to $\mathcal{R}_{TV}$ with a difference of over $3\%$.
Expectedly, the best-achieving model uses both, which should perform at least as well as the non-regularised version.
In practice, it could disregard any regularisations by setting the respective scalar to $0$ during the cross-validation.
In the following, we use the model with one single model, RFF and both regularisation, named model `S+RFF+TVN+TD'.

\begin{table*}[t]
    \begin{tabular}{c|cc|c|c|cccccc}
    \toprule
    \multirow{2}{*}{Data} &  \multirow{2}{*}{M3C2 \cite{lague2013accurate}}    &    \multirow{2}{*}{OT \cite{fiorucci2023optimal}}          & None  & SIREN & \multicolumn{6}{c}{RFF}  \\ \cmidrule{4-11} % \midrule
                    &  &  & S+TVN & S   & D & D+TVN & S & S+TVN     & S+TD    & S+TVN+TD \\ \toprule
    (1)             & 0.2987                   & 0.4065                            & 0.3722                    & 0.4014                    & 0.5013                & 0.4468                    & 0.5249                & {\color[HTML]{C9211E} 0.5587} & 0.5473                        & 0.4968                         \\
    (2)             & 0.5373                   & {\color[HTML]{000000} 0.552}      & 0.4522                    & 0.5398                    & 0.5739                & 0.5933                    & 0.5645                & {\color[HTML]{C9211E} 0.6117} & 0.6034                        & 0.5952                         \\
    (3)             & 0.3872                   & 0.3926                            & 0.3311                    & 0.3857                    & 0.4654                & 0.4317                    & 0.5187                & 0.4694                        & {\color[HTML]{C9211E} 0.5400} & 0.5333                         \\
    (4)             & 0.3501                   & 0.3989                            & 0.3354                    & 0.3901                    & 0.4862                & 0.497                     & 0.5138                & 0.5110                        & 0.5340                        & {\color[HTML]{C9211E} 0.5399}  \\
    (5)             & 0.3778                   & {\color[HTML]{C9211E} 0.4817}     & 0.3797                    & 0.4048                    & 0.4255                & 0.4316                    & 0.4295                & 0.4326                        & 0.4055                        & 0.4717  \\ \midrule
    Avg      & 0.3902                   & 0.4463                            & 0.3741                    & 0.4243                    & 0.4904                & 0.4800                    & 0.5102                & 0.5167                        & 0.5260                        & {\color[HTML]{C9211E} 0.5274}   \\ \bottomrule
    \end{tabular}
    \caption{Comparaison to state-of-the-art on the $IoU$ metric, we only report the best configuration when no feature mapping and SIREN. We show in red the best performing model in each row. D denotes the model with two DNN given by equation (\ref{eq:loss2}) and S the model with a single DNN given by equation (\ref{eq:loss1}). The complete table can be found in Table \ref{fig:full-results} of the Appendix.}
    \label{tab:results}
\end{table*}

\subsection{Application to Cultural heritage}
The two-fold purpose of using LiDAR point clouds to identify looting activities is to validate model `S+RFF+TVN+TD' on real (non-simulated) bi-temporal pairs of LiDAR point clouds and assess its capability to detect looting, which is a pressing problem on a global scale. 
We processed one bi-temporal pair of Lidar points clouds acquired over the Phnom Kulen region (Cambodia), where there are temples and ancient dams of Angkor-era largely obscured by thick and closed canopies. 
Still, LiDAR deals well with this environment thanks to its capability to penetrate landscapes covered by continuous vegetation. 
The first cloud $pc_0$ was acquired in $2012$, and the second cloud $pc_1$ in $2015$. 
Fig.~\ref{fig:looting} shows the changes related to looting detected by model `S+RFF+TVN+TD'. 
Archaeologists drew the red bounding box to identify an area where looting occurred. 
The archaeologists verified the predicted changes through visual inspection and confirmed that all the looting pits inside the bounding box were correctly identified. 
The false positive in the top right part can be easily filtered due to the $20$ meters diameter, which is too big to be considered a looting pit.

% Figure environment removed
 


% \subsection{PINNS} 
% \textcolor{red}{TODO and move}
% Inspired by PINNS, we would like to regularise our predictive function.
% In particular, we believe that a sparse gradient over $t$ is a desirable property.
% Due to the uneven natural terrain (hills, rivers, ..) and man-made terrain (buildings) we do not want sparsity of the localisation parameters gradients nor do we want to restrict their magnitude.
% We do however want to allow for sharp spikes over the time parameter, i.e. allowing for changes due to built and demolished buildings.
% we experiment with the two following penalisation term:
% \begin{align*}
%    R_{eg} (\theta) & = \bigg|\frac{\partial f_{\theta}}{\partial t} \bigg|_1 \\
% \end{align*}

\section{Conclusion}
The amount of Earth observation acquired with 3D LiDAR data is rising exponentially, which opens up the possibility of monitoring human activity through change detection algorithms.
In particular, we focused on urban planning and identifying looting activities.
Thanks to advances in DNN, we can now estimate and reconstruct large areas with high precision that allows passing the reconstructed surfaces to downstream tasks.
However, the amount of training data and their discrete modelling limits their application to real scenarios.
To address these issues, we propose a novel unsupervised grid-agnostic scheme for change detection based on surface reconstruction and clustering, which achieves $0.5274\%$ \textit{IoU}, surpassing previous state-of-the-art by $10\%$ on average.
Moreover, we demonstrated in this paper that RRF mapping outperforms SIREN for change detection on point cloud data acquired from LiDAR and allows us to identify looting activity correctly.

% The amount of 3D LiDAR data acquired is rising exponentially, and with it, the need for fully unsupervised methods for change detection.


\vspace{-0.3cm}\section*{Code availability}
% Parts of this section are removed for anonymity, but the code used will be publicly available in the final version.
The code is made fully available at the following URL: \href{https://github.com/PeterJackNaylor/NN-4-change-detection/}{NN-4-change-detection}
The code runs efficiently with hyperparameter selection thanks to the Optuna \cite{akiba2019optuna} package. We use GPU computation with PyTorch \cite{NEURIPS2019_9015} and combine each experiment into a pipeline with Nextflow \cite{di2017nextflow} for reproducibility.

\section*{Aknowledgement}
% This section is removed for anonymity.
MY was supported by MEXT KAKENHI 20H04243 and partly supported by MEXT KAKENHI 21H04874. MF was supported by the European Union's Horizon 2020 research and innovation programme under grant agreement No 101027956. We thankful Damian Evans for providing us with a pair of LiDAR point clouds for looting identification. We are also thankful for the RAIDEN computing system and its support team at the RIKEN AIP, which we used for our experiments.



\clearpage
{\small
\balance
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}
\clearpage

\renewcommand{\thesection}{A.\arabic{section}}
\renewcommand{\thetable}{A.\arabic{table}}
\renewcommand{\thefigure}{A.\arabic{figure}}
\setcounter{figure}{0} 
\setcounter{section}{0} 
\setcounter{table}{0} 

\onecolumn

\vspace{1cm}
\begin{center}
    \Huge Supplementary: \\ Implicit neural representation for change detection
\end{center}
\vspace{1cm}

We show in the Supplementary Material results of all the methods applieds to each data configuration in Table~\ref{fig:full-results}.
We summarise all the possible tables chosen that we optimise from in Table~\ref{tab:model-recap}.
In addition, we give the AUC plot between RFF, SIREN and no feature mapping in Fig.~\ref{fig:auc}.
Finally, we show an additional output crop from the training on data configuration (3) in Fig.~\ref{fig:whole_visualisation} and the whole map in Fig.~\ref{fig:crop2_visualisation}.

\begin{table*}[htbp]
   \small
   \centering
   \subfloat[AUC for no feature mapping]{%
     \resizebox{0.47\linewidth}{!}{
      \begin{tabular}{c|cccccc}
         \toprule
         Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
         \midrule
         (1)  &  0.9012 &      0.8861 &  0.9699 &       0.9200 &           0.9664 &      0.9547 \\
         (2)  &  0.9721 &      0.9710 &  0.9868 &       0.9839 &           0.9882 &      0.9868 \\
         (3)  &  0.9104 &      0.8507 &  0.9553 &       0.9572 &           0.9600 &      0.9640 \\
         (4)  &  0.8993 &      0.8734 &  0.9563 &       0.9571 &           0.8296 &      0.9612 \\
         (5)  &  0.9306 &      0.9287 &  0.9780 &       0.9773 &           0.8652 &      0.9742 \\
         Avg  &  0.9227 &      0.9020 &  0.9692 &       0.9591 &           0.9219 &      0.9682 \\
         \bottomrule
      \end{tabular}}
   }
   \subfloat[IoU for no feature mapping]{%
     \resizebox{0.47\linewidth}{!}{
     \begin{tabular}{c|cccccc}
      \toprule
      Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
      \midrule
      (1)  &  0.2373 &      0.2365 &  0.2114 &       0.1936 &           0.3526 &      0.3722 \\
      (2)  &  0.3824 &      0.3722 &  0.4672 &       0.4512 &           0.4872 &      0.4522 \\
      (3)  &  0.2627 &      0.2076 &  0.3175 &       0.3031 &           0.3149 &      0.3311 \\
      (4)  &  0.2124 &      0.2053 &  0.3519 &       0.3234 &           0.2323 &      0.3354 \\
      (5)  &  0.2035 &      0.1992 &  0.3132 &       0.2635 &           0.1724 &      0.3797 \\
      Avg  &  0.2597 &      0.2442 &  0.3322 &       0.3069 &           0.3119 &      0.3741 \\
      \bottomrule
      \end{tabular}}
   }
   \hspace{0.5cm}%
   \subfloat[AUC for RFF]{%
     \resizebox{0.47\linewidth}{!}{
      \begin{tabular}{c|cccccc}
         \toprule
         Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
         \midrule
         (1)  &  0.9811 &      0.9810 &  0.9818 &       0.9804 &           0.9858 &      0.9843 \\
         (2)  &  0.9818 &      0.9819 &  0.9834 &       0.9809 &           0.9867 &      0.9826 \\
         (3)  &  0.9762 &      0.9776 &  0.9748 &       0.9743 &           0.9796 &      0.9805 \\
         (4)  &  0.9763 &      0.9759 &  0.9762 &       0.9766 &           0.9793 &      0.9814 \\
         (5)  &  0.9754 &      0.9792 &  0.9764 &       0.9683 &           0.9773 &      0.9804 \\
         Avg  &  0.9782 &      0.9791 &  0.9785 &       0.9761 &           0.9818 &      0.9818 \\
         \bottomrule
         \end{tabular}}
   }
   \subfloat[IoU for RFF]{%
     \resizebox{0.47\linewidth}{!}{
      \begin{tabular}{c|cccccc}
         \toprule
         Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
         \midrule
         (1)  &  0.5012 &      0.4467 &  0.5249 &       0.5473 &           0.4967 &      0.5586 \\
         (2)  &  0.5738 &      0.5932 &  0.5645 &       0.6033 &           0.5952 &      0.6116 \\
         (3)  &  0.4654 &      0.4316 &  0.5187 &       0.5399 &           0.5333 &      0.4694 \\
         (4)  &  0.4862 &      0.4969 &  0.5138 &       0.5340 &           0.5399 &      0.5110 \\
         (5)  &  0.4255 &      0.4315 &  0.4295 &       0.4055 &           0.4717 &      0.4325 \\
         Avg  &  0.4904 &      0.4800 &  0.5103 &       0.5260 &           0.5274 &      0.5166 \\
         \bottomrule
      \end{tabular}}
   }
   \hspace{0.5cm}%
   \subfloat[AUC for SIREN]{%
     \resizebox{0.47\linewidth}{!}{
      \begin{tabular}{c|cccccc}
         \toprule
         Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
         \midrule
         (1)  &  0.9780 &      0.9745 &  0.9774 &       0.9749 &           0.9777 &      0.9804 \\
         (2)  &  0.9813 &      0.9779 &  0.9844 &       0.9824 &           0.9793 &      0.9830 \\
         (3)  &  0.9702 &      0.9745 &  0.9755 &       0.9641 &           0.9660 &      0.9627 \\
         (4)  &  0.9716 &      0.9795 &  0.9699 &       0.9748 &           0.9740 &      0.9721 \\
         (5)  &  0.9759 &      0.9774 &  0.9754 &       0.9326 &           0.9748 &      0.9737 \\
         Avg  &  0.9754 &      0.9768 &  0.9765 &       0.9658 &           0.9744 &      0.9744 \\
         \bottomrule
         \end{tabular}}
   }
   \subfloat[IoU for SIREN]{%
     \resizebox{0.47\linewidth}{!}{
     \begin{tabular}{c|cccccc}
      \toprule
      Data    &  D &  D+TVN &  S &  S+TD &  S+TD+TVN &  TVN \\
      \midrule
      (1)  &  0.4147 &      0.3845 &  0.4014 &       0.3812 &           0.3599 &      0.4307 \\
      (2)  &  0.5255 &      0.4440 &  0.5397 &       0.4908 &           0.4395 &      0.4843 \\
      (3)  &  0.4084 &      0.3993 &  0.3857 &       0.4291 &           0.3723 &      0.3836 \\
      (4)  &  0.3777 &      0.3833 &  0.3901 &       0.3725 &           0.4013 &      0.4107 \\
      (5)  &  0.3865 &      0.3796 &  0.4047 &       0.2957 &           0.3744 &      0.3553 \\
      Avg  &  0.4226 &      0.3981 &  0.4243 &       0.3939 &           0.3895 &      0.4129 \\
      \bottomrule
      \end{tabular}}
   }
   \caption{Performance each data configuration for AUC and IoU. D denotes the model with two DNN given by equation (\textcolor{red}{1}) and S the model with a single DNN given by equation (\textcolor{red}{2}).}
   \label{fig:full-results}
 \end{table*}

\begin{table*}[b]
   \centering
   \resizebox{\textwidth}{!}{
   \begin{tabular}{c|c|c|c|c|c|c|c|c}
   \multicolumn{9}{c}{Models}                                                                                                                                            \\ \toprule
   default & default-BN  & default-L& skip-double           & skip-L-double         & skip-XL-double         & skip-ten             & skip-ten-only         & skip-twenty               \\ \midrule
   \multicolumn{9}{c}{Input $\gamma(\mathbf{v})$} \\ \midrule
   \multicolumn{2}{c|}{ } & FC-1024 &                      & FCS-1024 $(\times 2)$ & FCS-1024 $(\times 4$) & \multicolumn{3}{c}{ }                                           \\ \midrule
   \multicolumn{2}{c|}{ } & FC-512  & FCS-512 $(\times 2)$ & FCS-512 $(\times 2)$  & FCS-512 $(\times 4)$  & FCS-512 $(\times 10)$ & \multicolumn{2}{c}{ }                      \\ \midrule
   FC-256  & FC-256 + BN  & FC-256  & FCS-256 $(\times 2)$ & FCS-256 $(\times 2)$  & FCS-256 $(\times 3)$  & FCS-256               & FCS-256 $(\times 10)$ & FCS-256 $(\times 20)$ \\ \midrule
   FC-128  & FC-128 + BN  & FC-128  & FCS-128              & FCS-128 $(\times 2)$  & FCS-128 $(\times 2)$  & FCS-128               &                       & FCS-128 $(\times 2)$   \\ \midrule
   FC-64   & FC-64 + BN   & FC-64   & FCS-64               & FCS-64 $(\times 2)$   & FCS-64 $(\times 2)$   & FCS-64                &                       & FCS-64 $(\times 2)$    \\ \bottomrule 
   \multicolumn{9}{c}{Linear mapping to a 1 dimensional output} \\
   \bottomrule                                                                                               
   \end{tabular}
   }
\caption{Neural Network models. FC denotes fully connected layers with a given activation. BN-denotes batch normalisation. FCS denotes fully connected layers with a skip layer. For the transition when downsampling the dimension for fully connected skip layers, we add a simple FC that maps from one dimension to the other.}
\label{tab:model-recap}
\end{table*}


% \clearpage
% \newpage

% Figure environment removed

% Figure environment removed

% Figure environment removed


\end{document}