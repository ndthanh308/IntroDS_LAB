\section{Proofs}
\subsection{Proof of Theorem~\ref{thm:method:cls-guidance}}\label{app:proof-thm}
\begin{proof}
    Let $\alpha$ and $(1-\alpha)$ be the prior probabilities of positive and negative examples under $p_\theta(\rvx_t; t)$. Note that $\alpha$ remains independent of $t$ because
    \begin{align*}
        \alpha & = \int p_\theta(\rvx_t; t) p_\theta(\rvx_0 | \rvx_t) p_\theta(y=1 | \rvx_0)\, d\rvx_0 \,d\rvx_t
        = \int p_\theta(\rvx_t; t) p_\theta(\rvx_0 | \rvx_t) \oracle(\rvx_0)\, d\rvx_0 \,d\rvx_t                 \\
               & = \int p_\theta(\rvx_0, \rvx_t; t) \oracle(\rvx_0) \,d\rvx_0 d\rvx_t
        = \int p_\theta(\rvx_0; 0) \oracle(\rvx_0) \,d\rvx_0.
    \end{align*}
    The objective in \cref{eq:cls-time-cross-entropy} is equal to
    \begin{multline}
        -\meanp{t}{\meanp{p_\theta(\rvx_t)}{\meanp{p_\theta(\rvx_0 | \rvx_t)}{\oracle(\rvx_0)} \log \cls_\phi(\rvx_t; t) + \meanp{p_\theta(\rvx_0 | \rvx_t)}{(1 - \oracle(\rvx_0))} \log (1 - \cls_\phi(\rvx_t; t))}}\\
        = -\meanp{t}{\meanp{p_\theta(\rvx_t)}{p(y=1|\rvx_t) \log \cls_\phi(\rvx_t; t) + p(y=0|\rvx_t) \log (1 - \cls_\phi(\rvx_t; t))}}
    \end{multline}
    This is equivalent to \cref{eq:cls-cross-entropy} after replacing $q$ with $p_\theta$, i.e. sampling from the reverse process instead of the forward. Therefore, its optimal solution follows \cref{eq:cls-optimal}. Hence,
    \begin{align*}
            & \ \nabla_{\rvx_t} \log p_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log \cls_{\phi^*}(\rvx_t; t)                                                              \\
        =\  & \nabla_{\rvx_t} \log p_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log \alpha p_\theta(\rvx | y=1; t)                                                          \\
            & \hspace{3cm} - \nabla_{\rvx_t} \log \Big[ \overbrace{\alpha p_\theta(\rvx_t|y=1; t) + (1 - \alpha) p_\theta(\rvx_t|y=0; t)}^{p_\theta(\rvx_t; t)} \Big] \\
        =\  & \nabla_{\rvx_t} \log \alpha p_\theta(\rvx_t | y=1; t) = \nabla_{\rvx_t} \log p_\theta(\rvx_t | y=1; t)
    \end{align*}
\end{proof}

\subsection{Proof of Corollary~\ref{thm:method:improved-model}}\label{app:proof-corr}

\begin{proof}
    Since $p_{\theta, \phi^*}$ is defined as the distribution generated by simulating the SDE in \eqref{eq:reverse-process}, its score function $\nabla_{\rvx_t} \log p_{\theta, \phi^*}(\rvx_t; t)$ is by definition equal to $s_{\theta, \phi^*}(\rvx_t; t)$ \citep{risken1996fokker, song2019generative}. Similarly for the baseline DM we have $s_{\theta}(\rvx_t; t) = \nabla_{\rvx_t} p_\theta(\rvx_t; t)$.
    Therefore,
    \begin{align}
        \nabla_{\rvx_t} \log p_{\theta, \phi^*}(\rvx_t; t)
         & = s_{\theta, \phi^*}(\rvx_t; t)
        \overset{\text{\cref{eq:gen-neg-score}}}{=\joinrel=} s_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log C_{\phi^*}(\rvx_t; t) \\
         & = \nabla_{\rvx_t} \log p_{\theta}(\rvx_t; t) + \nabla_{\rvx_t} \log C_{\phi^*}(\rvx_t; t)
        \overset{\text{Thm. \ref{thm:method:cls-guidance}}}{=\joinrel=} \nabla_{\rvx_t} \log p_{\theta}(\rvx_t | y=1)
    \end{align}
    %Since the score functions of the two distributions are equal, the distributions themselves will be equal too and it concludes the proof for the first item.
    Here we derived $s_{\theta, \phi^*}(\rvx_t; t) = \nabla_{\mathbf{x}_t} \log p_\theta(\rvx_t | y=1)$. By~\citep{anderson1982reverse}, we proved the first statement.

    The second statement  follows by decomposing $p_{\theta}(\rvx_t | y=1)$:
    \begin{align*}
        p_{\theta, \phi^*}(\rvx) = p_\theta(\rvx | y=1) \propto p_\theta(\rvx) \oracle(\rvx)\quad
        \Rightarrow \quad p_{\theta,\phi^*}(\rvx) = 0\quad \forall \rvx \in \Omega^\complement.
    \end{align*}
    For the last statement, we have
    \begin{align*}
                                                        & \left.\begin{matrix}
                                                                    p_{\theta, \phi^*}(\rvx) = \frac{p_{\theta}(\rvx) \oracle(\rvx)}{\int p_{\theta}(\rvx) \oracle(\rvx) \,d \rvx} \\[1.1em]
                                                                    \int p_{\theta}(\rvx) \oracle(\rvx) \,d \rvx \leq 1                                                            \\
                                                                \end{matrix}\right\}
        \Rightarrow p_{\theta, \phi^*}(\rvx) \geq p_{\theta}(\rvx)\quad \forall \rvx \in \Omega                                                                                                                                                 \\
        \overset{\gD \subseteq \Omega}{\Longrightarrow} & \log p_{\theta, \phi^*}(\gD)
        = \sum_{\rvx \in \gD} \log p_{\theta, \phi^*}(\rvx)
        \geq \sum_{\rvx \in \gD} \log p_{\theta}(\rvx) = \log p_{\theta}(\gD).
    \end{align*}
\end{proof}

% Figure environment removed

We demonstrate this corollary with a one-dimension density in \cref{fig:one-dimensional}. We show a ``base distribution'' $p(x)$ and the positive and negative regions $\Omega$ and $\Omega^\complement$, respectively. We can see that the distribution $p(x|y=1)$ assigns no mass to $\Omega^\complement$ and has a larger mass assigned to any point in $\Omega$.


\subsection{Equivalence of cross-entropy loss minimization and KL divergence minimization}\label{app:bayes-optimal-kl}
\begin{claim}
    minimizing the cross-entropy loss between the classifier output and the true label is equivalent to minimizing the KL divergence between the classifier output and the Bayes optimal classifier.
\end{claim}

\begin{proof}
    The Bayes optimal classifier $C^*(\rvx_t; t)$ approximates $q(y = 1 | \rvx_t)$. Let $p_\phi(y | \rvx_t)$ be the distribution represented by the learned classifier $C_\phi(\rvx_t; t)$ i.e., $p_\phi(y=1|\rvx_t) = C_\phi(\rvx_t; t)$. For an arbitrary diffusion time step $t$, the expected KL divergence between the Bayes optimal and the learned classifier therefore is
    \begin{align}
          & \meanp{q(\rvx_t)}{\kl{q(y | \rvx_t)}{p_\phi(y | \rvx_t)}}\nonumber                                                                               \\
        = & \ \meanp{q(\rvx_t)}{\meanp{q(y | \rvx_t)}{q(y|\rvx_t)\log\frac{q(y|\rvx_t)}{p_\phi(y | \rvx_t)}}}                                                \\
        = & \ \meanp{q(\rvx_t)}{q(y=1|\rvx_t)\log\frac{q(y=1|\rvx_t)}{C_\phi(\rvx_t; t)} + q(y=0|\rvx_t) \log\frac{q(y=0|\rvx_t)}{1 - C_\phi(\rvx_t; t)}}    \\
        = & \ \meanp{q(\rvx_t)}{H(q(y | \rvx_t))} - \meanp{q(\rvx_t)}{q(y=1 | \rvx_t) \log C_\phi(\rvx_t; t) + q(y=0 | \rvx_t)\log (1 - C_\phi(\rvx_t; t))}.
    \end{align}
    The first term is the expected entropy of the optimal classifier and is independent of $\phi$. Therefore,
    \begin{align}
          & \argmin_\phi \meanp{q(\rvx_t)}{\kl{q(y | \rvx_t)}{p_\phi(y | \rvx_t)}}\nonumber                                          \\
        = & \ \argmin_\phi - \meanp{q(\rvx_t)}{q(y=1 | \rvx_t) \log C_\phi(\rvx_t; t) + q(y=0 | \rvx_t)\log (1 - C_\phi(\rvx_t; t))} \\
        = & \ \argmin_\phi \text{CE}(q(y|\rvx_t), p_\phi(y|\rvx_t)).
    \end{align}
\end{proof}
Note that \cref{eq:cls-cross-entropy} is the expected cross entropy for different time steps $t$.


\subsection{Connection between Eq.~(\ref{eq:cls-time-cross-entropy}) and Eq.~(\ref{eq:true-objective})}
In this section we make the connection between \cref{eq:cls-cross-entropy} and \method{}'s objective (\cref{eq:true-objective}) more clear. The objective in \cref{eq:cls-time-cross-entropy}, ignoring the outer expectation with respect to $t$, is equal to

\begin{align}
            & - \Big(\meanp{p_\theta(\rvx_0, \rvx_t)}{\gO(\rvx_0)\log \cls_\phi(\rvx_t;t)+ (1 - \gO(\rvx_0))\log(1 - \cls_\phi(\rvx_t; t)}\Big)                                                                       \\
    =       & \ -\textcolor{blue}{\int p_\theta(\rvx_0, \rvx_t)} \Big( \textcolor{blue}{p(y=1|\rvx_0)}\log\cls_\phi(\rvx_t; t) + \textcolor{blue}{p(y=0 | \rvx_0)}\log(1 - \cls_\phi(\rvx_t;t)) \Big)\,d\rvx_0d\rvx_t \\
    %%
    =       & \ -\int \textcolor{blue}{p_\theta(y=1) p_\theta(x_0 | y=1) p_\theta(x_t | x_0)} \log\cls_\phi(\rvx; t)\,d\rvx_0d\rvx_t \nonumber                                                                                          \\
            & \ +\int \textcolor{blue}{p_\theta(y=0) p_\theta(x_0 | y=0) p_\theta(x_t | x_0)} \log(1 - \cls_\phi(\rvx_t;t))\,d\rvx_0d\rvx_t                                                                                             \\
    \approx & \ -\int p_\theta(y=1) p_\theta(x_0 | y=1) \textcolor{blue}{q(x_t | x_0)} \log\cls_\phi(\rvx; t)\,d\rvx_0d\rvx_t \nonumber                                                                               \\
            & \ +\int p_\theta(y=0) p_\theta(x_0 | y=0) \textcolor{blue}{q(x_t | x_0)} \log(1 - \cls_\phi(\rvx_t;t))\,d\rvx_0d\rvx_t                                                                                  \\
    %%
    =       & \  p(y=1) \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)}}\nonumber                                                                                                  \\
            & \ + p(y=0) \meanp{p_\theta(\rvx_0 | y=0)}{\meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))}}
\end{align}
Noting that $\alpha := p(y=1)$ recovers \cref{eq:true-objective}.


\subsection{\method{}'s objective function}\label{app:objective-is}
Here we show why \cref{eq: classifier loss} is an importance sampling estimator of the original objective function in \cref{eq:true-objective}.

\begin{align}
    \gL_{\phi}^{\text{cls}}(\alpha) := & \ \alpha \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)}}\nonumber                                            \\
    +                                  & \ (1 - \alpha) \meanp{p_\theta(\rvx_0 | y=0)}{\meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))}}                                       \\
    =                                  & \ - \meanp{p_\theta(y)}{ \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} }.
\end{align}
Now we apply importance sampling to $p_\theta(y)$ by sampling from $\pi(y)$ as the proposal distribution. Therefore,
\begin{align}
    \gL_{\phi}^{\text{cls}}(\alpha) = & \ - \meanp{p_\theta(y)}{ \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} }                       \\
    =                                 & \ - \meanp{\pi(y)}{ \frac{p_\theta(y)}{\pi(y)} \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} } \\
    =                                 & \ \frac{p_\theta(y=1)}{\pi(y=1)} \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t | \rvx_0)}{-\log C_\phi(\rvx_t; t)}}\nonumber                                           \\
                                      & + \frac{p_\theta(y=0)}{\pi(y=0)} \meanp{p_\theta(\rvx_0|y=0)}{\meanp{q(\rvx_t | \rvx_0)}{-\log (1 - C_\phi(\rvx_t; t))}}                                              \\
    =                                 & \ \meanp{p_\theta(\rvx_0|y=1)}{ \frac{\alpha}{\pi(y=1)} \meanp{q(\rvx_t | \rvx_0)}{-\log C_\phi(\rvx_t; t)}}\nonumber                                                 \\
                                      & + \meanp{p_\theta(\rvx_0|y=0)}{ \frac{1 - \alpha}{\pi(y=0)} \meanp{q(\rvx_t | \rvx_0)}{-\log (1 - C_\phi(\rvx_t; t))}} \label{eq:loss-is-proof}
\end{align}
In our case, $\pi(y)$ is a uniform Bernoulli distribution i.e., $\pi(y=1) = \pi(y=0) = 0.5$. Therefore, minimizing \cref{eq: classifier loss} is equivalent to minimizing a Mone Carlo estimate of \cref{eq:loss-is-proof}.

\section{Experimental details}

\subsection{Toy Experiment}
\paragraph{Architecture} We use a fully connected network with 2 residual blocks as shown in \cref{fig:toy-arch}. The hidden layer size in our experiment is 256 and timestep embeddings (output of the sinusiodal embedding layer) is 128. Our classifier has a similar architecture, the only difference is that the classifier has a different output dimension of one. Our baseline DM and classifier networks both have around 330k parameters.
\paragraph{Training the baseline DM} We train our models baseline models on a single GPU, (we use either of GeForce GTX 1080 Ti or GeForce GTX TITAN X) for 30,000 iterations. We use the Adam optimizer \citep{kingma2014adam} with a batch size of $3 \times 10^{-4}$ and full-batch training i.e., our batch size is 1000 which is the same as the training dataset size.
\paragraph{Training the classifiers} Each classifier is trained on a fully-synthetic dataset of 100k samples which consists of 50k positive and 50k negative samples. This dataset is generated with 100 diffusion steps. We train the classifier for 20k iterations with a batch size of 8192. We use Adam optimizer with a learning rate of $3 \times 10^{-3}$.
\paragraph*{Distillation} The distilled models have the same architecture and hyperparameters as the baseline DM model. They are trained for 250k iterations on the true dataset with a batch size of 1000. We use Adam optimizer with a learning rate of $3 \times 10^{-4}$.
\paragraph{Diffusion process} We use the EDM framework in this experiment with a preconditioning similar to the one proposed in \citet{karras2022elucidating}. In particular, the following precoditioning is applied to the the network in \cref{fig:toy-arch}, called $F_\theta(\rvx_t, t)$, to get $D_\theta(\rvx_t; t)$ which returns an estimate of $\rvx_0$.
\begin{equation}
    D_\theta(\rvx_t; t) = \frac{\sigma_{\text{data}}^2}{\sigma(t)^2 + \sigma_{\text{data}}^2} \rvx_t + \sigma(t)F_\theta\left( \frac{1}{\sqrt{\sigma(t)^2 + \sigma_{\text{data}}^2}} \rvx_t; \frac{1}{4} \ln(\sigma(t)) \right).
\end{equation}
Following \citep{karras2022elucidating}, we simply let $\sigma_{\text{data}} = 0.5$.

In total, we spent around 250 GPU-hours for this experiment.
% Figure environment removed
\subsection{Infractions in Traffic Scene Generation}
\paragraph{Overview}\label{para:overview} This section provides additional details for the traffic scene generation task. The architecture for training the baseline DM model, classifiers and distillation models is majorly based on transformers introduced by \citet{vaswani2017attention} . In particular, the architecture backbone consists of an encoder, a stack of attention residual blocks, and a decoder. Each of them will be discussed in detail later.
The original data input shape is $[B, A, F]$ corresponding to $A$ vehicles and $F$ feature dimensions in a batch with $B$ many scenes.

In terms of parameters, the attention layers comprise the majority portion of the entire architectures so that the difference in decoder is relatively small, and the resulting architectures all contain approximately $6.3$ million parameters. We use NVIDIA A100 GPUs for training and validating models, synthetic datasets generation with around $400$ GPU-hours in total. We train each model with a batch size of $64$ and Adam optimizer with a learning rate of $10^{-4}$.


\paragraph{Encoder and time embeddings}
To generate input features, we use sinusoidal positional embeddings to embed the diffusion time and 2-layer MLP with activation function SiLU to embed the original data separately into $H = 196$ hidden feature dimensions. The sum of the two embeddings is the input that is fed into the attention-based architecture.

\paragraph{Self-attention Layer and Cross-attention Layer}
The major implementation of multi-head ($k=4$) attention blocks is built on Transformer~\citep{vaswani2017attention}.
Applying self-attention across agents enables model to learn the multi-agent interactions, while applying map-conditional cross-attention between agents and map allows agents to interact with the road representations.
To prepare road image for model input, we use a convolutional neural network and a feed-forward network~\citep{carion2020end} to generate a lower-resolution map $m'\in\sR^{196\times 32\times 32}$ from the original image $m\in\sR^{3\times 256\times 256}$. Since the transformer architecture is permutation-invariant, we add a 2D positional encoding~\citep{parmar2018image, bello2019attention} based on $m'$ on the top of the map representation to preserve the spatial information of the image.

\paragraph{Relative Positional Encodings (RPEs)}During experiments, we find the collision rate is much higher than the offroad rate. In order to effectively lower the frequency of or completely avoid vehicle collision occurrence, we attempt to capture the relative positions by performing relative positional encodings (RPEs) in self-attention residual blocks and enforce the vehicles being aware of the other vehicles in close proximity in each scene.
Following~\citep{shaw2018self, wu2021rethinking, harvey2022flexible}, we compute the distances of each pair of vehicles and summarise into a tensor of shape $[B, A, A]$, where $d^b_{ij}$ is the distance between vehicle $i$ and $j$ in the $b^{\text{th}}$ scene.
% Look-up table~\citep{shaw2018self} is not preferable since the agent count is various across the scenes. 
We choose to use sinusoidal embeddings (similar to how we embed diffusion time $t$) to parameterize $d^b_{ij}$ rather than logarithm function $f_{\text{RPE}}(d^b_{ij}) = \log(1 + d^b_{ij})$, as we need to adequately amplify the pairwise distances between vehicles when it is comparably small.
We perform this operation together with diffusion time embedding at each diffusion time step, and we regard their sum as the complete pairwise distance embeddings.
The resulting embedding tensor $\rvp$ is of the shape $[B, A, A, H]$, where $\rvp^b_{ij}$ is the encoding vector of length $H$ representing the pairwise distance of vehicle $i$ and $j$ in the $b^{\text{th}}$ scene.

In each scene, we have an input sequence, $\rvx = (\rvx_1, \cdots, \rvx_A)$, and each $\rvx_i$ is linearly transformed to query $\rvq_i = W^Q \rvx_i$, key $\rvk_i = W^K \rvx_i$ and value $\rvv_i = W^V \rvx_i$.
We also apply linear transformation onto RPEs to obtain query $\rvp_{ij}^Q = U^Q \rvp_{ij}$, key $\rvp_{ij}^K = U^K \rvp_{ij}$ and value $\rvp_{ij}^V = U^V \rvp_{ij}$.
Then the add-on output from the self-attention residual block is the aggregated outputs of the vanilla transformer and the relative-position-aware transformer:
\begin{align}
    \rvx_i^{\text{output}}        & = \rvx_i + \sum_{j=1}^A \alpha_{ij} (\rvv_j + \rvp_{ij}^V)                                                                                                           \\
    \text{where}\quad \alpha_{ij} & = \frac{\exp(e_{ij})}{\sum_{k=1}^A \exp(e_{ik})}\ \text{and}\  e_{ij} = \frac{\rvq_i^\top \rvk_j + \rvp_{ij}^{Q^\top}\rvk_j + \rvq_i^\top\rvp_{ij}^K}{\sqrt{d_\rvx}}
\end{align}

\paragraph{Decoder}
The settings for baseline, distillation models and classifiers are almost identical except the decoder for producing the final output. For baseline and distillation models, we apply 2-layer MLP and reconstruct the output of the shape $[B, A, H]$ from the final attention layer into $[B, A, D]$ through the decoder.
To ensure we output individual label for each vehicle with by-agent classifiers, and a collective label for a scene with by-scene classifiers, and we conduct the operations as follows.
The decoder takes the hidden representation of the shape $[B, A, H]$ and produces a tensor with feature dimension $F'=1$ with a 2-layer MLP, which is the predicted labels from the by-agent classifiers.
For by-scene classifier, we add additional MLP layer to extract the first column from the second dimension of the by-agent classifier resulting predictions and obtain a tensor of the shape $[B, 1, 1]$.


\subsection{Motion Diffusion}
We used the official implementation of MDM\footnote{\href{https://github.com/GuyTevet/motion-diffusion-model}{\url{https://github.com/GuyTevet/motion-diffusion-model}}} for our Motion Diffusion experiment. For the baseline DM, we used their officially released best pretrained checkpoint of text to motion task on HumanML3D dataset. We generate a synthetic dataset of around 250k positive and 250k negative examples from the baseline DM which is a DDPM-based model with 1000 diffusion steps. We then define our classifier architecture using their code base. Following our other experiments, our classifier architecture is the same as the baseline DM model. We train the classifier with a batch size of 256 and a learning rate of $10^{-4}$ for 100k iterations. Otherwise, we use the same hyperparameters as in \citep{tevet2023human}. All the training and data generation is done on A100 GPUs.

To compute the FID scores, in accordance with \citep{tevet2023human}, we generate one motion for each caption in the HumanML3D test set, resulting in a total of 4,626 generated motions. In contrast to \citep{tevet2023human}, we refrain from applying classifier-free guidance during the sample generation process. Subsequently, we calculate the FID score between the generated motions and the ground-truth motions in the test set. The FID metric quantifies the distance in the latent space of a motion encoder, which has been pre-trained using contrastive learning \citep{guo2022generating}.



The total compute used for this experiment (generating the datasets and training the classifiers) was around 600 GPU-hours.

\section{Additional results}


% Figure environment removed

\subsection{Alternative approaches to conditional generation with diffusion models}\label{app:alternative-approaches}
In our journey towards solving this problem, we explored multiple different approaches. Here we briefly mention a few of the more promising ones.

\paragraph{Diffusion bridges} \citet{liu2023learning} proposes a framework for diffusion modeling of constrained domains. Their proposed method requires adding a guidance term to the diffusion process which comes in form of an expectation. This expectation is only tractable for simple constraints such as the one in our toy experiment. We applied this method on our toy experiment which lead to practically zero infraction, without overfitting. As the update term is intractable for the larger scale experiment, we tried to learn an estimation of it, but we failed to reliably estimate the update term.

\paragraph{Learning in the joint space of $(\rvx, y)$} Inspired by \citet{weilbach2022graphically}, we trained a model to estimate all the marginals and conditionals on the joint space of $(\rvx, y)$ where $q(\rvx | y=1)$ is the training set, $q(\rvx | y=0)$ is a synthetically generated set of negative examples and $q(y=1)$ is a hyper-parameter. We assign a probability vector for choosing a training task among learning conditional probabilities $q(\rvx|y), q(y|x)$ and joint probability $q(\rvx, y)$ collectively with one architecture parameterized by $\theta$ with implicit classifier learner $p_\theta(y | \rvx)$. Intuitively, feeding the model with both positive and negative samples helps the model learn the boundary between the positive region $\Omega$ and negative region $\Omega^\complement$. At test time, we only sample from $p_\theta(\rvx|y=1)$. Our preliminary results suggested that it helps reducing the infraction rate, but is outperformed by \method{}.

\subsection{Ablation on label imbalance}\label{app:ablation-imbalance}
As mentioned in \cref{sec:method:classifier}, naively generating synthetic datasets for training the classifier causes a major label imbalance issue hinders training of the classifier. In this section we perform an ablation study to empirically demonstrate its effect in our experiments.

As a reminder, \method{} guarantees an equal number of positive and negative examples in the synthetic datasets, and it employs importance sampling to address any distribution shift introduced.
In the ablated experiment (referred to as "imbalanced" in the results), the ratio of positive to negative examples is model-dependent, being equal to the model's infraction rate.
It results in a gradual increase in the dominance of positive examples as the model's infraction rate decreases.

\paragraph*{Toy experiment} We repeat our toy experiment with and without \method{}'s imbalance correction. In each iteration, we create a synthetic dataset of 20,000 samples and train a binary classifier for 10,000 iterations. The other details are the same as the experiment in the main text. We show the performance of these models in \cref{fig:2d-ablation-imbalance}.
We observe that ``imbalaced'', the ablated version of \method{}, is consistently outperformed by \method{} in infraction rate. Furthermore, the performance gap between the two methods widens as the models improve.
However, it is worth noting that \method{} achieves a comparable ELBO to that of ``imbalanced'' despite these infraction rate differences.

\paragraph*{Traffic Scene Generation} We conduct a similar ablation as described above, where we train classifiers on a imbalanced datasets of the same size as our method. The results of this ablated experiment are presented in \cref{tab:traffic-scene-ablation}, and they are compared with the results reported in the main text.


\begin{table}[tb]
    %\scriptsize
    \caption{Results for traffic scene generation, in terms of collision, offroad, and overall infractions as well as ELBO. Two varieties (``by-scene'' and ``by-agent'') for the classifier are presented, as well as results with (\method{}) and without importance sampling. The final two rows provide the results of distilling the models labelled with $\dagger$ and *.}
    \centering
    \label{tab:traffic-scene-ablation}
    \begin{tabular}{lllll}
        \toprule
        Method              & Collision ($\%$) $\downarrow$ & Offroad ($\%$) $\downarrow$ & Infraction ($\%$) $\downarrow$ & r-ELBO ($\times 10^2$) $\uparrow$ \\
        \midrule
        baseline DM         & $28.3\pm 0.70$                & $1.3\pm 0.14$               & $29.3\pm 0.64$                 & $-27.5\pm0.01$                    \\
        \midrule
        by-scene            & $23.3\pm 0.7$                 & $1.0\pm 0.28$               & $24.1\pm 0.67$                 & $-27.6\pm 0.01$                   \\
        by-scene imbalanced & $23.8 \pm 0.6$                & $1.0\pm 0.3$                & $24.6 \pm 0.54$                & $-27.6\pm 0.01$                   \\
        \midrule
        by-agent            & $16.4\pm 0.5$                 & $0.9\pm 0.12$               & $17.2\pm 0.44$                 & $-27.7\pm 0.01$                   \\
        by-agent imbalanced & $17.8 \pm 1.21$               & $0.9\pm 0.16$               & $18.6\pm 1.3$                  & $-27.7\pm 0.01$                   \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{More visualization results for traffic scene generation}\label{app:ic-more-results}
% Figure environment removed
In \cref{fig:ic-more-results} we report more visualization results from our traffic scene generation experiment. This figure follows from and adds more details to \cref{fig:banner}.

\subsection{Overfitting in the toy experiment}\label{app:overfitting}
Here we report our results for overfitting the baseline DM in the toy experiment. We run an experiment with 250,000 training iterations, much larger than the 30,000 iterations in the reported results. As we can see in \cref{fig:toy-overfit}, the infraction rate keeps decreasing. However, the model starts overfitting after around 30,000 iterations, as measure by the ELBO on a held-put set. This suggests that the architecture is expressive enough to model sharp jumps in the learned density. However, simply training it on a small dataset without incorporating any prior on ``where to allocate its capacity'' fails because the model does not receive any signal on where the actual ``sharp jump'' is. \method{}, on the other hand, provides this kind of signal through the oracle-assisted guidance.
% Figure environment removed
