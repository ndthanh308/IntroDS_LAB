\begin{algorithm}[!t]
    \caption{\method{}}\label{alg: iterative training}
    \begin{algorithmic}
        \State Input: dataset $\gD$, oracle $\oracle$, balanced synthetic dataset size $N$
        \State $i \leftarrow 0$
        \State $\theta_i \leftarrow \argmin_\theta \gL^{\text{DM}}_\theta$ \Comment{train baseline DM, \cref{eq:score-matching}}
        \State ${s_i} \leftarrow s_{\theta_i}(\rvx_t; t)$
        \While{not done}
            \State $\synth^+_i,\synth^-_i \leftarrow$ generate samples from DM with score function ${s_i}$ and label with $\oracle$
            \While{$\min(|\synth^+_i|,|\synth^-_i|)<N$}
             \State $\synth^+,\synth^- \leftarrow$ generate more samples  from DM with score function ${s_i}$ and label with $\oracle$
             \State $\synth^+_i \leftarrow \synth^+_i \cup \synth^+$, $\synth^-_i \leftarrow \synth^-_i \cup \synth^-$
            \EndWhile
            \State $\alpha_i \leftarrow |\synth^+_i| / (|\synth^+_i| + |\synth^-_i|)$ \Comment{Estimate class prior probabilities for Bayes optimal classifier} 
            \State $\synth^+_i \leftarrow \subsample(N,\synth^+_i), \synth^-_i \leftarrow \subsample(N,\synth^-_i)$ \Comment{balance dataset for IS classifier training}
            \State $\phi_i \leftarrow \argmin_\phi \hat{\gL}^{\text{cls}}_\phi(\alpha_i, \synth^+_i, \synth^-_i)$ \Comment{train guidance classifier, \cref{eq: classifier loss}}
            \State $i \leftarrow i+1$

            \If{distill}
                \State $\psi \leftarrow \argmin_\psi \gL^{\text{dtl}}_\psi$\Comment{distill into single DM, \cref{eq: distillation}}
                \State ${s_i} \leftarrow  s_{\psi}(\rvx_t; t)$ 
            \Else  \Comment{``stack'' guidance classifiers}
                \State ${s_i} \leftarrow {s_{i-1}} + \nabla_{\rvx_t}\log C_{\phi_i}(\rvx_t; t)$ \Comment{See \cref{eq:gen-neg-score}}
            \EndIf
        \EndWhile
        \State \Return DM score function $s_i$
    \end{algorithmic}
\end{algorithm}