%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\input{math_commands.tex}
\usepackage{subfigure}
\usepackage[T1]{fontenc}

\DeclareMathOperator{\subsample}{select}
\def\method{Gen-neG}
\def\oracle{\ensuremath{\gO}}
\def\infr{\ensuremath{\alpha}}
\def\synth{\ensuremath{{\widehat{\gD}}}}
\def\cls{\ensuremath{C}}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Score-based Generative Modeling with Oracle-assisted Guidance}

\begin{document}

\twocolumn[
\icmltitle{Don't be so Negative!\\Score-based Generative Modeling with Oracle-assisted Guidance}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Saeid Naderiparizi}{equal,ubccs,inverted}
\icmlauthor{Xiaoxuan Liang}{equal,ubccs,inverted}
\icmlauthor{Setareh Cohan}{ubccs}
\icmlauthor{Berend Zwartsenberg}{inverted}
\icmlauthor{Frank Wood}{ubccs,inverted,mila}
\end{icmlauthorlist}

\icmlaffiliation{ubccs}{Department of Computer Science, University of British Columbia, Vancouver, Canada}
\icmlaffiliation{inverted}{Inverted AI, Vancouver, Canada}
\icmlaffiliation{mila}{Montr\'eal Institute for Learning Algorithms (MILA)}

\icmlcorrespondingauthor{Saeid Naderiparizi}{saeidnp@cs.ubc.ca}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Score-based diffusion models are a powerful class of generative models, widely utilized across diverse domains. Despite significant advancements in large-scale tasks such as text-to-image generation, their application to constrained domains has received considerably less attention.
This work addresses model learning in a setting where, in addition to the training dataset, there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling methodology, \method{}, that leverages this additional side-information.
\method{} builds on classifier guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. 
We empirically establish the utility of \method{} in applications including collision avoidance in self-driving simulators and  safety-guarded human motion generation.
\end{abstract}

\section{Introduction}\label{sec:intro}

What should we do when we train a generative model that generates samples known to be invalid within the constraints of the data domain?
For instance, when generating traffic scenes, road users cannot overlap each other. Likewise, in robotics, adherence to numerous physics-based constraints is essential for maintaining the appropriate motion and configuration of the robot. Typically, generative models are only trained to maximize the likelihood of a set of ``good'' training data samples.
Nevertheless, when sampling from a fully trained, highly expressive model, some fraction of generated samples fall into the category of ``bad'' samples.
Here we consider the problem of generative modeling where in addition to the conventional training dataset of good samples, we are also given access to constraints in the form of an oracle, which provides insights into whether a given sample is considered bad. Such oracles are ubiquitous in practice and are often a simple function implemented by domain experts.


Modern deep generative models are sufficiently parameterized that they can effectively be trained to result in a model placing a mixture of Dirac measures directly on the training data \citep{somepalli2023diffusion, somepalli2023understanding, carlini2023extracting}.
However, training such models on large amounts of data \citep{rombach2022high} or imposing regularization (such as smaller architectures or fewer integration steps) ensures that they generalize rather than memorize \citep{arpit2017closer,zhang2021understanding}. This also results in these models placing mass in the invalid part of the support \citep{hanneke2018actively}.
In this paper, we assume a modeling regime where the model generalizes effectively. Within this context, our objective is to reduce the probability mass assigned to invalid outputs while avoiding overfitting. Consequently, our contribution can be viewed as a method for controlling the specific type of the model's generalization.


The simplest way to use an oracle is to deploy the model with a rejection sampling loop in which the oracle is used to filter the output and return the constraint-satisfying samples~\citep{kim2023consistency}. 
Depending on circumstances this may constitute an acceptable final ``generative model'', but this solution comes at a (potentially unacceptable) computational cost.  Consider the concrete example of real-time autonomous vehicle path planning and model predictive control \citep{zhong2022guided}.
This task involves generating the next control action for an autonomous vehicle based the past and current state of itself and its surroundings. The generated action must avoid collisions and other types of invalid behavior, collectively referred to as ``infractions.'' This is an extremely challenging task that requires low latency and high success rates.
To guarantee low latency, it is essential to generate a sufficiently large number of parallel samples to ensure obtaining at least one valid sample with high probability.
Assume that a generative model trained on trajectories with no infractions produces infracting trajectories for all vehicles with probability $\epsilon$ (state of the art models \citep{lee2017desire, djuric2018short, gupta2018social, cui2019multimodal, ngiam2021scene, scibior2021imagining, niedoba2023diffusion} can have high infraction rates.)
Generating at least one non-infracting sample with $1-\delta$ probability without looping the rejection sampler requires $\frac{\log \delta}{\log \epsilon}$ parallel samples.  Depending on the specific concrete value of $1-\delta$ required (e.g. 1 chance in a billion of having latency arising from rejection sampling looping imposed) and the baseline trajectory model rejection rate (e.g. 30-50\% is not atypical) this could require running many parallel samplers (in this concrete example around 30). Depending on model size and available realtime edge computational capacity, this quantity may be prohibitively large.  Other examples of this nature arise in many control as inference problems \citep{levine2018reinforcement}.

Minimizing $\epsilon$ directly i.e., restricting the generative model to only place mass on the positive support region indicated by the oracle, is the most natural approach to combat this problem.  Working towards this goal includes a body of work on amortized rejection sampling \citep{warrington2020coping,naderiparizi2022amortized} and the body of related work on generative adversarial networks (GANs) \citep{goodfellow2014generative}.  Of course in the GAN setting, the discriminator (which can be used in a rejection sampling loop for improved performance \citep{azadi2018discriminator, che2020your}) is learned rather than being a fixed, pre-defined oracle as in the case we consider. 

% Figure environment removed

We focus specifically on learning with constraints in score-based models.
What we reveal in this study is a \textit{necessary condition}, essential for the accurate functioning of classifier guidance in this problem domain, which, to the best of our knowledge, has been surprisingly overlooked until now.
Following recent findings on discriminator guidance in diffusion processes \citep{kim2022refining}, we introduce a new methodology for classifier guidance.
Our approach involves training and employing a series of differentiable classifiers, trained on synthetic samples generated from a sequence of classifier-guided diffusion models and labeled by the oracle.
The resulting sequence of multiply classifier-guided diffusion models effectively reduce the rejection rate while empirically maintaining a competitive probability mass assigned to validation samples. 
We demonstrate that comparable performance can be achieved with a reduced computational overhead by distilling the sequence of classifiers.

We evaluate our proposed methodology, which we call \textbf{Gen}erative modeling with \textbf{neG}ative examples (\method{})
on several problems, including modeling motion capture sequence data in a way that eliminates ground plane violations, and static traffic scene vehicle arrangements that avoid collisions and off-road placements.


\section{Background}

\subsection{Score-based Diffusion Models}\label{background:score-based}
Score-based diffusion models \citep{sohl2015deep, song2019generative, ho2020denoising, song2021scorebased}, also referred to as diffusion models (DMs) are a class of generative models that are defined through a stochastic process which gradually adds noise to samples from a data distribution $q_0(\rvx_0)$, such that when simulated forward from $t=0$ the marginal distribution at time $T$ is $q_T(\rvx_T) \approx \pi(\rvx_T)$ for some known $\pi(\rvx_T)$ typically equal to $\gN(\vzero, \mI)$. This is known as the ``forward process'' and is formulated as an SDE
\begin{equation}
    d\rvx_t = f(\rvx_t, t) dt + g(t) d \rvw, \quad \rvx_0 \sim q_0(\rvx_0),
    \label{eq:forward-process}
\end{equation}
where $f$ and $g$ are predefined drift and diffusion coefficients of $\rvx_t$ and $\rvw$ is the standard Wiener process.
DMs define another stochastic process known as the ``reverse process'' defined as
\begin{equation}
    d \rvx_t = [f(\rvx_t, t) - g(t)^2 s_\theta(\rvx_t; t)] dt + g(t) d\bar{\rvw}, \, \rvx_T \sim \pi(\rvx_T),
    \label{eq:reverse-process}
\end{equation}
where $\bar{\rvw}$ is the infinitesimal reverse time and reverse Wiener process, respectively. If $s_\theta$ matches the score function of the marginals of the forward process, the terminal distribution of the reverse process coincides with $q_0(\rvx_0)$~\citep{anderson1982reverse}.
Formally,
\begin{equation}
    s_\theta(\rvx_t; t) = \nabla_{\rvx_t} \log q_t(\rvx_t) \Rightarrow p_\theta(\rvx_0; 0) = q_0(\rvx_0),
\end{equation}
where $p_\theta(\rvx_t; t)$ is the marginal distribution of the reverse process.

In order to approximate the score function $\nabla_{\rvx_t} \log q_t(\rvx_t)$, DMs minimize the following score matching objective function~\citep{hyvarinen2005estimation,vincent2011connection,song2019generative}:
\begin{equation}
    \mathcal{L}^{\text{DM}}_\theta = \meanp{t,\rvx_0,\rvx_t} {\gamma_t \norm{s_\theta(\rvx_t; t) - \nabla_{\rvx_t} \log q(\rvx_t | \rvx_0)}^2},
    \label{eq:score-matching}
\end{equation}
where $\rvx_0 \sim q(\rvx_0)$, $\rvx_t \sim q(\rvx_t | \rvx_0)$, $t$ is sampled from a distribution over $[0, T]$, and $\gamma_t$ is a positive weighting term. Importantly, the Wiener process in \cref{eq:forward-process} allows direct sampling from the marginals of the forward distributions~\citep{song2021scorebased}, i.e. $q(\rvx_t | \rvx_0) = \gN(\alpha_t \rvx_0, \sigma_t)$, with $\alpha_t$ and $\sigma_t$ determined by the drift and diffusion coefficients in \cref{eq:forward-process}. This formulation moreover allows the evaluation of the conditional score function ($\nabla_{\rvx_t} \log q(\rvx_t | \rvx_0)$) in closed form.

Many of the DMs reported in the literature operate on discrete time steps~\citep{ho2020denoising,nichol2021improved}, and can be considered as particular discretizations of the presented framework.

In the remainder of this paper we use $q$ to denote the forward process, $s_\theta$ for the score function of the reverse process and $p_\theta$ as the distribution generated by running \cref{eq:reverse-process} backward in time. This applies to the marginals, conditionals, and posteriors as well. Furthermore, to reduce notational clutter throughout the rest of the paper, we omit the explicit mention of $\theta$ and $\phi$ and $t$ when their meaning is evident from the context.

\subsection{Classifier Guidance} \label{sec:background:classifier-guidance}

A distinctive and remarkable property of DMs is the ability to utilize an unconditional diffusion model to draw samples from its class-conditional distributions at inference time without requiring re-training or fine-tuning \citep{Dhariwal2021diffusion,song2021scorebased}. However, doing so typically utilizes a time-dependent classifier $q(y | \rvx_t) = \int q(y | \rvx_0) q(\rvx_0 | \rvx_t)\,d\rvx_0$
(alternative approaches include \citep{wu2023practical}).
Here, $q(y|\rvx_0)$ is a traditional classifier, that predicts the class probabilities for each $y$ given a datum $\rvx_0$ from the dataset. While $q(y | \rvx_t)$ classifies a noisy datum $\rvx_t$ sampled from $q_t(\rvx_t) = \int q(\rvx_t | \rvx_0) q(\rvx_0) \,d\rvx_0$.

Classifier guidance follows from the identity $\nabla_{\rvx_t}\log q(\rvx_t | y) = \nabla_{\rvx_t}\log q(\rvx_t) + \nabla_{\rvx_t}\log q(y | \rvx_t)$. Since the score function of the DM $s_\theta(\rvx_t; t) \approx \nabla_{\rvx_t}\log q(\rvx_t)$, we have
\begin{equation}
    s_\theta(\rvx_t | y; t) = s_\theta(\rvx_t; t) + \nabla_{\rvx_t}\log q(y | \rvx_t).
\end{equation}

\paragraph{Binary classification}
A special case of the above classifier guidance that we use in this paper is when there are only two classes. We provide here a brief overview of such a binary classification task and the notation associated with it.
Let $q(\rvx | y=1)$ and $q(\rvx | y=0)$ be the distribution of positive and negative examples, respectively. Let $\alpha = q(y = 1)$ and $1 - \alpha = q(y = 0)$ be the prior probabilities $q(y)$ of positive and negative examples. We then have $q(\rvx) = \alpha q(\rvx_t | y=1) + (1 - \alpha) q(\rvx_t | y=0)$. A binary classifier $C_\phi: \gX, [0, T] \rightarrow  [0, 1]$, can then be trained to approximate $q(y=1|\rvx_t)$ by minimizing the expected cross-entropy loss
\begin{equation}
\begin{split}
    \gL_{\phi}^{\text{CE}} =
    - \E_{t, \rvx_t} \Bigr[ q(y=1|\rvx_t) \log \cls_\phi(\rvx_t; t) + \\
    q(y=0|\rvx_t) \log (1 - \cls_\phi(\rvx_t; t)) \Bigr],
\end{split}
    \label{eq:cls-cross-entropy}
\end{equation}
where $\rvx_t \sim q(\rvx_t)$. Minimizing the cross-entropy loss between the classifier output and the true label is equivalent to minimizing the KL divergence between the classifier output and the Bayes optimal classifier \citep[Chapter~4]{sugiyama2012density}.
Therefore, \cref{eq:cls-cross-entropy} is minimized when
\begin{equation}
    \begin{split}
    \cls_{\phi^*}(\rvx_t; t) &= q(y = 1 | \rvx_t)\\
    &= \frac{\alpha q(\rvx_t | y=1)}{\alpha q(\rvx_t | y=1) + (1 - \alpha) q(\rvx_t | y=0)}.
    \label{eq:cls-optimal}
    \end{split}
\end{equation}
Hence, $s_\theta(\rvx_t | y=1; t) = s_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log \cls_{\phi^*}(\rvx_t; t)$.
Note that this minimizer critically depends on the class prior probabilities $\alpha$ and $1 - \alpha$. \method{} works by ensuring that these are properly accounted for.

\section{Methodology}

In this section, we describe \textbf{Gen}erative modeling with \textbf{neG}ative examples (\method{}), a method for guiding the sampling of diffusion models to satisfy the constraints imposed by an oracle function.
\method{} has two stages. It starts by training a DM on available training data following standard DM training procedures (e.g. \cref{background:score-based}) without utilizing the oracle. We refer to this model as the ``baseline DM'' throughout.
In the second stage of \method{}, we draw samples from this baseline DM, label them using the oracle, and train a binary classifier using those samples, which we later use for guidance.
Next, we use the obtained classifier to guide our baseline DM, and the combination of both constitutes a new generative model.  \method{} establishes this combined model as a new DM and then repeats the process of sampling, classifying using the oracle, and training a time dependent classifier to form yet another model. 
Refinement using this iterative process can be repeated until the desired performance is obtained. We refer to this type of refinement as ``stacking''. Optionally, if better computational performance is desired, the stacked model can be distilled into a new model at any desired stage. 
As mentioned before, an important feature of \method{} is to properly account for the prior class probabilities $\alpha$ and $1 - \alpha$ in the training of all classifiers, which is formalized later in this section and demonstrated later in \cref{sec:experiments}. An overview of \method is shown in \cref{fig:gen-neg-overview} and \cref{alg: iterative training}.

\paragraph{Problem formulation and notation} Let $\gD = \{\rvx^i\}_{i=1}^N \sim q(\rvx)$ be a dataset of i.i.d. samples from an unknown data distribution $q$. Furthermore, let $\oracle: \gX \rightarrow \{0, 1\}$ be an oracle function that assigns each point in the data space $\gX$ a binary label. In other words, this oracle partitions the data space into two disjoint sets $\gX = \Omega \cup \Omega^\complement$ such that $\oracle(\rvx) = \1_{\Omega}(\rvx)$.
Moreover, assume $\gD \subseteq \Omega$ i.e., all training examples satisfy the oracle constraints.
Our objective is to learn a score-based diffusion model that (i) maximizes the likelihood of $\gD$ and (ii) avoids allocating probability to $\Omega^\complement$.

\subsection{Bayes Optimal Classifier Guidance for Diffusion models} \label{sec:method:classifier}
The core component of \method{} is in the second stage where a \emph{Bayes optimal} diffusion-time dependent classifier that discriminates between positive and negative samples in $\Omega$ and $\Omega^\complement$ respectively, is used to guide the baseline DM.
There are two main questions that \method{} answers. (i) Which data and class distribution should the classifier be Bayes optimal with respect to such that classifier guidance does not modify the sampling distribution in the oracle approved region? (ii) How can one train such a classifier?

Classifier guidance in score-based models is typically used to generate samples from a specific pre-defined class on the training dataset. As such, the classifier and generative model share the same training distribution.
In our case, however, there is no pre-defined dataset of positive and negative classes. Even the training dataset $\gD$ only includes samples from one class, since they all satisfy the oracle constraints.
Despite that, as we show later in \cref{sec:experiments}, classifier guidance using a Bayes optimal classifier for the binary classification task on the fully-synthetic data generated by the same baseline DM leads to (i) zero infraction and (ii) improved likelihood estimation on the data distribution, including a \textit{held-out test set}.
% Figure environment removed


The goal of \method{} is therefore to solve this classification task on synthetic data. Formally, the data is distributed as $p_\theta(\rvx_0)$, the labels are $y = \oracle(\rvx_0)$, and the noise distribution is $p_\theta(\rvx_t | \rvx_0)$.
One can obtain the Bayes optimal classifier for this task using a cross-entropy objective similar to \cref{eq:cls-cross-entropy} in which $q$ is replaced by $p_\theta$. This objective can be equivalently written as
\begin{equation}
\begin{split}
    \gL_{\phi, \theta}^{\text{CE}} = - \E_{t, \rvx_0, \rvx_t} \Bigr[\oracle{}(\rvx_0) \log \cls_\phi(\rvx_t; t) + \\
    (1 - \oracle{}(\rvx_0)) \log (1 - \cls_\phi(\rvx_t; t)) \Bigr],
\end{split}
    \label{eq:cls-cross-entropy-synth}
\end{equation}
where $(\rvx_0, \rvx_t) \sim p_\theta(\rvx_0, \rvx_t)$ (see the Appendix for details). In order to avoid the computational cost of sampling from the baseline DM to compute this objective, \method{} approximates $p_\theta(\rvx_0, \rvx_t) \approx p_\theta(\rvx_0) q(\rvx_t | \rvx_0)$. This is similar to the approximation in \citet{kim2022refining}. The objective function of \method{} is therefore
\begin{equation}
\begin{split}
    \gL_\phi^{\text{cls}} = - \E_{t, p_\theta(\rvx_0)} \Bigr[ \E_{q(\rvx_t | \rvx_0)} \Big[ \oracle{}(\rvx_0) \log \cls_\phi(\rvx_t; t) + \\
    (1 - \oracle{}(\rvx_0)) \log (1 - \cls_\phi(\rvx_t; t)) \Big] \Bigr].
\end{split}
    \label{eq:gen-neg-true-objective}
\end{equation}
For notational simplicity, we drop the dependence of $\gL_\phi^{\text{cls}}$ on $\theta$, including in the equation above. Once trained, the classifier is incorporated into the baseline DM by
\begin{equation}
    s_{\theta, \phi}(\rvx_t; t) = s_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log C_\phi(\rvx_t; t).
    \label{eq:gen-neg-score}
\end{equation}
We denote the marginal distributions generated by the oracle-assisted DM, implicitly defined through \cref{eq:gen-neg-score} as $p_{\theta,\phi}(\rvx_t; t)$.


\paragraph{Training the classifier}
Training the classifier in our approach presents a noteworthy challenge due to the major label imbalance within the synthetic dataset $\synth$ generated by the model. This imbalance emerges when the baseline is already close to the target distribution, resulting in a scarcity of negative examples. 
Meanwhile, these negative examples play a crucial role in guiding the model at the boundary between positive and negative examples, where the model requires the most guidance.

\method{} addresses this challenge by sampling a balanced dataset $\synth$ from the model, ensuring the same number of positive and negative examples.
However, this changes the class prior probabilities from the true marginal distribution over labels $\alpha$ and $1 - \alpha$ which in turn changes the optimal classifier the cross-entropy objective targets  (see \cref{eq:cls-optimal}).
We show evidence of this happening in \cref{fig:exp-checkerboard-samples}.
\method{} crucially employs importance sampling in the classifier's training objective to rectify the  bias introduced by having to balance the dataset to achieve high classifier accuracy in training.


\input{algorithm.tex}

Given a balanced dataset $\synth = \synth^+ \cup \synth^-$ where $\synth^+ \sim p(\rvx_0 | y=1)$, $\synth^- \sim p(\rvx_0 | y=0)$, $N = |\synth^+| = |\synth^-|$, and $\alpha = p_\theta(y=1)$,
\begin{align}
    \label{eq: classifier loss}
    &\hat{\gL}_{\phi}^{\text{cls}}(\alpha, \synth^+, \synth^-) \nonumber\\
    :=& \frac{1}{N} \sum_{\rvx_0 \in \synth^+} \alpha \meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)}\nonumber\\
    + &\frac{1}{N} \sum_{\rvx_0 \in \synth^-} (1 - \alpha) \meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))},
\end{align}
is an importance sampling estimator of the objective function in \cref{eq:gen-neg-true-objective}; proof in \cref{app:objective-is}.


\subsection{Iterative Training by Stacking Classifiers} With an optimal classifier minimizing \cref{eq:cls-cross-entropy-synth}, the DM with score function $s_{\theta,\phi}$ will have improved likelihood and zero infraction (see \cref{thm:method:improved-model}). However, in practice the trained classifier is only an estimate because learning the true decision boundary would require an infeasible synthetic dataset size and optimization budget, thus infractions may not be entirely eliminated.

To alleviate this problem, we note that once the classifier is trained, the guided score function $s_{\theta,\phi}(\rvx)$ itself defines a new diffusion model.
Consequently, we can employ a similar procedure to train a new classifier on $s_{\theta,\phi}$, aiming to further lower its infraction rate. This iterative approach involves training successive classifiers and incorporating them into the model, progressively enhancing its performance and reducing the infraction rate.


% Figure environment removed

\subsection{Model Distillation}
Adding a stack of classifiers to the model linearly increases its computational cost, since each new classifier requires a forward and backward pass each time the score function is evaluated. To avoid this, we show that it is possible and sometimes beneficial to distill the classifiers into a combined diffusion model.

Let $s_{\theta, \mPhi}$ be a ``teacher model'' consisting of a baseline model $s_{\theta}$ and a stack of classifiers $\{\cls_\phi\}_{\phi \in \mPhi}$. We distill $s_{\theta, \mPhi}$ into a new ``student model'' $s_{\psi}^{\text{dtl}}$, possibly with the same architecture as the baseline model, by minimizing the following distillation loss
\begin{equation}
    \mathcal{L}_\psi^{\text{dtl}} = \meanp{\mathbf{x}_0\sim q(\mathbf{x}_0), t} {\gamma_t \norm{s_{\theta, \mPhi}(\mathbf{x}_t; t) - s_\psi^{\text{dtl}}(\mathbf{x}_t; t)}^2},
    \label{eq:distillation}
\end{equation}
where $\gamma_t$ is the weight term, similar to the training objective of diffusion models. Here, $\mathcal{L}^{\text{dtl}}$ makes the outputs of the student model match that of the teacher.
Algorithm~\ref{alg: iterative training} summarises \method{}.

\subsection{Theory}
Here we provide the theoretical grounding for why the classifier \method{} targets results in improved likelihood estimation and avoids violating the constraints.
\begin{theorem}
    \label{thm:method:cls-guidance}
    Let $p_\theta(\rvx)$ be the distribution learned by a baseline DM with marginal distributions denoted by $p_\theta(\rvx_t; t)$ and let $p_\theta(y=1 | \rvx_0)= \oracle(\rvx_0)$. Further, let $\cls_{\phi^*}: \gX, [0, T] \rightarrow [0, 1]$ be the Bayes-optimal time-dependent binary classifier arising from perfectly optimizing the following cross-entropy objective
    \begin{equation}
    \begin{split}
        \gL_{\phi, \theta}^{\text{CE}}= -\E_t \Big[ \E_{p_\theta(\rvx_0, \rvx_t)} \big[ \oracle(\rvx_0) \log \cls_\phi(\rvx_t; t) + \\
        (1 - \oracle(\rvx_0)) \log (1 - \cls_\phi(\rvx_t; t)) \big] \Big]
        \label{eq:cls-time-cross-entropy}
    \end{split}
    \end{equation}
    then
    \begin{equation}
    \begin{split}
        \nabla_{\rvx_t} \log p_\theta(\rvx_t | y=1 ; t) = \nabla_{\rvx_t} \log p_\theta(\rvx_t; t) +  \\
        \nabla_{\rvx_t} \log \cls_{\phi^*}(\rvx_t; t).
    \end{split}
    \end{equation}
\end{theorem}
In other words, by using a Bayes-optimal binary classifier for guidance, we target exactly the score function of positive (oracle-approved) examples, without modifying the underlying distribution in the oracle-approved region.
\begin{corollary}
    \label{thm:method:improved-model}
    For an optimal classifier $\cls_{\phi^*}$, 
    \begin{enumerate}
        \item $p_{\theta, \phi^*}(\rvx_t) = p_{\theta}(\rvx_t | y=1)$,
        \item There is no mass on $\Omega^\complement$, i.e.  $\int_{\rvx\in\Omega^\complement} p_{\theta, \phi^*} (\rvx) = 0$,
        \item For any dataset $\gD \subseteq \Omega$, $p_{\theta, \phi^*}(\gD) \geq p_{\theta}(\gD)$.
    \end{enumerate}
\end{corollary}
\cref{thm:method:improved-model} suggests that our \method{} methodology can improve the baseline DM in terms of both infraction rate and test dataset likelihood.

See the proofs for the \cref{thm:method:cls-guidance,thm:method:improved-model} in \cref{app:proof-thm,app:proof-corr}.

\section{Experiments}
\label{sec:experiments}
We evaluate \method{} on three datasets: a 2D checkerboard, collision avoidance in traffic scenario generation, and safety-guarded human motion generation.
In each experiment we report a likelihood-based metric on a held out dataset to measure distributional shifts and a form of infraction metric which measures faithfulness to the oracle. We release the source code for the checkerboard and motion generation experiments\footnote{\url{https://github.com/plai-group/gen-neg}}.

\subsection{Checkerboard}\label{sec:exp:toy}


% Figure environment removed

To develop some insight, we start by demonstrating the principles and performance of \method{} on a dataset of 2-dimensional points uniformly distributed on a checkerboard grid as shown in \cref{fig:exp-checkerboard-samples:a}.
We apply EDM \citep{karras2022elucidating}, a continuous-time DM, to this problem.
The training dataset only contains 1000 points.
This makes the model prone to over-fitting. As shown in \cref{fig:exp-checkerboard-scores} and further explored in \cref{app:overfitting}(blue dots), training the model for long causes strong overfitting.
We therefore stop training of the baseline DM before it starts overfitting measured by the evidence lower bound (ELBO) on a held-out validation set. \cref{fig:exp-checkerboard-samples:b} shows samples from this baseline DM and the blue dot with a red edge in \cref{fig:exp-checkerboard-scores} shows its measured performance.

\cref{fig:exp-checkerboard-samples:c} shows samples from the first iteration of \method{} and the orange stars in \cref{fig:exp-checkerboard-scores} show the metrics for the first five iterations of \method{}. These results show \method{} dramatically reduces the rate of infractions while still matching the data distribution for non-infracting regions.
\cref{fig:2d-ablation-imbalance} in the appendix shows distilling various \method{} models maintains a comparable performance.

We test the effectiveness \method{} against training on a larger dataset. The green triangles in \cref{fig:exp-checkerboard-scores} show the performance of models trained on datasets up to $10^6$ points ($1000\times$ larger than our original dataset). Even the first iteration of \method{} achieves significantly lower infraction rates compared to any of these models.
This also emphasizes the importance of negative samples, consistent with \citet{giannone2023learning}.

An alternative approach to learning constrained distributions with diffusion models is diffusion bridges \citep{liu2023learning} that provably produce no infraction. However, it requires analytical access to the constraints and quantities that are only tractable under very simple constraints. As such it is not applicable to our problem setting. Therefore, it should be treated as an upper bound to \method{}'s performance. As shown by the brown diamond in \cref{fig:exp-checkerboard-scores}, this method achieves zero infractions with a better ELBO.

\citet{kong2023data} proposed an algorithm for data redaction in GANs that is applicable to our oracle-based constraints. Since GANs do not provide ELBO estimates, we only compute MMD and infraction rates for this baseline. We train this model on our problem and choose the two checkpoints with the best value for either metric. The purple crosses in the right panel of \cref{fig:exp-checkerboard-scores} show our results. The MMD scores are significantly worse than those of diffusion-based models, including \method{}. Additionally, \method{} quickly outperforms this baseline in infraction rate.

The \method{} models in this experiment reach near-zero infraction rates. This makes the balanced synthetic dataset generation step slow. We explore an importance sampling-based approach to avoid this slowdown. Our approach and its results are presented in \cref{fig:app:faster-sampling} but we leave further exploration for future research.

\begin{table*}[!t]
  \caption{Results for traffic scene generation, in terms of collision, offroad, and overall infractions as well as reweighted ELBO (r-ELBO). We compare \method{} against a normalizing flow baseline~\citep{zwartsenberg2023conditional}, a classifier trained on imbalanced synthetic dataset, and a classifier without importance sampling and a time-independent classifier. The final two rows provide the results of distilling the models labeled with $\dagger$ and *.}
  \centering
  \small
  \label{tab: 2nd experiment}
  \begin{tabular}{lllll}
    \toprule
    Method     & Collision ($\%$) $\downarrow$  & Offroad ($\%$) $\downarrow$ & Infraction ($\%$) $\downarrow$ & r-ELBO ($\times 10^{-2}$) $\uparrow$  \\
    \midrule
    Baseline DM & $28.3\pm 0.70$  & $1.3\pm 0.14$ & $29.3\pm 0.64$ &  $-27.5\pm0.01$  \\
    Normalizing flow~\citep{zwartsenberg2023conditional} & $91.2\pm 0.27$ & $13.1\pm 0.48$ & $91.9\pm 0.25$ & --- \\
    \midrule
    Time-independent classifier & $20.7\pm 0.59$& $0.9\pm 0.09$ & $21.4\pm 0.63$ & $-244\pm 30.4$ \\
    Imbalanced classifier (ablation) & $17.8 \pm 1.21$ & $0.9\pm 0.16$ & $18.6\pm 1.30$ & $-27.7\pm 0.01$        \\
    w/o IS classifier (ablation) & $14.6\pm 0.49$ & $0.8\pm 0.13$ & $15.2\pm 0.50$ & $-28.0\pm 0.01$  \\
    \method{}$^\dagger$ (iteration 1)    & $16.4\pm 0.5$ &  $0.9\pm 0.12$ & $17.2\pm 0.44$ & $-27.7\pm 0.01$  \\
    \method{}$^*$ (iteration 2)  & $11.6\pm 0.65$ & $0.6\pm 0.10$ & $12.2\pm 0.60$ & $-28.0\pm 0.01$ \\
        \midrule
    \method{}\ (distillation of $\dagger$) &$12.2\pm0.42 $ & $0.8\pm0.06$ &$12.9\pm0.36$ & $\mathbf{-26.8\pm 0.01}$  \\
    \method{}\ (distillation of *) & $\mathbf{5.1\pm 0.24}$ & $\mathbf{0.5\pm 0.09}$ & $\mathbf{5.6\pm 0.20}$ & $-27.0\pm 0.01$\\
    \bottomrule
  \end{tabular}
\end{table*}

\subsection{Traffic Scene Generation}
\label{sec:IC-experiment}
We continue to the task of traffic scene generation where vehicles of varying sizes are placed on a given two-dimensional map.
Traditionally implemented by rule based systems~\citep{yang1996microscopic, lopez2018microscopic}, this task has recently been approached using generative modeling techniques~\citep{tan2021scenegen, zwartsenberg2023conditional}. 
In both of these prior works, the common approach has been to discard any samples that violate predefined rules, such as a vehicle being outside the designated driving area (``offroad'') or overlapping with another vehicle (``collision'').
Rejecting such samples, while effective, can be computationally wasteful, particularly when rule violations occur frequently. Hence, in this context, we employ \method{} to enhance performance.
The specific task we consider is to generate $12$ vehicles in a given scene, conditioned on a rendered representation of the roadway.
Each vehicle is represented by its position, length, width, orientation and velocity for a total of $7$ dimensions per vehicle.
Vehicles are sampled \emph{jointly}, meaning that the features are in $\mathbb{R}^{N\times 7}$. 
We train the baseline DM employing the formalism in DDPM~\citep{ho2020denoising} with a transformer-based denoising  network~\citep{vaswani2017attention} on a private dataset. Our architecture consists of self-attention layers and map-conditional cross-attention layers in an alternating order.
We use relative positional encodings (RPEs)~\citep{shaw2018self, wu2021rethinking}.
Further details are provided in \cref{app:sec:details-ic}.
Relevant examples (including infracting, and non-infracting ones) and road geometry can be seen in \cref{fig:banner}.


\cref{tab: 2nd experiment} summarizes the results of this experiment. We compare against the baseline DM model and various guided models. Further, we compare against a prior work on this problem based on normalizing flows (NFs) \citep{zwartsenberg2023conditional}. Comparing to the baseline DM, \method{} lowers the infraction rates while maintaining a comparable distribution match. \method{} significantly outperforms the NF baseline because DMs are much more expressive generative models. The infraction rates reported here for the NF baseline are worse than those of \citet{zwartsenberg2023conditional}. This can be attributed to the higher average traffic density in our dataset compared to the INTERACTION dataset~\citep{zhan2019interaction} which \citet{zwartsenberg2023conditional} uses. In \cref{sec:app:ic-interaction} we empirically verify this by training the baseline DM on the INTERACTION dataset.

In the second section of \cref{tab: 2nd experiment} we report results of various guided diffusion models using a synthetic dataset generated by the baseline DM and labelled by the oracle. First, we consider a common approach of classifier guidance in which a time-independent classifier pre-trained \textit{on clean data} is utilized to perform (approximate) classifier guidance \citep{wu2023practical,bansal2023universal}.
In this approach, one-step estimate of $\rvx_0 \approx \frac{\rvx_t + \sigma_t^2 s_\theta(\rvx_t; t)}{\alpha_t}$ is obtained using the diffusion model. This estimate is then passed to the pre-trained classifier.
This method enhances the infraction rate but exhibits a significant decrease in ELBO, indicating a strong distributional misalignment. We also present ablations where we omit the importance sampling (``w/o IS'') step or forego balancing the dataset (``imbalanced classifier'') in \method{}. The ``w/o IS'' ablation improves infraction rates, but they both deteriorate the ELBO.

Different iterations of \method{}, however, shows even better infraction rates. The presence of lower ELBO can be justified by the approximations in \method{}'s objective function and classifiers not being trained to optimality. This is why the results deviate from theory to some extent. On the other hand, training the baseline DM is the only stage where we explicitly maximize the ELBO. Classifiers trained on all the other iterations only implicitly improve ELBO through guiding the model to not allocate probability mass on the invalid region. Finally, in the third section of \cref{tab: 2nd experiment} we demonstrate that our approach of distilling the resulting models back into a single one works well here too, sometimes even surpassing their teacher models. This can be attributed to knowledge distillation effects \citep{hinton2015distilling}.
Overall we find that \method{} works as expected, and provides a competitive infraction rate boost over our baseline model, without shifting the distribution.
To relate this to the introduction, as explained in \cref{sec:app:computational-cost}, using \method{} in producing non-infracting scenes for autonomous vehicle synthetic data generation would reduce GPU costs by 57\% on average.

\subsection{Motion Diffusion}
Our final experiment focuses on human motion generation. Diffusion models have been successfully applied to motion generation and editing tasks \citep{tevet2023human, zhang2024motiondiffuse, shafir2023human, xie2023omnicontrol, cohan2024flexible}. While these models produce diverse and realistic results, they often lack physical plausibility \citep{yuan2022physdiff}. For instance, issues like ground penetration frequently occur in the generated examples. Such imperfections can affect the quality of the generated motions and limit the model's applicability in real-world scenarios.

\begin{table*}[!t]
  \caption{Results of the Motion Diffusion experiment. ``Inf. per step'' is the average rate of generated motion frames with infraction while ``infraction'' is the average rate of generated motions that at least including one infracting frame. r-ELBO is a reweighted ELBO with the same weighting as in diffusion loss.}
  \centering
  \small
  \label{tab:mdm}
  \begin{tabular}{llllll}
    \toprule
    Method     & Infraction ($\%$) $\downarrow$ & Inf. per step ($\%$) $\downarrow$ & r-ELBO ($\times 10^{-2}$) $\uparrow$  & FID $\downarrow$  & KID ($\times 10^{-3}$) $\downarrow$\\
    \midrule
    MDM (baseline DM) & $27.66 \pm 0.77$  & $7.84 \pm 0.27$ & $-1.06 \pm 0.02$ & $0.445 \pm 0.040$ & $8.27 \pm 2.14$\\
    \method{} (Ours) & $24.25 \pm 0.35$ & $6.12 \pm 0.19$ & $\mathbf{-1.01 \pm 0.03}$ & $\mathbf{0.414 \pm 0.030}$ & $\mathbf{6.99 \pm 0.78}$ \\
    w/o IS (ablation)  & $\mathbf{22.85 \pm 0.18}$ & $\mathbf{5.47 \pm 0.18}$ & $-1.13 \pm 0.06$ & $\mathbf{0.415 \pm 0.030}$ & $8.40 \pm 1.83$\\
    \bottomrule
  \end{tabular}
\end{table*}

For our baseline DM, we use the pre-trained checkpoints provided by Motion Diffusion Model (MDM) \citep{tevet2023human}, tailored for text-conditioned motion generation. MDM is a DDPM model with a transformer-based architecture trained on the HumanML3D dataset \citep{guo2022generating}. It uses a pre-trained CLIP embedding module \citep{radford2021learning} for conditioning on the text descriptions.
To address the issue of ground penetration, we implement an oracle that labels motions with ground penetration at any point in their duration as negative.
We employ \method{} with a classifier having the same architecture as MDM, but the CLIP encoder, as the classifier is not text-conditional.

\cref{tab:mdm} summarizes our results of one iteration of \method{} on this dataset.
We report infraction rate, reweighted ELBO (referring to a uniform schedule of $\gamma_t$ in \cref{eq:score-matching}). We also report Fr\'echet Inception Distance (FID) \citep{heusel2017gans}, and Kernel Inception Distance (KID) \citep{binkowski2018demystifying} to measure quality of samples. \method{} improves on all metrics. While the ablation of omitting the IS weighting produces lower infraction rates compared to \method{}, it worsens the reweighted ELBO and KID. Hence, \method{} improves infraction, with a improved model likelihood and sample quality.
We conjecture the relatively smaller improvement in the motion experiment is because the baseline DM predicts $\rvx_0$ \citep{zhong2022guided}.

\section{Related Work}
\citet{hanneke2018actively} proposes a theoretical framework for oracle-based constraints. They however, do not provide practical considerations.
More recently, use of negative and invalid data have been explored to improve the training of generative models. \citet{sinha2021negative} uses heuristic functions to augment the training set of GANs with negative samples, while \citep{giannone2023learning} utilizes a pre-defined negative set.
Meanwhile, data redaction approaches propose methods for removing undesirable learned concepts from pre-trained generative models in safety and security applications \citep{gandikota2023erasing,schramowski2023safe}. Similarly, \citep{kong2023data} explores various data redaction methods, with the validity-based approach being the most relevant to our oracle-assisted guidance, although in the context of GAN literature. They implicitly carry out data redaction by integrating it into the discriminator and fine-tuning the generator. Another approach to constrained generative modeling explicitly incorporates the constraints in the model, similar to a final layer that maps to the constraint set \citep{stoian2024realistic}.

Several studies have explored constrained score-based modeling employing techniques such as diffusion bridges \citep{wu2022diffusion,liu2023learning}, barrier methods \citep{fishman2023diffusion}, or reflected diffusion \citep{lou2023reflected,fishman2023diffusion} or mirror diffusion \citep{liu2023mirror}.
Despite being effective, they rely on constraint-specific information such as closed form, linear, or convex constraints. This imposes strong limitations, making them impractical for general problems where such information is unavailable.

\section{Conclusion}
We have proposed a framework to incorporate constraints into diffusion models. These constraints are defined through an oracle function that categorizes samples as either \emph{good} or \emph{bad}. Importantly, such a flexibility allows for simple integration with human feedback. We have demonstrated our model on different modalities demonstrating how it can benefit safety constraints.

The current limitations we recognize, and the possible future directions for this work are (i) incorporating the true training dataset into the later iterations of the method, as the training dataset only affects the baseline DM. The next stages solely use synthetic data. Although we show theoretically that our guidance only improves the model, this lack of revisiting the true dataset in presence of practical errors and approximations poses challenges for large-scale adoption of our method. Our preliminary experiments of visiting the true dataset at the distillation time have not been successful yet. (ii) Avoiding stacking of classifiers, instead directly learning an artifact that can replace the previous classifier in our method, similar to \citep{de2021diffusion}, is vital to the computational  complexity of the method as the current computational cost scales linearly with the number of classifiers. (iii) Extending \method{} on tabular diffusion to support tabular data~\citep{kotelnikov2209tabddpm}, which compasses a mixture of continuous and categorical data to generalize our work is an inspiring area to future research. (iv) Bridging the gap the diffusion bridge-based approaches and our work which is practically applicable to a larger set of applications is another avenue for future developments.

\section*{Acknowledgments}
We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Canada CIFAR AI Chairs Program, Inverted AI, MITACS, the Department of Energy through Lawrence Berkeley National Laboratory, and Google. This research was enabled in part by technical support and computational resources provided by the Digital Research Alliance of Canada Compute Canada (alliancecan.ca), the Advanced Research Computing at the University of British Columbia (arc.ubc.ca), and Amazon.

\section*{Impact Statement}
Our work is intended to {\em improve} generative modeling performance by eliciting improved generalization performance.  Better generalization performance will lead to energy savings as less computation is required to generate ``good'' samples and better performance of systems that can be deployed for societal good such as self-driving cars that crash less often, robotic exoskeletons that are more safely and accurately performant.

\bibliography{bib}
\bibliographystyle{icml2024}

\newpage
\onecolumn
\appendix
\input{appendix.tex}
\end{document}