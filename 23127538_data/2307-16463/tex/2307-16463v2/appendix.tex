\section{Proofs}
\subsection{Proof of Theorem~\ref{thm:method:cls-guidance}}\label{app:proof-thm}
\begin{proof}
    Let $\alpha$ and $(1-\alpha)$ be the prior probabilities of positive and negative examples under $p_\theta(\rvx_t; t)$. Note that $\alpha$ remains independent of $t$ because
    \begin{align*}
        \alpha & = \int p_\theta(\rvx_t; t) p_\theta(\rvx_0 | \rvx_t) p_\theta(y=1 | \rvx_0)\, d\rvx_0 \,d\rvx_t
        = \int p_\theta(\rvx_t; t) p_\theta(\rvx_0 | \rvx_t) \oracle(\rvx_0)\, d\rvx_0 \,d\rvx_t                 \\
               & = \int p_\theta(\rvx_0, \rvx_t; t) \oracle(\rvx_0) \,d\rvx_0 d\rvx_t
        = \int p_\theta(\rvx_0; 0) \oracle(\rvx_0) \,d\rvx_0.
    \end{align*}
    The objective in \cref{eq:cls-time-cross-entropy} is equal to
    \begin{multline}
        -\meanp{t}{\meanp{p_\theta(\rvx_t)}{\meanp{p_\theta(\rvx_0 | \rvx_t)}{\oracle(\rvx_0)} \log \cls_\phi(\rvx_t; t) + \meanp{p_\theta(\rvx_0 | \rvx_t)}{(1 - \oracle(\rvx_0))} \log (1 - \cls_\phi(\rvx_t; t))}}\\
        = -\meanp{t}{\meanp{p_\theta(\rvx_t)}{p(y=1|\rvx_t) \log \cls_\phi(\rvx_t; t) + p(y=0|\rvx_t) \log (1 - \cls_\phi(\rvx_t; t))}}
    \end{multline}
    This is equivalent to \cref{eq:cls-cross-entropy} after replacing $q$ with $p_\theta$, i.e. sampling from the reverse process instead of the forward. Therefore, its optimal solution follows \cref{eq:cls-optimal}. Hence,
    \begin{align*}
            & \ \nabla_{\rvx_t} \log p_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log \cls_{\phi^*}(\rvx_t; t)                                                              \\
        =\  & \nabla_{\rvx_t} \log p_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log \alpha p_\theta(\rvx_t | y=1; t)                                                          \\
            & \hspace{3cm} - \nabla_{\rvx_t} \log \Big[ \overbrace{\alpha p_\theta(\rvx_t|y=1; t) + (1 - \alpha) p_\theta(\rvx_t|y=0; t)}^{p_\theta(\rvx_t; t)} \Big] \\
        =\  & \nabla_{\rvx_t} \log \alpha p_\theta(\rvx_t | y=1; t) = \nabla_{\rvx_t} \log p_\theta(\rvx_t | y=1; t)
    \end{align*}
\end{proof}

\subsection{Proof of Corollary~\ref{thm:method:improved-model}}\label{app:proof-corr}

\begin{proof}
    Since $p_{\theta, \phi^*}$ is defined as the distribution generated by simulating the SDE in \eqref{eq:reverse-process}, its score function $\nabla_{\rvx_t} \log p_{\theta, \phi^*}(\rvx_t; t)$ is by definition equal to $s_{\theta, \phi^*}(\rvx_t; t)$ \citep{risken1996fokker, song2019generative}. Similarly for the baseline DM we have $s_{\theta}(\rvx_t; t) = \nabla_{\rvx_t} \log p_\theta(\rvx_t; t)$.
    Therefore,
    \begin{align}
        \nabla_{\rvx_t} \log p_{\theta, \phi^*}(\rvx_t; t)
         & = s_{\theta, \phi^*}(\rvx_t; t)
        \overset{\text{\cref{eq:gen-neg-score}}}{=\joinrel=} s_\theta(\rvx_t; t) + \nabla_{\rvx_t} \log C_{\phi^*}(\rvx_t; t) \\
         & = \nabla_{\rvx_t} \log p_{\theta}(\rvx_t; t) + \nabla_{\rvx_t} \log C_{\phi^*}(\rvx_t; t)
        \overset{\text{Thm. \ref{thm:method:cls-guidance}}}{=\joinrel=} \nabla_{\rvx_t} \log p_{\theta}(\rvx_t | y=1)
    \end{align}
    Here we derived $s_{\theta, \phi^*}(\rvx_t; t) = \nabla_{\mathbf{x}_t} \log p_\theta(\rvx_t | y=1)$. By~\citep{anderson1982reverse}, we proved the first statement.

    The second statement  follows by decomposing $p_{\theta}(\rvx_t | y=1)$:
    \begin{align*}
        p_{\theta, \phi^*}(\rvx) = p_\theta(\rvx | y=1) \propto p_\theta(\rvx) \oracle(\rvx)\quad
        \Rightarrow \quad p_{\theta,\phi^*}(\rvx) = 0\quad \forall \rvx \in \Omega^\complement.
    \end{align*}
    For the last statement, we have
    \begin{align*}
        & \left.\begin{matrix}
        p_{\theta, \phi^*}(\rvx) = \frac{p_{\theta}(\rvx) \oracle(\rvx)}{\int p_{\theta}(\rvx) \oracle(\rvx) \,d \rvx} \\[1.1em]
        \int p_{\theta}(\rvx) \oracle(\rvx) \,d \rvx \leq 1 \\
                \end{matrix}\right\}
        \Rightarrow p_{\theta, \phi^*}(\rvx) \geq p_{\theta}(\rvx)\quad \forall \rvx \in \Omega  \\
        \overset{\gD \subseteq \Omega}{\Longrightarrow} & \log p_{\theta, \phi^*}(\gD)
        = \sum_{\rvx \in \gD} \log p_{\theta, \phi^*}(\rvx)
        \geq \sum_{\rvx \in \gD} \log p_{\theta}(\rvx) = \log p_{\theta}(\gD).
    \end{align*}
\end{proof}

% Figure environment removed

We demonstrate this corollary with a one-dimension density in \cref{fig:one-dimensional}. We show a ``base distribution'' $p(x)$ and the positive and negative regions $\Omega$ and $\Omega^\complement$, respectively. We can see that the distribution $p(x|y=1)$ assigns no mass to $\Omega^\complement$ and has a larger mass assigned to any point in $\Omega$.


\subsection{Equivalence of cross-entropy loss minimization and KL divergence minimization}\label{app:bayes-optimal-kl}
This is a well-known result in the literature. Nonetheless, we include the result and its proof here for completeness and ease of reference.
\begin{claim}
    minimizing the cross-entropy loss between the classifier output and the true label is equivalent to minimizing the KL divergence between the classifier output and the Bayes optimal classifier.
\end{claim}

\begin{proof}
    The Bayes optimal classifier $C^*(\rvx_t; t)$ approximates $q(y = 1 | \rvx_t)$. Let $p_\phi(y | \rvx_t)$ be the distribution represented by the learned classifier $C_\phi(\rvx_t; t)$ i.e., $p_\phi(y=1|\rvx_t) = C_\phi(\rvx_t; t)$. For an arbitrary diffusion time step $t$, the expected KL divergence between the Bayes optimal and the learned classifier therefore is
    \begin{align}
          & \meanp{q(\rvx_t)}{\kl{q(y | \rvx_t)}{p_\phi(y | \rvx_t)}}\nonumber  \\
        = & \ \meanp{q(\rvx_t)}{\meanp{q(y | \rvx_t)}{\log\frac{q(y|\rvx_t)}{p_\phi(y | \rvx_t)}}} \\
        = & \ \meanp{q(\rvx_t)}{q(y=1|\rvx_t)\log\frac{q(y=1|\rvx_t)}{C_\phi(\rvx_t; t)} + q(y=0|\rvx_t) \log\frac{q(y=0|\rvx_t)}{1 - C_\phi(\rvx_t; t)}}  \\
        = & \ \meanp{q(\rvx_t)}{H(q(y | \rvx_t))} - \meanp{q(\rvx_t)}{q(y=1 | \rvx_t) \log C_\phi(\rvx_t; t) + q(y=0 | \rvx_t)\log (1 - C_\phi(\rvx_t; t))}.
    \end{align}
    The first term is the expected entropy of the optimal classifier and is independent of $\phi$. Therefore,
    \begin{align}
          & \argmin_\phi \meanp{q(\rvx_t)}{\kl{q(y | \rvx_t)}{p_\phi(y | \rvx_t)}}\nonumber   \\
        = & \ \argmin_\phi - \meanp{q(\rvx_t)}{q(y=1 | \rvx_t) \log C_\phi(\rvx_t; t) + q(y=0 | \rvx_t)\log (1 - C_\phi(\rvx_t; t))} \\
        = & \ \argmin_\phi \text{CE}(q(y|\rvx_t), p_\phi(y|\rvx_t)).
    \end{align}
\end{proof}
Note that \cref{eq:cls-cross-entropy} is the expected cross entropy for different time steps $t$.


\subsection{Connection between Eq.~(\ref{eq: classifier loss}) and Eq.~(\ref{eq:cls-time-cross-entropy})}
In this section we make the connection between \cref{eq:gen-neg-true-objective} and \cref{eq:cls-time-cross-entropy} more clear. We start with \cref{eq:cls-time-cross-entropy}, ignoring the outer expectation with respect to $t$, is equal to

\begin{align}
    & - \Big(\meanp{p_\theta(\rvx_0, \rvx_t)}{\gO(\rvx_0)\log \cls_\phi(\rvx_t;t)+ (1 - \gO(\rvx_0))\log(1 - \cls_\phi(\rvx_t; t)}\Big)  \\
    = & \ -\textcolor{blue}{\int p_\theta(\rvx_0, \rvx_t)} \Big( \textcolor{blue}{p_\theta(y=1|\rvx_0)}\log\cls_\phi(\rvx_t; t) + \textcolor{blue}{p_\theta(y=0 | \rvx_0)}\log(1 - \cls_\phi(\rvx_t;t)) \Big)\,d\rvx_0d\rvx_t \\
    %%
    = & \ -\int \textcolor{blue}{p_\theta(y=1) p_\theta(x_0 | y=1) p_\theta(x_t | x_0)} \log\cls_\phi(\rvx; t)\,d\rvx_0d\rvx_t \nonumber \\
    & \ -\int \textcolor{blue}{p_\theta(y=0) p_\theta(x_0 | y=0) p_\theta(x_t | x_0)} \log(1 - \cls_\phi(\rvx_t;t))\,d\rvx_0d\rvx_t  \\
    \approx & \ -\int p_\theta(y=1) p_\theta(x_0 | y=1) \textcolor{blue}{q(x_t | x_0)} \log\cls_\phi(\rvx; t)\,d\rvx_0d\rvx_t \nonumber \\
    & \ -\int p_\theta(y=0) p_\theta(x_0 | y=0) \textcolor{blue}{q(x_t | x_0)} \log(1 - \cls_\phi(\rvx_t;t))\,d\rvx_0d\rvx_t  \\
    %%
    = & \  \alpha \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)}}  + (1 -\alpha) \meanp{p_\theta(\rvx_0 | y=0)}{\meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))}} \\
    :=& \frac{1}{N} \sum_{\rvx_0 \in \synth^+} \alpha \meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)} + \frac{1}{N} \sum_{\rvx_0 \in \synth^-} (1 - \alpha) \meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))},
\end{align}
which recovers \cref{eq: classifier loss}.


\subsection{\method{}'s objective function}\label{app:objective-is}
Here we show why \cref{eq: classifier loss} is an importance sampling estimator of the original objective function in \cref{eq:gen-neg-true-objective}.

\begin{align}
    \gL_{\phi}^{\text{cls}}(\alpha) := & \ \alpha \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t|\rvx_0)}{- \log \cls_\phi(\rvx_t; t)}}\nonumber  \\
    +  & \ (1 - \alpha) \meanp{p_\theta(\rvx_0 | y=0)}{\meanp{q(\rvx_t|\rvx_0)}{- \log (1 - \cls_\phi(\rvx_t; t))}} \\
    =  & \ - \meanp{p_\theta(y)}{ \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} }.
\end{align}
Now we apply importance sampling to $p_\theta(y)$ by sampling from $\pi(y)$ as the proposal distribution. Therefore,
\begin{align}
    \gL_{\phi}^{\text{cls}}(\alpha) = & \ - \meanp{p_\theta(y)}{ \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} }  \\
    = & \ - \meanp{\pi(y)}{ \frac{p_\theta(y)}{\pi(y)} \meanp{p_\theta(\rvx_0|y)}{\meanp{q(\rvx_t | \rvx_0)}{y \log C_\phi(\rvx_t; t) + (1-y) \log (1 - C_\phi(\rvx_t; t)}} } \\
    = & \ \frac{p_\theta(y=1)}{\pi(y=1)} \meanp{p_\theta(\rvx_0|y=1)}{\meanp{q(\rvx_t | \rvx_0)}{-\log C_\phi(\rvx_t; t)}}\nonumber \\
    & + \frac{p_\theta(y=0)}{\pi(y=0)} \meanp{p_\theta(\rvx_0|y=0)}{\meanp{q(\rvx_t | \rvx_0)}{-\log (1 - C_\phi(\rvx_t; t))}} \\
    = & \ \meanp{p_\theta(\rvx_0|y=1)}{ \frac{\alpha}{\pi(y=1)} \meanp{q(\rvx_t | \rvx_0)}{-\log C_\phi(\rvx_t; t)}}\nonumber \\
    & + \meanp{p_\theta(\rvx_0|y=0)}{ \frac{1 - \alpha}{\pi(y=0)} \meanp{q(\rvx_t | \rvx_0)}{-\log (1 - C_\phi(\rvx_t; t))}} \label{eq:loss-is-proof}
\end{align}
In our case, $\pi(y)$ is a uniform Bernoulli distribution i.e., $\pi(y=1) = \pi(y=0) = 0.5$. Therefore, minimizing \cref{eq: classifier loss} is equivalent to minimizing a Mone Carlo estimate of \cref{eq:loss-is-proof}.

\section{Experimental details}

\subsection{Checkerboard Experiment}
\paragraph{Architecture} We use a fully connected network with 2 residual blocks as shown in \cref{fig:toy-arch}. The hidden layer size in our experiment is 256 and timestep embeddings (output of the sinusiodal embedding layer) is 128. Our classifier has a similar architecture, the only difference is that the classifier has a different output dimension of one. Our baseline DM and classifier networks both have around 330k parameters.
\paragraph{Training the baseline DM} We train our models baseline models on a single GPU, (we use either of GeForce GTX 1080 Ti or GeForce GTX TITAN X) for 30,000 iterations. We use the Adam optimizer \citep{kingma2014adam} with a batch size of $3 \times 10^{-4}$ and full-batch training i.e., our batch size is 1000 which is the same as the training dataset size.
\paragraph{Training the classifiers} Each classifier is trained on a fully-synthetic dataset of 100k samples which consists of 50k positive and 50k negative samples. This dataset is generated with 100 diffusion steps. We train the classifier for 20k iterations with a batch size of 8192. We use Adam optimizer with a learning rate of $3 \times 10^{-3}$.
\paragraph*{Distillation} The distilled models have the same architecture and hyperparameters as the baseline DM model. They are trained for 250k iterations on the true dataset with a batch size of 1000. We use Adam optimizer with a learning rate of $3 \times 10^{-4}$.
\paragraph{Diffusion process} We use the EDM framework in this experiment with a preconditioning similar to the one proposed in \citet{karras2022elucidating}. In particular, the following precoditioning is applied to the the network in \cref{fig:toy-arch}, called $F_\theta(\rvx_t, t)$, to get $D_\theta(\rvx_t; t)$ which returns an estimate of $\rvx_0$.
\begin{equation}
    D_\theta(\rvx_t; t) = \frac{\sigma_{\text{data}}^2}{\sigma(t)^2 + \sigma_{\text{data}}^2} \rvx_t + \sigma(t)F_\theta\left( \frac{1}{\sqrt{\sigma(t)^2 + \sigma_{\text{data}}^2}} \rvx_t; \frac{1}{4} \ln(\sigma(t)) \right),
\end{equation}
where $\sigma_{\text{data}} = 1$. Since smaller noise levels are important in our application, we changed the training distribution of $t$ from the log-Normal used in \citet{karras2022elucidating} to a log-uniform with the support of $\sigma_{\text{max}} = 80$ and $\sigma_{\text{min}} = 2 \times 10^{-3}$. In total, we spent around 250 GPU-hours for this experiment.
% Figure environment removed
\paragraph{Sampling} We use the second-order Heun solver of \citet{karras2022elucidating} with 100 sampling steps and $S_{\text{churn}} = 10$. We modify the schedule of $\sigma$ to a log-linear schedule from $\sigma_{\text{max}}$ to $\sigma_{\text{min}}$.

\subsection{Traffic Scene Generation}\label{app:sec:details-ic}
\paragraph{Overview} This section provides additional details for the traffic scene generation task. The architectures for training the baseline DM model, classifiers and distillation models are majorly based on transformers introduced by \citet{vaswani2017attention} . In particular, the architecture backbone consists of an encoder, a stack of attention residual blocks, and a decoder. Each of them will be discussed in detail later.
The original data input shape is $[B, A, F]$ corresponding to $A$ vehicles and $F$ feature dimensions in a batch with $B$ many scenes.

In terms of parameters, the attention layers comprise the major portion of the entire architectures. This leads to the difference in decoder being relatively minor, and the resulting architectures all contain approximately $6.3$ million parameters. We use NVIDIA A100 GPUs for training and validating models, synthetic datasets generation with around $400$ GPU-hours in total. We train each model with a batch size of $64$ and Adam optimizer with a learning rate of $10^{-4}$.


\paragraph{Encoder and time embeddings}
To generate input features, we use sinusoidal positional embeddings to embed the diffusion time step and 2-layer MLP with activation function SiLU to embed the original data separately into $H = 196$ hidden feature dimensions. The sum of the two embeddings is the input that is fed into the attention-based architecture.

\paragraph{Self-attention and cross-attention layers}
The major implementation of multi-head ($k=4$) attention blocks is built on Transformer~\citep{vaswani2017attention}.
Applying self-attention across agents enables model to learn the multi-agent interactions, while applying map-conditional cross-attention between agents and map allows agents to interact with the road representations.
To prepare road image for model input, we use a convolutional neural network and a feed-forward network~\citep{carion2020end} to generate a lower-resolution map $m'\in\sR^{196\times 32\times 32}$ from the original image $m\in\sR^{3\times 256\times 256}$. Since the transformer architecture is permutation-invariant, we add a 2D positional encoding~\citep{parmar2018image, bello2019attention} based on $m'$ on the top of the map representation to preserve the spatial information of the image.

\paragraph{Relative Positional Encodings (RPEs)}During experiments, we find the collision rate is much higher than the offroad rate. In order to effectively lower the frequency of or completely avoid vehicle collision occurrence, we manage to capture the relative positions by performing relative positional encodings (RPEs) in self-attention residual blocks and enforce the vehicles being aware of the other vehicles in close proximity in each scene.
Following~\citet{shaw2018self, wu2021rethinking, harvey2022flexible}, we compute the distances of each pair of vehicles and summarise into a tensor of shape $[B, A, A]$, where $d^b_{ij}$ is the distance between vehicle $i$ and $j$ in the $b^{\text{th}}$ scene.
We choose to use sinusoidal embeddings (similar to how we embed diffusion time $t$) to parameterize $d^b_{ij}$ rather than logarithm function $f_{\text{RPE}}(d^b_{ij}) = \log(1 + d^b_{ij})$, as we need to adequately amplify the pairwise distances between vehicles when it is comparably small.
We perform this operation together with diffusion time embedding at each diffusion time step, and we regard their sum as the complete pairwise distance embeddings.
The resulting embedding tensor $\rvp$ is of the shape $[B, A, A, H]$, where $\rvp^b_{ij}$ is the encoding vector of length $H$ representing the pairwise distance of vehicle $i$ and $j$ in the $b^{\text{th}}$ scene.

In each scene, we have an input sequence, $\rvx = (\rvx_1, \cdots, \rvx_A)$, and each $\rvx_i$ is linearly transformed to query $\rvq_i = W^Q \rvx_i$, key $\rvk_i = W^K \rvx_i$ and value $\rvv_i = W^V \rvx_i$.
We also apply linear transformation onto RPEs to obtain query $\rvp_{ij}^Q = U^Q \rvp_{ij}$, key $\rvp_{ij}^K = U^K \rvp_{ij}$ and value $\rvp_{ij}^V = U^V \rvp_{ij}$.
Then the add-on output from the self-attention residual block is the aggregated outputs of the vanilla transformer and the relative-position-aware transformer:
\begin{align}
    \rvx_i^{\text{output}}        & = \rvx_i + \sum_{j=1}^A \alpha_{ij} (\rvv_j + \rvp_{ij}^V)                                                                                                           \\
    \text{where}\quad \alpha_{ij} & = \frac{\exp(e_{ij})}{\sum_{k=1}^A \exp(e_{ik})}\ \text{and}\  e_{ij} = \frac{\rvq_i^\top \rvk_j + \rvp_{ij}^{Q^\top}\rvk_j + \rvq_i^\top\rvp_{ij}^K}{\sqrt{d_\rvx}}
\end{align}

\paragraph{Decoder}
The settings for baseline, distillation models and classifiers are almost identical except the decoder for producing the final output. For baseline and distillation models, we apply 2-layer MLP and reconstruct the output of the shape $[B, A, H]$ from the final attention layer into $[B, A, D]$ through the decoder.
To ensure we output individual label for each vehicle with classifiers, we conduct the operations as follows.
The decoder takes the hidden representation of the shape $[B, A, H]$ and produces a tensor with feature dimension $F'=1$ with a 2-layer MLP, which is the predicted labels from the classifiers.


\subsection{Motion Diffusion}
For the task of text-conditional motion generation, we use the HumanML3D dataset \citep{guo2022generating}. This dataset contains 14,616 human motions annotated by 44,970 textual descriptions. It includes motions between 2 and 10 seconds in length and their total length amounts to 28.59 hours. Each motion is between $36$ and $196$ frames, with the majority of them comprising $196$ frames. Each frame is represented by a $263$-dimensional feature vector, resulting in a dimensionality of over $51,000$ for the largest motions.

We used the official implementation of MDM\footnote{\href{https://github.com/GuyTevet/motion-diffusion-model}{\url{https://github.com/GuyTevet/motion-diffusion-model}}} for our Motion Diffusion experiment. For the baseline DM, we used their officially released best pretrained checkpoint of text-to-motion task on HumanML3D dataset. We generate a synthetic dataset of around 250k positive and 250k negative examples from the baseline DM which is a DDPM-based model with 1000 diffusion steps. We then define our classifier architecture using their code base. Following our other experiments, our classifier architecture is the same as the baseline DM model. We train the classifier with a batch size of 128 and a learning rate of $10^{-4}$ for 300k iterations. Otherwise, we use the same hyperparameters as in \citet{tevet2023human}. All the training and data generation is done on A100 GPUs.

To compute the FID scores, in accordance with \citet{tevet2023human}, we generate one motion for each caption in the HumanML3D test set, resulting in a total of 4,626 generated motions.

The total compute used for this experiment (generating the datasets and training the classifiers) was around 600 GPU-hours.

% Figure environment removed

\section{Ablation studies}

\subsection{Label imbalance}\label{app:ablation-imbalance}
As mentioned in \cref{sec:method:classifier}, generating synthetic datasets for training the classifiers without careful planning leads to significant label imbalance issues, which impede the training process. In this section we perform an ablation study to empirically demonstrate its detrimental impact in our experiments.

As a reminder, \method{} requires an equal number of positive and negative examples in the synthetic datasets, and it employs importance sampling to address any distribution shift introduced.
In the ablated experiment (referred to as "imbalanced" in the results), the ratio of positive to negative examples is model-dependent, being equal to the model's infraction rate.
It results in a gradual increase in the dominance of positive examples as the model's infraction rate decreases.

\paragraph*{Checkerboard experiment} We repeat our checkerboard experiment with and without \method{}'s imbalance correction. In each iteration, we create a synthetic dataset of 20,000 samples and train a binary classifier for 10,000 iterations. The other details are the same as the experiment in the main text. We show the performance of these models in \cref{fig:2d-ablation-imbalance}.
We observe that ``imbalaced'', the ablated version of \method{}, is consistently outperformed by \method{} in infraction rate. Furthermore, the performance gap between the two methods widens as the models improve.
However, it is worth noting that \method{} achieves a comparable ELBO to that of ``imbalanced'' despite these infraction rate differences.

\paragraph*{Traffic Scene Generation} We conduct a similar ablation as described above, where we train classifiers on a imbalanced datasets of the same size as our method. The results of this ablated experiment are presented in \cref{tab: 2nd experiment} in the main text.

\subsection{Synthetic dataset size}
We conducted an ablation on the number of samples required on the checkerboard and the traffic scene generation tasks.~\cref{fig:toy-ablation-n} and~\cref{tab:ic-ablation-n} show our results. We observe that the infraction rate constantly decreases irrespective of the dataset size. However, in order to avoid distribution shift we need a large enough dataset, as is evident from the ELBO plot. It is important to emphasize that the classifiers are trained on fully synthetic data generated by the model itself. Therefore, in principle we have access to an unbounded number of samples. As our results show, for the best performance, it is important to ensure the sample size is sufficiently large.

\section{Additional results}


% Figure environment removed

\begin{table}[!t]
\centering
  \caption{Ablation study for the traffic scene generation experiment on synthetic dataset size}
  \label{tab: 2nd experiment ablation study}
  \begin{tabular}{lllll}
    \toprule
    Classifier dataset size     & Collision ($\%$) $\downarrow$  & Offroad ($\%$) $\downarrow$ & Infraction ($\%$) $\downarrow$ & r-ELBO ($\times 10^{-2}$) $\uparrow$  \\
    \midrule
    baseline DM  & $28.3\pm 0.7$  & $1.3\pm 0.1$ & $29.3\pm 0.6$ &  $-27.5\pm0.01$   \\
        \midrule
    $8,000$ & $23.8 \pm 0.4$ & $0.9\pm 0.2$ & $24.5 \pm 0.5$ & $-28.0\pm 0.01$ \\
    $40,000$ & $19.7\pm 0.8$ & $0.8\pm 0.2$ & $20.3\pm 0.9$ & $-27.8\pm0.01$\\
    $80,000$ & $17.6\pm 0.7$ & $0.7\pm 0.2$ & $18.2\pm 0.6$ & $-27.7\pm 0.01$\\
    $400,000$ & $16.6 \pm 0.7$ & $0.8\pm 0.2$ & $17.5\pm 0.7$ & $-27.7\pm 0.01$ \\
    $800,000$ & $16.4\pm 0.5$ &  $0.9\pm 0.1$ & $17.2\pm 0.4$ & $-27.7\pm 0.01$ \\
    \bottomrule
  \end{tabular}
  \label{tab:ic-ablation-n}
\end{table}


\subsection{Computational cost and sampling latency} \label{sec:app:computational-cost}
Here we discuss the latency of different models considered in this paper. We first report the wall-clock time of generating samples from models. However, in order to deploy any of these models, one should ensure a generated sample is valid. Therefore, all the models should be used together with rejection sampling. We discuss the latency in conjunction with rejection sampling in the second part of this section.

\paragraph{Latency} We compute the average wall-clock time of the baseline models and iterations of \method{} for different experiments and present the results in~\cref{tab:comp-time}.
We observe that additional classifiers linearly increase the running time (for conciseness, we have not included further iterations of \method{} on checkerboard experiments in \cref{tab:comp-time} as its runtime simply continues growing linearly). Moreover, in the Traffic Scenes and checkerboard experiments, the overhead of each classifier is larger than the runtime of the baseline model alone (almost 1.5 times). This is because the architecture of our classifier is the same as the baseline model and for each forward pass of a classifier-guided model, one forward and one backward pass through the classifier is required. However, in the Motion diffusion experiment, since the classifier is not text-conditional, this overhead is relatively smaller. We also report the total time to generate one batch of samples for all the models in \cref{tab: 2nd experiment}. This table also includes the number of parameters for each model as a proxy for memory consumption. It is important to note that our distilled models have the exact same latency and memory consumption as the baseline diffusion model, since we use the same architecture for the distilled model.

\paragraph{Rejection sampling} To ensure that a sample from a model is valid, deploying any of these models requires using them with rejection sampling.
Assume a model with infraction rate of $\epsilon$ and sampling time of $t$ seconds is given. A rejection sampling loop with this model takes $\frac{1}{1 - \epsilon} t$ seconds. However, in a different, more realistic setting, we require an accepted sample in ``one iteration'' of running the model. Since all models have some infraction rate, we instead generate a batch of samples and require at least one non-infracting sample with high probability. As stated in \cref{sec:intro}, generating at least one non-infracting sample with $(1 - \delta)$ probability requires $\frac{\log \delta}{\log \epsilon}$ parallel samples. Therefore, any improvement to $\epsilon$ leads to improved computational complexity. In particular,  we get the following reductions in the required number of samples:


\begin{table}[!t]
\centering
    \caption{Wall-clock time of one denoising step}
    \begin{tabular}{llll}
        \toprule
        & \multicolumn{3}{c}{Time per diffusion step (s) ($\times 10^{-3})$}\\
        \cmidrule{2-4}
        Method & Checkerboard & Traffic Scenes & Motion Diffusion \\
        \midrule
        Baseline DM / distilled \method{} & $0.2 \pm 1 \times 10^{-2}$ & $32 \pm 9\times 10^{-3}$ & $83 \pm 2 \times 10^{-1}$ \\
        \method{} (iteration 1) & $0.5 \pm 2 \times 10^{-2}$ & $86 \pm 2\times 10^{-2} $ & $171 \pm 8 \times 10^{-1}$ \\
        \method{} (iteration 2) & $0.8 \pm 2 \times 10^{-2}$ & $138 \pm 3\times 10^{-2}$ & - \\
        \bottomrule
    \end{tabular}
    \label{tab:comp-time}
\end{table}

\begin{table}[!t]
\centering
    \caption{Sampling time of the traffic scenes experiment (1000 steps)}
    \begin{tabular}{lll}
    \toprule
    Method & Parameters (M)  & Latency (s) \\
    \midrule
    Baseline DM & $6.3$ & $38.58 \pm 0.20$ \\
    \method{} (iteration 1) & $12.6$ & $86.71 \pm 0.19$ \\
    \method{} (iteration 2) & $18.9$ & $138.73 \pm 0.42$ \\
    \method{} (distilled) & $6.3$ & $38.43 \pm 0.37$ \\
    Time-Independent classifier & $12.6$ & $126.40 \pm 0.20$ \\
    Imbalanced classifier & $12.6$ & $86.88 \pm 0.20$ \\
    w/o IS classifier & $12.6$ & $86.81 \pm 0.21$ \\
    \bottomrule
    \end{tabular}
    \label{tab:latency-ic}
\end{table}

\begin{itemize}
    \item 47\% on the checkerboard experiment (baseline vs. distillation of 5 iterations of Gen-neG),
    \item 57\% on the traffic scenes experiment (baselines vs. distillation of stacked Gen-neG),
    \item 9\% on the motion diffusion experiment.
\end{itemize}

We conjecture the relatively smaller improvement in the motion diffusion experiment is because the baseline DM predicts $\rvx_0$. A follow up to MDM, argues that $\rvx_0$-prediction models are hard to guide (see appendix A of \citet{zhong2022guided}).

\subsection{Faster synthetic dataset generation}
With the iterative stacking of the classifiers in \method{}, as the model becomes better in avoiding invalid samples, it becomes harder to generate a balanced synthetic dataset. To further reduce the computational cost for cases where the infraction rate is very small, one can employ Sequential Importance Sampling (SIS) using a proposal distribution with higher infraction rate. Concretely, assume we use an SDE solver with pre-defined timesteps $t_0=0 < t_1 < \ldots < t_T=1$ to generate the samples. Therefore the loss function in \cref{eq:gen-neg-true-objective} becomes 
\begin{equation}
    \gL_\phi^{\text{cls}} = \mathbb{E}_{p_\theta(\mathbf{x}_0)}\Big[l(\mathbf{x}_0)\Big] = \mathbb{E}_{p_\theta(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})}\Big[l(\mathbf{x}_0)\Big] = \mathbb{E}_{\pi(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})}\Big[\frac{p_\theta(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})}{\pi(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})} l(\mathbf{x}_0)\Big],
\end{equation}
where $l(\rvx_0) = \meanp{t, q(\rvx_t | \rvx_0)}{ \oracle{}(\rvx_0) \log \cls_\phi(\rvx_t; t) + (1 - \oracle{}(\rvx_0)) \log (1 - \cls_\phi(\rvx_t; t)) }$ and $\pi$ is a proposal distribution defined as another diffusion model for example, an earlier iteration of Gen-neG. Therefore, $\frac{p(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})}{\pi(\mathbf{x}_{t_0}, \ldots, \mathbf{x}_{t_T})} = \prod_{i=1}^T\frac{p(\mathbf{x}_{t-1} | \mathbf{x}_t)}{\pi(\mathbf{x}_{t-1} | \mathbf{x}_t)}$ in which both the numerator and denominator are Gaussian distributions with the same variances but different means.

% Figure environment removed

\cref{fig:app:faster-sampling} shows the results of this approach on the checkerboard experiment. It shows that SIS dataset sampling time grows linearly and independent of the infraction rate while maintaining a comparable performance. This linear growth is due to the linear growth in complexity of (non-distilled) Gen-neG models.



\begin{table}[!t]
\centering
  \caption{Comparison of a diffusion model and normalizing flow model trained on the INTERACTION dataset. The normalizing flow results are taken from \citet{zwartsenberg2023conditional}.}
  \label{tab:app:ic-interaction}
  \begin{tabular}{llll}
    \toprule
    Method    & Collision ($\%$) $\downarrow$  & Offroad ($\%$) $\downarrow$ & Infraction ($\%$) $\downarrow$  \\
    \midrule
    Diffusion model  & $7.34\pm 0.16$  & $6.94\pm 0.06$ & $13.62\pm 0.13$   \\
    Normalizing flow~\citep{zwartsenberg2023conditional} & $9.00\pm 1.00$& $12.00\pm 1.00$ & $20.00\pm 1.00$ \\
    \bottomrule
  \end{tabular}
\end{table}


\subsection{INTERACTION dataset}\label{sec:app:ic-interaction}
Here we report our results of training a baseline DM model on the INTERACTION dataset~\citep{zhan2019interaction}. \cref{tab:app:ic-interaction} shows the superior performance of our diffusion model compared to the normalizing flow results from \citet{zwartsenberg2023conditional}. The lower infraction rates compared to \cref{tab: 2nd experiment} suggests that the INTERACTION dataset is a simpler dataset compared to the one we used in \cref{sec:IC-experiment}. One can further improve upon the diffusion model in \cref{tab:app:ic-interaction} by using it as a baseline DM in \method{}.


\subsection{More visualization results for the traffic scene generation experiment}\label{app:ic-more-results}
In \cref{fig:ic-more-results} we report more visualization results from our traffic scene generation experiment. This figure follows from and adds more details to \cref{fig:banner}.

\subsection{Overfitting in the checkerboard experiment}\label{app:overfitting}
Here we present our results regarding the overfitting of the baseline DM in the checkerboard experiment. We run an experiment with 200,000 training iterations, much larger than the 30,000 iterations in the reported results. As we can see in \cref{fig:toy-overfit}, the infraction rate keeps decreasing. However, the model starts overfitting after around 30,000 iterations, as measured by the ELBO on a held-out set. This suggests that the architecture is expressive enough to model sharp jumps in the learned density. However, simply training it on a small dataset without incorporating any prior on ``where to allocate its capacity'' fails because the model does not receive any signal on where the actual ``sharp jump'' is. \method{}, on the other hand, provides this kind of signal through the oracle-assisted guidance.
% Figure environment removed
% Figure environment removed