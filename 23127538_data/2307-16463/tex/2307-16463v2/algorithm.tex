\begingroup
\crefname{equation}{Eq.}{Eqs.}
\Crefname{equation}{Eq.}{Eqs.}

\begin{algorithm}[!t]
    \caption{\method{}}\label{alg: iterative training}
    \begin{algorithmic}[1]
        \STATE {\bfseries Input:} dataset $\gD$, oracle $\oracle$, synthetic dataset size $N$
        \STATE $i \leftarrow 0$
        \STATE $\theta_i \leftarrow \argmin_\theta \gL^{\text{DM}}_\theta$ \COMMENT{train baseline DM, \cref{eq:score-matching}}
        \STATE ${s_i} \leftarrow s_{\theta_i}(\rvx_t; t)$
        \REPEAT
            \STATE $\synth^+_i,\synth^-_i \leftarrow \varnothing$
            \REPEAT
              \STATE $\synth^+,\synth^- \leftarrow$ generate more samples  from DM with score function ${s_i}$ and label with $\oracle$
              \STATE $\synth^+_i \leftarrow \synth^+_i \cup \synth^+$, $\synth^-_i \leftarrow \synth^-_i \cup \synth^-$
            \UNTIL{$\min(|\synth^+_i|,|\synth^-_i|)<N$}
            \STATE $\alpha_i \leftarrow |\synth^+_i| / (|\synth^+_i| + |\synth^-_i|)$ \COMMENT{Estimate class prior probabilities for Bayes optimal classifier} 
            \STATE $\synth^+_i \leftarrow \subsample(N,\synth^+_i), \synth^-_i \leftarrow \subsample(N,\synth^-_i)$ \COMMENT{balance dataset for IS classifier training}
            \STATE $\phi_i \leftarrow \argmin_\phi \hat{\gL}^{\text{cls}}_\phi(\alpha_i, \synth^+_i, \synth^-_i)$ \COMMENT{train guidance classifier, \cref{eq: classifier loss}}
            \STATE $i \leftarrow i+1$
            \IF{distill}
            \STATE $\psi \leftarrow \argmin_\psi \gL^{\text{dtl}}_\psi$ \COMMENT{See \cref{eq:distillation}}
            \STATE ${s_i} \leftarrow  s_{\psi}(\rvx_t; t)$ 
            \ELSE
                % \COMMENT{``stack'' guidance classifiers}
                \STATE ${s_i} \leftarrow {s_{i-1}} + \nabla_{\rvx_t}\log C_{\phi_i}(\rvx_t; t)$ \COMMENT{See \cref{eq:gen-neg-score}}
            \ENDIF
        \UNTIL{done}
        \STATE {\bfseries Output:} DM score function $s_i$
    \end{algorithmic}
\end{algorithm}

\endgroup % End of local abbreviation settings