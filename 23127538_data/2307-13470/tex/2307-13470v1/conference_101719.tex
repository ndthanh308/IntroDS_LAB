\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%\usepackage{cite}
%\usepackage{amsmath,amssymb,amsfonts}
%\usepackage{algorithmic}
%\usepackage{graphicx}
%\usepackage{textcomp}


\usepackage[T1]{fontenc}% optional T1 font encoding
\usepackage{float}
\usepackage{multirow}% http://ctan.org/pkg/multirow
\usepackage{array}
\usepackage{makecell}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)
\usepackage{enumerate}   
%\usepackage{amsmath}
%\usepackage[margin=1.5in]{geometry}    % For margin alignment
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
%\usepackage{arevmath}     % For math symbols
\usepackage{amsfonts}

\usepackage[noend]{algpseudocode}
\usepackage{optidef}
\usepackage{subcaption}
\usepackage[dvipsnames]{xcolor}
%\usepackage{siunitx,booktabs}
\usepackage{siunitx,booktabs}
\usepackage{xcolor}

\usepackage{amsthm}

\usepackage[justification=justified]{caption}

\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{multirow}
\newtheorem{defi}{Definition}[section]

\newtheorem{theorem}{Theorem}%[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}{Example}%[section]
\newtheorem{prop}{Proposition}%[section]
\newtheorem{assumption}{Assumption}%[section]
%\newcommand\Mycomb[2][^{|I_{pv}|}]{\prescript{#1\mkern-0.5mu}{}C_{#2}}
%\newcommand{\rulesep}{\unskip\dotfill{\color{red}\vrule}\dotfill\ignorespaces}
\newcommand{\rulesep}{\unskip\ \vrule }





\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Combinatorial Auctions and Graph Neural Networks for Local Energy Flexibility Markets}

\author{\IEEEauthorblockN{Awadelrahman M. A. Ahmed}
\IEEEauthorblockA{\textit{Department of Informatics} \\
\textit{University of Oslo}, Norway \\
aahmed@ifi.uio.no}
\and
\IEEEauthorblockN{Frank Eliassen,~\IEEEmembership{Member,~IEEE,}}
\IEEEauthorblockA{\textit{Department of Informatics} \\
	\textit{University of Oslo}, Norway \\
	frank@ifi.uio.no}
\and
\IEEEauthorblockN{Yan Zhang,~\IEEEmembership{Fellow,~IEEE}}
\IEEEauthorblockA{\textit{Department of Informatics} \\
	\textit{University of Oslo}, Norway \\
yanzhang@ieee.org}

}

\maketitle

\begin{abstract}

This paper \footnote{Accepted in The IEEE PES ISGT Europe 2023 (ISGT Europe 2023), Grenoble, France, on October, 2023.} proposes a new combinatorial auction framework for local energy flexibility markets, which addresses the issue of prosumers' inability to bundle multiple flexibility time intervals. To solve the underlying NP-complete winner determination problems, we present a simple yet powerful heterogeneous tri-partite graph representation and design graph neural network-based models. Our models achieve an average optimal value deviation of less than 5\% from an off-the-shelf optimization tool and show linear inference time complexity compared to the exponential complexity of the commercial solver. Contributions and results demonstrate the potential of using machine learning to efficiently allocate energy flexibility resources in local markets and solving optimization problems in general.

%Establishing local energy markets between prosumers and system operators can effectively extract energy flexibilities from prosumers' resources. However, current market mechanisms do not adequately address prosumers' needs to bundle multiple time intervals, resulting in unfavorable individual flexibility allocations. This paper presents two novel contributions and two main results. Firstly, we propose a combinatorial auction framework for local energy flexibility markets that enables prosumers to express complementarities and substitutabilities of flexibility intervals. Secondly, we propose a simple yet powerful heterogeneous tri-partite graph representation for the underlying NP-complete winner determination problems, which we efficiently solved by designing graph neural network-based models. We extensively tested and evaluated the models on problems of different complexities. The first result is the proposed models achieving an average optimal value deviation of below 5\% from the expert solver. The second result is that the models showed a linear time complexity compared to the exponential complexity of the expert solver.

%Abstract—Establishing energy markets at local level is indispensable to effectively extract energy flexibilities embedded in the prosumers’ energy resources. Based on the availability of their resources, prosumers can offer flexibility services at different time intervals. While prosumers are inclined to bundle multiple time intervals to offer collective energy flexibility offers, current energy market mechanisms are not designed to address this need, what exposes the prosumers to be allocated unfavourable individual flexibility intervals. To enable the prosumers to express complementarities and substitutabilities of flexibility intervals, this paper proposes a combinatorial auction framework for local flexibility markets. To efficiently solve the combinatorial auction underlying NP-complete winner determination problem, this paper illustrates the design of graph neural network-based models to imitate an expert solver. We start by designing a bottom-up procedure to generate problem instance datasets emulating real world problems. Then, to naturally process the winner determination problems by our graph-based models, we propose a simple yet expressive heterogeneous tri-partite graph representation. The models have been extensively tested and evaluated on problems of different complexities. The models’ predicted allocations achieved below 5\% average optimal value deviation from the expert solver. Results also show linear time complexity for the models’ inference compared to the expert solver exponential complexity.
\end{abstract}

\begin{IEEEkeywords}
graph neural networks, combinatorial auctions, energy flexibility, local energy markets
\end{IEEEkeywords}


\section{Introduction}


The increasing adoption of photovoltaic (PV) energy at the distribution level due to the decreasing costs of solar and battery storage systems poses significant challenges for modern power systems. The European Commission has recognized the importance of managing local energy communities to integrate distributed energy resources and enable end-users to become active energy service providers \cite{european2017directive}. Energy flexibility refers to a system's capacity to utilize its resources in response to fluctuations in the net \cite{lannoye2012evaluation}. Prosumers who own PV systems can offer energy flexibility services at a local level, which can be combined and utilized in the energy market with the help of aggregators. In this market, prosumers serve as sellers, while the distribution system operator (DSO) is the buyer, and flexibility services serve as the commodity.



%The decreasing costs of solar and battery storage systems have resulted in increased adoption of photovoltaic (PV) energy at the distribution level. However, this creates significant challenges for modern power systems. The European Commission recognizes the need for managing local energy communities to integrate distributed energy resources and enable end-users to become active energy service providers \cite{european2017directive}. Prosumers, who own PV systems, can offer energy flexibility services at a local level by supplying ramp-up services that use local resources or ramp-down services that consume energy from the grid. Although these resources are small and dispersed, they can be combined and utilized in the energy market with the help of aggregators. In this market, prosumers serve as sellers, while the distribution system operator is the buyer, and flexibility services serve as the commodity \cite{lannoye2012evaluation}.

%The decreasing costs of solar and battery storage systems have led to more adoption of photovoltaic (PV) energy at the distribution level, creating formidable challenges for modern power systems. The European Commission supports energy policies that manage local energy communities and offer localized solutions \cite{european2017directive}. Integrating distributed energy resources could empower end-users to become active energy service providers. Energy flexibility refers to a system's capacity to allocate its resources to react to changes in net \cite{lannoye2012evaluation}. Owners of photovoltaic systems acting as prosumers can supply energy flexibility services at a local level, namely ramp-up services that use local resources, or ramp-down services that consume energy from the grid. Although these resources are small and widely dispersed, they can be combined and utilized in the energy market with the help of aggregators. These intermediaries collect the resources to meet the minimum bid thresholds for the energy market, which are typically in the megawatt range. In the local energy market, prosumers serve as sellers, while the distribution system operator is the buyer, and flexibility services serve as the commodity.


%The plummeting costs of solar and battery storage systems in recent decades have resulted in a significant increase in the adoption of photovoltaic (PV) energy at the distribution level. This trend, however, has introduced formidable challenges to modern power systems, which prompted the European Commission to endorse energy policies aimed at managing local energy communities and providing localized solutions \cite{european2017directive}. Nevertheless, it is worth noting that the integration of distributed energy resources has the potential to transform end users from passive energy consumers to active energy services providers.

%The cost of solar and battery storage systems showed rapid declines in the recent decades \cite{feldman2021us}. The price affordability led to the increase of photovoltaic (PV) penetration at the distribution level imposing significant challenges to modern power systems. To alleviate this, the European Commission endorsed energy policies to manage the local energy communities and provide local solutions \cite{european2017directive}. Nonetheless, a positive side is that the distributed energy resources can upgrade end users from being passive energy consumers to being active energy services providers.


% Energy flexibility refers to a system's capacity to utilize its resources in response to fluctuations in the net \cite{lannoye2012evaluation}). In the context of energy prosumers, owners of PVs can contribute energy flexibility services at the local level. This study examines the provision of ramp-up and ramp-down services by leveraging local flexible resources. While these resources are small and widely distributed, they can be aggregated by intermediaries known as aggregators, which are essential in facilitating participation in the energy market. Aggregators' role involves accumulating these resources to meet the energy market minimum bid thresholds, which are in the megawatt scale. In the local energy market, prosumers act as sellers, while the distribution system operator (DSO) serves as the buyer, and flexibility services represent the commodity.

%Energy flexibility is defined as the ability of a system to deploy its resources to respond to changes in net \cite{lannoye2012evaluation}. Acting as prosumers, owners of PVs can provide energy flexibility services at the local level. This work focuses on the provision of two flexibility services, that are ramp-up services by providing energy from the local flexible resources or ramp-down services by consuming energy from the grid. Albeit the fact that these local resources are small and highly distributed, they can be aggregated to participate effectively in the energy market. Essentially, aggregators are required as intermediate players between the resources and the power system operators. Aggregators' role is to gather those resources to be significant enough to meet the energy market minimum bid thresholds, which are in the scale of megawatts in the best cases. In the context of local energy market, the prosumers act as sellers and the distribution system operator (DSO) is the buyer and the flexibility services represent the commodity.

Combinatorial auctions allow owners of PV and energy storage systems (ESS) to bundle flexibility intervals, rather than bidding for them individually. For example, a prosumer with two PV production time intervals, $a$ and $b$, can choose to provide energy flexibility immediately in the same time interval, or store energy in the ESS and bid for a later use. Additionally, the prosumer can supplement any one of the PV production intervals with an off-peak interval, $c$, for example, at night, by using the ESS. Combinatorial auctions enable prosumers to submit more competitive bids by offering more choices and mitigating the risk of losing desirable flexibility-provision intervals. In this example, the prosumer can bundle all options as a list of mutually exclusive time-interval-combinations $ \big[ \{a\} , \{b\} , \{c\} , \{a \land b\} , \{a \land c\} , \{b \land c\} \big] $ with a corresponding list of power values. Combinatorial auctions have been successful in other domains, such as airport time slots allocation and spectrum auctions \cite{cramton2006combinatorial}, but they have not been thoroughly studied in the energy domain, especially in local energy markets. Previous research on local energy markets has focused on sequential auction mechanisms as in \cite{olivella2018local,xuincentive}, which do not satisfy the bundling need in local flexibility markets (LFMs). 

However, in LFMs, each time interval comprises multiple flexibility dividends that can be won by different bids and prosumers. This divisibility notion leads to complex winner determination problems (WDP) posing a challenge for implementing combinatorial auctions. This paper addresses these challenges by proposing a machine learning approach that efficiently solves the winner determination problem using graph neural networks (GNN). The approach considers multi-minded bidders proposing a reverse combinatorial auction framework for LFMs. By learning an end-to-end model that maps WDP instances to solutions, computation complexity is decoupled and efficient allocation time is achieved. This work mainly focuses on the management layer of energy market and does not address regulations or physical layer challenges.

We introduce the combinatorial auction framework in Section \ref{sec2} and discuss the LFM-WDP and its complexity in Section \ref{sec3}. Then, we propose our LFM neural combinatorial auction in Section \ref{sec4} and discuss our  GNN model for learning the LFM-WDP solution in Section \ref{sec5}. We present numeric results in Section \ref{sec6} and concluding remarks in Section \ref{sec7}.


%The rest of this paper is structured as follows: Section \ref{sec2} introduces the combinatorial auction framework. In Section \ref{sec3}, we discuss the LFM-WDP and its complexity. Section \ref{sec4} presents our proposed LFM neural combinatorial auction, including instance generation and graph representation. Section \ref{sec5} discusses the GNN model for learning the LFM-WDP solution. Experimental results are presented in Section \ref{sec6}, and concluding remarks are presented in Section \ref{sec7}.

%The rest of this paper is organized as follows. Section \ref{sec2} presents the proposed combinatorial auction framework. In Section \ref{sec3}, we formulate the LFM-WDP and discuss its complexity. Then we present our proposed LFM neural combinatorial auction along with LFM-WDP instance generation and its graph representation in Section \ref{sec4}. In Section \ref{sec5}, we discuss our GNN model for learning the LFM-WDP solution.  We follow that by presenting the experimental results evaluation in Section \ref{sec6} and concluding the paper and gives insights into the future work in Section \ref{sec7}.



%To justify the need for combinatorial auctions in local energy markets, consider a prosumer who possesses a photovoltaic (PV) system and an energy storage system (ESS) and is willing to provide energy flexibility services. For each PV production interval, the prosumer has two options: to either provide the flexibility immediately or postpone it by storing energy in the ESS. For example, if there are two PV production intervals $ \{a,b\} $, the prosumer can choose to provide energy instantly in three ways: either in$ \{a\}$ or  $ \{b\} $  or both intervals together $ \{a \land b\} $. The prosumer can also supplement any one of the PV production intervals with an off-peak interval $ \{c\} $, for example, at night, by using the ESS and can offer  $ \{a \land c\} $ , $ \{b \land c\}$ or  $\{c\}$. To mitigate the risk of losing desirable flexibility-provision intervals, combinatorial auctions allow prosumers to bundle intervals rather than bid for individual ones. This approach offers more choices and enables prosumers to submit more competitive bids, as opposed to the sequential auction mechanism where only a single bid is allowed.  In this example, the prosumer can submit all options combined in a bundle$ \big( \{a\} \lor \{b\} \lor \{c\} \lor \{a \land b\} \lor \{a \land c\} \lor \{b \land c\} \big) $. Combinatorial auctions have been successfully applied in other domains such as airport time slots allocation and spectrum auctions \cite{cramton2006combinatorial}. However, they have not been thoroughly studied in the energy domain, especially in local energy markets.


%To motivate the need for combinatorial auctions in local energy markets, let us consider a prosumer who owns a PV and an energy storage system (ESS) and is willing to provide ramp-up services, i.e., feeding energy into the grid. This prosumer has two possible choices for each interval of the PV production; that are either to instantly provide the flexibility during that interval or postpone the provision by storing energy in the ESS. For instance, if there are two PV production intervals $ \{a,b\} $, the prosumer can choose to instantly provide energy in three possibilities, that are either any one of the intervals $ \{a\}$ or $ \{b\} $ or both intervals together $ \{a \land b\} $. The prosumer can also utilize the ESS to expand the choices by complementing any one of the PV production intervals with another off-peak interval $ \{c\} $, at night for instance, and can offer $ \{a \land c\} $ , $ \{b \land c\}$ or $\{c\}$. This interval-bundling process leads to producing multiple choices that can either \textit{complement} each other, (e.g. $ \{a\} $ and $ \{b\} $) or \textit{substitute} each other (e.g. $ \{a \land b\} $ or $ \{a \land c\} $). In the sequential auctions, the prosumer is allowed to submit only a single bid of theses options. Based on that bid, the sequential auction mechanism decides if the prosumer is a winner or not. Even though, the prosumer could have avoided the loosing situation if allowed to submit multiple bids. 
%
%Allowing prosumers to \textit{bundle} flexibility-provision intervals to alleviate the risk of allocating the less favourable ones is what motivates to consider \textit{combinatorial auctions}. Basically, the setting of combinatorial auctions can provide the means to avoid, what so-called, the exposure problem of obtaining incomplete complementary intervals. Under combinatorial auctions, prosumers can combine different flexibility intervals and form bundles rather than bidding for an individual interval. In our previous example, the prosumer is allowed to submit all its options combined in a bundle $ \big( \{a\} \lor \{b\} \lor \{c\} \lor \{a \land b\} \lor \{a \land c\} \lor \{b \land c\} \big) $. Combinatorial auctions are successfully applied in other domains such as airport time slots allocation and spectrum auctions \cite{cramton2006combinatorial}, but they have not been studied thoroughly in the energy domain, especially local energy markets.  




%Since PV-ESS prosumers can provide \textit{at most} one of those two options, they can express that in the form of \textit{substitutable} flexibility offerings. Whereas, prosumers with EVs can shift their charging only if another time slot is guaranteed to charge their vehicles, hence EV prosumers' offerings form \textit{complementarities}. For instance, a PV owner that have two PV peak production hours can provide three different bids, that are either any one of the hours or both hours together. Moreover, the prosumer can expand its flexibility using ESS to complement any of the PV ours with another slot at night for instance. 
%
% 





%Prior works in the literature have focused on local energy markets in the context of auctions, mainly using sequential auction mechanisms. However, none of these works address the substitutability and complementarity needs of local flexibility markets. A local flexibility market mechanism for aggregators is described in \cite{olivella2018local}, while \cite{xuincentive} presents a market mechanism for demand response providers. The authors in \cite{torbaghan2016local} focus on minimizing grid operation cost. To address challenges in the local flexibility market context, this work focuses on three intrinsic difficulties. The winner determination problem is proven to be NP-complete \cite{fujishima1999taming}, and solutions fall within three classes: local search algorithms, restricted bid space, and tree search algorithms \cite{sandholm2006optimal}.


%
%Although there is a handful amount of work in the literature approaches local energy markets in the auctions context, to the best of our knowledge, all these works are based on sequential auction mechanisms. A generic description of local flexibility market mechanism for aggregators is presented in \cite{olivella2018local}. The authors in \cite{rosen2013auction} proposed a local market design for energy trading between prosumers and balance responsible parties. A market mechanism for demand response providers to participate alongside conventional generators in proposed in \cite{xuincentive}. The authors in \cite{torbaghan2016local} focused on minimizing the grid operation cost designing day ahead and real time markets. Even though these sequential mechanisms can be solved effectively by simple heuristic approaches such as ant colony optimization as in \cite{lezama2020learning}, they are single-type resource auctions and do not suite the local flexibility markets need of substitutability and complementarity. In contrast to those works, we adopt the combinatorial auction mechanism as it allows for the fulfilment of the substitutability and complementarity behaviour of the flexibility intervals.
%
%Despite of those motivations, combinatorial notion embeds challenges which escalate in the local flexibility market (LFM) context. To address these challenges, this work untangles three difficulties which are intrinsic in the LFM. Firstly, flexibility items (i.e., intervals) in LFMs are perfectly divisible, in the sense that prosumers can bid for multiple flexibility hours. Having multi-unit items is more complex than the combinatorial auctions for indivisible items discussed in the literature. Secondly, the problem of determining winning bids which maximize the auctioneer's revenue under specific constraints is referred to as the winner determination problem (WDP) and it is proven to be NP-complete \cite{fujishima1999taming}. This challenge is not specific to LFMs, however the optimization algorithm search space in LFMs is huge compared to problems discussed in the literature. Lastly, there is no gold standard allocations data set that can be used to evaluate our design, thus generating WDP instances is essential to evaluate and validate our work.
%
%The approaches in the literature to solve WDP in commodities' combinatorial auctions fall within three fundamental classes. First, designing local search algorithms that are provably fast as they are based on approximating WDP \cite{boughaci2010local}. However, the solution of those methods falls far from optimal because WDPs can not be approximated. Second, by restricting the bids space in the the subset level or the items level such as in \cite{saez2008testing}. This translates to requiring prosumers to select bundles from a pre-define set or include limited number items. This restriction leads to economic inefficiencies as the restricted set can be far from prosumer preferences. The third approach is designing tree search algorithms that provably find an optimal solution \cite{sandholm2006optimal}, these algorithms are slow due to the NP-completeness of WDPs. 

%This work proposes a new machine learning approach to solve the LFM-WDP. The approach uses graph neural networks to encode relational features and capture complex bid interactions, allowing for efficient allocation times. The proposed approach considers multi-minded bidders who can bid on multiple bundles. The main contributions of this work are threefold. Firstly, a reverse combinatorial auction framework is proposed for local energy flexibility markets. Secondly, graph representations are developed for the LFM-WDPs. Lastly, end-to-end machine learning models are developed and tested to solve LFM-WDPs, which input a WDP-LFM instance and output the optimal solution.

%This work presents a novel machine learning approach to tackle the winning determination problem in local flexibility markets (LFM-WDP). The approach is based on the hypothesis that LFM-WDP instances follow an underlying probability distribution defined by the application domain which can enable an end-to-end model to map problem instances to their solutions. By using graph neural networks (GNNs), relational features are encoded to capture the interaction of each bid with the entire bid space, addressing complex relationships. The proposed approach considers multi-minded bidders who can bid on multiple bundles, leading to efficient allocation time that meets latency requirements. This work focuses on the virtual layer of energy market operations and recommends an extended study to evaluate the impact of proposed solutions on the physical layer while considering power system regulation and physical layer challenges.


%To tackle the WDP in LFMs, this work takes an entirely different path which is leveraging machine learning to alleviate the computation burden. In fact, we base our use case of machine-leaning on a hypothesis. That is the WDP instances follow an underlying unknown probability distribution which is defined by the application domain, i.e. LFMs in our case. In other words, we aim to learn an end-to-end model that can directly map problem instances to their solutions. Note that, one cannot hope for an oracle algorithm that can efficiently solve any generic WDP instance unless problems follow a common pattern. If we learn such a model, we can decouple the computation complexity and embed it into an off-line learning phase from the quick inference phase by using the trained models to solve WDPs. This decoupling can lead to efficient allocation time that does not violate the latency requirements of the system as the standard solvers might do. An alternative approach is using reinforcement learning, however inefficient exploration and computational scalability for large-scale optimization problems are the key challenges of this approach as discussed in \cite{nair2020solving}.

%We build our LFM-WDP models based on the graph neural networks (GNNs) framework \cite{scarselli2008graph}. Our motivation for using GNNs is that it allows for encoding relational features so we can represent how each bid interacts with the entire bid space. Representing WDPs as graphs allows us to encode the competition between bidders as well as the bids logic for each bidder. GNNs framework can address these complex relationships to a decent level. We are also motivated by the few works that used GNNs to solve optimization problems, such as \cite{bunz2017graph} who tried to solve the satisfiability problem. To solve WDP in other domains, the authors in \cite{lee2020fast} applied GNN models in the cloud computing domain. In spite of their relevant work, they only studied single-minded bidders which restricts bidders to submit a single bundle, this abolishes the essence of combinatorial auctions and limits the bidders flexibility. In contrast, our work considers the multi-minded bidder where bidders are allowed to bid multiple bundles.
%
%Form the  practical perspective, it is important to point out that this work focus is within the scope of the virtual layer of the energy market operations. This work does not discuss regulations and power system physical layer challenges. Our proposed novel solutions, due to the still restrictive regulations, are obviously hindered to be put in commercial practice. Moreover, a separate extended study has to incorporate grid challenges to ensure the imposed impact on the physical layer.
%
%The main contributions of this work are threefold. Firstly, a reverse combinatorial auction framework is proposed for local energy flexibility markets. Secondly, graph representations are developed for the local flexibility market winner determination problems (LFM-WDP). Lastly, end-to-end machine learning models are developed and tested to solve LFM-WDPs, which input a WDP-LFM instance and output the optimal solution.








\section{Local Flexibility Market Combinatorial Auction }\label{sec2}


We define the flexibility interval as the time period during which the DSO requires a provision of ramp-up (e.g., energy supply or load shedding) or ramp-down (e.g., energy reduction or load increase) in active power units. We assume that the DSO communicates its flexibility requirements through a flexibility curve representing the needed amount of flexibility units.  Figure \ref{fsysmodel} shows the proposed auction framework which involves the DSO as the buyer and aggregators as sellers, each managing a portfolio of prosumers with flexible resources and accessing their forecasting modules. Aggregators bid on behalf of their prosumers, considering preferences and available resources. The flexibility market operator evaluates bids and determines winners by solving an LFM-WDP. Next, we mathematically model the flexibility request and resources and define the bids' formats and auction objective.


%We define the flexibility interval, a key attribute of the LFM, as the time period during which the DSO needs a provision of a ramp-up (e.g., energy supply or load shedding) or a ramp-down (e.g., energy reduction or load increase). In this work, the flexibility units are indicated in active power units such as kilowatts or megawatts, and the DSO communicates its flexibility requirement as a flexibility curve $\mathcal{F}$ consisting of time intervals, with each interval representing the amount of flexibility units needed. 

%Firstly, we define the flexibility interval, which is the commodity of the LFM, to be the time interval in which the DSO requires provisioning a ramp-up or a ramp-down flexibility. Ramp-ups designate the requirement of either energy supply or load shedding and is represented by positive flexibility units. Whereas, ramp-downs designate the requirement of either energy reduction or load increasing and represented by negative flexibility units. In this work, we set the flexibility units to be in active power units (e.g., kilowatt or megawatt) and we assume that the DSO encodes its flexibility request as a flexibility curve $\mathcal{F}$ of $T$ time intervals. Each interval has a capacity of flexibility units representing how much ramp-up or ramp-down units is required at that particular time. An example of a DSO flexibility request is shown in Figure \ref{flex}. It represents three ramp-ups $ \{1,2,3\} $ and two ramp-downs $ \{5,6\} $ while nothing required at $ \{4\} $ .


%% Figure environment removed

%The proposed reverse combinatorial auction market framework and its temporal structure are illustrated in Figure \ref{fsysmodel}. The structure comprises the DSO and aggregators as market participants, with the DSO as the buyer and multiple aggregators representing the sellers. Each aggregator manages a portfolio of prosumers who own flexible resources and has access to forecasting modules and prosumer preference logs. The DSO sends a flexibility curve $\mathcal{F}$ to a flexibility market operator, which broadcasts it to the participating aggregators. Aggregators submit bids as bundles on behalf of their prosumers, taking into account their preferences and available resources. Then, the flexibility market operator  determines the winners after evaluating the availability of flexibility resources and solving a WDP. Next, we mathematically model the flexibility request and resources and define the bids' formats and auction objective.

%The proposed market framework and its temporal structure is shown in Figure \ref{fsysmodel}. The DSO and the aggregators are the market participants. The DSO is the buyer and there are multiple aggregators representing the sellers. Each aggregator has a portfolio of prosumers that own flexible resources in their premises. Aggregators can have logs of prosumer preferences which reveal how prosumers would like their flexible resources to be managed. Each aggregator has a forecasting module which can predict variables such as solar power production. The process starts with the DSO placement of a flexibility request by sending the flexibility curve $ \mathcal{F}$ to the flexibility market operator which in its turn broadcasts $ \mathcal{F}$ to its $ m $ participating aggregators. After evaluating their flexibility resources availability, aggregators generate and submit bids as bundles on behalf of the prosumers, taking into account their preferences. Next, the flexibility market operator decides and announce the winners after solving a winner determination problem (WDP).



%% Figure environment removed
%%
% Figure environment removed
%%




\subsection{Flexibility modelling}\label{flexsection} 

\subsubsection{Flexibility curve}
The flexibility curve $ \mathcal{F}$ explicitly represents the amount of ramp-up or ramp-down requested by the DSO for each interval, with real power units as flexibility units. We represent $\mathcal{F}$ as $\{u_1,u_2,...,u_T\}$, where $ u_j \in \mathbb{Z}$ is the quantity of requested flexibility units at interval $j \in \mathit{I_M}=\{0,1,2,...,T\}$ and $ sign(u_j) $ indicates a ramp-up (+) or a ramp-down (-) flexibility request. An example of a DSO flexibility request when $T$ is 6 can be expressed as $\mathcal{F}=\{25,10,15,0,-30,-10\}$ megawatt, denoting requesting ramp-ups flexibility units at $ \{1,2,3\} $ and ramp-downs $ \{5,6\} $ flexibility units with no requirement at $ \{4\} $.
%The flexibility curve $ \mathcal{F}$ is an explicit representation of how much ramp-up or ramp-down the DSO requests at each interval. In this work, we take the real power units as our flexibility units. So, $\mathcal{F}=\{u_1,u_2,...,u_T\}$, where $ u_j \in \mathbb{Z}$ is the quantity of the requested flexibility units at interval $j \in \mathit{I_M}=\{0,1,2,...,T\}$. For instance, the curve in Figure \ref{flex} has $ T = 6 $ and can be represented as $\mathcal{F}=\{25,10,15,0,-30,-10\}$ megawatt.


\subsubsection{PV system}

We investigate prosumers with fixed nominal PV production and no advanced peak-point-tracking technology, that have roof-top solar systems that follow a bell-curve pattern with peak power output at midday. In our study, we assume that PV systems are not connected to the grid during the bidding process, showing that they can only offer ramp-up services by contributing their production. We define $\mathit{I_{pv}}$ as the set of peak production intervals and $\sigma^{pv}_j$ as the forecasted PV power output in flexibility units in each interval $j$.

%
%We study residential prosumers with roof-top solar systems that have fixed nominal peak power values and lack sophisticated peak-point-tracking technology. The PV power output typically follows a bell-curve pattern, peaking at midday. We assume that PV systems are not connected to the grid during the bidding process, meaning that PVs can feed their own buildings only when they are not engaged in the local market. As a result, PV systems can only offer ramp-up services by contributing their production. Note that if this assumption is not in place, PVs can also provide ramp-down services by disconnecting if they are feeding the grid during bidding. We define the set $\mathit{I_{pv}}$ as the peak production intervals, and $\sigma^{pv}_j$ as the forecasted PV power output in flexibility units, such that $\sigma^{pv}_j \in \mathbb{Z^+}$ for each interval $j$.

%We consider residential prosumers with roof-top solar systems which  have specific nominal peak power values and are not equipped with sophisticated peak-point-tracking units. Hence, the PV power output is typically bell-curved reaching its peak production at noon. We also assume that PV units are inactive with respect to the grid at the moment of bidding, meaning that PVs can only feed their own buildings when they are not engaged in the local market. With this assumption PV systems can only provide ramp-up service by providing their production. Note that in the absence of this assumption PVs can also provide ramp-down services, by disconnection, if they are feeding the grid at the bidding phase. Considering the forecasted PV power outputs, let $\mathit{I_{pv}} \subseteq  \mathit{I_M}$ be the set of peak production intervals and $ \sigma^{pv}_j \leq u_j$ is the PV production in flexibility units such that $ \sigma^{pv}_j \in \mathbb{Z^+} $ for each interval $ j $.
%

%\subsubsection{Electric vehicles}
%Prosumers with EVs can provide down- and up-ramping services. Hence, let $\mathit{I_{ev}} \in  \mathit{I_M}$ to be the set of production time intervals and $ \phi^{ev}_j \leq \phi_j$ is the EV production/consumption in term of flexibility units such that $ \phi^{ev}_j \in \mathbb{Z} $ for every time interval $ j $.


\subsubsection{PV-storage system}

As a practical consideration, we study the case where a portion of PV owners are equipped with sufficient energy storage systems (PV-ESS) which can be used to store energy either from their own PV units or the grid and can be used to feed energy to the grid. Under this consideration, PV-ESS can provide both ramp-up and ramp-down flexibility services. Hence, let $\mathit{I_{ss}} \subseteq \mathit{I_M}$ be the set of intervals of storage availability and $ \sigma^{ss}_j \leq u_j$ is the storage system charging/discharging power in flexibility units such that $ \sigma^{ss}_j \in \mathbb{Z} $ for each interval $ j $, where $ sign(\sigma^{ss}_j) $ indicates a ramp-up (+) or a ramp-down (-) flexibility service.


%As a practical consideration, we study the case where a portion of PV owners are equipped with sufficient energy storage systems (PV-ESS) which can be used to store energy either from their own PV units or the grid and can be used to feed energy to the grid. Under this consideration, PV-ESS can provide both ramp-up and ramp-down flexibility services. Hence, let $\mathit{I_{ss}} \subseteq  \mathit{I_M}$ be the set of intervals of storage availability and $ \sigma^{ss}_j \leq u_j$ is the storage system charging/discharging power in flexibility units such that $ \sigma^{ss}_j \in \mathbb{Z} $ for each interval $ j $, where $ sign(\sigma^{ss}_j) $ indicates a ramp-up (+) or a ramp-down (-) flexibility service.


\subsubsection{Flexibility provision}
We can summarize the available flexibility provision options based on the resources of prosumers as follows. A PV system can provide a ramp-up service by connecting to the grid while an ESS can provide both ramp-up by discharging energy to the grid and ramp-down by charging energy from the grid. 
%For example, PV-ESS owners can bid for interval 3 by feeding energy to the grid, or charge their energy storage system during interval 4 to provide ramp-down flexibility services, as illustrated in Figure \ref{flex}.

%We summarize the different possibilities for flexibility provision  based on prosumers' resources. PVs can only provide ramp-up flexibility services by feeding energy to grid. For instance, PV owners can bid for time interval 1 in Figure \ref{flex}. Whereas, PVs which are equipped with PV-ESS can either provide ramp-up services by feeding energy to the grid, for instance bidding for interval 3 or ramp-down services by charging in interval 4 in Figure \ref{flex}.

%\subsubsection{Rebound effect}
%We also aim to consider minimizing the rebound effect which is caused by the post-provision state of the flexibility resources, i.e., after the end of the flexibility activation period. This primarily occurs when re-charging the batteries after providing a ramp-up service. We model the rebound property by including an information bit in the bid indicating the number of the expected rebound time intervals. Encoding this information within the bid helps the DSO to avoid creating another peak.  PV-ESS cause the rebound effect if the charging interval after the service provision is does not match a (negative) flexibility interval in $ \mathcal{F}$, for instance if the bid is for interval 3 and the recharge intervals are other than 4 and 5 in Fig.(\ref{flex}).


%\begin{table}[!h]
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%			\hline
%			\textbf{Resource} & \textbf{Ramp} & \textbf{Rebound}   &  \textbf{Action} \\
%			\hline
%			PV& \makecell[c]{ $\uparrow$ } &  \makecell[c]{  0 }  & \makecell[l]{ connect to grid}    \\ 
%			\hline
%			PV-ESS & \makecell[c]{ $\uparrow$ \\ $\downarrow$   \\ $\uparrow$ $\downarrow$} &  \makecell[c]{$ u^{ss}_j  $  \\ 0   \\0}  & \makecell[l]{ discharge ESS to grid \\   charge ESS from grid  \\ ESS charging/discharging match $ \mathcal{F}$}    \\ 
%			\hline
%		\end{tabular}
%	\caption{Flexibility resources summary}\label{flexdic}
%\end{table}

%
%\begin{table}[!h]
%	\centering
%	\begin{tabular}{|c|c|l|}
%		\hline
%		\textbf{Resource} & \textbf{Ramp}   & \textbf{ \ \ \ \ \ \ \ \  Action} \\
%		\hline
%		PV& \makecell[c]{ $\uparrow$ }  & \makecell[l]{ connect to grid}    \\ 
%		\hline
%		
%		\multirow{3}{*}{ \ \ 	PV-ESS }  	& $\uparrow$	& discharge ESS to grid	\\ \cline{2-3}
%		
%		& $\downarrow$ 	& charge ESS from grid	\\ \cline{2-3}
%		& $\uparrow$ $\downarrow$	& ESS charge AND discharge match  $ \mathcal{F}$	 
%		\\ \hline
%	\end{tabular}
%	\caption{Provided flexibility services}\label{flexdic}
%\end{table}


%\subsection{Bids format}





%
%\begin{algorithm}
%	\caption{Combinatorial auction procedure}
%	\begin{algorithmic}[1]
	%		\Procedure{Local Agent 1}{}%{$a,b$}%       \Comment{This is a test}
	%		\State Receive the flexibility curve  $\mathcal{F}$
	%		\State Generate high bundle space $\mathcal{B}$ 
	%		\State Generate bundle value set  $\mathcal{V}$
	%		\State Send $\mathcal{B}$ and $\mathcal{V}$ pairs to the Master Agent
	%		\EndProcedure
	%		
	%		\Procedure{Master Agent}{}%{$a,b$}%       \Comment{This is a test}
	%		\State Receive $\mathcal{B}$ and $\mathcal{V}$ pairs from Local Agents
	%		\State Solve WDP (\ref{WDP0})  to determine  $\mathcal{A}$
	%		\State Send $\mathcal{A}$  to the Local Agents
	%		\EndProcedure
	%		
	%		\Procedure{Local Agent 2}{}%{$a,b$}%       \Comment{This is a test}
	%		\State Receive  $\mathcal{A}$
	%		\State Perform flexibility activations
	%		\EndProcedure
	%	\end{algorithmic}
%\end{algorithm}

\subsection{Combinatorial Bid Format and Auction Objective}
In this study, we consider a multi-minded bidder scenario where bidders can submit multiple and mutually exclusive bids. This scenario differs from the single-minded bidder case where bidders are only allowed to to submit a single bid. Such mutual exclusivity nature is referred to as XOR bids in the standard scheme in combinatorial auctions logic \cite{cramton2006combinatorial}.


%As a practical consideration in this work, we study a multi-minded bidder case in which bidders are allowed to bid multiple bids. This is in contrast to the single-minded bidder's case where bidders are restricted to submit a single representative bid. In our LFM, bidders are requested to submit multiple mutually exclusive bids, a scheme which is referred to as XOR bids in the standard combinatorial auctions logical language \cite{cramton2006combinatorial}. 

\subsubsection{Valuation function}

The flexibility request $ \mathcal{F}$ of $ \tau \leq T$ flexibility intervals forms a subset space of $2^\tau$ for possible bids. Hence, we define a valuation function to represent prosumer $p$ assigned value to the subset space, as $v_p:  2^\tau \rightarrow \mathbb{R}$. Now, let $\mathit{S}$ be a desired subset by prosumer $p$ which consists of $\{\sigma^{pv}_1,\sigma^{pv}_2,...,\sigma^{pv}_m\}$ produced by PV and $\{\sigma^{ss}_1,\sigma^{ss}_2,...,\sigma^{ss}_q\}$ fed from the ESS. The cost is a principal component of the valuation from the prosumer perspective. Therefore, we can model the valuation of subset $S$ as

\begin{equation}
	\label{costeq}
	v_p(S)= \sum_{m^{'}=1}^{m} \alpha_{m^{'}}.\sigma^{pv}_{m^{'}} + \sum_{q^{'}=1}^{q} \beta_{q^{'}}.\sigma^{ss}_{q^{'}} + \gamma_p(S)
\end{equation}
where $\alpha_{m^{'}}$ and $\beta_{q^{'}}$ are the cost coefficients of time interval $m^{'}$ delivered by the PV and time interval $q^{'}$ which is stored in the ESS, respectively. Note that $\alpha_{m^{'}}$ is ideally 0, as no cost is connected to the PV generation and $\beta_{q^{'}} =2(1-\eta)$ where $\eta$ is the charging and discharging efficiencies and $\gamma_p(S)$ is a term representing the prosumer profit for subset $S$.


\subsubsection{Bid format}
From a prosumer set $\mathit{N}=\{1,2,...,n\}$, prosumer $\mathit{p}$ can submit multiple bids and we denote the bid $\mathit{i}$ as $\mathit{b}{p_i}$ which offers flexibility quantities $\mathit{S}i$ from the flexibility curve $\mathcal{F}$. Then, we can calculate the prosumer's valuation $v_p(\mathit{S}{i})$ using (\ref{costeq}). The bid consists of the subset of flexibility items and the prosumer's value for that subset, denoted as ${\mathit{(S{p_i},v_p(S_{p_i}))}}$. The flexibility market platform receives bids from all prosumers through their aggregators as $\mathcal{B} ={ \mathit{(S_{1_1},v_1(S_{1_1})),... ,(S_{n_{\kappa_{n}}},v_n(S_{n_{\kappa_{n}}}))} }$, where $ \kappa_{p}$ is the number of bids submitted by prosumer $p$. The subset $\mathit{S}{p_i} = \{\sigma_{j}\}$ is a set of flexibility units in time intervals $ j\in\mathit{I_S}_{p_i} \subseteq \mathit{I_M}$, where $\sigma_j$ is the quantity of flexibility units. It is important to note that a prosumer may submit multiple bids, hence $\kappa_{p} \in \mathbb{Z^+}$.

%With the flexibility curve $\mathcal{F}$  and a set of prosumers  $\mathit{N}=\{1,2,...,n\}$, to construct the bid $\mathit{b}_{p_i}$ of prosumer $ p $ offering a set of flexibility quantities $\mathit{S}_i$, we can obtain prosumer $ p $ valuation as $v_p(\mathit{S}_{i})$ by utilizing (\ref{costeq}). Those two components, i.e., the subset of flexibility items and the value of the particular prosumer to that particular subset assemble the bid $ i $ of prosumer $ p $ that is the tuple $ \mathit{b}_{p_i} =\{\mathit{(S_{p_i},v_p(S_{p_i}))}\} $. The market platform then receives bids from all prosumers through their respective aggregators as $ \mathcal{B} =\{ \mathit{(S_{1_1},v_1(S_{1_1})),... ,(S_{n_{\kappa_{n}}},v_n(S_{n_{\kappa_{n}}}))} \} $; where $ \kappa_{p}$ is number of submitted bids by prosumer $p$. The subset $\mathit{S}_{p_i} = \{\sigma_{j}\}$ and $ \sigma_j$ is the quantity of flexibility units in time interval $ j\in\mathit{I_S}_{p_i} \subseteq \mathit{I_M}$. We re-emphasize our consideration of a multi-minded prosumer submitting multiple bids, hence $ \kappa_{p} \in \mathbb{Z^+} $. 

%...,(S_{1},v_n(S_{1})),...,(S_{\kappa},v_1(S_{\kappa})),

%With the flexibility curve $\mathcal{F}$  and a set of prosumers  $\mathit{N}=\{1,2,...,n\}$, to construct the bid $\mathcal{B}_i$, let $ \mathit{I_S}_i \subseteq \mathit{I_M}$ to be its desired subset of time intervals where $ s \leq T $ and $\mathit{S}_i = \{\sigma_{j \in \mathit{I_S}_i}(\mathit{S}_i)\}$ to be the set of flexibility units quantity, where $ \sigma_j(\mathit{S}_i) $ is the quantity of flexibility in time interval  $ j\in\mathit{I_S}_i$. We can obtain the valuation of prosumer $ p $ for bundle  $\mathit{S}_i$ as $v_p(\mathit{S}_i)$ by utilizing (\ref{costeq}). In addition to that, we indicate the rebound effect as  $\mathit{R}_i = \{r_{j \in \mathit{I_M}}\}$, where $ r_j $ is the rebounded quantity to time interval $ j $. Those are the three components of the bid structure, i.e., the flexibility items subset, the rebounded quantity and the value of the particular prosumer to that particular subset. So, bid $ i $ of prosumer $ j $ is the triplet $ \mathcal{B}_i =\{\mathit{(S_i,R_i,v_j(S_i))}\} $. The market platform then receives the bid space represented by all aggregators ( with their prosumers) as $ \mathcal{B} =\{ \mathit{(S_1,R_1,v_1(S_1)),...,(S_{\kappa},R_{\kappa},v_n(S_{\kappa}))} \} $. Where $ \kappa  $ is the total number of submitted bids by all aggregators.



%bundle: [(time, quantity), ...  ,(time, quantity), rebound index, price], XOR bids.
%
%
%pricing scheme, pricing problem, budget balance.
%
%On this end we face the following problems:
%\begin{enumerate}
%	\item How can we express the bids? i.e., expressiveness VS simplicity dilemma? 
%	\item The amount of communication traffic of $ n $ prosumers will be $ \mathcal{O}(n \times 2^{c \times m }) $ which is exponential in the number of time intervals. 
%	\item What incentives can guarantee that bidders bid their actual valuation? As we adopt the private valuation model.
%\end{enumerate}

%\textit{\textbf{For more investigation:} (i) Bundling reduction = dimensionality reduction? bundle is a multi feature data points that we want to map them to a lower number of items that are relevant to the bidder?
	%(ii) The local agent may be required to forecast load and PV production. 
	%Secondly, considering combined heat and power (CHP) prosumers? do not forget the marginal cost for CHP in the model.}


%\subsection{Learning Combinatorial Bidding Strategy}


%$ \mathcal{F} $ 

%\subsection{Auction Objective}
\subsubsection{Auction objective}

The objective of the auction mechanism is to determine the optimal allocation set, which is a combination of bids that collectively provide the required flexibility represented by $\mathcal{F}$ with the minimum total cost. This is achieved by solving an NP-complete optimization problem, i.e., the LFM-WDP, while adhering to essential constraints. Details about LFM-WDP and solution is in the next sections.

%Lastly, we set the auction mechanism objective to be the determination of the optimal allocations set. That is the combination of bids which collectively suffice the provision of flexibility represented by $\mathcal{F}$ with the minimum total cost. This allocation is determined by solving an NP-complete optimization problem, i.e., the WDP, respecting essential constraints. More rigorous mathematical definition for the WDP and elaboration on the proposed solution methods are in the following sections.




\section{Local Flexibility Market Winner Determination Problem}\label{sec3}
Our LFM combinatorial auction aims to minimize the total cost while allocating flexibility intervals to prosumers based on their bids. In general, solving WDPs in combinatorial auctions is a challenging computation problem due to the NP-completeness of multidimensional weighted set packing problems, as proven in \cite{fujishima1999taming}. This challenge arises from the overlapping nature of bid items. Our contribution is to use machine learning to address this problem in the LFM-WDP.
%
%In our proposed LFM combinatorial auction, the underlying winner determination problem (LFM-WDP) objective is to answer the question : \textit{given the set of all prosumers' bids, $ \mathcal{B} $; find the allocation of flexibility intervals to the prosumers that minimizes the total cost}. Solving WDPs is a fundamental computation challenge in combinatorial auctions. The source of difficulty is what has been proven in \cite{fujishima1999taming} that the WDPs are equivalent to the weighted set packing problems which are NP-complete. This is mainly due to the overlapping natures of the bids' items. A major contribution of our work is to leverage machine learning to solve the LFM-WDP. 

\subsection{ LFM-WDP Formulation} 
We mathematically formulate the LFM-WDP as a combinatorial optimization problem considering the divisibility nature of flexibility items mentioned earlier. Given a flexibility curve $\mathcal{F}=\{u_j\}$, where $j\in\mathit{I_M}$ and a bid space $\mathcal{B} ={\mathit{(S_{p_i},v_p(S_{p_i}))}}$, $ \forall i\in {1,..., \kappa_{p}}$ and $\forall p\in N$, the objective is to find the set $\mathcal{A}$ of winning bids that minimizes total costs. We formulate the LFM-WDP as 

%To mathematically formulate our LFM-WDP as a combinatorial optimization problem, we note here that we consider the divisibility of the flexibility items that is described in section \ref{flexsection}. Hence, with a flexibility curve $\mathcal{F}=\{u_j\}$, where $ j\in\mathit{I_M}$ and bid space $ \mathcal{B} =\{\mathit{(S_{p_i},v_p(S_{p_i}))}\} $, $ \forall i\in \{1,..., \kappa_{p}\}$ and $ \forall p\in N$, the objective is to find the set $\mathcal{A}$ of winning bids that minimizes the total costs as 

\begin{mini!}[2]
	{\mathbf{x}}{J= \sum_{ \forall i,\forall p}   v_p(\mathit{S}_{p_i}) x_{p_i}} {\label{WDP0}}{}\label{WDPa}
	\addConstraint{\sum_{\forall i,\forall p} \sigma_{j}(\mathit{S}_{p_i}) x_{p_i} }{\geq u_j; \ \forall  j \in \mathit{I_{S_{p_i}}} } \label{WDPc1}
	\addConstraint{\sum_{\forall i}x_{p_i}}{\leq 1; \ \forall  p \in \mathit{N} } \label{WDPc2} 
	%	\addConstraint{\sum_{i =1}^{\kappa} R_i x_i(\mathit{S}_i) }{\leq R_{DSO} } \label{WDPc3}  
	\addConstraint{x_{p_i}}{\in \{0,1\}; \ \forall i,\forall p }  \label{WDPc4}%  b_i: \ \ b_i\in \mathcal{B}; \ \ if \  x_i=1}
\end{mini!}  
where $ \mathbf{x} $ is the decision variables' vector. The solution should satisfy the set of constraints we discuss below.

Firstly, it is plausible that the bid space may not exactly match the required flexibility curve due to the limitations in prosumers' bids controlled by their PV and ESS nominal capacity. As a result, the constraint $\sum \mathcal{A} = \mathcal{F}$ may not be feasible. To account for this, we assume the DSO can tolerate allocating more bids than needed to exceed flexibility requirements, rather than allocating less flexibility for lower costs. This is shown in the relaxation $\sum \mathcal{A} \geq \mathcal{F}$ and encourages prosumer engagement, increasing the \textit{thickness} of the long-term market. Constraint (\ref{WDPc1}) satisfies this requirement, and the relaxation is implicitly upper-bounded by the minimization problem (\ref{WDPa}). Secondly, prosumers are constrained to submit mutually exclusive bids, meaning that they can win at most one bid. This constraint is expressed in (\ref{WDPc2}). Thirdly, we impose an integrality constraint (\ref{WDPc4}) to account for the indivisibility of bids, as prosumers are unwilling to accept partial bids. Note that \textit{bid} indivisibility is distinct from \textit{item} divisibility in the flexibility curve. The latter allows multiple units to be allocated to different prosumers within a given time interval, whereas the former dictates that each bid is a unitary entity that can only either be won or lost.


%Firstly, there are plausible cases in which the bid space $ \mathcal{B}$ does not include a subset  that exactly meets the flexibility curve. This is mainly due to the fact that prosumers' bids are controlled by their PV and ESS nominal capacity. Those systems are scarcely equipped with automatic regulators to fine-tune their output to low incremental values, i.e., the constraint of having $ \sum \mathcal{A} = \mathcal{F}$ can be practically infeasible. So, we assume that the DSO can tolerate allocating more bids that can exceed its flexibility requirements over allocating less flexibility for lower costs. This DSO tolerance assumption translates to the relaxation $\sum \mathcal{A} \geq \mathcal{F}$. Easing this requirement will also encourage prosumers engagement in the LFM. Hence increasing the long-term market \textit{ thickness} which an economic characteristic that indicates bringing a large proportion of participants. This requirement is satisfied by meeting constraint (\ref{WDPc1}). Note that, this relaxation is implicitly upper-bounded by the minimization problem itself (\ref{WDPa}) as we aim to find the allocations that at least fulfil the flexibility curve while having minimal costs.




%Secondly, the prosumers are requested to submit mutually exclusive bids, i.e., in an XOR language. This means the prosumers are restricted to win at most one bid of their submitted bids. This is represented by the constraint (\ref{WDPc2}).


%Thirdly, we assume the DSO has a rebound limit of $ R_{DSO} $ which should not be exceeded by the accumulated rebounded values in the allocated bids. This is respected by constraint (\ref{WDPc3}).


%We also consider the case where the bids are indivisible and the prosumers are not willing to win partial bids, thus we add the integrality constraint (\ref{WDPc4}). Note that this bids' indivisibility is different from items divisibility of the flexibility curve. We still consider divisible items where each time interval in the flexibility curve can contain multiple units which can be allocated to different prosumers, however each bid is an indivisible entity that can be either a winning bid or a losing bid and can not partially win. 

%As we mentioned earlier, WDPs are equivalent to the weighted set packing problem, however, this form is a more complex form of the standard multidimensional knapsack problem \cite{freville2004multidimensional}, which is a  well-known NP-complete combinatorial optimization. 

%Then we can join both requirements by defining a vector $e \in \{0,1\}^n : e_{p}=1$ if the bid is submitted by prosumer $ p $ and  $ e_{p}=0 $ otherwise. This vector can also be seen as a one-hot encoding for the designated prosumer. We then enate each bid with its corresponding vector $ e $. The extended encoded bid is represented by $\mathit{S}'_i = \{\sigma_{j \in \mathit{I_S}}(\mathit{S}_i),e_i\} \equiv \{\sigma_{j \in \mathit{I_{S'}}}(\mathit{S}'_i) \}$, where $ \mathit{I_{S'}}= \{1,...,m+n\} $. We also concatenate a vector $\delta \in \{1\}^n$ to the flexibility curve $\mathcal{F}$ to generate $\mathcal{F}'$. Therefore, this new flexibility curve contains  $ n $ new \textit{dummy} items each of which has a capacity of 1, represented as $\mathcal{F}'=\{u_1,u_2,...,u_m,\delta_1,\delta_2,...,\delta_n\} \equiv \{c_1,c_2,...,c_{m+n}\}$. With this arrangement we can represent both requirements in constraint  (\ref{WDPb}).  

%It is well known that the \textit{standard} knapsack problem is not a strong NP-complete problem and solvable in pseudo-polynomial time. However, the multidimensional version is different and it is proved in  \cite{garey1979guide} proved that it is strongly NP-complete and exact techniques are in practice only applicable to instances of small to moderate size. The non-negativity condition is valid by the problem nature, as $ c_j $ can be considered positive in both up-regulation or down-regulation cases, as that information can be represented separately from the problem formula. Meaning that, this information is given to the prosumer that bids positive values in both cases. More precisely, it is natural to assume, without loss of generality, that $ v_i > 0$, $c_j > 0$, $ 0 \leq \sigma_{ij} \leq c_j $ and $\sum_{i=1}^{\kappa}\sigma_{ij} >c_j$  for all  $j \in \mathit{I_{S'}}$ and all $ i \in  \{1,2,..., \kappa \}$.
%
%The best success for solving the MKP has been obtained with tabu-search algorithms embedding effective preprocessing. Recently, impressive results have also been obtained by an implicit enumeration \cite{vasquez2005improved}, a convergent algorithm, and an exact method based on a multilevel search strategy. But still the computation efficiency and implementation difficulty are their main drawbacks.


%$\mathit{S}' = \{\sigma_1(\mathit{S}),\sigma_2(\mathit{S}),...,\sigma_s(\mathit{S})\}$
% meaning that $\sum_{b_j \in a} b_j \leq q_j,  \forall j  \in   \mathit{M} $, and the set of all allocations is denoted by $\mathcal{A}$, each allocation is a pair of bundles and price $ a_j=(a_{bj},a_{pj}) $.

% whereas $\mathit{B} =\{b_1,b_2,...,b_k\}$ represents bundle of those $ k $ 

%time intervals such that  $b_j \leq q_j, \forall  j \in  \mathit{M}$. In other words $b_j$ is the multiplicity of time interval  $ j $ in $\mathit{B}$.

%Note that to define a bid for a bundle $ \mathit{S} $, prosumer $ i $ should provide  how much \textit{quantity}   he is willing to offer i.e., $\mathit{B}$ as well as how much \textit{price} he is willing to get for bundle i.e., $ \mathit{P} $.  
%\textit{[[ Here we can assume that the prosumer submit only one bid that encoded in a logical language, or multiple bids and then there will be more than $ n $ bids.]]}

%
%\begin{maxi!}[2]
%	{x}{J= \sum_{i =1}^{\kappa}   v_i(\mathit{S}_i) x_i(\mathit{S}_i)} {\label{WDP00}}{}\label{WDPaa}
%	\addConstraint{\sum_{i =1}^{\kappa} \sigma_{ji}(\mathit{S}'_i) x_i(\mathit{S}_i) }{\leq c_j; \ \forall  j \in \mathit{I_{S'}} } \label{WDP1}  
%	\addConstraint{x_i(\mathit{S}_i)}{\in \{0,1\}; \ \forall  i \in  \{1,2,..., \kappa \}}  \label{WDP11}%  b_i: \ \ b_i\in \mathcal{B}; \ \ if \  x_i=1}
%\end{maxi!}  
%




%\begin{mini!}[2]
%	{\mathbf{x}}{J= \sum_{i =1}^{\kappa}   v_i(\mathit{S}_i) x_i(\mathit{S}_i)} {\label{WDP0}}{}\label{WDPa}
%	\addConstraint{\sum_{i =1}^{\kappa} \sigma_{ji}(\mathit{S}'_i) x_i(\mathit{S}_i) }{\geq c_j; \ \forall  j \in \mathit{I_{S'}} } \label{WDP2}  
%	\addConstraint{x_i(\mathit{S}_i)}{\in \{0,1\}; \ \forall  i \in  \{1,2,..., \kappa \}}  \label{WDP22}%  b_i: \ \ b_i\in \mathcal{B}; \ \ if \  x_i=1}
%\end{mini!}  



%
%In other words, each bid $ i $ offers $ \sigma_{ji} $ units of flexibility in the $ i $th time interval for $ v_i $ units of price if allocated. The goal is to find a subset of bids that yields maximum total monetary value without exceeding the flexibility capacities $ c_j $.
%\begin{example}
%	\normalfont 
%	
%	To illustrate that let us assume an aggregator has only 4 prosumers in its portfolio of which two prosumers have only PV units, one prosumer has a PV and a battery, and one prosumer has an EV. Assume that the DSO requires up regulations of 4 units at hour 12:00 and 2 units at 13:00 during the day and a down regulation of 1 unit at hour 20:00 in the evening. Suppose, luckily, the maximum power point is at 12 noon and all PV units are expected to provide their rate peak power. Then Table \ref{bidsTab} and \ref{bidsTab2} shows how the values of our example relates to the variables in the WDP in (\ref{WDP0}).
%	
%	$\mathit{I_M}=\{1,2,...,11,12,13,...,20,...,24\}$ 
%	
%	$\mathcal{F}=\{ \ 0,0,...,0,\ 2,\ \ 2,...,1,...,\ \ \ 0\}$ 
%	
%	$\mathit{N}=\{ 1,2,3,4\}$ 
%	
%	Suppose the bids submitted by the prosumers are as shown in Table (\ref{bidsTab}).
%	
%	
%	
%	\begin{table}[!h]
%		\centering
%		\begin{tabular}{|c|c|c|c|c|c|c|}
%			\hline
%			$ I_{12} $ &$  I_{13} $ & $ I_{20 } $ & $ v_1 $ & $ v_2 $ & $v_3 $ &$  v_4 $   \\
%			\hline
%			0 & 0& 0 & - &-  &-  &  - \\
%			\hline
%			0&  0 & 1& - & - & - &  7 \\
%			\hline
%			0 & 2& 0& - & 13 & 12 & -  \\
%			\hline
%			0 & 2& 1& - &-  & - &  - \\
%			\hline
%			2 & 0& 0& 10 & 9 & 15 & -  \\
%			\hline
%			2  & 0& 1&  -&-  & - &  - \\
%			\hline
%			2 & 2 &0  & - & - & - &  - \\
%			\hline
%			2 & 2 & 1& - & - & - &  - \\
%			\hline
%			
%		\end{tabular}
%		\caption{Targeted Bids}\label{bidsTab}
%	\end{table}
%	
%	
%	
%	
%	
%	\begin{table}[!h]
%		\centering
%		\begin{tabular}{|c|c|c|c||c|c|c|c||c|}
%			\hline
%			&$ I_{12} $ 	&$  I_{13} $ & $ I_{20 } $ & $ e_1 $ & $ e_2 $ & $e_3 $ &$  e_4 $  &$  v $  \\
%			\hline
%			$\mathit{S_1 }$	&	0	& 0& 1 & 0 &0  &0  &  1 & 7 \\
%			\hline
%			$\mathit{S_2 }$		&0&  2 & 0& 0 & 1 &0 &  0 & 13 \\
%			\hline
%			$\mathit{S_3 }$		&0 & 2& 0& 0 & 0 & 1 & 0 & 12  \\
%			\hline
%			$\mathit{S_4 }$		&	2 & 0& 0& 1 &0  & 0 &  0 & 9\\
%			\hline
%			$\mathit{S_5 }$		&	2 & 0& 0& 0 & 1 & 0 & 0 & 10 \\
%			\hline
%			$\mathit{S_6 }$		&	2  & 0& 0&  0&0  & 1 &  0 &15\\
%			\hline
%			$\mathcal{F}'$		&	\textbf{4}  & \textbf{2} & \textbf{ 1} &  \textbf{1} & \textbf{1}  & \textbf{1} &  \textbf{1} &-\\
%			
%			
%			\hline
%			
%		\end{tabular}
%		\caption{Bid space concatenated with encoded prosumers}\label{bidsTab2}
%	\end{table}
%	
%\end{example}

\subsection{LFM-WDP Complexity}\label{complex} 

%\textbf{Q1: Why LFM-WDP is difficult?? Generating LFM Difficult CO problems}

The complexity of an optimization problem is typically measured by its encoding length, which is the number of binary symbols required to store an instance of the problem. Tractable problems have a number of operations that are bounded by a polynomial function in the encoding length. For our LFM-WDP, the number of bids made by  $ n $ prosumers is in $ \mathcal{O}(n \times 2^{c . T }) $, which is exponential in the number of time intervals and biddable items. Although our LFM-WDP in (\ref{WDP0}) is an integer linear programming problem, it has two characteristics that place it in the difficult category. 

Firstly, the integrality condition in (\ref{WDPc4}) makes the feasible region non-convex and shifts the problem to mixed-integer linear NP-hard problems. Secondly, the correlation between valuations $v_p$ and their corresponding multiplicities ($ \sum_{\forall j \in \mathit{I_{S_{p_i}}} } \sigma_{j}$) in each bid $\mathit{b}_{p_i}$ determines the problem's classification as either uncorrelated or strongly-correlated, with the latter being more challenging. Our LFM-WDP belongs to the \textit{strongly-correlated} problem class. Although the correlation between the values and multiplicities of items appears to be stochastic, our problem exhibits significant correlation. For example, when instances are generated with equation (\ref{costeq}) for up to 24 intervals in the flexibility curve and 200 submitted bids, the correlation factor is approximately 0.9. This class of problems has been extensively analyzed in the Knapsack problem literature \cite{pisinger2005hard} and to address this difficulty, we leverage machine learning.


% For instance, we can approximate the value $ v_p $ that we modelled in equation (\ref{costeq}) by the uniform distribution $ U $  as

%Principally, the complexity of an optimization problem is measured based on its encoding length, that is the number of binary symbols needed to store an instance of that problem. A problem is said to be tractable if the number of operations for any instance is bounded by a polynomial function in the encoding length \cite{megiddo1987complexity}. For our LFM-WDP, if the flexibility interval can be divided into $ c $ biddable items, the total number of bids by $ n $ prosumers will be  $ \mathcal{O}(n \times 2^{c \times T }) $ which is exponential in the number of time intervals and items per interval. 
%
%Even though our LFM-WDP in (\ref{WDP0}) is an obvious linear programming problem, two characteristics shove it to the difficult class of problems. The first is adding the integrality condition (\ref{WDPc4}) to the formula which makes the feasible region of the problem to be non-convex and shifts it to the mixed-integer linear NP-hard problems. The second is related to another categorization based on how the valuations, $ v_p $ in (\ref{costeq}), are correlated with their corresponding multiplicities ($ \sum_{\forall  j \in \mathit{I_{S_{p_i}}} } \sigma_{j}$) in each bid $\mathit{b}_{p_i}$. \textit{Uncorrelated} instances are easy to solve whereas \textit{strongly-correlated} instances are hard to solve. Our LFM-WDP falls within the second class, i.e., the \textit{strongly-correlated} problems. For instance, we can approximate the value $ v_p $ that we modelled in equation (\ref{costeq}) by the uniform distribution $ U $  as

%\begin{equation}
%\label{wakcorr}
%v_p(\mathit{S}_{p_i})  \sim U\Big(\sum_{\forall  j \in \mathit{I_{S_{p_i}}}} \frac{\sigma_{j}}{|I_{S_{p_i}}|} +cv_1 \ , \ \sum_{\forall  j \in \mathit{I_{S_{p_i}}}} \frac{\sigma_{j}}{|I_{S_{p_i}}|} +cv_2 \Big) 
%\end{equation}
%where $ cv_1 $ and $ cv_2 $ are user define positive correlation values.


%Despite the randomness in the correlation, this problem has a significant correlation between the values and multiplicities of the items. For instance, if we generated instances based on equation (\ref{costeq}) for up to 24 intervals in the flexibility curve and 200 submitted bids, the correlation factor is around 0.9. The difficulty of solving this class of problems is extensively analysed in the Knapsack problem context in \cite{pisinger2005hard} and we focus here to leverage machine learning to alleviate that.  

%This is because most of solving  algorithms are based on sorting the items according to decreasing efficiencies (efficiency is the ratio between the valuations and the multiplicities) hoping that will result in high variations in multiplicities that makes it easy to find the optimal cut for allocations. But with this correlation, sorting by efficiencies also correspond to a sorting by valuations as well. Thus, there is a limited variation in the multiplicities, making it difficult to satisfy the capacity constraint.



%% Figure environment removed
%
%
%


%(a) The instances are ill-conditioned in the sense that there is a large gap between the continuous	and integer solution of the problem.

%For instance a small market of 20 prosumers and 12 flexibility time intervals each of which is quantized to 5 biddable quantities needs approximately 20 Terabyte of memory. This is a strong indication that the bids representation is tightly connected to the algorithmic complexity.

%To measure the methods complexity, we consider the worst-case analysis to give an upper bound for any problem instance of the LFM-WDP. This upper bound is a function of the size of the problem instance. Particularly, it is a function of the number of time intervals $ m $, the maximum number of flexibility units in the time intervals $ c_{max} :=max\{ c_{j}|j=1,2,...,m\}$, the maximum value  $ v_{max} :=max\{ v_{i}|i=1,2,...,n\}$.

%Basically there are three main classes based on the asymptotic running time of algorithms. We mention them ordered from the most pleasant one to the least pleasant ones in term of complexity. The first is polynomial as $ \mathcal{O}(n), \mathcal{O}(n\log n)$ and $ {\mathcal{O}(n^k)} $ where $ k $ is constant. The second class is pseudo-polynomial such as  $ \mathcal{O}(n c)$ and $ { \mathcal{O}(n^2c_{max})} $. The third is the exponential function such as $ \mathcal{O}(2^n) $ and  $ \mathcal{O}(3^n) $.






%Although many impressive results have been obtained in the last decade for solving problems with thousands of integer variables or even more, it seems that the LFM-WDP remains rather difficult to handle when an optimal solution is wanted.



%\begin{enumerate}
%	\item Adding the integrality condition (\ref{WDPc4}) to the formula which makes the feasible region of the problem to be non-convex and shift is to the mixed-integer linear NP-complete problems. 

%	This have been approached by some research by omitting this integrality constraint by linear programming relaxation and optimizing over all real values. This is by replacing the constraint by $0 \leq  x_i(\mathit{S_k}) \leq 1, \ \forall  i \in  \mathit{N} $. However, a Lagrangean relaxation framework is not appropriate to tackle the simple and homogeneous structure of the general multidimensional knapsack problem \cite{freville2004multidimensional}.



%The uncorrelated instances that randomly generated with the uniform distribution are rather easy to solve. However, in LFM the values are correlated with the multiplicities of the time intervals. 

%\textit{Q: Can we investigate the question, raised by \cite{frieze1984approximation}, of computing the asymptotic value of the random variable $ J^* $ for fixed $ (m+n) $.} In other words, can we provide analytic characterization of $ J^* $  under quite restrictive assumptions over the stochastic model. 

%\textbf{Q2: What is wrong with the MIP commercial solvers e.g. Gurobi? }
%
%We target to investigate 2 efficiencies: solution quality and time efficiency.
%The claim is that, even though MIP solvers reached very stable effective level, the computed solution is not always the optimal. \cite{lodi2013heuristic} Shows that some structural characteristics of MIP solvers and of computation for MIP problems reveal a heuristic nature of the solvers.
%
%\textit{\textbf{First}} We show that the constraints coefficient matrix $ \mathit{S}'_i $ in equation (\ref{WDP0}) does not have a \textbf{“clean”} special structure to be exploited through combinatorial algorithms, either exact or heuristic (and meta-heuristic), but instead is a collection of heterogeneous groups of constraints, as it is often the case in real-world applications.
%
%\subsection{Multi-dimensional knapsack literature review}
%In the literature there are classically 3 main classes of solving methods for multi-dimensional knapsack problem.
%\subsubsection{Exact methods}
%
%$ \\ $
%Albeit different algorithms were formulated to furnish good upper and lower bounds; because of NP perfection, the exact approaches are based mostly on a type of branch and bound and commercial solvers like CPLEX only can resolve instances of medium and small sizes (max. 750 and 2 constraints) optimally. Therefore, for solving MKPs instances of large size, several metaheuristic and heuristics techniques have been developed like rough algorithm, multi-stage algorithms, tabu search, simulated annealing, scatter search, and GAs.
%
%\subsubsection{Metaheuristic}
%GA
%
%\subsection{Difference between LF-WDP and Knapsack Problem}
%\begin{enumerate}
%	\item A subset in LFM-WDP can have multiple values, as multiple prosumers may bid on the same subset. In KP each item has a unique value. Concretely saying, LFM-WDP has items values $ v_i(\mathit{S}_i) \in \mathbb{R}^{m+n}$ whereas the standard knapsack problem has items values $ v_i \in \mathbb{R} $.
%	\item The allocation subsets do not have to be disjoint. This because there are multiple units of the same time interval. However in the standard knapsack problem the allocation sets are disjoint, as it is a single item problem. 
%\end{enumerate}
%
%Algorithms for knapsack problems always have to compromise between performance on finding the optimal solution and the computation behaviour.

%\subsection{DP drawbacks}
%\begin{enumerate}
%	\item Due to the excessive space requirements as the single constraint case, instances can be solved only for small values of $\kappa$ and the capacities $ c_j $. Neither of DP variations are an effective self-containing solution for the LFM-WDP. Most DP methods target as maximum as 80 variables and capacities of 2 to 7 at maximum.  \textit{CPLEX, without modifying its default parameters, could solve only 95 instances to optimality in less than 3 hours and a memory size of 250 Mb. Noting that in the rest of the problems, CPLEX usually terminated because the memory size was exceeded by the tree expansion.}
%	\item Uncorrelated instances randomly generated with the uniform distribution are rather easy to solve. \textit{Please verify the LFM-WDP case?}
%\end{enumerate}
%
%\subsection{NN drawbacks}
%In particular, due to the strategic choice of a penalty function which transforms the MKP into an unconstrained problem, NN tends to produce final solutions that violate constraints as firstly investigated by \cite{ohlsson1993neural}.
%\textbf{Example:}

%
%\begin{algorithm}
%	\caption{Combinatorial auction algorithm}
%	\begin{algorithmic}[1]
%		\Procedure{Local Agent 1}{}%{$a,b$}%       \Comment{This is a test}
%		\State Receive the flexibility curve  $\mathcal{F}$
%		\State Generate high dimension bundle space $\mathcal{B}$ 
%		\State Train an auto-encoder model $\mathcal{M}$
%		\State Encode $\mathcal{B}$ to low dimension bundle space $\mathcal{B'}$ 
%		\State Attach the bundle value to  $\mathcal{B'}$
%		\State Send the low dimension bids to the Master Agent
%		\EndProcedure
%		
%		\Procedure{Master Agent}{}%{$a,b$}%       \Comment{This is a test}
%		\State Receive the low dimension bids from Local Agents
%		\State Generate the low dimension space flexibility curve $\mathcal{F'}$
%		\State Solve WDP in the low dimension space and send allocations to Local Agents
%		\EndProcedure
%		
%		\Procedure{Local Agent 2}{}%{$a,b$}%       \Comment{This is a test}
%		\State Receive allocations
%		\State Decode allocations
%		\State Perform flexibility activations
%		\EndProcedure
%	\end{algorithmic}
%\end{algorithm}

%
%The aggregator main task is to decide the optimal winners and prices that fulfil some constraints. The master agent is a smart agent that determines the allocations set $\mathcal{A}$ that consists of $S$ winning bids and it is a subset of $\mathcal{B}$ , i.e., $\mathcal{A}=\{a_1,a_2,...,a_S\}$ which satisfies the DSO flexibility requested in the form of flexibility curve $\mathcal{F}$. The objective could be minimizing the DSO costs or maximizing the social welfare among prosumers.  To minimize the total costs, the master agent aims to solve the combinatorial optimization problem  (\ref{WDP}). Where $\mathcal{X}$ is a binary decision variable set $\mathcal{X}=\{x_1,x_2,...,x_R\}$ and $\mathcal{V}=\{v_1,v_2,...,v_R\}$ is the bids space values offered by prosumers. The WDP is commonly solved under the constraint that each item is allocated to at most one bidder, however in our LFM context one time interval can be allocated to multiple prosumers provided that the sum of the allocations must not exceed the flexibility curve values in each time interval. 
%
%\begin{mini!}{x}{\mathcal{X} . \mathcal{V}} {\label{WDP}}{}
%	\addConstraint{\sum\limits_{i=1}^{S} a_i}{= \mathcal{F}}  
%	\addConstraint{a_i}{\in \mathcal{ B}  \ \ \  \forall \ i }
%\end{mini!}
%
%The literature solves the WDP taking two main paths, exact methods which require enormous amount of storage with complicated calculations and inexact methods which do not guarantee the global optimality of the solution using e.g. genetic algorith (GA) and particle swarm optimization (PSO).  
%
%The winner determination problem (WDP) is a combinatorial optimization problem that arises naturally in many combinatorial auctions. The issue is that it is not straightforward to determine which bids ought to win and which should lose. The goal of the WDP is to determine the feasible and optimal subset of the bids. The WDP is equivalent to the weighted set packing problem, and so is NP-complete \cite{fujishima1999taming}.

%\section{Base lines}
%Gurobi, SCIP 6.0.1
%To achieve benchmarking, We set up scenario where a 24-hours flexibility curve is received from DSO as (time interval, capacity) pairs, $\mathcal{F}= [(7,15), (9,10), (10,9), (12,5),(15,1),(16,5),(19,6),(21,6)]$ as plotted in Fig. \ref{F}. We assume an aggregator which has 50 prosumers with PV-battery systems. The peak production is selected randomly from the hours [10, 11, 12, 13, 14,15] with random number of hours. The bid space of each prosumer is generated based on the requested time intervals. For each prosumer the bid space is $\sum\limits_{k=0}^{K}$\(\binom{n}{k}\), where $n$ is the number of flexibility intervals required by the DSO and $K$ is the number of intervals that the PV can provide energy by the specified prosumer. This produced more than 2500 bids submitted to the aggregator. To bench mark our work we applied GA algorithm to solve the WDP in (\ref{WDP0}).

%\subsection{Benchmark 1: Relaxation}
%
%We can start with the classical base solution for the integrality condition (\ref{WDPc}). That is linear programming relaxation and optimizing over all real values. This is achieved by replacing the constraint by $0 \leq  x_i(\mathit{S_k}) \leq 1, \ \forall  i \in  \mathit{N} $. However, there is a caveat which concern the optimality of the solution. Particularly, the result can only form an upper bound, and also is does not guarantee the constraints satisfaction. This is obvious as by solve the relaxed linear program we obtain a fractional optimal solution which should be “rounded” to obtain an integral feasible solution. The integrality gap is $ J_R-J^* $, where $ J_R $ is the solution of the relaxed version of $ J $. \textit{We should show how big it is}. An allocation set is shown in Fig. \ref{allocationsLP}.
%
%A stronger bound can be obtained using Lagrangian relaxation or surrogate relaxation, however passing from one constraint to two or more constraints generates a significant difficulty gap as shown in \cite{freville2005multidimensional}. This will be our lower bound reference that we will calculate the gap  \%  of our solution. We can relate our developed model to the LP by calculating the improvement gap as in  (\ref{gap})
%
%\begin{equation}
%	\label{gap}
%	gap = \frac{Z^{LP}-Z^*}{Z^{LP}}
%\end{equation}
%
%In the context of constrained problems, the penalty functions are used where a penalty term is added to the objective function aiming to penalize constraint violation. But the performance of penalty-type method is not always satisfactory due to the choice of appropriate penalty parameter values. Hence, other alternative constraint handling techniques have emerged in the last decades. 



%section{Benchmark Methods} \label{exp}
%\subsection{Benchmark 2: Exact Methods: Dynamic Programming}
%
%We modelled our LFM-WDP as an integer programming problem (\ref{WDP0}) that has a similar general structure but not equivalent to the multidimensional knapsack problem. As there are several additional constraints in LFM-WDP that can lead to a special extension to the standard knapsack problem. In this section we utilize dynamic programming to achieve a pseudo-polynomial time solution. We mean by pseudo-polynomial algorithm is that whose worst case time does not depend on the number of inputs, as in the naive solution we discussed earlier, but on the numeric value of the input. 
%
%Dynamic programming is a framework which can be used for optimization problems to do an exhaustive search but in a strategic way that can accomplish a near polynomial time complexity. That is in the broad sense by breaking a problem to sub-problems and then memoize (i.e., remember) and reuse the solution of those sub-problems when they appear during the solving procedure.  
%
%To utilize dynamic programming we need firstly to create sub-problems to get the optimal substructure that lead to solve the actual problem. It is important to use proofs to determine optimal substructure in order to use dynamic programming. In technical language, we should proof that the \textit{principle of optimality} (aka Bellman principle) holds for our LFM-WDP. Two ways are proof by \textit{contradiction} and proof by \textit{induction}.  Note the running time to solve the actual problem is a multiplication of the number of sub-problems times the time to solve the sub-problem. 
%
%
%\begin{lemma}
%\normalfont 
% For $ 1 \leq i \leq n $, $ 0 \leq w \leq W $ :
%\[ V[i,w]=max(V[i-1,w],v_i+V[i-1,w-w_i]) \]
%
%
%
%
%\end{lemma}
%
%\begin{proof}
%	To prove it by contradiction try and assume that the statement is false,
%	proceed from there and at some point you will arrive to a contradiction.
%\end{proof}


%In other words to achieve a polynomial time dynamic programming we should : guess, find recursion, memoize.


%To calculate the \texttt{running time = number of sub-problems$ \times  $run time of the sub-problem}.

%We can also represent LFM-WDP as a DAG.
%$\mathcal{O}((2u)^m \times n) \approx \mathcal{O}(u^m) $  OR
%A memory lighter approach is to restrict the search space to a subset of atomic bids that each prosumer can submit. This is accomplished by allowing logical bidding languages to be used to represent the bids.
%
%
%This challenge is not specific to our application, however the optimization algorithm search space we face is huge compared to the examples discussed in the literature. We will set a benchmark WDP solver based on the recent publications and compete with it in performance developing our own method.



\section{LFM Neural combinatorial auction}\label{sec4}
Despite previous discussions on the complexity and difficulty of LFM-WDP, we intend to utilize the inherent similarity among the problem instances. We hypothesize that LFM-WDP instances follow an underlying unknown probability distribution  $ \mathcal{P} $, which is determined by the nature of LFMs. This research aims to use graph neural networks (GNNs) as a machine learning framework to learn this unknown probability distribution. The primary objective is to develop a machine learning model capable of mapping LFM-WDP instances to their optimal solutions. This will be achieved by adopting a supervised learning approach, imitating an off-the-shelf solver as an expert system and producing plausible optimal solutions. Our specific aim is to learn a function $h: \mathcal{X} \rightarrow \mathcal{Y}$, where $ \mathcal{X} $ represents the LFM-WDP representation, and $ \mathcal{Y} $ denotes the corresponding target set of solutions. To achieve this, we followed our design framework shown in Figure \ref{blockdiag} .
% Figure environment removed


%Notwithstanding our previous discussion on LFM-WDP complexity and hardness, we aim to take advantage of the problem instances inherent similarity that arises in the LFM domain which tends to produce a restricted set of problem instances. We hypothesise that LFM-WDP instances follow some underlying unknown probability distribution $ \mathcal{P} $ that is determined by LFMs application domain. In this work, we mainly aim to leverage graph neural networks (GNNs) as a machine learning framework to learn this unknown probability distribution. Our objective is to learn an end-to-end model that can map problem instances to their optimal solutions. We adopt a supervised-learning approach to \textit{imitate} an of-the-shelf solver as an expert system and produce plausible optimal solutions. More concretely, we aim to learn a function $h: \mathcal{X} \rightarrow \mathcal{Y}$, where $ \mathcal{X} $ is some representation of our LFM-WDPs and $ \mathcal{Y} $ is their corresponding target set of solutions. To accomplish such goal we set the design framework shown in Figure \ref{blockdiag} that we discuss next. 




\subsection{LFM-WDP Instance Generation and Expert Solver}\label{instgen}
One challenge of our machine learning approach is the lack of gold standard data, i.e., $ \mathcal{X} \times \mathcal{Y} $ pairs. To address this, we have developed a procedure for generating LFM-WDPs. We adopt a bottom-up approach using a real PV production dataset from prosumers \cite{ausgridds}. Specifically, we start by forming a set of possible bids that prosumers would bid for and then generate the corresponding flexibility curve. This approach provides more control over the complexity of the generated instances compared to randomly generating flexibility curves and then generating bids.

%One of the challenges to our machine learning approach is the lack of gold standard  data, i.e., $ \mathcal{X} \times \mathcal{Y} $ pairs. To overcome this, we start by developing an LFM-WDPs generation procedure. We utilize a set of prosumers' large real PV production dataset \cite{ausgridds} and take a bottom-up approach, in the sense that we firstly form a set of possible bids that the prosumers would bid for and then generate the flexibility curve accordingly. This allows for more control on the complexity of the generated instances, in contrast to start by randomly generating a flexibility curve and generate the bids afterwards.

From a prosumer perspective, a biddable time interval is one that has PV production $ u_{prod} $ greater than a threshold portion $\epsilon$ of its full capacity $u_{max}$. Then, we define prosumer $ i $'s biddable set as	$I_{pv}= \big\{ j : u_{prod_j} \geq \epsilon \cdot u_{max_j}; \ \ j \in I_M\big\}$ with its PV production set as $ U_{pv}= \big\{ u_{prod_j}; \ \ \forall j \in I_{pv}\big\}$.




%\begin{equation}
%\label{Ipv}
%I_{pv}= \big\{ j : u_{prod_j} \geq \epsilon \cdot u_{max_j}; \ \ j \in I_M\big\}
%\end{equation}
%with its PV production set as
%\begin{equation}
%\label{Upv}
%U_{pv}= \big\{ u_{prod_j}; \ \ \forall j \in I_{pv}\big\}
%\end{equation}

Considering the two types of prosumers, those who own only PVs and those equipped with ESS, the former can only bid on all subsets of time intervals in $I_{pv}$ with their corresponding values from $U_{pv}$ to provide ramp-up services, resulting in $\sum_{k=1}^{|I_{pv}|}{{|I_{pv}|}\choose {k}}$ bids, the latter can bid on additional ramp-up and ramp-down intervals.

We define the availability of ESS time intervals as any time interval when the PV production is not sufficient as $ I_{ss}= \big\{ j : u_{prod_j} < \epsilon.u_{max_j}; \ \ \forall j \in I_M \big\}$. Then the biddable intervals for this group of prosumers is as  $ I_{pv\_ss}  =  \big\{ \{ j : u_{prod_j} \geq \epsilon.u_{max_j}\} \cup \{ k : u_{prod_k} < \epsilon.u_{max_k}\} ; \forall j, k \in I_M \big\}$ thus the bids space that serves for the ramp-up flexibility requests contains all subsets of $ S_{pv\_ss} \subseteq \big\{ I_{pv\_ss}: \big( \big|I_{pv\_ss}:j \in I_{pv} \big| + \big|I_{pv\_ss}:k \in I_{ss} \big|\big) \leq \big|I_{pv}\big|\big\} $.

%\end{equation}
%\begin{equation}
%	\label{Ipvesssub}
%	\begin{split}
%		S_{pv\_ss} \subseteq \big\{ I_{pv\_ss}: \big(& \big|I_{pv\_ss}:j \in I_{pv} \big| + \\ 
%		& \big|I_{pv\_ss}:k \in I_{ss} \big|\big) \leq \big|I_{pv}\big|\big\} 
%	\end{split}
%\end{equation}

%Considering the two types of prosumers, a group which own only PVs and a group which is equipped with ESS, prosumers who are not equipped with ESS can only provide ramp-up services bidding on all subsets of time intervals in $I_{pv}$ with their corresponding values from $ U_{pv} $, as shown in Table \ref{flexdic}, this will construct $\sum\limits_{k=1}^{|I_{pv}|}{{|I_{pv}|}\choose {k}}$ bids. On the other hand, prosumers equipped with ESS have the option to bid in additional ramp-up intervals as well as ramp-down intervals, as in Table \ref{flexdic}. We define the availability of ESS time intervals to be any time interval that the PV production is not sufficient as  $  $
%\begin{equation}
%\label{Iess}
%I_{ss}= \big\{ j : u_{prod_j} < \epsilon.u_{max_j}; \ \ \forall j \in I_M \big\}
%\end{equation}


%\begin{equation}
%	\label{Ipvess}
%I_{pv\_ss}= \big\{ j ; \ \ \forall j \in I_M \big\}
%\end{equation}

%\begin{equation}
%\label{Ipvess}
%\begin{split}
%I_{pv\_ss}  =   \big\{ & \{ j : u_{prod_j} \geq \epsilon.u_{max_j}\} \ \cup \\
%& \{ k : u_{prod_k} < \epsilon.u_{max_k}\} ; \ \forall j, k \in I_M \big\}
%\end{split}
%\end{equation}


With ESS, prosumers can bid on ramp-down intervals if their ESS is fully discharged, for every subset $S \in I_{pv_ess}$, with up to $|S|$ ramp-down intervals. We calculate the price using formula ($\ref{costeq}$) and accumulate the flexibility curve proportionally to the sum of the bids, as  $\mathcal{F} = \bigg\{\eta \sum\limits_{i=1}^{\kappa}  \sigma_j(\mathit{S}_i); \ \forall j \in I_M\bigg\}$ where $ 0 <\eta \leq 1$ is the proportionality factor and we use it to control the correlation and hence the problems complexity, $\kappa$ is the number of bids, $\sigma_j(S_i)$ is the offered flexibility by the subset $ S_i $ for time interval $ j $.

%Assuming that the storage systems are correctly sized to store the PV production, for the ramp-down intervals, prosumers can submit bids if their ESS is fully discharged. That corresponds to every subset $ S \in I_{pv\_ess} $ can have up to  $ |S| $ of ramp-down intervals.
%
%Accordingly, we calculate the price using our previous formula ($ \ref{costeq} $). Then, we accumulate the flexibility curve to be proportional to the sum of the bids as ...
%\begin{equation}
%\label{flex_gen}
%\mathcal{F} = \bigg\{\eta \sum\limits_{i=1}^{\kappa}  \sigma_j(\mathit{S}_i); \ \forall j \in I_M\bigg\}
%\end{equation}
%where $ 0 <\eta \leq 1$ is the proportionality factor, $\kappa$ is the number of bids, $\sigma_j(S_i)$ is the offered flexibility by the subset $ S_i $ for time interval $ j $.



%then the subset for prosumer  $ i $ is $\mathit{S}_i = \{\sigma_{j \in \mathit{I_S}_i}(\mathit{S}_i)\}$ to be the set of flexibility units quantity, where $ \sigma_j(\mathit{S}_i) $ is the quantity of flexibility in time interval  $ j\in\mathit{I_S}_i$. 


%\subsection{Expert Solver}
Commercial mixed-integer linear programming (MILP) solvers use advanced algorithms to converge to optimal solutions, but they still make heuristic decisions during runtime. For example, in branch-and-bound algorithms, variable and node selections are critical decisions, while the Gomory cut in cutting-plane approaches requires computation time. We aim to learn these underlying heuristics during the GNN-based model's training phase.

To obtain the corresponding set of labels $\mathcal{Y}$, we represent our LFM-WDPs as equations set (\ref{WDP0}) and use a mixed-integer linear programming solver (MILP) as an expert based on the data generated in the previous step.



%To produce the corresponding set of labels $ \mathcal{Y} $, we formulate our LFM-WDPs in the form of equations set (\ref{WDP0}) and take advantage of a mixed-integer linear programs solver (MILP) as an expert using the data generated in the previous step. 
%
%It worth noting that even though  commercial solvers use advanced algorithms to converge to the optimal solutions, they make crucial decisions heuristically at the run time. For instance in branch-and-bound algorithm, the variable and node selections are largely seen as the most crucial decisions. In the cutting-planes approach the Gomory cut is tricky to be calculated and requires computation time. We aim to learn those underlying heuristics at the training phase of the GNN-based model.


\subsection{LFM-WDP Graph Representation}




To effectively utilize GNNs for solving the LFM-WDPs, it is critical to create an appropriate graph representation. A graph is a mathematical structure that consists of nodes and edges, representing measurable elements and their relationships. Nodes and edges have quantifiable features, which enable the analysis and comparison of different graphs. The proposed graph representation for LFM-WDPs includes bid nodes $\textbf{x}$, flexibility (Flex) nodes $\beta$, and mutual exclusiveness (MUX) nodes $\alpha$. The graph representation addresses the similarity issue of node features by incorporating neighbouring node features into the analysis.

Each bid node $x^i_p$ corresponds to a bid $i$ from prosumer $p$, with a feature vector [$f^i_p$] concatenating the bid price and items, i.e., $ [f^i_p] = [v_p(S_i) \mathbin\Vert S_i] $. Flexibility nodes $\beta_\tau$ represent the total capacity of each interval in the flexibility curve $\mathcal{F}$, with a distinctive feature vector $u_\tau \in \mathbb{R}$. MUX nodes $\alpha_i$ represent the XOR condition of prosumer $i$ bids, with features set to unit vectors.

Edges between nodes capture the relational structure of the problem. In this graph representation, nodes of the same type are not connected, and it is categorized as a tri-partite graph. Bid nodes are connected to the corresponding flexibility nodes by undirected edges, with bid quantity [$\sigma_{i\tau}$] as the edge feature. Prosumer nodes are connected to the corresponding MUX nodes by undirected edges, with features set to unit vectors. 



%In order to utilize graph neural networks (GNNs), a proper graph representation must be created for LFM-WDPs. A graph is a mathematical structure consisting of nodes and edges. Nodes represent measurable elements in a problem, while edges represent relationships or dependencies between these elements. Nodes and edges can be characterized by quantifiable features, allowing for the analysis and comparison of different graphs. Encoding LFM-WDPs as graphs is critical because optimization problems are non-structural graph instances that lack natural and unique graph representations.

%To make use of the graph neural networks (GNNs) framework, a proper graph representation for LFM-WDPs should be designed. Conceptually, a graph is a set of distinct nodes (also known as vertices) which can be of different types. Those nodes are joined optionally to each other by edges which represents some sort of relationships or dependencies between nodes. At the abstract level, the nodes in a graph can encode any set of quantifiable elements in the represented problem which have a notion of relationship between them, whereas edges can represent this notion of relationship between those elements. Both nodes and edges can be characterized by features that are quantifiable attributes. How to encode our LFM-WDPs as graphs is a crucial step. That is because optimization problems are non-structural cases of graphs, in the sense its graph representation is not naturally explicit and is not unique. Next, we discuss building our proposed graph representation which exploits problem instances relational structure that can naturally be processed by GNNs.

%
%As we consider the multi-minded bidder case where bidders are allowed to bid multiple bids, however at most one bid will be allocated to each prosumer (i.e., XOR bids), 

%Our proposed graph representation in Figure \ref{wdpgraph} encodes the LFM-WDP in bid nodes $\textbf{x}$, flexibility (Flex) nodes $\beta$, and mutual exclusiveness (MUX) nodes  $\alpha$. A bid node $x^i_p$ represents a bid $i$ from prosumer $p$, with a feature vector [$f^i_p$] that concatenates the bid price and items; thus $ [f^i_p] = [v_p(S_i) \mathbin\Vert S_i] $. Flexibility nodes $\beta_\tau$ represent the total capacity of each interval in the flexibility curve $\mathcal{F}$, with a distinctive feature vector $u_\tau \in \mathbb{R}$. A MUX node $\alpha_i$ represents the XOR condition of prosumer $i$ bids with features set to unit vectors.
%
%Edges between nodes capture the relational structure. With no edges between nodes of the same type, our graph representation is placed in the tri-partite class of graphs. Bid nodes are connected to the involved flexibility nodes by undirected edges, with bid quantity [$\sigma_{i\tau}$] as the edge feature. Nodes that belong to the same prosumer are connected to a MUX node by an undirected edge  with features set to unit vectors.


%We introduce the proposed graph representation in Figure \ref{wdpgraph} which encodes the LFM-WDP in three types of nodes. The first are bid nodes $ \textbf{x} $ where each node represents a bid. We denote each bid node as $ x^i_p $ which represents the bid $ i $ of prosumer $ p $ and has a feature vector $  f^i_p $ to be the concatenation of the bid price (valuation) and items; thus $ f^i_p = [v_p(S_i) \mathbin\Vert S_i $]. The second type of nodes consists of the flexibility nodes $\beta$ and represents the total capacity of each interval in the flexibility curve $\mathcal{F}$. Each $\beta_\tau$ node represents time interval $ \tau $ and has its capacity as a distinctive feature vector $ u_\tau \in \mathbb{R} $. The third type of nodes is the mutual exclusiveness nodes $\alpha$ where $\alpha_i$ represents the XOR condition of prosumer $ i $ bids and has its features are set to 1. The mutual exclusiveness nodes $\alpha$ can be thought of as being similar to a flexibility node x with a capacity of 1. 

%Adding edges will capture the relational structure between the nodes. We emphasis that there are no edges between nodes of the same type. This condition places our graph representation in the tri-partite \cite{garey1979computers} class of graphs. Each bid node is connected by an undirected edge with the flexibility nodes that are involved in the represented bid. For instance bid node $ x^i_j $ is connected to flexibility node $\beta_\tau$ only if it is included in its bundle. The bid-flexibility edges are featured by the bid quantity  $\sigma_{i\tau}$ of that time interval. On the other hand, all node bids that belong to the same prosumer are connected to a single mutual exclusiveness node by an undirected edge which has a feature of 1.

% Figure environment removed


Thus the proposed graph consists of bid nodes $\mathcal{\text{X}}$, flexibility nodes $ \boldsymbol{\beta} $, mutual exclusiveness nodes $ \boldsymbol{\alpha} $, and edges $\mathcal{E}$ is denoted by $\mathcal{G(\text{X}, \boldsymbol{\beta}, \boldsymbol{\alpha},E)}$. Its adjacency matrix, $\textbf{A} \in \mathbb{R}^{n_{nodes} \times n_{nodes}}$, with $n_{nodes}=\kappa + T + n$, serves as a measure of the LFM-WDP size. 

%Then, a graph that consists of a set of bid nodes $ \mathcal{\text{X}}$, a set of flexibility nods $ \mathcal{\boldsymbol{\beta}}$, a set of mutual exclusiveness nodes $\mathcal{\boldsymbol{\alpha}}$ and a set of edges $ \mathcal{E}$ is compactly represented as $\mathcal{G(\text{X}, \boldsymbol{\beta}, \boldsymbol{\alpha},E)}$ with its adjacency matrix $ \textbf{A} \in \mathbb{R}^{n_{nodes} \times n_{nodes}}$, where $ n_{nodes}=\kappa + T + n $. The adjacency matrix $ \textbf{A} $ rows and columns are indexed by the graph nodes indexes and the matrix entries are the corresponding edge features. This adjacency matrix can be a measure of the LFM-WDP size. 
%
%Even though bids may have the same price for different flexibility intervals, this proposed graph representation can alleviate the similarity issue of the node features by allowing to construct different and more expressive neighbourhood structures for the nodes. In other words, this graph representation does not only consider node individual features but also its neighbouring nodes' features.

%The factor graphs sets are \textit{variable} nodes and \textit{factor} nodes. Variable nodes represents the variables in the optimization problem and could be continuous or having only discrete states also states can be observed (i.e., known) or hidden (i.e., unknown). Factor nodes show how the objective factorizes (breaks up) to local terms.  Graph vertices have feature vectors in contrast to the work in \cite{li2018combinatorial}


%The vertices has two/three types $\mathit{v}$ or $\mathit{u}$  the edges are represented by a tuple $\mathit{e_{ij}=(u_i,v_j,r_{ij})}$.


%Graph embedding can represent the sparse adjacency matrix to a lower dimensional condensed matrix that is easy to work with capturing the important features (e.g. the relationship or dependency between nodes). This condensed representation makes it easy to work with in machine learning models.

%$\mathcal{G(V,E)}$

%\begin{maxi!}[2]
%	{x}{ \sum_{i \in \mathit{N}} \sum_{\mathit{S_k} \subseteq \mathit{M}} v_i(\mathit{S_k}) x_i(\mathit{S_k}); \  \forall k \in \mathit{K} }  {\label{WDP_basic_0}}{}\label{WDP_basic_a} 
%	\addConstraint{\sum_{i \in \mathit{N} } \sum_{\mathit{S_k} \subseteq \mathit{M}, \mathit{S} \ni j} \sigma_{ji}(\mathit{S_k}) x_i(\mathit{S_k}) }{\leq u_j; \ \forall  j \in  \mathit{M}} \label{WDP_basic_b}    
%	\addConstraint{\sum_{\mathit{S_k} \subseteq \mathit{M} }   x_i(\mathit{S_k}) }{\leq 1; \ \forall  i \in  \mathit{N}}  \label{WDP_basic_c} 
%	\addConstraint{\sum_{\mathit{S_k} \subseteq \mathit{M} } v_i(\mathit{S_k}) x_i(\mathit{S_k}) }{\leq V_{DSO}; \ \forall  i \in  \mathit{N}}  \label{WDP_basic_d} 
%	\addConstraint{x_i(\mathit{S_k})}{\in \{0,1\}; \ \forall  i \in  \mathit{N} , \forall  k \in \mathit{K}}  \label{WDP_basic}%  b_i: \ \ b_i\in \mathcal{B}; \ \ if \  x_i=1}
%\end{maxi!} 


%
%
%\begin{maxi!}[2]
%	{x}{J= \sum_{i =1}^{\kappa}   v_i(\mathit{S}_i) x_i(\mathit{S}_i)} {\label{WDP02}}{}\label{WDPa2}
%	\addConstraint{\sum_{i =1}^{\kappa} \sigma_{ji}(\mathit{S}'_i) x_i(\mathit{S}_i) }{\leq c_j; \ \forall  j \in \mathit{I_{S'}} } \label{WDPb2}  
%	\addConstraint{x_i(\mathit{S}_i)}{\in \{0,1\}; \ \forall  i \in  \{1,2,..., \kappa \}}  \label{WDPc2}%  b_i: \ \ b_i\in \mathcal{B}; \ \ if \  x_i=1}
%\end{maxi!} 


%
%%




%\subsection{Study option 3 :  Branch and Bound Supported by Machine Learning} 

%\subsection{Study option 3 :  Outer Approximation Supported by Machine Learning} 




%
%
%A question here: can we produce some notion of spatial locality, meaning that we can reproduce an image-like graph representation that can be input to CNN. This should have some sort of node ordering invariance, maybe an ordering step should proceed applying CNN. 
%
%
%%
%% Figure environment removed


%\begin{itemize}
%	\item 
%\end{itemize}

%{\color{blue}

\section{GNN Model for the Neural Solver}\label{sec5}

We tackle the LFM-WDP as a multi-label node classification problem in which we aim to learn a function $h \colon \mathcal{G} \to \mathcal{Y} \in \mathbb{Z^{\kappa}}$ given $\mathcal{G(\text{X}, \boldsymbol{\beta}, \boldsymbol{\alpha},E)}$. Unlike conventional classification tasks with mutually exclusive class labels, in our case,  multiple nodes can be assigned to the same class. Our approach is fully-supervised, where a model is trained to classify nodes across multiple labeled graphs. We propose a combinatorial auction graph neural network and discuss its design choices, including inter-node communication, message transformation and aggregation, network depth, learning objective and loss function. This approach differs from semi-supervised node classification problems, which focus on assigning labels to partially labeled nodes in a single massive graph.

%Unlike conventional classification tasks with mutually exclusive class labels, multiple nodes can be assigned to the same class in our LFM-WDP. Our case also differs from the semi-supervised node classification problems heavily addressed in graph theory literature, which focus on assigning labels to partially labelled nodes in a single massive graph. Instead, we adopt a fully-supervised approach by training a model to classify nodes within multiple fully labelled graphs. We propose a combinatorial auction graph neural network and here we discuss its design choices, including how nodes communicate with neighbors, how messages are transformed and aggregated, the depth of the network, and the learning objective and loss function.

%
%We approach the LFM-WDP as a node classification problem that is: given $\mathcal{G(\text{X}, \boldsymbol{\beta}, \boldsymbol{\alpha},E)}$ we aim to learn a function $h \colon \mathcal{G} \to  \mathcal{Y} \in \mathbb{Z^{\kappa}}$. In contrast to the conventional classification tasks with mutual exclusive class labels, our LFM-WDP is a multi-label classification problem; meaning that multiple nodes can be assigned to the same class. LFM-WDP as a node classification problem has a subtle difference from the node-classification problems addressed heavily in the graph theory literature. In the literature the node-classification deals with a single massive graph where nodes are partially labelled and the task is to learn how to assign labels to the unlabelled nodes in a semi-supervised way. In contrast, we take a fully-supervised approach by having many graphs of nodes that are fully labelled and we build a model to classify nodes within unseen graphs.
%
%To discuss the design details of our proposed combinatorial auction graph neural network, this section explores the design space comprised of four principal design choices and demonstrates our decisions to enhance our solution's architectural design. These choices decide how nodes form their information to communicate with neighbours in a form of messages, how to transform and aggregate multiple node messages to inform a receiving node, how deep the network is stacking multiple layers of nodes together and what is the learning objective and loss function.  

%This requires particular machine learning methods that support predicting multiple mutually non-exclusive classes.

%An important challenge here is under-fitting, as the feature space is not rich to capture the patterns! Adding more relevant features??

%We will pass the graph features along with its adjacency matrix to a graph neural network to map every node to a latent space feature representation. Then we apply a function (classifier) which is shared between all nodes to perform the node classification, see Fig. \ref{cnn4wdp} which shows the hidden representation.
%
%
%We aim to predict the optimal assignment for bids by assigning probability value for each bit to be selected. We adopt the vertex-level framework where a unique output value is calculated for every bid-vertex. We do that by applying the output function to every vertex in the graph.

%% Figure environment removed

%Black box MIP solvers options are:
%\begin{enumerate}
%	\item Commercial solvers: Gurobi, CPLEX and FICO Xpress.
%	\item Non-commercial solvers: SCIP, CBC, MIPCL and GLPK.
%	\item Solver-independent language: CVXPY.
%\end{enumerate}
%
%
%Those solvers hit around some exact algorithms which are:
%\begin{enumerate}
%	\item Branch-and-bound.
%	\item Cutting planes (Choose this?).
%	\item Branch-and-cut (a hybrid between 2 first methods, applying cutting planes first and then branch and bound)
%\end{enumerate}
%
%A fail over strategy can include using a standard LP algorithm.
%
%


%
%\subsection{Optimal Winner Determination Algorithms}
%
%In this subsection, we will discuss the tree search algorithms that provably find an optimal solution in the literature. The goal of search algorithms is to make a set of decisions to simulate all possible ways of making them. Even this is too exhaustive, some \textit{selective} search techniques still provable to find an optimal solution. To design such algorithm we must as many questions:
%
%\begin{enumerate}
%	\item Search formulation options to answer on what basis the algorithm will branch the following or on all of them:
%	\begin{itemize}
%		\item Branch on items. The question at every node is "What bid should this item be assigned to". Consequently, even with reducing the branching factor the worst case will be polynomial in the number of bids but exponential in the number of items, meaning that the number of leaves is $\mathcal{O}(m^{m})$.
%		
%		\item Branch on bids. The question at every node is "Should this bid be accepted or rejected". It has faster performance compared to the branch on items.
%		
%		\item Multivariate branching by asking question like "Of these 11 bids are there 3 winners". Generally, one can use any hyperplane as a branching question.
%	\end{itemize}
%
%
%	\item Search strategy options to answer on what order the algorithm will search the tree:
%\begin{itemize}
%	\item Depth-First Search. 
%	
%	\item Depth-First Branch-and-bound Search: it is the simplest \textit{informed} search tree branch is pruned when the incumbent is less than the value of $ g+h $ ($ g $ is the sum of prices of accepted bids and $ h $ indicates how much revenue is left out in the unassigned yet items).
%	
%	\item $A^*$ and Best-Bound Search: 
%	
%	\item Iterative deepening $ A^* $ Search
%	
%	\item Exotic Search:
%\end{itemize}
%
%\end{enumerate}

%\subsection{ML in branching for MILP}
%
%The intuition is that there are crucial decisions that are made at the running time of the algorithm and those can be smartly decided upon using some machine learning. For instance in branch-and-bound algorithm, the variable and node selections are largely seen as the most crucial decisions. In the cutting-planes approach the Gomory cut is tricky to be calculated. Similar decision classes are made in other exact methods for MILP. 
%
%\textbf{Variable selection criteria} (1)For a long time, a classical choice has been branching on the most fractional variable, i.e., in the 0-1 case the closest to 0.5 (sometimes referred to as most \textit{infeasible branching}, MIB in short). That rule has been computationally shown by \cite{achterberg2005branching} to be worse than a complete random choice. (2)\textit{ Strong branching}. (3) \textit{pseudocost branching} (4) \textit{reliability branching} (4) \textit{hybrid branching}
%
%\textbf{Node selection criteria} (1) \textit{Best-first} (2) \textit{depth-first} (3) Hybrid between the two criteria.
%
%An important note here, in general, there is no deep understanding of the theory underneath branching so the application of machine learning methods seems quite appealing \cite{lodi2017learning}.
%
%\textbf{We are aiming to define a branching strategy extracting novel kinds of information and combining them in original ways to guide the search of the branch-and-bound tree in an efficient way.} Particularly, we will exploit (possibly a large quantity of) collected data, and employing the learning framework to come up with more informed and complex decision functions, estimating a good branching strategy. We focus mainly on the\textit{ online learning}
%
%Two ways to think of integrating ML in CO: 
%\begin{itemize}
%	\item Primal heuristics learning (diving).
%	\item Branching policy learning.
%\end{itemize}
%- primal heuristics learning


%\subsection{WDP as a probabilistic inference}
%The difference between  complete and heuristic search strategies is that complete strategies perform a systematic examination of all possible solutions of the search space whereas heuristic strategies only concentrate on a part of them following a known algorithm.
%
%The behaviour of evolutionary computation algorithms such as GAs depends to a large extent on associated parameters like operators and probabilities of crossing and mutation, size of the population,  and the number of generations. It requires experience in parametrizing these algorithms. Even automating the parameter selection will be itself an additional optimization problem.
%
%To formulate the WDP as a probabilistic inference problem, to use machine learning algorithms, normally the probabilistic model learnt in the e.g. in EDA is n-dimensional where $ n $ is the number of decision variables.
%
%We need to learn a probabilistic model that better represents the interdependencies between variables. In our problem, the variables are dependent as the selection of one item affects the selection of the other one which establishes a conditional probability scheme $ p(x_i | x_j) $ 
%
%%\subsection{NeuroCA suggested frameworks}

%\section{LFM Graph Neural Network Design} 


%\begin{enumerate}
%	\item Layer-level (intra-layer) message formation (computation) approach.
%	\item Layer-level (intra-layer) message transformation (aggregation) approach.
%	\item Stacking approach of different layers together to form a deep network.
%	\item Graph manipulation, e.g. feature augmentation?
%	\item Learning objective. e.g. supervised/unsupervised, node prediction/graph prediction ?
%\end{enumerate} 

%% Figure environment removed





\subsection{Computation Graphs and Messages} 

To demonstrate the computation of each node's features considering its neighborhood, we unfold our proposed graph representation to obtain a computation graph for each node. A simplified example of an LFM-WDP of 2 prosumers, each one submits 2 bids competing on 2 flexibility intervals is illustrated in Figure \ref{redgraph}. Each node aggregates information from its neighbors to compute its feature representation. Note that our proposed heterogeneous tri-partite graph representation offers a simple yet expressive advantage, that is bid nodes communicate with their constraint-type neighbors through a two-hop computation. The immediate neighbors of a bid node are only constraint-type nodes, either $\beta$ or $\alpha$, and vice versa. Therefore, a two-hop depth is sufficient to capture the overall graph structure, as bid nodes can only communicate their information through constraint nodes. This simplifies our GNN model, as each bid node's state is updated after being aware of the constraints' state.




%To show how the features of each node is computed considering its neighbourhood, we firstly unfold the graph representation we proposed in Figure \ref{wdpgraph} over nodes to obtain a computation graph for every node. The unfolding leads to each node having its unique computation graph based on its own neighbouring nodes and how it is connected to them. Each node aggregates the transformed information from its neighbours to get its surroundings state. A simplified example of an LFM-WDP of 2 prosumers, each one submits 2 bids competing on 2 flexibility intervals is illustrated in Figure \ref{redgraph}. The figure shows a two-hop computational graph, i.e., only considering the node neighbours and the neighbours of its neighbours in the computation. 


% Figure environment removed
	%
	%% Figure environment removed

%Our proposed heterogeneous tri-partite graph representation offers a simple yet expressive advantage. The immediate neighbors of a bid node are only constraint-type nodes, either $\beta$ or $\alpha$, and vice versa. Therefore, a two-hops depth is sufficient to capture the overall graph structure, as bid nodes can only communicate their information through constraint nodes. This simplifies our GNN model, as each bid node information is transformed by a constraint node before being delivered to another bid node, allowing for bid nodes to update their state after being aware of the constraints' state.

%The simplicity yet expressiveness is a principal by-product advantage of our proposed heterogeneous tri-partite graph representation. That is all the immediate neighbours of a bid node x are constraint-type nodes only, either $\beta$ or $\alpha$. At the same time, all neighbours of the constraint nodes are bid-type nodes only. This indicates that two-hops depth is sufficient to capture the overall graph structure from each node perspective, in the sense that bid nodes can only communicate their information together through constraints' nodes. With this, each bid node information (we will call this information a message later) gets transformed by a constraint node before being delivered to another bid node. In other words, bid nodes will update their state after being aware of the constraints state. This is what simplifies our GNN model as we discuss next.   

%The simple version of this is a linear transformation by it by a weight matrix. This computation graph can be reduced as in Fig. \ref{redgraph} . Both computation graphs represent a single block consists of 2 layers of our neural network. Because of our bipartite graph nature, we do not see a mixture of the two nodes types in each layer, i.e., one layer predicts the constraints nodes and the next layer predicts the bids nodes. The block takes only the bids nodes labels from the previous layer output and outputs the embeddings for the next block in case we use a deeper graph level model. The block is shown in Fig. \ref{gnnblock}. The full model is in Fig. \ref{LFM-WDP Model}.




%
%% Figure environment removed
%
%
%
%
%% Figure environment removed

%\subsection{Message Formation and Aggregation} 

%We define a message function $\mathcal{M}(.)$ to convert the feature representation of a neighboring node $r$ to an information message $m_r$. This message is sent to node $q$, which has its own feature representation $h_q$ and set of neighboring nodes $r_q$. We also generate a self-message from node $q$'s features and incorporate it into the aggregation process using the order-invariant function $AGG$ and non-linearity function $\Gamma$. 
We use a message function $\mathcal{M}(r)$ to convert the feature representation of a neighboring node $r$ to a message $m_r$. This message is sent to node $q$, which has its own feature representation $h_q$ and set of neighboring nodes $r_q$. We also generate a self-message from node $q$'s features and incorporate it into the aggregation process using an order-invariant function $AGG$ and non-linearity function $\Gamma$. This process generates the message for step $t+1$ which computed as: 

%We define a message function $\mathcal{M}(.)$ that transforms the feature representation of a neighbouring node $r$ to an information message $m_r$ sent to node $q$ with feature representation $h_q$ and set of neighbouring nodes $r_q; \forall r \in r_q$. Additionally, we compute a message from node $q$'s own features and self-feed it to retain its information. We aggregate the received messages with an order-invariant function $AGG$ and a non-linearity function $\Gamma$ to add expressiveness. The message at step $t+1$ is computed as
%
%For any node $ q $, of any type, that has its feature representation $ h_q $ and set of neighbouring nodes $ r_q $ with their feature representation $ h_r; \forall r \in r_q$, we define a message function $ \mathcal{M}(.)$ that transforms the feature representation of a node $ r $  to information message $ m_r $ and sends to a node $ q $; i.e., $ m_r=\mathcal{M}(h_r) $. To retain information of the node $ q $ itself, we also compute a message of its own features and self-feed it. All messages received by node $ q $ will be transformed with an order-invariant aggregating operation $ AGG $ and a non-linearity function $\Gamma$ to add expressiveness. So we compute the message at step $ t+1 $ as
\begin{equation}
\label{message}
h^{t+1}_q= \Gamma \bigg( \Big< \text{AGG} \big\{\mathcal{M}(h^{t}_r), \forall r \in r_q \big\}, \mathcal{M}(h^{t}_q) \Big> \bigg)
\end{equation}
The intuition behind this is that nodes in the graph seek a condensed representation of their neighbors' state, which is merged with their own state to form a new state.




%
%In the aggregation, a question is if all messages should be treated equally? can we use an attention layer to learn how important a message is for a parent node in the computation graph? I think adding the attention aggregation to focus on which constraint to focus on, i.e. adding the attention layer just before predicting the bid label. The intuitive goal here is to ask the question to a node whether it is important to focus on the flexibility constraints or the its mutual exclusive constraints. Even though those constraints are equally important but for some bids violating one constraint is easier than violating the other, and hence paying attention to that constraint is more important.
%
%After aggregation we apply a non-linear function to add expressiveness.
%\subsubsection{Convolution Operator }
%\subsubsection{Weight Kernels Construction}
%\subsubsection{Graph Clustering and Pooling} PCA? K-Means?
%\subsubsection{Transposed Convolution}


%\subsection{Pricing Scheme}

%We propose to develop a pricing scheme which enforces the budget balance and individual rationality properties, the goal is to prevent the market from deficiencies that requires subsidies, and the attractiveness to the participants as much as possible. 

\subsection{GNN Design} 
%\subsubsection{Stacking Layers} 
To update their states, nodes need to decide how many hops to use when aggregating messages from their neighbors to update their state. In a cyclic graph, this can lead to over-smoothing and saturation at the majority class. We avoid this issue by leveraging the unique connectivity of bids in our graph. Bids competing for the same flexibility interval are connected through representative flexibility nodes $\beta$, while bids from the same prosumer are linked through mutual exclusiveness nodes $\alpha$. This design ensures that two hops capture node dependencies and prevent over-smoothing.

%A crucial decision to make is how many hops a node should consider to aggregate messages from its surroundings to update its own state. Even though the intuition might be, the higher the depth the more informative update will result, we face a limitation in our particular graph. That is there are many cycles in the graph, and the deeper it gets the more cyclic behaviour we get. This leads to an over-smoothing problem and leads the nodes to saturate at the majority class in the set, e.g., all bids are set to be losing bids and end by having an empty allocation set.% $  \mathcal{A} = \emptyset $ .
%
%To avoid this, we recall the particularity of our graph in Figure \ref{redgraph}. That is the bids which are competing on the same flexibility interval are only connected through their representative flexibility nodes $\beta$. This includes both cases, whether the two bids are submitted by different or  the same prosumer. This ensures that the information (message) is propagated from one bid's node to the other competing bids' nodes through that common flexibility node. Moreover, if two bid nodes represent bids for the same prosumer, the messages are communicated through a mutual exclusiveness node $\alpha$. This demonstrates that two-hops depth is sufficient to capture node dependencies, hence we avoid the over-smoothing problem.

%\subsubsection{Learning Objective and Loss Function} 

We wrap our GNN model within a classifier function to classify nodes in the LFM-WDP problem. To address the targets imbalance, we introduced a weighting factor $\lambda$ in the binary cross-entropy loss $\ell$, which is the proportion of the minority class in the data. The loss function is defined as:

\begin{equation}
	\label{loss1}
	\ell = \frac{-1}{\kappa}\sum_{i=1}^{\kappa}\lambda_i \Big[ y_i \log(p(y_i)) + (1 - y_i)\log(1-p(y_i))\Big]
\end{equation}
where $\lambda_i$ is the weight of the class of node $i$, and $y_i$ is the true label of node $i$. This loss term calculates the difference between the bids allocations generated by the GNN-based model and those generated by the expert solver.

Additionally, we included the mean square error between expert solver optimal value $J$ and model optimal value $J^*$ calculated from predicted optimal allocations in the loss function to optimize the learning process towards expert solver optimal solutions. The weighting factor $\zeta$ balances the contribution of this term to the total loss. Then the total loss function be

\begin{equation}
	\label{loss}
	\mathcal{L} = \frac{1}{Q} \sum_{i=1}^{Q} \ell_i + \zeta . (J_i-J^*_i)^2
\end{equation}
where $\ell_i$ is the loss of the $i$-th problem instance and $J_i$ and $J_i^*$ are the expert solver optimal value and model optimal value, respectively, of the $i$-th problem instance. The total loss is minimized during training on $Q$ problem instances.

Furthermore, in order to ensure compliance with the XOR constraint, we assign the bid with the highest probability value from the classifier within the conflicting set if conflicts arise.


%
%
%We utilized a GNN model within a classifier function to classify nodes in the LFM-WDP problem. Our loss function addresses three concerns at two levels. Firstly, LFM-WDP graphs are imbalanced, favoring fewer bids over losing bids. We solve this by introducing a weighting factor $\eta$ in the binary cross-entropy loss $\ell$, which is the proportion of the minority class in the data as
%
%\begin{equation}\label{loss1}
%	\ell =   \frac{-1}{\kappa}\sum_{i=1}^  {\kappa}\eta_i \Big[ y_i  \log(p(y_i)) + (1 y_i)\log(1-p(y_i))\Big]
%\end{equation}
%where  $\eta_i$ is the weight of the class of node $ i $. This loss term calculates how far the bids allocations generated by the GNN-based model are from the ones generated by the expert solver. 
%The second concern is to optimize the learning process of  $ N $ LFM-WDPs towards expert solver optimal solutions. This is achieved by including the mean square error between expert solver optimal value  $ J $ and model optimal value $ J^* $ calculated from predicted optimal allocations as a second term in the loss function. The weighting factor $ \zeta $ is used to balance the contribution of this term to the total loss, which is minimized during training on $ Q $  problem instances is 
%
%\begin{equation}
%	\label{loss}
%	\mathcal{L} = \frac{1}{Q} \sum_{i=1}^{Q} \ell_i + \zeta . (J_i-J^*_i)^2
%\end{equation}
%
%Moreover, to enforce mutual exclusiveness (XOR) constraint compliance, we rank conflicting allocated bids by their associated classifier probability values and allocate the bid with the highest probability within the conflicting set.

%Since we approached the LFM-WDP as a node classification problem, we wrap a classifier function around our GNN model. To facilitate an effective learning process, we design our loss function considering three concerns at two levels. The first concern is at the individual problem instance level, i.e., for each LFM-WDP graph, where the bid nodes classes are hugely imbalanced, intuitively this means that the auctions allocation process mostly ends with allocating a minor number of bids compared to the losing bids. To prevent this imbalance to affect our model predictive accuracy, we incorporate a weighting factor $\eta$ which is the proportion of the number of the minority class in the data to the binary cross entropy loss $ \ell $ as




%
%\begin{equation}\label{loss1}
%\begin{split}
%\ell =   - \frac{1}{\kappa}\sum_{i=1}^  {\kappa}  	\eta_i \Big[&  y_i  \log(p(y_i)) + \\
%& (1- y_i)\log(1-p(y_i))\Big]
%\end{split}
%\end{equation}

%
%Our second concern is optimizing the learning process for $N$ LFM-WDPs to converge towards expert solver optimal solutions. We achieve this by adding a mean square error term between expert solver optimal value $J$ and model optimal value $J^*$, calculated from predicted optimal allocations, to our loss function. During training on $Q$ problem instances, we balance its contribution to the total loss using the weighting factor $\zeta$ as
%


%The second concern is at the dataset level, i.e. for $ N $ LFM-WDPs, the aim is to guide the learning process towards the expert solver optimal solutions. This is realized by adding the mean square error between the experts solver optimal value $ J $ and the model optimal value calculated from the predicted optimal allocations $ J^* $ as a second term in the loss function with a weighting factor $ \zeta $. So, the total loss that we aim to minimize during training on $ Q $ problem instances is 


%Lastly, to ensure the mutual exclusiveness (i.e., XOR) constraint compliance, we rank the conflicting allocated bids based on their classifier associated probability values and allocate the bid with the highest probability within the conflicting set. 





%
%\begin{equation}
%	\label{loss2}
%	\mathcal{L}_1=\sum_{u \in \mathcal{\text{X}}} -\log (\text{softmax}(z_u,y_u)) 
%\end{equation}
%
%\begin{equation}
%	\label{softmax}
%	\text{softmax}(z_u,y_u) =\sum_{i=1}^c  y_u[i]  \frac{ \exp [{z_u}^{T}{w_i}]}{\sum_{j=1}^c \exp [{z_u}^{T}{w_j}]} 
%\end{equation}
%
%
%\begin{equation}
%	\label{prob}
%	\text{max} (z_u \forall u \in x_i : )
%\end{equation}

%
%\section{LFM-WDP Instance Generation}
%On the dataset, PV peak production is on average 45\% of the reported capacity. We assume then the threshold of participating in the market is 10\% of the capacity of each house.
%
%1. To synthesize our instances, we must follow a specific distribution (e.g. decay distribution) \cite{sandholm2002algorithm}.
%
%2. Realistic dataset. \cite{ausgridds} 
%
%3. Scale.
%
%
%Thinking about WDP as a MKP, the current state of the art algorithms can be challenged computationally in two ways based on which algorithm we are targeting to wear out. Dynamic programming based algorithms can be challenged by introducing larger coefficients to the standard easy problems. This is due to the nature of the dynamic programming pseudo-polynomial complexity of $ \mathcal{O}(n \times C)$, where $ n $ and $ C $ are the number of items and capacity respectively. Dynamic programming algorithms can have nearly exponential complexity with larger C coefficient. Most, if not all, modern successful solvers are based on branch-and-bound algorithm those methods can be challenged by classes of problem instances for which most upper bounds perform badly. CPLEX running time increases with the number of bids and the maximum number of units as experimentally mentioned in \cite{lee2020fast}.
%
%Strongly correlated KP instance  are difficult due to the fact they are ill-conditioned in the sense that there is a large gap between the dual (continuous) problem solution and the primal (integer) solution of the problem. 
%
%We synthesize our instances such that LFM bids correspond to a real-life situation where the return is proportional to the investment plus some fixed charge for each bundle. Hence building a computationally challenging strong correlated instances.
%
%
%
%Following a common practice, we assume that the number of bids is larger than the number of flexibility intervals, i.e., $\kappa$ > $\sum u_i $. Without loss of generality, we set the ratio between the number of bids and the number of intervals as 100.



%\section{Model Bias, Fairness, Explainability}
%
%Fairness metrics?
%Features importance graph?
%

%\section{Notes and Questions}\label{QandA}
%It has been stated in \cite{lehmann2006winner} that one cannot hope for a general-purpose algorithm that can efficiently solve every instance of the WDP in the combinatorial auctions. This motivates the need for tailoring algorithms to deal with the particularities of each auction domain and LFM is has it own specificities.
%
%\begin{enumerate} \addtocounter{enumi}{-1}
%	\item In order to develop a neural framework for learning to solve WDP, first we need a suitable design pattern that would allow solving WDP via neural networks. To solve the optimization problem we perform two separate steps: (1) To model the optimization problem. (2) Use an efficient algorithm to solve it.
%	\item What is the best exact dynamic programming algorithm for the multi-dimension knapsack problem complexity in the literature?
%	\item What are the bounds, guarantees or worst-case scenarios obtained by NeuroCA. Machine learning is approximate, will that compromise the overall theoretical guarantees?
%	\item Dominant strategy can be proofed by \textit{case analysis}
%\end{enumerate}
%
%\subsection{GMP algorithms notes} 
%In particular, the basic procedure to solve CSPs via probabilistic inference is (GMP-guided sequential decimation):
%
%\begin{enumerate}[I] 
%	\item run a specific GMP algorithm on the factor graph until convergence.
%	\item based on the incoming fixed-point messages to each variable node, pick the variable with the highest certainty regarding a satisfying assignment
%	\item set the most 	certain variable to the corresponding value, simplify the factor graph if possible and repeat the entire process over and over until all variables are set
%\end{enumerate}
%
%\subsection{LFM-WDP solution by Dynamic Programming}
%
%Dynamic programming provides the solution for LFM-WDP in $ \mathcal{O}(  \prod_{j=1}^{m} (c_j+1)) $ (as a multidimensional knapsack problem as in \cite{freville2004multidimensional}).
%
%\subsection{ More Questions}
%\begin{itemize}
%
%	\item How to represent WDP optimization problem as a graph (bipartite graph)? factor graph? What represents nodes? variable nodes? factor nodes? constraints? edges? factor functions defined as?
%
%	\item How to represent a minimization problem as a graph? CSP is an assignment problem, can we represent the constraints of the WDP \textit{constraints} as \textit{boolean functions} to fit the CSP problem?
%	\item Review NeroSAT model paper
%	\item General Message Passing GMP, convergence to a fixed-point this relates to valuable information in the messages and hence solving CSP. What is \textit{fixed-point} in GMP?
%	\item What is the buyer utility function? can we learn it?
%	\item How to deal with continuous variables? Which BP algorithms are proposed for that? Can we quantize the continuous variables to preserve the BP efficiency?
%	\item Divide and concur methods are suitable for continuous variables but fails when the problem is non-convex.
%	\item What is the measure that we can optimize on the graph (e.g. factor graphs represents P(X) as a multiplication of potential functions, which represent constraints, or $\epsilon$ allowance constraints).
%	\item Representation Learning: Geometric Deep Learning: Graph Neural Networks (NeuroSAT framework, Circuit-SAT framework, Recurrent Relational Networks for Sudoku)
%	\item  Algorithms to check (Message Passing algorithms):
%		\begin{itemize}
%		\item Belief Propagation (BP) (aka Sum-Product algorithm)	is used to find the marginal distribution of each variable in the factor graph, i.e., $P(X_1), P(X_2),...$ The values of $X_1,X_2,...$ correspond to the modes of $P(X)$
%		\item Max-marginals: Max-Product, Min-Sum and and Warning Propagation  are used to find the solution of the actual optimization problem, $X_1, X_2,...$ that maximizes the optimization problem.
%		\item BP-guided decimation (General Message Passing Algo)
%		\item SP-guided decimation	based on the Survey Propagation (SP) algorithm (General Message Passing Algo)
%		\end{itemize}
%	\item A graph can be probabilistically represented as a normalized (by 1/Z) of a potential functions.
%	\item Kevin Murphy book
%	\item Combinatorial auctions optimization is NP-complete \cite{fujishima1999taming}, the question is are the  specific instances of it that we care about may be in an easier regime than the worst case?
%	\item Market power analysis in the case of using VCG payment rule, and see how the benefits change if there is an agent/bid with market power, means that getting a higher payment than others.
%\end{itemize}
%
%\subsection{Algorithm Exactness}
%Does the proposed method guarantees an optimal solution every time?
%
%
%\section{Belief Propagation Algorithm}
%
%Belief Propagation (BP) (aka Sum-Product algorithm)	is an iterative message-passing algorithm used to solve inference problems, optimization problems, and constraint satisfaction problems. 
%
%In our problem BP can be used to find the marginal distribution of each variable in the factor graph, i.e., $P(X_1), P(X_2),...$ The values of $X_1,X_2,...$ correspond to the modes of $P(X)$. 
%
%\textit{Factor graphs} can be seen as data structures among variety of other “graphical models” e.g. Bayesian networks [22], Markov random fields [23] and  normal factor graphs used to visualize and precisely define optimization problems. Factor graphs take a function $g(X)$ and represent it as a product of \textit{smaller} functions depends on a subset of $ X $. They are bipartite graphs, i.e., The objective function of a graph with M local cost functions can be represented as in (\ref{costeq2}):
%
%\begin{equation}
%	\label{costeq2}
%	C(X)= \sum_{a=1}^{M} C_a(X_a) 
%\end{equation}
%
%The graph should be supplemented by a lookup table or explicit function for each local cost (i.e., factor node) of its local variables.
%
%It is emphasized that factor graphs only give a principled way of \textit{representing} an optimization or probabilistic inference problem. We will then need to separately choose an algorithm to \textit{solve} it. The question here, can we represent the optimization problem using factor graphs and solve it using ML? Many different variants of message-passing algorithms exist, and of course there are many other optimization algorithms (e.g. simulated annealing) that can be used once the problem is represented as a factor graph.
%
%\subsection{BP questions}
%\begin{itemize}
%	\item How to soften hard constraints in local cost nodes.
%\end{itemize}


%{\color{blue}

\section{Numeric Evaluation}\label{sec6}
We quantitatively evaluate and analyze our approach using the solar home electricity dataset \cite{ausgridds}, which provides 30-minute which we resampled to 60-minute measurements of 300 homes' rooftop solar systems. We create LFM-WDP instances and their graph representations using the bottom-up generation procedure described in section \ref{instgen}, and use the Gurobi solver \cite{gurobi} as an expert to generate target solutions. We vary the number [100, 200, 300] of homes and bids per prosumer [1,2,3] to analyze model sensitivity, resulting in 18 cases in Figure \ref{fig1:a}. We produce a graph representation for each problem instance based on specific parameters that are the number of prosumers, the number of bids per prosumer, the prices, units of each bid and the DSO flexibility curve. We use the PyG library \cite{Fey2019} for graph generation, and the model consists of a 2-layer feed-forward network with ReLU activation for state update, and a 2-layer classifier appended to the GNN output. Each case is split into training and test sets and trained with a learning rate of 0.0001.

%To conduct our evaluation and analysis, we utilize the solar home electricity dataset \cite{ausgridds} which is a half-hour PV production measurement of 300-homes' rooftop solar systems. We use 30-minutes frequency as well as resampled 60-minutes' measurements. To rigorously evaluate our approach,  we define metrics that can quantitatively measure the accuracy of our results as well as showing visualizations of output examples as a qualitative measure.  

%\subsection{LFM-WDP Instances Generation and GNN Model}




%We generate LFM-WDP instances and their graph representations using the bottom-up generation procedure described in section \ref{instgen}. We use Gurobi mathematical optimization solver \cite{gurobi} as our expert solver and produce target solutions for each instance. To analyze model sensitivity, we form subsets of homes as prosumers in a local flexibility market, varying the number of homes and bids per prosumer. This results in 18 cases with different bid totals (Table \ref{resultstab}). For each problem instance, we produce a graph representation based on specific parameters such as prosumer and bid counts, prices, units, and DSO flexibility curve. We normalize only the node features, not the edge features, as it performs better. We use PyG library \cite{Fey2019} for graph generation. Model state update equation (\ref{message}) is a 2-layer feed-forward network with ReLU activation, and the classifier is 2-layers appended to the GNN output. Each case data is split into training and test sets, and trained with the loss function defined in (\ref{loss}) using a learning rate of 0.0001.

%We start by generating the LFM-WDP instances and their graph representations. We follow the bottom-up generation procedure that we introduced in section \ref{instgen} and produce the corresponding target solution using Gurobi mathematical optimization solver \cite{gurobi} as our expert solver that we aim to imitate. To analyse the sensitivity of our models, we formed subsets of homes to act as prosumers in a local flexibility market. The sets are of different number of homes that are [100, 200, 300] homes and we tested multiple cases of different number of bids per prosumer that are  [1, 2, 3]. These combinations led to the 18 cases with different total number of bids as shown in Table \ref{resultstab}. 


%An example of a DSO flexibility curve and 3 bids of a single prosumer are shown in Figure \ref{flexexp}. 

%The generated instances are then stored in the dataset format shown in Figure \ref{txtds} to be easily accessed by the graph generation step.

%For each generated problem instance, a graph representation is produced considering the particularities of each instance. These particularities are defined by the number of participating prosumers, the number of bids per prosumer, the prices and units of each bid and the DSO flexibility curve. We applied the normalization for the node features only while keeping the edge features in their raw values as this gave better performance than normalizing the edges' features. The graph generation step is implemented upon PyG library \cite{Fey2019}.
%
%The model state update equation (\ref{message}) is realized by a 2-layer feed-forward network with a ReLU activation function and the classifier is realised by appending 2-layers to the GNN output. Then each case data is split into training and test sets and trained with the loss function we defined earlier in (\ref{loss}) with learning rate of 0.0001.


%% Figure environment removed


%%
%%% Figure environment removed
%


%\subsection{Evaluation Metrics}
%
%
%In big single graphs, a way to split nodes to train-validation-test sets is by inductive setting which is breaking edges between the sets. In our case we are have already independent data-points (graphs), so we can split it the usual way. We have a well-define inductive setting.


%\subsection{Results Evaluation}
%We begin by qualitatively evaluating and visualizing the model solutions. We show, in Figure \ref{tsnesample}, solution samples of three different LFM-WDPs and how GNN model allocate bids to fit the DSO flexibility curve compared to the expert solver. In Figure \ref{samples}, the bars show the accumulation of the winning bids where each colour represents a different bid. The bids are fulfilling the flexibility curve in both ramp-ups and -downs. To visualize how the model segregates losing and winning bids' nodes in different clusters, Figure \ref{tsne} shows the output of the classifier layer embeddings' using tSNE \cite{hinton2002stochastic}. However, we can also observe a level of overlap which we commence next to quantify upon four different evaluation metrics. 



%% Figure environment removed
%%
%%% Figure environment removed



%% Figure environment removed


%
%
%
%\begin{table*}[htpb]
%    \centering
%	\caption{Model details}\label{resultstab}
%	\begin{tabular}{|p{22mm}|p{10mm}|p{10mm}|p{6mm}|p{4mm}|p{12mm}|p{14mm}|p{15mm}|p{18.7mm}|}
%
%	 \hline
%
% 
%	 \multirow{2}{*}{\centering intervals (frequency)}%{\centering intervals }
%	 & \multicolumn{1}{p{10mm}|}{\centering num. \\ prosumers}
%	 & \multicolumn{1}{p{10mm}|}{\centering bids per \\ prosumer }
%	 & \multicolumn{1}{p{6mm}|}{\centering total \\ bids  }
%	 & \multirow{2}{*}{F1}%{\centering F1}
%	 	 & \multirow{2}{*}{\centering NRMSE*}%{\centering F1} 
%%	 & \multicolumn{1}{p{12mm}|}{\centering NRMSE*}
%%	 & \multicolumn{1}{p{14mm}|}{\centering NRMSE }
%	 & \multirow{2}{*}{ \centering NRMSE}%{\centering F1} 
%	 & \multirow{2}{*}{\centering $\Delta$ NRMSE \%}%{\centering F1} 
%%	 & \multicolumn{1}{p{13mm}|}{\centering $\Delta$ NRMSE \%}
%%	 & \multicolumn{1}{p{7mm}|}{\centering $\Delta$ J \%}
%	 & \multirow{2}{*}{ \ \ $ \Delta \ J $  \% $\pm $ $ \sigma^2 $}%{\centering F1} 
%
%	\\ \hline 
%	\end{tabular}
%
%
%
%	\begin{tabular}{|p{22mm}|p{10mm}|p{10mm}|p{6mm}|p{4mm}|p{12mm}|p{14mm}|S[table-format=2.2,table-column-width=15mm]|S[table-format=3.2,table-column-width=8mm]@{\,\( \pm \)\,}S[table-format=2.2,table-column-width=8mm]|}
%
%	\multirow{9}{*}{ \ \ 24 (60-minutes)}  	& \multirow{3}{*}{ \ \  100} 	& \hfil 1  		&\centering  100 	& \centering 0.78 	&\centering 1.23 &\centering 1.16 &\centering  6.03 & 1.39  & \ 3.03	\\ \cline{3-10}
%										& 								& \centering 2  & \centering 200 	&\centering  0.68   &\centering 1.35 &\centering 1.25 &\centering  8.00 &  8.00  & 15.22 \\ \cline{3-10}
%										& 	  							& \centering 3  & \centering 300 	&\centering 0.75 	 	& \centering 0.55 &\centering 0.59 &\centering  -6.78 & - 1.40  & \ 6.43 \\ \cline{2-10}
%								   		& \multirow{3}{*}{ \ \  200} 	& \centering 1  & \centering 200 	& \centering 0.67 	  &\centering 0.85 &\centering 0.84 &\centering  1.19 &  5.78  & \ 9.23 \\ \cline{3-10}
%										&								& \centering 2  &\centering 400 	& \centering0.72 	  	& \centering 0.78 &\centering 0.71 &\centering  9.86 & 2.45  & 4.46 	\\ \cline{3-10}
%								   		& 								& \centering 3  &\centering 600 	& \centering0.65 	  	& \centering 0.61 &\centering 0.55 &\centering 10.91 &  2.20  & \ 3.24 \\ \cline{2-10}
%								   		& \multirow{3}{*}{ \ \ 300} 	& \centering 1  & \centering 300	& \centering0.79 	  & \centering 0.96 &\centering 0.89 &\centering 7.87  & - 6.87  & \ 9.54 \\ \cline{3-10}
%								   		& 								& \centering 2  &\centering 600 	& \centering0.79 	  	& \centering 0.77 &\centering 0.67 &\centering 14.93 &  9.70  & 12.65 	\\ \cline{3-10}
%								   		& 								& \centering 3  & \centering 900 	&\centering 0.85 	  	&\centering  0.55 &\centering 0.56 &\centering - 1.79  &  1.35  & \ 5.57    
%		\\ \hline \hline
%	\multirow{9}{*}{ \ \ 48 (30-minutes)}  	& \multirow{3}{*}{ \ \ 100}		& \centering 1  &\centering 100 	& \centering 0.78 	  	&\centering  0.83 &\centering 0.77 &\centering 7.79   & - 1.87  &\ 5.55 \\ \cline{3-10}
%										& 								& \centering 2  &\centering 200 	& \centering 0.90   	& \centering 0.49 &\centering 0.54 &\centering - 9.26   & - 2.77  & \ 6.58	\\ \cline{3-10}
%										&								& \centering 3  &\centering 300 	& \centering0.60  	& \centering  0.40 &\centering 0.39 &\centering 2.56   & 1.61  & \ 7.58 \\ \cline{2-10}
%										& \multirow{3}{*}{ \ \ 200} 	&\centering  1  &\centering 200 	&\centering 0.66   	& \centering 0.59 &\centering 0.61 &\centering -3.28   & 10.47 & 15.32 	\\ \cline{3-10}
%										& 								&\centering  2  &\centering 400 	&\centering 0.76 	  	& \centering 0.46 &\centering 0.41 &\centering 12.20  & 4.57  & \ 9.55 \\ \cline{3-10}
%										& 								& \centering 3  &\centering  600 	&\centering 0.80   	& \centering 0.33 &\centering 0.31 &\centering 6.45   & 7.16  & 13.32 	\\ \cline{2-10}
%										& \multirow{3}{*}{ \ \ 300} 	& \centering 1  & \centering 300 	&\centering  0.92   	& \centering 0.56 &\centering 0.58 &\centering -3.45   & 8.74  & 16.56 	\\ \cline{3-10}
%										& 								& \centering 2  & \centering 600 	& \centering 0.78   	&\centering  0.81 &\centering 0.77 &\centering 5.19   & 3.80  &13.15 	\\ \cline{3-10}
%										& 								& \centering 3  & \centering 900 	& \centering  0.83 		& \centering 0.97 &\centering 1.01 &\centering -3.96   & 7.23  & 10.15         
%										\\ \hline  \hline
%			\end{tabular}
%		\begin{tabular}{|p{22mm}|p{10mm}|p{10mm}|p{6mm}|p{4mm}|p{12mm}|p{14mm}|p{15mm}|p{18.7mm}|}
%	Average								& \centering -	& \centering - & \centering - & \centering 0.76  & \centering 0.73 & \centering 0.70 & \centering  6.75   & \hfil  4.52  
%	\\ \hline
%	
%	Standard Deviation						& \centering -	&\centering - & \centering - & \centering 0.09   &\centering 0.28 & \centering 0.26 & \centering 3.76   & \hfil 3.52 
%		\\ \hline
%	\end{tabular}
%
%\end{table*}
% 
% 

%%---------------------------------------------------------------------------------------------------
%\begin{table*}[htpb]
%\centering
%\caption{Numerical Results}\label{resultstab}
%\begin{tabular}{|p{22mm}|p{11mm}|p{20mm}|p{10mm}|p{4mm}|p{15mm}|p{18.7mm}|}
%
%\hline
%
%\multirow{2}{*}{\centering intervals (frequency)}%{\centering intervals }
%%		& \multicolumn{1}{p{10mm}|}{\centering num. \\ prosumers}
%& \multirow{2}{*}{\centering prosumers}%{\centering F1} 
%& \multirow{2}{*}{\centering bids per prosumer}%{\centering F1} 
%%		& \multicolumn{1}{p{10mm}|}{\centering bids per \\ prosumer }
%& \multirow{2}{*}{total bids}%{\centering F1}
%%		& \multicolumn{1}{p{6mm}|}{\centering total \\ bids  }
%
%& \multirow{2}{*}{F1}%{\centering F1}
%%	 & \multicolumn{1}{p{12mm}|}{\centering NRMSE*}
%%	 & \multicolumn{1}{p{14mm}|}{\centering NRMSE }
%& \multirow{2}{*}{\centering $\Delta$ NRMSD \%}%{\centering F1} 
%%	 & \multicolumn{1}{p{13mm}|}{\centering $\Delta$ NRMSE \%}
%%	 & \multicolumn{1}{p{7mm}|}{\centering $\Delta$ J \%}
%& \multirow{2}{*}{ \ \ $ \Delta \ J $  \% $\pm $ $ \sigma^2 $}%{\centering F1} 
%
%\\ \hline 
%\end{tabular}
%
%
%
%\begin{tabular}{|p{22mm}|p{11mm}|p{20mm}|p{10mm}|p{4mm}|S[table-format=2.2,table-column-width=15mm]|S[table-format=3.2,table-column-width=8mm]@{\,\( \pm \)\,}S[table-format=2.2,table-column-width=8mm]|}
%
%\multirow{9}{*}{ \ \ 24 (60-minutes)}  	& \multirow{3}{*}{ \ \  100} 	& \hfil 1  		&\centering  100 	& \centering 0.78  &\centering  6.03 & 1.39  & \ 3.03	\\ \cline{3-8}
%& 								& \centering 2  & \centering 200 	&\centering  0.68  &\centering  8.00 &  8.00  & 15.22 \\ \cline{3-8}
%& 	  							& \centering 3  & \centering 300 	&\centering 0.75   &\centering  -6.78 & - 1.40  & \ 6.43 \\ \cline{2-8}
%& \multirow{3}{*}{ \ \  200} 	& \centering 1  & \centering 200 	& \centering 0.67 	  &\centering  1.19 &  5.78  & \ 9.23 \\ \cline{3-8}
%&								& \centering 2  &\centering 400 	& \centering0.72 	  	 &\centering  9.86 & 2.45  & 4.46 	\\ \cline{3-8}
%& 								& \centering 3  &\centering 600 	& \centering0.65 	  	 &\centering 10.91 &  2.20  & \ 3.24 \\ \cline{2-8}
%& \multirow{3}{*}{ \ \ 300} 	& \centering 1  & \centering 300	& \centering0.79 	  &\centering 7.87  & - 6.87  & \ 9.54 \\ \cline{3-8}
%& 								& \centering 2  &\centering 600 	& \centering0.79 	  	 &\centering 14.93 &  9.70  & 12.65 	\\ \cline{3-8}
%& 								& \centering 3  & \centering 900 	&\centering 0.85 	  	 &\centering - 1.79  &  1.35  & \ 5.57    
%\\ \hline \hline
%\multirow{9}{*}{ \ \ 48 (30-minutes)}  	& \multirow{3}{*}{ \ \ 100}		& \centering 1  &\centering 100 	& \centering 0.78 	  	&\centering 7.79   & - 1.87  &\ 5.55 \\ \cline{3-8}
%& 								& \centering 2  &\centering 200 	& \centering 0.90   	 &\centering - 9.26   & - 2.77  & \ 6.58	\\ \cline{3-8}
%&								& \centering 3  &\centering 300 	& \centering0.60  	&\centering 2.56   & 1.61  & \ 7.58 \\ \cline{2-8}
%& \multirow{3}{*}{ \ \ 200} 	&\centering  1  &\centering 200 	&\centering 0.66   	 &\centering -3.28   & 10.47 & 15.32 	\\ \cline{3-8}
%& 								&\centering  2  &\centering 400 	&\centering 0.76 	  &\centering 12.20  & 4.57  & \ 9.55 \\ \cline{3-8}
%& 								& \centering 3  &\centering  600 	&\centering 0.80   	 &\centering 6.45   & 7.16  & 13.32 	\\ \cline{2-8}
%& \multirow{3}{*}{ \ \ 300} 	& \centering 1  & \centering 300 	&\centering  0.92    &\centering -3.45   & 8.74  & 16.56 	\\ \cline{3-8}
%& 								& \centering 2  & \centering 600 	& \centering 0.78   	 &\centering 5.19   & 3.80  &13.15 	\\ \cline{3-8}
%& 								& \centering 3  & \centering 900 	& \centering  0.83 		 &\centering -3.96   & 7.23  & 10.15         
%\\ \hline  \hline
%\end{tabular}
%\begin{tabular}{|p{22mm}|p{11mm}|p{20mm}|p{10mm}|p{4mm}|p{15mm}|p{18.7mm}|}
%Average								& \centering -	& \centering - & \centering - & \centering 0.76  &  \centering  6.75 \%   & \hfil  4.52 \%  
%\\ \hline
%
%Standard Deviation						& \centering -	&\centering - & \centering - & \centering 0.09  & \centering 3.76   & \hfil 3.52 
%\\ \hline
%\end{tabular}
%
%\end{table*}
%
%%---------------------------------------------------------------------------------------------------


To ensure a thorough evaluation, we have developed four evaluation metrics. Firstly, we evaluate our approach's ability to produce optimal allocations that match those produced by the expert solver. Due to imbalanced data, we utilize the macro F1-score metric, which assesses the metric for each class independently and calculates the average and we compare it with a reference feed-forward neural network (FNN) base model as in Figures \ref{fig1:a}. The GNN achieved 0.76 F1 average score. Secondly, we calculate the deviation percentage of the GNN's optimal value from that of the expert solver's reference optimal value, denoted as $ \Delta J $. Thirdly, while the F1-score and $ \Delta J $ quantify the GNN's closeness to the expert solver's solution, we moreover evaluate how well the allocations fulfill the DSO request $\mathcal{F}$. For this, we introduce the normalized root mean square difference (NRMSD), measured as


%For rigorous quantitative evaluation, we develop four evaluation metrics.
%Firstly, we assess the ability of our approach to produce optimal allocations similar to the expert solver's. Due to imbalanced data, accuracy is not a suitable metric. Instead, we use macro F1-score, which calculates the metric for each class independently and takes the average. We also compare the GNN F1 with a reference feed forward neural network (FNN). Secondly, we calculate the deviation percentage of the GNN optimal value from the expert solver reference optimal value. We denote that as $ \Delta J $. The  F1-score and $ \Delta J $, only quantify how close are the GNN model's solution from the expert solver solution. Since it is essential to quantify how the allocations meet the DSO flexibility request $\mathcal{F}$, we define the normalized root mean square difference (NRMSD) as our third metric as 

% Figure environment removed

\begin{equation}\label{nrmseexpert}
	\text{NRMSD \ } = \frac{1}{T} \sum_{t=1}^{T}  \frac{ \sum_{k=1}^{\kappa} \mathcal{A}_k - \mathcal{F} }{\mathcal{F}}
\end{equation}
we  then calculate the percentage difference between the GNN and the expert solver NRMSD values and denote it as $\Delta \text{ NMRSD}$. As in Figure \ref{fig1:b}, the average $\Delta \text{NMRSD}$ is 6.75\%, which indicates that the model allocations exceed the DSO flexibility request by an average of 6.75\%. This results leads to an average $\Delta J$ of 4.52\%. 

Lastly, as this work aims to use machine learning to approximate the bids' allocation process, reducing the complexity of the LFM-WDP computation. We found that the GNN model is 98\% more computationally efficient than our expert solver. However, there is an off-line training time overhead averaging 30 minutes. Figures \ref{fig2:a} and \ref{fig2:b} demonstrate the exponential relationship between the expert solver computation time and the number of bids, with an exponential increase on the y-axis scale.

%It is the harmonic mean of macro precision and macro recall as
%
%%First, we evaluate to what extent our proposed approach is able to produce the same optimal allocations that the expert solver allocated. Due to the unbalance bids allocations data, the accuracy is not a proper metric. To avoid the domination of the majority class, we calculate the macro F1-score which compute the metric independently for each class and then take the average and it is the harmonic mean of the macro precision and macro recall as
%
%\begin{equation}\label{Precision}
%\text{Precision} = \frac{1}{C} \sum_{c=1}^{C} \frac{TP_c}{TP_c + FP_c}
%\end{equation}
%
%\begin{equation}\label{recall}
%\text{Recall} = \frac{1}{C} \sum_{c=1}^{C} \frac{TP_c}{TP_c + FN_c}
%\end{equation}
%
%\begin{equation}\label{F1}
%\text{F1} = 2 \times   \frac{ \text{Precision} \times \text{Recall}}{ \text{Precision} + \text{Recall}}
%\end{equation}
%where $ TP $ is the count of the true positives, $ FP $ is the count of false positives $ FN $ is the count of false negatives and $ C $ is the number of classes which is 2 in our case. Our approach achieves an average allocation match of 76\% with the expert solver allocations, as shown in Table \ref{resultstab}. 
%This is also reflected in the tSNE plots in Figure \ref{tsne}, where there is an overlap between the allocated bids (red dots) and the losing bids (blue dots) clusters, indicating some level of confusion that can be quantified as 24\% on average.

%As in Table \ref{resultstab}, our approach allocations match on average 76\% of the expert solver allocations. This can also be visualised in the tSNE plots in Figure \ref{tsne}, as there is an overlap between the allocated bids (i.e. red dots) and the losing bids (i.e. blue dots) clusters which reflects some level of confusion that can be quantified as 24\% on average.


%Through training epochs, we noticed that the models oscillates between either assigning most of the nodes to the negative class (not allocating the bids) or positive class (allocating all the bids). This is a result of the similarity between the nodes computation graphs.

%
%
%\subsubsection{Optimality Deviation}
%Second, we calculate the deviation percentage of the GNN optimal value from the expert solver reference optimal value. We denote that as $ \Delta J $
%\begin{equation}\label{deltaoptimal}
%\Delta J  = \frac{ J^*-J}{J}\times 100 \%
%\end{equation}
%where $ J $ is the expert solver optimal value and $ J^* $ is the optimal value calculated from the model predicted allocations. 


%In Table \ref{resultstab}, the positive $ \Delta J$ values indicate that our approach ended up with higher optimal values than the expert solver and the negative values indicate the opposite. We can also notice the relatively high standard deviation ($\sigma^2$) compared to the mean of each case. This indicates an uncertainty that is caused by the probabilistic nature of machine learning based models. The average value of $ \Delta J $ of all experiments is 4.52\%. 

%\subsubsection{Normalized Root Mean Square Difference} 
%The previously discussed metrics, F1-score and $ \Delta J $, only quantify how close are the GNN model's solution from the expert solver solution. Moreover, since it is essential to quantify how the allocations meet the DSO flexibility request $\mathcal{F}$, we define the normalized root mean square difference (NRMSD) metric for our GNN model and the expert solver, respectively, as 
%%\begin{equation}\label{nrmsegnn}
%%\text{NRMSD}^* =  \frac{1}{T} \sum_{t=1}^{T}  \frac{ \sum_{k=1}^{\kappa} \mathcal{A^*}_k - \mathcal{F} }{\mathcal{F}}
%%\end{equation}
%
%
%\begin{equation}\label{nrmseexpert}
%\text{NRMSD \ } = \frac{1}{T} \sum_{t=1}^{T}  \frac{ \sum_{k=1}^{\kappa} \mathcal{A}_k - \mathcal{F} }{\mathcal{F}}
%\end{equation}

%as 
%
%\begin{equation}\label{nrmseperscent}
%\Delta \text{ NMRSD}  = \frac{ \text{NMRSD}^* - \text{MMRSD}}{\text{NMRSD}} \times 100 \%
%\end{equation}


%
%% Figure environment removed
%
%
%We  then calculate the percentage difference between two values and denote it as $\Delta \text{ NMRSD}$. The average $\Delta \text{NMRSD}$ in Table \ref{fig1:b} is 6.75\%, which indicates that the model allocations exceed the DSO flexibility request by an average of 6.75\%. This results leads to an average $\Delta J$ of 4.52\%.
%
%The average value of $\Delta \text{NMRSD}$ in Table \ref{fig1:b} is 6.75\%, indicating that the model allocations exceed the DSO flexibility request by an average of 6.75\%, while still fulfilling the request. This excess in allocations leads to an average $\Delta J$ of 4.52\%, as observed in our results of optimality deviation.

%In Table \ref{resultstab}, the average value of $ \Delta \text{ NMRSD} $ is 6.75\%.  The intuitive interpretation is that the allocations produced by the model fulfil the DSO flexibility request but exceeding the expert solver by 6.75\% on average. For instance if the expert solver managed to exactly fulfil the DSO flexibility request, then our model exceeds the request by 6.75\%. Recalling our results of optimality deviation, we notice that this excess in allocations leads to an average $ \Delta J$ of 4.52\%.

%\subsubsection{Time Complexity Evaluation}




%
%
%Moreover, as we emphasized earlier that this work aims to use machine learning to approximate the bids' allocation process, reducing the complexity of the LFM-WDP computation. We compared the GNN model inference times with the computation time of our expert solver, Gurobi, and found that the GNN model is 98\% more computationally efficient. However, there is an off-line training time overhead averaging 30 minutes.  Figures \ref{fig2:a} and \ref{fig2:b} demonstrate the exponential relationship between the expert solver computation time and the number of bids, with an exponential increase of 2 on the y-axis scale when the number of intervals doubles from 24 to 48.


%
% As discussed in section \ref{complex}, LFM-WDP computation complexity is exponential in the number of time intervals and flexibility units per interval. To support this, we compared the GNN model inference times with the computation time of our chosen expert solver, Gurobi \cite{gurobi}. Figure \ref{experttime} demonstrates the exponential relationship between the expert solver computation time and the number of bids, with an exponential increase of 2 on the y-axis scale when the number of intervals doubles from 24 to 48. On the other hand, the GNN model inference time is 98\% computationally more efficient than the expert solver, as shown in the right plot of Figure \ref{experttime}. However, there is an off-line training time as an overhead, which averages 30 minutes.

%As we mentioned earlier, a major motivation for this work is to approximate the bids' allocation process by utilizing machine learning. This diminishes the LFM-WDP computation complexity by fracturing it into an off-line learning phase and a fast inference phase using the trained models. As we discussed earlier, in section \ref{complex}, the LFM-WDP computation complexity is exponential in the number of time intervals and flexibility units per each interval. 
%
%To support this, we examine the GNN model inference times compared with the computation time of our chosen expert solver, Gurobi \cite{gurobi}. We run the experiments on an i5-7200U CPU @ 2.50GHz. The left plot of Figure \ref{experttime} shows the exponential relationship between the expert solver computation time and the number of bids on the x-axis. Note that the number of bids is proportional to the number of items per interval. We can also observe an exponential increase on the y-axis scale, by a factor of 2, when the number of intervals increase linearly by factor of 2 from 24 to 48 intervals. On the right plot of Figure \ref{experttime}, we see the GNN model inference time is 98\% computationally more efficient on average than the expert solver. However, there is an off-line training time as an overhead which is 30 minutes on average.


%% Figure environment removed






\section{Conclusion}\label{sec7}



This paper proposes a new combinatorial auction framework for local energy flexibility markets leveraging graph neural networks to address the winner determination problem. Through experimentation on various problem complexities, the proposed models demonstrate a linear time complexity compared to the expert solver's exponential time complexity. The results also indicate that, on average, the proposed models achieve a 76\% match with expert solver allocations, while maintaining an average deviation of less than 5\% from the optimal value. Furthermore, the average deviation in meeting the flexibility needs of the system operator using the proposed models is found to be below 7\% compared to the expert solver.


%This paper presents a novel combinatorial auction framework and proposes new graph neural network-based models to address the winner determination problem in local energy flexibility markets. Through experimentation on diverse problem sets, the models are found to possess a time complexity that is linear in comparison to the expert solver's exponential time complexity. On average, the models match up to 76\% of expert solver allocations, and their average deviation from the optimal value is less than 5\%. Moreover, the models' average deviation in meeting the DSO flexibility requirement is less than 7\% when compared to the expert solver.

%Proposing a combinatorial auction framework, this paper uses graph neural network-based models to solve the winner determination problem in local energy flexibility markets. The models were tested on problems with varying complexities and found to have a linear time complexity compared to the expert solver's exponential time complexity. While the models match up with 76\% of expert solver allocations on average, the average deviation on the optimal value is below 5\%. The models' deviation average for fulfilling the DSO flexibility need compared to the expert solver is below 7\%.

%To utilize energy flexibility provided by prosumers' distributed energy resources effectively at the power system level, it is essential to design efficient local energy flexibility markets. Flexibility aggregators have a central role in accumulating prosumers' small flexible resources to meet the market minimum bid thresholds. Combinatorial auctions are suitable for establishing efficient local flexibility markets by allowing prosumers to bundle multiple items and submit multiple bids. Winning determination problem is an NP-complete optimization problem that determines the optional allocations.
%
%This paper proposes a combinatorial auction framework for local energy flexibility markets. A major part of this work is dedicated to design graph neural network-based models to imitate an expert solver in solving the winner determination problem. A bottom-up procedure to generate problem instances is proposed aiming to emulate the real world problems. To naturally consume the data by the GNN models, a heterogeneous tri-partite graph representation is developed to allow capturing all nodes' dependencies in the problem. 
%
%The models were extensively tested on problems with different complexities and evaluated on different metrics. Results showed a linear time complexity of the models' inference compared to the expert' solver exponential time complexity. Even though the models' predicted allocations that match up with 76\% of the expert solver allocations on average, the average deviation on the optimal value is only below 5\%. We also compared how the models fulfil the flexibility need of the DSO compared to the expert solver, the models deviation average is below 7\%.
%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.

%\section*{References}
%
%
% 
%
%For papers published in translation journals, please give the English 
 

\bibliographystyle{IEEEtran}

\bibliography{LFM_proposal3}



%\begin{thebibliography}{00}
%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
%\end{thebibliography}
%\vspace{12pt}
%\color{red}
%IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
