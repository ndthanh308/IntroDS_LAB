%%
%% Copyright 2022 OXFORD UNIVERSITY PRESS
%%
%% This file is part of the 'oup-authoring-template Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'oup-authoring-template Bundle' is
%% given in the file `manifest.txt'.
%%
%% Template article for OXFORD UNIVERSITY PRESS's document class `oup-authoring-template'
%% with bibliographic references
%%

%%%CONTEMPORARY%%%
\documentclass[unnumsec,webpdf,contemporary,large]{oup-authoring-template}%
%\documentclass[unnumsec,webpdf,contemporary,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,contemporary,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,contemporary,small]{oup-authoring-template}

%%%MODERN%%%
%\documentclass[unnumsec,webpdf,modern,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,webpdf,modern,medium]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,modern,small]{oup-authoring-template}

%%%TRADITIONAL%%%
%\documentclass[unnumsec,webpdf,traditional,large]{oup-authoring-template}
%\documentclass[unnumsec,webpdf,traditional,large,namedate]{oup-authoring-template}% uncomment this line for author year citations and comment the above
%\documentclass[unnumsec,namedate,webpdf,traditional,medium]{oup-authoring-template}
%\documentclass[namedate,webpdf,traditional,small]{oup-authoring-template}

%\onecolumn % for one column layouts

%\usepackage{showframe}

\graphicspath{{Fig/}}

% line numbers
%\usepackage[mathlines, switch]{lineno}
%\usepackage[right]{lineno}

\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}%
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.
\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%
\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}
\usepackage{amsmath,amsmath,amssymb}
\usepackage{mathtools}
\usepackage{subfiles}
\DeclareMathOperator*{\argmax}{arg\,max}
\begin{document}

\journaltitle{Briefings in Bioinformatics}
\DOI{DOI HERE}
\copyrightyear{2023}
\pubyear{2023}
\access{Advance Access Publication Date: Day Month Year}
\appnotes{Paper}

\firstpage{1}

%\subtitle{Subject Section}

\title[Artificial intelligence-aided protein engineering]{Artificial intelligence-aided protein engineering: from topological data analysis to deep protein language models}

\author[1]{Yuchi Qiu}
\author[1,2,3,$\ast$]{Guo-Wei Wei}

%\authormark{Author Name et al.}

\address[1]{\orgdiv{Department of Mathematics}, \orgname{Michigan State University}, \orgaddress{\street{East Lansing}, \postcode{48824}, \state{MI}, \country{USA}}}
\address[2]{\orgdiv{Department of Biochemistry and Molecular Biology}, \orgname{Michigan State University}, \orgaddress{\street{East Lansing}, \postcode{48824}, \state{MI}, \country{USA}}}
\address[3]{\orgdiv{Department of Electrical and Computer Engineering}, \orgname{Michigan State University}, \orgaddress{\street{East Lansing}, \postcode{48824}, \state{MI}, \country{USA}}}



\corresp[$\ast$]{Corresponding author. \href{weig@msu.edu}{weig@msu.edu}}

\received{Date}{0}{Year}
\revised{Date}{0}{Year}
\accepted{Date}{0}{Year}

%\editor{Associate Editor: Name}

%\abstract{
%\textbf{Motivation:} .\\
%\textbf{Results:} .\\
%\textbf{Availability:} .\\
%\textbf{Contact:} \href{name@email.com}{name@email.com}\\
%\textbf{Supplementary information:} Supplementary data are available at \textit{Journal Name}
%online.}

\abstract{Protein engineering is an emerging field in biotechnology that has the potential to revolutionize various areas, such as antibody design, drug discovery, food security, ecology, and more. However, the mutational space involved is too vast to be handled through experimental means alone. Leveraging accumulative protein databases, machine learning (ML) models, particularly those based on natural language processing (NLP), have considerably expedited protein engineering. Moreover, advances in topological data analysis (TDA) and artificial intelligence-based protein structure prediction, such as AlphaFold2, have made more powerful structure-based ML-assisted protein engineering strategies possible. This review aims to offer a comprehensive, systematic, and indispensable set of methodological components, including TDA and NLP, for protein engineering and to facilitate their future development.}
\keywords{Topological data analysis; Protein language models; Protein engineering; Deep learning and machine learning}

% \boxedtext{
% \begin{itemize}
% \item Key boxed text here.
% \item Key boxed text here.
% \item Key boxed text here.
% \end{itemize}}

\maketitle
\section{Key points}
\begin{itemize}
\item Machine learning and deep learning techniques are revolutionizing protein engineering.
\item Topological data analysis enables advanced structure-based machine learning-assisted protein engineering approaches.
\item Deep protein language models extract critical evolutionary information from large-scale sequence databases.
\end{itemize}
\section{Introduction}
Protein engineering aims to design and discover proteins with desirable functions, such as improving the phenotype of living organisms, enhancing enzyme catalysis, and boosting antibody efficacy \cite{narayanan2021machine}. It has tremendous impacts on drug discovery, enzyme development and applications, the development of biosensors, diagnostics, and other biotechnology, as well as understanding the fundamental principles of the protein structure-function relationship and achieving environmental sustainability and diversity. Protein engineering has the potential to continue to drive innovation and improve our lives in the future.

Two traditional protein engineering approaches include directed evolution \cite{arnold1998design} and rational design \cite{karplus2005molecular, boyken2016novo}. Directed evolution is a process used to create proteins or enzymes with improved or novel functions \cite{romero2009exploring}. The method involves introducing mutations into the genetic code of a target protein and screening the resulting variants for improved function. The process is "directed" because it is guided by the desired outcome, such as increased activity, stability, specificity, binding affinity, and fitness. Rational design involves using knowledge of protein structure and function to engineer desirable specific changes to the protein sequence and/or structure \cite{boyken2016novo, bhardwaj2016accurate}. Both approaches resort to experimental screening of astronomically large mutational space, i.e., $20^N$ for protein of $N$ amino acid residues, which is expensive, time-consuming, and intractable \cite{pierce2002protein}. As a result, only a small fraction of the mutational space can be explored experimentally even with the most advanced high-throughput screening technology.

Recently, data-driven machine learning has emerged as a new approach for directed evolution and protein engineering \cite{siedhoff2020machine, mazurenko2019machine}. Machine learning-assisted protein engineering (MLPE) refers to the use of machine learning models and techniques to improve the efficiency and effectiveness of protein engineering. MLPE not only reduces the cost and expedites the process of protein engineering, but also optimizes the screening and selection of protein variants \cite{diaz2023using}, leading to the higher efficiency and productivity. Specifically, by using machine learning to analyze and predict the effects of mutations on protein function, researchers can rapidly generate and test large numbers of variants, which establish the protein-to-fitness map (i.e., fitness landscape) from sparsely sampled experimental data \cite{wittmann2021advances, yang2019machine}. This approach accelerates the process of protein engineering.

The process of data-driven MLPE typically involves several elements, including data collection and preprocessing, model design, feature extraction and selection, algorithm selection and design, model training and validation, experimental validation, and iterative model optimization. Driven by technological advancements in high-throughput sequencing and screening technologies, there has been a substantial accumulation of general-purpose experimental datasets on protein sequences, structures, and functions \cite{berman2000protein, uniprot2021uniprot}. These datasets, along with numerous protein-engineering specific deep mutational scanning (DMS) libraries \cite{notin2022tranception}, provide valuable resources for machine learning training and validation.    

Data representation and feature extraction are crucial steps in the design of machine learning models, as they help to reduce the complexity of biological data and enable more effective model training and prediction. There are several typical types of feature embedding methods, including sequence-based, structure-based \cite{cang2018integration, wang2020topology}, physics-based \cite{schymkowitz2005foldx, leman2020macromolecular}, and hybrid methods \cite{qiu2023persistent}. Among them, sequence-based embeddings have been dominant due to the success of various natural language processing (NLP) methods such as long short-term memory (LSTM) \cite{alley2019unified}, autoencoders \cite{riesselman2018deep}, and Transformers \cite{rives2021biological}, which allow unsupervised pre-training on large-scale sequence data. Structure-based embeddings take advantage of existing protein three-dimensional (3D) structures in the Protein Data Bank (PDB) \cite{berman2000protein} and advanced structure predictions such as AlphaFold2 \cite{jumper2021highly}. These methods further exploit advanced mathematical tools, such as topological data analysis (TDA) \cite{edelsbrunner2010computational, zomorodian2005computing}, differential geometry \cite{nguyen2019dg,wee2021ollivier}, or graph approaches \cite{nguyen2019agl}. Physics-based methods utilize physical models, such as density functional theory \cite{ryczko2019deep}, molecular mechanics \cite{butler2018machine}, Poisson-Boltzmann model \cite{chen2021mlimc}, etc. While these methods are highly interpretable, their performance often depends on model parametrization. Hybrid methods may select a combination of two or more types of features. 


The designs and selections of MLPE algorithms depend on the availability of data and efficiency of experiments. In real-world scenarios, where smaller training datasets are prevalent, simpler machine learning algorithms such as support vector machines and ensemble methods are often employed for small training datasets, which is often the case in real scenarios. In contrast, deep neural networks  are more suitable for larger training datasets. Although regression tasks are typically used to distinguish one set of mutations from another \cite{siedhoff2020machine}, unsupervised zero-shot learning methods can also be utilized to address scenarios with limited data availability \cite{hsu2022learning, wittmann2021informed}. The iterative interplay between experiments and models is another crucial component in MLPE by iteratively screening new data to refine the models. Consequently, the selection of an appropriate MLPE model is influenced by factors like experimental frequency and throughput. This iterative refinement process enables MLPE to deliver optimized protein engineering outcomes.

MLPE has the potential to significantly accelerate the development of new and improved proteins, revolutionizing numerous areas of science and technology (\autoref{fig2}). 
Despite considerable advances in MLPE, challenges remain in many aspects, such as data preprocessing, feature extraction, integration with advanced algorithms, and iterative optimization through experimental validation. This review examines published works and offers insights into these technical advances. We place particular emphasis on the advanced mathematical TDA approaches, aiming to make them accessible to general readers. Furthermore, we review current advanced NLP-based models and efficient MLPE approaches. Last, we discuss potential future directions in the field.

% Figure environment removed 
\section{Sequence-based deep protein language models}
\label{sec_PLM}

In artificial intelligence, natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally \cite{khurana2023natural}. NLP covers a wide range of tasks, including language translation, sentiment analysis, chatbot development, speech recognition, and information extraction,  among others. The development and advancement of various machine learning models have been instrumental in tackling the complex challenges posed by NLP tasks.

Similar to human language, the primary structure of a protein is also represented by a string of amino acids, with 20 canonical amino acids. The analogy between protein sequences and human languages has inspired the development of computational methods for analyzing and understanding proteins using models adopted from NLP (\autoref{fig2}a). The self-supervised sequence-based protein language models have been applied to study the underlying patterns and relationships within protein sequences, predict their structural and functional properties, and facilitate protein engineering. These language models are pretrained on a given data allowing to model protein properties for each given protein. There are two major types of protein language models utilizing different resources of protein data \cite{hsu2022learning} (\autoref{table_seq}). The first one is the local evolutionary models which focus on homologs of the target protein such as multiple sequence alignments (MSAs) to learn the evolutionary information from the mostly related mutations. The second one is the global evolutionary models which learn from large protein sequence databases such as UniProt \cite{uniprot2021uniprot} and Pfam \cite{el2019pfam}. 

%\include{table_seq.tex}
{\small 
\begin{table*}[h]
\centering
\begin{tabular}{|p{2.6cm}|p{2.5cm}|p{1.2cm}|p{0.7cm}|p{1cm}|p{2.2cm}|p{2cm}|p{1.4cm}|}
\hline
\multirow{2}{*}{\textbf{Model} }        & \multirow{2}{*}{\textbf{Architecture}}& \multirow{2}{*}{\textbf{Max len}}& \multirow{2}{*}{\textbf{Dim}}& \multirow{2}{*}{\textbf{\# para}} &\multicolumn{2}{|c|}{\textbf{Pretrained data}}& \multirow{2}{*}{Time$^1$}\\ 
\cline{6-7}
& &&&& \textbf{Source} &\textbf{Size}&      \\ \hline
\multicolumn{8}{|c|}{\textbf{Local Models}} \\ \hline
Profile HMMs \cite{shihab2013predicting}                   & Hidden Markov&-- &--          &--&MSAs &--&Oct 2012\\ \hline
EvMutation \cite{hopf2017mutation}           & Potts Models  &--&--&--&MSAs  &-- &Jan 2017               \\ \hline
MSA Transformer \cite{rao2021msa}        & Transformer&1024&768&100M     & UniRef50 \cite{uniprot2021uniprot} &26M    &Feb 2021           \\ \hline
DeepSequence \cite{riesselman2018deep}           & VAEs&--&--&--&MSAs&--&Dec 2017 \\ \hline
EVE \cite{frazer2021disease}                    & Bayesian VAEs &--&--&--&MSAs &--&Oct 2021 \\ \hline
\multicolumn{8}{|c|}{\textbf{Global Models}} \\ \hline
%TAPE CNNs \cite{rao2019evaluating}                   & CNNs & Pfam \cite{el2019pfam} &31M  \\ \hline
TAPE ResNet \cite{rao2019evaluating}                 & ResNet&1024 &256&38M  & Pfam \cite{el2019pfam} &31M& Jun 2019             \\ \hline
TAPE LSTM \cite{rao2019evaluating} 	          & LSTM&1024 &2048&38M& Pfam \cite{el2019pfam} &31M& Jun 2019\\ \hline
TAPE Transformer \cite{rao2019evaluating}     & Transformer&1024 &512&38M   & Pfam \cite{el2019pfam} &31M& Jun 2019            \\ \hline
Bepler \cite{bepler2018learning} 		              & LSTM &512  &100&22M& Pfam \cite{el2019pfam} &31M   &Feb 2019                       \\ \hline
UniRep \cite{alley2019unified}                 & LSTM &512 &1900&18M&UniRef50 \cite{uniprot2021uniprot}&      24M   &Mar 2019                \\ \hline
eUniRep \cite{biswas2021low} & LSTM   &512&1900&18M&UniRef50 \cite{uniprot2021uniprot}; MSAs &      24M                     &Jan 2020                      \\ \hline
ESM-1b \cite{rives2021biological}                    & Transformer &1024&1280&650M&UniRef50 \cite{uniprot2021uniprot} &250M& Dec 2020                   \\ \hline
ESM-1v \cite{meier2021language}                    & Transformer  &1024&1280&650M&                  UniRef90 \cite{uniprot2021uniprot} &98M&Jul 2021 \\ \hline
ESM-IF1 \cite{hsu2022esm}&Transformer&--&512&124M&UniRef50 \cite{uniprot2021uniprot}; CATH \cite{orengo1997cath}&12M sequences; 16K structures&Sep 2022\\ \hline
ProGen \cite{madani2023large}                 & Transformer&512 &--&1.2B&UniParc \cite{uniprot2021uniprot}; UniprotKB \cite{uniprot2021uniprot}; Pfam \cite{el2019pfam}; NCBI Taxonomy \cite{federhen2012ncbi} &281M&               Jul 2021 \\ \hline
ProteinBERT \cite{brandes2022proteinbert}            & Transformer &1024&--&16M&UniRef90 \cite{uniprot2021uniprot}&106M               &May 2021     \\ \hline
Tranception \cite{notin2022tranception}            & Transformer     &1024&1280&700M              &UniRef100 \cite{uniprot2021uniprot}&250M&May 2022   \\ \hline
ESM-2 \cite{lin2023evolutionary}           & Transformer &1024  &5120&15B&    UniRef90                 \cite{uniprot2021uniprot}&65M&Oct 2022   \\ \hline
\end{tabular}
\caption{Summary of protein language models. \# para: number of parameters which are only provided for deep learning models. Max len: maximum length of input sequence. Dim: latent space dimension. Size: pre-trained data size where it refers to number of sequences without specification except MSA transformer includes 26 millions of MSAs. K: thousands; M: millions; B: billions. $^1$: Time for the first preprint. The input data size, hidden layer dimension, and number of parameters are only provided for global models. }
\label{table_seq}
\end{table*}
}
\subsection{Local evolutionary models}

To train a local evolutionary model, MSAs search strategies such as jackhmmer \cite{eddy2011accelerated} and EvCouplings \cite{hopf2019evcouplings} are first employed. Taking  MSAs as inputs, local evolutionary models learn the probabilistic distribution of mutations for a target protein. Probabilistic models, including Hidden Markov Models (HMMs) \cite{rabiner1989tutorial, shihab2013predicting} and Potts-based models \cite{hopf2017mutation},   are popular in modeling mutational effects.  Transformer models have been introduced to learn distribution from MSAs. The MSA Transformer \cite{rao2021msa} introduces a row- and column-attention mechanism. Recent years, variational autoencoders (VAEs) \cite{kingma2013auto} serve as the alternate to model MSAs by including the dependency between residues and aligning all sequences to a probability distribution. The VAE model DeepSequence \cite{riesselman2018deep} and the Bayesian VAE model EVE \cite{frazer2021disease} exhibit excellent performance in modeling mutational effects \cite{livesey2020using, hsu2022learning, qiu2023persistent}.  

\subsection{Global evolutionary models}

With  large-size  data, global evolutionary models usually adopt the large NLP models. Convolutional Neural networks (CNNs) \cite{kim-2014-convolutional} models and residual network (ResNet) \cite{he2016deep} have been employed for protein sequence analysis \cite{rao2019evaluating}. Large-scale models, such as long short-term memory (LSTM)  \cite{hochreiter1997long}, have also gained popularity as seen in Bepler \cite{bepler2018learning}, UniRep \cite{alley2019unified}, and eUniRep \cite{biswas2021low}. In recent years, the Transformer architecture has achieved state-of-the-art performance in NLP by introducing the attention mechanism and the self-supervised learning via the masked filling training strategy \cite{vaswani2017attention, devlin2018bert}. Inspired by these advances, Transformer-based protein language models provide new opportunities for building global evolutionary models. A variety of Transformer-based models have been developed such as evolutionary scale modeling (ESM) \cite{rives2021biological, meier2021language}, ProGen \cite{madani2023large}, ProteinBERT \cite{brandes2022proteinbert}, Tranception \cite{notin2022tranception} and ESM-2 \cite{lin2023evolutionary}. 

\subsection{Hybrid approach via fine-tune pre-training}

Although global evolutionary models can learn a variety of sequences derived from natural evolution, they face challenges in concentrating on local information when predicting the effects of site-specific mutations in a target protein. To enhance the performance of global evolutionary models, fine-tuning strategies are subsequently implemented. Specifically, fine-tune strategy further refines the pre-trained global models with local information using MSAs or target training data. The fine-tuned eUniRep \cite{biswas2021low} shows significant improvement over UniRep \cite{alley2019unified}. Similar improvement was also reported for ESM models \cite{rives2021biological, meier2021language}. The Tranception model also proposed a hybrid approach combining a global autoregressive inference and a local retrieval inference from MSAs \cite{notin2022tranception}. Tranception achieved the advanced performance over other global and local models. 

With various language models proposed, comprehensive studies on various models and the strategy in building downstream model is necessary. A study explored different approaches utilizing the sequence embedding to build downstream models \cite{detlefsen2022learning}. Two other studies further benchmarked many unsupervised and supervised models in predicting protein fitness \cite{livesey2020using, hsu2022learning}.

\section{Structure-based topological data analysis (TDA) models}

Aided by advanced NLP algorithms, sequence-based models have become the dominant approach in MLPE \cite{yang2019machine, wittmann2021advances}. However, sequence-based models suffer from a lack of appropriate description of stereochemical information, such as cis-trans isomerism, conformational isomerism, enantiomers, etc. Therefore, sequence embeddings cannot distinguish stereoisomers, which are widely present in biological systems and play a crucial role in many chemical and biological processes. Structure-based models offer a solution to this problem. TDA has became a successful tool in building structure-based models for MLPE \cite{qiu2023persistent}. 

TDA is a mathematical framework based on algebraic topology \cite{edelsbrunner2008persistent, zomorodian2004computing}, which allows us to characterize complex geometric data, identify underlying geometric shapes, and uncover topological structures present in the data. TDA finds its applications in a wide range of fields, including neuroscience, biology, materials science, and computer vision. It is especially useful in situations where the data is complex, high-dimensional, and noisy, and where traditional statistical methods may not be effective. In this section, we provide an overview of various types of TDA methods (\autoref{table_tda}). In addition, we review graph neural networks, which are deep learning frameworks cognizant of topological structures, along with their applications in protein engineering. For those readers who are interested in the deep mathematical details of TDA, we have added a supplementary section dedicated to two TDA methods - persistent homology and persistent spectral graph (PSG) in Supplementary Methods.  

% Figure environment removed 

{\small 
\begin{table*}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{Method  } & \textbf{Topological space}  & \textbf{Node attribute } & \textbf{Edge attribute } \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Homology-based}}\\ 
        \hline
        Persistent Homology \cite{edelsbrunner2008persistent, zomorodian2004computing}  &Simplicial complex & None&None\\
        \hline
        Element-specific PH (ESPH) \cite{cang2018integration}&Simplicial complex & Group labeled & Group labeled  \\
        \hline
        Persistent Cohomology \cite{cang2020persistent} &Simplicial complex & Labeled &  Labeled \\
        \hline
        Persistent Path Homology \cite{chowdhury2018persistent} &Path complex &Path &Directed \\ \hline
				Persistent Flag Homology \cite{lutgehetmann2020computing}& Flag complex& None & Directed \\  \hline
				Evolutionary homology \cite{cang2020evolutionary}& Simplicial complex & Weighted & Weighted \\ \hline
				Weighted persistent homology \cite{meng2020weighted}& Simplicial complex & Weighted & Weighted \\
        \hline
        \multicolumn{4}{|c|}{\textbf{Laplacian-based}}\\ 
        \hline
				 Persistent Spectral Graph \cite{wang2020persistent,memoli2022persistent} &Simplicial complex  &None &None \\ 
				 \hline    
        Persistent Hodge Laplacians \cite{chen2021evolutionary} &Manifold &Continuum&Continuum\\
        \hline
        Persistent Sheaf Laplacians \cite{wei2021persistent} &Cellular complex& Labeled & Sheaf relation\\
        \hline
        Persistent Path Laplacians \cite{wang2023persistent} &Path complex  &Path &Direction \\
        \hline
        Persistent Hypergraph \cite{liu2021persistent}&Hypergraph&Hypernode &Hyperedge \\
        \hline
        Persistent Directed Hypergraphs \cite{chen2023persistent} &Hypergraph&Hypernode &Directed hyperedge \\
       	\hline
    \end{tabular}
        \caption{Summary of topological data analysis (TDA) methods for structures.}
        \label{table_tda}
\end{table*}
}
 \subsection{Homology }\label{sec_TDA} 

The basic idea behind TDA is to represent the data as a point cloud in a high-dimensional topological space, and then study the topological invariants of this space, such as the genus number, Betti number, and Euler characteristic. Among them, the Betti numbers, specifically Betti zero, Betti one, and Betti two, can be interpreted as representing connectedness, holes, and voids, respectively \cite{kaczynski2004computational, wasserman2018topological}. These numbers can be computed as the ranks of the corresponding homology groups in appropriate dimensions.

Homology groups are algebraic structures that are associated with topological spaces \cite{kaczynski2004computational}. They provide information about the topological connectivity of geometric objects. The basic idea behind homology is to consider the cycles and boundaries of a space. Loosely speaking, a cycle is a set of points in the space that form a closed loop, while a boundary is a set of points that form the boundary of some region in the space.
The homology group of a space is defined as the group of cycles modulo the group of boundaries. That is, we identify two cycles that differ by a boundary and consider them to be equivalent. The resulting homology group encodes information about the Betti numbers of the space.

Homology theory has many applications in mathematics and science. It is used to classify topological spaces in category theory, to study the properties of manifolds in differential geometry and algebraic geometry, and to analyze data in various scientific fields \cite{kaczynski2004computational}. However, the original homology groups offer truly geometry-free representations and are too abstract to carry sufficient geometric information of data.
Persistent homology was designed to improve homology groups' ability for data analysis.



\subsection{Persistent homology}

Persistent homology is a relatively new tool in algebraic topology that is designed to incorporate multiscale topological analysis of data \cite{edelsbrunner2008persistent, zomorodian2004computing}. The basic idea behind persistent homology is to construct a family of geometric shapes of the original data by filtration (\autoref{fig1}c). Filtration systematically enlarges the radius of each data point in a point cloud, leading to a family of topological spaces with distinct topological dimensions and connectivity. Homology groups are built from the family of shapes, giving rise to systematic changes in topological invariants, or Betti numbers, at various topological dimensions and geometric scales. Topological invariants based on Betti numbers are expressed in terms of persistence barcodes \cite{ghrist2008barcodes} (\autoref{fig1}d), persistence diagrams \cite{cohen2005stability}, persistence landscapes \cite{bubenik2015statistical}, or persistence images \cite{adams2017persistence}. Persistent topological representations are widely used in applications, particularly in association with machine learning models \cite{cang2015topological}.
 
Persistent homology is the most important approach in TDA (see Table \ref{table_tda} for a summary of major TDA approaches). It reveals the shape of data in terms of the topological invariants and has had tremendous success in scientific applications, including image and signal processing \cite{clough2020topological}, machine learning \cite{pun2018persistent}, biology \cite{cang2015topological}, and neuroscience\cite{stolz2017persistent}. Nonetheless, to effectively analyze complex biomolecular data, persistent homology requires further refinement and adjustment. \cite{wei2023topological}. 



\subsection{Persistent cohomology and element-specific persistent homology}

One major limitation of persistent homology is that it fails to describe heterogeneous information of data point \cite{cang2020persistent}. In other words, it treats all entries in the point cloud equally without considering other important information about the data. Biomolecules, for example, contain many different element types and each atom may have a different atomic partial charge, atomic interaction environment, and electrostatic potential function that cannot be captured by persistent homology. Thus, it is crucial to have a topological technique that can incorporate both geometric and nongeometric information into a unified framework.

Persistent cohomology was developed to provide such a mathematical paradigm \cite{cang2020persistent}. In this framework, nongeometric information can either be prescribed globally or reside locally on atoms, bonds, or many-body interactions. In topological terminology, nongeometric information is defined on simplicial complexes. This persistent cohomology-based approach can capture multiscale geometric features and reveal non-geometric interaction patterns through topological invariants, or enriched persistence barcodes. It has been demonstrated that persistent cohomology outperforms other methods in benchmark protein-ligand binding affinity prediction datasets \cite{cang2020persistent}, which is a non-trivial problem in computational drug discovery. 
 
An alternative approach for addressing the limitation of persistent homology is to use element-specific persistent homology (ESPH) \cite{cang2018integration}. The motivation behind ESPH is the same as that for persistent cohomology, but ESPH is relatively simple. Basically, atoms in the original biomolecule are grouped according to their element types, such as C, N, O, S, H, etc. Then, their combinations, such as CC, CN, CO, etc., are identified, and persistent homology analysis is applied to the atoms in each element combination, resulting in ESPH analysis. As a result, ESPH reduces geometric and biological complexities and embeds chemical and biological information into topological abstraction. The ESPH approach was used to win the D3R Grand Challenges, a worldwide competition series in computer-aided drug design \cite{nguyen2019mathematical}.

\subsection{Persistent topological Laplacians}

However, aforementioned TDA methods are still limited in describing complex data, such as its lack of description of non-topological changes (i.e., homotopic shape evolution) \cite{qiu2023persistent}, its incapability of coping with directed networks and digraphs (i.e., atomic partial charges and polarizations, gene regulation networks), and its inability to characterize structured data (e.g., functional groups, binding domains, and motifs) \cite{wei2023topological}. These limitations necessitate the development of innovative strategies.

Persistent topological Laplacians (PTLs) are a new class of mathematical tools designed to overcome the aforementioned challenges in TDA \cite{wei2023topological}. One of the first methods in this class is the PSG \cite{wang2020persistent}, also known as persistent  combinatorial  Laplacians  \cite{wang2020persistent} or persistent Laplacians
\cite{memoli2022persistent}. PSGs have both harmonic spectra with zero eigenvalues and non-harmonic spectra with non-zero eigenvalues (\autoref{fig1}e). The harmonic spectra recover all the topological invariants from persistent homology, while the non-harmonic spectra capture the homotopic shape evolution of data that cannot be described by persistent homology \cite{wei2023topological}. PSGs have been used for accurate forecasting of emerging dominant SARS-CoV-2 variants BA.4/BA.5 \cite{chen2022persistent}, facilitating machine learning-assisted protein engineering predictions \cite{qiu2023persistent}, and other applications \cite{meng2021persistent}.

Like persistent homology, persistent Laplacians are limited in their ability to handle directed networks and atomic polarizations. To address these limitations, persistent path Laplacians have been developed \cite{wang2023persistent}. Their harmonic spectra recover the topological invariants of persistent path homology \cite{chowdhury2018persistent}, while their non-harmonic spectra capture homotopic shape evolution. Both persistent path Laplacians and persistent path homology were developed as a generalization of the path complex \cite{grigor2020path}.

None of the PTLs mentioned above are capable of handling different types of elements in a molecule as persistent cohomology does. To overcome this limitation, persistent sheaf Laplacians \cite{wei2021persistent} were designed, inspired by persistent cohomology \cite{cang2020persistent}, persistent Laplacians \cite{wang2020persistent}, and sheaf Laplacians for cellular sheaves \cite{hansen2019toward}. The aim of persistent sheaf Laplacians is to discriminate between different objects in a point cloud. By associating a set of non-trivial labels with each point in a point cloud, a persistent module of sheaf cochain complexes is created, and the spectra of persistent sheaf Laplacians encode both geometrical and non-geometrical information \cite{wei2021persistent}. The theory of persistent sheaf Laplacians is an elegant method for the fusion of different types of data and opens the door to future developments in TDA,   geometric data analysis, and algebraic data analysis. 

 
Persistent hypergraph Laplacians enable the topological description of internal structures or organizations in data \cite{liu2021persistent}. Persistent hyperdigraph Laplacians further allow for the topological Laplacian modeling of directed hypergraphs \cite{chen2023persistent}. These persistent topological Laplacians can be utilized to describe intermolecular and intramolecular interactions. As protein structures are inherently multiscale, it is natural to apply persistent hypergraph Laplacians and persistent hyperdigraph Laplacians to delineate the protein structure-function relationship.  

Finally,  unlike all the aforementioned PTLs,  evolutionary de Rham-Hodge Laplacians or persistent Hodge Laplacians are defined on a family of filtration-induced differentiable manifolds \cite{chen2021evolutionary}. They are particularly valuable for the  multiscale topological analysis of volumetric data. Technically, a similar algebraic topology structure is shared by persistent Hodge Laplacians and persistent Laplacians, but the former is a continuum theory for volumetric data and the latter is a discrete formulation for point cloud. As such,  their underlying mathematical definitions, i.e., differential forms on manifolds and simplicial complexes on graphs, are sharply different. 

  
%A protein 3D structure can be represented by a point cloud data where each point corresponds to an atom with its coordinate (\autoref{fig1}a and \ref{fig1}b). To model the structure of point cloud, topological data analysis (TDA), particularly persistent homology \cite{zomorodian2005computing, edelsbrunner2010computational}, is a popular tool to extract the multiscale shape information   \cite{cang2018integration. Recently, persistent topological Laplacians \cite{wang2020persistent, wei2021persistent,memoli2022persistent, meng2021persistent, wang2022persistent, chen2023persistent} were proposed to extract more comprehensive shape evolution in data.  In this section, we provide a brief review of these popular TDA methods for protein embedding. 




%In addition, the rest of the spectra, i.e., the non-harmonic part, capture additional  geometric information. The family of spectra of the persistent Laplacians reveals the homotopic shape evolution \cite{qiu2023persistent}.

%\subsection{Building TDA-based machine learning models}

%TDA features are a part of machine learning models. They are combined with machine learning algorithms to achieve predictive goals. 


%\subsubsection{Element-specific and site-specific strategy}

%In molecular structure modeling, the site- and element-specific strategies are used to cooperate TDA methods \cite{cang2017analysis}. The site-specific strategy focuses on local structure to highlight local interactions and reduce computational complexity. For example, in learning protein mutational impacts, site-specific strategy focus on atoms near the mutational site to characterize the morphological difference between wild type and mutants \cite{wang2020topology, cang2017analysis, qiu2023persistent}. The element-specific strategy considers the pairwise interactions between two element types to encode appropriate physical and chemical interactions in embedding. For examples, hydrogen bonds can be inferred from the topological invariants of the oxygen-nitrogen element combination, whereas  hydrophobicity can be extracted from the carbon-carbon element combination \cite{cang2017analysis, cang2018integration, qiu2023persistent}. 

%\subsubsection{TDA feature construction}

%TDA provides a powerful tool for geometric simplification for the complex protein structures. However, different proteins have different number of atoms which are non-scalable despite coupling with TDA simplification. Therefore, an informative topological representation with scalable dimension is required for building machine learning model \cite{cang2015topological}. 

%For persistent homology, persistence barcode is one popular but non-scalable representation. A simple approach to make it scalable is to use statistical values of all persistent bars \cite{cang2017analysis}. For example, one can sum up lengths of persistence bars. More advanced approaches, such as persistent signatures \cite{hofer2017deep}, persistence landscape \cite{bubenik2015statistical}, and  persistent image \cite{adams2017persistence}, provide more stable representations, but result in higher dimensional features. The calibration for the feature dimension needs to be taken into consideration to prevent overfitting and underfitting issues.

%For persistent Laplacians, the statistical values of its eigenvalues provide a relatively low-dimensional representation \cite{qiu2023persistent}. Other scalable representation can be further explored. For example, one can extract a fixed number of non-zero eigenvalues in addition to the counts of harmonic spectra. One can also incorporate the eigenvector information.


\subsection{Deep graph neural networks and topological deep learning}



Similar to topological data analysis, graph- and topology-based deep learning models have been proposed to capture connectivity and shape information of protein structure data. Graph neural networks (GNNs) consider the low-order interactions between vertices by aggregating information from neighbor vertices. A variety of popular graph neural network layers has been proposed, such as convolution graph networks (GCN) \cite{kipf2016semi}, graph attention networks (GAT) \cite{velivckovic2017graph}, graph sample and aggregate (GraphSAGE) \cite{hamilton2017inductive},  Graph Isomorphism Network (GIN) \cite{xu2018powerful}, and gated graph neural network \cite{li2015gated}. 

With variety of architectures of GNN layers, self-supervised learning models are widely used for representation learning of graph-based data. Graph autoencoder (GAE) and variational graph autoencoder (VGAE) consist of both encoder and decoder, where the decoder employ a linear inner product to reconstruct adjacent matrix \cite{kipf2016variational}. While most of graph-based self-supervised models only have encoder. Deep graph infomax (DGI) maximizes mutual information between a graph's local and global features to achieve self-supervised learning \cite{velivckovic2018deep}. Graph contrastive learning (GRACE) constructs positive and negative pairs from a single graph, and trains a GNN to differentiate between them \cite{you2020graph}. Self-supervised graph transformer (SSGT) uses masked node prediction to train the model. Given a masked graph, it tries to predict the masked node's attributes from the unmasked nodes \cite{rong2020self}. 

In applications to learning protein structures, GCNs have been widely applied to building structure-to-function map of proteins \cite{li2021structure, gligorijevic2021structure}. Moreover, self-supervised models provide powerful pre-trained model in learning representation of protein structures. GeoPPI \cite{liu2021deep} proposed a graph neural network-based autoencoder to extract structural embedding at the protein-protein binding interface. The subsequent downstream models allow accurate predictions for protein-protein binding affinity upon mutations \cite{liu2021deep} and further design effective antibody against SARS-CoV-2 variants \cite{shan2022deep}. GRACE has been applied to learn geometric representation of protein structures \cite{zhang2022protein}. To adopt the critical biophysical properties and interactions between residues and atoms in protein structures, graph-based self-supervised learning models have been customized to achieve the specific functions. The inverse protein folding protocol was proposed to capture the complex structural dependencies between residues in its representation learning \cite{ingraham2019generative, hsu2022esm}. OAGNNs was proposed to better sense the geometric characteristics such as nner-residue torsion angles, inter-residue orientations in its representation learning \cite{li2022orientation}.

 Topological deep learning,  proposed by Cang and Wei in 2017 \cite{cang2017topologynet}, is an emerging paradigm.   It integrates topological representations with deep neural networks   for protein fitness learning and prediction \cite{cang2017topologynet, nguyen2019mathematical,qiu2023persistent}. Similar graph and topology-based deep learning architectures have also been proposed to capture connectivity and shape information of protein structure data \cite{chen2022persistent,chen2023persistent}. 
Inspired by TDA,  high-order interactions among neural nodes  were proposed in $k$-GNNs \cite{morris2019weisfeiler} and simplicial neural networks \cite{ebli2020simplicial}.




\section{Artificial intelligence-aided protein engineering}

Protein engineering is a typical black-box optimization problem, which focuses on finding the optimal solution without explicitly knowing the objective function and its gradient. In protein engineering, the goal in designing algorithms for this problem is to efficiently search for the best sequence within a large search space:


\begin{equation}
\label{eq_box}
x^{*}=\argmax_{x\in \mathcal{S}}f(x),
\end{equation}
where $\mathcal{S}$ is an unlabeled candidate sequence library, $x$ is a sequence in the library and $f(x)$ is the unknown sequence-to-fitness map for optimization. The fitness landscape, $f(\mathcal{S})$, is a high-dimensional surface that maps amino acid sequences to properties such as activity, selectivity, stability, and other physicochemical features.  

There are two practical challenges in protein engineering. First, the fitness landscape is usually epistatic \cite{wu2016adaptation, podgornaia2015pervasive}, where the contribution of individual amino acid residues to protein fitness have dependency to each other. The interdependence leads to complex, non-linear interactions among different residues. In other word, the fitness landscape contains large number of local optima. For example, in a four-site mutational fitness landscape for GB1 protein with $20^4=160,000$ mutations, 30 local maximum fitness peaks were found \cite{wu2016adaptation}. Either traditional directed evolution experiments such as single-mutation walk and recombination, or machine learning models, is difficult to find the global optima without trapped at local one. Second, protein engineering process usually collects limited number of data comparing to the huge sequence library. There are an enormous number of ways to mutate any given protein: for a 300-amino-acid protein, there are 5,700 possible single-amino-acid substitutions and 32,381,700 ways to make just two substitutions with the 20 canonical amino acids \cite{yang2019machine}. Even with high-throughput experiments, only a small fraction of the sequence library can be screened. Despite this, many systems only have low-throughput assays such as membrane proteins \cite{zhang2023structural}, making the process more difficult. 

With enriched data-driven protein modeling approaches from protein sequences to structures, recent advanced machine learning methods have been widely developed to accelerate protein engineering in silico (\autoref{fig2}a) \cite{narayanan2021machine, wittmann2021advances, yang2019machine, freschlin2022machine, hie2022adaptive}. Utilizing a limited experimental capacity, machine learning models can effectively augment the fitness evaluation process, enabling the exploration of a vast search space $\mathcal{S}$. This approach facilitates the discovery of optimal solutions within complex design spaces, despite constraints on the number of trials or experiments.

Using a limited number of experimentally labeled sequences, machine learning models can carry out zero-shot or few-shot predictions \cite{wittmann2021advances}. The accuracy of these predictions largely depends on the distribution of the training data, which influences the model's ability to generalize to new sequences. Concretely, if the training data is representative or closer to a given sequence, the model is more likely to make accurate predictions for that specific sequence. Conversely, if the training data is not representative or distant from the given sequence, the model's predictive accuracy may be compromised, leading to less reliable results. Therefore, MLPE are usually an iterative process between machine learning models and experimental screens. Incorporating the exploration-exploitation trade-off in this context is essential for achieving optimal results. During the iterative process, the model must balance exploration, where it seeks uncertain regions that machine learning models have low accuracy, with exploitation, where it refines and maximizes fitness based on previously gained knowledge.  A right balance is critical to preventing overemphasis on either exploration or exploitation leading, which may lead to suboptimal solutions. In particular, the epistatic nature of protein fitness landscapes influences the exploration-exploitation trade-off in the design process.

MLPE methods need to take the experimental capacity into account when attempt to balance the exploitation-exploration. In this section, we discuss different strategies upon the number of experimental capacity. First, we discuss zero-shot strategy when no labeled experimental data is available. Second, we discuss supervised models for performing greedy search (i.e., exploitation). Last, we discuss uncertainty quantification models that balance exploration and exploitation trade-off.

\subsection{Unsupervised zero-shot strategy}

First, we review the zero-shot strategy that interrogates protein fitness with an unsupervised manner (\autoref{fig2}b and \autoref{table_comp}). This is designed for the scenarios in the early stage designs where no experiments have been conducted or the experimentally labeled data is too limited allowing accurate fitness predictions from supervised models \cite{wittmann2021advances, qiu2023persistent}. They delineate a fitness landscape at the early stage of protein engineering. Essential residues can be identified and prioritized for mutational experiments, allowing for a more targeted approach to protein engineering \cite{riesselman2018deep}. Additionally, the initial fitness landscape can be utilized to filter out protein candidates with a low likelihood of exhibiting the desired functionality.  By focusing on sequences with higher probabilities, protein engineering process can be made more efficient and effective \cite{wittmann2021informed}.

Zero-shot predictions rely on the model's ability to recognize patterns in naturally observed proteins, enabling it to make informed predictions for new sequences without having direct training data for the target protein. As discussed in Section \ref{sec_PLM}, protein language models, particularly generative models, learn the distribution of naturally observed proteins which are usually functional. The learned distribution can be used to assess the likelihood that a newly designed protein lies within the distribution of naturally occurring proteins, thus providing valuable insights into its potential functionality and stability \cite{wittmann2021advances}. 

VAEs are popular local evolutionary models for zero-shot predictions such as DeepSequence \cite{riesselman2018deep} and EVE models \cite{frazer2021disease}. In VAEs, the conditional probability distribution $p(x\vert z, \theta)$ is the decoder in a form of neural network with parameters $\theta$, where $x$ is the sequence being query and $z$ is its latent space variable. Similar, encoder, $q(z\vert x,\phi)$, is modeled by another neural network with parameters $\phi$ to approximate the true posterior distribution $p(z\vert x)$. For a given sequence $x$, its probabilistic likelihood in VAEs is $p(x\vert \theta)$ parameterized by parameters $\theta$. Direct computation of this probability, $p(x\vert \theta)=\int p(x\vert z,\theta)dz$, is intractable in the general case.  The evidence lower bound (ELBO) forming a variation inference \cite{kingma2013auto} provides a lower bound of the log likelihood:
\begin{equation}
\log p(x\vert\theta)\geq {\rm ELBO}(x)=\mathbb{E}_q\log p(x\vert z, \theta)-{\rm KL}\left(q(z\vert x,\phi)\| p(z)\right).
\end{equation}
ELBO is taken as the scoring function to quantify the mutational likelihood of each query sequence. The ELBO-based zero-shot predictions show advanced performance reported in multiple works \cite{hsu2022learning,livesey2020using,qiu2023persistent}. 

Transformer is the currently state-of-the-art model which has been used in many supervised tasks \cite{rives2021biological}. It learns a global distribution of nature proteins. It has also been proved to have advanced performance for zero-shot predictions \cite{hsu2022learning, meier2021language}. The training of Transformer uses mask filling that refers to the process of predicting masked amino acid in a given input sequence by leveraging the contextual information encoded in the Transformer's self-attention mechanism \cite{vaswani2017attention, devlin2018bert}. The mask filling procedure creates a classification layer on the top of the Transformer architecture. Given a sequence $x$, the masked filling classifier generate probability distributions for amino acids at masked positions. Suppose $x$ has $L$ amino acids $x=x_1x_2\cdots x_L$, by masking a single amino acid at $i$-th position, the classifier calculates the conditional probability of $p(x_i\vert x^{(-i)})$, where $x^{(-i)}$ is the remaining sequence excluding the masked $i$-th position. To reduce the computational cost, the pseudo-log-likelihoods (PLLs) are usually used to estimate the log-likelihood of a given sequence \cite{hsu2022learning, wittmann2021informed}:
\begin{equation}
\mbox{PLL}(s)=\sum_{i=1}^L\log P(s_i\vert s^{(-i)}).
\end{equation}
The PPLs assume the independence between amino acids. To consider the dependence between amino acids, one can calculate the conditional probability by summing up all possible factorization \cite{wittmann2021informed}. But this approach leads to much higher computational cost.

Furthermore, many different strategies have been employed to make zero-shot predictions. Fine-tune model can improve the predictions by combining both local and global evolutionary models \cite{biswas2021low}. Tranception scores combine global autoregressive inference and an local MSAs retrieval inference to make more accurate predictions. In addition to these sequence-based models, the structure-based GNN-based models including ESM-1F \cite{hsu2022esm} and RGC \cite{tian2023sequence} have also been proposed by utilizing large-scale structural data from AlphaFold2. However, the structure-based model is still limited in accuracy comparing to sequence-based models.

\subsection{Supervised regression models}
Supervised regression models are among the most prevalent approaches used in guiding protein engineering, as they enable greedy search strategies to maximize protein fitness (\autoref{fig2}c). These models, including statistical, machine learning, and deep learning techniques, rely on a set of labeled data as their training set to predict the fitness landscape. By leveraging the information contained within the training data, supervised regression models can effectively estimate the relationship between protein sequences and their fitness, providing valuable insights for protein engineering and optimization \cite{yang2019machine, narayanan2021machine}.

A variety of supervised models have been applied to predict protein fitness. In general, statistical models and machine learning models such as linear regression \cite{fox2007improving}, ridge regression \cite{hsu2022learning}, support vector machine (SVM) \cite{guo2008using}, random forest \cite{zhang2020mutabind2}, gradient boosting tree \cite{cang2017analysis} have accurate performance for small training set. And deep learning methods such as deep neural networks \cite{aghazadeh2021epistatic}, convolutional neural networks (CNNs) \cite{wang2020topology}, attention-based neural networks \cite{dallago2021flip} are more accurate with large size of training data. However, in protein engineering, the size of training data increases sequentially which make the supervised models difficult to provide accurate performance all time. Alternatively, the ensemble regression was proposed to provide robust fitness predictions despite of training data size \cite{wittmann2021advances, bryant2021deep}. The ensemble regression average predictions from multiple supervised models and they provide more accurate and robust performance than single model \cite{qiu2023persistent}. To remove the inaccurate models in the average, cross-validation is usually used to rank accuracy of each model and only top models are taken to average the predictions. Paired with the zero-shot strategy, the ensemble regression trained on informed training set pre-selected by zero-shot predictions can efficiently pick up the global optimal protein with a few round of experiments \cite{wittmann2021informed, qiu2021cluster, qiu2022clade}. And such approach has been applied to enable resource-efficient engineering CRISPR-Cas9 genome editor activities \cite{thean2022machine}.

Rather than the architectures of supervised models, the predictive accuracy highly rely on the amount of information obtained from the featurization process (\autoref{table_comp}). The physical-chemical properties extract the properties of individual amino acids or atoms \cite{georgiev2009interpretable}. The energy-based scores provide descriptions for the overall property of the target protein \cite{schymkowitz2005foldx}. However, neither of them successfully take the complex interactions between residues and atoms into account. To tackle this challenge, recent mathematics-initiated topological and geometric descriptors achieved great success in predicting protein fitness including protein-protein interactions \cite{wang2020topology}, protein stability \cite{cang2017analysis}, enzyme activity, and antibody effectivity \cite{qiu2023persistent}. The aforementioned descriptors (Section \ref{sec_TDA}) extract structural information from atoms at different characteristic lengths. Furthermore, the sequence-based protein language models provide another featurization strategies. The deep pre-trained models have the latent space which provide the informative representation of each given sequence. Building supervised models from the deep embedding exhibits accurate performance \cite{qiu2023persistent, shen2022svsbi}. Recent works combine different types of sequence-based features \cite{luo2021ecnet, hsu2022learning} or combine structure-based and sequence-based features \cite{qiu2023persistent} show the complementary roles of different featurization approaches. 

{\small 
\begin{table}[]
\centering
\begin{tabular}{|l|r|r|r|r|}
\hline
\multicolumn{5}{|c|}{\textbf{Zero-shot predictors}}\\
\hline
\multirow{2}{*}{Model name} & \multicolumn{4}{|c|}{training set size} \\
\cline{2-5}
&\multicolumn{4}{|c|}{$0$} \\
\hline
ESM-1b PLL \cite{rives2021biological, hsu2022learning} & \multicolumn{4}{|c|}{0.435} \\
\hline
eUniRep PLL \cite{georgiev2009interpretable} & \multicolumn{4}{|c|}{0.411} \\
\hline
EVE  \cite{frazer2021disease} & \multicolumn{4}{|c|}{0.497} \\
\hline
Tranception \cite{notin2022tranception} & \multicolumn{4}{|c|}{0.478} \\
\hline
DeepSequence \cite{riesselman2018deep} & \multicolumn{4}{|c|}{\textbf{0.504}} \\
\hline
\multicolumn{5}{|c|}{\textbf{Supervised models}}\\
\hline
\multirow{2}{*}{Embedding name} & \multicolumn{4}{|c|}{training set size} \\
\cline{2-5}
& $24$ & $96$ & $168$ & $240$ \\
\hline
Persistent Homology \cite{qiu2023persistent} & 0.263 & 0.432 & 0.496 & 0.534 \\
\hline
Persistent Laplacian  \cite{qiu2023persistent} & \textbf{0.280} & \textbf{0.457} & \textbf{0.525} & \textbf{0.564} \\
\hline
ESM-1b \cite{rives2021biological} & 0.219 & 0.421 & 0.494 & 0.537 \\
\hline
eUniRep \cite{biswas2021low} & 0.259 & 0.432 & 0.485 & 0.515 \\
\hline
Georgiev \cite{georgiev2009interpretable} & 0.169 & 0.326 & 0.402 & 0.446 \\
\hline
UniRep \cite{alley2019unified} & 0.183 & 0.347 & 0.420 & 0.462 \\
\hline
Onehot & 0.132 & 0.317 & 0.400 & 0.450 \\
\hline
Bepler \cite{bepler2018learning}& 0.139 & 0.287 & 0.353 & 0.396 \\
\hline
TAPE LSTM \cite{rao2019evaluating} & 0.259 & 0.436 & 0.492 & 0.522 \\
\hline
TAPE ResNet \cite{rao2019evaluating} & 0.080 & 0.216 & 0.305 & 0.358 \\
\hline
TAPE Transformer \cite{rao2019evaluating} & 0.146 & 0.304 & 0.371 & 0.418 \\
\hline
\end{tabular}
\caption{\textbf{Comparisons for fitness predictors. } 
Results were adopted from TopFit  \cite{qiu2023persistent}. Performance was reported by average Spearman correlation over 34 DMS datasets and 20 repeats. Supervised model use ensemble regression from 18 regression models \cite{qiu2023persistent}. }
\label{table_comp}
\end{table}
}
\subsection{Active learning models for exploration-exploitation balance}

With the extensive accurate protein-to-fitness machine learning models, active learning further designs iterative strategy between models and experiments to sequentially optimize fitness with the consideration of exploitation-exploration trade-off (\autoref{fig2}d-e) \cite{hie2022adaptive}. 

To balance the exploitation-exploration trade-off, the supervised models require to predict not only the protein fitness but also quantify the uncertainty of the given protein \cite{greenman2022benchmarking}. The most popular uncertainty quantification in protein engineering is Gaussian process (GP)  \cite{rasmussen2003gaussian}, which automatically calibrate the balance. Especially, GP using the upper confidence bounds (UCBs) acquisition has efficient convergent rate theoretically for solving the black-box optimization (\autoref{eq_box}). A variety protein engineering employed GP to accelerate the fitness optimization. For examples, the light-gated channelrhodopsins (ChRs) were engineered to improve photocurrence and light sensitivity \cite{bedbrook2019machine, bedbrook2017machine}, green fluorescent protein has been engineered to become yellow fluorescence \cite{saito2018machine}, acyl-ACP reductase was engineered to improve fatty alcohol production \cite{greenhalgh2021machine}, P450 enzyme has been engineered to improve thermostability \cite{romero2013navigating}.

The tree-based search strategy is also efficient by building a hierarchical search path, such as the hierarchical optimistic optimization (HOO) \cite{bubeck2011x}, the deterministic optimistic optimization (DOO), and the simultaneous optimistic optimization (SOO) \cite{munos2011optimistic}. To handle the discrete mutational space in protein engineering, an unsupervised clustering approach was employed to construct the hierarchical tree structure \cite{qiu2021cluster,qiu2022clade}. 

Recently, researchers have turned to generative models to quantify uncertainty in protein engineering, employing methods such as Variational Autoencoders (VAEs) \cite{kingma2013auto, riesselman2018deep, frazer2021disease}, generative adversarial networks (GANs) \cite{creswell2018generative,gupta2019feedback}, and autoregressive language models \cite{shin2021protein, notin2022tranception}. Generative models are a class of machine learning algorithms that aim to learn the underlying data distribution of a given dataset, in order to generate new, previously unseen data points that resemble the training data. These models capture the inherent structure and patterns present in the data, enabling them to create realistic and diverse samples that share the same characteristics as the original data. For examples, ProGen \cite{madani2023large} is a large language model that generate functional protein sequences across diverse families. A Transformer-based antibody language models utilize fine-tuning processes to assist design antibody \cite{bachas2022antibody}. Recently, a novel Transformer-based model called ReLSO has been introduced \cite{castro2022transformer}. This innovative approach simultaneously generates protein sequences and predicts their fitness using its latent space representation. The attention-based relationships learned by the jointly trained ReLSO model offer valuable insights into sequence-level fitness attribution, opening up new avenues for optimizing proteins. 

%Ensemble machine learning has been applied to optimize adeno-associated virus 2 (AAV2) capsid protein variants that remain viable for packaging of a DNA payload \cite{bryant2021deep}.

%RITA is another large generative protein language models promotes protein engineering with zero-shot prediction, next amino acid prediction, and enzyme function predictions \cite{hesslow2022rita}.

\section{Conclusions and future directions}
In this review, we  discuss the advanced deep protein language models for protein modeling.
We further provide an introduction of topological data analysis methods and their applications in protein modeling.  Relying on both structure-based and sequence-based models, MLPE methods were widely developed to accelerate protein engineering. In the future, various machine learning and deep learning will have potential perspectives in protein engineering. 

\subsection{Accurate structure prediction methods enhanced accurate structure-based models}


Comparing to sequence data, three-dimensional protein structural data offer more comprehensive and explicit descriptions of the biophysical properties of a protein and its fitness. As a result, structure-based models usually provide superb performance than sequence-based models for supervised tasks with small training set \cite{qiu2023persistent, cang2017analysis}.

As protein sequence  databases continue to grow, self-supervised models demonstrate their ability to effectively model proteins using large-scale data. The protein sequence database provides a vast amount of resources for building sequence-based models, such as UniProt \cite{uniprot2021uniprot} database contains hundreds of millions sequences. In contrast, protein structure databases are comparatively limited in size. The largest among them, Protein Data Bank (PDB), contains only 205 thousands of protein structures as of 2023 \cite{berman2000protein}. Due to the abundance of data resources, sequence-based models typically outperform structure-based models significantly \cite{tian2023sequence}.

To address the limited availability of structure data, researchers have focused on developing highly accurate deep learning techniques aimed at enabling large-scale structure predictions. These state-of-the-art methodologies have the potential to significantly expand the database of known protein structures. Two prominent methods are AlphaFold2 \cite{jumper2021highly} and RosettaFold \cite{baek2021accurate}, which have demonstrated remarkable capabilities in predicting protein structures with atomic-level accuracy. By harnessing the power of cutting-edge deep learning algorithms, these tools have successfully facilitated the accurate prediction of protein structures, thus contributing to the expansion of the structural database.

Both AlphaFold2 and RosettaFold are alignment-based, which rely on MSAs of the target protein for structure prediction. Alignment-based approaches can be highly accurate when there are sufficient number of homologous sequences (that is, MSAs depth) in the database. Therefore, these methods may have reduced accuracy with low MSAs depth in database. In addition, the MSAs search is time consuming which slows down the prediction speed. Alternatively, alignment-free methods have also been proposed to tackle these limitations \cite{kandathil2023machine}. An early work RGN2 \cite{chowdhury2022single} exhibits more accurate predictions than AlphaFold2 on orphans proteins which lack of MSAs. 
Supervised transformer protein language models predict orphan protein structures   \cite{wang2022single}. With the development of variety of large-scale protein language models in recent years, the alignment-free structural prediction methods incorporate with these models to exhibit their accuracy and efficiency. For example, ESMFold \cite{lin2023evolutionary} and OmegaFold \cite{wu2022high} achieve similar accuracy with AlphaFold2 with faster speed. Moreover, extensive language model-based methods were developed for structural predictions of single-sequence and orphan proteins \cite{fang2022helixfold,barrett2022so,wu2022tfold,weissenow2022ultra}. Large-scale protein language models will provide powerful toolkit for protein structural predictions. 

In building protein fitness model, the structural TDA-based model has exemplified that the AlphaFold2 structure is as reliable as the experimental structure \cite{qiu2023persistent}. The zero-shot model, ESM-IF1, also shows advanced performance with coupling with the large structure AlphaFold database \cite{hsu2022esm}. In the light of the revolutionary structure predictive models, structure-based models will open up a new avenue in protein engineering, from directed evolution to de novo design \cite{bordin2022novel,chidyausiku2022novo}. More sophisticated TDA methods will be demanded to handle the large-scale datasets. Large-scale deep graph neural networks will need to be further developed, for example, to consider the high-order   interactions using simplicial neural networks \cite{ebli2020simplicial, keros2022dist2cycle}.


\subsection{Large highthroughput datasets enabled larger scale models}

Current MLPE methods are usually designed for limited training set. The ensemble regression is an effective approach to accurately learn the fitness landscape with small but increasing size of training sets from deep mutational scanning  \cite{wittmann2021informed}. 



The breakthrough biotechnology, next-generation sequencing (NGS) \cite{schuster2008next} largely enhances the capacity of DMS for collecting supervised fitness data in various protein systems \cite{podgornaia2015pervasive,wu2016adaptation, sarkisyan2016local}. The resulting large-scale deep mutational scanning  databases expand the exploration range of protein engineering. Deeper machine learning models are emerging to enhance the accuracy and adaptivity for protein engineering.


\section{Competing interests}
No competing interest is declared.

\section{Author contributions statement}
Y.Q. and G.W.W conceived, wrote, and revised the manuscript.

\section{Acknowledgments}
This work was supported in part by NIH grants  R01GM126189 and  R01AI164266, NSF grants DMS-2052983,  DMS-1761320, and IIS-1900473,  NASA grant 80NSSC21M0023,  Michigan Economic Development Corporation, MSU Foundation,  Bristol-Myers Squibb 65109, and Pfizer. 

\clearpage
\bibliographystyle{unsrt}
%\bibliographystyle{plain}
%\bibliography{ref}

\begin{thebibliography}{100}

\bibitem{narayanan2021machine}
Harini Narayanan, Fabian Dingfelder, Alessandro Butt{\'e}, Nikolai Lorenzen,
  Michael Sokolov, and Paolo Arosio.
\newblock Machine learning for biologics: opportunities for protein
  engineering, developability, and formulation.
\newblock {\em Trends in pharmacological sciences}, 42(3):151--165, 2021.

\bibitem{arnold1998design}
Frances~H Arnold.
\newblock Design by directed evolution.
\newblock {\em Accounts of chemical research}, 31(3):125--131, 1998.

\bibitem{karplus2005molecular}
Martin Karplus and John Kuriyan.
\newblock Molecular dynamics and protein function.
\newblock {\em Proceedings of the National Academy of Sciences},
  102(19):6679--6685, 2005.

\bibitem{boyken2016novo}
Scott~E Boyken, Zibo Chen, Benjamin Groves, Robert~A Langan, Gustav Oberdorfer,
  Alex Ford, Jason~M Gilmore, Chunfu Xu, Frank DiMaio, Jose~Henrique Pereira,
  et~al.
\newblock De novo design of protein homo-oligomers with modular hydrogen-bond
  network--mediated specificity.
\newblock {\em Science}, 352(6286):680--687, 2016.

\bibitem{romero2009exploring}
Philip~A Romero and Frances~H Arnold.
\newblock Exploring protein fitness landscapes by directed evolution.
\newblock {\em Nature reviews Molecular cell biology}, 10(12):866--876, 2009.

\bibitem{bhardwaj2016accurate}
Gaurav Bhardwaj, Vikram~Khipple Mulligan, Christopher~D Bahl, Jason~M Gilmore,
  Peta~J Harvey, Olivier Cheneval, Garry~W Buchko, Surya~VSRK Pulavarti,
  Quentin Kaas, Alexander Eletsky, et~al.
\newblock Accurate de novo design of hyperstable constrained peptides.
\newblock {\em Nature}, 538(7625):329--335, 2016.

\bibitem{pierce2002protein}
Niles~A Pierce and Erik Winfree.
\newblock Protein design is np-hard.
\newblock {\em Protein engineering}, 15(10):779--782, 2002.

\bibitem{siedhoff2020machine}
Niklas~E Siedhoff, Ulrich Schwaneberg, and Mehdi~D Davari.
\newblock Machine learning-assisted enzyme engineering.
\newblock {\em Methods in enzymology}, 643:281--315, 2020.

\bibitem{mazurenko2019machine}
Stanislav Mazurenko, Zbynek Prokop, and Jiri Damborsky.
\newblock Machine learning in enzyme engineering.
\newblock {\em ACS Catalysis}, 10(2):1210--1223, 2019.

\bibitem{diaz2023using}
Daniel~J Diaz, Anastasiya~V Kulikova, Andrew~D Ellington, and Claus~O Wilke.
\newblock Using machine learning to predict the effects and consequences of
  mutations in proteins.
\newblock {\em Current Opinion in Structural Biology}, 78:102518, 2023.

\bibitem{wittmann2021advances}
Bruce~J Wittmann, Kadina~E Johnston, Zachary Wu, and Frances~H Arnold.
\newblock Advances in machine learning for directed evolution.
\newblock {\em Current opinion in structural biology}, 69:11--18, 2021.

\bibitem{yang2019machine}
Kevin~K Yang, Zachary Wu, and Frances~H Arnold.
\newblock Machine-learning-guided directed evolution for protein engineering.
\newblock {\em Nature methods}, 16(8):687--694, 2019.

\bibitem{berman2000protein}
Helen~M Berman, John Westbrook, Zukang Feng, Gary Gilliland, Talapady~N Bhat,
  Helge Weissig, Ilya~N Shindyalov, and Philip~E Bourne.
\newblock The protein data bank.
\newblock {\em Nucleic acids research}, 28(1):235--242, 2000.

\bibitem{uniprot2021uniprot}
Uniprot: the universal protein knowledgebase in 2021.
\newblock {\em Nucleic Acids Research}, 49(D1):D480--D489, 2021.

\bibitem{notin2022tranception}
Pascal Notin, Mafalda Dias, Jonathan Frazer, Javier~Marchena Hurtado, Aidan~N
  Gomez, Debora Marks, and Yarin Gal.
\newblock Tranception: protein fitness prediction with autoregressive
  transformers and inference-time retrieval.
\newblock In {\em International Conference on Machine Learning}, pages
  16990--17017. PMLR, 2022.

\bibitem{cang2018integration}
Zixuan Cang and Guo-Wei Wei.
\newblock Integration of element specific persistent homology and machine
  learning for protein-ligand binding affinity prediction.
\newblock {\em International journal for numerical methods in biomedical
  engineering}, 34(2):e2914, 2018.

\bibitem{wang2020topology}
Menglun Wang, Zixuan Cang, and Guo-Wei Wei.
\newblock A topology-based network tree for the prediction of protein--protein
  binding affinity changes following mutation.
\newblock {\em Nature Machine Intelligence}, 2(2):116--123, 2020.

\bibitem{schymkowitz2005foldx}
Joost Schymkowitz, Jesper Borg, Francois Stricher, Robby Nys, Frederic
  Rousseau, and Luis Serrano.
\newblock The foldx web server: an online force field.
\newblock {\em Nucleic acids research}, 33(suppl\_2):W382--W388, 2005.

\bibitem{leman2020macromolecular}
Julia~Koehler Leman, Brian~D Weitzner, Steven~M Lewis, Jared Adolf-Bryfogle,
  Nawsad Alam, Rebecca~F Alford, Melanie Aprahamian, David Baker, Kyle~A
  Barlow, Patrick Barth, et~al.
\newblock Macromolecular modeling and design in rosetta: recent methods and
  frameworks.
\newblock {\em Nature methods}, 17(7):665--680, 2020.

\bibitem{qiu2023persistent}
Yuchi Qiu and Guo-Wei Wei.
\newblock Persistent spectral theory-guided protein engineering.
\newblock {\em Nature Computational Science}, pages 1--15, 2023.

\bibitem{alley2019unified}
Ethan~C Alley, Grigory Khimulya, Surojit Biswas, Mohammed AlQuraishi, and
  George~M Church.
\newblock Unified rational protein engineering with sequence-based deep
  representation learning.
\newblock {\em Nature Methods}, 16(12):1315--1322, 2019.

\bibitem{riesselman2018deep}
Adam~J Riesselman, John~B Ingraham, and Debora~S Marks.
\newblock Deep generative models of genetic variation capture the effects of
  mutations.
\newblock {\em Nature Methods}, 15(10):816--822, 2018.

\bibitem{rives2021biological}
Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason
  Liu, Demi Guo, Myle Ott, C~Lawrence Zitnick, Jerry Ma, et~al.
\newblock Biological structure and function emerge from scaling unsupervised
  learning to 250 million protein sequences.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(15), 2021.

\bibitem{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock {\em Nature}, 596(7873):583--589, 2021.

\bibitem{edelsbrunner2010computational}
Herbert Edelsbrunner and John Harer.
\newblock {\em Computational topology: an introduction}.
\newblock American Mathematical Soc., 2010.

\bibitem{zomorodian2005computing}
Afra Zomorodian and Gunnar Carlsson.
\newblock Computing persistent homology.
\newblock {\em Discrete \& Computational Geometry}, 33(2):249--274, 2005.

\bibitem{nguyen2019dg}
Duc~Duy Nguyen and Guo-Wei Wei.
\newblock Dg-gl: differential geometry-based geometric learning of molecular
  datasets.
\newblock {\em International journal for numerical methods in biomedical
  engineering}, 35(3):e3179, 2019.

\bibitem{wee2021ollivier}
JunJie Wee and Kelin Xia.
\newblock Ollivier persistent ricci curvature-based machine learning for the
  protein--ligand binding affinity prediction.
\newblock {\em Journal of Chemical Information and Modeling}, 61(4):1617--1626,
  2021.

\bibitem{nguyen2019agl}
Duc~Duy Nguyen and Guo-Wei Wei.
\newblock Agl-score: algebraic graph learning score for protein--ligand binding
  scoring, ranking, docking, and screening.
\newblock {\em Journal of chemical information and modeling}, 59(7):3291--3304,
  2019.

\bibitem{ryczko2019deep}
Kevin Ryczko, David~A Strubbe, and Isaac Tamblyn.
\newblock Deep learning and density-functional theory.
\newblock {\em Physical Review A}, 100(2):022512, 2019.

\bibitem{butler2018machine}
Keith~T Butler, Daniel~W Davies, Hugh Cartwright, Olexandr Isayev, and Aron
  Walsh.
\newblock Machine learning for molecular and materials science.
\newblock {\em Nature}, 559(7715):547--555, 2018.

\bibitem{chen2021mlimc}
Jiahui Chen, Weihua Geng, and Guo-Wei Wei.
\newblock Mlimc: Machine learning-based implicit-solvent monte carlo.
\newblock {\em Chinese journal of chemical physics}, 34(6):683--694, 2021.

\bibitem{hsu2022learning}
Chloe Hsu, Hunter Nisonoff, Clara Fannjiang, and Jennifer Listgarten.
\newblock Learning protein fitness models from evolutionary and assay-labeled
  data.
\newblock {\em Nature biotechnology}, pages 1--9, 2022.

\bibitem{wittmann2021informed}
Bruce~J Wittmann, Yisong Yue, and Frances~H Arnold.
\newblock Informed training set design enables efficient machine
  learning-assisted directed protein evolution.
\newblock {\em Cell Systems}, 12(11):1026--1045, 2021.

\bibitem{khurana2023natural}
Diksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh.
\newblock Natural language processing: State of the art, current trends and
  challenges.
\newblock {\em Multimedia tools and applications}, 82(3):3713--3744, 2023.

\bibitem{el2019pfam}
Sara El-Gebali, Jaina Mistry, Alex Bateman, Sean~R Eddy, Aur{\'e}lien Luciani,
  Simon~C Potter, Matloob Qureshi, Lorna~J Richardson, Gustavo~A Salazar,
  Alfredo Smart, et~al.
\newblock The pfam protein families database in 2019.
\newblock {\em Nucleic acids research}, 47(D1):D427--D432, 2019.

\bibitem{shihab2013predicting}
Hashem~A Shihab, Julian Gough, David~N Cooper, Peter~D Stenson, Gary~LA Barker,
  Keith~J Edwards, Ian~NM Day, and Tom~R Gaunt.
\newblock Predicting the functional, molecular, and phenotypic consequences of
  amino acid substitutions using hidden markov models.
\newblock {\em Human mutation}, 34(1):57--65, 2013.

\bibitem{hopf2017mutation}
Thomas~A Hopf, John~B Ingraham, Frank~J Poelwijk, Charlotta~PI Sch{\"a}rfe,
  Michael Springer, Chris Sander, and Debora~S Marks.
\newblock Mutation effects predicted from sequence co-variation.
\newblock {\em Nature biotechnology}, 35(2):128--135, 2017.

\bibitem{rao2021msa}
Roshan~M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter
  Abbeel, Tom Sercu, and Alexander Rives.
\newblock Msa transformer.
\newblock In {\em International Conference on Machine Learning}, pages
  8844--8856. PMLR, 2021.

\bibitem{frazer2021disease}
Jonathan Frazer, Pascal Notin, Mafalda Dias, Aidan Gomez, Joseph~K Min, Kelly
  Brock, Yarin Gal, and Debora~S Marks.
\newblock Disease variant prediction with deep generative models of
  evolutionary data.
\newblock {\em Nature}, 599(7883):91--95, 2021.

\bibitem{rao2019evaluating}
Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Xi~Chen, John Canny,
  Pieter Abbeel, and Yun~S Song.
\newblock Evaluating protein transfer learning with tape.
\newblock {\em Advances in Neural Information Processing Systems}, 32:9689,
  2019.

\bibitem{bepler2018learning}
Tristan Bepler and Bonnie Berger.
\newblock Learning protein sequence embeddings using information from
  structure.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{biswas2021low}
Surojit Biswas, Grigory Khimulya, Ethan~C Alley, Kevin~M Esvelt, and George~M
  Church.
\newblock Low-n protein engineering with data-efficient deep learning.
\newblock {\em Nature methods}, 18(4):389--396, 2021.

\bibitem{meier2021language}
Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives.
\newblock Language models enable zero-shot prediction of the effects of
  mutations on protein function.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{hsu2022esm}
Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam
  Lerer, and Alexander Rives.
\newblock Learning inverse folding from millions of predicted structures.
\newblock In {\em International Conference on Machine Learning}, pages
  8946--8970. PMLR, 2022.

\bibitem{orengo1997cath}
Christine~A Orengo, Alex~D Michie, Susan Jones, David~T Jones, Mark~B
  Swindells, and Janet~M Thornton.
\newblock Cath--a hierarchic classification of protein domain structures.
\newblock {\em Structure}, 5(8):1093--1109, 1997.

\bibitem{madani2023large}
Ali Madani, Ben Krause, Eric~R Greene, Subu Subramanian, Benjamin~P Mohr,
  James~M Holton, Jose~Luis Olmos~Jr, Caiming Xiong, Zachary~Z Sun, Richard
  Socher, et~al.
\newblock Large language models generate functional protein sequences across
  diverse families.
\newblock {\em Nature Biotechnology}, pages 1--8, 2023.

\bibitem{federhen2012ncbi}
Scott Federhen.
\newblock The ncbi taxonomy database.
\newblock {\em Nucleic acids research}, 40(D1):D136--D143, 2012.

\bibitem{brandes2022proteinbert}
Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial.
\newblock Proteinbert: a universal deep-learning model of protein sequence and
  function.
\newblock {\em Bioinformatics}, 38(8):2102--2110, 2022.

\bibitem{lin2023evolutionary}
Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita
  Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et~al.
\newblock Evolutionary-scale prediction of atomic-level protein structure with
  a language model.
\newblock {\em Science}, 379(6637):1123--1130, 2023.

\bibitem{eddy2011accelerated}
Sean~R Eddy.
\newblock Accelerated profile hmm searches.
\newblock {\em PLoS computational biology}, 7(10):e1002195, 2011.

\bibitem{hopf2019evcouplings}
Thomas~A Hopf, Anna~G Green, Benjamin Schubert, Sophia Mersmann, Charlotta~PI
  Sch{\"a}rfe, John~B Ingraham, Agnes Toth-Petroczy, Kelly Brock, Adam~J
  Riesselman, Perry Palmedo, et~al.
\newblock The evcouplings python framework for coevolutionary sequence
  analysis.
\newblock {\em Bioinformatics}, 35(9):1582--1584, 2019.

\bibitem{rabiner1989tutorial}
Lawrence~R Rabiner.
\newblock A tutorial on hidden markov models and selected applications in
  speech recognition.
\newblock {\em Proceedings of the IEEE}, 77(2):257--286, 1989.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{livesey2020using}
Benjamin~J Livesey and Joseph~A Marsh.
\newblock Using deep mutational scanning to benchmark variant effect predictors
  and identify disease mutations.
\newblock {\em Molecular systems biology}, 16(7):e9380, 2020.

\bibitem{kim-2014-convolutional}
Yoon Kim.
\newblock Convolutional neural networks for sentence classification.
\newblock In {\em Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pages 1746--1751, Doha, Qatar,
  October 2014. Association for Computational Linguistics.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{detlefsen2022learning}
Nicki~Skafte Detlefsen, S{\o}ren Hauberg, and Wouter Boomsma.
\newblock Learning meaningful representations of protein sequences.
\newblock {\em Nature communications}, 13(1):1914, 2022.

\bibitem{edelsbrunner2008persistent}
Herbert Edelsbrunner, John Harer, et~al.
\newblock Persistent homology-a survey.
\newblock {\em Contemporary mathematics}, 453(26):257--282, 2008.

\bibitem{zomorodian2004computing}
Afra Zomorodian and Gunnar Carlsson.
\newblock Computing persistent homology.
\newblock In {\em Proceedings of the twentieth annual symposium on
  Computational geometry}, pages 347--356, 2004.

\bibitem{cang2020persistent}
Zixuan Cang and Guo-Wei Wei.
\newblock Persistent cohomology for data with multicomponent heterogeneous
  information.
\newblock {\em SIAM journal on mathematics of data science}, 2(2):396--418,
  2020.

\bibitem{chowdhury2018persistent}
Samir Chowdhury and Facundo M{\'e}moli.
\newblock Persistent path homology of directed networks.
\newblock In {\em Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1152--1169. SIAM, 2018.

\bibitem{lutgehetmann2020computing}
Daniel L{\"u}tgehetmann, Dejan Govc, Jason~P Smith, and Ran Levi.
\newblock Computing persistent homology of directed flag complexes.
\newblock {\em Algorithms}, 13(1):19, 2020.

\bibitem{cang2020evolutionary}
Zixuan Cang, Elizabeth Munch, and Guo-Wei Wei.
\newblock Evolutionary homology on coupled dynamical systems with applications
  to protein flexibility analysis.
\newblock {\em Journal of applied and computational topology}, 4:481--507,
  2020.

\bibitem{meng2020weighted}
Zhenyu Meng, D~Vijay Anand, Yunpeng Lu, Jie Wu, and Kelin Xia.
\newblock Weighted persistent homology for biomolecular data analysis.
\newblock {\em Scientific reports}, 10(1):2079, 2020.

\bibitem{wang2020persistent}
Rui Wang, Duc~Duy Nguyen, and Guo-Wei Wei.
\newblock Persistent spectral graph.
\newblock {\em International journal for numerical methods in biomedical
  engineering}, 36(9):e3376, 2020.

\bibitem{memoli2022persistent}
Facundo M{\'e}moli, Zhengchao Wan, and Yusu Wang.
\newblock Persistent laplacians: Properties, algorithms and implications.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(2):858--884,
  2022.

\bibitem{chen2021evolutionary}
Jiahui Chen, Rundong Zhao, Yiying Tong, and Guo-Wei Wei.
\newblock Evolutionary de rham-hodge method.
\newblock {\em Discrete and continuous dynamical systems. Series B},
  26(7):3785, 2021.

\bibitem{wei2021persistent}
Xiaoqi Wei and Guo-Wei Wei.
\newblock Persistent sheaf laplacians.
\newblock {\em arXiv preprint arXiv:2112.10906}, 2021.

\bibitem{wang2023persistent}
Rui Wang and Guo-Wei Wei.
\newblock Persistent path laplacian.
\newblock {\em Foundations of Data Science}, 5:26--55, 2023.

\bibitem{liu2021persistent}
Xiang Liu, Huitao Feng, Jie Wu, and Kelin Xia.
\newblock Persistent spectral hypergraph based machine learning (psh-ml) for
  protein-ligand binding affinity prediction.
\newblock {\em Briefings in Bioinformatics}, 22(5):bbab127, 2021.

\bibitem{chen2023persistent}
Dong Chen, Jian Liu, Jie Wu, and Guo-Wei Wei.
\newblock Persistent hyperdigraph homology and persistent hyperdigraph
  laplacians.
\newblock {\em arXiv preprint arXiv:2304.00345}, 2023.

\bibitem{kaczynski2004computational}
Tomasz Kaczynski, Konstantin~Michael Mischaikow, and Marian Mrozek.
\newblock {\em Computational homology}, volume~3.
\newblock Springer, 2004.

\bibitem{wasserman2018topological}
Larry Wasserman.
\newblock Topological data analysis.
\newblock {\em Annual Review of Statistics and Its Application}, 5:501--532,
  2018.

\bibitem{ghrist2008barcodes}
Robert Ghrist.
\newblock Barcodes: the persistent topology of data.
\newblock {\em Bulletin of the American Mathematical Society}, 45(1):61--75,
  2008.

\bibitem{cohen2005stability}
David Cohen-Steiner, Herbert Edelsbrunner, and John Harer.
\newblock Stability of persistence diagrams.
\newblock In {\em Proceedings of the twenty-first annual symposium on
  Computational geometry}, pages 263--271, 2005.

\bibitem{bubenik2015statistical}
Peter Bubenik et~al.
\newblock Statistical topological data analysis using persistence landscapes.
\newblock {\em J. Mach. Learn. Res.}, 16(1):77--102, 2015.

\bibitem{adams2017persistence}
Henry Adams, Tegan Emerson, Michael Kirby, Rachel Neville, Chris Peterson,
  Patrick Shipman, Sofya Chepushtanova, Eric Hanson, Francis Motta, and Lori
  Ziegelmeier.
\newblock Persistence images: A stable vector representation of persistent
  homology.
\newblock {\em Journal of Machine Learning Research}, 18, 2017.

\bibitem{cang2015topological}
Zixuan Cang, Lin Mu, Kedi Wu, Kristopher Opron, Kelin Xia, and Guo-Wei Wei.
\newblock A topological approach for protein classification.
\newblock {\em Computational and Mathematical Biophysics}, 3(1), 2015.

\bibitem{clough2020topological}
James~R Clough, Nicholas Byrne, Ilkay Oksuz, Veronika~A Zimmer, Julia~A
  Schnabel, and Andrew~P King.
\newblock A topological loss function for deep-learning based image
  segmentation using persistent homology.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  44(12):8766--8778, 2020.

\bibitem{pun2018persistent}
Chi~Seng Pun, Kelin Xia, and Si~Xian Lee.
\newblock Persistent-homology-based machine learning and its applications--a
  survey.
\newblock {\em arXiv preprint arXiv:1811.00252}, 2018.

\bibitem{stolz2017persistent}
Bernadette~J Stolz, Heather~A Harrington, and Mason~A Porter.
\newblock Persistent homology of time-dependent functional networks constructed
  from coupled time series.
\newblock {\em Chaos: An Interdisciplinary Journal of Nonlinear Science},
  27(4):047410, 2017.

\bibitem{wei2023topological}
Guo-Wei Wei.
\newblock Topological data analysis hearing the shapes of drums and bells.
\newblock {\em arXiv preprint arXiv:2301.05025}, 2023.

\bibitem{nguyen2019mathematical}
Duc~Duy Nguyen, Zixuan Cang, Kedi Wu, Menglun Wang, Yin Cao, and Guo-Wei Wei.
\newblock Mathematical deep learning for pose and binding affinity prediction
  and ranking in d3r grand challenges.
\newblock {\em Journal of computer-aided molecular design}, 33:71--82, 2019.

\bibitem{chen2022persistent}
Jiahui Chen, Yuchi Qiu, Rui Wang, and Guo-Wei Wei.
\newblock Persistent laplacian projected omicron ba. 4 and ba. 5 to become new
  dominating variants.
\newblock {\em Computers in Biology and Medicine}, 151:106262, 2022.

\bibitem{meng2021persistent}
Zhenyu Meng and Kelin Xia.
\newblock Persistent spectral--based machine learning (perspect ml) for
  protein-ligand binding affinity prediction.
\newblock {\em Science Advances}, 7(19):eabc5329, 2021.

\bibitem{grigor2020path}
AA~Grigoryan, Yong Lin, Yu~V Muranov, and Shing-Tung Yau.
\newblock Path complexes and their homologies.
\newblock {\em Journal of Mathematical Sciences}, 248:564--599, 2020.

\bibitem{hansen2019toward}
Jakob Hansen and Robert Ghrist.
\newblock Toward a spectral theory of cellular sheaves.
\newblock {\em Journal of Applied and Computational Topology}, 3:315--358,
  2019.

\bibitem{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock {\em arXiv preprint arXiv:1609.02907}, 2016.

\bibitem{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{hamilton2017inductive}
Will Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock {\em arXiv preprint arXiv:1810.00826}, 2018.

\bibitem{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock {\em arXiv preprint arXiv:1511.05493}, 2015.

\bibitem{kipf2016variational}
Thomas~N Kipf and Max Welling.
\newblock Variational graph auto-encoders.
\newblock {\em arXiv preprint arXiv:1611.07308}, 2016.

\bibitem{velivckovic2018deep}
Petar Veli{\v{c}}kovi{\'c}, William Fedus, William~L Hamilton, Pietro Li{\`o},
  Yoshua Bengio, and R~Devon Hjelm.
\newblock Deep graph infomax.
\newblock {\em arXiv preprint arXiv:1809.10341}, 2018.

\bibitem{you2020graph}
Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang
  Shen.
\newblock Graph contrastive learning with augmentations.
\newblock {\em Advances in neural information processing systems},
  33:5812--5823, 2020.

\bibitem{rong2020self}
Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
  Junzhou Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12559--12571, 2020.

\bibitem{li2021structure}
Shuangli Li, Jingbo Zhou, Tong Xu, Liang Huang, Fan Wang, Haoyi Xiong, Weili
  Huang, Dejing Dou, and Hui Xiong.
\newblock Structure-aware interactive graph neural networks for the prediction
  of protein-ligand binding affinity.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 975--985, 2021.

\bibitem{gligorijevic2021structure}
Vladimir Gligorijevi{\'c}, P~Douglas Renfrew, Tomasz Kosciolek, Julia~Koehler
  Leman, Daniel Berenberg, Tommi Vatanen, Chris Chandler, Bryn~C Taylor, Ian~M
  Fisk, Hera Vlamakis, et~al.
\newblock Structure-based protein function prediction using graph convolutional
  networks.
\newblock {\em Nature communications}, 12(1):3168, 2021.

\bibitem{liu2021deep}
Xianggen Liu, Yunan Luo, Pengyong Li, Sen Song, and Jian Peng.
\newblock Deep geometric representations for modeling effects of mutations on
  protein-protein binding affinity.
\newblock {\em PLoS computational biology}, 17(8):e1009284, 2021.

\bibitem{shan2022deep}
Sisi Shan, Shitong Luo, Ziqing Yang, Junxian Hong, Yufeng Su, Fan Ding, Lili
  Fu, Chenyu Li, Peng Chen, Jianzhu Ma, et~al.
\newblock Deep learning guided optimization of human antibody against
  sars-cov-2 variants with broad neutralization.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(11):e2122954119, 2022.

\bibitem{zhang2022protein}
Zuobai Zhang, Minghao Xu, Arian Jamasb, Vijil Chenthamarakshan, Aurelie Lozano,
  Payel Das, and Jian Tang.
\newblock Protein representation learning by geometric structure pretraining.
\newblock {\em arXiv preprint arXiv:2203.06125}, 2022.

\bibitem{ingraham2019generative}
John Ingraham, Vikas Garg, Regina Barzilay, and Tommi Jaakkola.
\newblock Generative models for graph-based protein design.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{li2022orientation}
Jiahan Li, Shitong Luo, Congyue Deng, Chaoran Cheng, Jiaqi Guan, Leonidas
  Guibas, Jianzhu Ma, and Jian Peng.
\newblock Orientation-aware graph neural networks for protein structure
  representation learning.
\newblock 2022.

\bibitem{cang2017topologynet}
Zixuan Cang and Guo-Wei Wei.
\newblock Topologynet: Topology based deep convolutional and multi-task neural
  networks for biomolecular property predictions.
\newblock {\em PLoS computational biology}, 13(7):e1005690, 2017.

\bibitem{morris2019weisfeiler}
Christopher Morris, Martin Ritzert, Matthias Fey, William~L Hamilton, Jan~Eric
  Lenssen, Gaurav Rattan, and Martin Grohe.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 4602--4609, 2019.

\bibitem{ebli2020simplicial}
Stefania Ebli, Micha{\"e}l Defferrard, and Gard Spreemann.
\newblock Simplicial neural networks.
\newblock {\em arXiv preprint arXiv:2010.03633}, 2020.

\bibitem{wu2016adaptation}
Nicholas~C Wu, Lei Dai, C~Anders Olson, James~O Lloyd-Smith, and Ren Sun.
\newblock Adaptation in protein fitness landscapes is facilitated by indirect
  paths.
\newblock {\em Elife}, 5:e16965, 2016.

\bibitem{podgornaia2015pervasive}
Anna~I Podgornaia and Michael~T Laub.
\newblock Pervasive degeneracy and epistasis in a protein-protein interface.
\newblock {\em Science}, 347(6222):673--677, 2015.

\bibitem{zhang2023structural}
Yao Zhang, Yuhan Jiang, Kaifu Gao, Dexin Sui, Peixuan Yu, Min Su, Guo-Wei Wei,
  and Jian Hu.
\newblock Structural insights into the elevator-type transport mechanism of a
  bacterial zip metal transporter.
\newblock {\em Nature Communications}, 14(1):385, 2023.

\bibitem{freschlin2022machine}
Chase~R Freschlin, Sarah~A Fahlberg, and Philip~A Romero.
\newblock Machine learning to navigate fitness landscapes for protein
  engineering.
\newblock {\em Current Opinion in Biotechnology}, 75:102713, 2022.

\bibitem{hie2022adaptive}
Brian~L Hie and Kevin~K Yang.
\newblock Adaptive machine learning for protein engineering.
\newblock {\em Current opinion in structural biology}, 72:145--152, 2022.

\bibitem{tian2023sequence}
Xiaochen Tian, Ziyin Wang, Kevin~K Yang, Jin Su, Hanwen Du, Qiuguo Zheng,
  Guibing Guo, Min Yang, Fei Yang, and Fajie Yuan.
\newblock Sequence vs. structure: Delving deep into data driven protein
  function prediction.
\newblock {\em bioRxiv}, pages 2023--04, 2023.

\bibitem{fox2007improving}
Richard~J Fox, S~Christopher Davis, Emily~C Mundorff, Lisa~M Newman, Vesna
  Gavrilovic, Steven~K Ma, Loleta~M Chung, Charlene Ching, Sarena Tam, Sheela
  Muley, et~al.
\newblock Improving catalytic function by prosar-driven enzyme evolution.
\newblock {\em Nature biotechnology}, 25(3):338--344, 2007.

\bibitem{guo2008using}
Yanzhi Guo, Lezheng Yu, Zhining Wen, and Menglong Li.
\newblock Using support vector machine combined with auto covariance to predict
  protein--protein interactions from protein sequences.
\newblock {\em Nucleic acids research}, 36(9):3025--3030, 2008.

\bibitem{zhang2020mutabind2}
Ning Zhang, Yuting Chen, Haoyu Lu, Feiyang Zhao, Roberto~Vera Alvarez,
  Alexander Goncearenco, Anna~R Panchenko, and Minghui Li.
\newblock Mutabind2: predicting the impacts of single and multiple mutations on
  protein-protein interactions.
\newblock {\em Iscience}, 23(3):100939, 2020.

\bibitem{cang2017analysis}
Zixuan Cang and Guo-Wei Wei.
\newblock Analysis and prediction of protein folding energy changes upon
  mutation by element specific persistent homology.
\newblock {\em Bioinformatics}, 33(22):3549--3557, 2017.

\bibitem{aghazadeh2021epistatic}
Amirali Aghazadeh, Hunter Nisonoff, Orhan Ocal, David~H Brookes, Yijie Huang,
  O~Ozan Koyluoglu, Jennifer Listgarten, and Kannan Ramchandran.
\newblock Epistatic net allows the sparse spectral regularization of deep
  neural networks for inferring fitness functions.
\newblock {\em Nature communications}, 12(1):5225, 2021.

\bibitem{dallago2021flip}
Christian Dallago, Jody Mou, Kadina~E Johnston, Bruce~J Wittmann, Nicholas
  Bhattacharya, Samuel Goldman, Ali Madani, and Kevin~K Yang.
\newblock Flip: Benchmark tasks in fitness landscape inference for proteins.
\newblock {\em bioRxiv}, pages 2021--11, 2021.

\bibitem{bryant2021deep}
Drew~H Bryant, Ali Bashir, Sam Sinai, Nina~K Jain, Pierce~J Ogden, Patrick~F
  Riley, George~M Church, Lucy~J Colwell, and Eric~D Kelsic.
\newblock Deep diversification of an aav capsid protein by machine learning.
\newblock {\em Nature Biotechnology}, 39(6):691--696, 2021.

\bibitem{qiu2021cluster}
Yuchi Qiu, Jian Hu, and Guo-Wei Wei.
\newblock Cluster learning-assisted directed evolution.
\newblock {\em Nature Computational Science}, 1(12):809--818, 2021.

\bibitem{qiu2022clade}
Yuchi Qiu and Guo-Wei Wei.
\newblock Clade 2.0: Evolution-driven cluster learning-assisted directed
  evolution.
\newblock {\em Journal of Chemical Information and Modeling},
  62(19):4629--4641, 2022.

\bibitem{thean2022machine}
Dawn~GL Thean, Hoi~Yee Chu, John~HC Fong, Becky~KC Chan, Peng Zhou, Cynthia~CS
  Kwok, Yee~Man Chan, Silvia~YL Mak, Gigi~CG Choi, Joshua~WK Ho, et~al.
\newblock Machine learning-coupled combinatorial mutagenesis enables
  resource-efficient engineering of crispr-cas9 genome editor activities.
\newblock {\em Nature Communications}, 13(1):2219, 2022.

\bibitem{georgiev2009interpretable}
Alexander~G Georgiev.
\newblock Interpretable numerical descriptors of amino acid space.
\newblock {\em Journal of Computational Biology}, 16(5):703--723, 2009.

\bibitem{shen2022svsbi}
Li~Shen, Hongsong Feng, Yuchi Qiu, and Guo-Wei Wei.
\newblock Svsbi: Sequence-based virtual screening of biomolecular interactions.
\newblock {\em arXiv preprint arXiv:2212.13617}, 2022.

\bibitem{luo2021ecnet}
Yunan Luo, Guangde Jiang, Tianhao Yu, Yang Liu, Lam Vo, Hantian Ding, Yufeng
  Su, Wesley~Wei Qian, Huimin Zhao, and Jian Peng.
\newblock Ecnet is an evolutionary context-integrated deep learning framework
  for protein engineering.
\newblock {\em Nature communications}, 12(1):1--14, 2021.

\bibitem{greenman2022benchmarking}
Kevin~P Greenman, Ava Soleimany, and Kevin~K Yang.
\newblock Benchmarking uncertainty quantification for protein engineering.
\newblock In {\em ICLR2022 Machine Learning for Drug Discovery}, 2022.

\bibitem{rasmussen2003gaussian}
Carl~Edward Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In {\em Summer school on machine learning}, pages 63--71. Springer,
  2003.

\bibitem{bedbrook2019machine}
Claire~N Bedbrook, Kevin~K Yang, J~Elliott Robinson, Elisha~D Mackey, Viviana
  Gradinaru, and Frances~H Arnold.
\newblock Machine learning-guided channelrhodopsin engineering enables
  minimally invasive optogenetics.
\newblock {\em Nature methods}, 16(11):1176--1184, 2019.

\bibitem{bedbrook2017machine}
Claire~N Bedbrook, Kevin~K Yang, Austin~J Rice, Viviana Gradinaru, and
  Frances~H Arnold.
\newblock Machine learning to design integral membrane channelrhodopsins for
  efficient eukaryotic expression and plasma membrane localization.
\newblock {\em PLoS computational biology}, 13(10):e1005786, 2017.

\bibitem{saito2018machine}
Yutaka Saito, Misaki Oikawa, Hikaru Nakazawa, Teppei Niide, Tomoshi Kameda,
  Koji Tsuda, and Mitsuo Umetsu.
\newblock Machine-learning-guided mutagenesis for directed evolution of
  fluorescent proteins.
\newblock {\em ACS synthetic biology}, 7(9):2014--2022, 2018.

\bibitem{greenhalgh2021machine}
Jonathan~C Greenhalgh, Sarah~A Fahlberg, Brian~F Pfleger, and Philip~A Romero.
\newblock Machine learning-guided acyl-acp reductase engineering for improved
  in vivo fatty alcohol production.
\newblock {\em Nature communications}, 12(1):5825, 2021.

\bibitem{romero2013navigating}
Philip~A Romero, Andreas Krause, and Frances~H Arnold.
\newblock Navigating the protein fitness landscape with gaussian processes.
\newblock {\em Proceedings of the National Academy of Sciences},
  110(3):E193--E201, 2013.

\bibitem{bubeck2011x}
S{\'e}bastien Bubeck, R{\'e}mi Munos, Gilles Stoltz, and Csaba Szepesv{\'a}ri.
\newblock X-armed bandits.
\newblock {\em Journal of Machine Learning Research}, 12(5), 2011.

\bibitem{munos2011optimistic}
R{\'e}mi Munos.
\newblock Optimistic optimization of a deterministic function without the
  knowledge of its smoothness.
\newblock {\em Advances in neural information processing systems}, 24:783--791,
  2011.

\bibitem{creswell2018generative}
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta,
  and Anil~A Bharath.
\newblock Generative adversarial networks: An overview.
\newblock {\em IEEE signal processing magazine}, 35(1):53--65, 2018.

\bibitem{gupta2019feedback}
Anvita Gupta and James Zou.
\newblock Feedback gan for dna optimizes protein functions.
\newblock {\em Nature Machine Intelligence}, 1(2):105--111, 2019.

\bibitem{shin2021protein}
Jung-Eun Shin, Adam~J Riesselman, Aaron~W Kollasch, Conor McMahon, Elana Simon,
  Chris Sander, Aashish Manglik, Andrew~C Kruse, and Debora~S Marks.
\newblock Protein design and variant prediction using autoregressive generative
  models.
\newblock {\em Nature communications}, 12(1):2403, 2021.

\bibitem{bachas2022antibody}
Sharrol Bachas, Goran Rakocevic, David Spencer, Anand~V Sastry, Robel Haile,
  John~M Sutton, George Kasun, Andrew Stachyra, Jahir~M Gutierrez, Edriss
  Yassine, et~al.
\newblock Antibody optimization enabled by artificial intelligence predictions
  of binding affinity and naturalness.
\newblock {\em bioRxiv}, pages 2022--08, 2022.

\bibitem{castro2022transformer}
Egbert Castro, Abhinav Godavarthi, Julian Rubinfien, Kevin Givechian, Dhananjay
  Bhaskar, and Smita Krishnaswamy.
\newblock Transformer-based protein generation with regularized latent space
  optimization.
\newblock {\em Nature Machine Intelligence}, 4(10):840--851, 2022.

\bibitem{baek2021accurate}
Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey
  Ovchinnikov, Gyu~Rie Lee, Jue Wang, Qian Cong, Lisa~N Kinch, R~Dustin
  Schaeffer, et~al.
\newblock Accurate prediction of protein structures and interactions using a
  three-track neural network.
\newblock {\em Science}, 373(6557):871--876, 2021.

\bibitem{kandathil2023machine}
Shaun~M Kandathil, Andy~M Lau, and David~T Jones.
\newblock Machine learning methods for predicting protein structure from single
  sequences.
\newblock {\em Current Opinion in Structural Biology}, 81:102627, 2023.

\bibitem{chowdhury2022single}
Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant
  Kharkar, Koushik Roy, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang,
  George~M Church, et~al.
\newblock Single-sequence protein structure prediction using a language model
  and deep learning.
\newblock {\em Nature Biotechnology}, 40(11):1617--1623, 2022.

\bibitem{wang2022single}
Wenkai Wang, Zhenling Peng, and Jianyi Yang.
\newblock Single-sequence protein structure prediction using supervised
  transformer protein language models.
\newblock {\em Nature Computational Science}, 2(12):804--814, 2022.

\bibitem{wu2022high}
Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng
  Su, Zuofan Wu, Qi~Xie, Bonnie Berger, et~al.
\newblock High-resolution de novo structure prediction from primary sequence.
\newblock {\em BioRxiv}, pages 2022--07, 2022.

\bibitem{fang2022helixfold}
Xiaomin Fang, Fan Wang, Lihang Liu, Jingzhou He, Dayong Lin, Yingfei Xiang,
  Xiaonan Zhang, Hua Wu, Hui Li, and Le~Song.
\newblock Helixfold-single: Msa-free protein structure prediction by using
  protein language model as an alternative.
\newblock {\em arXiv preprint arXiv:2207.13921}, 2022.

\bibitem{barrett2022so}
Thomas~D Barrett, Amelia Villegas-Morcillo, Louis Robinson, Benoit Gaujac,
  David Admete, Elia Saquand, Karim Beguir, and Arthur Flajolet.
\newblock So manyfolds, so little time: Efficient protein structure prediction
  with plms and msas.
\newblock {\em bioRxiv}, pages 2022--10, 2022.

\bibitem{wu2022tfold}
Jiaxiang Wu, Fandi Wu, Biaobin Jiang, Wei Liu, and Peilin Zhao.
\newblock tfold-ab: fast and accurate antibody structure prediction without
  sequence homologs.
\newblock {\em bioRxiv}, pages 2022--11, 2022.

\bibitem{weissenow2022ultra}
Konstantin Weissenow, Michael Heinzinger, Martin Steinegger, and Burkhard Rost.
\newblock Ultra-fast protein structure prediction to capture effects of
  sequence variation in mutation movies.
\newblock {\em bioRxiv}, pages 2022--11, 2022.

\bibitem{bordin2022novel}
Nicola Bordin, Christian Dallago, Michael Heinzinger, Stephanie Kim, Maria
  Littmann, Clemens Rauer, Martin Steinegger, Burkhard Rost, and Christine
  Orengo.
\newblock Novel machine learning approaches revolutionize protein knowledge.
\newblock {\em Trends in Biochemical Sciences}, 2022.

\bibitem{chidyausiku2022novo}
Tamuka~M Chidyausiku, Soraia~R Mendes, Jason~C Klima, Marta Nadal, Ulrich
  Eckhard, Jorge Roel-Touris, Scott Houliston, Tibisay Guevara, Hugh~K Haddox,
  Adam Moyer, et~al.
\newblock De novo design of immunoglobulin-like domains.
\newblock {\em Nature Communications}, 13(1):5661, 2022.

\bibitem{keros2022dist2cycle}
Alexandros~D Keros, Vidit Nanda, and Kartic Subr.
\newblock Dist2cycle: A simplicial neural network for homology localization.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 7133--7142, 2022.

\bibitem{schuster2008next}
Stephan~C Schuster.
\newblock Next-generation sequencing transforms today's biology.
\newblock {\em Nature methods}, 5(1):16--18, 2008.

\bibitem{sarkisyan2016local}
Karen~S Sarkisyan, Dmitry~A Bolotin, Margarita~V Meer, Dinara~R Usmanova,
  Alexander~S Mishin, George~V Sharonov, Dmitry~N Ivankov, Nina~G Bozhanova,
  Mikhail~S Baranov, Onuralp Soylemez, et~al.
\newblock Local fitness landscape of the green fluorescent protein.
\newblock {\em Nature}, 533(7603):397--401, 2016.

\end{thebibliography}

\clearpage
\subfile{supplement}



\end{document}
