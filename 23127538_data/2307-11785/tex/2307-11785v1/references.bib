@article{DBLP:journals/corr/abs-2004-14507,
  author    = {Qingfu Zhu and
               Weinan Zhang and
               Ting Liu and
               William Yang Wang},
  title     = {Counterfactual Off-Policy Training for Neural Response Generation},
  journal   = {CoRR},
  volume    = {abs/2004.14507},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.14507},
  archivePrefix = {arXiv},
  eprint    = {2004.14507},
  timestamp = {Sun, 03 May 2020 17:39:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-14507.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-1811-06272,
  author    = {Lars Buesing and
               Theophane Weber and
               Yori Zwols and
               S{\'{e}}bastien Racani{\`{e}}re and
               Arthur Guez and
               Jean{-}Baptiste Lespiau and
               Nicolas Heess},
  title     = {Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search},
  journal   = {CoRR},
  volume    = {abs/1811.06272},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.06272},
  archivePrefix = {arXiv},
  eprint    = {1811.06272},
  timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-06272.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{johansson2018learning,
      title={Learning Representations for Counterfactual Inference}, 
      author={Fredrik D. Johansson and Uri Shalit and David Sontag},
      year={2018},
      eprint={1605.03661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@InProceedings{pmlr-v124-veitch20a, title = {Adapting Text Embeddings for Causal Inference}, author = {Veitch, Victor and Sridhar, Dhanya and Blei, David}, booktitle = {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)}, pages = {919--928}, year = {2020}, editor = {Jonas Peters and David Sontag}, volume = {124}, series = {Proceedings of Machine Learning Research}, month = {03--06 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v124/veitch20a/veitch20a.pdf}, url = { http://proceedings.mlr.press/v124/veitch20a.html }, abstract = {Does adding a theorem to a paper affect its chance of acceptance? Does labeling a post with the author’s gender affect the post popularity? This paper develops a method to estimate such causal effects from observational text data, adjusting for confounding features of the text such as the subject or writing quality. We assume that the text suffices for causal adjustment but that, in practice, it is prohibitively high-dimensional. To address this challenge, we develop causally sufficient embeddings, low- dimensional document representations that preserve sufficient information for causal identification and allow for efficient estimation of causal effects. Causally sufficient embeddings combine two ideas. The first is supervised dimensionality reduction: causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. The second is efficient language modeling: representations of text are designed to dispose of linguistically irrelevant information, and this information is also causally irrelevant. Our method adapts language models (specifically, word embeddings and topic models) to learn document embeddings that are able to predict both treatment and outcome. We study causally sufficient embeddings with semi-synthetic datasets and find that they improve causal estimation over related embedding methods. We illustrate the methods by answering the two motivating questions—the effect of a theorem on paper acceptance and the effect of a gender label on post popularity. Code and data available at github.com/vveitch/causal-text-embeddings-tf2.} }


@article{DBLP:journals/corr/abs-1910-10683,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {CoRR},
  volume    = {abs/1910.10683},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.10683},
  archivePrefix = {arXiv},
  eprint    = {1910.10683},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{ritter-etal-2011-data,
    title = "Data-Driven Response Generation in Social Media",
    author = "Ritter, Alan  and
      Cherry, Colin  and
      Dolan, William B.",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D11-1054",
    pages = "583--593",
}


@misc{shen2018improving,
      title={Improving Variational Encoder-Decoders in Dialogue Generation}, 
      author={Xiaoyu Shen and Hui Su and Shuzi Niu and Vera Demberg},
      year={2018},
      eprint={1802.02032},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{DBLP:journals/corr/abs-2104-03057,
  author    = {Chenxin An and
               Ming Zhong and
               Yiran Chen and
               Danqing Wang and
               Xipeng Qiu and
               Xuanjing Huang},
  title     = {Enhancing Scientific Papers Summarization with Citation Graph},
  journal   = {CoRR},
  volume    = {abs/2104.03057},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.03057},
  archivePrefix = {arXiv},
  eprint    = {2104.03057},
  timestamp = {Tue, 13 Apr 2021 16:46:17 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-03057.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{feng2021survey,
      title={A Survey of Data Augmentation Approaches for NLP}, 
      author={Steven Y. Feng and Varun Gangal and Jason Wei and Sarath Chandar and Soroush Vosoughi and Teruko Mitamura and Eduard Hovy},
      year={2021},
      eprint={2105.03075},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{labeau-cohen-2019-experimenting,
    title = "Experimenting with Power Divergences for Language Modeling",
    author = "Labeau, Matthieu  and
      Cohen, Shay B.",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1421",
    doi = "10.18653/v1/D19-1421",
    pages = "4104--4114",
    abstract = "Neural language models are usually trained using Maximum-Likelihood Estimation (MLE). The corresponding objective function for MLE is derived from the Kullback-Leibler (KL) divergence between the empirical probability distribution representing the data and the parametric probability distribution output by the model. However, the word frequency discrepancies in natural language make performance extremely uneven: while the perplexity is usually very low for frequent words, it is especially difficult to predict rare words. In this paper, we experiment with several families (alpha, beta and gamma) of power divergences, generalized from the KL divergence, for learning language models with an objective different than standard MLE. Intuitively, these divergences should affect the way the probability mass is spread during learning, notably by prioritizing performances on high or low-frequency words. In addition, we implement and experiment with various sampling-based objectives, where the computation of the output layer is only done on a small subset of the vocabulary. They are derived as power generalizations of a softmax approximated via Importance Sampling, and Noise Contrastive Estimation, for accelerated learning. Our experiments on the Penn Treebank and Wikitext-2 show that these power divergences can indeed be used to prioritize learning on the frequent or rare words, and lead to general performance improvements in the case of sampling-based learning.",
}

@inproceedings{xu-etal-2018-diversity,
    title = "Diversity-Promoting {GAN}: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation",
    author = "Xu, Jingjing  and
      Ren, Xuancheng  and
      Lin, Junyang  and
      Sun, Xu",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1428",
    doi = "10.18653/v1/D18-1428",
    pages = "3940--3949",
    abstract = "Existing text generation methods tend to produce repeated and {''}boring{''} expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for {''}novel{''} and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.",
}


@article{DBLP:journals/corr/LiMSRJ17,
  author    = {Jiwei Li and
               Will Monroe and
               Tianlin Shi and
               Alan Ritter and
               Dan Jurafsky},
  title     = {Adversarial Learning for Neural Dialogue Generation},
  journal   = {CoRR},
  volume    = {abs/1701.06547},
  year      = {2017},
  url       = {http://arxiv.org/abs/1701.06547},
  archivePrefix = {arXiv},
  eprint    = {1701.06547},
  timestamp = {Mon, 13 Aug 2018 16:46:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LiMSRJ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@misc{vinyals2015neural,
      title={A Neural Conversational Model}, 
      author={Oriol Vinyals and Quoc Le},
      year={2015},
      eprint={1506.05869},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{serban2016generative,
      title={Generative Deep Neural Networks for Dialogue: A Short Review}, 
      author={Iulian Vlad Serban and Ryan Lowe and Laurent Charlin and Joelle Pineau},
      year={2016},
      eprint={1611.06216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{luan2016lstm,
      title={LSTM based Conversation Models}, 
      author={Yi Luan and Yangfeng Ji and Mari Ostendorf},
      year={2016},
      eprint={1603.09457},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{miao-blunsom-2016-language,
    title = "Language as a Latent Variable: Discrete Generative Models for Sentence Compression",
    author = "Miao, Yishu  and
      Blunsom, Phil",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1031",
    doi = "10.18653/v1/D16-1031",
    pages = "319--328",
}


@InProceedings{pmlr-v70-wen17a, title = {Latent Intention Dialogue Models}, author = {Tsung-Hsien Wen and Yishu Miao and Phil Blunsom and Steve Young}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {3732--3741}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/wen17a/wen17a.pdf}, url = { http://proceedings.mlr.press/v70/wen17a.html }, abstract = {Developing a dialogue agent that is capable of making autonomous decisions and communicating by natural language is one of the long-term goals of machine learning research. The traditional approaches either rely on hand-crafting a small state-action set for applying reinforcement learning that is not scalable or constructing deterministic models for learning dialogue sentences that fail to capture the conversational stochasticity. In this paper, however, we propose a Latent Intention Dialogue Model that employs a discrete latent variable to learn underlying dialogue intentions in the framework of neural variational inference. Additionally, in a goal-oriented dialogue scenario, the latent intentions can be interpreted as actions guiding the generation of machine responses, which can be further refined autonomously by reinforcement learning. The experiments demonstrate the effectiveness of discrete latent variable models on learning goal-oriented dialogues, and the results outperform the published benchmarks on both corpus-based evaluation and human evaluation.} }


@misc{haidar2019textkdgan,
      title={TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks}, 
      author={Md. Akmal Haidar and Mehdi Rezagholizadeh},
      year={2019},
      eprint={1905.01976},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rajeswar2017adversarial,
      title={Adversarial Generation of Natural Language}, 
      author={Sai Rajeswar and Sandeep Subramanian and Francis Dutil and Christopher Pal and Aaron Courville},
      year={2017},
      eprint={1705.10929},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{luketina2019survey,
      title={A Survey of Reinforcement Learning Informed by Natural Language}, 
      author={Jelena Luketina and Nantas Nardelli and Gregory Farquhar and Jakob Foerster and Jacob Andreas and Edward Grefenstette and Shimon Whiteson and Tim Rocktäschel},
      year={2019},
      eprint={1906.03926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ramamurthy2020nlpgym,
      title={NLPGym -- A toolkit for evaluating RL agents on Natural Language Processing Tasks}, 
      author={Rajkumar Ramamurthy and Rafet Sifa and Christian Bauckhage},
      year={2020},
      eprint={2011.08272},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{LI2020103853,
title = {Transfer learning in computer vision tasks: Remember where you come from},
journal = {Image and Vision Computing},
volume = {93},
pages = {103853},
year = {2020},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2019.103853},
url = {https://www.sciencedirect.com/science/article/pii/S0262885619304469},
author = {Xuhong Li and Yves Grandvalet and Franck Davoine and Jingchun Cheng and Yin Cui and Hang Zhang and Serge Belongie and Yi-Hsuan Tsai and Ming-Hsuan Yang},
keywords = {Transfer learning, Parameter regularization, Computer vision},
abstract = {Fine-tuning pre-trained deep networks is a practical way of benefiting from the representation learned on a large database while having relatively few examples to train a model. This adjustment is nowadays routinely performed so as to benefit of the latest improvements of convolutional neural networks trained on large databases. Fine-tuning requires some form of regularization, which is typically implemented by weight decay that drives the network parameters towards zero. This choice conflicts with the motivation for fine-tuning, as starting from a pre-trained solution aims at taking advantage of the previously acquired knowledge. Hence, regularizers promoting an explicit inductive bias towards the pre-trained model have been recently proposed. This paper demonstrates the versatility of this type of regularizer across transfer learning scenarios. We replicated experiments on three state-of-the-art approaches in image classification, image segmentation, and video analysis to compare the relative merits of regularizers. These tests show systematic improvements compared to weight decay. Our experimental protocol put forward the versatility of a regularizer that is easy to implement and to operate that we eventually recommend as the new baseline for future approaches to transfer learning relying on fine-tuning.}
}


@inproceedings{ruder-etal-2019-transfer,
    title = "Transfer Learning in Natural Language Processing",
    author = "Ruder, Sebastian  and
      Peters, Matthew E.  and
      Swayamdipta, Swabha  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorials",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-5004",
    doi = "10.18653/v1/N19-5004",
    pages = "15--18",
    abstract = "The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.",
}


@misc{raffel2020exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}




@misc{kaushik2020learning,
      title={Learning the Difference that Makes a Difference with Counterfactually-Augmented Data}, 
      author={Divyansh Kaushik and Eduard Hovy and Zachary C. Lipton},
      year={2020},
      eprint={1909.12434},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{zmigrod-etal-2019-counterfactual,
    title = "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
    author = "Zmigrod, Ran  and
      Mielke, Sabrina J.  and
      Wallach, Hanna  and
      Cotterell, Ryan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1161",
    doi = "10.18653/v1/P19-1161",
    pages = "1651--1661",
    abstract = "Gender stereotypes are manifest in most of the world{'}s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82{\%} and 73{\%} at the level of tags and accuracies of 90{\%} and 87{\%} at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
}



@misc{li2015hierarchical,
      title={A Hierarchical Neural Autoencoder for Paragraphs and Documents}, 
      author={Jiwei Li and Minh-Thang Luong and Dan Jurafsky},
      year={2015},
      eprint={1506.01057},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{jin2021bidirectional,
      title={Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation}, 
      author={Chen Jin and Zhuangwei Shi and Weihua Li and Yanbu Guo},
      year={2021},
      eprint={2105.09681},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{luong2015effective,
      title={Effective Approaches to Attention-based Neural Machine Translation}, 
      author={Minh-Thang Luong and Hieu Pham and Christopher D. Manning},
      year={2015},
      eprint={1508.04025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2017dailydialog,
      title={DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset}, 
      author={Yanran Li and Hui Su and Xiaoyu Shen and Wenjie Li and Ziqiang Cao and Shuzi Niu},
      year={2017},
      eprint={1710.03957},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{goodfellow2014generative,
      title={Generative Adversarial Networks}, 
      author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
      year={2014},
      eprint={1406.2661},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{radford2016unsupervised,
      title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}, 
      author={Alec Radford and Luke Metz and Soumith Chintala},
      year={2016},
      eprint={1511.06434},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{NIPS2016_7c9d0b1f,
 author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf},
 volume = {29},
 year = {2016}
}




@misc{glover2016modeling,
      title={Modeling documents with Generative Adversarial Networks}, 
      author={John Glover},
      year={2016},
      eprint={1612.09122},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Li2017AdversarialLF,
  title={Adversarial Learning for Neural Dialogue Generation},
  author={J. Li and Will Monroe and Tianlin Shi and S{\'e}bastien Jean and Alan Ritter and Dan Jurafsky},
  journal={ArXiv},
  year={2017},
  volume={abs/1701.06547}
}


@misc{yu2017seqgan,
      title={SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient}, 
      author={Lantao Yu and Weinan Zhang and Jun Wang and Yong Yu},
      year={2017},
      eprint={1609.05473},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sutskever2014sequence,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ranzato2016sequence,
      title={Sequence Level Training with Recurrent Neural Networks}, 
      author={Marc'Aurelio Ranzato and Sumit Chopra and Michael Auli and Wojciech Zaremba},
      year={2016},
      eprint={1511.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lamb2016professor,
      title={Professor Forcing: A New Algorithm for Training Recurrent Networks}, 
      author={Alex Lamb and Anirudh Goyal and Ying Zhang and Saizheng Zhang and Aaron Courville and Yoshua Bengio},
      year={2016},
      eprint={1610.09038},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Schmidt19,
  author={Florian Schmidt},
  title={Generalization in Generation: A closer look at Exposure Bias},
  year={2019},
  cdate={1546300800000},
  pages={157-167},
  url={https://doi.org/10.18653/v1/D19-5616},
  booktitle={NGT@EMNLP-IJCNLP}
}



@INPROCEEDINGS{Williams92simplestatistical,
    author = {Ronald J. Williams},
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {229--256}
}


@misc{GPT,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@INBOOK{RNNReport,
  author={Rumelhart, David E. and McClelland, James L.},
  booktitle={Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}, 
  title={Learning Internal Representations by Error Propagation}, 
  year={1987},
  volume={},
  number={},
  pages={318-362},
  doi={}}
  
  @article{LSTMPaper,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}


@misc{GRUPaper,
      title={Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}, 
      author={Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
      year={2014},
      eprint={1412.3555},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@ARTICLE{BiDir,
  author={Schuster, M. and Paliwal, K.K.},
  journal={IEEE Transactions on Signal Processing}, 
  title={Bidirectional recurrent neural networks}, 
  year={1997},
  volume={45},
  number={11},
  pages={2673-2681},
  doi={10.1109/78.650093}}