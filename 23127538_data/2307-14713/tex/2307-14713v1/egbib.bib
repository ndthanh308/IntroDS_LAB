@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{liu2020disentangling,
  title={Disentangling and Unifying Graph Convolutions for Skeleton-Based Action Recognition},
  author={Liu, Ziyu and Zhang, Hongwen and Chen, Zhenghao and Wang, Zhiyong and Ouyang, Wanli},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={143--152},
  year={2020}
}

@article{medical-gait,
author = {Jana, Shantanu and Das, Nibaran and Basu, Subhadip and Nasipuri, Mita},
year = {2021},
month = {11},
pages = {1-20},
title = {Survey of Human Gait Analysis and Recognition for Medical and Forensic Applications},
volume = {13},
journal = {International Journal of Digital Crime and Forensics},
doi = {10.4018/IJDCF.289432}
}

@article{chen2020simple,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}

@article{sports-gait,
author = {Erin Ross and Anthony Milian and Mason Ferlic and Samuel Reed and Adam S. Lepley},
title ={A Data-Driven Approach to Running Gait Assessment Using Inertial Measurement Units},
journal = {Video Journal of Sports Medicine},
volume = {2},
number = {5},
pages = {26350254221102464},
year = {2022},
doi = {10.1177/26350254221102464},
URL = {https://doi.org/10.1177/26350254221102464},
eprint = { https://doi.org/10.1177/26350254221102464},
abstract = { Background:Running is an extremely common exercise, both recreationally and competitively. Combined with clinical assessment, technology-driven biomechanical gait analysis can be used to examine markers of performance and injury risk in runners.Indications:The indication is to provide clinicians and sports science researchers a framework for using inertial measurement units (IMU) for data-driven, quantitative gait assessments.Technique Description:This video details practical application of IMU use in biomechanical gait assessments. Details on participant and equipment setup, in-session protocols, and selection of gait variables are included.Results:Following collection of demographic and anthropometric outcomes, IMUs should be placed on rigid segments of the lower extremity, sacrum, and trunk. In our model, we place IMUs on the foot, shank, thigh, sacrum, and lower thoracic spine. Following static anatomical calibration, running gait biomechanics are evaluated at multiple speeds using IMUs, 2-dimensional high-speed video cameras, and an instrumented treadmill. The high-speed video and IMU data are analyzed together at various parts of the gait cycle, including foot strike, mid-stance, toe-off, and flight. Many kinematic and kinetic variables (ie, unilateral discrete joint angles, joint excursions, joint moments, spatiotemporal outcomes, etc) can be selected for analysis, ideally via a collaboration between the sports science, athletic, and sports medicine teams. A collaborative approach should also be used to determine how this information will be used to alter training programs or influence injury risk in the running athlete.Discussion/Conclusion:This report details how to use a data-driven approach to evaluate running gait biomechanics using IMU technology. This framework for gait analysis is most applicable, and effective, when the team of researchers works in conjunction with coaches, sport scientists, and athletes. Utilizing this framework, training can be adapted based on the objective and clinical assessment to reduce injury risk and improve performance in the gait assessment. }
}


@Article{s21248387,
AUTHOR = {Cosma, Adrian and Radoi, Ion Emilian},
TITLE = {WildGait: Learning Gait Representations from Raw Surveillance Streams},
JOURNAL = {Sensors},
VOLUME = {21},
YEAR = {2021},
NUMBER = {24},
ARTICLE-NUMBER = {8387},
URL = {https://www.mdpi.com/1424-8220/21/24/8387},
PubMedID = {34960479},
ISSN = {1424-8220},
ABSTRACT = {The use of gait for person identification has important advantages such as being non-invasive, unobtrusive, not requiring cooperation and being less likely to be obscured compared to other biometrics. Existing methods for gait recognition require cooperative gait scenarios, in which a single person is walking multiple times in a straight line in front of a camera. We address the challenges of real-world scenarios in which camera feeds capture multiple people, who in most cases pass in front of the camera only once. We address privacy concerns by using only motion information of walking individuals, with no identifiable appearance-based information. As such, we propose a self-supervised learning framework, WildGait, which consists of pre-training a Spatio-Temporal Graph Convolutional Network on a large number of automatically annotated skeleton sequences obtained from raw, real-world surveillance streams to learn useful gait signatures. We collected and compiled the largest pretraining dataset to date of anonymized walking skeletons called Uncooperative Wild Gait, containing over 38k tracklets of anonymized walking 2D skeletons. We make the dataset available to the research community. Our results surpass the current state-of-the-art pose-based gait recognition solutions. Our proposed method is reliable in training gait recognition methods in unconstrained environments, especially in settings with scarce amounts of annotated data.},
DOI = {10.3390/s21248387}
}

@ARTICLE{reid-survey,
author = {M. Ye and J. Shen and G. Lin and T. Xiang and L. Shao and S. H. Hoi},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title = {Deep Learning for Person Re-Identification: A Survey and Outlook},
year = {2022},
volume = {44},
number = {06},
issn = {1939-3539},
pages = {2872-2893},
abstract = {Person re-identification (Re-ID) aims at retrieving a person of interest across multiple non-overlapping cameras. With the advancement of deep neural networks and increasing demand of intelligent video surveillance, it has gained significantly increased interest in the computer vision community. By dissecting the involved components in developing a person Re-ID system, we categorize it into the closed-world and open-world settings. The widely studied closed-world setting is usually applied under various research-oriented assumptions, and has achieved inspiring success using deep learning techniques on a number of datasets. We first conduct a comprehensive overview with in-depth analysis for closed-world person Re-ID from three different perspectives, including deep feature representation learning, deep metric learning and ranking optimization. With the performance saturation under closed-world setting, the research focus for person Re-ID has recently shifted to the open-world setting, facing more challenging issues. This setting is closer to practical applications under specific scenarios. We summarize the open-world Re-ID in terms of five different aspects. By analyzing the advantages of existing methods, we design a powerful AGW baseline, achieving state-of-the-art or at least comparable performance on twelve datasets for four different Re-ID tasks. Meanwhile, we introduce a new evaluation metric (mINP) for person Re-ID, indicating the cost for finding all the correct matches, which provides an additional criteria to evaluate the Re-ID system for real applications. Finally, some important yet under-investigated open issues are discussed.},
keywords = {annotations;cameras;training;training data;feature extraction;data models;deep learning},
doi = {10.1109/TPAMI.2021.3054775},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}


@ARTICLE{gait-survey,
AUTHOR={Harris, Elsa J. and Khoo, I-Hung and Demircan, Emel},   
TITLE={A Survey of Human Gait-Based Artificial Intelligence Applications},      
JOURNAL={Frontiers in Robotics and AI},      
VOLUME={8},           
YEAR={2022},       
URL={https://www.frontiersin.org/articles/10.3389/frobt.2021.749274},       
DOI={10.3389/frobt.2021.749274},      
ISSN={2296-9144},   
ABSTRACT={We performed an electronic database search of published works from 2012 to mid-2021 that focus on human gait studies and apply machine learning techniques. We identified six key applications of machine learning using gait data: 1) Gait analysis where analyzing techniques and certain biomechanical analysis factors are improved by utilizing artificial intelligence algorithms, 2) Health and Wellness, with applications in gait monitoring for abnormal gait detection, recognition of human activities, fall detection and sports performance, 3) Human Pose Tracking using one-person or multi-person tracking and localization systems such as OpenPose, Simultaneous Localization and Mapping (SLAM), etc., 4) Gait-based biometrics with applications in person identification, authentication, and re-identification as well as gender and age recognition 5) “Smart gait” applications ranging from smart socks, shoes, and other wearables to smart homes and smart retail stores that incorporate continuous monitoring and control systems and 6) Animation that reconstructs human motion utilizing gait data, simulation and machine learning techniques. Our goal is to provide a single broad-based survey of the applications of machine learning technology in gait analysis and identify future areas of potential study and growth. We discuss the machine learning techniques that have been used with a focus on the tasks they perform, the problems they attempt to solve, and the trade-offs they navigate.}
}

@inproceedings{sanyal2021learning,
  title={Learning realistic human reposing using cyclic self-supervision with 3d shape, pose, and appearance consistency},
  author={Sanyal, Soubhik and Vorobiov, Alex and Bolkart, Timo and Loper, Matthew and Mohler, Betty and Davis, Larry S and Romero, Javier and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11138--11147},
  year={2021}
}

@Article{gait-vit,
AUTHOR = {Cosma, Adrian and Catruna, Andy and Radoi, Emilian},
TITLE = {Exploring Self-Supervised Vision Transformers for Gait Recognition in the Wild},
JOURNAL = {Sensors},
VOLUME = {23},
YEAR = {2023},
NUMBER = {5},
ARTICLE-NUMBER = {2680},
URL = {https://www.mdpi.com/1424-8220/23/5/2680},
PubMedID = {36904884},
ISSN = {1424-8220},
ABSTRACT = {The manner of walking (gait) is a powerful biometric that is used as a unique fingerprinting method, allowing unobtrusive behavioral analytics to be performed at a distance without subject cooperation. As opposed to more traditional biometric authentication methods, gait analysis does not require explicit cooperation of the subject and can be performed in low-resolution settings, without requiring the subject&rsquo;s face to be unobstructed/clearly visible. Most current approaches are developed in a controlled setting, with clean, gold-standard annotated data, which powered the development of neural architectures for recognition and classification. Only recently has gait analysis ventured into using more diverse, large-scale, and realistic datasets to pretrained networks in a self-supervised manner. Self-supervised training regime enables learning diverse and robust gait representations without expensive manual human annotations. Prompted by the ubiquitous use of the transformer model in all areas of deep learning, including computer vision, in this work, we explore the use of five different vision transformer architectures directly applied to self-supervised gait recognition. We adapt and pretrain the simple ViT, CaiT, CrossFormer, Token2Token, and TwinsSVT on two different large-scale gait datasets: GREW and DenseGait. We provide extensive results for zero-shot and fine-tuning on two benchmark gait recognition datasets, CASIA-B and FVG, and explore the relationship between the amount of spatial and temporal gait information used by the visual transformer. Our results show that in designing transformer models for processing motion, using a hierarchical approach (i.e., CrossFormer models) on finer-grained movement fairs comparatively better than previous whole-skeleton approaches.},
DOI = {10.3390/s23052680}
}




@inproceedings{fu-etal-2019-cyclical,
    title = "Cyclical Annealing Schedule: A Simple Approach to Mitigating {KL} Vanishing",
    author = "Fu, Hao  and
      Li, Chunyuan  and
      Liu, Xiaodong  and
      Gao, Jianfeng  and
      Celikyilmaz, Asli  and
      Carin, Lawrence",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1021",
    doi = "10.18653/v1/N19-1021",
    pages = "240--250",
    abstract = "Variational autoencoders (VAE) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. VAE objective consists of two terms, the KL regularization term and the reconstruction term, balanced by a weighting hyper-parameter $\beta$. One notorious training difficulty is that the KL term tends to vanish. In this paper we study different scheduling schemes for $\beta$, and show that KL vanishing is caused by the lack of good latent codes in training decoder at the beginning of optimization. To remedy the issue, we propose a cyclical annealing schedule, which simply repeats the process of increasing $\beta$ multiple times. This new procedure allows us to learn more meaningful latent codes progressively by leveraging the results of previous learning cycles as warm re-restart. The effectiveness of cyclical annealing schedule is validated on a broad range of NLP tasks, including language modeling, dialog response generation and semi-supervised text classification.",
}

@article{mercatali2021disentangling,
  title={Disentangling generative factors in natural language with discrete variational autoencoders},
  author={Mercatali, Giangiacomo and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2109.07169},
  year={2021}
}

@inproceedings{girshick2015fast,
  title={Fast r-cnn},
  author={Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1440--1448},
  year={2015}
}

@inproceedings{shi2019two,
  title={Two-stream adaptive graph convolutional networks for skeleton-based action recognition},
  author={Shi, Lei and Zhang, Yifan and Cheng, Jian and Lu, Hanqing},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12026--12035},
  year={2019}
}

@article{razavi2019generating,
  title={Generating diverse high-fidelity images with vq-vae-2},
  author={Razavi, Ali and Van den Oord, Aaron and Vinyals, Oriol},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{guo2022contrastive,
  title={Contrastive learning from extremely augmented skeleton sequences for self-supervised action recognition},
  author={Guo, Tianyu and Liu, Hong and Chen, Zhan and Liu, Mengyuan and Wang, Tao and Ding, Runwei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={1},
  pages={762--770},
  year={2022}
}

@article{10.1145/3490235,
author = {Filipi Gon\c{c}alves dos Santos, Claudio and Oliveira, Diego de Souza and A. Passos, Leandro and Gon\c{c}alves Pires, Rafael and Felipe Silva Santos, Daniel and Pascotti Valem, Lucas and P. Moreira, Thierry and Cleison S. Santana, Marcos and Roder, Mateus and Paulo Papa, Jo and Colombo, Danilo},
title = {Gait Recognition Based on Deep Learning: A Survey},
year = {2022},
issue_date = {February 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3490235},
doi = {10.1145/3490235},
abstract = {In general, biometry-based control systems may not rely on individual expected behavior or cooperation to operate appropriately. Instead, such systems should be aware of malicious procedures for unauthorized access attempts. Some works available in the literature suggest addressing the problem through gait recognition approaches. Such methods aim at identifying human beings through intrinsic perceptible features, despite dressed clothes or accessories. Although the issue denotes a relatively long-time challenge, most of the techniques developed to handle the problem present several drawbacks related to feature extraction and low classification rates, among other issues. However, deep learning-based approaches recently emerged as a robust set of tools to deal with virtually any image and computer-vision-related problem, providing paramount results for gait recognition as well. Therefore, this work provides a surveyed compilation of recent works regarding biometric detection through gait recognition with a focus on deep learning approaches, emphasizing their benefits and exposing their weaknesses. Besides, it also presents categorized and characterized descriptions of the datasets, approaches, and architectures employed to tackle associated constraints.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {34},
numpages = {34},
keywords = {deep learning, biometrics, Gait recognition}
}

@article{tian2020makes,
  title={What makes for good views for contrastive learning?},
  author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6827--6839},
  year={2020}
}

@article{flamary2021pot,
  author  = {R{\'e}mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur{\'e}lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L{\'e}o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
  title   = {POT: Python Optimal Transport},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {78},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-451.html}
}

@inproceedings{bonneel2011displacement,
  title={Displacement interpolation using Lagrangian mass transport},
  author={Bonneel, Nicolas and Van De Panne, Michiel and Paris, Sylvain and Heidrich, Wolfgang},
  booktitle={Proceedings of the 2011 SIGGRAPH Asia conference},
  pages={1--12},
  year={2011}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{gupta2021quo,
  title={Quo vadis, skeleton action recognition?},
  author={Gupta, Pranay and Thatipelli, Anirudh and Aggarwal, Aditya and Maheshwari, Shubh and Trivedi, Neel and Das, Sourav and Sarvadevabhatla, Ravi Kiran},
  journal={International Journal of Computer Vision},
  volume={129},
  number={7},
  pages={2097--2112},
  year={2021},
  publisher={Springer}
}

@misc{xing2021deep,
  title={Deep learning-based action recognition with 3D skeleton: a survey},
  author={Xing, Yuling and Zhu, Jia},
  year={2021},
  publisher={Wiley Online Library}
}

@article{ouisir,
	author =	{Weizhi An and Shiqi Yu and Yasushi Makihara and Xinhui Wu and Chi Xu and Yang Yu and Rijun Liao and Yasushi Yagi},
	title =	{Performance Evaluation of Model-based Gait on Multi-view Very Large Population Database with Pose Sequences},
	journal =	{IEEE Trans. on Biometrics, Behavior, and Identity Science},	
	year =	{2020},
}

@inproceedings{yan2018spatial,
  title={Spatial temporal graph convolutional networks for skeleton-based action recognition},
  author={Yan, Sijie and Xiong, Yuanjun and Lin, Dahua},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{tevet2022motionclip,
  title={Motionclip: Exposing human motion generation to clip space},
  author={Tevet, Guy and Gordon, Brian and Hertz, Amir and Bermano, Amit H and Cohen-Or, Daniel},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXII},
  pages={358--374},
  year={2022},
  organization={Springer}
}

@InProceedings{temos-eccv-2022,
author="Petrovich, Mathis
and Black, Michael J.
and Varol, G{\"u}l",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="TEMOS: Generating Diverse Human Motions from Textual Descriptions",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="480--497",
abstract="We address the problem of generating diverse 3D human motions from textual descriptions. This challenging task requires joint modeling of both modalities: understanding and extracting useful human-centric information from the text, and then generating plausible and realistic sequences of human poses. In contrast to most previous work which focuses on generating a single, deterministic, motion from a textual description, we design a variational approach that can produce multiple diverse human motions. We propose TEMOS, a text-conditioned generative model leveraging variational autoencoder (VAE) training with human motion data, in combination with a text encoder that produces distribution parameters compatible with the VAE latent space. We show the TEMOS framework can produce both skeleton-based animations as in prior work, as well more expressive SMPL body motions. We evaluate our approach on the KIT Motion-Language benchmark and, despite being relatively straightforward, demonstrate significant improvements over the state of the art. Code and models are available on our webpage.",
isbn="978-3-031-20047-2"
}



@inproceedings{li2022danceformer,
  title={Danceformer: Music conditioned 3d dance generation with parametric motion transformer},
  author={Li, Buyu and Zhao, Yongchi and Zhelun, Shi and Sheng, Lu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={2},
  pages={1272--1279},
  year={2022}
}

@inproceedings{yang2018pose,
  title={Pose guided human video generation},
  author={Yang, Ceyuan and Wang, Zhe and Zhu, Xinge and Huang, Chen and Shi, Jianping and Lin, Dahua},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={201--216},
  year={2018}
}

@inproceedings{wang2022towards,
  title={Towards diverse and natural scene-aware 3d human motion synthesis},
  author={Wang, Jingbo and Rong, Yu and Liu, Jingyuan and Yan, Sijie and Lin, Dahua and Dai, Bo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={20460--20469},
  year={2022}
}

@article{zhang2020perpetual,
  title={Perpetual motion: Generating unbounded human motion},
  author={Zhang, Yan and Black, Michael J and Tang, Siyu},
  journal={arXiv preprint arXiv:2007.13886},
  year={2020}
}

@article{raab2023single,
  title={Single Motion Diffusion},
  author={Raab, Sigal and Leibovitch, Inbal and Tevet, Guy and Arar, Moab and Bermano, Amit H and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2302.05905},
  year={2023}
}

@inproceedings{petrovich2021action,
  title={Action-conditioned 3D human motion synthesis with transformer VAE},
  author={Petrovich, Mathis and Black, Michael J and Varol, G{\"u}l},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10985--10995},
  year={2021}
}

@inproceedings{liu2022learning,
  title={Learning hierarchical cross-modal association for co-speech gesture generation},
  author={Liu, Xian and Wu, Qianyi and Zhou, Hang and Xu, Yinghao and Qian, Rui and Lin, Xinyi and Zhou, Xiaowei and Wu, Wayne and Dai, Bo and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10462--10472},
  year={2022}
}

@inproceedings{li2022skeleton2humanoid,
  title={Skeleton2Humanoid: Animating Simulated Characters for Physically-plausible Motion In-betweening},
  author={Li, Yunhao and Yu, Zhenbo and Zhu, Yucheng and Ni, Bingbing and Zhai, Guangtao and Shen, Wei},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={1493--1502},
  year={2022}
}

@INPROCEEDINGS{gaitgan,
  author={Yu, Shiqi and Chen, Haifeng and Reyes, Edel B. García and Poh, Norman},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)}, 
  title={GaitGAN: Invariant Gait Feature Extraction Using Generative Adversarial Networks}, 
  year={2017},
  volume={},
  number={},
  pages={532-539},
  doi={10.1109/CVPRW.2017.80}}

@inproceedings{guo2023back,
  title={Back to mlp: A simple baseline for human motion prediction},
  author={Guo, Wen and Du, Yuming and Shen, Xi and Lepetit, Vincent and Alameda-Pineda, Xavier and Moreno-Noguer, Francesc},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4809--4819},
  year={2023}
}

@inproceedings{gaitgraph,
  author={Teepe, Torben and Khan, Ali and Gilg, Johannes and Herzog, Fabian and H\"ormann, Stefan and Rigoll, Gerhard},
  booktitle={2021 IEEE International Conference on Image Processing (ICIP)}, 
  title={Gait{G}raph: Graph Convolutional Network for Skeleton-Based Gait Recognition}, 
  year={2021},
  pages={2314-2318},
  doi={10.1109/ICIP42928.2021.9506717}
}

@ARTICLE{chao2019gaitset,
  author={Chao, Hanqing and Wang, Kun and He, Yiwei and Zhang, Junping and Feng, Jianfeng},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={GaitSet: Cross-view Gait Recognition through Utilizing Gait as a Deep Set}, 
  year={2021},
  pages={1-1},
  doi={10.1109/TPAMI.2021.3057879}}

@article{lin2022gaitgl,
  title={GaitGL: Learning Discriminative Global-Local Feature Representations for Gait Recognition},
  author={Lin, Beibei and Zhang, Shunli and Wang, Ming and Li, Lincheng and Yu, Xin},
  journal={arXiv preprint arXiv:2208.01380},
  year={2022}
}

@inproceedings{wang2020self,
  title={Self-supervised video representation learning by pace prediction},
  author={Wang, Jiangliu and Jiao, Jianbo and Liu, Yun-Hui},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XVII 16},
  pages={504--521},
  year={2020},
  organization={Springer}
}

@ARTICLE{9721551,
  author={Li, Na and Zhao, Xinbo},
  journal={IEEE Transactions on Multimedia}, 
  title={A Strong and Robust Skeleton-based Gait Recognition Method with Gait Periodicity Priors}, 
  year={2022},
  volume={},
  number={},
  pages={1-1},
  doi={10.1109/TMM.2022.3154609}}

@Misc{gopinath2020fairmotion,
  author =       {Gopinath, Deepak and Won, Jungdam},
  title =        {fairmotion - Tools to load, process and visualize motion capture data},
  howpublished = {Github},
  year =         {2020},
  url =          {https://github.com/facebookresearch/fairmotion}
}

@misc{li2021learn,
      title={Learn to Dance with AIST++: Music Conditioned 3D Dance Generation}, 
      author={Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
      year={2021},
      eprint={2101.08779},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{siyao2022bailando,
    title={Bailando: 3D dance generation via Actor-Critic GPT with Choreographic Memory},
    author={Siyao, Li and Yu, Weijiang and Gu, Tianpei and Lin, Chunze and Wang, Quan and Qian, Chen and Loy, Chen Change and Liu, Ziwei},
    booktitle={CVPR},
    year={2022}
}

@incollection{maiorca2022evaluating,
  title={Evaluating the Quality of a Synthesized Motion with the Fr{\'e}chet Motion Distance},
  author={Maiorca, Antoine and Yoon, Youngwoo and Dutoit, Thierry},
  booktitle={ACM SIGGRAPH 2022 Posters},
  pages={1--2},
  year={2022}
}

@article{shin2021translation,
  title={Translation-equivariant image quantizer for bi-directional image-text generation},
  author={Shin, Woncheol and Lee, Gyubok and Lee, Jiyoung and Lee, Joonseok and Choi, Edward},
  journal={arXiv preprint arXiv:2112.00384},
  year={2021}
}

@article{yu2021vector,
  title={Vector-quantized image modeling with improved VQGAN},
  author={Yu, Jiahui and Li, Xin and Koh, Jing Yu and Zhang, Han and Pang, Ruoming and Qin, James and Ku, Alexander and Xu, Yuanzhong and Baldridge, Jason and Wu, Yonghui},
  journal={arXiv preprint arXiv:2110.04627},
  year={2021}
}

@article{binkowski2018demystifying,
  title={Demystifying mmd gans},
  author={Bi{\'n}kowski, Miko{\l}aj and Sutherland, Danica J and Arbel, Michael and Gretton, Arthur},
  journal={arXiv preprint arXiv:1801.01401},
  year={2018}
}

@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@misc{zeghidour2021soundstream,
    title   = {SoundStream: An End-to-End Neural Audio Codec},
    author  = {Neil Zeghidour and Alejandro Luebs and Ahmed Omran and Jan Skoglund and Marco Tagliasacchi},
    year    = {2021},
    eprint  = {2107.03312},
    archivePrefix = {arXiv},
    primaryClass = {cs.SD}
}

@inproceedings{ma2022progressively,
  title={Progressively generating better initial guesses towards next stages for high-quality human motion prediction},
  author={Ma, Tiezheng and Nie, Yongwei and Long, Chengjiang and Zhang, Qing and Li, Guiqing},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6437--6446},
  year={2022}
}

@inproceedings{mao2019learning,
  title={Learning trajectory dependencies for human motion prediction},
  author={Mao, Wei and Liu, Miaomiao and Salzmann, Mathieu and Li, Hongdong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9489--9497},
  year={2019}
}

@article{app13042084,
AUTHOR = {Cheng, Yao and Zhang, Guichao and Huang, Sifei and Wang, Zexi and Cheng, Xuan and Lin, Juncong},
TITLE = {Synthesizing 3D Gait Data with Personalized Walking Style and Appearance},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {4},
ARTICLE-NUMBER = {2084},
URL = {https://www.mdpi.com/2076-3417/13/4/2084},
ISSN = {2076-3417},
ABSTRACT = {Extracting gait biometrics from videos has been receiving rocketing attention given its applications, such as person re-identification. Although deep learning arises as a promising solution to improve the accuracy of most gait recognition algorithms, the lack of enough training data becomes a bottleneck. One of the solutions to address data deficiency is to generate synthetic data. However, gait data synthesis is particularly challenging as the inter-subject and intra-subject variations of walking style need to be carefully balanced. In this paper, we propose a complete 3D framework to synthesize unlimited, realistic, and diverse motion data. In addition to walking speed and lighting conditions, we emphasize two key factors: 3D gait motion style and character appearance. Benefiting from its 3D nature, our system can provide various gait-related data, such as accelerometer data and depth map, not limited to silhouettes. We conducted various experiments using the off-the-shelf gait recognition algorithm and draw the following conclusions: (1) the real-to-virtual gap can be closed when adding a small portion of real-world data to a synthetically trained recognizer; (2) the amount of real training data needed to train competitive gait recognition systems can be reduced significantly; (3) the rich variations in gait data are helpful for investigating algorithm performance under different conditions. The synthetic data generator, as well as all experiments, will be made publicly available.},
DOI = {10.3390/app13042084}
}

@article{zhang2022motiondiffuse,
      title   =   {MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model}, 
      author  =   {Zhang, Mingyuan and
                   Cai, Zhongang and
                   Pan, Liang and
                   Hong, Fangzhou and
                   Guo, Xinying and
                   Yang, Lei and
                   Liu, Ziwei},
      year    =   {2022},
      journal =   {arXiv preprint arXiv:2208.15001},
}

@article{xie2022vector,
  title={Vector Quantized Diffusion Model with CodeUnet for Text-to-Sign Pose Sequences Generation},
  author={Xie, Pan and Zhang, Qipeng and Li, Zexian and Tang, Hao and Du, Yao and Hu, Xiaohui},
  journal={arXiv preprint arXiv:2208.09141},
  year={2022}
}

@article{zhang2023t2m,
  title={T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations},
  author={Zhang, Jianrong and Zhang, Yangsong and Cun, Xiaodong and Huang, Shaoli and Zhang, Yong and Zhao, Hongwei and Lu, Hongtao and Shen, Xi},
  journal={arXiv preprint arXiv:2301.06052},
  year={2023}
}

@inproceedings{cai2021unified,
  title={A unified 3d human motion synthesis model via conditional variational auto-encoder},
  author={Cai, Yujun and Wang, Yiwei and Zhu, Yiheng and Cham, Tat-Jen and Cai, Jianfei and Yuan, Junsong and Liu, Jun and Zheng, Chuanxia and Yan, Sijie and Ding, Henghui and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11645--11655},
  year={2021}
}

@inproceedings{grew,
    title={Gait Recognition in the Wild: A Benchmark},
    author={Zheng Zhu and Xianda Guo and Tian Yang and Junjie Huang and 
        Jiankang Deng and Guan Huang and Dalong Du and Jiwen Lu and Jie Zhou},
    booktitle={IEEE International Conference on Computer Vision (ICCV)},
    year={2021}              
}

@inproceedings{barsoum2018hp,
  title={Hp-gan: Probabilistic 3d human motion prediction via gan},
  author={Barsoum, Emad and Kender, John and Liu, Zicheng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition workshops},
  pages={1418--1427},
  year={2018}
}

@inproceedings{yan2019convolutional,
  title={Convolutional sequence generation for skeleton-based action synthesis},
  author={Yan, Sijie and Li, Zhizhong and Xiong, Yuanjun and Yan, Huahan and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4394--4402},
  year={2019}
}

@inproceedings{fvg,
  author = { Ziyuan Zhang and Luan Tran and Xi Yin and Yousef Atoum and Jian Wan and Nanxin Wang and Xiaoming Liu },
  title = { Gait Recognition via Disentangled Representation Learning },
  booktitle = { In Proceeding of IEEE Computer Vision and Pattern Recognition },
  address = { Long Beach, CA },
  month = { June },
  year = { 2019 },
}


@inproceedings{casia,
  title={A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition},
  author={Yu, Shiqi and Tan, Daoliang and Tan, Tieniu},
  booktitle={18th International Conference on Pattern Recognition (ICPR'06)},
  volume={4},
  pages={441--444},
  year={2006},
  organization={IEEE}
}

@inproceedings{zheng2022gait3d,
title={Gait Recognition in the Wild with Dense 3D Representations and A Benchmark},
author={Jinkai Zheng and Xinchen Liu and Wu Liu and Lingxiao He and Chenggang Yan and Tao Mei},
booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2022}
}
	

@Article{cosma22gaitformer,
  AUTHOR = {Cosma, Adrian and Radoi, Emilian},
  TITLE = {Learning Gait Representations with Noisy Multi-Task Learning},
  JOURNAL = {Sensors},
  VOLUME = {22},
  YEAR = {2022},
  NUMBER = {18},
  ARTICLE-NUMBER = {6803},
  URL = {https://www.mdpi.com/1424-8220/22/18/6803},
  ISSN = {1424-8220},
  DOI = {10.3390/s22186803}
}

@article{jaiswal2020survey,
  title={A survey on contrastive self-supervised learning},
  author={Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  journal={Technologies},
  volume={9},
  number={1},
  pages={2},
  year={2020},
  publisher={MDPI}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{wang2022pedestrian,
  title={Pedestrian attribute recognition: A survey},
  author={Wang, Xiao and Zheng, Shaofei and Yang, Rui and Zheng, Aihua and Chen, Zhe and Tang, Jin and Luo, Bin},
  journal={Pattern Recognition},
  volume={121},
  pages={108220},
  year={2022},
  publisher={Elsevier}
}

@article{cyclical-lr,
  author    = {Leslie N. Smith},
  title     = {No More Pesky Learning Rate Guessing Games},
  journal   = {CoRR},
  volume    = {abs/1506.01186},
  year      = {2015},
  url       = {http://arxiv.org/abs/1506.01186},
  eprinttype = {arXiv},
  eprint    = {1506.01186},
  timestamp = {Mon, 13 Aug 2018 16:47:53 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Smith15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kingma2014adam,
  added-at = {2023-03-05T10:30:55.000+0100},
  author = {Kingma, Diederik P and Ba, Jimmy},
  biburl = {https://www.bibsonomy.org/bibtex/28bdee324b737a54ae326ea6db6e8950b/jascal_panetzky},
  interhash = {57d2ac873f398f21bb94790081e80394},
  intrahash = {8bdee324b737a54ae326ea6db6e8950b},
  journal = {arXiv preprint arXiv:1412.6980},
  keywords = {imported},
  timestamp = {2023-03-05T10:34:04.000+0100},
  title = {Adam: A method for stochastic optimization},
  year = 2014
}



@inproceedings{lancucki2020robust,
  title={Robust training of vector quantized bottleneck models},
  author={{\L}a{\'n}cucki, Adrian and Chorowski, Jan and Sanchez, Guillaume and Marxer, Ricard and Chen, Nanxin and Dolfing, Hans JGA and Khurana, Sameer and Alum{\"a}e, Tanel and Laurent, Antoine},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2020},
  organization={IEEE}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{yan2021videogpt,
  title={Videogpt: Video generation using vq-vae and transformers},
  author={Yan, Wilson and Zhang, Yunzhi and Abbeel, Pieter and Srinivas, Aravind},
  journal={arXiv preprint arXiv:2104.10157},
  year={2021}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@article{kynkaanniemi2019improved,
  title={Improved precision and recall metric for assessing generative models},
  author={Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{naeem2020reliable,
  title={Reliable fidelity and diversity metrics for generative models},
  author={Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
  booktitle={International Conference on Machine Learning},
  pages={7176--7185},
  year={2020},
  organization={PMLR}
}

@article{borji2019pros,
  title={Pros and cons of gan evaluation measures},
  author={Borji, Ali},
  journal={Computer Vision and Image Understanding},
  volume={179},
  pages={41--65},
  year={2019},
  publisher={Elsevier}
}

@article{liu2018improved,
  title={An improved evaluation framework for generative adversarial networks},
  author={Liu, Shaohui and Wei, Yi and Lu, Jiwen and Zhou, Jie},
  journal={arXiv preprint arXiv:1803.07474},
  year={2018}
}

@article{inception,
  author    = {Christian Szegedy and
               Vincent Vanhoucke and
               Sergey Ioffe and
               Jonathon Shlens and
               Zbigniew Wojna},
  title     = {Rethinking the Inception Architecture for Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1512.00567},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.00567},
  eprinttype = {arXiv},
  eprint    = {1512.00567},
  timestamp = {Mon, 13 Aug 2018 16:49:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SzegedyVISW15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{esser2020taming,
  title={Taming transformers for high-resolution image synthesis. In 2021 IEEE},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  booktitle={CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={12868--12878},
  year={2020}
}

@article{dowson1982frechet,
  title={The Fr{\'e}chet distance between multivariate normal distributions},
  author={Dowson, DC and Landau, BV666017},
  journal={Journal of multivariate analysis},
  volume={12},
  number={3},
  pages={450--455},
  year={1982},
  publisher={Elsevier}
}