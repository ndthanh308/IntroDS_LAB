\label{sec:dataset}

In order to train a sufficiently large and general autoencoder model, we assess that current gait datasets are too small. Even though datasets such as DenseGait \cite{cosma22gaitformer} and GREW \cite{grew} are collected "in-the-wild" outdoor environments using surveillance cameras, they nonetheless lack some walking registers such as treadmill walking, more aggressive camera angles and indoor environments. However, by combining the major large-scale gait datasets into a single dataset, we can ensure more diversity of walking registers. In Table \ref{tab:walkpile} we showcase the existing datasets that comprises our pretraining dataset. We used \textbf{DenseGait} \cite{cosma22gaitformer} and \textbf{GREW} \cite{grew}, two similar in-the-wild datasets for their diverse walking sequences in outdoor environments, \textbf{OU-ISIR} \cite{ouisir} for more controlled walking in indoor and treadmill registers, and \textbf{Gait3D} \cite{zheng2022gait3d}, and indoor "in-the-wild" dataset collected in a supermarket setting. After concatenation of all skeleton sequences from the datasets, we obtain 875,543 walking sequences, totalling 1220.06 hours. To increase the size as much as possible, we also included the testing / distractor splits of each dataset whenever possible. We purposely did not include controlled, small scale datasets such as CASIA-B \cite{casia}, as we use them for downstream evaluation.

All walking sequences in this dataset are 2D skeletons in COCO pose format. We chose 2D poses to unify all datasets, as every dataset is providing 2D poses by default, while only some are also providing silhouettes or body meshes. Even though many gait processing models have good results using and appearance-based approach with silhouettes \cite{chao2019gaitset,lin2022gaitgl}, pose sequences only encode movement and abstract away any appearance information, preserving the privacy of walking individuals \cite{gaitgraph,cosma22gaitformer}. Skeleton sequences are a more interpretable and a plethora of models employ them for motion synthesis \cite{siyao2022bailando,zhang2023t2m,tevet2022motionclip,temos-eccv-2022}.

Skeleton sequences from each datasets are pre-processed in the same way. We filtered out skeletons that have too small or too large joint variance, which corresponds to static or erratic movement, respectively. We found that this procedure ensures that only properly moving skeletons are kept in the dataset. Furthermore, skeleton sequences are normalized and aligned at the pelvis, using the following formulae, considering that each of the $J = 18$ joints have $(x, y)$ coordinates:

\begin{equation*}
    \hat{x}_{joint} = \frac{x_{joint} - x_{pelvis}}{|x_{R.shoulder} - x_{L.shoulder}|}
    \label{eq:eq1}
\end{equation*}
\begin{equation*}
    \hat{y}_{joint} = \frac{y_{joint} - y_{pelvis}}{|y_{neck} - y_{pelvis}|}
    \label{eq:eq2}
\end{equation*}

\begin{table}[hbt!]
    \centering
    \resizebox{0.85\linewidth}{!}{
        \begin{tabular}{c|ccc}
            \textbf{Dataset} & \textbf{Split} & \textbf{\# Sequences} & \textbf{Duration (hr.)}\\
            \midrule\midrule
            \multirow{2}{*}{DenseGait \cite{cosma22gaitformer}} & Train & 217,954 & 614.75\\
            & Validation & 10,733 & 36.53 \\
            \midrule
    
            \multirow{3}{*}{GREW \cite{grew}} & Train & 102,888 & 175.92 \\
            & Test & 24,000 & 65.37\\
            & Distractor & 226,588 & 154.82\\
            \midrule
    
            \multirow{2}{*}{OU-ISIR \cite{ouisir}} & Train & 133,872 & 57.82 \\
            & Test & 134,199 & 57.92 \\
            \midrule
    
            \multirow{2}{*}{Gait3D \cite{zheng2022gait3d}} & Train & 18,940 & 42.70 \\
            & Test & 6,369 & 14.23 \\
            \midrule
    
            \textbf{Total} & & \textbf{875,543} &  \textbf{1220.06} \\
            
        \end{tabular}
    }
    \caption{Datasets that make up our pretraining dataset. We combined all the major in-the-wild and controlled datasets (including all splits) into a single, large-scale and diverse dataset. The dataset contains gait samples from a diverse set of walking registers, environments and camera angles. }
    \label{tab:walkpile}
\end{table}

In this manner, every skeleton sequence is aligned spatially and the differences in height and width of individuals are essentially eliminated. Consequently, only movement is encoded irrespective of the screen coordinates, distance to camera or appearance cues. We employ minimal augmentations to the skeleton sequences, adopting only random temporal cropping and walking pace modifications \cite{9721551,wang2020self,cosma22gaitformer}. We crop each skeleton sequence to be $T = 64$ frames long.