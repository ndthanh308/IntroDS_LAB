Works in motion sythetisation are predominantly directed towards generating controllable, general actions for use in animation \cite{siyao2022bailando,li2022skeleton2humanoid,raab2023single,cai2021unified}. Yan et al. \cite{yan2019convolutional} proposed a convolutional architecture named Convolutional Sequence Generation Network (CSGN) for generating skeleton sequences for action recognition. The authors employed spatial graph downsampling and temporal downsampling to generate the whole sequence in a single pass, using latent vectors sampled from gaussian processes. Petrovich \cite{petrovich2021action} employed a transformer VAE model conditioned on the action. 

Li et. al \cite{li2022skeleton2humanoid} proposed a method for performing motion "in-betweening" using physically plausible constraints. Raab et al. \cite{raab2023single} perform motion in-betweening by using diffusion models. Wang et al. \cite{wang2022towards} constructed a method for generating movement animations which also takes the target environment into account.

Some works tackle the problem of motion prediction \cite{guo2023back,ma2022progressively,mao2019learning}. Ma et al. \cite{ma2022progressively} used a graph-convolutional network for motion prediction of skeleton sequences. Zhang et al. \cite{zhang2020perpetual} generate unbounded motion sequences conditioned only on a single starting skeleton. The authors employ an RNN-based architecture to procedurally generate skeletons. 

Motion generation techniques have also been used for sign language generation \cite{liu2022learning,xie2022vector}. Liu et al. \cite{liu2022learning} used a cross-modal approach for audio to sign pose sequence generation using a GRU-based model. Xie et al. \cite{xie2022vector} used a VQ-VAE to generate sign pose sequences, using a discrete diffusion prior model. Zhang et al. \cite{zhang2023t2m} propose a Motion VQ-VAE for text-conditioned action generation, and demonstrate that a simple VQ-VAE recipe \cite{razavi2019generating} can have very good performance for this data modality without any major bells and whistles. 

In the area of gait recognition, synthesising walks has been only briefly studied in the past, partially due to the lack of large-scale datasets, and the unique constraints of this settings. Works in self-supervised for images\cite{chen2020simple,guo2022contrastive,tian2020makes} point out that the high quality data augmentation is crucial for learning good representations. Tian et al.\cite{tian2020makes} argues that optimal views for self-supervised contrastive learning are task-dependent. For instance, in gait analysis, Yu et al. \cite{gaitgan} train a generative adversarial network to generate silhouette sequences that are invariant to walking confounding factors such as viewpoint and clothing change. However, the goal was downstream identification and not generation in itself. Yao et al. \cite{app13042084} propose a framework for walking synthetisation based on an autoencoder and a parametric body model, but their experiments are mainly based on silhouette-based identification models.  Different from previous works, we are interested in manipulating the walking variation and viewpoint of existing walks.