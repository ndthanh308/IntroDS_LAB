% \vspace{-3mm}
\section{Conclusion}\label{sec:conclusion}

We propose a novel framework for adaptively learning regions of interest for Bayesian optimization. Our model maintains two Gaussian processes: One global model for identifying the ROIs as (adaptive) superlevel-sets; the other surrogate model for acquiring data in these high-confidence ROIs. We proposed to take the width of the intersection of the point-wise confidence intervals of both GPs as the acquisition function to achieve a theoretical guarantee on both the convergence rate of the filtering and optimization process. We demonstrate our algorithm in promising real-world experiment design scenarios, including protein engineering and material science. Our results show that {\algname} compares favorably against state-of-the-art BO approaches under similar settings---especially in high-dimensional and structured tasks with non-stationary dynamics---while having fewer hyperparameters to fine-tune. 

{More generally, we propose the principled framework combining the power of a coarse global model for filtering low-interest areas and a fine-grained local model for focused optimization, supported by theoretical insights.} We show the potential of the framework by integrating Thompson sampling, and the extensions to other acquisition functions that are not based on confidence intervals are also of interest for future work. We demonstrate the practical issues of taking the intersection of all historical CIs and discuss the cost of only taking the intersection of CIs at each time step. This raises the demand for future studies on addressing the dynamics of (deep) kernel learning.
