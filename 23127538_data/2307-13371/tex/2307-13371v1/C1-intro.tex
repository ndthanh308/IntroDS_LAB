\section{Introduction}
Bayesian optimization (BO) is a popular statistic-model-based sequential optimization method in various fields of science and engineering, including scientific experimental design \citep{yang2019machine}, robotics planning \citep{Berkenkamp2016SafeOpt,sui2018stagewise}, self-tuning systems \citep{zhang2022learning} and hyperparameter optimization \citep{snoek2012practical}. These applications often involve optimizing a black-box function that is expensive to evaluate, where the statistics-guided efficient optimization algorithm is desired. The common practice in BO is to employ Gaussian processes (GPs) \citep{rasmussen:williams:2006} as a statistic surrogate model for the unknown objective function due to its  mathematical simplicity %%in Bayesian inference 
as well as the promising capability in terms of learning and inference, which allows for defining effective acquisition functions.



Despite strong empirical and theoretical results under certain assumptions (e.g., smoothness) \citep{srinivas2009gaussian,wang2017max,wang2016optimization}, BO has struggled in many real-world settings due to the \emph{high-dimensional}, \emph{large-scale}, and \emph{heterogeneous} nature of optimization tasks. Besides the well-known curse of dimensionality \citep{bengio2005curse}, the heterogeneity and scarcity of training data in real-world tasks make it challenging to fit a single (global) GP for data acquisition \citep{eriksson2019scalable}. Meanwhile, purely relying on local characteristics has been proven to be ineffective for global optimization, due to the ignorance of the correlations on observations that are normally captured by the GP.
The trade-off between exploiting data locality and exploring uncertainty at a global scale emerges as a critical problem in real-world BO settings, especially when the global smoothness assumption no longer holds.


% Figure environment removed


Historically, various partitioning-based BO methods have been proposed to tackle this challenge. These methods, often based on certain clustering heuristics, learn the \emph{regions of interest} (ROI) to better reflect the data locality. A common issue for existing heuristics is the added layer of complexity for model fine-tuning, which involves optimizing extra hyperparameters such as the number of ROIs \citep{eriksson2019scalable}, maximum leaf size in the tree-structured partitioning methods, and methods to generalize the partition learned on the accumulated observations to the whole search space \citep{8187198}.

We propose a novel nonparametric approach for partitioning-based BO that demonstrates strong empirical performance in real-world tasks, while having few hyperparameters to maintain. The proposed algorithm is inspired by the \emph{level-set estimation} (LSE) problem, where a level-set corresponds to a set of points for which the black-box objective function takes value above (or below) some \emph{given} threshold. Given a threshold, \citet{10.5555/2540128.2540322} show that one can leverage the point-wise confidence interval to actively identify the level-set with a theoretical guarantee. In the context of Bayesian optimization, the threshold could reduce to the lower confidence bound of the global optima.

\paragraph{Our contribution} Following the above insight, we propose the novel Bayesian optimization framework with adaptive estimation of regions of interest. As illustrated in \Figref{fig:overview}, The algorithm partitions the search space based on confidence intervals and identifies the superlevel-set as the ROIs of high confidence contain the global optimum. We propose a novel acquisition function that relies on both the global model and the ROI model to capture the locality while not sacrificing global knowledge through optimization.
We further provide rigorous theoretical analyses showing that the proposed acquisition function can, in a principled way, exhibit an improved regret bound compared to its canonical BO counterpart without the filtering component. We demonstrate the effectiveness of the proposed framework with an empirical study on several synthetic and real-world optimization tasks.


