\clearpage
\section{Experiment}\label{sec:exp}
\paragraph{Experimental setup}

% Figure environment removed

We compare three baseline algorithms in our experiments against \interCI, \roiCI, and \roiTS. The Deep-Kernel-based Bayesian Optimization initialized with a pre-trained AutoEncoder (DKBO-AE) applies the deep kernel where a pre-trained AutoEncoder \footnote{The AutoEncoder is trained with random unlabeled samples.} initializes the neural network $q$ {\citep{zhang2022learning}}. The neural network consists of three hidden layers with 1000, 500, and 50 neurons, and ReLU non-linearity respectively. {The output layer is one-dimensional.} We use squared exponential kernel or linear kernel as the base kernel, i.e. $k_{\SE}(\instance,\instance')=\sigma^2_{\SE}\exp(-\frac{(\instance-\instance')^2}{2l})$ or $k_{\Linear}(\instance,\instance')=\sigma^2_{\Linear}(\instance^T\instance)$, for the deep kernel, and Thompson Sampling \citep{chapelle2011empirical} for the acquisition function $\alpha$. Two other partition-based BO algorithms LA-MCTS \citep{wang2020learning} and TuRBO \citep{eriksson2019scalable} serve as the baselines. Note that DKBO-AE is used as the subroutine for LA-MCTS, {TuRBO-DK}, and \roiTS. The neural network architecture, base kernel and acquisition function are the same. 
\roiCI and \interCI also share the same deep kernel except for applying different acquisition functions. The comparison between \roiTS and DKBO-AE serves as the ablation study of \textbf{the proposed partitioning method}. The comparison between \roiCI and \interCI also serves as the ablation study of \textbf{taking the intersection of CI} as defined in \eqref{eq:acqLCBt+UCBt}. 

One crucial problem in practice is tuning the hyperparameters. For each of the algorithms, the same 10 randomly picked points serve as the warm-up set. We take the default hyperparameters from the open-sourced LA-MCTS \footnote{\scriptsize \url{https://github.com/facebookresearch/LaMCTS}} and TuRBO \footnote{\scriptsize\url{https://botorch.org/tutorials/turbo_1}} implementation. Note that we choose TuRBO-1 implementation for TuRBO where there is one trust region through the optimization, as previous work has shown its robust performance in various tasks \citep{eriksson2019scalable}. For \interCI, we set $\delta$ in \lemref{lem:CI} to be 0.2. In addition, we find that using the $\beta^{1/2}_t$ in \lemref{lem:CI} to identify the ROI could be over-conservative in that it can not filter many areas and let \interCI regress to DKBO-AE with two similar GPs. Through the experiments, we fix $\beta^{1/2}_t=0.2$ only when identifying ROIs as in \lineref{alg:ln:filtering} of \algoref{alg:main}. For all the tested algorithms, the base kernels are squared exponential kernels except for Nanophotonics and Water Converter where we applied linear kernels as the base kernel. We defer the detailed study of parameter choices in \interCI to the appendix.

% Figure environment removed

\paragraph{Datasets}
We study the performance of \algname on two synthetic tasks and four pre-collected datasets described below.

\subparagraph{1D-Toy.}
We create a synthetic dataset 1D-Toy of one dimension to illustrate the process of \interCI as is shown in \secref{sec:method}. The function is defined on $\instance\in[-1,1]$ as $f(\instance) = \sin(64\vert{\instance}\vert^ 4) - (\instance-0.2)^2$. This toy function consists of two high-frequency areas on both sides and a low-frequency area in the middle. The neural network is pre-trained on 100 data points.

\subparagraph{HDBO-200D.}
We create a synthetic dataset Sum-200D of 200 dimensions. Each dimension is independently sampled from a standard normal distribution to maximize the uncertainty on that dimension and examine the algorithm's capability to solve the medium-dimensional problem. We want to maximize the label $f(\instance) = \sum^{200}_{i=1}{e^{x_i}}$ which bears an additive structure and of non-linearity. The neural network is pre-trained on 100 data points. 

\subparagraph{Water Converter Configuration-{32D}.} 
This UCI dataset we use consists of positions and absorbed power outputs of wave energy converters (WECs) from the southern coast of Sydney. The applied converter model is a fully submerged three-tether converter called CETO. 16 WECs 2D-coordinates are placed and optimized in a size-constrained environment. Note its values are at the order of $O(10^6)$.

\subparagraph{Nanophotonics Structure Design{-5D}.}
We wish to optimize a weighted figure of merit quantifying the fitness of the transmission spectrum for hyperspectral imaging as assessed by a numerical solver~\citep{song:18.2}. This problem has a 5-dimensional input corresponding to the physical design dimensions of a potential filter. 
Although the input is not high-dimensional, the function represents a discrete solution of Maxwell's equations and has a complex value landscape. 

\subparagraph{GB1-{118D}.}
We use a protein dataset in which the objective is to maximize stability fitness predictions for the Guanine nucleotide-binding protein GB1 given different sequence mutations in a target
region of 4 residues \citep{wu2019machine}. Specifically, we use the ESM embedding generated by a transformer protein language model \citep{rives2021biological}.

\subparagraph{Rosetta Protein Design-{86D}.} 
We use another protein engineering dataset describing a set of antigen/antibody binding calculations. These calculations, executed using supercomputing resources, estimate the change in binding free energy at the interface between each of 71769 modified antibodies and the SARS-CoV-2 spike protein, as compared to the single reference antibody from which they are derived. Estimations of binding free energy ($\Delta\Delta G$) are calculated using protein-structure-based Rosetta Flex simulation software \citep{das2008macromolecular, barlow2018flex}.  These calculations took several CPU hours each and were produced during an antibody design process \citep{desautels2020rapid,desautels2022computationally}.  
Inputs are described with an 80-dimensional feature vector that, relative to the reference sequence, describes changes in the interface between the antibody and the corresponding target region on the SARS-CoV-2 spike. 
This is a particularly relevant problem setting when trying to rapidly choose antibody candidates to respond to a new disease in a timely fashion.

\paragraph{Confidence Intervals}
As shown in \figref{fig:exp:CI}, the CIs of $f^*$ through the optimization of \interCI do not constantly narrow. Instead, on Nanophotonics the width generally remains the same through the 90 iterations, indicating the challenge of fitting these datasets with limited data points and therefore optimizing it with the underfitted GPs. 

The intersection CI is consistently narrower than the other two, where there is no consistent superiority against each other. Though on HDBO the ROI curve is above the global curve, the resulting intersection CI still improves upon global CI, indicating that the maximizer of $\acqUCBt_{t}$ and $\acqLCBt_{t}$ are different for global GP and ROI GP.

The dynamics of kernel learning results in empty intersections on Rosetta, where occasionally the width of CI for $f^*$ turns out to be zero, showing the potential problem in taking the intersection of all historical CIs. Future improvement on \interCI could be better aligning the CI of both ROI and global GPs through different iterations to allow taking the intersection of all historical CIs as in \propref{prop:regret}.

The intersection curve converges faster to non-zero values on both HDBO and GB1 showing the benefits of taking the intersection of global CI and ROI CI as it better captures the localities with ROI GP while not losing information of global GP. However, the width of CI could not directly serve as the indicator for the optimization performance. On GB1, the intersection curve is uniformly better than both the ROI and global CI, while in \figref{fig:exp:simpleRegret}, \interCI does not outperform DKBO-AE, \roiTS, or \algname-ROI-UCB as the CI for $f^*$ is still larger than 3.3.

% TD: Improve the table.
\newcommand\tableW{3.6cm}
\newcommand\tableCW{1.4cm}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\begin{table*}[ht]
\renewcommand*{\arraystretch}{2}
\centering
\resizebox{\linewidth}{!}{%
% \begin{tabular}{ |M{1.7cm}||M{2cm}|M{1.6cm}|M{\tableCW}|M{2.3cm}| M{\tableW}|M{3.0cm}| }
\begin{tabular}{ |M{1.7cm}||M{2cm}|M{2cm}|M{2cm}|M{\tableW}| M{\tableW}|M{\tableW}| }
 % \hline
 % \multicolumn{7}{|c|}{Method List} \\
 \hline
    \textbf{Methods} & DKBO-AE & LA-MCTS & TuRBO & \roiTS & \roiCI & \interCI  \\
 \hline\hline
    \textbf{BO Model} & Global & Local & Local & Local & Local & Global + Local \\
 \hline
    \textbf{Acquisition Function} & $f_{g,t} \sim GP_{f_g, t}$   & \multicolumn{3}{c|}{$\roif_{t} \sim GP_{\roif, t}$}  &  $ \UCBit_{\roif, t}(\instance) -  \LCB_{\roif, t}(\instance)$ &
    $\acqUCBt_{t}(\instance) - \acqLCBt_{t}(\instance)$\\
 \hline
    \textbf{Average Ranking} &   3.67  & 3.33   & 5.83 & \textbf{2.00} &  3.50 & \textbf{2.33}\\

 \hline
\end{tabular}}
\small
\caption{\rebuttal{Comparison of different methods tested in the experiments. The BO model row shows the model on which the acquisition function is defined. LA-MCTS relies on global Monte-Carlo tree search, and both TuRBO and \roiTS rely on a global GP to identify the ROI/Trust Region, despite being tagged as `local' for the BO model. The acquisition functions for DKBO-AE, LA-MCTS, TuRBO and \roiTS are Thompson sampling on different GPs. The acquisition functions for \roiCI and \interCI are defined in \eqref{eq:acqROI-ci} and \eqref{eq:acqCI}, respectively. The average ranking corresponds to the ultimate simple regrets shown in \figref{fig:exp:simpleRegret}}. We highlight the rankings of the proposed methods.} \label{table:methods}

\end{table*}

\paragraph{Optimization Performance} The experiment results in \figref{fig:exp:simpleRegret} demonstrate the robust performance of \interCI which consistently matches or outperforms the best baseline. In contrast, LA-MCTS consistently matches or outperforms TuRBO-DK,  but lags behind DKBO-AE on the 1D Toy which indicates its potential inefficiency in the tasks of high-frequency areas hindering its partitioning of the search space. Note that we also find that using SVM to generalize the partition on $Y_t$ to $\searchSpace$ in LA-MCTS occasionally fails possibly due to the intrinsic complexity of the partition learned on $Y_t$ demanding methods of greater capability, while the level-set partition of \interCI is regularized by the smoothness of the global Gaussian process $\mathcal{GP}_{f_g}$. We reject the failed LA-MCTS trials.

{TuRBO-DK} matches \interCI performance on GB1 and but loses to DKBO-AE on all other cases.  By construction, 1D Toy and HDBO-200D could have a large amount of distant local maximum, while {TuRBO-DK} relies on the locality of the observation to identify the trust regions. {TuRBO-DK} could be potentially trapped in the local maximum and the performance degrades in the scenario where the multiple modules are distant from each other while the gap between sub-optimal and optimal observation is significant. In contrast, \interCI is capable of identifying multiple regions of interest with the level-set partitioning without specifying the desired number of regions.

On 1D-Toy dataset which is composed of the low-value high-frequency areas and the high-value low-frequency area, \interCI significantly outperforms the baselines and reaches the near-optimal area within 30 iterations. Due to the complexity of the low-value areas that make up a large portion of the objective, the GPs underfit especially at the earlys stage where access to observation is limited as is shown in \figref{fig:overview}. At this phase, the DKBO-AE stably outperforms \interCI potentially without the distraction from the under-fitting ROI GP. While on HDBO datasets which by construction bears relative uniform smoothness, the partition-based algorithms other than {TuRBO-DK} all enjoy similar benefits at the beginning stage.

On Nanophotonics, Water-Converter and Rosetta, \interCI matches or outperforms the baselines including \roiCI, while losing to \roiCI and \roiTS on GB1. This ablation study indicates the necessity of taking the intersection of CI in most scenarios, while revealing that more aggressive filtering of \roiCI could sometime be beneficial. \roiTS matches \algname on HDBO, Nanophotonics,  Water-Converter and Rosetta, reflecting that the {ROI} GP could be as informative as the combination of the global and {ROI} GP in some cases. The fact that \roiTS uniformly outperforms LA-MCTS and TuRBO-DK on all the experiments requires further study on integrating Thompson sampling into a \algname-style framework with a similar theoretical guarantee.

\rebuttal{We summarize the different methods tested in the experiments with \tabref{table:methods}. The comparison between \roiTS with both LA-MCTS and TuRBO shows the effectiveness of identifying the ROI on a point-basis on the given discretization with confidence intervals of a global GP. The comparison between \roiCI and \interCI highlights the benefits on integrating information from global model into optimization on ROI especially when using the confidence interval as acquisition function.}




