\begin{appendix}

\section{Proofs}
\subsection{Proof of \lemref{lem:CI} and \corref{cor:CI}}
\begin{proof}
% \TBD{Consider removing UCB.}
Similar to lemma 5.1 of \cite{srinivas2009gaussian}, with probability at least $1-\delta$, $\forall \instance\in \discreteSet, \forall t\geq 1, \forall f \in \{f_g, \hat{f}\}$,
$$\vert f(\instance) - \mu_{f, t-1}(\instance)\vert \leq \beta_{t}^{1/2}\sigma_{f, t-1}(\instance)$$

Note that we also take the union bound on $ f \in \{f_g, \hat{f}\}$.

Then  $\forall t\geq 1, \forall f \in \{f_g, \hat{f}\}$, 
$$P\paren{f^* \leq \UCBit_{f, t}(\instance^*) \leq \UCBit_{f, t, \max}} \geq 1-\delta$$
According to \eqref{eq:acqLCB+UCB}, $\forall t\geq 1$
$$P\paren{f^* \leq \max_{\instance \in \searchSpace} \acqUCB_t(x)} \geq 1-\delta$$


Symmetrically, $ \forall \instance\in \discreteSet, \forall t\geq 1, \forall f \in \{f_g, \hat{f}\}$,
$$P\paren{f^* \geq f(\instance) \geq \LCB_{f, t}(\instance)} \geq 1-\delta$$
Then $ \forall t\geq 1,$
$$P\paren{\UCBit_{f_g, t}(\instance) \geq f^* \geq \LCB_{f_g, t, \max}} \geq 1-\delta$$
according to the definition of $\roi$, $P\paren{\instance^* \in \roi_t} \geq 1-\delta$.

Also, according to \eqref{eq:acqLCB+UCB}, $\forall t\geq 1$
$$P\paren{f^* \geq \max_{\instance \in \searchSpace} \acqLCB_t(x)} \geq 1-\delta$$
\end{proof}

\subsection{Proof of \propref{prop:regret}}

The following two lemmas shows that the width of the interval is bounded by the maximum of \acq.

\begin{lem} \label{lem:acqCIBound}
Under the same conditions assumed in \lemref{lem:CI} except for $\beta_t=2\log(2\vert \discreteSet \cap \roi \vert \pi_t/ \delta)$, with acquisition function $\acq(\instance) = \vert \acqCI_{t}(\instance) \vert$, $\forall t\geq 1, \forall f \in \{f_g, \hat{f}\}$, let $\instance'' = \argmax_{\instance \in \discreteSet \cap \roi}\vert \acqCI_{t}(\instance) \vert$ we have $\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance) \leq \rho_\CI\beta^{1/2}_{t}\sigma_{f,t-1}(\instance)$. Here $\rho_\CI \leq \rho_\UCBit \leq 2$.
\end{lem}

\begin{proof}
$\forall t\geq 1, \forall f \in \{f_g, \hat{f}\}$
\begin{align*}
    \max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance)
    & \leq \acqUCB_{t}(\instance')- \acqLCB_{t}(\instance')\\
    & \leq 2\beta_{t}^{1/2}\sigma_{f, t-1}(\instance')\\
    & \leq \argmax_{\instance \in \discreteSet \cap \roi}\vert \acqCI_{t}(\instance) \vert\\
    & = 2\beta_{t}^{1/2}\sigma_{f, t-1}(\instance'')
\end{align*}
\end{proof}


The followings finish the proof of \propref{prop:regret}.\\
\begin{proof}
By lemma 5.4 of \citet{srinivas2009gaussian}, with $\beta_t=2\log(2\vert \discreteSet \cap \roi \vert \pi_t/ \delta)$, $\forall f \in \{f_g, \hat{f}\}, \sum_{t=1}^{T} (2\beta_{t}^{1/2}\sigma_{f, t-1}(\instance_t))^2 \leq C_1\beta_T\maxInfo_{f, T}$. Taking the union bound of \lemref{lem:CI} and \corref{cor:CI2}, with probability at least $ 1-2\delta$, $\forall f \in \{f_g, \hat{f}\}$,
\begin{align*}
     \sum_{t=1}^{T} \paren{\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance)}^2
    &\leq \sum_{t=1}^{T}(\rho_\acq \beta_{t}^{1/2}\sigma_{f, t-1}(\instance_t))^2\\
    &\leq \rho_\acq^2C_1\beta_T\maxInfo_{f, T}/4
\end{align*}
According to \eqref{eq:acqLCB+UCB}, $\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance)$ is monotonically decreasing. By Cauchy-Schwaz, with probability at least $ 1-2\delta$, $\forall f \in \{f_g, \hat{f}\}$, \begin{align*}
    \rho_\acq^2C_1\beta_T\maxInfo_{f, T}/4
    &\geq \sum_{t=1}^{T} \paren{\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance)}^2\\
    &\geq \frac{1}{T}(\sum_{t=1}^{T} \max_{\instance \in \discreteSet \cap \roi}\acqUCB_{t}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{t}(\instance))^2\\
    &\geq T \paren{\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{T}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{T}(\instance)}^2
\end{align*}
Assume with probability at least $1-2\delta$,
\begin{align*}
    \paren{\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{T}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{T}(\instance)}^2
    \leq \rho_\acq^2C_1\beta_T\maxInfoBALLET_{T}/4T 
    \leq \epsilon^2
\end{align*}
Hence, with the smallest $T$ satisfying $T \geq \frac{\rho^2_\acq \beta_T \maxInfoBALLET_T C_1}{4 \epsilon^2}$, $\Pr{\max_{\instance \in \discreteSet \cap \roi}\acqUCB_{T}(\instance) - \max_{\instance \in \discreteSet \cap \roi}\acqLCB_{T}(\instance)\leq \epsilon} \geq 1-2\delta$.
\end{proof}


\section{Discussions}
\paragraph{Smoothness improvement on ROI}

In near-optimal areas, the smoothness of the objective should be no worse than the smoothness in the larger (global) area. \cite{srinivas2009gaussian} discussed the role of smoothness in reducing $\maxInfo$. As indicated by \propref{prop:regret}, the benefits to optimization of a smoother kernel learned on ROI instead of the kernel learned on the globe could be reflected in the reduced $\maxInfoBALLET$ in the regret bound compared to $\maxInfo_{f_g}$ without the filtering of BALLET. 

%\clearpage
\section{Supplemental Experimental Results}

In this section, we include an extended empirical study of \algname, compared against a broader collection of baseline algorithms with varying hyperparameters. Specifically, we show the results for the following algorithms:
\begin{itemize}\denselist
    \item \textsl{BALLET-ICI-RBF}: \interCI with RBF (squared-exponential) base kernel $k(\instance, \instance') = \exp\left(- \frac{d(\instance, \instance')^2}{2l^2} \right)$.
    \item \textsl{BALLET-ICI-Lin}: \interCI with linear base kernel $k(\instance, \instance') = \sigma_0 ^ 2 + \instance \cdot \instance'$ (with prior $N(0, \sigma_0^2)$ on the bias).
    \item \textsl{DKBO-AE-RBF}. \DKBO with RBF base kernel \citep{zhang2022learning}.
    \item \textsl{DKBO-AE-Lin}. \DKBO with linear base kernel.
    \item \textsl{\LAMCTS}. The Latent Action Monte Carlo Tree Search algorithm (LA-MCTS) of \citet{wang2020learning}.
    \item \textsl{TuRBO-m}. The Trust region Bayesian optimization (\turbo) algorithm of \citet{eriksson2019scalable}, where $m$ specifies the variant of \turbo that maintains $m$ local models in parallel.
\end{itemize}

\begin{table}[h]
\centering
\scalebox{.82}{
    %\begin{tabular}{l|l|l|l|l|l|l}
    \begin{tabular}{l l l l l l l}
    \toprule
        ~ & \textbf{1-D toy} & \textbf{HDBO} & \textbf{Nanophotonics} & \textbf{WaterConverter} & \textbf{GB1} & \textbf{Rosetta} \\ 
        \midrule
        ~ & $T=40$ & $T=40$ & $T=90$ & $T=90$ & $T=90$ & $T=90$ \\ \hline
        \textsc{\algname-ICI-RBF} & \bm{$0.03\pm 0.01$} & \bm{$85.90\pm 7.29$} & $76.65\pm 9.55$ & $33664.62\pm 0.00$ & \bm{$4.81\pm 0.15$} & \bm{$3.86\pm 0.49$} \\ \hline
        \textsc{\algname-ICI-Lin} & $0.10\pm 0.03$ & $110.81\pm 7.93$ & {$34.49\pm 4.39$} & \bm{$20084.66\pm 2928.84$} & $6.33\pm 0.28$ & $5.11\pm 0.18$ \\ \hline
        \textsc{\DKBO-RBF}  & $0.05\pm 0.02$ & $90.75 \pm 16.01$ & $89.49\pm 3.44$ & $28591.49\pm 2560.23$ & $5.02\pm 0.41$ & $4.89\pm 0.16$ \\ \hline
        \textsc{\DKBO-Lin} & $0.07\pm 0.01$ & $92.84\pm 6.22$ & $82.94\pm 4.50$ & $33664.63 \pm 0$ & $6.44\pm 0.19$ & $4.12\pm 0.46$ \\ \hline
        \LAMCTS & $0.10\pm 0.04$ & $95.47\pm 4.84$ & \bm{$30.79\pm 10.28$} & $26814.43\pm 1593.76$ & $5.59\pm 0.40$ & $5.09\pm 0.32$ \\ \hline
        \textsc{\turbo-1} & $0.31\pm 0.00$ & $136.80\pm 0.00$ & $96.58\pm 0.00$ & $33664.69\pm 0.00$ & $5.34\pm 0.52$ & $6.67\pm 0.00$ \\ \hline
        \textsc{\turbo-2} & $0.07\pm 0.06$ & $105.51\pm 5.47$ & $50.60\pm 15.46$ & $28450.65\pm 1691.96$ & $4.95\pm 0.45$ & $5.75\pm 0.32$ \\ \hline
        \textsc{\turbo-4} & $0.04\pm 0.03$ & $93.74\pm 10.25$ & $63.60\pm 3.48$ & $32800.93\pm 3148.98$ & $5.72\pm 0.72$ & $5.46\pm 0.22$\\ 
        \bottomrule
    \end{tabular}
    }
    \caption{
    Simple regret (Mean $\pm$ SE) at the $T^{\text{th}}$ iteration on the 6 datasets described in \secref{sec:exp}. Here, $T$ aligns with the optimization horizon reported in \figref{fig:exp:simpleRegret} for each dataset. The top results are highlighted in bold.}\label{tab:res:parameter}
    \vspace{-5mm}
\end{table}

As shown in \tabref{tab:res:parameter}, \interCI (with different choices of base kernels) consistently outperforms other baselines on all datasets but Nanophotonics. On Nanophotonics, while there is a small gap between \interCI and \LAMCTS, \LAMCTS is relatively unstable with a larger variance (SE). This is consistent with the results reported in \figref{fig:exp:simpleRegret}.

\paragraph{Hyperpameter choice}

We further provide results on \algname's performance with varying $\beta$ when filtering. \Figref{fig:beta-vs-regret} shows the simple regret of \interCI on the \textit{Nanophotonics} dataset. We observe that---although our regret bounds in \secref{sec:interci} rely on specific choices of $\beta_t^{1/2}$ for filtering -- the empirical results are robust within a range of small values. Also, using the $\beta_T^{1/2}=6.2$ as the analytic results in \propref{prop:regret} failed to match the performance of the fixed $\beta_t^{1/2}\leq 1$, showing its over-conservative problem.

% Figure environment removed
%\end{wrapfigure}

\section{Additional Results}
\subsection{TRUVAR Results}
We do not include TRUVAR by \citep{bogunovic2016truncated} in the main paper for the following reasons. (1) TRUVAR is \textbf{not a partition-based BO method} that aims at resolving the heteroscedasticity in BO by learning local models;
(2) It is prohibitive to run for large candidate sets as TRUVAR's acquisition function requires estimating the posterior variance reduction for all the remaining candidates. 
We observe on the 1D-toy dataset the simple regret is $0.121\pm 0.033$ by TRUVAR v.s. $0.0031 \pm 0.011$ by BALLET-ICI.

\vspace{-3mm}
\subsection{Exact-GP results}
\todo{add new graphs}
We Compare Exact-GP results on 1-D Toy, Nanophotonics, and Water converter configuration datasets with DKBO-AE as an ablation study of deep kernel learning. The choice of kernels and hypereparameters are identical to the deep kernel discussed in \secref{sec:exp} except for removing the latent space mapping and kernel interpolation. As is shown in \figref{fig:exp_append_RCI}, Exact-GP is consistently outperformed by DKBO-AE and \interCI.

% \vspace{-3mm}
\subsection{RCI results}
We compare DKBO-AE-RCI directly with DKBO-AE as the direct ablation study of the proposed acquisition function. The choice of kernels and hypereparameters are identical to the deep kernel discussed in \secref{sec:exp}. The acquisition function 
$ \UCBit_{\globalf, t}(\instance) - \LCB_{\globalf, t}$, which is similar to \eqref{eq:acqROI-ci}, is maximized over $\searchSpace$ instead of $\roi$. 

% Figure environment removed

As is shown \figref{fig:exp_append_RCI}, \interCI outperforms the baselines except on GB1, indicating the advantage of leveraging both global and local information together. \roiCI performs the best and DKBO-AE-RCI outperforms \interCI on GB1. This shows the benefits of identifying the ROI and optimizes on it, and the harm a potential discrepancy between the global model and the ROI model could be to the optimization.

\subsection{Larger Horizon for Protein Datasets}
\rebuttal{We provide additional large-scale empirical results on the Rosetta-86D and GB1-119D pre-collected protein design datasets. The results are collected from 10 independent 300-iteration trials for each experiment. 
\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
& DK-AE & BALLET-ROI & BALLET-Intersection & Avg Run Time (sec) \\
\hline
TS       & 4.58$\pm$0.16 (1.15e-03) & \textbf{3.73$\pm$0.17} & nan & 1010.7 \\
UCB      & 4.78$\pm$0.22 (1.05e-03) & 4.60$\pm$0.33 (1.89e-02) & 4.44$\pm$0.31 (3.50e-02) & 723.2 \\
EI       & 5.02$\pm$0.34 (2.11e-03) & 4.48$\pm$0.13 (1.45e-03) & nan & 789.2 \\
CI       & 4.61$\pm$0.16 (9.98e-04) & 4.72$\pm$0.35 (1.20e-02) & 4.21$\pm$0.40 (1.51e-01) & 667.7 \\
LA-MCTS  & 5.60$\pm$0.32 (5.84e-05) & nan & nan & 2521.7 \\
TuRBO-DK & 6.30$\pm$0.50 (9.64e-05) & nan & nan & 1205.4 \\
\hline
\end{tabular}
\caption{Performance comparison on GB1.}\label{tab:gb1_performance}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\hline
& DK-AE & BALLET-ROI & BALLET-Intersection & Avg Run Time (sec) \\
\hline
TS       & 3.95$\pm$0.18 (3.84e-04) & 3.51$\pm$0.40 (1.97e-02) & nan & 126.1 \\
UCB      & 3.55$\pm$0.30 (8.25e-03) & 2.43$\pm$0.42 (3.99e-01) & 2.43$\pm$0.42 (3.99e-01) & 93.9 \\
EI       & 3.38$\pm$0.38 (2.90e-02) & 3.01$\pm$0.39 (1.01e-01) & nan & 91.1 \\
CI       & 3.56$\pm$0.38 (1.43e-02) & 4.31$\pm$0.35 (5.38e-04) & \textbf{2.28$\pm$0.35} & 82.5 \\
LA-MCTS  & 3.02$\pm$0.46 (1.19e-01) & nan & nan & 1028.1 \\
TuRBO-DK & 3.80$\pm$0.32 (3.32e-03) & nan & nan & 433.1 \\
\hline
\end{tabular}
\caption{Performance comparison on Rosetta.}\label{tab:rosetta_performance}
\end{table}

We summarize the simple regrets of these two additional experiments in \tabref{tab:gb1_performance} and \tabref{tab:rosetta_performance}. We highlight the best results and report the p-value against all other methods in the parenthes. The results are shown in three columns: (1) using deep kernel only, (2) applying the acquisition function within the identified ROI, and (3) using the intersection of confidence intervals from both global and ROI models as the acquisition function. The average running times for a single trial of the right-most (fastest within the row) results are provided for each row. 


% Figure environment removed

The results demonstrate that using the \algname framework with each acquisition function leads to improvements over the DKBO-AE version. Additionally, comparing BALLET-Intersection and BALLET-ROI reveals the benefits of using the intersection of confidence intervals. The filtering ratio curve shown in \figref{fig: additional_ratio}, combined with these results, suggests that the proposed \interCI method accelerates the optimization process by shrinking the ROI more rapidly.}
% \newpage


\section{Discussions}

\subsection{Computational Cost}
In deep kernel learning, which is shown to bear strong empirical performance in regression and optimization 
(e.g.\citep{pmlr-v51-wilson16, wistuba2021few}), 
the learning cost is $\mathcal{O}(n)$ for $n$ training points, and the prediction cost is
$\mathcal{O}(1)$ 
per test point and is more efficient than the exact GP in terms of computational cost. Compared to the \textbf{significant experiment cost} in the real-world application BALLET is proposed for (e.g., cosmological design, protein study), the computational cost is negligible. % and hence not a major concern in BALLET. 
Meanwhile, the runtime of other partition-based algorithms depends on the 
\textbf{hyperparameters} of the partitioning heuristics,
e.g., K-means iterations in LA-MCTS, the number and size of trust regions in TuRBO.

\subsection{Limitation and Future Work}
We summarize the following limitations throughout the paper.
\begin{itemize}
    \item The analysis only applies to given discretization, while sampling and related work focus on the issue;
    \item It Doesnâ€™t help to learn an ROI GP when the objective has global uniformity. The global kernel itself forms a good surrogate GP.
    \item The analysis should be able to extend to more acquisition functions.
    \item Lack of analysis on top of the (deep) kernel learning. Though different from applying an exact GP through the optimization process, deep kernel learning has shown strong performance in regression and optimization tasks. The gap between DK-based BO and exact GP-based BO remains to be filled.
\end{itemize}



\end{appendix}
