\begin{abstract}

%Bayesian optimization (BO) has been applied in black-box optimization tasks which are common in domains including optimal experimental design, self-tuning systems, and hyperparameter optimization. Despite the strong theoretical guarantees for BO in canonical settings, it remains a significant challenge for BO to scale to high-dimensional and non-stationary scenarios. Recent works attempt to exploit the locality of the black-box functions in BO --- by partitioning the search space, these algorithms learn a Gaussian process model over regions of interest that better captures the locality of the black-box function. Various heuristics have been proposed along this direction, most relying on additional hyperparameters to be fine-tuned for specific tasks. (e.g., number of local regions/models to be considered, number of examples in each partition, etc.). 
We study Bayesian optimization (BO) in high-dimensional and non-stationary scenarios.  Existing algorithms for such scenarios typically require extensive hyperparameter tuning, which limits their practical effectiveness.
%Despite the promising performance of Bayesian optimization (BO) in canonical black-box optimization tasks, it remains a significant challenge for BO to scale to high-dimensional and non-stationary scenarios. 
%Recent works attempt to exploit the locality of the black-box functions in BO by partitioning the search space and learning a Gaussian process model over regions of interest. However, most existing heuristics are sensitive to many hyperparameters (e.g., the number of partitions, the number of examples in each partition etc.), which require significant effort to be fine-tuned.
%that better captures the locality of the black-box function. Various heuristics have been proposed along this direction, most relying on additional hyperparameters to be fine-tuned for specific tasks. (e.g., number of local regions/models to be considered, number of examples in each partition, etc.). 
%In this paper, 
We propose a framework, called \algname, which adaptively filters for a high-confidence region of interest (ROI) as a superlevel-set of a nonparametric probabilistic model such as a Gaussian process (GP).  Our approach is easy to tune, and is able to focus on local region of the optimization space that can be tackled by existing BO methods.  The key idea is to use two probabilistic models: a coarse GP to identify the ROI, and a localized GP for optimization within the ROI. We show theoretically that \algname can efficiently shrink the search space, and can exhibit a tighter regret bound than standard BO without ROI filtering.  We demonstrate empirically the effectiveness of \algname on both synthetic and real-world optimization tasks.
%We propose \algname, a simple yet effective framework for adaptively learning \emph{regions of interest} for Bayesian optimization. 
%Our model maintains two Gaussian processes: one global model for adaptively filtering the regions of interest as \emph{superlevel-sets}; the other model for acquiring data in the high-confidence regions of interest. This gives us the benefit of a nonparametric model for learning multiple regions of interest while having few hyperparameters to maintain. Our theoretical analyses show that \algname can efficiently shrink the search space with a confidence-interval-based acquisition, and exhibits a tighter regret bound than its counterpart without the filtering component. We empirically demonstrate the effectiveness of \algname on both synthetic and real-world optimization tasks.

% Bayesian optimization (BO) has been applied in black-box optimization tasks which are common in domains including optimal experimental design, self-tuning systems and hyperparameter optimization. Despite the strong theoretical guarantees for BO in canonical settings, it remains a significant challenge for BO to scale to high-dimensional and non-stationary scenarios. %the large-scale, high-dimensional and non-stationary characteristics of the real-world optimization tasks challenge Bayesian optimization and make it less competitive with other optimization methods. 
% %\TBD{It has been known that learning a global surrogate model for the complex black-box objective function of local heterogeneity and further, picking query candidates with a global acquisition function defined on the global surrogate model in the Bayesian optimization algorithms hinder the algorithm leverage of the locality of the objective function.} 
% %For black-box functions with non-stationary dynamics, relying on a single surrogate model for global optimization often leads to inferior performance. 
% Recent works attempt to exploit the locality of the black-box functions in BO ---
% %Recent works attempt to conduct global optimization with local optimization processes. 
% by partitioning the search space, these algorithms learn a Gaussian process model over regions of interests that better captures the locality of the black-box function. %given a few observations.Learning to partition online is also challenging in the optimization process given the limited access to the black-box function.
% Various heuristics have been proposed along this direction, most relying on additional hyperparameters (e.g., number of local regions/models to be considered, number of examples in each partition, etc.) to be fine-tuned for specific tasks. %Motivated by this challenge, 
% In this paper, we propose a simple yet effective framework for adaptively learning regions of interest for Bayesian optimization. Our model maintains two Gaussian processes: one global model for identifying the (local) regions of interest as (adaptive) level-sets; the other model for acquiring data in the high-confidence regions of interest. 
% %a global estimation of the upper confidence bound for the online partition of the search space and \TBD{allowing the algorithm to only learn one local Gaussian process and optimize on the high-confidence areas}. 
% %More concretely, the global Gaussian process is used to learn an adaptive level-set which, with high confidence, excludes labeled examples that are suboptimal. 
% This gives us the benefit of a non-parametric model for learning multiple regions of interest, while having few hyperparameters to maintain. We demonstrate the effectiveness of {\algname} empirically on both synthetic and real-world optimization tasks. %\yuxin{call out the protein engineering task as the application highlight that align with the theme of workshop}

% % \fengxue{Consider different naming {BALLET}.} 
% %We conduct extensive empirical study to examine the locality-based BO algorithms, and propose a simple framework that does not rely on tuning redundant hyperparameters,
% %Various heuristics have been proposed \TBD{which incurs additional challenge in fine-tuning the redundant hyperparameters of the heuristics.}. We propose a novel framework \TBD{\algname} using a global estimation of the upper confidence bound for the online partition of the search space and \TBD{allowing the algorithm to only learn one local Gaussian process and optimize on the high-confidence areas}. We demonstrate the effectiveness of \TBD{\algname} empirically on both synthetic and real-world optimization tasks.

% % \TBD{Deep kernerl?} 


\end{abstract}