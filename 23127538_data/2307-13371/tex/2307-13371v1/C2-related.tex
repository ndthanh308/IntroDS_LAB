\section{Related Work}
\paragraph{High-dimensional Bayesian optimization}

BO often uses Gaussian processes as a (mathematically) simple yet powerful tool to parametrize the black-box function. 
However, GPs are difficult to fit and \rebuttal{be applied in optimization} in the high-dimensional setting due to the curse of dimensionality; thus classical BO algorithms need to be modified for high-dimensional function classes \citep{djolonga2013high}. 

A class of methods leverages additional structures, such as additive GPs, to mitigate the challenge of training a single global GP. For instance, LineBO restricts its search space to a one-dimensional subspace with reduced sample complexity at each step \citep{kirschner2019adaptive}. GP-ThreDS relies on Hölder condition on the unknown objective to prune the search space, avoids the discretization cost exponential to the dimensionality of the search space, and speeds up the optimization \citep{salgia2021domain}. In contrast, we aim at the applications where no Hölder smoothness is guaranteed, while valid discrete candidates in the search space are given.


\rebuttal{Another line of work assumes that despite the high dimensionality of the original input, the effective dimensionality is smaller. Therefore, it is feasible to embed the high-dimensional input space into a lower-dimensional subspace using techniques such as random projection and variable selection, while preserving the desired properties for optimization \citep{song2022monte, wang2016bayesian, Letham2020Re, HeSBO19, papenmeier2022increasing}. Additionally, \citet{mcintire2016sparse, moss2023inducing} propose that a reduced set of points can effectively represent the original high-dimensional space without significantly sacrificing uncertainty quantification. They introduce sparse GP as an efficient surrogate for high-throughput Bayesian optimization. Although these works share the spirit of conducting Bayesian optimization on a reduced complexity set compared to the original high-dimensional input space, they can be integrated into any GP-based optimization framework. Hence we do not make comparison with them in experiments.}

\fengxue{Actually the original content could be well summarized by HDBO as a paragraph title. Hence, I keep it while differentiating the works most related to ours from other HDBO methods. and refer to the survey offered by the reviewer for a more comprehensive discussion over HDBO.}
Our proposed method relates most closely to methods with input space partitions \citep{wabersich2016advancing, eriksson2019scalable, wang2020learning, pmlr-v133-sazanovich21a}. Notably, TurBO \citep{eriksson2019scalable} maintains a collection of local GPs and allocates queries with a multi-armed bandit procedure. LA-MCTS \citep{wang2020learning} learns a partition of the input space and uses Monte Carlo tree search (MCTS) to decide a subspace to apply BO. Compared with the proposed \algname, these partitioning methods rely on heuristics and add extra complexity to the optimization task with hyperparameters of these heuristics, e.g., TuRBO relies on the number of trust regions and LA-MCTS relies on leaf size, a hyperparameter in UCB for the subspace selection and one for the partitioning algorithm. \footnote{\rebuttal{Due to the diverse range of interests and fields of work in HDBO, we recommend referring to the recent survey conducted by \cite{10.1145/3545611} for a more extensive discussion on High-dimensional Bayesian Optimization with Gaussian Processes.}}



\paragraph{Partition-based Bayesian active learning and optimization} 
Partition-based methods are common in BO with safety constraints \citep{pmlr-v133-sazanovich21a, sui2018stagewise, makarova2021risk}. These methods %make use of the 
use LCB from GPs to partition the input space into safe and unsafe subspaces. Subsequent optimizaton queries are restricted to the safe subspaces only. Another related work is the level set estimation (LSE) method by \citet{10.5555/2540128.2540322}, where the authors use both UCB and LCB to narrow down regions where a particular function value is likely to exist. A unified framework \rebuttal{TRUVAR} for BO and LSE task \citep{bogunovic2016truncated} proposes a similar filtering method but does not learn a local surrogate model. Instead, the filtering is used to constrain its acquisition function.  Our method {inherits} the spirit of LSE to leverage the confidence interval to adaptively partition the search space. % in a simple yet principled way. 


\paragraph{Partition-based optimization methods}
More broadly speaking, partitioning the input space is a general strategy employed by several optimization methods \citep{munos2011optimistic, 8187198, 7352306, JMLR:v22:18-220, kawaguchi2016global}. Simultaneous optimistic optimization (SOO) algorithm \citep{munos2011optimistic, 8187198}, which is a non-Bayesian approach that intelligently partitions the space based on observed experiments to effectively balance exploration and exploitation of the objective. A modification of SOO, named Locally Oriented Global Optimization (LOGO) \citep{kawaguchi2016global}, achieves both fast convergence in practice and a finite-time error bound in theory. However, 
these non-Bayesian approaches have seen more degraded empirical performance on high-dimensional functions than their Bayesian counterparts \citep{JMLR:v22:18-220}. % likely due to the absence of the potential benefits of posterior inference.
