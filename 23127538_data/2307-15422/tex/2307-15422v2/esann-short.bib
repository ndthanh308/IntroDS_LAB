@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@inproceedings{10.5555/645940.671380,
author = {B. Gu et al.},
title = {Modelling Classification Performance for Large Data Sets},
year = {2001},
isbn = {3540422986},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {For many learning algorithms, their learning accuracy will increase as the size of training data increases, forming the well-known learning curve. Usually a learning curve can be fitted by interpolating or extrapolating some points on it with a specified model. The obtained learning curve can then be used to predict the maximum achievable learning accuracy or to estimate the amount of data needed to achieve an expected learning accuracy, both of which will be especially meaningful to data mining on large data sets. Although some models have been proposed to model learning curves, most of them do not test their applicability to large data sets. In this paper, we focus on this issue. We empirically compare six potentially useful models by fitting learning curves of two typical classification algorithms--C4.5 (decision tree) and LOG (logistic discrimination) on eight large UCI benchmark data sets. By using all available data for learning, we fit a full-length learning curve; by using a small portion of the data, we fit a part-length learning curve. The models are then compared in terms of two performances: (1) how well they fit a full-length learning curve, and (2) how well a fitted part-length learning curve can predict learning accuracy at the full length. Experimental results show that the power law (y = a - b * x-c) is the best among the six models in both the performances for the two algorithms and all the data sets. These results support the applicability of learning curves to data mining.},
booktitle = {Proceedings of the Second International Conference on Advances in Web-Age Information Management},
pages = {317â€“328},
numpages = {12},
series = {WAIM '01}
}

@book{ghosh2006introduction,
  title={An introduction to Bayesian analysis: theory and methods},
  author={JK. Ghosh et al.},
  volume={},
  year={},
  publisher={}
}

@software{jax2018github,
  author = {J. Bradbury et al.},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{gavin2019levenberg,
  title={The Levenberg-Marquardt algorithm for nonlinear least squares curve-fitting problems},
  author={HP. Gavin},
  journal={Department of Civil and Environmental Engineering, Duke University},
  volume={19},
  year={2019}
}

@inproceedings{
    wu2018understanding,
    title={Understanding Short-Horizon Bias in Stochastic Meta-Optimization},
    author={Y. Wu et al.},
    booktitle={International Conference on Learning Representations},
    year={2018},
    url={https://openreview.net/forum?id=H1MczcgR-},
}

@inproceedings{bansal2022jahsbench,
  title={JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search},
  author={A. Bansal et al.},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@inproceedings{
    bohdal2023pasha,
    title={{PASHA}: Efficient {HPO} and {NAS} with Progressive Resource Allocation},
    author={O. Bohdal et al.},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=syfgJE6nFRW}
}

@inproceedings{pfisterer2022yahpo,
  title={Yahpo gym-an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization},
  author={F. Pfisterer et al.},
  booktitle={International Conference on Automated Machine Learning},
  pages={3--1},
  year={2022},
  organization={PMLR}
}

@article{yan2021bench,
  title={Nas-bench-x11 and the power of learning curves},
  author={Y. Shen et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22534--22549},
  year={2021}
}

@article{mohr2023fast,
  title={Fast and informative model selection using learning curve cross-validation},
  author={F. Mohr et al.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}

@inproceedings{mohr2023lcdb,
  title={LCDB 1.0: An extensive learning curves database for classification tasks},
  author={F. Mohr et al.},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19--23, 2022, Proceedings, Part V},
  pages={3--19},
  year={2023},
  organization={Springer}
}

@article{hutter_learning_2021,
  title={Learning curve theory},
  author={M. Hutter},
  journal={arXiv preprint arXiv:2102.04074},
  year={2021}
}

@article{bergstra_algorithms_2011,
  title={Algorithms for hyper-parameter optimization},
  author={J. Bergstra et al.},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{yu_hyper-parameter_2020,
  title={Hyper-parameter optimization: A review of algorithms and applications},
  author={T. Yu et al.},
  journal={arXiv preprint arXiv:2003.05689},
  year={2020}
}

@article{elsken_neural_2018,
  title={Neural architecture search: A survey},
  author={T. Elsken et al.},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1997--2017},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={K. Jamieson et al.},
  booktitle={Artificial intelligence and statistics},
  pages={240--248},
  year={2016},
  organization={PMLR}
}

@article{li_system_2020,
  title={A system for massively parallel hyperparameter tuning},
  author={L. Li et al.},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={230--246},
  year={2020}
}

@article{li2017hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={L. Li et al.},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6765--6816},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{domhan_speeding_2015,
  title={Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves},
  author={T. Domhan et al.},
  booktitle={Twenty-fourth international joint conference on artificial intelligence},
  year={2015}
}

@article{mohr_learning_2022,
  title={Learning Curves for Decision Making in Supervised Machine Learning--A Survey},
  author={F. Mohr et al.},
  journal={arXiv preprint arXiv:2201.12150},
  year={2022}
}

@article{baker_accelerating_2017,
  title={Accelerating neural architecture search using performance prediction},
  author={B. Baker et al.},
  journal={arXiv preprint arXiv:1705.10823},
  year={2017}
}

@inproceedings{klein_learning_2017,
  title={Learning curve prediction with Bayesian neural networks},
  author={A. Klein et al.},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@article{zimmer2021auto,
  title={Auto-pytorch: Multi-fidelity metalearning for efficient and robust autodl},
  author={L. Zimmer et al.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={9},
  pages={3079--3090},
  year={2021},
  publisher={IEEE}
}

@article{eggensperger_hpobench_2021,
  title={HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO},
  author={K. Eggensperger et al.},
  journal={arXiv preprint arXiv:2109.06716},
  year={2021}
}

@article{white_how_2021,
  title={How powerful are performance predictors in neural architecture search?},
  author={C. White et al.},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28454--28469},
  year={2021}
}

@inproceedings{wu2020practical,
  title={Practical multi-fidelity Bayesian optimization for hyperparameter tuning},
  author={J. Wu et al.},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={788--798},
  year={2020},
  organization={PMLR}
}

@inproceedings{falkner_bohb_2018,
  title={BOHB: Robust and efficient hyperparameter optimization at scale},
  author={S. Falkner et al.},
  booktitle={International Conference on Machine Learning},
  pages={1437--1446},
  year={2018},
  organization={PMLR}
}

@inproceedings{klein2017fast,
  title={Fast bayesian optimization of machine learning hyperparameters on large datasets},
  author={A. Klein et al.},
  booktitle={Artificial intelligence and statistics},
  pages={528--536},
  year={2017},
  organization={PMLR}
}

@article{swersky_freeze-thaw_2014,
  title={Freeze-thaw Bayesian optimization},
  author={K. Swersky et al.},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}


@inproceedings{bergstra_making_2013,
  title={Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author={J. Bergstra et al.},
  booktitle={International conference on machine learning},
  pages={115--123},
  year={2013},
  organization={PMLR}
}

@article{hutter2014algorithm,
  title={Algorithm runtime prediction: Methods \& evaluation},
  author={F. Hutter et al.},
  journal={Artificial Intelligence},
  volume={206},
  pages={79--111},
  year={2014},
  publisher={Elsevier}
}

@article{klein_tabular_2019,
  title={Tabular benchmarks for joint architecture and hyperparameter optimization},
  author={A. Klein et al.},
  journal={arXiv preprint arXiv:1905.04970},
  year={2019}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={F. Pedregosa et al.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}