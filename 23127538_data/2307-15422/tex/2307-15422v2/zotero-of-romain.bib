
@article{pfisterer_yahpo_2022,
	title = {{YAHPO} Gym - An Efficient Multi-Objective Multi-Fidelity Benchmark for Hyperparameter Optimization},
	abstract = {When developing and analyzing new hyperparameter optimization methods, it is vital to empirically evaluate and compare them on well-curated benchmark suites. In this work, we propose a new set of challenging and relevant benchmark problems motivated by desirable properties and requirements for such benchmarks. Our new surrogate-based benchmark collection consists of 14 scenarios that in total constitute over 700 multi-fidelity hyperparameter optimization problems, which all enable multi-objective hyperparameter optimization. Furthermore, we empirically compare surrogate-based benchmarks to the more widely-used tabular benchmarks, and demonstrate that the latter may produce unfaithful results regarding the performance ranking of {HPO} methods. We examine and compare our benchmark collection with respect to defined requirements and propose a single-objective as well as a multi-objective benchmark suite on which we compare 7 single-objective and 7 multi-objective optimizers in a benchmark experiment. Our software is available at [https://github.com/slds-lmu/yahpo\_gym].},
	author = {Pfisterer, Florian and Schneider, Lennart and Moosbauer, Julia and Binder, Martin and Bischl, Bernd},
	date = {2022},
	langid = {english},
}

@incollection{hutter_sequential_2011,
	location = {Berlin, Heidelberg},
	title = {Sequential Model-Based Optimization for General Algorithm Configuration},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_40},
	abstract = {State-of-the-art algorithms for hard computational problems often expose many parameters that can be modiﬁed to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm conﬁguration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the ﬁrst time to general algorithm conﬁguration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm conﬁguration procedure by optimizing a local search and a tree search solver for the propositional satisﬁability problem ({SAT}), as well as the commercial mixed integer programming ({MIP}) solver {CPLEX}. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best conﬁguration approach.},
	pages = {507--523},
	booktitle = {Learning and Intelligent Optimization},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	urldate = {2022-03-01},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-3-642-25566-3_40},
	note = {Series Title: Lecture Notes in Computer Science},
}

@inproceedings{awad_dehb_2021,
	location = {Montreal, Canada},
	title = {{DEHB}: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization},
	isbn = {978-0-9992411-9-6},
	url = {https://www.ijcai.org/proceedings/2021/296},
	doi = {10.24963/ijcai.2021/296},
	shorttitle = {{DEHB}},
	abstract = {Modern machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of Hyperparameter Optimization ({HPO}) more important than ever. Here, we combine the advantages of the popular bandit-based {HPO} method Hyperband ({HB}) and the evolutionary search approach of Differential Evolution ({DE}) to yield a new {HPO} method which we call {DEHB}. Comprehensive results on a very broad range of {HPO} problems, as well as a wide range of tabular benchmarks from neural architecture search, previous {HPO} methods we are aware of, especially for high-dimensional problems with discrete input dimensions. For example, {DEHB} is up to 1000× faster than random search. It is also efﬁcient in computational time, conceptually simple and easy to implement, positioning it well to become a new default {HPO} method.},
	eventtitle = {Thirtieth International Joint Conference on Artificial Intelligence \{{IJCAI}-21\}},
	pages = {2147--2153},
	booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
	urldate = {2023-03-15},
	date = {2021-08},
	langid = {english},
}

@article{wistuba_supervising_2022,
	title = {Supervising the Multi-Fidelity Race of Hyperparameter Configurations},
	abstract = {Multi-fidelity (gray-box) hyperparameter optimization techniques ({HPO}) have recently emerged as a promising direction for tuning Deep Learning methods. However, existing methods suffer from a sub-optimal allocation of the {HPO} budget to the hyperparameter configurations. In this work, we introduce {DyHPO}, a Bayesian Optimization method that learns to decide which hyperparameter configuration to train further in a dynamic race among all feasible configurations. We propose a new deep kernel for Gaussian Processes that embeds the learning curve dynamics, and an acquisition function that incorporates multi-budget information. We demonstrate the significant superiority of {DyHPO} against state-of-the-art hyperparameter optimization methods through large-scale experiments comprising 50 datasets (Tabular, Image, {NLP}) and diverse architectures ({MLP}, {CNN}/{NAS}, {RNN}).},
	journaltitle = {Thirty-Sixth Conference on Neural Information Processing Systems},
	author = {Wistuba, Martin and Kadra, Arlind and Grabocka, Josif},
	date = {2022},
	langid = {english},
}

@article{northcutt_confident_2021,
	title = {Confident Learning: Estimating Uncertainty in Dataset Labels},
	volume = {70},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/12125},
	doi = {10.1613/jair.1.12125},
	shorttitle = {Confident Learning},
	abstract = {Learning exists in the context of data, yet notions of conﬁdence typically focus on model predictions, not label quality. Conﬁdent learning ({CL}) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with conﬁdence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized {CL} which is provably consistent and experimentally performant. We present suﬃcient conditions where {CL} exactly ﬁnds label errors, and show {CL} performance exceeding seven recent competitive approaches for learning with noisy labels on the {CIFAR} dataset. Uniquely, the {CL} framework is not coupled to a speciﬁc data modality or model (e.g., we use {CL} to ﬁnd several label errors in the presumed error-free {MNIST} dataset and improve sentiment classiﬁcation on text data in Amazon Reviews). We also employ {CL} on {ImageNet} to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for {ResNet}) by cleaning data prior to training. These results are replicable using the open-source cleanlab release.},
	pages = {1373--1411},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	author = {Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
	urldate = {2023-03-06},
	date = {2021-04-14},
	langid = {english},
}

@article{domingos_unied_nodate,
	title = {A Uniﬁed Bias-Variance Decomposition},
	abstract = {The bias-variance decomposition is a very useful and widely-used tool for understanding machine-learning algorithms. It was originally developed for squared loss. In recent years, several authors have proposed decompositions for zero-one loss, but each has signiﬁcant shortcomings. In particular, all of these decompositions have only an intuitive relationship to the original squared-loss one. In this article, we deﬁne bias and variance for an arbitrary loss function, and show that the resulting decomposition specializes to the standard one for the squared-loss case, and to a close relative of Kong and Dietterich’s (1995) one for the zero-one case. The same decomposition also applies to variable misclassiﬁcation costs. We show a number of interesting consequences of the uniﬁed deﬁnition. For example, Schapire et al.’s (1997) notion of “margin” can be expressed as a function of the zero-one bias and variance, making it possible to formally relate a classiﬁer ensemble’s generalization error to the base learner’s bias and variance on training examples. We have applied the proposed decomposition to decision tree learning, nearest-neighbor learning and boosting on a large suite of benchmark datasets, and made several signiﬁcant observations.},
	author = {Domingos, Pedro},
	langid = {english},
}

@article{breiman_statistical_nodate,
	title = {Statistical Modeling: The Two Cultures},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in ﬁelds outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a ﬁeld is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	journaltitle = {{THE} {TWO} {CULTURES}},
	author = {Breiman, Leo},
	langid = {english},
}

@misc{kuan_back_2022,
	title = {Back to the Basics: Revisiting Out-of-Distribution Detection Baselines},
	url = {http://arxiv.org/abs/2207.03061},
	shorttitle = {Back to the Basics},
	abstract = {We study simple methods for out-of-distribution ({OOD}) image detection that are compatible with any already trained classiﬁer, relying on only its predictions or learned representations. Evaluating the {OOD} detection performance of various methods when utilized with {ResNet}-50 and Swin Transformer models, we ﬁnd methods that solely consider the model’s predictions can be easily outperformed by also considering the learned representations. Based on our analysis, we advocate for a dead-simple approach that has been neglected in other studies: simply ﬂag as {OOD} images whose average distance to their K nearest neighbors is large (in the representation space of an image classiﬁer trained on the in-distribution data).},
	publisher = {{arXiv}},
	author = {Kuan, Johnson and Mueller, Jonas},
	urldate = {2023-02-21},
	date = {2022-07-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.03061 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{seedat_data-iq_2022,
	title = {Data-{IQ}: Characterizing subgroups with heterogeneous outcomes in tabular data},
	url = {http://arxiv.org/abs/2210.13043},
	shorttitle = {Data-{IQ}},
	abstract = {High model performance, on average, can hide that models may systematically underperform on subgroups of the data. We consider the tabular setting, which surfaces the unique issue of outcome heterogeneity - this is prevalent in areas such as healthcare, where patients with similar features can have different outcomes, thus making reliable predictions challenging. To tackle this, we propose Data-{IQ}, a framework to systematically stratify examples into subgroups with respect to their outcomes. We do this by analyzing the behavior of individual examples during training, based on their predictive conﬁdence and, importantly, the aleatoric (data) uncertainty. Capturing the aleatoric uncertainty permits a principled characterization and then subsequent stratiﬁcation of data examples into three distinct subgroups (Easy, Ambiguous, Hard). We experimentally demonstrate the beneﬁts of Data-{IQ} on four real-world medical datasets. We show that Data-{IQ}’s characterization of examples is most robust to variation across similarly performant (yet different) models, compared to baselines. Since Data-{IQ} can be used with any {ML} model (including neural networks, gradient boosting etc.), this property ensures consistency of data characterization, while allowing ﬂexible model selection. Taking this a step further, we demonstrate that the subgroups enable us to construct new approaches to both feature acquisition and dataset selection. Furthermore, we highlight how the subgroups can inform reliable model usage, noting the signiﬁcant impact of the Ambiguous subgroup on model generalization.},
	publisher = {{arXiv}},
	author = {Seedat, Nabeel and Crabbé, Jonathan and Bica, Ioana and van der Schaar, Mihaela},
	urldate = {2023-02-21},
	date = {2022-10-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2210.13043 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{swayamdipta_dataset_2020,
	location = {Online},
	title = {Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.746},
	doi = {10.18653/v1/2020.emnlp-main.746},
	shorttitle = {Dataset Cartography},
	abstract = {Large datasets have become commonplace in {NLP} research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps—a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example—the model’s conﬁdence in the true class, and the variability of this conﬁdence across epochs—obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of ambiguous regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are easy to learn for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model ﬁnds hard to learn; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-ofdistribution generalization.},
	eventtitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	pages = {9275--9293},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Swayamdipta, Swabha and Schwartz, Roy and Lourie, Nicholas and Wang, Yizhong and Hajishirzi, Hannaneh and Smith, Noah A. and Choi, Yejin},
	urldate = {2023-02-21},
	date = {2020},
	langid = {english},
}

@inproceedings{balaprakash_scalable_2019,
	title = {Scalable Reinforcement-Learning-Based Neural Architecture Search for Cancer Deep Learning Research},
	url = {http://arxiv.org/abs/1909.00311},
	doi = {10.1145/3295500.3356202},
	abstract = {Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of datadriven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a timeconsuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
	pages = {1--33},
	booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	author = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Wild, Stefan and Vishwanath, Venkatram and Xia, Fangfang and Brettin, Tom and Stevens, Rick},
	urldate = {2023-02-21},
	date = {2019-11-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.00311 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{egele_asynchronous_2022,
	title = {Asynchronous Distributed Bayesian Optimization at {HPC} Scale},
	url = {http://arxiv.org/abs/2207.00479},
	abstract = {Bayesian optimization ({BO}) is a widely used approach for computationally expensive black-box optimization such as simulator calibration and hyperparameter optimization of deep learning methods. In {BO}, a dynamically updated computationally cheap surrogate model is employed to learn the input-output relationship of the black-box function; this surrogate model is used to explore and exploit the promising regions of the input space. Multipoint {BO} methods adopt a single manager/multiple workers strategy to achieve high-quality solutions in shorter time. However, the computational overhead in multipoint generation schemes is a major bottleneck in designing {BO} methods that can scale to thousands of workers. We present an asynchronous-distributed {BO} ({ADBO}) method wherein each worker runs a search and asynchronously communicates the input-output values of black-box evaluations from all other workers without the manager. We scale our method up to 4,096 workers and demonstrate improvement in the quality of the solution and faster convergence. We demonstrate the effectiveness of our approach for tuning the hyperparameters of neural networks from the Exascale computing project {CANDLE} benchmarks.},
	publisher = {{arXiv}},
	author = {Egele, Romain and Gouneau, Joceran and Vishwanath, Venkatram and Guyon, Isabelle and Balaprakash, Prasanna},
	urldate = {2023-02-21},
	date = {2022-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.00479 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{egele_autodeuq_2022,
	title = {{AutoDEUQ}: Automated Deep Ensemble with Uncertainty Quantification},
	url = {http://arxiv.org/abs/2110.13511},
	shorttitle = {{AutoDEUQ}},
	abstract = {Deep neural networks are powerful predictors for a variety of tasks. However, they do not capture uncertainty directly. Using neural network ensembles to quantify uncertainty is competitive with approaches based on Bayesian neural networks while beneﬁting from better computational scalability. However, building ensembles of neural networks is a challenging task because, in addition to choosing the right neural architecture or hyperparameters for each member of the ensemble, there is an added cost of training each model. To address this issue, we propose {AutoDEUQ}, an automated approach for generating an ensemble of deep neural networks. Our approach leverages joint neural architecture and hyperparameter search to generate ensembles. We use the law of total variance to decompose the predictive variance of deep ensembles into aleatoric (data) and epistemic (model) uncertainties. We show that {AutoDEUQ} outperforms probabilistic backpropagation, Monte Carlo dropout, deep ensemble, distribution-free ensembles, and hyper ensemble methods on a number of regression benchmarks.},
	publisher = {{arXiv}},
	author = {Egele, Romain and Maulik, Romit and Raghavan, Krishnan and Lusch, Bethany and Guyon, Isabelle and Balaprakash, Prasanna},
	urldate = {2023-02-21},
	date = {2022-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.13511 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{maulik_recurrent_2020,
	title = {Recurrent Neural Network Architecture Search for Geophysical Emulation},
	url = {http://arxiv.org/abs/2004.10928},
	abstract = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. Constructing neural networks for forecasting such data is nontrivial, however, and often requires trial and error. To address these limitations, we focus on developing proper-orthogonaldecomposition-based long short-term memory networks ({PODLSTMs}). We develop a scalable neural architecture search for generating stacked {LSTMs} to forecast temperature in the {NOAA} Optimum Interpolation Sea-Surface Temperature data set. Our approach identiﬁes {POD}-{LSTMs} that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
	publisher = {{arXiv}},
	author = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
	urldate = {2023-02-21},
	date = {2020-08-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.10928 [physics]},
	keywords = {Physics - Computational Physics},
}

@article{dorier_hpc_nodate,
	title = {{HPC} Storage Service Autotuning Using Variational-Autoencoder-Guided Asynchronous Bayesian Optimization},
	abstract = {Distributed data storage services tailored to speciﬁc applications have grown popular in the high-performance computing ({HPC}) community as a way to address I/O and storage challenges. These services offer a variety of speciﬁc interfaces, semantics, and data representations. They also expose many tuning parameters, making it difﬁcult for their users to ﬁnd the best conﬁguration for a given workload and platform. To address this issue, we develop a novel variational-autoencoderguided asynchronous Bayesian optimization method to tune {HPC} storage service parameters. Our approach uses transfer learning to leverage prior tuning results and use a dynamically updated surrogate model to explore the large parameter search space in a systematic way. We implement our approach within the {DeepHyper} open-source framework, and apply it to the autotuning of a high-energy physics workﬂow on Argonne’s Theta supercomputer. We show that our transfer-learning approach enables a more than 40× search speedup over random search, compared with a 2.5× to 10× speedup when not using transfer learning. Additionally, we show that our approach is on par with state-of-the-art autotuning frameworks in speed and outperforms them in resource utilization and parallelization capabilities.},
	pages = {13},
	author = {Dorier, Matthieu and Egele, Romain},
	langid = {english},
}

@article{sorscher_beyond_nodate,
	title = {Beyond neural scaling laws: beating power law scaling via data pruning},
	abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on {ResNets} trained on {CIFAR}-10, {SVHN}, and {ImageNet}. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on {ImageNet}. We find most existing high performing metrics scale poorly to {ImageNet}, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
	author = {Sorscher, Ben and Geirhos, Robert and Ganguli, Surya and Shekhar, Shashank and Morcos, Ari S},
	langid = {english},
}

@misc{mazumder_dataperf_2022,
	title = {{DataPerf}: Benchmarks for Data-Centric {AI} Development},
	url = {http://arxiv.org/abs/2207.10062},
	shorttitle = {{DataPerf}},
	abstract = {Machine learning ({ML}) research has generally focused on models, while the most prominent datasets have been employed for everyday {ML} tasks without regard for the breadth, difﬁculty, and faithfulness of these datasets to the underlying problem. Neglecting the fundamental importance of datasets has caused major problems involving data cascades in real-world applications and saturation of dataset-driven criteria for model quality, hindering research growth. To solve this problem, we present {DataPerf}, a benchmark package for evaluating {ML} datasets and datasetworking algorithms. We intend it to enable the “data ratchet,” in which training sets will aid in evaluating test sets on the same problems, and vice versa. Such a feedback-driven strategy will generate a virtuous loop that will accelerate development of data-centric {AI}. The {MLCommons} Association will maintain {DataPerf}.},
	publisher = {{arXiv}},
	author = {Mazumder, Mark and Banbury, Colby and Yao, Xiaozhe and Karlaš, Bojan and Rojas, William Gaviria and Diamos, Sudnya and Diamos, Greg and He, Lynn and Kiela, Douwe and Jurado, David and Kanter, David and Mosquera, Rafael and Ciro, Juan and Aroyo, Lora and Acun, Bilge and Eyuboglu, Sabri and Ghorbani, Amirata and Goodman, Emmett and Kane, Tariq and Kirkpatrick, Christine R. and Kuo, Tzu-Sheng and Mueller, Jonas and Thrush, Tristan and Vanschoren, Joaquin and Warren, Margaret and Williams, Adina and Yeung, Serena and Ardalani, Newsha and Paritosh, Praveen and Zhang, Ce and Zou, James and Wu, Carole-Jean and Coleman, Cody and Ng, Andrew and Mattson, Peter and Reddi, Vijay Janapa},
	urldate = {2023-02-06},
	date = {2022-07-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.10062 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{bergstra_algorithms_2011,
	title = {Algorithms for Hyper-Parameter Optimization},
	abstract = {Several recent advances to the state of the art in image classiﬁcation benchmarks have come from better conﬁgurations of existing techniques rather than novel approaches to feature learning. Traditionally, hyper-parameter optimization has been the job of humans because they can be very efﬁcient in regimes where only a few trials are possible. Presently, computer clusters and {GPU} processors make it possible to run more trials and we show that algorithmic approaches can ﬁnd better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks ({DBNs}). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufﬁciently efﬁcient for learning neural networks for several datasets, but we show it is unreliable for training {DBNs}. The sequential algorithms are applied to the most difﬁcult {DBN} learning problems from [1] and ﬁnd signiﬁcantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y{\textbar}x) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.},
	pages = {9},
	author = {Bergstra, James S and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	date = {2011},
	langid = {english},
}

@article{jamieson_non-stochastic_2015,
	title = {Non-stochastic Best Arm Identiﬁcation and Hyperparameter Optimization},
	abstract = {Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identiﬁcation problem. We identify an attractive algorithm for this setting that makes no assumptions on the convergence behavior of the arms’ losses, has no free-parameters to adjust, provably outperforms the uniform allocation baseline in favorable conditions, and performs comparably (up to log factors) otherwise. Next, by leveraging the iterative nature of many learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identiﬁcation. Our empirical results show that, by allocating more resources to promising hyperparameter settings, our approach achieves comparable test accuracies an order of magnitude faster than the uniform strategy. The robustness and simplicity of our approach makes it well-suited to ultimately replace the uniform strategy currently used in most machine learning software packages.},
	pages = {9},
	author = {Jamieson, Kevin and Talwalkar, Ameet},
	date = {2015},
	langid = {english},
}

@article{bergstra_making_2013,
	title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
	abstract = {Many computer vision algorithms depend on conﬁguration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes diﬃcult to know whether a given technique is genuinely better, or simply better tuned.},
	pages = {9},
	author = {Bergstra, J and Yamins, D and Cox, D D},
	date = {2013},
	langid = {english},
}

@misc{white_how_2021,
	title = {How Powerful are Performance Predictors in Neural Architecture Search?},
	url = {http://arxiv.org/abs/2104.01177},
	abstract = {Early methods in the rapidly developing ﬁeld of neural architecture search ({NAS}) required fully training thousands of neural networks. To reduce this extreme computational cost, dozens of techniques have since been proposed to predict the ﬁnal performance of neural architectures. Despite the success of such performance prediction methods, it is not well-understood how different families of techniques compare to one another, due to the lack of an agreed-upon evaluation metric and optimization for different constraints on the initialization time and query time. In this work, we give the ﬁrst large-scale study of performance predictors by analyzing 31 techniques ranging from learning curve extrapolation, to weight-sharing, to supervised learning, to zero-cost proxies. We test a number of correlation- and rank-based performance measures in a variety of settings, as well as the ability of each technique to speed up predictor-based {NAS} frameworks. Our results act as recommendations for the best predictors to use in different settings, and we show that certain families of predictors can be combined to achieve even better predictive power, opening up promising research directions. Our code, featuring a library of 31 performance predictors, is available at https://github.com/automl/naslib.},
	publisher = {{arXiv}},
	author = {White, Colin and Zela, Arber and Ru, Binxin and Liu, Yang and Hutter, Frank},
	urldate = {2023-01-19},
	date = {2021-10-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.01177 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{saxe_random_nodate,
	title = {On Random Weights and Unsupervised Feature Learning},
	abstract = {Recently two anomalous results in the literature have shown that certain feature learning architectures can yield useful features for object recognition tasks even with untrained, random weights. In this paper we pose the question: why do random weights sometimes do so well? Our answer is that certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights. Based on this we demonstrate the viability of extremely fast architecture search by using random weights to evaluate candidate architectures, thereby sidestepping the timeconsuming learning process. We then show that a surprising fraction of the performance of certain state-of-the-art methods can be attributed to the architecture alone.},
	author = {Saxe, Andrew M and Koh, Pang Wei and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y},
	langid = {english},
}

@article{yan_nas-bench-x11_2021,
	title = {{NAS}-Bench-x11 and the Power of Learning Curves},
	abstract = {While early research in neural architecture search ({NAS}) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of {NAS} research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-ﬁdelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, {NAS}-Bench-111, {NAS}-Bench-311, and {NAS}-Bench-{NLP}11, that output the full training information for each architecture, rather than just the ﬁnal validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-ﬁdelity algorithms, showing that it leads to improvements over popular single-ﬁdelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11.},
	author = {Yan, Shen and White, Colin and Savani, Yash and Hutter, Frank},
	date = {2021},
	langid = {english},
}

@article{klein_fast_2016,
	title = {Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets},
	abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for large datasets, training and validating a single conﬁguration often takes hours, days, or even weeks, which limits the achievable performance. To accelerate hyperparameter optimization, we propose a generative model for the validation error as a function of training set size, which is learned during the optimization process and allows exploration of preliminary conﬁgurations on small subsets, by extrapolating to the full dataset. We construct a Bayesian optimization procedure, dubbed {FABOLAS}, which models loss and training time as a function of dataset size and automatically trades off high information gain about the global optimum against computational cost. Experiments optimizing support vector machines and deep neural networks show that {FABOLAS} often ﬁnds high-quality solutions 10 to 100 times faster than other state-of-the-art Bayesian optimization methods or the recently proposed bandit strategy Hyperband.},
	author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
	date = {2016},
	langid = {english},
}

@article{domhan_speeding_2015,
	title = {Speeding up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves},
	abstract = {Deep neural networks ({DNNs}) show very strong performance on many machine learning problems, but they are very sensitive to the setting of their hyperparameters. Automated hyperparameter optimization methods have recently been shown to yield settings competitive with those found by human experts, but their widespread adoption is hampered by the fact that they require more computational resources than human experts. Humans have one advantage: when they evaluate a poor hyperparameter setting they can quickly detect (after a few steps of stochastic gradient descent) that the resulting network performs poorly and terminate the corresponding evaluation to save time. In this paper, we mimic the early termination of bad runs using a probabilistic model that extrapolates the performance from the ﬁrst part of a learning curve. Experiments with a broad range of neural network architectures on various prominent object recognition benchmarks show that our resulting approach speeds up state-of-the-art hyperparameter optimization methods for {DNNs} roughly twofold, enabling them to ﬁnd {DNN} settings that yield better performance than those chosen by human experts.},
	author = {Domhan, Tobias and Springenberg, Jost Tobias and Hutter, Frank},
	date = {2015},
	langid = {english},
}

@incollection{montavon_stochastic_2012,
	location = {Berlin, Heidelberg},
	title = {Stochastic Gradient Descent Tricks},
	volume = {7700},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {http://link.springer.com/10.1007/978-3-642-35289-8_25},
	abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent ({SGD}). This chapter provides background material, explains why {SGD} is a good learning algorithm when the training set is large, and provides useful recommendations.},
	pages = {421--436},
	booktitle = {Neural Networks: Tricks of the Trade},
	publisher = {Springer Berlin Heidelberg},
	author = {Bottou, Léon},
	editor = {Montavon, Grégoire and Orr, Geneviève B. and Müller, Klaus-Robert},
	urldate = {2023-01-16},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-35289-8_25},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{nickson_automated_2014,
	title = {Automated Machine Learning on Big Data using Stochastic Algorithm Tuning},
	url = {http://arxiv.org/abs/1407.7969},
	abstract = {We introduce a means of automating machine learning ({ML}) for big data tasks, by performing scalable stochastic Bayesian optimisation of {ML} algorithm parameters and hyper-parameters. More often than not, the critical tuning of {ML} algorithm parameters has relied on domain expertise from experts, along with laborious handtuning, brute search or lengthy sampling runs. Against this background, Bayesian optimisation is ﬁnding increasing use in automating parameter tuning, making {ML} algorithms accessible even to non-experts. However, the state of the art in Bayesian optimisation is incapable of scaling to the large number of evaluations of algorithm performance required to ﬁt realistic models to complex, big data. We here describe a stochastic, sparse, Bayesian optimisation strategy to solve this problem, using many thousands of noisy evaluations of algorithm performance on subsets of data in order to effectively train algorithms for big data. We provide a comprehensive benchmarking of possible sparsiﬁcation strategies for Bayesian optimisation, concluding that a Nystro¨m approximation offers the best scaling and performance for real tasks. Our proposed algorithm demonstrates substantial improvement over the state of the art in tuning the parameters of a Gaussian Process time series prediction task on real, big data.},
	publisher = {{arXiv}},
	author = {Nickson, Thomas and Osborne, Michael A. and Reece, Steven and Roberts, Stephen J.},
	urldate = {2023-01-16},
	date = {2014-07-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1407.7969 [stat]},
	keywords = {Statistics - Machine Learning},
}

@misc{radhakrishnan_feature_2022,
	title = {Feature learning in neural networks and kernel machines that recursively learn features},
	url = {http://arxiv.org/abs/2212.13881},
	abstract = {Neural networks have achieved impressive results on many technological and scientiﬁc tasks. Yet, their empirical successes have outpaced our fundamental understanding of their structure and function. By identifying mechanisms driving the successes of neural networks, we can provide principled approaches for improving neural network performance and develop simple and eﬀective alternatives. In this work, we isolate the key mechanism driving feature learning in fully connected neural networks by connecting neural feature learning to the average gradient outer product. We subsequently leverage this mechanism to design Recursive Feature Machines ({RFMs}), which are kernel machines that learn features. We show that {RFMs} (1) accurately capture features learned by deep fully connected neural networks, (2) close the gap between kernel machines and fully connected networks, and (3) surpass a broad spectrum of models including neural networks on tabular data. Furthermore, we demonstrate that {RFMs} shed light on recently observed deep learning phenomena such as grokking, lottery tickets, simplicity biases, and spurious features. We provide a Python implementation to make our method broadly accessible [{GitHub}].},
	publisher = {{arXiv}},
	author = {Radhakrishnan, Adityanarayanan and Beaglehole, Daniel and Pandit, Parthe and Belkin, Mikhail},
	urldate = {2023-01-10},
	date = {2022-12-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2212.13881 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{baker_accelerating_2017,
	title = {Accelerating Neural Architecture Search using Performance Prediction},
	url = {http://arxiv.org/abs/1705.10823},
	abstract = {Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model conﬁgurations. In this paper, we show that standard frequentist regression models can predict the ﬁnal performance of partially trained model conﬁgurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict ﬁnal performance in both visual classiﬁcation and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model conﬁgurations.},
	publisher = {{arXiv}},
	author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
	urldate = {2023-01-10},
	date = {2017-11-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1705.10823 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yan_nas-bench-x11_nodate,
	title = {{NAS}-Bench-x11 and the Power of Learning Curves},
	abstract = {While early research in neural architecture search ({NAS}) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of {NAS} research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-ﬁdelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, {NAS}-Bench-111, {NAS}-Bench-311, and {NAS}-Bench-{NLP}11, that output the full training information for each architecture, rather than just the ﬁnal validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-ﬁdelity algorithms, showing that it leads to improvements over popular single-ﬁdelity algorithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11.},
	author = {Yan, Shen and White, Colin and Savani, Yash and Hutter, Frank},
	langid = {english},
}

@article{klein_learning_2017,
	title = {Learning Cruve Prediction with Bayesian Neural Networks},
	abstract = {Different neural network architectures, hyperparameters and training protocols lead to different performances as a function of time. Human experts routinely inspect the resulting learning curves to quickly terminate runs with poor hyperparameter settings and thereby considerably speed up manual hyperparameter optimization. The same information can be exploited in automatic hyperparameter optimization by means of a probabilistic model of learning curves across hyperparameter settings. Here, we study the use of Bayesian neural networks for this purpose and improve their performance by a specialized learning curve layer.},
	author = {Klein, Aaron and Falkner, Stefan and Springenberg, Jost Tobias and Hutter, Frank},
	date = {2017},
	langid = {english},
}

@misc{balandat_botorch_2020,
	title = {{BoTorch}: A Framework for Efficient Monte-Carlo Bayesian Optimization},
	url = {http://arxiv.org/abs/1910.06403},
	shorttitle = {{BoTorch}},
	abstract = {Bayesian optimization provides sample-efﬁcient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce {BOTORCH}, a modern programming framework for Bayesian optimization that combines Monte-Carlo ({MC}) acquisition functions, a novel sample average approximation optimization approach, autodifferentiation, and variance reduction techniques. {BOTORCH}’s modular design facilitates ﬂexible speciﬁcation and optimization of probabilistic models written in {PyTorch}, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel “one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efﬁciency of {BOTORCH} relative to other popular libraries.},
	publisher = {{arXiv}},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
	urldate = {2023-01-05},
	date = {2020-12-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.06403 [cs, math, stat]},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{poloczek_multi-information_2016,
	title = {Multi-Information Source Optimization},
	url = {http://arxiv.org/abs/1603.00389},
	abstract = {We consider Bayesian optimization of an expensive-to-evaluate black-box objective function, where we also have access to cheaper approximations of the objective. In general, such approximations arise in applications such as reinforcement learning, engineering, and the natural sciences, and are subject to an inherent, unknown bias. This model discrepancy is caused by an inadequate internal model that deviates from reality and can vary over the domain, making the utilization of these approximations a non-trivial task. We present a novel algorithm that provides a rigorous mathematical treatment of the uncertainties arising from model discrepancies and noisy observations. Its optimization decisions rely on a value of information analysis that extends the Knowledge Gradient factor to the setting of multiple information sources that vary in cost: each sampling decision maximizes the predicted benefit per unit cost. We conduct an experimental evaluation that demonstrates that the method consistently outperforms other state-of-the-art techniques: it finds designs of considerably higher objective value and additionally inflicts less cost in the exploration process.},
	publisher = {{arXiv}},
	author = {Poloczek, Matthias and Wang, Jialei and Frazier, Peter I.},
	urldate = {2023-01-05},
	date = {2016-11-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1603.00389 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{mohr_lcdb_2021,
	title = {{LCDB} 1.0: An Extensive Learning Curves Database for Classiﬁcation Tasks},
	abstract = {The use of learning curves for decision making in supervised machine learning is standard practice, yet understanding of their behavior is rather limited. To facilitate a deepening of our knowledge, we introduce the Learning Curve Database ({LCDB}), which contains empirical learning curves of 20 classiﬁcation algorithms on 246 datasets. One of the {LCDB}’s unique strength is that it contains all (probabilistic) predictions, which allows for building learning curves of arbitrary metrics. Moreover, it uniﬁes the properties of similar high quality databases in that it (i) deﬁnes clean splits between training, validation, and test data, (ii) provides training times, and (iii) provides an {API} for convenient access (pip install lcdb). We demonstrate the utility of {LCDB} by analyzing some learning curve phenomena, such as convexity, monotonicity, peaking, and curve shapes. Improving our understanding of these matters is essential for eﬃcient use of learning curves for model selection, speeding up model training, and to determine the value of more training data.},
	pages = {17},
	author = {Mohr, Felix and Viering, Tom J and Loog, Marco and van Rijn, Jan N},
	date = {2021},
	langid = {english},
}

@incollection{fromont_fast_2015,
	location = {Cham},
	title = {Fast Algorithm Selection Using Learning Curves},
	volume = {9385},
	isbn = {978-3-319-24464-8 978-3-319-24465-5},
	url = {http://link.springer.com/10.1007/978-3-319-24465-5_26},
	abstract = {One of the challenges in Machine Learning to ﬁnd a classiﬁer and parameter settings that work well on a given dataset. Evaluating all possible combinations typically takes too much time, hence many solutions have been proposed that attempt to predict which classiﬁers are most promising to try. As the ﬁrst recommended classiﬁer is not always the correct choice, multiple recommendations should be made, making this a ranking problem rather than a classiﬁcation problem. Even though this is a well studied problem, there is currently no good way of evaluating such rankings. We advocate the use of Loss Time Curves, as used in the optimization literature. These visualize the amount of budget (time) needed to converge to a acceptable solution. We also investigate a method that utilizes the measured performances of classiﬁers on small samples of data to make such recommendation, and adapt it so that it works well in Loss Time space. Experimental results show that this method converges extremely fast to an acceptable solution.},
	pages = {298--309},
	booktitle = {Advances in Intelligent Data Analysis {XIV}},
	publisher = {Springer International Publishing},
	author = {van Rijn, Jan N. and Abdulrahman, Salisu Mamman and Brazdil, Pavel and Vanschoren, Joaquin},
	editor = {Fromont, Elisa and De Bie, Tijl and van Leeuwen, Matthijs},
	urldate = {2023-01-02},
	date = {2015},
	langid = {english},
	doi = {10.1007/978-3-319-24465-5_26},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{kandasamy_multi-delity_2017,
	title = {Multi-ﬁdelity Bayesian Optimisation with Continuous Approximations},
	abstract = {Bandit methods for black-box optimisation, such as Bayesian optimisation, are used in a variety of applications including hyper-parameter tuning and experiment design. Recently, multiﬁdelity methods have garnered considerable attention since function evaluations have become increasingly expensive in such applications. Multiﬁdelity methods use cheap approximations to the function of interest to speed up the overall optimisation process. However, most multi-ﬁdelity methods assume only a ﬁnite number of approximations. On the other hand, in many practical applications, a continuous spectrum of approximations might be available. For instance, when tuning an expensive neural network, one might choose to approximate the cross validation performance using less data N and/or few training iterations T . Here, the approximations are best viewed as arising out of a continuous two dimensional space (N, T ). In this work, we develop a Bayesian optimisation method, {BOCA}, for this setting. We characterise its theoretical properties and show that it achieves better regret than than strategies which ignore the approximations. {BOCA} outperforms several other baselines in synthetic and real experiments.},
	author = {Kandasamy, Kirthevasan and Dasarathy, Gautam and Schneider, Jeff and Poczos, Barnabas},
	date = {2017},
	langid = {english},
}

@misc{mohr_fast_2021,
	title = {Fast and Informative Model Selection using Learning Curve Cross-Validation},
	url = {http://arxiv.org/abs/2111.13914},
	abstract = {Common cross-validation ({CV}) methods like k-fold cross-validation or Monte-Carlo cross-validation estimate the predictive performance of a learner by repeatedly training it on a large portion of the given data and testing on the remaining data. These techniques have two major drawbacks. First, they can be unnecessarily slow on large datasets. Second, beyond an estimation of the ﬁnal performance, they give almost no insights into the learning process of the validated algorithm. In this paper, we present a new approach for validation based on learning curves ({LCCV}). Instead of creating train-test splits with a large portion of training data, {LCCV} iteratively increases the number of instances used for training. In the context of model selection, it discards models that are very unlikely to become competitive. We run a large scale experiment on the 67 datasets from the {AutoML} benchmark and empirically show that in over 90\% of the cases using {LCCV} leads to similar performance (at most 1.5\% difference) as using 5/10-fold {CV}. However, it yields substantial runtime reductions of over 20\% on average. Additionally, it provides important insights, which for example allow assessing the beneﬁts of acquiring more data. These results are orthogonal to other advances in the ﬁeld of {AutoML}.},
	publisher = {{arXiv}},
	author = {Mohr, Felix and van Rijn, Jan N.},
	urldate = {2023-01-02},
	date = {2021-11-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.13914 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{mohr_towards_nodate,
	title = {Towards Model Selection using Learning Curve Cross-Validation},
	abstract = {Cross-validation ({CV}) methods such as leave-one-out cross-validation, k-fold cross-validation, and Monte-Carlo cross-validation estimate the predictive performance of a learner by repeatedly training it on a large portion of the given data and testing on the remaining data. These techniques have two drawbacks. First, they can be unnecessarily slow on large datasets. Second, providing only point estimates, they give almost no insights into the learning process of the validated algorithm. In this paper, we propose a new approach for validation based on learning curves ({LCCV}). Instead of creating train-test splits with a large portion of training data, {LCCV} iteratively increases the number of training examples used for training. In the context of model selection, it eliminates models that can be safely dismissed from the candidate pool. We run a large scale experiment on the 67 datasets from the {AutoML} benchmark, and empirically show that {LCCV} in over 90\% of the cases results in similar performance (at most 0.5\% diﬀerence) as 10-fold {CV}, but provides additional insights on the behaviour of a given model. On top of this, {LCCV} achieves runtime reductions between 20\% and over 50\% on half of the 67 datasets from the {AutoML} benchmark. This can be incorporated in various {AutoML} frameworks, to speed up the internal evaluation of candidate models. As such, these results can be used orthogonally to other advances in the ﬁeld of {AutoML}.},
	author = {Mohr, Felix},
	langid = {english},
}

@inproceedings{nguyen_meta-learning_2022,
	location = {Padua, Italy},
	title = {Meta-learning from Learning Curves: Challenge Design and Baseline Results},
	isbn = {978-1-72818-671-9},
	url = {https://ieeexplore.ieee.org/document/9892534/},
	doi = {10.1109/IJCNN55064.2022.9892534},
	shorttitle = {Meta-learning from Learning Curves},
	abstract = {Meta-learning has been widely studied and implemented in many Automated Machine Learning systems to improve the process of selecting and training Machine Learning models for new tasks, by leveraging expertise acquired on previously observed tasks. We design a novel meta-learning challenge aiming at learning-to-learn from one of the most essential model evaluation data, the learning curve. It consists of multiple model evaluations collected during the process of training. A metalearner is expected to apply a learned policy to learning curves of partially trained models on the task at hand, to rapidly ﬁnd the best task solution, without training all potential models to convergence. This implies learning the exploration-exploitation trade-off. Our challenge is split into two phases: a development phase and a ﬁnal test phase. In each phase, a meta-learner is meta-trained and meta-tested on validation learning curves (development phase) or test learning curves (ﬁnal test phase). During meta-training, the meta-learner is allowed to learn from the provided learning curves in any possible way. In meta-testing, we borrowed the common Reinforcement Learning setting in which an agent (a meta-learner) learns by interacting with an environment storing pre-computed learning curves. A metalearner must pay a cost (corresponding to the actual training and testing time) to reveal learning curve information progressively. The meta-learner is evaluated and ranked based on the average area under its learning curves. This challenge was accepted as part of the ofﬁcial selection of {WCCI} 2022 competitions.},
	eventtitle = {2022 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2022 International Joint Conference on Neural Networks ({IJCNN})},
	publisher = {{IEEE}},
	author = {Nguyen, Manh Hung and Sun-Hosoya, Lisheng and Grinsztajn, Nathan and Guyon, Isabelle},
	urldate = {2022-12-21},
	date = {2022-07-18},
	langid = {english},
}

@misc{nguyen_meta-learning_2022-1,
	title = {Meta-learning from Learning Curves Challenge: Lessons learned from the First Round and Design of the Second Round},
	url = {http://arxiv.org/abs/2208.02821},
	shorttitle = {Meta-learning from Learning Curves Challenge},
	abstract = {Meta-learning from learning curves is an important yet often neglected research area in the Machine Learning community. We introduce a series of Reinforcement Learning-based meta-learning challenges, in which an agent searches for the best suited algorithm for a given dataset, based on feedback of learning curves from the environment. The ﬁrst round attracted participants both from academia and industry. This paper analyzes the results of the ﬁrst round (accepted to the competition program of {WCCI} 2022), to draw insights into what makes a meta-learner successful at learning from learning curves. With the lessons learned from the ﬁrst round and the feedback from the participants, we have designed the second round of our challenge with a new protocol and a new meta-dataset. The second round of our challenge is accepted at the {AutoML}-Conf 2022 and currently on-going.},
	publisher = {{arXiv}},
	author = {Nguyen, Manh Hung and Sun, Lisheng and Grinsztajn, Nathan and Guyon, Isabelle},
	urldate = {2022-12-21},
	date = {2022-08-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2208.02821 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{song_general_nodate,
	title = {A General Framework for Multi-ﬁdelity Bayesian Optimization with Gaussian Processes},
	abstract = {How can we eﬃciently gather information to optimize an unknown function, when presented with multiple, mutually dependent information sources with diﬀerent costs? For example, when optimizing a physical system, intelligently trading oﬀ computer simulations and real-world tests can lead to signiﬁcant savings. Existing multi-ﬁdelity Bayesian optimization methods, such as multi-ﬁdelity {GP}-{UCB} or Entropy Search-based approaches, either make simplistic assumptions on the interaction among diﬀerent ﬁdelities or use simple heuristics that lack theoretical guarantees. In this paper, we study multiﬁdelity Bayesian optimization with complex structural dependencies among multiple outputs, and propose {MF}-{MI}-Greedy, a principled algorithmic framework for addressing this problem. In particular, we model diﬀerent ﬁdelities using additive Gaussian processes based on shared latent relationships with the target function. Then we use cost-sensitive mutual information gain for eﬃcient Bayesian optimization. We propose a simple notion of regret which incorporates the varying cost of diﬀerent ﬁdelities, and prove that {MF}-{MI}-Greedy achieves low regret. We demonstrate the strong empirical performance of our algorithm on both synthetic and real-world datasets.},
	author = {Song, Jialin and Chen, Yuxin and Yue, Yisong},
	langid = {english},
}

@misc{eggensperger_hpobench_2022,
	title = {{HPOBench}: A Collection of Reproducible Multi-Fidelity Benchmark Problems for {HPO}},
	url = {http://arxiv.org/abs/2109.06716},
	shorttitle = {{HPOBench}},
	abstract = {To achieve peak predictive performance, hyperparameter optimization ({HPO}) is a crucial component of machine learning and its applications. Over the last years, the number of efﬁcient algorithms and tools for {HPO} grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-ﬁdelity {HPO} methods. To close this gap, we propose {HPOBench}, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multiﬁdelity benchmark problems. {HPOBench} allows to run this extendable set of multi-ﬁdelity {HPO} benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate {HPOBench}’s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide {HPOBench} here: https: //github.com/automl/{HPOBench}.},
	publisher = {{arXiv}},
	author = {Eggensperger, Katharina and Müller, Philipp and Mallik, Neeratyoy and Feurer, Matthias and Sass, René and Klein, Aaron and Awad, Noor and Lindauer, Marius and Hutter, Frank},
	urldate = {2022-12-15},
	date = {2022-10-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.06716 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wu_practical_2019,
	title = {Practical Multi-fidelity Bayesian Optimization for Hyperparameter Tuning},
	url = {http://arxiv.org/abs/1903.04703},
	abstract = {Bayesian optimization is popular for optimizing time-consuming black-box objectives. Nonetheless, for hyperparameter tuning in deep neural networks, the time required to evaluate the validation error for even a few hyperparameter settings remains a bottleneck. Multi-ﬁdelity optimization promises relief using cheaper proxies to such objectives — for example, validation error for a network trained using a subset of the training points or fewer iterations than required for convergence. We propose a highly ﬂexible and practical approach to multi-ﬁdelity Bayesian optimization, focused on efﬁciently optimizing hyperparameters for iteratively trained supervised learning models. We introduce a new acquisition function, the trace-aware knowledge-gradient, which efﬁciently leverages both multiple continuous ﬁdelity controls and trace observations — values of the objective at a sequence of ﬁdelities, available when varying ﬁdelity using training iterations. We provide a provably convergent method for optimizing our acquisition function and show it outperforms state-of-the-art alternatives for hyperparameter tuning of deep neural networks and large-scale kernel learning.},
	publisher = {{arXiv}},
	author = {Wu, Jian and Toscano-Palmerin, Saul and Frazier, Peter I. and Wilson, Andrew Gordon},
	urldate = {2022-12-15},
	date = {2019-03-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1903.04703 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{last_predicting_2007,
	location = {Omaha, {NE}, {USA}},
	title = {Predicting and Optimizing Classifier Utility with the Power Law},
	isbn = {978-0-7695-3033-8 978-0-7695-3019-2},
	url = {http://ieeexplore.ieee.org/document/4476671/},
	doi = {10.1109/ICDMW.2007.31},
	abstract = {When data collection is costly and/or takes a significant amount of time, an early prediction of the classifier performance is extremely important for the design of the data mining process. Power law has been shown in the past to be a good predictor of decisiontree error rates as a function of the sample size. In this paper, we show that the optimal training set size for a given dataset can be computed from a learning curve characterized by a power law. Such a curve can be approximated using a small subset of potentially available data and then used to estimate the expected trade-off between the error rate and the amount of additional observations. The proposed approach to projected optimization of classifier utility is demonstrated and evaluated on several benchmark datasets.},
	eventtitle = {2007 Seventh {IEEE} International Conference on Data Mining - Workshops ({ICDM} Workshops)},
	pages = {219--224},
	booktitle = {Seventh {IEEE} International Conference on Data Mining Workshops ({ICDMW} 2007)},
	publisher = {{IEEE}},
	author = {Last, Mark},
	urldate = {2022-12-15},
	date = {2007-10},
	langid = {english},
}

@article{weiss_maximizing_2006,
	title = {Maximizing classifier utility when training data is costly},
	volume = {8},
	issn = {1931-0145, 1931-0153},
	url = {https://dl.acm.org/doi/10.1145/1233321.1233325},
	doi = {10.1145/1233321.1233325},
	abstract = {Classification is a well-studied problem in machine learning and data mining. Classifier performance was originally gauged almost exclusively using predictive accuracy. However, as work in the field progressed, more sophisticated measures of classifier utility that better represented the value of the induced knowledge were introduced. Nonetheless, most work still ignored the cost of acquiring training examples, even though this affects the overall utility of a classifier. In this paper we consider the costs of acquiring the training examples in the data mining process; we analyze the impact of the cost of training data on learning, identify the optimal training set size for a given data set, and analyze the performance of several progressive sampling schemes, which, given the cost of the training data, will generate classifiers that come close to maximizing the overall utility.},
	pages = {31--38},
	number = {2},
	journaltitle = {{ACM} {SIGKDD} Explorations Newsletter},
	shortjournal = {{SIGKDD} Explor. Newsl.},
	author = {Weiss, Gary M. and Tian, Ye},
	urldate = {2022-12-15},
	date = {2006-12},
	langid = {english},
}

@article{weiss_maximizing_2008,
	title = {Maximizing classifier utility when there are data acquisition and modeling costs},
	volume = {17},
	issn = {1384-5810, 1573-756X},
	url = {http://link.springer.com/10.1007/s10618-007-0082-x},
	doi = {10.1007/s10618-007-0082-x},
	abstract = {Classiﬁcation is a well-studied problem in data mining. Classiﬁcation performance was originally gauged almost exclusively using predictive accuracy, but as work in the ﬁeld progressed, more sophisticated measures of classiﬁer utility that better represented the value of the induced knowledge were introduced. Nonetheless, most work still ignored the cost of acquiring training examples, even though this cost impacts the total utility of the data mining process. In this article we analyze the relationship between the number of acquired training examples and the utility of the data mining process and, given the necessary cost information, we determine the number of training examples that yields the optimum overall performance. We then extend this analysis to include the cost of model induction—measured in terms of the {CPU} time required to generate the model. While our cost model does not take into account all possible costs, our analysis provides some useful insights and a template for future analyses using more sophisticated cost models. Because our analysis is based on experiments that acquire the full set of training examples, it cannot directly be used to ﬁnd a classiﬁer with optimal or near-optimal total utility. To address this issue we introduce two progressive sampling strategies that are empirically shown to produce classiﬁers with near-optimal total utility.},
	pages = {253--282},
	number = {2},
	journaltitle = {Data Mining and Knowledge Discovery},
	shortjournal = {Data Min Knowl Disc},
	author = {Weiss, Gary M. and Tian, Ye},
	urldate = {2022-12-15},
	date = {2008-10},
	langid = {english},
}

@inproceedings{last_improving_2009,
	location = {Paris, France},
	title = {Improving data mining utility with projective sampling},
	isbn = {978-1-60558-495-9},
	url = {http://portal.acm.org/citation.cfm?doid=1557019.1557076},
	doi = {10.1145/1557019.1557076},
	abstract = {Overall performance of the data mining process depends not just on the value of the induced knowledge but also on various costs of the process itself such as the cost of acquiring and pre-processing training examples, the {CPU} cost of model induction, and the cost of committed errors. Recently, several progressive sampling strategies for maximizing the overall data mining utility have been proposed. All these strategies are based on repeated acquisitions of additional training examples until a utility decrease is observed. In this paper, we present an alternative, projective sampling strategy, which fits functions to a partial learning curve and a partial run-time curve obtained from a small subset of potentially available data and then uses these projected functions to analytically estimate the optimal training set size. The proposed approach is evaluated on a variety of benchmark datasets using the {RapidMiner} environment for machine learning and data mining processes. The results show that the learning and run-time curves projected from only several data points can lead to a cheaper data mining process than the common progressive sampling methods.},
	eventtitle = {the 15th {ACM} {SIGKDD} international conference},
	pages = {487},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining - {KDD} '09},
	publisher = {{ACM} Press},
	author = {Last, Mark},
	urldate = {2022-12-15},
	date = {2009},
	langid = {english},
}

@misc{eggensperger_hpobench_2022-1,
	title = {{HPOBench}: A Collection of Reproducible Multi-Fidelity Benchmark Problems for {HPO}},
	url = {http://arxiv.org/abs/2109.06716},
	shorttitle = {{HPOBench}},
	abstract = {To achieve peak predictive performance, hyperparameter optimization ({HPO}) is a crucial component of machine learning and its applications. Over the last years, the number of efﬁcient algorithms and tools for {HPO} grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-ﬁdelity {HPO} methods. To close this gap, we propose {HPOBench}, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multiﬁdelity benchmark problems. {HPOBench} allows to run this extendable set of multi-ﬁdelity {HPO} benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate {HPOBench}’s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide {HPOBench} here: https: //github.com/automl/{HPOBench}.},
	publisher = {{arXiv}},
	author = {Eggensperger, Katharina and Müller, Philipp and Mallik, Neeratyoy and Feurer, Matthias and Sass, René and Klein, Aaron and Awad, Noor and Lindauer, Marius and Hutter, Frank},
	urldate = {2022-11-29},
	date = {2022-10-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2109.06716 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{mohr_learning_2022,
	title = {Learning Curves for Decision Making in Supervised Machine Learning -- A Survey},
	url = {http://arxiv.org/abs/2201.12150},
	abstract = {Learning curves are a concept from social sciences that has been adopted in the context of machine learning to assess the performance of a learning algorithm with respect to a certain resource, e.g. the number of training examples or the number of training iterations. Learning curves have important applications in several contexts of machine learning, most importantly for the context of data acquisition, early stopping of model training and model selection. For example, by modelling the learning curves, one can assess at an early stage whether the algorithm and hyperparameter configuration have the potential to be a suitable choice, often speeding up the algorithm selection process. A variety of approaches has been proposed to use learning curves for decision making. Some models answer the binary decision question of whether a certain algorithm at a certain budget will outperform a certain reference performance, whereas more complex models predict the entire learning curve of an algorithm. We contribute a framework that categorizes learning curve approaches using three criteria: the decision situation that they address, the intrinsic learning curve question that they answer and the type of resources that they use. We survey papers from literature and classify them into this framework.},
	publisher = {{arXiv}},
	author = {Mohr, Felix and van Rijn, Jan N.},
	urldate = {2022-11-24},
	date = {2022-01-28},
	eprinttype = {arxiv},
	eprint = {2201.12150 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{hataya_faster_2019,
	title = {Faster {AutoAugment}: Learning Augmentation Strategies using Backpropagation},
	url = {http://arxiv.org/abs/1911.06987},
	shorttitle = {Faster {AutoAugment}},
	abstract = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster {AutoAugment}, achieves significantly faster searching than prior work without a performance drop.},
	publisher = {{arXiv}},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	urldate = {2022-11-24},
	date = {2019-11-16},
	eprinttype = {arxiv},
	eprint = {1911.06987 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{mehrabi_survey_2022,
	title = {A Survey on Bias and Fairness in Machine Learning},
	url = {http://arxiv.org/abs/1908.09635},
	abstract = {With the widespread use of {AI} systems and applications in our everyday lives, it is important to take fairness issues into consideration while designing and engineering these types of systems. Such systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain groups or populations. We have recently seen work in machine learning, natural language processing, and deep learning that addresses such challenges in different subdomains. With the commercialization of these systems, researchers are becoming aware of the biases that these applications can contain and have attempted to address them. In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect {AI} applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in {AI} systems. In addition to that, we examined different domains and subdomains in {AI} showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in {AI} systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	publisher = {{arXiv}},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	urldate = {2022-11-24},
	date = {2022-01-25},
	eprinttype = {arxiv},
	eprint = {1908.09635 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{roberts_autows-bench-101_2022,
	title = {{AutoWS}-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels},
	url = {http://arxiv.org/abs/2208.14362},
	shorttitle = {{AutoWS}-Bench-101},
	abstract = {Weak supervision ({WS}) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisybut-cheap label estimates expressed by labeling functions ({LFs}). While it has been used successfully in many domains, weak supervision’s application scope is limited by the difﬁculty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the {LF} design process using a small set of ground truth labels. In this work, we introduce {AutoWS}-Bench-101: a framework for evaluating automated {WS} ({AutoWS}) techniques in challenging {WS} settings—a set of diverse application domains on which it has been previously difﬁcult or impossible to apply traditional {WS} techniques. While {AutoWS} is a promising direction toward expanding the application-scope of {WS}, the emergence of powerful methods such as zero-shot foundation models reveals the need to understand how {AutoWS} techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of {AutoWSBench}-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an {AutoWS} method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning.1 We observe that in many settings, it is necessary for {AutoWS} methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and {AutoWS}-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of {AutoWS} methods.},
	publisher = {{arXiv}},
	author = {Roberts, Nicholas and Li, Xintong and Huang, Tzu-Heng and Adila, Dyah and Schoenberg, Spencer and Liu, Cheng-Yu and Pick, Lauren and Ma, Haotian and Albarghouthi, Aws and Sala, Frederic},
	urldate = {2022-11-15},
	date = {2022-08-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2208.14362 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{hamadi_parallel_2012,
	location = {Berlin, Heidelberg},
	title = {Parallel Algorithm Configuration},
	volume = {7219},
	isbn = {978-3-642-34412-1 978-3-642-34413-8},
	url = {http://link.springer.com/10.1007/978-3-642-34413-8_5},
	abstract = {State-of-the-art algorithms for solving hard computational problems often expose many parameters whose settings critically affect empirical performance. Manually exploring the resulting combinatorial space of parameter settings is often tedious and unsatisfactory. Automated approaches for ﬁnding good parameter settings are becoming increasingly prominent and have recently lead to substantial improvements in the state of the art for solving a variety of computationally challenging problems. However, running such automated algorithm conﬁguration procedures is typically very costly, involving many thousands of invocations of the algorithm to be conﬁgured. Here, we study the extent to which parallel computing can come to the rescue. We compare straightforward parallelization by multiple independent runs with a more sophisticated method of parallelizing the model-based conﬁguration procedure {SMAC}. Empirical results for conﬁguring the {MIP} solver {CPLEX} demonstrate that near-optimal speedups can be obtained with up to 16 parallel workers, and that 64 workers can still accomplish challenging conﬁguration tasks that previously took 2 days in 1–2 hours. Overall, we show that our methods make effective use of large-scale parallel resources and thus substantially expand the practical applicability of algorithm conﬁguration methods.},
	pages = {55--70},
	booktitle = {Learning and Intelligent Optimization},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Hamadi, Youssef and Schoenauer, Marc},
	urldate = {2022-11-07},
	date = {2012},
	langid = {english},
	doi = {10.1007/978-3-642-34413-8_5},
	note = {Series Title: Lecture Notes in Computer Science},
}

@misc{klein_tabular_2019,
	title = {Tabular Benchmarks for Joint Architecture and Hyperparameter Optimization},
	url = {http://arxiv.org/abs/1905.04970},
	abstract = {Due to the high computational demands executing a rigorous comparison between hyperparameter optimization ({HPO}) methods is often cumbersome. The goal of this paper is to facilitate a better empirical evaluation of {HPO} methods by providing benchmarks that are cheap to evaluate, but still represent realistic use cases. We believe these benchmarks provide an easy and efﬁcient way to conduct reproducible experiments for neural hyperparameter search. Our benchmarks consist of a large grid of conﬁgurations of a feed forward neural network on four different regression datasets including architectural hyperparameters and hyperparameters concerning the training pipeline. Based on this data, we performed an in-depth analysis to gain a better understanding of the properties of the optimization problem, as well as of the importance of different types of hyperparameters. Second, we exhaustively compared various different state-of-the-art methods from the hyperparameter optimization literature on these benchmarks in terms of performance and robustness.},
	publisher = {{arXiv}},
	author = {Klein, Aaron and Hutter, Frank},
	urldate = {2022-11-07},
	date = {2019-05-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1905.04970 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{singh_comparative_2014,
	title = {{COMPARATIVE} {STUDY} {ID}3, {CART} {AND} C4.5 {DECISION} {TREE} {ALGORITHM}: A {SURVEY}},
	abstract = {Decision tree learning algorithm has been successfully used in expertsystems in capturing knowledge. The main task performed in these systems isusing inductive methods to the given values of attributes of an unknown objectto determine appropriate classification according to decision tree rules.It is oneof the most effective forms to represent and evaluate the performance of algorithms, due to its various eye catchingfeatures: simplicity, comprehensibility, no parameters, and being able to handle mixed-type data. There are many decision tree algorithm available named {ID}3, C4.5, {CART}, {CHAID}, {QUEST}, {GUIDE}, {CRUISE}, and {CTREE}. We have explained three most commonly used decision tree algorithm in this paper to understand their use and scalability on different types of attributes and feature. {ID}3(Iterative Dichotomizer 3) developed by J.R Quinlan in 1986, C4.5 is an evolution of {ID}3, presented by the same author (Quinlan, 1993).{CART} stands for Classification and Regression Trees developed by Breiman et al.in 1984).},
	pages = {7},
	journaltitle = {International Journal of Advanced Information Science and Technology},
	author = {Singh, Sonia and Gupta, Priyanka},
	date = {2014},
	langid = {english},
}

@misc{bull_convergence_2011,
	title = {Convergence rates of efficient global optimization algorithms},
	url = {http://arxiv.org/abs/1101.3501},
	abstract = {In the eﬃcient global optimization problem, we minimize an unknown function f , using as few observations f (x) as possible. It can be considered a continuum-armed-bandit problem, with noiseless data, and simple regret. Expected-improvement algorithms are perhaps the most popular methods for solving the problem; in this paper, we provide theoretical results on their asymptotic behaviour.},
	publisher = {{arXiv}},
	author = {Bull, Adam D.},
	urldate = {2022-11-07},
	date = {2011-10-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1101.3501 [math, stat]},
	keywords = {90C26 (Primary), 68Q32, 62C10, 62L05 (Secondary), Mathematics - Optimization and Control, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{hutter_algorithm_2013,
	title = {Algorithm Runtime Prediction: Methods \& Evaluation},
	url = {http://arxiv.org/abs/1211.0906},
	shorttitle = {Algorithm Runtime Prediction},
	abstract = {Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm’s runtime as a function of problem-speciﬁc instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic conﬁguration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and—perhaps most importantly—a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisﬁability ({SAT}), travelling salesperson ({TSP}) and mixed integer programming ({MIP}) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of {SAT}, {MIP}, and {TSP} instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.},
	publisher = {{arXiv}},
	author = {Hutter, Frank and Xu, Lin and Hoos, Holger H. and Leyton-Brown, Kevin},
	urldate = {2022-11-07},
	date = {2013-10-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1211.0906 [cs, stat]},
	keywords = {68T20, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, I.2.6, I.2.8, Statistics - Machine Learning},
}

@misc{eriksson_scalable_2020,
	title = {Scalable Global Optimization via Local Bayesian Optimization},
	url = {http://arxiv.org/abs/1910.01739},
	abstract = {Bayesian optimization has recently emerged as a popular method for the sampleefﬁcient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difﬁcult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the {TuRBO} algorithm that ﬁts a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that {TuRBO} outperforms stateof-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.},
	publisher = {{arXiv}},
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R. and Turner, Ryan and Poloczek, Matthias},
	urldate = {2022-11-07},
	date = {2020-02-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.01739 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{scornet_trees_2021,
	title = {Trees, forests, and impurity-based variable importance},
	url = {http://arxiv.org/abs/2001.04295},
	abstract = {Tree ensemble methods such as random forests [Breiman, 2001] are very popular to handle high-dimensional tabular data sets, notably because of their ability to detect sparse signals and their resulting good predictive accuracy. However, when machine learning is used for decision-making problems, settling for the best predictive procedures may not be reasonable since enlightened decisions require to understand the phenomena underlying the data, which is accessible only with an in-depth comprehension of the algorithm prediction process. Unfortunately, random forests are not intrinsically interpretable since their prediction results from averaging several hundreds of decision trees. A classic approach to gain knowledge on this so-called black-box algorithm is to compute variable importances, that are employed to assess the predictive impact of each input variable. Variable importances are then used to rank or select variables and thus play a great role in data analysis. Mean Decrease Impurity ({MDI}) is one of the two variable importance measures in random forests. However, there is no theoretical justiﬁcation to use {MDI}: we do not even know what this indicator estimates. In this paper, we analyze {MDI} and prove that if input variables are independent and in absence of interactions, {MDI} provides a variance decomposition of the output, where the contribution of each variable is clearly identiﬁed. We also study models exhibiting dependence between input variables or interaction, for which the variable importance is intrinsically ill-deﬁned.},
	publisher = {{arXiv}},
	author = {Scornet, Erwan},
	urldate = {2022-11-07},
	date = {2021-12-24},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2001.04295 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{mehta_nas-bench-suite_2022,
	title = {{NAS}-Bench-Suite: {NAS} Evaluation is (Now) Surprisingly Easy},
	url = {http://arxiv.org/abs/2201.13396},
	shorttitle = {{NAS}-Bench-Suite},
	abstract = {The release of tabular benchmarks, such as {NAS}-Bench-101 and {NAS}-Bench-201, has signiﬁcantly lowered the computational overhead for conducting scientiﬁc research in neural architecture search ({NAS}). Although they have been widely adopted and used to tune real-world {NAS} algorithms, these benchmarks are limited to small search spaces and focus solely on image classiﬁcation. Recently, several new {NAS} benchmarks have been introduced that cover signiﬁcantly larger search spaces over a wide range of tasks, including object detection, speech recognition, and natural language processing. However, substantial differences among these {NAS} benchmarks have so far prevented their widespread adoption, limiting researchers to using just a few benchmarks. In this work, we present an in-depth analysis of popular {NAS} algorithms and performance prediction methods across 25 different combinations of search spaces and datasets, ﬁnding that many conclusions drawn from a few {NAS} benchmarks do not generalize to other benchmarks. To help remedy this problem, we introduce {NAS}-Bench-Suite, a comprehensive and extensible collection of {NAS} benchmarks, accessible through a uniﬁed interface, created with the aim to facilitate reproducible, generalizable, and rapid {NAS} research. Our code is available at https://github.com/automl/naslib.},
	publisher = {{arXiv}},
	author = {Mehta, Yash and White, Colin and Zela, Arber and Krishnakumar, Arjun and Zabergja, Guri and Moradian, Shakiba and Safari, Mahmoud and Yu, Kaicheng and Hutter, Frank},
	urldate = {2022-11-07},
	date = {2022-02-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2201.13396 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{co-reyes_evolving_2022,
	title = {Evolving Reinforcement Learning Algorithms},
	url = {http://arxiv.org/abs/2101.03958},
	abstract = {We propose a method for meta-learning reinforcement learning algorithms by searching over the space of computational graphs which compute the loss function for a value-based model-free {RL} agent to optimize. The learned algorithms are domain-agnostic and can generalize to new environments not seen during training. Our method can both learn from scratch and bootstrap off known existing algorithms, like {DQN}, enabling interpretable modiﬁcations which improve performance. Learning from scratch on simple classical control and gridworld tasks, our method rediscovers the temporal-difference ({TD}) algorithm. Bootstrapped from {DQN}, we highlight two learned algorithms which obtain good generalization performance over other classical control tasks, gridworld type tasks, and Atari games. The analysis of the learned algorithm behavior shows resemblance to recently proposed {RL} algorithms that address overestimation in value-based methods.},
	publisher = {{arXiv}},
	author = {Co-Reyes, John D. and Miao, Yingjie and Peng, Daiyi and Real, Esteban and Levine, Sergey and Le, Quoc V. and Lee, Honglak and Faust, Aleksandra},
	urldate = {2022-11-07},
	date = {2022-07-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.03958 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{radosavovic_designing_2020,
	title = {Designing Network Design Spaces},
	url = {http://arxiv.org/abs/2003.13678},
	abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call {RegNet}. The core insight of the {RegNet} parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the {RegNet} design space and arrive at interesting ﬁndings that do not match the current practice of network design. The {RegNet} design space provides simple and fast networks that work well across a wide range of ﬂop regimes. Under comparable training settings and ﬂops, the {RegNet} models outperform the popular {EfﬁcientNet} models while being up to 5× faster on {GPUs}.},
	publisher = {{arXiv}},
	author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2022-11-07},
	date = {2020-03-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2003.13678 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{lakshminarayanan_mondrian_2016,
	title = {Mondrian Forests for Large-Scale Regression when Uncertainty Matters},
	url = {http://arxiv.org/abs/1506.03805},
	abstract = {Many real-world regression problems demand a measure of the uncertainty associated with each prediction. Standard decision forests deliver eﬃcient state-of-the-art predictive performance, but high-quality uncertainty estimates are lacking. Gaussian processes ({GPs}) deliver uncertainty estimates, but scaling {GPs} to large-scale data sets comes at the cost of approximating the uncertainty estimates. We extend Mondrian forests, ﬁrst proposed by Lakshminarayanan et al. (2014) for classiﬁcation problems, to the large-scale non-parametric regression setting. Using a novel hierarchical Gaussian prior that dovetails with the Mondrian forest framework, we obtain principled uncertainty estimates, while still retaining the computational advantages of decision forests. Through a combination of illustrative examples, real-world large-scale datasets, and Bayesian optimization benchmarks, we demonstrate that Mondrian forests outperform approximate {GPs} on large-scale regression tasks and deliver better-calibrated uncertainty assessments than decision-forest-based methods.},
	publisher = {{arXiv}},
	author = {Lakshminarayanan, Balaji and Roy, Daniel M. and Teh, Yee Whye},
	urldate = {2022-11-07},
	date = {2016-05-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.03805 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{loshchilov_sgdr_2017,
	title = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	url = {http://arxiv.org/abs/1608.03983},
	shorttitle = {{SGDR}},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the {CIFAR}-10 and {CIFAR}-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of {EEG} recordings and on a downsampled version of the {ImageNet} dataset. Our source code is available at https://github.com/loshchil/{SGDR}},
	publisher = {{arXiv}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2022-11-07},
	date = {2017-05-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1608.03983 [cs, math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
}

@misc{lin_focal_2018,
	title = {Focal Loss for Dense Object Detection},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-{CNN}, where a classiﬁer is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classiﬁed examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call {RetinaNet}. Our results show that when trained with the focal loss, {RetinaNet} is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	publisher = {{arXiv}},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	urldate = {2022-11-07},
	date = {2018-02-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1708.02002 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{kingma_auto-encoding_2014,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efﬁcient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efﬁcient by ﬁtting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reﬂected in experimental results.},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2022-11-07},
	date = {2014-05-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{xu_modeling_2019,
	title = {Modeling Tabular data using Conditional {GAN}},
	url = {http://arxiv.org/abs/1907.00503},
	abstract = {Modeling the probability distribution of rows in tabular data and generating realistic synthetic data is a non-trivial task. Tabular data usually contains a mix of discrete and continuous columns. Continuous columns may have multiple modes whereas discrete columns are sometimes imbalanced making the modeling difﬁcult. Existing statistical and deep neural network models fail to properly model this type of data. We design {CTGAN}, which uses a conditional generator to address these challenges. To aid in a fair and thorough comparison, we design a benchmark with 7 simulated and 8 real datasets and several Bayesian network baselines. {CTGAN} outperforms Bayesian methods on most of the real datasets whereas other deep learning methods could not.},
	publisher = {{arXiv}},
	author = {Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
	urldate = {2022-11-07},
	date = {2019-10-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1907.00503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_towards_2022,
	title = {Towards Learning Universal Hyperparameter Optimizers with Transformers},
	url = {http://arxiv.org/abs/2205.13320},
	abstract = {Meta-learning hyperparameter optimization ({HPO}) algorithms from prior experiments is a promising approach to improve optimization efﬁciency over objective functions from a similar distribution. However, existing methods are restricted to learning from experiments sharing the same set of hyperparameters. In this paper, we introduce the {OPTFORMER}, the ﬁrst text-based Transformer {HPO} framework that provides a universal end-to-end interface for jointly learning policy and function prediction when trained on vast tuning data from the wild, such as the Google Vizier database, one of the world’s largest {HPO} datasets. Our extensive experiments demonstrate that the {OPTFORMER} can imitate at least 7 different {HPO} algorithms, which can be further improved via its function uncertainty estimates. Compared to a Gaussian Process, the {OPTFORMER} also learns a robust prior distribution for hyperparameter response functions, and can thereby provide more accurate and better calibrated predictions. This work paves the path to future extensions for training a Transformer-based model as a general {HPO} optimizer.},
	publisher = {{arXiv}},
	author = {Chen, Yutian and Song, Xingyou and Lee, Chansoo and Wang, Zi and Zhang, Qiuyi and Dohan, David and Kawakami, Kazuya and Kochanski, Greg and Doucet, Arnaud and Ranzato, Marc'aurelio and Perel, Sagi and de Freitas, Nando},
	urldate = {2022-11-07},
	date = {2022-10-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2205.13320 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hutter_learning_2021,
	title = {Learning Curve Theory},
	url = {http://arxiv.org/abs/2102.04074},
	abstract = {Recently a number of empirical “universal” scaling law papers have been published, most notably by {OpenAI}. ‘Scaling laws’ refers to power-law decreases of training or test error w.r.t. more data, larger neural networks, and/or more compute. In this work we focus on scaling w.r.t. data size n. Theoretical understanding of this phenomenon is largely lacking, except in ﬁnite-dimensional models for which error typically decreases with n−1/2 or n−1, where n is the sample size. We develop and theoretically analyse the simplest possible (toy) model that can exhibit n−β learning curves for arbitrary power β {\textgreater} 0, and determine whether power laws are universal or depend on the data distribution.},
	publisher = {{arXiv}},
	author = {Hutter, Marcus},
	urldate = {2022-11-07},
	date = {2021-02-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2102.04074 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{scholkopf_prior_nodate,
	title = {Prior Knowledge in Support Vector Kernels},
	abstract = {We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transfonnations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.},
	pages = {7},
	author = {Schölkopf, Bernhard and Simard, Patrice and Smola, Alex J and Vapnik, Vladimir},
	langid = {english},
}

@inproceedings{wilkins_acclaim_2022,
	location = {Heidelberg, Germany},
	title = {{ACCLAiM}: Advancing the Practicality of {MPI} Collective Communication Autotuning Using Machine Learning},
	isbn = {978-1-66549-856-2},
	url = {https://ieeexplore.ieee.org/document/9912679/},
	doi = {10.1109/CLUSTER51413.2022.00030},
	shorttitle = {{ACCLAiM}},
	abstract = {{MPI} collective communication is an omnipresent communication model for high-performance computing ({HPC}) systems. The performance of a collective operation depends strongly on the algorithm used to implement it. {MPI} libraries use inaccurate heuristics to select these algorithms, causing applications to suffer unnecessary slowdowns. Machine learning ({ML})based autotuners are a promising alternative. {ML} autotuners can intelligently select algorithms for individual jobs, resulting in near-optimal performance. However, these approaches currently spend more time training than they save by accelerating applications, rendering them impractical.},
	eventtitle = {2022 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	pages = {161--171},
	booktitle = {2022 {IEEE} International Conference on Cluster Computing ({CLUSTER})},
	publisher = {{IEEE}},
	author = {Wilkins, Michael and Guo, Yanfei and Thakur, Rajeev and Dinda, Peter and Hardavellas, Nikos},
	urldate = {2022-11-07},
	date = {2022-09},
	langid = {english},
}

@article{de_vries_survey_2011,
	title = {Survey of disruption causes at {JET}},
	volume = {51},
	issn = {0029-5515, 1741-4326},
	url = {https://iopscience.iop.org/article/10.1088/0029-5515/51/5/053018},
	doi = {10.1088/0029-5515/51/5/053018},
	abstract = {A survey has been carried out into the causes of all 2309 disruptions over the last decade of {JET} operations. The aim of this survey was to obtain a complete picture of all possible disruption causes, in order to devise better strategies to prevent or mitigate their impact. The analysis allows the effort to avoid or prevent {JET} disruptions to be more efﬁcient and effective. As expected, a highly complex pattern of chain of events that led to disruptions emerged. It was found that the majority of disruptions had a technical root cause, for example due to control errors, or operator mistakes. These bring a random, non-physics, factor into the occurrence of disruptions and the disruption rate or disruptivity of a scenario may depend more on technical performance than on physics stability issues. The main root cause of {JET} disruptions was nevertheless due to neo-classical tearing modes that locked, closely followed in second place by disruptions due to human error. The development of more robust operational scenarios has reduced the {JET} disruption rate over the last decade from about 15\% to below 4\%. A fraction of all disruptions was caused by very fast, precursorless unpredictable events. The occurrence of these disruptions may set a lower limit of 0.4\% to the disruption rate of {JET}. If one considers on top of that human error and all unforeseen failures of heating or control systems this lower limit may rise to 1.0\% or 1.6\%, respectively.},
	pages = {053018},
	number = {5},
	journaltitle = {Nuclear Fusion},
	shortjournal = {Nucl. Fusion},
	author = {de Vries, P.C. and Johnson, M.F. and Alper, B. and Buratti, P. and Hender, T.C. and Koslowski, H.R. and Riccardo, V. and {JET-EFDA Contributors}},
	urldate = {2022-11-07},
	date = {2011-05-01},
	langid = {english},
}

@article{balandat_botorch_nodate,
	title = {{BOTORCH}: A Framework for Efﬁcient Monte-Carlo Bayesian Optimization},
	abstract = {Bayesian optimization provides sample-efﬁcient global optimization for a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. We introduce {BOTORCH}, a modern programming framework for Bayesian optimization that combines Monte-Carlo ({MC}) acquisition functions, a novel sample average approximation optimization approach, autodifferentiation, and variance reduction techniques. {BOTORCH}’s modular design facilitates ﬂexible speciﬁcation and optimization of probabilistic models written in {PyTorch}, simplifying implementation of new acquisition functions. Our approach is backed by novel theoretical convergence results and made practical by a distinctive algorithmic foundation that leverages fast predictive distributions, hardware acceleration, and deterministic optimization. We also propose a novel “one-shot” formulation of the Knowledge Gradient, enabled by a combination of our theoretical and software contributions. In experiments, we demonstrate the improved sample efﬁciency of {BOTORCH} relative to other popular libraries.},
	pages = {15},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
	langid = {english},
}

@article{lakshminarayanan_mondrian_nodate,
	title = {Mondrian Forests: Efﬁcient Online Random Forests},
	abstract = {Ensembles of randomized decision trees, usually referred to as random forests, are widely used for classiﬁcation and regression tasks in machine learning and statistics. Random forests achieve competitive predictive performance and are computationally efﬁcient to train and test, making them excellent candidates for real-world prediction tasks. The most popular random forest variants (such as Breiman’s random forest and extremely randomized trees) operate on batches of training data. Online methods are now in greater demand. Existing online random forests, however, require more training data than their batch counterpart to achieve comparable predictive performance. In this work, we use Mondrian processes (Roy and Teh, 2009) to construct ensembles of random decision trees we call Mondrian forests. Mondrian forests can be grown in an incremental/online fashion and remarkably, the distribution of online Mondrian forests is the same as that of batch Mondrian forests. Mondrian forests achieve competitive predictive performance comparable with existing online random forests and periodically retrained batch random forests, while being more than an order of magnitude faster, thus representing a better computation vs accuracy tradeoff.},
	pages = {15},
	author = {Lakshminarayanan, Balaji and Roy, Daniel M and Teh, Yee Whye},
	langid = {english},
}

@misc{hollmann_tabpfn_2022,
	title = {{TabPFN}: A Transformer That Solves Small Tabular Classification Problems in a Second},
	url = {http://arxiv.org/abs/2207.01848},
	shorttitle = {{TabPFN}},
	abstract = {We present {TabPFN}, a trained Transformer that can do supervised classiﬁcation for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classiﬁcation methods. {TabPFN} is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. {TabPFN} is a Prior-Data Fitted Network ({PFN}) and is trained ofﬂine once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On 30 small datasets from the {OpenML}-{CC}18 suite, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art {AutoML} systems with up to 70× speedup. This increases to a 3 200× speedup when a {GPU} is available. We provide all our code, the trained {TabPFN}, an interactive browser demo and a Colab notebook at https://github.com/automl/{TabPFN}.},
	publisher = {{arXiv}},
	author = {Hollmann, Noah and Müller, Samuel and Eggensperger, Katharina and Hutter, Frank},
	urldate = {2022-11-07},
	date = {2022-10-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2207.01848 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{volpp_meta-learning_2020,
	title = {Meta-Learning Acquisition Functions for Transfer Learning in Bayesian Optimization},
	url = {http://arxiv.org/abs/1904.02642},
	abstract = {Transferring knowledge across tasks to improve data-efﬁciency is one of the open key challenges in the ﬁeld of global black-box optimization. Readily available algorithms are typically designed to be universal optimizers and, therefore, often suboptimal for speciﬁc tasks. We propose a novel transfer learning method to obtain customized optimizers within the well-established framework of Bayesian optimization, allowing our algorithm to utilize the proven generalization capabilities of Gaussian processes. Using reinforcement learning to meta-train an acquisition function ({AF}) on a set of related tasks, the proposed method learns to extract implicit structural information and to exploit it for improved data-efﬁciency. We present experiments on a simulation-to-real transfer task as well as on several synthetic functions and on two hyperparameter search problems. The results show that our algorithm (1) automatically identiﬁes structural properties of objective functions from available source tasks or simulations, (2) performs favourably in settings with both scarse and abundant source data, and (3) falls back to the performance level of general {AFs} if no particular structure is present.},
	publisher = {{arXiv}},
	author = {Volpp, Michael and Fröhlich, Lukas P. and Fischer, Kirsten and Doerr, Andreas and Falkner, Stefan and Hutter, Frank and Daniel, Christian},
	urldate = {2022-11-07},
	date = {2020-02-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.02642 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{makarova_automatic_2022,
	title = {Automatic Termination for Hyperparameter Optimization},
	url = {http://arxiv.org/abs/2104.08166},
	abstract = {Bayesian optimization ({BO}) is a widely popular approach for the hyperparameter optimization ({HPO}) in machine learning. At its core, {BO} iteratively evaluates promising con gurations until a user-de ned budget, such as wall-clock time or number of iterations, is exhausted. While the nal performance after tuning heavily depends on the provided budget, it is hard to pre-specify an optimal value in advance. In this work, we propose an e ective and intuitive termination criterion for {BO} that automatically stops the procedure if it is su ciently close to the global optimum. Our key insight is that the discrepancy between the true objective (predictive performance on test data) and the computable target (validation performance) suggests stopping once the suboptimality in optimizing the target is dominated by the statistical estimation error. Across an extensive range of real-world {HPO} problems and baselines, we show that our termination criterion achieves a better trade-o between the test performance and optimization time. Additionally, we nd that over tting may occur in the context of {HPO}, which is arguably an overlooked problem in the literature, and show how our termination criterion helps to mitigate this phenomenon on both small and large datasets.},
	publisher = {{arXiv}},
	author = {Makarova, Anastasia and Shen, Huibin and Perrone, Valerio and Klein, Aaron and Faddoul, Jean Baptiste and Krause, Andreas and Seeger, Matthias and Archambeau, Cedric},
	urldate = {2022-11-07},
	date = {2022-07-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2104.08166 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{falkner_bohb_2018,
	title = {{BOHB}: Robust and Efficient Hyperparameter Optimization at Scale},
	url = {http://arxiv.org/abs/1807.01774},
	shorttitle = {{BOHB}},
	abstract = {Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based conﬁguration evaluation approaches based on random search lack guidance and do not converge to the best conﬁgurations as quickly. Here, we propose to combine the beneﬁts of both Bayesian optimization and banditbased methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal conﬁgurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement.},
	publisher = {{arXiv}},
	author = {Falkner, Stefan and Klein, Aaron and Hutter, Frank},
	urldate = {2022-10-03},
	date = {2018-07-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01774 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_hyperband_2018,
	title = {Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization},
	url = {http://arxiv.org/abs/1603.06560},
	shorttitle = {Hyperband},
	abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select conﬁgurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic inﬁnite-armed bandit problem where a predeﬁned resource like iterations, data samples, or features is allocated to randomly sampled conﬁgurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
	publisher = {{arXiv}},
	author = {Li, Lisha and Jamieson, Kevin and {DeSalvo}, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	urldate = {2022-10-03},
	date = {2018-06-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1603.06560 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{facco_estimating_2017,
	title = {Estimating the intrinsic dimension of datasets by a minimal neighborhood information},
	volume = {7},
	issn = {2045-2322},
	url = {http://www.nature.com/articles/s41598-017-11873-y},
	doi = {10.1038/s41598-017-11873-y},
	pages = {12140},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Facco, Elena and d’Errico, Maria and Rodriguez, Alex and Laio, Alessandro},
	urldate = {2022-06-07},
	date = {2017-12},
	langid = {english},
}

@article{mintun_interaction_nodate,
	title = {On Interaction Between Augmentations and Corruptions in Natural Corruption Robustness},
	abstract = {Invariance to a broad array of image corruptions, such as warping, noise, or color shifts, is an important aspect of building robust models in computer vision. Recently, several new data augmentations have been proposed that signiﬁcantly improve performance on {ImageNet}-C, a benchmark of such corruptions. However, there is still a lack of basic understanding on the relationship between data augmentations and test-time corruptions. To this end, we develop a feature space for image transforms, and then use a new measure in this space between augmentations and corruptions called the Minimal Sample Distance to demonstrate a strong correlation between similarity and performance. We then investigate recent data augmentations and observe a signiﬁcant degradation in corruption robustness when the test-time corruptions are sampled to be perceptually dissimilar from {ImageNet}-C in this feature space. Our results suggest that test error can be improved by training on perceptually similar augmentations, and data augmentations may not generalize well beyond the existing benchmark. We hope our results and tools will allow for more robust progress towards improving robustness to image corruptions. We provide code at https://github.com/facebookresearch/augmentation-corruption.},
	pages = {13},
	author = {Mintun, Eric and Kirillov, Alexander and Xie, Saining},
	langid = {english},
}

@article{shorten_survey_2019,
	title = {A survey on Image Data Augmentation for Deep Learning},
	volume = {6},
	issn = {2196-1115},
	url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on {GANs} are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	pages = {60},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {J Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	urldate = {2022-05-27},
	date = {2019-12},
	langid = {english},
}

@article{vapnik_principles_nodate,
	title = {Principles of Risk Minimization for Learning Theory},
	abstract = {Learning is posed as a problem of function estimation, for which two principles of solution are considered: empirical risk minimization and structural risk minimization. These two principles are applied to two different statements of the function estimation problem: global and local. Systematic improvements in prediction power are illustrated in application to zip-code recognition.},
	pages = {8},
	author = {Vapnik, Vladimir},
	langid = {english},
}

@misc{maron_learning_2020,
	title = {On Learning Sets of Symmetric Elements},
	url = {http://arxiv.org/abs/2002.08599},
	abstract = {Learning from unordered sets is a fundamental learning setup, recently attracting increasing attention. Research in this area has focused on the case where elements of the set are represented by feature vectors, and far less emphasis has been given to the common case where set elements themselves adhere to their own symmetries. That case is relevant to numerous applications, from deblurring image bursts to multi-view 3D shape recognition and reconstruction. In this paper, we present a principled approach to learning sets of general symmetric elements. We ﬁrst characterize the space of linear layers that are equivariant both to element reordering and to the inherent symmetries of elements, like translation in the case of images. We further show that networks that are composed of these layers, called Deep Sets for Symmetric elements layers ({DSS}), are universal approximators of both invariant and equivariant functions, and that these networks are strictly more expressive than Siamese networks. {DSS} layers are also straightforward to implement. Finally, we show that they improve over existing setlearning architectures in a series of experiments with images, graphs and point-clouds.},
	publisher = {{arXiv}},
	author = {Maron, Haggai and Litany, Or and Chechik, Gal and Fetaya, Ethan},
	urldate = {2022-05-26},
	date = {2020-11-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.08599 [cs, stat]},
	note = {Number: {arXiv}:2002.08599},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhou_meta-learning_2021,
	title = {Meta-Learning Symmetries by Reparameterization},
	url = {http://arxiv.org/abs/2007.02933},
	abstract = {Many successful deep learning architectures are equivariant to certain transformations in order to conserve parameters and improve generalization: most famously, convolution layers are equivariant to shifts of the input. This approach only works when practitioners know the symmetries of the task and can manually construct an architecture with the corresponding equivariances. Our goal is an approach for learning equivariances from data, without needing to design custom task-speciﬁc architectures. We present a method for learning and encoding equivariances into networks by learning corresponding parameter sharing patterns from data. Our method can provably represent equivariance-inducing parameter sharing for any ﬁnite group of symmetry transformations. Our experiments suggest that it can automatically learn to encode equivariances to common transformations used in image processing tasks. We provide our experiment code at https: //github.com/{AllanYangZhou}/metalearning-symmetries.},
	publisher = {{arXiv}},
	author = {Zhou, Allan and Knowles, Tom and Finn, Chelsea},
	urldate = {2022-05-24},
	date = {2021-03-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2007.02933 [cs, stat]},
	note = {Number: {arXiv}:2007.02933},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lin_st-gan_2018,
	title = {{ST}-{GAN}: Spatial Transformer Generative Adversarial Networks for Image Compositing},
	url = {http://arxiv.org/abs/1803.01837},
	shorttitle = {{ST}-{GAN}},
	abstract = {We address the problem of ﬁnding realistic geometric corrections to a foreground object such that it appears natural when composited into a background image. To achieve this, we propose a novel Generative Adversarial Network ({GAN}) architecture that utilizes Spatial Transformer Networks ({STNs}) as the generator, which we call Spatial Transformer {GANs} ({ST}-{GANs}). {ST}-{GANs} seek image realism by operating in the geometric warp parameter space. In particular, we exploit an iterative {STN} warping scheme and propose a sequential training strategy that achieves better results compared to naive training of a single generator. One of the key advantages of {ST}-{GAN} is its applicability to high-resolution images indirectly since the predicted warp parameters are transferable between reference frames. We demonstrate our approach in two applications: (1) visualizing how indoor furniture (e.g. from product images) might be perceived in a room, (2) hallucinating how accessories like glasses would look when matched with real portraits.},
	publisher = {{arXiv}},
	author = {Lin, Chen-Hsuan and Yumer, Ersin and Wang, Oliver and Shechtman, Eli and Lucey, Simon},
	urldate = {2022-05-24},
	date = {2018-03-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.01837 [cs]},
	note = {Number: {arXiv}:1803.01837},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{gens_deep_nodate,
	title = {Deep Symmetry Networks},
	abstract = {The chief difﬁculty in object recognition is that objects’ classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups’ effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on {NORB} and {MNIST}-rot show that symnets over the afﬁne group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.},
	pages = {9},
	author = {Gens, Robert and Domingos, Pedro M},
	langid = {english},
}

@article{lamport_time_1978,
	title = {Time, Clocks, and the Ordering of Events in a Distributed System},
	volume = {21},
	abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.},
	pages = {8},
	number = {7},
	author = {Lamport, Leslie},
	date = {1978},
	langid = {english},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} Guiding Principles for scientific data management and stewardship},
	volume = {3},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	pages = {160018},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, {IJsbrand} Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	urldate = {2022-05-05},
	date = {2016-12},
	langid = {english},
}

@article{gebru_datasheets_2021,
	title = {Datasheets for Datasets},
	url = {http://arxiv.org/abs/1803.09010},
	abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
	journaltitle = {{arXiv}:1803.09010 [cs]},
	author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and Daumé {III}, Hal and Crawford, Kate},
	urldate = {2022-05-05},
	date = {2021-12-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.09010},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
}

@article{chen_fair_2022,
	title = {A {FAIR} and {AI}-ready Higgs boson decay dataset},
	volume = {9},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-021-01109-0},
	doi = {10.1038/s41597-021-01109-0},
	abstract = {Abstract
            To enable the reusability of massive scientific datasets by humans and machines, researchers aim to adhere to the principles of findability, accessibility, interoperability, and reusability ({FAIR}) for data and artificial intelligence ({AI}) models. This article provides a domain-agnostic, step-by-step assessment guide to evaluate whether or not a given dataset meets these principles. We demonstrate how to use this guide to evaluate the {FAIRness} of an open simulated dataset produced by the {CMS} Collaboration at the {CERN} Large Hadron Collider. This dataset consists of Higgs boson decays and quark and gluon background, and is available through the {CERN} Open Data Portal. We use additional available tools to assess the {FAIRness} of this dataset, and incorporate feedback from members of the {FAIR} community to validate our results. This article is accompanied by a Jupyter notebook to visualize and explore this dataset. This study marks the first in a planned series of articles that will guide scientists in the creation of {FAIR} {AI} models and datasets in high energy particle physics.},
	pages = {31},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Chen, Yifan and Huerta, E. A. and Duarte, Javier and Harris, Philip and Katz, Daniel S. and Neubauer, Mark S. and Diaz, Daniel and Mokhtar, Farouk and Kansal, Raghav and Park, Sang Eon and Kindratenko, Volodymyr V. and Zhao, Zhizhen and Rusack, Roger},
	urldate = {2022-05-03},
	date = {2022-12},
	langid = {english},
}

@article{kirsch_batchbald_2019,
	title = {{BatchBALD}: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning},
	url = {http://arxiv.org/abs/1906.08158},
	shorttitle = {{BatchBALD}},
	abstract = {We develop {BatchBALD}, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. {BatchBALD} is a greedy linear-time 1 − 1/e-approximate algorithm amenable to dynamic programming and eﬃcient caching. We compare {BatchBALD} to the commonly used approach for batch data acquisition and ﬁnd that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We ﬁnish by showing that, using {BatchBALD} to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data eﬃciency improvements in batch acquisition.},
	journaltitle = {{arXiv}:1906.08158 [cs, stat]},
	author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
	urldate = {2022-04-19},
	date = {2019-10-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.08158},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gal_deep_2017,
	title = {Deep Bayesian Active Learning with Image Data},
	url = {http://arxiv.org/abs/1703.02910},
	abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difﬁculties when used in an active learning setting. First, active learning ({AL}) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many {AL} acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a signiﬁcant improvement on existing active learning approaches. We demonstrate this on both the {MNIST} dataset, as well as for skin cancer diagnosis from lesion images ({ISIC}2016 task).},
	journaltitle = {{arXiv}:1703.02910 [cs, stat]},
	author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
	urldate = {2022-04-19},
	date = {2017-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.02910},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kawaguchi_bayesian_nodate,
	title = {Bayesian Optimization with Exponential Convergence},
	abstract = {This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the δ-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence [1] requires access to the δ-cover sampling, which was considered to be impractical [1, 2]. Our approach eliminates both requirements and achieves an exponential convergence rate.},
	pages = {9},
	author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Lozano-Pérez, Tomás},
	langid = {english},
}

@article{galuzio_mobopt_2020,
	title = {{MOBOpt} — multi-objective Bayesian optimization},
	volume = {12},
	issn = {23527110},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2352711020300911},
	doi = {10.1016/j.softx.2020.100520},
	abstract = {This work presents a new software, programmed as a Python class, that implements a multiobjective Bayesian optimization algorithm. The proposed method is able to calculate the Pareto front approximation of optimization problems with fewer objective functions evaluations than other methods, which makes it appropriate for costly objectives. The software was extensively tested on benchmark functions for optimization, and it was able to obtain Pareto Function approximations for the benchmarks with as many as 20 objective function evaluations, those results were obtained for problems with different dimensionalities and constraints.},
	pages = {100520},
	journaltitle = {{SoftwareX}},
	shortjournal = {{SoftwareX}},
	author = {Galuzio, Paulo Paneque and de Vasconcelos Segundo, Emerson Hochsteiner and Coelho, Leandro dos Santos and Mariani, Viviana Cocco},
	urldate = {2022-04-15},
	date = {2020-07},
	langid = {english},
}

@article{bischl_mlrmbo_2018,
	title = {{mlrMBO}: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions},
	url = {http://arxiv.org/abs/1703.03373},
	shorttitle = {{mlrMBO}},
	abstract = {We present {mlrMBO}, a ﬂexible and comprehensive R toolbox for model-based optimization ({MBO}), also known as Bayesian optimization, which addresses the problem of expensive black-box optimization by approximating the given objective function through a surrogate regression model. It is designed for both single- and multi-objective optimization with mixed continuous, categorical and conditional parameters. Additional features include multi-point batch proposal, parallelization, visualization, logging and error-handling. {mlrMBO} is implemented in a modular fashion, such that single components can be easily replaced or adapted by the user for speciﬁc use cases, e.g., any regression learner from the mlr toolbox for machine learning can be used, and inﬁll criteria and inﬁll optimizers are easily exchangeable. We empirically demonstrate that {mlrMBO} provides state-of-the-art performance by comparing it on diﬀerent benchmark scenarios against a wide range of other optimizers, including {DiceOptim}, {rBayesianOptimization}, {SPOT}, {SMAC}, Spearmint, and Hyperopt.},
	journaltitle = {{arXiv}:1703.03373 [stat]},
	author = {Bischl, Bernd and Richter, Jakob and Bossek, Jakob and Horn, Daniel and Thomas, Janek and Lang, Michel},
	urldate = {2022-04-15},
	date = {2018-12-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.03373},
	keywords = {Statistics - Machine Learning},
}

@article{morvan_whats_2021,
	title = {What's a good imputation to predict with missing values?},
	url = {http://arxiv.org/abs/2106.00311},
	abstract = {How to learn a good predictor on data with missing values? Most efforts focus on ﬁrst imputing as well as possible and second learning on the completed data to predict the outcome. Yet, this widespread practice has no theoretical grounding. Here we show that for almost all imputation functions, an impute-then-regress procedure with a powerful learner is Bayes optimal. This result holds for all missing-values mechanisms, in contrast with the classic statistical results that require missing-at-random settings to use imputation in probabilistic modeling. Moreover, it implies that perfect conditional imputation is not needed for good prediction asymptotically. In fact, we show that on perfectly imputed data the best regression function will generally be discontinuous, which makes it hard to learn. Crafting instead the imputation so as to leave the regression function unchanged simply shifts the problem to learning discontinuous imputations. Rather, we suggest that it is easier to learn imputation and regression jointly. We propose such a procedure, adapting {NeuMiss}, a neural network capturing the conditional links across observed and unobserved variables whatever the missing-value pattern. Experiments conﬁrm that joint imputation and regression through {NeuMiss} is better than various two step procedures in our experiments with ﬁnite number of samples.},
	journaltitle = {{arXiv}:2106.00311 [cs, stat]},
	author = {Morvan, Marine Le and Josse, Julie and Scornet, Erwan and Varoquaux, Gaël},
	urldate = {2022-04-11},
	date = {2021-11-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2106.00311},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bourlard_auto-association_1988,
	title = {Auto-association by multilayer perceptrons and singular value decomposition},
	volume = {59},
	issn = {0340-1200, 1432-0770},
	url = {http://link.springer.com/10.1007/BF00332918},
	doi = {10.1007/BF00332918},
	abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo6ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r61e of the different parameters.},
	pages = {291--294},
	number = {4},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol. Cybern.},
	author = {Bourlard, H. and Kamp, Y.},
	urldate = {2022-04-11},
	date = {1988-09},
	langid = {english},
}

@article{balestriero_effects_2022,
	title = {The Effects of Regularization and Data Augmentation are Class Dependent},
	url = {http://arxiv.org/abs/2204.03632},
	abstract = {Regularization is a fundamental technique to prevent over-ﬁtting and to improve generalization performances by constraining a model’s complexity. Current Deep Networks heavily rely on regularizers such as Data-Augmentation ({DA}) or weight-decay, and employ structural risk minimization, i.e. cross-validation, to select the optimal regularization hyper-parameters. In this study, we demonstrate that techniques such as {DA} or weight decay produce a model with a reduced complexity that is unfair across classes. The optimal amount of {DA} or weight decay found from cross-validation leads to disastrous model performances on some classes e.g. on Imagenet with a resnet50, the “barn spider” classiﬁcation test accuracy falls from 68\% to 46\% only by introducing random crop {DA} during training. Even more surprising, such performance drop also appears when introducing uninformative regularization techniques such as weight decay. Those results demonstrate that our search for ever increasing generalization performance -averaged over all classes and samples- has left us with models and regularizers that silently sacriﬁce performances on some classes. This scenario can become dangerous when deploying a model on downstream tasks e.g. an Imagenet pre-trained resnet50 deployed on {INaturalist} sees its performances fall from 70\% to 30\% on class \#8889 when introducing random crop {DA} during the Imagenet pre-training phase. Those results demonstrate that designing novel regularizers without class-dependent bias remains an open research question.},
	journaltitle = {{arXiv}:2204.03632 [cs, stat]},
	author = {Balestriero, Randall and Bottou, Leon and {LeCun}, Yann},
	urldate = {2022-04-11},
	date = {2022-04-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2204.03632},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_bayesian_2016,
	title = {Bayesian Optimization in a Billion Dimensions via Random Embeddings},
	volume = {55},
	issn = {1076-9757},
	url = {https://jair.org/index.php/jair/article/view/10983},
	doi = {10.1613/jair.4806},
	abstract = {Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm conﬁguration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identiﬁed its scaling to high-dimensions as one of the holy grails of the ﬁeld. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random {EMbedding} Bayesian Optimization ({REMBO}) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of {REMBO}. Empirical results conﬁrm that {REMBO} can eﬀectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that {REMBO} achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.},
	pages = {361--387},
	journaltitle = {Journal of Artificial Intelligence Research},
	shortjournal = {jair},
	author = {Wang, Ziyu and Hutter, Frank and Zoghi, Masrour and Matheson, David and De Feitas, Nando},
	urldate = {2022-04-07},
	date = {2016-02-19},
	langid = {english},
}

@article{wilson_maximizing_2018,
	title = {Maximizing acquisition functions for Bayesian optimization},
	url = {http://arxiv.org/abs/1805.10196},
	abstract = {Bayesian optimization is a sample-efﬁcient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes’ decision rule, but this ideal is difﬁcult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We ﬁrst show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including {EI} and {UCB}, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	journaltitle = {{arXiv}:1805.10196 [cs, stat]},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	urldate = {2022-04-07},
	date = {2018-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.10196},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gobert_adaptive_nodate,
	title = {Adaptive Space Partitioning for Parallel Bayesian Optimization},
	abstract = {This paper presents a new approach to provide large and well-chosen batches of points to evaluate in parallel in the context of batch-parallel Global Optimization. The method combines Bayesian Optimization and Design Space Partitioning in order to independently select different candidates according to the expected information they could provide if included in the design of experiment. The information in question is given by a ﬁgure of merit computed thanks to the chosen bayesian model. The presented algorithm is designed to be ﬂexible and adaptable to various objective functions since it automatically adapts the space partition during the optimization process. Three benchmark functions are investigated in this study and reveal mixed results, but also interesting features for future works.},
	pages = {9},
	author = {Gobert, Maxime and Gmys, Jan and Melab, Nouredine and Tuyttens, Daniel},
	langid = {english},
}

@article{wilson_maximizing_2018-1,
	title = {Maximizing acquisition functions for Bayesian optimization},
	url = {http://arxiv.org/abs/1805.10196},
	abstract = {Bayesian optimization is a sample-efﬁcient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes’ decision rule, but this ideal is difﬁcult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, highdimensional, and intractable. We ﬁrst show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including {EI} and {UCB}, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	journaltitle = {{arXiv}:1805.10196 [cs, stat]},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	urldate = {2022-04-01},
	date = {2018-12-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1805.10196},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liu_when_2019,
	title = {When Gaussian Process Meets Big Data: A Review of Scalable {GPs}},
	url = {http://arxiv.org/abs/1807.01065},
	shorttitle = {When Gaussian Process Meets Big Data},
	abstract = {The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process ({GP}) regression, a well-known non-parametric and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable {GPs} have been presented. But they have not yet been comprehensively reviewed and analyzed in order to be well understood by both academia and industry. The review of scalable {GPs} in the {GP} community is timely and important due to the explosion of data size. To this end, this paper is devoted to the review on state-of-the-art scalable {GPs} involving two main categories: global approximations which distillate the entire data and local approximations which divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations which modify the prior but perform exact inference, posterior approximations which retain exact prior but perform approximate inference, and structured sparse approximations which exploit speciﬁc structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable {GPs} are reviewed. Finally, the extensions and open issues regarding the implementation of scalable {GPs} in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.},
	journaltitle = {{arXiv}:1807.01065 [cs, stat]},
	author = {Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
	urldate = {2022-03-31},
	date = {2019-04-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.01065},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{frazier_tutorial_2018,
	title = {A Tutorial on Bayesian Optimization},
	url = {http://arxiv.org/abs/1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantiﬁes the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function deﬁned from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-ﬁdelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the ﬁeld. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justiﬁed by a formal decision-theoretic argument, standing in contrast to previous ad hoc modiﬁcations.},
	journaltitle = {{arXiv}:1807.02811 [cs, math, stat]},
	author = {Frazier, Peter I.},
	urldate = {2022-03-31},
	date = {2018-07-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.02811},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{shahriari_taking_2016,
	title = {Taking the Human Out of the Loop: A Review of Bayesian Optimization},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	url = {https://ieeexplore.ieee.org/document/7352306/},
	doi = {10.1109/JPROC.2015.2494218},
	shorttitle = {Taking the Human Out of the Loop},
	abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable conﬁguration parameters. These parameters are often speciﬁed and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in signiﬁcant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
	pages = {148--175},
	number = {1},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
	urldate = {2022-03-29},
	date = {2016-01},
	langid = {english},
}

@article{cantu-paz_survey_nodate,
	title = {A Survey of Parallel Genetic Algorithms},
	abstract = {Genetic algorithms ({GAs}) are powerful search techniques that are used successfully to solve problems in many different disciplines. Parallel {GAs} are particularly easy to implement and promise substantial gains in performance. As such, there has been extensive research in this ﬁeld. This survey attempts to collect, organize, and present in a uniﬁed way some of the most representative publications on parallel genetic algorithms. To organize the literature, the paper presents a categorization of the techniques used to parallelize {GAs}, and shows examples of all of them. However, since the majority of the research in this ﬁeld has concentrated on parallel {GAs} with multiple populations, the survey focuses on this type of algorithms. Also, the paper describes some of the most signiﬁcant problems in modeling and designing multi-population parallel {GAs} and presents some recent advancements.},
	pages = {30},
	author = {Cantú-Paz, Erick},
	langid = {english},
}

@incollection{goos_asynchronous_2003,
	location = {Berlin, Heidelberg},
	title = {Asynchronous Genetic Algorithms for Heterogeneous Networks Using Coarse-Grained Dataflow},
	volume = {2723},
	isbn = {978-3-540-40602-0 978-3-540-45105-1},
	url = {http://link.springer.com/10.1007/3-540-45105-6_88},
	abstract = {Genetic algorithms ({GAs}) are an attractive class of techniques for solving a variety of complex search and optimization problems. Their implementation on a distributed platform can provide the necessary computing power to address large-scale problems of practical importance. On heterogeneous networks, however, the performance of a global parallel {GA} can be limited by synchronization points during the computation, particularly those between generations. We present a new approach for implementing asynchronous {GAs} based on the dataﬂow model of computation — an approach that retains the functional properties of a global parallel {GA}. Experiments conducted with an air quality optimization problem and others show that the performance of {GAs} can be substantially improved through dataﬂow-based asynchrony.},
	pages = {730--741},
	booktitle = {Genetic and Evolutionary Computation — {GECCO} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Baugh, John W. and Kumar, Sujay V.},
	editor = {Cantú-Paz, Erick and Foster, James A. and Deb, Kalyanmoy and Davis, Lawrence David and Roy, Rajkumar and O’Reilly, Una-May and Beyer, Hans-Georg and Standish, Russell and Kendall, Graham and Wilson, Stewart and Harman, Mark and Wegener, Joachim and Dasgupta, Dipankar and Potter, Mitch A. and Schultz, Alan C. and Dowsland, Kathryn A. and Jonoska, Natasha and Miller, Julian},
	editorb = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan},
	editorbtype = {redactor},
	urldate = {2022-03-29},
	date = {2003},
	langid = {english},
	doi = {10.1007/3-540-45105-6_88},
	note = {Series Title: Lecture Notes in Computer Science},
}

@article{hutter_algorithm_2013,
	title = {Algorithm Runtime Prediction: Methods \& Evaluation},
	url = {http://arxiv.org/abs/1211.0906},
	shorttitle = {Algorithm Runtime Prediction},
	abstract = {Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm’s runtime as a function of problem-speciﬁc instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic conﬁguration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and—perhaps most importantly—a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisﬁability ({SAT}), travelling salesperson ({TSP}) and mixed integer programming ({MIP}) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of {SAT}, {MIP}, and {TSP} instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously.},
	journaltitle = {{arXiv}:1211.0906 [cs, stat]},
	author = {Hutter, Frank and Xu, Lin and Hoos, Holger H. and Leyton-Brown, Kevin},
	urldate = {2022-03-29},
	date = {2013-10-26},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1211.0906},
	keywords = {68T20, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Performance, I.2.6, I.2.8, Statistics - Machine Learning},
}

@article{martin_asynchronous_nodate,
	title = {Asynchronous Parallel Evolutionary Algorithms: Leveraging Heterogeneous Fitness Evaluation Times for Scalability and Elitist Parsimony Pressure},
	abstract = {Many important problem classes lead to large variations in ﬁtness evaluation times, such as is often the case in Genetic Programming where the time complexity of executing one individual may diﬀer greatly from that of another. Asynchronous parallel evolutionary algorithms ({APEAs}) omit the generational synchronization step of traditional evolutionary algorithms which work in well-deﬁned cycles. They can provide scalability improvements proportional to the variation in ﬁtness evaluation times of the evolved individuals, and therefore should be considered when faced with the aforementioned problem classes. This paper provides an empirical analysis of the scalability improvements obtained by applying {APEAs} to such problem classes. Furthermore, {APEAs} often suﬀer from bias towards individuals with shorter ﬁtness evaluation times, simply because they propagate faster. This paper shows how to leverage this bias in order to provide a unique type of ”elitist” parsimony pressure which rewards more eﬃcient solutions with equal solution quality.},
	pages = {8},
	author = {Martin, Matthew A and Bertels, Alex R and Tauritz, Daniel R},
	langid = {english},
}

@article{gilmer_neural_2017,
	title = {Neural Message Passing for Quantum Chemistry},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to ﬁnd a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	journaltitle = {{arXiv}:1704.01212 [cs]},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2022-03-28},
	date = {2017-06-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1704.01212},
	keywords = {Computer Science - Machine Learning, I.2.6},
}

@article{vaswani_attention_nodate,
	title = {Attention is All you Need},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {11},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	langid = {english},
}

@article{hernandez-lobato_parallel_nodate,
	title = {Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space},
	abstract = {Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization ({BO}) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current {BO} methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling ({PDTS}). We show that, in small scale problems, {PDTS} performs similarly as parallel expected improvement ({EI}), a batch version of the most widely used {BO} heuristic. Additionally, in settings where parallel {EI} does not scale, {PDTS} outperforms other scalable baselines such as a greedy search, -greedy approaches and a random search method. These results show that {PDTS} is a successful solution for large-scale parallel {BO}.},
	pages = {10},
	author = {Hernández-Lobato, José Miguel and Requeima, James and Pyzer-Knapp, Edward O and Aspuru-Guzik, Alán},
	langid = {english},
}

@article{desautels_parallelizing_nodate,
	title = {Parallelizing Exploration-Exploitation Tradeoﬀs in Gaussian Process Bandit Optimization},
	pages = {51},
	author = {Desautels, Thomas and Krause, Andreas and Burdick, Joel W},
	langid = {english},
}

@article{kandasamy_parallelised_nodate,
	title = {Parallelised Bayesian Optimisation via Thompson Sampling},
	abstract = {We design and analyse variations of the classical Thompson sampling ({TS}) procedure for Bayesian optimisation ({BO}) in settings where function evaluations are expensive but can be performed in parallel. Our theoretical analysis shows that a direct application of the sequential Thompson sampling algorithm in either synchronous or asynchronous parallel settings yields a surprisingly powerful result: making n evaluations distributed among M workers is essentially equivalent to performing n evaluations in sequence. Further, by modelling the time taken to complete a function evaluation, we show that, under a time constraint, asynchronous parallel {TS} achieves asymptotically lower regret than both the synchronous and sequential versions. These results are complemented by an experimental analysis, showing that asynchronous {TS} outperforms a suite of existing parallel {BO} algorithms in simulations and in an application involving tuning hyper-parameters of a convolutional neural network. In addition to these, the proposed procedure is conceptually much simpler than existing work for parallel {BO}.},
	pages = {10},
	author = {Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Schneider, Jeff and Póczos, Barnabás},
	langid = {english},
}

@article{gonzalez_batch_2015,
	title = {Batch Bayesian Optimization via Local Penalization},
	url = {http://arxiv.org/abs/1505.08052},
	abstract = {The popularity of Bayesian optimization methods for eﬃcient exploration of parameter spaces has lead to a series of papers applying Gaussian processes as surrogates in the optimization of functions. However, most proposed approaches only allow the exploration of the parameter space to occur sequentially. Often, it is desirable to simultaneously propose batches of parameter values to explore. This is particularly the case when large parallel processing facilities are available. These could either be computational or physical facets of the process being optimized. Batch methods, however, require the modeling of the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. We investigate this issue and propose a highly eﬀective heuristic based on an estimate of the function’s Lipschitz constant that captures the most important aspect of this interaction—local repulsion—at negligible computational overhead. A penalized acquisition function is used to collect batches of points minimizing the non-parallelizable computational effort. The resulting algorithm compares very well, in run-time, with much more elaborate alternatives.},
	journaltitle = {{arXiv}:1505.08052 [stat]},
	author = {González, Javier and Dai, Zhenwen and Hennig, Philipp and Lawrence, Neil D.},
	urldate = {2022-03-25},
	date = {2015-10-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1505.08052},
	keywords = {Statistics - Machine Learning},
}

@article{snoek_practical_2012,
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	url = {http://arxiv.org/abs/1206.2944},
	abstract = {Machine learning algorithms frequently require careful tuning of model hyperparameters, regularization terms, and optimization parameters. Unfortunately, this tuning is often a "black art" that requires expert experience, unwritten rules of thumb, or sometimes brute-force search. Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand. In this work, we consider the automatic tuning problem within the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process ({GP}). The tractable posterior distribution induced by the {GP} leads to efficient use of the information gathered by previous experiments, enabling optimal choices about what parameters to try next. Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a large impact on the success or failure of Bayesian optimization. We show that thoughtful choices can lead to results that exceed expert-level performance in tuning machine learning algorithms. We also describe new algorithms that take into account the variable cost (duration) of learning experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured {SVMs} and convolutional neural networks.},
	journaltitle = {{arXiv}:1206.2944 [cs, stat]},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P.},
	urldate = {2022-03-25},
	date = {2012-08-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1206.2944},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schulman_proximal_2017,
	title = {Proximal Policy Optimization Algorithms},
	url = {http://arxiv.org/abs/1707.06347},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization ({PPO}), have some of the benefits of trust region policy optimization ({TRPO}), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test {PPO} on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that {PPO} outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	journaltitle = {{arXiv}:1707.06347 [cs]},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	urldate = {2018-08-24},
	date = {2017-07-19},
	eprinttype = {arxiv},
	eprint = {1707.06347},
	keywords = {Computer Science - Machine Learning},
}

@article{hafner_tensorflow_2017,
	title = {{TensorFlow} Agents: Efficient Batched Reinforcement Learning in {TensorFlow}},
	url = {http://arxiv.org/abs/1709.02878},
	shorttitle = {{TensorFlow} Agents},
	abstract = {We introduce {TensorFlow} Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in {TensorFlow}. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the {TensorFlow} execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce {BatchPPO}, an efficient implementation of the proximal policy optimization algorithm. By open sourcing {TensorFlow} Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.},
	journaltitle = {{arXiv}:1709.02878 [cs]},
	author = {Hafner, Danijar and Davidson, James and Vanhoucke, Vincent},
	urldate = {2018-09-10},
	date = {2017-09-08},
	eprinttype = {arxiv},
	eprint = {1709.02878},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{mnih_asynchronous_2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1602.01783},
	abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core {CPU} instead of a {GPU}. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
	journaltitle = {{arXiv}:1602.01783 [cs]},
	author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	urldate = {2018-08-29},
	date = {2016-02-04},
	eprinttype = {arxiv},
	eprint = {1602.01783},
	keywords = {Computer Science - Machine Learning},
}

@article{williams_simple_1992,
	title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	volume = {8},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00992696},
	doi = {10.1007/BF00992696},
	abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called {REINFORCE} algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
	pages = {229--256},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Williams, Ronald J.},
	urldate = {2018-09-08},
	date = {1992-05-01},
	langid = {english},
	keywords = {Reinforcement learning, connectionist networks, gradient descent, mathematical analysis},
}

@article{brockman_openai_2016,
	title = {{OpenAI} Gym},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {{OpenAI} Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of {OpenAI} Gym and the design decisions that went into the software.},
	journaltitle = {{arXiv}:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	urldate = {2018-09-10},
	date = {2016-06-05},
	eprinttype = {arxiv},
	eprint = {1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{silver_mastering_2017,
	title = {Mastering the game of Go without human knowledge},
	volume = {550},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/nature24270},
	doi = {10.1038/nature24270},
	pages = {354--359},
	number = {7676},
	journaltitle = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	urldate = {2018-11-08},
	date = {2017-10-18},
	langid = {english},
}

@article{schulman_high-dimensional_2015,
	title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
	url = {http://arxiv.org/abs/1506.02438},
	abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to {TD}(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
	journaltitle = {{arXiv}:1506.02438 [cs]},
	author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
	urldate = {2019-08-26},
	date = {2015-06-08},
	eprinttype = {arxiv},
	eprint = {1506.02438},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@article{djallel_bouneffouf_survey_2020,
	title = {Survey on Automated End-to-End Data Science?},
	url = {http://rgdoi.net/10.13140/RG.2.2.14681.34405},
	doi = {10.13140/RG.2.2.14681.34405},
	abstract = {Data science is labor-intensive and human experts are scarce but heavily involved in every aspect of it. This makes data science time consuming and restricted to experts with the resulting quality heavily dependent on their experience and skills. To make data science more accessible and scalable, we need its democratization. Automated Data Science ({AutoDS}) is aimed towards that goal and is emerging as an important research and business topic. We introduce and deﬁne the {AutoDS} challenge, followed by a proposal of a general {AutoDS} framework that covers existing approaches but also provides guidance for the development of new methods. We categorize and review the existing literature from multiple aspects of the problem setup and employed techniques. Then we provide several views on how {AI} could succeed in automating endto-end {AutoDS}. We hope this survey can serve as insightful guideline for the {AutoDS} ﬁeld and provide inspiration for future research.},
	author = {Djallel Bouneffouf and Aggarwal, Charu and Samulowitz, Horst and Buesser, Beat and Hoang, Thanh and Udayan Khurana and Sijia Liu and Tejaswini Pedapati and Parikshit Ram and Ambrish Rawat and Wistuba, Martin and Gray, Alexander},
	urldate = {2020-07-31},
	date = {2020},
	langid = {english},
	note = {Publisher: Unpublished},
}

@article{zhengying_overview_nodate,
	title = {Overview and unifying conceptualization of Automated Machine Learning},
	abstract = {We introduce a novel generic mathematical formulation of {AutoML}, resting on formal deﬁnitions of hyperparameter optimization ({HPO}) and meta-learning. In light of this formulation, we decompose various algorithms and show that {HPO} does not really address the {AutoML} problem, more than “classical” machine learning algorithms, while meta-learning does. Other branches of machine learning such as transfer learning and ensemble learning are also reviewed, re-formulated and uniﬁed. Thus, our framework allows us to systematically classify methods and provides us with formal tools to facilitate theoretical developments and future empirical research. Our brief survey of existing methods indicates that these tools already help us gain useful insights.},
	pages = {8},
	author = {Zhengying, Liu and Xu, Zhen and Madadi, Meysam and Junior, Julio Jacques and Escalera, Sergio and Rajaa, Shangeth and Guyon, Isabelle},
	langid = {english},
}

@article{erickson_autogluon-tabular_2020,
	title = {{AutoGluon}-Tabular: Robust and Accurate {AutoML} for Structured Data},
	url = {http://arxiv.org/abs/2003.06505},
	shorttitle = {{AutoGluon}-Tabular},
	abstract = {We introduce {AutoGluon}-Tabular, an open-source {AutoML} framework that requires only a single line of Python to train highly accurate machine learning models on an unprocessed tabular dataset such as a {CSV} file. Unlike existing {AutoML} frameworks that primarily focus on model/hyperparameter selection, {AutoGluon}-Tabular succeeds by ensembling multiple models and stacking them in multiple layers. Experiments reveal that our multi-layer combination of many models offers better use of allocated training time than seeking out the best. A second contribution is an extensive evaluation of public and commercial {AutoML} platforms including {TPOT}, H2O, {AutoWEKA}, auto-sklearn, {AutoGluon}, and Google {AutoML} Tables. Tests on a suite of 50 classification and regression tasks from Kaggle and the {OpenML} {AutoML} Benchmark reveal that {AutoGluon} is faster, more robust, and much more accurate. We find that {AutoGluon} often even outperforms the best-in-hindsight combination of all of its competitors. In two popular Kaggle competitions, {AutoGluon} beat 99\% of the participating data scientists after merely 4h of training on the raw data.},
	journaltitle = {{arXiv}:2003.06505 [cs, stat]},
	author = {Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
	urldate = {2020-07-20},
	date = {2020-03-13},
	eprinttype = {arxiv},
	eprint = {2003.06505},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{thornton_auto-weka_2013,
	title = {Auto-{WEKA}: Combined Selection and Hyperparameter Optimization of Classification Algorithms},
	pages = {847--855},
	booktitle = {Proc. of {KDD}-2013},
	author = {Thornton, C. and Hutter, F. and Hoos, H. H. and Leyton-Brown, K.},
	date = {2013},
}

@article{zimmer_auto-pytorch_2020,
	title = {Auto-{PyTorch} Tabular: Multi-Fidelity {MetaLearning} for Efficient and Robust {AutoDL}},
	url = {http://arxiv.org/abs/2006.13799},
	shorttitle = {Auto-{PyTorch} Tabular},
	abstract = {While early {AutoML} frameworks focused on optimizing traditional {ML} pipelines and their hyperparameters, a recent trend in {AutoML} is to focus on neural architecture search. In this paper, we introduce Auto-{PyTorch}, which brings the best of these two worlds together by jointly and robustly optimizing the architecture of networks and the training hyperparameters to enable fully automated deep learning ({AutoDL}). Auto-{PyTorch} achieves state-of-the-art performance on several tabular benchmarks by combining multi-fidelity optimization with portfolio construction for warmstarting and ensembling of deep neural networks ({DNNs}) and common baselines for tabular data. To thoroughly study our assumptions on how to design such an {AutoDL} system, we additionally introduce a new benchmark on learning curves for {DNNs}, dubbed {LCBench}, and run extensive ablation studies of the full Auto-{PyTorch} on typical {AutoML} benchmarks, eventually showing that Auto-{PyTorch} performs better than several state-of-the-art competitors on average.},
	journaltitle = {{arXiv}:2006.13799 [cs, stat]},
	author = {Zimmer, Lucas and Lindauer, Marius and Hutter, Frank},
	urldate = {2020-08-11},
	date = {2020-06-24},
	eprinttype = {arxiv},
	eprint = {2006.13799},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gijsbers_open_nodate,
	title = {An Open Source {AutoML} Benchmark},
	abstract = {In recent years, an active ﬁeld of research has developed around automated machine learning ({AutoML}). Unfortunately, comparing diﬀerent {AutoML} systems is hard and often done incorrectly. We introduce an open, ongoing, and extensible benchmark framework which follows best practices and avoids common mistakes. The framework is open-source, uses public datasets and has a website with up-to-date results. We use the framework to conduct a thorough comparison of 4 {AutoML} systems across 39 datasets and analyze the results.},
	pages = {8},
	author = {Gijsbers, Pieter and {LeDell}, Erin and Thomas, Janek and Poirier, Sebastien and Bischl, Bernd and Vanschoren, Joaquin},
	langid = {english},
}

@article{liu_autodc_2021,
	title = {{AutoDC}: Automated data-centric processing},
	url = {http://arxiv.org/abs/2111.12548},
	shorttitle = {{AutoDC}},
	abstract = {{AutoML} (automated machine learning) has been extensively developed in the past few years for the model-centric approach. As for the data-centric approach, the processes to improve the dataset, such as ﬁxing incorrect labels, adding examples that represent edge cases, and applying data augmentation, are still very artisanal and expensive. Here we develop an automated data-centric tool ({AutoDC}), similar to the purpose of {AutoML}, aims to speed up the dataset improvement processes. In our preliminary tests on 3 open source image classiﬁcation datasets, {AutoDC} is estimated to reduce roughly 80\% of the manual time for data improvement tasks, at the same time, improve the model accuracy by 10-15\% with the ﬁxed {ML} code.},
	journaltitle = {{arXiv}:2111.12548 [cs]},
	author = {Liu, Zac Yung-Chun and Roychowdhury, Shoumik and Tarlow, Scott and Nair, Akash and Badhe, Shweta and Shah, Tejas},
	urldate = {2021-12-15},
	date = {2021-11-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2111.12548},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@article{ghorbani_data_2019,
	title = {Data Shapley: Equitable Valuation of Data for Machine Learning},
	url = {http://arxiv.org/abs/1904.02868},
	shorttitle = {Data Shapley},
	abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on n data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data shapley value uniquely satisﬁes several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efﬁciently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other beneﬁts: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.},
	journaltitle = {{arXiv}:1904.02868 [cs, stat]},
	author = {Ghorbani, Amirata and Zou, James},
	urldate = {2021-10-25},
	date = {2019-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.02868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yona_whos_2021,
	title = {Who's responsible? Jointly quantifying the contribution of the learning algorithm and training data},
	url = {http://arxiv.org/abs/1910.04214},
	shorttitle = {Who's responsible?},
	abstract = {A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A on the same dataset D would have improved performance. As {ML} becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of {ML} accountability.},
	journaltitle = {{arXiv}:1910.04214 [cs, stat]},
	author = {Yona, Gal and Ghorbani, Amirata and Zou, James},
	urldate = {2021-10-25},
	date = {2021-05-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1910.04214},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{koh_understanding_2020,
	title = {Understanding Black-box Predictions via Influence Functions},
	url = {http://arxiv.org/abs/1703.04730},
	abstract = {How can we explain the predictions of a blackbox model? In this paper, we use inﬂuence functions — a classic technique from robust statistics — to trace a model’s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up inﬂuence functions to modern machine learning settings, we develop a simple, efﬁcient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to inﬂuence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that inﬂuence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visuallyindistinguishable training-set attacks.},
	journaltitle = {{arXiv}:1703.04730 [cs, stat]},
	author = {Koh, Pang Wei and Liang, Percy},
	urldate = {2021-11-01},
	date = {2020-12-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.04730},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_hydra_nodate,
	title = {{HyDRA}: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks},
	abstract = {The behaviors of deep neural networks ({DNNs}) are notoriously resistant to human interpretations. In this paper, we propose Hypergradient Data Relevance Analysis, or {HYDRA}, which interprets the predictions made by {DNNs} as effects of their training data. Existing approaches generally estimate data contributions around the ﬁnal model parameters and ignore how the training data shape the optimization trajectory. By unrolling the hypergradient of test loss w.r.t. the weights of training data, {HYDRA} assesses the contribution of training data toward test data points throughout the training trajectory. In order to accelerate computation, we remove the Hessian from the calculation and prove that, under moderate conditions, the approximation error is bounded. Corroborating this theoretical claim, empirical results indicate the error is indeed small. In addition, we quantitatively demonstrate that {HYDRA} outperforms inﬂuence functions in accurately estimating data contribution and detecting noisy data labels. The source code is available at https://github.com/cyyever/aaai hydra.},
	pages = {9},
	author = {Chen, Yuanyuan and Li, Boyang and Yu, Han and Wu, Pengcheng and Miao, Chunyan},
	langid = {english},
}

@article{johnson_training_nodate,
	title = {Training Deep Models Faster with Robust, Approximate Importance Sampling},
	abstract = {In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure ({RAIS}) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, {RAIS} provides much of the beneﬁt of exact importance sampling with drastically reduced overhead. Empirically, we ﬁnd {RAIS}-{SGD} and standard {SGD} follow similar learning curves, but {RAIS} moves faster through these paths, achieving speed-ups of at least 20\% and sometimes much more.},
	pages = {11},
	author = {Johnson, Tyler B and Guestrin, Carlos},
	langid = {english},
}

@article{katharopoulos_not_nodate,
	title = {Not All Samples Are Created Equal:  Deep Learning with Importance Sampling},
	abstract = {Deep neural network training spends most of the computation on examples that are properly handled, and could be ignored. We propose to mitigate this phenomenon with a principled importance sampling scheme that focuses computation on “informative” examples, and reduces the variance of the stochastic gradients during training. Our contribution is twofold: ﬁrst, we derive a tractable upper bound to the per-sample gradient norm, and second we derive an estimator of the variance reduction achieved with importance sampling, which enables us to switch it on when it will result in an actual speedup. The resulting scheme can be used by changing a few lines of code in a standard {SGD} procedure, and we demonstrate experimentally, on image classiﬁcation, {CNN} ﬁne-tuning, and {RNN} training, that for a ﬁxed wall-clock time budget, it provides a reduction of the train losses of up to an order of magnitude and a relative improvement of test errors between 5\% and 17\%.},
	pages = {10},
	author = {Katharopoulos, Angelos and Fleuret, François},
	langid = {english},
}

@article{ratner_data_2017,
	title = {Data Programming: Creating Large Training Sets, Quickly},
	url = {http://arxiv.org/abs/1605.07723},
	shorttitle = {Data Programming},
	abstract = {Large labeled training sets are the critical building blocks of supervised learning methods and are key enablers of deep learning techniques. For some applications, creating labeled training sets is the most time-consuming and expensive part of applying machine learning. We therefore propose a paradigm for the programmatic creation of training sets called data programming in which users express weak supervision strategies or domain heuristics as labeling functions, which are programs that label subsets of the data, but that are noisy and may conﬂict. We show that by explicitly representing this training set labeling process as a generative model, we can “denoise” the generated training set, and establish theoretically that we can recover the parameters of these generative models in a handful of settings. We then show how to modify a discriminative loss function to make it noise-aware, and demonstrate our method over a range of discriminative models including logistic regression and {LSTMs}. Experimentally, on the 2014 {TAC}-{KBP} Slot Filling challenge, we show that data programming would have led to a new winning score, and also show that applying data programming to an {LSTM} model leads to a {TAC}-{KBP} score almost 6 F1 points over a state-of-the-art {LSTM} baseline (and into second place in the competition). Additionally, in initial user studies we observed that data programming may be an easier way for non-experts to create machine learning models when training data is limited or unavailable.},
	journaltitle = {{arXiv}:1605.07723 [cs, stat]},
	author = {Ratner, Alexander and De Sa, Christopher and Wu, Sen and Selsam, Daniel and Ré, Christopher},
	urldate = {2021-10-28},
	date = {2017-01-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1605.07723},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{li_auto-fuzzyjoin_2021,
	location = {Virtual Event China},
	title = {Auto-{FuzzyJoin}: Auto-Program Fuzzy Similarity Joins Without Labeled Examples},
	isbn = {978-1-4503-8343-1},
	url = {https://dl.acm.org/doi/10.1145/3448016.3452824},
	doi = {10.1145/3448016.3452824},
	shorttitle = {Auto-{FuzzyJoin}},
	abstract = {Fuzzy similarity join is an important database operator widely used in practice. So far the research community has focused exclusively on optimizing fuzzy join scalability. However, practitioners today also struggle to optimize fuzzy-join quality, because they face a daunting space of parameters (e.g., distance-functions, distancethresholds, tokenization-options, etc.), and often have to resort to a manual trial-and-error approach to program these parameters in order to optimize fuzzy-join quality. This key challenge of automatically generating high-quality fuzzy-join programs has received surprisingly little attention thus far. In this work, we study the problem of “auto-program” fuzzyjoins. Leveraging a geometric interpretation of distance-functions, we develop an unsupervised Auto-{FuzzyJoin} framework that can infer suitable fuzzy-join programs on given input tables, without requiring explicit human input such as labelled training data. Using Auto-{FuzzyJoin}, users only need to provide two input tables 𝐿 and 𝑅, and a desired precision target 𝜏 (say 0.9). Auto-{FuzzyJoin} leverages the fact that one of the input is a reference table to automatically program fuzzy-joins that meet the precision target 𝜏 in expectation, while maximizing fuzzy-join recall (defined as the number of correctly joined records).},
	eventtitle = {{SIGMOD}/{PODS} '21: International Conference on Management of Data},
	pages = {1064--1076},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data},
	publisher = {{ACM}},
	author = {Li, Peng and Cheng, Xiang and Chu, Xu and He, Yeye and Chaudhuri, Surajit},
	urldate = {2021-10-25},
	date = {2021-06-09},
	langid = {english},
}

@article{guo_entity_2016,
	title = {Entity Embeddings of Categorical Variables},
	url = {http://arxiv.org/abs/1604.06737},
	abstract = {We map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables. We applied it successfully in a recent Kaggle competition and were able to reach the third position with relative simple features. We further demonstrate in this paper that entity embedding helps the neural network to generalize better when the data is sparse and statistics is unknown. Thus it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit. We also demonstrate that the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead. As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering.},
	journaltitle = {{arXiv}:1604.06737 [cs]},
	author = {Guo, Cheng and Berkhahn, Felix},
	urldate = {2021-01-03},
	date = {2016-04-22},
	eprinttype = {arxiv},
	eprint = {1604.06737},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning},
}

@article{salim_balsam_2019,
	title = {Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive {HPC} Workflows},
	url = {http://arxiv.org/abs/1909.08704},
	shorttitle = {Balsam},
	abstract = {We introduce the Balsam service to manage high-throughput task scheduling and execution on supercomputing systems. Balsam allows users to populate a task database with a variety of tasks ranging from simple independent tasks to dynamic multi-task workflows. With abstractions for the local resource scheduler and {MPI} environment, Balsam dynamically packages tasks into ensemble jobs and manages their scheduling lifecycle. The ensembles execute in a pilot "launcher" which (i) ensures concurrent, load-balanced execution of arbitrary serial and parallel programs with heterogeneous processor requirements, (ii) requires no modification of user applications, (iii) is tolerant of task-level faults and provides several options for error recovery, (iv) stores provenance data (e.g task history, error logs) in the database, (v) supports dynamic workflows, in which tasks are created or killed at runtime. Here, we present the design and Python implementation of the Balsam service and launcher. The efficacy of this system is illustrated using two case studies: hyperparameter optimization of deep neural networks, and high-throughput single-point quantum chemistry calculations. We find that the unique combination of flexible job-packing and automated scheduling with dynamic (pilot-managed) execution facilitates excellent resource utilization. The scripting overheads typically needed to manage resources and launch workflows on supercomputers are substantially reduced, accelerating workflow development and execution.},
	journaltitle = {{arXiv}:1909.08704 [cs]},
	author = {Salim, Michael A. and Uram, Thomas D. and Childers, J. Taylor and Balaprakash, Prasanna and Vishwanath, Venkatram and Papka, Michael E.},
	urldate = {2020-08-13},
	date = {2019-09-18},
	eprinttype = {arxiv},
	eprint = {1909.08704},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing},
}

@article{guyon_introduction_nodate,
	title = {An Introduction to Variable and Feature Selection},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better deﬁnition of the objective function, feature construction, feature ranking, multivariate feature selection, efﬁcient search methods, and feature validity assessment methods.},
	pages = {26},
	author = {Guyon, Isabelle and Elisseeff, Andre},
	langid = {english},
}

@article{brochu_tutorial_2010,
	title = {A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning},
	url = {http://arxiv.org/abs/1012.2599},
	abstract = {We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences.},
	journaltitle = {{arXiv}:1012.2599 [cs]},
	author = {Brochu, Eric and Cora, Vlad M. and de Freitas, Nando},
	urldate = {2020-07-08},
	date = {2010-12-12},
	eprinttype = {arxiv},
	eprint = {1012.2599},
	keywords = {Computer Science - Machine Learning, G.1.6, G.3, I.2.6},
}

@article{finn_model-agnostic_2017,
	title = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
	url = {http://arxiv.org/abs/1703.03400},
	abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
	journaltitle = {{arXiv}:1703.03400 [cs]},
	author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	urldate = {2020-05-31},
	date = {2017-07-18},
	eprinttype = {arxiv},
	eprint = {1703.03400},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00058655},
	doi = {10.1007/BF00058655},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classi cation and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause signi cant changes in the predictor constructed, then bagging can improve accuracy.},
	pages = {123--140},
	number = {2},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Breiman, Leo},
	urldate = {2021-05-06},
	date = {1996-08},
	langid = {english},
}

@article{raschka_model_2020,
	title = {Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning},
	url = {http://arxiv.org/abs/1811.12808},
	abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different ﬂavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to conﬁdence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F -test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
	journaltitle = {{arXiv}:1811.12808 [cs, stat]},
	author = {Raschka, Sebastian},
	urldate = {2021-06-02},
	date = {2020-11-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.12808},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{geman_neural_nodate,
	title = {Neural Networks and the Bias Variance Dilemma},
	pages = {58},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, {RenC}},
	langid = {english},
}

@article{babuji_targeting_2020,
	title = {Targeting {SARS}-{CoV}-2 with {AI}- and {HPC}-enabled Lead Generation: A First Data Release},
	url = {http://arxiv.org/abs/2006.02431},
	shorttitle = {Targeting {SARS}-{CoV}-2 with {AI}- and {HPC}-enabled Lead Generation},
	abstract = {Researchers across the globe are seeking to rapidly repurpose existing drugs or discover new drugs to counter the the novel coronavirus disease ({COVID}-19) caused by severe acute respiratory syndrome coronavirus 2 ({SARS}-{CoV}-2). One promising approach is to train machine learning ({ML}) and artificial intelligence ({AI}) tools to screen large numbers of small molecules. As a contribution to that effort, we are aggregating numerous small molecules from a variety of sources, using high-performance computing ({HPC}) to computer diverse properties of those molecules, using the computed properties to train {ML}/{AI} models, and then using the resulting models for screening. In this first data release, we make available 23 datasets collected from community sources representing over 4.2 B molecules enriched with pre-computed: 1) molecular fingerprints to aid similarity searches, 2) 2D images of molecules to enable exploration and application of image-based deep learning methods, and 3) 2D and 3D molecular descriptors to speed development of machine learning models. This data release encompasses structural information on the 4.2 B molecules and 60 {TB} of pre-computed data. Future releases will expand the data to include more detailed molecular simulations, computed models, and other products.},
	journaltitle = {{arXiv}:2006.02431 [cs, q-bio, stat]},
	author = {Babuji, Yadu and Blaiszik, Ben and Brettin, Tom and Chard, Kyle and Chard, Ryan and Clyde, Austin and Foster, Ian and Hong, Zhi and Jha, Shantenu and Li, Zhuozhao and Liu, Xuefeng and Ramanathan, Arvind and Ren, Yi and Saint, Nicholaus and Schwarting, Marcus and Stevens, Rick and van Dam, Hubertus and Wagner, Rick},
	urldate = {2020-07-07},
	date = {2020-05-27},
	eprinttype = {arxiv},
	eprint = {2006.02431},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@article{wu_moleculenet_2018,
	title = {{MoleculeNet}: A Benchmark for Molecular Machine Learning},
	url = {http://arxiv.org/abs/1703.00564},
	shorttitle = {{MoleculeNet}},
	abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces {MoleculeNet}, a large scale benchmark for molecular machine learning. {MoleculeNet} curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the {DeepChem} open source library). {MoleculeNet} benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
	journaltitle = {{arXiv}:1703.00564 [physics, stat]},
	author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
	urldate = {2021-03-19},
	date = {2018-10-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.00564},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Statistics - Machine Learning},
}

@article{duvenaud_convolutional_2015,
	title = {Convolutional Networks on Graphs for Learning Molecular Fingerprints},
	url = {http://arxiv.org/abs/1509.09292},
	abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular ﬁngerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
	journaltitle = {{arXiv}:1509.09292 [cs, stat]},
	author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
	urldate = {2021-03-19},
	date = {2015-11-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1509.09292},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{jiang_graph_2020,
	title = {Graph Neural Network Architecture Search for Molecular Property Prediction},
	url = {http://arxiv.org/abs/2008.12187},
	abstract = {Predicting the properties of a molecule from its structure is a challenging task. Recently, deep learning methods have improved the state of the art for this task because of their ability to learn useful features from the given data. By treating molecule structure as graphs, where atoms and bonds are modeled as nodes and edges, graph neural networks ({GNNs}) have been widely used to predict molecular properties. However, the design and development of {GNNs} for a given dataset rely on labor-intensive design and tuning of the network architectures. Neural architecture search ({NAS}) is a promising approach to discover high-performing neural network architectures automatically. To that end, we develop an {NAS} approach to automate the design and development of {GNNs} for molecular property prediction. Speciﬁcally, we focus on automated development of message-passing neural networks ({MPNNs}) to predict the molecular properties of small molecules in quantum mechanics and physical chemistry datasets from the {MoleculeNet} benchmark. We demonstrate the superiority of the automatically discovered {MPNNs} by comparing them with manually designed {GNNs} from the {MoleculeNet} benchmark. We study the relative importance of the choices in the {MPNN} search space, demonstrating that customizing the architecture is critical to enhancing performance in molecular property prediction and that the proposed approach can perform customization automatically with minimal manual effort.},
	journaltitle = {{arXiv}:2008.12187 [cs, q-bio, stat]},
	author = {Jiang, Shengli and Balaprakash, Prasanna},
	urldate = {2020-11-03},
	date = {2020-08-27},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.12187},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules, Statistics - Machine Learning},
}

@inproceedings{attali_weak_2007,
	location = {Beijing, China},
	title = {Weak witnesses for Delaunay triangulations of submanifolds},
	isbn = {978-1-59593-666-0},
	url = {http://portal.acm.org/citation.cfm?doid=1236246.1236267},
	doi = {10.1145/1236246.1236267},
	abstract = {The main result of this paper is an extension of de Silva’s Weak Delaunay Theorem to smoothly embedded curves and surfaces in Euclidean space. Assuming a sufﬁciently ﬁne sampling, we prove that i + 1 points in the sample span an i-simplex in the restricted Delaunay triangulation iff every subset of the i + 1 points has a weak witness.},
	eventtitle = {the 2007 {ACM} symposium},
	pages = {143},
	booktitle = {Proceedings of the 2007 {ACM} symposium on Solid and physical modeling  - {SPM} '07},
	publisher = {{ACM} Press},
	author = {Attali, Dominique and Edelsbrunner, Herbert and Mileyko, Yuriy},
	urldate = {2020-09-28},
	date = {2007},
	langid = {english},
}

@article{hofer_learning_nodate,
	title = {Learning Representations of Persistence Barcodes},
	abstract = {We consider the problem of supervised learning with summary representations of topological features in data. In particular, we focus on persistent homology, the prevalent tool used in topological data analysis. As the summary representations, referred to as barcodes or persistence diagrams, come in the unusual format of multi sets, equipped with computationally expensive metrics, they can not readily be processed with conventional learning techniques. While diﬀerent approaches to address this problem have been proposed, either in the context of kernel-based learning, or via carefully designed vectorization techniques, it remains an open problem how to leverage advances in representation learning via deep neural networks. Appropriately handling topological summaries as input to neural networks would address the disadvantage of previous strategies which handle this type of data in a task-agnostic manner. In particular, we propose an approach that is designed to learn a task-speciﬁc representation of barcodes. In other words, we aim to learn a representation that adapts to the learning problem while, at the same time, preserving theoretical properties (such as stability). This is done by projecting barcodes into a ﬁnite dimensional vector space using a collection of parametrized functionals, so called structure elements, for which we provide a generic construction scheme. A theoretical analysis of this approach reveals suﬃcient conditions to preserve stability, and also shows that diﬀerent choices of structure elements lead to great diﬀerences with respect to their suitability for numerical optimization. When implemented as a neural network input layer, our approach demonstrates compelling performance on various types of problems, including graph classiﬁcation and eigenvalue prediction, the classiﬁcation of 2D/3D object shapes and recognizing activities from {EEG} signals.},
	pages = {45},
	author = {Hofer, Christoph D and Kwitt, Roland and Niethammer, Marc},
	langid = {english},
}

@article{dasgupta_random_nodate,
	title = {Random projection trees for vector quantization},
	abstract = {A simple and computationally efﬁcient scheme for tree-structured vector quantization is presented. Unlike previous methods, its quantization error depends only on the intrinsic dimension of the data distribution, rather than the apparent dimension of the space in which the data happen to lie.},
	pages = {14},
	author = {Dasgupta, Sanjoy and Freund, Yoav},
	langid = {english},
}

@article{hofer_deep_2018,
	title = {Deep Learning with Topological Signatures},
	url = {http://arxiv.org/abs/1707.04041},
	abstract = {Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classiﬁcation experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.},
	journaltitle = {{arXiv}:1707.04041 [cs, math]},
	author = {Hofer, Christoph and Kwitt, Roland and Niethammer, Marc and Uhl, Andreas},
	urldate = {2020-10-05},
	date = {2018-02-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1707.04041},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Algebraic Topology},
}

@article{noauthor_phylogenetic_nodate,
	title = {Phylogenetic trees metric.pdf},
}

@article{turner_persistent_2014,
	title = {Persistent Homology Transform for Modeling Shapes and Surfaces},
	url = {http://arxiv.org/abs/1310.1030},
	abstract = {In this paper we introduce a statistic, the persistent homology transform ({PHT}), to model surfaces in \${\textbackslash}mathbb\{R\}{\textasciicircum}3\$ and shapes in \${\textbackslash}mathbb\{R\}{\textasciicircum}2\$. This statistic is a collection of persistence diagrams - multiscale topological summaries used extensively in topological data analysis. We use the {PHT} to represent shapes and execute operations such as computing distances between shapes or classifying shapes. We prove the map from the space of simplicial complexes in \${\textbackslash}mathbb\{R\}{\textasciicircum}3\$ into the space spanned by this statistic is injective. This implies that the statistic is a sufficient statistic for probability densities on the space of piecewise linear shapes. We also show that a variant of this statistic, the Euler Characteristic Transform ({ECT}), admits a simple exponential family formulation which is of use in providing likelihood based inference for shapes and surfaces. We illustrate the utility of this statistic on simulated and real data.},
	journaltitle = {{arXiv}:1310.1030 [math, stat]},
	author = {Turner, Katharine and Mukherjee, Sayan and Boyer, Doug M.},
	urldate = {2020-10-31},
	date = {2014-07-15},
	eprinttype = {arxiv},
	eprint = {1310.1030},
	keywords = {Mathematics - Statistics Theory},
}

@article{khrulkov_geometry_nodate,
	title = {Geometry Score: A Method For Comparing Generative Adversarial Networks},
	abstract = {One of the biggest challenges in the research of generative adversarial networks ({GANs}) is assessing the quality of generated samples and detecting various levels of mode collapse. In this work, we construct a novel measure of performance of a {GAN} by comparing geometrical properties of the underlying data manifold and the generated one, which provides both qualitative and quantitative means for evaluation. Our algorithm can be applied to datasets of an arbitrary nature and is not limited to visual data. We test the obtained metric on various real–life models and datasets and demonstrate that our method provides new insights into properties of {GANs}.},
	pages = {9},
	author = {Khrulkov, Valentin and Oseledets, Ivan},
	langid = {english},
}

@article{carriere_perslay_2020,
	title = {{PersLay}: A Neural Network Layer for Persistence Diagrams and New Graph Topological Signatures},
	url = {http://arxiv.org/abs/1904.09378},
	shorttitle = {{PersLay}},
	abstract = {Persistence diagrams, the most common descriptors of Topological Data Analysis, encode topological properties of data and have already proved pivotal in many diﬀerent applications of data science. However, since the metric space of persistence diagrams is not Hilbert, they end up being diﬃcult inputs for most Machine Learning techniques. To address this concern, several vectorization methods have been put forward that embed persistence diagrams into either ﬁnite-dimensional Euclidean space or implicit inﬁnite dimensional Hilbert space with kernels.},
	journaltitle = {{arXiv}:1904.09378 [cs, math, stat]},
	author = {Carrière, Mathieu and Chazal, Frédéric and Ike, Yuichi and Lacombe, Théo and Royer, Martin and Umeda, Yuhei},
	urldate = {2021-06-02},
	date = {2020-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1904.09378},
	keywords = {Computer Science - Computational Geometry, Computer Science - Machine Learning, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

@article{perea_sliding_2013,
	title = {Sliding Windows and Persistence: An Application of Topological Methods to Signal Analysis},
	url = {http://arxiv.org/abs/1307.6188},
	shorttitle = {Sliding Windows and Persistence},
	abstract = {We develop in this paper a theoretical framework for the topological study of time series data. Broadly speaking, we describe geometrical and topological properties of sliding window (or time-delay) embeddings, as seen through the lens of persistent homology. In particular, we show that maximum persistence at the point-cloud level can be used to quantify periodicity at the signal level, prove structural and convergence theorems for the resulting persistence diagrams, and derive estimates for their dependency on window size and embedding dimension. We apply this methodology to quantifying periodicity in synthetic data sets, and compare the results with those obtained using state-of-the-art methods in gene expression analysis. We call this new method {SW}1PerS which stands for Sliding Windows and 1-dimensional Persistence Scoring.},
	journaltitle = {{arXiv}:1307.6188 [math, stat]},
	author = {Perea, Jose and Harer, John},
	urldate = {2020-10-07},
	date = {2013-11-25},
	eprinttype = {arxiv},
	eprint = {1307.6188},
	keywords = {Mathematics - Algebraic Topology, Mathematics - Statistics Theory},
}

@article{naitzat_topology_2020,
	title = {Topology of deep neural networks},
	url = {http://arxiv.org/abs/2004.06093},
	abstract = {We study how the topology of a data set \$M = M\_a {\textbackslash}cup M\_b {\textbackslash}subseteq {\textbackslash}mathbb\{R\}{\textasciicircum}d\$, representing two classes \$a\$ and \$b\$ in a binary classification problem, changes as it passes through the layers of a well-trained neural network, i.e., with perfect accuracy on training set and near-zero generalization error (\${\textbackslash}approx 0.01{\textbackslash}\%\$). The goal is to shed light on two mysteries in deep neural networks: (i) a nonsmooth activation function like {ReLU} outperforms a smooth one like hyperbolic tangent; (ii) successful neural network architectures rely on having many layers, even though a shallow network can approximate any function arbitrary well. We performed extensive experiments on the persistent homology of a wide range of point cloud data sets, both real and simulated. The results consistently demonstrate the following: (1) Neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simple one as it passes through the layers. No matter how complicated the topology of \$M\$ we begin with, when passed through a well-trained neural network \$f : {\textbackslash}mathbb\{R\}{\textasciicircum}d {\textbackslash}to {\textbackslash}mathbb\{R\}{\textasciicircum}p\$, there is a vast reduction in the Betti numbers of both components \$M\_a\$ and \$M\_b\$; in fact they nearly always reduce to their lowest possible values: \${\textbackslash}beta\_k{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 0\$ for \$k {\textbackslash}ge 1\$ and \${\textbackslash}beta\_0{\textbackslash}bigl(f(M\_i){\textbackslash}bigr) = 1\$, \$i =a, b\$. Furthermore, (2) the reduction in Betti numbers is significantly faster for {ReLU} activation than hyperbolic tangent activation as the former defines nonhomeomorphic maps that change topology, whereas the latter defines homeomorphic maps that preserve topology. Lastly, (3) shallow and deep networks transform data sets differently -- a shallow network operates mainly through changing geometry and changes topology only in its final layers, a deep one spreads topological changes more evenly across all layers.},
	journaltitle = {{arXiv}:2004.06093 [cs, math, stat]},
	author = {Naitzat, Gregory and Zhitnikov, Andrey and Lim, Lek-Heng},
	urldate = {2021-06-02},
	date = {2020-04-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.06093},
	keywords = {Computer Science - Machine Learning, I.2.6, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

@incollection{hiot_kriging_2010,
	location = {Berlin, Heidelberg},
	title = {Kriging Is Well-Suited to Parallelize Optimization},
	volume = {2},
	isbn = {978-3-642-10700-9 978-3-642-10701-6},
	url = {http://link.springer.com/10.1007/978-3-642-10701-6_6},
	pages = {131--162},
	booktitle = {Computational Intelligence in Expensive Optimization Problems},
	publisher = {Springer Berlin Heidelberg},
	author = {Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
	editor = {Tenne, Yoel and Goh, Chi-Keong},
	editorb = {Hiot, Lim Meng and Ong, Yew Soon},
	editorbtype = {redactor},
	urldate = {2021-05-26},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-3-642-10701-6_6},
	note = {Series Title: Adaptation Learning and Optimization},
}

@article{masters_revisiting_2018,
	title = {Revisiting Small Batch Training for Deep Neural Networks},
	url = {http://arxiv.org/abs/1804.07612},
	abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a signiﬁcantly smaller memory footprint, which might also be exploited to improve machine throughput.},
	journaltitle = {{arXiv}:1804.07612 [cs, stat]},
	author = {Masters, Dominic and Luschi, Carlo},
	urldate = {2021-05-31},
	date = {2018-04-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1804.07612},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sun_survey_2019,
	title = {A Survey of Optimization Methods from a Machine Learning Perspective},
	url = {http://arxiv.org/abs/1906.06821},
	abstract = {Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various ﬁelds. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great signiﬁcance, which can offer guidance for both developments of optimization and machine learning research. In this paper, we ﬁrst describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Next, we summarize the applications and developments of optimization methods in some popular machine learning ﬁelds. Finally, we explore and give some challenges and open problems for the optimization in machine learning.},
	journaltitle = {{arXiv}:1906.06821 [cs, math, stat]},
	author = {Sun, Shiliang and Cao, Zehui and Zhu, Han and Zhao, Jing},
	urldate = {2021-01-14},
	date = {2019-10-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.06821},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{virmaux_lipschitz_nodate,
	title = {Lipschitz regularity of deep neural networks: analysis and efficient estimation},
	abstract = {Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is {NP}-hard and state-ofart methods may signiﬁcantly overestimate it. Then, we both extend and improve previous estimation methods by providing {AutoLip}, the ﬁrst generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efﬁcient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named {SeqLip} that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on {SeqLip} in order to tackle very large networks. Our experiments show that {SeqLip} can signiﬁcantly improve on the existing upper bounds. Finally, we provide an implementation of {AutoLip} in the {PyTorch} environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.},
	pages = {10},
	author = {Virmaux, Aladin and Scaman, Kevin},
	langid = {english},
}

@article{peng_pyglove_2021,
	title = {{PyGlove}: Symbolic Programming for Automated Machine Learning},
	url = {http://arxiv.org/abs/2101.08809},
	shorttitle = {{PyGlove}},
	abstract = {Neural networks are sensitive to hyper-parameter and architecture choices. Automated Machine Learning ({AutoML}) is a promising paradigm for automating these choices. Current {ML} software libraries, however, are quite limited in handling the dynamic interactions among the components of {AutoML}. For example, efﬁcient {NAS} algorithms, such as {ENAS} [1] and {DARTS} [2], typically require an implementation coupling between the search space and search algorithm, the two key components in {AutoML}. Furthermore, implementing a complex search ﬂow, such as searching architectures within a loop of searching hardware conﬁgurations, is difﬁcult. To summarize, changing the search space, search algorithm, or search ﬂow in current {ML} libraries usually requires a signiﬁcant change in the program logic. In this paper, we introduce a new way of programming {AutoML} based on symbolic programming. Under this paradigm, {ML} programs are mutable, thus can be manipulated easily by another program. As a result, {AutoML} can be reformulated as an automated process of symbolic manipulation. With this formulation, we decouple the triangle of the search algorithm, the search space and the child program. This decoupling makes it easy to change the search space and search algorithm (without and with weight sharing), as well as to add search capabilities to existing code and implement complex search ﬂows. We then introduce {PyGlove}, a new Python library that implements this paradigm. Through case studies on {ImageNet} and {NAS}-Bench-101, we show that with {PyGlove} users can easily convert a static program into a search space, quickly iterate on the search spaces and search algorithms, and craft complex search ﬂows to achieve better results.},
	journaltitle = {{arXiv}:2101.08809 [cs]},
	author = {Peng, Daiyi and Dong, Xuanyi and Real, Esteban and Tan, Mingxing and Lu, Yifeng and Liu, Hanxiao and Bender, Gabriel and Kraft, Adam and Liang, Chen and Le, Quoc V.},
	urldate = {2022-01-12},
	date = {2021-01-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2101.08809},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
}

@article{maddox_simple_nodate,
	title = {A Simple Baseline for Bayesian Uncertainty in Deep Learning},
	abstract = {We propose {SWA}-Gaussian ({SWAG}), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging ({SWA}), which computes the ﬁrst moment of stochastic gradient descent ({SGD}) iterates with a modiﬁed learning rate schedule, has recently been shown to improve generalization in deep learning. With {SWAG}, we ﬁt a Gaussian using the {SWA} solution as the ﬁrst moment and a low rank plus diagonal covariance also derived from the {SGD} iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically ﬁnd that {SWAG} approximates the shape of the true posterior, in accordance with results describing the stationary distribution of {SGD} iterates. Moreover, we demonstrate that {SWAG} performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including {MC} dropout, {KFAC} Laplace, {SGLD}, and temperature scaling.},
	pages = {12},
	author = {Maddox, Wesley J and Izmailov, Pavel and Garipov, Timur and Vetrov, Dmitry P and Wilson, Andrew Gordon},
	langid = {english},
}

@inproceedings{caruana_ensemble_2004,
	location = {Banff, Alberta, Canada},
	title = {Ensemble selection from libraries of models},
	url = {http://portal.acm.org/citation.cfm?doid=1015330.1015432},
	doi = {10.1145/1015330.1015432},
	abstract = {We present a method for constructing ensembles from libraries of thousands of models. Model libraries are generated using diﬀerent learning algorithms and parameter settings. Forward stepwise selection is used to add to the ensemble the models that maximize its performance. Ensemble selection allows ensembles to be optimized to performance metric such as accuracy, cross entropy, mean precision, or {ROC} Area. Experiments with seven test problems and ten metrics demonstrate the beneﬁt of ensemble selection.},
	eventtitle = {Twenty-first international conference},
	pages = {18},
	booktitle = {Twenty-first international conference on Machine learning  - {ICML} '04},
	publisher = {{ACM} Press},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru and Crew, Geoff and Ksikes, Alex},
	urldate = {2021-04-16},
	date = {2004},
	langid = {english},
}

@article{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-ofthe-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overﬁt the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation ({PBP}). Similar to classical backpropagation, {PBP} works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that {PBP} is signiﬁcantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that {PBP} provides accurate estimates of the posterior variance on the network weights.},
	journaltitle = {{arXiv}:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	urldate = {2021-04-26},
	date = {2015-07-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1502.05336},
	keywords = {Statistics - Machine Learning},
}

@article{wenzel_hyperparameter_2021,
	title = {Hyperparameter Ensembles for Robustness and Uncertainty Quantification},
	url = {http://arxiv.org/abs/2006.13570},
	abstract = {Ensembles over neural network weights trained from different random initialization, known as deep ensembles, achieve state-of-the-art accuracy and calibration. The recently introduced batch ensembles provide a drop-in replacement that is more parameter efﬁcient. In this paper, we design ensembles not only over weights, but over hyperparameters to improve the state of the art in both settings. For best performance independent of budget, we propose hyper-deep ensembles, a simple procedure that involves a random search over different hyperparameters, themselves stratiﬁed across multiple random initializations. Its strong performance highlights the beneﬁt of combining models with both weight and hyperparameter diversity. We further propose a parameter efﬁcient version, hyper-batch ensembles, which builds on the layer structure of batch ensembles and self-tuning networks. The computational and memory costs of our method are notably lower than typical ensembles. On image classiﬁcation tasks, with {MLP}, {LeNet}, {ResNet} 20 and Wide {ResNet} 28-10 architectures, we improve upon both deep and batch ensembles.},
	journaltitle = {{arXiv}:2006.13570 [cs, stat]},
	author = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
	urldate = {2021-04-16},
	date = {2021-01-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.13570},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{guo_calibration_2017,
	title = {On Calibration of Modern Neural Networks},
	url = {http://arxiv.org/abs/1706.04599},
	abstract = {Conﬁdence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classiﬁcation models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors inﬂuencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classiﬁcation datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling – a singleparameter variant of Platt Scaling – is surprisingly effective at calibrating predictions.},
	journaltitle = {{arXiv}:1706.04599 [cs]},
	author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
	urldate = {2021-05-04},
	date = {2017-08-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.04599},
	keywords = {Computer Science - Machine Learning},
}

@article{lakshminarayanan_simple_2017,
	title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
	url = {http://arxiv.org/abs/1612.01474},
	abstract = {Deep neural networks ({NNs}) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in {NNs} is a challenging and yet unsolved problem. Bayesian {NNs}, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require signiﬁcant modiﬁcations to the training procedure and are computationally expensive compared to standard (non-Bayesian) {NNs}. We propose an alternative to Bayesian {NNs} that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classiﬁcation and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian {NNs}. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on {ImageNet}.},
	journaltitle = {{arXiv}:1612.01474 [cs, stat]},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	urldate = {2021-04-19},
	date = {2017-11-03},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1612.01474},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{hansen_neural_1990,
	title = {Neural network ensembles},
	volume = {12},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/58871/},
	doi = {10.1109/34.58871},
	abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvalidation as a tool for optimizing network parameters and architecture. We show further that the remaining residual “generalization” error can be reduced by invoking ensembles of similar networks.},
	pages = {993--1001},
	number = {10},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Machine Intell.},
	author = {Hansen, L.K. and Salamon, P.},
	urldate = {2021-04-19},
	date = {1990-10},
	langid = {english},
}

@article{ovadia_can_2019,
	title = {Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift},
	url = {http://arxiv.org/abs/1906.02530},
	shorttitle = {Can You Trust Your Model's Uncertainty?},
	abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model’s output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and {nonBayesian} methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classiﬁcation problems and investigate the effect of dataset shift on accuracy and calibration. We ﬁnd that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
	journaltitle = {{arXiv}:1906.02530 [cs, stat]},
	author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
	urldate = {2021-05-21},
	date = {2019-12-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.02530},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zaidi_neural_2020,
	title = {Neural Ensemble Search for Performant and Calibrated Predictions},
	url = {http://arxiv.org/abs/2006.08573},
	abstract = {Ensembles of neural networks achieve superior performance compared to standalone networks not only in terms of accuracy on in-distribution data but also on data with distributional shift alongside improved uncertainty calibration. Diversity among networks in an ensemble is believed to be key for building strong ensembles, but typical approaches only ensemble different weight vectors of a ﬁxed architecture. Instead, we investigate neural architecture search ({NAS}) for explicitly constructing ensembles to exploit diversity among networks of varying architectures and to achieve robustness against distributional shift. By directly optimizing ensemble performance, our methods implicitly encourage diversity among networks, without the need to explicitly deﬁne diversity. We ﬁnd that the resulting ensembles are more diverse compared to ensembles composed of a ﬁxed architecture and are therefore also more powerful. We show signiﬁcant improvements in ensemble performance on image classiﬁcation tasks both for in-distribution data and during distributional shift with better uncertainty calibration.},
	journaltitle = {{arXiv}:2006.08573 [cs, stat]},
	author = {Zaidi, Sheheryar and Zela, Arber and Elsken, Thomas and Holmes, Chris and Hutter, Frank and Teh, Yee Whye},
	urldate = {2021-04-19},
	date = {2020-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.08573},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gal_dropout_2016,
	title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	url = {http://arxiv.org/abs/1506.02142},
	shorttitle = {Dropout as a Bayesian Approximation},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classiﬁcation do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks ({NNs}) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout {NNs} –extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacriﬁcing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classiﬁcation, using {MNIST} as an example. We show a considerable improvement in predictive log-likelihood and {RMSE} compared to existing state-of-the-art methods, and ﬁnish by using dropout’s uncertainty in deep reinforcement learning.},
	journaltitle = {{arXiv}:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	urldate = {2021-05-24},
	date = {2016-10-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1506.02142},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{gneiting_strictly_2007,
	title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	pages = {359--378},
	number = {477},
	journaltitle = {Journal of the American Statistical Association},
	shortjournal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	urldate = {2021-05-25},
	date = {2007-03},
	langid = {english},
}

@article{amini_deep_nodate,
	title = {Deep Evidential Regression},
	abstract = {Deterministic neural networks ({NNs}) are increasingly being deployed in safety critical domains, where calibrated, robust, and efﬁcient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian {NNs} to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the {NN} to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution ({OOD}) examples for training, thus enabling efﬁcient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and {OOD} test samples.},
	pages = {11},
	author = {Amini, Alexander and Schwarting, Wilko and Soleimany, Ava and Rus, Daniela},
	langid = {english},
}

@article{kohavi_bias_nodate,
	title = {Bias Plus Variance Decomposition for Zero-one loss functions},
	abstract = {We present a bias-variance decomposition of expected misclassi cation rate, the most commonly used loss function in supervised classi cation learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was o ered for the more commonly used zero-one (misclassi cation) loss functions until the recent work of Kong \& Dietterich (1995) and Breiman (1996). Their decomposition su ers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the {UCI} repository.},
	pages = {9},
	author = {Kohavi, Ron and Wolpert, David H},
	langid = {english},
}

@inproceedings{caruana_getting_2006,
	location = {Hong Kong, China},
	title = {Getting the Most Out of Ensemble Selection},
	url = {http://ieeexplore.ieee.org/document/4053111/},
	doi = {10.1109/ICDM.2006.76},
	abstract = {We investigate four previously unexplored aspects of ensemble selection, a procedure for building ensembles of classiﬁers. First we test whether adjusting model predictions to put them on a canonical scale makes the ensembles more effective. Second, we explore the performance of ensemble selection when different amounts of data are available for ensemble hillclimbing. Third, we quantify the beneﬁt of ensemble selection’s ability to optimize to arbitrary metrics. Fourth, we study the performance impact of pruning the number of models available for ensemble selection. Based on our results we present improved ensemble selection methods that double the beneﬁt of the original method.},
	eventtitle = {Sixth International Conference on Data Mining ({ICDM}'06)},
	pages = {828--833},
	booktitle = {Sixth International Conference on Data Mining ({ICDM}'06)},
	publisher = {{IEEE}},
	author = {Caruana, Rich and Munson, Art and Niculescu-Mizil, Alexandru},
	urldate = {2021-06-10},
	date = {2006-12},
	langid = {english},
	note = {{ISSN}: 1550-4786},
}

@article{abdar_review_2021,
	title = {A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges},
	volume = {76},
	issn = {15662535},
	url = {http://arxiv.org/abs/2011.06225},
	doi = {10.1016/j.inffus.2021.05.008},
	shorttitle = {A Review of Uncertainty Quantification in Deep Learning},
	abstract = {Uncertainty quantiﬁcation ({UQ}) plays a pivotal role in the reduction of uncertainties during both optimization and decision making, applied to solve a variety of real-world applications in science and engineering. Bayesian approximation and ensemble learning techniques are two of the most widely-used {UQ} methods in the literature. In this regard, researchers have proposed different {UQ} methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classiﬁcation and segmentation), natural language processing (e.g., text classiﬁcation, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in {UQ} methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlight the fundamental research challenges and directions associated with the {UQ} ﬁeld.},
	pages = {243--297},
	journaltitle = {Information Fusion},
	shortjournal = {Information Fusion},
	author = {Abdar, Moloud and Pourpanah, Farhad and Hussain, Sadiq and Rezazadegan, Dana and Liu, Li and Ghavamzadeh, Mohammad and Fieguth, Paul and Cao, Xiaochun and Khosravi, Abbas and Acharya, U. Rajendra and Makarenkov, Vladimir and Nahavandi, Saeid},
	urldate = {2021-06-24},
	date = {2021-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2011.06225},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
	volume = {110},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-021-05946-3},
	doi = {10.1007/s10994-021-05946-3},
	shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	pages = {457--506},
	number = {3},
	journaltitle = {Machine Learning},
	shortjournal = {Mach Learn},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	urldate = {2021-07-15},
	date = {2021-03},
	langid = {english},
}

@article{depeweg_decomposition_2018,
	title = {Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning},
	url = {http://arxiv.org/abs/1710.07283},
	abstract = {Bayesian neural networks with latent variables are scalable and ﬂexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further deﬁne a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.},
	journaltitle = {{arXiv}:1710.07283 [cs, stat]},
	author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
	urldate = {2021-07-21},
	date = {2018-06-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1710.07283},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{nix_estimating_1994,
	location = {Orlando, {FL}, {USA}},
	title = {Estimating the mean and variance of the target probability distribution},
	isbn = {978-0-7803-1901-1},
	url = {http://ieeexplore.ieee.org/document/374138/},
	doi = {10.1109/ICNN.1994.374138},
	eventtitle = {Proceedings of 1994 {IEEE} International Conference on Neural Networks ({ICNN}'94)},
	pages = {55--60 vol.1},
	booktitle = {Proceedings of 1994 {IEEE} International Conference on Neural Networks ({ICNN}'94)},
	publisher = {{IEEE}},
	author = {Nix, D.A. and Weigend, A.S.},
	urldate = {2021-07-21},
	date = {1994},
	langid = {english},
}

@article{kuleshov_accurate_2018,
	title = {Accurate Uncertainties for Deep Learning Using Calibrated Regression},
	url = {http://arxiv.org/abs/1807.00263},
	abstract = {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspeciﬁcation and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate — for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classiﬁcation. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and ﬁnd that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.},
	journaltitle = {{arXiv}:1807.00263 [cs, stat]},
	author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
	urldate = {2021-07-22},
	date = {2018-06-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.00263},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ilg_uncertainty_2018,
	title = {Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow},
	url = {http://arxiv.org/abs/1802.07095},
	abstract = {Optical ﬂow estimation can be formulated as an end-toend supervised learning problem, which yields estimates with a superior accuracy-runtime tradeoﬀ compared to alternative methodology. In this paper, we make such networks estimate their local uncertainty about the correctness of their prediction, which is vital information when building decisions on top of the estimations. For the ﬁrst time we compare several strategies and techniques to estimate uncertainty in a large-scale computer vision task like optical ﬂow estimation. Moreover, we introduce a new network architecture utilizing the Winner-Takes-All loss and show that this can provide complementary hypotheses and uncertainty estimates eﬃciently with a single forward pass and without the need for sampling or ensembles. Finally, we demonstrate the quality of the different uncertainty estimates, which is clearly above previous conﬁdence measures on optical ﬂow and allows for interactive frame rates.},
	journaltitle = {{arXiv}:1802.07095 [cs]},
	author = {Ilg, Eddy and Çiçek, Özgün and Galesso, Silvio and Klein, Aaron and Makansi, Osama and Hutter, Frank and Brox, Thomas},
	urldate = {2021-11-03},
	date = {2018-12-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.07095},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{tran_methods_2020,
	title = {Methods for comparing uncertainty quantifications for material property predictions},
	volume = {1},
	issn = {2632-2153},
	url = {https://iopscience.iop.org/article/10.1088/2632-2153/ab7e1a},
	doi = {10.1088/2632-2153/ab7e1a},
	pages = {025006},
	number = {2},
	journaltitle = {Machine Learning: Science and Technology},
	shortjournal = {Mach. Learn.: Sci. Technol.},
	author = {Tran, Kevin and Neiswanger, Willie and Yoon, Junwoong and Zhang, Qingyang and Xing, Eric and Ulissi, Zachary W},
	urldate = {2021-07-22},
	date = {2020-05-18},
	langid = {english},
}

@article{gustafsson_evaluating_2020,
	title = {Evaluating Scalable Bayesian Deep Learning Methods for Robust Computer Vision},
	url = {http://arxiv.org/abs/1906.01620},
	abstract = {While deep neural networks have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for example in automotive applications. In Bayesian deep learning, predictive uncertainty is commonly decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a neural network output the parameters of a certain probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no extensive comparison has been performed in a real-world setting. We therefore accept this task and propose a comprehensive evaluation framework for scalable epistemic uncertainty estimation methods in deep learning. Our proposed framework is speciﬁcally designed to test the robustness required in real-world computer vision applications. We also apply this framework to provide the ﬁrst properly extensive and conclusive comparison of the two current state-of-theart scalable methods: ensembling and {MC}-dropout. Our comparison demonstrates that ensembling consistently provides more reliable and practically useful uncertainty estimates. Code is available at https://github.com/ fregu856/evaluating\_bdl.},
	journaltitle = {{arXiv}:1906.01620 [cs, stat]},
	author = {Gustafsson, Fredrik K. and Danelljan, Martin and Schön, Thomas B.},
	urldate = {2021-11-03},
	date = {2020-04-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1906.01620},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{noe_new_2018,
	title = {On a New Improvement-Based Acquisition Function for Bayesian Optimization},
	url = {http://arxiv.org/abs/1808.06918},
	abstract = {Bayesian optimization ({BO}) is a popular algorithm for solving challenging optimization tasks. It is designed for problems where the objective function is expensive to evaluate, perhaps not available in exact form, without gradient information and possibly returning noisy values. Diﬀerent versions of the algorithm vary in the choice of the acquisition function, which recommends the point to query the objective at next. Initially, researchers focused on improvement-based acquisitions, while recently the attention has shifted to more computationally expensive informationtheoretical measures. In this paper we present two major contributions to the literature. First, we propose a new improvement-based acquisition function that recommends query points where the improvement is expected to be high with high conﬁdence. The proposed algorithm is evaluated on a large set of benchmark functions from the global optimization literature, where it turns out to perform at least as well as current state-of-the-art acquisition functions, and often better. This suggests that it is a powerful default choice for {BO}. The novel policy is then compared to widely used global optimization solvers in order to conﬁrm that {BO} methods reduce the computational costs of the optimization by keeping the number of function evaluations small. The second main contribution represents an application to precision medicine, where the interest lies in the estimation of parameters of a partial diﬀerential equations model of the human pulmonary blood circulation system. Once inferred, these parameters can help clinicians in diagnosing a patient with pulmonary hypertension without going through the standard invasive procedure of right heart catheterization, which can lead to side eﬀects and complications (e.g. severe pain, internal bleeding, thrombosis).},
	journaltitle = {{arXiv}:1808.06918 [cs, stat]},
	author = {Noè, Umberto and Husmeier, Dirk},
	urldate = {2021-05-26},
	date = {2018-08-21},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1808.06918},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sergeev_horovod_2018,
	title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
	url = {http://arxiv.org/abs/1802.05799},
	shorttitle = {Horovod},
	abstract = {Training modern deep learning models requires large amounts of computation, often provided by {GPUs}. Scaling computation from one {GPU} to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-{GPU} communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-{GPU} communication. Depending on the training library's {API}, the modification required may be either significant or minimal. Existing methods for enabling multi-{GPU} training under the {TensorFlow} library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-{GPU} training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-{GPU} communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in {TensorFlow}. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
	journaltitle = {{arXiv}:1802.05799 [cs, stat]},
	author = {Sergeev, Alexander and Del Balso, Mike},
	urldate = {2020-06-24},
	date = {2018-02-20},
	eprinttype = {arxiv},
	eprint = {1802.05799},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{devarakonda_adabatch_2018,
	title = {{AdaBatch}: Adaptive Batch Sizes for Training Deep Neural Networks},
	url = {http://arxiv.org/abs/1712.02029},
	shorttitle = {{AdaBatch}},
	abstract = {Training deep neural networks with Stochastic Gradient Descent, or its variants, requires careful choice of both learning rate and batch size. While smaller batch sizes generally converge in fewer training epochs, larger batch sizes offer more parallelism and hence better computational efficiency. We have developed a new training approach that, rather than statically choosing a single batch size for all epochs, adaptively increases the batch size during the training process. Our method delivers the convergence rate of small batch sizes while achieving performance similar to large batch sizes. We analyse our approach using the standard {AlexNet}, {ResNet}, and {VGG} networks operating on the popular {CIFAR}-10, {CIFAR}-100, and {ImageNet} datasets. Our results demonstrate that learning with adaptive batch sizes can improve performance by factors of up to 6.25 on 4 {NVIDIA} Tesla P100 {GPUs} while changing accuracy by less than 1\% relative to training with fixed batch sizes.},
	journaltitle = {{arXiv}:1712.02029 [cs, stat]},
	author = {Devarakonda, Aditya and Naumov, Maxim and Garland, Michael},
	urldate = {2020-06-30},
	date = {2018-02-13},
	eprinttype = {arxiv},
	eprint = {1712.02029},
	keywords = {68T05,, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, I.2.6, I.5.0, Statistics - Machine Learning},
}

@article{goyal_accurate_2018,
	title = {Accurate, Large Minibatch {SGD}: Training {ImageNet} in 1 Hour},
	url = {http://arxiv.org/abs/1706.02677},
	shorttitle = {Accurate, Large Minibatch {SGD}},
	abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous {SGD} offers a potential solution to this problem by dividing {SGD} minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the {SGD} minibatch size. In this paper, we empirically show that on the {ImageNet} dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains {ResNet}-50 with a minibatch size of 8192 on 256 {GPUs} in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\textasciitilde}90\% scaling efficiency when moving from 8 to 256 {GPUs}. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
	journaltitle = {{arXiv}:1706.02677 [cs]},
	author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	urldate = {2020-05-21},
	date = {2018-04-30},
	eprinttype = {arxiv},
	eprint = {1706.02677},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{maleki_scaling_2020,
	title = {Scaling Distributed Training with Adaptive Summation},
	url = {http://arxiv.org/abs/2006.02924},
	abstract = {Stochastic gradient descent ({SGD}) is an inherently sequential training algorithm--computing the gradient at batch \$i\$ depends on the model parameters learned from batch \$i-1\$. Prior approaches that break this dependence do not honor them (e.g., sum the gradients for each batch, which is not what sequential {SGD} would do) and thus potentially suffer from poor convergence. This paper introduces a novel method to combine gradients called Adasum (for adaptive sum) that converges faster than prior work. Adasum is easy to implement, almost as efficient as simply summing gradients, and is integrated into the open-source toolkit Horovod. This paper first provides a formal justification for Adasum and then empirically demonstrates Adasum is more accurate than prior gradient accumulation methods. It then introduces a series of case-studies to show Adasum works with multiple frameworks, ({TensorFlow} and {PyTorch}), scales multiple optimizers (Momentum-{SGD}, Adam, and {LAMB}) to larger batch-sizes while still giving good downstream accuracy. Finally, it proves that Adasum converges. To summarize, Adasum scales Momentum-{SGD} on the {MLPerf} Resnet50 benchmark to 64K examples before communication (no {MLPerf} v0.5 entry converged with more than 16K), the Adam optimizer to 64K examples before communication on {BERT}-{LARGE} (prior work showed Adam stopped scaling at 16K), and the {LAMB} optimizer to 128K before communication on {BERT}-{LARGE} (prior work used 64K), all while maintaining downstream accuracy metrics. Finally, if a user does not need to scale, we show {LAMB} with Adasum on {BERT}-{LARGE} converges in 30\% fewer steps than the baseline.},
	journaltitle = {{arXiv}:2006.02924 [cs]},
	author = {Maleki, Saeed and Musuvathi, Madan and Mytkowicz, Todd and Saarikivi, Olli and Xu, Tianju and Eksarevskiy, Vadim and Ekanayake, Jaliya and Barsoum, Emad},
	urldate = {2020-07-01},
	date = {2020-06-04},
	eprinttype = {arxiv},
	eprint = {2006.02924},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{laanait_exascale_2019,
	title = {Exascale Deep Learning for Scientific Inverse Problems},
	url = {http://arxiv.org/abs/1909.11150},
	abstract = {We introduce novel communication strategies in synchronous distributed Deep Learning consisting of decentralized gradient reduction orchestration and computational graph-aware grouping of gradient tensors. These new techniques produce an optimal overlap between computation and communication and result in near-linear scaling (0.93) of distributed training up to 27,600 {NVIDIA} V100 {GPUs} on the Summit Supercomputer. We demonstrate our gradient reduction techniques in the context of training a Fully Convolutional Neural Network to approximate the solution of a longstanding scientific inverse problem in materials imaging. The efficient distributed training on a dataset size of 0.5 {PB}, produces a model capable of an atomically-accurate reconstruction of materials, and in the process reaching a peak performance of 2.15(4) {EFLOPS}\$\_\{16\}\$.},
	journaltitle = {{arXiv}:1909.11150 [cond-mat, physics:physics, stat]},
	author = {Laanait, Nouamane and Romero, Joshua and Yin, Junqi and Young, M. Todd and Treichler, Sean and Starchenko, Vitalii and Borisevich, Albina and Sergeev, Alex and Matheson, Michael},
	urldate = {2020-07-01},
	date = {2019-09-24},
	eprinttype = {arxiv},
	eprint = {1909.11150},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Condensed Matter - Materials Science, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{egele_agebo-tabular_2020,
	title = {{AgEBO}-Tabular: Joint Neural Architecture and Hyperparameter Search with Autotuned Data-Parallel Training for Tabular Data},
	url = {http://arxiv.org/abs/2010.16358},
	shorttitle = {{AgEBO}-Tabular},
	abstract = {Developing high-performing predictive models for large tabular data sets is a challenging task. The state-of-theart methods are based on expert-developed model ensembles from different supervised learning methods. Recently, automated machine learning ({AutoML}) is emerging as a promising approach to automate predictive model development. Neural architecture search ({NAS}) is an {AutoML} approach that generates and evaluates multiple neural network architectures concurrently and improves the accuracy of the generated models iteratively. A key issue in {NAS}, particularly for large data sets, is the large computation time required to evaluate each generated architecture. While data-parallel training is a promising approach that can address this issue, its use within {NAS} is difﬁcult. For different data sets, the data-parallel training settings such as the number of parallel processes, learning rate, and batch size need to be adapted to achieve high accuracy and reduction in training time. To that end, we have developed {AgEBO}-Tabular, an approach to combine aging evolution ({AgE}), a parallel {NAS} method that searches over neural architecture space, and an asynchronous Bayesian optimization method for tuning the hyperparameters of the data-parallel training simultaneously. We demonstrate the efﬁcacy of the proposed method to generate high-performing neural network models for large tabular benchmark data sets. Furthermore, we demonstrate that the automatically discovered neural network models using our method outperform the stateof-the-art {AutoML} ensemble models in inference speed by two orders of magnitude while reaching similar accuracy values.},
	journaltitle = {{arXiv}:2010.16358 [cs, stat]},
	author = {Egele, Romain and Balaprakash, Prasanna and Vishwanath, Venkatram and Guyon, Isabelle and Liu, Zhengying},
	urldate = {2021-01-14},
	date = {2020-10-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.16358},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{elsken_neural_2018,
	title = {Neural Architecture Search: A Survey},
	url = {http://arxiv.org/abs/1808.05377},
	shorttitle = {Neural Architecture Search},
	abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
	journaltitle = {{arXiv}:1808.05377 [cs, stat]},
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	urldate = {2018-08-20},
	date = {2018-08-16},
	eprinttype = {arxiv},
	eprint = {1808.05377},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{zoph_neural_2016,
	title = {Neural Architecture Search with Reinforcement Learning},
	url = {http://arxiv.org/abs/1611.01578},
	abstract = {Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this {RNN} with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the {CIFAR}-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our {CIFAR}-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used {LSTM} cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on {PTB} and achieves a state-of-the-art perplexity of 1.214.},
	journaltitle = {{arXiv}:1611.01578 [cs]},
	author = {Zoph, Barret and Le, Quoc V.},
	urldate = {2018-08-20},
	date = {2016-11-04},
	eprinttype = {arxiv},
	eprint = {1611.01578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{zoph_learning_2017,
	title = {Learning Transferable Architectures for Scalable Image Recognition},
	url = {http://arxiv.org/abs/1707.07012},
	abstract = {Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the "{NASNet} search space") which enables transferability. In our experiments, we search for the best convolutional layer (or "cell") on the {CIFAR}-10 dataset and then apply this cell to the {ImageNet} dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named "{NASNet} architecture". We also introduce a new regularization technique called {ScheduledDropPath} that significantly improves generalization in the {NASNet} models. On {CIFAR}-10 itself, {NASNet} achieves 2.4\% error rate, which is state-of-the-art. On {ImageNet}, {NASNet} achieves, among the published works, state-of-the-art accuracy of 82.7\% top-1 and 96.2\% top-5 on {ImageNet}. Our model is 1.2\% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer {FLOPS} - a reduction of 28\% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of {NASNets} exceed those of the state-of-the-art human-designed models. For instance, a small version of {NASNet} also achieves 74\% top-1 accuracy, which is 3.1\% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by {NASNet} used with the Faster-{RCNN} framework surpass state-of-the-art by 4.0\% achieving 43.1\% {mAP} on the {COCO} dataset.},
	journaltitle = {{arXiv}:1707.07012 [cs, stat]},
	author = {Zoph, Barret and Vasudevan, Vijay and Shlens, Jonathon and Le, Quoc V.},
	urldate = {2018-08-20},
	date = {2017-07-21},
	eprinttype = {arxiv},
	eprint = {1707.07012},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jin_auto-keras_2018,
	title = {Auto-Keras: Efficient Neural Architecture Search with Network Morphism},
	url = {http://arxiv.org/abs/1806.10282},
	shorttitle = {Auto-Keras},
	abstract = {Neural architecture search ({NAS}) has been proposed to automatically tune deep neural networks, but existing search algorithms usually suffer from expensive computational cost. Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for {NAS} by enabling a more efficient training during the search. In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search by introducing a neural network kernel and a tree-structured acquisition function optimization algorithm, which more efficiently explores the search space. Intensive experiments have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods. Moreover, we build an open-source {AutoML} system on our method, namely Auto-Keras. The system runs in parallel on {CPU} and {GPU}, with an adaptive search strategy for different {GPU} memory limits.},
	journaltitle = {{arXiv}:1806.10282 [cs, stat]},
	author = {Jin, Haifeng and Song, Qingquan and Hu, Xia},
	urldate = {2018-11-07},
	date = {2018-06-26},
	eprinttype = {arxiv},
	eprint = {1806.10282},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{real_regularized_2018,
	title = {Regularized Evolution for Image Classifier Architecture Search},
	url = {http://arxiv.org/abs/1802.01548},
	abstract = {The effort devoted to hand-crafting neural network image classifiers has motivated the use of architecture search to discover them automatically. Although evolutionary algorithms have been repeatedly applied to neural network topologies, the image classifiers thus discovered have remained inferior to human-crafted ones. Here, we evolve an image classifier---{AmoebaNet}-A---that surpasses hand-designs for the first time. To do this, we modify the tournament selection evolutionary algorithm by introducing an age property to favor the younger genotypes. Matching size, {AmoebaNet}-A has comparable accuracy to current state-of-the-art {ImageNet} models discovered with more complex architecture-search methods. Scaled to larger size, {AmoebaNet}-A sets a new state-of-the-art 83.9\% / 96.6\% top-5 {ImageNet} accuracy. In a controlled comparison against a well known reinforcement learning algorithm, we give evidence that evolution can obtain results faster with the same hardware, especially at the earlier stages of the search. This is relevant when fewer compute resources are available. Evolution is, thus, a simple method to effectively discover high-quality architectures.},
	journaltitle = {{arXiv}:1802.01548 [cs]},
	author = {Real, Esteban and Aggarwal, Alok and Huang, Yanping and Le, Quoc V.},
	urldate = {2019-08-20},
	date = {2018-02-05},
	eprinttype = {arxiv},
	eprint = {1802.01548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Neural and Evolutionary Computing, I.2.6, I.5.1, I.5.2},
}

@article{balaprakash_scalable_2019,
	title = {Scalable Reinforcement-Learning-Based Neural Architecture Search for Cancer Deep Learning Research},
	url = {http://arxiv.org/abs/1909.00311},
	doi = {10.1145/3295500.3356202},
	abstract = {Cancer is a complex disease, the understanding and treatment of which are being aided through increases in the volume of collected data and in the scale of deployed computing power. Consequently, there is a growing need for the development of data-driven and, in particular, deep learning methods for various tasks such as cancer diagnosis, detection, prognosis, and prediction. Despite recent successes, however, designing high-performing deep learning models for nonimage and nontext cancer data is a time-consuming, trial-and-error, manual task that requires both cancer domain and deep learning expertise. To that end, we develop a reinforcement-learning-based neural architecture search to automate deep-learning-based predictive model development for a class of representative cancer data. We develop custom building blocks that allow domain experts to incorporate the cancer-data-specific characteristics. We show that our approach discovers deep neural network architectures that have significantly fewer trainable parameters, shorter training time, and accuracy similar to or higher than those of manually designed architectures. We study and demonstrate the scalability of our approach on up to 1,024 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
	pages = {1--33},
	journaltitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis on   - {SC} '19},
	author = {Balaprakash, Prasanna and Egele, Romain and Salim, Misha and Wild, Stefan and Vishwanath, Venkatram and Xia, Fangfang and Brettin, Tom and Stevens, Rick},
	urldate = {2019-12-30},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1909.00311},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_nas_2020,
	title = {{NAS} evaluation is frustratingly hard},
	url = {http://arxiv.org/abs/1912.12522},
	abstract = {Neural Architecture Search ({NAS}) is an exciting new field which promises to be as much as a game-changer as Convolutional Neural Networks were in 2012. Despite many great works leading to substantial improvements on a variety of tasks, comparison between different methods is still very much an open issue. While most algorithms are tested on the same datasets, there is no shared experimental protocol followed by all. As such, and due to the under-use of ablation studies, there is a lack of clarity regarding why certain methods are more effective than others. Our first contribution is a benchmark of \$8\$ {NAS} methods on \$5\$ datasets. To overcome the hurdle of comparing methods with different search spaces, we propose using a method's relative improvement over the randomly sampled average architecture, which effectively removes advantages arising from expertly engineered search spaces or training protocols. Surprisingly, we find that many {NAS} techniques struggle to significantly beat the average architecture baseline. We perform further experiments with the commonly used {DARTS} search space in order to understand the contribution of each component in the {NAS} pipeline. These experiments highlight that: (i) the use of tricks in the evaluation protocol has a predominant impact on the reported performance of architectures; (ii) the cell-based search space has a very narrow accuracy range, such that the seed has a considerable impact on architecture rankings; (iii) the hand-designed macro-structure (cells) is more important than the searched micro-structure (operations); and (iv) the depth-gap is a real phenomenon, evidenced by the change in rankings between \$8\$ and \$20\$ cell architectures. To conclude, we suggest best practices, that we hope will prove useful for the community and help mitigate current {NAS} pitfalls. The code used is available at https://github.com/antoyang/{NAS}-Benchmark.},
	journaltitle = {{arXiv}:1912.12522 [cs, stat]},
	author = {Yang, Antoine and Esperança, Pedro M. and Carlucci, Fabio M.},
	urldate = {2020-05-18},
	date = {2020-02-13},
	eprinttype = {arxiv},
	eprint = {1912.12522},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_geometry-aware_2020,
	title = {Geometry-Aware Gradient Algorithms for Neural Architecture Search},
	url = {http://arxiv.org/abs/2004.07802},
	abstract = {Many recent state-of-the-art methods for neural architecture search ({NAS}) relax the {NAS} problem into a joint continuous optimization over architecture parameters and their shared-weights, enabling the application of standard gradient-based optimizers. However, this training process remains poorly understood, as evidenced by the multitude of gradient-based heuristics that have been recently proposed. Invoking the theory of mirror descent, we present a unifying framework for designing and analyzing gradient-based {NAS} methods that exploit the underlying problem structure to quickly find high-performance architectures. Our geometry-aware framework leads to simple yet novel algorithms that (1) enjoy faster convergence guarantees than existing gradient-based methods and (2) achieve state-of-the-art accuracy on the latest {NAS} benchmarks in computer vision. Notably, we exceed the best published results for both {CIFAR} and {ImageNet} on both the {DARTS} search space and {NAS}-Bench-201; on the latter benchmark we achieve close to oracle-optimal performance on {CIFAR}-10 and {CIFAR}-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous parameterizations of discrete {NAS} search spaces.},
	journaltitle = {{arXiv}:2004.07802 [cs, math, stat]},
	author = {Li, Liam and Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
	urldate = {2020-05-26},
	date = {2020-04-16},
	eprinttype = {arxiv},
	eprint = {2004.07802},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{zela_understanding_2020,
	title = {Understanding and Robustifying Differentiable Architecture Search},
	url = {http://arxiv.org/abs/1909.09656},
	abstract = {Differentiable Architecture Search ({DARTS}) has attracted a lot of attention due to its simplicity and small search costs achieved by a continuous relaxation and an approximation of the resulting bi-level optimization problem. However, {DARTS} does not work robustly for new problems: we identify a wide range of search spaces for which {DARTS} yields degenerate architectures with very poor test performance. We study this failure mode and show that, while {DARTS} successfully minimizes validation loss, the found solutions generalize poorly when they coincide with high validation loss curvature in the architecture space. We show that by adding one of various types of regularization we can robustify {DARTS} to find solutions with less curvature and better generalization properties. Based on these observations, we propose several simple variations of {DARTS} that perform substantially more robustly in practice. Our observations are robust across five search spaces on three image classification tasks and also hold for the very different domains of disparity estimation (a dense regression task) and language modelling.},
	journaltitle = {{arXiv}:1909.09656 [cs, stat]},
	author = {Zela, Arber and Elsken, Thomas and Saikia, Tonmoy and Marrakchi, Yassine and Brox, Thomas and Hutter, Frank},
	urldate = {2020-05-30},
	date = {2020-01-28},
	eprinttype = {arxiv},
	eprint = {1909.09656},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ying_nas-bench-101_2019,
	title = {{NAS}-Bench-101: Towards Reproducible Neural Architecture Search},
	url = {http://arxiv.org/abs/1902.09635},
	shorttitle = {{NAS}-Bench-101},
	abstract = {Recent advances in neural architecture search ({NAS}) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing {NAS}-Bench-101, the first public architecture dataset for {NAS} research. To build {NAS}-Bench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on {CIFAR}-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the pre-computed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.},
	journaltitle = {{arXiv}:1902.09635 [cs, stat]},
	author = {Ying, Chris and Klein, Aaron and Real, Esteban and Christiansen, Eric and Murphy, Kevin and Hutter, Frank},
	urldate = {2020-06-01},
	date = {2019-05-14},
	eprinttype = {arxiv},
	eprint = {1902.09635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chen_fitting_2020,
	title = {Fitting the Search Space of Weight-sharing {NAS} with Graph Convolutional Networks},
	url = {http://arxiv.org/abs/2004.08423},
	abstract = {Neural architecture search has attracted wide attentions in both academia and industry. To accelerate it, researchers proposed weight-sharing methods which first train a super-network to reuse computation among different operators, from which exponentially many sub-networks can be sampled and efficiently evaluated. These methods enjoy great advantages in terms of computational costs, but the sampled sub-networks are not guaranteed to be estimated precisely unless an individual training process is taken. This paper owes such inaccuracy to the inevitable mismatch between assembled network layers, so that there is a random error term added to each estimation. We alleviate this issue by training a graph convolutional network to fit the performance of sampled sub-networks so that the impact of random errors becomes minimal. With this strategy, we achieve a higher rank correlation coefficient in the selected set of candidates, which consequently leads to better performance of the final architecture. In addition, our approach also enjoys the flexibility of being used under different hardware constraints, since the graph convolutional network has provided an efficient lookup table of the performance of architectures in the entire search space.},
	journaltitle = {{arXiv}:2004.08423 [cs, stat]},
	author = {Chen, Xin and Xie, Lingxi and Wu, Jun and Wei, Longhui and Xu, Yuhui and Tian, Qi},
	urldate = {2020-05-30},
	date = {2020-04-17},
	eprinttype = {arxiv},
	eprint = {2004.08423},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liu_darts_2019,
	title = {{DARTS}: Differentiable Architecture Search},
	url = {http://arxiv.org/abs/1806.09055},
	shorttitle = {{DARTS}},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on {CIFAR}-10, {ImageNet}, Penn Treebank and {WikiText}-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	journaltitle = {{arXiv}:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	urldate = {2020-06-02},
	date = {2019-04-23},
	eprinttype = {arxiv},
	eprint = {1806.09055},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dong_nas-bench-201_2020,
	title = {{NAS}-{BENCH}-201: {EXTENDING} {THE} {SCOPE} {OF} {RE}- {PRODUCIBLE} {NEURAL} {ARCHITECTURE} {SEARCH}},
	abstract = {Neural architecture search ({NAS}) has achieved breakthrough success in a great number of applications in the past few years. It could be time to take a step back and analyze the good and bad aspects in the ﬁeld of {NAS}. A variety of algorithms search architectures under different search space. These searched architectures are trained using different setups, e.g., hyper-parameters, data augmentation, regularization. This raises a comparability problem when comparing the performance of various {NAS} algorithms. {NAS}-Bench-101 has shown success to alleviate this problem. In this work, we propose an extension to {NAS}-Bench-101: {NAS}-Bench201 with a different search space, results on multiple datasets, and more diagnostic information. {NAS}-Bench-201 has a ﬁxed search space and provides a uniﬁed benchmark for almost any up-to-date {NAS} algorithms. The design of our search space is inspired from the one used in the most popular cell-based searching algorithms, where a cell is represented as a directed acyclic graph. Each edge here is associated with an operation selected from a predeﬁned operation set. For it to be applicable for all {NAS} algorithms, the search space deﬁned in {NAS}-Bench-201 includes all possible architectures generated by 4 nodes and 5 associated operation options, which results in 15,625 neural cell candidates in total. The training log using the same setup and the performance for each architecture candidate are provided for three datasets. This allows researchers to avoid unnecessary repetitive training for selected architecture and focus solely on the search algorithm itself. The training time saved for every architecture also largely improves the efﬁciency of most {NAS} algorithms and brings a more computational cost friendly {NAS} community for a broader range of researchers. We provide additional diagnostic information such as ﬁne-grained loss and accuracy, which can give inspirations to new designs of {NAS} algorithms. In further support of the proposed {NAS}-Bench201, we have analyzed it from many aspects and benchmarked 10 recent {NAS} algorithms, which verify its applicability.},
	pages = {16},
	author = {Dong, Xuanyi and Yang, Yi},
	date = {2020},
	langid = {english},
}

@article{zela_nas-bench-1shot1_2020,
	title = {{NAS}-{BENCH}-1SHOT1: {BENCHMARKING} {AND} {DISSECTING} {ONE}-{SHOT} {NEURAL} {ARCHITECTURE} {SEARCH}},
	abstract = {One-shot neural architecture search ({NAS}) has played a crucial role in making {NAS} methods computationally feasible in practice. Nevertheless, there is still a lack of understanding on how these weight-sharing algorithms exactly work due to the many factors controlling the dynamics of the process. In order to allow a scientiﬁc study of these components, we introduce a general framework for one-shot {NAS} that can be instantiated to many recently-introduced variants and introduce a general benchmarking framework that draws on the recent large-scale tabular benchmark {NAS}-Bench-101 for cheap anytime evaluations of one-shot {NAS} methods. To showcase the framework, we compare several state-of-the-art one-shot {NAS} methods, examine how sensitive they are to their hyperparameters and how they can be improved by tuning their hyperparameters, and compare their performance to that of blackbox optimizers for {NAS}-Bench-101.},
	pages = {20},
	author = {Zela, Arber and Siems, Julien and Hutter, Frank},
	date = {2020},
	langid = {english},
}

@article{wang_particle_2020,
	title = {Particle Swarm Optimisation for Evolving Deep Neural Networks for Image Classification by Evolving and Stacking Transferable Blocks},
	url = {http://arxiv.org/abs/1907.12659},
	abstract = {Deep Convolutional Neural Networks ({CNNs}) have been widely used in image classification tasks, but the process of designing {CNN} architectures is very complex, so Neural Architecture Search ({NAS}), automatically searching for optimal {CNN} architectures, has attracted more and more research interests. However, the computational cost of {NAS} is often too high to apply {NAS} on real-life applications. In this paper, an efficient particle swarm optimisation method named {EPSOCNN} is proposed to evolve {CNN} architectures inspired by the idea of transfer learning. {EPSOCNN} successfully reduces the computation cost by minimising the search space to a single block and utilising a small subset of the training set to evaluate {CNNs} during evolutionary process. Meanwhile, {EPSOCNN} also keeps very competitive classification accuracy by stacking the evolved block multiple times to fit the whole dataset. The proposed {EPSOCNN} algorithm is evaluated on {CIFAR}-10 dataset and compared with 13 peer competitors comprised of deep {CNNs} crafted by hand, learned by reinforcement learning methods and evolved by evolutionary computation approaches, which shows very promising results by outperforming all of the peer competitors with regard to the classification accuracy, number of parameters and the computational cost.},
	journaltitle = {{arXiv}:1907.12659 [cs]},
	author = {Wang, Bin and Xue, Bing and Zhang, Mengjie},
	urldate = {2020-06-24},
	date = {2020-03-21},
	eprinttype = {arxiv},
	eprint = {1907.12659},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{liu_evolving_2020,
	title = {Evolving Normalization-Activation Layers},
	url = {http://arxiv.org/abs/2004.02967},
	abstract = {Normalization layers and activation functions are critical components in deep neural networks that frequently co-locate with each other. Instead of designing them separately, we unify them into a single computation graph, and evolve its structure starting from low-level primitives. Our layer search algorithm leads to the discovery of {EvoNorms}, a set of new normalization-activation layers that go beyond existing design patterns. Several of these layers enjoy the property of being independent from the batch statistics. Our experiments show that {EvoNorms} not only work well on a variety of image classification models including {ResNets}, {MobileNets} and {EfficientNets} but also transfer well to Mask R-{CNN}, {SpineNet} for instance segmentation and {BigGAN} for image synthesis, significantly outperforming {BatchNorm} and {GroupNorm} based layers in many cases.},
	journaltitle = {{arXiv}:2004.02967 [cs, stat]},
	author = {Liu, Hanxiao and Brock, Andrew and Simonyan, Karen and Le, Quoc V.},
	urldate = {2020-06-03},
	date = {2020-04-28},
	eprinttype = {arxiv},
	eprint = {2004.02967},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{zheng_rethinking_2020,
	title = {Rethinking Performance Estimation in Neural Architecture Search},
	url = {http://arxiv.org/abs/2005.09917},
	abstract = {Neural architecture search ({NAS}) remains a challenging problem, which is attributed to the indispensable and time-consuming component of performance estimation ({PE}). In this paper, we provide a novel yet systematic rethinking of {PE} in a resource constrained regime, termed budgeted {PE} ({BPE}), which precisely and effectively estimates the performance of an architecture sampled from an architecture space. Since searching an optimal {BPE} is extremely time-consuming as it requires to train a large number of networks for evaluation, we propose a Minimum Importance Pruning ({MIP}) approach. Given a dataset and a {BPE} search space, {MIP} estimates the importance of hyper-parameters using random forest and subsequently prunes the minimum one from the next iteration. In this way, {MIP} effectively prunes less important hyper-parameters to allocate more computational resource on more important ones, thus achieving an effective exploration. By combining {BPE} with various search algorithms including reinforcement learning, evolution algorithm, random search, and differentiable architecture search, we achieve 1, 000x of {NAS} speed up with a negligible performance drop comparing to the {SOTA}},
	journaltitle = {{arXiv}:2005.09917 [cs]},
	author = {Zheng, Xiawu and Ji, Rongrong and Wang, Qiang and Ye, Qixiang and Li, Zhenguo and Tian, Yonghong and Tian, Qi},
	urldate = {2020-06-03},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.09917},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zela_towards_2018,
	title = {Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search},
	url = {http://arxiv.org/abs/1807.06906},
	shorttitle = {Towards Automated Deep Learning},
	abstract = {While existing work on neural architecture search ({NAS}) tunes hyperparameters in a separate post-processing step, we demonstrate that architectural choices and other hyperparameter settings interact in a way that can render this separation suboptimal. Likewise, we demonstrate that the common practice of using very few epochs during the main {NAS} and much larger numbers of epochs during a post-processing step is inefficient due to little correlation in the relative rankings for these two training regimes. To combat both of these problems, we propose to use a recent combination of Bayesian optimization and Hyperband for efficient joint neural architecture and hyperparameter search.},
	journaltitle = {{arXiv}:1807.06906 [cs, stat]},
	author = {Zela, Arber and Klein, Aaron and Falkner, Stefan and Hutter, Frank},
	urldate = {2020-07-01},
	date = {2018-07-18},
	eprinttype = {arxiv},
	eprint = {1807.06906},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{geada_bonsai-net_2020,
	title = {Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners},
	url = {http://arxiv.org/abs/2006.09264},
	shorttitle = {Bonsai-Net},
	abstract = {One-shot Neural Architecture Search ({NAS}) aims to minimize the computational expense of discovering state-of-the-art models. However, in the past year attention has been drawn to the comparable performance of naive random search across the same search spaces used by leading {NAS} algorithms. To address this, we explore the effects of drastically relaxing the {NAS} search space, and we present Bonsai-Net, an efficient one-shot {NAS} method to explore our relaxed search space. Bonsai-Net is built around a modified differential pruner and can consistently discover state-of-the-art architectures that are significantly better than random search with fewer parameters than other state-of-the-art methods. Additionally, Bonsai-Net performs simultaneous model search and training, dramatically reducing the total time it takes to generate fully-trained models from scratch.},
	journaltitle = {{arXiv}:2006.09264 [cs, stat]},
	author = {Geada, Rob and Prangle, Dennis and {McGough}, Andrew Stephen},
	urldate = {2020-07-13},
	date = {2020-06-12},
	eprinttype = {arxiv},
	eprint = {2006.09264},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{byla_deepswarm_2019,
	title = {{DeepSwarm}: Optimising Convolutional Neural Networks using Swarm Intelligence},
	url = {http://arxiv.org/abs/1905.07350},
	shorttitle = {{DeepSwarm}},
	abstract = {In this paper we propose {DeepSwarm}, a novel neural architecture search ({NAS}) method based on Swarm Intelligence principles. At its core {DeepSwarm} uses Ant Colony Optimization ({ACO}) to generate ant population which uses the pheromone information to collectively search for the best neural architecture. Furthermore, by using local and global pheromone update rules our method ensures the balance between exploitation and exploration. On top of this, to make our method more efficient we combine progressive neural architecture search with weight reusability. Furthermore, due to the nature of {ACO} our method can incorporate heuristic information which can further speed up the search process. After systematic and extensive evaluation, we discover that on three different datasets ({MNIST}, Fashion-{MNIST}, and {CIFAR}-10) when compared to existing systems our proposed method demonstrates competitive performance. Finally, we open source {DeepSwarm} as a {NAS} library and hope it can be used by more deep learning researchers and practitioners.},
	journaltitle = {{arXiv}:1905.07350 [cs, stat]},
	author = {Byla, Edvinas and Pang, Wei},
	urldate = {2020-07-02},
	date = {2019-05-17},
	eprinttype = {arxiv},
	eprint = {1905.07350},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.2.6, Statistics - Machine Learning},
}

@article{li_stacnas_2019,
	title = {{StacNAS}: Towards Stable and Consistent Differentiable Neural Architecture Search},
	url = {http://arxiv.org/abs/1909.11926},
	shorttitle = {{StacNAS}},
	abstract = {Differentiable Neural Architecture Search algorithms such as {DARTS} have attracted much attention due to the low search cost and competitive accuracy. However, it has been observed that {DARTS} can be unstable, especially when applied to new problems. One cause of the instability is the difficulty of two-level optimization. In addition, we identify two other causes: (1) Multicollinearity of correlated/similar operations leads to unpredictable change of the architecture parameters during search; (2) The optimization complexity gap between the proxy search stage and the final training leads to suboptimal architectures. Based on these findings, we propose a two-stage grouped variable pruning algorithm using one-level optimization. In the first stage, the best group is activated, and in the second stage, the best operation in the activated group is selected. Extensive experiments verify the superiority of the proposed method both for accuracy and for stability. For the {DARTS} search space, the proposed strategy obtains state-of-the-art accuracies on {CIFAR}-10, {CIFAR}-100 and {ImageNet}. Code is available at https://github.com/susan0199/stacnas.},
	journaltitle = {{arXiv}:1909.11926 [cs, stat]},
	author = {Li, Guilin and Zhang, Xing and Wang, Zitong and Li, Zhenguo and Zhang, Tong},
	urldate = {2020-08-05},
	date = {2019-12-10},
	eprinttype = {arxiv},
	eprint = {1909.11926},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{chu_fair_2020,
	title = {Fair {DARTS}: Eliminating Unfair Advantages in Differentiable Architecture Search},
	url = {http://arxiv.org/abs/1911.12126},
	shorttitle = {Fair {DARTS}},
	abstract = {Differentiable Architecture Search ({DARTS}) is now a widely disseminated weight-sharing neural architecture search method. However, it suffers from well-known performance collapse due to an inevitable aggregation of skip connections. In this paper, we first disclose that its root cause lies in an unfair advantage in exclusive competition. Through experiments, we show that if either of two conditions is broken, the collapse disappears. Thereby, we present a novel approach called Fair {DARTS} where the exclusive competition is relaxed to be collaborative. Specifically, we let each operation's architectural weight be independent of others. Yet there is still an important issue of discretization discrepancy. We then propose a zero-one loss to push architectural weights towards zero or one, which approximates an expected multi-hot solution. Our experiments are performed on two mainstream search spaces, and we derive new state-of-the-art results on {CIFAR}-10 and {ImageNet}. Our code is available on https://github.com/xiaomi-automl/fairdarts .},
	journaltitle = {{arXiv}:1911.12126 [cs, stat]},
	author = {Chu, Xiangxiang and Zhou, Tianbao and Zhang, Bo and Li, Jixiang},
	urldate = {2020-07-14},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {1911.12126},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ren_comprehensive_2020,
	title = {A Comprehensive Survey of Neural Architecture Search: Challenges and Solutions},
	url = {http://arxiv.org/abs/2006.02903},
	shorttitle = {A Comprehensive Survey of Neural Architecture Search},
	abstract = {Deep learning has made major breakthroughs and progress in many fields. This is due to the powerful automatic representation capabilities of deep learning. It has been proved that the design of the network architecture is crucial to the feature representation of data and the final performance. In order to obtain a good feature representation of data, the researchers designed various complex network architectures. However, the design of the network architecture relies heavily on the researchers' prior knowledge and experience. Therefore, a natural idea is to reduce human intervention as much as possible and let the algorithm automatically design the architecture of the network. Thus going further to the strong intelligence. In recent years, a large number of related algorithms for {\textbackslash}textit\{Neural Architecture Search\} ({NAS}) have emerged. They have made various improvements to the {NAS} algorithm, and the related research work is complicated and rich. In order to reduce the difficulty for beginners to conduct {NAS}-related research, a comprehensive and systematic survey on the {NAS} is essential. Previously related surveys began to classify existing work mainly from the basic components of {NAS}: search space, search strategy and evaluation strategy. This classification method is more intuitive, but it is difficult for readers to grasp the challenges and the landmark work in the middle. Therefore, in this survey, we provide a new perspective: starting with an overview of the characteristics of the earliest {NAS} algorithms, summarizing the problems in these early {NAS} algorithms, and then giving solutions for subsequent related research work. In addition, we conducted a detailed and comprehensive analysis, comparison and summary of these works. Finally, we give possible future research directions.},
	journaltitle = {{arXiv}:2006.02903 [cs, stat]},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
	urldate = {2020-07-20},
	date = {2020-06-01},
	eprinttype = {arxiv},
	eprint = {2006.02903},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maulik_recurrent_2020,
	title = {Recurrent Neural Network Architecture Search for Geophysical Emulation},
	url = {http://arxiv.org/abs/2004.10928},
	abstract = {Developing surrogate geophysical models from data is a key research topic in atmospheric and oceanic modeling because of the large computational costs associated with numerical simulation methods. Researchers have started applying a wide range of machine learning models, in particular neural networks, to geophysical data for forecasting without these constraints. However, constructing neural networks for forecasting such data is nontrivial and often requires trial and error. To that end, we focus on developing proper-orthogonal-decomposition-based long short-term memory networks ({POD}-{LSTMs}). We develop a scalable neural architecture search for generating stacked {LSTMs} to forecast temperature in the {NOAA} Optimum Interpolation Sea-Surface Temperature data set. Our approach identifies {POD}-{LSTMs} that are superior to manually designed variants and baseline time-series prediction methods. We also assess the scalability of different architecture search strategies on up to 512 Intel Knights Landing nodes of the Theta supercomputer at the Argonne Leadership Computing Facility.},
	journaltitle = {{arXiv}:2004.10928 [physics]},
	author = {Maulik, Romit and Egele, Romain and Lusch, Bethany and Balaprakash, Prasanna},
	urldate = {2020-07-20},
	date = {2020-04-22},
	eprinttype = {arxiv},
	eprint = {2004.10928},
	keywords = {Physics - Computational Physics},
}

@article{mellor_neural_2020,
	title = {Neural Architecture Search without Training},
	url = {http://arxiv.org/abs/2006.04647},
	abstract = {The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search ({NAS}) techniques to automate this design. However, {NAS} algorithms tend to be extremely slow and expensive; they need to train vast numbers of candidate networks to inform the search process. This could be remedied if we could infer a network's trained accuracy from its initial state. In this work, we examine how the linear maps induced by data points correlate for untrained network architectures in the {NAS}-Bench-201 search space, and motivate how this can be used to give a measure of modelling flexibility which is highly indicative of a network's trained performance. We incorporate this measure into a simple algorithm that allows us to search for powerful networks without any training in a matter of seconds on a single {GPU}. Code to reproduce our experiments is available at https://github.com/{BayesWatch}/nas-without-training.},
	journaltitle = {{arXiv}:2006.04647 [cs, stat]},
	author = {Mellor, Joseph and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
	urldate = {2020-07-22},
	date = {2020-06-08},
	eprinttype = {arxiv},
	eprint = {2006.04647},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pham_efficient_2018,
	title = {Efficient Neural Architecture Search via Parameter Sharing},
	url = {http://arxiv.org/abs/1802.03268},
	abstract = {We propose Efficient Neural Architecture Search ({ENAS}), a fast and inexpensive approach for automatic model design. In {ENAS}, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, {ENAS} is fast: it delivers strong empirical performances using much fewer {GPU}-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, {ENAS} discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the {CIFAR}-10 dataset, {ENAS} designs novel architectures that achieve a test error of 2.89\%, which is on par with {NASNet} (Zoph et al., 2018), whose test error is 2.65\%.},
	journaltitle = {{arXiv}:1802.03268 [cs, stat]},
	author = {Pham, Hieu and Guan, Melody Y. and Zoph, Barret and Le, Quoc V. and Dean, Jeff},
	urldate = {2020-08-25},
	date = {2018-02-11},
	eprinttype = {arxiv},
	eprint = {1802.03268},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{awad_differential_2020,
	title = {Differential Evolution for Neural Architecture Search},
	url = {http://arxiv.org/abs/2012.06400},
	abstract = {Neural architecture search ({NAS}) methods rely on a search strategy for deciding which architectures to evaluate next and a performance estimation strategy for assessing their performance (e.g., using full evaluations, multi-fidelity evaluations, or the one-shot model). In this paper, we focus on the search strategy. We introduce the simple yet powerful evolutionary algorithm of differential evolution to the {NAS} community. Using the simplest performance evaluation strategy of full evaluations, we comprehensively compare this search strategy to regularized evolution and Bayesian optimization and demonstrate that it yields improved and more robust results for 13 tabular {NAS} benchmarks based on {NAS}-Bench-101, {NAS}-Bench-1Shot1, {NAS}-Bench-201 and {NAS}-{HPO} bench.},
	journaltitle = {{arXiv}:2012.06400 [cs]},
	author = {Awad, Noor and Mallik, Neeratyoy and Hutter, Frank},
	urldate = {2021-01-12},
	date = {2020-12-11},
	eprinttype = {arxiv},
	eprint = {2012.06400},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{nguyen_optimal_2020,
	title = {Optimal Transport Kernels for Sequential and Parallel Neural Architecture Search},
	url = {http://arxiv.org/abs/2006.07593},
	abstract = {Neural architecture search ({NAS}) automates the design of deep neural networks. One of the main challenges in searching complex and non-continuous architectures is to compare the similarity of networks that the conventional Euclidean metric may fail to capture. Optimal transport ({OT}) is resilient to such complex structure by considering the minimal cost for transporting a network into another. However, the {OT} is generally not negative definite which may limit its ability to build the positive-definite kernels required in many kernel-dependent frameworks. Building upon tree-Wasserstein ({TW}), which is a negative definite variant of {OT}, we develop a novel discrepancy for neural architectures, and demonstrate it within a Gaussian process surrogate model for the sequential {NAS} settings. Furthermore, we derive a novel parallel {NAS}, using quality k-determinantal point process on the {GP} posterior, to select diverse and high-performing architectures from a discrete set of candidates. Empirically, we demonstrate that our {TW}-based approaches outperform other baselines in both sequential and parallel {NAS}.},
	journaltitle = {{arXiv}:2006.07593 [cs, stat]},
	author = {Nguyen, Vu and Le, Tam and Yamada, Makoto and Osborne, Michael A.},
	urldate = {2020-10-07},
	date = {2020-06-13},
	eprinttype = {arxiv},
	eprint = {2006.07593},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{williams_function_1991,
	title = {Function Optimization using Connectionist Reinforcement Learning Algorithms},
	volume = {3},
	issn = {0954-0091, 1360-0494},
	url = {https://www.tandfonline.com/doi/full/10.1080/09540099108946587},
	doi = {10.1080/09540099108946587},
	abstract = {We explore the design decisions involved in reducing neural network architecture search ({NAS}) to a reinforcement learning ({RL}) problem. We compare several reductions on the {NAS}-Bench-101 dataset, while holding the {RL} algorithm and search space constant. Based on our ﬁndings, we discuss how {NAS} differs from typical {RL} settings, and suggest guidelines for applying {RL} to {NAS} problems.},
	pages = {241--268},
	number = {3},
	journaltitle = {Connection Science},
	shortjournal = {Connection Science},
	author = {Williams, Ronald J. and Peng, Jing},
	urldate = {2021-03-13},
	date = {1991-01},
	langid = {english},
}

@article{swersky_freeze-thaw_2014,
	title = {Freeze-Thaw Bayesian Optimization},
	url = {http://arxiv.org/abs/1406.3896},
	abstract = {In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.},
	journaltitle = {{arXiv}:1406.3896 [cs, stat]},
	author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan Prescott},
	urldate = {2020-07-06},
	date = {2014-06-15},
	eprinttype = {arxiv},
	eprint = {1406.3896},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_system_2020,
	title = {A System for Massively Parallel Hyperparameter Tuning},
	url = {http://arxiv.org/abs/1810.05934},
	abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called {ASHA}, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that {ASHA} outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating {ASHA} in Determined {AI}'s end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
	journaltitle = {{arXiv}:1810.05934 [cs, stat]},
	author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
	urldate = {2020-05-30},
	date = {2020-03-15},
	eprinttype = {arxiv},
	eprint = {1810.05934},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{balaprakash_deephyper_2018,
	location = {Bengaluru, India},
	title = {{DeepHyper}: Asynchronous Hyperparameter Search for Deep Neural Networks},
	isbn = {978-1-5386-8386-6},
	url = {https://ieeexplore.ieee.org/document/8638041/},
	doi = {10.1109/HiPC.2018.00014},
	shorttitle = {{DeepHyper}},
	abstract = {Hyperparameters employed by deep learning ({DL}) methods play a substantial role in the performance and reliability of these methods in practice. Unfortunately, ﬁnding performanceoptimizing hyperparameter settings is a notoriously difﬁcult task. Hyperparameter search methods typically have limited production-strength implementations or do not target scalability within a highly parallel machine, portability across different machines, experimental comparison between different methods, and tighter integration with workﬂow systems. In this paper, we present {DeepHyper}, a Python package that provides a common interface for the implementation and study of scalable hyperparameter search methods. It adopts the Balsam workﬂow system to hide the complexities of running large numbers of hyperparameter conﬁgurations in parallel on high-performance computing ({HPC}) systems. We implement and study asynchronous modelbased search methods that consist of sampling a small number of input hyperparameter conﬁgurations and progressively ﬁtting surrogate models over the input-output space until exhausting a user-deﬁned budget of evaluations. We evaluate the efﬁcacy of these methods relative to approaches such as random search, genetic algorithms, Bayesian optimization, and hyperband on {DL} benchmarks on {CPU}- and {GPU}-based {HPC} systems.},
	eventtitle = {2018 {IEEE} 25th International Conference on High Performance Computing ({HiPC})},
	pages = {42--51},
	booktitle = {2018 {IEEE} 25th International Conference on High Performance Computing ({HiPC})},
	publisher = {{IEEE}},
	author = {Balaprakash, Prasanna and Salim, Michael and Uram, Thomas D. and Vishwanath, Venkat and Wild, Stefan M.},
	urldate = {2020-05-30},
	date = {2018-12},
	langid = {english},
}

@article{hutter_efficient_nodate,
	title = {An Efficient Approach for Assessing Hyperparameter Importance},
	abstract = {The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efﬁcient methods that can be used to gain such insight, leveraging random forest models ﬁt on the data already gathered by Bayesian optimization. We ﬁrst introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional {ANOVA} framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that—even in very highdimensional cases—most performance variation is attributable to just a few hyperparameters.},
	pages = {9},
	author = {Hutter, Frank and Hoos, Holger and Leyton-Brown, Kevin},
	langid = {english},
}

@article{escalante_particle_nodate,
	title = {Particle Swarm Model Selection},
	abstract = {This paper proposes the application of particle swarm optimization ({PSO}) to the problem of full model selection, {FMS}, for classiﬁcation tasks. {FMS} is deﬁned as follows: given a pool of preprocessing methods, feature selection and learning algorithms, to select the combination of these that obtains the lowest classiﬁcation error for a given data set; the task also includes the selection of hyperparameters for the considered methods. This problem generates a vast search space to be explored, well suited for stochastic optimization techniques. {FMS} can be applied to any classiﬁcation domain as it does not require domain knowledge. Different model types and a variety of algorithms can be considered under this formulation. Furthermore, competitive yet simple models can be obtained with {FMS}. We adopt {PSO} for the search because of its proven performance in different problems and because of its simplicity, since neither expensive computations nor complicated operations are needed. Interestingly, the way the search is guided allows {PSO} to avoid overﬁtting to some extend. Experimental results on benchmark data sets give evidence that the proposed approach is very effective, despite its simplicity. Furthermore, results obtained in the framework of a model selection challenge show the competitiveness of the models selected with {PSO}, compared to models selected with other techniques that focus on a single algorithm and that use domain knowledge.},
	pages = {36},
	author = {Escalante, Hugo Jair and Montes, Manuel and Sucar, Luis Enrique and Mx, Inaoep and Mx, Inaoep},
	langid = {english},
}

@article{karnin_almost_nodate,
	title = {Almost Optimal Exploration in Multi-Armed Bandits},
	abstract = {We study the problem of exploration in stochastic Multi-Armed Bandits. Even in the simplest setting of identifying the best arm, there remains a logarithmic multiplicative gap between the known lower and upper bounds for the number of arm pulls required for the task. This extra logarithmic factor is quite meaningful in nowadays large-scale applications. We present two novel, parameterfree algorithms for identifying the best arm, in two diﬀerent settings: given a target conﬁdence and given a target budget of arm pulls, for which we prove upper bounds whose gap from the lower bound is only doublylogarithmic in the problem parameters. We corroborate our theoretical results with experiments demonstrating that our algorithm outperforms the state-of-the-art and scales better as the size of the problem increases.},
	pages = {9},
	author = {Karnin, Zohar and Koren, Tomer and Somekh, Oren},
	langid = {english},
}

@article{yu_hyper-parameter_2020,
	title = {Hyper-Parameter Optimization: A Review of Algorithms and Applications},
	url = {http://arxiv.org/abs/2003.05689},
	shorttitle = {Hyper-Parameter Optimization},
	abstract = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization ({HPO}) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on {HPO}. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for {HPO}, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when {HPO} is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
	journaltitle = {{arXiv}:2003.05689 [cs, stat]},
	author = {Yu, Tong and Zhu, Hong},
	urldate = {2020-07-28},
	date = {2020-03-12},
	eprinttype = {arxiv},
	eprint = {2003.05689},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{snoek_practical_nodate,
	title = {Practical Bayesian Optimization of Machine Learning Algorithms},
	abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a “black art” requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process ({GP}). We show that certain choices for the nature of the {GP}, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured {SVMs} and convolutional neural networks.},
	pages = {9},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	langid = {english},
}

@article{tran_scalable3-bo_2021,
	title = {Scalable3-{BO}: Big Data meets {HPC} - A scalable asynchronous parallel high-dimensional Bayesian optimization framework on supercomputers},
	url = {http://arxiv.org/abs/2108.05969},
	shorttitle = {Scalable3-{BO}},
	abstract = {Bayesian optimization ({BO}) is a flexible and powerful framework that is suitable for computationally expensive simulation-based applications and guarantees statistical convergence to the global optimum. While remaining as one of the most popular optimization methods, its capability is hindered by the size of data, the dimensionality of the considered problem, and the nature of sequential optimization. These scalability issues are intertwined with each other and must be tackled simultaneously. In this work, we propose the Scalable\${\textasciicircum}3\$-{BO} framework, which employs sparse {GP} as the underlying surrogate model to scope with Big Data and is equipped with a random embedding to efficiently optimize high-dimensional problems with low effective dimensionality. The Scalable\${\textasciicircum}3\$-{BO} framework is further leveraged with asynchronous parallelization feature, which fully exploits the computational resource on {HPC} within a computational budget. As a result, the proposed Scalable\${\textasciicircum}3\$-{BO} framework is scalable in three independent perspectives: with respect to data size, dimensionality, and computational resource on {HPC}. The goal of this work is to push the frontiers of {BO} beyond its well-known scalability issues and minimize the wall-clock waiting time for optimizing high-dimensional computationally expensive applications. We demonstrate the capability of Scalable\${\textasciicircum}3\$-{BO} with 1 million data points, 10,000-dimensional problems, with 20 concurrent workers in an {HPC} environment.},
	journaltitle = {{arXiv}:2108.05969 [cs, math, stat]},
	author = {Tran, Anh},
	urldate = {2022-03-25},
	date = {2021-08-12},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2108.05969},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{degrave_magnetic_2022,
	title = {Magnetic control of tokamak plasmas through deep reinforcement learning},
	volume = {602},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-04301-9},
	doi = {10.1038/s41586-021-04301-9},
	abstract = {Abstract
            
              Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak à Configuration Variable
              1,2
              , including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and ‘snowflake’ configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained ‘droplets’ on {TCV}, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
	pages = {414--419},
	number = {7897},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de las Casas, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
	urldate = {2022-03-23},
	date = {2022-02-17},
	langid = {english},
}

@article{kates-harbeck_predicting_2019,
	title = {Predicting disruptive instabilities in controlled fusion plasmas through deep learning},
	volume = {568},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1116-4},
	doi = {10.1038/s41586-019-1116-4},
	pages = {526--531},
	number = {7753},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Kates-Harbeck, Julian and Svyatkovskiy, Alexey and Tang, William},
	urldate = {2022-03-23},
	date = {2019-04},
	langid = {english},
}

@article{snoek_scalable_2015,
	title = {Scalable Bayesian Optimization Using Deep Neural Networks},
	url = {http://arxiv.org/abs/1502.05700},
	abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions deﬁned by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically ﬁt using Gaussian processes ({GPs}). However, since {GPs} scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.},
	journaltitle = {{arXiv}:1502.05700 [stat]},
	author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md Mostofa Ali and Prabhat and Adams, Ryan P.},
	urldate = {2022-03-21},
	date = {2015-07-13},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1502.05700},
	keywords = {Statistics - Machine Learning},
}

@article{suzuki_multi-objective_nodate,
	title = {Multi-objective Bayesian Optimization using Pareto-frontier Entropy},
	abstract = {This paper studies an entropy-based multiobjective Bayesian optimization ({MBO}). Existing entropy-based {MBO} methods need complicated approximations to evaluate entropy or employ over-simpliﬁcation that ignores trade-off among objectives. We propose a novel entropybased {MBO} called Pareto-frontier entropy search ({PFES}), which is based on the information gain of Pareto-frontier. We show that our entropy evaluation can be reduced to a closed form whose computation is quite simple while capturing the trade-off relation in Pareto-frontier. We further propose an extension for the “decoupled” setting, in which each objective function can be observed separately, and show that the {PFES}-based approach derives a natural extension of the original acquisition function which can also be evaluated simply. Our numerical experiments show effectiveness of {PFES} through several benchmark datasets, and real-word datasets from materials science.},
	pages = {10},
	author = {Suzuki, Shinya and Takeno, Shion and Tamura, Tomoyuki and Shitara, Kazuki and Karasuyama, Masayuki},
	langid = {english},
}

@article{saleiro_aequitas_2019,
	title = {Aequitas: A Bias and Fairness Audit Toolkit},
	url = {http://arxiv.org/abs/1811.05577},
	shorttitle = {Aequitas},
	abstract = {Recent work has raised concerns on the risk of unintended bias in {AI} systems being used nowadays that can aﬀect individuals unfairly based on race, gender or religion, among other possible characteristics. While a lot of bias metrics and fairness deﬁnitions have been proposed in recent years, there is no consensus on which metric/deﬁnition should be used and there are very few available resources to operationalize them. Therefore, despite recent awareness, auditing for bias and fairness when developing and deploying {AI} systems is not yet a standard practice. We present Aequitas, an open source bias and fairness audit toolkit that was released in 2018 and it is an intuitive and easy to use addition to the machine learning workﬂow, enabling users to seamlessly test models for several bias and fairness metrics in relation to multiple population sub-groups. Aequitas facilitates informed and equitable decisions around developing and deploying algorithmic decision making systems for both data scientists, machine learning researchers and policymakers.},
	journaltitle = {{arXiv}:1811.05577 [cs]},
	author = {Saleiro, Pedro and Kuester, Benedict and Hinkson, Loren and London, Jesse and Stevens, Abby and Anisfeld, Ari and Rodolfa, Kit T. and Ghani, Rayid},
	urldate = {2022-03-01},
	date = {2019-04-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1811.05577},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{shouyong_jiang_use_2016,
	location = {Athens, Greece},
	title = {On the use of hypervolume for diversity measurement of Pareto front approximations},
	isbn = {978-1-5090-4240-1},
	url = {http://ieeexplore.ieee.org/document/7850225/},
	doi = {10.1109/SSCI.2016.7850225},
	abstract = {In multiobjective optimization, a good quality indicator is of great importance to the performance assessment of algorithms. This paper investigates the effectiveness of the widelyused hypervolume indicator, which is the only one found so far to strictly comply with the Pareto dominance. While hypervolume is of undisputed success to assess the quality of an approximation, it is sensitive to misleading cases, particularly for diversity assessment. To address this issue, this paper presents a modiﬁed hypervolume indicator based on linear projection for diversity evaluation. In addition to experimental studies to demonstrate the effectiveness of the proposed indicator, the indicator is introduced into the environmental selecction of an indicator-based multiobjective optimization evolutionary algorithm. Experiments show that the proposed indicator yields more evenly-distributed approximations than the original hypervolume indicator.},
	eventtitle = {2016 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
	pages = {1--8},
	booktitle = {2016 {IEEE} Symposium Series on Computational Intelligence ({SSCI})},
	publisher = {{IEEE}},
	author = {{Shouyong Jiang} and Yang, Shengxiang and Li, Miqing},
	urldate = {2022-03-01},
	date = {2016-12},
	langid = {english},
}

@article{loshchilov_cma-es_2016,
	title = {{CMA}-{ES} for Hyperparameter Optimization of Deep Neural Networks},
	url = {http://arxiv.org/abs/1604.07269},
	abstract = {Hyperparameters of deep neural networks are often optimized by grid search, random search or Bayesian optimization. As an alternative, we propose to use the Covariance Matrix Adaptation Evolution Strategy ({CMA}-{ES}), which is known for its state-of-the-art performance in derivative-free optimization. {CMA}-{ES} has some useful invariance properties and is friendly to parallel evaluations of solutions. We provide a toy example comparing {CMA}-{ES} and state-of-the-art Bayesian optimization algorithms for tuning the hyperparameters of a convolutional neural network for the {MNIST} dataset on 30 {GPUs} in parallel.},
	journaltitle = {{arXiv}:1604.07269 [cs]},
	author = {Loshchilov, Ilya and Hutter, Frank},
	urldate = {2022-01-31},
	date = {2016-04-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1604.07269},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{turner_bayesian_2020,
	title = {Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020},
	abstract = {This paper presents the results and insights from the black-box optimization ({BBO}) challenge at {NeurIPS} 2020 which ran from July–October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the ﬁrst black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The ﬁnal leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open source black-box optimization packages as well as random search.},
	pages = {24},
	author = {Turner, Ryan and Eriksson, David and {McCourt}, Michael and Kiili, Juha and Laaksonen, Eero and Xu, Zhen and Guyon, Isabelle},
	date = {2020},
	langid = {english},
}

@article{dodge_show_2019,
	title = {Show Your Work: Improved Reporting of Experimental Results},
	url = {http://arxiv.org/abs/1909.03004},
	shorttitle = {Show Your Work},
	abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
	journaltitle = {{arXiv}:1909.03004 [cs, stat]},
	author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
	urldate = {2022-01-31},
	date = {2019-09-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.03004},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@article{garcia-barcos_fully_2019,
	title = {Fully Distributed Bayesian Optimization with Stochastic Policies},
	url = {http://arxiv.org/abs/1902.09992},
	abstract = {Bayesian optimization has become a popular method for high-throughput computing, like the design of computer experiments or hyperparameter tuning of expensive models, where sample efﬁciency is mandatory. In these applications, distributed and scalable architectures are a necessity. However, Bayesian optimization is mostly sequential. Even parallel variants require certain computations between samples, limiting the parallelization bandwidth. Thompson sampling has been previously applied for distributed Bayesian optimization. But, when compared with other acquisition functions in the sequential setting, Thompson sampling is known to perform suboptimally. In this paper, we present a new method for fully distributed Bayesian optimization, which can be combined with any acquisition function. Our approach considers Bayesian optimization as a partially observable Markov decision process. In this context, stochastic policies, such as the Boltzmann policy, have some interesting properties which can also be studied for Bayesian optimization. Furthermore, the Boltzmann policy trivially allows a distributed Bayesian optimization implementation with high level of parallelism and scalability. We present results in several benchmarks and applications that show the performance of our method.},
	journaltitle = {{arXiv}:1902.09992 [cs, stat]},
	author = {Garcia-Barcos, Javier and Martinez-Cantin, Ruben},
	urldate = {2022-01-20},
	date = {2019-07-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.09992},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_batched_2018,
	title = {Batched Large-scale Bayesian Optimization in High-dimensional Spaces},
	url = {http://arxiv.org/abs/1706.01445},
	abstract = {Bayesian optimization ({BO}) has become an effective approach for black-box function optimization problems when function evaluations are expensive and the optimum can be achieved within a relatively small number of queries. However, many cases, such as the ones with high-dimensional inputs, may require a much larger number of observations for optimization. Despite an abundance of observations thanks to parallel experiments, current {BO} techniques have been limited to merely a few thousand observations. In this paper, we propose ensemble Bayesian optimization ({EBO}) to address three current challenges in {BO} simultaneously: (1) large-scale observations; (2) high dimensional input spaces; and (3) selections of batch queries that balance quality and diversity. The key idea of {EBO} is to operate on an ensemble of additive Gaussian process models, each of which possesses a randomized strategy to divide and conquer. We show unprecedented, previously impossible results of scaling up {BO} to tens of thousands of observations within minutes of computation.},
	journaltitle = {{arXiv}:1706.01445 [cs, math, stat]},
	author = {Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
	urldate = {2022-01-20},
	date = {2018-05-15},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1706.01445},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{rasp_data-driven_2021,
	title = {Data-driven medium-range weather prediction with a Resnet pretrained on climate simulations: A new model for {WeatherBench}},
	volume = {13},
	issn = {1942-2466, 1942-2466},
	url = {http://arxiv.org/abs/2008.08626},
	doi = {10.1029/2020MS002405},
	shorttitle = {Data-driven medium-range weather prediction with a Resnet pretrained on climate simulations},
	abstract = {Numerical weather prediction has traditionally been based on physical models of the atmosphere. Recently, however, the rise of deep learning has created increased interest in purely data-driven medium-range weather forecasting with ﬁrst studies exploring the feasibility of such an approach. To accelerate progress in this area, the {WeatherBench} benchmark challenge was deﬁned. Here, we train a deep residual convolutional neural network (Resnet) to predict geopotential, temperature and precipitation at 5.625 degree resolution up to 5 days ahead. To avoid overﬁtting and improve forecast skill, we pretrain the model using historical climate model output before ﬁne-tuning on reanalysis data. The resulting forecasts outperform previous submissions to {WeatherBench} and are comparable in skill to a physical baseline at similar resolution. We also analyze how the neural network creates its predictions and ﬁnd that, with some exceptions, it is compatible with physical reasoning. Finally, we perform scaling experiments to estimate the potential skill of data-driven approaches at higher resolutions.},
	number = {2},
	journaltitle = {Journal of Advances in Modeling Earth Systems},
	shortjournal = {J Adv Model Earth Syst},
	author = {Rasp, Stephan and Thuerey, Nils},
	urldate = {2022-01-14},
	date = {2021-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2008.08626},
	keywords = {Physics - Atmospheric and Oceanic Physics},
}

@article{rasp_weatherbench_2020,
	title = {{WeatherBench}: A benchmark dataset for data-driven weather forecasting},
	volume = {12},
	issn = {1942-2466, 1942-2466},
	url = {http://arxiv.org/abs/2002.00469},
	doi = {10.1029/2020MS002203},
	shorttitle = {{WeatherBench}},
	abstract = {Data-driven approaches, most prominently deep learning, have become powerful tools for prediction in many domains. A natural question to ask is whether data-driven methods could also be used to predict global weather patterns days in advance. First studies show promise but the lack of a common dataset and evaluation metrics make inter-comparison between studies difﬁcult. Here we present a benchmark dataset for data-driven medium-range weather forecasting, a topic of high scientiﬁc interest for atmospheric and computer scientists alike. We provide data derived from the {ERA}5 archive that has been processed to facilitate the use in machine learning models. We propose simple and clear evaluation metrics which will enable a direct comparison between different methods. Further, we provide baseline scores from simple linear regression techniques, deep learning models, as well as purely physical forecasting models. The dataset is publicly available at https://github.com/pangeo-data/{WeatherBench} and the companion code is reproducible with tutorials for getting started. We hope that this dataset will accelerate research in data-driven weather forecasting.},
	number = {11},
	journaltitle = {Journal of Advances in Modeling Earth Systems},
	shortjournal = {J. Adv. Model. Earth Syst.},
	author = {Rasp, Stephan and Dueben, Peter D. and Scher, Sebastian and Weyn, Jonathan A. and Mouatadid, Soukayna and Thuerey, Nils},
	urldate = {2022-01-12},
	date = {2020-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2002.00469},
	keywords = {Physics - Atmospheric and Oceanic Physics, Statistics - Machine Learning},
}

@article{bergstra_hyperopt_2015,
	title = {Hyperopt: a Python library for model selection and hyperparameter optimization},
	volume = {8},
	issn = {1749-4699},
	url = {https://iopscience.iop.org/article/10.1088/1749-4699/8/1/014008},
	doi = {10.1088/1749-4699/8/1/014008},
	shorttitle = {Hyperopt},
	abstract = {Sequential model-based optimization (also known as Bayesian optimization) is one of the most efﬁcient methods (per function evaluation) of function minimization. This efﬁciency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. This paper also gives an overview of Hyperopt-Sklearn, a software project that provides automatic algorithm conﬁguration of the Scikit-learn machine learning library. Following Auto-Weka, we take the view that the choice of classiﬁer and even the choice of preprocessing module can be taken together to represent a single large hyperparameter optimization problem. We use Hyperopt to deﬁne a search space that encompasses many standard components (e.g. {SVM}, {RF}, {KNN}, {PCA}, {TFIDF}) and common patterns of composing them together. We demonstrate, using search algorithms in Hyperopt and standard benchmarking data sets ({MNIST}, 20-newsgroups, convex shapes), that searching this space is practical and effective. In particular, we improve on best-known scores for the model space for both {MNIST} and convex shapes. The paper closes with some discussion of ongoing and future work.},
	pages = {014008},
	number = {1},
	journaltitle = {Computational Science \& Discovery},
	shortjournal = {Comput. Sci. Disc.},
	author = {Bergstra, James and Komer, Brent and Eliasmith, Chris and Yamins, Dan and Cox, David D},
	urldate = {2022-01-12},
	date = {2015-07-28},
	langid = {english},
}

@article{rahimi_random_nodate,
	title = {Random Features for Large-Scale Kernel Machines},
	abstract = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user speciﬁed shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classiﬁcation and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.},
	pages = {8},
	author = {Rahimi, Ali and Recht, Ben},
	langid = {english},
}

@article{hutchinson_towards_2021,
	title = {Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure},
	url = {http://arxiv.org/abs/2010.13561},
	shorttitle = {Towards Accountability for Machine Learning Datasets},
	abstract = {Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decisionmaking, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.},
	journaltitle = {{arXiv}:2010.13561 [cs]},
	author = {Hutchinson, Ben and Smart, Andrew and Hanna, Alex and Denton, Emily and Greer, Christina and Kjartansson, Oddur and Barnes, Parker and Mitchell, Margaret},
	urldate = {2021-12-20},
	date = {2021-01-29},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2010.13561},
	keywords = {Computer Science - Computers and Society, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@article{groger_there_2021,
	title = {There is no {AI} without data},
	volume = {64},
	issn = {0001-0782, 1557-7317},
	url = {https://dl.acm.org/doi/10.1145/3448247},
	doi = {10.1145/3448247},
	abstract = {Industry experiences on the data challenges of {AI} and the call for a data ecosystem for industrial enterprises.},
	pages = {98--108},
	number = {11},
	journaltitle = {Communications of the {ACM}},
	shortjournal = {Commun. {ACM}},
	author = {Gröger, Christoph},
	urldate = {2021-12-20},
	date = {2021-11},
	langid = {english},
}

@article{dumont_hyppo_2021,
	title = {{HYPPO}: A Surrogate-Based Multi-Level Parallelism Tool for Hyperparameter Optimization},
	url = {http://arxiv.org/abs/2110.01698},
	shorttitle = {{HYPPO}},
	abstract = {We present a new software, {HYPPO}, that enables the automatic tuning of hyperparameters of various deep learning ({DL}) models. Unlike other hyperparameter optimization ({HPO}) methods, {HYPPO} uses adaptive surrogate models and directly accounts for uncertainty in model predictions to ﬁnd accurate and reliable models that make robust predictions. Using asynchronous nested parallelism, we are able to signiﬁcantly alleviate the computational burden of training complex architectures and quantifying the uncertainty. {HYPPO} is implemented in Python and can be used with both {TensorFlow} and {PyTorch} libraries. We demonstrate various software features on time-series prediction and image classiﬁcation problems as well as a scientiﬁc application in computed tomography image reconstruction. Finally, we show that (1) we can reduce by an order of magnitude the number of evaluations necessary to ﬁnd the most optimal region in the hyperparameter space and (2) we can reduce by two orders of magnitude the throughput for such {HPO} process to complete.},
	journaltitle = {{arXiv}:2110.01698 [cs]},
	author = {Dumont, Vincent and Garner, Casey and Trivedi, Anuradha and Jones, Chelsea and Ganapati, Vidya and Mueller, Juliane and Perciano, Talita and Kiran, Mariam and Day, Marc},
	urldate = {2021-11-10},
	date = {2021-10-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.01698},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Machine Learning},
}

@article{daulton_differentiable_2020,
	title = {Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization},
	url = {http://arxiv.org/abs/2006.05078},
	abstract = {In many real-world scenarios, decision makers seek to efﬁciently optimize multiple competing objectives in a sample-efﬁcient fashion. Multi-objective Bayesian optimization ({BO}) is a common approach, but many of the best-performing acquisition functions do not have known analytic gradients and suffer from high computational overhead. We leverage recent advances in programming models and hardware acceleration for multi-objective {BO} using Expected Hypervolume Improvement ({EHVI})—an algorithm notorious for its high computational complexity. We derive a novel formulation of q-Expected Hypervolume Improvement ({qEHVI}), an acquisition function that extends {EHVI} to the parallel, constrained evaluation setting. {qEHVI} is an exact computation of the joint {EHVI} of q new candidate points (up to Monte-Carlo ({MC}) integration error). Whereas previous {EHVI} formulations rely on gradient-free acquisition optimization or approximated gradients, we compute exact gradients of the {MC} estimator via auto-differentiation, thereby enabling efﬁcient and effective optimization using ﬁrst-order and quasi-second-order methods. Our empirical evaluation demonstrates that {qEHVI} is computationally tractable in many practical scenarios and outperforms state-of-the-art multi-objective {BO} algorithms at a fraction of their wall time.},
	journaltitle = {{arXiv}:2006.05078 [cs, math, stat]},
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	urldate = {2021-11-10},
	date = {2020-10-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2006.05078},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{gupta_generative_2018,
	title = {Generative Recurrent Networks for \textit{De Novo} Drug Design},
	volume = {37},
	issn = {18681743},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/minf.201700111},
	doi = {10.1002/minf.201700111},
	pages = {1700111},
	number = {1},
	journaltitle = {Molecular Informatics},
	shortjournal = {Mol. Inf.},
	author = {Gupta, Anvita and Müller, Alex T. and Huisman, Berend J. H. and Fuchs, Jens A. and Schneider, Petra and Schneider, Gisbert},
	urldate = {2021-11-08},
	date = {2018-01},
	langid = {english},
}

@article{adam_machine_2020,
	title = {Machine learning approaches to drug response prediction: challenges and recent progress},
	volume = {4},
	issn = {2397-768X},
	url = {http://www.nature.com/articles/s41698-020-0122-1},
	doi = {10.1038/s41698-020-0122-1},
	shorttitle = {Machine learning approaches to drug response prediction},
	abstract = {Abstract
            Cancer is a leading cause of death worldwide. Identifying the best treatment using computational models to personalize drug response prediction holds great promise to improve patient’s chances of successful recovery. Unfortunately, the computational task of predicting drug response is very challenging, partially due to the limitations of the available data and partially due to algorithmic shortcomings. The recent advances in deep learning may open a new chapter in the search for computational drug response prediction models and ultimately result in more accurate tools for therapy response. This review provides an overview of the computational challenges and advances in drug response prediction, and focuses on comparing the machine learning techniques to be of utmost practical use for clinicians and machine learning non-experts. The incorporation of new data modalities such as single-cell profiling, along with techniques that rapidly find effective drug combinations will likely be instrumental in improving cancer care.},
	pages = {19},
	number = {1},
	journaltitle = {npj Precision Oncology},
	shortjournal = {npj Precis. Onc.},
	author = {Adam, George and Rampášek, Ladislav and Safikhani, Zhaleh and Smirnov, Petr and Haibe-Kains, Benjamin and Goldenberg, Anna},
	urldate = {2021-11-08},
	date = {2020-12},
	langid = {english},
}

@article{clyde_systematic_2020,
	title = {A Systematic Approach to Featurization for Cancer Drug Sensitivity Predictions with Deep Learning},
	url = {http://arxiv.org/abs/2005.00095},
	abstract = {By combining various cancer cell line ({CCL}) drug screening panels, the size of the data has grown signiﬁcantly to begin understanding how advances in deep learning can advance drug response predictions. In this paper we train {\textgreater}35,000 neural network models, sweeping over common featurization techniques. We found the {RNA}-seq to be highly redundant and informative even with subsets larger than 128 features. We found the inclusion of single nucleotide polymorphisms ({SNPs}) coded as count matrices improved model performance signiﬁcantly, and no substantial difference in model performance with respect to molecular featurization between the common open source {MOrdred} descriptors and Dragon7 descriptors. Alongside this analysis, we outline data integration between {CCL} screening datasets and present evidence that new metrics and imbalanced data techniques, as well as advances in data standardization, need to be developed.},
	journaltitle = {{arXiv}:2005.00095 [cs, q-bio]},
	author = {Clyde, Austin and Brettin, Tom and Partin, Alexander and Shaulik, Maulik and Yoo, Hyunseung and Evrard, Yvonne and Zhu, Yitan and Xia, Fangfang and Stevens, Rick},
	urldate = {2021-11-08},
	date = {2020-05-04},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2005.00095},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Genomics, Quantitative Biology - Quantitative Methods},
}

@article{xia_predicting_2018,
	title = {Predicting tumor cell line response to drug pairs with deep learning},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2509-3},
	doi = {10.1186/s12859-018-2509-3},
	abstract = {Background: The National Cancer Institute drug pair screening effort against 60 well-characterized human tumor cell lines ({NCI}-60) presents an unprecedented resource for modeling combinational drug activity.
Results: We present a computational model for predicting cell line response to a subset of drug pairs in the {NCI}-{ALMANAC} database. Based on residual neural networks for encoding features as well as predicting tumor growth, our model explains 94\% of the response variance. While our best result is achieved with a combination of molecular feature types (gene expression, {microRNA} and proteome), we show that most of the predictive power comes from drug descriptors. To further demonstrate value in detecting anticancer therapy, we rank the drug pairs for each cell line based on model predicted combination effect and recover 80\% of the top pairs with enhanced activity.
Conclusions: We present promising results in applying deep learning to predicting combinational drug response. Our feature analysis indicates screening data involving more cell lines are needed for the models to make better use of molecular features.},
	pages = {486},
	issue = {S18},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Xia, Fangfang and Shukla, Maulik and Brettin, Thomas and Garcia-Cardona, Cristina and Cohn, Judith and Allen, Jonathan E. and Maslov, Sergei and Holbeck, Susan L. and Doroshow, James H. and Evrard, Yvonne A. and Stahlberg, Eric A. and Stevens, Rick L.},
	urldate = {2021-11-08},
	date = {2018-12},
	langid = {english},
}

@article{wozniak_candlesupervisor_2018,
	title = {{CANDLE}/Supervisor: a workflow framework for machine learning applied to cancer research},
	volume = {19},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2508-4},
	doi = {10.1186/s12859-018-2508-4},
	shorttitle = {{CANDLE}/Supervisor},
	abstract = {Background: Current multi-petaflop supercomputers are powerful systems, but present challenges when faced with problems requiring large machine learning workflows. Complex algorithms running at system scale, often with different patterns that require disparate software packages and complex data flows cause difficulties in assembling and managing large experiments on these machines.
Results: This paper presents a workflow system that makes progress on scaling machine learning ensembles, specifically in this first release, ensembles of deep neural networks that address problems in cancer research across the atomistic, molecular and population scales. The initial release of the application framework that we call {CANDLE}/Supervisor addresses the problem of hyper-parameter exploration of deep neural networks.
Conclusions: Initial results demonstrating {CANDLE} on {DOE} systems at {ORNL}, {ANL} and {NERSC} (Titan, Theta and Cori, respectively) demonstrate both scaling and multi-platform execution.},
	pages = {491},
	issue = {S18},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Wozniak, Justin M. and Jain, Rajeev and Balaprakash, Prasanna and Ozik, Jonathan and Collier, Nicholson T. and Bauer, John and Xia, Fangfang and Brettin, Thomas and Stevens, Rick and Mohd-Yusof, Jamaludin and Cardona, Cristina Garcia and Essen, Brian Van and Baughman, Matthew},
	urldate = {2021-11-08},
	date = {2018-12},
	langid = {english},
}

@article{yoon_data_2019,
	title = {Data Valuation using Reinforcement Learning},
	url = {http://arxiv.org/abs/1909.11671},
	abstract = {Quantifying the value of data is a fundamental problem in machine learning. Data valuation has multiple important use cases: (1) building insights about the learning task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. To adaptively learn data values jointly with the target task predictor model, we propose a meta learning framework which we name Data Valuation using Reinforcement Learning ({DVRL}). We employ a data value estimator (modeled by a deep neural network) to learn how likely each datum is used in training of the predictor model. We train the data value estimator using a reinforcement signal of the reward obtained on a small validation set that reﬂects performance on the target task. We demonstrate that {DVRL} yields superior data value estimates compared to alternative methods across different types of datasets and in a diverse set of application scenarios. The corrupted sample discovery performance of {DVRL} is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning {DVRL} signiﬁcantly outperforms state-of-the-art by 14.6\% and 10.8\%, respectively.},
	journaltitle = {{arXiv}:1909.11671 [cs, stat]},
	author = {Yoon, Jinsung and Arik, Sercan O. and Pfister, Tomas},
	urldate = {2021-11-02},
	date = {2019-09-25},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1909.11671},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{motamedi_data-centric_2021,
	title = {A Data-Centric Approach for Training Deep Neural Networks with Less Data},
	url = {http://arxiv.org/abs/2110.03613},
	abstract = {While the availability of large datasets is perceived to be a key requirement for training deep neural networks, it is possible to train such models with relatively little data. However, compensating for the absence of large datasets demands a series of actions to enhance the quality of the existing samples and to generate new ones. This paper summarizes our winning submission to the “Data-Centric {AI}” competition. We discuss some of the challenges that arise while training with a small dataset, offer a principled approach for systematic data quality enhancement, and propose a {GAN}-based solution for synthesizing new data points. Our evaluations indicate that the dataset generated by the proposed pipeline offers 5\% accuracy improvement, while being signiﬁcantly smaller than the baseline.},
	journaltitle = {{arXiv}:2110.03613 [cs]},
	author = {Motamedi, Mohammad and Sakharnykh, Nikolay and Kaldewey, Tim},
	urldate = {2021-11-01},
	date = {2021-10-07},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.03613},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{hataya_faster_2019,
	title = {Faster {AutoAugment}: Learning Augmentation Strategies using Backpropagation},
	url = {http://arxiv.org/abs/1911.06987},
	shorttitle = {Faster {AutoAugment}},
	abstract = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster {AutoAugment}, achieves signiﬁcantly faster searching than prior work without a performance drop.},
	journaltitle = {{arXiv}:1911.06987 [cs]},
	author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
	urldate = {2021-11-01},
	date = {2019-11-16},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1911.06987},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{bach_learning_2017,
	title = {Learning the Structure of Generative Models without Labeled Data},
	url = {http://arxiv.org/abs/1703.00854},
	abstract = {Curating labeled training data has become the primary bottleneck in machine learning. Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources. The generative model’s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge. We propose a structure estimation method that maximizes the 1regularized marginal pseudolikelihood of the observed data. Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models. Simulations show that our method is 100× faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies. We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as {PubMed} journal abstracts.},
	journaltitle = {{arXiv}:1703.00854 [cs, stat]},
	author = {Bach, Stephen H. and He, Bryan and Ratner, Alexander and Ré, Christopher},
	urldate = {2021-10-28},
	date = {2017-09-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1703.00854},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ratner_snorkel_2017,
	title = {Snorkel: Rapid Training Data Creation with Weak Supervision},
	volume = {11},
	issn = {2150-8097},
	url = {http://arxiv.org/abs/1711.10160},
	doi = {10.14778/3157794.3157797},
	shorttitle = {Snorkel},
	abstract = {Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a ﬁrst-of-its-kind system that enables users to train stateof-the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the ﬁrst end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a ﬂexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8× faster and increase predictive performance an average 45.5\% versus seven hours of hand labeling. We study the modeling tradeoﬀs in this new setting and propose an optimizer for automating tradeoﬀ decisions that gives up to 1.8× speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Aﬀairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132\% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60\% of the predictive performance of large hand-curated training sets.},
	pages = {269--282},
	number = {3},
	journaltitle = {Proceedings of the {VLDB} Endowment},
	shortjournal = {Proc. {VLDB} Endow.},
	author = {Ratner, Alexander and Bach, Stephen H. and Ehrenberg, Henry and Fries, Jason and Wu, Sen and Ré, Christopher},
	urldate = {2021-10-28},
	date = {2017-11},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.10160},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rekatsinas_holoclean_2017,
	title = {{HoloClean}: Holistic Data Repairs with Probabilistic Inference},
	url = {http://arxiv.org/abs/1702.00820},
	shorttitle = {{HoloClean}},
	abstract = {We introduce {HoloClean}, a framework for holistic data repairing driven by probabilistic inference. {HoloClean} uniﬁes existing qualitative data repairing approaches, which rely on integrity constraints or external data sources, with quantitative data repairing methods, which leverage statistical properties of the input data. Given an inconsistent dataset as input, {HoloClean} automatically generates a probabilistic program that performs data repairing. Inspired by recent theoretical advances in probabilistic inference, we introduce a series of optimizations which ensure that inference over {HoloClean}’s probabilistic model scales to instances with millions of tuples. We show that {HoloClean} scales to instances with millions of tuples and ﬁnd data repairs with an average precision of ∼ 90\% and an average recall of above ∼ 76\% across a diverse array of datasets exhibiting different types of errors. This yields an average F1 improvement of more than 2× against state-of-the-art methods.},
	journaltitle = {{arXiv}:1702.00820 [cs]},
	author = {Rekatsinas, Theodoros and Chu, Xu and Ilyas, Ihab F. and Ré, Christopher},
	urldate = {2021-10-26},
	date = {2017-02-02},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1702.00820},
	keywords = {Computer Science - Databases},
}

@article{hirschfeld_uncertainty_2020,
	title = {Uncertainty Quantification Using Neural Networks for Molecular Property Prediction},
	volume = {60},
	issn = {1549-9596, 1549-960X},
	url = {https://pubs.acs.org/doi/10.1021/acs.jcim.0c00502},
	doi = {10.1021/acs.jcim.0c00502},
	abstract = {Uncertainty quantiﬁcation ({UQ}) is an important component of molecular property prediction, particularly for drug discovery applications where model predictions direct experimental design and where unanticipated imprecision wastes valuable time and resources. The need for {UQ} is especially acute for neural models, which are becoming increasingly standard yet are challenging to interpret. While several approaches to {UQ} have been proposed in the literature, there is no clear consensus on the comparative performance of these models. In this paper, we study this question in the context of regression tasks. We systematically evaluate several methods on ﬁve regression data sets using multiple complementary performance metrics. Our experiments show that none of the methods we tested is unequivocally superior to all others, and none produces a particularly reliable ranking of errors across multiple data sets. While we believe that these results show that existing {UQ} methods are not suﬃcient for all common use cases and further research is needed, we conclude with a practical recommendation as to which existing techniques seem to perform well relative to others.},
	pages = {3770--3780},
	number = {8},
	journaltitle = {Journal of Chemical Information and Modeling},
	shortjournal = {J. Chem. Inf. Model.},
	author = {Hirschfeld, Lior and Swanson, Kyle and Yang, Kevin and Barzilay, Regina and Coley, Connor W.},
	urldate = {2021-10-25},
	date = {2020-08-24},
	langid = {english},
}

@inproceedings{li_cleanml_2021,
	location = {Chania, Greece},
	title = {{CleanML}: A Study for Evaluating the Impact of Data Cleaning on {ML} Classification Tasks},
	isbn = {978-1-72819-184-3},
	url = {https://ieeexplore.ieee.org/document/9458702/},
	doi = {10.1109/ICDE51399.2021.00009},
	shorttitle = {{CleanML}},
	abstract = {Data quality affects machine learning ({ML}) model performances, and data scientists spend considerable amount of time on data cleaning before model training. However, to date, there does not exist a rigorous study on how exactly cleaning affects {ML} — {ML} community usually focuses on developing {ML} algorithms that are robust to some particular noise types of certain distributions, while database ({DB}) community has been mostly studying the problem of data cleaning alone without considering how data is consumed by downstream {ML} analytics. We propose a {CleanML} study that systematically investigates the impact of data cleaning on {ML} classiﬁcation tasks. The opensource and extensible {CleanML} study currently includes 14 realworld datasets with real errors, ﬁve common error types, seven different {ML} models, and multiple cleaning algorithms for each error type (including both commonly used algorithms in practice as well as state-of-the-art solutions in academic literature). We control the randomness in {ML} experiments using statistical hypothesis testing, and we also control false discovery rate in our experiments using the Benjamini-Yekutieli ({BY}) procedure. We analyze the results in a systematic way to derive many interesting and nontrivial observations. We also put forward multiple research directions for researchers.},
	eventtitle = {2021 {IEEE} 37th International Conference on Data Engineering ({ICDE})},
	pages = {13--24},
	booktitle = {2021 {IEEE} 37th International Conference on Data Engineering ({ICDE})},
	publisher = {{IEEE}},
	author = {Li, Peng and Rao, Xi and Blase, Jennifer and Zhang, Yue and Chu, Xu and Zhang, Ce},
	urldate = {2021-10-25},
	date = {2021-04},
	langid = {english},
}

@article{jumper_highly_2021,
	title = {Highly accurate protein structure prediction with {AlphaFold}},
	volume = {596},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-021-03819-2},
	doi = {10.1038/s41586-021-03819-2},
	abstract = {Abstract
            
              Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort
              1–4
              , the structures of around 100,000 unique proteins have been determined
              5
              , but this represents a small fraction of the billions of known protein sequences
              6,7
              . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’
              8
              —has been an important open research problem for more than 50 years
              9
              . Despite recent progress
              10–14
              , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, {AlphaFold}, in the challenging 14th Critical Assessment of protein Structure Prediction ({CASP}14)
              15
              , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of {AlphaFold} is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.},
	pages = {583--589},
	number = {7873},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and Žídek, Augustin and Potapenko, Anna and Bridgland, Alex and Meyer, Clemens and Kohl, Simon A. A. and Ballard, Andrew J. and Cowie, Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig and Reiman, David and Clancy, Ellen and Zielinski, Michal and Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol and Senior, Andrew W. and Kavukcuoglu, Koray and Kohli, Pushmeet and Hassabis, Demis},
	urldate = {2021-10-20},
	date = {2021-08-26},
	langid = {english},
}

@report{blaiszik_delta_2021,
	title = {The Delta Variant Had Negligible Impact on {COVID}-19 Vaccine Effectiveness in the {USA}},
	url = {http://medrxiv.org/lookup/doi/10.1101/2021.09.18.21263783},
	abstract = {We estimated local {COVID}-19 vaccine effectiveness using {RT}-{PCR} {COVID}-19 test data broken out by vaccination status from select localities in the U.S.A. between 15 May and 15 September 2021 while the {SARS}-{CoV}-2 Delta variant (B.1.617.2) was ascending from essentially zero prevalence to total dominance of the genome, and showed that the rise of the Delta variant had negligible effect on vaccine effectiveness.},
	institution = {Epidemiology},
	type = {preprint},
	author = {Blaiszik, Ben and Graziani, Carlo and Olds, James L. and Foster, Ian},
	urldate = {2021-10-20},
	date = {2021-09-22},
	langid = {english},
	doi = {10.1101/2021.09.18.21263783},
}

@article{balestriero_learning_2021,
	title = {Learning in High Dimension Always Amounts to Extrapolation},
	url = {http://arxiv.org/abs/2110.09485},
	abstract = {The notion of interpolation and extrapolation is fundamental in various ﬁelds from deep learning to function approximation. Interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset’s convex hull. Extrapolation occurs when x falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional ({\textgreater}100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation deﬁnition as an indicator of generalization performances.},
	journaltitle = {{arXiv}:2110.09485 [cs]},
	author = {Balestriero, Randall and Pesenti, Jerome and {LeCun}, Yann},
	urldate = {2021-10-20},
	date = {2021-10-18},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2110.09485},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}
