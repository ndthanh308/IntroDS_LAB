\section{Visualization of Learning Curves from HPOBench}~\label{sec:appendix-viz-learning-curves}

In this section, we display in Figure~\ref{fig:viz-learning-curves-hpobench-log-scales} the 200 randomly picked learning curves for the four benchmarks considered. It is interesting to observe these curves to notice their diversity. Also, many outliers are visible which makes regular and well behaving learning curves harder to distinguish (and impossible in a linear scale).
Then, we present 1,000 randomly selected learning curves colored by their final ranking in Figure~\ref{fig:all-ranking-curves-hpobench} and the corresponding heatmaps in Figure~\ref{fig:all-heatmap-hpobench}. In such visualization, we can identify clearly two groups among learning curves (good and bad candidates). Also we can notice that it is possible to identify good candidates only based on the first epoch.

% Figure environment removed

% Figure environment removed

% Figure environment removed

\section{Visualization of Learning Curves from LCBench}~\label{sec:appendix-viz-learning-curves-lcbench}

Similarly to what is done on HPOBench in the previous section we perform the same visualization now on LCBench~\cite{klein_tabular_2019}. We present 1,000 randomly selected learning curves colored by their final ranking in Figure~\ref{fig:all-ranking-curves-lcbench},\ref{fig:all-ranking-curves-lcbench-second-part} and the corresponding heatmaps in Figure~\ref{fig:all-heatmap-lcbench},\ref{fig:all-heatmap-lcbench-second-part}. In such visualization, we can identify clearly two groups among learning curves (good and bad candidates). Also we can notice that it is possible to identify good candidates only based on the first epoch.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


\section{Visualization of Learning Curves from YAHPO}~\label{sec:appendix-viz-learning-curves-YAHPO}

Similarly to what is done on LCBench in the previous section we perform the same visualization now on YAHPO~\cite{pfisterer2022yahpo}. We present 1,000 randomly selected learning curves, colored by their final ranking in Figure~\ref{fig:all-ranking-curves-yahpo},\ref{fig:all-ranking-curves-yahpo-second-part} and the corresponding heatmaps in Figure~\ref{fig:all-heatmap-yahpo},\ref{fig:all-heatmap-yahpo-second-part}. In such visualization, we can identify clearly two groups among learning curves (good and bad candidates). Also we can notice that it is possible to identify good candidates only based on the first epoch. From this results, it is clear that the 1-Epoch baseline would be performing similarly well on a majority of these problems. Now, LCBench only considers tabular datasets. Therefore, within YAHPO we look at NB301 based on Cifar-10 (computer vision dataset), the only scenario (other than the LCBench scenario) using epochs as the fidelity. We present the results in Fig.~\ref{fig:nb301-cifar10-yahpo}. From the heatmap, it is clear that the best configuration is already dominating bad configurations from the first epoch. Therefore, 1-Epoch would also perform well on this benchmark.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


\section{Visualization of Learning Curves from JAHS-BENCH}~\label{sec:appendix-viz-learning-curves-JAHSBENCH}

Similarly to was done on HPOBench, LCBench and YAHPO in the previous section we perform the diagnostic visualization now on JAHS-Bench-201~\cite{bansal2022jahsbench}. We present 1,000 randomly selected learning curves as done before, colored by their final ranking in Figure~\ref{fig:all-ranking-curves-jahsbench} and the corresponding heatmaps in Figure~\ref{fig:all-heatmap-jahsbench}. Same conclusions as previous benchmarks can be derived. The best curves are dominant over the red curves. This can be seen well in the top row of the heatmaps Figure~\ref{fig:all-heatmap-jahsbench} which remain blue from the start to the end.


% Figure environment removed

% Figure environment removed

\section{Experiments on HPOBench}~\label{sec:appendix-1-epoch-hpobench}

In this section, we present the four problems of the HPOBench benchmark in Figure~\ref{fig:model-based-pruning-with-random-search-on-hpobench-appendix} to complement the Naval Propulsion problem presented in the main part of our study. As it can be seen, all problems have similar outcomes.

% Figure environment removed

\section{Experiments on LCBench}~\label{sec:appendix-1-epoch-lcbench}

In this section, we compare the performance of our 1-Epoch baseline on the LCBench~\cite{klein_tabular_2019} benchmark. The experimental setting is similar to the experiments on HPOBench. A random sampling is used for the outer-loop optimization. The minimum fidelity on LCbench is 1 epoch of training, and the maximum is 50 epochs of training. The outer-loop is executed for 200 iterations. The Top-$K$ selection is run with $k=3$ for 1-Epoch. Table~\ref{tab:results-lcbench-random-search} summarizes the results of final test objective (in this case cross-entropy) as well as the final speed up in term of consumed training epochs. To be transpartent about the behaviour of the algorith we also display the search trajectories in Figure~\ref{fig:pruners-lcbench} and \ref{fig:pruners-lcbench-second-part}. As it can be observed, the test objectives found are close in general dispite a large difference in speed-up. Therefore, if the main bottleneck to execute hyperparameter optimization is compute, using 1-Epoch can be justified.


\begin{table}[!h]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{llllrr}
\toprule
               Dataset &     1-Epoch &         SHA &   50-Epochs &  S(1-Epochs) &  S(SHA) \\
\midrule
            APSFailure & 0.34 ± 0.00 & 0.33 ± 0.00 & 0.33 ± 0.00 &        28.71 &   12.29 \\
Amazon employee access & 0.59 ± 0.00 & 0.58 ± 0.00 & 0.58 ± 0.00 &        28.71 &   11.92 \\
            Australian & 0.48 ± 0.00 & 0.48 ± 0.00 & 0.47 ± 0.00 &        28.71 &   12.23 \\
         Fashion MNIST & 1.61 ± 0.01 & 1.60 ± 0.00 & 1.60 ± 0.00 &        28.71 &   11.10 \\
    KDDCup09 appetency & 0.41 ± 0.01 & 0.40 ± 0.01 & 0.37 ± 0.01 &        28.71 &   11.77 \\
             MiniBooNE & 0.44 ± 0.01 & 0.43 ± 0.00 & 0.43 ± 0.00 &        28.71 &   11.30 \\
                 adult & 0.51 ± 0.00 & 0.50 ± 0.00 & 0.50 ± 0.00 &        28.71 &   11.99 \\
              airlines & 0.66 ± 0.00 & 0.66 ± 0.00 & 0.66 ± 0.00 &        28.71 &   11.27 \\
                albert & 0.64 ± 0.00 & 0.64 ± 0.00 & 0.64 ± 0.00 &        28.71 &   11.59 \\
        bank marketing & 0.50 ± 0.00 & 0.49 ± 0.00 & 0.49 ± 0.00 &        28.71 &   13.31 \\
     blood transfusion & 0.63 ± 0.00 & 0.63 ± 0.00 & 0.63 ± 0.00 &        28.71 &   13.12 \\
                   car & 1.01 ± 0.02 & 0.90 ± 0.02 & 0.90 ± 0.01 &        28.71 &   11.40 \\
             christine & 0.59 ± 0.00 & 0.60 ± 0.00 & 0.59 ± 0.00 &        28.71 &   12.84 \\
                cnae 9 & 1.55 ± 0.02 & 1.54 ± 0.01 & 1.51 ± 0.01 &        28.71 &   11.85 \\
             connect 4 & 0.99 ± 0.03 & 0.93 ± 0.02 & 0.90 ± 0.02 &        28.71 &   11.66 \\
             covertype & 1.52 ± 0.01 & 1.51 ± 0.01 & 1.50 ± 0.01 &        28.71 &   12.13 \\
              credit g & 0.60 ± 0.00 & 0.59 ± 0.00 & 0.59 ± 0.00 &        28.71 &   12.28 \\
                dionis & 5.37 ± 0.08 & 5.24 ± 0.02 & 5.24 ± 0.02 &        28.71 &   11.06 \\
                fabert & 1.61 ± 0.01 & 1.59 ± 0.01 & 1.59 ± 0.00 &        28.71 &   12.54 \\
                helena & 4.55 ± 0.01 & 4.50 ± 0.01 & 4.50 ± 0.01 &        28.71 &   11.19 \\
                 higgs & 0.61 ± 0.01 & 0.61 ± 0.00 & 0.61 ± 0.00 &        28.71 &   11.10 \\
                jannis & 1.17 ± 0.01 & 1.14 ± 0.01 & 1.14 ± 0.01 &        28.71 &   11.53 \\
               jasmine & 0.55 ± 0.00 & 0.55 ± 0.00 & 0.54 ± 0.00 &        28.71 &   12.01 \\
          jungle chess & 0.81 ± 0.02 & 0.77 ± 0.01 & 0.77 ± 0.01 &        28.71 &   12.30 \\
                   kc1 & 0.58 ± 0.00 & 0.57 ± 0.00 & 0.56 ± 0.00 &        28.71 &   11.51 \\
              kr vs kp & 0.39 ± 0.01 & 0.36 ± 0.01 & 0.34 ± 0.00 &        28.71 &   11.96 \\
         mfeat factors & 1.54 ± 0.01 & 1.52 ± 0.00 & 1.51 ± 0.00 &        28.71 &   11.29 \\
                 nomao & 0.36 ± 0.00 & 0.36 ± 0.00 & 0.36 ± 0.00 &        28.71 &   12.33 \\
           numerai28.6 & 0.69 ± 0.00 & 0.69 ± 0.00 & 0.69 ± 0.00 &        28.71 &   12.37 \\
               phoneme & 0.56 ± 0.01 & 0.51 ± 0.01 & 0.51 ± 0.01 &        28.71 &   11.75 \\
               segment & 1.49 ± 0.02 & 1.43 ± 0.02 & 1.38 ± 0.01 &        28.71 &   11.20 \\
               shuttle & 1.22 ± 0.01 & 1.21 ± 0.01 & 1.19 ± 0.01 &        28.71 &   13.23 \\
               sylvine & 0.44 ± 0.01 & 0.40 ± 0.00 & 0.40 ± 0.00 &        28.71 &   11.64 \\
               vehicle & 1.13 ± 0.02 & 1.07 ± 0.01 & 1.05 ± 0.01 &        28.71 &   11.47 \\
               volkert & 1.99 ± 0.02 & 1.95 ± 0.01 & 1.95 ± 0.01 &        28.71 &   11.40 \\
\bottomrule
\end{tabular}
}
\caption{Table of results from the LCBench benchmark. The final test cross-entropy (mean ± standard error) are displayed as well as the speed up $S(.)$ for 1-Epoch and SHA with respect to using maximum fidelity (50 training epochs for this benchmark).}
\label{tab:results-lcbench-random-search}
\end{table}

% Figure environment removed

% Figure environment removed


\section{Details About HPOBench Search Space}

The search space for the 4 tabular benchmarks we used from HPOBench is given by~\cite{klein_tabular_2019} and detailed in table~\ref{tab:hpobench-tabular-search-space}. The backbone is a two-layers fully connected neural network with a linear output layer.

\begin{table}[!h]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameters} & \textbf{Choices}                            \\ \hline
Initial LR               & $\{0.0005, 0.001, 0.005, 0.01, 0.05, 0.1\}$ \\ \hline
Batch Size               & $\{8, 16, 32, 64\}$                         \\ \hline
LR Schedule              & $\{\text{cosine}, \text{fix}\}$             \\ \hline
Activation/Layer 1       & $\{\text{relu}, \text{tanh}\}$              \\ \hline
Activation/Layer 2       & $\{\text{relu}, \text{tanh}\}$              \\ \hline
Layer 1 Size             & $\{16, 32, 64, 128, 256, 512\}$             \\ \hline
Layer 2 Size             & $\{16, 32, 64, 128, 256, 512\}$             \\ \hline
Dropout/Layer 1          & $\{0.0, 0.3, 0.6\}$                         \\ \hline
Dropout/Layer 2          & $\{0.0, 0.3, 0.6\}$                         \\ \hline
\end{tabular}
\caption{Hyperparameter search space for the four tabular benchmarks from HPOBench.}
\label{tab:hpobench-tabular-search-space}
\end{table}


\section{Rober: Robust Bayesian Early Rejection}~\label{sec:appendix-rober}

In this section, we present shortly our improved version of learning curve extrapolation, we call it robust Bayesian early rejection (RoBER). The detailed algorithm of RoBER is provided in Algorithm~\ref{alg:rober-algorithm}. RoBER closely follows the proposed weighted probabilistic model (WPM) from~\cite{domhan_speeding_2015}. WPM consists in estimating the posterior distribution $P(\Theta|C)$ of a mixture distribution of parametric learning curves models, where $C=\{(z_1, y_1), ..., (z_i, y_i)\}$ is a partial learning curve of the observed scores and $\Theta$ is the concatenation of all the parameters (weights of the mixture and parameters of all learning curves models used in the mixture). Once this posterior distribution is known we can evaluate the probability of the score at the final training step $\hat{y}_\text{max}$ to be less than $y^*$ the best score observed so far (including all completed evaluations) which formally corresponds to $p := P(\hat{y}_\text{max} < y^* | C;\Theta)$. However, after noticing many instabilities in the optimization process of WPM, in RoBER we cut out the mixture distribution and consider only a single model. More precisely, we keep the \textit{mmf4} (Equation~\ref{eq:mmf4}) model proposed by~\cite{10.5555/645940.671380} and assessed in \cite{mohr2023lcdb} to be good for extrapolation.

\begin{equation}~\label{eq:mmf4}
   f_\text{mmf4}(z;\Theta) =  \frac{\Theta_0 \Theta_1 + \Theta_2 b(z)^{\Theta_3}}{\Theta_1 + b(z)^{\Theta_3}}
\end{equation}

Then, after observing at least four steps of the learning curve (number of parameters of the model), we estimate the model parameters $\Theta$ through non-linear least squares optimization (l. 17 in Algorithm~\ref{alg:rober-algorithm}). The continuous optimizer used is the Levenberg-Marquart algorithm~\cite{gavin2019levenberg} to which we can provide the exact Jacobian of the vector of residuals $\nabla_\Theta r(\textbf{z},\textbf{y};\Theta)$ through automatic differentiation with JAX~\cite{jax2018github} where $r$ is given by:

$$r(\textbf{z},\textbf{y};\Theta) = \textbf{y} - \mathcal{C}(\textbf{z};\Theta)$$

which combined with just-in-time compilation (JIT) allows for faster and robust estimation of the $\Theta$ parameters (previous works only used approximated gradient which makes it less stable). Now, that we have parameters correctly estimating the mean $y_i$ we are interested in incorporating uncertainty about the future and the noise between training steps. Therefore, we follow a Bayesian approach where we have $P(\Theta|C) \propto P(C|\Theta) \cdot P(\Theta)$. The prior on the parameters is given by $P(\Theta) := \mathcal{N}(\hat{\Theta}, 1)$. The likelihood is given by $P(C|\Theta) := \prod_{i=1}^n \mathcal{N}(y_i;\hat{y}_i, \sigma^2)$ where $\sigma^2 \sim \text{Exp}(1)$. Finally, the posterior distribution $P(\Theta|C)$ is not exactly computed but we can generate data (possible learning curves) from it using Monte-Carlo Markov Chain (MCMC) (line 18 in Algorithm~\ref{alg:rober-algorithm}) a common approach to generate data from an untractable densities. A candidate is then discarded only if $\tau \le p_\text{worse}$ with $\tau$ a threshold we are in practice setting quite high ($\tau=0.9$ in our experiments) to avoid discarding models with performance not certainly worse than the current best. In this sense, the method is quite conservative about model early rejecting because early steps will have high overall uncertainty.

Now that we explained our probabilistic model we give additional details on how we handle earliest steps where $z < |\Theta|$. Indeed, for these steps no least-square estimate is available has we have more degrees of freedom than the number of collected observations. We could remain in an over-parameterized regime and adopt a different optimization approach to alleviate this issue. Instead, we decide to place some safeguard to help us detect specific cases which are stagnation of the learning curve and unexpectedly bad candidates (commonly named "outliers" in data-science). The stagnation in learning curve is managed through a classic early stopping strategy (line 7 in Algorithm~\ref{alg:rober-algorithm}). Then, the outliers are detected through the simple inter-quantile range methodology, visually we look at the distribution of performances at a given step and consider them outlier when having a lower value than the lower whisker of a box-plot (lines 12-14 in Algorithm~\ref{alg:rober-algorithm}). It corresponds to discarding models with a score less than $Q_1 - 1.5 (Q_3 - Q_1)$ where $Q_1, Q_3$ are respectively the first and third quartiles of the distribution of observed scores at the given step

\begin{algorithm2e}
\small
% \DontPrintSemicolon
\SetInd{0.25em}{0.5em}
\SetAlgoLined

\SetKwInOut{Input}{Inputs}
\SetKwInOut{Output}{Output}
\SetKwProg{Fn}{Function}{ is}{end}

\SetKwFunction{evaluate}{evaluate\_with\_RoBER}
\SetKwFunction{agent}{Agent}

\SetKwFor{For}{for}{do}{end}

\Fn{\evaluate{\texttt{config},$\tau,z_\text{max}$}}{
    $C \gets [~]$ \;
    \For{$z \in [|1,z_\text{max}|]$}{
        $\texttt{score} \gets$ Execute training step for \texttt{config} \;
        Append \texttt{score} to observations $C$ \;
        ~\\
        {\color{blue}\tcc{Decide if we continue the training}}
        \uIf{$z_\text{max} \leq z$ \textbf{or} not improved for $n_\text{patience}$ steps}{
            \text{Exit the loop} \;
        }
        $y^* \gets$ Return the maximum objective among all evaluated configurations\;
        $z_\text{test} \gets \text{Return step at which objective is checked}$ \;
        \uIf{$z_\text{test} = z$}{
            $Q_1 \gets \text{25th percentile of objectives at step } z$\;
            $Q_3 \gets \text{75th percentile of objectives at step } z$\;
            \uIf{$z \leq \min(z_\text{min},n_\text{parameters})$ \textbf{and} $y_z < Q_1 - 1.5 \cdot (Q_3-Q_1)$}{
                \text{Exit the loop} \;
            }
            \uElse{
            $\hat{\Theta} \gets$ Estimate model parameters with non-linear least-squares \;
            $p \gets \text{Estimate probability of being worse } P(\hat{y}_\text{max} < y^*|C;\hat{\Theta})$ \;
            \uIf{$\tau \leq p$}{
                \text{Exit the loop} \;
                }
            }
        }
    }
    \Return \text{last element in }$C$ \;
}
    \caption{Robust Bayesian Early Rejection (RoBER)}
    \label{alg:rober-algorithm}
\end{algorithm2e}

\section{Experiments with RoBER}~\label{sec:appendix-rober-robust}

In this section, we show that RoBER is more robust than previously proposed learning curve extrapolation methods. The results are presented in Figure~\ref{fig:all-rober-vs-wpm}. It can be seen that RoBER is consistent on the four problems while WPM~\cite{domhan_speeding_2015} under-perform on two of the problems \ref{fig:rober-vs-wpm-navalpropulsion} and \ref{fig:rober-vs-wpm-slicelocalization}. In addition, experimentally we RoBER is much faster to query compared to WPM which makes it more usable.

% Figure environment removed


\section{More Details About our Bayesian Optimization Policy}
\label{sec:appendix-details-about-bo}

In our experiment, we also combined Bayesian optimization at the outer-optimization level with multi-fidelity agents at the inner-optimizer level. For this we use a Random-Forest surrogate model unlike very frequent Gaussian-Process based approaches or Tree-Parzen Estimation (TPE)~\cite{bergstra_algorithms_2011,bergstra_making_2013}. Our Random-Forest is slightly modified as suggested in~\cite{hutter2014algorithm} to use the "best random" split instead of the "best" split which improves significantly the estimation of epistemic uncertainty of the model (based on the Scikit-Learn~\cite{scikit-learn} implementation). Also, the objective is mapped to $[0,1]$ with a min-max standardization before applying a $\log(y_\text{min-max} + \epsilon)$ transformation on it. This equalize the errors on large and small objective values as the average error is being minimized by the learning algorithm. Then, we use the upper confidence-bound (UCB) acquisition function $\alpha(x) = \mu(x) + \kappa \cdot \sigma(x)$ with a cyclic exponential decay to manage the evolution of the exploration-exploitation parameter $\kappa$. This scheduler is able to repetitively perform exploration while having moments of strong exploitation (with a $kappa$ value cycling to converge to a value close to 0). We adopt this scheme after noticing the empirical efficiency of the TPE algorithm on HPO which has a strong exploitation scheme embedded in it.

\section{The Problem of Generalization in HPO}~\label{sec:appendix-generalization}

In this section, we present the difference between the evolution of the validation error used by the optimizer during hyperparameter optimization and the generalization error (real target). As it can be observed in Figure~\ref{fig:all-generalization-1epoch} looking at the evolution of the validation error can be miss-leading of the real generalization error.

% Figure environment removed

\section{Code}

The code is made publicly available with related documentation as part of an existing Hyperameter optimization Python package. Our experiments are organized as followed:
\begin{itemize}
    \item Benchmark definitions: \href{https://github.com/deephyper/benchmark}{github.com/deephyper/benchmark}
    \item Library containing the core software and algorithms: \href{https://github.com/deephyper/deephyper}{github.com/deephyper/deephyper}
    \item Organization, execution and analysis of the experiments of the paper: \href{https://github.com/deephyper/scalable-bo}{github.com/deephyper/scalable-bo}
\end{itemize}

The experiments for this paper are located in the \texttt{experiments/local/dhb/} folder of the \texttt{deephyper/scalable-bo} repository. The main entry point for the execution is \texttt{src/scalbo/scalbo/search/dbo.py}. Plots and analysis are all located in the \texttt{notebooks/learning-curves/} folder.