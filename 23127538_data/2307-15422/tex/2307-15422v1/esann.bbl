\begin{thebibliography}{10}

\bibitem{yu_hyper-parameter_2020}
T.~Yu et~al.
\newblock Hyper-parameter optimization: A review of algorithms and
  applications.
\newblock {\em arXiv preprint arXiv:2003.05689}, 2020.

\bibitem{jamieson2016non}
K.~Jamieson et~al.
\newblock Non-stochastic best arm identification and hyperparameter
  optimization.
\newblock In {\em Artificial intelligence and statistics}, pages 240--248.
  PMLR, 2016.

\bibitem{li2017hyperband}
L.~Li et~al.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6765--6816,
  2017.

\bibitem{bohdal2023pasha}
O.~Bohdal et~al.
\newblock {PASHA}: Efficient {HPO} and {NAS} with progressive resource
  allocation.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{mohr_learning_2022}
F.~Mohr et~al.
\newblock Learning curves for decision making in supervised machine learning--a
  survey.
\newblock {\em arXiv preprint arXiv:2201.12150}, 2022.

\bibitem{li_system_2020}
L.~Li et~al.
\newblock A system for massively parallel hyperparameter tuning.
\newblock {\em Proceedings of Machine Learning and Systems}, 2:230--246, 2020.

\bibitem{domhan_speeding_2015}
T.~Domhan et~al.
\newblock Speeding up automatic hyperparameter optimization of deep neural
  networks by extrapolation of learning curves.
\newblock In {\em Twenty-fourth international joint conference on artificial
  intelligence}, 2015.

\bibitem{klein2017fast}
A.~Klein et~al.
\newblock Fast bayesian optimization of machine learning hyperparameters on
  large datasets.
\newblock In {\em Artificial intelligence and statistics}, pages 528--536.
  PMLR, 2017.

\bibitem{falkner_bohb_2018}
S.~Falkner et~al.
\newblock Bohb: Robust and efficient hyperparameter optimization at scale.
\newblock In {\em International Conference on Machine Learning}, pages
  1437--1446. PMLR, 2018.

\bibitem{baker_accelerating_2017}
B.~Baker et~al.
\newblock Accelerating neural architecture search using performance prediction.
\newblock {\em arXiv preprint arXiv:1705.10823}, 2017.

\bibitem{klein_learning_2017}
A.~Klein et~al.
\newblock Learning curve prediction with bayesian neural networks.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{wu2020practical}
J.~Wu et~al.
\newblock Practical multi-fidelity bayesian optimization for hyperparameter
  tuning.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 788--798.
  PMLR, 2020.

\bibitem{wu2018understanding}
Y.~Wu et~al.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{white_how_2021}
C.~White et~al.
\newblock How powerful are performance predictors in neural architecture
  search?
\newblock {\em Advances in Neural Information Processing Systems},
  34:28454--28469, 2021.

\bibitem{eggensperger_hpobench_2021}
K.~Eggensperger et~al.
\newblock Hpobench: A collection of reproducible multi-fidelity benchmark
  problems for hpo.
\newblock {\em arXiv preprint arXiv:2109.06716}, 2021.

\bibitem{klein_tabular_2019}
A.~Klein et~al.
\newblock Tabular benchmarks for joint architecture and hyperparameter
  optimization.
\newblock {\em arXiv preprint arXiv:1905.04970}, 2019.

\bibitem{zimmer2021auto}
L.~Zimmer et~al.
\newblock Auto-pytorch: Multi-fidelity metalearning for efficient and robust
  autodl.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  43(9):3079--3090, 2021.

\bibitem{bansal2022jahsbench}
A.~Bansal et~al.
\newblock Jahs-bench-201: A foundation for research on joint architecture and
  hyperparameter search.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem{pfisterer2022yahpo}
F.~Pfisterer et~al.
\newblock Yahpo gym-an efficient multi-objective multi-fidelity benchmark for
  hyperparameter optimization.
\newblock In {\em International Conference on Automated Machine Learning},
  pages 3--1. PMLR, 2022.

\bibitem{hutter_learning_2021}
M.~Hutter.
\newblock Learning curve theory.
\newblock {\em arXiv preprint arXiv:2102.04074}, 2021.

\bibitem{10.5555/645940.671380}
B.~Gu et~al.
\newblock Modelling classification performance for large data sets.
\newblock In {\em Proceedings of the Second International Conference on
  Advances in Web-Age Information Management}, WAIM '01, page 317â€“328,
  Berlin, Heidelberg, 2001. Springer-Verlag.

\bibitem{mohr2023lcdb}
F.~Mohr et~al.
\newblock Lcdb 1.0: An extensive learning curves database for classification
  tasks.
\newblock In {\em Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2022, Grenoble, France, September 19--23,
  2022, Proceedings, Part V}, pages 3--19. Springer, 2023.

\bibitem{gavin2019levenberg}
HP. Gavin.
\newblock The levenberg-marquardt algorithm for nonlinear least squares
  curve-fitting problems.
\newblock {\em Department of Civil and Environmental Engineering, Duke
  University}, 19, 2019.

\bibitem{jax2018github}
J.~Bradbury et~al.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{bergstra_algorithms_2011}
J.~Bergstra et~al.
\newblock Algorithms for hyper-parameter optimization.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{bergstra_making_2013}
J.~Bergstra et~al.
\newblock Making a science of model search: Hyperparameter optimization in
  hundreds of dimensions for vision architectures.
\newblock In {\em International conference on machine learning}, pages
  115--123. PMLR, 2013.

\bibitem{hutter2014algorithm}
F.~Hutter et~al.
\newblock Algorithm runtime prediction: Methods \& evaluation.
\newblock {\em Artificial Intelligence}, 206:79--111, 2014.

\bibitem{scikit-learn}
F.~Pedregosa et~al.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\end{thebibliography}
