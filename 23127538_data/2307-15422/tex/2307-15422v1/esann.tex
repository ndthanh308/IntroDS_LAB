\documentclass{esann}

\usepackage[dvips]{graphicx}
\usepackage[utf8]{inputenc}

% \usepackage[a4paper, total={12.2cm, 19.3cm}]{geometry}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables

\usepackage{wrapfig}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
\usepackage{hyperref}

% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

%%% MATH REQUIREMENTS - START %%%
\usepackage{amsmath,amssymb,amsfonts,array}
\usepackage{algorithmic}
\usepackage[noline,ruled,algo2e,linesnumbered]{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}

% algorithm2e
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{end if}%

\newcommand{\R}{\mathbb{R}} % For the reals
\newcommand{\cD}{\mathcal{D}} % For a domain
\newcommand{\cC}{\mathcal{C}} % 
\newcommand{\cI}{\mathcal{I}} % 
\newcommand{\cR}{\mathcal{R}} % 
\newcommand{\cX}{\mathcal{X}} % 
\newcommand{\cB}{\mathcal{B}} %

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\ALC}{ALC}

\newcommand{\ucb}{\texttt{UCB}\xspace} % 
\newcommand{\qucb}{\texttt{qUCB}\xspace} % 
\newcommand{\fipl}{f_\text{IPL}} %
\newcommand{\dfipl}{f'_\text{IPL}} %

\usepackage{todonotes}
% \usepackage[disable]{todonotes}
\newcommand{\isabelle}[1]{\todo[inline,color=orange!40]{#1 -- Isabelle}} 
\newcommand{\romain}[1]{\todo[inline,color=purple!40]{#1 -- Romain}} 
\newcommand{\prasanna}[1]{\todo[inline,color=blue!40]{#1 -- Prasanna}} 
\newcommand{\yixuan}[1]{\todo[inline,color=red!40]{#1 -- Yixuan}} 

%%% Define colors
\definecolor{blue}{HTML}{1f77b4}
\definecolor{orange}{HTML}{ff7f0e}
\definecolor{green}{HTML}{2ca02c}
\definecolor{red}{HTML}{d62728}
\definecolor{purple}{HTML}{9467bd}
\definecolor{darkblue}{HTML}{000055}
\definecolor{darkred}{HTML}{880000}
%***********************************************************************
% !!!! IMPORTANT NOTICE ON TEXT MARGINS !!!!!
%***********************************************************************
%
% Please avoid using DVI2PDF or PS2PDF converters: some undesired
% shifting/scaling may occur when using these programs
% It is strongly recommended to use the DVIPS converters, and to submit
% PS file. You may submit a PDF file if and only if you use ADOBE ACROBAT
% to convert your PS file to PDF.
%
% Check that you have set the paper size to A4 (and NOT to letter) in your
% dvi2ps converter, in Adobe Acrobat if you use it, and in any printer driver
% that you could use.  You also have to disable the 'scale to fit paper' option
% of your printer driver.
%
% In any case, please check carefully that the final size of the top and
% bottom margins is 5.2 cm and of the left and right margins is 4.4 cm.
% It is your responsibility to verify this important requirement.  If these margin requirements and not fulfilled at the end of your file generation process, please use the following commands to correct them.  Otherwise, please do not modify these commands.
%
\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

%***********************************************************************
% !!!! USE OF THE esannV2 LaTeX STYLE FILE !!!!!
%***********************************************************************
%
% Some commands are inserted in the following .tex example file.  Therefore to
% set up your ESANN submission, please use this file and modify it to insert
% your text, rather than staring from a blank .tex file.  In this way, you will
% have the commands inserted in the right place.

\begin{document}
%style file for ESANN manuscripts
\title{Is One Epoch All You Need For Multi-Fidelity Hyperparameter Optimization?}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
\author{Romain Egel\'e$^{1,3}$, Isabelle Guyon$^{1,2}$, Yixuan Sun$^3$, Prasanna Balaprakash$^4$
%
% Optional short acknowledgment: remove next line if non-needed
% \thanks{This is an optional funding source acknowledgement.}
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
% Addresses and institutions (remove "1- " in case of a single institution)
1- LISN, U. Paris-Saclay, France;
2- Google, USA \& ChaLearn, USA; \\ 3- MCS, Argonne Ntl Lab, USA; 
4- CCSD, Oak Ridge Ntl Lab, USA\\
}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

\begin{abstract}

Hyperparameter optimization (HPO) is crucial for fine-tuning machine learning models, but it can be computationally expensive. To reduce costs, Multi-fidelity HPO (MF-HPO) leverages intermediate accuracy levels in the learning process and discards low-performing models early on.
We conducted a comparison of various representative MF-HPO methods against a simple baseline on classical benchmark data. The baseline involved discarding all models except the Top-$K$ after training for only one epoch, followed by further training to select the best model. Surprisingly, this baseline achieved similar results to its counterparts, while requiring an order of magnitude less computation.
Upon analyzing the learning curves of the benchmark data, we observed a few dominant learning curves, which explained the success of our baseline. This suggests that researchers should (1) always use the suggested baseline in benchmarks and (2) broaden the diversity of MF-HPO benchmarks to include more complex cases.
\end{abstract}

\section{Introduction}

Hyperparameter optimization (HPO) is a key component of AutoML systems. It aims to find the best configuration of a machine learning (ML) pipeline, which consists of data and model components. Hyperparameters (HP) are parameters that cannot be learned directly with the primarly learning algorithm (e.g., gradient descent), but affect the learning process and the performance of the pipeline. Those may include number of layers and units per layer in deep networks, learning rates, etc. HPO is usually formulated as a black-box optimization problem\cite{yu_hyper-parameter_2020}, where a function maps a HP configuration to a performance score. However, evaluating this function can be costly and time-consuming. Therefore, Multi-fidelity HPO (MF-HPO) methods have been proposed~\cite{jamieson2016non,li2017hyperband}, which use intermediate learning machine performance estimates to reduce the computational cost and speed up the optimization process.
In multi-fidelity algorithms, fidelity refers to the level of accuracy or resolution at which a machine learning model is trained and evaluated. Different levels of fidelity can be used to balance the trade-off between computational cost and accuracy. For example, a low fidelity level could involve training and evaluating a model on a small subset of the data, while a high fidelity level would involve using the entire dataset. Multi-fidelity algorithms use this concept to optimize hyperparameters and other model parameters efficiently by exploring the model space using a combination of different fidelity levels. The accuracy obtained at each fidelity level is used to guide the optimization process towards better performing models while minimizing the computational cost.

\begin{wrapfigure}{R}{0.55\textwidth}
    \centering
    % Figure removed
    \caption{\small Hyperparameter optimization and its components including {\color{green}input/output}, {\color{red} outer optimization loop exploring new HP configurations}, {\color{blue}inner optimization loop incrementally allocating new training ``steps"} and {\color{orange} model selection}.}
    \label{fig:components-hpo}
    \vspace{-0.1in}
\end{wrapfigure}

Fig.~\ref{fig:components-hpo} presents the generic components of a muti-fidelity HPO. The input to HPO is the HP search space. The output is a trained model with the corresponding hyperparameters. HPO is a bi-level optimization problem: the upper level ({\color{red}outer loop}) optimizes the HP of the ML pipeline while the lower level ({\color{blue}inner loop}) optimizes the parameters of the learning machine, given the HP configuration of the upper level. After completing the search process, a final step of {\color{orange}model selection} is performed before returning the trained model.
In regular HPO, the inner loop is halted by a given stopping criterion. A natural improvement is to consider the multi-fidelity setting, where intermediate performance estimates are obtained at different stages of the inner optimization process (as a function of the number of training examples or training epochs). This involves monitoring a "learning curve."

%%%% CLAIMS
This study evaluates the effectiveness of early discarding policies in the {\color{blue}inner loop}, while random search is used for the {\color{red}outer loop}. We use widely-used approaches for early discarding, performing denovo search without prior knowledge of learning curve typology or meta-learning involvement.

We consider two baselines at each extreme. The first evaluates each HP configuration candidate at maximum fidelity, i.e., after being fully trained within the overall time budget (100-Epochs). The second evaluates them at minimum fidelity, after only one epoch of training. We demonstrate the effectiveness of the 1-Epoch approach and explain why it is successful. Our findings provide a new perspective on previous studies that omitted this baseline. Only one study~\cite{bohdal2023pasha} included a similar baseline and reported results similar to ours. Therefore, we advocate for including the 1-Epoch baseline in further MF-HPO benchmarks.

\section{Related Work}~\label{sec:related-work}

Our study focuses on methods, which train only a single model at a time, but keep all check-points for further reference. Early discarding means switching to training a model with another HP configuration, before attaining the maximum number of epochs allowed. A typical example of such strategy, sometimes referred to as ``vertical'' model selection~\cite{mohr_learning_2022} is Asynchronous Successive Halving~\cite{li_system_2020} (SHA). Several methods can be adapted to this setting, including Hyperband~\cite{li2017hyperband}, which can explore different levels of fidelity to differentiate noisy learning curves; Learning Curve Extrapolation~\cite{domhan_speeding_2015} (LCE), which can observe early performances and extrapolate future performances to decide whether training should continue; FABOLAS~\cite{klein2017fast}, which learns correlations in the candidates' ranking between different levels of fidelity; Bayesian Optimization Hyperband~\cite{falkner_bohb_2018}, which embeds Bayesian optimization in Hyperband to sample candidates more efficiently; Learning Curve with Support Vector Regression~\cite{baker_accelerating_2017}, which predicts final performance based on the configuration and early observations; Learning Curve with Bayesian neural network~\cite{klein_learning_2017}, which is similar to the previous method but switches the model with a Bayesian neural network; and Trace Aware Knowledge-Gradient~\cite{wu2020practical}, which leverages an observed curve to update the posterior distribution of a Gaussian process more efficiently.

The previous works had some limitations, which were primarily due to the assumptions made about the learning curve. For example, methods based on SHA assume that the discarded learning curves will not cross in the future, since only the Top-$K$ models are allowed to continue at any given step. In this context, models that start slowly are often discarded.
This phenomenon is known as ``short-horizon bias''~\cite{wu2018understanding}. For methods based on LCE, they either assume a "biased" parametric model, or they are more generic but require a more expensive initialization\cite{white_how_2021}. The second limitation is that the configuration returned is often the lowest validation error observed during the search. However, this approach is quite limiting because it does not consider more sophisticated model selection schemes such as cross-validation or Hyperband. Therefore, we added the {\color{orange} model selection} block in Fig.~\ref{fig:components-hpo}.

Benchmarks play a critical role in the design and development of HPO methods. We have surveyed several recent benchmarks for learning curves that are continuously evolving, such as HPOBench~\cite{eggensperger_hpobench_2021,klein_tabular_2019}, LCBench~\cite{zimmer2021auto}, JAHS-Bench-201~\cite{bansal2022jahsbench}, and YAHPO-Gym~\cite{pfisterer2022yahpo}.

\section{Method}

Our experiments include representative state-of-the-art methods in multi-fidelity HPO research.
The first method considered is {\bf Successive Halving} (SHA)~\cite{jamieson2016non} which we consider in its asynchronous variant~\cite{li_system_2020} to be able to handle a stream of learners. SHA decides to continue the training loop at exponentially increasing steps called rungs (based on learning curves~\cite{hutter_learning_2021} theory). At each rung, it continues training only if the current observed score is among the Top-$K$ scores and discards the model otherwise. This method is error-prone when learning curves are noisy. To make SHA more robust {\bf Hyperband} (HB)~\cite{li2017hyperband} enforces the exploration of more levels of fidelity on top of SHA to reduce the impact of noise in decisions. However this is at the expense of consuming more training steps than SHA. Finally, the last approach we consider consists in using a surrogate model to predict the future of the learning curve, called {\bf Learning Curve Extrapolation} (LCE)~\cite{domhan_speeding_2015}. In theory, this approach is less likely to suffer from ``short-horizon'' bias unless the learning curve surrogate is biased. 
The method also suffers from instabilities in its original implementation \cite{domhan_speeding_2015}. 
To address this shortcoming, we are using our own implementation of this method (See the RoBER method described in supplemental material$^1$).

We compare these methods with two baselines: the maximum fidelity baseline ({\bf 100-Epochs}), which trains all methods for 100 Epochs and selects the best one and a mimimal fidelity baseline ({\bf 1-Epoch}), which comprises in fact two phases: (i) exploration search phase pre-selecting the Top-$K$ after only 1 Epoch (minimum fidelity blue loop in Fig.~\ref{fig:components-hpo}); (ii) model selection phase evaluating the Top-$K$ at maximum fidelity and returing the best model. 1-Epoch is expected to perform poorly if there is a strong ``short-horizon'' bias (see previous secton for a definiton) and noisy learning curves.

\section{Experimental Results}

\begin{wrapfigure}{R}{0.45\textwidth}
    \vspace{-0.2in}
    \centering
    % Figure removed
    \vspace{-0.15in}
    \caption{\small Comparing the performance (mean and standard error) of various early discarding strategies (all combined with with random search) on the Naval Propulsion problem from the HPOBench benchmark.}
    \label{fig:model-based-pruning-with-random-search-on-hpobench}
    \vspace{-0.15in}
\end{wrapfigure}

In our analysis we looked at four benchmarks: HPOBench, LCBench, YAHPO-Gym and JAHS-Bench. But, since they all yielded similar results, for brevity we only expose the Naval Propulsion problem from HPOBench. We provide complementary materials \footnote{\url{https://github.com/deephyper/scalable-bo/blob/main/esann-23/One_Epoch_Is_Often_All_You_Need_Extended.pdf}} with our full set of results.

To visualize the algorithm progress, we display the test error as a function of training epochs \cite{pfisterer2022yahpo} (Fig.\ref{fig:components-hpo}). This type of learning curve weighs training iterations (epochs) equally for all HP configurations, which may be deceiving since they can vary in computational cost. Still, this is a convenient simple method abstrating from details of implementation.
 We configured the number of iterations for the outer-loop optimization to 200 (red loop).
As a result, the maximum fidelity policy (100-Epochs),  consumes the maximum number of 20,000 training epochs. In contrast, its counterpart at minimum fidelity (1-Epoch, performed with a Top-3 model selection ($K=3$) at maximum fidelity after the search), consumes a fixed number of $200 \times 1 + 3 \times 100 = 500$ training epochs. 

Figure~\ref{fig:model-based-pruning-with-random-search-on-hpobench} shows the evolution of the test RMSE as a function of the number of training epochs used. The key observation is that the test RMSE for the 1-Epoch (blue curve) and 100-Epochs (orange curve) strategies is similar at the final point. However, the 1-Epoch policy uses 40 times fewer training epochs than its counterpart, the 100-Epoch policy (20,000/500=40), as per our experimental setup, which is evident from the last point of the blue and orange curves. Additionally, we note that the more complex agents (SHA, HB, and LCE) do not differ significantly from each other and all result in similar final test RMSE, but they consume significantly more epochs. Hyperband consumes slightly more resources than SHA, which is consistent with its design. SHA consumes the least training epochs, about 10 times fewer epochs than 100-Epochs, but still, four times more than 1-Epoch.

After observing the performance of training for only one epoch during the search, we will explain why. In Fig.~\ref{fig:viz-learning-curves-by-ranking-navalpropulsion}, we display 1,000 randomly sampled learning curves from the same benchmark, colored by their ranking at maximum fidelity. Low ranks, colored in blue, correspond to good models, while high ranks, colored in red, correspond to bad models. This visualization reveals that the groups of bad and good models can be identified in the first epoch of training, making selection with one epoch effective. Some noise exists between models, as seen in the blue curves, necessitating the top-$K$ tournament selection to differentiate among different models. The noise in the ranking for the same benchmark can be visualized in Fig.\ref{fig:viz-heatmap-raking-navalpropulsion}, where the ranking for the same models at each training epoch is displayed. The first half (before 50 epochs) has more noise than the second half, and the ranking of good models stabilizes faster, already after 5 epochs, compared to the ranking of bad models, which remains relatively noisy until the end.

% Figure environment removed

\section{Conclusion}
\label{sec:conclusion}

In this study, we investigated the optimization of hyperparameters for machine learning models with minimal training steps using MF-HPO agents. We compared various state-of-the-art multi-fidelity policies and discovered that a new simple baseline method, not considered in prior benchmarks, namely the 1-Epoch strategy assessing candidate configuration at lowest fidelity, performs surprisingly well. This was traced to the fact that good and bad models can be distinguished early in the training process. This phenomenon occurs frequently in benchmarks commonly used in the literature. Therefore, this point to the need to include 1-Epoch in forthcoming benchmarks and eventually designing new harder (possibly more realistic) benchmarks that will defeat it. Our work is limited to using ``epoch" as a unit of fidelity. While this is convenient and appealing to conduct studies independent of hardware implementation considerations, practical application settings may require considering wall time or other options as units of fidelity.

%This could be too costly for very large datasets, such as large text corpora used to train language models, or too little data for very small datasets.
% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************

\begin{footnotesize}

% Rewrite in a more compact way and also abbreviate the author list: ""

\bibliographystyle{unsrt}
\bibliography{esann-short}

\end{footnotesize}

% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\appendix
\include{esann-extended}

\end{document}
