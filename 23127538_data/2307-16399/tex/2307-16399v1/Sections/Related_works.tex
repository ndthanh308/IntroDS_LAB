\section{Related Works}
% Figure environment removed

%\subsection{Stylized Visual Captioning}
\noindent \textbf{Stylized Visual Captioning.} 
Stylized Visual Captioning \citep{mathews2016senticap,bin2021multi} aims to generate descriptions of images or videos that are both visually relevant and stylistically accurate. Previous works in this area can be categorized into supervised and unsupervised methods. Supervised approaches \cite{mathews2016senticap,you2018image,shuster2019engaging,li2021similar,bin2021multi,li2022taking} involve constructing paired stylized captions for visual content. However, building large-scale paired datasets can be laborious, and these methods are limited to certain styles in parallel datasets.
%\citet{bin2021multi} propose a model that mines various perspectives within a video and generate a description for each perspective, including the visual aspect, language style, and perception pattern. \citet{li2022taking}  propose a large-scale emotion and logic driven multilingual dataset for video paragraph captioning task. 

Therefore, most existing works focus on unsupervised methods. \citet{mathews2018semstyle} propose to generate semantic terms for visual content, and transform them into descriptions with various styles. Several approaches, including StyleNet \cite{gan2017stylenet}, Factual \cite{chen2018factual}, DLN \cite{chen2019unsupervised}, and MSCap \cite{guo2019mscap}, suggest different architectures for learning style-dependent matrices that capture style-related information.
%\citet{guo2019mscap} propose an adversarial learning framework with a style dependent generator, a caption discriminator and a style classifier to improve the overall performance. 
MemCap \cite{zhao2020memcap} and Senti-Trans \cite{wu2023sentimental} incorporate style knowledge to generate stylized descriptions.
%\citet{zhao2020memcap} introduce a memory module which captures style knowledge within content-related and style-related parts. \citet{wu2023sentimental} integrate both content and sentiment information from multiple modalities and incorporates prior sentimental knowledge to generate sentimental video descriptions. 
\citet{tan2022detach} propose to detach several text style representations for specific styles, and then attach them to image content to generate stylized captions. Recently, some works start to leverage the power of large language models. \citet{zeng2023conzic} propose a sampling-based methods to generate captions that incorporates controllable signals like predefined sentiments. \citet{nukrai2022capdec} and \citet{gu2022can} utilize the CLIP embedding space \cite{radford2021clip} and additional stylized training data to perform stylized captioning tasks, achieving impressive results. Although these unsupervised methods have shown effective results, they still require sufficient samples with desired style labels, and additional training is necessary to accommodate new styles. These limitations motivate us to explore the problem of few-shot stylized visual captioning. 

%\subsection{Few-shot Text Style Transfer}
\noindent \textbf{Few-shot Text Style Transfer.} 
%Most text style transfer researches fall into supervised or unsupervised approaches, which will be limited by pre-specific style labels. 
Recently, some works have started to explore the problem of few-shot text style transfer. This task does not require style labels during training, but instead uses a small number of labeled stylized examples as guidance during inference.
\citet{xu2020variational} propose to train a variational auto-encoder on unlabeled text, learning a text representation that features a controllable portion, which is restricted to lie on a k-dimensional simplex. To perform transfer, the controllable dimensions are manipulated with a basis vector that corresponds most strongly to the target style. 
Similarly, \citet{riley2020textsettr} fine-tune the T5 encoder \cite{raffel2020exploring} to extract a style vector from input text. This vector can then be employed to modify the latent representation in order to generate text with the target style. 
\citet{reif2021recipe}, \citet{luo2023prompt} and \citet{suzgun2022prompt} employ prompting-based methods. They utilize large language models (LLMs) and design specific prompts to rewrite texts in various styles. However, these methods usually require paired prompts that represent the relations and differences between different styles, which makes the process less flexible. Additionally, they face the problem of hallucinations, making them less reliable \cite{reif2021recipe} when compared to trained methods. To address these concerns, we utilize pre-trained models and fine-tune them to tackle our specific task, leveraging the benefits of LLMs and avoiding the drawbacks of prompt-based techniques at the same time.
