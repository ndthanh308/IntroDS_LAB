\vspace{-2pt}
\section{Method}

%\paragraph{\textbf{Problem Formulation}} 
Given the visual content (an image or a video) $x$, our goal is to generate a stylized description in the target style specified by a few example sentences. % $\mathcal{S}_{tgt}$. 
As the (vision, stylized-caption) paired data is not available for training, we leverage a factual captioning dataset that consists of  (vision, factual-caption) paired data denoted as $\mathcal{V}=\{(x_i, y_i)\}_{i=1}^N$, %of images (or videos) and their corresponding captions; 
as well as a text-only corpus that contains miscellaneous text without style labels, denoted as $\mathcal{T}=\{t_i\}_{j=1}^{M}$ . %that does not have style labels. 


\iffalse
Each sentence is decomposed into a sequence of discrete tokens $t_i=\{w^i_{1}, \ldots, w^i_{k_i}\}$ by the pre-trained tokenizer. %Our experiments consider 
We assume two adjacent sentences $t_i$ and $t_{i-1}$ in the same document possess the same style. This allows us to perform reconstruction tasks of $t_i$ https://www.overleaf.com/project/63f4b45c6f3d5a4ce8cc8de0based on the style vector of $t_{i-1}$. The textual corpus includes factual captions, enabling our style extractor to handle factual style as well.
\fi

\vspace{-6pt}

\paragraph{\textbf{Overall Framework}}\label{sec:framework} %As illustrated in Figure \ref{fig:frame}, we use a conditional encoder-decoder model as our generative model. 
In Figure \ref{fig:frame} (a), we illustrate the framework of our proposed FS-StyleCap. It comprises four key modules: (1) the \textit{Style Extractor $E_s$} that extracts the style-related information from text and represents it as a single style vector; %Additionally, we attempt to train a style-related CLS token, as demonstrated in our supplementary file.
(2) the \textit{Content Extractor} $E_c$ that extracts the content-related vectors; (3) the \textit{Generator} $G$, which accepts the input by fusing \footnote{In detail, we mean-pool the output hidden states of $E_s$ to obtain a style vector $s$, and add it to each content vector.} the content vectors and the style vector to generate captions in the target style; (4) a \textit{Visual Projection Module} $M^{V \rightarrow L}$ that projects the visual features to the shared visual-language embedding space. $E_c$/$E_s$ and $G$ can all leverage the power of large-scale language models, i.e. initialized with the weights of pre-trained T5 \cite{raffel2020exploring}. Specifically, $E_c$ and $E_s$ are initialized with the pre-trained encoder, while their abilities are differentiated after our fine-tuning; $G$ is initialized with the decoder, and can generate stylized descriptions conditioned on an additional style vector after training.
%It utilizes a style extractor $E_s$ and a content extractor $E_c$ to extract style-related and content-related representations, respectively.% The only architectural difference between $E_c$ and $E_s$ is that we mean-pool the hidden state sequence of the style extractor into a single style vector. 
%This vector is then added to content vectors, forming the input to our stylized description Generator $G$.  To leverage the power of large-scale language models, $E_c$/$E_s$ and $G$ are fine-tuned from pre-trained language models, such as T5 \cite{raffel2020exploring}. To address cross-modal alignment, we apply a transformer-based Visual Projection Module $M^{V \rightarrow L}$ to project high-level visual features to the appropriate dimension for the language model.

%\qin{describing why two-stage training first} 
Due to the lack of (vision, stylized-caption) paired data and style-labeled text for training, we leverage the available factual caption dataset $\mathcal{V}$ and a textual corpus $\mathcal{T}$ without style labels.   
To fully train the core components $E_s$, $E_c$, and $G$, we propose a two-stage training scheme for our framework. The style extractor $E_s$ only requires knowledge from the text modality. Thus in the first training stage (Section \ref{sec:style_extraction}), we use an unlabeled style corpus to train $E_s$. This stage involves three tasks: denoising reconstruction, noisy back translation, and style discrimination. In the second stage (Section \ref{sec:cross_modality}), we freeze $E_s$ and train other modules simultaneously with Visual Captioning tasks on $\mathcal{V}$ and Text Style Injection tasks on $\mathcal{T}$. As a result, our model can generate descriptions in any style based on the extracted style vector and visual content vectors.% (which have been projected to the multi-modal embedding space).
%In these tasks, 
\subsection{Text Style Extraction}\label{sec:style_extraction}
Large language models possess a strong ability in text representation \cite{riley2020textsettr,suzgun2022prompt}. Our objective is to distill the encoder's ability to extract style-related representations. 
%and enable the decoder to generate stylized text conditioned on a style vector and content vectors. 
To achieve this with an unlabeled corpus, we assume two adjacent sentences $t_i$ and $t_{i-1}$ in the same document possess the same style \cite{riley2020textsettr}. This allows us to perform reconstruction tasks for $t_i$ based on the style vector of $t_{i-1}$. Note that the text corpus also includes factual captions, all of which possess the factual style \cite{gan2017stylenet,tan2022detach}. This enables our style extractor to handle factual style as well. We train our text style extractor $E_s$ through three tasks as follows.%The proposed training losses are described below.
\paragraph{\textbf{Denoising Reconstruction}} Each sentence $t_i$ is decomposed into a sequence of discrete tokens $\{w^i_{1}, \ldots, w^i_{k_i}\}$ by the pre-trained tokenizer. Our noise function first corrupts the input sentence by dropping tokens in $t_i$ with probability $p$. Then, the content extractor $E_c$ extracts content-related vectors from the corrupted sentence $\tilde{t}_i$. As sentence $t_{i-1}$ is assumed to have the same style as $t_i$, the stylized generator attempts to reconstruct $t_i$ conditioned on the style vector of $t_{i-1}$. Since noise applied to the input $t_i$ can corrupt words that convey the input style, the decoder must learn to use the additional style vector to generate stylized descriptions. This will also encourage $E_s$ to extract style-related information. Our goal is to minimize the cross-entropy loss as follows:
\begin{equation}
\begin{split}  
\mathcal{L}_{DR} &= \mathcal{L}_\textrm{CE}\bigg(t_i, G\Big(E_c\big(\tilde{t}_i\big),E_s\big(t_{i-1}\big)\Big)\bigg) \\
&= -\sum_{n=1}^{K_i} \textrm{log } p_{\theta_G, \theta_{E_c}, \theta_{E_s}}(w_n^i|w_{1:n-1}^i,\tilde{t}_i,t_{i-1}),
\end{split}
\end{equation}
where $\theta_G, \theta_{E_c}, \theta_{E_s}$ are the parameters of generator $G$, content extractor $E_c$, and style extractor $E_s$. 

\paragraph{\textbf{Noisy Back Translation}} First, we corrupt $t_i$ to obtain $\tilde{t}_i$. Then, we apply the current model to transfer $\tilde{t}_i$ into $t'_i$ with another text style, using the style vector of a randomly sampled $t_j$. Finally, we apply our model to translate $t'_i$ back into the original style, as instructed by the style vector of $t_{i-1}$. As verified by \citet{riley2020textsettr}, this approach encourages the model to identify which tokens in the input $t'_i$ do not match the target style indicated by $t_{i-1}$ and change them. This improves the effectiveness of the style extractor. The loss function is defined as follows:
\begin{equation}
\begin{split}
\mathcal{L}_{NBT} &= \mathcal{L}_\textrm{CE}\bigg(t_i, G\Big(E_c\big(t'_i\big),E_s\big(t_{i-1}\big)\Big)\bigg)\\
 &= -\sum_{n=1}^{K_i} \textrm{log } p_{\theta_G, \theta_{E_c}, \theta_{E_s}}(w_n^i|w_{1:n-1}^i,t'_i,t_{i-1}) .  
\end{split}
\end{equation}

\paragraph{\textbf{Text Style Discrimination}} To further improve the quality of extracted style vectors, we propose a style-related loss. To achieve this, we train a style discriminator $D$, which can compute the similarity in text style between two sentences.  During the process of noisy back translation, $t'_i$ should have the same style as $t_j$ as it is generated based on the style vector of $t_j$. 
We calculate the style discrimination loss as follows:
\begin{equation}
    \mathcal{L}_{style} = -logD(t'_i, t_j),
\end{equation}
where the discriminator\footnote{The discriminator is fine-tuned with the RoBERTa-base \cite{liu2019roberta} model, applying the embedding of [CLS] token as sentence embedding.} is trained using contrastive learning \cite{hadsell2006dimensionality}, with positive samples from the same paragraph, and negative samples from other sentences in the training batch.
\paragraph{\textbf{Final Loss}}
The final loss term used for training is the average of the three losses, denoted as:
\begin{equation}\label{eqn:l_text}
   \mathcal{L}_{Text} =( \mathcal{L}_{DR}+ \mathcal{L}_{NBT}+\mathcal{L}_{style})/3.
\end{equation}
%where each loss is calculated from the same input.% sentence $t_i$.
\subsection{Cross-Modal Alignment}\label{sec:cross_modality}
Once we have the style extractor, which can obtain the style representation of any input text, the main challenge is to enable the generator to produce stylized captions based on the  extracted style vector and visual inputs. We use visual feature projection to achieve cross-modal alignment, and propose a multi-task training strategy to ensure the ability of style injection simultaneously. During this training process, the style extractor $E_s$ is frozen, while the content extractor $E_c$ and caption generator $G$ are fine-tuned. %from pre-trained T5 model.
\paragraph{\textbf{Visual Feature Projection}} \label{sec:visual_task} A transformer-based module $M^{V \rightarrow L}$ projects the visual representation to the appropriate input dimension for our language model. Concretely, we feed the transformer network with a concatenation of two inputs: the visual representation and a learnable constant embedding. The constant vector will extract semantic information from visual inputs, and achieve an adapted visual representations $x^{V \rightarrow L}$ with the correct dimension.

Given $x^{V \rightarrow L}_i$ and the style vector of another factual caption $y_{i-1}$, the generator aims to reconstruct $y_i=\{d^i_{1}, \ldots, d^i_{k_i}\}$, forming the captioning loss as:
\begin{equation}
\begin{split}  
\mathcal{L}_{Cap}&=\mathcal{L}_\textrm{CE}\bigg(y_i,G\Big(E_c\big(x^{V \rightarrow L}_i\big),E_s\big(y_{i-1}\big)\Big)\bigg)\\
& = -\sum_{n=1}^{K_i} \textrm{log } p_{\theta_{M^{V \rightarrow L}},\theta_{G}, \theta_{E_c}}(d_n^i|d_{1:n-1}^i,x_i,y_{i-1}).
\end{split}
\end{equation}

To better align projected visual representations with their related text embeddings, we optimize the L2 loss between the content vectors that are extracted 
from $x^{V \rightarrow L}_i$ and the text embedding of its factual caption $y_i$. The loss function is as follows:
\begin{equation}
\begin{split}  
\mathcal{L}_{V2L} &= \Big|\Big| E_c\big(x^{V \rightarrow L}_i\big),E_c\big(y_{i}\big)	\Big|\Big|.
\end{split}
\end{equation}

Our final visual loss function is as follows: 
\begin{equation}
\begin{split}  
\mathcal{L}_{Visual} & = (\mathcal{L}_{Cap} + \mathcal{L}_{V2L})/2.
\end{split}
\end{equation}
\paragraph{\textbf{Multi-task Training Procedure}} To ensure the ability of cross-modal alignment and text style injection at the same time, we apply a multi-task training strategy. The overall loss function is as follows:
\begin{equation}
    \mathcal{L}(\theta_{M^{V \rightarrow L}},\theta_G, \theta_{E_c}) = \mathcal{L}_{Visual} + \mathcal{L}_{Text},
\end{equation}
where $\theta_{M^{V \rightarrow L}}, \theta_G, \theta_{E_c}$ are the parameters of visual projector $M^{V \rightarrow L}$, caption generator $G$ and content extractor $E_c$. %Note that the parameters of style extractor are frozen, the parameters of $E_s$ and $E_c$ are shared in the task of visual captioning and text style injection.
\subsection{Inference Procedure} \label{sec:inference} Previous methods for stylized visual captioning aim to achieve several style-related knowledge for a fixed set of discrete styles. In contrast, our extracted style vector can represent multiple text style attributes simultaneously \cite{riley2020textsettr}. For example, an input sentence can cover the style of being positive, informal, etc. 


During inference, FS-StyleCap accepts a few example sentences provided by users, which may represent a combination of different style attributes. We use our style extractor to extract style vectors from each example sentence and then mean pool them to obtain the target style vector, denoted as $s_{tgt}$. %Assuming the exemplar pools are relatively diverse, this averaging would better represent the desired style. 
%\qin{this part is a bit abscure}
To inject a target style, we need to adjust the style vector towards the appropriate direction. 
%,  between the original (factual) and the target style, while retaining the original style \qin{??? retrain the original content?}. 
Specifically, we move away from the factual style that describes visual content objectively and neutrally, and towards the target style to some degree.
The style vector $s$ used for decoding is computed as $\lambda (s_{tgt} - s_{src}) + s_{src}$, where $s_{src}$ is the style vector of factual style, and the delta scale $\lambda$ is a hyper-parameter. Generally, $\lambda$ is an integer in the range of $[1,10]$, which can control the degree of stylization. In our experiments, we choose the value that achieves the best overall performance. The stylized caption is generated based on $s$ and the visual content vectors extracted from $x^{V \rightarrow L}$.
