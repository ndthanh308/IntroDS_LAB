
\section{Results and Discussion}


\subsection{Automatic Metric Results}
To evaluate the performance of our model in generating stylized visual captions, we conduct experiments on both image captioning and video captioning. 

Specifically, for sentimental image captioning, where a human-annotated dataset exists, we compare our results with other baselines on SentiCap. The full methods are trained on the entire text-only training set from SentiCap, while the few-shot methods only use a small number of stylized samples from the same set. Moreover, our proposed \textbf{FS-StyleCap} can handle multiple styles using a single model and do not require additional training for few-shot inference. As illustrated in Table \ref{table:senticap}, FS-StyleCap outperforms CapDec which is trained with the same $100$ stylized examples, both in visual relevance and style accuracy. We also surpass other full methods MSCap and StyleNet, and achieve a comparable overall performance to the state-of-the-art method CapDec when it is fully trained with a labeled corpus.  To measure our ability to extract text style from even fewer number of examples, we conduct experiments using only 1 and 5 example sentences, respectively. These experiments show credible results as well, better than CapDec trained with 100 samples.
 %In these experiments with very few examples, the quality of the sentences is critical. The qualitative examples are provided in our supplementary material. 
In addition, as \textit{factual} can also be figured as a style \cite{tan2022detach,mathews2018semstyle}, %\cite{tan2022detach,mathews2018semstyle}, 
we generate factual captions and evaluate their semantic relevance on MSCOCO. As shown in Table \ref{table:mscoco}, our model outperforms previous methods that can handle factual and other styles, and is comparable to methods that are specifically trained for standard image captioning, such as ClipCap \cite{mokady2021clipcap}.

\begin{table}[t]
\caption{Evaluation results on MSR-VTT. We apply the factual ground truths to evaluate the semantic
relevance.}
\vspace{-8pt}
\label{table:msrvtt} 
\centering
\fontsize{7.5}{10}\selectfont
\begin{tabular}{ccccccc}
\toprule
\multicolumn{7}{c}{\textbf{Positive}} \\
\midrule
\multicolumn{1}{c|}{\textbf{}} & \multicolumn{1}{c|}{\textbf{Model}} & \multicolumn{1}{c|}{\textbf{Data}} & \textbf{B-3} & \textbf{C} & \textbf{sACC} & \textbf{GM1} \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Few-Shot}} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{1} & \textbf{38.0} &	33.8 &	58.9 &	44.6 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{5} & 37.2 & 32.7 &	61.0 &	44.7 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{100} & 32.0 &	\textbf{36.6}	&72.8&	\textbf{\sethlcolor{bestresult}\hl{51.6}} \\
\midrule
\multicolumn{1}{c|}{Full} & \multicolumn{1}{c|}{Senti-Trans \cite{wu2023sentimental}} & \multicolumn{1}{c|}{2,994} & 28.9 & 25.0 & \textbf{97.8} & 49.5 \\
\midrule
\multicolumn{7}{c}{\textbf{Negative}} \\
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{Few-Shot}} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{1} & \textbf{48.2}	& 43.6	&65.5 &	53.4 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{5} & 47.9 &	45.0	& 71.8 &	56.8\\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{\textbf{FS-StyleCap}} & \multicolumn{1}{c|}{100} & 42.0 &\textbf{46.1}	&72.7	&\textbf{\sethlcolor{bestresult}\hl{57.9}} \\
\midrule
\multicolumn{1}{c|}{Full} & \multicolumn{1}{c|}{Senti-Trans \cite{wu2023sentimental}} & \multicolumn{1}{c|}{2,991} & 25.9 & 16.7 & \textbf{96.2} & 40.08 \\
\bottomrule
\end{tabular}
\vspace{-4pt}
\end{table}


For video captioning, since there is no dataset with paired sentimental captions, we use the factual ground truths in MSR-VTT to evaluate the visual relevance. As illustrated in Table \ref{table:msrvtt}, our proposed FS-StyleCap outperforms the full method Senti-Transformer, especially for the negative style. We find that a small set of captions in MSR-VTT tend to represent positive styles, such as ``pretty girl'', ``nice car'', etc. Our inference method (Section \ref{sec:inference}) can distinguish these styles to some extent by subtracting the style vector of MSR-VTT, while Senti-Transformer might not be able to do this.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}


\subsection{Human Evaluation Results}\label{sec:human_eval}
We conduct human evaluation experiments on six styles: (1) two standard styles of positive and negative; (2) four more fine-grained styles of romantic, adventurous, memorable and skeptical. For each style, 1-5 example sentences are provided as examples (as shown in Figure \ref{fig:task}). We compare our results with CapDec, trained with 100 example sentences. We also compare our model with a powerful large language model (LLM). We use ClipCap to generate factual captions for images, and then apply Copilot Hub \footnote{https://app.copilothub.ai/home} to perform text style transfer with 100 examples. Specially, we upload the stylized examples to train the model to incorporate the target style. This model is based on ChatGPT, specially designed for stylized generation. Following previous methods \cite{suzgun2022prompt,reif2021recipe}, we design several prompts for text style transfer, and finally use the following one which achieves the best performance: 
\begin{itemize} [itemsep=0.1em,parsep=0em,topsep=0em,partopsep=0em,leftmargin=1em,itemindent=0.2em]
    \item Here is a text, which is a COCO image caption: "\{input\}". Here is a rewrite of the text, which is more in your style but keeps its semantics: "
\end{itemize} The human evaluation results are obtained from 13 English-fluent volunteers. As shown in Figure \ref{fig:human_evaluation}, our method achieves the best overall performance, particularly in fine-grained styles. Our method outperforms other methods in terms of style accuracy, and its visual relevance is comparable to the results based on LLM. We find that training with numerous examples can improve the performance of the large language model and CapDec beyond their current capabilities.
%, as shown by the automatic results in Table \ref{table:senticap}.
However, this requires users to curate a larger number of style-labeled examples (i.e. from 5 to 2,000) and wait for additional training, which is less flexible.
%however, the generated captions might be verbose and face the problem of hallucination. More cases are shown in %\qin{can we provide evidence for this claim? can put in the appendix}

%\qin{updates?}(Add some summary about the evaluation results)



\subsection{Ablation Study}
We carry out several ablation studies to show the effectiveness of our proposed components:
% Figure environment removed

\begin{table}[t]
\centering
\fontsize{8}{12}\selectfont
\caption{\label{ablation_table}
Ablation studies on positive and negative style. The overall metric GM1 is measured by the geometric mean of CIDEr and sACC.}
\vspace{-8pt}
\begin{tabular}{l|ccccc}
\toprule
\textbf{Model} &\textbf{CIDEr$\uparrow$} & \textbf{sACC$\uparrow$} &\textbf{GM1$\uparrow$} \\
\midrule
\textbf{FS-StyleCap} & 66.26 &	79.10 & \textbf{72.40}\\
\midrule
w/o $\mathcal{L}_{DR}$ & 38.60	& \textbf{87.30} &	58.05\\
w/o $\mathcal{L}_{NBT}$  & 67.05	& 	67.85 &	67.45\\
w/o $\mathcal{L}_{Style}$ & \textbf{67.06} &	69.80 &	68.41\\
w/o $\mathcal{L}_{V2L}$ & 54.75 &	75.70 &	64.38\\
%w/o $\mathcal{L}_{Cap}$ \\
w/o $MultiTask$ & 2.45& 	81.20	& 14.10\\
\bottomrule 
\end{tabular}
\end{table}

%\qin{take home message?}
% Figure environment removed


% Figure environment removed

% Figure environment removed

\parskip=0.1em
\begin{itemize} [itemsep=0.1em,parsep=0em,topsep=0em,partopsep=0em,leftmargin=1em,itemindent=0.2em]
    \item \textbf{w/o $\mathcal{L}_{DR}/\mathcal{L}_{NBT}/\mathcal{L}_{Style}$.} To evaluate the effectiveness of each loss function for text style injection, we remove the denoising reconstruction loss ($\mathcal{L}_{DR}$), the noisy back translation loss ($\mathcal{L}_{NBT}$) and the style discrimination loss ($\mathcal{L}_{Style}$) from the objective function (Eqn \ref{eqn:l_text}).
    \item \textbf{w/o $\mathcal{L}_{V2L}$.} To improve the visual relevance, we add the V2L loss, which further aligns the projected visual embeddings to its correlated text embeddings. In order to show its efficiency, we remove this loss and compare the experimental results.
    \item \textbf{w/o MultiTask} To verify the effectiveness of the multi-task training strategy in our second training stage, we freeze all of the language models trained in stage 1, and only trains the visual projection module with the task of factual visual captioning. 
\end{itemize}
Table \ref{ablation_table} presents our ablation studies on the stylized image captioning task. We observe that: (1) Noisy back translation helps improve style injection quality, while noisy reconstruction helps maintain semantic relevance. With the combination of them, we could achieve a better result; (2) Style discrimination loss significantly improves the style accuracy; (3) Removing $\mathcal{L}_{V2L}$ reduces the visual relevance, which demonstrates its effectiveness to make cross-modal alignment; (4) As our model's parameters and the pre-training data in stage one are not so large, if we freeze the parameters of our language model ($E_s,E_c,G$) and only update the parameters of $M^{V \rightarrow L}$, the performance degrades significantly. In summary, our designed components/schemes all contribute to our FS-StyleCap performance. 



\subsection{Embedding Visualization}
To demonstrate the effectiveness of our extracted style vectors, we sample $1,000$ positive examples and $1,000$ negative examples, and visualize the distributions of their style vectors using tSNE plots~\cite{van2008visualizing}. The results shown in Figure \ref{fig:embedding} display that after fine-tuning with our proposed methods (refer to Section \ref{sec:style_extraction}), the style extractor is capable of separating different style attributes in the embedding space, which demonstrates its effectiveness in extracting information related to style.


\subsection{Qualitative Results}
Figure \ref{fig:img_cases} presents some stylized image captioning cases. We illustrate several styles to show that our model can handle multiple styles with a single model, and does not require further training for new style. 
Figure \ref{fig:video_cases} shows some cases of stylized video captioning. More cases can be found in our supplementary material.


