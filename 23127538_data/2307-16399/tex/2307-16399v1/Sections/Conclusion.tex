\section{Conclusion and Future Work}
In this paper, we tackle the task of Few-Shot Stylized Visual Captioning, which aims to describe images and videos with the text style guided by a few examples. We propose a framework FS-StyleCap, which consists of a conditional encoder-decoder language model and a visual projection module. We firstly train the style extractor on an unlabeled text corpus with proposed losses. After achieving the extractor to extract style representation, we address the problem of cross-modal alignment, enabling our model to generate stylized descriptions guided by any desired style. Experiments on automatic evaluation and human evaluation demonstrate the effectiveness of our method in generating captions with multiple desired styles, only guided by a few examples. In future work, we intend to experiment with larger text corpora and larger pre-trained models to expand the capabilities of FS-StyleCap, allowing it to handle more challenging styles. Additionally, we believe that our method has the potential to support multi-modal inputs and we plan to explore this possibility as well.
%\qin{conclusions can be further improved}
