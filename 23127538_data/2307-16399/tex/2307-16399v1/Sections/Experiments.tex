
\section{Experiments}
\begin{table*}[ht]
\caption{We compare our proposed \textbf{FS-StyleCap} with other methods on SentiCap. w.r.t. B-3 (BLEU-3), M (METEOR), C (CIDEr), CLIPS (CLIPScore), sACC (style accuracy), GM1 (geometric mean of sACC and CIDEr), GM2 (geometric mean of sACC and CLIPS). Full methods are trained using the entire text-only corpus with a style label; in the Few-Shot setting, CapDec is trained with the examples, while our model directly applies them to guide the generation process. The number of samples is displayed in the ``Data'' column. Methods marked with an asterisk (*) can handle multiple styles with a single model, but only our model is capable of handling arbitrary styles. We \sethlcolor{bestresult}\hl{color} the best overall scores in Few-Shot setting.}
\vspace{-8pt}
\label{table:senticap}
\fontsize{7.5}{13}\selectfont
\centering
%\begin{tabular}{ccc|c|ccccccc|c|ccccccc}
%\toprule
%\multirow{3}{*}{} & \multirow{3}{*}{\textbf{Model}} & \multirow{3}{*}{\textbf{\begin{tabular}[c]{@{}c@{}} \scriptsize Any\\ \scriptsize Style\end{tabular}}} & \multicolumn{8}{c|}{\textbf{Positive}} & \multicolumn{8}{c}{\textbf{Negative}} \\ \cline{4-19} &  &  & \textbf{\scriptsize Labeled} &  \textbf{B-3} & \textbf{M} & \textbf{C} & \scriptsize{\textbf{CLIP-S}} & \textbf{CLS} & \multicolumn{1}{c}{\textbf{\begin{tabular}[c]{@{}c@{}}GM\\ (C,CLS)\end{tabular}}}  & \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}GM\\ (CLIP-S,CLS)\end{tabular}}} & \textbf{\scriptsize Labeled} & \textbf{B-1} & \textbf{B-3} & \textbf{M} & \textbf{C} &  \scriptsize{\textbf{CLIP-S}} & \textbf{CLS} & \textbf{GEOMEAN} \\
\begin{tabular}
{cc|c|ccccccc|c|ccccccc}
\toprule
\multicolumn{2}{c|}{} & \multicolumn{8}{c|}{\textbf{Positive}} & \multicolumn{8}{c}{\textbf{Negative}} \\ \midrule
  &  & \multicolumn{1}{c|}{} & \multicolumn{4}{c}{\cellcolor[HTML]{F1F1F1}\textbf{\textit{Content}}} & \cellcolor[HTML]{F8F8F8}\textbf{\textit{Style}} & \multicolumn{2}{c|}{\cellcolor[HTML]{F1F1F1}\textbf{\textit{GEOMEAN}}} & \multicolumn{1}{c|}{} & \multicolumn{4}{c}{\cellcolor[HTML]{F1F1F1}\textbf{\textit{Content}}} & \cellcolor[HTML]{F8F8F8}\textbf{\textit{Style}} & \multicolumn{2}{c}{\cellcolor[HTML]{F1F1F1}\textbf{\textit{GEOMEAN}}} \\
\multirow{-2}{*}{} & \multirow{-2}{*}{\textbf{Model}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data \end{tabular}}}} & \textbf{B-3} & \textbf{M} & \textbf{C} & {\textbf{CLIPS}} & \textbf{sACC} & {\textbf{GM1}} & \multicolumn{1}{c|}{{\textbf{GM2}}} & \multicolumn{1}{c|}{\multirow{-2}{*}{\textbf{\begin{tabular}[c]{@{}c@{}}Data\end{tabular}}}} & \textbf{B-3} & \textbf{M} & \textbf{C} & {\textbf{CLIPS}} & \textbf{sACC} & {\textbf{GM1}} & {\textbf{GM2}} \\

 \midrule
%\multicolumn{1}{c|}{\multirow{6}{*}{\rotatebox{90}{Few-Shot}}} & CapDec \cite{nukrai2022capdec} &  1 &  14.3 & 14.5 &	43.5 &		62.3 &	78.0 &	58.3 &	69.7 & 1 	& 13.6	& 14.5 &	47.2 &		66.3	& 75.0 & 59.5 &	70.5 \\
%\multicolumn{1}{c|}{} & CapDec \cite{nukrai2022capdec} &  5 &  16.0 & 14.9 & 48.2	 &	63.7 &77.8 &	61.2 & 70.4 & 5 	& 14.6	 &	15.1 &	50.0 & 67.2	& 74.7 & 61.1 & 70.9 \\
\multicolumn{1}{c|}{\multirow{4}{*}{\rotatebox{90}{Few-Shot}}} & CapDec \cite{nukrai2022capdec} &  100  & 16.8 & 17.4 & 52.9	& 63.9 & 79.5 & 64.9 & 71.3 & 100 	& 14.1 & 14.5 & 48.8 & 64.5 & \textbf{76.9} & 61.3 & 70.4\\
\multicolumn{1}{c|}{} & \textbf{FS-StyleCap*} &  1  & \textbf{19.2} & 17.3 & 65.3 & \textbf{73.7} & 70.8 & 68.0 & 72.2 & 1  & 20.4 & 17.6 & 71.6 & 74.3 & 64.1 & 67.7 & 69.0\\
\multicolumn{1}{c|}{} & \textbf{FS-StyleCap*} &  5 & 18.8 & \bf 18.0 & \textbf{65.6} & 73.5 & 72.3 & 68.9 & 72.9 & 5  & \textbf{20.6} & \textbf{17.9} & \textbf{71.7} & \textbf{74.6} & 67.5 & 69.6 & 71.0\\
\multicolumn{1}{c|}{} & \textbf{FS-StyleCap*} &  100  & 17.7 & 17.9 & 62.8 & 72.7 & \textbf{82.8} & \textbf{\sethlcolor{bestresult}\hl{72.1}} & \textbf{\sethlcolor{bestresult}\hl{77.6}} & 100  & 20.0 & 17.3  & 68.8 & 74.4 &75.4  & \textbf{\sethlcolor{bestresult}\hl{72.0}} & \textbf{\sethlcolor{bestresult}\hl{74.9}} \\
\midrule
\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\rotatebox{90}{Full}}} & StyleNet \cite{gan2017stylenet} &  2,994   & 12.1 & 12.1 & 36.3 & - & 45.2 & 40.5 &- & 2,991  & 10.6 & 10.9 & 36.6 & - & 56.6 & 45.5 & - \\
\multicolumn{1}{c|}{} & MSCap* \cite{guo2019mscap} &  2,994 & 16.2 & 16.8 & 55.3 & - & \textbf{92.5} & 71.5 & - & 2,991  & 15.4 & 16.2 & 51.6 & - & \textbf{93.4} & 69.4 &- \\
\multicolumn{1}{c|}{} & CapDec \cite{nukrai2022capdec} &  2,994  & \textbf{26.9} & \textbf{20.0} & \textbf{64.5} & \textbf{72.4 }& 87.0 & \textbf{74.9} & \textbf{79.4} & 2,991  & \textbf{19.9}	& \textbf{19.0} &	\textbf{65.8} &	\textbf{71.5} &	79.7 & \textbf{72.4} & 	\textbf{75.5} \\
\bottomrule
\end{tabular}

\end{table*}


\subsection{Datasets}
%\paragraph
\noindent\textbf{\textit{MSCOCO}} \cite{lin2014microsoft}. For image captioning task, we leverage the popular MSCOCO datatset. We apply the Karpathy splits \cite{karpathy2015deep} which contains 113,287 images for training, 5,000 images for validation, and  5,000 images for testing. Each image contains 5 captions. 

\noindent\textbf{\textit{MSR-VTT}} \cite{xu2016msr}. For video captioning task, we leverage the MSR-VTT dataset, which includes 6,513 videos for training, 497 videos for validation, and 2,990 for testing. Each video contains 20 captions. 

\noindent\textbf{\textit{Personality-Captions}} \cite{shuster2019engaging}. Our unlabeled textual training corpus comes from the Personality-Captions dataset, which contains sentences conditioned on possible personality traits. We also add 1,000 factual descriptions, enabling our style extractor to handle factual style. During training, we remove the style labels and only use pairs of adjacent sentences from each personality set as input samples. We pre-process all texts to meet the normalized format \cite{li2018delete}, resulting in 136,894 training and 3,683 validation examples.

\noindent\textbf{\textit{SentiCap}} \cite{mathews2016senticap}. To evaluate the results of stylized image captioning, we apply the test set of SentiCap, a sentimental image captioning dataset developed from the MSCOCO dataset. Each image in the dataset is labeled with three positive and three negative captions. The positive and negative subsets have 998/673 and 997/503 images for training/testing, respectively.

\subsection{Implementation Details}
In our image captioning experiments, we extract image features using the CLIP (ViT-B/32) model \cite{radford2021clip}. For video captioning experiments, we follow \citet{tang2021clip4caption}, using TSN sampling to sample at most 20 frames from the video. We then encode these frames using a pre-trained CLIP (ViT-B/32) video encoder \cite{luo2022clip4clip}, and concatenate the outputs to form our visual input. During training, we apply T5-base as our pre-trained language model, which is fine-tuned for 20 epochs, with a batch size of 128. The learning rate for visual projection module is set to $1e-3$, while the others are set to $5e-4$. During inference, we sample 100 positive/negative examples from the training set of SentiCap to guide the stylized generation. We conduct experiments for multiple styles using 1 and 5 examples respectively. The examples are presented in our supplementary material.

\subsection{Baseline Models}
%\subsubsection{Stylized Image Captioning}
Stylized Image Captioning baseline models for comparison include:

\noindent\textbf{\textit{StyleNet}} \cite{gan2017stylenet}. This model proposes to learn different groups of matrices to capture factual and stylized knowledge.

\noindent\textbf{\textit{MSCap}} \cite{guo2019mscap}. This method is able to handle different predefined styles with a single model. They propose an adversarial network to improve the overall performances. 

\noindent\textbf{\textit{CapDec}} \cite{nukrai2022capdec}. This model applies the image features encoded by a pre-trained CLIP model and fine-tunes their text decoder from GPT-2 (large) \cite{solaiman2019release}, which achieves state-of-the-art performance on stylized image captioning. We compare our results with their model which is fully trained on the stylized corpora, as well as their model trained with only a few sentences.

%\subsubsection{Stylized Video Captioning}
For Stylized Video Captioning, the Sentimental Transformer \cite{wu2023sentimental} is used as the baseline, which integrates information from multiple modalities and incorporates prior sentimental knowledge to generate sentimental video descriptions. For fair comparison, we compare our model's results with their results when the audio modality is not used.

\begin{table}[t]
\caption{Automatic evaluation results on MSCOCO. The Factual method is specifically trained for factual image captioning, while the Stylized methods are capable of generating factual or stylized captions, and the best results are bolded.}
\vspace{-8pt}
\label{table:mscoco}
\centering
\fontsize{7.5}{11}\selectfont
\begin{tabular}{c|c|cccc}
\toprule
 & \textbf{Model} & \textbf{BLEU-4} & \textbf{METEOR} & \textbf{CIDEr} & \textbf{SPICE} \\
 \midrule
Factual & ClipCap \cite{mokady2021clipcap} & 33.53 & 27.45 & 113.08 & 21.05 \\
%\& CNN+RNN & 23.80 & 22.37 & 77.27 & 15.36\\
\midrule
\multirow{4}{*}{Stylized} & StyleNet \cite{gan2017stylenet} & 21.18 & 20.47 & 66.42 & 13.54\\
 & SemStyle \cite{mathews2018semstyle} & 23.79 & 21.91 & 76.97 & 15.75\\
 & DetachAttach \cite{tan2022detach} &  20.43 & 21.84 &  76.91 & 16.21\\
 & \textbf{FS-StyleCap} & \textbf{30.38} & \textbf{26.76} & \textbf{105.18} & \textbf{19.00}\\
\bottomrule 
\end{tabular}
\vspace{-3pt}
\end{table}

\subsection{Evaluation Metrics}
%\subsubsection{Automatic Evaluation}

Our \textbf{automatic evaluation metrics} for Few-Shot SVC consider two aspects:
%Our evaluation considers two aspects: 
the ability to generate fluent and relevant descriptions for visual content, and the ability to express in a target style.

Following previous methods \cite{guo2019mscap,li2021similar,tan2022detach}, we evaluate the sentence relevancy and fluency using widely used metrics \footnote{As in previous works, we utilize the open-source evaluation code from: https://github.com/jmhessel/pycocoevalcap.}, 
 including BLEU \cite{papineni2002bleu}, METEOR \cite{banerjee2005meteor}, and CIDEr \cite{vedantam2015cider}. We also report the perplexity scores in the supplementary material, which are evaluated by SRILM \cite{stolcke2002srilm}. In addition, we compute the CLIPScore \cite{hessel2021clipscore} to show the visual relevance, which measures the similarity between an image and its stylized caption.
To evaluate the style accuracy (sACC), we apply a pre-trained style classifier following previous works \cite{tan2022detach,riley2020textsettr}. This classifier is fine-tuned from a BERT-base model \cite{devlin2018bert}, using the training set of SentiCap. It achieves an accuracy of $98\%$ on the validation set.
Moreover, we follow previous works \cite{tan2022detach,hu2022text} to evaluate  the \textbf{overall performance} using the geometric mean score of semantic relevance and style accuracy, i.e. the geometric mean score of sACC and CIDEr.% by the geometric mean score of CLS and CIDEr.
%\paragraph{\textbf{Fluency}} As in previous methods \cite{tan2022detach}, we apply the SRILM \cite{stolcke2002srilm} on SentiCap to evalute the fluency of the generated captions.
%\subsubsection{Human Evaluation} 

In order to measure the effectiveness of our model to handle multiple styles, we also perform \textbf{human evaluation} in term of two aspects: (1) Visual Relevance that measures relationship between the stylized captions and source image. (2) Style appropriateness which means how well the descriptions express the target style specified by the example sentences. Following the standard in \cite{guo2019mscap,tan2022detach}, relevance is rated from 0 (unrelated) to 3 (very related), and style appropriateness from 0 (bad) to 3 (perfect). We conduct experiments on three models with six styles, and sample $5\%$ of the images from the test set of SentiCap. This results in $30 \times 6 \times 3$ Image-Stylized Caption pairs for human evaluation. %be evaluated. %For details and results, please see Section \ref{sec:human_eval}.

