\section{Introduction}
Traditional visual captioning \cite{guo2020normalized,hu2022scaling,wang2022git} aims to describe visual content with objective and neutral explanations, denoted as factual captions. %generate objective and neutral descriptions of visual content,  
However, recent research \cite{gan2017stylenet,mathews2016senticap,bin2021multi} has demonstrated that incorporating style into visual captioning can greatly enhance the user experience. By adding style, the captions become more attractive and emotionally appropriate. For example, ``The broccoli added with the chicken makes this the best pizza!'' is more appealing than ``A pan of pizza with broccoli and cheese''. Therefore, Stylized Visual Captioning ~\cite{mathews2016senticap,gan2017stylenet,wu2023sentimental} has attracted increasing research interest.

% Figure environment removed

Previous stylized visual captioning works can be divided into two categories: supervised and unsupervised methods. Supervised methods use a parallel training set to learn stylized descriptions for visual inputs. These methods are limited in the range of style types covered by existing datasets \cite{mathews2016senticap, gan2017stylenet}. As it is time-consuming and expensive to manually compose paired stylized captions for large-scale images or videos, most works have shifted toward unsupervised learning \cite{mathews2018semstyle,tan2022detach}, relying on an unpaired style-specific training corpus and a factual vision-caption dataset. Although these methods eliminate the need for paired stylized data, they are restrained by the pre-specified style labels of the stylized-text corpora. In other words, to generate descriptions with a new desired style, the models need to be trained or fine-tuned on a corresponding stylized-text corpus.


In this work, we focus on a more practical formulation of stylized visual captioning, namely Few-Shot Stylized Visual Captioning (\textbf{Few-Shot SVC}). As illustrated in Figure \ref{fig:task}, Few-Shot SVC aims to describe images or videos in any target style specified by a few examples during inference time. Our method directly extracts the text style from user-supplied examples and guides caption generation to meet the desired style. It even eliminates the need for training on a curated text-only corpus with annotated style attributes. Although more challenging than previous works, Few-Shot SVC provides significantly more flexibility.

Few-shot SVC faces two main challenges: (1) extracting style information from a small amount of example sentences, and (2) generating stylized descriptions that are compatible with both style information (textual modality) and visual content information (visual modality).
To address these challenges, we propose a framework called \textbf{FS-StyleCap}, which is trained on a text-only corpus without style labels, as well as a factual vision-caption dataset. This framework utilizes a conditional encoder-decoder language model that extracts style representations to guide the generation of stylized captions. Additionally, it employs a visual projection module to enable cross-modal alignment. We apply a two-step training scheme. First, we use the unlabeled text-only corpus to train a style extractor that can extract style representations from textual inputs. The extractor is fine-tuned from the encoder of a large-scale language model \cite{raffel2020exploring} with a strong ability to represent  text.
%which can be further separated into style representation and content representation. 
Inspired by TextSETTR~\cite{riley2020textsettr}, we assume that two adjacent sentences from the unlabeled corpus possess the same style. We train our model to extract a style vector from the first sentence and use it to perform reconstruction tasks on the second sentence. To further improve the quality of the style vector, we introduce a style-related loss, computed by a style discriminator that has been trained using contrastive learning \cite{hadsell2006dimensionality}. In the second stage, we freeze the style extractor, and address the problem of cross-modal alignment. As illustrated in Figure \ref{fig:frame}, we project high-level visual features into the joint embedding space, and then extract visual content vectors. The caption generator fuses the visual content vectors and a text style vector to generate a stylized description. Multi-task training strategy is applied to ensure the capacity for style injection and modality alignment. In detail, we simultaneously train on visual captioning tasks using the factual vision-caption dataset, and on text style injection tasks using the unlabeld style corpus. During inference, we extract the desired style vector from a few examples and use it to guide the generation of stylized visual captions.

In summary, the contributions of this work are as follows:
\parskip=0.1em
\begin{itemize}[itemsep=0.2em,parsep=0em,topsep=0em,partopsep=0em,leftmargin=1em,itemindent=0.2em]
\item To the best of our knowledge, this is the first work to tackle Few-Shot Stylized Visual Captioning. This task uses a few example sentences to guide the generation of stylized captions, without requiring further training.
\item We propose a framework called FS-StyleCap, which extracts the style representation from text input and aligns visual information with the text style vector to generate stylized captions.
%\item Our framework leverages the ability of large-scale pre-trained models, and can be easily improved with more effective models.
\item The automatic evaluation results of sentimental visual captioning outperform state-of-the-art models with few-shot stylized examples. Our approach even achieves comparable results with models that are fully trained on labeled style corpora.
\item As verified by human evaluation, our method demonstrates strong performance on multiple style attributes, with only 1-5 example sentences as guidance. 
\end{itemize}