% \todo{
% Reviewer 3: but Iâ€™d argue the authors need to make a case for how they envision improvisational augmented video being created, and the different creative opportunities it offers over prior work.
% }
\section{RealityCanvas}
\subsection{Overview}
Based on the above design space, we designed and developed \system{}, an AR sketching tool to augment a captured real-world interaction through responsive scribble animation effects. 
As a basic setup, \system{} uses a mobile device and a standard RGB camera. Therefore, the user can sketch elements with a finger, a pen, or a mouse on smartphones, tablets, and laptop computers.
The \system{} system uses computer vision for object tracking and interaction detection, allowing it to work with either live or recorded videos. 
For recorded videos, users can pause the video to sketch and define animations, which then start animating when the video is resumed. For live videos, users can embed animations onto a real-time video stream. In our setup, a performer can see the live sketched animation through an external monitor connected to the tablet, similar to~\cite{saquib2019interactive, liao2022realitytalk}, and collaborate with the user to create a live performance that can later be saved as a video. All sketched elements are displayed as 2D objects on a screen, without any depth effect based on camera movement. 
The system is developed using web technology and a live demo is available on our website~\footnote{\href{https://ilab.ucalgary.ca/realitycanvas}{https://ilab.ucalgary.ca/realitycanvas}}. To make AR sketched animation accessible to everyone, we will release the source code as an open-source project~\footnote{\href{https://github.com/ucalgary-ilab/realitycanvas}{https://github.com/ucalgary-ilab/realitycanvas}}.

\subsection{Two Key Design Goals}
\system{} is designed with two key goals in mind: \textit{expressiveness} and \textit{improvisation}. \textbf{\textit{Expressiveness}} refers to the range of animation effects that the system can provide, and we have designed six types of sketch animation effects (A1-A6 below) to cover a wide range of expressions, informed by the design space analysis of existing scribble animations. This allows for open-ended animation drawing, which was not possible with prior systems~\cite{suzuki2020realitysketch}. 
\textbf{\textit{Improvisation}}, on the other hand, enables users to spontaneously create animation \textit{in real-time} without prior preparation or planning, as opposed to prior systems~\cite{saquib2019interactive, perlin2018chalktalkvrar}. This is achieved through both freehand drawing and immediate response, which are crucial not only for faster creation but also for unexpected discoveries through playful exploration and experimentation. This design is guided by Victor's principle (\textit{``creators need an immediate connection to what they are creating''}~\cite{victor2012inventing}), and we believe that the support of improvisation enables the users to engage with this new medium to generate new and original ideas they may not have thought of before.

\subsection{Authoring Workflow}
At a high-level, \system{} provides the following three-step workflow: 1) object selection and tracking, 2) sketch elements, and 3) animate sketched elements.

\ \\
\subsub{Step 1. Object Selection and Tracking}
To create embedded and responsive animations, the sketched elements need to be tightly coupled with the body, objects, and/or environments. 
Thus, capturing and tracking an object is the first step to making the sketches responsive to physical motion.
To track an object, the user can simply pause the video input or ask the performer to hold still. Then, the user can select a point of an object (based on color tracking) or a part of a body (based on skeleton tracking).
Once selected, the system \removed{keeps tracking} overlays a green dot  on the selected point to indicate the tracked point on the body or object.

\ \\
\subsub{Step 2. Sketch Elements}
Once the user selects a tracking point, they can start drawing a sketched element.
The user can simply sketch any shape by freehand drawing in the scene.
The user can also change the color, thickness, and opacity of the \removed{pen} stroke when drawing.

\ \\
\subsub{Step 3. Animate Sketched Elements}
The key feature of \system{} is the animation process. 
Informed by the design space exploration, the system provides six different ways to animate the sketched elements through direct manipulation.
\begin{enumerate}
\item[\textbf{A1.}] \textbf{Object Binding:} Bind the sketched element to a \removed{tracking point and move its position} tracked point on an object or body part
\item[\textbf{A2.}] \textbf{Flip-book Animation:} \removed{Add new} Add multiple frames of sketches to create a flip-book animation effect
\item[\textbf{A3.}] \textbf{Action Trigger:} Define the trigger to specify when the sketched elements appear
\item[\textbf{A4.}] \textbf{Particle Effects:} Draw a line to spawn many cloned elements
\item[\textbf{A5.}] \textbf{Motion Trajectory:} Specify a tracked point to show the path by cloning elements
\item[\textbf{A6.}] \textbf{Contour Highlight:} Select the object to highlight its contour with a sketched line
\end{enumerate}
In the following, we describe each sketched animation technique in more detail. 


\subsection*{A1. Object Binding}
Once the user selects an object to be tracked, the user can start sketching an element.
By default, the sketched elements are automatically bound to the selected tracking point, so that the sketched element starts moving when the tracked object moves. 
For example, in Figure~\ref{fig:object-binding}, the user selects a shoulder as a tracking point, then draws a violin around the tracked point, so that the sketched violin starts moving when the shoulder moves.
In the same way, the user tracks a right hand and binds a sketched violin bow; then, the bow also moves based on the user's hand movement.

% Figure environment removed

This animation technique is useful for \textit{\textbf{object binding}}, in which the sketch is bound to a body or object to augment the real-world scene. 
In this simple object binding, the sketched element itself is static and only its position moves and its orientation \removed{or}and scale do\removed{es} not change.

\subsection*{A2. Flip-Book Animation}
To make more expressive animation, the system allows the user to create \textit{\textbf{flip-book animation}} effects.
In this animation technique, the user can draw an additional sketch as a new frame, and the system shows these sketches one by one to create an animation. 
For example, in Figure~\ref{fig:flip-book-animation}, the user first selects a red cup, then draws a circle around the selected point
while the performer holds still in live mode. Similarly, the user can pause the recorded video at a specific frame to sketch the circle. 
Then, the user selects the add a new frame button so that they can draw the next shapes for frame-by-frame animation. 
Once the user finishes drawing all frames, the user clicks the save button to apply the flip-book animation. 
In this way, the user can create an animated circle whose size appears to change.
The drawn animation is also bound to the tracked object, so that the animated circle moves when the tracked object (red cup) moves.
% the images appear to animate by simulating motion or some other change.


% Figure environment removed


\subsection*{A3. Action Trigger}
The above two animation techniques are always-visible animations.
On the other hand, \textit{\textbf{action trigger}} animations allow the user to create animations that appear only when a certain action occurs. 
To use this feature, the user can simply track multiple points and specify the user-defined trigger action.
In our system, the user can define the trigger based on the distance between two points.
For example, in Figure~\ref{fig:action-trigger}, the user first tracks left and right hands, and then the user draws a bumping sketched effect.
Once the user clicks the trigger button, then the system continuously tracks the distance between the two points. 
By default, the system triggers the sketched bumping effect when the distance between the two points decreases below a threshold of a user-defined pixel distance \removed{(based on a pixel distance on a screen)}.
The user can adjust the trigger parameters using a slider to determine the distance threshold value and a button to change the triggering direction. By toggling the direction, the user can trigger the action when the distance between two points increases rather than decreases. This allows for greater flexibility in defining the trigger conditions.

% use a maximum distance threshold rather than a minimum (i.e., ).

% Figure environment removed

By combining this with the above flip-book animation effect, the user can create even more expressive animations based on human action.
For example, Figure~\ref{fig:teaser} illustrates how the user can show a water splashing effect when the actor stomps the ground.
In this case, the user can select the left foot and the ground as triggering points using both body tracking and object tracking, then draw flip-book animations \removed{multiple frames} as trigger action effects. 
In this way, the user can create a variety of animations based on the user's action.


\subsection*{A4. Particle Effects}
The system also lets the user create \textit{\textbf{particle effects}} through improvisational sketching interactions.
To create particle effects, the user first sketches an element of the particle, such as a raindrop in Figure~\ref{fig:teaser} or snowflake in Figure~\ref{fig:particle-effects}.
Then, the user selects the tracked object and clicks the emit button, which allows the user to draw an \textit{emitting line} to spawn new elements for particle effects.
As shown in Figure~\ref{fig:particle-effects}, the same snowflake sketches are randomly emitted from the emitter line for the snow effects. The system offers two options for customizing the particle effect parameters: 1) the user can draw a \textit{motion path} to specify the path each particle follows, and 2) the user can change the \textit{speed} of particle movement using a slider in a menu. These options were inspired by the interactions presented in \textit{Draco}~\cite{kazi2014draco}. 

% Figure environment removed

Our interaction technique is inspired by Draco~\cite{kazi2014draco}'s kinetic texture and our tool also supports similar interactions, such as allowing the user to draw a trajectory of the emission so that the user can control the path the emitted particles follow.
But unlike Draco, which focuses on screen-based interaction, the drawn emitter line in \system{} is bound to the selected physical object so that if the object moves, the emitter line follows along. This enables a broader interaction space, such as a magic wand particle effect from a physical stick or showing an air flow bound to a dryer.


\subsection*{A5. Motion Trajectory}
In object binding, the sketched element simply moves and follows the tracked point, but by duplicating the sketched element for each tracked position, the system can also create a \textit{\textbf{motion trajectory}} like ghost effect of the sketched elements.
For example, in Figure~\ref{fig:motion-trajectory}, the user selects a left hand as a tracked point and sketches a simple dot. 
When the user clicks the motion button, the dots become a line to show the trajectory of the hand movement.
With a certain time period (by default 30 elements), the cloned elements start disappearing, so that the motion trajectory disappears with a certain length.

% Figure environment removed

This animation technique is useful when the user wants to visualize the trajectory of a tracked object, such as with a dancing motion or throwing a basketball.
The user can also change the parameters of the motion trajectory, such as how fast the elements should disappear, or the opacity or scale of each cloned element.
By leveraging these parameters, the user can also create more expressive motion effects.


\subsection*{A6. Contour Highlight}
Finally, the system lets the user create \textit{\textbf{contour highlight}} effects for a tracked body or object.
To do so, the user first selects the contour button and then taps the body or object the user wants to highlight.
By default, the enclosed object or body is highlighted with a drawn line.
This line can be changed according to the body motion or object movement, as \system{} always tracks and displays the outermost object contour seen in Figure~\ref{fig:contour-highlight}.
In a similar way, the user can also create animated contour highlights by selecting the appropriate option. 
For example, in Figure~\ref{fig:contour-highlight}, the user can also show an animated contour line around the body. 

% Figure environment removed

The user can also fill in the enclosed object. 
For example, the user can change the fill color of the body by combining the action-trigger effect. 
The user first selects the foot and defines the trigger based on the foot location, the user can create the highlighting effect along with the animated elements.

\subsection{Implementation Details}

\subsubsection{Sketching Interface}
\system{} uses HTML Canvas as the main sketching interface.
Since most of the sketched elements are 2D, we decided to embed sketches onto the 2D canvas screen. 
All sketched elements are drawn in SVG format on the canvas. 
The system also uses Konva.js and Anime.js as supporting graphics and animation libraries, respectively.
All elements are calculated and rendered as 2D objects.
For example, the distance between two tracked points is based on the pixel value on the screen, rather than the physical distance. 
Therefore, \removed{the }depth effects and \removed{or} 3-dimensional rotation \removed{is}are currently not supported (although the user can still draw sketches which look 3-dimensional using perspective effects).


% Figure environment removed

\subsubsection{Object Tracking}

For object tracking, we use a simple color-based tracking method that has been shown to provide fast and reliable real-time tracking for AR animation applications~\cite{suzuki2020realitysketch}.
In our current implementation, the system uses OpenCV JS to track objects based on their RGB values, with a certain threshold to filter out pixels outside of the specified range.
The system first obtains the RGB value based on the selected X-Y point by sampling the pixel at the corresponding coordinates in the video frame.
The system then filters out all pixels outside the specified RGB range. Then the system identifies the largest continuous contour as the tracked object. \removed{captures the largest contour object given the}. The RGB value range is ($r\pm10$, $g\pm10$, $b\pm10$), where ($r, g, b$) is the sampled RGB value of the selected point.
Given the largest contour, we obtain the center point of the object, which is used to determine the object's location in the video frame. 
This information is then used to track the object's movements over time and update its position on the screen accordingly. We evaluated the robustness of our color tracking system \added{by tracking a reflective object (a fake apple) and a non-reflective object (a tennis ball) under four different lighting conditions:}

\added{
    \begin{enumerate}
        \item Dim artificial lighting: Simulated using a single light source in the living room.
        \item Bright artificial lighting: Achieved by activating four light sources in the living room.
        \item Dim natural lighting: Simulated by creating subdued sunlight conditions in the living room.
        \item Bright natural lighting: Achieved by exposing the living room to direct sunshine.
    \end{enumerate}
}
\added{ The success of the tracking was determined by the accurate movement of a green dot, indicating successful object tracking, while failure was identified when the green dot failed to follow the object.} We found that the system performs more accurately in bright lights than in dim lighting. In dim artificial light, the system successfully tracked the object 5 times and failed to track it 5 times. However, in bright artificial light, it successfully tracked the object 8 times and failed to track it only 2 times. We also discovered that the mesh of an object can impact the accuracy of color tracking in bright light. Objects with reflective mesh in bright lighting often result in a high false negative rate. Our system performed best when the objects had distinct solid colors.


\subsubsection{Body Tracking}
In our current implementation, the system tracks body parts based on MediaPipe.
MediaPipe provides 33 body points based on the human skeleton. When the user clicks a point on the screen, the system identifies and \removed{keeps tracking}continuously tracks the closest body point among the 33 body points based on X-Y value.  
\removed{Additionally, we also use the same MediaPipe library for hand skeleton and facial anchors to track points on the hand and face when visible.} 
For body segmentation, we used MediaPipe and OpenCV JS to obtain the masked shape of the body. 
Based on the masked image, we convert the body shape into the simplified SVG path for contour line animation in each frame when the contour highlight is enabled.




\subsubsection{Prototype Setup}
Since all of the components including the color tracking and body tracking are client-side systems, we do not have any server to run the system.
During the prototyping phase, we tested with several devices, including Pixel 3 XL, Pixel 6, Linux Desktop, Windows machine, and Android tablet. (Due to the Safari browser's restriction, the iPhone/iPad did not work, although this problem should be resolved in the future.)
For the demo and user evaluation, we used Samsung Tab S8 Ultra Android Tablet (Display: 14.6-inch, CPU: Qualcomm SM8450 Snapdragon 8 Gen 1, GPU: Adreno 730, RAM: 12GB) with a built-in stylus pen.
In this setup, the system achieved the targeted 60 FPS at all times \added{with the tablet stock camera} during our testing for both color tracking and body tracking. \added{However, we observed a decrease in frame rate when using an external high-resolution camera.}

