
\section{Limitations and Future Work}
On top of the feedback and suggestions from the usability study and expert interview, we also see many potential future work directions to expand our approach.


% \todo{
% Reviewer 1: While I really like parts of Sec. 8, I feel it is ultimately too long. I would encourage the authors to prioritize 3-4 future work directions and only write about those.

% Reviewer 3: finally, I thought the end of the paper could have discussed more issues related to insights into how the integration of live video and sketch-based drawing could support expressive and improvisational works. Right now the discussion focuses primarily on technical limitations of the system or new hardware applications, which is fine, but not really related to the original goals of the system.
% }

% \todo{Reviewer 2:  "I think there is a trade-off here between preparation-based and improvisational authoring tools. The authors can discuss this pointer deeper"
% }


\subsubsection*{\textbf{3D Sketched Animation with HMDs}}
Our current implementation uses screen-based mobile AR, but integration with head-mounted displays (HMDs) like Microsoft Hololens or Meta Quest Pro will allow more blended and immersive experiences through mid-air drawing like \textit{Norman}~\cite{norman}.
However, this requires a more complex implementation, as it needs 3D object deformation, as opposed to a simple 2D line animation (e.g., \textit{Motion Amplifier}~\cite{kazi2016motion} vs. \textit{Layered 3D Animation}~\cite{ma2022layered}).
Future work should explore these 3D animations through either 3D deformation~\cite{ma2022layered}, simple frame-by-frame animations~\cite{norman}, or using 2D sketches on an invisible surface embedded in 3D space~\cite{kaimoto2022sketched}. 
% \removed{
% In this setting, the user may need to sketch with the mid-air gesture, but mid-air gestural sketching does not provide accurate and easy sketching for novice users~\cite{arora2017experimental}.
% To address this problem, future work could combine the tablet and HMDs, in which the user can sketch on the screen while the sketched element can be also seen through HMD, similar to \textit{SymbiosisSketch}~\cite{arora2018symbiosissketch} or \textit{PintAR}~\cite{gasques2019pintar}.
% By using the HMDs, the user can also easily collaborate and share with multiple people without needing to use a large display to stream the screen.
% }

% \removed{
% \ \\
% \subsub{Supporting Multi-Modal Input and Output}
% Currently, the system only supports visual modalities for both input and output, but other modalities are also possible.
% For example, the system could use sound as another input and output, for example using a specific sound as a trigger of action (e.g., show animations with the clapping sound, speech bubble animation when talking or singing) or combining the sound with a sketched animation (e.g., the sound effect for explosion or added music with the sound visualization like \textit{AR Music Visualizers}~\cite{tanprasert2022ar} and \textit{Music Bottles}~\cite{ishii2001bottles}. We believe the multi-modal input and output allow more expressive animations. 
% }

% \ \\
\subsubsection*{\textbf{Exploring More Complex and Multi-Modal Action Triggers}}
% \removed{Related to the above point,} 
Currently, the system only supports a simple image-based action trigger (e.g., the distance between two points), which limits complex animation behaviors for many use cases (e.g., storytelling, AR prototyping, etc.). 
If the system could support more complex and expressive action triggers, like the capability to define multi-modal triggers or a combination of multiple triggers, it would further expand possibilities. For example, if we could use a specific sound as a trigger, like a speech bubble animation when talking or experiences similar to \textit{Teachable Reality}~\cite{monteiro2023teachable}, \textit{AR Music Visualizers}~\cite{tanprasert2022ar}, and \textit{Music Bottles}~\cite{ishii2001bottles}). 
% \removed{
% While one of the challenges for such image-based complex actions is how to deal with the ambiguity of the triggering, if the user can easily train the certain action with machine learning (e.g., \textit{Teachable Machine}~\cite{carney2020teachable}), we expect this would be possible.
% }
% Alternatively, the user can also incorporate smartphone or IoT-based triggers, such as notifications, light bulb switches, audio speakers, etc, the system can also broaden the spectrum of interactive animations.
% To fully integrate more complex action, however, we expect new interaction techniques would need to be developed, similar to \textit{Kitty}~\cite{kazi2014kitty} for screen-based interactive animation. 

% Reviewer 2: Add 'GesturAR' here since the hand distance for manipulating sketches has been explored in GesturAR already.
% Zhijie: bibtex imported, Nov, 13.

% \ \\
\subsubsection*{\textbf{Leveraging Real-time Parameters}}
In previous works of real-time responsive sketches like \textit{RealitySketch}~\cite{suzuki2020realitysketch}, \textit{Interactive Body-driven Graphics}~\cite{saquib2019interactive}, \textit{GesturAR}~\cite{wang2021gesturar}, or \textit{Reactile}~\cite{suzuki2018reactile}, the real-world parameters are often incorporated (e.g., an idea of manipulating sketches based on a distance between hands has been explored in \textit{GesturAR}~\cite{wang2021gesturar}).
In our prototype, we did not explore this aspect except for a simple action trigger to make our interaction and system simple, but the integration of real-time parameters could enhance the expressive animation.
For example, based on the distance between two hands, the user could change the scale or sketched objects, rotate the objects, or change the number of particles in dynamic and responsive ways. 
% \todo{% Defining and binding these parameters in real-time requires a more complex workflow, but future work will investigate how }

% \ \\
\subsubsection*{\textbf{Integration with Physics Simulations and Other Pre-defined Animations}}
The way to create responsive sketched animation can be situated in the spectrum between user-defined and pre-defined animations~\cite{suzuki2020realitysketch}.
Currently, our sketched animation focuses more on the user-defined aspect, in which the user can define most of the animations on demand.
However, the integration with pre-defined animations can enrich the expression. 
For example, the system could integrate physics simulations to make the sketched animations interact with the real world, similar to \textit{Sketched Reality}~\cite{kaimoto2022sketched}.
Another pre-defined animation like character motion or pre-programmed behavior like \textit{RakugakiAR}~\cite{rakugakiar} or \textit{ChalkTalk AR}~\cite{perlin2018chalktalk, perlin2018chalktalkvrar} is another possibility to broaden the expression.
Future work can investigate this aspect to create more complex and expressive animation.

% \removed{
% \ \\
% \subsub{2D vs 3D Sketching}
% In our system, the embedded sketches are treated as 2D sketched objects on the screen, rather than 3D objects embedded in the scene.
% Due to the nature of 2D scribble animation and the simplicity of the implementation, we decided to choose 2D screen-based sketch, but 3D sketched animation should be explored in future work.
% One easy way to convert 2D sketches into 3D space is to use an invisible surface in the 3D space and sketch onto the surface. 
% This allows the sketch to be embedded in the 3D scene, while still using 2D sketches, similar to \textit{Sketched Reality}~\cite{kaimoto2022sketched} and \textit{Reactile}~\cite{suzuki2018reactile}. 
% Alternatively, the system could let the user sketch 3D lines, similar to \textit{SymbiosisSketch}~\cite{arora2018symbiosissketch}.
% However, this requires a more complex implementation for animation, as the system needs to animate through 3D object deformation, as opposed to a simple 2D line animation.
% On the other hand, 3D sketches would be necessary when integrating with HMDs.
% In the future, we are interested in exploring responsive sketched animation for 3D drawn objects. 
% }

% \removed{
% \ \\
% \subsub{More General and Robust Object Tracking}
% In our current implementation, we use simple object tracking based on the selected color matching. 
% We adapt this simple tracking method based on \textit{RealitySketch}~\cite{suzuki2020realitysketch}, which reports that color tracking provides the fastest tracking method for real-time sketched animation, compared to other sophisticated algorithms.
% However, this tracking method is not robust and generalizable enough for many situations. 
% For example, if the scene has multiple similar colored objects, the tracking may not work well.
% In the future, we expect the recent advances in computer vision to provide more general, robust, yet fast object tracking methods for our purpose, similar to the robust and fast body tracking algorithm (MediaPipe) which we used in our prototype.
% We look forward to integrating the new algorithms for future improvements. 
% }

% \removed{
% \ \\
% \subsub{Deployment and In-the-Wild User Study}
% Since our evaluation primarily focuses on the basic usability of our prototype, we hope to further explore and evaluate our system with in-the-wild user studies.
% To this end, we are interested in deploying our tool and collecting feedback from the general audience. 
% In particular, we are interested in how our system will be used in different use scenarios. 
% For example, we are interested in how novice users will use our system to create videos for their social media, and how preschool or elementary school teachers will use our system for classroom education or storytelling performance.
% Such in-depth studies would help us to gain insights about more limitations of the current features but also reveal the potential for real-time responsive sketched animation, inspiring future work. 
% }
