\input{table/mask_ft}


\section{Experimental Settings}

%We now introduce our evaluation setup, aimed at demonstrating the effectiveness of our pipeline.
% with both monocular \cite{Ranftl2022,Ranftl2021} and stereo \cite{lipson2021raft,li2022practical} networks.

\textbf{Implementation Details.}
We employ MiDaS \cite{Ranftl2022} and DPT \cite{Ranftl2021} as our monocular networks using the official pre-trained weights, %,\footnote{\url{https://github.com/isl-org/MiDaS} \\ Checkpoints: dpt\_large\_384 for DPT, midas\_v21\_384 for MiDaS}, 
given their excellent in-the-wild generalization performance. To fine-tune them, we iterate for 20 epochs with batch size 8 and a learning rate of $10^{-7}$ with exponential decay with gamma 0.95. We use random color and brightness and random horizontal flip augmentations. We pad/crop and resize images to match the pre-training resolution, i.e., 384 pixels for the long or short side, preserving aspect ratio with mirror pad or square crop, for MiDaS or DPT, respectively.
We normalize images as the original networks do.
%
Regarding stereo networks, we employ RAFT \cite{lipson2021raft} and CREStereo \cite{li2022practical}, using the official pre-trained weights, %\footnote{\scriptsize CREStereo:\url{https://github.com/ibaiGorordo/CREStereo-Pytorch}\\RAFT-Stereo:\url{https://github.com/princeton-vl/RAFT-Stereo}}, 
since they achieve the top rankings in the Middlebury dataset \cite{scharstein2014high} among published methods.
To fine-tune them, we run 20 epochs, with batch size 2, fixed learning rate $10^{-5}$. Following \cite{zamaramirez2022booster}, we randomly resize images to half or quarter of the original dataset resolution, randomly crop to 456$\times$884 and 448$\times$880 for RAFT and CREStereo respectively, and further randomly scale images and disparities by a factor $\in [0.9, 1.1]$. We assume 22 and 10 iterations during training for RAFT-Stereo and CREStereo, respectively. During testing, we run 32 and 20 iterations.
%
%When creating proxy labels with our masking strategy, we fix the random seed of color sampling to 0 in all experiments, and the number of iterations $N=5$. We did not observe any improvements with more iterations.
When creating virtual labels with our masking strategy, we fix the random seed of color sampling to 0.

\textbf{Datasets.}
Among the datasets, we selected Trans10K \cite{xie2020segmenting}, MSD\cite{Yang_2019_ICCV}, and Booster\cite{zamaramirez2022booster} as they focus on ToM surfaces and contain images acquired in many realistic environments. Trans10K contains 5\,003, 1\,003, 4\,431 images for the training, validation, and test set, respectively, featuring common transparent objects and  stuff. It provides segmentation masks with pixels categorized into 12 different classes that we collapse into 2 -- ToM (classes 1 to 11) or not. MSD contains 3\,066, and 958 images and binary segmentation masks for the training and test set, respectively, featuring mirrors. Booster contains 228, and 191 images for training and testing, respectively. The dataset provides disparity and segmentation maps for the training set, where the segmentation maps are categorized into 4 classes, which we group into 2
 -- classes 2-3 into ``ToM" category, classes 0-1 into ``Other" materials.
%
%Fine-tuning is conducted on Trans10K and MSD for monocular models, and on the Booster training split for stereo networks.
We fine-tune on Trans10K and MSD for monocular models and on the Booster training split for stereo networks, without using any depth ground truths.

\input{table/proxy}

\textbf{Evaluation Protocol.}
We evaluate the accuracy of the monocular networks using several metrics, including absolute error relative to the ground-truth value (ABS Rel.), the percentage of pixels having the maximum between the prediction/ground-truth and ground-truth/prediction ratios lower than a threshold ($\delta_i$, with $i$ being 1.05, 1.10, 1.15, 1,20, and 1.25), the mean absolute error (MAE) and Root Mean Squared Error (RMSE).
Additionally, we evaluate stereo networks using the metrics defined in Booster \cite{zamaramirez2022booster}, i.e. bad-2, bad-4, bad-6, bad-8, MAE, RMSE. 
%
Results are reported on all valid pixels (\textit{All}) or for those belonging to either ToM or other objects, in order to assess the impact of our strategy on the different kinds of surfaces.
%
For any metrics considered for stereo networks, the lower, the better -- annotated with $\downarrow$ in tables. The same applies to metrics used for monocular networks except for $\delta_i$, resulting in the higher, the better -- with $\uparrow$ being reported in tables.
%
As the predictions by monocular networks are up to an unknown scale factor, we rescale them according to the LSE criterion from \cite{Ranftl2022} defined in Eq. \ref{eq:rescaling}, yet using all valid pixels here.
%
Monocular networks are evaluated on the Booster training set, while stereo models are evaluated on the Booster test set. As for the latter, results split into  ``ToM" and ``Other" objects have been kindly computed by the Booster authors based on the segmentation classes we defined. %upon reception of  %via private communications.

\section{Experiments}
%In this section, we report the outcome of our findings. 
% on different Depth Proxy Generation methods and the use of Semantic Proxy Masks on both Monocular and Stereo Depth Estimation.


% Figure environment removed

\subsection{Monocular Depth Estimation.}

\textbf{Number of In-Paintings.}
We investigate the quality of the virtual depth labels by varying $N$.
When using $N=1$ we generate a single in-painted image that is forwarded to the monocular network, while with $N=5$ we generate virtual depths from 5 masked images with different colors which are then aggregated by selecting the pixel-wise depth median.
%two major in-painting strategies have been explored: i) \textit{Single} in-painting, in which a single masked image is generated with a color and then forwarded to the monocular depth network; ii) \textit{Virtual Depth} in-painting, in which $N=5$ masked images are generated with different colors, that are then aggregated by selecting the median of the predictions for each pixel.
In Tab. \ref{tab:ablation_mask_generalization}, we report the accuracy of depth maps produced by the two strategies, together with those of the \textit{Base} architectures, i.e. without applying any in-painting strategy.
Firstly, with both MiDaS and DPT, both in-painting strategies obtain virtual depths that are much more accurate for ToM regions w.r.t. the \textit{Base} architecture.
Secondly, \textit{N=5} maps yield slightly better results in most metrics, especially when looking at DPT performance.
We ascribe it to the higher robustness of the second strategy.
For the remaining experiments, we fix $N=5$ as we did not observed any further improvement with larger values. %to produce proxy depths.

\textbf{Fine-tuning Results (GT Segmentation).}
In Tab. \ref{tab:finetuning_mono}, we report results on the Booster train set, obtained after fine-tuning MiDaS and DPT on all available data from Trans10K and MSD. In the \textit{Base} row, we report the results of the network using the officially released weights without any further training, and we compare with those in row \textit{Ft. Virtual Depth}, i.e., the results of our method.
We notice that the accuracy on \textit{All} pixels is improved with our approach. In particular, we achieve a significant boost in performances for ToM surfaces, of 4.37, 5.72, 8.97, 11.12 and 11.57\%, 28.67mm, 0.03\%, 35.51mm for MiDaS \cite{Ranftl2022}, and 3.91, 7.19, 11.25, 16.8 and 16.97\%, 42.46mm, 0.04\%, 53.255mm for DPT\cite{Ranftl2022} in the $\delta_{1.25}$, $\delta_{1.20}$, $\delta_{1.15}$, $\delta_{1.10}$, $\delta_{1.05}$, MAE, Abs.Rel, and RMSE, respectively. 
We highlight that, after fine-tuning, the accuracy on \textit{ToM} is only slightly worse than on \textit{Other}.
Moreover, class \textit{Other} metrics are also slightly better, probably because of the enhanced features extracted by the network, which has a better understanding of the scene context.
Finally, we have reported in \textit{Ft. Base} the fine-tuning results obtained by self-training the networks on their own predictions without any in-painting strategy. As expected, without the appropriate virtual depth labels, the networks cannot effectively learn from the new dataset, yielding results comparable to the \textit{Base} architecture.
Experiments on additional datasets are in the supplement.

\input{table/stereo}
% Figure environment removed

\textbf{Fine-tuning Results (Proxy Segmentation).}
Even though obtaining semantic labels is cheaper than collecting depth ground truths, using the predictions of a segmentation network as proxy semantic annotations would accelerate the dataset collection process. Thus, we investigate the impact of replacing manually annotated masks in our pipeline with the predictions of %how much noise in the semantic maps influences our learning procedure by running 
Trans2Seg\cite{xie2020segmenting} and MirrorNet\cite{Yang_2019_ICCV}, pre-trained on the training set of Trans10K and MSD, respectively, on the unseen test set of each dataset. We use weights made available by the authors. %Then, we exploit these predictions in our proxy depth generation pipeline, and we finetune our monocular networks with these depth maps.
For a fair comparison, we also re-train again the models exploiting GT segmentations only on the test sets of the two datasets.
Tab. \ref{tab:proxy_vs_gt_mask} highlights that both models, using either GT - \textit{Ft. Virtual Depth (GT)} - or proxy segmentations  - \textit{Ft. Virtual Depth (Proxy)}, achieve much more accurate results compared to the \textit{Base} network. Interestingly, the two networks yield comparable results in the class \textit{Other}, while the one using GTs is slightly better than the other in the class \textit{ToM}, yet still comparable.
%
Finally, in row \textit{Virtual Depth (Proxy)}, we report the results of our in-painting methodology (i.e., without fine-tuning) but coloring pixels according to the proxy segmentations. We note that performances are even worse than the \textit{Base} method. Indeed, the segmentation network struggles to generalize to the Booster dataset, making the depth model incapable of estimating the correct values, e.g., due to some overextended in-painted ToM areas, as shown in Fig. \ref{fig:ablation_semantic_proxy_mono}.
Yet, depth networks, fine-tuned on the test set of MSD and Trans10K (row \textit{Ft. Virtual Depth (Proxy)}),
%, where the segmentations are less noisy, 
generalize properly on Booster.
%Also by looking at some qualitative results, shown in Fig. \ref{fig:ablation_semantic_proxy_mono}, we notice negligible differences between the two approaches, while being clearly better than the one by the \textit{Base} model.
% By looking at the qualitative results shown in Fig. \ref{fig:ablation_semantic_proxy_mono}, we notice negligible differences between the two approaches. Nonetheless,  their results are more accurate than the \textit{Base} model.



\input{table/stereo_proxy}
% Figure environment removed

% Figure environment removed

\subsection{Stereo Depth Estimation}

% We now study the effectiveness of our proposal when applied to fine-tune stereo networks.

\textbf{Virtual Disparity Generation Alternatives.} We inquire about two main alternatives to generate virtual disparities: i) \textit{Virtual Disparity}: masking both left and right images according to material segmentation masks -- as materials annotations are provided for the left image only, we warp it over the right image according to ground-truth disparity -- and then processing the two with the stereo network we are going to fine-tune similar to Monocular networks, ii) \textit{Merged}: merging disparity labels produced by the stereo model itself with those generated by original DPT weights \cite{Ranftl2021}, as detailed in Eq. \ref{eq:merging}.
Although the former might appear as the natural extension of our proposal from the monocular to the stereo case, we will demonstrate its ineffectiveness.

\textbf{Fine-tuning Results (GT Segmentation).} Tab. \ref{tab:stereo} collects the results obtained by fine-tuning RAFT-Stereo and CREStereo through our technique. From top to bottom, we report the results achieved by the original models (\textit{Base}) as well as the instances fine-tuned on their own predictions (\textit{Ft. Base}) or on pseudo labels obtained according to the two strategies (\textit{Ft. Virtual Depth}, \textit{Ft. Merged}).

Not surprisingly, fine-tuning the networks on their own predictions is harmful (RAFT-Stereo) or scarcely effective (CREStereo). Applying the first of the two strategies sketched before yields just a negligible improvement over the original models on ToM classes. 
This evidence confirms that our pipeline designed for the monocular case cannot na\"ively be extended to the stereo case by in-painting the two images since masking ToM objects with constant colors does not ease matching -- on the contrary, it introduces textureless regions, which are  likely to be labeled as planar surfaces by stereo models. 
Conversely,  the second strategy consistently improves the predictions with both RAFT-Stereo and CREStereo. In particular, the former achieves 9.23, 13.83, 15.62, and 16.69\% absolute reductions on bad-2, bad-4, bad-6, and bad-8, respectively, as well 7.62 and 9.13 reductions on MAE and RMSE on ToM regions. CREStereo obtains 14.93, 17.13, 17.48, and 17.54\% on bad metrics, and 7.40 and 8.91 reductions on MAE and RMSE. Moreover, the accuracy over \textit{Other} pixels is also improved, although with minor margins.
%Fig. \ref{fig:proxy_merge} provides a qualitative comparison between proxy labels obtained by the two strategies: the former produces a planar surface for the mirror, yet is completely misaligned with the wall; whereas the latter instead combines at the best the proxy labels from DPT on masked images with disparity labels.
Fig. \ref{fig:proxy_merge} provides a qualitative comparison between the labels obtained by the two strategies. The former produces a planar surface for the mirror completely misaligned with respect to the wall, whereas the latter combines the virtual depth labels from DPT on masked images with disparity labels at best.


\textbf{Fine-tuning Results (Proxy Segmentation).} Finally, we replace the manually annotated segmentation masks with those predicted by Trans2Seg and MirrorNet and then distill virtual disparities for fine-tuning both stereo networks. As pointed out before, both Trans2Seg and MirrorNet have not been trained on Booster. Thus, \textit{Merging} produces significant differences with respect to the use of manually annotated masks, as shown in Fig. \ref{fig:proxy_stereo}. Nevertheless, 
Table \ref{tab:stereo_proxy} shows that our pipeline improves the performance of both RAFT-Stereo and CREStereo on ToM objects, even in the case of extremely noisy proxy semantic annotations.
More precisely, CREStereo seems to benefit more from the Proxy segmentation configuration than RAFT-Stereo. Indeed, on the one hand, we can notice how RAFT-Stereo improves on ToM regions at the expense of accuracy on other pixels when using Proxy segmentations. This yields, on All pixels, an increase in the bad-2 and bad-4 error rates, whereas bad-6, bad-8, MAE, and RMSE remain lower.
On the other hand, CREStereo seems capable of exploiting fine-tuning much better, yielding more accurate results on any metric with both Proxy or GT masks.
This outcome proves that  our pipeline is effective for fine-tuning stereo models even without manually annotated masks. Nonetheless, segmenting images through human labeling unleashes its full potential, whose cost is much lower compared to that that would be required to annotate depths.
%whose costs are definitely lower compared to those required to annotate depth.


\subsection{Qualitative Results} 

To conclude, Fig. \ref{fig:qual_stereo} shows the effect of the fine-tuning carried out according to our proposal, with two examples for monocular (top) and stereo (bottom) networks from the Booster train and test sets respectively.
We highlight how MiDaS, DPT, RAFT-Stereo, and CREStereo learn to deal with ToM surfaces either when relying on proxy segmentation masks provided by neural networks or accurately annotated by humans. More qualitatives in the supplement.
%Nonetheless, we highlight how some failure cases still remain -- e.g., RAFT-Stereo cannot fully deal with the window (third row), while CREStereo is still fooled by the glass ju  g (fifth row).