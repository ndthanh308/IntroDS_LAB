\section{Introduction}
In our daily lives, we often interact with several objects of various appearances. Among them are those made of transparent or mirror surfaces (ToM), ranging from the glass windows of buildings to the reflective surfaces of cars and appliances. These might represent a hard challenge for an autonomous agent leveraging computer vision to operate in unknown environments. Specifically, among the many tasks involved in Spatial AI, accurately estimating depth information on these surfaces remains a challenging problem for both computer vision algorithms and deep networks \cite{zamaramirez2022booster}, yet necessary for proper interaction with the environment in robotic, autonomous navigation, picking, and other application fields. 
%
% Figure environment removed
%
This difficulty arises because ToM surfaces introduce misleading visual information about scene geometry, which makes depth estimation challenging not only for computer vision systems but even for humans -- e.g., we might not distinguish the presence of a glass door in front of us due to its transparency.
%This is because ToM objects can introduce misleading visual information about the geometry of the scene. In fact, the challenge of depth estimation on ToM surfaces is not only limited to computer vision systems, even humans can be fooled -- e.g., we might not distinguish the presence of a glass door in front of us, due to its transparency. 
On the one hand, the definition of depth itself might appear ambiguous in such cases: is \textit{depth} the distance to the scene behind the glass door or to the door itself? Nonetheless, from a practical point of view, we argue that the actual definition  depends on the task itself -- e.g., a mobile robot should definitely be aware of the presence of the glass door.
On the other hand, as humans can deal with this through experience, depth sensing techniques based on deep learning, e.g., monocular  \cite{Ranftl2022,Ranftl2021} or stereo \cite{lipson2021raft,li2022practical} networks, hold  the potential to address this challenge given sufficient training data \cite{zamaramirez2022booster}.

Unfortunately, light reflection and refraction over ToM  surfaces violate also the working principles of most active depth sensors, such as Time-of-Flight (ToF) cameras or devices projecting structured-light patterns. This  has two practical consequences:  i) it makes active sensors unsuited to deal with ToM objects in real-world applications, and ii) prevents the use of these sensors for collecting and annotating data to train deep neural networks to deal with ToM objects. As evidence of this, very few datasets featuring transparent objects provide ground-truth depth annotations, which have been obtained through very intensive human intervention \cite{zamaramirez2022booster}, graphical engines \cite{sajjan2020clear}, or based on  the availability of CAD models \cite{chen2022clearpose} for ToM objects.


In short, accurately perceiving the presence (and depth) of ToM objects represents an open challenge for both sensing technologies and deep learning frameworks.
%represents an open challenge for the computer vision community to different degrees, for both sensing technologies and deep learning frameworks.
Purposely, this paper proposes a simple yet effective strategy for obtaining training data and, thereby, dramatically boosting the accuracy of learning-based depth estimation frameworks dealing with ToM surfaces. Driven by the observation that ToM objects alone are responsible for misleading recent monocular networks \cite{Ranftl2022,Ranftl2021}, which would otherwise generalize well to most unseen environments, we argue that \textit{replacing} them with equivalent, yet opaque objects would allow restoring an environment layout in which such networks could accurately estimate the depth of the scene. To this end, we mask ToM objects in images by in-painting them with  arbitrary uniform colors.
% by overwriting them and in-paint the masked regions. 
Then, we employ a monocular depth network to generate a \textit{virtual} depth map out of the modified image. By repeating this process on a variety of images featuring ToM objects, we can easily and effectively annotate a dataset and then use it to train the same monocular network used to distill labels, which will now process the not-in-painted images. %, unaltered images and will be optimized over the generated proxy labels. 
As a result, the trained monocular network will learn to handle ToM objects, producing consistent depth even in their presence. 

Our main contributions can be resumed as follows:

\begin{itemize}
    \item We propose a simple yet very effective strategy to deal with ToM objects. We trick a monocular depth estimation network by replacing ToM objects with  virtually textured ones, inducing it to hallucinate their depths.

    \item We introduce a processing pipeline for fine-tuning a monocular depth estimation network to deal with ToM objects. Our pipeline exploits the network itself to generate virtual depth annotations and requires only segmentation masks delineating ToM objects -- either human-made or predicted by other networks \cite{xie2020segmenting,Yang_2019_ICCV} -- thus getting rid of the need for any depth annotations.

    \item We show how our strategy can be extended to other depth estimation settings, such as stereo matching. Our experiments on the Booster dataset \cite{zamaramirez2022booster} prove how monocular and stereo networks dramatically improve their prediction on ToM objects after being fine-tuned according to  our methodology.
\end{itemize}
Fig. \ref{fig:teaser} highlights some specific regions where monocular (top) and stereo (bottom) models struggle (middle column), and how they learn to handle ToM surfaces thanks to our strategy (rightmost column).

The project page is available at \url{https://cvlab-unibo.github.io/Depth4ToM/}.

%Fig. \ref{fig:teaser} provides a preview of how both monocular (top) and stereo (bottom) models actually learn to effectively handle ToM surfaces, thanks to our strategy.