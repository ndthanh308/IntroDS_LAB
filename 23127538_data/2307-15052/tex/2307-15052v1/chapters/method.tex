\section{Method}

%\matteo{cose da migliorare: i) specificare che le proxy vengono sempre dal modello stesso che si vuole addestrare}

Our goal is to generate depth annotations for images featuring ToM objects in a cheap and scalable manner. This allows for training deep networks to properly estimate their depth as the distance of the closest surface in front of the camera, rather than the distance of the scene content refracted/reflected through it.
Our strategy is simple yet dramatically effective and relies on the availability of recent pre-trained monocular depth estimation models \cite{Ranftl2022, Ranftl2021}, which are capable of strong generalization across a variety of  scenes though struggling to deal with ToM surfaces.
%
Based on the above state of affairs, we argue that ToM objects are often the sole elements harming the reliability of recent pre-trained  monocular depth estimation networks. Therefore, by virtually replacing these objects with textured artifacts that resemble their very same shapes, the monocular model may be possibly tricked and induced into estimating the depth of an opaque object, ideally placed at the very same spot in the scene. This methodology can be realized   by delineating ToM  objects, through manual annotations or a segmentation network, masking them from the image and then in-painting virtual textures within the masked areas.
%This is possible by properly detecting ambiguous objects, either through manual annotation or by means of a segmentation network, masking them from the image, and then applying in-painting. 
On the one hand, since a proper detection of ToM objects is crucial to our methodology, manual labeling indisputably results in the most accurate choice, though it comes with significant annotation costs. On the other hand, relying on a segmentation network would alleviate this cost: %requirement: while it would need some initial human annotations for training, then it could segment a widely larger amount of images for free. 
one would need some initial human annotations for training, but this would then allow to segment a large number of images for free.
Unfortunately, the overall effectiveness of our methodology would be inevitably affected by the accuracy of the trained segmentation model. However, we reckon that annotating images with segmentation masks requires, definitely,  a vastly lower effort compared to depth annotation \cite{zamaramirez2022booster,liu2021stereobj}. 
Hence, we settled on exploring both the aforementioned approaches.  

The reader may argue that, as a consequence of our intuition, training a depth network to deal with ToM objects might be unnecessary -- indeed, it would be sufficient to segment and in-paint such objects at deployment time before estimating depth. However, we retort that such a methodology would rely heavily on the actual accuracy of the model trained to segment ToM objects, which is not granted to generalize. Moreover, it would add non-negligible computational cost -- i.e., the inference by a second network.
On the contrary, an offline training or fine-tuning procedure allows for exploiting human-made annotation -- if available -- and, potentially, enable the trained network to learn how to properly estimate depth on ToM surfaces and to get rid of the second network, as well as design advanced strategies for other depth estimation frameworks, e.g. deep stereo networks.
Our experiments will highlight that the former strategy results ineffective, while we achieve a large boost in accuracy by fine-tuning depth models with our approach.

In the remainder, we describe our methodology to deal with ToM objects.
Given a dataset of images $\mathcal{I}$, our pipeline sketched in Fig. \ref{fig:framework_mono} builds as follows: %train a depth network to deal with ToM objects according to a pipeline made of three main steps: 
i) surface labeling, ii) in-painting and distillation, and iii) fine-tuning of the depth network on virtual labels. Additionally, we show how it can be revised to fine-tune also deep stereo networks.

% Figure environment removed


\textbf{Surface Labeling.} For any image $I_k \in \mathcal{I}$, we produce a segmentation mask $M_k$ classifying each pixel $p$ as

\begin{equation}
    M_k(p) = \begin{cases}
    1 & \text{if } I_k(p) \in \text{ToM surfaces } \\
    0 & \text{Otherwise} \\ 
    
\end{cases}
\end{equation}
by labeling pixels as either 1 or 0 if they belong to a ToM surface or not, respectively.
Such a segmentation mask can be  obtained either through manual annotation or by means of a segmentation network $\Theta$ as $M_k = \Theta(I_k)$.

\textbf{In-painting and Distillation.} Given an image $I_k$ and its corresponding segmentation mask $M_k$, we generated an augmented image $\tilde{I}_k$ applying an in-painting operation to replace the pixels belonging to ToM objects with a color $c$:

\begin{equation}
    \tilde{I}_k(p) = \begin{cases}
    c & \text{if } M_k(p) = 1 \\ 
    I_k & \text{otherwise } \\
\end{cases}
\end{equation}
Then, a virtual depth $\tilde{D}_k$ for image $I_k$ is obtained by forwarding $\tilde{I}_k$ to a monocular depth network $\Psi$ as $\tilde{D}_k = \Psi(\tilde{I}_k)$.
Colors are randomly sampled for every single frame $I_k$. However, depending on the image content, certain colors might result ineffective and increase the scene ambiguity -- e.g., by in-painting  white pixels into  a transparent object located in front of a white wall. To discourage these occurrences, we sample a set of $N$ custom colors $c_i, i \in [0, N-1]$, and  in-paint $I_k$ using each of these custom colors, so as to generate a set of $N$ augmented images $\tilde{I}^i_k$. Then, we obtain the final, \textit{Virtual Depth} $\tilde{D}_k$ by computing the per-pixel median between the $N$ depth maps %of the $N$ augmented images
%predicted by feeding each of the augmented images to $\Psi$

\begin{equation}
    \tilde{D}^*_k = \text{med} \Bigl\{ \Psi(\tilde{I}^i_k), i \in [0,N-1] \Bigr\}
\end{equation}

As depicted in Fig. \ref{fig:ablation_depth_proxy_mono}, in some cases, the in-painted color might be similar to the background -- e.g., the transparent object disappears when a single gray mask is used -- while it is visible by aggregating multi-color in-painting.

\input{table/mask_generalization}
% Figure environment removed


\textbf{Fine-Tuning on Virtual Labels.} The steps outlined so far allow for labeling a dataset $\mathcal{I}$ with virtual depth labels that are not influenced by the ambiguities of ToM objects. Then, our newly annotated dataset can be used to train or fine-tune a depth estimation network, thereby enabling it to handle the aforementioned difficult  objects robustly.
%to accurately handle the aforementioned, ambiguous objects.
Specifically, during training, the original images $I_k$ are forwarded to the network, and the predicted depth $\hat{D}_k$ is optimized with respect to the distilled virtual ground-truth map $\tilde{D}^*_k$ obtained from in-painted images.
This simple pipeline can dramatically improve the accuracy of monocular  depth estimation networks when dealing with ToM objects, as we will show in our experiments.

\textbf{Extension to Deep Stereo.} Our pipeline can be adapted to fine-tune deep stereo models as well, as shown in Fig. \ref{fig:framework_stereo}. Again, we argue that state-of-the-art stereo architectures \cite{lipson2021raft,li2022practical} already expose outstanding generalization capabilities while struggling with ToM objects, due to the task of matching pixels belonging to non-Lambertian surfaces being inherently ambiguous.
%capability, while struggling with ToM objects due to the inherently difficult matching problem they are called to deal with. 
Consequently, we exploit a monocular depth estimation network to obtain virtual depth annotations solely for these objects. Given a dataset $\mathcal{S}$ consisting of stereo pairs $(L_k,R_k)$, we distill virtual depth labels $\tilde{D}^*_k$ from $L_k$ and triangulate them into disparities $\tilde{d}^*_k$ according to extrinsic parameters of the stereo rig. Then, we predict a \textit{Base} disparity map $d_k$ by forwarding $(L_k,R_k)$ to the stereo network we aim to fine-tune. Eventually, we replace the disparities for ToM objects with those from $d_k$ according to $M_k$, this latter produced over $L_k$ this time. Formally, this operation namely \textit{Merging}, is defined as:

\begin{equation}
\label{eq:merging}
    d_k(p) = \begin{cases}
    d_k(p) & \text{if } M_k(p) = 0 \\ 
    \alpha_k\hat{d}^*_k(p) + \beta_k & \text{otherwise } \\
\end{cases}
\end{equation}
with $\alpha_k,\beta_k$ being scale and shift factors, as monocular predictions are up to an unknown scale factor. Following \cite{Ranftl2022}, $\alpha_k,\beta_k$ are estimated through Least Square Estimation (LSE) regression over $d_k$ for pixels not belonging to any ToM object, i.e., having $M_k(p)=0$:

\begin{equation}
\label{eq:rescaling}
    (\alpha_k,\beta_k) = \text{arg}\min_{\alpha,\beta} \sum_{p| M_k(p)=0} \Big( \alpha \hat{d}^*_k(p) + \beta - d_k(p) \Big)^2
\end{equation}



%This is often performed by means of median depth rescaling \cite{zhou2017unsupervised} or scale and shift estimation through LSE \cite{Ranftl2022}. %On the contrary, \matteo{TODO. Spieghiamo gi√† qui il rescaling e lo richiamiamo poi negli esperimenti?}





