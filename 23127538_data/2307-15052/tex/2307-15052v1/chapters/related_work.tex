% Figure environment removed

\section{Related Work} 

%In this section, we review the literature related to our work, specifically focusing on monocular and stereo networks as well as datasets that are strictly related to our task of addressing non-lambertian surfaces.

\textbf{Monocular Depth Estimation.} Early methods used CNNs for pixel-level regression \cite{garg2016unsupervised, monodepth17}. More recent approaches such as AdaBins \cite{bhat2021adabins}, DPT \cite{Ranftl2021}, and MiDaS \cite{Ranftl2022} use adaptive bins and vision transformers for depth regression and leverage large-scale depth training by mixing multiple datasets.
%
Self-supervised methods use view synthesis for image reconstruction, where predicted depth is combined with known or estimated camera pose to establish correspondences between adjacent images, exploiting either stereo pairs \cite{garg2016unsupervised, monodepth17} or monocular videos \cite{zhou2017unsupervised, monodepth2}.
%Some approaches use rectified stereo pairs with a fixed baseline \cite{garg2016unsupervised, monodepth17}, while others use adjacent frames from a monocular video \cite{zhou2017unsupervised, monodepth2}, albeit at the expense of sensitivity to dynamic objects and no scale recovery.
%
Recent works aim to improve the robustness of the photometric loss based on SSIM and L1 \cite{zhao2020monocular} by incorporating photometric uncertainty \cite{yang2020d3vo, poggi2020uncertainty}, feature descriptors \cite{zhan2018unsupervised, shu2020feature, spencer2020defeat}, 3D geometric constraints \cite{mahjourian2018unsupervised}, proxy supervision \cite{watson2019self, tosi2019learning}, optical flow \cite{yin2018geonet, tosi2020distilled}, or adversarial losses \cite{aleotti2018generative, pilzer2018unsupervised}.
Others propose architecture changes as in \cite{zhao2022monovit, guizilini20203d, pillai2019superdepth, johnston2020self, gonzalezbello2020forget}.
%s  use transformers \cite{zhao2022monovit}, 3D packing blocks \cite{guizilini20203d}, sub-pixel convolutions \cite{pillai2019superdepth}, and discrete disparity volumes \cite{johnston2020self, gonzalezbello2020forget}.
%Some proposed improvements in architecture include the use of transformers \cite{zhao2022monovit}, 3D packing blocks \cite{guizilini20203d}, sub-pixel convolutions \cite{pillai2019superdepth}, and discrete disparity volumes \cite{johnston2020self, gonzalezbello2020forget}.
Except for some works that address non-Lambertian depth estimation using depth completion approaches and sparse depth measurements from active sensors \cite{choi2021selfdeco, sajjan2020clear}, to the best of our knowledge, we are not aware of any previous single-view depth estimation network that can handle ToM surfaces.  

\textbf{Stereo Matching.} Traditional algorithms \cite{scharstein2002taxonomy} utilize handcrafted features to estimate a disparity map \cite{Secaucus_1994_ECCV, hirschmuller2007stereo, yang2008stereo, yang2010constant, liang2011hardware, taniai2014graph, kolmogorov2004energy, boykov2001fast}. Then, deep learning methods replaced traditional matching cost computation, as demonstrated in \cite{zbontar2016stereo}, and, eventually,  end-to-end approaches became the most effective solution for disparity estimation. These networks can be mainly categorized into 2D and 3D architectures, with the former adopting an encoder-decoder design \cite{mayer2016large, Pang_2017_ICCV_Workshops, Liang_2018_CVPR, saikia2019autodispnet, song2018edgestereo, yang2018segstereo, yin2019hierarchical, Tankovich_2021_CVPR} and the latter building a feature cost volume from extracted features on the image pair \cite{Kendall_2017_ICCV, chang2018psmnet, khamis2018stereonet, zhang2019ga, cheng2019learning, cheng2020hierarchical, duggal2019deeppruner, yang2019hierarchical, wang2019anytime, guo2019group, Shen_2021_CVPR}. A thorough review of these works can be found in \cite{poggi2021synergies}. Recent papers exploit iterative refinement paradigms \cite{lipson2021raft, li2022practical} or rely on  Vision Transformers \cite{li2021revisiting,guo2022context}. However, due to its inherently ill-posed nature, dealing with non-Lambertian surfaces, such as ToM objects, remains a very challenging problem for any kind of existing stereo approach.  %which are still inherently ill-posed. %However, deep learning-based stereo methods require large amounts of annotated data for training and often suffer from limited generalization capabilities. Self-supervised techniques have been proposed to alleviate this requirement by relying on photometric losses or traditional algorithms on stereo pairs or videos \cite{SsSMnet2017,Tonioni_2019_CVPR,Tonioni_2019_learn2adapt, lai2019bridging,wang2019unos,chi2021feature, Tonioni_2017_ICCV, aleotti2020reversing, Tonioni_2019_CVPR, Poggi2021continual}. 

\textbf{Non-Lambertian Object Perception.} %Several large-scale datasets have been created to tackle the challenges posed by ToM objects. %in the depth estimation and object estimation tasks. 
Due to the relevance of dealing with ToM objects, some recent datasets focus on them.
Trans10K \cite{xie2020segmenting} and MSD \cite{Yang_2019_ICCV} consist of over 10\,000 and 4\,000 real in-the-wild images of transparent objects and mirrors, respectively. Both datasets provide manually annotated segmentations of ToM materials, though none of them provide depth labels.
%
Others provide depth annotations:
ClearPose \cite{chen2022clearpose} includes over 350\,000 labeled real-world RGB-D frames of 63 household objects.
ClearGrasp \cite{sajjan2020clear} consists of over 50\,000 synthetic RGB-D images of transparent objects, as well as real-world test benchmark with 286 RGB-D images.
%
In addition, Booster \cite{zamaramirez2022booster} focuses on stereo matching, providing  high-resolution depth labels and stereo pairs acquired in indoor scenes with specular and transparent surfaces.
%
TOD \cite{Liu_2020_CVPR} contains 15 transparent objects, labeled with relevant 3D keypoints, comprising 48\,000 stereo and RGBD images.
%
StereOBJ-1M \cite{liu2021stereobj} also deals with stereo vision, but focuses on pose estimation for ToM objects and do not provides depth ground truths.
%
% Obtaining depth labels for these kinds of datasets is expensive, challenging, and time-consuming  since it requires either CAD models for ToM objects \cite{chen2022clearpose} or painting such objects in the scene \cite{sajjan2020clear,zamaramirez2022booster,Liu_2020_CVPR}.
Obtaining depth labels for these kinds of datasets is expensive, challenging, and time-consuming since it requires either CAD models for ToM objects \cite{chen2022clearpose}, painting such objects in the scene \cite{sajjan2020clear,zamaramirez2022booster,Liu_2020_CVPR} or relies on a complex multi-camera setup \cite{whelan2018reconstructing}.
%
In contrast, our proposal effectively sidesteps these challenges, by demonstrating that monocular and stereo networks can learn to deal with these objects in the absence of depth annotations.