\begin{thebibliography}{10}
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi
\expandafter\ifx\csname href\endcsname\relax
  \def\href#1#2{#2} \def\path#1{#1}\fi

\bibitem{kulstad1997leibniz}
M.~Kulstad, L.~Carlin, Leibniz’s philosophy of mind (1997).

\bibitem{gunning2019xai}
D.~Gunning, M.~Stefik, J.~Choi, T.~Miller, S.~Stumpf, G.-Z. Yang,
  Xai—explainable artificial intelligence, Science robotics 4~(37) (2019)
  eaay7120.

\bibitem{lapuschkin2019unmasking}
S.~Lapuschkin, S.~W{\"a}ldchen, A.~Binder, G.~Montavon, W.~Samek, K.-R.
  M{\"u}ller, Unmasking clever hans predictors and assessing what machines
  really learn, Nature communications 10~(1) (2019) 1096.

\bibitem{confalonieri2021historical}
R.~Confalonieri, L.~Coba, B.~Wagner, T.~R. Besold, A historical perspective of
  explainable artificial intelligence, Wiley Interdisciplinary Reviews: Data
  Mining and Knowledge Discovery 11~(1) (2021) e1391.

\bibitem{dovsilovic2018explainable}
F.~K. Do{\v{s}}ilovi{\'c}, M.~Br{\v{c}}i{\'c}, N.~Hlupi{\'c}, Explainable
  artificial intelligence: A survey, in: 2018 41st International convention on
  information and communication technology, electronics and microelectronics
  (MIPRO), IEEE, 2018, pp. 0210--0215.

\bibitem{huang2017safety}
X.~Huang, M.~Kwiatkowska, S.~Wang, M.~Wu, Safety verification of deep neural
  networks, in: Computer Aided Verification: 29th International Conference, CAV
  2017, Heidelberg, Germany, July 24-28, 2017, Proceedings, Part I 30,
  Springer, 2017, pp. 3--29.

\bibitem{dreossi2019verifai}
T.~Dreossi, D.~J. Fremont, S.~Ghosh, E.~Kim, H.~Ravanbakhsh,
  M.~Vazquez-Chanlatte, S.~A. Seshia, Verifai: A toolkit for the formal design
  and analysis of artificial intelligence-based systems, in: International
  Conference on Computer Aided Verification, Springer, 2019, pp. 432--442.

\bibitem{wu2020game}
M.~Wu, M.~Wicker, W.~Ruan, X.~Huang, M.~Kwiatkowska, A game-based approximate
  verification of deep neural networks with provable guarantees, Theoretical
  Computer Science 807 (2020) 298--329.

\bibitem{liu2021algorithms}
C.~Liu, T.~Arnon, C.~Lazarus, C.~Strong, C.~Barrett, M.~J. Kochenderfer,
  et~al., Algorithms for verifying deep neural networks, Foundations and
  Trends{\textregistered} in Optimization 4~(3-4) (2021) 244--404.

\bibitem{seshia2022toward}
S.~A. Seshia, D.~Sadigh, S.~S. Sastry, Toward verified artificial intelligence,
  Communications of the ACM 65~(7) (2022) 46--55.

\bibitem{huang2021statistical}
C.~Huang, Z.~Hu, X.~Huang, K.~Pei, Statistical certification of acceptable
  robustness for neural networks, in: Artificial Neural Networks and Machine
  Learning--ICANN 2021: 30th International Conference on Artificial Neural
  Networks, Bratislava, Slovakia, September 14--17, 2021, Proceedings, Part I
  30, Springer, 2021, pp. 79--90.

\bibitem{zhang2022proa}
T.~Zhang, W.~Ruan, J.~E. Fieldsend, Proa: A probabilistic robustness assessment
  against functional perturbations, in: Joint European Conference on Machine
  Learning and Knowledge Discovery in Databases, Springer, 2022, pp. 154--170.

\bibitem{shafaei2018uncertainty}
S.~Shafaei, S.~Kugele, M.~H. Osman, A.~Knoll, Uncertainty in machine learning:
  A safety perspective on autonomous driving, in: Computer Safety, Reliability,
  and Security: SAFECOMP 2018 Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and
  WAISE, V{\"a}ster{\aa}s, Sweden, September 18, 2018, Proceedings 37,
  Springer, 2018, pp. 458--464.

\bibitem{gawlikowski2021survey}
J.~Gawlikowski, C.~R.~N. Tassi, M.~Ali, J.~Lee, M.~Humt, J.~Feng, A.~Kruspe,
  R.~Triebel, P.~Jung, R.~Roscher, et~al., A survey of uncertainty in deep
  neural networks, arXiv preprint arXiv:2107.03342 (2021).

\bibitem{hullermeier2021aleatoric}
E.~H{\"u}llermeier, W.~Waegeman, Aleatoric and epistemic uncertainty in machine
  learning: An introduction to concepts and methods, Machine Learning 110
  (2021) 457--506.

\bibitem{gruber2023sources}
C.~Gruber, P.~O. Schenk, M.~Schierholz, F.~Kreuter, G.~Kauermann, Sources of
  uncertainty in machine learning -- a statisticians' view (2023).
\newblock \href {http://arxiv.org/abs/2305.16703} {\path{arXiv:2305.16703}}.

\bibitem{cheng2019runtime}
C.-H. Cheng, G.~N{\"u}hrenberg, H.~Yasuoka, Runtime monitoring neuron
  activation patterns, in: 2019 Design, Automation \& Test in Europe Conference
  \& Exhibition (DATE), IEEE, 2019, pp. 300--303.

\bibitem{henzinger2020outside}
T.~A. Henzinger, A.~Lukina, C.~Schilling, Outside the box: Abstraction-based
  monitoring of neural networks, in: ECAI 2020, IOS Press, 2020, pp.
  2433--2440.

\bibitem{cheng2020provably}
C.-H. Cheng, Provably-robust runtime monitoring of neuron activation patterns,
  in: 2021 Design, Automation \& Test in Europe Conference \& Exhibition
  (DATE), IEEE, 2021, pp. 1310--1313.

\bibitem{lukina2020into}
A.~Lukina, C.~Schilling, T.~A. Henzinger, Into the unknown: Active monitoring
  of neural networks, in: International Conference on Runtime Verification,
  Springer, 2021, pp. 42--61.

\bibitem{cheng2022prioritizing}
C.-H. Cheng, C.~Wu, E.~Seferis, S.~Bensalem, Prioritizing corners in ood
  detectors via symbolic string manipulation, in: International Symposium on
  Automated Technology for Verification and Analysis, Springer, 2022, pp.
  397--413.

\bibitem{musa_operational_1993}
J.~D. Musa, Operational profiles in software-reliability engineering, IEEE
  Software 10~(2) (1993) 14--32.

\bibitem{fukunaga_introduction_2013}
K.~Fukunaga, Introduction to statistical pattern recognition, Elsevier, 2013.

\bibitem{Nakkiran2020Deep}
P.~Nakkiran, G.~Kaplun, Y.~Bansal, T.~Yang, B.~Barak, I.~Sutskever, Deep double
  descent: Where bigger models and more data hurt, in: International Conference
  on Learning Representations, 2020.

\bibitem{10.1007/978-3-030-32304-2_15}
J.~Li, J.~Liu, P.~Yang, L.~Chen, X.~Huang, L.~Zhang, Analyzing deep neural
  networks with symbolic propagation: Towards higher precision and faster
  verification, in: SAS2019, Springer, 2019, pp. 296--319.

\bibitem{10.1145/3368089.3417918}
R.~Li, J.~Li, C.-C. Huang, P.~Yang, X.~Huang, L.~Zhang, B.~Xue, H.~Hermanns,
  Prodeep: A platform for robustness verification of deep neural networks, in:
  Proc. of the 28th ACM Joint Meeting on European Software Engineering
  Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE
  2020, ACM, New York, NY, USA, 2020, pp. 1630--1634.

\bibitem{10.1007/s00165-021-00548-1}
P.~Yang, J.~Li, J.~Liu, C.-C. Huang, R.~Li, L.~Chen, X.~Huang, L.~Zhang,
  Enhancing robustness verification for deep neural networks via symbolic
  propagation, Form. Asp. Comput. 33~(3) (2021) 407--435.

\bibitem{ruan_reachability_2018}
W.~Ruan, X.~Huang, M.~Kwiatkowska, Reachability {Analysis} of {Deep} {Neural}
  {Networks} with {Provable} {Guarantees}, in: Proc. of the 27th {Int}. {Joint}
  {Conf}. on {Artificial} {Intelligence}, {IJCAI}-18, 2018, pp. 2651--2659.

\bibitem{ruan_global_2019}
W.~Ruan, M.~Wu, Y.~Sun, X.~Huang, D.~Kroening, M.~Kwiatkowska, Global
  {Robustness} {Evaluation} of {Deep} {Neural} {Networks} with {Provable}
  {Guarantees} for the {Hamming} {Distance}, in: Proc. of the 28th {Int.}
  {Joint} {Conf.} on {Artificial} {Intelligence}, {IJCAI}-19, 2019, pp.
  5944--5952.

\bibitem{peipeiCIS2022}
P.~Xu, W.~Ruan, X.~Huang, Quantifying safety risks of deep neural networks,
  Complex \& Intelligent Systems (2022).

\bibitem{doi:10.1073/pnas.1903070116}
M.~Belkin, D.~Hsu, S.~Ma, S.~Mandal, Reconciling modern machine-learning
  practice and the classical bias--variance trade-off, Proceedings of the
  National Academy of Sciences 116~(32) (2019) 15849--15854.

\bibitem{huang2023survey}
X.~Huang, W.~Ruan, W.~Huang, G.~Jin, Y.~Dong, C.~Wu, S.~Bensalem, R.~Mu, Y.~Qi,
  X.~Zhao, K.~Cai, Y.~Zhang, S.~Wu, P.~Xu, D.~Wu, A.~Freitas, M.~A. Mustafa, A
  survey of safety and trustworthiness of large language models through the
  lens of verification and validation (2023).
\newblock \href {http://arxiv.org/abs/2305.11391} {\path{arXiv:2305.11391}}.

\bibitem{littlewood_reasoning_2012}
B.~Littlewood, J.~Rushby, Reasoning about the reliability of diverse
  two-channel systems in which one channel is ``possibly perfect'', IEEE
  Transactions on Software Engineering 38~(5) (2012) 1178--1194.

\bibitem{rushby_software_2009}
J.~Rushby, Software verification and system assurance, in: 7th {Int.} {Conf.}
  on {Software} {Engineering} and {Formal} {Methods}, IEEE, Hanoi, Vietnam,
  2009, pp. 3--10.

\bibitem{zhao2017modeling}
X.~Zhao, B.~Littlewood, A.~Povyakalo, L.~Strigini, D.~Wright, Modeling the
  probability of failure on demand (pfd) of a 1-out-of-2 system in which one
  channel is “quasi-perfect”, Reliability Engineering \& System Safety 158
  (2017) 230--245.

\bibitem{huang2022safari}
W.~Huang, X.~Zhao, G.~Jin, X.~Huang, Safari: Versatile and efficient
  evaluations for robustness of interpretability, in: Int. Conf. on Computer
  Vision (ICCV'23), 2023.

\bibitem{FRET}
A.~Dutle, C.~A. Mu{\~{n}}oz, E.~Conrad, A.~Goodloe, L.~Titolo, I.~Perez,
  S.~Balachandran, D.~Giannakopoulou, A.~Mavridou, T.~Pressburger, From
  requirements to autonomous flight: An overview of the monitoring {ICAROUS}
  project, in: Proc. 2nd Workshop on Formal Methods for Autonomous Systems,
  Vol. 329 of {EPTCS}, 2020, pp. 23--30.

\bibitem{BCHKMNP2022}
S.~Bensalem, C.-H. Cheng, X.~Huang, P.~Katsaros, A.~Molin, D.~Nickovic,
  D.~Peled, Formal specification for learning-enabled autonomous systems, in:
  FoMLAS2022, 2022.

\bibitem{BPQ}
A.~Balakrishnan, A.~G. Puranic, X.~Qin, A.~Dokhanchi, J.~V. Deshmukh,
  H.~Ben~Amor, G.~Fainekos, Specifying and evaluating quality metrics for
  vision-based perception systems, in: Design, Automation \& Test in Europe
  Conference \& Exhibition (DATE), 2019, pp. 1433--1438.
\newblock \href {https://doi.org/10.23919/DATE.2019.8715114}
  {\path{doi:10.23919/DATE.2019.8715114}}.

\bibitem{10.1007/978-3-030-88494-9_18}
A.~Balakrishnan, J.~Deshmukh, B.~Hoxha, T.~Yamaguchi, G.~Fainekos, Percemon:
  Online monitoring for perception systems, in: Runtime Verification,
  Springer, Cham, 2021, pp. 297--308.

\bibitem{dong2023reliability}
Y.~Dong, W.~Huang, V.~Bharti, V.~Cox, A.~Banks, S.~Wang, X.~Zhao, S.~Schewe,
  X.~Huang, Reliability assessment and safety arguments for machine learning
  components in system assurance, ACM Transactions on Embedded Computing
  Systems 22~(3) (2023) 1--48.

\bibitem{10.1007/978-3-031-17244-1_1}
X.~Huang, W.~Ruan, Q.~Tang, X.~Zhao, Bridging formal methods and machine
  learning with global optimisation, in: A.~Riesco, M.~Zhang (Eds.), Formal
  Methods and Software Engineering, Springer International Publishing, Cham,
  2022, pp. 1--19.

\bibitem{zhao2020safety}
X.~Zhao, A.~Banks, J.~Sharp, V.~Robu, D.~Flynn, M.~Fisher, X.~Huang, A safety
  framework for critical systems utilising deep neural networks, in: Computer
  Safety, Reliability, and Security: 39th International Conference, SAFECOMP
  2020, Lisbon, Portugal, September 16--18, 2020, Proceedings 39, Springer,
  2020, pp. 244--259.

\bibitem{katz2017reluplex}
G.~Katz, C.~Barrett, D.~L. Dill, K.~Julian, M.~J. Kochenderfer, Reluplex: An
  efficient {SMT} solver for verifying deep neural networks, in: International
  Conference on Computer Aided Verification, Springer, 2017, pp. 97--117.

\bibitem{ehlers2017formal}
R.~Ehlers, Formal verification of piece-wise linear feed-forward neural
  networks, in: Automated Technology for Verification and Analysis: 15th
  International Symposium, ATVA 2017, Pune, India, October 3--6, 2017,
  Proceedings 15, Springer, 2017, pp. 269--286.

\bibitem{narodytska2018formal}
N.~Narodytska, Formal analysis of deep binarized neural networks., in: IJCAI,
  2018, pp. 5692--5696.

\bibitem{narodytska2018verifying}
N.~Narodytska, S.~Kasiviswanathan, L.~Ryzhyk, M.~Sagiv, T.~Walsh, Verifying
  properties of binarized deep neural networks, in: Proceedings of the AAAI
  Conference on Artificial Intelligence, Vol.~32, 2018.

\bibitem{cheng2018verification}
C.-H. Cheng, G.~N{\"u}hrenberg, C.-H. Huang, H.~Ruess, Verification of
  binarized neural networks via inter-neuron factoring: (short paper), in:
  Verified Software. Theories, Tools, and Experiments: 10th International
  Conference, VSTTE 2018, Oxford, UK, July 18--19, 2018, Revised Selected
  Papers 10, Springer, 2018, pp. 279--290.

\bibitem{cheng2017maximum}
C.-H. Cheng, G.~N{\"u}hrenberg, H.~Ruess, Maximum resilience of artificial
  neural networks, in: Automated Technology for Verification and Analysis: 15th
  International Symposium, ATVA 2017, Pune, India, October 3--6, 2017,
  Proceedings 15, Springer, 2017, pp. 251--268.

\bibitem{lomuscio2017approach}
A.~Lomuscio, L.~Maganti, An approach to reachability analysis for feed-forward
  relu neural networks, arXiv preprint arXiv:1706.07351 (2017).

\bibitem{gehr2018ai2}
T.~Gehr, M.~Mirman, D.~Drachsler-Cohen, P.~Tsankov, S.~Chaudhuri, M.~Vechev,
  Ai2: Safety and robustness certification of neural networks with abstract
  interpretation, in: 2018 IEEE symposium on security and privacy (SP), IEEE,
  2018, pp. 3--18.

\bibitem{mirman2018differentiable}
M.~Mirman, T.~Gehr, M.~Vechev, Differentiable abstract interpretation for
  provably robust neural networks, in: International Conference on Machine
  Learning, 2018, pp. 3575--3583.

\bibitem{wong2018provable}
E.~Wong, Z.~Kolter, Provable defenses against adversarial examples via the
  convex outer adversarial polytope, in: International Conference on Machine
  Learning, 2018, pp. 5283--5292.

\bibitem{dvijotham2018dual}
K.~Dvijotham, R.~Stanforth, S.~Gowal, T.~A. Mann, P.~Kohli, A dual approach to
  scalable verification of deep networks., in: UAI, Vol.~1, 2018, p.~3.

\bibitem{wang2018formal}
S.~Wang, K.~Pei, J.~Whitehouse, J.~Yang, S.~Jana, Formal security analysis of
  neural networks using symbolic intervals, in: 27th $\{$USENIX$\}$ Security
  Symposium ($\{$USENIX$\}$ Security 18), 2018, pp. 1599--1614.

\bibitem{peck2017lower}
J.~Peck, J.~Roels, B.~Goossens, Y.~Saeys, Lower bounds on the robustness to
  adversarial perturbations, Advances in Neural Information Processing Systems
  30 (2017).

\bibitem{neumaier2004safe}
A.~Neumaier, O.~Shcherbina, Safe bounds in linear and mixed-integer linear
  programming, Mathematical Programming 99 (2004) 283--296.

\bibitem{ruan2018reachability}
W.~Ruan, X.~Huang, M.~Kwiatkowska, Reachability analysis of deep neural
  networks with provable guarantees, arXiv preprint arXiv:1805.02242 (2018).

\bibitem{weng2018evaluating}
T.-W. {Weng}, H.~{Zhang}, P.-Y. {Chen}, J.~{Yi}, D.~{Su}, Y.~{Gao}, C.-J.
  {Hsieh}, L.~{Daniel}, {Evaluating the Robustness of Neural Networks: An
  Extreme Value Theory Approach}, in: ICLR2018, 2018.

\bibitem{webbstatistical}
S.~Webb, T.~Rainforth, Y.~W. Teh, M.~P. Kumar, A statistical approach to
  assessing neural network robustness, in: International Conference on Learning
  Representations.

\bibitem{wang2021statistically}
B.~Wang, S.~Webb, T.~Rainforth, Statistically robust neural network
  classification, in: Uncertainty in Artificial Intelligence, PMLR, 2021, pp.
  1735--1745.

\bibitem{zhao2021assessing}
X.~Zhao, W.~Huang, A.~Banks, V.~Cox, D.~Flynn, S.~Schewe, X.~Huang, Assessing
  the reliability of deep learning classifiers through robustness evaluation
  and operational profiles, Workshop on AI Safety at IJCAI-21 (2021).

\bibitem{pei2017deepxplore}
K.~Pei, Y.~Cao, J.~Yang, S.~Jana, Deepxplore: Automated whitebox testing of
  deep learning systems, in: proceedings of the 26th Symposium on Operating
  Systems Principles, 2017, pp. 1--18.

\bibitem{ma2018deepgauge}
L.~Ma, F.~Juefei{-}Xu, J.~Sun, C.~Chen, T.~Su, F.~Zhang, M.~Xue, B.~Li, L.~Li,
  Y.~Liu, J.~Zhao, Y.~Wang, {DeepGauge}: Comprehensive and multi-granularity
  testing criteria for gauging the robustness of deep learning systems, in:
  Automated Software Engineering (ASE), 33rd IEEE/ACM International Conference
  on, 2018.

\bibitem{sun2018concolicb}
Y.~Sun, M.~Wu, W.~Ruan, X.~Huang, M.~Kwiatkowska, D.~Kroening, Deepconcolic:
  Testing and debugging deep neural networks, in: (ICSE2019), 2019.

\bibitem{du2019deepstellar}
X.~Du, X.~Xie, Y.~Li, L.~Ma, Y.~Liu, J.~Zhao, Deepstellar: Model-based
  quantitative analysis of stateful deep learning systems, in: Proc. of the
  27th ACM Joint Meeting on European Software Engineering Conference and
  Symposium on the Foundations of Software Engineering, 2019, pp. 477--487.

\bibitem{huang2021coverage}
W.~Huang, Y.~Sun, X.~Zhao, J.~Sharp, W.~Ruan, J.~Meng, X.~Huang,
  Coverage-guided testing for recurrent neural networks, IEEE Transactions on
  Reliability 71~(3) (2021) 1191--1206.

\bibitem{berend2021distribution}
D.~Berend, Distribution awareness for ai system testing, in: 2021 IEEE/ACM 43rd
  International Conference on Software Engineering: Companion Proceedings
  (ICSE-Companion), IEEE, 2021, pp. 96--98.

\bibitem{dola2021distribution}
S.~Dola, M.~B. Dwyer, M.~L. Soffa, Distribution-aware testing of neural
  networks using generative models, in: 2021 IEEE/ACM 43rd International
  Conference on Software Engineering (ICSE), IEEE, 2021, pp. 226--237.

\bibitem{byun2020manifold}
T.~Byun, A.~Vijayakumar, S.~Rayadurgam, D.~Cofer, Manifold-based test
  generation for image classifiers, in: 2020 IEEE International Conference On
  Artificial Intelligence Testing (AITest), IEEE, 2020, pp. 15--22.

\bibitem{toledo2021distribution}
F.~Toledo, D.~Shriver, S.~Elbaum, M.~B. Dwyer, Distribution models for
  falsification and verification of dnns, in: 2021 36th IEEE/ACM International
  Conference on Automated Software Engineering (ASE), IEEE, 2021, pp. 317--329.

\bibitem{huang2022hierarchical}
W.~Huang, X.~Zhao, A.~Banks, V.~Cox, X.~Huang, Hierarchical distribution-aware
  testing of deep learning, arXiv preprint arXiv:2205.08589 (2022).

\end{thebibliography}
