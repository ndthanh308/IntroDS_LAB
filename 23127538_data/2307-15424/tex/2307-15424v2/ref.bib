@article{wang2020understanding,
  title={Understanding the Behaviour of Contrastive Loss},
  author={Wang, Feng and Liu, Huaping},
  journal={arXiv preprint arXiv:2012.09740},
  year={2020}
}

@article{bishop1994mixture,
  title={Mixture density networks},
  author={Bishop, Christopher M},
  journal={Technical Report},
  year={1994},
  publisher={Aston University}
}

@misc{qian2023synthcity,
  doi = {10.48550/ARXIV.2301.07573},
  author = {Qian, Zhaozhi and Cebere, Bogdan-Constantin and van der Schaar, Mihaela},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Synthcity: facilitating innovative use cases of synthetic data in different data modalities},
  year = {2023},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{gui2021review,
  title={{A review on generative adversarial networks: Algorithms, theory, and applications}},
  author={Gui, Jie and Sun, Zhenan and Wen, Yonggang and Tao, Dacheng and Ye, Jieping},
  journal={IEEE transactions on knowledge and data engineering},
  volume={35},
  number={4},
  pages={3313--3332},
  year={2021},
  publisher={IEEE}
}

@article{stimper2023normflows,
  title={normflows: A PyTorch Package for Normalizing Flows},
  author={Stimper, Vincent and Liu, David and Campbell, Andrew and Berenz, Vincent and Ryll, Lukas and Sch{\"o}lkopf, Bernhard and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={arXiv preprint arXiv:2302.12014},
  year={2023}
}

@article{uria2013rnade,
  title={{RNADE: The real-valued neural autoregressive density-estimator}},
  author={Uria, Benigno and Murray, Iain and Larochelle, Hugo},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@article{nott2012regression,
  title={{Regression density estimation with variational methods and stochastic approximation}},
  author={Nott, David J and Tan, Siew Li and Villani, Mattias and Kohn, Robert},
  journal={Journal of Computational and Graphical Statistics},
  volume={21},
  number={3},
  pages={797--820},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{nielsen2020survae,
  title={Sur{VAE} flows: Surjections to bridge the gap between {VAE}s and flows},
  author={Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12685--12696},
  year={2020}
}

@article{grathwohl2018ffjord,
  title={Ffjord: Free-form continuous dynamics for scalable reversible generative models},
  author={Grathwohl, Will and Chen, Ricky TQ and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  journal={arXiv preprint arXiv:1810.01367},
  year={2018}
}

@article{chen2019residual,
  title={Residual flows for invertible generative modeling},
  author={Chen, Ricky TQ and Behrmann, Jens and Duvenaud, David K and Jacobsen, J{\"o}rn-Henrik},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@book{jensen2007bayesian,
  title={Bayesian networks and decision graphs},
  author={Jensen, Finn V and Nielsen, Thomas Dyhre},
  volume={2},
  year={2007},
  publisher={Springer}
}

@article{tabak2013family,
  title={A family of nonparametric density estimation algorithms},
  author={Tabak, Esteban G and Turner, Cristina V},
  journal={Communications on Pure and Applied Mathematics},
  volume={66},
  number={2},
  pages={145--164},
  year={2013},
  publisher={Wiley Online Library}
}

@article{tabak2010density,
  title={Density estimation by dual ascent of the log-likelihood},
  author={Tabak, Esteban G and Vanden-Eijnden, Eric},
  journal={Communications in Mathematical Sciences},
  volume={8},
  number={1},
  pages={217--233},
  year={2010},
  publisher={International Press of Boston}
}

@article{rippel2013high,
  title={High-dimensional probability estimation with deep density models},
  author={Rippel, Oren and Adams, Ryan Prescott},
  journal={arXiv preprint arXiv:1302.5125},
  year={2013}
}

@article{kynkaanniemi2019improved,
  title={Improved precision and recall metric for assessing generative models},
  author={Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine, Samuli and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{chopra2005learning,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Chopra, Sumit and Hadsell, Raia and LeCun, Yann},
  booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  volume={1},
  pages={539--546},
  year={2005},
  organization={IEEE}
}

@inproceedings{lecun2005loss,
  title={Loss functions for discriminative training of energy-based models},
  author={LeCun, Yann and Huang, Fu Jie},
  booktitle={International workshop on artificial intelligence and statistics},
  pages={206--213},
  year={2005},
  organization={PMLR}
}

@article{vaserstein1969markov,
  title={Markov processes over denumerable products of spaces, describing large systems of automata},
  author={Vaserstein, Leonid Nisonovich},
  journal={Problemy Peredachi Informatsii},
  volume={5},
  number={3},
  pages={64--72},
  year={1969},
  publisher={Russian Academy of Sciences, Branch of Informatics, Computer Equipment and~…}
}

@article{frechet1957distance,
  title={Sur la distance de deux lois de probabilit{\'e}},
  author={Fr{\'e}chet, Maurice},
  journal={Comptes Rendus Hebdomadaires des Seances de L Academie des Sciences},
  volume={244},
  number={6},
  pages={689--692},
  year={1957},
  publisher={GAUTHIER-VILLARS/EDITIONS ELSEVIER 23 RUE LINOIS, 75015 PARIS, FRANCE}
}

@article{heusel2017gans,
  title={{GANs} trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{binkowski2018demystifying,
  title={Demystifying {MMD GANs}},
  author={Bi{\'n}kowski, Miko{\l}aj and Sutherland, Danica J and Arbel, Michael and Gretton, Arthur},
  journal={arXiv preprint arXiv:1801.01401},
  year={2018}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@article{salimans2016improved,
  title={Improved techniques for training {GANs}},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{barratt2018note,
  title={A note on the inception score},
  author={Barratt, Shane and Sharma, Rishi},
  journal={arXiv preprint arXiv:1801.01973},
  year={2018}
}

@InProceedings{bachman2015variational,
  title = 	 {Variational Generative Stochastic Networks with Collaborative Shaping},
  author = 	 {Bachman, Philip and Precup, Doina},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1964--1972},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
   abstract = 	 {We develop an approach to training generative models based on unrolling a variational auto-encoder into a Markov chain, and shaping the chain’s trajectories using a technique inspired by recent work in Approximate Bayesian computation. We show that the global minimizer of the resulting objective is achieved when the generative model reproduces the target distribution. To allow finer control over the behavior of the models, we add a regularization term inspired by techniques used for regularizing certain types of policy search in reinforcement learning. We present empirical results on the MNIST and TFD datasets which show that our approach offers state-of-the-art performance, both quantitatively and from a qualitative point of view.}
}


@article{parzen1962estimation,
  title={On estimation of a probability density function and mode},
  author={Parzen, Emanuel},
  journal={The annals of mathematical statistics},
  volume={33},
  number={3},
  pages={1065--1076},
  year={1962},
  publisher={JSTOR}
}

@inproceedings{abadi2016tensorflow,
  title={$\{$TensorFlow$\}$: a system for $\{$Large-Scale$\}$ machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX symposium on operating systems design and implementation (OSDI 16)},
  pages={265--283},
  year={2016}
}

@misc{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Quan, John and Papamakarios, George and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Wang, Luyu and Stokowiec, Wojciech and Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@misc{bradbury2018jax,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{paszke2019pytorch,
  title={{Pytorch: An imperative style, high-performance deep learning library}},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yousefpour2021opacus,
  title={{Opacus: User-friendly differential privacy library in PyTorch}},
  author={Yousefpour, Ashkan and Shilov, Igor and Sablayrolles, Alexandre and Testuggine, Davide and Prasad, Karthik and Malek, Mani and Nguyen, John and Ghosh, Sayan and Bharadwaj, Akash and Zhao, Jessica and others},
  journal={arXiv preprint arXiv:2109.12298},
  year={2021}
}

@article{kloek1978bayesian,
  title={Bayesian estimates of equation system parameters: an application of integration by Monte Carlo},
  author={Kloek, Tuen and Van Dijk, Herman K},
  journal={Econometrica: Journal of the Econometric Society},
  pages={1--19},
  year={1978},
  publisher={JSTOR}
}

@techreport{goertzel1949quota,
  title={Quota sampling and importance functions in stochastic solution of particle problems},
  author={Goertzel, Gerald},
  year={1949}
}

@article{kahn1951estimation,
  title={Estimation of particle transmission by random sampling},
  author={Kahn, Herman and Harris, Theodore E},
  journal={National Bureau of Standards applied mathematics series},
  volume={12},
  pages={27--30},
  year={1951}
}

@inproceedings{renyi1961measures,
  title={On measures of entropy and information},
  author={R{\'e}nyi, Alfr{\'e}d and others},
  booktitle={Proceedings of the fourth Berkeley symposium on mathematical statistics and probability},
  volume={1},
  pages={547-561},
  year={1961},
  organization={Berkeley, California, USA}
}

@inproceedings{xu2021multi,
  title={Multi-VAE: Learning disentangled view-common and view-peculiar visual representations for multi-view clustering},
  author={Xu, Jie and Ren, Yazhou and Tang, Huayi and Pu, Xiaorong and Zhu, Xiaofeng and Zeng, Ming and He, Lifang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9234--9243},
  year={2021}
}

@article{yan2021deep,
  title={Deep multi-view learning methods: a review},
  author={Yan, Xiaoqiang and Hu, Shizhe and Mao, Yiqiao and Ye, Yangdong and Yu, Hui},
  journal={Neurocomputing},
  volume={448},
  pages={106--129},
  year={2021},
  publisher={Elsevier}
}

@article{quinlan1986induction,
  title={Induction of decision trees},
  author={Quinlan, J. Ross},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={81--106},
  year={1986},
  publisher={Springer}
}

@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  number={4},
  year={2006},
  publisher={Springer}
}

@misc{papernot2017semisupervised,
	archiveprefix = {arXiv},
	author = {Nicolas Papernot and Mart{\'\i}n Abadi and {\'U}lfar Erlingsson and Ian Goodfellow and Kunal Talwar},
	eprint = {1610.05755},
	primaryclass = {stat.ML},
	title = {Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data},
	year = {2017}}

@inproceedings{song2013stochastic,
	abstract = {Differential privacy is a recent framework for computation on sensitive data, which has shown considerable promise in the regime of large datasets. Stochastic gradient methods are a popular approach for learning in the data-rich regime because they are computationally tractable and scalable. In this paper, we derive differentially private versions of stochastic gradient descent, and test them empirically. Our results show that standard SGD experiences high variability due to differential privacy, but a moderate increase in the batch size can improve performance significantly.},
	author = {Song, Shuang and Chaudhuri, Kamalika and Sarwate, Anand D.},
	booktitle = {2013 {IEEE} {Global} {Conference} on {Signal} and {Information} {Processing}},
	doi = {10.1109/GlobalSIP.2013.6736861},
	file = {IEEE Xplore Full Text PDF:/Users/conor/Zotero/storage/9SPEADCE/Song et al. - 2013 - Stochastic gradient descent with differentially pr.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/conor/Zotero/storage/4ZBUBZ64/6736861.html:text/html},
	keywords = {Algorithm design and analysis, Data privacy, Linear programming, Logistics, Noise, Privacy, Signal processing algorithms},
	month = dec,
	pages = {245--248},
	title = {Stochastic gradient descent with differentially private updates},
	year = {2013}}

@article{balle2018privacy,
  title={{Privacy amplification by subsampling: Tight analyses via couplings and divergences}},
  author={Balle, Borja and Barthe, Gilles and Gaboardi, Marco},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{tran2020bayesian,
  title={Bayesian deep net GLM and GLMM},
  author={Tran, M-N and Nguyen, Nghia and Nott, David and Kohn, Robert},
  journal={Journal of Computational and Graphical Statistics},
  volume={29},
  number={1},
  pages={97--113},
  year={2020},
  publisher={Taylor \& Francis}
}

@misc{kasiviswanathan2011what,
	abstract = {Learning problems form an important category of computational tasks that generalizes many of the computations researchers apply to large real-life data sets. We ask, What concept classes can be learned privately, namely, by an algorithm whose output does not depend too heavily on any one input or specific training example? More precisely, we investigate learning algorithms that satisfy differential privacy, a notion that provides strong confidentiality guarantees in contexts where aggregate information is released about a database containing sensitive information about individuals. Our goal is a broad understanding of the resources required for private learning in terms of samples, computation time, and interaction. We demonstrate that, ignoring computational constraints, it is possible to privately agnostically learn any concept class using a sample size approximately logarithmic in the cardinality of the concept class. Therefore, almost anything learnable is learnable privately: specifically, if a concept class is learnable by a (nonprivate) algorithm with polynomial sample complexity and output size, then it can be learned privately using a polynomial number of samples. We also present a computationally efficient private probabilistically approximately correct learner for the class of parity functions. This result dispels the similarity between learning with noise and private learning (both must be robust to small changes in inputs), since parity is thought to},
	author = {Kasiviswanathan, Shiva Prasad and Lee, Homin K. and Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
	file = {Citeseer - Snapshot:/Users/conor/Zotero/storage/ZCEQKFJH/summary.html:text/html;Citeseer - Full Text PDF:/Users/conor/Zotero/storage/4U5FGIVR/Kasiviswanathan et al. - 2011 - What Can We Learn Priva℡y.pdf:application/pdf},
	title = {What {Can} {We} {Learn} {Privacy}},
	year = {2011}}

@misc{dwork2016concentrated,
	file = {dblp\: Concentrated Differential Privacy.:/Users/conor/Zotero/storage/I97RNLB5/DworkR16.html:text/html},
	title = {dblp: {Concentrated} {Differential} {Privacy}.}}

@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={9},
  number={3--4},
  pages={211--407},
  year={2014},
  publisher={Now Publishers, Inc.}
}
@article{wang2023differential,
  title={{Differential privacy in deep learning: Privacy and beyond}},
  author={Wang, Yanling and Wang, Qian and Zhao, Lingchen and Wang, Cong},
  journal={Future Generation Computer Systems},
  year={2023},
  publisher={Elsevier}
}
@inproceedings{aitsam2022differential,
  title={Differential {P}rivacy made easy},
  author={Aitsam, Muhammad},
  booktitle={International Conference on Emerging Trends in Electrical, Control, and Telecommunication Engineering},
  year={2022},
  organization={IEEE}
}

@techreport{bun2016concentrated,
	abstract = {"Concentrated differential privacy" was recently introduced by Dwork and Rothblum as a relaxation of differential privacy, which permits sharper analyses of many privacy-preserving computations. We present an alternative formulation of the concept of concentrated differential privacy in terms of the Renyi divergence between the distributions obtained by running an algorithm on neighboring inputs. With this reformulation in hand, we prove sharper quantitative results, establish lower bounds, and raise a few new questions. We also unify this approach with approximate differential privacy by giving an appropriate definition of "approximate concentrated differential privacy."},
	author = {Bun, Mark and Steinke, Thomas},
	keywords = {differential privacy, lower bounds},
	number = {816},
	shorttitle = {Concentrated {Differential} {Privacy}},
	title = {Concentrated {Differential} {Privacy}: {Simplifications}, {Extensions}, and {Lower} {Bounds}},
	year = {2016}}

@inproceedings{balle2018private,
	author = {Balle, Borja and Barthe, Gilles and Gaboardi, Marco},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	file = {Full Text PDF:/Users/conor/Zotero/storage/7KZERVJL/Balle et al. - 2018 - Privacy Amplification by Subsampling Tight Analys.pdf:application/pdf},
	publisher = {Curran Associates, Inc.},
	shorttitle = {Privacy {Amplification} by {Subsampling}},
	title = {Privacy {Amplification} by {Subsampling}: {Tight} {Analyses} via {Couplings} and {Divergences}}}

@inproceedings{wang2019subsampled,
	abstract = {We study the problem of subsampling in differential privacy (DP), a question that is the centerpiece behind many successful differentially private machine learning algorithms.  Specifically, we provide a tight upper bound on the Renyi Differential Privacy (RDP) [Mironov 2017] parameters for algorithms that: (1) subsample the dataset, and then (2) applies a randomized mechanism M to the subsample, in terms of the RDP parameters of M and the subsampling probability parameter. Our results generalize the moments accounting technique, developed by [Abadi et al. 2016] for the Gaussian mechanism, to any subsampled RDP mechanism.},
	author = {Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
	booktitle = {Proceedings of the {Twenty}-{Second} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	language = {en},
	month = apr,
	note = {ISSN: 2640-3498},
	pages = {1226--1235},
	publisher = {PMLR},
	title = {Subsampled {Renyi} {Differential} {Privacy} and {Analytical} {Moments} {Accountant}},
	year = {2019}}

@inproceedings{bassily2014private,
	abstract = {Convex empirical risk minimization is a basic tool in machine learning and statistics. We provide new algorithms and matching lower bounds for differentially private convex empirical risk minimization assuming only that each data point's contribution to the loss function is Lipschitz and that the domain of optimization is bounded. We provide a separate set of algorithms and matching lower bounds for the setting in which the loss functions are known to also be strongly convex.},
	address = {Philadelphia, PA, USA},
	author = {Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
	booktitle = {2014 {IEEE} 55th {Annual} {Symposium} on {Foundations} of {Computer} {Science}},
	doi = {10.1109/FOCS.2014.56},
	file = {Bassily et al. - 2014 - Private Empirical Risk Minimization Efficient Alg.pdf:/Users/conor/Zotero/storage/JJZKJCJP/Bassily et al. - 2014 - Private Empirical Risk Minimization Efficient Alg.pdf:application/pdf},
	isbn = {978-1-4799-6517-5},
	language = {en},
	month = oct,
	pages = {464--473},
	publisher = {IEEE},
	shorttitle = {Private {Empirical} {Risk} {Minimization}},
	title = {Private {Empirical} {Risk} {Minimization}: {Efficient} {Algorithms} and {Tight} {Error} {Bounds}},
	year = {2014}}

@misc{ruder_2016_gradient_descent,
	abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
	author = {Ruder, Sebastian},
	file = {Snapshot:/Users/conor/Zotero/storage/NE2SVX22/1609.html:text/html;Full Text PDF:/Users/conor/Zotero/storage/IK3VA933/Ruder - 2016 - An overview of gradient descent optimization algor.pdf:application/pdf},
	language = {en},
	month = sep,
	title = {An overview of gradient descent optimization algorithms},
	year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1609.04747v2}}

@inproceedings{dwork2018prediction,
  title={Privacy-preserving prediction},
  author={Dwork, Cynthia and Feldman, Vitaly},
  booktitle={Conference On Learning Theory},
  pages={1693--1702},
  year={2018},
  organization={PMLR}
}

@inproceedings{bassily2018agnostic,
	author = {Bassily, Raef and Thakkar, Om and Guha Thakurta, Abhradeep},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	file = {Full Text PDF:/Users/conor/Zotero/storage/GPWS6VMA/Bassily et al. - 2018 - Model-Agnostic Private Learning.pdf:application/pdf},
	publisher = {Curran Associates, Inc.},
    year = {2018},
	title = {Model-{Agnostic} {Private} {Learning}}}

@article{papernot2018scalable,
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ``student'' model the knowledge of an ensemble of ``teacher'' models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, {\'U}lfar},
	file = {Papernot et al. - 2018 - SCALABLE PRIVATE LEARNING WITH PATE.pdf:/Users/conor/Zotero/storage/V69QUJJX/Papernot et al. - 2018 - SCALABLE PRIVATE LEARNING WITH PATE.pdf:application/pdf},
	language = {en},
	pages = {34},
	title = {{SCALABLE} {PRIVATE} {LEARNING} {WITH} {PATE}},
    journal = {International {C}onference on {L}earning {R}epresentations}, 
	year = {2018}}

@inproceedings{dwork2009differential,
  title={Differential privacy and robust statistics},
  author={Dwork, Cynthia and Lei, Jing},
  booktitle={Proceedings of the forty-first annual ACM symposium on Theory of computing},
  pages={371--380},
  year={2009}
}

@article{dwork2014differential,
	author = {Dwork, Cynthia and Roth, Aaron},
	issn = {1551-305X},
	journal = {Foundations and Trends{\textregistered} in Theoretical Computer Science},
	number = {3--4},
	pages = {211--407},
	title = {The {Algorithmic} {Foundations} of {Differential} {Privacy}},
	volume = {9},
	year = {2014}}

@inproceedings{langley1994oblivious,
	author = {Langley, Pat and Sage, Stephanie},
	booktitle = {Working notes of the AAAI-94 workshop on case-based reasoning},
	organization = {Seattle, WA},
	pages = {113--117},
	title = {Oblivious decision trees and abstract cases},
	year = {1994}}

@inproceedings{chen2018neural,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	file = {Full Text PDF:/Users/conor/Zotero/storage/HTILDMBK/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:application/pdf},
	publisher = {Curran Associates, Inc.},
	title = {Neural {Ordinary} {Differential} {Equations}},
	volume = {31},
	year = {2018}}

@article{ruschendorf2009distributional,
	abstract = {We review the distributional transform of a random variable, some of its applications, and some related multivariate distributional transformations. The distributional transform is a useful tool, which allows in many respects to deal with general distributions in the same way as with continuous distributions. In particular it allows to give a simple proof of Sklar's Theorem in the general case. It has been used in the literature for stochastic ordering results. It is also useful for an adequate definition of the conditional value at risk measure and for many further purposes. We also discuss the multivariate quantile transform as well as the multivariate extension of the distributional transform and some of their applications. In the final section we consider an application to an extension of a limit theorem for the empirical copula process, also called empirical dependence function, to general not necessarily continuous distributions. This is useful for constructing and analyzing tests of dependence properties for general distributions.},
	author = {R{\"u}schendorf, Ludger},
	file = {Citeseer - Full Text PDF:/Users/conor/Zotero/storage/TUHADYGU/R{\"u}schendorf - 2009 - On the distributional transform, Sklar's Theorem, .pdf:application/pdf},
	journal = {Journal of Statistical Planning and Inference},
	pages = {3927},
	title = {On the distributional transform, {Sklar}'s {Theorem}, and the empirical copula process},
	year = {2009}}

@article{popov2019neural,
	abstract = {Nowadays, deep neural networks (DNNs) have become the main instrument for machine learning tasks within a wide range of domains, including vision, NLP, and speech. Meanwhile, in an important case of heterogenous tabular data, the advantage of DNNs over shallow counterparts remains questionable. In particular, there is no sufficient evidence that deep learning machinery allows constructing methods that outperform gradient boosting decision trees (GBDT), which are often the top choice for tabular problems. In this paper, we introduce Neural Oblivious Decision Ensembles (NODE), a new deep learning architecture, designed to work with any tabular data. In a nutshell, the proposed NODE architecture generalizes ensembles of oblivious decision trees, but benefits from both end-to-end gradient-based optimization and the power of multi-layer hierarchical representation learning. With an extensive experimental comparison to the leading GBDT packages on a large number of tabular datasets, we demonstrate the advantage of the proposed NODE architecture, which outperforms the competitors on most of the tasks. We open-source the PyTorch implementation of NODE and believe that it will become a universal framework for machine learning on tabular data.},
	author = {Popov, Sergei and Morozov, Stanislav and Babenko, Artem},
	file = {arXiv Fulltext PDF:/Users/conor/Zotero/storage/63ZYSBUD/Popov et al. - 2019 - Neural Oblivious Decision Ensembles for Deep Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/conor/Zotero/storage/TMHA6Z2R/1909.html:text/html},
	journal = {arXiv:1909.06312 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	month = sep,
	note = {arXiv: 1909.06312},
	title = {Neural {Oblivious} {Decision} {Ensembles} for {Deep} {Learning} on {Tabular} {Data}},
	year = {2019}}

@article{kamthe2021copula,
	annote = {Comment: Working paper},
	author = {Kamthe, Sanket and Assefa, Samuel and Deisenroth, Marc},
	journal = {arXiv:2101.00598 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications, Statistics - Machine Learning},
	month = jan,
	note = {arXiv: 2101.00598},
	title = {Copula {Flows} for {Synthetic} {Data} {Generation}},
	year = {2021}}

@inproceedings{vardhan2020synthetic,
	author = {Vardhan, L Vivek Harsha and Kok, Stanley},
	booktitle = {Proceedings of the Workshop on Economics of Privacy and Data Labor at the 37 th International Conference on Machine Learning},
	title = {Generating privacy-preserving synthetic tabular data using oblivious variational autoencoders},
	year = {2020}}

@inproceedings{lee2021invertible,
	author = {Lee, Jaehoon and Hyeong, Jihyeon and Jeon, Jinsung and Park, Noseong and Cho, Jihoon},
	language = {en},
	month = may,
	shorttitle = {Invertible {Tabular} {GANs}},
	title = {Invertible {Tabular} {GANs}: {Killing} {Two} {Birds} with {One} {Stone} for {Tabular} {Data} {Synthesis}},
	urldate = {2022-01-20},
	year = {2021}}

@article{li2021inverse,
	abstract = {Designing a generative model to synthesize realistic tabular data is of great significance in data science. Existing tabular data generative models have difficulty in handling complicated and diverse marginal distribution types due to the gradient vanishing problem, and these models pay little attention to the correlation between attributes. We propose a method that improves the generative adversarial network (GAN) with inverse cumulative distribution function for tabular data synthesis. This method first transforms continuous columns into uniform distribution data by using the cumulative distribution function, which can alleviate the gradient vanishing problem in model training. Then the method trains GAN with the transformed data, where the discriminator with label reconstruction function is presented to model the correlation among attributes accurately by introducing an auxiliary supervised task to help the correlations extraction. After that, we train a neural network for each continuous column to perform the inverse transformation of generated data into the target distribution, thereby the synthetic data is obtained. Experiments on simulated and real-world datasets show that our method compares favorably against the state-of-the-art methods in modeling tabular data.},
	author = {Li, Ban and Luo, Senlin and Qin, Xiaonan and Pan, Limin},
	doi = {10.1016/j.neucom.2021.05.098},
	file = {ScienceDirect Snapshot:/Users/conor/Zotero/storage/VCSVKDQX/S0925231221008614.html:text/html},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Cumulative distribution function, Data synthesis, Generative adversarial network, Tabular data},
	language = {en},
	month = oct,
	pages = {373--383},
	title = {Improving {GAN} with inverse cumulative distribution function for tabular data synthesis},
	volume = {456},
	year = {2021}}

@article{haghani2017artificial,
	author = {Haghani, Shima and Sedehi, Morteza and Kheiri, Soleiman},
	journal = {Journal of research in health sciences},
	number = {3},
	pages = {392},
	publisher = {Hamadan University of Medical Sciences},
	title = {Artificial neural network to modeling zero-inflated count data: Application to predicting number of return to blood donation},
	volume = {17},
	year = {2017}}

@inproceedings{lee2022differentially,
  title={Differentially private normalizing flows for synthetic tabular data generation},
  author={Lee, Jaewoo and Kim, Minjung and Jeong, Yonghyun and Ro, Youngmin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={7345--7353},
  year={2022}
}

@article{fallah2009nonlinear,
	author = {Fallah, Nader and Gu, Hong and Mohammad, Kazem and Seyyedsalehi, Seyyed Ali and Nourijelyani, Keramat and Eshraghian, Mohammad Reza},
	journal = {Neural Computing and Applications},
	number = {8},
	pages = {939--943},
	publisher = {Springer},
	title = {Nonlinear Poisson regression using neural networks: a simulation study},
	volume = {18},
	year = {2009}}

@article{cao2020rank,
	author = {Cao, Wenzhi and Mirjalili, Vahid and Raschka, Sebastian},
	doi = {10.1016/j.patrec.2020.11.008},
	issn = {0167-8655},
	journal = {Pattern Recognition Letters},
	month = {Dec},
	pages = {325--331},
	publisher = {Elsevier BV},
	title = {Rank consistent ordinal regression for neural networks with application to age estimation},
	url = {http://dx.doi.org/10.1016/j.patrec.2020.11.008},
	volume = {140},
	year = {2020},
	Bdsk-Url-1 = {http://dx.doi.org/10.1016/j.patrec.2020.11.008}}

@inproceedings{niu2016ordinal,
	author = {Niu, Zhenxing and Zhou, Mo and Wang, Le and Gao, Xinbo and Hua, Gang},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	title = {Ordinal Regression With Multiple Output CNN for Age Estimation},
	year = {2016}}

@techreport{ghahramani1996algorithm,
	author = {Ghahramani, Zoubin and Hinton, Geoffrey E and others},
	institution = {Technical Report CRG-TR-96-1, University of Toronto},
	title = {{The EM algorithm for mixtures of factor analyzers}},
	year = {1996}}

@article{tipping1999probabilistic,
	author = {Tipping, Michael E and Bishop, Christopher M},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	number = {3},
	pages = {611--622},
	publisher = {Wiley Online Library},
	title = {Probabilistic principal component analysis},
	volume = {61},
	year = {1999}}

@inproceedings{ranganath2015deep,
	author = {Ranganath, Rajesh and Tang, Linpeng and Charlin, Laurent and Blei, David},
	booktitle = {Artificial Intelligence and Statistics},
	organization = {PMLR},
	pages = {762--771},
	title = {Deep exponential families},
	year = {2015}}

@article{neal1990learning,
	author = {Neal, Radford M},
	journal = {Department of Computer Science, University of Toronto},
	number = {1283},
	pages = {1577},
	publisher = {Citeseer},
	title = {Learning stochastic feedforward networks},
	volume = {64},
	year = {1990}}

@article{dempster1977maximum,
	author = {Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {1},
	pages = {1--22},
	publisher = {Wiley Online Library},
	title = {Maximum likelihood from incomplete data via the EM algorithm},
	volume = {39},
	year = {1977}}

@inproceedings{ranganath2014black,
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
	booktitle = {Artificial intelligence and statistics},
	organization = {PMLR},
	pages = {814--822},
	title = {Black box variational inference},
	year = {2014}}

@article{blei2017variational,
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	doi = {10.1080/01621459.2017.1285773},
	issn = {1537-274X},
	journal = {Journal of the American Statistical Association},
	month = {Apr},
	number = {518},
	pages = {859--877},
	publisher = {Informa UK Limited},
	title = {Variational Inference: A Review for Statisticians},
	volume = {112},
	year = {2017}}

@article{hornik1989multilayer,
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	journal = {Neural networks},
	number = {5},
	pages = {359--366},
	publisher = {Elsevier},
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	year = {1989}}

@book{wainwright2008graphical,
	author = {Wainwright, Martin J and Jordan, Michael Irwin},
	publisher = {Now Publishers Inc},
	title = {Graphical models, exponential families, and variational inference},
	year = {2008}}

@article{dankar2013practicing,
	author = {Dankar, Fida Kamal and El Emam, Khaled},
	journal = {Trans. Data Priv.},
	number = {1},
	pages = {35--67},
	title = {Practicing differential privacy in health care: A review.},
	volume = {6},
	year = {2013}}

@article{buczak2010data,
	author = {Buczak, Anna L and Babin, Steven and Moniz, Linda},
	journal = {BMC medical informatics and decision making},
	number = {1},
	pages = {1--28},
	publisher = {BioMed Central},
	title = {Data-driven approach for creating synthetic electronic medical records},
	volume = {10},
	year = {2010}}

@article{walonoski2018synthea,
	author = {Walonoski, Jason and Kramer, Mark and Nichols, Joseph and Quina, Andre and Moesel, Chris and Hall, Dylan and Duffett, Carlton and Dube, Kudakwashe and Gallagher, Thomas and McLachlan, Scott},
	journal = {Journal of the American Medical Informatics Association},
	number = {3},
	pages = {230--238},
	publisher = {Oxford University Press},
	title = {Synthea: An approach, method, and software mechanism for generating synthetic patients and the synthetic electronic health care record},
	volume = {25},
	year = {2018}}

@article{price2019privacy,
	author = {Price, W Nicholson and Cohen, I Glenn},
	journal = {Nature medicine},
	number = {1},
	pages = {37--43},
	publisher = {Nature Publishing Group},
	title = {Privacy in the age of medical big data},
	volume = {25},
	year = {2019}}

@article{abouelmehdi2018big,
	author = {Abouelmehdi, Karim and Beni-Hessane, Abderrahim and Khaloufi, Hayat},
	journal = {Journal of Big Data},
	number = {1},
	pages = {1--18},
	publisher = {Springer},
	title = {Big healthcare data: preserving security and privacy},
	volume = {5},
	year = {2018}}

@misc{bondtaylor2021deep,
	archiveprefix = {arXiv},
	author = {Sam Bond-Taylor and Adam Leach and Yang Long and Chris G. Willcocks},
	eprint = {2103.04922},
	primaryclass = {cs.LG},
	title = {{Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models}},
	year = {2021}}

@misc{wilde2020foundations,
	archiveprefix = {arXiv},
	author = {Harrison Wilde and Jack Jewson and Sebastian Vollmer and Chris Holmes},
	eprint = {2011.08299},
	primaryclass = {cs.LG},
	title = {Foundations of Bayesian Learning from Synthetic Data},
	year = {2020}}

@article{burgard2017synthetic,
	author = {Burgard, Jan Pablo and Kolb, Jan-Philipp and Merkle, Hariolf and M{\"u}nnich, Ralf},
	journal = {AStA Wirtschafts-und Sozialstatistisches Archiv},
	number = {3},
	pages = {233--244},
	publisher = {Springer},
	title = {Synthetic data for open and reproducible methodological research in social sciences and official statistics},
	volume = {11},
	year = {2017}}

@article{nowok2016synthpop,
	author = {Nowok, Beata and Raab, Gillian M and Dibben, Chris},
	journal = {Journal of statistical software},
	number = {1},
	pages = {1--26},
	title = {{synthpop: Bespoke creation of synthetic data in R}},
	volume = {74},
	year = {2016}}

@inproceedings{abowd2018us,
	author = {Abowd, John M},
	booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages = {2867--2867},
	title = {The US Census Bureau adopts differential privacy},
	year = {2018}}

@inproceedings{dwork2019differential,
	author = {Dwork, Cynthia},
	booktitle = {Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
	pages = {1--1},
	title = {Differential privacy and the {US} census},
	year = {2019}}

@article{robbins1951stochastic,
	author = {Herbert Robbins and Sutton Monro},
	doi = {10.1214/aoms/1177729586},
	journal = {The Annals of Mathematical Statistics},
	number = {3},
	pages = {400 -- 407},
	publisher = {Institute of Mathematical Statistics},
	title = {{A Stochastic Approximation Method}},
	volume = {22},
	year = {1951}}

@article{rumelhart1986learning,
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	journal = {nature},
	number = {6088},
	pages = {533--536},
	publisher = {Nature Publishing Group},
	title = {Learning representations by back-propagating errors},
	volume = {323},
	year = {1986}}

@article{baydin2018automatic,
	author = {Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
	journal = {Journal of Machine Learning Research},
	number = {153},
	pages = {1-43},
	title = {{Automatic Differentiation in Machine Learning: a Survey}},
	volume = {18},
	year = {2018}}

@inproceedings{nair2010rectified,
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	address = {Madison, WI, USA},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	isbn = {9781605589077},
	location = {Haifa, Israel},
	numpages = {8},
	pages = {807--814},
	publisher = {Omnipress},
	series = {ICML'10},
	title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
	year = {2010}}

@article{tucker2020generating,
	author = {Tucker, Allan and Wang, Zhenchen and Rotalinti, Ylenia and Myles, Puja},
	journal = {NPJ digital medicine},
	number = {1},
	pages = {1--13},
	publisher = {Nature Publishing Group},
	title = {Generating high-fidelity synthetic patient data for assessing machine learning healthcare software},
	volume = {3},
	year = {2020}}

@article{hernandez2020privacy,
	author = {Hernandez, John B and Sadilek, Adam and Liu, Luyang and Nguyen, Dung and Kamruzzaman, Methun and Rader, Benjamin and Ingerman, Alex and Mellem, Stefan and Kairouz, Peter and Nsoesie, Elaine O and others},
	journal = {medRxiv},
	publisher = {Cold Spring Harbor Laboratory Press},
	title = {Privacy-first health research with federated learning},
	year = {2020}}

@article{dyda2021differential,
	doi = {https://doi.org/10.1016/j.patter.2021.100366},
	issn = {2666-3899},
	journal = {Patterns},
	keywords = {surveillance, data privacy, COVID-19},
	number = {12},
	pages = {100366},
	title = {Differential privacy for public health data: An innovative tool to optimize information sharing while protecting data confidentiality},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389921002282},
	volume = {2},
	year = {2021},
	Bdsk-Url-1 = {https://www.sciencedirect.com/science/article/pii/S2666389921002282},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.patter.2021.100366}}

@inproceedings{adibuzzaman2017big,
	author = {Adibuzzaman, Mohammad and DeLaurentis, Poching and Hill, Jennifer and Benneyworth, Brian D},
	booktitle = {AMIA Annual Symposium Proceedings},
	organization = {American Medical Informatics Association},
	pages = {384},
	title = {Big data in healthcare--the promises, challenges and opportunities from a research perspective: A case study with a model database},
	volume = {2017},
	year = {2017}}

@misc{viroli2017deep,
	archiveprefix = {arXiv},
	author = {Cinzia Viroli and Geoffrey J. McLachlan},
	eprint = {1711.06929},
	primaryclass = {stat.ML},
	title = {{Deep Gaussian Mixture Models}},
	year = {2017}}

@misc{rezende2016variational,
	archiveprefix = {arXiv},
	author = {Danilo Jimenez Rezende and Shakir Mohamed},
	eprint = {1505.05770},
	primaryclass = {stat.ML},
	title = {Variational Inference with Normalizing Flows},
	year = {2016}}

@article{su2019fvae,
	author = {Jianlin Su and Guang Wu},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1809-05861.bib},
	eprint = {1809.05861},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
	title = {f-VAEs: Improve VAEs with Conditional Flows},
	url = {http://arxiv.org/abs/1809.05861},
	volume = {abs/1809.05861},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1809.05861}}

@book{duda1973pattern,
	author = {Duda, Richard O and Hart, Peter E and others},
	publisher = {Wiley New York},
	title = {Pattern classification and scene analysis},
	volume = {3},
	year = {1973}}

@book{mengersen2011mixtures,
	author = {Mengersen, Kerrie L and Robert, Christian and Titterington, Mike},
	publisher = {John Wiley \& Sons},
	title = {Mixtures: estimation and applications},
	volume = {896},
	year = {2011}}

@book{bartholomew2011latent,
	author = {Bartholomew, D.J. and Knott, M. and Moustaki, I.},
	isbn = {9781119973706},
	lccn = {2011007711},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Statistics},
	title = {Latent Variable Models and Factor Analysis: A Unified Approach},
	year = {2011}}

@misc{dolatabadi2020invertible,
	archiveprefix = {arXiv},
	author = {Hadi M. Dolatabadi and Sarah Erfani and Christopher Leckie},
	eprint = {2001.05168},
	primaryclass = {stat.ML},
	title = {Invertible Generative Modeling using Linear Rational Splines},
	year = {2020}}

@misc{durkan2019neural,
	archiveprefix = {arXiv},
	author = {Conor Durkan and Artur Bekasov and Iain Murray and George Papamakarios},
	eprint = {1906.04032},
	primaryclass = {stat.ML},
	title = {Neural Spline Flows},
	year = {2019}}

@misc{durkan2019cubic,
	archiveprefix = {arXiv},
	author = {Conor Durkan and Artur Bekasov and Iain Murray and George Papamakarios},
	eprint = {1906.02145},
	primaryclass = {stat.ML},
	title = {Cubic-Spline Flows},
	year = {2019}}

@article{muller2019neural,
	author = {Thomas M{\"{u}}ller and Brian McWilliams and Fabrice Rousselle and Markus Gross and Jan Nov{\'{a}}k},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1808-03856.bib},
	eprint = {1808.03856},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Sat, 23 Jan 2021 01:11:14 +0100},
	title = {Neural Importance Sampling},
	volume = {abs/1808.03856},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1808.03856}}

@inproceedings{papamakarios2017masked,
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Masked Autoregressive Flow for Density Estimation},
	volume = {30},
	year = {2017}}

@inproceedings{vandenoord2016wavenet,
	author = {A{\"a}ron van den Oord and Sander Dieleman and Heiga Zen and Karen Simonyan and Oriol Vinyals and Alexander Graves and Nal Kalchbrenner and Andrew Senior and Koray Kavukcuoglu},
	booktitle = {Arxiv},
	title = {WaveNet: A Generative Model for Raw Audio},
	url = {https://arxiv.org/abs/1609.03499},
	year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1609.03499}}

@inproceedings{vandenoord2016pixel,
	author = {A{\"a}ron van den Oord and Nal Kalchbrenner},
	booktitle = {ICML},
	title = {Pixel RNN},
	year = {2016}}

@inproceedings{germain2015made,
  title={{MADE: Masked autoencoder for distribution estimation}},
  author={Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle={International conference on machine learning},
  pages={881--889},
  year={2015},
  organization={PMLR}
}

@misc{kingma2014autoencoding,
	archiveprefix = {arXiv},
	author = {Diederik P Kingma and Max Welling},
	eprint = {1312.6114},
	primaryclass = {stat.ML},
	title = {Auto-Encoding Variational Bayes},
	year = {2014}}

@article{lecuyer1990unified,
	author = {L'Ecuyer, Pierre},
	journal = {Management Science},
	number = {11},
	pages = {1364--1383},
	publisher = {INFORMS},
	title = {A unified view of the {IPA, SF, and LR} gradient estimation techniques},
	volume = {36},
	year = {1990}}

@article{mohamed2020monte,
	author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
	journal = {J. Mach. Learn. Res.},
	number = {132},
	pages = {1--62},
	title = {Monte Carlo Gradient Estimation in Machine Learning.},
	volume = {21},
	year = {2020}}

@article{nelder1972glm,
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	author = {J. A. Nelder and R. W. M. Wedderburn},
	issn = {00359238},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	number = {3},
	pages = {370--384},
	publisher = {[Royal Statistical Society, Wiley]},
	title = {Generalized Linear Models},
	volume = {135},
	year = {1972}}

@article{haykin2004comprehensive,
	author = {Haykin, Simon and Network, N},
	journal = {Neural networks},
	number = {2004},
	pages = {41},
	title = {A comprehensive foundation},
	volume = {2},
	year = {2004}}

@article{nijenhuis1974strong,
	author = {Albert Nijenhuis},
	issn = {00029890, 19300972},
	journal = {The American Mathematical Monthly},
	number = {9},
	pages = {969--980},
	publisher = {Mathematical Association of America},
	title = {Strong Derivatives and Inverse Mappings},
	url = {http://www.jstor.org/stable/2319298},
	volume = {81},
	year = {1974},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2319298}}

@misc{park2016dpem,
	archiveprefix = {arXiv},
	author = {Mijung Park and Jimmy Foulds and Kamalika Chaudhuri and Max Welling},
	eprint = {1605.06995},
	primaryclass = {cs.LG},
	title = {{DP-EM: Differentially Private Expectation Maximization}},
	year = {2016}}

@inproceedings{wu2016differentially,
	author = {Yuncheng Wu and Yao Wu and Hui Peng and Juru Zeng and Hong Chen and Cuiping Li},
	booktitle = {2016 IEEE/ACM 24th International Symposium on Quality of Service (IWQoS)},
	doi = {10.1109/IWQoS.2016.7590445},
	pages = {1-6},
	title = {Differentially private density estimation via {Gaussian} mixtures model},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IWQoS.2016.7590445}}

@article{kamath2019differentially,
  title={{Differentially private algorithms for learning mixtures of separated Gaussians}},
  author={Kamath, Gautam and Sheffet, Or and Singhal, Vikrant and Ullman, Jonathan},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{lee2021invertibleNeurip,
  title={{Invertible tabular GANs: Killing two birds with one stone for tabular data synthesis}},
  author={Lee, Jaehoon and Hyeong, Jihyeon and Jeon, Jinsung and Park, Noseong and Cho, Jihoon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={4263--4273},
  year={2021}
}

@inproceedings{nissim2007smooth,
	address = {New York, NY, USA},
	author = {Nissim, Kobbi and Raskhodnikova, Sofya and Smith, Adam},
	booktitle = {Proceedings of the Thirty-Ninth Annual ACM Symposium on Theory of Computing},
	doi = {10.1145/1250790.1250803},
	isbn = {9781595936318},
	keywords = {clustering, privacy preserving data mining, sensitivity, output perturbation, private data analysis},
	location = {San Diego, California, USA},
	numpages = {10},
	pages = {75--84},
	publisher = {Association for Computing Machinery},
	series = {STOC '07},
	title = {Smooth Sensitivity and Sampling in Private Data Analysis},
	year = {2007}}

@inproceedings{xu2012differentially,
	author = {Xu, Jia and Zhang, Zhenjie and Xiao, Xiaokui and Yang, Yin and Yu, Ge},
	booktitle = {2012 IEEE 28th International Conference on Data Engineering},
	doi = {10.1109/ICDE.2012.48},
	pages = {32-43},
	title = {Differentially Private Histogram Publication},
	year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICDE.2012.48}}

@article{chawla2005on,
	author = {Chawla, Shuchi and Dwork, Cynthia and McSherry, Frank and Talwar, Kunal},
	month = {01},
	title = {On the Utility of Privacy-Preserving Histograms},
	year = {2005}}



@misc{xie2018differentially,
	archiveprefix = {arXiv},
	author = {Liyang Xie and Kaixiang Lin and Shu Wang and Fei Wang and Jiayu Zhou},
	eprint = {1802.06739},
	primaryclass = {cs.LG},
	title = {Differentially Private Generative Adversarial Network},
	year = {2018}}

@article{mikolov2012statistical,
	author = {Mikolov, Tom{\'a}{\v{s}} and others},
	journal = {Presentation at Google, Mountain View, 2nd April},
	pages = {26},
	title = {Statistical language models based on neural networks},
	volume = {80},
	year = {2012}}

@article{abadi2016deep,
	author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
	journal = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
	publisher = {ACM},
	title = {Deep {L}earning with {D}ifferential {P}rivacy},
	year = {2016}}

@misc{vandermaaten2020tradeoffs,
	archiveprefix = {arXiv},
	author = {Laurens van der Maaten and Awni Hannun},
	eprint = {2007.05089},
	primaryclass = {cs.LG},
	title = {The Trade-Offs of Private Prediction},
	year = {2020}}

@inproceedings{duchi2013local,
	author = {Duchi, John C. and Jordan, Michael I. and Wainwright, Martin J.},
	booktitle = {2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
	doi = {10.1109/FOCS.2013.53},
	pages = {429-438},
	title = {Local Privacy and Statistical Minimax Rates},
	year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCS.2013.53}}

@inproceedings{dwork2010boosting,
	author = {Dwork, Cynthia and Rothblum, Guy N. and Vadhan, Salil},
	booktitle = {2010 IEEE 51st Annual Symposium on Foundations of Computer Science},
	doi = {10.1109/FOCS.2010.12},
	pages = {51-60},
	title = {Boosting and Differential Privacy},
	year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/FOCS.2010.12}}

@article{kairouz2017composition,
	abstract = {Sequential querying of differentially private mechanisms degrades the overall privacy level. In this paper, we answer the fundamental question of characterizing the level of overall privacy degradation as a function of the number of queries and the privacy levels maintained by each privatization mechanism. Our solution is complete: we prove an upper bound on the overall privacy level and construct a sequence of privatization mechanisms that achieves this bound. The key innovation is the introduction of an operational interpretation of differential privacy (involving hypothesis testing) and the use of a data processing inequality along with its converse. Our result improves over the state of the art, and has immediate connections to several problems studied in the literature.},
	author = {Kairouz, Peter and Oh, Sewoong and Viswanath, Pramod},
	file = {IEEE Xplore Full Text PDF:/Users/conor/Zotero/storage/ZZG2KWHG/Kairouz et al. - 2017 - The Composition Theorem for Differential Privacy.pdf:application/pdf},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Data privacy, Data processing, Databases, Differential privacy, Electronic mail, hypothesis testing, Privacy, Privatization, Testing},
	month = jun,
	note = {Conference Name: IEEE Transactions on Information Theory},
	number = {6},
	pages = {4037--4049},
	title = {The {Composition} {Theorem} for {Differential} {Privacy}},
	volume = {63},
	year = {2017}}

@inproceedings{mironov2017renyi,
	abstract = {We propose a natural relaxation of differential privacy based on the Re´nyi divergence. Closely related notions have appeared in several recent papers that analyzed composition of differentially private mechanisms. We argue that the useful analytical tool can be used as a privacy definition, compactly and accurately representing guarantees on the tails of the privacy loss. We demonstrate that the new definition shares many important properties with the standard definition of differential privacy, while additionally allowing tighter analysis of composite heterogeneous mechanisms.},
	address = {Santa Barbara, CA},
	author = {Mironov, Ilya},
	booktitle = {2017 {IEEE} 30th {Computer} {Security} {Foundations} {Symposium} ({CSF})},
	doi = {10.1109/CSF.2017.11},
	isbn = {978-1-5386-3217-8},
	language = {en},
	month = aug,
	pages = {263--275},
	publisher = {IEEE},
	title = {R{\'e}nyi {Differential} {Privacy}},
	year = {2017}}

@misc{bengio2013estimating,
	archiveprefix = {arXiv},
	author = {Yoshua Bengio and Nicholas L{\'e}onard and Aaron Courville},
	eprint = {1308.3432},
	primaryclass = {cs.LG},
	title = {Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
	year = {2013}}

@article{sweldens1998lifting,
	author = {Sweldens, Wim},
	journal = {SIAM journal on mathematical analysis},
	number = {2},
	pages = {511--546},
	publisher = {SIAM},
	title = {The lifting scheme: A construction of second generation wavelets},
	volume = {29},
	year = {1998}}

@book{kaplan2003advanced,
	author = {Kaplan, W.},
	isbn = {9780201799378},
	lccn = {2002018423},
	publisher = {Addison-Wesley},
	series = {Addison-Wesley higher mathematics},
	title = {Advanced Calculus},
	url = {https://books.google.com.au/books?id=wywnAQAAIAAJ},
	year = {2003},
	Bdsk-Url-1 = {https://books.google.com.au/books?id=wywnAQAAIAAJ}}

@misc{gulrajani2017improved,
	archiveprefix = {arXiv},
	author = {Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
	eprint = {1704.00028},
	primaryclass = {cs.LG},
	title = {Improved Training of {Wasserstein GANs}},
	year = {2017}}

@article{papamakarios2019normalizing,
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	journal = {arXiv preprint arXiv:1912.02762},
	title = {Normalizing flows for probabilistic modeling and inference},
	year = {2019}}

@article{winkler2019learning,
	author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
	journal = {arXiv preprint arXiv:1912.00042},
	title = {Learning likelihoods with conditional normalizing flows},
	year = {2019}}

@article{kingma2018glow,
	author = {Kingma, Diederik P and Dhariwal, Prafulla},
	journal = {arXiv preprint arXiv:1807.03039},
	title = {Glow: Generative flow with invertible 1x1 convolutions},
	year = {2018}}

@article{kingma2016improved,
	author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	journal = {Advances in neural information processing systems},
	pages = {4743--4751},
	title = {Improved variational inference with inverse autoregressive flow},
	volume = {29},
	year = {2016}}

@article{hoogeboom2019integer,
	author = {Hoogeboom, Emiel and Peters, Jorn WT and Berg, Rianne van den and Welling, Max},
	journal = {arXiv preprint arXiv:1905.07376},
	title = {Integer discrete flows and lossless compression},
	year = {2019}}

@article{dinh2016density,
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	journal = {arXiv preprint arXiv:1605.08803},
	title = {Density estimation using real {NVP}},
	year = {2016}}

@article{berg2018sylvester,
	author = {Berg, Rianne van den and Hasenclever, Leonard and Tomczak, Jakub M and Welling, Max},
	journal = {arXiv preprint arXiv:1803.05649},
	title = {Sylvester normalizing flows for variational inference},
	year = {2018}}

@inproceedings{behrmann2019invertible,
	author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky TQ and Duvenaud, David and Jacobsen, J{\"o}rn-Henrik},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {573--582},
	title = {Invertible residual networks},
	year = {2019}}

@article{theis2015note,
	author = {Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
	journal = {arXiv preprint arXiv:1511.01844},
	title = {A note on the evaluation of generative models},
	year = {2015}}

@article{mohamed2016learning,
	author = {Mohamed, Shakir and Lakshminarayanan, Balaji},
	journal = {arXiv preprint arXiv:1610.03483},
	title = {Learning in implicit generative models},
	year = {2016}}

@inproceedings{nowozin2016f,
	author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
	booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
	pages = {271--279},
	title = {f-gan: Training generative neural samplers using variational divergence minimization},
	year = {2016}}

@inproceedings{mescheder2018training,
	author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	booktitle = {International conference on machine learning},
	organization = {PMLR},
	pages = {3481--3490},
	title = {Which training methods for {gans} do actually converge?},
	year = {2018}}

@article{mackay1999density,
	author = {MacKay, David JC and Gibbs, Mark N},
	journal = {Statistics and neural networks: advances at the interface},
	pages = {129--145},
	publisher = {Oxford University Press on Demand},
	title = {Density networks},
	year = {1999}}

@inproceedings{li2015generative,
	author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
	booktitle = {International Conference on Machine Learning},
	organization = {PMLR},
	pages = {1718--1727},
	title = {Generative moment matching networks},
	year = {2015}}

@article{huszar2017variational,
	author = {Husz{\'a}r, Ferenc},
	journal = {arXiv preprint arXiv:1702.08235},
	title = {Variational inference using implicit distributions},
	year = {2017}}

@article{dziugaite2015training,
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M and Ghahramani, Zoubin},
	journal = {arXiv preprint arXiv:1505.03906},
	title = {Training generative neural networks via maximum mean discrepancy optimization},
	year = {2015}}

@article{dieng2019prescribed,
	author = {Dieng, Adji B and Ruiz, Francisco JR and Blei, David M and Titsias, Michalis K},
	journal = {arXiv preprint arXiv:1910.04302},
	title = {Prescribed generative adversarial networks},
	year = {2019}}

@article{el2021covid,
	author = {El Emam, Khaled and Mosquera, Lucy and Jonker, Elizabeth and Sood, Harpreet},
	journal = {JAMIA open},
	number = {1},
	pages = {ooab012},
	publisher = {Oxford University Press},
	title = {Evaluating the utility of synthetic COVID-19 case data},
	volume = {4},
	year = {2021}}

@article{el2020evaluating,
	author = {El Emam, Khaled and Mosquera, Lucy and Bass, Jason},
	journal = {Journal of medical Internet research},
	number = {11},
	pages = {e23139},
	publisher = {JMIR Publications Inc., Toronto, Canada},
	title = {Evaluating identity disclosure risk in fully synthetic health data: model development and validation},
	volume = {22},
	year = {2020}}

@inproceedings{yang2019grouped,
	author = {Yang, Fan and Yu, Zhongping and Liang, Yunfan and Gan, Xiaolu and Lin, Kaibiao and Zou, Quan and Zeng, Yifeng},
	booktitle = {2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
	organization = {IEEE},
	pages = {906--913},
	title = {Grouped correlational generative adversarial networks for discrete electronic health records},
	year = {2019}}

@article{fan2020relational,
	author = {Fan, Ju and Liu, Tongyu and Li, Guoliang and Chen, Junyou and Shen, Yuwei and Du, Xiaoyong},
	journal = {arXiv preprint arXiv:2008.12763},
	title = {Relational data synthesis using generative adversarial networks: A design space exploration},
	year = {2020}}

@phdthesis{brenninkmeijer2019generation,
	author = {Brenninkmeijer, Bauke and de Vries, A and Marchiori, E and Hille, Youri},
	school = {Master's Thesis, Radboud University, Nijmegen, The Netherlands, 2019.[Google~{\ldots}},
	title = {On the Generation and Evaluation of Tabular Data Using {gans}},
	year = {2019}}

@article{du2018learning,
	author = {Du, Chao and Xu, Kun and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
	journal = {arXiv preprint arXiv:1807.03870},
	title = {Learning implicit generative models by teaching explicit ones},
	year = {2018}}

@inproceedings{torfi2021evaluation,
	author = {Torfi, Amirsina and Beyki, Mohammadreza and Fox, Edward A},
	booktitle = {2020 25th International Conference on Pattern Recognition (ICPR)},
	organization = {IEEE},
	pages = {991--998},
	title = {On the Evaluation of Generative Adversarial Networks By Discriminative Models},
	year = {2021}}

@article{bissoto2019six,
	author = {Bissoto, Alceu and Valle, Eduardo and Avila, Sandra},
	journal = {arXiv preprint arXiv:1910.13076},
	title = {The six fronts of the generative adversarial networks},
	year = {2019}}

@article{camino2018generating,
	author = {Camino, Ramiro and Hammerschmidt, Christian and State, Radu},
	journal = {arXiv preprint arXiv:1807.01202},
	title = {Generating multi-categorical samples with generative adversarial networks},
	year = {2018}}

@inproceedings{kim2021oct,
	author = {Kim, Jayoung and Jeon, Jinsung and Lee, Jaehoon and Hyeong, Jihyeon and Park, Noseong},
	booktitle = {Proceedings of the Web Conference 2021},
	pages = {1506--1515},
	title = {OCT-GAN: Neural ODE-based Conditional Tabular {GANs}},
	year = {2021}}

@article{mendelevitch2020beyond,
	author = {Mendelevitch, Ofer and Lesh, Michael D},
	journal = {Security and Privacy From a Legal, Ethical, and Technical Perspective},
	pages = {141},
	publisher = {BoD--Books on Demand},
	title = {Beyond Differential Privacy: Synthetic Micro-Data Generation with Deep Generative Neural Networks},
	year = {2020}}

@article{zhao2021ctab,
	author = {Zhao, Zilong and Kunar, Aditya and Van der Scheer, Hiek and Birke, Robert and Chen, Lydia Y},
	journal = {arXiv preprint arXiv:2102.08369},
	title = {CTAB-GAN: Effective Table Data Synthesizing},
	year = {2021}}

@inproceedings{yale2019privacy,
	author = {Yale, Andrew and Dash, Saloni and Dutta, Ritik and Guyon, Isabelle and Pavao, Adrien and Bennett, Kristin},
	booktitle = {ESANN 2019-European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	title = {Privacy preserving synthetic health data},
	year = {2019}}

@article{rodriguez2017modernization,
	author = {Rodriguez, Ian M and Sexton12, William N and Singer, Phyllis E and Vilhuber, Lars},
	title = {The modernization of statistical disclosure limitation at the US Census Bureau}}

@article{snoke2018general,
	author = {Snoke, Joshua and Raab, Gillian M and Nowok, Beata and Dibben, Chris and Slavkovic, Aleksandra},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	number = {3},
	pages = {663--688},
	publisher = {Wiley Online Library},
	title = {General and specific utility measures for synthetic data},
	volume = {181},
	year = {2018}}

@article{beaulieu2019privacy,
	author = {Beaulieu-Jones, Brett K and Wu, Zhiwei Steven and Williams, Chris and Lee, Ran and Bhavnani, Sanjeev P and Byrd, James Brian and Greene, Casey S},
	journal = {Circulation: Cardiovascular Quality and Outcomes},
	number = {7},
	pages = {e005122},
	publisher = {Am Heart Assoc},
	title = {Privacy-preserving generative deep neural networks support clinical data sharing},
	volume = {12},
	year = {2019}}

@article{uniyal2021dpsgd,
	author = {Archit Uniyal and Rakshit Naidu and Sasikanth Kotti and Sahib Singh and Patrik Joslin Kenfack and Fatemehsadat Mireshghallah and Andrew Trask},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2106-12576.bib},
	eprint = {2106.12576},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 30 Jun 2021 16:14:10 +0200},
	title = {{DP-SGD} vs {PATE:} Which Has Less Disparate Impact on Model Accuracy?},
	volume = {abs/2106.12576},
	year = {2021}}

@inproceedings{ping2017datasynthesizer,
	abstract = {To facilitate collaboration over sensitive data, we present DataSynthesizer, a tool
that takes a sensitive dataset as input and generates a structurally and statistically
similar synthetic dataset with strong privacy guarantees. The data owners need not
release their data, while potential collaborators can begin developing models and
methods with some confidence that their results will work similarly on the real dataset.
The distinguishing feature of DataSynthesizer is its usability --- the data owner
does not have to specify any parameters to start generating and sharing data safely
and effectively.DataSynthesizer consists of three high-level modules --- DataDescriber,
DataGenerator and ModelInspector. The first, DataDescriber, investigates the data
types, correlations and distributions of the attributes in the private dataset, and
produces a data summary, adding noise to the distributions to preserve privacy. DataGenerator
samples from the summary computed by DataDescriber and outputs synthetic data. ModelInspector
shows an intuitive description of the data summary that was computed by DataDescriber,
allowing the data owner to evaluate the accuracy of the summarization process and
adjust any parameters, if desired.We describe DataSynthesizer and illustrate its use
in an urban science context, where sharing sensitive, legally encumbered data between
agencies and with outside collaborators is reported as the primary obstacle to data-driven
governance.The code implementing all parts of this work is publicly available at https://github.com/DataResponsibly/DataSynthesizer.},
	address = {New York, NY, USA},
	articleno = {42},
	author = {Ping, Haoyue and Stoyanovich, Julia and Howe, Bill},
	booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
	doi = {10.1145/3085504.3091117},
	isbn = {9781450352826},
	keywords = {Data Sharing, Differential Privacy, Synthetic Data},
	location = {Chicago, IL, USA},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {SSDBM '17},
	title = {DataSynthesizer: Privacy-Preserving Synthetic Datasets},
	url = {https://doi.org/10.1145/3085504.3091117},
	year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1145/3085504.3091117}}

@inproceedings{patki2016synthetic,
	author = {Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	booktitle = {2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
	doi = {10.1109/DSAA.2016.49},
	pages = {399-410},
	title = {The Synthetic Data Vault},
	year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/DSAA.2016.49}}

@misc{choi2018generating,
	archiveprefix = {arXiv},
	author = {Edward Choi and Siddharth Biswal and Bradley Malin and Jon Duke and Walter F. Stewart and Jimeng Sun},
	eprint = {1703.06490},
	primaryclass = {cs.LG},
	title = {Generating Multi-label Discrete Patient Records using Generative Adversarial Networks},
	year = {2018}}

@inproceedings{li2014dpsynthesizer,
	author = {Li, Haoran and Xiong, Li and Zhang, Lifan and Jiang, Xiaoqian},
	booktitle = {Proceedings of the VLDB Endowment International Conference on Very Large Data Bases},
	number = {13},
	organization = {NIH Public Access},
	pages = {1677},
	title = {{DPSynthesizer}: differentially private data synthesizer for privacy preserving data sharing},
	volume = {7},
	year = {2014}}

@misc{mottini2018airline,
	archiveprefix = {arXiv},
	author = {Alejandro Mottini and Alix Lheritier and Rodrigo Acuna-Agost},
	eprint = {1807.06657},
	primaryclass = {cs.LG},
	title = {Airline Passenger Name Record Generation using Generative Adversarial Networks},
	year = {2018}}

@article{park2014pegs,
	author = {Park, Yubin and Ghosh, Joydeep},
	journal = {Trans. Data Priv.},
	number = {3},
	pages = {253--282},
	title = {PeGS: Perturbed Gibbs Samplers that Generate Privacy-Compliant Synthetic Data.},
	volume = {7},
	year = {2014}}

@inproceedings{li2014dpsynthesizer,
	author = {Li, Haoran and Xiong, Li and Zhang, Lifan and Jiang, Xiaoqian},
	booktitle = {Proceedings of the VLDB Endowment International Conference on Very Large Data Bases},
	number = {13},
	organization = {NIH Public Access},
	pages = {1677},
	title = {DPSynthesizer: differentially private data synthesizer for privacy preserving data sharing},
	volume = {7},
	year = {2014}}

@inproceedings{abowd2008protective,
	author = {Abowd, John M and Vilhuber, Lars},
	booktitle = {International Conference on Privacy in Statistical Databases},
	organization = {Springer},
	title = {How protective are synthetic data?},
	year = {2008}}

@article{karr2006framework,
	author = {Karr, Alan F and Kohnen, Christine N and Oganian, Anna and Reiter, Jerome P and Sanil, Ashish P},
	journal = {The American Statistician},
	number = {3},
	pages = {224--232},
	publisher = {Taylor \& Francis},
	title = {A framework for evaluating the utility of data altered to protect confidentiality},
	volume = {60},
	year = {2006}}

@article{hothorn2006unbiased,
	author = {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
	journal = {Journal of Computational and Graphical statistics},
	number = {3},
	pages = {651--674},
	publisher = {Taylor \& Francis},
	title = {Unbiased recursive partitioning: A conditional inference framework},
	volume = {15},
	year = {2006}}

@inproceedings{chin2019generation,
	author = {Chin-Cheong, Kieran and Sutter, Thomas and Vogt, Julia E},
	booktitle = {workshop on machine learning for health (ML4H) at the 33rd conference on neural information processing systems (NeurIPS 2019)},
	organization = {ETH Zurich, Institute for Machine Learning},
	title = {Generation of heterogeneous synthetic electronic health records using {gans}},
	year = {2019}}

@misc{weldon2021generation,
	archiveprefix = {arXiv},
	author = {John Weldon and Tomas Ward and Eoin Brophy},
	eprint = {2109.02543},
	primaryclass = {cs.LG},
	title = {Generation of Synthetic Electronic Health Records Using a Federated GAN},
	year = {2021}}

@article{park2018data,
	author = {Park, Noseong and Mohammadi, Mahmoud and Gorde, Kshitij and Jajodia, Sushil and Park, Hongkyu and Kim, Youngmin},
	doi = {10.14778/3231751.3231757},
	issn = {2150-8097},
	journal = {Proceedings of the VLDB Endowment},
	month = {Jun},
	number = {10},
	pages = {1071--1083},
	publisher = {VLDB Endowment},
	title = {Data synthesis based on generative adversarial networks},
	volume = {11},
	year = {2018}}

@inproceedings{grover2018flow,
  title={{Flow-GAN: Combining maximum likelihood and adversarial learning in generative models}},
  author={Grover, Aditya and Dhar, Manik and Ermon, Stefano},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}
@inproceedings{mao2017least,
	author = {Mao, Xudong and Li, Qing and Xie, Haoran and Lau, Raymond Y.K. and Wang, Zhen and Smolley, Stephen Paul},
	booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
	doi = {10.1109/ICCV.2017.304},
	pages = {2813-2821},
	title = {Least Squares Generative Adversarial Networks},
	year = {2017}}

@inproceedings{arjovsky2017wasserstein,
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.},
	author = {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	editor = {Precup, Doina and Teh, Yee Whye},
	month = {06--11 Aug},
	pages = {214--223},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {{W}asserstein Generative Adversarial Networks},
	volume = {70},
	year = {2017}}

@misc{rezende2014stochastic,
	archiveprefix = {arXiv},
	author = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
	eprint = {1401.4082},
	primaryclass = {stat.ML},
	title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
	year = {2014}}

@article{goodfellow2014generative,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	journal = {Advances in neural information processing systems},
	title = {Generative adversarial nets},
	volume = {27},
	year = {2014}}

@article{sajjadi2018assessing,
  title={{Assessing generative models via precision and recall}},
  author={Sajjadi, Mehdi SM and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@inproceedings{alaa2022faithful,
  title={How {F}aithful is your {S}ynthetic {D}ata? {S}ample-level {M}etrics for {E}valuating and {A}uditing {G}enerative {M}odels},
  author={Alaa, Ahmed and Van Breugel, Boris and Saveliev, Evgeny S and van der Schaar, Mihaela},
  booktitle={International Conference on Machine Learning},
  year={2022},
  organization={PMLR}
}

@inproceedings{naeem2020reliable,
	abstract = {Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Frechet Inception Distance (FID) score. Since it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics.},
	author = {Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung and Choi, Yunjey and Yoo, Jaejun},
	booktitle = {Proceedings of the 37th International Conference on Machine Learning},
	editor = {III, Hal Daum{\'e} and Singh, Aarti},
	month = {13--18 Jul},
	pages = {7176--7185},
	pdf = {http://proceedings.mlr.press/v119/naeem20a/naeem20a.pdf},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	title = {Reliable Fidelity and Diversity Metrics for Generative Models},
	volume = {119},
	year = {2020}}

@misc{kamthe2021copula,
	archiveprefix = {arXiv},
	author = {Sanket Kamthe and Samuel Assefa and Marc Deisenroth},
	eprint = {2101.00598},
	primaryclass = {stat.ML},
	title = {Copula Flows for Synthetic Data Generation},
	year = {2021}}

@article{zhang2017privbayes,
	abstract = {Privacy-preserving data publishing is an important problem that has been the focus
of extensive study. The state-of-the-art solution for this problem is differential
privacy, which offers a strong degree of privacy protection without making restrictive
assumptions about the adversary. Existing techniques using differential privacy, however,
cannot effectively handle the publication of high-dimensional data. In particular,
when the input dataset contains a large number of attributes, existing methods require
injecting a prohibitive amount of noise compared to the signal in the data, which
renders the published data next to useless.To address the deficiency of the existing
methods, this paper presents PrivBayes, a differentially private method for releasing
high-dimensional data. Given a dataset D, PrivBayes first constructs a Bayesian network
N, which (i) provides a succinct model of the correlations among the attributes in
D and (ii) allows us to approximate the distribution of data in D using a set P of
low-dimensional marginals of D. After that, PrivBayes injects noise into each marginal
in P to ensure differential privacy and then uses the noisy marginals and the Bayesian
network to construct an approximation of the data distribution in D. Finally, PrivBayes
samples tuples from the approximate distribution to construct a synthetic dataset,
and then releases the synthetic data. Intuitively, PrivBayes circumvents the curse
of dimensionality, as it injects noise into the low-dimensional marginals in P instead
of the high-dimensional dataset D. Private construction of Bayesian networks turns
out to be significantly challenging, and we introduce a novel approach that uses a
surrogate function for mutual information to build the model more accurately. We experimentally
evaluate PrivBayes on real data and demonstrate that it significantly outperforms
existing solutions in terms of accuracy.},
	address = {New York, NY, USA},
	articleno = {25},
	author = {Zhang, Jun and Cormode, Graham and Procopiuc, Cecilia M. and Srivastava, Divesh and Xiao, Xiaokui},
	doi = {10.1145/3134428},
	issn = {0362-5915},
	issue_date = {November 2017},
	journal = {ACM Trans. Database Syst.},
	keywords = {synthetic data generation, Differential privacy, bayesian network},
	month = oct,
	number = {4},
	numpages = {41},
	publisher = {Association for Computing Machinery},
	title = {{PrivBayes: Private Data Release via Bayesian Networks}},
	volume = {42},
	year = {2017}}

@article{xu2019modeling,
  title={{Modeling tabular data using conditional GAN}},
  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@misc{stypulkowski2020representing,
	archiveprefix = {arXiv},
	author = {Micha{\l} Stypu{\l}kowski and Kacper Kania and Maciej Zamorski and Maciej Zi{\k e}ba and Tomasz Trzci{\'n}ski and Jan Chorowski},
	eprint = {2010.11087},
	primaryclass = {cs.CV},
	title = {Representing Point Clouds with Generative Conditional Invertible Flow Networks},
	year = {2020}}

@article{wolf2021deflow,
	author = {Valentin Wolf and Andreas Lugmayr and Martin Danelljan and Luc Van Gool and Radu Timofte},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2101-05796.bib},
	eprint = {2101.05796},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 22 Jan 2021 15:16:00 +0100},
	title = {DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows},
	url = {https://arxiv.org/abs/2101.05796},
	volume = {abs/2101.05796},
	year = {2021},
	Bdsk-Url-1 = {https://arxiv.org/abs/2101.05796}}

@article{kang2020gratis,
	author = {Kang, Yanfei and Hyndman, Rob J and Li, Feng},
	journal = {Statistical Analysis and Data Mining: The ASA Data Science Journal},
	number = {4},
	pages = {354--376},
	publisher = {Wiley Online Library},
	title = {GRATIS: GeneRAting TIme Series with diverse and controllable characteristics},
	volume = {13},
	year = {2020}}

@inproceedings{sun2019learning,
	author = {Sun, Yi and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
	booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
	number = {01},
	pages = {5049--5057},
	title = {Learning vine copula models for synthetic data generation},
	volume = {33},
	year = {2019}}

@inproceedings{waites2021differentially,
  title={{Differentially private normalizing flows for privacy-preserving density estimation}},
  author={Waites, Chris and Cummings, Rachel},
  booktitle={Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={1000--1009},
  year={2021}
}

@article{uria2016neural,
  title={{Neural autoregressive distribution estimation}},
  author={Uria, Benigno and C{\^o}t{\'e}, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={7184--7220},
  year={2016},
  publisher={JMLR. org}
}

@article{mirza2014conditional,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@inproceedings{amiri2022generating,
  title={{Generating Heavy-Tailed Synthetic Data with Normalizing Flows}},
  author={Amiri, Saba and Nalisnick, Eric and Belloum, Adam and Klous, Sander and Gommans, Leon},
  booktitle={The 5th Workshop on Tractable Probabilistic Modeling},
  year={2022}
}

@article{balunovic2021fair,
	author = {Balunovi{\'c}, Mislav and Ruoss, Anian and Vechev, Martin},
	journal = {arXiv preprint arXiv:2106.05937},
	title = {Fair Normalizing Flows},
	year = {2021}}

@article{chen2021synthetic,
	author = {Chen, Richard J and Lu, Ming Y and Chen, Tiffany Y and Williamson, Drew FK and Mahmood, Faisal},
	journal = {Nature Biomedical Engineering},
	pages = {1--5},
	publisher = {Nature Publishing Group},
	title = {Synthetic data in machine learning for medicine and healthcare},
	year = {2021}}

@book{hoag2008synthetic,
	author = {Hoag, Joseph E},
	publisher = {University of Arkansas},
	title = {Synthetic data generation: Theory, techniques and applications},
	year = {2008}}

@inproceedings{jordon2018pate,
	author = {Jordon, James and Yoon, Jinsung and Van Der Schaar, Mihaela},
	booktitle = {International conference on learning representations},
	title = {{PATE-GAN: Generating synthetic data with differential privacy guarantees}},
	year = {2018}}

@inproceedings{torkzadehmahani2019dp,
	author = {Torkzadehmahani, Reihaneh and Kairouz, Peter and Paten, Benedict},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
	pages = {0--0},
	title = {Dp-cgan: Differentially private synthetic data and label generation},
	year = {2019}}

@inproceedings{hittmeir2019utility,
	author = {Hittmeir, Markus and Ekelhart, Andreas and Mayer, Rudolf},
	booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
	pages = {1--6},
	title = {On the utility of synthetic data: An empirical evaluation on machine learning tasks},
	year = {2019}}

@article{bingham2019pyro,
	author = {Bingham, Eli and Chen, Jonathan P and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D},
	journal = {The Journal of Machine Learning Research},
	number = {1},
	pages = {973--978},
	publisher = {JMLR. org},
	title = {Pyro: Deep universal probabilistic programming},
	volume = {20},
	year = {2019}}

@article{kobyzev2020normalizing,
	author = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	publisher = {IEEE},
	title = {Normalizing flows: An introduction and review of current methods},
	year = {2020}}

@article{papamakarios2021normalizing,
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	journal = {Journal of Machine Learning Research},
	number = {57},
	pages = {1--64},
	title = {Normalizing flows for probabilistic modeling and inference},
	volume = {22},
	year = {2021}}

@article{azizi2021can,
	author = {Azizi, Zahra and Zheng, Chaoyi and Mosquera, Lucy and Pilote, Louise and El Emam, Khaled},
	journal = {BMJ open},
	number = {4},
	pages = {e043497},
	publisher = {British Medical Journal Publishing Group},
	title = {Can synthetic data be a proxy for real clinical trial data? A validation study},
	volume = {11},
	year = {2021}}

@inbook{Varga2008,
	abstract = {In this chapter, the use of synthetic training data for handwriting recognition is studied. After an overview of the previous works related to the field, the authors' main results regarding this research area are presented and discussed, including a perturbation model for the generation of synthetic text lines from existing cursively handwritten lines of text produced by human writers. The goal of synthetic text line generation is to improve the performance of an off-line cursive handwriting recognition system by providing it with additional training data. It can be expected that by adding synthetic training data the variability of the training set improves, which leads to a higher recognition rate. On the other hand, synthetic training data may bias a recognizer towards unnatural handwriting styles, which could lead to a deterioration of the recognition rate. The proposed perturbation model is evaluated under several experimental conditions, and it is shown that significant improvement of the recognition performance is possible even when the original training set is large and the text lines are provided by a large number of different writers.},
	address = {Berlin, Heidelberg},
	author = {Varga, Tam{\'a}s and Bunke, Horst},
	booktitle = {Machine Learning in Document Analysis and Recognition},
	editor = {Marinai, Simone and Fujisawa, Hiromichi},
	pages = {333--360},
	publisher = {Springer Berlin Heidelberg},
	title = {Perturbation Models for Generating Synthetic Training Data in Handwriting Recognition},
	year = {2008}}

@article{assefa2020generating,
	author = {Assefa, Samuel},
	journal = {Challenges and Pitfalls (June 23, 2020)},
	title = {Generating synthetic data in finance: opportunities, challenges and pitfalls},
	year = {2020}}

@inproceedings{mannino2019real,
	author = {Mannino, Miro and Abouzied, Azza},
	booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
	pages = {549--561},
	title = {Is this real? Generating synthetic data that looks real},
	year = {2019}}

@book{el2020practical,
	author = {El Emam, Khaled and Mosquera, Lucy and Hoptroff, Richard},
	publisher = {O'Reilly Media},
	title = {Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data},
	year = {2020}}

@article{kunar2021dtgan,
	author = {Kunar, Aditya and Birke, Robert and Chen, Lydia and Zhao, Zilong},
	journal = {arXiv preprint arXiv:2107.02521},
	title = {{DTGAN: Differential Private Training for Tabular {gans}}},
	year = {2021}}

@inproceedings{ha2019differential,
	author = {Ha, Trung and Dang, Tran Khanh and Dang, Tran Tri and Truong, Tuan Anh and Nguyen, Manh Tuan},
	booktitle = {2019 International Conference on Advanced Computing and Applications (ACOMP)},
	organization = {IEEE},
	pages = {97--102},
	title = {Differential privacy in deep learning: an overview},
	year = {2019}}

@inproceedings{fan2020survey,
	author = {Fan, Liyue},
	booktitle = {The AAAI Workshop on Privacy-Preserving Artificial Intelligence},
	title = {A survey of differentially private generative adversarial networks},
	year = {2020}}

@inproceedings{dwork2008differential,
	author = {Dwork, Cynthia},
	booktitle = {International conference on theory and applications of models of computation},
	file = {:dwork2008differential - Differential privacy_ A survey of results.pdf:PDF},
	groups = {differential-privacy},
	organization = {Springer},
	pages = {1--19},
	timestamp = {2021-09-24},
	title = {Differential privacy: A survey of results},
	year = {2008}}

@book{goodfellow2016deep,
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	publisher = {MIT press},
	title = {Deep learning},
	year = {2016}}

@inproceedings{pitman1936sufficient,
	author = {Pitman, Edwin James George},
	booktitle = {Mathematical Proceedings of the cambridge Philosophical society},
	organization = {Cambridge University Press},
	pages = {567--579},
	title = {Sufficient statistics and intrinsic accuracy},
	volume = {32},
	year = {1936}}

@inproceedings{darmois1935surles,
	author = {Darmois, g},
	booktitle = {C. R. Acad. Sci. Paris},
	pages = {1265-1266},
	title = {Sur les lois de probabilites a estimation exhaustive},
	year = {1935}}

@article{koopman1936distributions,
	author = {Koopman, Bernard Osgood},
	journal = {Transactions of the American Mathematical society},
	number = {3},
	pages = {399--409},
	publisher = {JSTOR},
	title = {On distributions admitting a sufficient statistic},
	volume = {39},
	year = {1936}}

@article{zhang2018differentially,
	author = {Xinyang Zhang and Shouling Ji and Ting Wang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1801-01594.bib},
	eprint = {1801.01594},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 17 Jul 2019 17:08:56 +0200},
	title = {Differentially Private Releasing via Deep Generative Model},
	volume = {abs/1801.01594},
	year = {2018}}

@inproceedings{chincheong2019generation,
	author = {Kieran Chin-Cheong and Thomas M. Sutter and Julia E. Vogt},
	booktitle = {NeurIPS 2019},
	title = {Generation of Heterogeneous Synthetic Electronic Health Records using {gans}},
	year = {2019}}

@article{xie2018differentially,
	author = {Liyang Xie and Kaixiang Lin and Shu Wang and Fei Wang and Jiayu Zhou},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1802-06739.bib},
	eprint = {1802.06739},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 13 May 2020 15:57:39 +0200},
	title = {Differentially Private Generative Adversarial Network},
	url = {http://arxiv.org/abs/1802.06739},
	volume = {abs/1802.06739},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1802.06739}}

@article{DBLP:journals/corr/abs-1801-01594,
	author = {Xinyang Zhang and Shouling Ji and Ting Wang},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1801-01594.bib},
	eprint = {1801.01594},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 17 Jul 2019 17:08:56 +0200},
	title = {Differentially Private Releasing via Deep Generative Model},
	url = {http://arxiv.org/abs/1801.01594},
	volume = {abs/1801.01594},
	year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/abs/1801.01594}}

@article{chen2020gswgan,
	author = {Dingfan Chen and Tribhuvanesh Orekondy and Mario Fritz},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2006-08265.bib},
	eprint = {2006.08265},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
	title = {{GS-WGAN:} {A} Gradient-Sanitized Approach for Learning Differentially Private Generators},
	volume = {abs/2006.08265},
	year = {2020},
	Bdsk-Url-1 = {https://arxiv.org/abs/2006.08265}}

@article{gulrajani2017improved,
	author = {Ishaan Gulrajani and Faruk Ahmed and Mart{\'{\i}}n Arjovsky and Vincent Dumoulin and Aaron C. Courville},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/GulrajaniAADC17.bib},
	eprint = {1704.00028},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Mon, 13 Aug 2018 16:47:43 +0200},
	title = {Improved Training of Wasserstein {gans}},
	url = {http://arxiv.org/abs/1704.00028},
	volume = {abs/1704.00028},
	year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1704.00028}}

@article{torkzadehmahani2020dpcgan,
	author = {Reihaneh Torkzadehmahani and Peter Kairouz and Benedict Paten},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-2001-09700.bib},
	eprint = {2001.09700},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 30 Jan 2020 18:46:36 +0100},
	title = {{DP-CGAN:} Differentially Private Synthetic Data and Label Generation},
	volume = {abs/2001.09700},
	year = {2020}}

@article{tantipongpipat2019differentially,
	author = {Uthaipon Tantipongpipat and Chris Waites and Digvijay Boob and Amaresh Ankit Siva and Rachel Cummings},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1912-03250.bib},
	eprint = {1912.03250},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Thu, 14 Oct 2021 09:15:37 +0200},
	title = {Differentially Private Mixed-Type Data Generation For Unsupervised Learning},
	volume = {abs/1912.03250},
	year = {2019}}

@article{torfi2022differentially,
  title={{Differentially private synthetic medical data generation using convolutional GANs}},
  author={Torfi, Amirsina and Fox, Edward A and Reddy, Chandan K},
  journal={Information Sciences},
  volume={586},
  pages={485--500},
  year={2022},
  publisher={Elsevier}
}

@article{doucet2023differentiable,
  title={{Differentiable samplers for deep latent variable models}},
  author={Doucet, Arnaud and Moulines, Eric and Thin, Achille},
  journal={Philosophical Transactions of the Royal Society A},
  volume={381},
  number={2247},
  pages={20220147},
  year={2023},
  publisher={The Royal Society}
}



@InProceedings{blackboxVI_ranganath14,
  title = 	 {{Black Box Variational Inference}},
  author = 	 {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  booktitle = 	 {Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages = 	 {814--822},
  year = 	 {2014},
  editor = 	 {Kaski, Samuel and Corander, Jukka},
  volume = 	 {33},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Reykjavik, Iceland},
  month = 	 {22--25 Apr},
  publisher =    {PMLR},
}

@article{croitoru2023diffusion,
  title={Diffusion models in vision: A survey},
  author={Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  publisher={IEEE}
}
@article{yang2022diffusion,
  title={Diffusion models: A comprehensive survey of methods and applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={arXiv preprint arXiv:2209.00796},
  year={2022}
}

@INPROCEEDINGS {Tanti2021,
author = {U. Tantipongpipat and C. Waites and D. Boob and A. Siva and R. Cummings},
booktitle = {2021 12th International Conference on Information, Intelligence, Systems \& Applications (IISA)},
title = {{Differentially Private Synthetic Mixed-Type Data Generation For Unsupervised Learning}},
year = {2021},
volume = {},
issn = {},
pages = {1-9},
abstract = {We introduce the DP-auto-GAN framework for synthetic data generation, which combines the low dimensional representation of autoencoders with the flexibility of Generative Adversarial Networks (GANs). This framework can be used to take in raw sensitive data and privately train a model for generating synthetic data that will satisfy similar statistical properties as the original data. This learned model can generate an arbitrary amount of synthetic data, which can then be freely shared due to the post-processing guarantee of differential privacy. Our framework is applicable to unlabeled mixed-type data, that may include binary, categorical, and real-valued data. We implement this framework on both binary data (MIMIC-III) and mixed-type data (ADULT), and compare its performance with existing private algorithms on metrics in unsupervised settings. We also introduce a new quantitative metric able to detect diversity, or lack thereof, of synthetic data.},
keywords = {measurement;differential privacy;mimics;gaussian distribution;generative adversarial networks;data models;unsupervised learning},
doi = {10.1109/IISA52424.2021.9555521},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jul}
}

@article{chen2018differentially,
	author = {Chen, Qingrong and Xiang, Chong and Xue, Minhui and Li, Bo and Borisov, Nikita and Kaarfar, Dali and Zhu, Haojin},
	journal = {arXiv preprint arXiv:1812.02274},
	title = {Differentially private data generative models},
	year = {2018}}

@article{acs2018differentially,
	author = {Acs, Gergely and Melis, Luca and Castelluccia, Claude and De Cristofaro, Emiliano},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	number = {6},
	pages = {1109--1121},
	publisher = {IEEE},
	title = {Differentially private mixture of generative neural networks},
	volume = {31},
	year = {2018}}



@article{holland1977robust,
	author = {Holland, Paul W and Welsch, Roy E},
	journal = {Communications in Statistics-theory and Methods},
	number = {9},
	pages = {813--827},
	publisher = {Taylor \& Francis},
	title = {Robust regression using iteratively reweighted least-squares},
	volume = {6},
	year = {1977}}

@article{oord2016wavenet,
	author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
	journal = {arXiv preprint arXiv:1609.03499},
	title = {Wavenet: A generative model for raw audio},
	year = {2016}}

@inproceedings{van2016pixel,
	author = {Van Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
	booktitle = {International conference on machine learning},
	organization = {PMLR},
	pages = {1747--1756},
	title = {Pixel recurrent neural networks},
	year = {2016}}

@misc{long2021gpate,
	archiveprefix = {arXiv},
	author = {Yunhui Long and Boxin Wang and Zhuolin Yang and Bhavya Kailkhura and Aston Zhang and Carl A. Gunter and Bo Li},
	eprint = {1906.09338},
	primaryclass = {cs.LG},
	title = {{G-PATE: Scalable Differentially Private Data Generator via Private Aggregation of Teacher Discriminators}},
	year = {2021}}

@inproceedings{lawson1961computation,
	abstract = {We will present a new algorithm for solving the following problem of least maximum approximation (also called uniform or Chebyshev approximation).},
	address = {New York, NY, USA},
	author = {Lawson, Charles L.},
	booktitle = {Proceedings of the 1961 16th ACM National Meeting},
	doi = {10.1145/800029.808547},
	isbn = {9781450373883},
	pages = {121.301--121.303},
	publisher = {Association for Computing Machinery},
	series = {ACM '61},
	title = {Computation of a Least Maximum Approximator as the Limit of Weighted Least Squares Approximators},
	year = {1961}}

@article{nwankpa2018activation,
	author = {Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
	bibsource = {dblp computer science bibliography, https://dblp.org},
	biburl = {https://dblp.org/rec/journals/corr/abs-1811-03378.bib},
	eprint = {1811.03378},
	eprinttype = {arXiv},
	journal = {CoRR},
	timestamp = {Fri, 23 Nov 2018 12:43:51 +0100},
	title = {Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
	volume = {abs/1811.03378},
	year = {2018}}
	
@techreport{mitchell1980biases,
  added-at = {2012-10-28T20:35:27.000+0100},
  address = {New Brunswick, NJ},
  author = {Mitchell, Tom M.},
  biburl = {https://www.bibsonomy.org/bibtex/28bc1ae3ea830f305f20f6484fefae3db/jil},
  institution = {Rutgers University},
  interhash = {d263e645b3bac437539f4cef55c84195},
  intrahash = {8bc1ae3ea830f305f20f6484fefae3db},
  keywords = {bias generalization inductive learning machine overfitting},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {The Need for Biases in Learning Generalizations},
  url = {http://dml.cs.byu.edu/~cgc/docs/mldm_tools/Reading/Need%20for%20Bias.pdf},
  year = {1980}}

@book{mclachlan1988mixture,
  title={Mixture models: Inference and applications to clustering},
  author={McLachlan, Geoffrey J and Basford, Kaye E},
  volume={38},
  year={1988},
  publisher={M. Dekker New York}
}

@inproceedings{bengio2012deep,
  title={Deep learning of representations for unsupervised and transfer learning},
  author={Bengio, Yoshua},
  booktitle={Proceedings of ICML workshop on unsupervised and transfer learning},
  pages={17--36},
  year={2012},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{barlow1989unsupervised,
  title={Unsupervised learning},
  author={Barlow, Horace B},
  journal={Neural computation},
  volume={1},
  number={3},
  pages={295--311},
  year={1989},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@incollection{hastie2009unsupervised,
  title={Unsupervised learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  booktitle={The elements of statistical learning},
  pages={485--585},
  year={2009},
  publisher={Springer}
}

@book{hinton1999unsupervised,
  title={Unsupervised learning: foundations of neural computation},
  author={Hinton, Geoffrey and Sejnowski, Terrence J},
  year={1999},
  publisher={MIT press}
}

@incollection{hastie2009overview,
  title={Overview of supervised learning},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  booktitle={The elements of statistical learning},
  pages={9--41},
  year={2009},
  publisher={Springer}
}

@book{tomczak2022deep,
  title= {Deep Generative Modeling},
  author= {Tomczak, Jakub},
  year= {2022},
  publisher ={Springer}}

@techreport{rosenblatt1961principles,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc Buffalo NY}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hopfield1986computing,
  title={Computing with neural circuits: A model},
  author={Hopfield, John J and Tank, David W},
  journal={Science},
  volume={233},
  number={4764},
  pages={625--633},
  year={1986},
  publisher={American Association for the Advancement of Science}
}

@article{cox1958regression,
  title={The regression analysis of binary sequences},
  author={Cox, David R},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={20},
  number={2},
  pages={215--232},
  year={1958},
  publisher={Wiley Online Library}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{keskar2017improving,
  title={Improving generalization performance by switching from {ADAM} to {SGD}},
  author={Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07628},
  year={2017}
}

@inproceedings{ballard1987modular,
  title={Modular learning in neural networks.},
  author={Ballard, Dana H},
  booktitle={AAAI},
  volume={647},
  pages={279--284},
  year={1987}
}

@article{tang2012deep,
  title={Deep mixtures of factor analysers},
  author={Tang, Yichuan and Salakhutdinov, Ruslan and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:1206.4635},
  year={2012}
}

@article{viroli2019deep,
  title={Deep {G}aussian mixture models},
  author={Viroli, Cinzia and McLachlan, Geoffrey J},
  journal={Statistics and Computing},
  volume={29},
  number={1},
  pages={43--51},
  year={2019},
  publisher={Springer}
}



@article{zhang2018advances,
  title={Advances in variational inference},
  author={Zhang, Cheng and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={2008--2026},
  year={2018},
  publisher={IEEE}
}

 @book{murphy2023probabilistic,
 author = "Kevin P. Murphy",
 title = "Probabilistic Machine Learning: Advanced Topics",
 publisher = "MIT Press",
 year = 2023,
 url = "http://probml.github.io/book2"
}

@article{zhou2020graph,
  title={{Graph neural networks: A review of methods and applications}},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{liu2022goggle,
  title={{GOGGLE: Generative modelling for tabular data by learning relational structure}},
  author={Liu, Tennison and Qian, Zhaozhi and Berrevoets, Jeroen and van der Schaar, Mihaela},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@book{mclachlan2007algorithm,
  title={The {EM} algorithm and extensions},
  author={McLachlan, Geoffrey J and Krishnan, Thriyambakam},
  volume={382},
  year={2007},
  publisher={John Wiley \& Sons}
}

@book{kroese2013handbook,
  title={Handbook of {Monte Carlo} methods},
  author={Kroese, Dirk P and Taimre, Thomas and Botev, Zdravko I},
  year={2013},
  publisher={John Wiley \& Sons}
}

@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3},
  pages={229--256},
  year={1992},
  publisher={Springer}
}
@inproceedings{rezende2015variational,
  title={{Variational inference with normalizing flows}},
  author={Rezende, Danilo and Mohamed, Shakir},
  booktitle={International conference on machine learning},
  pages={1530--1538},
  year={2015},
  organization={PMLR}
}
@article{burda2015importance,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}