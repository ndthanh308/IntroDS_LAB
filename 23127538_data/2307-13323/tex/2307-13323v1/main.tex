\documentclass[conference]{IEEEtran}
\usepackage{times}

% numbers option provides compact numerical references in the text. 
\usepackage[numbers]{natbib}
\usepackage[bookmarks=true]{hyperref}

\usepackage{graphicx}
\usepackage{amsmath,amsfonts}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{subfig}
\usepackage{float}

\pdfinfo{
   /Author (Homer Simpson)
   /Title  (Robots: Our new overlords)
   /CreationDate (D:20101201120000)
   /Subject (Robots)
   /Keywords (Robots;Overlords)
}

\begin{document}

% paper title
\title{Learning Autonomous Ultrasound via Latent Task Representation and Robotic Skills Adaptation}

% You will get a Paper-ID when submitting a pdf file to the conference system
% \author{Paper-ID [8]}

% \author{
% \authorblockN{Xutian Deng}
% \authorblockA{School of Computer Science\\
% Wuhan University\\
% Wuhan 430072, China\\
% Email: dengxutian@whu.edu.cn}
% \and
% \authorblockN{Junnan Jiang}
% \authorblockA{School of Computer Science\\
% Wuhan University\\
% Wuhan 430072, China\\
% Email: dengxutian@whu.edu.cn}
% \and
% \authorblockN{Xutian Deng}
% \authorblockA{School of Computer Science\\
% Wuhan University\\
% Wuhan 430072, China\\
% Email: dengxutian@whu.edu.cn}
% \and
% \authorblockN{Miao Li}
% \authorblockA{School of Computer Science\\
% Wuhan University\\
% Wuhan 430072, China\\
% Email: dengxutian@whu.edu.cn}
% }

% \author{\authorblockN{Michael Shell}
% \authorblockA{School of Electrical and\\Computer Engineering\\
% Georgia Institute of Technology\\
% Atlanta, Georgia 30332--0250\\
% Email: mshell@ece.gatech.edu}
% \and
% \authorblockN{Homer Simpson}
% \authorblockA{Twentieth Century Fox\\
% Springfield, USA\\
% Email: homer@thesimpsons.com}
% \and
% \authorblockN{James Kirk\\ and Montgomery Scott}
% \authorblockA{Starfleet Academy\\
% San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212\\
% Fax: (888) 555--1212}}


% avoiding spaces at the end of the author lines is not a problem with
% conference papers because we don't use \thanks or \IEEEmembership


% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
% \author{\authorblockN{Michael Shell\authorrefmark{1},
% Homer Simpson\authorrefmark{2},
% James Kirk\authorrefmark{3}, 
% Montgomery Scott\authorrefmark{3} and
% Eldon Tyrell\authorrefmark{4}}
% \authorblockA{\authorrefmark{1}School of Electrical and Computer Engineering\\
% Georgia Institute of Technology,
% Atlanta, Georgia 30332--0250\\ Email: mshell@ece.gatech.edu}
% \authorblockA{\authorrefmark{2}Twentieth Century Fox, Springfield, USA\\
% Email: homer@thesimpsons.com}
% \authorblockA{\authorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
% Telephone: (800) 555--1212, Fax: (888) 555--1212}
% \authorblockA{\authorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}

\author{\authorblockN{Xutian Deng\authorrefmark{1},
Junnan Jiang\authorrefmark{2},
Wen Cheng\authorrefmark{3} and
Miao Li\authorrefmark{4}}
\authorblockA{\authorrefmark{1}School of Computer Science, Wuhan University, Wuhan 430072, China\\
Email: \href{mailto:dengxutian@whu.edu.cn}{dengxutian@whu.edu.cn}}
\authorblockA{\authorrefmark{2}School of Power and Mechanical Engineering, Wuhan University, Wuhan 430072, China}
\authorblockA{\authorrefmark{3}Hospital of Wuhan University, Wuhan University, Wuhan 430072, China}
\authorblockA{\authorrefmark{4}School of Microelectronics, Wuhan University, Wuhan 430072, China\\
Email: \href{mailto:miao.li@whu.edu.cn}{miao.li@whu.edu.cn}}
}

\maketitle

\begin{abstract}

As medical ultrasound is becoming a prevailing examination approach nowadays, robotic ultrasound systems can facilitate the scanning process and prevent professional sonographers from repetitive and tedious work. Despite the recent progress, it is still a challenge to enable robots to autonomously accomplish the ultrasound examination, which is largely due to the lack of a proper task representation method, and also an adaptation approach to generalize learned skills across different patients. To solve these problems, we propose the latent task representation and the robotic skills adaptation for autonomous ultrasound in this paper. During the offline stage, the multimodal ultrasound skills are merged and encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account. During the online stage, the probability model will select and evaluate the optimal prediction. For unstable singularities, the adaptive optimizer fine-tunes them to near and stable predictions in high-confidence regions. Experimental results show that the proposed approach can generate complex ultrasound strategies for diverse populations and achieve significantly better quantitative results than our previous method.

\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}

Medical ultrasound has been widely employed in routine clinical examinations, which faces two main challenges: 1) long-term training for professional skills \cite{arger2005teaching} and 2) physical injury caused by repetitive work \cite{murphy2000update}. To this end, robotic ultrasound systems have been proposed to overcome these challenges \cite{salcudean2022robot, jiang2023robotic}, which can be further divided into teleoperated and autonomous \cite{li2021overview, ning2023autonomous, wang2023task} ones. Autonomous ultrasound contains three major spectra, human-guided \cite{wang2022full}, vision-based \cite{jiang2020automatic, jiang2023dopus} and policy-based, according to the complexity of strategies. Policy-based systems highlight the learning approaches of general ultrasound skills, e.g. learning from demonstrations \cite{droste2020automatic, deng2021robio, raina2023robotic} and reinforcement learning \cite{ning2021autonomic, li2021autonomous, ning2023inverse}. However, these studies have some weaknesses: 1) Since autonomous ultrasound is a high-dimensional robotic task, some indispensable modalities in latent task representation are ignored, e.g. probe orientations and contact forces. 2) The generalization of learned skills, i.e. the adaptation to patients with different physical conditions, has not been proposed and emphasized.

% Figure environment removed

Following these studies, our work improves the involved modalities with a fully self-supervised framework to perform multimodal fusion and task representation. Besides, we propose and verify an adaptive ultrasound strategy, which abstracts the learned skills into a probability model and accordingly fine-tunes predictions. Fig.~\ref{fig:workflow} shows the workflow of autonomous ultrasound. Our main contribution is twofold:
\begin{itemize}
\item For modeling ultrasound skills, a fully self-supervised latent representation method is proposed, which takes clinically demonstrated ultrasound images, probe orientations, and contact forces into account.
\item For improving the adaptation of learned skills, we propose a probability-based method to evaluate and fine-tune the ultrasound probe’s orientations and contact forces, which enhances the algorithm’s robustness and generalization for patients with different physical conditions.
\end{itemize}

\section{Methodology}

Fig.~\ref{fig:framework} presents the framework of methodology in this paper. Our goal is to learn freehand ultrasound skills from professional sonographers' demonstrations and further generalize them to some kind of adaptive robotic ultrasound skills for patients of different ages, genders, and body physiques.

% Figure environment removed

\subsection{Problem Formulation}

The data involved in the training process is as follows:
\begin{itemize}
\item $D=\{d_1,d_2,...,d_N\}=\{(x_i,p_i,f_i)\}_{i=1...N}$ denotes a high-dimensional dataset with $N$ observations.
\item $x_i\in\mathbb{R}^{224\times224\times1}$ denotes the $i$-th collected ultrasound image with cropped size.
\item $p_i\in\mathbb{R}^4$ denotes the probe orientation (quaternion).
\item $f_i\in\mathbb{R}^6$ denotes the contact force/torque.
\end{itemize}
Additionally, $w_i=\{p_i,f_i\}$ is the variable directly related to robot control. To perform autonomous ultrasound based on a prior dataset $D$, the target function is described as:
\begin{equation}
    \begin{aligned}
        \max \mathbb{E}[&logP(x_{t},\hat{w}_{t+1}|D)] \\
        s.t. \quad &\hat{w}_{t+1} = f_{AU}(x_t,w_t|D),
    \end{aligned}
\label{eq:formulation}
\end{equation}
where $\hat{w}_{t+1}$ is the prediction, $f_{AU}(\cdot)$ denotes the robotic policy, and $P(\cdot)$ represents the conditional probability.

\subsection{Latent Representation and Skills Adaptation}

To latently represent the autonomous ultrasound task, we employ self-supervised learning to determine the mapping rule from high- to low-dimensional spaces, and $v_i$ denotes the feature vector of $x_i$. The represented demonstrations are equivalent to $D^{'}=\{(v_i,p_i,f_i)\}_{i=1...N}$. To this end, demonstrations are replaced by nodes (concatenations of $v$, $p$, and $f$) in latent space, and learning ultrasound skills is equal to finding a proper distribution to measure the conditional probability in Eq.~\ref{eq:formulation}. We assume nodes in latent space follow the Gaussian mixture distribution:
\begin{equation}
P(D^{'}|\Omega) = \sum_{k=1}^{K} \pi_k \mathcal{N}(D^{'}|\mu_k,\Sigma_k).
\label{eq:gmm}
\end{equation}
where $\pi_k$ is the prior of $k$-th Gaussian component, $\mathcal{N}(\mu_k,\Sigma_k)$ is the Gaussian distribution with mean $\mu_k$ and covariance $\Sigma_k$.

% Figure environment removed

\subsubsection{Prediction Step}

The learned ultrasound skills can be reproduced by Gaussian Mixture Regression (GMR) \cite{billard2008robot}, and the conditional expectation of $w$ given $v$ is defined as:
\begin{equation}
\begin{aligned}
& P(w|v) \sim N(\hat{\mu}^w,\hat{\Sigma}^{w,w}), \\
& \hat{\mu}^w = \sum_{k=1}^K \pi_k \hat{\mu}_k^w, \quad \hat{\Sigma}^{w,w} = \sum_{k=1}^{K} \pi_k^2 \hat{\Sigma}_k^{w,w}.
\end{aligned}
\label{eq:gmr}
\end{equation}

\subsubsection{Evaluation Step}

The likelihood range $[a_k^{m\sigma},b_k^{m\sigma}]$ of $k$-th Gaussian component at $m$ standard deviation is:
\begin{equation}
\begin{aligned}
a_k^{m\sigma} &= \min Lik_k^{m\sigma}(d), \\
b_k^{m\sigma} &= \max Lik_k^{m\sigma}(d).
\end{aligned}
\label{eq::boundary}
\end{equation}
% For every predicted state $\hat{d}=\{v,\hat{w}\}$, its stability can be easily measured by:
The stability of $\hat{d}=\{v,\hat{w}\}$ can be measured by:
\begin{equation}
\begin{aligned}
Case 1: Lik_k(\hat{d}) \in [a_k^{m\sigma},b_k^{m\sigma}], \exists{k} \in [1,2,...,K] \\
Case 2: Lik_k(\hat{d}) \notin [a_k^{m\sigma},b_k^{m\sigma}], \forall{k} \in [1,2,...,K]
\end{aligned}
\label{eq::stable}
\end{equation}
If the prediction's likelihood is subject to $Case1$, it will be considered stable. Else if its likelihood is subject to $Case2$, the prediction will be considered unstable and needs to be fine-tuned in the following adaptation stage.

\subsubsection{Adaptation Step}

The adaptation scheme is shown in Fig.~\ref{fig:adaptation}. For unstable cases, selecting the GMR result in Eq.~\ref{eq:gmr} as the local optimum may be invalid. That is because similar samples are scarce in demonstrations, and the probability model is non-referential to singularities. To avoid this, $\hat{w}$ will be adjusted through the local exploration \cite{li2014learning}:
\begin{equation}
    \hat{w} = \operatorname*{arg\,min}_{\mu_k^w} l_k \quad s.t. \quad k={1,2,...,K},
\label{eq:adaptation}
\end{equation}
where $l_k$ is the Mahalanobis distance between the prediction and the center of $k$-th Gaussian component.

\section{Experiments}

\subsection{Clinical Demonstrations}

% Figure environment removed

% Our experimental setup is shown in Fig.~\ref{fig:hardware}. For monitoring probe orientations and contact forces, a probe holder is designed with inconspicuous sensors, including an inertial measurement unit (IMU) and a 6D force/torque sensor. The model of the built-in IMU is ICM20948. The 6D force/torque sensor is an ATI Gamma F/T sensor. The raw ultrasound images come from a Mindray DC-70 ultrasound machine and are forwarded to a laptop (Intel i5 CPU and Nvidia GTX 1650 GPU, Ubuntu20.04 LTS) through a video capture device (MAGEWELL USB Capture AIO).

This work was supported and supervised by the Medical Ethics Committee, School of Medicine, Wuhan University, Ethics Statement No.WHU2021-PMC002. Our experimental setup is shown in Fig.~\ref{fig:hardware}. The clinical experiment was at the Hospital of Wuhan University. The sonographer performed 5 times of left kidney examinations for each volunteer. The recording frequency was 10 Hz. A whole demonstration was started with the sonographer vertically holding the probe holder upon the target part. After $3\sim5$ seconds from the coordinate alignment and gravity calibration, the sonographer would start to perform freehand ultrasound examinations. Each demonstration lasted $40\sim80$ seconds. A total of 24 volunteers (14 males and 10 females) participated in this experiment. In order to make the individual differences in demonstrations more typical, we not only considered gender but also deliberately included people of different degrees of obesity and age groups in the experiment. Volunteers' BMI ranged from 16.4 (underweight) to 26.7 (overweight), and their ages ranged from 19 to 67. Totally, there were 53571 sets of multimodal data in our dataset.

\subsection{Latent Representation}

The ultrasound skills model first learns to encode ultrasound images into feature vectors and then concatenates three modalities as low-dimensional nodes. Then, the embedded nodes in latent space are encapsulated into a Gaussian Mixture Model (GMM). The modalities fusion pipeline is presented in Fig.~\ref{fig:fusion}. In detail, the ultrasound images' feature extraction is performed by Masked Auto Encoder (MAE) \cite{he2022masked}. The raw ultrasound images are with a $1920\times1080$ resolution. Through cropping, scaling, and grayscale, they are converted to $1\times224\times224$ preprocessed images. Each preprocessed one is divided into $8\times8$ patches with the size of $28\times28$. A random selector picks 40 out of 64 patches and the rest are masked out. The remaining 40 patches are reshaped to a $1\times40\times784$ feature vector, and thus processed by the encoder and decoder modules.

% Figure environment removed

The $1\times40\times784$ feature vector is too large to merge with the other modalities. Therefore, a channel summation is added to compress 784 values in each channel into one single feature value, and the ultrasound modality is replaced by a $1\times40$ feature vector. Now, by concatenating the $1\times40$ feature vector, $1\times4$ quaternion vector, and $1\times6$ force/torque vector, every state in original demonstrations can be mapped to a node in 50D latent space. To model these low-dimensional nodes as a probability distribution, we use the GMM with 16 Gaussian components (about 40000 parameters).

\subsection{Skills Adaptation}

\begin{table}[b]
\centering
\scriptsize
\vspace{-5mm}
\caption{\footnotesize Summary results of MC and GMM methods in five tasks.}
\label{tab:exp_results}
\renewcommand\arraystretch{1.5}
\setlength{\tabcolsep}{1mm}{
\begin{tabular}{cccccc}
\toprule
\multicolumn{2}{c}{Methods} & \makecell[c]{Pose error\\(degree)} & \makecell[c]{Force error\\(N)} & \makecell[c]{Torque error\\(Nm)} & FPS \\
\midrule
\multirow{8}{*}{MC}
& 50 samples    & $27.36\pm26.08$ & $2.05\pm0.86$ & $0.14\pm0.06$ & 66.07 \\
& 100 samples   & $26.43\pm25.90$ & $2.04\pm0.85$ & $0.14\pm0.06$ & 43.75 \\
& 200 samples   & $26.65\pm26.07$ & $1.94\pm0.83$ & $0.14\pm0.06$ & 34.97 \\
& 500 samples   & $25.55\pm25.43$ & $1.98\pm0.85$ & $0.14\pm0.06$ & 15.00 \\
& 1000 samples  & $24.69\pm25.32$ & $1.92\pm0.85$ & $0.14\pm0.06$ & 7.40 \\
& 2000 samples  & $27.06\pm26.26$ & $2.01\pm0.87$ & $0.13\pm0.06$ & 3.64 \\
& 5000 samples  & $23.63\pm25.17$ & $1.87\pm0.83$ & $0.13\pm0.06$ & 1.36 \\
& 10000 samples & $25.24\pm25.79$ & $2.29\pm0.91$ & $0.13\pm0.06$ & 0.61 \\ \cline{2-6} 
\multirow{3}{*}{\makecell[c]{GMM\\(Ours)}}
& 1-sigma region & $20.63\pm12.29$ & $1.22\pm0.47$ & $0.10\pm0.05$ & 304.98 \\
& 2-sigma region & $20.64\pm12.17$ & $1.21\pm0.45$ & $0.09\pm0.05$ & 312.01 \\
& 3-sigma region & $20.61\pm12.11$ & $1.21\pm0.46$ & $0.09\pm0.04$ & 306.22 \\
\bottomrule
\end{tabular}}
\end{table}

% Figure environment removed

% Figure environment removed

Once parameters of MAE and GMM are learned, the probability model in Eq.~\ref{eq:formulation} is equally determined. In this way, autonomous ultrasound can be reproduced by GMR. However, singularities are inevitable since demonstrations are always insufficient to fully cover all cases. To this end, we propose the robotic skills adaptation to alleviate this problem, which computes the likelihood bounds for every Gaussian distribution in $m$ standard deviation, and fine-tunes those unstable nodes with low likelihoods.

Our baseline is the Monte Carlo (MC) method \cite{deng2021scis, li2022learning}, where the probability model is replaced by a Multilayer Perceptron (MLP) model. Experimental results have shown that the MC method could learn demonstrated ultrasound skills and reproduce an expert-like strategy. But it tends to sacrifice more computing resources to acquire more acceptable results. Therefore, we study the upper limitation of the MC method and make a quantitative comparison with the GMM method in this experiment. In detail, $1\sigma$, $2\sigma$, and $3\sigma$ confidence regions are chosen for the GMM method respectively. While the numbers of samples are 50, 100, 200, 500, 1000, 2000, 5000, and 10000 in the MC method. For the intra-patient experiment, the first 4 demonstrations of every volunteer are used for training and the last for evaluation. For the inter-patient experiment, all demonstrations of $1\sim22$ volunteers are used for training, and those of $23\sim24$ volunteers are used for evaluation. In addition, we further evaluate the proposed method in different tasks (inter-gender, inter-age, and inter-BMI tasks) with significantly larger individual differences to explore our method's robustness and generalization. Summary results are presented in Fig.~\ref{fig:exp_results} and TABLE.~\ref{tab:exp_results}. The $3\sigma$ GMM policy (gray) is visualized in Fig.~\ref{fig:visualization}, which shows great agreement with the sonographer's demonstrations (blue).

\section{Conclusion}

Learning autonomous strategies for robotic ultrasound systems is still unsolved, which is mainly related to the absence of two fundamental capabilities: a proper representation method to model and reproduce, and an adaptive method to generalize and adjust the learned ultrasound skills. In this paper, we propose to learn robotic ultrasound skills from clinical freehand examinations demonstrated by a professional sonographer with latent representation and skills adaptation, which highlights feasibility and generalization for patients with different physical conditions. During the offline stage, the multimodal ultrasound skills are encapsulated into a low-dimensional probability model through a fully self-supervised framework, which takes ultrasound images, probe orientations, and contact forces into account. During the online stage, the optimal prediction will be selected and evaluated by the probability model. For any singularities with unstable and uncertain states, the adaptive optimizer fine-tunes them to near and stable predictions in high-confidence regions. The experimental results in intra-patient, inter-patient, inter-gender, inter-age, and inter-BMI tasks show that the proposed approach can generate complex strategies for diverse populations, with accuracy and speed significantly better than our previous method.

\section*{Acknowledgments}

% This work was supported by Wuhan University Specific Fund for Major School-level Internationalization Initiatives and Suzhou Key Industry Technology Innovation Project under the grant agreement number SYG202121.

This work was supported by the Fundamental Research Funds for the Central Universities (No.2042023KF0110).

\newpage

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


