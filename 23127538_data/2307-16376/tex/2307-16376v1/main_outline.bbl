% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{zhao2023survey}
W.~X. Zhao, K.~Zhou, J.~Li, T.~Tang, X.~Wang, Y.~Hou, Y.~Min, B.~Zhang,
  J.~Zhang, Z.~Dong \emph{et~al.}, ``A survey of large language models,''
  \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem{huang2022towards}
J.~Huang and K.~C.-C. Chang, ``Towards reasoning in large language models: A
  survey,'' \emph{arXiv preprint arXiv:2212.10403}, 2022.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell \emph{et~al.}, ``Language
  models are few-shot learners,'' \emph{Advances in neural information
  processing systems}, vol.~33, pp. 1877--1901, 2020.

\bibitem{salemi2023lamp}
A.~Salemi, S.~Mysore, M.~Bendersky, and H.~Zamani, ``Lamp: When large language
  models meet personalization,'' \emph{arXiv preprint arXiv:2304.11406}, 2023.

\bibitem{wu2023survey}
L.~Wu, Z.~Zheng, Z.~Qiu, H.~Wang, H.~Gu, T.~Shen, C.~Qin, C.~Zhu, H.~Zhu,
  Q.~Liu \emph{et~al.}, ``A survey on large language models for
  recommendation,'' \emph{arXiv preprint arXiv:2305.19860}, 2023.

\bibitem{lin2023can}
J.~Lin, X.~Dai, Y.~Xi, W.~Liu, B.~Chen, X.~Li, C.~Zhu, H.~Guo, Y.~Yu, R.~Tang
  \emph{et~al.}, ``How can recommender systems benefit from large language
  models: A survey,'' \emph{arXiv preprint arXiv:2306.05817}, 2023.

\bibitem{fan2023recommender}
W.~Fan, Z.~Zhao, J.~Li, Y.~Liu, X.~Mei, Y.~Wang, J.~Tang, and Q.~Li,
  ``Recommender systems in the era of large language models (llms),''
  \emph{arXiv preprint arXiv:2307.02046}, 2023.

\bibitem{resnick1994grouplens}
P.~Resnick, N.~Iacovou, M.~Suchak, P.~Bergstrom, and J.~Riedl, ``Grouplens: An
  open architecture for collaborative filtering of netnews,'' in
  \emph{Proceedings of the 1994 ACM conference on Computer supported
  cooperative work}, 1994, pp. 175--186.

\bibitem{pan2008one}
R.~Pan, Y.~Zhou, B.~Cao, N.~N. Liu, R.~Lukose, M.~Scholz, and Q.~Yang,
  ``One-class collaborative filtering,'' in \emph{2008 Eighth IEEE
  international conference on data mining}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2008, pp. 502--511.

\bibitem{koren2009matrix}
Y.~Koren, R.~Bell, and C.~Volinsky, ``Matrix factorization techniques for
  recommender systems,'' \emph{Computer}, vol.~42, no.~8, pp. 30--37, 2009.

\bibitem{wang2006unifying}
J.~Wang, A.~P. De~Vries, and M.~J. Reinders, ``Unifying user-based and
  item-based collaborative filtering approaches by similarity fusion,'' in
  \emph{Proceedings of the 29th annual international ACM SIGIR conference on
  Research and development in information retrieval}, 2006, pp. 501--508.

\bibitem{pazzani2007content}
M.~J. Pazzani and D.~Billsus, ``Content-based recommendation systems,'' in
  \emph{The adaptive web: methods and strategies of web personalization}.\hskip
  1em plus 0.5em minus 0.4em\relax Springer, 2007, pp. 325--341.

\bibitem{wang2011collaborative}
C.~Wang and D.~M. Blei, ``Collaborative topic modeling for recommending
  scientific articles,'' in \emph{Proceedings of the 17th ACM SIGKDD
  international conference on Knowledge discovery and data mining}, 2011, pp.
  448--456.

\bibitem{zhang2016collaborative}
F.~Zhang, N.~J. Yuan, D.~Lian, X.~Xie, and W.-Y. Ma, ``Collaborative knowledge
  base embedding for recommender systems,'' in \emph{Proceedings of the 22nd
  ACM SIGKDD international conference on knowledge discovery and data mining},
  2016, pp. 353--362.

\bibitem{liu2019nrpa}
H.~Liu, F.~Wu, W.~Wang, X.~Wang, P.~Jiao, C.~Wu, and X.~Xie, ``Nrpa: neural
  recommendation with personalized attention,'' in \emph{Proceedings of the
  42nd International ACM SIGIR Conference on Research and Development in
  Information Retrieval}, 2019, pp. 1233--1236.

\bibitem{wang2015collaborative}
H.~Wang, N.~Wang, and D.-Y. Yeung, ``Collaborative deep learning for
  recommender systems,'' in \emph{Proceedings of the 21th ACM SIGKDD
  international conference on knowledge discovery and data mining}, 2015, pp.
  1235--1244.

\bibitem{zhou2018deep}
G.~Zhou, X.~Zhu, C.~Song, Y.~Fan, H.~Zhu, X.~Ma, Y.~Yan, J.~Jin, H.~Li, and
  K.~Gai, ``Deep interest network for click-through rate prediction,'' in
  \emph{Proceedings of the 24th ACM SIGKDD international conference on
  knowledge discovery \& data mining}, 2018, pp. 1059--1068.

\bibitem{zhou2019deep}
G.~Zhou, N.~Mou, Y.~Fan, Q.~Pi, W.~Bian, C.~Zhou, X.~Zhu, and K.~Gai, ``Deep
  interest evolution network for click-through rate prediction,'' in
  \emph{Proceedings of the AAAI conference on artificial intelligence},
  vol.~33, no.~01, 2019, pp. 5941--5948.

\bibitem{wang2019neural}
X.~Wang, X.~He, M.~Wang, F.~Feng, and T.-S. Chua, ``Neural graph collaborative
  filtering,'' in \emph{Proceedings of the 42nd international ACM SIGIR
  conference on Research and development in Information Retrieval}, 2019, pp.
  165--174.

\bibitem{kang2018self}
W.-C. Kang and J.~McAuley, ``Self-attentive sequential recommendation,'' in
  \emph{2018 IEEE international conference on data mining (ICDM)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 197--206.

\bibitem{hidasi2015session}
B.~Hidasi, A.~Karatzoglou, L.~Baltrunas, and D.~Tikk, ``Session-based
  recommendations with recurrent neural networks,'' \emph{arXiv preprint
  arXiv:1511.06939}, 2015.

\bibitem{sun2019bert4rec}
F.~Sun, J.~Liu, J.~Wu, C.~Pei, X.~Lin, W.~Ou, and P.~Jiang, ``Bert4rec:
  Sequential recommendation with bidirectional encoder representations from
  transformer,'' in \emph{Proceedings of the 28th ACM international conference
  on information and knowledge management}, 2019, pp. 1441--1450.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin, ``Attention is all you need,''
  \emph{Advances in neural information processing systems}, vol.~30, 2017.

\bibitem{kaplan2020scaling}
J.~Kaplan, S.~McCandlish, T.~Henighan, T.~B. Brown, B.~Chess, R.~Child,
  S.~Gray, A.~Radford, J.~Wu, and D.~Amodei, ``Scaling laws for neural language
  models,'' \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark \emph{et~al.}, ``Training
  compute-optimal large language models,'' \emph{arXiv preprint
  arXiv:2203.15556}, 2022.

\bibitem{wang2018dkn}
H.~Wang, F.~Zhang, X.~Xie, and M.~Guo, ``Dkn: Deep knowledge-aware network for
  news recommendation,'' in \emph{Proceedings of the 2018 world wide web
  conference}, 2018, pp. 1835--1844.

\bibitem{huang2018improving}
J.~Huang, W.~X. Zhao, H.~Dou, J.-R. Wen, and E.~Y. Chang, ``Improving
  sequential recommendation with knowledge-enhanced memory networks,'' in
  \emph{The 41st international ACM SIGIR conference on research \& development
  in information retrieval}, 2018, pp. 505--514.

\bibitem{wang2018shine}
H.~Wang, F.~Zhang, M.~Hou, X.~Xie, M.~Guo, and Q.~Liu, ``Shine: Signed
  heterogeneous information network embedding for sentiment link prediction,''
  in \emph{Proceedings of the eleventh ACM international conference on web
  search and data mining}, 2018, pp. 592--600.

\bibitem{yu2013collaborative}
X.~Yu, X.~Ren, Q.~Gu, Y.~Sun, and J.~Han, ``Collaborative filtering with entity
  similarity regularization in heterogeneous information networks,''
  \emph{IJCAI HINA}, vol.~27, 2013.

\bibitem{shi2015semantic}
C.~Shi, Z.~Zhang, P.~Luo, P.~S. Yu, Y.~Yue, and B.~Wu, ``Semantic path based
  personalized recommendation on weighted heterogeneous information networks,''
  in \emph{Proceedings of the 24th ACM International on Conference on
  Information and Knowledge Management}, 2015, pp. 453--462.

\bibitem{ma2019jointly}
W.~Ma, M.~Zhang, Y.~Cao, W.~Jin, C.~Wang, Y.~Liu, S.~Ma, and X.~Ren, ``Jointly
  learning explainable rules for recommendation with knowledge graph,'' in
  \emph{The world wide web conference}, 2019, pp. 1210--1221.

\bibitem{huang2019explainable}
X.~Huang, Q.~Fang, S.~Qian, J.~Sang, Y.~Li, and C.~Xu, ``Explainable
  interaction-driven user modeling over knowledge graph for sequential
  recommendation,'' in \emph{proceedings of the 27th ACM international
  conference on multimedia}, 2019, pp. 548--556.

\bibitem{wang2018ripplenet}
H.~Wang, F.~Zhang, J.~Wang, M.~Zhao, W.~Li, X.~Xie, and M.~Guo, ``Ripplenet:
  Propagating user preferences on the knowledge graph for recommender
  systems,'' in \emph{Proceedings of the 27th ACM international conference on
  information and knowledge management}, 2018, pp. 417--426.

\bibitem{wang2019knowledge}
H.~Wang, M.~Zhao, X.~Xie, W.~Li, and M.~Guo, ``Knowledge graph convolutional
  networks for recommender systems,'' in \emph{The world wide web conference},
  2019, pp. 3307--3313.

\bibitem{wang2019kgat}
X.~Wang, X.~He, Y.~Cao, M.~Liu, and T.-S. Chua, ``Kgat: Knowledge graph
  attention network for recommendation,'' in \emph{Proceedings of the 25th ACM
  SIGKDD international conference on knowledge discovery \& data mining}, 2019,
  pp. 950--958.

\bibitem{tang2019akupm}
X.~Tang, T.~Wang, H.~Yang, and H.~Song, ``Akupm: Attention-enhanced
  knowledge-aware user preference model for recommendation,'' in
  \emph{Proceedings of the 25th ACM SIGKDD international conference on
  knowledge discovery \& data mining}, 2019, pp. 1891--1899.

\bibitem{zhao2019intentgc}
J.~Zhao, Z.~Zhou, Z.~Guan, W.~Zhao, W.~Ning, G.~Qiu, and X.~He, ``Intentgc: a
  scalable graph convolution framework fusing heterogeneous information for
  recommendation,'' in \emph{Proceedings of the 25th ACM SIGKDD International
  Conference on Knowledge Discovery \& Data Mining}, 2019, pp. 2347--2357.

\bibitem{petroni2019language}
F.~Petroni, T.~Rockt{\"a}schel, P.~Lewis, A.~Bakhtin, Y.~Wu, A.~H. Miller, and
  S.~Riedel, ``Language models as knowledge bases?'' \emph{arXiv preprint
  arXiv:1909.01066}, 2019.

\bibitem{roberts2020much}
A.~Roberts, C.~Raffel, and N.~Shazeer, ``How much knowledge can you pack into
  the parameters of a language model?'' \emph{arXiv preprint arXiv:2002.08910},
  2020.

\bibitem{petronicontext}
F.~Petroni, P.~Lewis, A.~Piktus, T.~Rockt{\"a}schel, Y.~Wu, A.~H. Miller, and
  S.~Riedel, ``How context affects language models' factual predictions,'' in
  \emph{Automated Knowledge Base Construction}.

\bibitem{jiang2020can}
Z.~Jiang, F.~F. Xu, J.~Araki, and G.~Neubig, ``How can we know what language
  models know?'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~8, pp. 423--438, 2020.

\bibitem{wang2020language}
C.~Wang, X.~Liu, and D.~Song, ``Language models are open knowledge graphs,''
  \emph{arXiv preprint arXiv:2010.11967}, 2020.

\bibitem{poerner2020bert}
N.~Poerner, U.~Waltinger, and H.~Sch{\"u}tze, ``E-bert: Efficient-yet-effective
  entity embeddings for bert,'' in \emph{Findings of the Association for
  Computational Linguistics: EMNLP 2020}, 2020, pp. 803--818.

\bibitem{heinzerling2021language}
B.~Heinzerling and K.~Inui, ``Language models as knowledge bases: On entity
  representations, storage capacity, and paraphrased queries,'' in
  \emph{Proceedings of the 16th Conference of the European Chapter of the
  Association for Computational Linguistics: Main Volume}, 2021, pp.
  1772--1791.

\bibitem{wang2021can}
C.~Wang, P.~Liu, and Y.~Zhang, ``Can generative pre-trained language models
  serve as knowledge bases for closed-book qa?'' in \emph{Proceedings of the
  59th Annual Meeting of the Association for Computational Linguistics and the
  11th International Joint Conference on Natural Language Processing (Volume 1:
  Long Papers)}, 2021, pp. 3241--3251.

\bibitem{guu2020retrieval}
K.~Guu, K.~Lee, Z.~Tung, P.~Pasupat, and M.~Chang, ``Retrieval augmented
  language model pre-training,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2020, pp. 3929--3938.

\bibitem{bordes2013translating}
A.~Bordes, N.~Usunier, A.~Garcia-Duran, J.~Weston, and O.~Yakhnenko,
  ``Translating embeddings for modeling multi-relational data,'' \emph{Advances
  in neural information processing systems}, vol.~26, 2013.

\bibitem{zhu2023llms}
Y.~Zhu, X.~Wang, J.~Chen, S.~Qiao, Y.~Ou, Y.~Yao, S.~Deng, H.~Chen, and
  N.~Zhang, ``Llms for knowledge graph construction and reasoning: Recent
  capabilities and future opportunities,'' \emph{arXiv preprint
  arXiv:2305.13168}, 2023.

\bibitem{zhang2020pretrain}
Z.~Zhang, X.~Liu, Y.~Zhang, Q.~Su, X.~Sun, and B.~He, ``Pretrain-kge: learning
  knowledge representation from pretrained language models,'' in \emph{Findings
  of the Association for Computational Linguistics: EMNLP 2020}, 2020, pp.
  259--266.

\bibitem{kumar2020building}
A.~Kumar, A.~Pandey, R.~Gadia, and M.~Mishra, ``Building knowledge graph using
  pre-trained language model for learning entity-aware relationships,'' in
  \emph{2020 IEEE International Conference on Computing, Power and
  Communication Technologies (GUCON)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2020, pp. 310--315.

\bibitem{kim2020multi}
B.~Kim, T.~Hong, Y.~Ko, and J.~Seo, ``Multi-task learning for knowledge graph
  completion with pre-trained language models,'' in \emph{Proceedings of the
  28th International Conference on Computational Linguistics}, 2020, pp.
  1737--1743.

\bibitem{choi2021mem}
B.~Choi, D.~Jang, and Y.~Ko, ``Mem-kgc: Masked entity model for knowledge graph
  completion with pre-trained language model,'' \emph{IEEE Access}, vol.~9, pp.
  132\,025--132\,032, 2021.

\bibitem{wang2021structure}
B.~Wang, T.~Shen, G.~Long, T.~Zhou, Y.~Wang, and Y.~Chang,
  ``Structure-augmented text representation learning for efficient knowledge
  graph completion,'' in \emph{Proceedings of the Web Conference 2021}, 2021,
  pp. 1737--1748.

\bibitem{xie2022discrimination}
X.~Xie, N.~Zhang, Z.~Li, S.~Deng, H.~Chen, F.~Xiong, M.~Chen, and H.~Chen,
  ``From discrimination to generation: knowledge graph completion with
  generative transformer,'' in \emph{Companion Proceedings of the Web
  Conference 2022}, 2022, pp. 162--165.

\bibitem{jiang2023text}
P.~Jiang, S.~Agarwal, B.~Jin, X.~Wang, J.~Sun, and J.~Han, ``Text-augmented
  open knowledge graph completion via pre-trained language models,''
  \emph{arXiv preprint arXiv:2305.15597}, 2023.

\bibitem{yan2021unified}
H.~Yan, T.~Gui, J.~Dai, Q.~Guo, Z.~Zhang, and X.~Qiu, ``A unified generative
  framework for various ner subtasks,'' in \emph{Proceedings of the 59th Annual
  Meeting of the Association for Computational Linguistics and the 11th
  International Joint Conference on Natural Language Processing (Volume 1: Long
  Papers)}, 2021, pp. 5808--5822.

\bibitem{li2022ultra}
B.~Li, W.~Yin, and M.~Chen, ``Ultra-fine entity typing with indirect
  supervision from natural language inference,'' \emph{Transactions of the
  Association for Computational Linguistics}, vol.~10, pp. 607--622, 2022.

\bibitem{kirstain2021coreference}
Y.~Kirstain, O.~Ram, and O.~Levy, ``Coreference resolution without span
  representations,'' in \emph{Proceedings of the 59th Annual Meeting of the
  Association for Computational Linguistics and the 11th International Joint
  Conference on Natural Language Processing (Volume 2: Short Papers)}, 2021,
  pp. 14--19.

\bibitem{cattan2021cross}
A.~Cattan, A.~Eirew, G.~Stanovsky, M.~Joshi, and I.~Dagan, ``Cross-document
  coreference resolution over predicted mentions,'' in \emph{Findings of the
  Association for Computational Linguistics: ACL-IJCNLP 2021}, 2021, pp.
  5100--5107.

\bibitem{lyu2021relation}
S.~Lyu and H.~Chen, ``Relation classification with entity type restriction,''
  in \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, 2021, pp. 390--395.

\bibitem{wang2019fine}
H.~Wang, C.~Focke, R.~Sylvester, N.~Mishra, and W.~Wang, ``Fine-tune bert for
  docred with two-step process,'' \emph{arXiv preprint arXiv:1909.11898}, 2019.

\bibitem{han2023pive}
J.~Han, N.~Collier, W.~Buntine, and E.~Shareghi, ``Pive: Prompting with
  iterative verification improving graph-based generative capability of llms,''
  \emph{arXiv preprint arXiv:2305.12392}, 2023.

\bibitem{trajanoska2023enhancing}
M.~Trajanoska, R.~Stojanov, and D.~Trajanov, ``Enhancing knowledge graph
  construction using large language models,'' \emph{arXiv preprint
  arXiv:2305.04676}, 2023.

\bibitem{west2022symbolic}
P.~West, C.~Bhagavatula, J.~Hessel, J.~Hwang, L.~Jiang, R.~Le~Bras, X.~Lu,
  S.~Welleck, and Y.~Choi, ``Symbolic knowledge distillation: from general
  language models to commonsense models,'' in \emph{Proceedings of the 2022
  Conference of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies}, 2022, pp. 4602--4625.

\bibitem{xi2023towards}
Y.~Xi, W.~Liu, J.~Lin, J.~Zhu, B.~Chen, R.~Tang, W.~Zhang, R.~Zhang, and Y.~Yu,
  ``Towards open-world recommendation with knowledge augmentation from large
  language models,'' \emph{arXiv preprint arXiv:2306.10933}, 2023.

\bibitem{razniewski2021language}
S.~Razniewski, A.~Yates, N.~Kassner, and G.~Weikum, ``Language models as or for
  knowledge bases,'' \emph{arXiv preprint arXiv:2110.04888}, 2021.

\bibitem{yu2023kola}
J.~Yu, X.~Wang, S.~Tu, S.~Cao, D.~Zhang-Li, X.~Lv, H.~Peng, Z.~Yao, X.~Zhang,
  H.~Li \emph{et~al.}, ``Kola: Carefully benchmarking world knowledge of large
  language models,'' \emph{arXiv preprint arXiv:2306.09296}, 2023.

\bibitem{ye2022packed}
D.~Ye, Y.~Lin, P.~Li, and M.~Sun, ``Packed levitated marker for entity and
  relation extraction,'' in \emph{Proceedings of the 60th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers)}, 2022, pp.
  4904--4917.

\bibitem{lang1995newsweeder}
K.~Lang, ``Newsweeder: Learning to filter netnews,'' in \emph{Machine learning
  proceedings 1995}.\hskip 1em plus 0.5em minus 0.4em\relax Elsevier, 1995, pp.
  331--339.

\bibitem{wang2016collaborative}
H.~Wang, X.~Shi, and D.-Y. Yeung, ``Collaborative recurrent autoencoder:
  Recommend while learning to fill in the blanks,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~29, 2016.

\bibitem{dong2017hybrid}
X.~Dong, L.~Yu, Z.~Wu, Y.~Sun, L.~Yuan, and F.~Zhang, ``A hybrid collaborative
  filtering model with deep structure for recommender systems,'' in
  \emph{Proceedings of the AAAI Conference on artificial intelligence},
  vol.~31, no.~1, 2017.

\bibitem{li2017collaborative}
X.~Li and J.~She, ``Collaborative variational autoencoder for recommender
  systems,'' in \emph{Proceedings of the 23rd ACM SIGKDD international
  conference on knowledge discovery and data mining}, 2017, pp. 305--314.

\bibitem{WuWHX23survey}
C.~Wu, F.~Wu, Y.~Huang, and X.~Xie, ``Personalized news recommendation: Methods
  and challenges,'' \emph{{ACM} Trans. Inf. Syst.}, vol.~41, no.~1, pp.
  24:1--24:50, 2023.

\bibitem{LeM14doc2vec}
Q.~V. Le and T.~Mikolov, ``Distributed representations of sentences and
  documents,'' in \emph{{ICML}}, ser. {JMLR} Workshop and Conference
  Proceedings, vol.~32.\hskip 1em plus 0.5em minus 0.4em\relax JMLR.org, 2014,
  pp. 1188--1196.

\bibitem{SongEH16mlp}
Y.~Song, A.~M. Elkahky, and X.~He, ``Multi-rate deep learning for temporal
  recommendation,'' in \emph{{SIGIR}}.\hskip 1em plus 0.5em minus 0.4em\relax
  {ACM}, 2016, pp. 909--912.

\bibitem{KumarKG17mlp}
V.~Kumar, D.~Khattar, S.~Gupta, M.~Gupta, and V.~Varma, ``Deep neural
  architecture for news recommendation,'' in \emph{{CLEF} (Working Notes)},
  ser. {CEUR} Workshop Proceedings, vol. 1866.\hskip 1em plus 0.5em minus
  0.4em\relax CEUR-WS.org, 2017.

\bibitem{okura2017embedding}
S.~Okura, Y.~Tagami, S.~Ono, and A.~Tajima, ``Embedding-based news
  recommendation for millions of users,'' in \emph{Proceedings of the 23rd ACM
  SIGKDD international conference on knowledge discovery and data mining},
  2017, pp. 1933--1942.

\bibitem{Mikolov2013word2vec}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' in \emph{{ICLR} (Workshop Poster)}, 2013.

\bibitem{wu2019cnnattn}
C.~Wu, F.~Wu, M.~An, J.~Huang, Y.~Huang, and X.~Xie, ``{NPA:} neural news
  recommendation with personalized attention,'' in \emph{{KDD}}.\hskip 1em plus
  0.5em minus 0.4em\relax {ACM}, 2019, pp. 2576--2584.

\bibitem{An19cnnrnn}
M.~An, F.~Wu, C.~Wu, K.~Zhang, Z.~Liu, and X.~Xie, ``Neural news recommendation
  with long- and short-term user representations,'' in \emph{{ACL}
  {(1)}}.\hskip 1em plus 0.5em minus 0.4em\relax Association for Computational
  Linguistics, 2019, pp. 336--345.

\bibitem{wu19selfattn}
C.~Wu, F.~Wu, S.~Ge, T.~Qi, Y.~Huang, and X.~Xie, ``Neural news recommendation
  with multi-head self-attention,'' in \emph{{EMNLP/IJCNLP} {(1)}}.\hskip 1em
  plus 0.5em minus 0.4em\relax Association for Computational Linguistics, 2019,
  pp. 6388--6393.

\bibitem{wu20selfattn}
C.~Wu, F.~Wu, T.~Qi, and Y.~Huang, ``User modeling with click preference and
  reading satisfaction for news recommendation,'' in \emph{{IJCAI}}.\hskip 1em
  plus 0.5em minus 0.4em\relax ijcai.org, 2020, pp. 3023--3029.

\bibitem{khattar2018cnn}
D.~Khattar, V.~Kumar, V.~Varma, and M.~Gupta, ``Weave{\&}rec: {A} word
  embedding based 3-d convolutional network for news recommendation,'' in
  \emph{{CIKM}}.\hskip 1em plus 0.5em minus 0.4em\relax {ACM}, 2018, pp.
  1855--1858.

\bibitem{Zhu19DAN}
Q.~Zhu, X.~Zhou, Z.~Song, J.~Tan, and L.~Guo, ``{DAN:} deep attention neural
  network for news recommendation,'' in \emph{{AAAI}}.\hskip 1em plus 0.5em
  minus 0.4em\relax {AAAI} Press, 2019, pp. 5973--5980.

\bibitem{qiu2021u}
Z.~Qiu, X.~Wu, J.~Gao, and W.~Fan, ``U-bert: Pre-training user representations
  for improved recommendation,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~35, no.~5, 2021, pp. 4320--4327.

\bibitem{zhang2021unbert}
Q.~Zhang, J.~Li, Q.~Jia, C.~Wang, J.~Zhu, Z.~Wang, and X.~He, ``Unbert:
  User-news matching bert for news recommendation.'' in \emph{IJCAI}, 2021, pp.
  3356--3362.

\bibitem{wu2021empowering}
C.~Wu, F.~Wu, T.~Qi, and Y.~Huang, ``Empowering news recommendation with
  pre-trained language models,'' in \emph{Proceedings of the 44th International
  ACM SIGIR Conference on Research and Development in Information Retrieval},
  2021, pp. 1652--1656.

\bibitem{liu2022boosting}
Q.~Liu, J.~Zhu, Q.~Dai, and X.~Wu, ``Boosting deep ctr prediction with a
  plug-and-play pre-trainer for news recommendation,'' in \emph{Proceedings of
  the 29th International Conference on Computational Linguistics}, 2022, pp.
  2823--2833.

\bibitem{wu2022mm}
C.~Wu, F.~Wu, T.~Qi, C.~Zhang, Y.~Huang, and T.~Xu, ``Mm-rec: Visiolinguistic
  model empowered multimodal news recommendation,'' in \emph{Proceedings of the
  45th International ACM SIGIR Conference on Research and Development in
  Information Retrieval}, 2022, pp. 2560--2564.

\bibitem{yu2022tiny}
Y.~Yu, F.~Wu, C.~Wu, J.~Yi, and Q.~Liu, ``Tiny-newsrec: Effective and efficient
  plm-based news recommendation,'' in \emph{Proceedings of the 2022 Conference
  on Empirical Methods in Natural Language Processing}, 2022, pp. 5478--5489.

\bibitem{zou2021pre}
L.~Zou, S.~Zhang, H.~Cai, D.~Ma, S.~Cheng, S.~Wang, D.~Shi, Z.~Cheng, and
  D.~Yin, ``Pre-trained language model based ranking in baidu search,'' in
  \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&
  Data Mining}, 2021, pp. 4014--4022.

\bibitem{liu2021pre}
Y.~Liu, W.~Lu, S.~Cheng, D.~Shi, S.~Wang, Z.~Cheng, and D.~Yin, ``Pre-trained
  language model for web-scale retrieval in baidu search,'' in
  \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&
  Data Mining}, 2021, pp. 3365--3375.

\bibitem{muhamed2021ctr}
A.~Muhamed, I.~Keivanloo, S.~Perera, J.~Mracek, Y.~Xu, Q.~Cui, S.~Rajagopalan,
  B.~Zeng, and T.~Chilimbi, ``Ctr-bert: Cost-effective knowledge distillation
  for billion-parameter teacher models,'' in \emph{NeurIPS Efficient Natural
  Language and Speech Processing Workshop}, 2021.

\bibitem{he2022ptm4tag}
J.~He, B.~Xu, Z.~Yang, D.~Han, C.~Yang, and D.~Lo, ``Ptm4tag: sharpening tag
  recommendation of stack overflow posts with pre-trained models,'' in
  \emph{Proceedings of the 30th IEEE/ACM International Conference on Program
  Comprehension}, 2022, pp. 1--11.

\bibitem{zhang2022twhin}
X.~Zhang, Y.~Malkov, O.~Florez, S.~Park, B.~McWilliams, J.~Han, and
  A.~El-Kishky, ``Twhin-bert: A socially-enriched pre-trained language model
  for multilingual tweet representations,'' \emph{arXiv preprint
  arXiv:2209.07562}, 2022.

\bibitem{rahmani2023improving}
S.~Rahmani, A.~Naghshzan, and L.~Guerrouj, ``Improving code example
  recommendations on informal documentation using bert and query-aware lsh: A
  comparative study,'' \emph{arXiv preprint arXiv:2305.03017}, 2023.

\bibitem{ding2021zero}
H.~Ding, Y.~Ma, A.~Deoras, Y.~Wang, and H.~Wang, ``Zero-shot recommender
  systems,'' \emph{arXiv preprint arXiv:2105.08318}, 2021.

\bibitem{hou2022towards}
Y.~Hou, S.~Mu, W.~X. Zhao, Y.~Li, B.~Ding, and J.-R. Wen, ``Towards universal
  sequence representation learning for recommender systems,'' in
  \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining}, 2022, pp. 585--593.

\bibitem{hou2023learning}
Y.~Hou, Z.~He, J.~McAuley, and W.~X. Zhao, ``Learning vector-quantized item
  representation for transferable sequential recommenders,'' in
  \emph{Proceedings of the ACM Web Conference 2023}, 2023, pp. 1162--1171.

\bibitem{yuan2023go}
Z.~Yuan, F.~Yuan, Y.~Song, Y.~Li, J.~Fu, F.~Yang, Y.~Pan, and Y.~Ni, ``Where to
  go next for recommender systems? id-vs. modality-based recommender models
  revisited,'' \emph{arXiv preprint arXiv:2303.13835}, 2023.

\bibitem{fu2023exploring}
J.~Fu, F.~Yuan, Y.~Song, Z.~Yuan, M.~Cheng, S.~Cheng, J.~Zhang, J.~Wang, and
  Y.~Pan, ``Exploring adapter-based transfer learning for recommender systems:
  Empirical studies and practical insights,'' \emph{arXiv preprint
  arXiv:2305.15036}, 2023.

\bibitem{bao2023tallrec}
K.~Bao, J.~Zhang, Y.~Zhang, W.~Wang, F.~Feng, and X.~He, ``Tallrec: An
  effective and efficient tuning framework to align large language model with
  recommendation,'' \emph{arXiv preprint arXiv:2305.00447}, 2023.

\bibitem{kang2023llms}
W.-C. Kang, J.~Ni, N.~Mehta, M.~Sathiamoorthy, L.~Hong, E.~Chi, and D.~Z.
  Cheng, ``Do llms understand user preferences? evaluating llms on user rating
  prediction,'' \emph{arXiv preprint arXiv:2305.06474}, 2023.

\bibitem{chen2023palr}
Z.~Chen, ``Palr: Personalization aware llms for recommendation,'' \emph{arXiv
  preprint arXiv:2305.07622}, 2023.

\bibitem{zhang2023recommendation}
J.~Zhang, R.~Xie, Y.~Hou, W.~X. Zhao, L.~Lin, and J.-R. Wen, ``Recommendation
  as instruction following: A large language model empowered recommendation
  approach,'' \emph{arXiv preprint arXiv:2305.07001}, 2023.

\bibitem{wang2018reinforcement}
X.~Wang, Y.~Chen, J.~Yang, L.~Wu, Z.~Wu, and X.~Xie, ``A reinforcement learning
  framework for explainable recommendation,'' in \emph{2018 IEEE international
  conference on data mining (ICDM)}.\hskip 1em plus 0.5em minus 0.4em\relax
  IEEE, 2018, pp. 587--596.

\bibitem{gao2019explainable}
J.~Gao, X.~Wang, Y.~Wang, and X.~Xie, ``Explainable recommendation through
  attentive multi-view learning,'' in \emph{Proceedings of the AAAI Conference
  on Artificial Intelligence}, vol.~33, no.~01, 2019, pp. 3622--3629.

\bibitem{leeself}
S.~Lee, X.~Wang, S.~Han, X.~Yi, X.~Xie, and M.~Cha, ``Self-explaining deep
  models with logic rule reasoning,'' in \emph{Advances in Neural Information
  Processing Systems}, 2022.

\bibitem{nye2021show}
M.~Nye, A.~J. Andreassen, G.~Gur-Ari, H.~Michalewski, J.~Austin, D.~Bieber,
  D.~Dohan, A.~Lewkowycz, M.~Bosma, D.~Luan \emph{et~al.}, ``Show your work:
  Scratchpads for intermediate computation with language models,'' \emph{arXiv
  preprint arXiv:2112.00114}, 2021.

\bibitem{lampinen2022can}
A.~K. Lampinen, I.~Dasgupta, S.~C. Chan, K.~Matthewson, M.~H. Tessler,
  A.~Creswell, J.~L. McClelland, J.~X. Wang, and F.~Hill, ``Can language models
  learn from explanations in context?'' \emph{arXiv preprint arXiv:2204.02329},
  2022.

\bibitem{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, E.~Chi, Q.~Le, and D.~Zhou, ``Chain
  of thought prompting elicits reasoning in large language models,''
  \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem{zelikman2022star}
E.~Zelikman, Y.~Wu, J.~Mu, and N.~Goodman, ``Star: Bootstrapping reasoning with
  reasoning,'' \emph{Advances in Neural Information Processing Systems},
  vol.~35, pp. 15\,476--15\,488, 2022.

\bibitem{zhang2020explainable}
Y.~Zhang, X.~Chen \emph{et~al.}, ``Explainable recommendation: A survey and new
  perspectives,'' \emph{Foundations and Trends{\textregistered} in Information
  Retrieval}, vol.~14, no.~1, pp. 1--101, 2020.

\bibitem{schafer1999recommender}
J.~B. Schafer, J.~Konstan, and J.~Riedl, ``Recommender systems in e-commerce,''
  in \emph{Proceedings of the 1st ACM conference on Electronic commerce}, 1999,
  pp. 158--166.

\bibitem{linden2003amazon}
G.~Linden, B.~Smith, and J.~York, ``Amazon. com recommendations: Item-to-item
  collaborative filtering,'' \emph{IEEE Internet computing}, vol.~7, no.~1, pp.
  76--80, 2003.

\bibitem{gomez2015netflix}
C.~A. Gomez-Uribe and N.~Hunt, ``The netflix recommender system: Algorithms,
  business value, and innovation,'' \emph{ACM Transactions on Management
  Information Systems (TMIS)}, vol.~6, no.~4, pp. 1--19, 2015.

\bibitem{sinha2002role}
R.~Sinha and K.~Swearingen, ``The role of transparency in recommender
  systems,'' in \emph{CHI'02 extended abstracts on Human factors in computing
  systems}, 2002, pp. 830--831.

\bibitem{xian2021ex3}
Y.~Xian, T.~Zhao, J.~Li, J.~Chan, A.~Kan, J.~Ma, X.~L. Dong, C.~Faloutsos,
  G.~Karypis, S.~Muthukrishnan \emph{et~al.}, ``Ex3: Explainable
  attribute-aware item-set recommendations,'' in \emph{Proceedings of the 15th
  ACM Conference on Recommender Systems}, 2021, pp. 484--494.

\bibitem{wang2022reinforced}
X.~Wang, Q.~Li, D.~Yu, and G.~Xu, ``Reinforced path reasoning for
  counterfactual explainable recommendation,'' \emph{arXiv preprint
  arXiv:2207.06674}, 2022.

\bibitem{verma2022recxplainer}
S.~Verma, A.~Beniwal, N.~Sadagopan, and A.~Seshadri, ``Recxplainer: Post-hoc
  attribute-based explanations for recommender systems,'' \emph{arXiv preprint
  arXiv:2211.14935}, 2022.

\bibitem{zhang2022neuro}
W.~Zhang, J.~Yan, Z.~Wang, and J.~Wang, ``Neuro-symbolic interpretable
  collaborative filtering for attribute-based recommendation,'' in
  \emph{Proceedings of the ACM Web Conference 2022}, 2022, pp. 3229--3238.

\bibitem{li2017neural}
P.~Li, Z.~Wang, Z.~Ren, L.~Bing, and W.~Lam, ``Neural rating regression with
  abstractive tips generation for recommendation,'' in \emph{Proceedings of the
  40th International ACM SIGIR conference on Research and Development in
  Information Retrieval}, 2017, pp. 345--354.

\bibitem{dong2017learning}
L.~Dong, S.~Huang, F.~Wei, M.~Lapata, M.~Zhou, and K.~Xu, ``Learning to
  generate product reviews from attributes,'' in \emph{Proceedings of the 15th
  Conference of the European Chapter of the Association for Computational
  Linguistics: Volume 1, Long Papers}, 2017, pp. 623--632.

\bibitem{li2020generate}
L.~Li, Y.~Zhang, and L.~Chen, ``Generate neural template explanations for
  recommendation,'' in \emph{Proceedings of the 29th ACM International
  Conference on Information \& Knowledge Management}, 2020, pp. 755--764.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' \emph{Neural
  computation}, vol.~9, no.~8, pp. 1735--1780, 1997.

\bibitem{cho2014learning}
K.~Cho, B.~Van~Merri{\"e}nboer, C.~Gulcehre, D.~Bahdanau, F.~Bougares,
  H.~Schwenk, and Y.~Bengio, ``Learning phrase representations using rnn
  encoder-decoder for statistical machine translation,'' \emph{arXiv preprint
  arXiv:1406.1078}, 2014.

\bibitem{li2021personalized}
L.~Li, Y.~Zhang, and L.~Chen, ``Personalized transformer for explainable
  recommendation,'' in \emph{Proceedings of the 59th Annual Meeting of the
  Association for Computational Linguistics and the 11th International Joint
  Conference on Natural Language Processing (Volume 1: Long Papers)}, 2021.

\bibitem{zhan2023towards}
H.~Zhan, L.~Li, S.~Li, W.~Liu, M.~Gupta, and A.~C. Kot, ``Towards explainable
  recommendation via bert-guided explanation generator,'' in \emph{ICASSP
  2023-2023 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2023, pp.
  1--5.

\bibitem{ni2019justifying}
J.~Ni, J.~Li, and J.~McAuley, ``Justifying recommendations using
  distantly-labeled reviews and fine-grained aspects,'' in \emph{Proceedings of
  the 2019 conference on empirical methods in natural language processing and
  the 9th international joint conference on natural language processing
  (EMNLP-IJCNLP)}, 2019, pp. 188--197.

\bibitem{liu2023multimodal}
Z.~Liu, Y.~Ma, M.~Schubert, Y.~Ouyang, W.~Rong, and Z.~Xiong, ``Multimodal
  contrastive transformer for explainable recommendation,'' \emph{IEEE
  Transactions on Computational Social Systems}, 2023.

\bibitem{qu2022explanation}
Y.~Qu and H.~Nobuhara, ``Explanation generated for sequential recommendation
  based on transformer model,'' in \emph{2022 Joint 12th International
  Conference on Soft Computing and Intelligent Systems and 23rd International
  Symposium on Advanced Intelligent Systems (SCIS\&ISIS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2022, pp. 1--6.

\bibitem{bai2020fusing}
P.~Bai, Y.~Xia, and Y.~Xia, ``Fusing knowledge and aspect sentiment for
  explainable recommendation,'' \emph{IEEE Access}, vol.~8, pp.
  137\,150--137\,160, 2020.

\bibitem{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.
  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill \emph{et~al.}, ``On the
  opportunities and risks of foundation models,'' \emph{arXiv preprint
  arXiv:2108.07258}, 2021.

\bibitem{li2023personalized}
L.~Li, Y.~Zhang, and L.~Chen, ``Personalized prompt learning for explainable
  recommendation,'' \emph{ACM Transactions on Information Systems}, vol.~41,
  no.~4, pp. 1--26, 2023.

\bibitem{bills2023language}
S.~Bills, N.~Cammarata, D.~Mossing, H.~Tillman, L.~Gao, G.~Goh, I.~Sutskever,
  J.~Leike, J.~Wu, and W.~Saunders, ``Language models can explain neurons in
  language models,'' 2023.

\bibitem{wu2023interpretability}
Z.~Wu, A.~Geiger, C.~Potts, and N.~D. Goodman, ``Interpretability at scale:
  Identifying causal mechanisms in alpaca,'' \emph{arXiv preprint
  arXiv:2305.08809}, 2023.

\bibitem{li2023making}
Y.~Li, Z.~Lin, S.~Zhang, Q.~Fu, B.~Chen, J.-G. Lou, and W.~Chen, ``Making large
  language models better reasoners with step-aware verifier,'' 2023.

\bibitem{turpin2023language}
M.~Turpin, J.~Michael, E.~Perez, and S.~R. Bowman, ``Language models don't
  always say what they think: Unfaithful explanations in chain-of-thought
  prompting,'' \emph{arXiv preprint arXiv:2305.04388}, 2023.

\bibitem{li2021hidden}
S.~Li, H.~Liu, T.~Dong, B.~Z.~H. Zhao, M.~Xue, H.~Zhu, and J.~Lu, ``Hidden
  backdoors in human-centric language models,'' in \emph{Proceedings of the
  2021 ACM SIGSAC Conference on Computer and Communications Security}, 2021,
  pp. 3123--3140.

\bibitem{wang2023robustness}
J.~Wang, X.~Hu, W.~Hou, H.~Chen, R.~Zheng, Y.~Wang, L.~Yang, H.~Huang, W.~Ye,
  X.~Geng \emph{et~al.}, ``On the robustness of chatgpt: An adversarial and
  out-of-distribution perspective,'' \emph{arXiv preprint arXiv:2302.12095},
  2023.

\bibitem{han2023information}
R.~Han, T.~Peng, C.~Yang, B.~Wang, L.~Liu, and X.~Wan, ``Is information
  extraction solved by chatgpt? an analysis of performance, evaluation
  criteria, robustness and errors,'' \emph{arXiv preprint arXiv:2305.14450},
  2023.

\bibitem{wei2022emergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama,
  M.~Bosma, D.~Zhou, D.~Metzler \emph{et~al.}, ``Emergent abilities of large
  language models,'' \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem{liu2023chatgpt}
J.~Liu, C.~Liu, R.~Lv, K.~Zhou, and Y.~Zhang, ``Is chatgpt a good recommender?
  a preliminary study,'' \emph{arXiv preprint arXiv:2304.10149}, 2023.

\bibitem{dai2023uncovering}
S.~Dai, N.~Shao, H.~Zhao, W.~Yu, Z.~Si, C.~Xu, Z.~Sun, X.~Zhang, and J.~Xu,
  ``Uncovering chatgpt's capabilities in recommender systems,'' \emph{arXiv
  preprint arXiv:2305.02182}, 2023.

\bibitem{wang2023zero}
L.~Wang and E.-P. Lim, ``Zero-shot next-item recommendation using large
  pretrained language models,'' \emph{arXiv preprint arXiv:2304.03153}, 2023.

\bibitem{hou2023large}
Y.~Hou, J.~Zhang, Z.~Lin, H.~Lu, R.~Xie, J.~McAuley, and W.~X. Zhao, ``Large
  language models are zero-shot rankers for recommender systems,'' \emph{arXiv
  preprint arXiv:2305.08845}, 2023.

\bibitem{li2023preliminary}
X.~Li, Y.~Zhang, and E.~C. Malthouse, ``A preliminary study of chatgpt on news
  recommendation: Personalization, provider fairness, fake news,'' \emph{arXiv
  preprint arXiv:2306.10702}, 2023.

\bibitem{dong2022survey}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui,
  ``A survey for in-context learning,'' \emph{arXiv preprint arXiv:2301.00234},
  2022.

\bibitem{dai2022can}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei, ``Why can gpt learn
  in-context? language models secretly perform gradient descent as meta
  optimizers,'' \emph{arXiv preprint arXiv:2212.10559}, 2022.

\bibitem{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and
  L.~Zettlemoyer, ``Rethinking the role of demonstrations: What makes
  in-context learning work?'' \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem{levy2022diverse}
I.~Levy, B.~Bogin, and J.~Berant, ``Diverse demonstrations improve in-context
  compositional generalization,'' \emph{arXiv preprint arXiv:2212.06800}, 2022.

\bibitem{xieexplanation}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma, ``An explanation of in-context
  learning as implicit bayesian inference,'' in \emph{International Conference
  on Learning Representations}.

\bibitem{olsson2022context}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann,
  A.~Askell, Y.~Bai, A.~Chen \emph{et~al.}, ``In-context learning and induction
  heads,'' \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem{akyurek2022learning}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou, ``What learning
  algorithm is in-context learning? investigations with linear models,''
  \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem{geng2022recommendation}
S.~Geng, S.~Liu, Z.~Fu, Y.~Ge, and Y.~Zhang, ``Recommendation as language
  processing (rlp): A unified pretrain, personalized prompt \& predict paradigm
  (p5),'' in \emph{Proceedings of the 16th ACM Conference on Recommender
  Systems}, 2022, pp. 299--315.

\bibitem{cui2022m6}
Z.~Cui, J.~Ma, C.~Zhou, J.~Zhou, and H.~Yang, ``M6-rec: Generative pretrained
  language models are open-ended recommender systems,'' \emph{arXiv preprint
  arXiv:2205.08084}, 2022.

\bibitem{liu2021learnable}
S.~Liu, C.~Gao, Y.~Chen, D.~Jin, and Y.~Li, ``Learnable embedding sizes for
  recommender systems,'' \emph{arXiv preprint arXiv:2101.07577}, 2021.

\bibitem{liu2020automated}
H.~Liu, X.~Zhao, C.~Wang, X.~Liu, and J.~Tang, ``Automated embedding size
  search in deep recommender systems,'' in \emph{Proceedings of the 43rd
  International ACM SIGIR Conference on Research and Development in Information
  Retrieval}, 2020, pp. 2307--2316.

\bibitem{deng2021deeplight}
W.~Deng, J.~Pan, T.~Zhou, D.~Kong, A.~Flores, and G.~Lin, ``Deeplight: Deep
  lightweight feature interactions for accelerating ctr predictions in ad
  serving,'' in \emph{Proceedings of the 14th ACM international conference on
  Web search and data mining}, 2021, pp. 922--930.

\bibitem{ginart2021mixed}
A.~A. Ginart, M.~Naumov, D.~Mudigere, J.~Yang, and J.~Zou, ``Mixed dimension
  embeddings with application to memory-efficient recommendation systems,'' in
  \emph{2021 IEEE International Symposium on Information Theory (ISIT)}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 2786--2791.

\bibitem{wang2022autofield}
Y.~Wang, X.~Zhao, T.~Xu, and X.~Wu, ``Autofield: Automating feature selection
  in deep recommender systems,'' in \emph{Proceedings of the ACM Web Conference
  2022}, 2022, pp. 1977--1986.

\bibitem{lin2022adafs}
W.~Lin, X.~Zhao, Y.~Wang, T.~Xu, and X.~Wu, ``Adafs: Adaptive feature selection
  in deep recommender system,'' in \emph{Proceedings of the 28th ACM SIGKDD
  Conference on Knowledge Discovery and Data Mining}, 2022, pp. 3309--3317.

\bibitem{tsang2020feature}
M.~Tsang, D.~Cheng, H.~Liu, X.~Feng, E.~Zhou, and Y.~Liu, ``Feature interaction
  interpretability: A case for explaining ad-recommendation systems via neural
  interaction detection,'' \emph{arXiv preprint arXiv:2006.10966}, 2020.

\bibitem{yuanfei2019autocross}
L.~Yuanfei, W.~Mengshuo, Z.~Hao, Y.~Quanming, T.~WeiWei, C.~Yuqiang, Y.~Qiang,
  and D.~Wenyuan, ``Autocross: Automatic feature crossing for tabular data in
  real-world applications,'' \emph{arXiv preprint arXiv:1904.12857}, 2019.

\bibitem{liu2020autofis}
B.~Liu, C.~Zhu, G.~Li, W.~Zhang, J.~Lai, R.~Tang, X.~He, Z.~Li, and Y.~Yu,
  ``Autofis: Automatic feature interaction selection in factorization models
  for click-through rate prediction,'' in \emph{Proceedings of the 26th ACM
  SIGKDD International Conference on Knowledge Discovery \& Data Mining}, 2020,
  pp. 2636--2645.

\bibitem{liu2020autogroup}
B.~Liu, N.~Xue, H.~Guo, R.~Tang, S.~Zafeiriou, X.~He, and Z.~Li, ``Autogroup:
  Automatic feature grouping for modelling explicit high-order feature
  interactions in ctr prediction,'' in \emph{Proceedings of the 43rd
  international ACM SIGIR conference on research and development in information
  retrieval}, 2020, pp. 199--208.

\bibitem{chen2019bayesian}
Y.~Chen, P.~Ren, Y.~Wang, and M.~de~Rijke, ``Bayesian personalized feature
  interaction selection for factorization machines,'' in \emph{Proceedings of
  the 42nd International ACM SIGIR Conference on Research and Development in
  Information Retrieval}, 2019, pp. 665--674.

\bibitem{xie2021fives}
Y.~Xie, Z.~Wang, Y.~Li, B.~Ding, N.~M. G{\"u}rel, C.~Zhang, M.~Huang, W.~Lin,
  and J.~Zhou, ``Fives: Feature interaction via edge search for large-scale
  tabular data,'' in \emph{Proceedings of the 27th ACM SIGKDD Conference on
  Knowledge Discovery \& Data Mining}, 2021, pp. 3795--3805.

\bibitem{su2021detecting}
Y.~Su, R.~Zhang, S.~Erfani, and Z.~Xu, ``Detecting beneficial feature
  interactions for recommender systems,'' in \emph{Proceedings of the AAAI
  conference on artificial intelligence}, vol.~35, no.~5, 2021, pp. 4357--4365.

\bibitem{song2020autoctr}
Q.~Song, D.~Cheng, H.~Zhou, J.~Yang, Y.~Tian, and X.~Hu, ``Towards automated
  neural interaction discovery for click-through rate prediction,'' in
  \emph{Proceedings of the 26th ACM SIGKDD International Conference on
  Knowledge Discovery \& Data Mining}, 2020, pp. 945--955.

\bibitem{zhao2021ameir}
P.~Zhao, K.~Xiao, Y.~Zhang, K.~Bian, and W.~Yan, ``Ameir: Automatic behavior
  modeling, interaction exploration and mlp investigation in the recommender
  system.'' in \emph{IJCAI}, 2021, pp. 2104--2110.

\bibitem{wei2021autoias}
Z.~Wei, X.~Wang, and W.~Zhu, ``Autoias: Automatic integrated architecture
  searcher for click-trough rate prediction,'' in \emph{Proceedings of the 30th
  ACM International Conference on Information \& Knowledge Management}, 2021,
  pp. 2101--2110.

\bibitem{cheng2022nasr}
M.~Cheng, Z.~Liu, Q.~Liu, S.~Ge, and E.~Chen, ``Towards automatic discovering
  of deep hybrid network architecture for sequential recommendation,'' in
  \emph{Proceedings of the ACM Web Conference 2022}, 2022, pp. 1923--1932.

\bibitem{yu2023gptnas}
C.~Yu, X.~Liu, C.~Tang, W.~Feng, and J.~Lv, ``Gpt-nas: Neural architecture
  search with the generative pre-trained model,'' \emph{arXiv preprint
  arXiv:2305.05351}, 2023.

\bibitem{ying2019bench}
C.~Ying, A.~Klein, E.~Christiansen, E.~Real, K.~Murphy, and F.~Hutter,
  ``Nas-bench-101: Towards reproducible neural architecture search,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 7105--7114.

\bibitem{zheng2023cangpt}
M.~Zheng, X.~Su, S.~You, F.~Wang, C.~Qian, C.~Xu, and S.~Albanie, ``Can gpt-4
  perform neural architecture search?'' \emph{arXiv preprint arXiv:2304.10970},
  2023.

\bibitem{nasir2023llmatic}
M.~U. Nasir, S.~Earle, J.~Togelius, S.~James, and C.~Cleghorn, ``Llmatic:
  Neural architecture search via large language models and quality-diversity
  optimization,'' \emph{arXiv preprint arXiv:2306.01102}, 2023.

\bibitem{chen2023evoprompting}
A.~Chen, D.~M. Dohan, and D.~R. So, ``Evoprompting: Language models for
  code-level neural architecture search,'' \emph{arXiv preprint
  arXiv:2302.14838}, 2023.

\bibitem{shang2015neural}
L.~Shang, Z.~Lu, and H.~Li, ``Neural responding machine for short-text
  conversation,'' \emph{arXiv preprint arXiv:1503.02364}, 2015.

\bibitem{vinyals2015neural}
O.~Vinyals and Q.~Le, ``A neural conversational model,'' \emph{arXiv preprint
  arXiv:1506.05869}, 2015.

\bibitem{sordoni2015neural}
A.~Sordoni, M.~Galley, M.~Auli, C.~Brockett, Y.~Ji, M.~Mitchell, J.-Y. Nie,
  J.~Gao, and B.~Dolan, ``A neural network approach to context-sensitive
  generation of conversational responses,'' \emph{arXiv preprint
  arXiv:1506.06714}, 2015.

\bibitem{wu2019deep}
W.~Wu and R.~Yan, ``Deep chit-chat: Deep learning for chatbots,'' in
  \emph{Proceedings of the 42nd International ACM SIGIR Conference on Research
  and Development in Information Retrieval}, 2019, pp. 1413--1414.

\bibitem{qiu2015convolutional}
X.~Qiu and X.~Huang, ``Convolutional neural tensor network architecture for
  community-based question answering,'' in \emph{Twenty-Fourth international
  joint conference on artificial intelligence}, 2015.

\bibitem{wan2016deep}
S.~Wan, Y.~Lan, J.~Guo, J.~Xu, L.~Pang, and X.~Cheng, ``A deep architecture for
  semantic matching with multiple positional sentence representations,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~30, no.~1, 2016.

\bibitem{sun2018conversational}
Y.~Sun and Y.~Zhang, ``Conversational recommender system,'' in \emph{The 41st
  international acm sigir conference on research \& development in information
  retrieval}, 2018, pp. 235--244.

\bibitem{greco2017converse}
C.~Greco, A.~Suglia, P.~Basile, and G.~Semeraro, ``Converse-et-impera:
  Exploiting deep learning and hierarchical reinforcement learning for
  conversational recommender systems,'' in \emph{AI* IA 2017 Advances in
  Artificial Intelligence: XVIth International Conference of the Italian
  Association for Artificial Intelligence, Bari, Italy, November 14-17, 2017,
  Proceedings 16}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2017, pp.
  372--386.

\bibitem{yao2013recurrent}
K.~Yao, G.~Zweig, M.-Y. Hwang, Y.~Shi, and D.~Yu, ``Recurrent neural networks
  for language understanding.'' in \emph{Interspeech}, 2013, pp. 2524--2528.

\bibitem{mesnil2013investigation}
G.~Mesnil, X.~He, L.~Deng, and Y.~Bengio, ``Investigation of
  recurrent-neural-network architectures and learning methods for spoken
  language understanding.'' in \emph{Interspeech}, 2013, pp. 3771--3775.

\bibitem{goddeau1996form}
D.~Goddeau, H.~Meng, J.~Polifroni, S.~Seneff, and S.~Busayapongchai, ``A
  form-based dialogue manager for spoken language applications,'' in
  \emph{Proceeding of Fourth International Conference on Spoken Language
  Processing. ICSLP'96}, vol.~2.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  1996, pp. 701--704.

\bibitem{henderson2013deep}
M.~Henderson, B.~Thomson, and S.~Young, ``Deep neural network approach for the
  dialog state tracking challenge,'' in \emph{Proceedings of the SIGDIAL 2013
  Conference}, 2013, pp. 467--471.

\bibitem{mrkvsic2016neural}
N.~Mrk{\v{s}}i{\'c}, D.~O. S{\'e}aghdha, T.-H. Wen, B.~Thomson, and S.~Young,
  ``Neural belief tracker: Data-driven dialogue state tracking,'' \emph{arXiv
  preprint arXiv:1606.03777}, 2016.

\bibitem{cuayahuitl2015strategic}
H.~Cuay{\'a}huitl, S.~Keizer, and O.~Lemon, ``Strategic dialogue management via
  deep reinforcement learning,'' \emph{arXiv preprint arXiv:1511.08099}, 2015.

\bibitem{zhou2016context}
H.~Zhou, M.~Huang, and X.~Zhu, ``Context-aware natural language generation for
  spoken dialogue systems,'' in \emph{Proceedings of COLING 2016, the 26th
  International Conference on Computational Linguistics: Technical Papers},
  2016, pp. 2032--2041.

\bibitem{duvsek2016sequence}
O.~Du{\v{s}}ek and F.~Jur{\v{c}}{\'\i}{\v{c}}ek, ``Sequence-to-sequence
  generation for spoken dialogue via deep syntax trees and strings,''
  \emph{arXiv preprint arXiv:1606.05491}, 2016.

\bibitem{wen2016network}
T.-H. Wen, D.~Vandyke, N.~Mrksic, M.~Gasic, L.~M. Rojas-Barahona, P.-H. Su,
  S.~Ultes, and S.~Young, ``A network-based end-to-end trainable task-oriented
  dialogue system,'' \emph{arXiv preprint arXiv:1604.04562}, 2016.

\bibitem{bordes2016learning}
A.~Bordes, Y.-L. Boureau, and J.~Weston, ``Learning end-to-end goal-oriented
  dialog,'' \emph{arXiv preprint arXiv:1605.07683}, 2016.

\bibitem{zhang2019dialogpt}
Y.~Zhang, S.~Sun, M.~Galley, Y.-C. Chen, C.~Brockett, X.~Gao, J.~Gao, J.~Liu,
  and B.~Dolan, ``Dialogpt: Large-scale generative pre-training for
  conversational response generation,'' \emph{arXiv preprint arXiv:1911.00536},
  2019.

\bibitem{lei2020estimation}
W.~Lei, X.~He, Y.~Miao, Q.~Wu, R.~Hong, M.-Y. Kan, and T.-S. Chua,
  ``Estimation-action-reflection: Towards deep interaction between
  conversational and recommender systems,'' in \emph{Proceedings of the 13th
  International Conference on Web Search and Data Mining}, 2020, pp. 304--312.

\bibitem{lei2020interactive}
W.~Lei, G.~Zhang, X.~He, Y.~Miao, X.~Wang, L.~Chen, and T.-S. Chua,
  ``Interactive path reasoning on graph for conversational recommendation,'' in
  \emph{Proceedings of the 26th ACM SIGKDD international conference on
  knowledge discovery \& data mining}, 2020, pp. 2073--2083.

\bibitem{deng2021unified}
Y.~Deng, Y.~Li, F.~Sun, B.~Ding, and W.~Lam, ``Unified conversational
  recommendation policy learning via graph-based reinforcement learning,'' in
  \emph{Proceedings of the 44th International ACM SIGIR Conference on Research
  and Development in Information Retrieval}, 2021, pp. 1431--1441.

\bibitem{li2018towards}
R.~Li, S.~Ebrahimi~Kahou, H.~Schulz, V.~Michalski, L.~Charlin, and C.~Pal,
  ``Towards deep conversational recommendations,'' \emph{Advances in neural
  information processing systems}, vol.~31, 2018.

\bibitem{wang2022barcor}
T.-C. Wang, S.-Y. Su, and Y.-N. Chen, ``Barcor: Towards a unified framework for
  conversational recommendation systems,'' \emph{arXiv preprint
  arXiv:2203.14257}, 2022.

\bibitem{wang2022towards}
X.~Wang, K.~Zhou, J.-R. Wen, and W.~X. Zhao, ``Towards unified conversational
  recommender systems via knowledge-enhanced prompt learning,'' in
  \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining}, 2022, pp. 1929--1937.

\bibitem{wang2022recindial}
L.~Wang, H.~Hu, L.~Sha, C.~Xu, D.~Jiang, and K.-F. Wong, ``Recindial: A unified
  framework for conversational recommendation with pretrained language
  models,'' in \emph{Proceedings of the 2nd Conference of the Asia-Pacific
  Chapter of the Association for Computational Linguistics and the 12th
  International Joint Conference on Natural Language Processing}, 2022, pp.
  489--500.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' \emph{arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever \emph{et~al.}, ``Improving
  language understanding by generative pre-training,'' 2018.

\bibitem{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray \emph{et~al.}, ``Training language models to
  follow instructions with human feedback,'' \emph{Advances in Neural
  Information Processing Systems}, vol.~35, pp. 27\,730--27\,744, 2022.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar \emph{et~al.}, ``Llama: Open
  and efficient foundation language models,'' \emph{arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{gao2023chat}
Y.~Gao, T.~Sheng, Y.~Xiang, Y.~Xiong, H.~Wang, and J.~Zhang, ``Chat-rec:
  Towards interactive and explainable llms-augmented recommender system,''
  \emph{arXiv preprint arXiv:2303.14524}, 2023.

\bibitem{friedman2023leveraging}
L.~Friedman, S.~Ahuja, D.~Allen, T.~Tan, H.~Sidahmed, C.~Long, J.~Xie,
  G.~Schubiner, A.~Patel, H.~Lara \emph{et~al.}, ``Leveraging large language
  models in conversational recommender systems,'' \emph{arXiv preprint
  arXiv:2305.07961}, 2023.

\bibitem{wang2023rethinking}
X.~Wang, X.~Tang, W.~X. Zhao, J.~Wang, and J.-R. Wen, ``Rethinking the
  evaluation for conversational recommendation in the era of large language
  models,'' \emph{arXiv preprint arXiv:2305.13112}, 2023.

\bibitem{yao2023tree}
S.~Yao, D.~Yu, J.~Zhao, I.~Shafran, T.~L. Griffiths, Y.~Cao, and K.~Narasimhan,
  ``Tree of thoughts: Deliberate problem solving with large language models,''
  \emph{arXiv preprint arXiv:2305.10601}, 2023.

\bibitem{wang2023plan}
L.~Wang, W.~Xu, Y.~Lan, Z.~Hu, Y.~Lan, R.~K.-W. Lee, and E.-P. Lim,
  ``Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by
  large language models,'' \emph{arXiv preprint arXiv:2305.04091}, 2023.

\bibitem{yao2022react}
S.~Yao, J.~Zhao, D.~Yu, N.~Du, I.~Shafran, K.~Narasimhan, and Y.~Cao, ``React:
  Synergizing reasoning and acting in language models,'' \emph{arXiv preprint
  arXiv:2210.03629}, 2022.

\bibitem{madaan2022memory}
A.~Madaan, N.~Tandon, P.~Clark, and Y.~Yang, ``Memory-assisted prompt editing
  to improve gpt-3 after deployment,'' \emph{arXiv preprint arXiv:2201.06009},
  2022.

\bibitem{qin2023tool}
Y.~Qin, S.~Hu, Y.~Lin, W.~Chen, N.~Ding, G.~Cui, Z.~Zeng, Y.~Huang, C.~Xiao,
  C.~Han \emph{et~al.}, ``Tool learning with foundation models,'' \emph{arXiv
  preprint arXiv:2304.08354}, 2023.

\bibitem{mialon2023augmented}
G.~Mialon, R.~Dess{\`\i}, M.~Lomeli, C.~Nalmpantis, R.~Pasunuru, R.~Raileanu,
  B.~Rozi{\`e}re, T.~Schick, J.~Dwivedi-Yu, A.~Celikyilmaz \emph{et~al.},
  ``Augmented language models: a survey,'' \emph{arXiv preprint
  arXiv:2302.07842}, 2023.

\bibitem{yang2022re3}
K.~Yang, N.~Peng, Y.~Tian, and D.~Klein, ``Re3: Generating longer stories with
  recursive reprompting and revision,'' \emph{arXiv preprint arXiv:2210.06774},
  2022.

\bibitem{schick2022peer}
T.~Schick, J.~Dwivedi-Yu, Z.~Jiang, F.~Petroni, P.~Lewis, G.~Izacard, Q.~You,
  C.~Nalmpantis, E.~Grave, and S.~Riedel, ``Peer: A collaborative language
  model,'' \emph{arXiv preprint arXiv:2208.11663}, 2022.

\bibitem{hao2022language}
Y.~Hao, H.~Song, L.~Dong, S.~Huang, Z.~Chi, W.~Wang, S.~Ma, and F.~Wei,
  ``Language models are general-purpose interfaces,'' \emph{arXiv preprint
  arXiv:2206.06336}, 2022.

\bibitem{izacard2022few}
G.~Izacard, P.~Lewis, M.~Lomeli, L.~Hosseini, F.~Petroni, T.~Schick,
  J.~Dwivedi-Yu, A.~Joulin, S.~Riedel, and E.~Grave, ``Few-shot learning with
  retrieval augmented language models,'' \emph{arXiv preprint
  arXiv:2208.03299}, 2022.

\bibitem{thoppilan2022lamda}
R.~Thoppilan, D.~De~Freitas, J.~Hall, N.~Shazeer, A.~Kulshreshtha, H.-T. Cheng,
  A.~Jin, T.~Bos, L.~Baker, Y.~Du \emph{et~al.}, ``Lamda: Language models for
  dialog applications,'' \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{nakano2021webgpt}
R.~Nakano, J.~Hilton, S.~Balaji, J.~Wu, L.~Ouyang, C.~Kim, C.~Hesse, S.~Jain,
  V.~Kosaraju, W.~Saunders \emph{et~al.}, ``Webgpt: Browser-assisted
  question-answering with human feedback,'' \emph{arXiv preprint
  arXiv:2112.09332}, 2021.

\bibitem{liu2022mind}
R.~Liu, J.~Wei, S.~S. Gu, T.-Y. Wu, S.~Vosoughi, C.~Cui, D.~Zhou, and A.~M.
  Dai, ``Mind's eye: Grounded language model reasoning through simulation,''
  \emph{arXiv preprint arXiv:2210.05359}, 2022.

\bibitem{gao2022pal}
L.~Gao, A.~Madaan, S.~Zhou, U.~Alon, P.~Liu, Y.~Yang, J.~Callan, and G.~Neubig,
  ``Pal: Program-aided language models,'' \emph{arXiv preprint
  arXiv:2211.10435}, 2022.

\bibitem{ahn2022can}
M.~Ahn, A.~Brohan, N.~Brown, Y.~Chebotar, O.~Cortes, B.~David, C.~Finn, C.~Fu,
  K.~Gopalakrishnan, K.~Hausman \emph{et~al.}, ``Do as i can, not as i say:
  Grounding language in robotic affordances,'' \emph{arXiv preprint
  arXiv:2204.01691}, 2022.

\bibitem{shen2023hugginggpt}
Y.~Shen, K.~Song, X.~Tan, D.~Li, W.~Lu, and Y.~Zhuang, ``Hugginggpt: Solving ai
  tasks with chatgpt and its friends in huggingface,'' \emph{arXiv preprint
  arXiv:2303.17580}, 2023.

\bibitem{wu2023visual}
C.~Wu, S.~Yin, W.~Qi, X.~Wang, Z.~Tang, and N.~Duan, ``Visual chatgpt: Talking,
  drawing and editing with visual foundation models,'' \emph{arXiv preprint
  arXiv:2303.04671}, 2023.

\bibitem{liang2023taskmatrix}
Y.~Liang, C.~Wu, T.~Song, W.~Wu, Y.~Xia, Y.~Liu, Y.~Ou, S.~Lu, L.~Ji, S.~Mao
  \emph{et~al.}, ``Taskmatrix. ai: Completing tasks by connecting foundation
  models with millions of apis,'' \emph{arXiv preprint arXiv:2303.16434}, 2023.

\bibitem{schick2023toolformer}
T.~Schick, J.~Dwivedi-Yu, R.~Dess{\`\i}, R.~Raileanu, M.~Lomeli,
  L.~Zettlemoyer, N.~Cancedda, and T.~Scialom, ``Toolformer: Language models
  can teach themselves to use tools,'' \emph{arXiv preprint arXiv:2302.04761},
  2023.

\bibitem{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi, ``Blip: Bootstrapping language-image
  pre-training for unified vision-language understanding and generation,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2022, pp. 12\,888--12\,900.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer, ``High-resolution
  image synthesis with latent diffusion models,'' in \emph{Proceedings of the
  IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  10\,684--10\,695.

\bibitem{cai2023large}
T.~Cai, X.~Wang, T.~Ma, X.~Chen, and D.~Zhou, ``Large language models as tool
  makers,'' \emph{arXiv preprint arXiv:2305.17126}, 2023.

\bibitem{shuster2022blenderbot}
K.~Shuster, J.~Xu, M.~Komeili, D.~Ju, E.~M. Smith, S.~Roller, M.~Ung, M.~Chen,
  K.~Arora, J.~Lane \emph{et~al.}, ``Blenderbot 3: a deployed conversational
  agent that continually learns to responsibly engage,'' \emph{arXiv preprint
  arXiv:2208.03188}, 2022.

\bibitem{liu2023reta}
J.~Liu, J.~Jin, Z.~Wang, J.~Cheng, Z.~Dou, and J.-R. Wen, ``Reta-llm: A
  retrieval-augmented large language model toolkit,'' \emph{arXiv preprint
  arXiv:2306.05212}, 2023.

\bibitem{pan2023unifying}
S.~Pan, L.~Luo, Y.~Wang, C.~Chen, J.~Wang, and X.~Wu, ``Unifying large language
  models and knowledge graphs: A roadmap,'' \emph{arXiv preprint
  arXiv:2306.08302}, 2023.

\bibitem{vempati2020enabling}
S.~Vempati, K.~T. Malayil, V.~Sruthi, and R.~Sandeep, ``Enabling
  hyper-personalisation: Automated ad creative generation and ranking for
  fashion e-commerce,'' in \emph{Fashion Recommender Systems}.\hskip 1em plus
  0.5em minus 0.4em\relax Springer, 2020, pp. 25--48.

\bibitem{thomaidou2013automated}
S.~Thomaidou, I.~Lourentzou, P.~Katsivelis-Perakis, and M.~Vazirgiannis,
  ``Automated snippet generation for online advertising,'' in \emph{Proceedings
  of the 22nd ACM international conference on Information \& Knowledge
  Management}, 2013, pp. 1841--1844.

\bibitem{zhang2022automatic}
X.~Zhang, Y.~Zou, H.~Zhang, J.~Zhou, S.~Diao, J.~Chen, Z.~Ding, Z.~He, X.~He,
  Y.~Xiao \emph{et~al.}, ``Automatic product copywriting for e-commerce,'' in
  \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  vol.~36, no.~11, 2022, pp. 12\,423--12\,431.

\bibitem{lei2022plato}
Z.~Lei, C.~Zhang, X.~Xu, W.~Wu, Z.-Y. Niu, H.~Wu, H.~Wang, Y.~Yang, and S.~Li,
  ``Plato-ad: A unified advertisement text generation framework with multi-task
  prompt learning,'' in \emph{Proceedings of the 2022 Conference on Empirical
  Methods in Natural Language Processing: Industry Track}, 2022, pp. 512--520.

\bibitem{bartz2008natural}
K.~Bartz, C.~Barr, and A.~Aijaz, ``Natural language generation for
  sponsored-search advertisements,'' in \emph{Proceedings of the 9th ACM
  Conference on Electronic Commerce}, 2008, pp. 1--9.

\bibitem{fujita2010automatic}
A.~Fujita, K.~Ikushima, S.~Sato, R.~Kamite, K.~Ishiyama, and O.~Tamachi,
  ``Automatic generation of listing ads by reusing promotional texts,'' in
  \emph{Proceedings of the 12th international conference on electronic
  commerce: Roadmap for the future of electronic business}, 2010, pp. 179--188.

\bibitem{hughes2019generating}
J.~W. Hughes, K.-h. Chang, and R.~Zhang, ``Generating better search engine text
  advertisements with deep reinforcement learning,'' in \emph{Proceedings of
  the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data
  Mining}, 2019, pp. 2269--2277.

\bibitem{wang2021reinforcing}
X.~Wang, X.~Gu, J.~Cao, Z.~Zhao, Y.~Yan, B.~Middha, and X.~Xie, ``Reinforcing
  pretrained models for generating attractive text advertisements,'' in
  \emph{Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&
  Data Mining}, 2021, pp. 3697--3707.

\bibitem{chen2022personalized}
C.~Chen, X.~Wang, X.~Yi, F.~Wu, X.~Xie, and R.~Yan, ``Personalized chit-chat
  generation for recommendation using external chat corpora,'' in
  \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining}, 2022, pp. 2721--2731.

\bibitem{zhang2021chase}
C.~Zhang, J.~Zhou, X.~Zang, Q.~Xu, L.~Yin, X.~He, L.~Liu, H.~Xiong, and D.~Dou,
  ``Chase: Commonsense-enriched advertising on search engine with explicit
  knowledge,'' in \emph{Proceedings of the 30th ACM International Conference on
  Information \& Knowledge Management}, 2021, pp. 4352--4361.

\bibitem{kanungo2021ad}
Y.~S. Kanungo, S.~Negi, and A.~Rajan, ``Ad headline generation using
  self-critical masked language model,'' in \emph{Proceedings of the 2021
  Conference of the North American Chapter of the Association for Computational
  Linguistics: Human Language Technologies: Industry Papers}, 2021, pp.
  263--271.

\bibitem{wei2022creater}
P.~Wei, X.~Yang, S.~Liu, L.~Wang, and B.~Zheng, ``Creater: Ctr-driven
  advertising text generation with controlled pre-training and contrastive
  fine-tuning,'' \emph{arXiv preprint arXiv:2205.08943}, 2022.

\bibitem{kanungo2022cobart}
Y.~S. Kanungo, G.~Das, and S.~Negi, ``Cobart: Controlled, optimized,
  bidirectional and auto-regressive transformer for ad headline generation,''
  in \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery
  and Data Mining}, 2022, pp. 3127--3136.

\bibitem{chen2019towards}
Q.~Chen, J.~Lin, Y.~Zhang, H.~Yang, J.~Zhou, and J.~Tang, ``Towards
  knowledge-based personalized product description generation in e-commerce,''
  in \emph{Proceedings of the 25th ACM SIGKDD International Conference on
  Knowledge Discovery \& Data Mining}, 2019, pp. 3040--3050.

\bibitem{cao2023comprehensive}
Y.~Cao, S.~Li, Y.~Liu, Z.~Yan, Y.~Dai, P.~S. Yu, and L.~Sun, ``A comprehensive
  survey of ai-generated content (aigc): A history of generative ai from gan to
  chatgpt,'' \emph{arXiv preprint arXiv:2303.04226}, 2023.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio, ``Generative adversarial nets,'' \emph{Advances
  in Neural Information Processing Systems}, vol.~27, 2014.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{dinh2014nice}
L.~Dinh, D.~Krueger, and Y.~Bengio, ``Nice: Non-linear independent components
  estimation,'' \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel, ``Denoising diffusion probabilistic models,''
  \emph{Advances in Neural Information Processing Systems}, vol.~33, pp.
  6840--6851, 2020.

\bibitem{kenton2019bert}
J.~D. M.-W.~C. Kenton and L.~K. Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' in \emph{Proceedings
  of NAACL-HLT}, 2019, pp. 4171--4186.

\bibitem{dosovitskiyimage}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly \emph{et~al.},
  ``An image is worth 16x16 words: Transformers for image recognition at
  scale,'' in \emph{International Conference on Learning Representations}.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo, ``Swin
  transformer: Hierarchical vision transformer using shifted windows,'' in
  \emph{Proceedings of the IEEE/CVF international conference on computer
  vision}, 2021, pp. 10\,012--10\,022.

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual
  models from natural language supervision,'' in \emph{International conference
  on machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2021, pp.
  8748--8763.

\bibitem{chenadaspeech}
M.~Chen, X.~Tan, B.~Li, Y.~Liu, T.~Qin, T.-Y. Liu \emph{et~al.}, ``Adaspeech:
  Adaptive text to speech for custom voice,'' in \emph{International Conference
  on Learning Representations}.

\bibitem{li2016commonsense}
X.~Li, A.~Taheri, L.~Tu, and K.~Gimpel, ``Commonsense knowledge base
  completion,'' in \emph{Proceedings of the 54th Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers)}, 2016, pp.
  1445--1455.

\bibitem{feng2020codebert}
Z.~Feng, D.~Guo, D.~Tang, N.~Duan, X.~Feng, M.~Gong, L.~Shou, B.~Qin, T.~Liu,
  D.~Jiang \emph{et~al.}, ``Codebert: A pre-trained model for programming and
  natural languages,'' in \emph{Findings of the Association for Computational
  Linguistics: EMNLP 2020}, 2020, pp. 1536--1547.

\bibitem{chatgpt}
{OpenAI}, ``{ChatGPT: A Large-Scale Generative Model for Conversation},''
  \emph{OpenAI Blog}, 2020.

\bibitem{ramesh2021zero}
A.~Ramesh, M.~Pavlov, G.~Goh, S.~Gray, C.~Voss, A.~Radford, M.~Chen, and
  I.~Sutskever, ``Zero-shot text-to-image generation,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2021, pp. 8821--8831.

\bibitem{chen2021evaluating}
M.~Chen, J.~Tworek, H.~Jun, Q.~Yuan, H.~P. d.~O. Pinto, J.~Kaplan, H.~Edwards,
  Y.~Burda, N.~Joseph, G.~Brockman \emph{et~al.}, ``Evaluating large language
  models trained on code,'' \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{midjourney}
{Midjourney}, ``{Midjourney},'' Retrieved from \url{https://midjourney.com},
  2022.

\bibitem{wang2023generative}
W.~Wang, X.~Lin, F.~Feng, X.~He, and T.-S. Chua, ``Generative recommendation:
  Towards next-generation recommender paradigm,'' \emph{arXiv preprint
  arXiv:2304.03516}, 2023.

\bibitem{borji2023categorical}
A.~Borji, ``A categorical archive of chatgpt failures,'' \emph{arXiv preprint
  arXiv:2302.03494}, 2023.

\bibitem{carlini2021extracting}
N.~Carlini, F.~Tramer, E.~Wallace \emph{et~al.}, ``Extracting training data
  from large language models.'' 2021.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever \emph{et~al.},
  ``Language models are unsupervised multitask learners.''

\bibitem{hu2021lora}
E.~J. Hu, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, W.~Chen
  \emph{et~al.}, ``Lora: Low-rank adaptation of large language models,'' in
  \emph{International Conference on Learning Representations}, 2021.

\bibitem{dettmers2023qlora}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer, ``Qlora: Efficient
  finetuning of quantized llms,'' \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem{bai2022constitutional}
Y.~Bai, S.~Kadavath, S.~Kundu, A.~Askell, J.~Kernion, A.~Jones, A.~Chen,
  A.~Goldie, A.~Mirhoseini, C.~McKinnon \emph{et~al.}, ``Constitutional ai:
  Harmlessness from ai feedback,'' \emph{arXiv preprint arXiv:2212.08073},
  2022.

\end{thebibliography}
