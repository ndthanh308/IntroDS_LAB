% Written by Xingmei
Automated Machine Learning (AutoML) is widely applied in recommender systems to eliminate the costly manual setup with trials and errors. The search space in recommender systems can be categorized in (1) Embedding size (2) Feature (3) Feature interaction (4) Model architecture. Embedding size search, such as~\cite{liu2021learnable,liu2020automated,deng2021deeplight,ginart2021mixed} seeks for appropriate embedding size for each feature to avoid resources overconsumption. Searching for features consisting of raw feature search\cite{wang2022autofield,lin2022adafs} and synthetic feature search\cite{tsang2020feature,yuanfei2019autocross}, which selects a subset from the set of original or cross features to maintain informative features to reduce both computation and space cost. Feature interaction search, such as~\cite{liu2020autofis,liu2020autogroup, chen2019bayesian,xie2021fives,su2021detecting}, automatically filters out feature interactions that are not helpful. Model architecture search, like~\cite{song2020autoctr,zhao2021ameir,wei2021autoias,cheng2022nasr}, expands the search space to the integral architectures. The search strategy shifts from the discrete reinforcement learning process, which iteratively samples architectures for training and is time-consuming, into the differentiable searching, which adaptively selects architectures within one-shot learning to circumvent the computational burden, for more efficient convergence. The evaluation for each sampled architecture then acts as the signal to adjust the selections. That is, there is a decision maker who memorizes the prior results of previous architecture choices and analyzes the prior results to give the next recommended choice. 



The emergent LLMs actually have excellent memorization and reasoning capability that would work for automated learning. Several works have attempted to validate the potential of automated machine learning with LLMs. Preliminarily, GPT-NAS~\cite{yu2023gptnas} takes advantage of generative capability of LLMs. The architecture of networks are formulated into sequential characters, and thus the generation of network architectures can be easily achieved through the generative pre-training models. NAS-Bench-101~\cite{ying2019bench} is utilized for pre-training and the state-of-the-art results are used for fine-tuning. The generative pre-training models produce reasonable architectures, which would reduce the search space for later genetic algorithms for searching optimal architectures. The relatively advanced reasoning ability is further evaluated in GENIUS~\cite{zheng2023cangpt}, where GPT-4 is employed as a black-box agent to generate potential better-performing architectures according to previous trials including tried architectures with their evaluation performance. According to the results, GPT-4 can generate good architecture networks, showing the potential for more complicated tasks. Yet it is too difficult for LLMs to directly make decisions on challenging technical problems only by prompting. To balance efficiency and interpretability, one approach is to integrate the LLMs into certain search strategies, where the genetic algorithm guides the search process and LLMs generate the candidate crossovers. LLMatic~\cite{nasir2023llmatic} and EvoPrompting~\cite{chen2023evoprompting} use code-LLMs as mutation and crossover operators for a genetic NAS algorithm. During evolution, each generation has a certain probability of deciding whether to perform crossover or mutation to produce new offspring. Crossover and mutation are generated by prompting LLMs. Such a solution integrates LLM into the genetic search algorithm, which would achieve better performances than direct reasoning.
% over the whole space. 



The research mentioned above brings valuable insights to the field of automated learning in recommender systems. However, there are several challenges that need to be addressed. Firstly, the search space in recommender systems is considerably more complex, encompassing diverse types of search space and facing significant volume issues. This complexity poses a challenge in effectively exploring and optimizing the search space.
Secondly, compared to the common architecture search in other domains, recommender systems lack a strong foundation of knowledge regarding the informative components within the search space, especially the effective high-order feature interactions. Unlike well-established network structures in other areas, recommender systems operate in various domains and scenarios, resulting in diverse and domain-specific components. 
Addressing these challenges and advancing the understanding of the search space and informative components in recommender systems will pave the way for significant improvements in automated learning approaches.



