% update by Xu Huang (2023/06/27)
% ============ Version 2 Start (update on 2023/06/27)========================
Conversational recommender system (CRS) is a specialized type of recommendation tool that aims to uncover users' interests and preferences through dialogue, enabling personalized recommendations and real-time adjustment of recommendation strategies based on user feedback. Compared to traditional recommender systems, conversational recommender systems have the advantage of real-time understanding of user intents and the ability to adapt recommendations based on user feedback. Typically, a conversational recommender system consists of two main components: a dialogue module and a recommendation module. 
In this section, we will primarily focus on discussing the dialogue module, which plays a crucial role in facilitating effective user-system interactions and understanding user preferences.

In a conversational recommender system, the dialogue module typically takes the form of a dialogue system. Dialogue systems can generally be classified into two main categories: chit-chat and task-oriented. The former focuses on open-domain question answering, and two major methods are commonly employed: generative and retrieval-based methods. Generative methods~\cite{shang2015neural,vinyals2015neural,sordoni2015neural} utilize a sequence-to-sequence model structure to generate responses, while retrieval-based methods~\cite{wu2019deep,qiu2015convolutional,wan2016deep} transform the task of generating responses into a retrieval problem by searching for the most relevant response in a response database based on the dialogue context. In conversational recommender systems, task-oriented dialogue systems are more often required, as they are specifically designed to assist users in accomplishing specific tasks.
For task-oriented dialogue systems, a common approach~\cite{sun2018conversational,greco2017converse} is to treat the response generation as a pipeline and handle it separately using four components: dialogue understanding~\cite{yao2013recurrent,mesnil2013investigation}, dialogue state tracking~\cite{goddeau1996form,henderson2013deep,mrkvsic2016neural}, dialogue policy learning~\cite{cuayahuitl2015strategic,sun2018conversational}, and natural language generation~\cite{zhou2016context,duvsek2016sequence}. Another approach is to employ an end-to-end method~\cite{wen2016network,bordes2016learning,zhang2019dialogpt}, training an encoder-decoder model to handle all the processing steps collectively. The first approach suffers from scalability issues and lacks synergy between the components, while the second approach requires a substantial amount of supervised data for training.


Based on the classification of dialogue systems, common approaches in conversational recommender systems can also be divided into two categories: attribute-based QA (question-answering) and generative methods. 
The attribute-based QA approach~\cite{sun2018conversational,lei2020estimation,lei2020interactive,deng2021unified} utilizes a pipeline method within the dialogue system. In each dialogue turn, the system needs to decide whether to ask the user a question or provide a recommendation. The decision-making process, particularly regarding which attribute to ask about, is typically handled by a policy network. On the other hand, generative methods do not explicitly model the decision-making process. Instead, they often employ an end-to-end training approach, where a sequence-to-sequence model generates output directly from a shared vocabulary of words and items. Whether the generated output is chit-chat, a question, or a recommendation is implicitly determined during the generation process.
Compared to attribute-based QA methods, generative methods~\cite{li2018towards,wang2022barcor,wang2022towards,wang2022recindial} appear to be simpler and more scalable. However, they require a large amount of supervised training data. With the advancement of pre-trained language models (PLMs) in the field of natural language processing, particularly models like BERT~\cite{devlin2018bert} and GPT~\cite{radford2018improving}, the capabilities of pre-trained models in language understanding and generation have become increasingly powerful. Researchers have found that fine-tuning pre-trained models with a small amount of supervised data can yield impressive results on specific tasks.
This discovery has led to the application of PLMs in generative conversational recommender systems. For example, DialoGPT~\cite{zhang2019dialogpt} achieved promising dialogue intelligence by fine-tuning GPT-2 on dialogue data collected from platforms like Reddit. Subsequently, BARCOR~\cite{wang2022barcor}, RecInDial~\cite{wang2022recindial}, and UniCRS~\cite{wang2022towards} utilized DialoGPT for constructing conversational recommender systems, with variations in their action decision strategies. While PLMs reduce the dependency of generative dialogue models on extensive data, the fine-tuning process still incurs significant computational time and requires the collection of high-quality domain-specific training data due to the large parameter space of the models.

With the increase in model parameters and training data, the intelligence and knowledge capacity of models continues to improve. OpenAI has been expanding the model parameters and training data while employing techniques such as RLHF (Reinforcement Learning from Human Feedback) and Instruction Tuning to further fine-tune GPT-3~\cite{brown2020language}. This has led to the emergent abilities of models like InstructGPT~\cite{ouyang2022training} and subsequent models like ChatGPT, which exhibit incredible intelligence and have opened the doors to new intelligent dialogue systems based on large language models (LLMs). Furthermore, Google's BARD and META's LLaMA~\cite{touvron2023llama} are also large language dialogue models that have been proposed and demonstrated remarkable performance in conversational abilities. The Vicuna model, for instance, utilizes dialogue corpora shared by users in using ChatGPT to fine-tune the open-source LLaMA model, with the team claiming it can achieve over 90\% of ChatGPT's capability. This series of successive LLM introductions has brought new insights to conversational recommender systems.
Due to the utilization of extensive open-domain corpora during LLM training, it possesses inherent conversational recommendation capabilities and can provide reasonable recommendations in open domains such as movies, music, and games. 

However, there are still significant \textbf{challenges} in building an enterprise-level CRS. 
The first challenge is the lack of awareness of large models about private domain data. It is well known that most of the training data for LLMs, such as GPT-3, comes from publicly available sources on the internet. As a result, these models may lack visibility into the data that resides within information platforms, making their modeling and understanding capabilities of such data relatively poor. To address this challenge, there are currently two approaches being explored: fine-tuning~\cite{zhang2019dialogpt} and tool learning~\cite{gao2023chat,friedman2023leveraging}. Fine-tuning involves tuning LLM using private domain-specific dialogue data. There are two major concerns in the approach. First, massive high-quality domain-specific dialogue data is required to tune the extremely large model. However, in most recommendation scenarios, data primarily consists of explicit or implicit user-item interactions, which may lack conversational context. Therefore, generating high-quality dialogue data from interaction data is a key concern in the approach. In RecLLM~\cite{friedman2023leveraging} and iEvaLM~\cite{wang2023rethinking}, researchers have proposed using LLMs to construct a user simulator for generating conversational data. Besides, the fine-tuning technique plays a crucial role in determining the ultimate quality of LLMs. A well-designed and effective fine-tuning strategy can lead to significant improvements in the model's performance and capabilities, such as instruction tuning and RLHF proposed in InstructGPT~\cite{brown2020language}. 
Tool learning is another approach to address this challenge, and its main idea is to treat traditional recommendation models as tools to be utilized, such as Matrix Factorization (MF) and DeepFM. For a more detailed explanation of tool learning, please refer to Section~\ref{tool_learning}. Since recommendation models are domain-specific, LLM can leverage these models to obtain recommendation results and recommend them to the users in the response. In this approach, there are two main technical points: the construction of the tool model and the engineering of prompts to guide the LLM in the proper utilization of the tool. First of all, conventional recommendation models generally use id or categorical features as input, while users always give their requirements or preferences in natural language in conversations. Therefore, unstructured text features should be taken into consideration in tool construction. In Chat-Rec~\cite{gao2023chat}, a conventional recommendation model and a text embedding-based model(text-embedding-ada-002) are used as tools. RecLLM~\cite{friedman2023leveraging} adapted a language model enhanced dual-encoder model and several text retrieval methods as the recommendation engine. On the other hand, despite the strong intelligence and reasoning capabilities of LLMs, effectively harnessing these abilities requires well-crafted prompts for guidance. For instance, the Chain of Thought proposed by Jason~\cite{wei2022chain} could trigger LLM to reason and engage in step-by-step thinking, which benefits the tool-using capability. Subsequent studies like ToT~\cite{yao2023tree}, Plan-and-Solve~\cite{wang2023plan} and ReAct~\cite{yao2022react} have proposed more advanced techniques for prompt design to assist in guiding LLM to engage in deeper thinking and tool planning. 

The second challenge lies in the issue of memory and comprehension in long conversations. Due to the input constraints of LLMs, models like ChatGPT can support a maximum of 4096 tokens in a single call, including both input and output. In multi-turn dialogue scenarios, longer dialogue contexts often meet the risk of exceeding this token limit. The simplest approach to tackle this challenge is to trim the dialogue by discarding earlier turns. However, in conversational recommender systems, users may express a significant amount of personal information and interests in the early stages of the conversation. The omission of such information directly impacts the accuracy of recommendations. To address this issue, several relevant works have proposed solutions. MemPrompt~\cite{madaan2022memory} enhances the prompt by incorporating a memory module, enabling GPT-3 to possess stronger long-dialogue memory capability. Similarly, RecLLM~\cite{friedman2023leveraging} leverages LLM to extract user profiles and store them as factual statements in user memory. When processing user queries, relevant facts are retrieved based on text similarity.  
% ============ Version 2 End ========================