Content-based recommenders provide an effective solution for mitigating the sparse feedback issue in recommender systems. By leveraging the attributes and characteristics of items, these systems achieve a more profound understanding of their properties, facilitating accurate matching with user preferences. However, the content features used in content-based recommendation may also exhibit sparsity. Relying solely on the recommended supervision signal, such as clicking and browsing, might not fully exploit the potential benefits of these features. To overcome this challenge, language models emerge as powerful fundamental algorithms that act as content interpreters in processing textual features. Their utilization enhances the effectiveness of recommender systems by effectively understanding and interpreting textual content, leading to improved recommendations. 

% Figure environment removed


\subsection{Conventional Content Interpreter}
Conventional content interpreter includes statistical model, neural network, and advanced NLP network, as summarized in Figure~\ref{fig:content_interpreter}. These approaches primarily focus on transforming content information, such as textual data, into feature embeddings to facilitate the recommendation process.

Statistical models like TF-IDF, Minimum Description Length (MDL)~\cite{lang1995newsweeder}, and bag-of-words have been traditionally used to encode textual data such as news articles and documents into continuous value vectors. However, with the advancement of deep learning techniques, researchers have explored various neural network architectures to learn more expressive content representations. Instead of relying solely on statistical embeddings, some approaches initialize the vectors with bag-of-words representations and then employ autoencoder-based models to learn more powerful representations.
For example, CDL~\cite{wang2015collaborative} combines the latent vectors obtained from autoencoders with the original ID embeddings to enhance content representations. CRAE~\cite{wang2016collaborative} introduces a collaborative recurrent autoencoder that captures the word order in texts, enabling the modeling of content sequences in collaborative filtering scenarios. Dong et al. ~\cite{dong2017hybrid} propose a stacked denoising autoencoder that reconstructs item/user ratings and textual information simultaneously, allowing for the joint modeling of collaborative and textual knowledge.
CVAE~\cite{li2017collaborative} introduces a collaborative variational autoencoder that learns probabilistic textual features. While autoencoders are effective in learning low-dimensional representations from text data, they may struggle to capture semantic information effectively~\cite{WuWHX23survey}. In some cases, approaches like doc2vec~\cite{LeM14doc2vec} are used to construct content embeddings~\cite{SongEH16mlp, KumarKG17mlp} and learn hidden representations. Okura et al.~\cite{okura2017embedding} evaluate different network architectures, including word-models and GRU networks, for representing user states.
  
Following the advancements in neural natural language processing (NLP) models, more sophisticated architectures such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Neural Attention models have been employed as content interpreters to extract contextual information and capture user preferences. These models take sentence inputs, such as news titles, reviews, or comments, and transform them into word embedding matrices using random initialization or word2vec embeddings~\cite{Mikolov2013word2vec}. Various architectures, including CNNs, attention networks, and hybrid models, are utilized to learn representations of sentences.
For example, NPA~\cite{wu2019cnnattn} and LSTUR~\cite{An19cnnrnn} incorporate attention mechanisms to determine the importance of words after CNN layers. NRMS~\cite{wu19selfattn} and CPRS~\cite{wu20selfattn} utilize multi-head self-attention networks to learn word representations. These models are effective in capturing long-context dependencies and understanding the semantic information in the text.
In addition to text modeling, language models are also used as content interpreters to capture user interests based on their historical interactions. For instance, WE3CN~\cite{khattar2018cnn} employs a 3D CNN to extract temporal features from the historical data. DKN~\cite{wang2018dkn} utilizes an attention mechanism to aggregate historical information related to candidate items. DAN~\cite{Zhu19DAN} proposes an attention-based LSTM to capture richer hidden sequence features. These models leverage different neural architectures to enhance the representation of text in the context of recommendation systems.
It is worth noting that these models still have limitations in terms of depth and the ability to effectively generalize semantic information. 







\subsection{Language Model based Content Interpreter}
In recent years, there has been a growing interest in incorporating more powerful pre-trained language models, such as BERT and GPT, into recommendation systems. These language models have shown exceptional performance in various natural language processing tasks and have sparked researchers' inspiration to leverage them for capturing deep semantic representations and incorporating world knowledge in recommendation systems.
However, applying pre-trained language models to recommendation tasks presents two main challenges. Firstly, there is a misalignment of goals between general-purpose language models and the specific objectives of recommendation systems. To address this, researchers have proposed approaches that fine-tune the pre-trained models or design task-specific pre-training tasks to adapt them to recommendation tasks. For example, U-BERT~\cite{qiu2021u} employs BERT as a content interpreter and introduces masked opinion token prediction and opinion rating prediction as pre-training tasks to better align BERT with recommendation objectives. Similarly, other works~\cite{zhang2021unbert, wu2021empowering, liu2022boosting, wu2022mm, yu2022tiny} have utilized pre-trained BERT to initialize the news encoder for news recommendation, enhancing the representation of textual features. The pre-trained model, ERNIE, is also utilized to enhance the representation ability of queries and documents~\cite{zou2021pre, liu2021pre}.
The second challenge is reducing the online inference latency caused by pre-trained language models, which can be computationally expensive. Researchers have explored techniques such as knowledge distillation and model optimization to obtain lightweight and efficient models suitable for online services. For instance, CTR-BERT~\cite{muhamed2021ctr} employs knowledge distillation to obtain a cache-friendly model for click-through rate prediction, addressing the latency issue.

Moreover, pre-trained language models have been applied beyond mainstream recommendation tasks. They have been integrated into various recommendation scenarios, including tag recommendation~\cite{he2022ptm4tag}, tweet representations~\cite{zhang2022twhin}, and code example recommendation~\cite{rahmani2023improving}, to enhance the representation of textual features in those specific domains. Additionally, some recent works~\cite{ding2021zero, hou2022towards, hou2023learning, yuan2023go} have explored using only textual features as inputs to recommendation models, leveraging pre-trained language models to alleviate cold-start problems and enable cross-domain recommendations. This paradigm offers advantages in alleviating cold-start problems and facilitating cross-domain recommendations based on the universality of natural language. ZESREC~\cite{ding2021zero} uses BERT to obtain universal continuous representations of item descriptions for zero-shot recommendation. Unisrec~\cite{hou2022towards} focuses on cross-domain sequential recommendation and employs a lightweight MoE-enhanced module to incorporate the fixed BERT representation into the recommendation task. VQ-Rec~\cite{hou2023learning} further aligns the textual embeddings produced by pre-trained language models to the recommendation task with the help of vector quantization. Fu et al.~\cite{fu2023exploring} explore layerwise adaptor tuning to achieve parameter-efficient transferable recommendations.


\begin{table*}[th]
\newcommand{\tabincell}[2]{
\begin{tabular}{@{}#1@{}}#2\end{tabular}
}
    \centering
    \caption{LLMs for Content Interpreter}
    \label{tab:my_label}

    \resizebox{1.\textwidth}{!}{
\begin{tabular}{c|c|c|c|c}
\toprule
Approach & Task & LLM backbone  & Tuning Strategy  & Datasets    \\ \midrule
TALLRe~\cite{bao2023tallrec} & Sequential Recommendation &  LLaMA-7B & Instruct Tuning \& Fine Tuning & MovieLens100k, BookCrossing \\ \midrule
LLMs-Rec~\cite{kang2023llms}& Rating Prediction & Flan-T5-Base, Flan-T5-XXL & Fine Tuning & MovieLens-1M, Amazon Book \\ \midrule
PALR~\cite{chen2023palr} & Item Recommendation & LLaMa-7B &  Instruction Tuning & MovieLens-1M, Amazon Beauty \\ \midrule
InstructRec~\cite{zhang2023recommendation} &  \tabincell{c}{sequential recommendation \\ personalized search} &Flan-T5-XL & Instruction Tuning & Amazon-Games, CDs \\ \bottomrule
\end{tabular}
}
\end{table*}

While the pre-trained language models empower the text understanding with the benefit of capturing world knowledge first, the development of pre-trained large language model provides great emergency ability in the fields of reasoning and generalization. 
TALLRe~\cite{bao2023tallrec} explores the ability of large language models for the  sequential recommendation. They observe that original language models perform poorly in zero-shot and few-shot scenarios, while recommendation-specific instruction-tuned language models demonstrate superior performance in few-shot learning and cross-domain generalization. Similarly, Kang et al.~\cite{kang2023llms} propose a similar instruction tuning method for rating prediction recommendation tasks based on the T5 backbone. They find that the tuned language models, which leverage data efficiency, outperform traditional recommenders. PALR~\cite{chen2023palr} further enhances the construction pipeline of recommendation-specific instruction tuning, which first employs large language models to generate reasoning as additional features based on the user's behavior history. Next, a small set of candidates is retrieved using any existing model based on the user profile. Finally, to adapt general-purpose language models to the recommendation task, they convert the generated reasoning features, user interaction history, and retrieved candidates into natural language instruction data and fine-tune a language model.
Existing instruction tuning methods of language models for recommendation scenarios typically focus on a single type of recommendation task, limiting the full utilization of language models' strong generalization ability. InstructRec~\cite{zhang2023recommendation} addresses this limitation by formulating recommendation as an instruction-following procedure. They design various instruction templates to accommodate different recommendation tasks and employ GPT-3.5 to generate high-quality instruction data based on the user's historical data and templates. The language models fine-tuned using this instruction data can effectively handle a wide range of recommendation tasks and cater to users' diverse information requirements.
