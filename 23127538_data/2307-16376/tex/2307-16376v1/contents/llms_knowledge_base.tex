% Rewritten by Jin Chen

% 0622

Knowledge base provides rich information with semantics, attracting increasing attention for the usage of the knowledge base in the recommender systems. Particularly, the knowledge graphs, where nodes represent entities and edges represent relations in the heterogeneous information graph, are the common format of knowledge bases and introduced as side information to enhance the performance of recommenders. Knowledge graphs help understand the mutual relations between users and items and also provides better explainability for recommenders. Existing methods that incorporate knowledge graphs in recommender systems can be classified into three main groups: embedding-based methods, path-based methods and the unified methods. Embedding-based methods, such as CKE~\cite{zhang2016collaborative} and DKN~\cite{wang2018dkn}, KSR~\cite{huang2018improving}, SHINE~\cite{wang2018shine}, utilize semantic representations of users and items. These methods aim to capture the underlying semantic relationships between entities in the knowledge graph, which can improve the quality of recommendations. Path-based approaches, such as Hete-MF~\cite{yu2013collaborative}, SemRec~\cite{shi2015semantic}, RuleRec~\cite{ma2019jointly}, EIUM~\cite{huang2019explainable},exploit the semantic connectivity information present in the knowledge graph to regularize the user and item representations. These methods consider the paths between users and items in the graph and leverage them to incorporate interpretability into the recommendation process. Unified methods, such as RippleNet~\cite{wang2018ripplenet}, KGCN~\cite{wang2019knowledge}, KGAT~\cite{wang2019kgat}, AKUPM~\cite{tang2019akupm}, IntentGC~\cite{zhao2019intentgc} refine the representations of entities in the knowledge graph by leveraging embedding propagation techniques. These methods propagate the embeddings of entities through the graph structure, allowing information to flow across connected entities and refining the representations accordingly.


However, the knowledge graphs adopted in recommender systems is limited and with low usability. Reviewing the various knowledge graph datasets for recommender systems, covering the domains of movie, book, news, product, etc., these datasets are still significantly sparse compared to the vast amount of human knowledge, particularly the lack of facts, due to the expensive supervision to construct the knowledge graph. Building a comprehensive and accurate knowledge graph would be a complex and resource-intensive task, which would include data collection, integration, and cleaning to assure data quality and consistency. Limited by the expensive cost of labelling the knowledge graphs, there would usually exist missing entities or relations. The user preferences for these entities or paths may be ignored, and the recommendation performance suffers. 


The ability of Large Language Models to retrieve factual knowledge as explicit knowledge bases~\cite{petroni2019language,roberts2020much,petronicontext,jiang2020can,wang2020language,poerner2020bert,petronicontext,jiang2020can,heinzerling2021language,wang2021can,guu2020retrieval} has been stirred discussed, which presents an opportunity to construct more comprehensive knowledge graphs within recommender systems. Tracing back to the work~\cite{petroni2019language}, large language models have shown their impressive power in storing factual information, such as entities and common-sense, and then commonsense knowledge can be reliably transferred to downtown tasks. 
\textbf{Existing methods in knowledge graphs fall short of handling incomplete KGs~\cite{bordes2013translating} and constructing KGs with text corpus~\cite{zhu2023llms}} and many researchers attempt to leverage the power of LLM to solve the two tasks, i.e., the knowledge completion~\cite{zhang2020pretrain} and knowledge construction~\cite{kumar2020building}. \textbf{For knowledge graph completion}, which refers to the task of missing facts in the given knowledge graph, recent efforts have been paid to encode text or generate facts for knowledge graphs. MTL-KGC~\cite{kim2020multi} encoders the text sequences to predict the possibility of the tuples. MEMKGC~\cite{choi2021mem} predicts the masked entities of the triple. StAR~\cite{wang2021structure} utilizes Siamese textual encoders to separately encode the entities. GenKGC~\cite{xie2022discrimination} uses the decoder-only language models to directly generate the tail entity. TagReal~\cite{jiang2023text} generates high-quality prompts from the external text corpora. AutoKG~\cite{zhu2023llms} directly adopts the LLMs, such as ChatGPT and GPT-4, and design tailored prompts to predict the tail entity.
As for the another important task, i.e., \textbf{knowledge graph construction}, which refers to creating a structured representation of knowledge, LLMs can be applied in the process of constructing knowledge graphs, including entity discovery~\cite{yan2021unified,li2022ultra}, coreference resolution~\cite{kirstain2021coreference,cattan2021cross} and relation extraction~\cite{lyu2021relation,wang2019fine}. LLMs can also achieve the end-to-end construction~\cite{han2023pive,kumar2020building,wang2020language,trajanoska2023enhancing,jiang2023text} to directly build KGs from raw text. 
LLMs enables the knowledge distillation to construct knowledge graphs. symbolic-kg~\cite{west2022symbolic} distills commonsense facts from GPT3 and then finetune the small student model to generate knowledge graphs.
These models have demonstrated the capacity to store large volumes of knowledge, providing a viable option for improving the scope and depth of knowledge graphs. Furthermore, these advancements have prompted research into the direct transfer of stored knowledge from LLMs to knowledge graphs, eliminating the need for human supervision. This interesting research throws light on the possibilities of automating knowledge graph completion utilizing cutting-edge big language models.


By leveraging the capabilities of LLMs, recommender systems would benefit from a more extensive and up-to-date knowledge base. Firstly, missing faculty information can be completed to construct more extensive knowledge graphs and thus the relations between entities can be extracted for better recommenders. Secondly, in contrast to the preceding exclusively in-domain data, the large language model itself contains plenty of cross-domain information that can help achieve cross-domain recommendations, such as recommending appropriate movies based on the user's favorite music songs. To sum up, the stored knowledge can be utilized to enhance recommendation accuracy, relevance, and personalization, ultimately improving the overall performance of recommender systems. Existing work~\cite{xi2023towards} prompts the large language models to generate the factual knowledge about movies to enhance the performance of CTR prediction models. To better utilize the factual knowledge, a \textit{Knowledge Adaptation} module is adopted for better contextual information extraction.  

It is worth noting that the \textbf{phantom} problem of large language models can be a challenge when applied to recommendation tasks. The inherent nature of large language models can introduce ambiguity or inaccurate provenance~\cite{razniewski2021language}. This issue can emerge as the introduction of extraneous information or even noise into the recommendation process. The large language models may generate responses that, while syntactically correct, lack informative context or relevance. 
According to the KoLA~\cite{yu2023kola}, a benchmark for evaluating word knowledge of LLMs,  even the top-ranked GPT4 just achieves 0.012 in Precision and 0.013 in Recall on the task \textit{Named Entity Recognition}, which falls far short of the performance (0.712 in Precision and 0.706 in Recall) of the task specific models PL-Marker~\cite{ye2022packed}. Such a finding suggests that common sense is still far from being sufficiently captured by LLM.
By aggregating the results with irrelevant or deceptive information, this can damage the usefulness of the recommendation system.