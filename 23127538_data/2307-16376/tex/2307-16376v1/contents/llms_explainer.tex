% Written by Chenwang
In addition to valuing the suggestions made by a recommendation model, users are also interested in the comprehensible justifications for these recommendations~\cite{wang2018reinforcement,gao2019explainable}. This is crucial as most recommender systems are black boxes whose inner workings are inscrutable to human understanding~\cite{leeself}, diminishing user trust. Taking drug recommendations, for instance, it is unacceptable to recommend drugs with good curative effects simply but fail to give reasons why they are effective. To this end, explainable recommendations aim to couple high-quality suggestions with accessible explanations. This not only helps to improve the model's transparency, persuasiveness, and reliability, but also facilitates the identification and rectification of potential errors through insightful explanations. These benefits have been extensively documented in recent work~\cite{nye2021show,lampinen2022can,wei2022chain,zelikman2022star}. For instance, ~\cite{lampinen2022can} conducted a study that involved addressing 40 difficult tasks and evaluating the impact of explanations on zero-shot and few-shot scenarios. Their findings demonstrated that explanations have a positive effect on model performance by establishing a connection between examples and interpretation.

Traditional approaches mainly focus on template-based explanations, which can be broadly categorized into item-based, user-based, and attribute-based explanations\cite{zhang2020explainable}. Item-based explainable methods relate recommendations to familiar items~\cite{schafer1999recommender}, explaining that \textit{the recommended item bears similarity to others the user prefers}, which are prevalent on platforms like Amazon~\cite{linden2003amazon} and Netflix~\cite{gomez2015netflix}. However, due to its collaboration, it may underperform in personalized recommendations requiring diversity and can struggle to identify relevant items among industrial settings with vast items efficiently. In contrast, user-based explanations~\cite{sinha2002role} leverage social relationships to make recommendations by explaining that \textit{users with similar interests also favor the recommended item}. The user's social property makes these explanations more persuasive, encouraging users to try the recommendations. However, the variance in user preferences may render this approach less impactful in gauging actual preference. Lastly, attribute-based explanations focus on highlighting the attributes of recommended items that users might find appealing, essentially conveying "\textit{these features might interest you}". This method demands customization according to each user's interests, yielding higher accuracy and satisfaction. Thus, they are at the forefront of research~\cite{wang2018reinforcement, xian2021ex3, wang2022reinforced, verma2022recxplainer,zhang2022neuro}.

Obviously, such explanations typically employ pre-defined and formulaic formats, such as explanations based on similar items or friends. Although capable of conveying essential information, such inflexible formats may diminish the user experience and satisfaction by lacking adaptability and personalization~\cite{wang2018reinforcement}. For this reason, natural language generation approaches have received increasing attention. Early work~\cite{li2017neural,dong2017learning,li2020generate} mainly relied on recurrent neural networks (e.g., LSTM~\cite{hochreiter1997long}, GRU~\cite{cho2014learning}). Limited by the model's expressiveness, they often suffer from the issue of insufficient diversity. With the excellent performance of Transformer-based models in various natural language tasks, some work attempts to integrate Transformer-based models into explainable recommendations. ~\cite{li2021personalized} use the position vectors corresponding to the user (item) IDs to predict interpreted tokens. Subsequent work~\cite{zhan2023towards} has shown that the generated explanation cannot justify the user's preference by synthesizing irrelevant descriptions. Therefore, Ni et al.~\cite{ni2019justifying} used such information as guided input to BERT to obtain a controllable justification. Considering that such auxiliary information is not always available in real-world scenarios, ExBERT~\cite{zhan2023towards} only requires historical explanations written by users, and utilizes a multi-head self-attention based encoder to capture the relevance between these explanations and user-item pairs. Recently, MMCT~\cite{liu2023multimodal}, EG4Rec~\cite{qu2022explanation}, and KAER~\cite{bai2020fusing} have further carried out finer-grained modeling of information such as visual images, time series, and emotional tendencies to obtain high-quality interpretations.


Due to the limited expressive power of traditional language models, natural language generation methods are prone to long-range dependence problems~\cite{zhan2023towards}, that is, the input of long texts will appear to generate explanations that lack diversity and coherence in content. In addition, these explanation methods are tightly coupled with specific recommendation models (e.g., NETE~\cite{li2020generate}), or directly design a new recommendation model (e.g., NRT~\cite{li2017neural}, PETER~\cite{li2021personalized}), and they are often powerless when faced with existing advanced recommendation models, which limits their generalizability. This is also a flaw in template-based methods. Notably, in industrial settings, recommendation algorithms frequently involve not just a single model but a cascade or integration of multiple models, and these elaborate combinations further exacerbate the difficulty of deciphering recommendations.

Thanks to LLMs' remarkable generative ability in language tasks, making them ideal for tackling the aforementioned challenges~\cite{bommasani2021opportunities}. Firstly, with the leverage of extensive training data, LLMs adeptly harness human language, encompassing context, metaphors, and complex syntax. This equips them to craft customized explanations that are precise, natural, and adaptable to various user preferences~\cite{li2020generate,li2021personalized,li2023personalized}, mitigating the limitations of conventional, formulaic explanations. 
Secondly, the unique in-context learning capabilities of LLMs, such as zero-shot prompting, few-shot prompting, and chain-of-thought prompting, enable them to garner real-time user feedback during interactions, furnish recommendation outcomes, and their corresponding interpretations, fostering bidirectional human-machine alignment. Recent study~\cite{bills2023language} has demonstrated the potential of LLMs in elucidating the intricacies of complex models, as evidenced by GPT-4 autonomously interpreting the function of GPT-2's each neuron by inputting appropriate prompts and the corresponding neuron activation. This showcases an innovative approach to interpreting deep learning-based recommendation models. It’s critical to highlight that this interpretation technique is agnostic to the model's architecture, distinguishing it from traditional interpretations that are bound to specific algorithms. Thus, recommendation interpretations founded on LLMs pave the way for a versatile and scalable interpretational framework with broader applicability.

Although LLMs have inherently significant advantages in recommendation explanations, it is imperative to recognize potential issues. Firstly, akin to recommendation models, LLMs are essentially black boxes that are difficult for humans to understand. We cannot identify what concepts they give explanations based on~\cite{wu2023interpretability}. Also, the explanation given may be insincere; that is, the explanations are inconsistent with their recommended behaviors. Some recent developments~\cite{li2023making,wei2022chain} involve utilizing chains of thought to prompt reasoning for improved interpretability; however, the opacity of the reasoning process of each step remains a concern, and ~\cite{turpin2023language} has questioned the possible unfaithful explanations of chain-of-thought prompting. Secondly, the extensive data utilized by LLMs may encompass human biases and erroneous content~\cite{li2021hidden}. Consequently, even if the explanation aligns with the model’s recommendation behavior, both the explanation and recommendation could be flawed. Monitoring and calibrating these models to ensure fairness and accuracy in explainable recommendations is essential. Lastly, generative models exhibit varying levels of proficiency across different tasks, leading to inconsistencies in performance. Identical semantic cues could yield disparate recommendation explanations. This inconsistency has been substantiated by recent studies~\cite{wang2023robustness,han2023information} focusing on the LLMs' robustness. Addressing these issues calls for exploring techniques to mitigate or even circumvent low-reliability explanatory behavior, and investigating how LLMs can be trained to consistently generate reliable recommendation explanations, especially under adversarial conditions, is a worthwhile avenue for further research.