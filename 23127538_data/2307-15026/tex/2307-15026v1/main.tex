\pdfoutput=1
\documentclass[pdftex,prl,aps,twocolumn,twoside,nofootinbib,showkeys,showpacs,10pt]{revtex4-2}
% \documentclass[aps,prl,twocolumn,10pt]{revtex4-1}

% To have section numbers
\setcounter{secnumdepth}{4}

\usepackage[T1]{fontenc}
\fontencoding{T1}  
\usepackage[utf8]{inputenc}

\usepackage[normalem]{ulem}

\usepackage{todonotes}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{P}
\newcommand{\Inf}{\operatorname{Inf}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\com}{\mathbb{C}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cD}{\mathcal{D}}
\newcommand{\cB}{\mathcal{B}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\1}{\mathbbm{1}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\dom}{\operatorname{dom}}
\newcommand{\D}{\mathfrak{D}}
\newcommand{\symp}{\mathbb{S}}
\newcommand{\spec}{\operatorname{spec}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\J}{\mathcal{J}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\un}{\mathbf{1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\renewcommand{\L}{\mathcal L}
\newcommand{\ad}{a^\dagger}
\newcommand{\KMS}{\text{KMS}}
\renewcommand{\d}{d}
\renewcommand{\phi}{\varphi}
\renewcommand{\epsilon}{\varepsilon}

\usepackage{enumitem}
\usepackage{tikz}
\usepackage{amsfonts}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage[breakable]{tcolorbox}
\usepackage{bbm}
\usepackage{url}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{upgreek }
\usepackage{dsfont}
\usepackage{collectbox}
\usepackage{braket}
\usepackage{quantikz}

\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}
\newcommand{\DrawBox}[1][]{%
    \tikz[overlay,remember picture]{
    \draw[red,#1]
      ($(left)+(-0.2em,0.9em)$) rectangle
      ($(right)+(0.2em,-0.3em)$);}
}


\pagestyle{plain}

\usepackage[plain]{fancyref} %references with object type, no pages
\usepackage[ruled]{algorithm2e}

% \usepackage{algorithmicx}
% \usepackage{algorithm}
\usepackage{algpseudocode}
% \def\NoNumber#1{{\def\alglinenumber##1{}\State #1}\addtocounter{ALG@line}{-1}}

\setlength{\topmargin}{-1.6cm}
\setlength{\textheight}{23cm}

%\usepackage{hyperref}
\usepackage[colorlinks = true, citecolor = red, urlcolor=blue]{hyperref}

\usepackage{cleveref}

\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\makeatletter
\newcommand{\setthmtag}[1]{% \settheoremtag{<tag>}
  \let\oldthethm\thethm% Store \thetheorem
  \newcommand{\thethm}{#1}% Redefine it to a fixed value
  \g@addto@macro\endthm{% At \end{theorem}, ...
    \addtocounter{thm}{-1}% ...restore theorem counter value and...
    \global\let\thethm\oldthethm}% ...restore \thetheorem
  }
\makeatother

\newtheorem{prop}[thm]{Proposition}
\newtheorem*{prop*}{Proposition}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem*{lemma*}{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem*{cor*}{Corollary}
\newtheorem{cj}[thm]{Conjecture}
\newtheorem*{cj*}{Conjecture}
\newtheorem{definition}[thm]{Definition}
\newtheorem*{Def*}{Definition}
\newtheorem{question}[thm]{Question}
\newtheorem{problem}[thm]{Problem}
\newtheorem{assum}{Assumption}
\newtheorem{assumst}{Assumption}

\usepackage{titlesec}

% \titleformat{\section}
% {\normalfont\Large\bfseries}{\thesection}{1em}{}
% \titleformat{\subsection}
% {\normalfont\large\bfseries}{\thesubsection}{1em}{}
% \titleformat{\subsubsection}
% {\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}
% \titleformat{\paragraph}[runin]
% {\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
% \titleformat{\subparagraph}[runin]
% {\normalfont\normalsize\bfseries}{\thesubparagraph}{1em}{}


\theoremstyle{definition}
\newtheorem{rem}{Remark}
\newtheorem*{rem*}{Remark}
\newtheorem*{note}{Note}
\newtheorem{ex}{Example}
\newtheorem{axiom}{Axiom}


\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\bq{\begin{quote}}
\def\eq{\end{quote}}
\def\ben{\begin{enumerate}}
\def\een{\end{enumerate}}
\def\bit{\begin{itemize}}
\def\eit{\end{itemize}}
\def\nn{\nonumber}
\def\fr{\frac}
\def\hl{\hline}
\def\ra{\rightarrow}
\def\Ra{\Rightarrow}
\def\la{\leftarrow}
\def\La{\Leftarrow}
\def\lb{\left(}
\def\rb{\right)}
\def\lset{\lbrace}
\def\rset{\rbrace}
\def\lk{\left\langle}
\def\rk{\right\rangle}
%\def\l|{\left|}
\def\r|{\right|}
\def\lbr{\left[}
\def\rbr{\right]}
\def\i{\text{id}}
\newcommand\C{\mathbbm{C}}
\newcommand\Q{\mathbbm{Q}}
\newcommand\Z{\mathbbm{Z}}
\newcommand\cM{\mathcal{M}}

\newcommand\R{\mathbbm{R}}
\newcommand\N{\mathbbm{N}}
\newcommand\M{\mathcal{M}}
\newcommand\B{\mathcal{B}}
\newcommand\Bo{\mathfrak{B}}
\newcommand\Co{\mathfrak{C}}
\newcommand\Hi{\mathfrak{h}}
\newcommand\Proj{\mathfrak{P}}
\newcommand{\U}{\mathcal{U}}
\newcommand\F{\mathbb{F}}
\newcommand{\Tm}{\mathcal{T}}
\newcommand{\Rm}{\mathcal{R}}
\newcommand{\Sm}{\mathcal{S}}
\newcommand{\Em}{\mathcal{E}}
\newcommand{\Dm}{\mathcal{D}}
\newcommand{\Cm}{\mathcal{C}}
\newcommand{\Lm}{\mathcal{L}}
\newcommand{\Km}{\mathcal{K}}
\newcommand{\Mm}{\mathcal{M}}
\newcommand{\Hm}{\mathcal{H}}
\newcommand{\Pm}{\mathcal{P}}
\newcommand{\Gm}{\mathcal{G}}
\newcommand{\Um}{\mathcal{U}}
\newcommand{\Vm}{\mathcal{V}}
\newcommand{\Xst}{X}
\newcommand{\Yst}{Y}
\newcommand{\LN}{\mathcal{LN}}
\newcommand{\n}{\mathcal{N}}
\newcommand{\cQ}{\mathcal{Q}}
%\newcommand\Unity{\mathds{1}}

\newcommand{\Ltimes}{\oplus}

\newcommand{\unitary}{\mathfrak{un}}
\newcommand{\chan}{\mathfrak{ch}}
\newcommand{\sgroup}{\mathfrak{sg}}

\newcommand{\chanSet}{\mathfrak{C}}
%newcommand Daniel
\newcommand{\ketbra}[2]{|#1\rangle\langle #2|}
\newcommand{\one}{\mathds{1}}
\newcommand{\id}{\text{id}}
\newcommand{\scalar}[2]{\langle#1|#2\rangle}
\newcommand{\rl}[2]{S\lb#1\|#2\rb}
\newcommand{\lioud}{\liou_{dep}}
\newcommand{\Diag}{\text{Diag}}
\newcommand{\daniel}[1]{\textcolor{red}{[daniel] #1}}
\newcommand{\highlight}[1]{\textcolor{black}{#1}}
\newcommand{\cC}{\mathcal{C}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\acc}{\textit{ACCEPT}}
\newcommand{\gs}{\textit{GibbsSampler}}
\newcommand{\gibbs}[1]{\frac{e^{#1}}{\tr{e^{#1}}}}
\newcommand{\setC}{\mathbb{C}}
\newcommand{\setR}{\mathbb{R}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\tcO}{\tilde{\mathcal{O}}}
\newcommand{\cU}{\mathcal{U}}
\newcommand\be{\begin{equation}}
\newcommand\ee{\end{equation}}
\newcommand{\ab}{\mathbf{a},\mathbf{b}}
\newcommand{\danieltodo}[1]{\color{red}#1\color{black}}
\newcommand{\mybox}{%
    \collectbox{%
        \setlength{\fboxsep}{1.3pt}%
       \setlength{\arrayrulewidth}{1.4pt}
        \setlength{\fboxrule}{1.2pt} 
         %\setlength{\width}{\columnwidth}
        \fbox{\BOXCONTENT}%
    }%
}

%%% FOR COMMENTS
% Matthias Caro 
\newcommand{\mcc}[1]{{\color[RGB]{220, 20, 60}{[MCC: #1]}}}

\newcommand{\anb}[1]{{\color{orange}{#1}}}

% revision commenting tools
\newcommand{\stkout}[1]{\ifmmode\text{\sout{\ensuremath{#1}}}\else\sout{#1}\fi}
\newif\ifverbose
% verbose: true: show corrections, false: hide corrections
\verbosetrue
%\verbosefalse
% ins: stuff that is new
\newcommand{\ins}[1]{\ifverbose\textcolor{blue}{#1}\else#1\fi}
% edit: stuff that has been replaced with other stuff
\newcommand{\edit}[2]{\ifverbose\textcolor{red}{\stkout{#1} #2}\else#2\fi}
% del: stuff that has been removed
\newcommand{\del}[1]{\ifverbose\textcolor{red}{\stkout{#1}}\fi}

\widowpenalty10000
\clubpenalty10000

\begin{document}
\title{\texorpdfstring{Dissipation-enabled bosonic Hamiltonian learning \\
via new information-propagation bounds}{Dissipation-enabled bosonic Hamiltonian learning
via new information-propagation bounds}}





\author{\begingroup
%\hypersetup{urlcolor=navyblue}
\href{https://orcid.org/0000-0003-0471-2745}{Tim M\"{o}bus
\endgroup}
}
\email[Tim MÃ¶bus ]{tim.moebus@tum.de}
\affiliation{Zentrum Mathematik, Technische Universit\"{a}t M\"{u}nchen, 85748 Garching, Germany}

\author{\begingroup
%\hypersetup{urlcolor=navyblue}
\href{https://orcid.org/0000-0003-4796-7633}{Andreas Bluhm
\endgroup}
}
\email[Andreas Bluhm ]{andreas.bluhm@univ-grenoble-alpes.fr}
 \affiliation{Univ.\ Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France}


\author{\begingroup
%\hypersetup{urlcolor=navyblue}
\href{https://orcid.org/0000-0001-9009-2372}{Matthias C. Caro
\endgroup}
}
\email[Matthias C. Caro ]{mcaro@caltech.edu}
 \affiliation{Institute for Quantum Information and Matter, Caltech, Pasadena, CA, USA\\Dahlem Center for Complex Quantum Systems, Freie Universit\"at Berlin, Berlin, Germany}

\author{\begingroup
%\hypersetup{urlcolor=navyblue}
\href{https://orcid.org/0000-0003-0134-5257}{Albert H. Werner
\endgroup}
}
\email[Albert H. Werner]{werner@math.ku.dk}
 \affiliation{Department of Mathematical Sciences, University of Copenhagen, Universitetsparken 5, 2100
Copenhagen, Denmark}


\author{\begingroup
%\hypersetup{urlcolor=navyblue}
\href{https://orcid.org/0000-0001-7712-6582}{Cambyse Rouz\'{e}
\endgroup}
}
\email[Cambyse Rouz\'{e} ]{cambyse.rouze@tum.de}
 \affiliation{Inria, T\'{e}l\'{e}com Paris - LTCI, Institut Polytechnique de Paris, 91120 Palaiseau, France\\ Zentrum Mathematik, Technische Universit\"{a}t M\"{u}nchen, 85748 Garching, Germany}
 
\xdefinecolor{mygreen}      {RGB}{108,187,69}
\newcommand{\tm}[1]{{\color{mygreen}TM:~#1}}
\newcommand{\ca}[1]{{\color{blue}CR:~#1}}

\begin{abstract}
    %Context:
    Reliable quantum technology requires %precise 
    knowledge of the dynamics governing the underlying %quantum 
    system. This problem of characterizing and benchmarking quantum devices or experiments in continuous time is referred to as the Hamiltonian learning problem. 
    %Research Gap: 
    In contrast to multi-qubit systems, learning guarantees for the dynamics of bosonic %quantum 
    systems have hitherto remained mostly unexplored.
    %Results:
    For $m$-mode Hamiltonians given as polynomials in annihilation and creation operators with modes arranged on a lattice, we %first 
    establish a simple moment criterion in terms of the particle number operator which ensures that learning strategies from the finite-dimensional setting extend to the bosonic setting, requiring only coherent states and heterodyne detection on the experimental side. We then propose an enhanced procedure based on added dissipation that even works if the Hamiltonian time evolution violates this moment criterion: With high success probability it learns all coefficients of the Hamiltonian to accuracy $\varepsilon$ using a total evolution time of $\mathcal{O}(\varepsilon^{-2}\log(m))$.
    %Methods:
    Our protocol %only 
    involves the experimentally reachable resources of projected coherent state preparation, dissipative regularization akin to recent quantum error correction schemes involving cat qubits stabilized by a nonlinear multi-photon driven dissipation process, and heterodyne measurements. 
    As a crucial step in our analysis, we establish our moment criterion and a new Lieb-Robinson type bound for the evolution generated by an arbitrary bosonic Hamiltonian of bounded degree in the annihilation and creation operators combined with photon-driven dissipation.
    %Impact:
    Our work demonstrates that a broad class of bosonic Hamiltonians can be efficiently learned from simple quantum experiments, and %we believe that 
    our bosonic Lieb-Robinson bound may independently serve as a versatile tool for studying evolutions on continuous variable systems.
\end{abstract}


\maketitle

\section{Introduction}
In recent years, significant progress was achieved in realizing large quantum devices, comprising tens to hundreds of qubits, using various hardware architectures \cite{arute2019quantum,zhong2020quantum,scholl2021quantum,ebadi2021quantum}. This advancement represents a major step towards achieving quantum computers and simulators capable of solving complex problems that classical computers cannot handle \cite{cirac2012goals,nielsen2002quantum}. However, further progress in this direction necessitates a careful understanding of the underlying Hamiltonians generating dynamics on the hardware. At the core of this task, commonly known as the Hamiltonian learning problem \cite{eisert2020quantum,shulman2014suppressing,Zhang2014Quantum,Zhang2015Identification,DiFranco2009Hamiltonian,Sheldon2016Procedure,Sone2017Hamiltonian,Samach2021Lindblad,Wang2015Hamiltonian,Wang2017Experimental,Valenti2019Hamiltonian,Valenti2021Scalable,Granade2012Robust,Wiebe2014Hamiltonian,Wiebe2014Quantum,Shabani2011Estimation,Qi2019Determining,Chertkov2018Computational,Bairey2019Learning,Anshu2021Sample,Evans2019Scalable,Zhi2020Hamiltonian,Haah2021Optimal,Silva2011Practical,Bairey2020Learning,Zubida2021Optimal, rouze2021learning,Gu2022Practical,Yu2022Practical,franca2022efficient, caro2022learning,wilde2022scalably,onorati2023efficient,dutkiewicz2023advantage,huang2023learning}, lies the challenge of handling the exponential growth of the state space dimension as the number of qubits increases.

Previously proposed approaches have addressed the challenge of Hamiltonian parameter estimation by making certain assumptions. Some assume the availability of a trusted quantum simulator capable of simulating the unknown Hamiltonian \cite{Wiebe2014Hamiltonian,Wiebe2014Quantum}. Others assume the ability to prepare specific states of the Hamiltonian, such as steady states and Gibbs states \cite{Qi2019Determining,Bairey2019Learning,Anshu2021Sample,Haah2021Optimal,rudinger2015compressed,rouze2021learning,onorati2023efficient}, partly inspired by analogous problems in the classical literature \cite{santhanam2012information,Ravikumar2010,bresler2015efficiently,vuffray2016interaction,klivans2017learning,hamilton2017information,wu2019sparse,bresler2017learning}. Alternatively, several studies \cite{Silva2011Practical,Bairey2020Learning,Zubida2021Optimal} have exploited the observation that Hamiltonian (and Lindbladian dynamics) can be described by a Master equation. This observation allows for a simple linear relationship between the time derivatives of expectation values and the parameters of the Hamiltonian. 


However, the approaches mentioned above suffer from a significant drawback: they rely on finite difference methods to estimate the time derivatives. Achieving a high precision with this approach is challenging due to the experimental limitations imposed by the finite operation time of gates and measurements. To estimate a Hamiltonian parameter with an additive error $\epsilon$, the system must be probed at time intervals of size $\mathcal{O}(\epsilon)$, and the expectation values of observables must be estimated with a precision of $\mathcal{O}(\epsilon^2)$. Thus, the overall sample complexity required to estimate each parameter is $\mathcal{O}(\epsilon^{-4})$.

More recently, Ref.~\cite{franca2022efficient} proposed a protocol that mitigates these challenging experimental requirements. Their protocol uses a time resolution of $\mathcal{O}(\operatorname{polylog}(\epsilon^{-1}))$, an exponential improvement over previous approaches. Moreover, it provides an overall sample complexity of $\mathcal{O}(\epsilon^{-2}\operatorname{polylog}(m, \epsilon^{-1}))$ for retrieving all parameters of a geometrically local $m$-qubit Hamiltonian up to precision $\epsilon$. 
Using long-time Hamiltonian evolution interleaved with control operations, the dependency of the complexity on the precision $\epsilon$ was further improved to the  so-called Heisenberg-limited scaling of $\mathcal{O}(\epsilon^{-1}\ln(m))$ in \cite{huang2023learning}, where their notion of complexity corresponds to the total evolution time.


The results and methodologies presented by \cite{franca2022efficient,huang2023learning}, e.g.~Lieb-Robinson bounds \cite{LiebRobinson1972,Poulin2010,Kliesch2014,Hastings2010,Kliesch2014,Kuwahara2020} and cluster expansion techniques \cite{wild2023classical}, apply to locally finite-dimensional systems. 
However, recent years have seen {remarkable progress in the use of} infinite-dimensional, continuous variable (CV) quantum systems. These systems encompass a wide range of applications in various areas of quantum information, including quantum communication \cite{braunstein2005quantum,holevo2001evaluating,wolf2007quantum,takeoka2014fundamental,pirandola2017fundamental,wilde2017converse,rosati2018narrow,lami2023exact}, sensing \cite{aasi2013enhanced,zhang2018noon,meyer2001experimental,mccormick2019quantum}, simulations \cite{flurin2017observing}, computing and error correction \cite{becker2021energy,gottesman2001encoding,mirrahimi2014dynamically,ofek2016extending,michael2016new,guillaud2019repetition}, enabled by advances in non-classical radiation sources \cite{ourjoumtsev2006generating,kurochkin2014distillation,huang2015optical,reimer2016generation,eichler2011observation,zhong2013squeezing}.
Given the %significant
technological and experimental relevance of CV systems, there is a pressing need for rapid and effective learning
of CV systems \cite{becker2022classical,gandhari2022continuous}. This paper {addresses} this need {with} a rigorous procedure {for learning bosonic Hamiltonians}, thereby establishing a %very 
general approach to benchmark CV quantum technologies. 
We achieve this by considering an engineered photon driven dissipation as regularization and demonstrate that the resulting regularized evolution allows for Lieb-Robinson (LR) type information propagation bounds. 
This approach is required because directly passing through finite-dimensional LR bounds via projecting onto low Fock number states leads to an undesirably system-size dependent error and is challenging due to the unboundedness of the generators.
This insight is combined with tools from polynomial approximation theory for derivative estimation. 

\section{The setup}

We consider an $m$-mode bosonic system, where each mode is associated to a vertex $v\in V$ of a graph $G=(V,E)$, whose edges $e\in E$ model the interactions between different modes. $G$ is assumed to be known in advance. This assumption is common in much of the literature on learning Hamiltonians from dynamics \cite{Haah2021Optimal, franca2022efficient, Gu2022Practical, huang2023learning, li2023heisenberglimited}. The total number of vertices is denoted by $|V|=m$, and the maximum degree of a vertex by $\gamma$. 
We assume the Hamiltonian to be a sum of two-body interactions on the edges of $G$, with each interaction described by a degree-$d$ polynomial in annihilation and creation operators.
That is, the Hamiltonian of the system is assumed to take the following form:
\begin{align}\label{unboundedH}
    H\coloneqq \sum_{e\in E}\,H_{e}\,,
\end{align}
where for any edge $e=\{i,j\}\in E$,
\begin{align}
    H_e
    \coloneqq \sum_{k,\ell,k',\ell'=0}^d\,\lambda^{(e)}_{k\ell k'\ell'}\,(a_i^\dagger)^k\,a_i^\ell\,(a_j^\dagger)^{k'}\,a_j^{\ell'}\,.
\end{align}
for some complex coefficients $\lambda^{(e)}_{k\ell k'\ell'}$ with $|\lambda^{(e)}_{k\ell k'\ell'}|\le L$. 
Here, $a_i$ denotes the creation operator at vertex $i$, whereas $a_i^\dagger$ is its associated creation operator. 
In what follows and to simplify the exposition, we will assume that $\lambda^{(e)}_{k\ell 00}=\lambda^{(e)}_{00k'\ell'}=0$, that is, the Hamiltonian has no on-site potentials, although this assumption can be easily removed.
Our first assumption is that we have access to incoherent copies of the evolution $e^{-it H}$ at times $t\ge 0$ that we can choose freely, acting on initial coherent states $|\alpha\rangle$, $\alpha=(\alpha_1,\dots,\alpha_m)\in\mathbb{C}^{m}$. We recall that these states are eigenvectors of the annihilation operators:
\begin{align}\label{aonalpha}
a_i|\alpha\rangle=\alpha_i|\alpha\rangle\,,\qquad i\in[m],\,\alpha_i\in\mathbb{C}\,.
\end{align}
Given $\delta\in (0,1)$ and $\epsilon>0$, the task is to find an approximation $\hat{\lambda}_{k\ell k' \ell'}^{(e)}$ of the coefficients ${\lambda}_{k\ell k' \ell'}^{(e)}$ with success probability $1-\delta$ and with accuracy 
$\|\lambda-\hat{\lambda}\|_{\ell^\infty}\coloneqq \max_{e\in E}\max_{k,l,k',\ell'}|{\lambda}_{k\ell k' \ell'}^{(e)}-\hat{\lambda}_{k\ell k' \ell'}^{(e)}|\le \epsilon$. For this, we are allowed to use the evolution $e^{-iHt}$ generated by $H$ and measure each mode separately in order to obtain information about $H$. The main resource that we want to minimize is the total evolution time required to achieve this goal. Using \eqref{aonalpha}, we directly get that for any $\alpha\in\mathbb{C}^{m}$, $\beta\in\mathbb{C}^2$, and $e=\{i,j\}\in E$:
\begin{align}\label{coefffromgenerator}
   \bra{\alpha} \big[H,\ketbra{\beta}{\beta}_e\otimes I_{e^c}\big]\ket{\alpha}
    =g_e(\alpha, \beta)\lvert \braket{\alpha_e|\beta}\rvert^2\,,
    \end{align}
    where
    \begin{align*}
  g_e(\alpha, \beta)&\coloneqq  \sum_{\substack{j\ne j'\sim i\\k\ell k'\ell'}}\,\lambda^{(\{i,j'\})}_{k\ell k'\ell'}\left( \overline{\alpha}_{i}^k \overline{\alpha}_{j'}^{k'} \beta_i^\ell \alpha_{j'}^{\ell'} - \overline{\beta}_i^k \overline{\alpha}_{j'}^{k'} \alpha_i^\ell \alpha_{j'}^{\ell'} \right) \\
  & +\sum_{\substack{i\ne i'\sim j\\k\ell k'\ell'}}\,\lambda^{(\{i',j\})}_{k\ell k'\ell'}\left( \overline{\alpha}_{j}^k \overline{\alpha}_{i'}^{k'} \beta_j^\ell \alpha_{i'}^{\ell'} - \overline{\beta}_j^k \overline{\alpha}_{i'}^{k'} \alpha_j^\ell \alpha_{i'}^{\ell'} \right)\\
  & +\sum_{k\ell k'\ell'}\lambda^{(e)}_{k\ell k'\ell'}\left(\overline{\alpha}_i^{k}\overline{\alpha}_j^{k'}\beta_i^{\ell}\beta_j^{\ell'}-\overline{\beta}_i^k\overline{\beta}_j^{k'}\alpha_i^\ell\alpha_j^{\ell'}  \right)\,.
\end{align*}
Here, $i \sim j$ means that $i$ and $j$ are neighbors in $G$. Note: The function $\alpha\mapsto g_e(\alpha,\beta)$ only depends on variables $\alpha_{i'}$ with $i'$ at distance at most $1$ of the edge $e$. 
We see that the function $g_e(\alpha,\beta)$ is a multivariate polynomial of degree $d$ whose coefficients are intimately related to the Hamiltonian coefficients %of interest 
associated to the edge $e$. This suggests that the coefficients can be learned through suitable local heterodyne measurements of the evolved states, that is, few-mode measurements with effect operators given by projectors onto coherent states. Indeed, in \Cref{Lemmacoeffpoly}, we prove the following fact:

\begin{lemma}\label{coeffdiff}
The real and imaginary parts of the coefficient $\lambda_{k\ell k'\ell'}^{(e)}$ can be expressed in terms of partial derivatives of $g_e$ as
    \begin{align*}
        \operatorname{Im}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)
        &=\frac{\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, \beta)\big|_{\alpha,\beta=0}}{2ik'! \ell'! k!\ell!}\, ,\\
        \operatorname{Re}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)&=-\frac{\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, e^{-i\frac{\pi}{2(\ell+\ell')} }\beta)\big|_{\alpha,\beta=0}}{2ik'! \ell'! k!\ell!}\,.
    \end{align*}
    
\end{lemma}


\section{Taylor expansion and particle number control for many-body CV Hamiltonians}

Before discussing the construction of our estimator in detail, let us first recall %consider 
the %simpler 
situation of a local Hamiltonian associated to a locally finite-dimensional quantum system, e.g.~a quantum lattice spin system where each vertex corresponds to a single qubit. %There, it is an easy observation that the task reduces to that of learning the coefficients of the many-qubit Hamiltonian $H_{\operatorname{qubit}}$ in a local Pauli basis. 
There, the Hamiltonian learning task is that of learning the coefficients of the many-qubit Hamiltonian $H_{\operatorname{qubit}}$ in a local Pauli basis.
Therefore, as observed in \cite{Haah2021Optimal, franca2022efficient, Gu2022Practical, caro2022learning}, for a fixed edge $e=\{i,j\}$, it is sufficient to learn coefficients of the form $\lambda_{\sigma_{e}}\coloneqq \tr\big([H^{\operatorname{qubit}},\rho]\sigma_{e}\big)$, where $\sigma_{e}=\sigma_i\otimes \sigma_j\otimes I_{e^c}$ denotes the tensor product of two arbitrary Pauli matrices $\sigma_i$ and $\sigma_j$ supported on the vertices $i$ and $j$ of the graph, and where the state $\rho$ can be chosen to be a tensor product of biased qubit states of the form $\frac{I\pm \sigma_k}{2}$. 
A simple Taylor expansion of the evolution up to second order gives, denoting $\rho_t\coloneqq e^{-itH^{\operatorname{qubit}}}\rho e^{itH^{\operatorname{qubit}}}$,
\small
\begin{align}\label{Taylor}
    \Big|\lambda_{\sigma_{e}}-\frac{\tr\big[(\rho_t-\rho)\sigma_{e}\big]}{t}\Big|\le \frac{t}{2}\max_{s\le t}\Big|\tr\big[\rho_s(\cH^{\operatorname{qubit}})^2(\sigma_{e})\big]\Big| \,,
\end{align}
\normalsize
 where $-\cH^{\operatorname{qubit}}(O)=i[H^{\operatorname{qubit}},O]$ denotes the generator of the unitary evolution in the Heisenberg picture. The latter bound can be further simplified due to the locality of $H_{\operatorname{qubit}}$, which leads to an approximation of the coefficient $\lambda_{\sigma_{e}}$ via a relative difference with additive error at most
\begin{align}\label{eq:operatornormbound}
    \frac{t}{2}\Big\|\big[H^{\operatorname{qubit}}_{e\partial\partial},\big[H^{\operatorname{qubit}}_{e\partial},\sigma_{e}\big]\big]\Big\|_\infty\,.
\end{align}
Above, $H^{\operatorname{qubit}}_{e\partial}$ refers to the interactions intersecting sites $i$ and $j$, whereas $H^{\operatorname{qubit}}_{e\partial\partial}$ refers to those interactions that intersect $i$, $j$, as well as the support of $H_{e\partial}^{\operatorname{qubit}}$. Since the operator norm above is independent of the overall size of the graph, on can therefore learn $\lambda_{\sigma_e}$ as long as $t=\mathcal{O}(\epsilon)$. As we mentioned in the introduction, combining this with a standard estimator of time-evolved averages $\tr[\rho_t\sigma_{e}]$ results in a sub-optimal sample complexity of order $\mathcal{O}(\epsilon^{-4}\ln(m))$, and therefore a total evolution time of order $\mathcal{O}(\epsilon^{-3}\ln(m))$. Nevertheless, even this basic approach does not directly carry over to our continuous variable setting. This is due to the intrinsic unboundedness of the Hamiltonian in \eqref{unboundedH}. 

Instead, we need to consider a state-dependent upper bound similar to \eqref{Taylor}. We recall that the local observables considered in our setting are the two-mode effect operators $P_\beta^{(e)}\coloneqq \frac{1}{\pi^2}\ketbra{\beta}{\beta}_e\otimes I_{e^c}$ corresponding to a local heterodyne measurement. Initializing the evolution on the pure coherent state $\rho\coloneqq \ketbra{\alpha}{\alpha}$, a reasoning identical to the one leading to \eqref{Taylor} gives the upper bound $\big| \frac{1}{\pi^2}\,g(\alpha,\beta)|\langle{\alpha_e}|{\beta}\rangle|^2-t^{-1}\tr\big[(\rho_t-\rho)P_\beta^{(e)}\big]\big|\le \frac{t}{2}\max_{s\le t} \big|\tr\big[\rho_s\cH^2(P_{\beta}^{(e)})\big]\big|$, for $\rho_s\coloneqq e^{-itH}\rho e^{itH}$ and $-\cH(O)=i[H,O]$. As for the qubit case, $\cH^2(P_\beta^{(e)})$ simplifies into a double commutator involving Hamiltonian terms $H_{e'}$ corresponding to edges $e'$ at distance at most $1$ to $e$, where the distance between two sets of vertices is defined as the minimal distance between any two vertices belonging to each of the sets. Let us also denote the Hamiltonians involved  by $H_{e\partial}=\sum_{\operatorname{dist}(e,e')=0}H_{e'}$ and $H_{e\partial\partial}=\sum_{\operatorname{dist}(e,e')\le 1}H_{e'}$. 
The resulting bound can be further controlled in terms of the moments of the time-evolved particle number observable: 
\begin{align}
    \Big|\tr\big[\rho_s\cH^2(P_\beta^{(e)})\big]\Big|&\le \big\|\cH_{e\partial}\circ \cH_{e\partial\partial}(\rho_s)\big\|_1\nonumber\\
    &=\mathcal{O}(1)\cdot L^2(d+1)^8\,\gamma^2 M^{(2d)}_{e\partial\partial}(s)\,,
    \label{Taylorboundinfinite}
\end{align}
where $\|X\|_1\coloneqq \tr|X|$ denotes the trace norm and where, given a region $R\subset V$ and an integer $k\in\mathbb{N}$, $M^{(k)}_{R}(s)\coloneqq \tr\big[\rho_s \prod_{i\in R}(I+N_i)^k\big]$, for $N_i=a_i^\dagger a_i$ denoting the particle number observable at site $i$. In some interesting cases, these moments are known to be controllable by functions of time that are independent of the system size.
For example, for the celebrated Bose-Hubbard model \cite{schuch2011information, Kuwahara2020}:
\begin{align}\label{momentbound}
    M_R^{(k)}(s)= \mathcal{O}(1)
\end{align}
for $k,|R|,s=\mathcal{O}(1)$. Our reasoning above implies that the finite-dimensional method described at the beginning of this section extends to the CV case whenever such a moment bound holds.

\section{Dissipative regularizers}

However, the moment condition \eqref{momentbound} is not shared by all Hamiltonians of the form given in \eqref{unboundedH}. To enforce such a property for the time-evolved particle number observable, we resort to the use of a dissipative regularizing processes arising from the field of CV quantum error correction and known as photon-driven dissipation \cite{Guillaud.2019,Guillaud.2023}. 
The generator of this process takes the following Lindblad form: Given $\alpha_i\in \mathbb{C}$, $i\in V$, $\alpha=\{\alpha_i\}_i$ and $p\in\mathbb{N}$, 
\begin{align}\label{dissipationL}
  \cL^{(\alpha,p)}_V\coloneqq \sum_{j\in V}\,\cL[L_j^{(\alpha_j,p)}],\,
\end{align}
where for each vertex $i\in V$, $L_i^{(\alpha_i,p)}\coloneqq a_i^p-\alpha_{i}^p I_{i}$. Here, we recall that given a jump operator $L$, the Lindbladian $\cL[L]$ is defined as $\cL[L](\rho)\coloneqq L\rho L^\dagger-\frac{1}{2}\big\{L^\dagger L,\rho\big\}$. The generator of the whole regularized process is then described by
\begin{align}\label{Lindbladtotaldissiph}
    \widetilde{\cL}^{(\alpha)}\coloneqq \cH+ \cL^{(\alpha,p)}_V\,.
\end{align}
Importantly, note that the parameters $\alpha$ are chosen identical to the ones corresponding to our input coherent state $\ket{\alpha}$. The consequence of this choice is that the dissipation acts trivially on that state, $\cL_{V}^{(\alpha,p)}(\ketbra{\alpha}{\alpha})=0$. Hence, the identity \eqref{coefffromgenerator} still holds when replacing $\cH$ by $\widetilde{\cL}^{(\alpha)}$. Moreover, turning on the dissipation, and thus considering the time-evolved state $\rho_t=e^{t\widetilde{\cL}^{(\alpha)}}(\rho)$, has the effect of enforcing the moment condition \eqref{momentbound}:
\begin{thm}[Informal version of \Cref{lemmamoments}]
    Given $d,p,t,|\alpha_i|=\mathcal{O}(1)$ for all $i\in V$, for any $k\in \mathbb{N}$ with $d<k$, $p\ge \lceil\frac{2kd}{k-d}+2\rceil$, and any region $R\subset V$, 
    \begin{align}\label{ineqregion'}
        M_R^{(k)}(t)\le e^{\mathcal{O}(k|R|\ln(k|R|)t)}\,.
    \end{align}    
\end{thm}
Plugging the bound \eqref{ineqregion'} into \eqref{Taylorboundinfinite} for $k=2d$, we arrive at the following finite relative difference approximation of the functions $g_e(\alpha,\beta)$: 
\small
\begin{align}\label{eqboundmomentfinitediff}
    \Big| \frac{1}{\pi^2}\,g(\alpha,\beta)|\langle{\alpha_e}|{\beta}\rangle|^2-t^{-1}\tr\big[(\rho_t-\rho)P_\beta^{(e)}\big]\Big|= \mathcal{O}(t)
\end{align}
\normalsize
as $t\to 0$, as long as $p\ge4d+2$. 

\section{Simple learning algorithm from moment bounds}\label{sec:vanillaalgo}

Next, we %want to 
turn the bound found in \eqref{eqboundmomentfinitediff} into a guarantee for learning the coefficients $\lambda^{(e)}_{k\ell k'\ell'}$. Our method is detailed in \Cref{protocollearning1} below, where we denote the indicator function of the set $R_\beta\coloneqq [0,\beta_1]\times \dots \times [0,\beta_4]$, $\beta\in\mathbb{R}^4$, as $1_{R_\beta}$, and where $B_{\mathbb{R}^K}$ is the $\ell^\infty$ unit ball in $\mathbb{R}^K$ centered at $0$. We also fix nets of $B_{\mathbb{R}^K}$ which we denote by $\cN_{K}$ and will comment on later. The estimators $\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ constructed in \Cref{protocollearning1} are sums of i.i.d.~random variables with average 
\begin{align}\label{eqexpectationestimatorintro}
    \mathbb{E}\Big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\Big]= t^{-1}\int_{R_\beta} \frac{\tr\big[(\rho_t-\rho)P_{\beta'}^{(e)}\big]}{|\langle \alpha_e|\beta'\rangle|^2}\,d^4\beta'\,,
\end{align}
where $\rho\coloneqq \ketbra{\alpha}{\alpha}$ and $\rho_t=e^{t\widetilde{\cL}^{(\alpha)}}(\rho)$. 

\medskip 


\begin{algorithm}[H]
  \caption{BosonicHamiltonian}\label{protocollearning1}
  \KwIn{ $T\in\mathbb{N}$.}
  \KwOut{$\hat{Q}_{\alpha,\beta}^{(e,T)}(t)\in\mathbb{R}$ $\forall e\in E$, $\alpha\in\cN_{2},\,\beta\in \cN_{4}$.}
  \BlankLine
 Partition $E$ into $\nu=\mathcal{O}(1)$ sets $E_1,\dots E_\nu$  such that $\forall j\in[\nu], \forall e_1,e_2\in E_j$, $e_1\cap e_2=\emptyset$;\\
  \For{$j \in [\nu]$, $\alpha\in\cN_{2}$}{
  \For{$i_\alpha\in[T]$}{Prepare state $|\psi_\alpha\rangle\coloneqq \otimes_{e\in E_j}|\alpha\rangle_{e}\otimes |0\rangle_{E_j^c}$;\\
  %Run evolution generated by $\widetilde{\cL}^{(\alpha)}$ on $|\psi_\alpha\rangle$;
  Evolve $|\psi_\alpha\rangle$ under $\widetilde{\cL}^{(\alpha)}$ for time $t$;\\
  Perform heterodyne measurements on $ E_j$;\\
  $\rightarrow$ get outcomes $\beta^{(i_\alpha)}_e\in \mathbb{R}^4$ $\forall e\in E_j$;}}
  \Return{$\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\coloneqq  \frac{1}{Tt}\sum_{i_\alpha=1}^T\, \Big(\frac{1_{R_\beta}(\beta_e^{(i_\alpha)})}{|\langle \alpha_e|\beta_e^{(i_\alpha)}\rangle|^2}-1\Big)$.}
  \end{algorithm}

\medskip

Then, a joint use of Hoeffding's concentration inequality together with the union bound allows us to conclude that, with probability $1-\delta$, $\delta\in (0,1)$, for any $\alpha\in \cN_2,\,\beta\in\cN_{4}$, the random variables $\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ satisfy 
\begin{align}\label{eqexpectationestimator2intro}
\Big|\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)-\mathbb{E}\big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\big]\Big|\le {\epsilon}_{\operatorname{stat}}
\end{align}
as long as the number of measurements for each $\alpha, j\in[\nu]$ is at least (see \Cref{learningsection} for details):
\begin{align*}
    T=\,\mathcal{O}\left( \frac{1}{(t\cdot {\epsilon}_{\operatorname{stat}})^2}\ln\left(\frac{|E|\cdot |\cN_2|\cdot |\cN_4|}{\delta }\right)\right)\,.
\end{align*}
Next, for any $\alpha\in \mathbb{R}^2,\,\beta=(\beta_1,\dots,\beta_4)\in\mathbb{C}^2\cong \mathbb{R}^4$, denoting
\begin{align}
    Q^{(e)}_\alpha(\beta)&\coloneqq \frac{1}{\pi^2}\,\int_{R_\beta}\,g_e(\alpha,\beta')\,d^4\beta'\,,
\end{align}
we get from \eqref{eqboundmomentfinitediff} together with the fact that $g_e(\alpha,\beta)=g((\{\alpha_{e'}\}_{e'\in E_j},0_{E_j^c}),\beta)$, where $j\in[\nu]$ is such that $e\in E_j$, that 
\begin{align}\label{eqboundmomentfinitediff23intro}
    \Big|Q^{(e)}_{\alpha}(\beta)-\mathbb{E}\big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\big]\Big|=\mathcal{O}( t)\,.
\end{align}
In summary, combining \eqref{eqboundmomentfinitediff23intro} with \eqref{eqexpectationestimator2intro}, we see that, with probability $1-\delta$, on the net $(\alpha,\beta)\in\cN_{2}\times \cN_{4}$:
\begin{align*}
    \Big| Q^{(e)}_{\alpha}(\beta)- \hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\Big|=\mathcal{O}(  t+\epsilon_{\operatorname{stat}})\,.
\end{align*}
Moreover, the number $\nu$ of partitions of the graph is simply related to its chromatic number, which scales linearly with the degree of the interaction graph, that is, the graph in which edges become vertices and two edges are connected if they share a vertex. In turn, the chromatic number scales linearly with the degree $\gamma$ of the underlying graph $G$. Therefore, the total number of samples needed in \Cref{protocollearning1} is equal to $T\cdot |\cN_{2}|\cdot \nu=\mathcal{O}(\gamma \cdot T\cdot |\cN_2|)$. 
Next, we use an algorithm based on multivariate Lagrange interpolation that reconstructs a polynomial $\widetilde{Q}^{(e)}:(\alpha,\beta)\mapsto \widetilde{Q}^{(e)}_\alpha(\beta)$ with the same degree and coefficient constraints as $Q^{(e)}_{\alpha}(\beta)$ from the values of the estimator $\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ at the net points. Imposing that this algorithm outputs a polynomial that satisfies $\widetilde{Q}^{(e)}_\alpha(\beta)=\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ on the net, a simple polynomial analysis proves that $\widetilde{Q}^{(e)}$ approximates $Q^{(e)}$ on the unit ball $B_{\mathbb{R}^6}$ up to $\epsilon$ by choosing $\cN_4$ and $\cN_2$ of constant size only depending on the degree $d$. Finally, since $Q^{(e)}_{\alpha}(\beta)$ is defined via the integral of the density $g_{e}$ over certain rectangles, and since the coefficients $\lambda^{(e)}_{k\ell k'\ell'}$ were shown in \Cref{coeffdiff} to correspond to certain multivariate derivatives of $g_e$ at $0$, defining the coefficients $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ by performing the same differentiations on $\widetilde{Q}^{(e)}$, we can conclude by invoking the multivariate Markov brothers' inequality \cite{harris2008multivariate,harris2002markov}, which enables us to control the difference between the derivatives of $Q^{(e)}$ and $\widetilde{Q}^{(e)}$ in terms of $\sup_{(\alpha,\beta)\in B_{\mathbb{R}^6}}|Q_{\alpha}^{(e)}(\beta)-\widetilde{Q}_{\alpha}^{(e)}(\beta)|$. We arrive at the following result:


\begin{cor}[Vanilla strategy, see \Cref{Vanillastrategythm}]
    Combining \Cref{protocollearning2} with a polynomial reconstruction, we get an estimator $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ of $\lambda^{(e)}_{k\ell k'\ell'}$ such that, with probability $1-\delta$, we have $|\hat{\lambda}^{(e)}_{k\ell k'\ell'}-\lambda^{(e)}_{k\ell k'\ell'}|\le \epsilon$ for all $k\ell k'\ell'$, as long as the evolution time for each run scales as $t=\mathcal{O}(\epsilon)$ and the total number of samples is at least
    \begin{align*}
        N_{\operatorname{samp}}=\mathcal{O}\left( \frac{1}{{\epsilon}^4}\ln\left(\frac{|E|}{\delta}\right)\right)\,.
    \end{align*}
    Our scheme uses a total evolution time $T_{\operatorname{evo}}=\widetilde{\mathcal{O}}\big(\epsilon^{-3}\ln\big(\frac{|E|}{\delta}\big)\big)$.
\end{cor}


\section{\texorpdfstring{Improved sample complexity via \\ continuous variable Lieb-Robinson bounds}{Improved sample complexity via continuous variable Lieb-Robinson bounds}}

As mentioned in the introduction, the $\epsilon$-dependence of $N_{\operatorname{samp}}$ in the finite-dimensional setting was recently improved from $\mathcal{O}(\epsilon^{-4})$ to $\mathcal{O}(\epsilon^{-2})$ in \cite{franca2022efficient}.
The main idea behind the proof of \cite{franca2022efficient} in the many-qubit setting is two-fold: 
First, they use a Lieb-Robinson type finite information propagation bound to justify that initially local observables stay almost local at finite time. That is, given any edge $e\in E$, a region $R_e\supset e$ and any $t\ge 0$:
\begin{align}\label{finitedimLR}
    \Big\|\big(e^{-\cH^{\operatorname{qubit}}t}-e^{-\cH^{\operatorname{qubit}}_{R_e}t}\big)(\sigma_e)\Big\|_\infty\le \,c\,|R_e|\,e^{vt-\mu \operatorname{dist}(e,R_e^c)}\,,
\end{align}
where $|R_e|$ denotes the cardinality of $R_e$, $\operatorname{dist}(\cdot\,,\cdot)$ is the graph distance, and $c,v,\mu$ are parameters depending on the Hamiltonian and the graph structure. When these parameters are independent of the system size, \eqref{finitedimLR} implies that the global evolution generates a linear light-cone, which means that, for $t=\mathcal{O}(1)$, averages of the form $\tr\big[\rho e^{-\cH^{\operatorname{qubit}}t}(\sigma_e) \big]$ can be approximated by $\tr\big[\rho e^{-\cH_{R_e}^{\operatorname{qubit}}t}(\sigma_e) \big]$ to error $\epsilon_{\operatorname{LR}}$ for $\operatorname{dist}(e,R_e^c)=\mathcal{O}\big( \ln\big(\frac{|R_e|}{\epsilon_{\operatorname{LR}}}\big)\big)$. Assuming that the graph is the regular $D$-dimensional lattice $[-n,n]^D$ with side-length $2n+1$ and $D=\mathcal{O}(1)$, and choosing $R_e$ as a rectangle of side-length $2r+2$ centered around the edge $e$, we get that $r=\mathcal{O}(\operatorname{polylog}\big(\frac{1}{\epsilon_{\operatorname{LR}}}\big))$ suffices. 

Second, instead of controlling the reduced evolution on $R_e$ by the relative finite time difference as in \Cref{sec:vanillaalgo}, one can approximate it to error $\epsilon_{\operatorname{poly}}$ by its Taylor polynomials $T_{R_e,i}(t)$ of degree $i=\mathcal{O}(\ln(1/\epsilon_{\operatorname{poly}}))$. Since the time derivative of the latter at $t=0$ coincides with $\tr\big[\cH_{R}^{\operatorname{qubit}}(\rho)\sigma_e]$, one can then read off the parameters from a polynomial regression combined with another use of the univariate Markov brothers' inequality. Note that this method does not require the direct estimation of the generator by a relative finite time difference even on the region $R_e$, which leads to the improved sample complexity $N_{\operatorname{samp}}=\widetilde{\mathcal{O}}(\epsilon^{-2}\ln(|E|/\delta))$ achieved in \cite{franca2022efficient}. However, as the method requires to estimate the output state at times $t=\Theta(1)$, the total evolution time scales similarly $T_{\operatorname{evo}}=\widetilde{\mathcal{O}}(\epsilon^{-2}\ln(|E|/\delta))$, hence failing to achieve the Heisenberg limited scaling found in \cite{huang2023learning}.

The existence of local bosonic models violating LR bounds \cite{PhysRevLett.102.240501} prevents us from directly repeating the argument of \cite{franca2022efficient}.
To circumvent this obstacle, we prove a similar information propagation bound adapted to the evolution generated by the Lindbladian $\widetilde{\cL}^{(\alpha)}$ introduced in \eqref{Lindbladtotaldissiph}. In what follows, we assume that $V=[-n,n]^D$ and given $M\in\mathbb{N}$, we denote the finite rank projection on a region $R\subset V$ of vertices by $P^{(M)}_{R}\coloneqq \prod_{j\in R}P^{(M)}_{j}\equiv P$, where $P^{(M)}_{i}=\sum_{k\le M}|k\rangle\langle k|_i$ denotes the projection onto the lowest $M+1$ Fock states on site $i$. We also write 
\begin{align*}
    \widetilde{\cL}_{\mathring{R}}^{(M)}\coloneqq \sum_{i\in \mathring{R}}\cL[P(a_i^p-\alpha_i^p)P]+\sum_{e\in E_{\mathring{R}}}\cH[PH_eP]\,,
\end{align*}
where $\mathring{R}\subset R$ denotes the interior of $R$, that is, the maximal set such that $\mathring{R}\partial=\{v\in V, \operatorname{dist}(v,\mathring{R})\le 1\}\subset R$.

\begin{thm}[Dissipative bosonic Lieb-Robinson type bound, see \Cref{Prop:bosonicLR}]\label{ThmLRintro}
For $p\ge 2d+2$, there exist constants $\kappa,a,b,c>0$ depending on $p$ such that, for any $T\subset R\subset V$ with $|T|=\mathcal{O}(1)$ and $\operatorname{dist}(\partial\mathring{R},T)\ge \kappa$, and for all $\alpha\in \mathbb{R}^{2|V|}$ with $|\alpha_i|=\mathcal{O}(1)$,
\begin{align*}
&\left\|\tr_{T^c}\left[ \Big(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)\!(|\alpha\rangle\langle \alpha|) \!\right]\right\|_1\!\le\! a  |R|^{\frac{7}{2}}e^{-b
     \operatorname{dist}(\partial\mathring{R},T)^{c}} \label{LRBsimple}
\end{align*}
for $M= \mathcal{O}\big( \operatorname{dist}(\partial\mathring{R},T)^{\frac{1}{p}}\big)$.
\end{thm}
To prove \Cref{ThmLRintro}, we follow an approach pioneered by \cite{Kuwahara2020,kuwahara2022optimal} for Bose-Hubbard type models. Using \Cref{ThmLRintro}, we can adapt the method developed in \cite{franca2022efficient} to the present bosonic setting (for other ways of deriving different Lieb-Robinson type bounds for Bose-Hubbard Hamiltonians, we refer the interested reader \cite{Faupin2022}). The relevant estimators are produced via \Cref{protocollearningLRprojected-main} and then further processed for derivative estimation similar in spirit to \Cref{sec:vanillaalgo}.



We next highlight some relevant differences between our approach, based on \Cref{protocollearningLRprojected-main}, and the strategies of \Cref{sec:vanillaalgo} and \cite{franca2022efficient}.
First, we now work with projected coherent input states $\propto P|\alpha\rangle$. This is important because, in contrast to \cite{franca2022efficient}, here the step of Taylor expanding the exponential of the finite rank projected generator does not directly lead to an expression polynomial in $\alpha$ and $\beta$, due to exponential normalization factors. Acting on states $\propto P|\alpha\rangle$ instead of $|\alpha\rangle$ ensures that there is only one such normalization factor common to all terms that appear, which further is known if the values of $\alpha$ and $\beta$ are known and can thus be accounted for in the estimator.

{Second, the simple polynomial reconstruction method used in \Cref{sec:vanillaalgo} no longer is sufficient, due to its sample complexity scaling unfavorably with the polynomial degree. This is problematic for our intended application because after applying the LR bound, the Taylor expansion and correcting for exponential prefactors, we obtain a polynomial whose degree depends polylogarithmically on the inverse of the desired accuracy. Therefore, we design an improved multivariate polynomial reconstruction and derivative estimation method through an iterative application of a result from \cite{kane2017robust} for the univariate case combined with the one-dimensional Markov brothers' inequality.}

Third, changing the input states from $|\alpha\rangle$ to $\propto P|\alpha\rangle$ means that the action of the engineered dissipation on the input states is no longer trivial, in contrast to the analysis in \Cref{sec:vanillaalgo}. However, as both the dissipation and the finite rank projection are known, we can correct for the resulting additive offset at the level of the estimated derivatives without losing in accuracy.

Finally, whereas suitable higher order derivatives of the polynomial estimators were immediately related to the Hamiltonian coefficients in \Cref{sec:vanillaalgo}, the connection here is less straightforward. This complication arises because we no longer have entries of $\alpha$ and $\beta$ as eigenvalues for the input states after projecting onto the finite rank subspace. To circumvent this issue, we connect higher order derivatives of our estimators to real and imaginary parts of the matrix elements of the interaction $H_e$ in the Fock basis. Explicitly relating these matrix elements to the coefficients $\lambda_{k\ell k' \ell'}^{(e)}$, we then design an iterative procedure to recover the coefficients. 
Overall, we obtain the following reconstruction guarantee:

\begin{thm}[Refined strategy, see \Cref{refinedstrategy}]
    Combining \Cref{protocollearningLRprojected-main} with a polynomial reconstruction, we get an estimator $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ of $\lambda^{(e)}_{k\ell k'\ell'}$ such that, with probability $1-\delta$, $|\hat{\lambda}^{(e)}_{k\ell k'\ell'}-\lambda^{(e)}_{k\ell k'\ell'}|\le \epsilon$  for all $k\ell k'\ell'$, as long as the evolution time for each run scales as $t=\mathcal{O}(1)$ and the total number of samples is at least
    \begin{align*}
        N_{\operatorname{samp}}=\widetilde{\mathcal{O}}\left( \frac{1}{{\epsilon}^2}\ln\left(\frac{|E|}{\delta}\right)\right)\,.
    \end{align*}
    Our scheme uses a total evolution time of $T_{\operatorname{evo}}=\widetilde{\mathcal{O}}\big(\epsilon^{-2}\ln\big(\frac{|E|}{\delta}\big)\big)$.
\end{thm}
\begin{algorithm}[H]
  \caption{RefinedBosonicHamiltonian}\label{protocollearningLRprojected-main}
  \KwIn{ $T\in\mathbb{N}$, $0<b_1<b_2$, finite sets $\mathcal{S}_1\subset [b_1,b_2]$, $\mathcal{S}_2\subset B_{\mathbb{R}^2}$, $\mathcal{S}_4\subset B_{\mathbb{R}^4}$.}
  \KwOut{$\hat{L}_{\alpha,\beta}^{(e,T)}(t)\in\mathbb{R}$ $\forall e\in E$, $\alpha\in\mathcal{S}_{2},\,\beta\in \mathcal{S}_{4}$, $t\in\mathcal{S}_1$.}
  \BlankLine
  Partition $E$ into $\nu=\mathcal{O}(\operatorname{polylog}(\epsilon_{\mathrm{LR}}^{-1}))$ sets $E_1,\dots, E_\nu$  such that $\forall j\in[\nu], \forall e_1\neq e_2\in E_j$, $R_{e_1}\cap R_{e_2}=\emptyset$;\\
  \For{$j \in [\nu]$, $\alpha\in\mathcal{S}_{2}$, $t\in\mathcal{S}_1$}{
  \For{$i_{\alpha,t}\in[T]$}{Prepare state $|\psi_\alpha\rangle\coloneqq \tfrac{ \bigotimes_{e\in E_j}P^{(M)}_e|\alpha\rangle_{e}\otimes |0\rangle_{e^c}}{\left\|\bigotimes_{e\in E_j}P^{(M)}_e|\alpha\rangle_{e}\otimes |0\rangle_{e^c}\right\|}$\,;\\
  Run evolution generated by $\widetilde{\cL}^{(\alpha)}$ on $|\psi_\alpha\rangle$ up to time $t$;\\
  Perform heterodyne measurements on $ E_j$;\\
  $\rightarrow$ get outcomes $\beta^{(i_{\alpha,t})}_e\in \mathbb{R}^4$ $\forall e\in E_j$;}}
  \Return{$\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)\coloneqq  \frac{C_{\alpha_e}}{T}\sum_{i_{\alpha,t}=1}^T\, e^{|\beta_e^{(i_{\alpha,t})}|^2}\,1_{R_\beta}(\beta_e^{(i_{\alpha,t})})$, where $C_{\alpha_e}=\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)$ for $e=\{i,j\}$.}
\end{algorithm}

\medskip

Thus, the refined strategy improves the $\varepsilon$-dependence for both the number of samples and the total evolution time compared to the vanilla strategy. However, this comes at the price of using input states that are more difficult to prepare than coherent states. If measurements of the local number operator can be implemented, then states $\propto P\ket{\alpha}$ may be prepared with a non-zero success probability by postselecting on small enough measured local photon numbers.



\section{Justifying the modified evolution}

It remains to justify the modification of the Hamiltonian evolution by the one with added dissipation introduced in \Cref{Lindbladtotaldissiph}. 
Here, we provide a mathematical justification by extending Trotter-Kato's product formula to the case of unbounded generators, which shows that the evolution generated by the sum $\cH+\cL_V^{(\alpha,p)}$ can be approached by interlacing short periods of pure Hamiltonian evolution with periods of pure dissipation.

\begin{prop}[See \Cref{thm:trotter}]\label{introproptrotter}
	For $p\geq 2d+2$, $|\alpha_i|,t,d=\mathcal{O}(1)$, for all $i \in V$, for all states $\rho$ with finite local moments of order $4p$, and for all $n\in\N$
	\begin{equation*}
        \begin{aligned}
            \norm{\left[\left(e^{\frac{t}{n}\cH}e^{\frac{t}{n}\cL_V^{(\alpha,p)}}\right)^n-e^{t\widetilde{\cL}^{\alpha}}\right](\rho)}_1=\mathcal{O}\left(\frac{m}{\sqrt{n}}+\frac{m^2}{n}\right)\,.
        \end{aligned}
	\end{equation*}
\end{prop}
Therefore, as long as one is capable to exchange Hamiltonian and dissipative evolutions at a rate proportional to the square of the system size, the replacement of the former by their sum is justified. The proof of \Cref{introproptrotter} follows Chernoff's proof for the convergence of the Trotter-Kato product formula in the strong topology \cite{Chernoff.1968} (see also Section 4 in \cite{Moebus.2023}). 


\section{Conclusion}

We have established the first learning algorithm with rigorous guarantees for a broad class of bosonic Hamiltonians, namely those given as polynomials in annihilation and creation operators.
This extends recent progress in Hamiltonian learning to the experimentally and theoretically significant realm of CV quantum systems.
Novel LR-type bounds for dissipatively regularized evolutions of CV systems played a crucial role in our procedure and in themselves form a relevant contribution to the toolbox of CV quantum many-body physics.

A central question left open by our work is whether the total evolution time of bosonic Hamiltonian learning can be improved to achieve the Heisenberg scaling for our broad class of Hamiltonians. Recent progress in the case of many-qubit systems suggests that this will require more involved experimental setups involving long-time evolution interleaved with control operations \cite{huang2023learning, dutkiewicz2023advantage}. Whether and how such techniques can be combined with our approach based on dissipative regularization is an interesting direction for future research.

\section*{Acknowledgments}
% \mcc{Other people to thank for discussions?}
While finishing this work, we became aware of the concurrent, independent work \cite{li2023heisenberglimited}. We thank the authors of \cite{li2023heisenberglimited} for sharing a draft of their paper with us and for insightful discussions. Moreover, we would like to thank Daniel Stilck FranÃ§a, Marius Lemm, and Robert KÃ¶nig for valuable feedback and discussions on the topic.

T.M. and C.R. acknowledge the support of the Munich Center for Quantum Sciences and Technology, C.R. that of the Humboldt Foundation. A.B. is supported by the French National Research Agency in the framework of the ``France 2030â program (ANR-11-LABX-0025-01) for the LabEx PERSYVAL.
M.C.C.~is supported by a DAAD PRIME fellowship. 
The Institute for Quantum Information and Matter is an NSF Physics Frontiers Center.
A.H.W. thanks the VILLUM FONDEN
for its support with a Villum Young Investigator Grant
(Grant No. 25452) and the
QMATH Centre of Excellence (Grant No. 10059). 


\bibliographystyle{ieeetr}
\bibliography{references}


\onecolumngrid

\newpage 

\appendix
\section*{Supplementary material}



\section{Preliminaries}

\subsection{Operators and norms}

Given a separable Hilbert space $\cH$, we denote by $\cB(\cH)$ the space of bounded linear operators on $\cH$, and by $\cT_p(\cH)$, the \textit{Schatten $p$-class}, which is the Banach subspace of $\cB(\cH)$ formed by all bounded linear operators whose Schatten $p$-norm, defined as $\|X\|_{p}=\left(\tr|X|^p\right)^{1/p}$,  is finite. The identities on both $\cH$ and $\cB(\cH)$ are denoted by $I$. Henceforth, we refer to $\cT_1(\cH)$ as the set of \textit{trace class} operators. The set of quantum states (or density matrices), that is positive semi-definite operators $\rho \in \cT_1(\cH)$ of unit trace, is denoted by $\cD(\cH)$. The Schatten $1$-norm, $\|\cdot\|_1$, is the {trace norm}, and the corresponding induced distance (e.g.\ between quantum states) is the {trace distance}. The Schatten $2$-norm, $\|\cdot\|_2$, coincides with the \textit{Hilbert--Schmidt norm}.

For a pair of positive semi-definite operators, $A,B$ with domains $\dom(A),\dom(B) \subseteq \cH$, $A\geq B$ if and only if $\dom\left(A^{1/2}\right)\subseteq \dom\left(B^{1/2}\right)$ and $\left\|A^{1/2}\ket{\psi}\right\|^2\geq \left\|B^{1/2}\ket{\psi}\right\|^2$ for all $\ket{\psi}\in \dom\left(A^{1/2}\right)$. 
If $\rho$ is a quantum state with spectral decomposition $\rho=\sum_i p_i |{\phi_i}\rangle\langle \phi_i|$, and $A$ is a positive semi-definite operator, the \textit{expected value} of $A$ on $\rho$ is defined as
\begin{equation}\tr[\rho A]\coloneqq \sum_{i:\, p_i>0} p_i \left\|A^{1/2}|{\phi_i}\rangle \right\|^2 \in \RR_+\cup \{+\infty\}\, ;
\label{expected positive}
\end{equation}
here we use the convention that $\tr[\rho A]=+\infty$ if the above series diverges or if there exists an index $i$ for which $p_i>0$ and $\ket{\phi_i}\notin \dom\left(A^{1/2}\right)$. This definition can be extended to a generic densely defined self-adjoint operator $A$ on $\cH$, by considering its decomposition $A=A_+-A_-$ into positive and negative parts, with $A_\pm$ being positive semi-definite operators with mutually orthogonal supports. The operator $A$ is said to have a \textit{finite expected value on $\rho$} if $(i)$ $\ket{\phi_i}\in \dom\big(A_+^{1/2}\big)\cap \dom\big(A_-^{1/2}\big)$ for all $i$ for which $p_i>0$, and $(ii)$ the two series $\sum_i p_i \big\|A_\pm^{1/2} \ket{\phi_i}\big\|^2$ both converge. In this case, the following quantity is called the \textit{expected value} of $A$ on $\rho$:
\begin{equation}
\tr[\rho A]\coloneqq \sum_{i:\, p_i>0} p_i \left\|A_+^{1/2} \ket{\phi_i}\right\|^2 + \sum_{i:\, p_i<0} p_i \left\|A_-^{1/2} \ket{\phi_i}\right\|^2
\label{expected}
\end{equation}
Obviously, for a pair of operators $A,B$ satisfying $A\geq B$, we have that $\tr[\rho A]\geq \tr[\rho B]$.

Here we adopt standard notations from quantum information theory: given a multipartite quantum system with associated Hilbert space $\cH=\cH_A\otimes \cH_B$, and a state $\rho\in \cD(\cH)$, we denote by $\rho_A\coloneqq\tr_{\cH_B}(\rho)$ the marginal state on system $A$. Similarly, an observable, i.e. a self-adjoint operator $X$ on $\cH_A$ is identified with the observable $X_A\otimes I_B$ on the system $AB$, that is over the joint Hilbert space $\cH_A\otimes \cH_B$, where $I_B$ denotes the identity operator on $\cH_B$. A quantum channel with input system $A$ and output system $B$ is any completely positive, trace-preserving (CPTP) linear map $\cN:\cT_1(\cH_A)\to\cT_1(\cH_B)$, where $\cH_A, \cH_B$ are the Hilbert spaces corresponding to $A,B$, respectively. We denote the identity superoperator over a system $A$ by $\id_A$.  



\subsection{Phase-space formalism}\label{secphasespaceformalism}

In this paper, given $m\in\NN$, we are concerned with the Hilbert space $\cH_m\coloneqq L^2(\RR^m)$ of a so-called $m$-mode oscillator, which is the space of square-integrable functions on $\RR^m$. For more details, we refer to \cite[Section 12]{Holevo.2012}. We denote by $x_j$ and $p_j$ the canonical position and momentum operators on the $j^{\text{th}}$ mode. The $j^{\text{th}}$ creation and annihilation operators $a_j=(x_j+ip_j)/\sqrt{2}$ and $a_j^\dag=(x_j-ip_j)/\sqrt{2}$ satisfy the well-known \textit{canonical commutation relations} (CCR):
\begin{align}
\label{CCRlie}
    [a_j,a_k]=0\,,\qquad [a_j,a_k^\dagger]=\delta_{jk}I\,,
\end{align}
In terms of the vector of canonical operators $R\coloneqq (x_1,p_1,\dots,x_m,p_m)$, the above relations take the compact form $[R_j,R_{k}]=i (\Omega_{m})_{jk}$, where $\Omega_m$ denotes the $2m\times 2m$ standard symplectic form defined as
\begin{equation} \label{comm}
\Omega_{m} \coloneqq \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}^{\oplus m}\, .
\end{equation}
We will often omit the subscript $m$ if the number of modes is fixed. The \textit{total photon number} operator is defined by
\begin{equation}
N\coloneqq \sum_{j=1}^m a_j^\dag a_j =\sum_{j=1}^m N_j= \sum_{j=1}^m \frac{x_j^2 + p_j^2}{2} - \frac{m}{2}\, ,
\label{total_photon_number}
\end{equation}
where $N_j\coloneqq a_j^\dagger a_j$. The operator $N$ is diagonal in the multi-mode Fock basis $\{|\mathbf{k}\rangle\}_{k\in \mathbb{N}^m}$, with 
\begin{equation}
N\,|\mathbf{k}\rangle=\left(\sum_{i=1}^m k_i\right)\,|\mathbf{k}\rangle\,,\qquad \mathbf{k}\equiv (k_1,\dots, k_m)\,.
\end{equation}
Next next simple results will prove very useful in our derivations \cite{Gondolfetal2023}: given a real-valued measurable function $f:\mathbb{Z}\to\mathbb{R}$, and any $k\in[m]$, 
\begin{equation}\label{eq:symmetry-function}
    \begin{aligned}
        a&f(N_k+jI)&&=&&f(N_k + (j+1)I)a,&\quad\quad a^\dagger&f(N_k-jI)\,&&=&&f(N_k - (j+1)I)a^\dagger\,,\\
        &f(N_k-jI)a\,&&=&a&f(N_k - (j+1)I)\,,&\quad\quad &f(N_k+jI)a^\dagger&&=&a^\dagger&f(N_k + (j+1)I)\,,
    \end{aligned}
\end{equation}
where the identities are satisfied e.g.~on the subspace $\cH_f$ of finite linear combinations of Fock states. We will often deal with functions defined on $\mathbb{N}$ and extend them by $0$ to the entire set $\mathbb{Z}$. In what follows, we denote by $\cT_f$ the set of states which can be expressed as finite linear combinations of rank-one operators of support and range in $\cH_f$. We also use the canonical commutation relation to write $(\ad)^la^l$ as a function of $N$ : 
    \begin{align}\label{eq:aadag}
            &(\ad_k)^la_k^l=(N_k-(l-1)I)(N_k-(l-2)I)\cdots(N_k-I)N_k\equiv N_k[-l+1:0]\,,\\
            & a_k^l(\ad_k)^l=(N_k+I)(N_k+2I)\cdots(N_k+(l-1)I)(N_k+lI)\equiv N_k[1:l]\,, \label{eq:aadag2} 
    \end{align}
where given two integers $i\le j$, we denote $N_k[i:j]\coloneqq (N_k+iI)\cdots (N_k+jI)$ and where $N_k[i:j]=I$ whenever $i>j$ by convention. In what follows, we are also going to use coherent states: given $\alpha=(\alpha_1,\dots,\alpha_m)\in\mathbb{C}^m$, define
\begin{align}\label{Fockbasisdecompositioncoherent}
|\alpha\rangle \coloneqq \otimes_i|\alpha_i\rangle\,,\qquad\text{ where }\qquad |\alpha_i\rangle\coloneqq e^{-\frac{1}{2}|\alpha_i|^2}\, \sum_{n=0}^\infty\, \frac{\alpha_i^n }{\sqrt{n!}}\,|n\rangle\,.
\end{align}
The inner product between two single mode coherent states satisfies
\begin{equation*}
\langle \beta|\alpha\rangle=e^{-\frac{1}{2}(|\alpha|^2+|\beta|^2)+\alpha^\intercal\overline{\beta}}\textrm{ and } |\langle \beta|\alpha\rangle|^2=e^{-|\alpha-\beta|^2}\,.
\end{equation*}
Finally, we recall that a heterodyne measurement is simply a continuous-valued POVM whose effects correspond to rank-one projections onto coherent states, i.e.~for any Borel set $A\subset \mathbb{R}^2$,
\begin{equation*}
    M(A)=\frac{1}{\pi}\int_{A}\ketbra{\beta}{\beta}~ d^2\beta
\end{equation*}
determines a positive operator-valued measurement (POVM) because $M(\mathbb{R}^2)=I$.



\subsection{Sobolev preserving quantum Markov semigroups}

In the following, we are concerned with Markovian evolutions, that is, semigroups of completely positive, trace-preserving maps, over bosonic systems that are generated by a Hamiltonian and a dissipative part (or Lindbladian). Given a possibly unbounded operator $L$ and a self-adjoint operator $H$ that can both be expressed in terms of polynomials of the creation and annihilation operators over a suitably chosen domain, we denote the superoperators
\begin{align*}
    \cL[L]\coloneqq L(.)L^\dagger -\frac{1}{2}\,\{L^\dagger L,\,.\}\,\qquad \text{ and }\qquad \cH[H]\coloneqq -i[H,\,.\,]\,.
\end{align*}
Formally, the evolutions in the Sch\"{o}dinger picture considered in this work will be exponentials of sums of superoperators of the form of $\cL$ and $\cH$, i.e.
\begin{align*}
    \cP_t\coloneqq e^{t(\sum_i\cL[L_i]+\cH[H])}\,,\qquad t\in\mathbb{R}_+\,,
\end{align*}
for some unbounded operators $L_i$ and a self-adjoint operator $H$ on $\cH_m$. In this work, the operators $L_i$ and $H$ will be further assumed to be polynomials in the creation and annihilation operators on $\cH_m$. We also introduce a useful set of conditions on $\widetilde{\cL}\coloneqq \sum_i \cL[L_i]+\cH[H]$:
\begin{assum}[Photon-number moment stability]\label{assum}
For any $k>0$, there are $c_k,\mu_k> 0$ such that, for any state $\rho\in \cT_f$:
\begin{align}\label{eq:Ass2}
    \tr\Big[\widetilde{\cL}(\rho) \bigotimes_{j=1}^m(N_j+I)^{k}\Big] \leq - c_k\tr\Big[\rho \bigotimes_{j=1}^m(N_j+I)^{k}\Big] + \mu_k\,.
\end{align}
\end{assum}
 Under \Cref{assum}, the authors of \cite{Gondolfetal2023} showed that the operator $\widetilde{\cL}$ generates a semigroup of quantum channels $e^{t\widetilde{\cL}}$ with the further so-called \emph{Sobolev preservation property}  
\begin{equation}
    \|e^{t\widetilde{\cL}}(\rho)\|_{W^{2k,1}}\leq\max\Big\{\frac{\mu_k}{c_k},\|\rho\|_{W^{2k,1}}\Big\}.
\end{equation}
Here, the Sobolev norm is defined via
\begin{equation}\label{eq:bosonic-symmetric-weight}
    \cW^{2k}(X)\coloneqq\bigotimes_{j=1}^m(N_j+I)^{k/2} X \bigotimes_{j=1}^m(N_j+I)^{k/2}\equiv \bigotimes_{j=1}^m\cW_j^{2k}(X)
\end{equation}
by
\begin{equation}\label{eq:sobolev-space}
\|\cdot\|_{W^{2k,1}}\coloneqq\|\cW^{2k}(.)\|_{1}\,.
\end{equation}
In particular, it was shown in \cite{Gondolfetal2023} that the domain of $\cW^{2k}$ becomes a Banach space when equipped with this norm, which we will also write as $W^{2k,1}$.
More generally, for any region $R\subset [m]$, we denote the Sobolev norm over $R$ as
\begin{align}
\|\cdot\|_{W^{2k,1}(R)}\coloneqq \|\cW^{2k}_R(.)\|_1\,,\qquad \text{ where } \quad \cW^{2k}_R\coloneqq \bigotimes_{j\in R}\cW_j^{2k}\,. 
\end{align}
For later use, we also define the following operator 
\begin{equation}\label{eq:sum-sobolev-operator}
    \cW_{\Sigma}^{2k}(X)\coloneqq\sum_{j=1}^m(N_j+I)^{k/2}X(N_j+I)^{k/2}\,.
\end{equation}
For a quantum state $\rho$, the following bound follows from Young's inequality:
\begin{equation*}
    \big\|\cW^{k/m}(\rho)\big\|_1=\tr\Big[\sqrt{\rho}\bigotimes_{j=1}^m(N_j+I)^{k/(2m)}\sqrt{\rho}\Big]\leq\frac{1}{m}\tr\Big[\sqrt{\rho}\sum_{j=1}^m(N_j+I)^{k/2}\sqrt{\rho}\Big]=\frac{1}{m}\big\|\cW_{\Sigma}^{k}(\rho)\big\|_1\,.
\end{equation*}

\begin{lemma}\label{coherentSobolev}
    For any $\alpha\in\mathbb{C}$, the coherent state $|\alpha\rangle$ satisfies
    \begin{align*}
        \||\alpha\rangle\langle\alpha|\|_{W^{2k,1}}\le 2^k \left(\frac{k}{\ln(k/|\alpha|^2+1)}\right)^k
    \end{align*}
\end{lemma}

\begin{proof}
It is well-known that the state $|\alpha\rangle$ generates a Poisson distribution over the spectrum of $N$. Indeed:
\begin{align*}
    \||\alpha\rangle\langle\alpha|\|_{W^{2k,1}}&=\langle \alpha|(N+I)^k |\alpha\rangle\\
    &=e^{-|\alpha|^2} \sum_{n=0}^\infty\, \frac{|\alpha|^{2n}}{n!}\, (n+1)^k\\
    &=\sum_{l=0}^k\binom{k}{l}\,\sum_{n=0}^\infty n^l\,\frac{|\alpha|^{2n}}{n!}e^{-|\alpha|^2}\\
    &\le 2^k \left(\frac{k}{\ln(k/|\alpha|^2+1)}\right)^k\,,
\end{align*}
where we have used a simple bound on the $k$-th moment of the Poisson distribution, see e.g.~\cite{Ahle2022}.
\end{proof}

\subsubsection{Bosonic Hamiltonian}


The goal of this article is to learn Hamiltonians that can be expressed as polynomials of annihilation and creation operators with some known local structure. More precisely, we consider an $m$-mode bosonic system, where each mode is associated to a vertex $v\in V$ of a graph $G=(V,E)$, whose edges $e\in E$ model the interactions between different modes. $G$ is assumed to be known in advance. The total number of vertices is denoted by $|V|=m$ and the maximum degree of a vertex by $\gamma$. The Hamiltonian of the system is assumed to take the following form:
    \begin{align}\label{eq:Hamiltonian}
        \cH_E=-i[H_E,\,\cdot\,]\qquad\text{where}\quad H_E\coloneqq \sum_{e\in E}\,H_{e}~\, ,\text{for $e=\{i,j\}$,}\quad H_e\coloneqq \sum_{k,\ell,k',\ell'=0}^d\,\lambda^{(e)}_{k\ell k'\ell'}\,(a_i^\dagger)^k\,a_i^\ell\,(a_j^\dagger)^{k'}\,a_j^{\ell'}
    \end{align}
for some complex coefficients $\lambda^{(e)}_{k\ell k'\ell'}$ with $|\lambda^{(e)}_{k\ell k'\ell'}|\le L$. Note that we can fix $\lambda_{0000}=0$, due to gauge freedom.
By uniqueness of the decomposition and self-adjointness of each $H_e$, we deduce that $\lambda^{(e)}_{k\ell k'\ell'}=\overline{\lambda}^{(e)}_{\ell k \ell' k'}$. Here, $a_i$ denotes to the creation operator at vertex $i$, whereas $a_i^\dagger$ is its associated creation operator. 
To avoid superfluous technicalities, we further assume that $\lambda^{(e)}_{00k'\ell'}=\lambda^{(e)}_{k\ell 00}=0$. As we will see later one, this assumption will allow us to learn the coefficients exclusively from two-mode heterodyne measurements, but one can remove it by further assuming access to single-mode measurements.


In the following, given a subset $R\subset V$ of vertices, we define the associated subgraph $(R,E_R)$, where the set of edges is $E_R=E\cap (R\times R)$. On this subgraph, we denote the reduced Hamiltonian by $\cH_{E_R}=-i[H_{E_R},\,\cdot\,]$. 
The dynamics generated by $H_E$ do not satisfy \Cref{assum} in general. For this reason, the maps $e^{-itH_E}$ in general fail to be approximable by their finite Taylor sums, even on smooth inputs such as coherent states. Similarly, finite propagation bounds such as cluster expansion and Lieb-Robinson estimates are not applicable for such Hamiltonians. 
As these tools play a crucial role in the analysis of the sample optimal protocols in \cite{franca2022efficient,huang2023learning}, their proofs do not directly extend to the present continuous variable setting.

\subsubsection{Dissipative regularizers}


To overcome this issue, we introduce a class of local, Sobolev preserving dissipative generators, which we call \textit{dissipative regularizers}. 
These generators, which arise from the theory of continuous variables quantum error correcting codes \cite{Guillaud.2019,Guillaud.2023}, take the following form: Given $\alpha\in\mathbb{C}^{m}$ and $p\in\mathbb{N}$,
    \begin{align}\label{pphtondissipation}
        \cL_V^{(\alpha,p)}\coloneqq \sum_{j\in V}\cL[L_j^{(\alpha_j,p)}],\qquad \text{ where }\quad L_j^{(\alpha_j,p)}\coloneqq a_j^p-\alpha_j^p I\,.
    \end{align}
On a subgraph $(R,E_R)$, we denote the dissipation by $\cL_R^{(\alpha,p)}$ and the overall generators of the evolutions on $V$ and $R$ are denoted by 
\begin{equation}
    \widetilde{\cL}^{(\alpha)}=\cL_V^{(\alpha,p)}+\cH_E\,,\qquad\qquad\text{respectively}\qquad \qquad\widetilde{\cL}_R=\cL_R^{(\alpha,p)}+\cH_{E_R}\,.
\end{equation}
While $\widetilde{\cL}_R$ also depends on $\alpha$, we do not make this dependence explicit notationally to avoid overloading the notation.
As we will see in section \Cref{sec:decoupling}, the added dissipation permits a decoupling of the evolution over separated regions, which can thus be analyzed independently when choosing a suitable parameter vector $\alpha$ for the input coherent state as well as for the jump operators $L_i^{(\alpha_i,p)}$.


\section{Dissipative decoupling via bosonic Lieb-Robinson bounds}\label{sec:decoupling}


In this section, we show that the joint use of locality and Sobolev preservation enables a decoupling of the evolution into finite size regions of $V$, which in turn opens the possibility of using polynomial approximation methods. On a technical level, we prove a new dissipative bosonic Lieb-Robinson type bound akin to the ones recently derived in the context of the Bose-Hubbard model in \cite{Kuwahara2020}. Before this, we introduce a few standard objects that will be useful to state our bound. Given the graph $G=(V,E)$ and a non-increasing function $F:[0,\infty)\to (0,\infty)$, we denote
\begin{align*}
    &\|F\|\coloneqq \sup_{x\in V}\sum_{y\in V}F(\operatorname{dist}(x,y))\\
    &C\coloneqq \sup_{x,y\in V}\,\sum_{z\in V}\,\frac{F(\operatorname{dist}(x,z))\,F(\operatorname{dist}(y,z))}{F(\operatorname{dist}(x,y))}\,.
\end{align*}
Then, for any $\mu>0$, we define the function $F_\mu(x)=e^{-\mu x}F(x)$ with corresponding constants $\|F_\mu\|\le \|F\|$ and $C_\mu\le C$.
In particular, for the finite-size $D$-dimensional lattice $V=[-n,n]^D$, given any $\upsilon>0$, a reasonable choice for $F$ is $F(x)\coloneqq (1+x)^{-D-\upsilon}$ \cite{nachtergaele2006propagation}. In that case, the constant $C$ defined above can be controlled as $C\le 2^{D+\upsilon+1}\sum_{v\in \mathbb{Z}}\frac{1}{(1+|v|)^{D+\upsilon}}<\infty$, where $|n|$ denotes the $\ell_1$-norm of $v\in \mathbb{Z}$.

\begin{thm}[Dissipative bosonic Lieb-Robinson type bound]\label{Prop:bosonicLR}
    Given $M\in\mathbb{N}$, we denote the finite rank projection on a region $R\subset V$ of vertices as $P^{(M)}_{R}\coloneqq \prod_{j\in R}P^{(M)}_{j}\equiv P$, where $P^{(M)}_{i}=\sum_{n\le M}|n\rangle\langle n|_i$. Next, we denote $$\widetilde{\cL}_{\mathring{R}}^{(M)}\coloneqq \sum_{i\in \mathring{R}}\cL[P(a_i^p-\alpha_i^p)P]+\sum_{e\in E_{\mathring{R}}}\cH[PH_eP]\,,$$
    where $\mathring{R}\subset R$ denotes the interior of $R$, that is, the maximal set such that $\mathring{R}\partial=\{v\in V~|~ \operatorname{dist}(v,\mathring{R})\le 1\}\subset R$.
    Then, for all $d=\mathcal{O}(1)$, $k\in \mathbb{N}$ with $d<k$, $\lceil\frac{2kd}{k-d}+2\rceil\le p=\mathcal{O}(1)$, $t,L,|\alpha_i|=\mathcal{O}(1)$, and any bounded observable $O_T$ supported on a region $T\subsetneq R$,
    \begin{align*}
        &\frac{\left|\tr\left[ O_T\Big(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)(|\alpha\rangle\langle \alpha|) \right]\right|}{\|O_T\|_\infty} \le \left\{ \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(M+1)^{\frac{k}{2}-p}}\,+ \, \frac{\kappa_1\,|E_{\partial\mathring{R}}|(M+1)^p}{C_\mu}e^{\kappa_2(M+1)^pC_\mu }\sum_{x\in\partial\mathring{R}}\sum_{y\in T}F_\mu(\operatorname{dist}(x,y))\right\}\,,
    \end{align*}
    where $\partial\mathring{R}\coloneqq \mathring{R}\partial\backslash \mathring{R}$, $E_{\partial\mathring{R}}\coloneqq \{e=\{i,j\}\in E~|~\,i\in \mathring{R},j\in \partial\mathring{R} \}$, and for some constants $c_1,c_2\ge 1,\kappa_1,\kappa_2>0$ independent of $|T|$, $|R|$ and $|V|$. In particular, as long as 
    $$0 \le\frac{c_2p}{\ln\Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)}\le \frac{1}{4}\qquad \text{ and }\qquad  \operatorname{dist}(\partial\mathring{R},T)^{1-\frac{1}{4c_2p}}\ge \frac{2c_2}{\mu}\Big(\frac{\mu}{2\kappa_2 C_\mu}\Big)^{\frac{1}{4c_2 p}}\,,$$
    there is a constant $\kappa_3>0$ such that
    \begin{align}
        &\left|\tr\left[ O_T\Big(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)(|\alpha\rangle\langle \alpha|) \right]\right|\le \kappa_3\,\|O_T\|_\infty\, { |E_R||T||R|^{\frac{3}{2}}\operatorname{dist}(\partial\mathring{R},T)}\,e^{-c_2  
         \Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)^{\frac{1}{4c_2p}}   } \label{LRBsimple}
    \end{align}
    for $M +1=  \left(\frac{\mu \operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\right)^{1/p}$. Finally, the same bounds hold true after replacing the input coherent state by a projected version thereof, that is $|\alpha_i\rangle\leftarrow \frac{P_i^{(M)}|\alpha_i\rangle}{\| P_i^{(M)}|\alpha_i\rangle\|}$.
\end{thm}
To prove \Cref{Prop:bosonicLR}, we first consider the evolution of the density of bosons initially confined to a vertex $i$. In what follows, we denote 
\begin{align*}
M^{(k)}_i(t)\coloneqq \tr[e^{t\widetilde{\cL}^{(\alpha)}}(\rho) (N_i+I)^k]\,.
\end{align*}
as well as the vector $M^{(k)}(t)\coloneqq (M^{(k)}_1(t),\dots , M^{(k)}_m(t))$.

\begin{thm}\label{lemmamoments}
For any $k\in \mathbb{N}$ with $d<k$ and $p\ge \lceil\frac{2kd}{k-d}+2\rceil$, any $t\ge 0$, and all $j\in V$:
\begin{align}\label{ineqMarkovchain}
    M_j^{(k)}(t)\le 4^{p+1}e^{k \Gamma''t}\,\max_{i\in V}\big\{\|\rho_i\|_{W^{2k,1}}\big\}+\Upsilon_k(t)\,.
\end{align}
for some constant $\Gamma''$ and a function $\Upsilon_k$ defined in \eqref{defupperboundMkt} and such that, for $t,p,d,|\alpha_i|=\mathcal{O}(1)$, $\Upsilon_k(t)=k^{\mathcal{O}(k)}$, and $\Gamma''=\mathcal{O}(1)$. Moreover, for any region $R\subset V$, the Sobolev norm of the reduced state $\rho_R(t)\coloneqq \tr_{R^c}[\rho_t]$ satisfies
\begin{align}\label{ineqregion}
    \|\rho_R(t)\|_{W^{2k,1}(R)}\le \max_{j\in R}M_j^{k|R|}(t)\le 4^{p+1}e^{(k|R|) \Gamma''t}\,\max_{i\in V}\big\{\|\rho_i\|_{W^{2(k|R|),1}}\big\}+\Upsilon_{k|R|}(t)\,.
\end{align}
\end{thm}


\begin{proof}
We use \Cref{lem:assumption-hamiltonian} and \Cref{lem:assumption-dissipation}, so that, for $f(N_i)$, $\Delta$ and $\Delta_k'$ defined in \Cref{lem:assumption-dissipation}:
\begin{align}
    \widetilde{\cL}^{(\alpha)\dagger}(f(N_i))&= \sum_{e\ni i}  \cH_e^\dagger(f(N_i))+  \cL[a_i^p-\alpha_i^p]^\dagger(f(N_i))\nonumber\\
    &\le \sum_{e=\{i,j\}\ni i}\, 4Ld^6d!\,(N_i+I)^d(N_j+I)^d\,f(N_i+dI)\nonumber \\
    &\qquad \qquad -k1_{N_i>p}\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}+k p 4^{p+1}\,\Delta\left(\frac{2\times 4^{p+1}\Delta(p-2)}{p-1}\right)^{p-2}\,f(N_i)+\Delta_k'\nonumber\\
    &\overset{(1)}{\le} 8 k Ld^6d!\, \sum_{e=\{i,j\}\ni i}\,(N_i+I)^{d+k}(N_j+I)^d\nonumber \\
    &\qquad \qquad -k1_{N_i>p}\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}+k \underbrace{p 4^{p+1}\,\Delta\left(\frac{2\times 4^{p+1}\Delta(p-2)}{p-1}\right)^{p-2}}_{=:\Gamma}\,f(N_i)+\Delta_k' \label{eq:1density}
\end{align}
where $(1)$ follows from \Cref{lemmatechnicineq}, whose aim is to control powers of $N_i+I$ by $f(N_i)$, as $d \leq p$ and 
\begin{equation*}
    f(N_i+dI) \leq f(N_i + p I) \leq f(N_i) + k N_i^{k-1} \leq (k+1)N_i^k \leq 2k(N_i+I)^k.
\end{equation*}
Next, by Young's inequality, we have that 
\begin{align*}
(N_i+I)^{k+d}(N_j+I)^{d}\le \frac{k-d}{k}\,(N_i+I)^{\frac{(k+d)k}{(k-d)}}+\frac{d}{k}\,(N_j+I)^{k}\,.
\end{align*}
Plugging this bound into \eqref{eq:1density}, we find, for $d<k$
\begin{align}
    \widetilde{\cL}^{(\alpha)\dagger}(f(N_i))&\le 8kLd^6d!\gamma \,\frac{k-d}{k}(N_i+I)^{\frac{(k+d)k}{k-d}}-k1_{N_i>p}\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}+k\Gamma\,f(N_i)+\Delta_k'\nonumber\\
    &\qquad +8kLd^6d! \frac{d}{k}\,\sum_{e=\{i,j\}\ni i}\,(N_j+I)^{k}\nonumber\\
    &\overset{(2)}{\le}  8kLd^6d! \gamma\,(N_i+I)^{\frac{(k+d)k}{k-d}}-k1_{N_i>p}\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}\nonumber\\
    &\qquad +4^{p+3}kpLd^6d! \frac{d}{k}\,\sum_{e=\{i,j\}\ni i}\,f(N_j)+\gamma \frac{4^{p+3}}{3}Ld^6d!d k^{k} +\gamma 8p^kkLd^6d!\frac{d}{k} +\Delta_k' +k\Gamma\,f(N_i)\,, \nonumber \\
    &\le k \Gamma\,f(N_i)+1_{N_i>p}\left(8kLd^6d! \gamma\,(N_i+I)^{\frac{(k+d)k}{k-d}}-k\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}\right)\label{eqpresque}\\
    &\qquad +k\underbrace{p4^{p+3}Ld^7d! }_{=:\Gamma'}\,\sum_{e=\{i,j\}\ni i}\,f(N_j)+\gamma \frac{4^{p+3}}{3}Ld^6d!d k^{k} + \gamma 8p^kkLd^dd!+\Delta_k'+8k Ld^6d! \gamma\,(p+1)^{\frac{(k+d)k}{k-d}}\,,  \nonumber
\end{align}
where $(2)$ follows once again from \Cref{lemmatechnicineq}. Next, since we chose $p$ so that $k-1+p\ge \frac{(k+d)k}{k-d}+1$, a simple polynomial optimization gives (see \Cref{polymax}):
\begin{align*}
    8kLd^6d! \gamma\,(N_i+I)^{\frac{(k+d)k}{k-d}}-k\,\frac{(N_i+I)^{k-1+p}}{2\times 4^{p+1}}\le \left(\frac{16Ld^6d! \gamma \frac{(k+d)k}{k-d} 4^{p+1}}{ k-1+p}\right)^{\frac{(k+d)k}{k-d}} 8kLd^6d! \gamma =:\Delta_k''\,.
\end{align*}
Plugging this bound into \eqref{eqpresque}, we have found that 
\begin{align*}
    \widetilde{\cL}^{(\alpha)\dagger}(f(N_i))\le k\Gamma\,f(N_i)+k\Gamma'\sum_{e=\{i,j\}\ni i} f(N_j)\, + \underbrace{\gamma \frac{4^{p+3}}{3}Ld^6d!d k^{k}+ \gamma 8p^kkLd^7d!+\Delta_k'+8kLd^6d! \gamma\,(p+1)^{\frac{(k+d)k}{k-d}}+\Delta_k''}_{=:\Delta_k'''}\,.
\end{align*}
Next, we denote the vector $F^{(k)}(t)\coloneqq (F_1^{(k)}(t),\dots , F_m^{(k)}(t))^T$ of coordinates $F^{(k)}_i(t)\coloneqq \tr[e^{t\widetilde{\cL}^{(\alpha)}}(\rho)f(N_i)]$ and the $m\times m$ adjacency matrix with coordinates $[E]_{ij}=1$ if $i\sim j$, and $0$ else. Given the matrix 
$[A]_{ij}\coloneqq k\Gamma \delta_{ij}+k\Gamma'E_{ij}$ as well as the constant vector $Y_k\coloneqq \Delta_k'''(1,\dots, 1)^T$, we have proved that
\begin{align*}
    \frac{d}{dt}F^{(k)}(t)\le A F^{(k)}(t) +Y_k\,,
\end{align*}
where the inequality is meant coordinate-wise. Next, let $\gamma_i$ be the degree of the vertex $i$. Let $K_i = \Delta_k'''/(k (\Gamma +\Gamma^\prime \gamma_i ))$. Then, we can write 
\begin{align*}
    \frac{d}{dt}F_i^{(k)}(t)&\le k \Gamma (F_i^{(k)}(t) + K_i) + k \Gamma^\prime \sum_{e=\{i,j\} \ni i} (F_j^{(k)}(t)+K_i) \\
    & \le k \Gamma (F_i^{(k)}(t) + K) + k \Gamma^\prime \sum_{e=\{i,j\} \ni i} (F_j^{(k)}(t)+K) \, ,
    \end{align*}
where $K = \Delta_k'''/(k \Gamma )$ . Defining $Z_i^{(k)}(t):=F_i^{(k)}(t) + K$, we obtain
    \begin{align*}
    \frac{d}{dt}Z^{(k)}(t)&\le A Z^{(k)}(t)\, .
\end{align*}
Then, using standard results on differential inequalities and the fact that $A$ is entrywise positive (see, e.g., \cite{Sremr2006}), we conclude that
\begin{align*}
    F^{(k)}(t)\le Z^{(k)}(t)&\le e^{At}Z^{(k)}(0)\\
    &\le \|e^{At}\|_{\ell^\infty \to \ell^\infty}\,\Big\|F^{(k)}(0)+K\Big\|_{\ell^\infty}(1,\dots, 1)^T\\
    &\le e^{\|A\|_{\ell^\infty \to \ell^\infty}t}\,\Big\|F^{(k)}(0)+(k\Gamma)^{-1}Y_k\Big\|_{\ell^\infty}(1,\dots, 1)^T\\
    &\le e^{k({\Gamma + \gamma \Gamma^\prime)} t}\,\Big\|F^{(k)}(0)+(k\Gamma)^{-1}Y_k\Big\|_{\ell^\infty}(1,\dots, 1)^T\,.
\end{align*}
One final use of \Cref{lemmatechnicineq} gives, with $\Gamma''\coloneqq \Gamma+\gamma \Gamma'$, 
\begin{align}
    M^{(k)}(t)&\le 4^{p+1}p\,F^{(k)}(t)+4^{p+1}\Big( \frac{k^k}{3}+(p-1)^k\Big)(1,\dots, 1)^T\nonumber \\
    &\le \Big[4^{p+1}pe^{k\Gamma''t}\,\Big\|F^{(k)}(0)+(k\Gamma)^{-1}Y_k\Big\|_{\ell^\infty}+4^{p+1}\Big( \frac{k^k}{3}+(p-1)^k \Big)\Big](1,\dots, 1)^T\,\nonumber\\
    &\le 4^{p+1}e^{k\Gamma''t}\,\|M^{(k)}(0)\|_{\ell^\infty}+\underbrace{4^{p+1}\Big\|e^{k\Gamma''t}\frac{\Delta_k'''}{k\Gamma} + \frac{k^k}{3}+(p-1)^k \Big\|_{\ell^\infty}}_{=:\Upsilon_k(t)}\label{defupperboundMkt}
\end{align}
and the bound \eqref{ineqMarkovchain} follows after observing that $\|M^{(k)}(0)\|_{\ell^\infty}=\max_{i\in V}\big\{\|\rho_i\|_{W^{2k,1}}\big\}$. The bound \eqref{ineqregion} directly follows by a use of the arithmetic and geometric means inequality
\begin{align*}
    \prod_{i\in R}(N_i+I)^k\le \frac{1}{|R|}\sum_{i\in R}(N_i+I)^{k|R|}\,.
\end{align*}
\end{proof}

Next, given $M\in\mathbb{N}$, we recall that the finite rank projection on a region $R\subset V$ of vertices is defined as $P^{(M)}_{R}\coloneqq \prod_{j\in R}P^{(M)}_{j}$, where $P^{(M)}_{i}=\sum_{n\le M}|n\rangle\langle n|_i$. In what follows, we also denote the generator projected onto the low energy subspace in region $R$ as
\begin{align*}
    \widetilde{\cL}^{(R,M)}=\sum_{i\in R}\,\cL[P(a_i^p-\alpha_i^p)P]+\sum_{i\in R^c}\cL[a_i^p-\alpha_i^p]+\sum_{e\in E_R}\cH[P H_e P]+\sum_{e\in E\backslash E_R}\cH[H_e]\,,
\end{align*}
where $P\equiv   P^{(M)}_{R}$. Given $S\subset R$, we also denote $\widetilde{\cL}^{(R,M)}_{S}$ the restriction of the generator $\widetilde{\cL}^{(R,M)}$ onto vertices in $S$ and edges in $E_S$. In particular $\widetilde{\cL}^{(R,M)}_{S}$ is bounded. The moment bound of \Cref{lemmamoments} allows us to make the following crucial step on our way towards proving \Cref{Prop:bosonicLR}.

\begin{lemma} For all $k\in \mathbb{N}$ with $d<k$, $p=\lceil\frac{2kd}{k-d}+2\rceil$, $|\alpha_i|\le \eta,\,i\in V$ and any state $\rho$ with $\max_{i\in V}\big\{\|\rho_i\|_{W^{2k,1}}\big\}<\infty$,
\small
\begin{align*}
    %&
    \Big\|(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho)\Big\|_1%\\
    %&\qquad\qquad\qquad 
    \le  \left(6+ {8}t \left(|R|(\sqrt{p!}+\eta^p)^2+|E_R|\,L(d+1)^{4}d!\right)\right)  \,\frac{\sqrt{|R|(4^{p+1}e^{k\Gamma''t}\,\max_{i\in R}\big\{\|\rho_i\|_{W^{2k,1}}\big\}+\Upsilon_k(t))}}{(M+1)^{\frac{k}{2}-p}}\,,
\end{align*}
\normalsize
where the constant $\Gamma''$ and the function $\Upsilon_k(t)$ are those introduced in \Cref{lemmamoments}. In particular, for $\rho=|\alpha\rangle\langle\alpha|$ and for $t,d,L,|\alpha|=\mathcal{O}(1)$, there exist constants $c_1,c_2\ge 1$ such that
\begin{align}\label{eq:LRalphahalfbound}
    \Big\|(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(|\alpha\rangle\langle \alpha|)\Big\|_1\le \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(M+1)^{\frac{k}{2}-p}}\,.
\end{align}

\end{lemma}


\begin{proof}
Denoting $X_t\coloneqq (e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho) $, $P=P^{(M)}_R$ and $\overline{P}= I-P$, we have
\begin{align*}
X_t&=PX_t P+\overline{P} X_t\overline{P}+\overline{P} X_tP+PX_t\overline{P}\,.
\end{align*}
Next, we bound the last three terms in trace norm: denoting $\rho_t\coloneqq e^{t\widetilde{\cL}^{(\alpha)}}(\rho)$ and $\rho^{(R,M)}_t\coloneqq e^{t\widetilde{\cL}^{(R,M)}}(\rho)$,
\begin{align*}
    \|PX_t\overline{P}\|_1&=\Big\|P (e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho)\overline{P} \Big\|_1\\
    &\le \|P\rho_t\overline{P}\|_1+ \|P\rho^{(R,M)}_t\overline{P}\|_1 \\
     &\leq\|P\sqrt{\rho_t}\|_2\,\Big\|\sqrt{\rho_t}\overline{P}\Big\|_2+ \Big\|P\sqrt{\rho^{(R,M)}_t}\Big\|_2\Big\|\sqrt{\rho^{(R,M)}_t}\overline{P}\Big\|_2\\
    &\le \Big\|\sqrt{\rho_t}\,\overline{P}\Big\|_2 + \Big\|\sqrt{\rho^{(R,M)}_t}\overline{P}\Big\|_2 \\
    &=\sqrt{\tr\big[\overline{P} \rho_t\big]}+ \sqrt{\tr\big[\overline{P}\,\rho^{(R,M)}_t\overline{P}\,\big]}\\
    &\overset{(1)}{=}\sqrt{\tr\big[\,\overline{P} \rho_t\big]}+ \sqrt{\tr\big[\overline{P}\rho \big]}\,.
\end{align*}
Here, we used at several points that $\|AB\|_1 \leq \|A\|_2 \|B\|_2$. Furthermore, in $(1)$, we used that $\overline{P}\widetilde{\cL}^{(R,M)}(\cdot  )\overline{P}=0$ by definition of the generator. Similarly, we find that
\begin{align*}
    \|\overline{P}X_t\overline{P}\|_1,\,\|\overline{P}X_tP\|_1 \le \sqrt{\tr\big[\,\overline{P} \rho_t\big]}+ \sqrt{\tr\big[\overline{P}\rho \big]}\,.
\end{align*}
It remains to bound $\|PX_t P\|_1$. By Duhamel's formula, 
\begin{equation*}
    \begin{aligned}
        \left\|P(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho)P\right\|_1&\le \int_0^t\,\Big\|Pe^{(t-s)\widetilde{\cL}^{(R,M)}}(\widetilde{\cL}^{(R,M)}-\widetilde{\cL}^{(\alpha)})e^{s\widetilde{\cL}^{(\alpha)}}(\rho)P\Big\|_1\,ds\\
        &\overset{(2)}{=}\int_0^t\,\Big\|e^{(t-s)\widetilde{\cL}^{(R,M)}}\big(P(\widetilde{\cL}^{(R,M)}-\widetilde{\cL}^{(\alpha)})e^{s\widetilde{\cL}^{(\alpha)}}(\rho)P\big)\Big\|_1\,ds\\
        &\le \int_0^t\,\Big\|P(\widetilde{\cL}^{(R,M)}-\widetilde{\cL}^{(\alpha)})e^{s\widetilde{\cL}^{(\alpha)}}(\rho)P\Big\|_1\,ds\\
        &= \int_0^t\,\Big\|P(\widetilde{\cL}_R^{(R,M)}-\widetilde{\cL}_R)\,(\rho_s)P\Big\|_{1}\,ds\,.
    \end{aligned}
\end{equation*}
In $(2)$, we used that $P\widetilde{\cL}^{(R,M)}(\cdot )P=\widetilde{\cL}^{(R,M)}(P\cdot P)$, which again follows by the definition of the generator. Next, we simplify the integrand above, which consists of the following terms: 
\begin{equation*}
    \begin{aligned}
        P(\widetilde{\cL}_R^{(R,M)}-\widetilde{\cL}_R)\,(\rho_s)P=P\left(\sum_{i\in R}\,\cL[P(a_i^p-\alpha_i^p)P]-\sum_{i\in R}\cL[a_i^p-\alpha_i^p]+\sum_{e\in E_R}\cH[P H_e P]-\sum_{e\in E_R}\cH[H_e]\right)(\rho_s)P\, .
    \end{aligned}
\end{equation*}
First, we consider the dissipative terms, for which we use the notation $L_i=a_i^p-\alpha_i^p$. There, we have the following property:
\begin{equation*}
    \begin{aligned}
        L_iP&=(a_i^p-\alpha_i^p)P=Pa_i^pP-\alpha_i^pP=PL_i P\,,&\qquad PL_i^\dagger&=PL_i^\dagger P\, .
    \end{aligned}
\end{equation*}
This implies that
\begin{align*}
 &   P(\cL[L_i]-\cL[PL_iP])(\rho_s)P\\
&    \qquad\qquad=P(\cL[L_i]-\cL[PL_iP])(((I-P)+P)\rho_s)P\\
    &\qquad\qquad=PL_i\overline{P}\rho_sL_i^\dagger P+PL_iP\rho_sL_i^\dagger P-\frac{1}{2}P\{L_i^\dagger L_i,P\rho_s\}P-\frac{1}{2}P\{L_i^\dagger L_i,\overline{P}\rho_s\}P-P\cL[PL_iP](P\rho_s)P\\
    &\qquad\qquad=PL_i\overline{P}\rho_sL_i^\dagger P+PL_iP\rho_s\overline{P}L_i^\dagger P-\frac{1}{2}P\{L_i^\dagger L_i,P\rho_s\overline{P}\}P-\frac{1}{2}P\{L_i^\dagger L_i,\overline{P}\rho_s\}P\\
    &\qquad\qquad=PL_i\overline{P}\rho_sL_i^\dagger P+PL_iP\rho_s\overline{P}L_i^\dagger P-\frac{1}{2}P\left(P\rho_s\overline{P}L_i^\dagger P L_iP+PL_i^\dagger P L_i \overline{P}\rho_s P\right)\,.
\end{align*}
Then, we continue with the Hamiltonian difference: For each edge $e\in E_R$, we have
\begin{equation*}
    \begin{aligned}
        P(\cH[H_e]-\cH[PH_e P])(\rho_s)P&=P(H_e((I-P)+P)-PH_e P)\rho_sP-P\rho_s(((I-P)+P)H_e-PH_e P)P\\
        &=PH_e\overline{P}\rho_sP-P\rho_s\overline{P}H_eP
    \end{aligned}
\end{equation*}
Therefore,
\begin{align*}
    P\big(\widetilde{\cL}_R-\widetilde{\cL}^{(R,M)}_R\big)(\rho_s )P&=\sum_{i\in R}PL_i\overline{P}\rho_sL_i^\dagger P+PL_iP\rho_s\overline{P}L_i^\dagger P-\frac{1}{2}P\left(P\rho_s\overline{P}L_i^\dagger P L_iP+PL_i^\dagger PL_i \overline{P}\rho_s P\right)\\
    &\quad+\sum_{e\in E_R}{P}H_e \overline{P}\rho_s P-P\rho_s \overline{P} H_e {P}\,,
\end{align*}
so that
\begin{align*}
    \Big\|P(\widetilde{\cL}^{(R,M)}_R-\widetilde{\cL}_R)(\rho_s)P\Big\|_1&\le\sum_{i\in R}\left(\|PL_i\overline{P}\|_\infty\|L_i^\dagger P\|_\infty+\|P L_i^\dagger PL_i\overline{P}\|_\infty\right)\|\overline{P}\rho_s\|_1\\
    &\qquad+\sum_{i\in R}\left(\|PL_iP\|_\infty\|\overline{P}L_i^\dagger P\|_\infty+\|\overline{P}L_i^\dagger PL_iP\|_\infty\right)\|\rho_s\overline{P}\|_1\\
    &\qquad+\sum_{e\in E_R} \|PH_e\overline{P}\|_\infty \,\|\overline{P}\rho_s\|_1+\|\overline{P}H_eP\|_\infty\, \|\rho_s\overline{P}\|_1\\
    &\le\Biggl(\sum_{i\in R}\left(\|PL_i\overline{P}\|_\infty\|L_i^\dagger P\|_\infty+\|P L_i^\dagger P L_i\overline{P}\|_\infty+\|PL_iP\|_\infty\|\overline{P}L_i^\dagger P\|_\infty+\|\overline{P}L_i^\dagger PL_iP\|_\infty\right)\\
    &\qquad+\sum_{e\in E_R}\left( \|PH_e\overline{P}\|_\infty +\|\overline{P}H_eP\|_\infty\right)\Biggr)\sqrt{\tr\big[\rho_s \overline{P}\big]}\\
    &\le\Biggl(4\sum_{i\in R}\|PL_i\|_\infty\|L_i^\dagger P\|_\infty+\sum_{e\in E_R} \|PH_e\overline{P}\|_\infty +\|\overline{P}H_eP\|_\infty\Biggr)\sqrt{\tr\big[\rho_s \overline{P}\big]}\,,
\end{align*}
where we have used that
\begin{equation*}
    \|\overline{P}\rho_s\|_1\leq\|\overline{P}\sqrt{\rho_s}\|_2 \|\sqrt{\rho_s}\|_2=\sqrt{\tr\big[\rho_s \overline{P}\big]}\quad\text{and}\quad
    \|\rho_s\overline{P}\|_1\leq\|\sqrt{\rho_s}\overline{P}\|_2 \|\sqrt{\rho_s}\|_2=\sqrt{\tr\big[\rho_s \overline{P}\big]}\,.
\end{equation*}
All in all, we have found that 
\begin{equation}\label{almosteqfinish}
    \begin{aligned}
        \Big\|(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho)\Big\|_1&\le \Biggl(3+ t \Bigl(4\sum_{i\in R}\|PL_i\|_\infty\|L_i^\dagger P\|_\infty+\sum_{e\in E_R} \|PH_e\overline{P}\|_\infty +\|\overline{P}H_eP\|_\infty\Bigr)\Biggr)
        \cdot 2\,\max_{s\in[0,t]}\,\sqrt{\tr\big[\,\overline{P} \rho_s\big]}\,.
    \end{aligned}
\end{equation}
Next, with slight abuse of notation, we denote by $N_i(t)$ the random variable corresponding to the measurement of the local number operator $N_i$, i.e.~$\mathbb{P}(N_i(t)=n)=\tr[\rho_t\ket{n}\bra{n}_i]$. By Markov's inequality, we get for $k\in \mathbb{N}$ with $d<k$ and $p> \frac{k-d+2kd}{k-d}$,
\begin{align}\label{Markoveq}
    \mathbb{P}_{i,> M}^{(t)}\coloneqq \mathbb{P}\Big(N_i(t)> M\Big)=\mathbb{P}\Big((N_i(t)+1)^k> (M+1)^k\Big)\le \frac{M_i^{(k)}(t)}{(M+1)^k}\le \frac{4^{p+1}e^{k\Gamma'' t}\,\max_{i \in V}\|\rho_i\|_{W^{2k,1}}+\Upsilon_k(t)}{(M+1)^k}\,,
\end{align}
where the last inequality follows from \Cref{lemmamoments}. Moreover, 
\begin{align}\label{unionbound}
\tr\big[\rho_t \overline{P}\big]=1-\tr\big[\rho_t P\big]=1-\mathbb{P}\Big(\forall i\in R,\,N_i(t)\le M\Big)=\mathbb{P}\Big(\exists i\in R:\,N_i(t)> M\Big)\le |R|\,\max_{i\in R}\mathbb{P}\Big(N_i(t)> M\Big)\,.
\end{align}
Next, we further bound the quantities appearing on the right-hand side of \Cref{almosteqfinish,Markoveq}. Using \Cref{finite-bounds-generator}, we can bound the terms in \eqref{almosteqfinish} as follows:
\begin{align}
    &\|P H_e\overline{P}\|_\infty+\|\overline{P}H_eP\|_\infty\le 2L(d+1)^{4}(M+1)^{2d}d!\label{PHPbar} \\
    &    \|PL_i\|_\infty\|L_i^\dagger P\|_\infty\leq\left(\sqrt{p!}(M+1)^{p/2}+|\alpha_i|^p\right)^2\,. \label{PLandLP}
\end{align}
Inserting \eqref{Markoveq}, \eqref{unionbound}, \eqref{PHPbar} and \eqref{PLandLP}  into \eqref{almosteqfinish}, and using that $2d \leq p$ and that $|\alpha_i|\le \eta$ for all $i$, we end up with
\begin{align*}
   & \Big\|(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(R,M)}})(\rho)\Big\|_1\\
    &\qquad \qquad \le  \left(6+ {8}t \left(|R|(\sqrt{p!}+\eta^p)^2+|E_R|\,L(d+1)^{4}d!\right)\right)  \,\frac{\sqrt{|R|(4^{p+1}e^{k\Gamma''t}\,\max_{i\in V}\big\{\|\rho_i\|_{W^{2k,1}}\big\}+\Upsilon_k(t))}}{(M+1)^{\frac{k}{2}-p}}\,,
\end{align*}
In particular, using the moment bound of \Cref{coherentSobolev} for $\rho=|\alpha\rangle\langle \alpha|$ with $t,d,L,|\alpha|=\mathcal{O}(1)$, we find constants $c_1,c_2\ge 1$ such that
\begin{align*}
    \Big\|(e^{t\widetilde{\cL}}-e^{t\widetilde{\cL}^{(R,M)}})(|\alpha\rangle\langle \alpha|)\Big\|_1\le \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(M+1)^{\frac{k}{2}-p}}\,.
\end{align*}

        
\end{proof}



We are now ready to prove \Cref{Prop:bosonicLR}.




\begin{proof}[Proof of \Cref{Prop:bosonicLR}]
We choose $\mathring{R}\subset R$ as the interior of $R$, that is, the maximal set such that $\mathring{R}\partial=\{v\in V, \operatorname{dist}(v,\mathring{R})\le 1\}\subset R$. By Duhamel's identity once again, recalling that $\widetilde{\cL}_{\mathring{R}}^{(M)}\coloneqq \sum_{i\in \mathring{R}}\cL[P(a_i^p-\alpha_i^p)P]+\sum_{e\in E_{\mathring{R}}}\cH[PH_eP]$ and $\rho_0=|\alpha\rangle\langle \alpha|$, we have
\begin{align}
\tr\Big[O_T\,\Big(e^{t\widetilde{\cL}^{(R,M)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)(\rho_0)\Big]&=\int_0^t\,\tr\Big[ O_T\,e^{s\widetilde{\cL}^{(M)}_{\mathring{R}}}\big(\widetilde{\cL}^{(R,M)}-\widetilde{\cL}^{(M)}_{\mathring{R}}\big)e^{(t-s)\widetilde{\cL}^{(R,M)}}(\rho_0) \Big]\,ds\nonumber \\
&\le \int_0^t\,\Big\| \big(\widetilde{\cL}^{(R,M)\dagger}-\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}\big)e^{s\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}}(O_T)\Big\|_\infty\,ds\nonumber \\
&\le t\,\max_{s\in [0,t]}\,\Big\| \big(\widetilde{\cL}^{(R,M)\dagger}-\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}\big)e^{s\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}}(O_T)\Big\|_\infty\nonumber \\
&= t\,\max_{s\in [0,t]}\,\Big\| \overline{{\cL}}^{(R,M)\dagger}_{\partial \mathring{R}}e^{s\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}}(O_T)\Big\|_\infty\,,\label{eq:Duhamelgood}
\end{align}
with $\partial\mathring{R}\coloneqq \mathring{R}\partial\backslash \mathring{R}$, $E_{\partial\mathring{R}}\coloneqq \{e=\{i,j\}\in E|\,i\in \mathring{R},j\in \partial\mathring{R} \}$, and $\overline{{\cL}}^{(R,M)}_{\partial \mathring{R}}\coloneqq \sum_{e\in E_{\partial \mathring{R}}}\cH[PH_eP]$, and where the last identity follows from the fact that $e^{s\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}}(O_T)$ is supported on $\mathring{R}$ for all $s\ge 0$. 
Since we have now reduced our problem to a finite-dimensional setting, we can use the dissipative Lieb-Robinson bound in \cite[Theorem 2]{nachtergaele2011lieb}. We have that
\begin{align}
&\Big\| \overline{{\cL}}^{(R,M)}_{\partial \mathring{R}}e^{s\widetilde{\cL}^{(M)\dagger}_{\mathring{R}}}(O_T)\Big\|_\infty\le \frac{\| \overline{{\cL}}^{(R,M)\dagger }_{\partial \mathring{R}}\|_{\operatorname{cb}}\|O_T\|_\infty}{C_\mu}\,e^{\|\Psi_{\mathring{R}}\|_{\mu}C_\mu\,s}\sum_{x\in \partial \mathring{R}}\sum_{y\in T}F_{\mu}(\operatorname{dist}(x,y))\,,\label{finiteLRbound}
\end{align}
where (see Assumption 1 in \cite{nachtergaele2011lieb} for details)
\begin{align*}
\big\|\Psi_{\mathring{R}}\big\|_\mu\coloneqq  \max\left\{\sup_{e\in E_{\mathring{R}}}\frac{\|\cH[PH_eP]\|_{\operatorname{cb}}}{F_\mu(1)}\,,~ \sup_{i\in V}\,\frac{\|\cL[P(a_i^p-\alpha_i^p)P]\|_{\operatorname{cb}}}{F_\mu(0)}\right\}\,.
\end{align*}
Next, we aim at bounding the completely bounded norm above. We have
\begin{align*}
\big\| \overline{{\cL}}^{(R,M)\dagger }_{\partial \mathring{R}}\big\|_{\operatorname{cb}}&\le 2\sum_{e\in E_{\partial \mathring{R}}}\|PH_eP\|_\infty\\
&\le 2|E_{\partial\mathring{R}}|\, L(d+1)^4d!(M+1)^{2d}\\
&\le 2|E_{\partial\mathring{R}}|\, L(d+1)^4d!(M+1)^{p}\\
&=: |E_{\partial\mathring{R}}| \kappa_1\,(M+1)^p\,.
\end{align*}
Next, the norm $\big\|\Psi_{\mathring{R}}\big\|_\mu$ can be bounded using \Cref{finite-bounds-generator} as follows:
\begin{align*}
    \big\|\Psi_{\mathring{R}}\big\|_\mu&=  \max\left\{\sup_{e\in E_{\mathring{R}}}\frac{\|\cH[PH_eP]\|_{\operatorname{cb}}}{F_\mu(1)}\,,~ \sup_{i\in V}\,\frac{\|\cL[P(a_i^p-\alpha_i^p)P]\|_{\operatorname{cb}}}{F_\mu(0)}\right\}\\
    &\le \max\left\{\frac{2L(d+1)^4d!(M+1)^{2d}}{F_\mu(1)}\,,~ {4\frac{ p!(M+1)^p+|\alpha_i|^{2p}}{F_\mu(0)}}\right\}\\
    &\le \frac{2L(d+1)^4d!(M+1)^{2d}}{F_\mu(1)}+ {4\frac{ p!(M+1)^p+\max_i|\alpha_i|^{2p}}{F_\mu(0)}}\\
    &\le (M+1)^p\,\left(\frac{2L(d+1)^4d!}{F_\mu(1)}+{ 4\frac{p!+\max_i|\alpha_i|^{2p}}{F_\mu(0)}}\right)\\
    &=:\kappa_2\,(M+1)^p \,.
\end{align*}
Combining these with \eqref{finiteLRbound}, \eqref{eq:Duhamelgood} and \eqref{eq:LRalphahalfbound}, we get
\begin{align*}
    &\left|\tr\left[ O_T\Big(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)(|\alpha\rangle\langle \alpha|) \right]\right|\\
    &\qquad\qquad\qquad \qquad \le \|O_T\|_\infty\,\left\{ \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(M+1)^{\frac{k}{2}-p}}\,+t \, \frac{\kappa_1\,|E_{\partial\mathring{R}}|(M+1)^p}{C_\mu}e^{\kappa_2(M+1)^pC_\mu t}\sum_{x\in\partial\mathring{R}}\sum_{y\in T}F_\mu(\operatorname{dist}(x,y))\right\}\,.
\end{align*}
Next, we can further get the bound
\begin{align*}
    &\left|\tr\left[ O_T\Big(e^{t\widetilde{\cL}^{(\alpha)}}-e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}\Big)(|\alpha\rangle\langle \alpha|) \right]\right|\\
    &\qquad\qquad  \le \|O_T\|_\infty\,\underbrace{\left\{ \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(M+1)^{\frac{k}{2}-p}}\,+t \, \frac{\kappa_1\,|E_{\partial\mathring{R}}|(M+1)^p}{C_\mu}e^{\kappa_2(M+1)^pC_\mu t}\min\{|\partial\mathring{R}|,|T|\}e^{-\mu \operatorname{dist}(\partial\mathring{R},T)}\|F\|\right\}}_I\,.
\end{align*}
In order to bound the quantity $I$, we will now concretely choose $M$ and $k$. For simplicity, let $t \leq 1$. We start by choosing 
\begin{equation*}
    M +1=  \left(\frac{\mu \operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\right)^{1/p} \, .
\end{equation*}
Then, 
\begin{equation*}
    I \leq \frac{c_1\,|E_R|\,|R|^{\frac{3}{2}}\, k^{c_2k}}{(\tfrac{\mu}{2\kappa_2 C_\mu}\operatorname{dist}(\partial\mathring{R},T))^{\frac{k}{2p}-1}}\,+\, \frac{\kappa_1\,|E_{\partial\mathring{R}}|\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2\,C_\mu^2}|T|e^{-\frac{\mu}{2} \operatorname{dist}(\partial\mathring{R},T)}\|F\|\, 
\end{equation*}
Next, we choose, for any $\varphi>0$,
\begin{equation*}
    k = (M+1)^{\frac{\frac{1}{2}-\varphi}{c_2}}\,.
\end{equation*}
Then, 
\begin{align*}
  I&\le \frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu }\left(c_1|E_R||R|^{\frac{3}{2}}\left(\frac{1}{(M+1)^{\varphi}}\right)^k+\frac{\kappa_1|E_{\partial\mathring{R}}||T|\|F\|}{C_\mu}e^{-\frac{\mu}{2}\operatorname{dist}(\partial\mathring{R},T)}\right)\\
   &= \frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu }\left(c_1|E_R||R|^{\frac{3}{2}}e^{-\frac{\varphi}{p}  
     \Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)^{\frac{1-2\varphi}{2c_2p}}\ln\Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)   }+\frac{\kappa_1|E_{\partial\mathring{R}}||T|\|F\|}{C_\mu}e^{-\frac{\mu}{2}\operatorname{dist}(\partial\mathring{R},T)}\right)\,.
\end{align*}
Optimizing over $\varphi$, we then choose $\varphi\coloneqq \frac{c_2p}{\ln\big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\big)}$, which we assume to be non-negative, and hence the bound becomes
\begin{align*}
    I\le  \frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu }\left(c_1|E_R||R|^{\frac{3}{2}}e^{-c _2 
     \Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)^{\frac{1-2\varphi}{2c_2p}}   }+\frac{\kappa_1|E_{\partial\mathring{R}}||T|\|F\|}{C_\mu}e^{-\frac{\mu}{2}\operatorname{dist}(\partial\mathring{R},T)}\right)\,.
\end{align*}
Further, assuming that $\varphi \le 1/4$, we finally get the bound
\begin{align*}
I&\le  \frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu }\left(c_1|E_R||R|^{\frac{3}{2}}e^{-c_2  
     \Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)^{\frac{1}{4c_2p}}   }+\frac{\kappa_1|E_{\partial\mathring{R}}||T|\|F\|}{C_\mu}e^{-\frac{\mu}{2}\operatorname{dist}(\partial\mathring{R},T)}\right)\\
     &\le  \frac{\mu \big(c_1+\frac{\kappa_1}{C_\mu}\big)|E_R||T||R|^{\frac{3}{2}}\|F\|\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu }\left(\,e^{-c_2  
     \Big(\frac{\mu\operatorname{dist}(\partial\mathring{R},T)}{2\kappa_2C_\mu}\Big)^{\frac{1}{4c_2p}}   }+e^{-\frac{\mu}{2}\operatorname{dist}(\partial\mathring{R},T)}\right) \,.
\end{align*}
\eqref{LRBsimple} finally follows from the condition that $\operatorname{dist}(\partial\mathring{R},T)^{1-\frac{1}{4c_2p}}\ge \frac{2c_2}{\mu}\big(\frac{\mu}{2\kappa_2 C_\mu}\big)^{\frac{1}{4c_2 p}}$, since $c_2\ge 1$, and choosing $\kappa_3\coloneqq \frac{\mu \big(c_1+\frac{\kappa_1}{C_\mu}\big)\|F\|}{\kappa_2C_\mu }$. Finally, the fact that the same bounds hold true after replacing the input coherent state by a projected version thereof, that is $|\alpha_i\rangle\leftarrow P_i^{(M)}|\alpha_i\rangle$, is simply justified as follows: for a single mode, we can explicitly compute the normalization factor to be 
\begin{align*}
    \big\| P_i^{(M)} \ket{\alpha_i}\big\| 
    = \sqrt{e^{-|\alpha_i|^2} \sum_{k=0}^{M} \frac{|\alpha_i|^{2k}}{k!}} \, .
\end{align*}
In particular, if we focus on $|\alpha_i|\leq 1$, then we have $\| P_i^{(M)} \ket{\alpha_i}\|^{-1} \leq e^{1/2}$.
Third, as $P_i^{(M)}$ is a finite rank projection onto bounded photon numbers, we see that for any single mode:
\begin{align*}
    \left\|\frac{P_i^{(M)}\ketbra{\alpha_i}{\alpha_i}P_i^{(M)}}{\| P_i^{(M)} \ket{\alpha_i}\|^2} \right\|_{W^{2k,1}}
    \leq \| P_i^{(M)}\ket{\alpha_i}\|^{-2} \|\ketbra{\alpha_i}{\alpha_i} \|_{W^{2k,1}}
    \leq e\cdot \|\ketbra{\alpha_i}{\alpha_i} \|_{W^{2k,1}}\, ,
\end{align*}
where we again assumed $|\alpha_i|\leq 1$. Thus, up to a constant prefactor, we can control the Sobolev norm of these normalized projected coherent state inputs by those of regular coherent state inputs, which we can in turn control via \Cref{coherentSobolev}. This conclude our proof given that a Sobolev bound of the above kind is the only property needed on the input state for the result to hold.
\end{proof}

\section{Technical lemmas}\label{sec:technical-lemmas}
In this section, we prove the main ingredients for the proof of \Cref{lemmamoments}. 

\begin{lemma}\label{polymax}
    For $\alpha,\beta > 0$ and $a>b>0$, the polynomial $p(X)=-\alpha X^a+\beta X^{b}$ satisfies
    \begin{align*}
        \sup_{X \geq 0}\,p(X)\le  \left(\frac{\beta b}{\alpha a}\right)^{\frac{b}{a-b}}\,\beta \,.
    \end{align*}
\end{lemma}
\begin{proof}
    Searching for zeroes of the derivative of $p$, we see that  it attains its maximum on $[0,\infty)$ at $X_0 = \left(\tfrac{\beta b}{\alpha a}\right)^{1/(a-b)}$. Plugging this into the polynomial, we get $p(X_0) = \left(\tfrac{\beta b}{\alpha a}\right)^{\frac{b}{a-b}}\,\beta\, \left(1 - \tfrac{b}{a}\right)\le \left(\tfrac{\beta b}{\alpha a}\right)^{\frac{b}{a-b}}\,\beta.$
\end{proof}


\begin{lemma}\label{lem:two-point-hamiltonian-bound}
    Let $\ell_1,\ell_2,k_1,k_2\in\N$ with $\min\{\ell_1,k_1\}=\min\{\ell_2,k_2\}=0$, $z\in\C$, and $h:\mathbb{N}^2\rightarrow\mathbb{R}$ a positive function that is increasing in each of its variables. Then,
    \begin{equation*}
        \begin{aligned}
            za_1^{\ell_1}a_2^{\ell_2}h(N_1,N_2)(\ad_1)^{k_1}(\ad_2)^{k_2}+\overline{z}a_1^{k_1}a_2^{k_2}h(N_1,N_2)(\ad_1)^{\ell_1}(\ad_2)^{\ell_2}\leq (M_1+1)(M_2+1)|z|\widetilde{h}_{M_1,M_2}(N_1+M_1I,N_2+M_2I)\,,
        \end{aligned}
    \end{equation*}
    where $M_{1}\coloneqq\max\{\ell_{1},k_{1}\}$, $M_2\coloneqq \max\{\ell_2,k_2\}$ and $$\widetilde{h}_{M_1,M_2}(n_1,n_2)={\prod_{j=n_1-M_1+1}^{n_1} \sqrt{j}\prod_{i=n_2-M_2+1}^{n_2}\sqrt{i}} \,\,\,h(n_1,n_2)\,1_{n_1\ge M_1}1_{n_2\ge M_2}\,,$$
    where we introduced the notation $1_{x\ge M}$ for the indicator function on the set $\{x:\,x\ge M\}$, and where by convention we take $\prod_{i=a}^b=1$ when $a>b$.
\end{lemma}
\begin{proof}
    We define $K\coloneqq za_1^{\ell_1}a_2^{\ell_2}h(N_1,N_2)(\ad_1)^{k_1}(\ad_2)^{k_2}+\overline{z}a_1^{k_1}a_2^{k_2}h(N_1,N_2)(\ad_1)^{\ell_1}(\ad_2)^{\ell_2}$ and represent it in the $2$-mode Fock basis:
    \begin{equation*}
        \begin{aligned}
            K&=\sum_{n_1,n_2}\,h(n_1,n_2)\,  \big( z a_1^{\ell_1}a_2^{\ell_2}\,|n_1,n_2\rangle\langle n_1,n_2|(a_1^\dagger)^{k_1}(a_2^\dagger)^{k_2}+\overline{z} a_1^{k_1}a_2^{k_2}|n_1,n_2\rangle\langle n_1,n_2|(a_1^\dagger)^{\ell_1}(a_2^\dagger)^{\ell_2} \big)\\
            &=\sum_{\substack{n_1\geq M_1\\n_2\geq M_2}} g_{\substack{\ell_1,\ell_2\\k_1,k_2}}(n_1,n_2)\left(z\ket{n_1-\ell_1,n_2-\ell_2}\bra{n_1-k_1,n_2-k_2}+\overline{z}\ket{n_1-k_1,n_2-k_2}\bra{n_1-\ell_1,n_2-\ell_2}\right)\,, 
        \end{aligned}
    \end{equation*}
    where 
    \begin{equation*}
        \begin{aligned}
            g_{\substack{\ell_1,\ell_2\\k_1,k_2}}(n_1,n_2)&\coloneqq h(n_1,n_2) \sqrt{n_1(n_1-1)\dots(n_1-\ell_1+1)n_1\dots (n_1-k_1+1)n_2\dots (n_2-\ell_2+1)n_2\dots (n_2-k_2+1)}\,. 
        \end{aligned}
    \end{equation*}
By assumption, since $\min\{\ell_1,k_1\}=\min\{\ell_2,k_2\}=0$, we have that $g_{\substack{\ell_1,\ell_2\\k_1,k_2}}(n_1,n_2)=\widetilde{h}_{M_1,M_2}(n_1,n_2)$ for $n_1\ge M_1$, $n_2\ge M_2$, and 
\begin{align*}
K=\sum_{n_1,n_2\in \NN}\widetilde{h}_{M_1,M_2}(n_1+M_1,n_2+M_2)\left(z\ket{n_1+k_1,n_2+k_2}\bra{n_1+\ell_1,n_2+\ell_2}+\overline{z}\ket{n_1+\ell_1,n_2+\ell_2}\bra{n_1+k_1,n_2+k_2}\right)\,.
\end{align*}
   Next, we split the above sum into sums of matrix blocks as depicted in the following matrix in the Fock basis:
    \[\left(\begin{array}{*{6}{c}}
        \tikzmark{left}0 &0 &\ast &0 &0 &0 \\               
        0 &0 &0 &\ast &0 &0 \\      
        \ast &0 &0\tikzmark{right} &0 &\ast &0 \\\DrawBox[thick]
        0 &\ast &0 &\tikzmark{left}0 &0 &\ast \\
        0 &0 &\ast &0 &0 &0 \\
        0 &0 &0 &\ast &0 &0\tikzmark{right} \\\DrawBox[thick]
    \end{array}\right)\,.\]
    For fixed $k_1,k_2,\ell_1,\ell_2$, and $n_{1},n_2\in\N$, the operator 
    \begin{equation}\label{eq:block-matrix}
        \begin{aligned}
z\ket{n_1+k_1,n_2+k_2}\bra{n_1+\ell_1,n_2+\ell_2}+\overline{z}\ket{n_1+\ell_1,n_2+\ell_2}\bra{n_1+k_1,n_2+k_2}
        \end{aligned}
    \end{equation}
    is of the form 
    \begin{equation}\label{eq:rank2}
        z\ket{e_{M-1}}\bra{e_0}+\overline{z}\ket{e_0}\bra{e_{M-1}}\,,
    \end{equation}
    where $\{e_j\}_{j=0}^{M-1}$ is an orthonormal basis of an $M$ dimensional Hilbert space and $M=(M_1+1)(M_2+1)$. The space is spanned by
    \begin{equation*}
        \{\ket{n_1+i, n_2+j}\}_{i \in \{0, \ldots, M_1\},j \in \{0, \ldots, M_2\}  } \, .
    \end{equation*}
    Then, the above operator has the following eigenvalues and eigenvectors:
    \begin{center}
        \begin{tabular}{ c c c }
            Eigenvalue & Eigenvectors & Multiplicity \\[0.5ex]\hline
            $0$ & $\{e_1,...,e_{M-2}\}$ & $M-2$ \\  
            $|z|$ & $|z|\ket{e_0}+z\ket{e_{M-1}}$ & $1$\\
            $-|z|$ & $|z|\ket{e_0}-z\ket{e_{M-1}}$ & $1$
        \end{tabular}.
    \end{center}
    The matrix given in \Cref{eq:block-matrix} can therefore be upper bounded as
    \begin{align*}
 z\ket{e_{M-1}}\bra{e_0}+\overline{z}\ket{e_0}\bra{e_{M-1}}\le |z|\,    \sum_{i=0}^{M-1}|e_i\rangle\langle e_i|\,.
    \end{align*}
    Since each of the projections upper bounding any operator of the form \eqref{eq:rank2} appears at most $M$ times in the sum, we find
    \begin{equation*}
        \begin{aligned}
           K&\le \sum_{n_1,n_2\in \NN}\widetilde{h}_{M_1,M_2}(n_1+M_1,n_2+M_2)|z|\sum_{i=0}^{M_1}\sum_{j=0}^{M_2}|n_1+i,n_2+j\rangle\langle n_1+i,n_2+j|\\
            &\leq (M_1+1)(M_2+1)\,|z|\,\widetilde{h}_{M_1,M_2}(N_1+M_1I,N_2+M_2I)\,,
        \end{aligned}
    \end{equation*}
    where we also used that $\widetilde{h}$ is increasing in each of its variables.
\end{proof}
Next, we apply \Cref{lem:two-point-hamiltonian-bound} in order to derive an upper bound on the contribution of the Hamiltonian $H=\sum_e H_e$ introduced in \eqref{eq:Hamiltonian} to \Cref{eq:Ass2}. 
We recall that for each edge $e=\{i,j\}\in E$:
    \begin{align}\label{eq:Hamiltonian1}
        H_e\coloneqq \sum_{k,\ell,k',\ell'=0}^d\,\lambda^{(e)}_{k\ell k'\ell'}\,(a_i^\dagger)^k\,a_i^\ell\,(a_j^\dagger)^{k'}\,a_j^{\ell'}\,.
    \end{align}
for some complex coefficients $\lambda^{(e)}_{k\ell k'\ell'}$ with $|\lambda^{(e)}_{k\ell k'\ell'}|\le L$. 


\begin{lemma}\label{lem:assumption-hamiltonian}
    For any increasing, product function $f:\mathbb{N}^m\to\mathbb{R}_+$ of the form $f(n_1,\dots, n_m)=f_1(n_1)\dots f_m(n_m)$ with $f_j:\N\rightarrow\R_{+}$, any state $\rho\in\cT_f$, and any edge $e=\{i,j\}$ with $i<j$:
    \begin{align*}
        -i\tr\big[[H_e,\rho]\,f(N_1,\dots, N_m)\big]\leq 4L\,d^6\,d!\, \tr\big[\rho\,(N_i+I)^{d}\,(N_j+I)^d\,f(N_i+d,N_j+d)\big]\,,
    \end{align*}
    where we denoted $f(N_i+d,N_j+d)\equiv f(N_1,\dots,N_{i-1},N_{i}+dI,N_{i+1},\dots,N_{j-1},N_j+dI,N_{j+1},\dots, N_m)$, by slight abuse of notation.
\end{lemma}

\begin{proof}
    For $d=0$ the bound is trivial, so assume $d\geq1$. Without loss of generality, we choose $\{i,j\}=\{1,2\}$ and denote $N=(N_1,...,N_m)$. 
    Then, since $\lambda^{(e)}_{k\ell k'\ell'}=\overline{\lambda}^{(e)}_{\ell k \ell'k'}$ for all $k,\ell,k',\ell'$ and using \eqref{eq:aadag}, we have
    \begin{equation*}
        \begin{aligned}
            i[H_e,f(N)]&=i\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\left\{\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}+h.c.\right)f(N)\right.\\
            &\hspace{25ex}-\left.f(N)\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}+h.c.\right)\right\}\\
            &+i\sum_{\substack{0\leq k\leq\ell\leq d\\ d\ge k'> \ell'\ge 0}}\left\{\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,(a_2^{\dagger})^{k'-\ell'}N_2[1-\ell':0]\,+h.c.\right)f(N)\right.\\
            &\hspace{25ex}-\left.f(N)\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,(a_2^{\dagger})^{k'-\ell'}\,N_2[1-\ell':0]+h.c.\right)\right\}\\
            &\equiv \Delta_1+\Delta_2\,.
        \end{aligned}
    \end{equation*}
    where by $h.c.$ we mean the hermitian conjugate. Next, we examine the operator $\Delta_1$, since the analysis of $\Delta_2$ turns out to be identical. In what follows, we also write $f(a,b)\coloneqq f(a,b,N_3,\dots ,N_m)$ by slight abuse of notation. Then,
    \begin{align*}
\Delta_1&=i\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\left\{\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}+h.c.\right)f(N)\right.\\
            &\hspace{20ex}-\left.f(N)\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}+h.c.\right)\right\}\\
&\overset{(1)}{=}i\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\left\{\left(\lambda^{(e)}_{k\ell k'\ell'}\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}f(N)-h.c.\right)\right.\\
            &\hspace{20ex}+\left.\left(-\lambda^{(e)}_{k\ell k'\ell'}f(N)\,N_1[1-k:0]\,a_1^{\ell-k}\,N_2[1-k':0]\,a_2^{\ell'-k'}+h.c.\right)\right\}\\
            &\overset{(2)}{=}i\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\left\{\left(\lambda^{(e)}_{k\ell k'\ell'}\,a_1^{\ell-k}a_2^{\ell'-k'}N_1[1-\ell:k-\ell]\,N_2[1-\ell':k'-\ell']\,f(N)-h.c.\right)\right.\\
            &\hspace{20ex}+\left.\left(-\lambda^{(e)}_{k\ell k'\ell'}a_1^{\ell-k}a_2^{\ell'-k'}f(N_1+k-\ell,N_2+k'-\ell')\,N_1[1-\ell:k-\ell]\,\,N_2[1-\ell':k'-\ell']\,+h.c.\right)\right\}\,.
    \end{align*}
    In $(1)$, we simply reordered the four terms in the brackets $\{\dots \}$ above; in $(2)$, we used the relations \eqref{eq:symmetry-function}. In the last line, we have extended the function $f$ by zero to $\mathbb Z^m$.
    Next, we apply \Cref{lem:two-point-hamiltonian-bound} and get
      \begin{align}
       \Delta_1&\leq\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}|\lambda^{(e)}_{k\ell k'\ell'}|(\ell-k+1)(\ell'-k'+1)\,\big(A_{k\ell k'\ell'}+B_{k\ell k'\ell'}\big)\nonumber\\
            &\le L\,\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\,(\ell-k+1)(\ell'-k'+1)\,\big(A_{k\ell k'\ell'}+B_{k\ell k'\ell'}\big) \,,\label{eqHalmost}
\end{align}
    where 
    \begin{align*}
&A_{k\ell k' \ell'}\coloneqq \sqrt{N_1[1:\ell-k]N_2[1:\ell'-k']}\,N_1[1-k:0]\,\,N_2[1-k':0]\,f(N_1+\ell-k,N_2+\ell'-k')\,,\\
&B_{k\ell k' \ell'}\coloneqq\sqrt{N_1[1:\ell-k]N_2[1:\ell'-k']}\,N_1[1-k:0]\,\,N_2[1-k':0]\,f(N_1,N_2)\,.
\end{align*}
Next, using $(x+1)\cdots (x+l)\leq l!(x+1)^l$ for $x \geq 0$, we get
\begin{align}
   A_{k\ell k'\ell'}\le \sqrt{(\ell-k)!\, (\ell'-k')!}\,(N_1+I)^{\frac{\ell-k}{2}}\,(N_2+I)^{\frac{\ell'-k'}{2}}N_1[1-k:0]\,\,N_2[1-k':0]\,f(N_1+\ell-k,N_2+\ell'-k')\,.
\end{align}
Moreover, 
\begin{align*}
N_1[1-k:0]=\sum_{n}\,n\dots (n+1-k)|n\rangle\langle n|\,=\sum_{n\ge k}\,n\dots (n+1-k)|n\rangle\langle n|\le \sum_{n\ge k}\,(n+1)^k|n\rangle\langle n|\le (N_1+I)^k\,,
\end{align*}
and similarly $N_2[1-k':0]\le (N_2+1)^{k'}$. Therefore,
\begin{align*}
A_{k\ell k' \ell'}&\le  \sqrt{(\ell-k)!\, (\ell'-k')!}\,(N_1+I)^{\frac{\ell+k}{2}}\,(N_2+I)^{\frac{\ell'+k'}{2}}\,f(N_1+\ell-k,N_2+\ell'-k')\\
&\le d!\,(N_1+I)^{\frac{\ell+k}{2}}\,(N_2+I)^{\frac{\ell'+k'}{2}}\,f(N_1+\ell-k,N_2+\ell'-k')\,.
\end{align*}
Similarly, we find
\begin{align*}
B_{k\ell k' \ell'}\le  d!\,(N_1+I)^{\frac{\ell+k}{2}}\,(N_2+I)^{\frac{\ell'+k'}{2}}\,f(N_1,N_2)\,.
\end{align*}
Plugging these bounds into \eqref{eqHalmost}, we have
\begin{align}
\Delta_1&\leq L\,\sum_{\substack{0\leq k\leq\ell\leq d\\0\leq k'\leq \ell'\leq d}}\,(\ell-k+1)(\ell'-k'+1)\,\big(A_{k\ell k'\ell'}+B_{k\ell k'\ell'}\big) \\
&\le  2L\,(d+1)^6\,d!\, (N_1+I)^{d}\,(N_2+I)^d\,f(N_1+d,N_2+d)\,.
\end{align}
A very similar analysis gives $\Delta_2\le 2L\,(d+1)^6\,d!\, (N_1+I)^{d}\,(N_2+I)^d\,f(N_1+d,N_2+d)$, and the result follows. 
\end{proof}

Next, we consider the dissipative regularizer
\begin{align*}
        \cL_V^{(\alpha,p)}\coloneqq \sum_{j\in V}\cL[L_j^{(\alpha_j,p)}],\qquad \text{ where}\quad L_j^{(\alpha_j,p)}\coloneqq a_j^p-\alpha_j^p I\,.
    \end{align*}

In the next lemma, we prove that the generator $\cL$ satisfies \Cref{assum} with some suitable constants. We note that a similar bound was already derived in \cite{Gondolfetal2023}. 
\begin{lemma}\label{lem:assumption-dissipation}
For any two integers $k\ge 1$, $p\ge 2$,  
   let $f:\mathbb{N}\to\mathbb{R}$ be defined as follows: $f(0)=f(1)=\dots =f(p-1)=0$ and for any $m\ge 0$, $f(m+p)=f(m)+km^{k-1}$. Then, for any $\alpha\in\mathbb{C}$,
\begin{align}\label{boundlemmadissipgen}
\cL[a^p-\alpha^p]^\dagger(f(N))\le -k1_{N>p}\,\frac{(N+I)^{k-1+p}}{2\times 4^{p+1}}+k p 4^{p+1}\,\Delta\left(\frac{2\times 4^{p+1}\Delta(p-2)}{p-1}\right)^{p-2}\,f(N)+\Delta_k'\,,
\end{align}
    where $\Delta\coloneqq \frac{p(p+1)}{2}+|\alpha|^p(p+1)\sqrt{p!}$ and $\Delta_k'=k^{\mathcal{O}(k)}$ is defined in \eqref{eqDeltakprime}.
\end{lemma}
\begin{proof}
 By \Cref{eq:symmetry-function}, we have for a general function $f:\mathbb{N}\to \mathbb{R}$:
    \begin{equation*}
        \begin{aligned}
            \cL[a^p-\alpha^p]^\dagger(f(N))&=(\ad)^pf(N)a^p-\frac{1}{2}\Big((\ad)^pa^pf(N)+f(N)(\ad)^pa^p\Big)\\
            &\quad+\frac{1}{2}(\overline{\alpha}^pa^pf(N)-\overline{\alpha}^pf(N)a^p+\alpha^p f(N)(\ad)^p-\alpha^p(\ad)^pf(N))\\
            &=(\ad)^pa^p\Big(f(N-pI )-f(N)\Big)\\
            &\quad+\frac{1}{2}\left[\overline{\alpha}^pa^p\Big(f(N)-f(N-pI)\Big)+\alpha^p\Big(f(N)-f(N-pI)\Big)(\ad)^p\right]\,.
        \end{aligned}
    \end{equation*}
Here, again we extend $f$ by zero to $\mathbb Z$. Next, we define the function
    \begin{equation}\label{eq:f-g-l-function}
        g_p(x) = \begin{cases}
            f(x) - f(x - p) & x \ge p;\\
            f(x) & p > x \ge 0;\\
            0 & 0 > x\,.
        \end{cases}
    \end{equation}  
  Using the canonical commutation relation to write $(a^\dagger)^pa^p$ as a function of $N$ (cf.~\eqref{eq:aadag}), we thus have that
    \begin{align}\label{eqdecompositiononetwo}
       \cL[a^p-\alpha^p]^\dagger(f(N))
       &=-N[-p+1:0] g_p(N)+\frac{1}{2}(\overline{\alpha}^pa^pg_p(N)+\alpha^pg_p(N)(\ad)^p)\,.
    \end{align}
Now, since the function $g_p$ is positive and increasing (since $f$ is, see Lemma \ref{lemmatechnicineq}), the second term on the right-hand side of \eqref{eqdecompositiononetwo} can be upper bounded by \Cref{lem:two-point-hamiltonian-bound}, 
    \begin{equation*}
        \begin{aligned}
            \Big|\frac{1}{2}(\overline{\alpha}^pa^pg_p(N)+{\alpha}^pg_p(N)(\ad)^p)\Big|&{\leq}\,(p+1)\,{|\alpha|^p}g_p(N+pI)\sqrt{(N+pI )\cdots (N+I )} \\
        &{=}|\alpha|^p {k(p+1)}\, N^{k-1}\sqrt{(N+pI )\cdots (N+I )}\\
&\overset{(1)}{\leq}|\alpha|^p {k(p+1)\sqrt{p!}}
   \, (N+I)^{k-1+\frac{p}{2}}\,.
   \end{aligned}
    \end{equation*}    
  In (1), we have used again that $(x+1)\cdots (x+l)\leq l!(x+1)^l$ for $x \geq 0$. Therefore, we have proved that 
    \begin{equation*}
        \begin{aligned}
            \cL[a^p-\alpha^p]^\dagger(f(N))&\le -N[-p+1:0]\,g_p(N)+|\alpha|^p{k(p+1)}\sqrt{p!}\,(N+I )^{k-1+\frac{p}{2}}\,.
        \end{aligned}
    \end{equation*}
    Next, we upper bound the first term above: 
    \begin{equation*}
        \begin{aligned}
        N[-p+1:0]\,g_p(N)&=kN[-p+1:0](N-pI)^{k-1}\\
            &\overset{(3)}{\geq}k\,1_{N>p}\,(N-pI)^{k-1}\big((N+I)^p-\frac{p(p+1)}{2}(N+I)^{p-1}\big)\\
             &\ge k\,\,1_{N>p}\,(N-pI)^{k-1+p}-k\frac{p(p+1)}{2} 1_{N>p}(N+I)^{k+p-2}\, .
        \end{aligned}
    \end{equation*}
Here, we used in the first line that the kernel of $N[-p+1:0]$ simplifies the expression for $g_p(N)$. In $(3)$ we used that 
    \begin{align*}
N[-p+1:0]&=\sum_{n\ge 0}\,(n-p+1)\dots n \,|n\rangle\langle n|\\
&=\sum_{n\ge p}\,(n-p+1)\dots n\,|n\rangle\langle n|\\
&\overset{(4)}{\ge} (N+I)^p-\frac{(p+1)p}{2}(N+I)^{p-1}\,,
\end{align*}
where $(4)$ comes from \Cref{lem:bounds-ccr-l-product} below. Moreover, as shown in \cite[Equation (S.135)]{Kuwahara2020}, $(m-1)^{k-1+p}+\frac{(k-1+p)^{k-1+p}}{4}\ge \frac{m^{k-1+p}}{4}$. Iterating this inequality we find (see also \eqref{eq:iterate-S136}):
\begin{align*}
(N-pI)^{k-1+p}1_{N>p}\ge \left(\frac{(N+I)^{k-1+p}}{4^{p+1}}-(k-1+p)^{k-1+p}\frac{4^{p+1}-1}{3\times 4^{p+1}}\right)1_{N>p}\,.
\end{align*}
Therefore,
\begin{align*}
    N_1[-p+1:0]\,g_p(N_1)\ge k 1_{N>p}\frac{(N+I)^{k+p-1}}{4^{p+1}}-k1_{N>p} (k-1+p)^{k-1+p}\frac{4^{p+1}-1}{3\times 4^{p+1}}-k\frac{p(p+1)}{2}1_{N>p}(N+1)^{k+p-2}
\end{align*}
To sum up, we showed that
    \begin{equation}\label{eq-ex:l-dissipation-upper-bound}
        \begin{aligned}
            \cL[a^p-\alpha^p]^\dagger(f(N))&\le k1_{N>p}\,\Big(-\frac{(N+I)^{k-1+p}}{4^{p+1}}+\frac{p(p+1)}{2}(N+I)^{k+p-2}+|\alpha|^p(p+1)\sqrt{p!}(N+1)^{k-1+\frac{p}{2}}\Big)\\
            &+|\alpha|^pk(p+1)\sqrt{p!}\,1_{N\le p}(N+I)^{k-1+\frac{p}{2}}+ k1_{N>p} (k-1+p)^{k-1+p}\frac{4^{p+1}-1}{3\times 4^{p+1}}\\
            &\le k1_{N>p}(N+I)^k\,\Big(-\frac{(N+I)^{p-1}}{4^{p+1}}+\frac{p(p+1)}{2}(N+I)^{p-2}+|\alpha|^p(p+1)\sqrt{p!}(N+1)^{\frac{p}{2}-1}\Big)\\
            &+\underbrace{|\alpha|^pk(p+1)\sqrt{p!}\,(p+I)^{k-1+\frac{p}{2}}+ k (k-1+p)^{k-1+p}\frac{4^{p+1}-1}{3\times 4^{p+1}}}_{=:\Delta_{k}}\\
            &\le k1_{N>p}(N+I)^k\,\Big(-\frac{(N+I)^{p-1}}{4^{p+1}}+\underbrace{\Big(\frac{p(p+1)}{2}+|\alpha|^p(p+1)\sqrt{p!}\Big)}_{=:\Delta}(N+I)^{p-2}\Big)+\Delta_k
        \end{aligned}
    \end{equation}
 Next, by \Cref{polymax} applied to the polynomial $-\frac{1}{2\times 4^{p+1}}X^{p-1}+\Delta X^{p-2}$, we find 
\begin{align}
\cL[a^p-\alpha^p]^\dagger(f(N))&\le -k1_{N>p}\,\frac{(N+I)^{k-1+p}}{2\times 4^{p+1}}+k (N+I)^k\,\Delta\left(\frac{2\times 4^{p+1}\Delta(p-2)}{p-1}\right)^{p-2}+\Delta_k\nonumber \\
&\le -k1_{N>p}\,\frac{(N+I)^{k-1+p}}{2\times 4^{p+1}}+k 4^{p+1}\,\Delta\left(\frac{2\times 4^{p+1}\Delta(p-2)}{p-1}\right)^{p-2}p\,f(N)\nonumber \\
&\qquad+\underbrace{\Delta_k+4^{p+1}(p-1)^k+k^k\frac{4^{p+1}-1}{3}}_{=:\Delta_k'}\,,\label{eqDeltakprime}
\end{align}
where the last line follows from \Cref{lemmatechnicineq}.
\end{proof}


The main difference between the bound \eqref{boundlemmadissipgen} and the one found in \cite{Gondolfetal2023} lies in the choice of the function $f$ which will allow us to keep better track of the dependence of the bounds on the parameter $k$. However, as we show in \Cref{lemmatechnicineq} below, one can easily translate such bounds into bounds of the form required by \Cref{assum}.


\begin{lemma}\label{lemmatechnicineq}
Let $f:\mathbb{N}\to\mathbb{R}$ defined as follows: $f(0)=f(1)=\dots =f(p-1)=0$ and for any $m\ge 0$,
\begin{align*}
f(m+p)=f(m)+km^{k-1}\,.
\end{align*}
then, $f(m)$ is increasing in $m$ and for any integer $\ell$,
\begin{align*}
\frac{1}{p} \left(\frac{(m+\ell)^k}{4^{p+\ell}}-k^k\frac{4^{p+\ell}-1}{3\times 4^{p+\ell}}-(p-1)^k\right) \le {\frac{1}{p}}((m-p)^k-(p-1)^k) \le f(m)\le m^k\,.
\end{align*}
\end{lemma}

\begin{proof}

The statement is clearly true for $0 \leq m \leq p-1$. For the induction, let us assume that it is true for all $m^\prime \leq m+p$ and some $m \in \mathbb N$. Then,
\begin{equation*}
    f(m+p+1)-f(m+p) = k((m+1)^{k-1} - m^{k-1}) + f(m+1) - f(m) \geq 0
\end{equation*}
by the induction hypothesis. For any $m=q p+r$ for some $q,r\in\mathbb{N}$ with $q\ge 1$ and $0\le r<p$, 
\begin{align*}
f(m)&=f(r)+k\Big(r^{k-1}+(r+p)^{k-1}+\dots + (r+qp-p)^{k-1}\Big)\\
&\le k\int_0^q(r+xp)^{{k-1}}dx\\
&\le (r+qp)^k-r^k\\
&\le m^k
\end{align*}
and the same bound trivially holds for $m\le p$. Moreover,
\begin{align}
f(m)&=f(r)+k\sum_{j=0}^{q-1}(r+jp)^{k-1}\nonumber\\
&\ge k\int_0^{q-1}(r+xp)^{k-1}\,dx\nonumber\\
&={\frac{1}{p}}((r+(q-1)p)^k-r^k)\nonumber\\
&={\frac{1}{p}}((m-p)^k-(p-1)^k)\,.\label{eqboundfm}
\end{align}   
Moreover, as shown in \cite[Equation (S.135)]{Kuwahara2020},  
\begin{equation} \label{eq:from-m-to-m-1}
    (m-1)^k+\frac{k^k}{4}\ge \frac{m^k}{4}.
\end{equation}
Iterating this inequality we find:
\begin{align}\label{eq:iterate-S136}
(m-p)^{k}\ge \frac{(m+\ell)^k}{4^{p+\ell}}-k^k\frac{4^{p+\ell}-1}{3\times 4^{p+\ell}}\,,
\end{align}
so that, combining the above bound with \eqref{eqboundfm}, the result follows.


\end{proof}


The following lemma was proved in \cite{Gondolfetal2023}:

\begin{lemma}\label{lem:bounds-ccr-l-product}
    Let $p\in\N$, $p\ge 2$, and $n\geq p$, then
    \begin{equation*}
        \begin{aligned}
            (n+1)^p-\frac{(p+1)p}{2}(n+1)^{p-1}&\leq&(n-p+1)\cdots n\,.
        \end{aligned}
    \end{equation*}
    Moreover, the above left hand side is negative for $n\le p$.
\end{lemma}

\begin{lemma}\label{finite-bounds-generator}
In the notations of \Cref{sec:decoupling},
\begin{align}
&\|PH_e\|_\infty,\,\|H_eP\|_\infty\le  L(d+1)^4d!(M+1)^{2d}\\
&   \|L_i^\dagger P\|_\infty,\,\|PL_i\|_\infty\leq \sqrt{p!}(M+1)^{p/2}+|\alpha_i|^p\,.
\end{align}

\end{lemma}
\begin{proof}
We begin with the Hamiltonian bounds. We only bound the norm $\|PH_e\|_\infty$ since $\|H_e P\|_\infty$ follows the same proof. We have
\begin{align*}
    \|PH_e\|_\infty &=\Big\|\sum_{k\ell k' \ell'=0}^d\lambda^{(e)}_{k\ell k' \ell'}\,P(a_i^\dagger)^{k}a_i^{\ell}(a_j^\dagger)^{k'}a_j^{\ell'} \Big\|_\infty\le L\,(d+1)^4\,\max_{k \ell k' \ell'}\,\Big\|P(a_i^\dagger)^{k}a_i^{\ell}(a_j^\dagger)^{k'}a_j^{\ell'} \Big\|_\infty\,.
\end{align*}
so that the identities \eqref{eq:symmetry-function},\eqref{eq:aadag}, and \eqref{eq:aadag2} (in (3)) show
\begin{equation}\label{eq:inf-norm-hamiltonian}
    \begin{aligned}
        \Big\|P(a_i^\dagger)^{k}a_i^{\ell}(a_j^\dagger)^{k'}a_j^{\ell'} \Big\|_\infty&\le \Big\|P(a_i^\dagger)^{k}a_i^{\ell}(a_j^\dagger)^{k'}a_j^{\ell'}(a_j^\dagger)^{\ell'}a_j^{k'}(a_i^\dagger)^{\ell}a_i^{k} \Big\|^{1/2}_\infty\\
        &\overset{(3)}{=} \Big\|P\, N_i[1-k:0]\, N_i[1-k:\ell-k]\, N_j[1-k':0]\, N_j[1-k':\ell'-k] \Big\|^{1/2}_\infty\\
        &\leq \Big\|P\, N_i[1-k:0]\, N_i[1:\ell]\, N_j[1-k':0]\, N_j[1:\ell'] \Big\|^{1/2}_\infty\,
    \end{aligned}
\end{equation}
Then the same idea as in the proof of Lemma \ref{lem:assumption-hamiltonian} proves the upper bound
\begin{align*}
    \Big\|P(a_i^\dagger)^{k}a_i^{\ell}(a_j^\dagger)^{k'}a_j^{\ell'} \overline{P}\Big\|_\infty&\overset{(4)}{\leq} (M+1)^{\frac{k+k'}{2}}\sqrt{\ell!(M+1)^\ell \ell'!(M+1)^{\ell'} }\\
    &\leq (M+1)^{2d}d!\,.
\end{align*}
Next, 
\begin{equation*}
    \begin{aligned}
        \|PL_i\|_\infty\leq\|Pa_i^p\|_\infty+|\alpha_i|^p= \sqrt{(M+1)\cdots(M+p)}+|\alpha_i|^p\leq \sqrt{p!}(M+1)^{p/2}+|\alpha_i|^p\,,
    \end{aligned}
\end{equation*}
and
\begin{equation*}
    \begin{aligned}
        \|L_i^\dagger P\|_\infty\leq\|(\ad_i)^pP\|_\infty+|\alpha_i|^p=\sqrt{(M+1)\cdots(M+p)}+|\alpha_i|^p\leq \sqrt{p!}(M+1)^{p/2}+|\alpha_i|^p\,.
    \end{aligned}
\end{equation*}
The result follows.

\end{proof}

\section{Learning Bosonic Hamiltonians}\label{learningsection}

In this section, we show how the results of \Cref{sec:decoupling} can be used to derive learning procedures for the coefficients $\lambda^{(e)}_{k\ell k'\ell'}.$ For this, we will make use of the following two lemmas:
 \begin{lemma}[Hoeffding's inequality \cite{hoeffding1963probability}]\label{lem:hoeffy}
 Let $X_1,\dots, X_N$ be independent random variables such that $a\le X_j\le b$, $a\le b$, almost surely. Consider the sum $S_N\coloneqq\sum_{j=1}^N X_j$. Then
 \begin{align*}
 \mathbb{P}\Big(|S_N-\mathbb{E}[S_N]|\ge\epsilon\Big)\le 2\operatorname{exp}\left(-\frac{2\epsilon^2}{N(b-a)^2}\right)\,.
 \end{align*}
 \end{lemma}


\begin{lemma}[Multivariate Markov brothers' inequality, see \cite{harris2008multivariate,harris2002markov}] \label{Markovbroth}
Given a real-valued polynomial $P$ of $K$ variables, and of maximum degree $D$, for all $k\le D$ 
\begin{align*}
\sup\big\{\max_{x_1,\dots,x_k}|\partial_{x_1}\dots \partial_{x_k}P(x)|\,:\,\|x\|_2\le 1\big\}\le 2^{2k-1}T^{(k)}_D(1)\,\sup\big\{|P(x)|\,:\,\|x\|_2\le 1\big\}\,.
\end{align*}
where $\|x\|_2$ denotes the Euclidean norm of $x\in \mathbb{R}^K$, and 
\begin{align*}
T^{(k)}_D(1)\coloneqq \frac{D^2(D^2-1^2)(D^2-2^2)\dots (D^2-(k-1)^2)}{1\cdot 3\cdot 5\dots (2k-1)}\,.
\end{align*}

\end{lemma} 

\subsection{Learning from moment bounds}\label{vanillaalgobound}

First, we recall the definition $P_\beta^{(e)}\coloneqq \frac{1}{\pi^2}\ketbra{\beta}{\beta}_e\otimes I_{e^c}$, then we turn the bound found in \eqref{eqboundmomentfinitediff}, that is, for all $\alpha,\beta\in \mathbb{C}^2$,
\begin{align}\label{eqboundmomentfinitediff1}
\Big| \frac{1}{\pi^2}\,g_e(\alpha,\beta)|\langle{\alpha}|{\beta}\rangle|^2-t^{-1}\tr\big[(\rho_t-\rho)P_\beta^{(e)}\big]\Big|= \mathcal{O}(t)\,.
\end{align}
into a guarantee for the successful learning of the coefficients $\lambda^{(e)}_{k\ell k'\ell'}$. 
We recall the definition of the polynomials $g_e(\alpha,\beta)$ for $e=\{i,j\}\in E$ and $\alpha\in\mathbb{C}^m,\beta\in \mathbb{C}^2$ from the main text:
\begin{align*}
g_e(\alpha, \beta)&\coloneqq  \sum_{\substack{j\ne j'\sim i\\k\ell k'\ell'}}\,\lambda^{(\{i,j'\})}_{k\ell k'\ell'}\left( \overline{\alpha}_{i}^k \overline{\alpha}_{j'}^{k'} \beta_i^\ell \alpha_{j'}^{\ell'} - \overline{\beta}_i^k \overline{\alpha}_{j'}^{k'} \alpha_i^\ell \alpha_{j'}^{\ell'} \right) \\
  &\qquad +\sum_{\substack{i\ne i'\sim j\\k\ell k'\ell'}}\,\lambda^{(\{i',j\})}_{k\ell k'\ell'}\left( \overline{\alpha}_{j}^k \overline{\alpha}_{i'}^{k'} \beta_j^\ell \alpha_{i'}^{\ell'} - \overline{\beta}_j^k \overline{\alpha}_{i'}^{k'} \alpha_j^\ell \alpha_{i'}^{\ell'} \right)\\
  &\qquad +\sum_{k\ell k'\ell'}\lambda^{(e)}_{k\ell k'\ell'}\left(\overline{\alpha}_i^{k}\overline{\alpha}_j^{k'}\beta_i^{\ell}\beta_j^{\ell'}-\overline{\beta}_i^k\overline{\beta}_j^{k'}\alpha_i^\ell\alpha_j^{\ell'}  \right)\,.
\end{align*}
In the next Lemma, we exploit the structure of $g_e$ to extract the coefficients from multiple differentiations of it.
 \begin{lemma}\label{Lemmacoeffpoly}
 Assuming that $\lambda^{(e)}_{k\ell 00}=\lambda^{(e)}_{00k'\ell'}=0$, we get
 \begin{align*}
    \operatorname{Im}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)=\frac{\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, \beta)\big|_{\alpha,\beta=0}}{2ik'! \ell'! k!\ell!}\quad \text{ and }\quad \operatorname{Re}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)=-\frac{\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, e^{-i\frac{\pi}{2(\ell+\ell')} }\beta)\big|_{\alpha,\beta=0}}{2ik'!\ell'! k!\ell!}\,.
 \end{align*}
 \end{lemma}

\begin{proof}
We first take $\alpha\in \mathbb{R}^m,\beta\in\mathbb{R}^2$. Then for $e=\{i,j\}$
\begin{align*}
g_e(\alpha, \beta)&\coloneqq  \sum_{\substack{j\ne j'\sim i\\k\ell k'\ell'}}\,\lambda^{(\{i,j'\})}_{k\ell k'\ell'}\left( {\alpha}_{i}^k {\alpha}_{j'}^{k'+\ell'} \beta_i^\ell  - {\beta}_i^k {\alpha}_{j'}^{k'+\ell'} \alpha_i^\ell \right) \\
  &\qquad +\sum_{\substack{i\ne i'\sim j\\k\ell k'\ell'}}\,\lambda^{(\{i',j\})}_{k\ell k'\ell'}\left( {\alpha}_{j}^k {\alpha}_{i'}^{k'+\ell'} \beta_j^\ell  - {\beta}_j^k {\alpha}_{i'}^{k'+\ell'} \alpha_j^\ell  \right)\\
  &\qquad +\sum_{k\ell k'\ell'}\lambda^{(e)}_{k\ell k'\ell'}\left({\alpha}_i^{k}{\alpha}_j^{k'}\beta_i^{\ell}\beta_j^{\ell'}-{\beta}_i^k{\beta}_j^{k'}\alpha_i^\ell\alpha_j^{\ell'}  \right)\,.
\end{align*}
Using that $\overline{\lambda}^{(e)}_{k\ell k'\ell'}={\lambda}^{(e)}_{\ell k\ell'k'}$, and setting $\alpha_{i'}=\alpha_{j'}=0$ for any vertices $i',j'$ at distance one of $e$, we see that the last sum above simplifies to
\begin{align*}
g_e(\alpha,\beta)=\sum_{k\ell k'\ell'}\big(\lambda^{(e)}_{k\ell k'\ell'}-\overline{\lambda}^{(e)}_{k\ell k'\ell'}\big){\alpha}_i^{k}{\alpha}_j^{k'}\beta_i^{\ell}\beta_j^{\ell'}=2i\sum_{k\ell k'\ell'}\operatorname{Im}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)\,{\alpha}_i^{k}{\alpha}_j^{k'}\beta_i^{\ell}\beta_j^{\ell'}\,.
\end{align*}
Therefore, we get that
\begin{align*}
\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, \beta)\big|_{\alpha,\beta=0}=2ik!k'!\ell !\ell'!\operatorname{Im}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)\,.
\end{align*}
Similarly, we can then show that, by replacing {$\beta$} by $e^{i\pi K}{\beta}$ and choosing $K=-\frac{1}{2(\ell+\ell')}$, we get
\begin{align*}
\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_i}^{\ell}\partial_{\beta_j}^{\ell'}\,g_e(\alpha, e^{-i\frac{\pi}{2(\ell+\ell')} }\beta)\big|_{\alpha,\beta=0}=-2i k!k'!\ell !\ell'!\operatorname{Re}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)\,.
\end{align*}
\end{proof} 

Therefore, we see that successive derivatives of $g_e$ at $0$ are indeed associated to the coefficients $\lambda^{(e)}_{k\ell k'\ell '}$. Next, we propose a method for learning the polynomials $g_e$ and their derivatives. First, we learn some other polynomial associated to $g_e$, namely their integral in $\beta$ over the set $R_\beta\coloneqq [0,\beta_1]\times \dots \times [0,\beta_4]$, for $\alpha$ and $\beta$  belonging to a given net. This is detailed in \Cref{protocollearning2} below, where we denote the indicator function of the set $R_\beta$ as $1_{R_\beta}$, and $B_{\mathbb{R}^K}$ the unit $\ell^2$ ball in $\mathbb{R}^K$ centered at $0$. We denote nets of $B_{\mathbb{R}^K}$ by $\cN_{K}$, which we will specify later.

\medskip 

\begin{algorithm}[H]
  \caption{BosonicHamiltonian (Restatement of \Cref{protocollearning1})}\label{protocollearning2}
  \KwIn{ $T\in\mathbb{N}$, nets $\cN_{2}, \cN_{4}$.}
  \KwOut{$\hat{Q}_{\alpha,\beta}^{(e,T)}(t)\in\mathbb{R}$ $\forall e\in E$, $\alpha\in\cN_{2},\,\beta\in \cN_{4}$.}
  \BlankLine
 Partition $E$ into $\nu=\mathcal{O}(1)$ sets $E_1,\dots E_\nu$  such that $\forall j\in[\nu], \forall e_1,e_2\in E_j$, $e_1\cap e_2=\emptyset$;\\
  \For{$j \in [\nu]$, $\alpha\in\cN_{2}$}{
  \For{$i_\alpha\in[T]$}{Prepare state $|\psi_\alpha\rangle\coloneqq \otimes_{e\in E_j}|\alpha\rangle_{e}\otimes |0\rangle_{E_j^c}$;\\
  Run evolution generated by $\widetilde{\cL}^{(\alpha)}$ on $|\psi_\alpha\rangle$ up to time $t$;\\
  Perform heterodyne measurements on $ E_j$;\\
  $\rightarrow$ get outcomes $\beta^{(i_\alpha)}_e\in \mathbb{R}^4$ $\forall e\in E_j$;}}
  \Return{$\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\coloneqq  \frac{1}{Tt}\sum_{i_\alpha=1}^T\, \Big(\frac{1_{R_\beta}(\beta_e^{(i_\alpha)})}{|\langle \alpha_e|\beta_e^{(i_\alpha)}\rangle|^2}-1\Big)$.}
  \end{algorithm}

\medskip
The estimators $\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ constructed in \Cref{protocollearning1} are sums of i.i.d.~random variables of average
\begin{align}\label{eqexpectationestimator}
\mathbb{E}\Big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\Big]= t^{-1}\int_{R_\beta} \frac{\tr\big[(\rho_t-\rho)P_{\beta'}^{(e)}\big]}{|\langle \alpha_e|\beta'\rangle|^2}\,d^4\beta'\,,
\end{align}
where $\rho\coloneqq \ketbra{\alpha}{\alpha}$ and $\rho_t=e^{t\widetilde{\cL}^{(\alpha)}}(\rho)$. 
Moreover, a joint use of Hoeffding's concentration inequality together with the union bound permits to conclude that, with probability $1-\delta$, $\delta\in (0,1)$, for any $\alpha\in\cN_{2}$, $\beta\in\cN_4$, {and any $e \in E$,} the random variables $\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)$ satisfy 
\begin{align}\label{eqexpectationestimator2}
    \Big|\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)-\mathbb{E}\big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\big]\Big|\le \epsilon_{\operatorname{stat}}
\end{align}
as long as the number of measurements for each $\alpha, j\in[\nu]$ satisfies:
\begin{align}\label{eqT11}
    T= \, \mathcal{O}\left(\frac{1}{(t\cdot {\epsilon_{\operatorname{stat}}})^2}\ln\left(\frac{|E|\cdot |\cN_{2}|\cdot |\cN_{ 4}|}{\delta}\right)\right)\,.
\end{align}
Next, for any $\alpha\in\mathbb{C}\cong{\mathbb{R}^2},\beta=(\beta_{i,R},\beta_{i,I},\beta_{j,R}, \beta_{j,I})\in\mathbb{C}^2\cong \mathbb{R}^4$, where $\beta_i=\beta_{i,R}+i \beta_{i,I}$ and $\beta_j=\beta_{j,R}+i\beta_{j,I}$, denoting
\begin{align}\label{eqQe}
    Q^{(e)}_\alpha(\beta)&\coloneqq \frac{1}{\pi^2}\,\int_{R_\beta}\,g_e(\alpha,\beta')\,d^4\beta'\,,
\end{align}
we get from \eqref{eqboundmomentfinitediff} together with the fact that $g_e(\alpha,\beta)=g((\{\alpha_{e'}\}_{e'\in E_j},0_{E_j^c}),\beta)$, where $j\in[\nu]$ is such that $e\in E_j$, that 
\begin{align}\label{eqboundmomentfinitediff23}
    \Big|Q^{(e)}_{\alpha}(\beta)-\mathbb{E}\big[\hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\big]\Big|=\mathcal{O}(t)\,.
\end{align}
In summary, combining \eqref{eqboundmomentfinitediff23} with \eqref{eqexpectationestimator2}, we see that for $(\alpha,\beta)\in\cN_2\times \cN_4$
\begin{align}\label{eqQQhatbound}
    \Big| Q^{(e)}_{\alpha}(\beta)- \hat{Q}^{(e,T)}_{(\alpha,\beta)}(t)\Big|=\mathcal{O}({\epsilon}_{\operatorname{stat}}+ t)\,.
\end{align}
The connection to the parameters $\lambda^{(e)}_{k\ell k'\ell'}$ can then be made through the use of \Cref{Lemmacoeffpoly}. 
By the definition \eqref{eqQe} of $Q_\alpha^{(e)}$, we see that 
\begin{align}
    g_e(\alpha, \beta_1,0,\beta_3,0)
    =\pi^2 \partial_{\beta_{i,R}}\partial_{\beta_{i,I}}\partial_{\beta_{j,R}}\partial_{\beta_{j,I}}Q^{(e)}_\alpha( \beta)\big|_{\substack{\beta_{i,I},\beta_{j,I}=0\\ {\beta_{i,R}=\beta_1, \beta_{j,R}=\beta_3}}}\,.
\end{align}
Together with \Cref{Lemmacoeffpoly}, we directly find that 
\begin{align}
    &\operatorname{Re}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)=- \pi^2 \frac{\partial_{\beta_{i,I}}\partial_{\beta_{j,I}}\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_{i,R}}^{\ell+1}\partial_{\beta_{j,R}}^{\ell'+1}\,Q^{(e)}_{\alpha}( e^{-i\frac{\pi}{2(\ell+\ell')}}\beta)\big|_{\alpha,\beta=0}}{2ik'\ell' k!\ell!}\,,\label{Realeq}\\
    &\operatorname{Im}\big(\lambda^{(e)}_{k\ell k'\ell'}\big)= \pi^2 \frac{\partial_{\beta_{i,I}}\partial_{\beta_{j,I}}\partial^k_{{\alpha}_i}\partial^{k'}_{{\alpha}_j}\partial_{\beta_{i,R}}^{\ell+1}\partial_{\beta_{j,R}}^{\ell'+1}\,Q^{(e)}_\alpha( \beta)\big|_{\alpha,\beta=0}}{2ik'\ell' k!\ell!}\,.\label{Imaginaryeq}
\end{align}
Next, we perform {an exact} polynomial interpolation {of the data $(\alpha, \beta, \hat{Q}_{(\alpha,\beta)}^{(e,T)}(t))$ generated by \Cref{protocollearning2} using multivariate Lagrange polynomials}. This results in a polynomial $\widetilde{Q}^{(e)}:(\alpha,\beta)\mapsto \widetilde{Q}_\alpha^{(e)}(\beta)$
such that $\widetilde{Q}_\alpha^{(e)}(\beta)=\hat{Q}_{(\alpha,\beta)}^{(e,T)}(t)$ for all $\alpha\in\cN_{2}$, $\beta \in \cN_{4}$. 
It follows from \Cref{lemmapolyinterpolation} with $m=6$ and $n=6(d+1)$  that for any $(\alpha,\beta)\in [-1,1]^{6}$,
\begin{align*}
    \big|Q_\alpha^{(e)}(\beta)-\widetilde{Q}^{(e)}_\alpha(\beta)\big|&\le \max_{(\alpha,\beta)\in\cN_{2}\times \cN_{4}}\, |Q_\alpha^{(e)}(\beta)-\widetilde{Q}^{(e)}_\alpha(\beta)|\,\frac{\big(\binom{6d+12}{6d+6}+1\big)!}{|\Delta|}\\
    &=\mathcal{O}( \epsilon_{\operatorname{stat}}+t)\,,
\end{align*}
where $|\Delta|$ is a positive parameter that simply depends on $d$ and the net nodes. The nets $\cN_{2}$ and $\cN_{4}$ can in particular be chosen to have cardinality only depending on the degree of $Q^{(e)}_{\alpha}(\beta)$, {which is $\cO(d)$}.
Therefore, combining \eqref{Realeq} and \eqref{Imaginaryeq} with a multivariate Markov brothers' inequality (see \Cref{Markovbroth}), we find that for any coefficient $\lambda^{(e)}_{k\ell k'\ell'}$, defining $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ by replacing the $Q$'s by $\widetilde{Q}$'s in \Cref{Realeq,Imaginaryeq}, we finally get for all $k,\ell,k',\ell'$ that
\begin{align*}
    |{\lambda}^{(e)}_{k\ell k'\ell'}-\hat{\lambda}^{(e)}_{k\ell k'\ell'}|&=\mathcal{O}\big({\epsilon}_{\operatorname{stat}}+t\big)\,,
\end{align*}
where we assumed that $d,L=\mathcal{O}(1)$. Imposing each of the variables in the above brackets to be of order $\epsilon$, we arrive at the main result of this section:

\begin{cor}[Vanilla strategy]\label{Vanillastrategythm}
   Combining \Cref{protocollearning2} with an {exact polynomial interpolation of its output}, we get {estimators} $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ of $\lambda^{(e)}_{k\ell k'\ell'}$ such that for all $k\ell k'\ell'$, $|\hat{\lambda}^{(e)}_{k\ell k'\ell'}-\lambda^{(e)}_{k\ell k'\ell'}|\le \epsilon$ with probability $1-\delta$ as long as the evolution time for each run scales as $t=\mathcal{O}(\epsilon)$ and the total number of samples is at least
    \begin{align*}
        N_{\operatorname{samp}}=\mathcal{O}\left( \frac{1}{{\epsilon}^4}\ln\left(\frac{|E|}{\delta}\right)\right)\,.
    \end{align*}
    Our scheme uses a total evolution time of $T_{\operatorname{evo}}=\mathcal{O}\big(\epsilon^{-3}\ln\big(\frac{|E|}{\delta}\big)\big)$.

\end{cor}

\begin{proof}
The total number of samples needed in \Cref{protocollearning1} is equal to $T\cdot |\cN_{2}|\cdot \nu$, where $|\cN_{2}|\cdot \nu$ corresponds to the total number of preparation setups. Here, the number $\nu$ of partitions of the graph is simply related to its chromatic number, and scales linearly with the degree of the interaction graph, which itself scales linearly with the degree $\gamma$ of the underlying graph $G$. (Namely, a greedy coloring shows that the chromatic number of $G$ is at most $\gamma + 1$). The result follows from \eqref{eqT11} with $\epsilon_{\operatorname{stat}}=\cO(\epsilon)$.
\end{proof}





\subsection{Improved sample complexity via Lieb Robinson bounds}\label{LRsamplecomplexity}

We now aim at improving the $\epsilon$-dependence of the sample complexity found in \Cref{Vanillastrategythm} using the Lieb-Robinson estimates from \Cref{Prop:bosonicLR}. For the sake of simplicity, we assume that $V=[-n,n]^D$ is a regular $D$-dimensional lattice.
We begin by describing an estimation algorithm that serves as a central subroutine in our learning procedure. 
Before presenting the algorithm, we fix some notation. For an edge $e\in E$, let $R_e\subset V$ be a $D$-dimensional rectangle centered around $e$ with side length $\mathcal{O}(\operatorname{polylog}(\varepsilon_{\mathrm{LR}}^{-1}))$ for $\varepsilon_{\mathrm{LR}}$ small enough. 
Also, let $P_{R}^{(M)}$ be the finite rank projection on a region $R$ as introduced in \Cref{Prop:bosonicLR}. Finally, recall that we use $B_{\mathbb{R}^K}$ to denote the $\infty$-norm unit ball in $\mathbb{R}^K$ centered at $0$.
With this, we can now write down \Cref{protocollearningLRprojected}:

\begin{algorithm}[H]
  \caption{RefinedBosonicHamiltonian (Restatement of \Cref{protocollearningLRprojected-main})}\label{protocollearningLRprojected}
  \KwIn{ $T\in\mathbb{N}$, $0<b_1<b_2$, finite sets $\mathcal{S}_1\subset [b_1,b_2]$, $\mathcal{S}_2\subset B_{\mathbb{R}^2}$, $\mathcal{S}_4\subset B_{\mathbb{R}^4}$.}
  \KwOut{$\hat{L}_{\alpha,\beta}^{(e,T)}(t)\in\mathbb{R}$ $\forall e\in E$, $\alpha\in\mathcal{S}_{2},\,\beta\in \mathcal{S}_{4}$, $t\in\mathcal{S}_1$.}
  \BlankLine
  Partition $E$ into $\nu=\mathcal{O}(\operatorname{polylog}(\varepsilon_{\mathrm{LR}}^{-1}))$ sets $E_1,\dots, E_\nu$  such that $\forall j\in[\nu], \forall e_1\neq e_2\in E_j$, $R_{e_1}\cap R_{e_2}=\emptyset$;\\
  \For{$j \in [\nu]$, $\alpha\in\mathcal{S}_{2}$, $t\in\mathcal{S}_1$}{
  \For{$i_{\alpha,t}\in[T]$}{Prepare state $|\psi_\alpha\rangle\coloneqq \tfrac{ \bigotimes_{e\in E_j}P^{(M)}_e|\alpha\rangle_{e}\otimes |0\rangle_{e^c}}{\left\|\bigotimes_{e\in E_j}P^{(M)}_e|\alpha\rangle_{e}\otimes |0\rangle_{e^c}\right\|}$\,;\\
  Run evolution generated by $\widetilde{\cL}^{(\alpha)}$ on $|\psi_\alpha\rangle$ up to time $t$;\\
  Perform heterodyne measurements on $ E_j$;\\
  $\rightarrow$ get outcomes $\beta^{(i_{\alpha,t})}_e\in \mathbb{R}^4$ $\forall e\in E_j$;}}
  \Return{$\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)\coloneqq  \frac{\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)}{T}\sum_{i_{\alpha,t}=1}^T\, e^{|\beta_e^{(i_{\alpha,t})}|^2}\,1_{R_\beta}(\beta_e^{(i_{\alpha,t})})$, where $e=\{i,j\}$.}
\end{algorithm}

\medskip


Here, we can see that a claimed partitioning of $E$ is possible with $\nu=\mathcal{O}(|R_e|)=\mathcal{O}(\operatorname{polylog}(\varepsilon_{\mathrm{LR}}^{-1}))$ sets as follows: Start with a covering of $V$ with non-overlapping rectangles and define $E_1$ to be the set containing all edges that are central in any of those rectangles. To obtain the other $E_j$, consider shifts of the initial covering and take the edges that are center points for the shifted rectangles. The number of shifts needed per direction depends on the side length of the desired rectangles, thus the overall number of partition elements scales with the volume of the desired rectangles.

\medskip

We now explain what quantities are being estimated in \Cref{protocollearningLRprojected} and how we can use those estimates for learning the Hamiltonian coefficients.
First, observe that the constructed estimators by definition have the following true averages: If $e=\{i,j\}$, then
\begin{align*}
    \mathbb{E}\left[\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)\right] 
    = \int_{R_\beta}\frac{\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)}{e^{-|\beta'|^2}} \operatorname{tr}\left[P_{\beta'}^{(e)}\, e^{t\widetilde{\mathcal{L}}}(|\psi_\alpha\rangle\langle\psi_\alpha|)\right]d^4\beta'\,.
\end{align*}
A joint use of Hoeffding's concentration inequality and the union bound yields that
\begin{align*}
    \Big|\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)-\mathbb{E}\big[\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)\big]\Big|\le \epsilon_{\operatorname{stat}}
\end{align*}
holds simultaneously for all $e\in E$, $\alpha\in\mathcal{S}_{2},\,\beta\in \mathcal{S}_{4}$, $t\in\mathcal{S}_1$ with probability $1-\delta$ as long as the number of measurements for each $\alpha, j\in[\nu]$ satisfies:
\begin{align*}
    T= \, \mathcal{O}\left(\frac{1}{{\epsilon^2_{\operatorname{stat}}}}\ln\left(\frac{|E|\cdot |\mathcal{S}_{2}|\cdot |\mathcal{S}_{ 4}|\cdot |\mathcal{S}_1|}{\delta}\right)\right)\,.
\end{align*}
That is, the estimates $\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)$ are indeed reliable approximations to their true averages, with high probability. 
Notice that, compared with the bound found in \eqref{eqT11}, the bound on $T$ here does not scale inverse polynomially with $t$.

We now consider these true averages in more detail and relate them to our quantities of interest. To this end, first note that, whereas previously we worked with input states arising from coherent states $\ket{\alpha}_e$, we now consider input states arising from states proportional to projected coherent states $P_e^{(M)}|\alpha\rangle_e$, with $P_{e}^{(M)}$ the finite rank projection from \Cref{Prop:bosonicLR}. Some remarks about these input states are in order.
First, we note that a projected coherent state can (probabilistically) be prepared as follows: First prepare $\ket{\alpha}_e$, then measure the photon number operator on $e$, and post-select on small enough photon number.
Second, we consider the Lieb-Robinson type bound provided in \Cref{Prop:bosonicLR}:
For $p\ge 2d+2$, there exist constants $\eta,a,b,c>0$ depending on $p$ such that, for any $T\subset R\subset V$ with $|T|=\mathcal{O}(1)$ and $\operatorname{dist}(\partial\mathring{R},T)\ge \eta$, and for all $\alpha\in \mathbb{R}^{2|V|}$ with $|\alpha_i|=\mathcal{O}(1)$,
\begin{align}
    \left\|\tr_{T^c}\left[ \rho_t-\rho_t^{(M)}\right]\right\|_1
    \le a\,  |R|^{\frac{7}{2}}\,e^{-b
     \operatorname{dist}(\partial\mathring{R},T)^{c}} \label{LRBsimpleprime}
\end{align}
for $M= \mathcal{O}\big( \operatorname{dist}(\partial\mathring{R},T)^{\frac{1}{p}}\big)$, where we denoted $\rho_t=e^{t\widetilde{\mathcal{L}}}(\ketbra{\psi_\alpha}{\psi_\alpha})$, $\rho^{(M)}_t=e^{t\widetilde{\mathcal{L}}^{(M)}_{\mathring{R}}}(\ketbra{\psi_\alpha}{\psi_\alpha}))$. Using \Cref{LRBsimpleprime}, we can adapt the method developed in \cite{franca2022efficient} to the present bosonic setting. 
For our purposes, when considering any edge $e$, we fix $T=e$ for $e\in E$, and choose $R=R_e$ to be a rectangle centered around the edge $e$ as above. Then, the effect operator $P_\beta^{(e)}= \frac{1}{\pi^2}\ketbra{\beta}{\beta}_e\otimes I_{e^c}$ satisfies
\begin{align*}
    \Big|\tr\big[(\rho_t-\rho^{(M)}_t){P_\beta^{(e)}}\big]\Big|\le \epsilon_{\operatorname{LR}}
\end{align*}
for $\operatorname{dist}(\partial\mathring{R}_e,e)=\mathcal{O}(\operatorname{polylog}(\epsilon_{\operatorname{LR}}^{-1}))$ and $\epsilon_{\operatorname{LR}}$ small enough.
Next, we approximate the (finite-dimensional) superoperator exponential $e^{t\widetilde{\cL}^{(M)}_{\mathring{R}}}$ by its Taylor expansion of order $i$: 
\begin{align*}
    \left|\tr\big[\rho_t^{(M)}{P_\beta^{(e)}}\big]-T_{R_e,i}(t)\right|\le \frac{(t\|\widetilde{\cL}^{(M)}_{\mathring{R}_e}\|_{\diamond})^{i+1}}{(i+1)!}\, , \qquad\qquad \text{ where }\qquad T_{R_e,i}(t)\coloneqq \sum_{j=0}^i\,\frac{t^j}{j!}\,\tr\Big[(\widetilde{\cL}_{\mathring{R}_e}^{(M)})^j\left(|\psi_\alpha\rangle\langle \psi_\alpha|\right){P_\beta^{(e)}}\Big]\,.
\end{align*}
The diamond norm above can be further controlled using \Cref{finite-bounds-generator} as follows: 
\begin{align*}
    \|\cL^{(M)}_{\mathring{R}_e}\|_{\diamond}&\le 2\sum_{e\in E_{R_e}}\,\|PH_e P\|_\infty + 2\sum_{i\in R_e} \|PL_iP\|_\infty^2\\
    &\le 2|E_{R_e}| L(d+1)^4d!(M+1)^{2d}+2 |R_e|\,\Big(\sqrt{p!}(M+1)^{p/2}+\max_{i\in R_e}\{|\alpha_i|^p\}\Big)^2\\
    &\le \kappa\,|R_e|^2\,,
\end{align*}
for some constant $\kappa>0$, where we recall that $|R_e|=\mathcal{O}(\operatorname{polylog}(\epsilon_{\operatorname{LR}}^{-1}))$ Therefore, using Stirling approximation \cite{robbins1955remark-stirling}, we have that 
\begin{align*}
    \left|\tr\big[\rho_t^{(M)} {P_\beta^{(e)}}\big]-T_{R_e,i}(t)\right|\le \epsilon_{\operatorname{poly}}\qquad\qquad \text{ for }\qquad i=\ln\left(\frac{\sqrt{2\pi}}{\epsilon_{\operatorname{poly}}}\right)+2et\kappa|R_e|^2\,.
\end{align*}
Next, the functions $T_{R_e,i}(t)$ can be  expressed as
\begin{align}
    T_{R_e,i}(t)
    &=\sum_{j=0}^i\,\frac{t^j}{j!}\,\tr\Big[(\widetilde{\cL}_{\mathring{R}_e}^{(M)})^j\left(|\psi_\alpha\rangle\langle \psi_\alpha|\right){P_\beta^{(e)}}\Big]\\
    &= \sum_{j=0}^i\,\frac{t^j}{j!}\,\tr\Big[(\widetilde{\cL}_{\mathring{R}_e}^{(M)})^j\left(|\psi_\alpha\rangle\langle \psi_\alpha|\right) P{P_\beta^{(e)}} P\Big]\, .
\end{align}
Extracting the normalization factors, we can write this as
\begin{align*}
    T_{R_e,i}(t) = \frac{e^{-|\alpha_e|^2-|\beta|^2}}{\pi^2\,\big\| P^{(M)}_e\ket{\alpha_e}\big\|^2} \,g_e (\alpha, \beta, t)\, ,
\end{align*}
where $g_e (\alpha, \beta, t)$ is now a polynomial in all its variables and has degree at most $M'=\mathcal{O}(\operatorname{polylog}(\varepsilon_{\operatorname{LR}}^{-1},\varepsilon_{\operatorname{poly}}^{-1}))$. Additionally, as we pick $\alpha$ such that $\alpha\rvert_{R_e\setminus\{e\}} = 0 _{R_e\setminus\{e\}}$, we can explicitly write out the normalization and obtain
\begin{align*}
    T_{R_e,i}(t) = \frac{e^{-|\beta|^2}}{\pi^2\,\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)} \,g_e (\alpha, \beta, t)\, ,
\end{align*}
with $e=\{i,j\}$. Up to this point, we have argued that 
\begin{align*}
    \left\|(\alpha,\beta,t)\mapsto \operatorname{tr}\Big[{P^{(e)}_\beta}e^{t\widetilde{\mathcal{L}}}\left(|\psi_\alpha\rangle\langle \psi_\alpha|\right)\Big] - \frac{e^{-|\beta|^2}}{{\pi^2}\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)} \,g_e (\alpha, \beta, t) \right\| \leq \varepsilon_{\mathrm{poly}} + \varepsilon_{\mathrm{LR}}\, .
\end{align*}
Combining this approximation guarantee with the observations that we made about our estimators above, if we define
\begin{align*}
    L^{(e)}_\alpha(\beta,t)\coloneqq {\frac{1}{\pi^2}}\int_{R_\beta}\,g_e(\alpha_e,\beta',t)\,d^4\beta'\,,
\end{align*}
we see that, for $|\alpha|,|\beta|=\mathcal{O}(1)$ as in our algorithm,
\begin{align*}
    \left\lvert\mathbb{E}\left[\hat{L}^{(e,T)}_{(\alpha,\beta)}(t)\right] - L^{(e)}_\alpha(\beta,t) \right\rvert = \mathcal{O}(\varepsilon_{\operatorname{poly}}+\varepsilon_{\operatorname{LR}})\, ,
\end{align*}
and therefore 
\begin{align*}
    \left\lvert \hat{L}^{(e,T)}_{(\alpha,\beta)}(t) - L^{(e)}_\alpha(\beta,t) \right\rvert = \mathcal{O}(\varepsilon_{\operatorname{poly}}+\varepsilon_{\operatorname{LR}} + \epsilon_{\operatorname{stat}})\, ,
\end{align*}
holds simultaneously for all $e\in E$, $\alpha\in\mathcal{S}_{2},\,\beta\in \mathcal{S}_{4}$, $t\in\mathcal{S}_1$ with probability $\delta$, with the number $T$ of measurements chosen as detailed above.

Next, for each edge $e\in E$ we choose the sets $\mathcal{S}_1$, $\mathcal{S}_2$ and $\mathcal{S}_4$ to be generated by $\mathcal{O}\big(M'^{7}\big) $ points as described in \Cref{algoderivatives} for estimating higher order derivatives on the resulting estimators $\hat{L}^{(e,T)}_{\alpha,\beta}(t)$, $(\alpha,\beta,t)\in \mathcal{S}_2\times \mathcal{S}_4\times\mathcal{S}_1$. 
The procedure in \Cref{coroboundwithtimedifference}, which is based on \Cref{algoderivatives}, then produces outputs $\widetilde{L}^{(i_1,\dots, i_{7})}(0,\dots ,0)$ satisfying the bound in \eqref{boundderivativestimebla} with $k_{\max}=d+1$, $a=6$, $\sigma=\mathcal{O}(\varepsilon_{\operatorname{poly}}+\varepsilon_{\operatorname{LR}} + \epsilon_{\operatorname{stat}})$ and $M=M'$. That is,

\begin{align*}
    \Big|\widetilde{L}^{(i_1,\dots, i_{7})}(0,\dots ,0)-\partial_{\alpha_1}^{i_1}\partial_{\alpha_2}^{i_2}\partial_{\beta_1}^{i_3}\cdots \partial_{\beta_4}^{i_6}\partial_t^{i_7}|_{\alpha,\beta,t=0}L_{\alpha}^{(e)}(\beta,t)\Big|&= 
     \big(3\cdot 2^{2d+1}\, (M')^{2d+2}\big)^{6}\, 3e(M')^2\mathcal{O}(\epsilon_{\operatorname{stat}}+\epsilon_{\operatorname{LR}}+\epsilon_{\operatorname{poly}})\\
&=\mathcal{O}\Big(\operatorname{polylog}\big(\epsilon_{\operatorname{LR}}^{-1},\epsilon_{\operatorname{poly}}^{-1} \big)\,(\epsilon_{\operatorname{stat}}+\epsilon_{\operatorname{LR}}+\epsilon_{\operatorname{poly}})\Big)\,,
\end{align*}
To summarize, we see that, taking $\epsilon_{\operatorname{LR}},\epsilon_{\operatorname{poly}},\epsilon_{\operatorname{stat}}=\mathcal{O}\big(\epsilon\operatorname{polylog}(1/\epsilon)^{-1}\big)$, with probability $1-\delta$, we have constructed $\epsilon$-approximate estimates $\widetilde{L}^{(i_1,\dots, i_7)}(0,\dots ,0)$ of $\partial_{\alpha_1}^{i_1}\partial_{\alpha_2}^{i_2}\partial_{\beta_1}^{i_3}\cdots \partial_{\beta_4}^{i_6}\partial_t^{i_7}|_{\alpha,\beta,t=0}L_{\alpha}^{(e)}(\beta,t)$.


It remains  to verify that the coefficients $\lambda_{k\ell k'\ell'}^{(e)}$ of $H$ can be reconstructed from these very coefficients. First, we have 
\begin{align*}
    \partial_{\beta_1}\dots \partial_{\beta_4}\partial_t L^{(e)}_{\alpha}(\beta,0)
    &=\partial_t {\frac{1}{\pi^2}}g_e(\alpha_e,\beta,0)\\
    &= \frac{e^{|\beta|^2}\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)}{\norm{P_e^{(M)}\ket{\alpha_e}}^2}\, \langle \alpha|P_e^{(M)}(\widetilde{\mathcal{L}}_{\mathring{R}_e}^{(M)\dagger})(P_e^{(M)}{P_{\beta}^{(e)}} P_e^{(M)}\otimes I_{e^c})P_e^{(M)}|\alpha\rangle\,.
\end{align*}
The contribution of the Hamiltonian part to the expression on the r.h.s.~for some edge $e=\{i,j\}$ is given by decomposing the coherent states in the Fock basis as in \eqref{Fockbasisdecompositioncoherent}: denoting $P\equiv P_e^{(M)}$, 
\begin{align*}
    h_e(\alpha_e,\beta)
    &\coloneqq i\sum_{e'\in E_{R_e}}\,e^{|\beta|^2}\left(\sum_{k=0}^m \frac{|\alpha_i|^{2k}}{k!}\right)\cdot \left( \sum_{k=0}^m \frac{|\alpha_j|^{2k}}{k!}\right)\cdot\frac{1}{\norm{P\ket{\alpha_e}}^2}\,\langle \alpha|P \big[PH_{e'}P,P(|\beta\rangle\langle \beta|_e\otimes I_{e^c})P\big]P|\alpha\rangle\\
    &=i\, e^{|\alpha_{e}|^2+|\beta|^2}\,\langle \alpha_e| P\big[PH_{e}P,P|\beta\rangle\langle \beta|_e P\big] P|\alpha_e\rangle\\
    &=i \sum_{u,v,u',v'=0}^M\,\frac{\alpha_i^u\alpha_j^{u'}\beta_i^{v}\beta_j^{v'}}
    {\sqrt{u!v!u'!v'!}}\,  \big(\langle uu'|H_e|vv'\rangle-\langle vv'|H_e|uu'\rangle\big)\\
    &=-2\sum_{u,v,u',v'=0}^{M}\,\frac{\alpha_i^u\alpha_j^{u'}\beta_i^{v}\beta_j^{v'}}
    {\sqrt{u!v!u'!v'!}}\,  \operatorname{Im}\big(\langle uu'|H_e|vv'\rangle\big)\,,
\end{align*}
where we also used the assumption that $\lambda^{(e')}_{00k'\ell'}=\lambda^{(e')}_{k\ell 00}=0$, so that $\langle \alpha|P\big[PH_{e'}P,P|\beta\rangle\langle\beta|_e\otimes I_{e^c}P\big]P|\alpha\rangle=0$ for $e\ne e'$. Therefore, one can access the imaginary parts of the matrix elements of the interaction $H_e$ in the Fock basis up to local Fock numbers $d$ by differentiating the polynomial $h_e$ up to $d$ times in $\alpha_i,\alpha_j,\beta_i$ and $\beta_j$ at $0$.
Importantly, while the action of the engineered dissipation on the projected states is no longer trivial, for known values of $\alpha$ and $\beta$, it contributes only an offset after differentiating and evaluating at $0$. This known additive term can be corrected for without worsening the accuracy, so that approximate access to the derivatives of $g_e$ indeed gives approximate access to the derivatives of $h_e$.
A similar procedure to the one described up to now, by appending phases of the form $e^{-i\frac{\pi}{2(u+v')}}$ to $\beta$ similarly to what was done in \Cref{vanillaalgobound}, allows to extract the real parts of these coefficients. It therefore remains to argue that the coefficients $\lambda^{(e)}_{k\ell k'\ell'}$ can be retrieved from $\langle uu'|H_e|vv'\rangle$. Indeed,
\begin{align*}
    \langle uu'|H_e|vv'\rangle
    &=\sum_{k\ell k'\ell'}\lambda^{(e)}_{k\ell k'\ell'}\,\langle u|(a_i^{\dagger})^ka_i^{\ell}|v\rangle \,\langle u' |(a_j^\dagger)^{k'}a_j^{\ell'}|v'\rangle\\
    &=\sum_{k\ell k'\ell'} \lambda^{(e)}_{k\ell k'\ell'} \sqrt{v\dots (v-\ell+1)\,u\dots (u-k+1) v'\dots (v'-\ell'+1)u'\dots (u'-k'+1)}\, \delta_{u-k,v-\ell}\,
    \delta_{u'-k',v'-\ell'}\\
    &= \sum_{kk'}\lambda_{k(v+k-u)k' (v'+k'-u') }^{(e)}\,\sqrt{v\dots (u-k+1)\,u\dots (u-k+1) v'\dots (u'-k'+1)u'\dots (u'-k'+1)}\,.
\end{align*}
Then, we start with the lowest level non-zero coefficients of the form $\lambda^{(e)}_{0101}$. For this, let us take $u=0=u'$ and $v=1=v'$. The sum above indeed reduces to $\langle 00|H_e|11\rangle= \lambda_{0101}^{(e)}$. It is not difficult to observe that higher level coefficients can be learned by increasing the values of $u,v,u',v'$ iteratively. 
Here, note that we only care about summands with $k,k'\leq d=\mathcal{O}(1)$, for which the factors under the square root are in turn at most $\mathcal{O}(1)$, and that the resulting relations can always only involve a number of coefficients that depends on $d=\mathcal{O}(1)$. 
Therefore, we have proved the following result:

\begin{thm}[Refined strategy]\label{refinedstrategy}
    The method described in the previous paragraph provides estimators $\hat{\lambda}^{(e)}_{k\ell k'\ell'}$ of $\lambda^{(e)}_{k\ell k'\ell'}$ such that, with probability $1-\delta$, we have $|\hat{\lambda}^{(e)}_{k\ell k'\ell'}-\lambda^{(e)}_{k\ell k'\ell'}|\le \epsilon$ simultaneously for all $k\ell k'\ell'$, as long as the evolution time for each run scales as $t=\widetilde{\mathcal{O}}(1)$ and the total number of samples is at least 
    \begin{align*}
    N_{\operatorname{samp}}=\widetilde{\mathcal{O}}\left( \frac{1}{{\epsilon}^2}\ln\left(\frac{|E|}{\delta}\right)\right)\,.
    \end{align*}
     Our scheme uses a total evolution time of $T_{\operatorname{evo}}=\widetilde{\mathcal{O}}\big(\epsilon^{-2}\ln\big(\frac{|E|}{\delta}\big)\big)$.
\end{thm}


\section{Multivariate polynomial interpolation}

In this section, we provide a simple error bound for the Lagrange interpolation of multivariate polynomials.
We consider a polynomial $f(X_1,\dots , X_m)$ of $m$ variables of degree $n$ of the form
\begin{align}\label{eq:lagrange-expansion}
    f(X_1,\dots ,X_m)=\sum_{\|\mathbf{e}_i\|_{\ell_1}\le  n}\,\alpha_{\mathbf{e}_i}\,X^{\mathbf{e}_i}
\end{align}
where the sum is over at most $\rho\coloneqq \binom{n+m}{n}$ terms, and where we adopted the notation $\mathbf{e}_i=(e_{i1},\dots e_{im})$ for $e_{ij}\in\mathbb{N}$ with the constraint $\|\mathbf{e}_i\|_{\ell_1}=\sum_j{e_{ij}}\le n$, and $X^{\mathbf{e}_i}=X^{e_{i1}}\dots X^{e_{im}}$. We also assume that we are given $\rho$ distinct points $x_1,\dots,x_\rho\in\mathbb{R}^m$ with their images $f(x_i)=y_i$. The function $f$ can hence written as \cite{Saniee2008}
\begin{align*}
    f(X_1,\dots,X_m)=\sum_{i=1}^\rho\,y_i\,\frac{\Delta_i(X)}{\Delta}\,,
\end{align*}
 with 
 \begin{align*}
\Delta=\det\begin{pmatrix}
x_1^{\mathbf{e}_1} & x_1^{\mathbf{e}_2} & \cdots & x_1^{\mathbf{e}_\rho} \\
x_2^{\mathbf{e}_1} & x_2^{\mathbf{e}_2} & \cdots & x_2^{\mathbf{e}_\rho} \\
\vdots & \vdots & \ddots & \vdots \\
x_\rho^{\mathbf{e}_1} & x_\rho^{\mathbf{e}_2} & \cdots & x_\rho^{\mathbf{e}_\rho}
\end{pmatrix}
 \end{align*}
and where $\Delta_i(X)$ is obtained by replacing the $i$-th row in $\Delta$ by
\[
\begin{pmatrix}
\multicolumn{4}{c}{\dotfill} \\
X^{\mathbf{e}_1} & X^{\mathbf{e}_2} & \cdots & X^{\mathbf{e}_\rho} \\
\multicolumn{4}{c}{\dotfill} 
\end{pmatrix}\,.
\]
Next, we restrict ourselves to the domain $[-1,1]^m$, and pick the points $x_1,\dots , x_\rho$ on a net in such a way that $|\Delta|>0$. The following lemma is immediate {from bounding the terms in \eqref{eq:lagrange-expansion}}.

\begin{lemma}\label{lemmapolyinterpolation}
    In the notations of the previous paragraph, and assuming that $|y_i|\le \epsilon$ for all $i$,
    \begin{align*}
        \|f\|_{L^\infty([-1,1]^m)}\le \,\epsilon\, \frac{\rho\cdot \rho!}{|\Delta|}\,.
    \end{align*}
\end{lemma}
The approximation found in \Cref{lemmapolyinterpolation} depends super-exponentially on the degree of the polynomial. If we simply want to learn derivatives of $f$ at $0$, we can achieve a better polynomial degree-scaling by using the following improvement of \Cref{lemmapolyinterpolation} for single variable polynomials in an iterative manner. 


\begin{lemma}[See \texorpdfstring{\cite[Theorem I.4]{kane2017robust}}]\label{lemmaKane}
    There exists an algorithm which, for any polynomial $f:[-1,1]\to\mathbb{R}$ of degree $M$, takes $n=\mathcal{O}(M)$ inputs $x_1,\dots , x_n\in [-1,1]$ with $x_j\in[\cos\frac{\pi j}{n},\cos\frac{\pi (j-1)}{n}]$, queries noisy outputs $y_1,\dots ,y_n$ such that $|y_i-f(x_i)|\le \sigma$, and returns a polynomial $\widetilde{f}:[-1,1]\to\mathbb{R}$ of degree $M$ such that 
    \begin{align*}
    \|f-\widetilde{f}\,\|_{L^\infty([-1,1])}\le 3\,\sigma\,.
    \end{align*}
    Moreover, the recovery time is polynomial in the number $n$ of inputs.
\end{lemma}

Using the algorithm of \Cref{lemmaKane} in an iterative manner on a multivariate polynomial on $[-1,1]^a$, we can devise \Cref{algoderivatives},
which outputs a good approximation of higher order derivatives of a multivariate polynomial at $0$.  

\medskip

\begin{algorithm}[H]
  \caption{PolyDerivatives }\label{algoderivatives}
  \label{protocol}
  \KwIn{$\sigma$-noisy query access to polynomial $f:[-1,1]^a\to \mathbb{R}$ of degree at most $M\in \mathbb{N}$ in each variable, $k_{\max}\in\mathbb{N}$.}
  \KwOut{List of all higher order partial derivatives of $f$ at $0$ up to order $k_{\operatorname{max}}$ in each variable.}
  \BlankLine
 (i) Query all $n^a$ inputs $(x_{1,k_1}, \ldots, x_{a,k_a})$ such that $x_{j,k_j}\in[\cos\frac{\pi k_j}{n},\cos\frac{\pi (k_j-1)}{n}]$ for all $1\leq j\leq a$ and $1\leq k_j\leq n$;\\
 (ii) Call algorithm described in \Cref{lemmaKane} for all fixed combinations of $x_{j,k_j}$, $j>1$, call the output polynomials $x_1\mapsto \widetilde{f}(x_1,(x_{j,k_j})_{j>1})\equiv \widetilde{f}_{(x_{j,k_j})_{j>1}}(x_1)$;\\
 (iii) Differentiate all polynomials $x_1\mapsto \widetilde{f}_{(x_{j,k_j})_{j>1}}(x_1)$ $k_{\max}$ times at $0$; denote those derivatives by $\widetilde{f}^{(i_1)}(0,(x_{j,k_j})_{j>1})$, $i_1\in[k_{\max}]\cup \{0\}$;\\
 Repeat steps (i)-(ii)-(iii) for $f$ replaced by $x_2\mapsto f^{(i_1)}(0,x_2,(x_{j,k_j})_{j>2})$ and the noisy oracle returning values of $\widetilde{f}^{(i_1)}(0,\cdot)$; denote the outputs $\widetilde{f}^{(i_1,i_2)}(0,0,(x_{j,k})_{j>2})$;\\
 Iterate the procedure $a-2$ times; output numbers $\widetilde{f}^{(i_1,\dots, i_a)}(0,\dots, 0)$.
  \end{algorithm}

\medskip


In the next proposition, we show that \Cref{algoderivatives} indeed outputs a good approximation of the higher order derivatives of the polynomial $f$ at $0$.

\begin{prop}\label{proppolydeiff}
    Let $f:[-1,1]^a\to \mathbb{R}$ be a multivariate polynomial of degree at most $M$. 
    A number $n=\mathcal{O}(M)$ of inputs per variable suffice to ensure that the outputs of \Cref{algoderivatives} satisfy, simultaneously for all $i_1,\dots,i_m\in \{0, 1,\ldots, k_{\mathrm{max}}\}$:
    \begin{align}\label{boundmultidiff}
        \Big|\widetilde{f}^{(i_1,\dots,i_a)}(0,\dots ,0)-\partial_{x_1}^{i_1}\dots \partial_{x_a}^{i_a}f(0,\dots , 0)\Big|\le \big(3\cdot 2^{2k_{\max}-1}\, M^{2k_{\max}}\big)^{a}\, \sigma \,.
    \end{align}
\end{prop}
\begin{proof}
    We first consider steps (ii)-(iii) of \Cref{algoderivatives}. By \Cref{lemmaKane}, we have that,
    \begin{align*}
        \Big\| \widetilde{f}(\cdot, (x_{j,k_j})_{j>1})- f(\cdot, (x_{j,k_j})_{j>1}) \Big\|_{L^\infty([-1,1])}\le 3\sigma \qquad \forall x_{j,k_j},\, j>1\,.
    \end{align*}
    By the one dimensional Markov brothers' inequality (\Cref{Markovbroth}), the derivatives $\widetilde{f}^{(i_1)}(0,(x_{j,k_j})_{j>1})$ all satisfy 
    \begin{align*}
        \Big|\widetilde{f}^{(i_1)}(0,(x_{j,k_j})_{j>1})- \partial_{x_1}^{i_1}f(0,(x_{j,k_j})_{j>1})\Big|\le 2^{2i_1-1}\,\frac{M^2(M^2-1^2)\dots (M^2-(i_1-1)^2)}{1\cdot 3\cdot \dots (2i_1-1)} 3 \sigma\le\,3\cdot  2^{2k_{\max}-1}\, M^{2k_{\max}}\, \sigma\,.
    \end{align*}
    We repeat the procedure for the second variable $x_2$. Again, by \Cref{lemmaKane}, we get that 
    \begin{align*}
        \Big\| \widetilde{f}^{(i_1)}(0, \cdot ,(x_{j,k_j})_{j>2})- \partial_{x_1}^{i_1}f(0,\cdot, (x_{j,k_j})_{j>2}) \Big\|_{L^\infty([-1,1])}\le 9\cdot  2^{2k_{\max}-1}\, M^{2k_{\max}}\, \sigma  \qquad \forall i_1~\forall x_{j,k_j},\, j>2\,.
    \end{align*}
    And therefore, conditioned on this success event, by Markov brothers' inequality (\Cref{Markovbroth}),
    \begin{align*}
        \Big| \widetilde{f}^{(i_1,i_2)}(0, 0 ,(x_{j,k_j})_{j>2})- \partial_{x_1}^{i_1} \partial_{x_2}^{i_2}f(0,0, (x_{j,k_j})_{j>2}) \Big|\le   \big(3\cdot 2^{2k_{\max}-1}\, M^{2k_{\max}}\big)^2\, \sigma  \qquad \forall i_1, i_2~~\forall x_{j,k_j},\, j>2\,.
    \end{align*}
    We repeat these steps $a-2$ times. In the last step, we get that
    \begin{align*}
        \Big| \widetilde{f}^{(i_1,\dots, i_a)}(0, \dots ,0)- \partial_{x_1}^{i_1}\dots \partial_{x_a}^{i_a}f(0,\dots ,0) \Big|\le   \big(3\cdot 2^{2k_{\max}-1}\, M^{2k_{\max}}\big)^{a}\, \sigma\qquad \forall i_1,\ldots,i_a\,.
    \end{align*}
    The result follows.
\end{proof}

In our application of polynomial derivative estimation to the time derivative, the domain of interest is an interval of the form $[0,b]$, $b>0$. Strictly speaking, this is not covered by \Cref{proppolydeiff}. 
To estimate derivatives for such a domain, we will use the following consequence of the Markov brothers' inequality, which can be obtained from \Cref{proppolydeiff} analogously to how \cite[Corollary E.1]{franca2022efficient} was proved starting from \cite[Theorem E.1]{franca2022efficient}:

\begin{lemma}\label{lemmapolydifftime}
    Let $f:\mathbb{R}\to\mathbb{R}$ be a polynomial of degree $M$.
    Let $\delta, \sigma>0$. Take $0<b_1= M^{-2}$ and $b_2 = 2+b_1$.
    Then $n=\mathcal{O}\left(M\right)$ pairs $(x_i,y_i)\in [b_1,b_2]\times\mathbb{R}$ with inputs $x_1,\ldots,x_n\in [b_1, b_2]$ such that $x_j\in [b_1+\cos\frac{\pi j}{n},b_1+\cos\frac{\pi (j-1)}{n}]$ for all $j$ and satisfying
    \begin{align*}
        \lvert f(x_i) - y_i\rvert\leq\sigma
    \end{align*}
    suffice to obtain a degree-$M$ polynomial $\widetilde{f}$ that satisfies
    \begin{align*}
        \lvert f'(0) - \widetilde{f}'(0)\rvert 
        \leq 3eM^2\sigma\, .
    \end{align*}
\end{lemma}

In fact, the reconstructed polynomial $\widetilde{f}$ in \Cref{lemmapolydifftime} is that of \Cref{lemmaKane}, only that the domain is adapted from $[-1,1]$ to $[b_1,b_2]$. In particular, the reconstruction time scales efficiently with the number $n$ of inputs and thus with the degree $M$. For our Hamiltonian learning application, we consider a polynomial $f:[0,b_2]\times [-1,1]^a\to\mathbb{R}$, and we require estimates for higher order partial derivatives that always only contain a single partial derivative w.r.t.~the first variable. We can straightforwardly combine \Cref{proppolydeiff} and \Cref{lemmapolydifftime} to produce such estimates. This combination yields:

\begin{cor}\label{coroboundwithtimedifference}
    Let $f:[0,b_2]\times [-1,1]^a\to\mathbb{R}$, $(t,x_1,\ldots,x_a)\mapsto f(t,x_1,\ldots,x_a)$, be a multivariate polynomial of degree at most $M$ in each of its variables.
    There is an algorithm that, using $\sigma$-noisy estimates of values of $f$ on $N =\mathcal{O}\big(M^{a+1}\big)$ inputs, outputs estimates $\widetilde{f}^{(i_1,\ldots,i_a)}(0,\ldots,0)$ that simultaneously satisfy
    \begin{align}\label{boundderivativestimebla}
        \Big| \widetilde{f}^{(i_1,\dots, i_a)}(0, \dots ,0)- \partial_t \partial_{x_1}^{i_1}\dots \partial_{x_a}^{i_a}f(0,\dots ,0) \Big|
        \le   \big(3\cdot 2^{2k_{\max}-1}\, M^{2k_{\max}}\big)^{a}\, 3eM^2\sigma\qquad \forall i_1,\ldots,i_a\in\{0,\ldots,k_{\mathrm{max}}\}\,.
    \end{align}
\end{cor}



\section{Trotter-Kato product formula}\label{sec:trotter}

For our protocol, we assumed access not just to the evolution under the unknown Hamiltonian, but rather to an evolution with added engineered dissipation.
In this section, we justify access to this overall dissipative evolution by verifying that it can be well approximated in terms of products of short-time single Hamiltonian evolution and short-time single dissipation via Trotter-Kato's product formula. This is the content of the following result. Here, we recall that (see \eqref{eq:sum-sobolev-operator})
\begin{equation*}
    \cW_{\Sigma}^{2k}\coloneqq\sum_{j\in V}(N_j+I)^{k/2}\cdot(N_j+I)^{k/2}\,.
\end{equation*}
\begin{prop}\label{thm:trotter}
	Let $(\cH_E,\cT_f)$ be the Hamiltonian considered in \eqref{eq:Hamiltonian} and $(\cL_V^{(\alpha,p)},\cT_f)$ the $p$-photon dissipation defined in \eqref{pphtondissipation} with common domain $\cT_f$ defined in \Cref{secphasespaceformalism}. If $p\geq 2d+2$ and $|\alpha_i|=\mathcal{O}(1)$ for all $i \in V$, then for all $\rho\in W^{8p,1}$, $t\ge 0$ and $n\in\N$
	\begin{equation*}
		\norm{\left(e^{\frac{t}{n}\cH_E}e^{\frac{t}{n}\cL_V^{(\alpha,p)}}\right)^n(\rho)-e^{t(\cH_E+\cL_V^{(\alpha,p)})}(\rho)}_1\leq\frac{2c}{\sqrt{n}}\|\cW_\Sigma^{4p}(\rho)\|_1+\frac{3cm^2}{n}\left(4^{p+1}e^{4p \Gamma''t}\,\max_{\substack{i\in V}}\big\{\|\rho_i\|_{W^{8p,1}}\big\}+\Upsilon_{4p}(t)\right)\,,
	\end{equation*}
    where we recall that $m=|V|$, $c$ is the relative bound given in \Cref{lem:rel-bounds}, and $\Gamma'',\Upsilon_{4p}$ were introduced in \Cref{lemmamoments}.
\end{prop}

The proof follows Chernoff's proof for the convergence of the Trotter-Kato product formula in the strong topology (see \cite{Chernoff.1968} or Section 4 in \cite{Moebus.2023}) and uses the following three ingredients. First, we restate the Chernoff $\sqrt{n}$-Lemma \cite{Chernoff.1968}:
\begin{lemma}[Chernoff $\sqrt{n}$-Lemma]\label{lem:chnernoff}
    Let $C$ be a contraction on $\cT_1(\cH_m)$. Then for all $n\in\N$, the operator $e^{n(C-I)}$ is a contraction and for all $\rho\in \cT_1$
    \begin{equation*}
        \|C^n\rho-e^{n(C-I)}\rho\|_1\leq\sqrt{n}\|(C-I)\rho\|_1\,.
    \end{equation*}
\end{lemma}
Second, we use relative bounds on concatenations of $\cH_E$ and $\cL_V^{(\alpha,p)}$ with respect to $\mathcal{W}_\Sigma^{2k}$:
\begin{lemma}[Relative bounds]\label{lem:rel-bounds}
    Let $(\cH_E,\cT_f)$ be the Hamiltonian considered in \eqref{eq:Hamiltonian} and $(\cL_V^{(\alpha,p)},\cT_f)$ the $p$-photon dissipation introduced in \eqref{pphtondissipation} with  $|\alpha_i|\leq\eta$ for all $i\in V$. Then, for all states $\rho\in\dom(\cW_{\Sigma}^{2k})$ for $k$ appropriately
    \begin{align}
        \|\cH_E\rho\|_1&\leq2\gamma L(d+1)^4d! \,\|\cW_{\Sigma}^{8d}(\rho)\|_1&&\eqqcolon c_1\|\cW_{\Sigma}^{8d}(\rho)\|_1\,,\label{eq:rel-bound-cH}\\
        \|\cH_E^2\rho\|_1&\leq4\gamma^2L^2(1+d)^8(d!)^2(1+d)^{2d}\|(\cW_\Sigma^{8d})^2(\rho)\|&&\eqqcolon c_2\|(\cW_{\Sigma}^{8d})^2(\rho)\|_1\,,\label{eq:rel-bound-cH2}\\
        \|\cL_V^{(\alpha,p)}(\rho)\|_1&\leq8\max\{\sqrt{p!},\eta^p\sqrt{p!},\eta^{2p}\}\,\|\cW_{\Sigma}^{4p}(\rho)\|_1&&\eqqcolon c_3\|\cW_{\Sigma}^{4p}(\rho)\|_1\,,\label{eq:rel-bound-cV}\\
        \|(\cL_V^{(\alpha,p)})^2(\rho)\|_1&\leq64\max\{1,\eta^{4p}\}\max\{p!(1+p)^{p}\}\|(\cW_{\Sigma}^{4p})^2(\rho)\|_1&&\eqqcolon c_4\|(\cW_{\Sigma}^{4p})^2(\rho)\|_1\,,\label{eq:rel-bound-cV2}\\
        \|\cH_E\cL_V^{(\alpha,p)}(\rho)\|_1&\leq16\gamma L(1+d)^4d!\sqrt{p!}(1+p)^d\max\{1,\eta^{2p}\}\|\cW_{\Sigma}^{8d}\cW_{\Sigma}^{4p}(\rho)\|&&\eqqcolon c_5\|\cW_{\Sigma}^{8d}\cW_{\Sigma}^{4p}(\rho)\|_1\,,\label{eq:rel-bound-cHcV}
    \end{align}
    and we define $c=\max\{c_1,c_2,c_3,c_4,c_5\}$.
\end{lemma}

For our third ingredient, we assume $p\geq4d+2$ and $k\geq2d$, which implies $d<k$ and $p\ge \lceil\frac{2kd}{k-d}+2\rceil$, so that we can use the bound given in \eqref{ineqMarkovchain} of \Cref{lemmamoments}:
\begin{equation*}
    \tr[e^{s\widetilde{\cL}}(\rho) (N_i+I)^k]\le 4^{p+1}e^{k \Gamma''s}\,\max_{i\in V}\big\{\|\rho_i\|_{W^{2k,1}}\big\}+\Upsilon_k(s)
\end{equation*}
for some constant $\Gamma''$ and a function $\Upsilon_k$ such that, for $s,p,d,|\alpha_i|=\mathcal{O}(1)$, $\Upsilon_k(s)=k^{\mathcal{O}(k)}$, and $\Gamma''=\mathcal{O}(1)$.
Combining these three tools, we can now prove the Trotter product formula. 


Note that the statements given here are strongly adapted to the setup of the paper. However, they can be generalized to contractive semigroups on Banach spaces satisfying a relative boundedness condition with respect to a reference operator (here $\cW_{\Sigma}^k$) satisfying certain conditions.


\begin{proof}[Proof of \Cref{thm:trotter}]
    First, without loss of generality we absorb $t$ into the generators and define $F(n)\coloneqq e^{\frac{1}{n}\cH_E}e^{\frac{1}{n}\cL_V^{(\alpha,p)}}$. Then, Chernoff's $\sqrt{n}$-Lemma in combination with Lemma II.1.3 in \cite{Engel.2000} imply
    \begin{equation*}
        \begin{aligned}
            \big\|F(n)^n(\rho)-e^{n(F(n)-I)}(\rho)\|_1&\leq\sqrt{n}\,\big\|(F(n)-I)(\rho)\big\|_1\\
            &\leq\sqrt{n}\,\left\|\left(e^{\frac{1}{n}\cH_E}\left(e^{\frac{1}{n}\cL_V^{(\alpha,p)}}-I\right)+e^{\frac{1}{n}\cH_E}-I\right)(\rho)\right\|_1\\
            &\leq\frac{1}{\sqrt{n}}\left(\|\cL_V^{(\alpha,p)}(\rho)\|_1+\|\cH_E(\rho)\|_1\right)\,.
        \end{aligned}
    \end{equation*}
    Next, by \Cref{lem:rel-bounds}, there is a constant $c\geq0$ such that 
    \begin{equation}\label{eq-trotter:chernoff-bound}
        \big\|F(n)^n(\rho)-e^{n(F(n)-I)}(\rho)\big\|_1\leq\frac{c}{\sqrt{n}}\left(\|\cW_{\Sigma}^{4p}(\rho)\|_1+\|\cW_{\Sigma}^{8d}(\rho)\|_1\right)\,.
    \end{equation}
    In the next step, we exploit the perturbation theory introduced in \cite[Theorem 2.11]{Gondolfetal2023} 
    % \mcc{We have to make sure that this Theorem reference is consistent with what appears on the arXiv.}, 
    so that: 
        \begin{equation*}
        \begin{aligned}
            \big\|e^{n(F(n)-I)}(\rho)-e^{\cH_E+\cL_V^{(\alpha,p)}}(\rho)\big\|_1&\leq\int_0^1\big\|e^{(1-s)n(F(n)-I)}\left(n(F(n)-I)-\left(\cH_E+\cL_V^{(\alpha,p)}\right)\right)e^{s(\cH_E+\cL_V^{(\alpha,p)})}(\rho)\big\|_1\,ds\\
            &\leq\int_0^1\big\|\left(n(F(n)-I)-\left(\cH_E+\cL_V^{(\alpha,p)}\right)\right)e^{s(\cH_E+\cL_V^{(\alpha,p)})}(\rho)\big\|_1\,ds\,.
        \end{aligned}
    \end{equation*}
    where we used in the last line that $s\mapsto e^{(1-s)n(F(n)-I)}$ is a contraction semigroup by \Cref{lem:chnernoff} and the definition of $F(n)$. Next, we define $\rho_s\coloneqq e^{s(\cH_E+\cL_V^{(\alpha,p)})}(\rho)$ and use Lemma II.1.3 in \cite{Engel.2000} again to show
     \begin{equation*}
        \begin{aligned}
            \left(n(F(n)-I)-\cH_E-\cL_V^{(\alpha,p)}\right)(\rho_s)&=\left(n\left(e^{\frac{1}{n}\cH_E}e^{\frac{1}{n}\cL_V^{(\alpha,p)}}-e^{\frac{1}{n}\cH_E}+e^{\frac{1}{n}\cH_E}-I\right)-\cH_E-\cL_V^{(\alpha,p)}\right)(\rho_s)\\
            &=\int_0^1\left(\left(e^{\frac{1}{n}\cH_E}e^{\frac{v_1}{n}\cL_V^{(\alpha,p)}}-I\right)\cL_V^{(\alpha,p)}+\left(e^{\frac{v_1}{n}\cH_E}-I\right)\cH_E\right)(\rho_s)dv_1\\
            &=\int_0^1\left(\left(e^{\frac{1}{n}\cH_E}e^{\frac{v_1}{n}\cL_V^{(\alpha,p)}}-e^{\frac{1}{n}\cH_E}+e^{\frac{1}{n}\cH_E}-I\right)\cL_V^{(\alpha,p)}+\left(e^{\frac{v_1}{n}\cH_E}-I\right)\cH_E\right)(\rho_s)dv_1\\
            &=\frac{1}{n}\int_{[0,1]^2}\left(v_1e^{\frac{1}{n}\cH_E}e^{\frac{v_1v_2}{n}\cL_V^{(\alpha,p)}}(\cL_V^{(\alpha,p)})^2+e^{\frac{v_2}{n}\cH_E}\cH_E\cL_V^{(\alpha,p)}+v_1e^{\frac{v_1v_2}{n}\cH_E}\cH_E^2\right)(\rho_s)d^2v\,,
        \end{aligned}
    \end{equation*}
    so that 
    \begin{equation*}
        \begin{aligned}
            \big\|e^{n(F(n)-I)}(\rho)-e^{\cH_E+\cL_V^{(\alpha,p)}}(\rho)\big\|_1&\leq\frac{1}{n}\int_0^1\Bigl(\big\|(\cL_V^{(\alpha,p)})^2(\rho_s)\big\|_1+\big\|\cH_E\cL_V^{(\alpha,p)}(\rho_s)\big\|_1+\big\|(\cH_E)^2(\rho_s)\big\|_1\Bigr)\,ds\\
            &\leq\frac{c}{n}\int_0^1\Bigl(\big\|(\cW_{\Sigma}^{4p})^2(\rho_s)\big\|_1+\big\|\cW_{\Sigma}^{8d}\cW_{\Sigma}^{4p}(\rho_s)\big\|_1+\big\|(\cW_\Sigma^{8d})^2(\rho_s)\big\|_1\Bigr)\,ds\\
            &\leq\frac{3c}{n}\int_0^1\big\|(\cW_{\Sigma}^{4p})^2(\rho_s)\big\|_1ds\,,
        \end{aligned}
    \end{equation*}
    by the relative bounds in \Cref{lem:rel-bounds}. Moreover, note that $\rho_s$ is a state and $\cW_\Sigma$ is positivity preserving, so that all norms can be written as a trace and the assumption $2d+2\leq p$ explains the last inequality. Next, by Young's inequality:
    \begin{equation*}
        \begin{aligned}
            \big\|(\cW_{\Sigma}^{4p})^2(\rho_s)\big\|_1&=\sum_{i,j\in V}\tr\big[\sqrt{\rho_s}(N_i+I)^{2p}(N_j+I)^{2p}\sqrt{\rho_s}\,\big]\\
            &\le \frac{1}{2}\sum_{i,j\in V}\tr\big[\sqrt{\rho_s}\left((N_i+I)^{4p}+(N_j+I)^{4p}\right)\sqrt{\rho_s}\,\big]\\
            &=m\,\|\cW_{\Sigma}^{8p}(\rho_s)\|_1\,.
        \end{aligned}
    \end{equation*}
    Finally, we use the assumption $p\geq 4d+2$ and \Cref{lemmamoments} to show 
    \begin{equation}\label{eq-trotter:diff}
        \begin{aligned}
            \big\|e^{n(F(n)-I)}(\rho)-e^{\cH_E+\cL_V^{(\alpha,p)}}(\rho)\big\|_1&\leq\frac{3\,c\,m}{n}\int_0^1\big\|\cW_{\Sigma}^{8p}(\rho_s)\big\|_1ds\\
            &=\frac{3\,c\,m}{n}\int_0^1\left(\sum_{j\in V}\tr\big[\rho_s(N_j+I)^{4p}\big]\right)ds\\
            &\leq\frac{3\,c\,m}{n}\int_0^1m\left(4^{p+1}e^{4p \Gamma''s}\,\max_{i\in V}\big\{\|\rho_i\|_{W^{8p,1}}\big\}+\Upsilon_{4p}(s)\right)ds\\
            &\leq\frac{3\,c\,m^2}{n}\left(4^{p+1}e^{4p \Gamma''}\,\max_{\substack{i\in V}}\big\{\|\rho_i\|_{W^{8p,1}}\big\}+\Upsilon_{4p}(1)\right)\,.
        \end{aligned}
    \end{equation}
    \Cref{lemmamoments} is applicable because $p\geq 4d+2$ and $k=2p$ shows $d<k$ and $p\ge \big\lceil\frac{2kd}{k-d}+2\big\rceil$. Combining the bound in \eqref{eq-trotter:chernoff-bound} and in \eqref{eq-trotter:diff} finishes the proof.
\end{proof}
In the next step, we prove the relative bounds of \Cref{lem:rel-bounds} used in the proof above: 
\begin{proof}[Proof of \Cref{lem:rel-bounds}]
    Before diving into the proof of the relative bounds of the generators w.r.t.~$\cW_{\Sigma}^k$, we shortly repeat two useful tools for derivations with $a_i$ and $\ad_i$ for $i\in V$: first,
    \begin{align*}
            &(\ad_i)^pa_i^p=(N_i-(p-1)I)(N_i-(p-2)I)\cdots(N_i-I)N_i\equiv N_i[-p+1:0]\,,\\
            & a_i^p(\ad_i)^p=(N_i+I)(N_i+2I)\cdots(N_i+(p-1)I)(N_i+pI)\equiv N_i[1:p] 
    \end{align*}
    (see \eqref{eq:aadag} and \eqref{eq:aadag2}), and second the commutation relations for a given real-valued measurable function $f:\mathbb{Z}\to\mathbb{R}$, 
    \begin{equation*}
        \begin{aligned}
            a&f(N_k+jI)&&=&&f(N_k + (j+1)I)a,&\quad\quad a^\dagger&f(N_k-jI)\,&&=&&f(N_k - (j+1)I)a^\dagger\,,\\
            &f(N_k-jI)a\,&&=&a&f(N_k - (j+1)I)\,,&\quad\quad &f(N_k+jI)a^\dagger&&=&a^\dagger&f(N_k + (j+1)I)
        \end{aligned}
    \end{equation*}
    (see \eqref{eq:symmetry-function}). In the first part of the proof, we show for $q\in\R_{\geq 0}$ and $i\in V$ as well as $(i,j),(j,h)\in E$      
    \begin{align}
        \|(N_i+I)^{q}L_i^{(\alpha_i,p)}(N_i+I)^{-q-p/2}\|_\infty&\leq2\max\{1,\eta^{p}\}\,,\label{eq:rel-NL}\\
        \|(N_i+I)^{q}(L_i^{(\alpha_i,p)})^\dagger(N_i+I)^{-q-p/2}\|_\infty&\leq2\max\{\sqrt{p!}(1+p)^q,\eta^{p}\}\,,\label{eq:rel-NLdag}\\
        \|(N_i+I)^q(N_j+I)^qH_{(j,h)}(N_i+I)^{-q}(N_j+I)^{-d-q}(N_h+I)^{-d}\|_\infty&\leq L(1+d)^4d!(1+d)^{q+1_{i=h}q}\,.\label{eq:rel-NH}
    \end{align}
    The first two bound \eqref{eq:rel-NL} reduces for $\ket{\phi}\in\cH_E$ to 
    \begin{equation*}
        \begin{aligned}
            \|(N_i+I)^qa_i^p(N_i+I)^{-q-p/2}\ket{\varphi}\|^2&= \bra{\varphi}(N_i+(1-p)I)^q(N_i+I)^{-q-p/2}(\ad_i)^p a_i^p(N_i+I)^{-q-p/2}(N_i+(1-p)I)^q\ket{\varphi}\\
            &= \bra{\varphi}N_i[1-p:0](N_i+I)^{-2q-p}(N_i+(1-p)I)^{2q}\ket{\varphi}\\
            &\leq\|\varphi\|^2
        \end{aligned}
    \end{equation*}
    where we used the commutation relation \eqref{eq:symmetry-function} for $f(n)=(n+1)^q1_{n\geq0}$ so that $(N+I)^qa^p=a^p(N+(1-p)I)^q$, $N_i[1-p:0]\leq(N_i+I)^p$, and $(N_i+(1-p)I)^q\leq (N_i+I)^q$. Note that $(N_i+(1-p)I)^q$ is defined by $f(n)=(n+1)^q1_{n\geq0}$ and thereby Fock basis elements less than $p-1$ are set to be zero. Similarly, the bound \eqref{eq:rel-NLdag} follows from
    \begin{equation*}
        \begin{aligned}
            \|(N_i+I)^q(\ad_i)^p(N_i+I)^{-q-p/2}\ket{\varphi}\|^2&= \bra{\varphi}(N_i+I)^{-q-p/2} (N_i+(1+p)I)^qa_i^p(\ad_i)^p(N_i+(1+p)I)^q(N_i+I)^{-q-p/2}\ket{\varphi}\\
            &= \bra{\varphi}N_i[1:p](N_i+I)^{-2q-p}(N_i+(1+p)I)^{2q}\ket{\varphi}\\
            &\leq p!(1+p)^{2q}\|\varphi\|^2
        \end{aligned}
    \end{equation*}
    where we have additionally used $N_i[1:p]\leq p!(N_i+I)^p$ and $(N_i+(1+p)I)^q\leq(1+p)^q(N_i+I)^q$. With help of these two bounds, we can directly prove \eqref{eq:rel-NL} and \eqref{eq:rel-NLdag}. i.e.~
    \begin{equation*}
        \begin{aligned}
            \|(N_i+I)^{q}L_i^{(\alpha_i,p)}(N_i+I)^{-q-p/2}\|_\infty&\leq\|(N_i+I)^{q}a_i^p(N_i+I)^{-q-p/2}\|_\infty+\eta^p\leq2\max\{1,\eta^{p}\}\,,\\
            \|(N_i+I)^{q}(L_i^{(\alpha_i,p)})^\dagger(N_i+I)^{-q-p/2}\|_\infty&\leq\|(N_i+I)^{q}(\ad)_i^p(N_i+I)^{-q-p/2}\|_\infty+\eta^p\leq2\max\{\sqrt{p!}(1+p)^q,\eta^{p}\}\,,
        \end{aligned}
    \end{equation*}
    because $\|(N+I)^{-q}\|_\infty\leq1$, which we will use from time to time. Next, we consider the Hamiltonian inequality \eqref{eq:rel-NH}. For this, we follow the derivation in the proof of \Cref{finite-bounds-generator}, in particular \eqref{eq:inf-norm-hamiltonian}: By definition 
    \begin{equation*}
        H_{\{j,h\}}=\sum_{k,\ell, k', \ell'=0}^d\lambda^{(j,h)}_{k\ell k' \ell'}\,(a_j^\dagger)^{k}a_j^{\ell}(a_h^\dagger)^{k'}a_h^{\ell'}\,.
    \end{equation*}
    Then, the same tricks as before (\eqref{eq:symmetry-function}, \eqref{eq:aadag}, \eqref{eq:aadag2}) or as used in \Cref{finite-bounds-generator} show
    \begin{equation*}
        \begin{aligned}
            \|&(N_i+I)^q(N_j+I)^q(a_j^\dagger)^{k}a_j^{\ell}(a_h^\dagger)^{k'}a_h^{\ell'}(N_i+I)^{-q}(N_j+I)^{-d-q}(N_h+I)^{-d}\ket{\phi}\|^2\\
            &=\bra{\phi}N_j[1-\ell:0]N_j[1-\ell:k-\ell]N_h[1-\ell':0]N_h[1-\ell':k'-\ell']\\
            &\qquad\quad\cdot(N_i+(1+1_{i=h}(k'-\ell')I)^{2q}(N_j+(1+k-\ell)I)^{2q}(N_i+I)^{-2q}(N_j+I)^{-2d-2q}(N_h+I)^{-2d}\ket{\phi}\\
            &\leq(d!)^2(1+d)^{2q+1_{i=h}2q}
        \end{aligned}
    \end{equation*}
    so that
    \begin{equation*}
        \begin{aligned}
            \|&(N_i+I)^q(N_j+I)^q\,H_{(j,h)}\,(N_i+I)^{-q}(N_j+I)^{-d-q}(N_h+I)^{-d}\|_\infty\leq L(1+d)^4d!(1+d)^{q+1_{i=h}q}
        \end{aligned}
    \end{equation*}
    proves \Cref{eq:rel-NH} and finishes the first part of the proof. In the second part, we use the bounds in \eqref{eq:rel-NL}, \eqref{eq:rel-NLdag}, and \eqref{eq:rel-NH} to prove relative bounds on $\cL_V^{(\alpha,p)}$, $\cH_E$, $(\cL_V^{(\alpha,p)})^2$, $\cH_E\cL_V^{(\alpha,p)}$, and $\cH_E^2$, i.e.~\eqref{eq:rel-bound-cH}-\eqref{eq:rel-bound-cHcV}. We start with the bound \eqref{eq:rel-bound-cH} for which HÃ¶lder's inequality and \eqref{eq:rel-NH} with $q=0$ show for $\rho\in\dom(\cW_{\Sigma}^{8d})$
    \begin{equation*}
        \begin{aligned}
            \|\cH_E\rho\|_1&\leq \sum_{e=\{i,j\}\in E}\|H_{e}(N_i+I)^{-d}(N_{{j}}+I)^{-d}(N_{{i}}+I)^{d}(N_j+I)^{d}\rho\|_1+\|\rho H_{e}\|_1\\
            &\leq L(d+1)^4d!\sum_{\{i,j\}\in E}\|(N_i+I)^{d}(N_j+I)^{d}\rho(N_i+I)^{d}(N_j+I)^{d}(N_i+I)^{-d}(N_j+I)^{-d}\|_1+\|\rho H_{e}\|_1\\
            &\leq 2L(d+1)^4d!\sum_{\{i,j\}\in E}\tr[(N_i+I)^{d}(N_j+I)^{d}\rho(N_i+I)^{d}(N_j+I)^{d}]\\
            &= 2L(d+1)^4d!\sum_{\{i,j\}\in E}\tr[\sqrt{\rho}(N_i+I)^{2d}(N_j+I)^{2d}\sqrt{\rho}]\\
            &\leq L(d+1)^4d!\sum_{\{i,j\}\in E}\left(\|(N_i+I)^{2d}\rho(N_i+I)^{2d}\|_1+\|(N_j+I)^{2d}\rho(N_i+I)^{2d}\|_1\right)\\
            &\leq 2\gamma L(d+1)^4d! \,\|\cW_{\Sigma}^{8d}(\rho)\|_1,
        \end{aligned}
    \end{equation*}
    where we used Young's inequality in the fifth step. In the last inequality, we used that, as the graph has degree $\gamma$, every vertex in $V$ has at most $\gamma$ neighbors and the positivity of $(N_i+I)^{2d}\rho(N_i+I)^{2d}$ to rewrite the 1-norm as a trace so that
    \begin{equation*}
        \sum_{i\in V}\,\|(N_i+I)^{2d}\rho(N_i+I)^{2d}\|_1=\sum_{i\in V}\,\tr[(N_i+I)^{2d}\rho(N_i+I)^{2d}]=\|\cW_{\Sigma}^{8d}(\rho)\|_1\,.
    \end{equation*}

    Next, we continue proving the bound on the dissipation in \eqref{eq:rel-bound-cV} using again HÃ¶lder's inequality and \eqref{eq:rel-NL} for $q=0$. In the following auxiliary bound, we will see that one advantage of the proven bounds \eqref{eq:rel-NL}-\eqref{eq:rel-NH} is that these are iteratively applicable. For $i\in V$,
    \begin{equation}\label{eq:rel-LdagL}
        \begin{aligned}
            \|(L_i^{(\alpha_i,p)})^\dagger L_i^{(\alpha_i,p)}(N_i+I)^{-p}\|_\infty&=\|(L_i^{(\alpha_i,p)})^\dagger (N_i+I)^{-p/2}(N_i+I)^{p/2}L_i^{(\alpha_i,p)}(N_i+I)^{-p}\|_\infty\\
            &\leq4\max\{\sqrt{p!},\eta^p\sqrt{p!},\eta^{2p}\}
        \end{aligned}
    \end{equation}
    by applying \eqref{eq:rel-NLdag} with $q=0$ and \eqref{eq:rel-NL} with $q=p/2$. 
    Let $\rho\in\dom(\cW_{\Sigma}^{4p})$ 
    \begin{equation}\label{eq:rel-boung-cL}
        \begin{aligned}
            \|\cL_V^{(\alpha,p)}\rho\|_1&\leq\sum_{i\in V}\|L_i^{(\alpha_i,p)}(N_i+I)^{-p+p}\rho(N_i+I)^{p-p}(L_i^{(\alpha_i,p)})^\dagger\|_1+\|(L_i^{(\alpha_i,p)})^\dagger L_i^{(\alpha_i,p)}(N_i+I)^{-p+p}\rho\|_1\\
            &\leq\sum_{i\in V}\|L_i^{(\alpha_i,p)}(N_i+I)^{-p/2}\|_\infty\,\|(N_i+I)^{-p/2}\|_\infty^2\,\|(N_i+I)^{p}\rho(N_i+I)^{p}\|_1\,\|(N_i+I)^{-p/2}(L_i^{(\alpha_i,p)})^\dagger\|_\infty\\
            &\qquad+\|(L_i^{(\alpha_i,p)})^\dagger L_i^{(\alpha_i,p)}(N_i+I)^{-p}\|_\infty\,\|(N_i+I)^{p}\rho(N_i+I)^{p}\|_1\,\|(N_i+I)^{-p}\|_\infty\,.
        \end{aligned}
    \end{equation}
    Then, we use $\|A\|_\infty=\|A^\dagger\|_\infty$ for $A\in\cT_1$, apply the bounds \eqref{eq:rel-NL} with $q=0$ and \eqref{eq:rel-LdagL}, and use $\|(N_i+I)^{-p}\|_\infty\leq1$ to show
    \begin{equation*}
        \begin{aligned}
            \|\cL_V^{(\alpha,p)}\rho\|_1
            &\leq\sum_{i\in V}4\max\{1,\eta^{2p}\}\,\|(N_i+I)^{p}\rho(N_i+I)^{p}\|_1+4\max\{\sqrt{p!},\eta^p\sqrt{p!},\eta^{2p}\}\,\|(N_i+I)^{p}\rho(N_i+I)^{p}\|_1\\
            &\leq8\max\{\sqrt{p!},\eta^p\sqrt{p!},\eta^{2p}\}\sum_{i\in V}\,\|(N_i+I)^{p}\rho(N_i+I)^{p}\|_1\\
            &\leq8\max\{\sqrt{p!},\eta^p\sqrt{p!},\eta^{2p}\}\,\|\cW_{\Sigma}^{4p}(\rho)\|_1\,,
        \end{aligned}
    \end{equation*}
    where we have used again the positivity of $(N_i+I)^p\rho(N_i+I)^p$.
    
    Next, we prove a relative bound on $\cH_E^2$ \eqref{eq:rel-bound-cH2} using the bound \eqref{eq:rel-NH}. For $\rho\in\dom((\cW_{\Sigma}^{2d})^2)$, \eqref{eq:rel-NH} with $q=0$ shows
    \begin{equation*}
        \begin{aligned}
            \|\cH_E^2(\rho)\|_1&\leq\sum_{e=\{i,j\}\in E}\|H_e\cH_E(\rho)\|_1+\|\cH_E(\rho)H_e\|_1\\
            &\leq 2L(1+d)^4d!\sum_{e=\{i,j\}\in E}\|(N_i+I)^d(N_j+I)^d\cH_E(\rho)(N_i+I)^d(N_j+I)^d\|_1\,.
        \end{aligned}
    \end{equation*}
    Then, \eqref{eq:rel-NH} with $q=d$ proves
    \begin{equation*}
        \begin{aligned}
            \|(N_i&+I)^d(N_j+I)^dH_{e'}\rho(N_i+I)^d(N_j+I)^d\|_1\\
            &\leq L(1+d)^4d!(1+d)^{2d}\|(N_i+I)^d(N_j+I)^d(N_{i'}+I)^d(N_{j'}+I)^d\rho(N_i+I)^d(N_j+I)^d(N_{i'}+I)^d(N_{j'}+I)^d\|_1\\
            &=L(1+d)^4d!(1+d)^{2d}\tr[\sqrt{\rho}(N_i+I)^{2d}(N_{i'}+I)^{2d}(N_j+I)^{2d}(N_{j'}+I)^{2d}\sqrt{\rho}]\,.
        \end{aligned}
    \end{equation*}
    Then, Young's inequality shows 
    \begin{equation*}
        \begin{aligned}
            (N_i+I)^{2d}(N_{i'}+I)^{2d}(N_j+I)^{2d}(N_{j'}+I)^{2d}\leq\frac{1}{2}\left((N_i+I)^{4d}(N_{i'}+I)^{4d}+(N_j+I)^{4d}(N_{j'}+I)^{4d}\right)
        \end{aligned}
    \end{equation*}
    so that
    \begin{equation*}
        \begin{aligned}
            \|\cH_E^2(\rho)\|_1&\leq4L^2(1+d)^8(d!)^2(1+d)^{2d}\sum_{e,e'\in E}\tr[\sqrt{\rho}(N_i+I)^{2d}(N_{i'}+I)^{2d}(N_j+I)^{2d}(N_{j'}+I)^{2d}\sqrt{\rho}]\\
            &\leq2L^2(1+d)^8(d!)^2(1+d)^{2d}\sum_{e,e'\in E}\tr[\sqrt{\rho}\left((N_i+I)^{4d}(N_{i'}+I)^{4d}+(N_j+I)^{4d}(N_{j'}+I)^{4d}\right)\sqrt{\rho}]\\
            &\leq4\gamma^2L^2(1+d)^8(d!)^2(1+d)^{2d}\|(\cW_\Sigma^{8d})^2(\rho)\|
        \end{aligned}
    \end{equation*}
    where we used again $|E|\leq\gamma|V|$. Next, we continue with the case $(\cL_V^{(\alpha,p)})^2$ \eqref{eq:rel-bound-cV2}. Here, we use again \eqref{eq:rel-NL} and \eqref{eq:rel-NLdag} in the same iterative way as before:
    \begin{equation*}
        \begin{aligned}
            \|(\cL_V^{(\alpha,p)})^2(\rho)\|_1&\leq64\max\{1,\eta^{4p}\}p!(1+p)^{p}\sum_{i,j\in V}\|(N_i+I)^{p}(N_j+I)^{p}(\rho)(N_i+I)^{p}(N_j+I)^{p}\|_1\\
            &\leq64\max\{1,\eta^{4p}\}p!(1+p)^{p}\|(\cW_{\Sigma}^{4p})^2(\rho)\|_1\,.
        \end{aligned}
    \end{equation*}
    Finally, we consider $\cH_E\cL_V^{(\alpha,p)}$ which is a combination from both proofs above, i.e.~uses \eqref{eq:rel-NH} with $q=0$, \eqref{eq:rel-NLdag} with $q=d$, and \eqref{eq:rel-NL}:
    \begin{equation*}
        \begin{aligned}
            \|\cH_E\cL_V^{(\alpha,p)}(\rho)\|_1&\leq 16L(1+d)^4d!\max\{\sqrt{p!}(1+p)^d,\sqrt{p!}(1+p)^d\eta^p,\eta^{2p}\}\\
            &\qquad\qquad\cdot\sum_{e=\{i,j\}\in E}\sum_{h\in V}\|(N_i+I)^d(N_j+I)^d(N_h+I)^p\rho(N_i+I)^d(N_j+I)^d(N_h+I)^p\|_1\\
            &\leq 16\gamma L(1+d)^4d!\max\{\sqrt{p!}(1+p)^d,\sqrt{p!}(1+p)^d\eta^p,\eta^{2p}\}\|\cW_{\Sigma}^{8d}\cW_{\Sigma}^{4p}(\rho)\|\\
            &\leq 16\gamma L(1+d)^4d!\sqrt{p!}(1+p)^d\max\{1,\eta^{2p}\}\|\cW_{\Sigma}^{8d}\cW_{\Sigma}^{4p}(\rho)\|
        \end{aligned}
    \end{equation*}
    where we have used Young's inequality on 
    \begin{equation*}
        (N_i+I)^{2d}(N_j+I)^{2d}(N_h+I)^{2p}\leq\frac{1}{2}\left((N_i+I)^{4d}+(N_j+I)^{4d}\right)(N_h+I)^{2p}\,.
    \end{equation*}
    This finishes the proof. 
\end{proof}
\end{document}