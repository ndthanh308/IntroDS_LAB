\section{Experiments}
In this section, we elaborate on the dataset setting, implementation details and experimental results.

% Figure environment removed

\subsection{Dataset}
We perform experiments on a private audiovisual dataset $\mathcal{D}$.
% collected by researchers from Penn State University. 
The dataset consists of paired voice recordings and scanned 3D facial shapes from 1,026 people, with 364 males and 662 females. The scanned 3D face is stored in the mesh format with 6790 points for each face. The voice recordings are about 2 minutes long for each speaker. We reduce the influencing factors to the voice and face by (1) asking participants to speak a set of specified sentences, (2) asking participants to speak without emotion, (3) control the age of participants (roughly 18-28 years old). In addition, to prevent the models from taking the gender shortcuts, we split the dataset $\mathcal{D}$ by gender, and experiments are individually performed on male and female subsets. For each subset, we adopt 7/1/1/1 splitting for $\mathcal{D}_t$/$\mathcal{D}_{v1}$ /$\mathcal{D}_{v2}$/$\mathcal{D}_e$. In training, the voice recordings are randomly trimmed to segments of 6 to 8 seconds, while we use the entire recordings in testing. The ground truth AMs are normalized to zero mean and unit variance. For voice features, we extract 64-dimensional log Mel-spectrograms using an analysis window of 25ms, with the hop of 10ms between frames. We perform mean and variance normalization of each Mel-frequency bin.

\subsection{Implementation Details}
% The CNN architectures are given in Table 5.2. 
We leverage a backbone $\mathcal{E}$ to learn voice code $e$ which is a simple convolutional neural network. The detailed network structure is presented in the supplementary materials. $F_k$ and $G_k$ share the backboneâ€™s learnable parameters but have individual parameters for their heads. We use a single layer fully-connected network for each head.
% The numbers within the parentheses represent the size and number of filters, while the subscripts represent the stride and padding. So, for example, (3, 64)/2,1 denotes a 1D convolutional layer with 64 filters of size 3, where the stride and padding are 2 and 1, respectively. Modules in brackets are equipped with shortcut connections.
For the variance head, we add an exponential activation to the last layer of $G_k$ for non-negative positive output. We follow the typical settings of stochastic gradient descent (SGD) for optimization. 
Minibatch size is 64. The momentum, learning rate, and weight decay values are 0.9, 0.1, and 0.0005, respectively. The training is completed at 5k iterations.  Since the phonatory module requires a long training procedure, we first train it with the voice code encoder $\mathcal{E}$ for 60k steps on our training set $\mathcal{D}_t$. We follow the training setting in \cite{ho2020denoising} to train the phonatory module. The other parameter setting follows \cite{ho2020denoising}. We directly normalized the voice signal as input to the network instead of first converting it to Log-Mel spectrum. To ensure statistical significance, we perform N = 100 repeated experiments to compute the $CI_u$. For the experiments at phoneme level, we leverage Wav2Vec \cite{baevski2020wav2vec} to cut the long voice recordings into phonemes.

\subsection{Predictable AM Analysis}
\label{sec:AM}
For AM prediction, the estimation models are trained on $\mathcal{D}_t$ and selected based on their performance on $\mathcal{D}_{v1}$ (hyperparameter tuning). For AM selection, the predictable AMs are selected based on the upper bound of the CI $(CI_u)$ on $\mathcal{D}_{v2}$. The performance can be evaluated by the mean error of each AM and its CI.

\cref{fig:AM_res} shows the results, including 20 AMs with highest $1-CI_u$ and 4 AMs with lowest $1-CI_u$. The gray bars are the results on the entire validation set $\mathcal{D}_{v2}$, while the red and yellow ones are the results of 75\% and 50\% voice samples with lowest uncertainty $\hat{w}$ on $\mathcal{D}_{v2}$, respectively. The self-constructed female subset has the same size as the male subset. Higher $1-CI_u$ indicates better results and the normalized error of 0 indicates the chance-level performance.  As suggested by our hypothesis testing formulation, the AMs with $1-CI_u>0$ are considered predictable from voice. In this sense, we have discovered a number of predictable female AMs (see the gray bars and their $CI_u$ in \cref{fig:AM_res} (b)). By filtering out the voice samples with high uncertainties, we achieve even higher $1-CI_u$ (see the red and yellow bars and their $CI$s). The improved performance indicates that more AMs are discovered as predictable from voice. The complete results of all AMs are given in the appendix. The results empirically demonstrate that the information of 3D facial shape is indeed encoded in the voices and can be discovered by our analysis pipeline.

\begin{table}[t]
\centering
\begin{tabular}{c|ccc}
    Phonation Module &  100\% $\hat{w}$  & 75\% $\hat{w}$ & 50\% $\hat{w}$ \\
    \hline
    \Checkmark & 0.953$\pm$ 0.009 & 0.909$\pm$0.024 & 0.842$\pm$0.030 \\
    \XSolidBrush & 0.952$\pm$0.014 & 0.927$\pm$0.030 & 0.879$\pm$0.041 \\
\end{tabular}
\caption{Effect of the phonatory module. We measure the normalized mean squared error between predicted and ground-truth AM among all AMs with different confidence thresholds.}
\label{tab:phonatory1}
\end{table}

% Figure environment removed

To intuitively locate the predictable AMs on the 3D face, we visualize them in \cref{fig:AM_vis}. We clearly observe that most of the predictable AMs are around nose and mouth, and many of them are shared between male and female subsets. This is consistent with the fact that nose and mouth shapes affect pronunciation.

We also notice that the performance of female subset is much better than that of the male subset. To investigate whether the improvements come from the larger data scale (364 males $v.s.$ 662 females), we perform another set of repeated experiments on a self-constructed female subset, which has the same size as the male subset,\ie, 364 females. Surprisingly, the results on the new subset are still better than those on the male subset, as shown in \cref{fig:AM_res} (c). This is possible because the female subjects have higher nasalance scores on the nasal sentences \cite{vampola2020influence} among other things, which provides useful information for predicting the AMs around the nose. Here we note that our experiments have revealed that measurements around the nose are highly correlated to voice. More investigations are left for future work.

On the other hand, some AMs have not been shown to be predictable from voice. This observation suggests that voices may only associate with a few specific regions of the 3D facial shape, like the nose and mouth. For the AMs with higher errors than chance level, we do not claim they are not predictable from voice. Instead, we fail to demonstrate their predictability based on our current empirical results. The possible reasons include imperfect modeling, limited data, data noise, etc. 

\begin{table}[t]
\centering
\begin{tabular}{c|cc}
    Phonatory Module & Predictable & Unpredictable \\
    \hline
    \Checkmark & 0.628$\pm$0.021 & 0.990$\pm$0.032\\
    \XSolidBrush & 0.730$\pm$0.048 & 1.002$\pm$0.031\\

\end{tabular}
\caption{Effect of phonatory module for predictable and unpredictable AMs. We measure the normalized mean squared error between predicted and ground-truth AM among all predictable and unpredictable AMs. Interestingly, we find phonatory module only improves predictable AMs.}
\label{tab:phonatory2}
\end{table}

\subsection{Effect of Phonatory Module}
As presented in \cref{tab:phonatory1}, it is evident that utilizing the phonatory module during training enhances the accuracy of predicted AMs. Our evaluation involved computing the normalized error across all AMs with various confidence thresholds. Although the models with and without the phonatory module exhibited a marginal difference in error when evaluating all the data, the ones trained with the phonatory module showed a clear improvement in error when considering more confident samples.

Furthermore, we conducted an error evaluation for predictable and unpredictable AMs as depicted in \cref{tab:phonatory2}. We observed that utilizing the phonatory module resulted in a 0.102-point decrease in normalized error for predictable AMs, highlighting its effectiveness in improving the prediction performance. Interestingly, the phonatory module did not have any apparent effect on unpredictable AMs. Overall, the results indicate that utilizing the phonatory module during training is beneficial for predicting AMs, particularly for predictable ones.

% Figure environment removed


\subsection{Phoneme-level Analysis}
% Figure environment removed

We also experiment with the voice-face correlation at the phoneme level. For this experiment, we train and evaluate estimators by taking one phoneme as input each time. We computed the average $1 - CI_u$ value for each phoneme across all AMs, as shown in \cref{fig:phoneme}. Our results indicate that \texttt{/\textipa{i:}/} had the highest average $1 - CI_u$ value of 0.199, while \texttt{/\textipa{b}/} had the lowest value of -0.06. When the $1 - CI_u$ value is less than 0, it suggests that AMs are generally unpredictable from the corresponding phoneme.

We observed that the three phonemes with the lowest and negative $1 - CI_u$ values were \texttt{/\textipa{t}/}, \texttt{/\textipa{b}/}, and \texttt{/\textipa{d}/}, all of which are plosive consonants. During the pronunciation of plosive consonants, there is a complete stoppage of airflow followed by a sudden release of air through minimal mouth opening and closing. As a result, there is minimal movement of the facial muscles and structures, making it challenging for the model to predict AMs based solely on these phonemes.

In contrast, most vowels achieved good performance in the test set, with all of the top 6 phonemes belonging to vowels with $1 - CI_u > 0.10$. Compared to consonants, the production of vowels does not involve constriction of airflow in the vocal tract. Instead, the facial muscles have relatively greater movement during the pronunciation of these phonemes, such as jaw movement due to mouth opening or lip spreading. Thus, vowel phonemes may carry more information about facial features, making it easier for the model to capture the hidden correlation when predicting AMs.

\subsection{3D Facial Shape Reconstruction}
In \cref{sec:AM}, we have discovered a number of predictable AMs, from which we choose 10 AMs with the highest $1-CI_u$ for the subsequent reconstructions on male and female subsets.

To evaluate the performance, we compute the per-vertex errors between the reconstructed 3D facial shape and their ground truths. We also filter out a portion of voice samples with the highest uncertainties and evaluate the errors in the remaining data. The filter-out rate is from 0\% to 50\%, as shown from left to right in \cref{fig:3d_diff}.

Unsurprisingly, we achieve the lowest errors around the nose region for male and female subsets, consistent with the AM estimations. Moreover, the reconstruction errors decrease significantly by filtering out the voice samples with the highest uncertainties. This indicates that the learned uncertainty is effectively associated with the reconstruction quality and allows the system to decide whether to trust the model or not.

