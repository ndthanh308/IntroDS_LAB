\section{Method}
In this section, we first introduce the task formulation and then demonstrate our method in detail.
\subsection{Formulation}
We aim to reconstruct any speaker's 3D facial shape from their voice recordings. Given a set of paired voice recordings and 3D facial shapes $\{(v_i, f_i)\}$ from different individuals, where $v_i$ is a voice recording spoken by the $i$-th person and $f_i$ is a 3D facial shape scanned from the speaker of $v_i$. The goal is to reconstruct the 3D facial shape $f$ of any speaker from their voice recording $v$. In our method, we introduce anthropometric measurements (AMs) $m=\{m^{(1)},\cdots,m^{(k)}\}$ computed from $f$ as a proxy, where $K$ is a positive integer and $m^k$ ($k\in[1,K]$) denotes the $k$-th AM. Accordingly, the overall dataset is denoted as $\mathcal{D}=\{(v_i,f_i,m_i)\}$. To statistically analyze the results, we construct an additional validation set for empirically validating the dependency. Specifically, the dataset $\mathcal{D}$ is split into a training set $\mathcal{D}_t$ for model learning, a validation set $\mathcal{D}_{v1}$ for model selection, a validation set $\mathcal{D}_{v2}$ for AM selection, and an evaluation set $\mathcal{D}_e$ for evaluating the reconstructed 3D facial shapes. All splits have no overlap.

\subsection{Pipeline Overview}
As shown in \cref{fig:pipeline}, the proposed method has three main components - facial AM prediction, AM-guided reconstruction and an auxiliary phonatory module. On one hand, we predict the AMs that are potentially correlated with voice production from anthropometry literature \cite{ramanathan2006modeling,zhuang2010facial,shan2021anthropometric,farkas2004anthropometric,ghafourzadeh2019part}. An estimator $\mathcal{E}$ is trained with uncertainty learning with a voice code $e$. On the other hand, inspired by the voice production mechanism, we introduce a phonatory module as a constraint to facilitate the training of AM prediction. In particular, a diffusion-based voice generation module is involved as the phonatory module which aims to imitate the voice identity conditioning on the voice code $e$. After that, we select the AMs predictable from voice for hypothesis testing. The null hypothesis is made for each AM and states the AM is unpredictable from voice. We can successfully reject the corresponding null hypothesis if any AM estimation is better than chance on a held-out validation set with statistical significance. The final 3D facial shapes can be reconstructed by a fitting process \cite{blanz2003face} based on the predictable AMs. This is conducted by adjusting a set of coefficients in low-dimensional space, such that the differences between the AMs of the generated 3D facial shape and the predicted AMs are minimized. Intuitively, if there are more predictable AMs spanning different locations of a face, the reconstruction can be more indistinguishable.

\subsection{Facial AM Prediction}
In this section, we illustrate our method to predict facial AMs from voice.

% Figure environment removed

\para{AM summarization.}
There is a large body of literature on anthropometry. Extensive studies show that many AMs of human faces can be associated with voice production \cite{ramanathan2006modeling,zhuang2010facial,shan2021anthropometric,farkas2004anthropometric,ghafourzadeh2019part}. We summarize the most commonly used AMs as shown in \cref{fig:AMs} (the complete list of AMs is available in the appendix). The chosen AMs are categorized as proportion, angles and distance of a set of face landmarks. Those intra-face features are more robust than 3D coordinate representations as the variations resulting from spatial misalignment are completely eliminated. 

\para{Uncertainty-aware AM estimation.}
The AM prediction is conducted by an estimator trained with an uncertainty-aware scheme. Let $F_k(v;\mathcal{E}_k, \omega_k):v\mapsto \mathbb{R}$ be an estimator that maps voice recording $v$ into the $k$-th predicted AMs, where $\mathcal{E}_k$ and $\omega_k$ are the learnable parameters. As this is a regression problem, we leverage 
\begin{equation}
    \{\mathcal{E}_k^*, \omega_k^*\}=\argmin_{\mathcal{E}_k,\omega_k}\frac{1}{|\mathcal{D}_t|}\sum_{(v,m^{(k)})\in\mathcal{D}_t}(F_k(v;\mathcal{E}_k, \omega_k)-m^{(k)})^2
\end{equation}
as the training objective for the $k$-th AM. $|\mathcal{D}_t|$ is the number of the triplets (voice, face and AMs) in dataset $\mathcal{D}_t$. By incorporating uncertainty into the estimator learning, the prediction becomes a random variable rather than a single value. We leverage a Gaussian distribution to the prediction. The estimator $F_k(v;\mathcal{E}_k,\omega_k)$ maps $v$ into the mean of the $i$-th predicted AM. Similarly, we define an uncertainty estimator $G_L(v;\mathcal{E}_k,\phi_k):v\mapsto\mathbb{R}^+\cup\{0\}$ that $v$ into the variance of the $k$-th predicted AM. Again, $\mathcal{E}_k$ and $\phi_k$ are the learnable parameters. The predicted AM and its ground truth become $\mathcal{N}(F_k(v), G_k(v))$ and $\mathcal{N}(m^{(0)},0)$ respectively \cite{kendall2017uncertainties}. Given two random variables, a more reasonable learning objective is to minimize their KL divergence.
\begin{equation}
\begin{aligned}
    \{\mathcal{E}_k^*,\omega_k^*,\phi_k^*\}=\argmin_{\mathcal{E}_k,\omega_k,\phi_k}\frac{1}{|\mathcal{D}_t|}\sum_{(v,m^{(k)})\in\mathcal{D}_t}\frac{(F_k(v;\mathcal{E}_k,\omega_k)-m^{(k)})^2}{G_k(v;\mathcal{E}_k,\phi_k)}\\+\mathrm{ln}G_k(v;\mathcal{E}_k,\phi_k)
\end{aligned}
\end{equation}
For a fixed $(F_k(v;\mathcal{E}_k,\omega_k)-m^{(k)})^2$, there is an optimal variance $G_k(v;\mathcal{E}_k,\phi_k)=(F_k(v;\mathcal{E}_k,\omega_k)-m^{(k)})^2$ such that the loss function is minimized. Thereby the uncertainty estimator $G_k$ is learned to produce a small variance if the prediction error is small and vice versa. On the contrary, a smaller variance indicates that the predicted AM is more likely to yield a small prediction error,\ie, close to the ground truth. In this way, we can choose to trust the predicted AMs when the predicted variances are small, and defer the voice recordings to human experts otherwise. An extreme case is $G_k(v)\equiv1$ where the uncertainty learning objective degrades to the regular regression model.

\para{Temporal aggregation.}
In practice, following the convention of voice understanding, the long voice recording $v$ is fed into the network in the form of multiple short segments $\{v^{(1)},\cdots,v^{(L)}\}$. We obtain a sequence of means and variances of the predicted AM. During training, we compute the loss for each segment individually and average them as the training loss. While during evaluation, the predicted AM and its uncertainty are given by aggregating the predictions among all segments. Assuming the short segments from a long recording are class-conditionally independent, the formulations of aggregation are 
\begin{equation}
\begin{aligned}
&\hat{m}^{(k)}=\sum_{l=1}^L\frac{w^{(k)}}{G_k(v^{(l)})}\cdot F_k(v^{(l)}),\\
&\frac{1}{w^{(k)}}=\sum_{l=1}^L\frac{1}{G_k(v^{(l)})}
\end{aligned}
\end{equation}
where $\hat{m}^{(k)}$ is the aggregated mean and also the predicted $k$-th AM. However, the aggregated variance $w^{(k)}$ is not used as the uncertainty of the predicted $k$-th AM since the conditional independence assumption does not always hold in cases such as noises, silences, the computed aggregated variance will be biased by the number of voice segments in the long recording. So we calibrate the uncertainty as $\hat{w}^{(k)}=L\cdot w^{(k)}$. 

\para{Predictable AM identification.}
We have collected a number of AMs and trained estimators for predicting them. However, only a few of the AMs are actually predictable from voice, which we had anticipated while designing the task. To identify those AMs, we use hypothesis testing to them. Formally, we can write the null and alternative hypotheses for the $k$-th AM as 
\begin{itemize}
\item[] $H_0:$ the AM $m^{(k)}$ is NOT predictable from voice
\item[] $H_1:$ the AM $m^{(k)}$ is predictable from voice
\end{itemize}
In order to reject $H_0$, we only need to find a counterexample to show that voice is indeed useful in predicting AM $m^{(k)}$. An effective example is to compare the estimators with and without the voice input. If there exists a learned estimator $F_k(v)$ performing better than the chance-level estimator $C_k$ without using voice input and the results are statistically significant, we can successfully reject $H_0$ and accept $H_1$. Here the chance-level estimator for the $k$-th AM is a constant $C_k=\frac{1}{|\mathcal{D}_t|}\sum_{m^{(k)}\in\mathcal{D}_t}m^{(k)}$, which is the mean $m^{(k)}$ of the training set $\mathcal{D}_t$. So the null and alternative hypothesis can be rewritten as
\begin{itemize}
\centering
    \item[] \hspace{-1cm}$H_0:\mu(\epsilon_k/\epsilon_k^C)\leq 1$
    \item[] \hspace{-1cm}$H_1:\mu(\epsilon_k/\epsilon_k^C)\geq 1$
\end{itemize}
where $\epsilon_k$ and $\epsilon_k^C$ are the mean square errors of estimators with and without voice inputs on validation set $\mathcal{D}_{v2}$, respectively. The formulations of $\epsilon_k$ and $\epsilon_k^C$ are given as $\epsilon_k=\frac{1}{|\mathcal{D}_{v2}|}\sum_{m^{(k)}\in\mathcal{D}_{v2}}(\hat{m}^{(k)}-m^{(k)})^2$ and $\epsilon_k^C=\frac{1}{|\mathcal{D}_{v2}|}\sum_{m^{(k)}\in\mathcal{D}_{v2}}(C_k-m^{(k)})^2$.
Since the true variance of $\epsilon_k/\epsilon_k^C$ is unknown, the type of hypothesis testing is one-sided paired-sample t-test. The upper bound of the confidence interval (CI) is given by
\begin{equation}
    CI_u=\mu(\epsilon_k/\epsilon_k^C)+t_{1-\alpha,\nu}\cdot\frac{\sigma(\epsilon_k/\epsilon_k^C)}{\sqrt{N}}
\end{equation}
where $\mu(\cdot)$ and $\sigma(\cdot)$ are the functions for computing mean and standard deviation respectively. $N$ is the number of the repeated experiments and we set $N=100$ here. $\alpha$ and $\nu=N-1$ are the significance level and the degree of freedom respectively. For the purpose of this section, we adopt the significance level of 5\% and then we can read $t_{0.95,N-1}$ from t-distribution table. Now we can determine whether to reject $H_0$ and accept $H_1$,\ie, the AM $m^{(k)}$ is predictable from voice. According to the experimental results, the probability that the aforementioned decision is correct is higher than 95\%,\ie, statistically significant. In contrast, $CI_u\geq 1$ implies that we fail to reject $H_0$, for the current experimental results are not statistically significant enough. Note that failing to reject $H_0$ does not imply we accept $H_0$.

We emphasize that it is necessary to compute $\epsilon_k^C$ and $\epsilon_k$ on $\mathcal{D}_{v2}$ rather than $\mathcal{D}_t$ or $\mathcal{D}_{v1}$. This is because our estimators are trained on $\mathcal{D}_t$ and selected by the errors on $\mathcal{D}_{v1}$, we can easily get significantly lower $\epsilon_k$ and $\epsilon_k^C$ on these splits. 

\para{Optional phonatory module.}
Inspired by linear predictive coding (LPC) \cite{markel1976linear} which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particular, we leverage a diffusion-based \cite{ho2020denoising} voice generation method to model the time-domain speech signals. As shown in \cref{fig:pipeline}, the diffusion model converts the noise distribution to a speech $\Tilde{v}$ controlled by the voice code $e$ extracted from speech $v$. During training speech $v^\prime$ which shares speaker identity with $v$ is fed to the diffusion model as ground-truth. Please note that the phonatory module only serves as an additional training constraint and is not applied during inference. Let $x_0,\cdots,x_T$ be a sequence of variables with the same dimension where $t$ is the index for diffusion time steps. Then the diffusion process transforms $x_0$ into a Gaussian noise $x_T$ through a chain of Markov transitions with a set of variance schedule $\beta_1,\cdots,\beta_T$. Specifically, each transformation is performed according to the Markov transition probability $q(x_t|x_{t-1},e)$ assumed to be independent of the style code $e$ as
\begin{equation}
    q(x_t|x_{t-1},e)=\mathcal{N}(x_t;\sqrt{1-\beta_tx_{t-1}}, \beta_tI).
\end{equation}
Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a conditional distribution $p_\theta(x_{0:T-1}|x_T,c)$. Through the reverse transitions $p_\theta(x_{0:T-1}|x_T,c)$, the variables are gradually restored to a speech signal with style code condition. The phonatory module actually models a distribution $q(x_0|c)$. By applying the parameterization trick \cite{kingma2013auto}, we obtain the additional training constraint as
\begin{equation}
    \{\mathcal{E}^*,\theta^*\}=\argmin_{\mathcal{E},\theta}=\mathbb{E}_{x_0,\epsilon,t}\|\epsilon-\epsilon_\theta(\sqrt{\Bar{\alpha}_t}x_0+\sqrt{1-\Bar{\alpha}_t}\epsilon,t,e)\|_1
\end{equation}
where $\alpha_t=1-\beta_t$ and $\Bar{\alpha}_t=\prod^t_{t^\prime=1}\alpha_{t^\prime}$. As shown in \cref{fig:pipeline}, the $\theta$ is a Net \cite{ronneberger2015u} with cross-attention \cite{rombach2022high}. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain $\Tilde{v}$ here.

\subsection{AM-Guided 3D Facial Shape Reconstruction}
To reconstruct the 3D facial shape, we first need to predict AMs of the voice recordings in $\mathcal{D}_e$ first. Subsequently, we generate the 3D facial shapes based on the predicted AMs by an optimization-based method. To do so, we first project the 3D facial shapes into a low-dimensional linear space \cite{blanz2003face}. By adjusting the coefficients in low-dimensional space, we obtain different re-projected 3D facial shapes. The learning objective is to find a set of coefficients, such that the differences between the AMs of the re-projected 3D facial shape and the predicted AMs are minimized. Specifically, we construct a big matrix $B=[b_1,b_2,\cdots]\in\mathbb{R}^{3T\times |\mathcal{D}_t|}$ where each column $b_i\in\mathbb{R}^{3T\times 1}$ is a long vector obtained by flattening a 3D facial shape $f_i\in\mathbb{R}^{T\times 3}$. $T$ is the number of vertices on 3D faces. Since $3T\gg|\mathcal{D}_t|$, we compute the project matrix $P\in\mathbb{R}^{3T\times d} (d\gg 3T)$ using eigenfaces \cite{blanz2003face} on $B$. Now any flattened 3D facial shape $b$ can be approximated by re-projecting a low-dimensional vector $\beta\in\mathbb{R}^{b\times 1}$ in the form of $P\beta$. We define the computation of AM as $Q_k(b):b\mapsto \mathbb{R}$, which maps any flattened 3D facial shape $b$ into the $k$-th AM of $b$. Since $Q_k(\cdot)$ computes a distance, a proportion, or an angle of the 3D facial shape, it is a differentiable function. The optimization objective is given below.
\begin{equation}
    \beta^*=\argmin_{\beta}\lambda\|\beta\|_2^2+\sum_{k=1}^K(Q_k(P\beta)-\hat{m}^{(k)})^2\cdot z^{(k)}
\end{equation}
where $\lambda$ is the loss weight balancing two terms. The reconstructed 3D facial shape is given by $\hat{b}=P\beta^*$. 