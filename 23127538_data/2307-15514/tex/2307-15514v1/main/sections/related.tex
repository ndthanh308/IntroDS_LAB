%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}\label{sec:related}

6D pose estimation approaches can be designed to use different input data.
RGB methods~\cite{segdriven,singlestage,sopose,zebrapose} rely on photometric information only, while RGBD methods~\cite{pvn3d,ffb6d,surfemb,e2ek,geometricaware6d} also use range information in addition to RGB.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{RGB-based 6D pose estimation.}
SO-Pose~\cite{sopose} proposes an end-to-end method for 6D pose estimation that explicitly models self-occlusion maps (i.e., portions of the target object that are hidden by camera orientation). 
It computes 2D-3D correspondences for each visible point of the object, and feeds them with self-occlusion maps to a pose regression module.
ZebraPose~\cite{zebrapose} proposes a strategy to learn surface descriptors on the image. 
First, it encodes the query object models through a hierarchical process that defines partitions of object vertices. 
Then, it trains a neural network on the image to match the output features with the features at corresponding vertices. 
At inference time, it finds correspondences by similarity, and solves the PnP problem with RANSAC.
The authors show that the vertex encoding process is crucial for performance improvement. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent \textbf{RGBD-based 6D pose estimation.}
PVN3D~\cite{pvn3d} extends PVNet~\cite{pvnet} by incorporating 3D point cloud information. 
The core of this approach is a keypoint voting mechanism, where each point regresses the offset to a reference keypoint.
A semantic segmentation module is also used to identify the points belonging to each object in the scene.
PVN3D is a two-stage method, as it passes the final correspondences to a RANSAC-based~\cite{ransac} algorithm for 6D pose estimation.
FFB6D~\cite{ffb6d} adopts an analogous method to PVN3D~\cite{pvn3d}, but introduces a novel convolutional architecture with Fusion Modules.
These modules enable the model to combine photometric (RGB) and geometrical (D) features for learning a better point cloud representation.
E2EK~\cite{e2ek} proposes an end-to-end trainable method by extending FFB6D~\cite{ffb6d}. 
It clusters and filters the features computed by FFB6D based on confidence, and then processes them by an MLP-like network that regresses the pose. 
Wu et al.~\cite{geometricaware6d} addresses the problem of objects that are symmetric to rotation with a two-stage method. 
They extend FFB6D~\cite{ffb6d} by introducing a novel triplet loss based on geometric consistency. 
Symmetry is leveraged by considering symmetric points as positives, thus forcing them to have similar features.

Unlike methods that employ sophisticated combinations of deep network architectures to process RGB and depth modalities \cite{ffb6d,geometricaware6d}, our approach
uses deep networks based on sparse convolutions to process coloured point clouds with a single framework.
Sparse convolutions are designed to process point clouds efficiently \cite{minkowski}.
We also split the pose estimation problem into two subproblems, i.e.~feature learning and point cloud registration.
This allows us to evaluate the quality of the learned features by using metrics such as Feature Matching Recall~\cite{deng2018ppfnet}, which fosters interpretability of our model. 
Unlike Wu et al.~\cite{geometricaware6d}, we do not have to use a detector to crop the region with the candidate object before processing the point cloud with our network.
Our experiments show that we can outperform the nearest competitors E2EK~\cite{e2ek} and Wu et al.~\cite{geometricaware6d} by 5.7 and 1.9 ADD(S)-0.1d on the LMO dataset, respectively, without using a detector.
