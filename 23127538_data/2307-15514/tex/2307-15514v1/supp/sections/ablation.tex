\section{Ablation study on YCBV}
\label{sec:ablation_ycbv}

In Tab.~\ref{tab:ablation_ycbv} we report the results of our ablation study on YCBV~\cite{ycbv}.
We choose the Large Marker object and train a single model on it for each modification we applied.
Each model is trained for 20 epochs on the standard training set.
For the computation of the Feature Matching Recall (FMR), we set the distance threshold $\tau_1=10$ voxels and the inlier ratio threshold $\tau_2=5$\%, to account for the different density of the scene point cloud in YCBV.
All the other settings and parameters are the same as those in our ablation study on LMO~\cite{lmo} in the main paper.

We can observe that some changes do not increment performance, but instead cause a slight drop, in particular when adapting the safety threshold to the object dimension (third row, $-0.4$) and when colour augmentation is applied (sixth row, $-$0.3).
These additions do not benefit this particular object, but are instead advantageous when averaging all the object in the dataset.

We can note that, as in the ablation study on the LMO dataset in the main paper, the most significant improvements in ADD-S AUC result from applying the safety threshold ($+$1.5), adding RGB information ($+$5.5), and using the Adam optimiser ($+$12.3).
\input{supp/tables/ablation_15}

\section{Additional ablation study on LMO}

We include an ablation study on the $t_\text{scale}$ hyperparameter, which is used to set the radius of the ball volume in which negative mining around a certain point is not allowed. We train on the Can object of LMO using the standard setting, and varying only $t_\text{scale}$. The results are shown in Tab.~\ref{tab:ablation_ycbv}.
We can observe that our choice of $t_\text{scale} = 0.1$ leads to the best result. When $t_\text{scale}$ is increased, many candidate points are forbidden to be used as negatives, therefore decreasing the final performance. On the other hand, a lower $t_\text{scale}$ implies negative pairs composed by points which are near in the 3D space. This reduces the performance, as similar points are forced to have different descriptors. Notably, the worst results is obtained when $t_\text{scale} = 0.1$, i.e. when no negative candidates are excluded.

\input{supp/tables/threshold_h}