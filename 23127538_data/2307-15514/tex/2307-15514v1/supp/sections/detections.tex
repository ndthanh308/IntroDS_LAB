\section{Contribution of the object detection prior}
\label{sec:detections}

\input{supp/tables/detection_lmo}
\input{supp/tables/detection_ycbv}

We report in Tabs.~\ref{tab:detection_lmo} \& \ref{tab:detection_ycbv} the performance of the YOLOv8 detector~\cite{yolov8} on the test set of LMO~\cite{lmo} and YCBV~\cite{ycbv}, respectively.
Following \cite{coco}, we report the area-under-the-curve of the Average Precision, obtained by varying the IoU threshold with respect to a ground truth detection from 0.5 to 0.95 with a step of 0.05 ($\text{AP}$).
We also report the recall on Average Precision, obtained with a fixed IoU threshold of 0.5 ($\text{AP}_{50}$).
We also measure how the performances in ADD(S)-0.1d and ADD-S-0.1d change when using a detector.

% Figure environment removed


Because LMO and YCBV use ADD(S)-0.1d and ADD-S AUC, respectively, to measure the performance change, we consider ADD(S)-0.1d for LMO and in ADD-S-0.1d for YCBV.
As in the original definition~\cite{hodavn2016evaluation}, a success for the ADD-S-0.1d metric is obtained when the ADD-S error is below $0.1 D_O$, where $D_O$ is the object diameter, otherwise it is considered a failure.
The same applies to ADD(S)-0.1d.
Therefore, we define the following metrics:

\begin{itemize}
    \item  $\Delta_{S \to F}$ : the percentage of object instances for which there is a success when not using a detector and a failure when using it.
    \item $\Delta_{F \to S}$ : the percentage of object instances for which there is a failure when not using a detector and a success when using it.
\end{itemize}



In Tab.~\ref{tab:detection_lmo} we report the metrics on LMO~\cite{lmo}.
We can observe that the detector performance is correlated with the size of the object: Ape, Cat and Duck are very small and often occluded, and therefore perform worse than Can, Holepuncher, and Drill which are bigger.
The lower performance on Glue and Eggbox objects is because they are often confused by the detector. 
An example of this can be observed in  Fig.~\ref{fig:lmo_fail}(b).
This behaviour is also present with the Can object in Fig.~\ref{fig:lmo_fail}(a).
Despite these errors, we note that the Eggbox object greatly benefits from the detector (+24.8 in $\Delta_{F \to S}$), while for the other objects the performance gain is less significant.

In Tab.~\ref{tab:detection_ycbv} we show the detection performances on YCBV~\cite{ycbv}.
Unlike LMO, the introduction of the object detector is beneficial for all the objects, as the $\Delta_{F \to S}$ is always higher then the $\Delta_{S \to F}$ (i.e.~the detector solves more pose errors then it introduces for every object).
We can note how the detector helps in solving the problem of object similarity: the Large Clamps and Extra Large Clamps objects are amongst the ones that benefit the most from it. 
As an example, in Fig.~\ref{fig:ycbv_fail}(a) we show how the registration of the two objects (Large Clamps in dark blue, Extra Large Clamps in dark green) differs depending on the use of the detector. 
With prior detection (second row) the two clamps are registered correctly.
Without the detection (third row) the model registers both of them on the pose of the Extra Large Clamps.
We observe in Tab.~\ref{tab:detection_ycbv} that the Scissors object appears to be an outlier in the detector performance (AP$_{50}$ of 27.6 against an average of 95.8).
By examining the ground-truth detections we observed that they are noisy in the case of occlusion: sometimes the image portion where the object should be is also included in the bounding box, even if the object is not visible. 
See Fig.~\ref{fig:scissors} for an example. 
Despite this, the Scissors performance in pose estimation still benefits from the detector.
