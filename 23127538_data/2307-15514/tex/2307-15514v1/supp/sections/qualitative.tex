\section{Additional qualitative results}
\label{sec:qualitative}

In Figs.~\ref{fig:lmo_success} \& \ref{fig:ycbv_success} we show examples of correct pose predictions on LMO~\cite{lmo} and YCBV~\cite{ycbv}, respectively.
The top row shows the ground-truth poses, while the bottom row shows the results obtained with our experimental setting.

In Figs.~\ref{fig:lmo_fail} \& \ref{fig:ycbv_fail} we show examples of wrong pose predictions on LMO~\cite{lmo} and YCBV~\cite{ycbv}, respectively. 
To highlight the detector contribution, we show the ground-truth poses (top row), our results with the object detection prior (middle row), and our results without the object detection prior (bottom row).

In LMO~\cite{lmo} we observe that the detector appears to be strongly influenced by the colour of the object, as it confuses the Can, Eggbox, and Glue objects because they show similar colours (see Fig.~\ref{fig:lmo_fail}(a-b-c)).

Something similar occurs in Fig.~\ref{fig:lmo_fail}(d), where the pose prediction for Holepuncher is correct when the object detector prior is not used, and wrong when it is used.
This is due to the colour similarity between Holepuncher and the toy car behind the Glue object.

In YCBV~\cite{ycbv} the object detector handles important errors (Figs.~\ref{fig:ycbv_fail}(b-c)) and improves the pose prediction accuracy (Figs.~\ref{fig:ycbv_fail}(f-g-h)). 
We also show a failure case where a wrong detection causes an inaccurate pose prediction (the Extra Large Clamp object in Fig.~\ref{fig:ycbv_fail}(e)).

To examine the point-level features learned by FCGF6D, we select pairs of point clouds and visualise the distance in the feature space of each point with respect to a reference point.
Consider Figs.~\ref{fig:feat_all_lmo} \& \ref{fig:feat_all_ycbv} for LMO and YCBV, respectively.
Given a pair $(O, S)$, respectively object and scene, we randomly select three points belonging to a correspondence on $O$, then we compute the distance in the feature space of each point in $O$ and $S$ from it.
The distance is then normalised for better visualisation.
We show the RGB point clouds on the left, and the input pairs with the feature distance on the right.
The reference point is depicted as a red point on $O$.

In LMO~\cite{lmo} (Fig.~\ref{fig:feat_all_lmo}), we can observe that the distance in the scene point clouds is small near the point corresponding to the reference one. 
We can also observe that the model can learn a certain degree of symmetry: in the second visualisation of the Can object, the distance in feature space of a point which is symmetric to the reference one is small.

In Fig.~\ref{fig:feat_all_ycbv} we show the visualisation on YCBV~\cite{ycbv}.
In general the distribution of the distances appears to be noisier and less smooth than that in LMO. 
We believe that this is due to the stronger sampling on $S$ that we perform on YCBV, unlike the one done on LMO (20K points against 50K points).
Another possible cause is that the RGB test images of YCBV are of a lower quality than the ones of LMO.
As in the LMO visualisation, the corresponding point of the reference point in the scene has a small distance in the feature space.
We can also observe the resulting distance in the case of similar objects: in the rightmost case of the Extra Large Clamp, one of the points on a similar object (the Large Clamp) is similar to the reference one in the feature space.
Because our features are learned to describe a local patch, their quality can be limited by the presence of similar geometric structures in other objects.
Also in this case we can observe how symmetry influences the feature space, by considering the visualisation of the Mug object.
We can observe that, especially in the second and third pairs, the most similar points on $S$ are the ones on the radial symmetry axis of $O$.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed
