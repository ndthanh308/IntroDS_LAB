%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}\label{sec:results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}

We evaluate \ourmethod on the LineMod-Occluded (LMO)~\cite{lmo} and the YCB-Video (YCBV)~\cite{ycbv} datasets.

\noindent \textbf{LMO}~\cite{lmo}
contains RGBD images of real scenes with different configurations of objects placed on a table.
It provides the ground-truth 6D pose of eight of these objects, which are always present in the scene. 
Objects are poorly textured, of varying dimensions and placed in a cluttered scene, featuring a variety of lightning conditions.
We use the original test set of 1,213 real images, while for the training set the works we use as comparison use different combinations of synthetic and real images: the methods they use to generate the synthetic images and the number of samples for each type are not always clearly defined~\cite{pvn3d, ffb6d, e2ek, geometricaware6d}.
Differently, we only use the Photo Realistic Rendering (PBR) set of 50,000 synthetic images provided by the BOP challenge~\cite{bop-challenge} as it contains a large variety of pose configurations.
Following \cite{pvn3d}, we adopt an hole filling algorithm~\cite{hole_filling} to improve the depth quality on both training and test images.




\noindent \textbf{YCBV}~\cite{ycbv}
contains RGBD images of real scenes with different configurations of 21 objects taken from the YCB dataset~\cite{calli2015ycb}.
Objects have similar geometry (e.g.~boxes and cans) and are placed in various poses (e.g.~some objects are placed on top of others).
Unlike LMO, the objects are placed in different contexts.
We use the original test set of 20,738 real images.
As for LMO, state-of-the-art methods use different combinations of synthetic and real data~\cite{pvn3d, ffb6d, e2ek, geometricaware6d}.
For training, we choose 4,000 synthetic, 4,000 real, and 4,000 PBR images provided by the BOP challenge~\cite{bop-challenge} because we found that using only the PBR images leads to unsatisfactory results.
Also for YCBV we adopt a hole filling algorithm~\cite{hole_filling} on both train and test depth images as done in~\cite{pvn3d}.

\input{main/tables/lmo_adds}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Implementation details}

\noindent \textbf{LMO setting.}
Experiments on LMO share the following hyperparameters.
The input pair $(O, S)$ is first sampled to $V_O=$ 4,000 and $V_S=$ 50,000 points, respectively, and then quantised with a step of $Q=2$mm.
As feature extractor we use a MinkUNet34 \cite{minkowski} with output dimension $F=32$.
The correspondence estimation threshold used for the positive mining is $\tau_P=4$mm, and the maximum number of correspondences extracted is set to 1,000.
The safety threshold $\tau_{NO}$ is defined proportionally to the object $O$ diameter by setting $t_{\text{scale}}=0.1$ (see Fig.~\ref{fig:loss_detail}).
The hardest negative mining on $\mathcal{X}_O$ is performed in $\widetilde{\mathcal{N}}_i$.
When mining the hardest negatives on $\mathcal{X}_S$, instead of considering the full candidates set $\widetilde{\mathcal{N}}_j$ we randomly sample 10,000 points from it to reduce the spatial complexity.
HC loss margins are set as $\mu_P=0.1$, $\mu_N=10$, and coefficients are set to $\lambda_P = 1$, $\lambda_{NO} = 0.6$, and $\lambda_{NS} = 0.4$.
The feature extractor is trained on 50,000 PBR images for 12 epochs.
The pose is obtained by using the TEASER++~\cite{yang2020teaser} algorithm.

\noindent \textbf{YCBV setting.}
Experiments on YCBV share the same LMO hyperparameters except in the following cases.
We set $V_S=$ 20,000, as we found that it works on par with the original $V_S$ of LMO.
We believe this happens because YCBV objects are less occluded and their geometries are less complex than LMO objects.
As feature extractor we use a MinkUNet50 model \cite{minkowski}, trained on 12,000 mixed images for 110 epochs.
The pose is obtained with a RANSAC-based algorithm from Open3D~\cite{open3d}.
Experimentally, on YCBV we found that RANSAC yields better results than TEASER++.
We believe that this happens because TEASER++ is heavily based on correspondences~\cite{yang2020teaser} and for YCBV we use a lower resolution for the scene compared to LMO, which in turn reduces the number of correspondences.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation metrics}

We use the ADD and ADD-S metrics that are defined as
% ==============================
\vspace{-2mm}
\begin{equation*}
\begin{aligned}
\text{ADD} &=
\frac{1}{V_O} \sum_{\textbf{x} \in \mathcal{X}_O} \left\lVert
(\textbf{R} \textbf{x} + \textbf{t}) - 
( \hat{\textbf{R}} \textbf{x} + \hat{\textbf{t}} ) 
\right\rVert,\\
\text{ADD-S} &=
\frac{1}{V_O} \sum_{\textbf{x}_1 \in \mathcal{X}_O} 
\min_{\textbf{x}_2 \in \mathcal{X}_O}
\left\lVert
(\textbf{R}\textbf{x}_1 + \textbf{t}) -
(\hat{\textbf{R}} \textbf{x}_2 + \hat{\textbf{t}})
\right\rVert,
\end{aligned}
\vspace{-2mm}
\end{equation*}
% ==============================
where $\mathbf{R}, \mathbf{t}$ and $\hat{\mathbf{R}}, \hat{\mathbf{t}}$ are the translation and rotation components of the predicted and the ground-truth poses of $\mathcal{X}_O \in \mathbb{R}^{V_O \times 3}$, respectively.
ADD(S) computes the ADD for non-symmetric objects and the ADD-S for symmetric ones.
Performance on LMO is assessed in term of the ADD(S)-0.1d metric~\cite{pvn3d, ffb6d, e2ek, geometricaware6d}, which computes the percentage of ADD(S) errors lower than $10\%$ of the object diameter~\cite{hodavn2016evaluation}.
Performance on YCBV is assessed in term of the ADD-S AUC metric~\cite{ycbv, pvn3d, ffb6d}.
The area-under-the-curve (AUC) of ADD-S is obtained by computing the cumulative percentage of ADD-S errors lower than a threshold varying from 1mm to 100mm.
Note that in ADD(S)-0.1d the success thresholds are relative to the object diameters, while in ADD-S AUC they are absolute.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Quantitative results}

Tab.~\ref{tab:lmo_adds} reports the results on LMO~\cite{lmo} in term of ADD(S)-0.1d:
for completeness we added the two best performing RGB methods (top), while the other ones are RGBD methods (bottom).
As reported in the Prior column, most methods rely on additional priors, either in the form of object detections (Det) or of object segmentation masks (Seg).
\ourmethod outperforms all the other methods by a large margin without using any prior (penultimate row):
it outperforms Wu et al.~by 1.9\%, E2EK by 5.7\%, DCL-Net by 8.4\%, FFB6D by 12.8\%, PR-GCN by 14.0\%, and PVN3D by 15.8\%.
Note that Wu et al.~\cite{geometricaware6d} and E2EK~\cite{e2ek} train a different deep neural network for each object (DNNs column), whereas we train only a single deep neural network, saving learning parameters and training time.
Moreover, when we use the object detections obtained with YOLOv8~\cite{yolov8} (last row), the performance of \ourmethod further improves, outperforming Wu et al.~by 3.5\%, E2EK by 7.3\%, and all the other methods by more than 10.0\%.
Note that detectors are prone to errors: when detections are wrong, the object pose will be wrong too. 
We can observe that the detector is more effective with Duck and Eggbox.
The first is a particularly small object, therefore more likely to be occluded. 
The second undergoes frequent occlusions (other objects are on top of it in several images), thus making localisation difficult without a detector.
To further understand the negative impact of the detector, we compute the percentage of poses which are wrong when we use detections and correct when we do not use detections. 
For Ape, Can and Glue, this percentage is 3.3\%, 1.7\%, and 5.1\%, respectively. 
Please refer to the Supplementary Material for a comprehensive analysis of the detector impact.

\input{main/tables/ycbv_rgbd_addsauc}

Tab.~\ref{tab:ycbv_rgbd_addsauc} reports the results on YCBV~\cite{ycbv} in ADD-S AUC compared with other RGBD-based methods.
The row Prior indicates eventual additional priors used by each method.
The default configuration of \ourmethod does not require any input prior and uses a deep neural network for all the objects.
\ourmethod outperforms recent competitors that do not use input priors: it outperforms FFB6D by 0.8\% and PVN3D by 1.7\%.
E2EK~\cite{e2ek} and Wu et al.~\cite{geometricaware6d} instead consider input priors in the form of object segmentation masks and object detections, respectively, and train a model for each object (DNNs row).
When we use input priors in the form of detections, \ourmethod outperforms E2EK by $2.4\%$ and slightly underperforms Wu et al.~by $-0.6\%$.
We also observe that, thanks to multi-scale representation provided by the UNet, we obtain good performance also on symmmetric objects without the need of specific techniques to handle symmetry.
Note that we employed detections in both Tabs.~\ref{tab:lmo_adds}\&\ref{tab:ycbv_rgbd_addsauc} to illustrate their potential use in improving registration efficacy, though not obligatory.
Specifically in Tab.~2, when we compare with methods based on the same assumptions as ours, \ourmethod achieves state-of-the-art performance, see comparison with PVN3D \cite{pvn3d} and FFB6D \cite{ffb6d}.
When we compare with methods that use 21 models instead of 1 (as ours), we fall slightly behind the best (see comparison with E2EK \cite{e2ek} and Wu et al.~\cite{geometricaware6d}).


% ==================================================================
% Figure environment removed
% ==================================================================



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Qualitative results}

Fig.~\ref{fig:lmo_qualitative} shows some examples of successes and failures on the test set of LMO dataset.
The upper row shows the ground-truth poses, and the bottom one shows the poses predicted by our model.
Note how FCGF6D is capable of estimating the correct pose even in case of partial objects (i.e. the glue in the first image).
However, our model fails in case of partial objects with ambiguities (the duck in the second image), or of atypical occlusions (the eggbox in the second image: the training set do not contain this degree of occlusions).

Fig.~\ref{fig:ycbv_qualitative} shows some examples of successes and failures on the test set of YCBV. FCGF6D appears prone to rotation errors (the large clamp in the first image), especially in case of partially occluded objects (the bleach cleanser in the second image). However, the poses are generally accurate.





% ==================================================================
% Figure environment removed
% ==================================================================














%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation study}

\input{main/tables/ablation}

% ==================================================================
% Figure environment removed
% ==================================================================

We conduct an ablation study on the Drill object of the LMO dataset by training FCGF6D for five epochs.
We choose the closest setting to FCGF as baseline: 
no safety threshold in the loss, 
shared network weights, 
no RGB information, 
SGD optimiser with $\text{lr}_{\text{init}}=10^{-3}$,
exponential scheduler with $\gamma = 0.99$.
We perform a single experiment for each added component to assess their individual contribution.
As metrics, we use ADD(S), Relative Rotation Error (RRE), Relative Translational Error (RTE), and Feature Matching Recall (FMR) \cite{FCGF,GEDI}.
RRE and RTE show how the two pose components (rotation and translation) are affected.
FMR indirectly measures the number of iterations required by a registration algorithm, e.g.~RANSAC, to estimate the transformation between two point clouds.
We set the inlier distance threshold as $\tau_1$ = 5 voxels, and the inlier recall ratio as $\tau_2$ = 5\%.

Tab.~\ref{tab:ablation} shows that the largest contributions in ADD(S)-0.1d are: introducing the safety threshold in the loss (+17.8), adding RGB information (+34.2), and adopting Adam optimiser (+20.2). 
We also note that the gain in ADD(S)-0.1d is not always consistent with the FMR: when RGB augmentation is added, there is a gain in ADD(S)-0.1d of 2.1, but the FMR drops by 6.5.
A more detailed analysis of FMR with different values of $\tau_1$ and $\tau_2$ is shown in Fig.~\ref{fig:fmr}.





\subsection{Training and inference time}

The training time is about one week for each dataset using two NVIDIA A40 GPUs.
Tab.~\ref{tab:times} reports the comparison of the number of parameters, inference GPU memory footprint, and inference time (using a GeForce RTX 3050 GPU) on YCBV. 
We were unable to test E2EK \cite{e2ek} as the code is unavailable, whereas we used the authors' original code for the other papers.
\ourmethod has a significantly smaller memory footprint than the main competitors, and the inference time is comparable. 
In a scenario where multiple objects are expected, our closest competitor~\cite{geometricaware6d} uses a different model for each object, thereby requiring more memory. 
Our method requires less memory because we train only a single model.
Note that using the whole scene as input is advantageous in a practical scenario where $N$ instances of the same object are present. 
Here, we need a single forward pass, followed by $N$ registrations. 
Instead, methods that rely on image crops~\cite{prgcn,dclnet, geometricaware6d} require a forward pass for each instance.

\tabcolsep 4pt
\renewcommand{\arraystretch}{0.7}
\begin{table}[]
\centering
\caption{Inference time and memory footprint. 
Time is for a single image, and includes network inference (inf.) and registration (reg.) times. $N$ is the number of trained models.}
\vspace{-3mm}
\resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccl}
        \toprule
        Method & DNNs & Params [M] & Memory [GB] & Time [ms] (inf.+reg.) \\
        \toprule
        PVN3D \cite{pvn3d} & 1 & 38.6 & 3.17 & 417 (154 + 263) \\
        FFB6D \cite{ffb6d} & 1 & 33.8 & 2.46 & 285 (146 + 139) \\
        Wu et al.~\cite{geometricaware6d} & $N$ & 23.8$\times N$ & 2.04$\times N$ & 144 (143 + 1) \\
        \midrule
        Ours & 1 & 63.5 & 1.3 & 156 (118 + 38) \\
        \bottomrule
    \end{tabular}
\label{tab:times}
}
\end{table}
