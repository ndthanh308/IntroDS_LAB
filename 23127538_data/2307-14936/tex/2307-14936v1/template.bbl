\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Zan et~al.(2023)Zan, Chen, Zhang, Lu, Wu, Guan, Yongji, and
  Lou]{zan2022neural}
Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang
  Yongji, and Jian-Guang Lou.
\newblock Large language models meet {NL}2{C}ode: A survey.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 7443--7464,
  Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.acl-long.411}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Tom,
  Eccles, Keeling, Gimeno, Lago, Hubert, Choy, de, d’Autume, Babuschkin,
  Chen, Huang, Welbl, Gowal, Alexey, Cherepanov, Molloy, Mankowitz, Robson,
  Kohli, de, Freitas, Kavukcuoglu, and Vinyals]{alphacode}
Yujia Li, David~H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,
  R{\'e}mi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin~Dal Lago,
  Thomas Hubert, Peter Choy, Cyprien de, Masson d’Autume, Igor Babuschkin,
  Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov,
  James Molloy, Daniel~Jaymin Mankowitz, Esme~Sutherland Robson, Pushmeet
  Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378:\penalty0 1092 -- 1097, 2022.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph,
  Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat,
  Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat,
  Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei,
  Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah
  Fiedel.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock \emph{ArXiv}, abs/2204.02311, 2022.

\bibitem[Christopoulou et~al.(2022)Christopoulou, Lampouras, Gritta, Zhang,
  Guo, Li, Zhang, Xiao, Shen, Li, Yu, yu~Yan, Zhou, Wang, Ma, Iacobacci, Wang,
  Liang, Wei, Jiang, Wang, and Liu]{pangu-coder}
Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng
  Guo, Zhong-Yi Li, Qi~Zhang, Meng Xiao, Bo~Shen, Lin Li, Hao Yu, Li~yu~Yan,
  Pingyi Zhou, Xin Wang, Yu~Ma, Ignacio Iacobacci, Yasheng Wang, Guangtai
  Liang, Jia Wei, Xin Jiang, Qianxiang Wang, and Qun Liu.
\newblock {PanGu-Coder}: Program synthesis with function-level language
  modeling.
\newblock \emph{ArXiv}, abs/2207.11280, 2022.

\bibitem[Huggingface(2021)]{codeparrot}
Huggingface.
\newblock {Training CodeParrot from Scratch}, 2021.
\newblock \url{https://huggingface.co/blog/codeparrot}.

\bibitem[Xu et~al.(2022)Xu, Alon, Neubig, and Hellendoorn]{polycoder}
Frank~F. Xu, Uri Alon, Graham Neubig, and Vincent~J. Hellendoorn.
\newblock A systematic evaluation of large language models of code.
\newblock \emph{Proceedings of the 6th ACM SIGPLAN International Symposium on
  Machine Programming}, 2022.

\bibitem[Zan et~al.(2022{\natexlab{a}})Zan, Chen, Yang, Lin, Kim, Guan, Wang,
  Chen, and Lou]{cert}
Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji
  Wang, Weizhu Chen, and Jian-Guang Lou.
\newblock {CERT}: Continual pre-training on sketches for library-oriented code
  generation.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2022{\natexlab{a}}.

\bibitem[Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis,
  Muennighoff, Mishra, Gu, Dey, Umapathi, Anderson, Zi, Poirier, Schoelkopf,
  Troshin, Abulkhanov, Romero, Lappert, Toni, del R'io, Liu, Bose,
  Bhattacharyya, Zhuo, Yu, Villegas, Zocca, Mangrulkar, Lansky, Nguyen,
  Contractor, Villa, Li, Bahdanau, Jernite, Hughes, Fried, Guha, de~Vries, and
  von Werra]{santacoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki,
  Carlos~Mu{\~n}oz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alexander Gu,
  Manan Dey, Logesh~Kumar Umapathi, Carolyn~Jane Anderson, Yangtian Zi,
  J.~Poirier, Hailey Schoelkopf, Sergey~Mikhailovich Troshin, Dmitry
  Abulkhanov, Manuel Romero, Michael~Franz Lappert, Francesco~De Toni,
  Bernardo~Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya,
  Terry~Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David
  Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau,
  Yacine Jernite, Sean~Christopher Hughes, Daniel Fried, Arjun Guha, Harm
  de~Vries, and Leandro von Werra.
\newblock {SantaCoder}: don't reach for the stars!
\newblock \emph{ArXiv}, abs/2301.03988, 2023.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone,
  Akiki, Li, Chim, et~al.]{starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
  Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, et~al.]{mbpp}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[OpenAI(2023)]{gpt4-report}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, abs/2303.08774, 2023.
\newblock \doi{10.48550/arXiv.2303.08774}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2303.08774}.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,
  Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi, et~al.]{phi-1}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie
  Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
  de~Rosa, Olli Saarikivi, et~al.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}, 2023.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and
  Jiang]{wizardcoder}
Ziyang Luo, Can Xu, Pu~Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang
  Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.
\newblock {WizardCoder}: Empowering code large language models with
  evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023.

\bibitem[Lu et~al.(2022)Lu, Duan, Han, Guo, Hwang, and Svyatkovskiy]{reacc}
Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey
  Svyatkovskiy.
\newblock Reacc: A retrieval-augmented code completion framework.
\newblock \emph{arXiv preprint arXiv:2203.07722}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Zhang, Liu, Zan, Mao, Lou, and
  Chen]{repocoder}
Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi~Mao, Jian-Guang
  Lou, and Weizhu Chen.
\newblock Repocoder: Repository-level code completion through iterative
  retrieval and generation.
\newblock \emph{arXiv preprint arXiv:2303.12570}, 2023.

\bibitem[Liu et~al.(2023)Liu, Zhu, Xiao, Fu, Han, Yang, and Ye]{rltf}
Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye.
\newblock Rltf: Reinforcement learning from unit test feedback, 2023.

\bibitem[Le et~al.(2022)Le, Wang, Gotmare, Savarese, and Hoi]{coderl}
Hung Le, Yue Wang, Akhilesh~Deepak Gotmare, Silvio Savarese, and Steven~CH Hoi.
\newblock {CodeRL}: Mastering code generation through pretrained models and
  deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2207.01780}, abs/2207.01780, 2022.

\bibitem[Shojaee et~al.(2023)Shojaee, Jain, Tipirneni, and Reddy]{ppocoder}
Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan~K Reddy.
\newblock Execution-based code generation using deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2301.13816}, 2023.

\bibitem[Ouyang et~al.(2022{\natexlab{a}})Ouyang, Wu, Jiang, Almeida,
  Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Zhang, Nguyen, Zan, Lin, Lou, and Chen]{codet}
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and
  Weizhu Chen.
\newblock Codet: Code generation with generated tests.
\newblock \emph{arXiv preprint arXiv:2207.10397}, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Xia, Zou, Dong, Wang, Xue, Wang, Shen, Wang,
  Li, Su, Yang, and Tang]{codegeex}
Qinkai Zheng, Xiao Xia, Xu~Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan
  Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang.
\newblock {CodeGeeX}: A pre-trained model for code generation with multilingual
  evaluations on humaneval-x.
\newblock \emph{ArXiv}, abs/2303.17568, 2023.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock {BLOOM}: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Chai et~al.(2022)Chai, Wang, Pang, Sun, Tian, and Wu]{ernie-code}
Yekun Chai, Shuohuan Wang, Chao Pang, Yu~Sun, Hao Tian, and Hua Wu.
\newblock {ERNIE-Code}: Beyond english-centric cross-lingual pretraining for
  programming languages.
\newblock \emph{arXiv preprint arXiv:2212.06742}, 2022.

\bibitem[Chandel et~al.(2022)Chandel, Clement, Serrato, and Sundaresan]{jupyt5}
Shubham Chandel, Colin~B. Clement, Guillermo Serrato, and Neel Sundaresan.
\newblock Training and evaluating a jupyter notebook data science assistant.
\newblock \emph{ArXiv}, abs/2201.12901, 2022.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Alon, Xu, JIang, and
  Neubig]{doccoder}
Shuyan Zhou, Uri Alon, Frank~F Xu, Zhengbao JIang, and Graham Neubig.
\newblock {DocCoder}: Generating code by retrieving and reading docs.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{a}}.

\bibitem[Zan et~al.(2022{\natexlab{b}})Zan, Chen, Lin, Guan, Wang, and
  Lou]{apicoder}
Daoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou.
\newblock When language model meets private library.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2022{\natexlab{b}}.

\bibitem[Fried et~al.(2023)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong,
  Yih, Zettlemoyer, and Lewis]{incoder}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
  Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock {InCoder}: A generative model for code infilling and synthesis.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Bavarian et~al.(2022)Bavarian, Jun, Tezak, Schulman, McLeavey, Tworek,
  and Chen]{fim}
Mohammad Bavarian, Heewoo Jun, Nikolas~A. Tezak, John Schulman, Christine
  McLeavey, Jerry Tworek, and Mark Chen.
\newblock Efficient training of language models to fill in the middle.
\newblock \emph{ArXiv}, abs/2207.14255, 2022.

\bibitem[Nguyen et~al.(2023)Nguyen, Karampatziakis, and Chen]{mim}
Anh Nguyen, Nikos Karampatziakis, and Weizhu Chen.
\newblock {Meet in the Middle}: A new pre-training paradigm.
\newblock \emph{arXiv preprint arXiv:2303.07295}, 2023.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Liu, Xu, Iyer, Sun, Mao, Ma,
  Efrat, Yu, Yu, et~al.]{lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023{\natexlab{b}}.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{instruction-2}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem[Ouyang et~al.(2022{\natexlab{b}})Ouyang, Wu, Jiang, Almeida,
  Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton,
  Miller, Simens, Askell, Welinder, Christiano, Leike, and
  Lowe]{DBLP:conf/nips/Ouyang0JAWMZASR22}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul~F. Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.
\newblock URL
  \url{http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html}.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and Huang]{rrhf}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.
\newblock Rrhf: Rank responses to align language models with human feedback
  without tears.
\newblock \emph{arXiv preprint arXiv:2304.05302}, 2023.

\bibitem[Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and
  Zhang]{dong2023raft}
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang,
  Kashun Shum, and Tong Zhang.
\newblock {RAFT}: Reward ranked finetuning for generative foundation model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023.

\bibitem[Nijkamp et~al.(2022)Nijkamp, Pang, Hayashi, Tu, Wang, Zhou, Savarese,
  and Xiong]{codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio
  Savarese, and Caiming Xiong.
\newblock {CodeGen}: An open large language model for code with multi-turn
  program synthesis.
\newblock \emph{arXiv preprint arXiv:2203.13474}, 2022.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and
  Jiang]{wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang.
\newblock {Wizardlm}: Empowering large language models to follow complex
  instructions.
\newblock \emph{arXiv preprint arXiv:2304.12244}, 2023.

\bibitem[Shazeer(2019)]{DBLP:journals/corr/abs-1911-02150}
Noam Shazeer.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{CoRR}, abs/1911.02150, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.02150}.

\bibitem[Wang et~al.(2023)Wang, Le, Gotmare, Bui, Li, and Hoi]{codet5plus}
Yue Wang, Hung Le, Akhilesh~Deepak Gotmare, Nghi~DQ Bui, Junnan Li, and
  Steven~CH Hoi.
\newblock Codet5+: Open code large language models for code understanding and
  generation.
\newblock \emph{arXiv preprint arXiv:2305.07922}, 2023.

\bibitem[Yu et~al.(2023)Yu, Shen, Ran, Zhang, Zhang, Ma, Liang, Li, Xie, and
  Wang]{codereval}
Hao Yu, Bo~Shen, Dezhi Ran, Jiaxin Zhang, Qi~Zhang, Yuchi Ma, Guangtai Liang,
  Ying Li, Tao Xie, and Qianxiang Wang.
\newblock Codereval: A benchmark of pragmatic code generation with generative
  pre-trained models.
\newblock \emph{arXiv preprint arXiv:2302.00288}, 2023.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\end{thebibliography}
