@article{ppocoder,
  title={Execution-based code generation using deep reinforcement learning},
  author={Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={arXiv preprint arXiv:2301.13816},
  year={2023}
}

@misc{rltf,
      title={RLTF: Reinforcement Learning from Unit Test Feedback}, 
      author={Jiate Liu and Yiqin Zhu and Kaiwen Xiao and Qiang Fu and Xiao Han and Wei Yang and Deheng Ye},
      year={2023},
      eprint={2307.04349},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{DBLP:journals/corr/abs-1911-02150,
  author       = {Noam Shazeer},
  title        = {Fast Transformer Decoding: One Write-Head is All You Need},
  journal      = {CoRR},
  volume       = {abs/1911.02150},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.02150},
  eprinttype    = {arXiv},
  eprint       = {1911.02150},
  timestamp    = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-02150.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/nips/DaoFERR22,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/67d57c32e20fd0a7a302cb81d36e40d5-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/DaoFERR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302},
  year={2023}
}

@article{alphacode,
  title={Competition-level code generation with AlphaCode},
  author={Yujia Li and David H. Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and R{\'e}mi Leblond and Tom and Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and Thomas Hubert and Peter Choy and Cyprien de and Masson dâ€™Autume and Igor Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey and Cherepanov and James Molloy and Daniel Jaymin Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de and Freitas and Koray Kavukcuoglu and Oriol Vinyals},
  journal={Science},
  year={2022},
  volume={378},
  pages={1092 - 1097}
}

@article{pangu-coder,
  title={{PanGu-Coder}: Program Synthesis with Function-Level Language Modeling},
  author={Fenia Christopoulou and Gerasimos Lampouras and Milan Gritta and Guchun Zhang and Yinpeng Guo and Zhong-Yi Li and Qi Zhang and Meng Xiao and Bo Shen and Lin Li and Hao Yu and Li-yu Yan and Pingyi Zhou and Xin Wang and Yu Ma and Ignacio Iacobacci and Yasheng Wang and Guangtai Liang and Jia Wei and Xin Jiang and Qianxiang Wang and Qun Liu},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.11280}
}

@article{palm,
  title={{PaLM}: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.02311}
}


@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@misc{codeparrot,
  author = {Huggingface},
  urldate = {2021},
  year = {2021},
  title = {{Training CodeParrot from Scratch}},
  note  = {\url{https://huggingface.co/blog/codeparrot}}
}

@article{polycoder,
  title={A systematic evaluation of large language models of code},
  author={Frank F. Xu and Uri Alon and Graham Neubig and Vincent J. Hellendoorn},
  journal={Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
  year={2022}
}

@inproceedings{cert,
  title={{CERT}: Continual Pre-Training on Sketches for Library-Oriented Code Generation},
  author={Daoguang Zan and Bei Chen and Dejian Yang and Zeqi Lin and Minsu Kim and Bei Guan and Yongji Wang and Weizhu Chen and Jian-Guang Lou},
  booktitle={International Joint Conference on Artificial Intelligence},
  year={2022}
}

@article{santacoder,
  title={{SantaCoder}: don't reach for the stars!},
  author={Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Mu{\~n}oz Ferrandis and Niklas Muennighoff and Mayank Mishra and Alexander Gu and Manan Dey and Logesh Kumar Umapathi and Carolyn Jane Anderson and Yangtian Zi and J. Poirier and Hailey Schoelkopf and Sergey Mikhailovich Troshin and Dmitry Abulkhanov and Manuel Romero and Michael Franz Lappert and Francesco De Toni and Bernardo Garc'ia del R'io and Qian Liu and Shamik Bose and Urvashi Bhattacharyya and Terry Yue Zhuo and Ian Yu and Paulo Villegas and Marco Zocca and Sourab Mangrulkar and David Lansky and Huu Nguyen and Danish Contractor and Luisa Villa and Jia Li and Dzmitry Bahdanau and Yacine Jernite and Sean Christopher Hughes and Daniel Fried and Arjun Guha and Harm de Vries and Leandro von Werra},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.03988}
}

@article{starcoder,
  title={StarCoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@inproceedings{deepspeed,
  author       = {Samyam Rajbhandari and
                  Jeff Rasley and
                  Olatunji Ruwase and
                  Yuxiong He},
  editor       = {Christine Cuicchi and
                  Irene Qualters and
                  William T. Kramer},
  title        = {ZeRO: memory optimizations toward training trillion parameter models},
  booktitle    = {Proceedings of the International Conference for High Performance Computing,
                  Networking, Storage and Analysis, {SC} 2020, Virtual Event / Atlanta,
                  Georgia, USA, November 9-19, 2020},
  pages        = {20},
  publisher    = {{IEEE/ACM}},
  year         = {2020},
  url          = {https://doi.org/10.1109/SC41405.2020.00024},
  doi          = {10.1109/SC41405.2020.00024},
  timestamp    = {Wed, 04 May 2022 13:02:27 +0200},
  biburl       = {https://dblp.org/rec/conf/sc/RajbhandariRRH20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{codegeex,
  title={{CodeGeeX}: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X},
  author={Qinkai Zheng and Xiao Xia and Xu Zou and Yuxiao Dong and Shanshan Wang and Yufei Xue and Zi-Yuan Wang and Lei Shen and Andi Wang and Yang Li and Teng Su and Zhilin Yang and Jie Tang},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.17568}
}

@article{bloom,
  title={{BLOOM}: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}

@article{ernie-code,
  title={{ERNIE-Code}: Beyond English-Centric Cross-lingual Pretraining for Programming Languages},
  author={Chai, Yekun and Wang, Shuohuan and Pang, Chao and Sun, Yu and Tian, Hao and Wu, Hua},
  journal={arXiv preprint arXiv:2212.06742},
  year={2022}
}

@article{jupyt5,
  title={Training and Evaluating a Jupyter Notebook Data Science Assistant},
  author={Shubham Chandel and Colin B. Clement and Guillermo Serrato and Neel Sundaresan},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.12901}
}

@inproceedings{doccoder,
  title={{DocCoder}: Generating Code by Retrieving and Reading Docs},
  author={Zhou, Shuyan and Alon, Uri and Xu, Frank F and JIang, Zhengbao and Neubig, Graham},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{apicoder,
  title={When Language Model Meets Private Library},
  author={Daoguang Zan and Bei Chen and Zeqi Lin and Bei Guan and Yongji Wang and Jian-Guang Lou},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2022}
}

@article{lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{codegen2,
  title={{Codegen2}: Lessons for training llms on programming and natural languages},
  author={Nijkamp, Erik and Hayashi, Hiroaki and Xiong, Caiming and Savarese, Silvio and Zhou, Yingbo},
  journal={arXiv preprint arXiv:2305.02309},
  year={2023}
}

@article{wizardcoder,
  title={{WizardCoder}: Empowering Code Large Language Models with Evol-Instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}
@article{gunasekar2023textbooks,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{wizardlm,
  title={{Wizardlm}: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}
@article{T0,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
  journal={arXiv preprint arXiv:2110.08207},
  year={2021}
}

@article{BLOOMZ,
  title={Crosslingual generalization through multitask finetuning},
  author={Muennighoff, Niklas and Wang, Thomas and Sutawika, Lintang and Roberts, Adam and Biderman, Stella and Scao, Teven Le and Bari, M Saiful and Shen, Sheng and Yong, Zheng-Xin and Schoelkopf, Hailey and others},
  journal={arXiv preprint arXiv:2211.01786},
  year={2022}
}

@article{Tk-Instruct,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and others},
  journal={arXiv preprint arXiv:2204.07705},
  year={2022}
}

@article{FLAN-T5,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}



@article{phi-1,
  title={Textbooks Are All You Need},
  author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
  journal={arXiv preprint arXiv:2306.11644},
  year={2023}
}

@article{coderl,
  title={{CodeRL}: Mastering code generation through pretrained models and deep reinforcement learning},
  author={Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2207.01780},
  year={2022},
  volume={abs/2207.01780}
}

@article{coderl-2,
  title={Tuning Models of Code with Compiler-Generated Reinforcement Learning Feedback},
  author={Jain, Abhinav and Adiole, Chima and Chaudhuri, Swarat and Reps, Thomas and Jermaine, Chris},
  journal={arXiv preprint arXiv:2305.18341},
  year={2023}
}

@inproceedings{zan2022neural,
    title = "Large Language Models Meet {NL}2{C}ode: A Survey",
    author = "Zan, Daoguang  and
      Chen, Bei  and
      Zhang, Fengji  and
      Lu, Dianjie  and
      Wu, Bingchao  and
      Guan, Bei  and
      Yongji, Wang  and
      Lou, Jian-Guang",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.411",
    pages = "7443--7464",
    abstract = "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are {``}Large Size, Premium Data, Expert Tuning{''}. In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
}

@inproceedings{incoder,
title={{InCoder}: A Generative Model for Code Infilling and Synthesis},
author={Daniel Fried and Armen Aghajanyan and Jessy Lin and Sida Wang and Eric Wallace and Freda Shi and Ruiqi Zhong and Scott Yih and Luke Zettlemoyer and Mike Lewis},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023}
}

@article{fim,
  title={Efficient Training of Language Models to Fill in the Middle},
  author={Mohammad Bavarian and Heewoo Jun and Nikolas A. Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},
  journal={ArXiv},
  year={2022},
  volume={abs/2207.14255}
}

@article{mim,
  title={{Meet in the Middle}: A New Pre-training Paradigm},
  author={Nguyen, Anh and Karampatziakis, Nikos and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.07295},
  year={2023}
}

@article{instruction-2,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{codet,
  title={Codet: Code generation with generated tests},
  author={Chen, Bei and Zhang, Fengji and Nguyen, Anh and Zan, Daoguang and Lin, Zeqi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2207.10397},
  year={2022}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE},
  doi={10.1109/SOCPAR.2014.7008025}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@inproceedings{DBLP:conf/nips/Ouyang0JAWMZASR22,
  author       = {Long Ouyang and
                  Jeffrey Wu and
                  Xu Jiang and
                  Diogo Almeida and
                  Carroll L. Wainwright and
                  Pamela Mishkin and
                  Chong Zhang and
                  Sandhini Agarwal and
                  Katarina Slama and
                  Alex Ray and
                  John Schulman and
                  Jacob Hilton and
                  Fraser Kelton and
                  Luke Miller and
                  Maddie Simens and
                  Amanda Askell and
                  Peter Welinder and
                  Paul F. Christiano and
                  Jan Leike and
                  Ryan Lowe},
  title        = {Training language models to follow instructions with human feedback},
  booktitle    = {NeurIPS},
  year         = {2022},
  url          = {http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/Ouyang0JAWMZASR22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt4-report,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.08774},
  doi          = {10.48550/arXiv.2303.08774},
  eprinttype    = {arXiv},
  eprint       = {2303.08774},
  timestamp    = {Mon, 20 Mar 2023 15:23:19 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-08774.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{pangucoder,
  title={{PanGu-coder}: Program synthesis with function-level language modeling},
  author={Christopoulou, Fenia and Lampouras, Gerasimos and Gritta, Milan and Zhang, Guchun and Guo, Yinpeng and Li, Zhongqi and Zhang, Qi and Xiao, Meng and Shen, Bo and Li, Lin and others},
  journal={arXiv preprint arXiv:2207.11280},
  year={2022}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{codegen,
  title={{CodeGen}: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{codet5plus,
  title={Codet5+: Open code large language models for code understanding and generation},
  author={Wang, Yue and Le, Hung and Gotmare, Akhilesh Deepak and Bui, Nghi DQ and Li, Junnan and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2305.07922},
  year={2023}
}

@article{instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{reacc,
  title={Reacc: A retrieval-augmented code completion framework},
  author={Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  journal={arXiv preprint arXiv:2203.07722},
  year={2022}
}


@article{repocoder,
  title={Repocoder: Repository-level code completion through iterative retrieval and generation},
  author={Zhang, Fengji and Chen, Bei and Zhang, Yue and Liu, Jin and Zan, Daoguang and Mao, Yi and Lou, Jian-Guang and Chen, Weizhu},
  journal={arXiv preprint arXiv:2303.12570},
  year={2023}
}

@article{codereval,
  title={CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
  author={Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Xie, Tao and Wang, Qianxiang},
  journal={arXiv preprint arXiv:2302.00288},
  year={2023}
}

@misc{key2022i,
  title={I Speak, You Verify: Toward Trustworthy Neural Program Synthesis}, 
  author={Darren Key and Wen-Ding Li and Kevin Ellis},
  year={2022},
  eprint={2210.00848},
  archivePrefix={arXiv},
  primaryClass={cs.SE}
}

@article{dong2023raft,
  title={{RAFT}: Reward rAnked FineTuning for Generative Foundation Model Alignment}, 
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  year={2023},
  journal={arXiv preprint arXiv:2304.06767}
}


@article{mbpp,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}