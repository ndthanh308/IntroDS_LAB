
@misc{easymocap,  
    title = {EasyMoCap},
    year = {2021},
    url = {https://github.com/zju3dv/EasyMocap}
}

@misc{nimblephysics,
  title = {NimblePhysics},
  year = {2022},
  journal = {GitHub repository},
  url = {https://github.com/keenon/nimblephysics}
}

@misc{levanter,
  title = {Levanter},
  year = {2022},
  journal = {GitHub repository},
  url = {https://github.com/stanford-crfm/levanter/}
}

@misc{haliax,
  title = {Haliax},
  year = {2022},
  journal = {GitHub repository},
  url = {https://github.com/stanford-crfm/haliax/}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@software{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{bilney_concurrent_2003,
  title = {Concurrent Related Validity of the {{GAITRite}} Walkway System for Quantification of the Spatial and Temporal Parameters of Gait},
  author = {Bilney, Belinda and Morris, Meg and Webster, Kate},
  year = {2003},
  month = feb,
  journal = {Gait \& Posture},
  volume = {17},
  number = {1},
  pages = {68--74},
  issn = {0966-6362},
  doi = {10.1016/s0966-6362(02)00053-x},
  abstract = {The GAITRite is a portable gait analysis tool for automated measurement of spatiotemporal gait parameters. Although frequently used for clinical and research purposes, the concurrent validity of GAITRite has not been validated against a criterion measure. The aim of this experiment was to investigate the concurrent validity and test retest reliability of the GAITRite carpet walkway system for quantification of spatial and temporal parameters of the footstep pattern. Twenty-five healthy adults aged 21-71 years (mean 40.5 years, S.D. 17.2) performed three walk trials at self-selected pace, three at fast pace and three at slow pace. For each trial, data were simultaneously collected from the GAITRite and a Clinical Stride Analyzer, which has established reliability and validity. At preferred, slow and fast walking pace there were very high correlations between the two measurement systems for gait speed (ICC (2,1)=0.99), stride length (ICC (2,1)=0.99) and cadence (ICC (2,1)=0.99). Correlations between the electronic carpet and the stride analyser were moderate to high for single limb support (SLS) time (ICC (2,1)=0.69-0.91) and weak for the proportion of the gait cycle spent in double limb support (ICC (2,1)=0.44-0.57). The reliability of repeated measures for the GAITRite was good at preferred and fast speed for speed (ICC (3,1)=0.93-0.94), cadence (ICC (3,1)=0.92-0.94), stride length (ICC (3,1)=0.97), single support (ICC (3,1)=0.85-0.93) and the proportion of the gait cycle spent in double limb support (ICC (3,1)=0.89-0.92). The repeatability of the GAITRite measures were more variable at slow speed (ICC (3,1)=0.76-0.91). These results indicate that the GAITRite system has strong concurrent validity and test retest reliability, in addition to being a portable, simple clinical tool for the objective assessment of gait.},
  langid = {english},
  pmid = {12535728},
  keywords = {Adult,Aged,{Diagnosis, Computer-Assisted},Disability Evaluation,Female,Gait,Humans,Male,Middle Aged,Reproducibility of Results,Software}
}

@misc{bommasani_opportunities_2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2022-07-14},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/rcotton/Zotero/storage/VM27TT5A/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf;/Users/rcotton/Zotero/storage/FP6QGEH8/2108.html}
}

@article{chen_simple_2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = feb,
  urldate = {2020-02-14},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  file = {/Users/rcotton/Zotero/storage/TB65H722/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Visual Representations(2).pdf}
}

@misc{cotton_improved_2023,
  title = {Improved {{Trajectory Reconstruction}} for {{Markerless Pose Estimation}}},
  author = {Cotton, R. James and Cimorelli, Anthony and Shah, Kunal and Anarwala, Shawana and Uhlrich, Scott and Karakostas, Tasos},
  year = {2023},
  month = mar,
  number = {arXiv:2303.02413},
  eprint = {2303.02413},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.02413},
  urldate = {2023-03-09},
  abstract = {Markerless pose estimation allows reconstructing human movement from multiple synchronized and calibrated views, and has the potential to make movement analysis easy and quick, including gait analysis. This could enable much more frequent and quantitative characterization of gait impairments, allowing better monitoring of outcomes and responses to interventions. However, the impact of different keypoint detectors and reconstruction algorithms on markerless pose estimation accuracy has not been thoroughly evaluated. We tested these algorithmic choices on data acquired from a multicamera system from a heterogeneous sample of 25 individuals seen in a rehabilitation hospital. We found that using a top-down keypoint detector and reconstructing trajectories with an implicit function enabled accurate, smooth and anatomically plausible trajectories, with a noise in the step width estimates compared to a GaitRite walkway of only 8mm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rcotton/Zotero/storage/4YKZ4GLX/Cotton et al. - 2023 - Improved Trajectory Reconstruction for Markerless .pdf;/Users/rcotton/Zotero/storage/CC38VUY5/2303.html}
}

@article{cotton_letter_2022,
  title = {Letter to the {{Editor}}: "{{Precision Rehabilitation}}: {{Optimizing Function}}, {{Adding Value}} to {{Health Care}}"},
  shorttitle = {Precision {{Rehabilitation}}},
  author = {Cotton, R. James and Segal Rick, Richard L. and Seamon, Bryant A. and Sahu, Amrita and McLeod, Michelle M. and Davis, Randal D. and Ramey, Sharon Landesman and French, Margaret A. and Roemmich, Ryan T. and Daley, Kelly and Beier, Meghan and Penttinen, Sharon and Raghavan, Preeti and Searson, Peter and Wegener, Stephen and Celnik, Pablo},
  year = {2022},
  month = sep,
  journal = {Archives of Physical Medicine and Rehabilitation},
  volume = {103},
  number = {9},
  pages = {1883--1884},
  issn = {1532-821X},
  doi = {10.1016/j.apmr.2022.04.017},
  langid = {english},
  pmcid = {PMC9979846},
  pmid = {35690092},
  keywords = {Data Collection,Delivery of Health Care,Humans,Medicine}
}

@misc{cotton_markerless_2023,
  title = {Markerless {{Motion Capture}} and {{Biomechanical Analysis Pipeline}}},
  author = {Cotton, R. James and DeLillo, Allison and Cimorelli, Anthony and Shah, Kunal and Peiffer, J. D. and Anarwala, Shawana and Abdou, Kayan and Karakostas, Tasos},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10654},
  eprint = {2303.10654},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.10654},
  urldate = {2023-03-21},
  abstract = {Markerless motion capture using computer vision and human pose estimation (HPE) has the potential to expand access to precise movement analysis. This could greatly benefit rehabilitation by enabling more accurate tracking of outcomes and providing more sensitive tools for research. There are numerous steps between obtaining videos to extracting accurate biomechanical results and limited research to guide many critical design decisions in these pipelines. In this work, we analyze several of these steps including the algorithm used to detect keypoints and the keypoint set, the approach to reconstructing trajectories for biomechanical inverse kinematics and optimizing the IK process. Several features we find important are: 1) using a recent algorithm trained on many datasets that produces a dense set of biomechanically-motivated keypoints, 2) using an implicit representation to reconstruct smooth, anatomically constrained marker trajectories for IK, 3) iteratively optimizing the biomechanical model to match the dense markers, 4) appropriate regularization of the IK process. Our pipeline makes it easy to obtain accurate biomechanical estimates of movement in a rehabilitation hospital.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rcotton/Zotero/storage/QVH9Z4P5/Cotton et al. - 2023 - Markerless Motion Capture and Biomechanical Analys.pdf;/Users/rcotton/Zotero/storage/XBANJ9K3/2303.html}
}

@article{cotton_posepipe_2022,
  title = {{{PosePipe}}: {{Open-Source Human Pose Estimation Pipeline}} for {{Clinical Research}}},
  shorttitle = {{{PosePipe}}},
  author = {Cotton, R. James},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.08792 [cs, q-bio]},
  eprint = {2203.08792},
  primaryclass = {cs, q-bio},
  urldate = {2022-03-17},
  abstract = {There has been significant progress in machine learning algorithms for human pose estimation that may provide immense value in rehabilitation and movement sciences. However, there remain several challenges to routine use of these tools for clinical practice and translational research, including: 1) high technical barrier to entry, 2) rapidly evolving space of algorithms, 3) challenging algorithmic interdependencies, and 4) complex data management requirements between these components. To mitigate these barriers, we developed a human pose estimation pipeline that facilitates running state-of-the-art algorithms on data acquired in clinical context. Our system allows for running different implementations of several classes of algorithms and handles their interdependencies easily. These algorithm classes include subject identification and tracking, 2D keypoint detection, 3D joint location estimation, and estimating the pose of body models. The system uses a database to manage videos, intermediate analyses, and data for computations at each stage. It also provides tools for data visualization, including generating video overlays that also obscure faces to enhance privacy. Our goal in this work is not to train new algorithms, but to advance the use of cutting-edge human pose estimation algorithms for clinical and translation research. We show that this tool facilitates analyzing large numbers of videos of human movement ranging from gait laboratories analyses, to clinic and therapy visits, to people in the community. We also highlight limitations of these algorithms when applied to clinical populations in a rehabilitation setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Software Engineering,Quantitative Biology - Quantitative Methods},
  file = {/Users/rcotton/Zotero/storage/R7T2ZTLT/Cotton - 2022 - PosePipe Open-Source Human Pose Estimation Pipeli.pdf;/Users/rcotton/Zotero/storage/JF487734/2203.html}
}

@inproceedings{cotton_transforming_2022,
  title = {Transforming {{Gait}}: {{Video-Based Spatiotemporal Gait Analysis}}},
  shorttitle = {Transforming {{Gait}}},
  booktitle = {2022 44th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Cotton, R. James and McClerklin, Emoonah and Cimorelli, Anthony and Patel, Ankit and Karakostas, Tasos},
  year = {2022},
  month = jul,
  pages = {115--120},
  issn = {2694-0604},
  doi = {10.1109/EMBC48229.2022.9871036},
  abstract = {Human pose estimation from monocular video is a rapidly advancing field that offers great promise to human movement science and rehabilitation. This potential is tempered by the smaller body of work ensuring the outputs are clinically meaningful and properly calibrated. Gait analysis, typically performed in a dedicated lab, produces precise measurements including kinematics and step timing. Using over 7000 monocular video from an instrumented gait analysis lab, we trained a neural network to map 3D joint trajectories and the height of individuals onto interpretable biomechanical outputs including gait cycle timing and sagittal plane joint kinematics and spatiotemporal trajectories. This task specific layer produces accurate estimates of the timing of foot contact and foot off events. After parsing the kinematic outputs into individual gait cycles, it also enables accurate cycle-by-cycle estimates of cadence, step time, double and single support time, walking speed and step length.},
  keywords = {Biomechanics,Computer Science - Computer Vision and Pattern Recognition,Kinematics,Neural networks,Pose estimation,Quantitative Biology - Quantitative Methods,Three-dimensional displays,Timing,Trajectory},
  file = {/Users/rcotton/Zotero/storage/FPZJFXZV/Transforming_Gait_Video-Based_Spatiotemporal_Gait_Analysis.pdf;/Users/rcotton/Zotero/storage/K3VQ69E3/Cotton et al. - 2022 - Transforming Gait Video-Based Spatiotemporal Gait.pdf;/Users/rcotton/Zotero/storage/3LMP5LBQ/2203.html;/Users/rcotton/Zotero/storage/UJBMVUVA/9871036.html}
}

@misc{devlin_bert_2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1810.04805},
  urldate = {2022-06-21},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/rcotton/Zotero/storage/J2J83XT4/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf;/Users/rcotton/Zotero/storage/JWCWU7KT/1810.html}
}

@article{endo_gaitforemer_2022,
  title = {{{GaitForeMer}}: {{Self-Supervised Pre-Training}} of {{Transformers}} via {{Human Motion Forecasting}} for {{Few-Shot Gait Impairment Severity Estimation}}},
  shorttitle = {{{GaitForeMer}}},
  author = {Endo, Mark and Poston, Kathleen L. and Sullivan, Edith V. and {Fei-Fei}, Li and Pohl, Kilian M. and Adeli, Ehsan},
  year = {2022},
  month = sep,
  journal = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  volume = {13438},
  pages = {130--139},
  doi = {10.1007/978-3-031-16452-1_13},
  abstract = {Parkinson's disease (PD) is a neurological disorder that has a variety of observable motor-related symptoms such as slow movement, tremor, muscular rigidity, and impaired posture. PD is typically diagnosed by evaluating the severity of motor impairments according to scoring systems such as the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). Automated severity prediction using video recordings of individuals provides a promising route for non-intrusive monitoring of motor impairments. However, the limited size of PD gait data hinders model ability and clinical potential. Because of this clinical data scarcity and inspired by the recent advances in self-supervised large-scale language models like GPT-3, we use human motion forecasting as an effective self-supervised pre-training task for the estimation of motor impairment severity. We introduce GaitForeMer, Gait Forecasting and impairment estimation transforMer, which is first pre-trained on public datasets to forecast gait movements and then applied to clinical data to predict MDS-UPDRS gait impairment severity. Our method outperforms previous approaches that rely solely on clinical data by a large margin, achieving an F1 score of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we show how public human movement data repositories can assist clinical use cases through learning universal motion representations. The code is available at https://github.com/markendo/GaitForeMer.},
  langid = {english},
  pmcid = {PMC9635991},
  pmid = {36342887},
  keywords = {code,Few-shot learning,Gait analysis,read,Transformer},
  file = {/Users/rcotton/Zotero/storage/XA63MURR/Endo et al. - 2022 - GaitForeMer Self-Supervised Pre-Training of Trans.pdf}
}

@book{fda-nih_biomarker_working_group_fda-nih_2021,
  title = {{{FDA-NIH Biomarker Working Group}}},
  author = {{FDA-NIH Biomarker Working Group}},
  year = {2021},
  month = jan,
  journal = {BEST (Biomarkers, EndpointS, and other Tools) Resource [Internet]},
  publisher = {{Food and Drug Administration (US)}},
  urldate = {2022-04-16},
  abstract = {Aloka G. Chakravarty},
  langid = {english},
  file = {/Users/rcotton/Zotero/storage/VN94S4KM/NBK338449.html}
}

@article{french_precision_2022,
  title = {Precision {{Rehabilitation}}: {{Optimizing Function}}, {{Adding Value}} to {{Health Care}}},
  shorttitle = {Precision {{Rehabilitation}}},
  author = {French, Margaret A. and Roemmich, Ryan T. and Daley, Kelly and Beier, Meghan and Penttinen, Sharon and Raghavan, Preeti and Searson, Peter and Wegener, Stephen and Celnik, Pablo},
  year = {2022},
  month = feb,
  journal = {Archives of Physical Medicine and Rehabilitation},
  pages = {S0003-9993(22)00213-1},
  issn = {1532-821X},
  doi = {10.1016/j.apmr.2022.01.154},
  abstract = {Precision medicine efforts are underway in many medical disciplines; however, the power of precision rehabilitation has not yet been explored. Precision medicine aims to deliver the right intervention, at the right time, in the right setting, for the right person, ultimately bolstering the value of the care that we provide. To date, precision medicine efforts have rarely focused on function at the level of a person, but precision rehabilitation is poised to change this and bring the focus on function to the broader precision medicine enterprise. To do this, subgroups of individuals must be identified based on their level of function via precise measurement of their abilities in the physical, cognitive, and psychosocial domains.~Adoption of electronic health records, advances in data storage and analytics, and improved measurement technology make this shift possible. Here we detail critical components of the precision rehabilitation framework, including (1) the synergistic use of various study designs, (2) the need for standardized functional measurements, (3) the importance of precise and longitudinal measures of function, (4) the utility of comprehensive databases, (5) the importance of predictive analyses, and (6) the need for system and team science. Precision rehabilitation has the potential to revolutionize clinical care, optimize function for all individuals, and magnify the value of rehabilitation in health care; however, to reap the benefits of precision rehabilitation, the rehabilitation community must actively pursue this shift.},
  langid = {english},
  pmid = {35181267},
  keywords = {Personalized medicine,Precision medicine,Rehabilitation},
  file = {/Users/rcotton/Zotero/storage/LMEZH2IE/French et al. - 2022 - Precision Rehabilitation Optimizing Function, Add.pdf}
}

@article{ghorbani_movi_2021,
  title = {{{MoVi}}: {{A}} Large Multi-Purpose Human Motion and Video Dataset},
  shorttitle = {{{MoVi}}},
  author = {Ghorbani, Saeed and Mahdaviani, Kimia and Thaler, Anne and Kording, Konrad and Cook, Douglas James and Blohm, Gunnar and Troje, Nikolaus F.},
  year = {2021},
  month = jun,
  journal = {PLOS ONE},
  volume = {16},
  number = {6},
  pages = {e0253157},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0253157},
  urldate = {2023-02-20},
  abstract = {Large high-quality datasets of human body shape and kinematics lay the foundation for modelling and simulation approaches in computer vision, computer graphics, and biomechanics. Creating datasets that combine naturalistic recordings with high-accuracy data about ground truth body shape and pose is challenging because different motion recording systems are either optimized for one or the other. We address this issue in our dataset by using different hardware systems to record partially overlapping information and synchronized data that lend themselves to transfer learning. This multimodal dataset contains 9 hours of optical motion capture data, 17 hours of video data from 4 different points of view recorded by stationary and hand-held cameras, and 6.6 hours of inertial measurement units data recorded from 60 female and 30 male actors performing a collection of 21 everyday actions and sports movements. The processed motion capture data is also available as realistic 3D human meshes. We anticipate use of this dataset for research on human pose estimation, action recognition, motion modelling, gait analysis, and body shape reconstruction.},
  langid = {english},
  keywords = {Biomechanics,Cameras,Computer software,Computer vision,Grasshoppers,Human motion dataset,IMU,Kinematics,Motion,optical motion capture,Skeletal joints,video capture},
  file = {/Users/rcotton/Zotero/storage/3QJDEJ6C/Ghorbani et al. - 2021 - MoVi A large multi-purpose human motion and video.pdf;/Users/rcotton/Zotero/storage/6MHQCGRP/full-text.pdf}
}

@article{karashchuk_anipose_2020,
  title = {Anipose: A Toolkit for Robust Markerless {{3D}} Pose Estimation},
  author = {Karashchuk, Pierre and Rupp, Katie L. and Dickinson, Evyn S. and Sanders, Elischa and Azim, Eiman and Brunton, Bingni W. and Tuthill, John C.},
  year = {2020},
  month = may,
  journal = {bioRxiv},
  pages = {2020.05.26.117325},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.05.26.117325},
  urldate = {2020-06-03},
  abstract = {Quantifying movement is critical for understanding animal behavior. Advances in computer vision now enable markerless tracking from 2D video, but most animals live and move in 3D. Here, we introduce Anipose, a Python toolkit for robust markerless 3D pose estimation. Anipose consists of four components: (1) a 3D calibration module, (2) filters to resolve 2D tracking errors, (3) a triangulation module that integrates temporal and spatial constraints, and (4) a pipeline to structure processing of large numbers of videos. We evaluate Anipose on four datasets: a moving calibration board, fruit flies walking on a treadmill, mice reaching for a pellet, and humans performing various actions. Because Anipose is built on popular 2D tracking methods (e.g., DeepLabCut), users can expand their existing experimental setups to incorporate robust 3D tracking. We hope this open-source software and accompanying tutorials ([anipose.org][1]) will facilitate the analysis of 3D animal behavior and the biology that underlies it. \#\#\# Competing Interest Statement The authors have declared no competing interest.  [1]: http://anipose.org}
}

@article{khosla_supervised_2020,
  title = {Supervised {{Contrastive Learning}}},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  year = {2020},
  month = apr,
  urldate = {2020-04-27},
  abstract = {Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1\%, setting a new state of the art number of 78.8\% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.},
  file = {/Users/rcotton/Zotero/storage/4NWUSYJ2/full-text.pdf}
}

@article{kidger_equinox_2021,
  title = {Equinox: Neural Networks in {{JAX}} via Callable {{PyTrees}} and Filtered Transformations},
  shorttitle = {Equinox},
  author = {Kidger, Patrick and Garcia, Cristian},
  year = {2021},
  month = oct,
  urldate = {2021-11-02},
  abstract = {JAX and PyTorch are two popular Python autodifferentiation frameworks. JAX is based around pure functions and functional programming. PyTorch has popularised the use of an object-oriented (OO) class-based syntax for defining parameterised functions, such as neural networks. That this seems like a fundamental difference means current libraries for building parameterised functions in JAX have either rejected the OO approach entirely (Stax) or have introduced OO-to-functional transformations, multiple new abstractions, and been limited in the extent to which they integrate with JAX (Flax, Haiku, Objax). Either way this OO/functional difference has been a source of tension. Here, we introduce `Equinox', a small neural network library showing how a PyTorch-like class-based approach may be admitted without sacrificing JAX-like functional programming. We provide two main ideas. One: parameterised functions are themselves represented as `PyTrees', which means that the parameterisation of a function is transparent to the JAX framework. Two: we filter a PyTree to isolate just those components that should be treated when transforming (`jit', `grad' or `vmap'-ing) a higher-order function of a parameterised function -- such as a loss function applied to a model. Overall Equinox resolves the above tension without introducing any new programmatic abstractions: only PyTrees and transformations, just as with regular JAX. Equinox is available at \textbackslash url\{https://github.com/patrick-kidger/equinox\}.},
  langid = {english},
  file = {/Users/rcotton/Zotero/storage/IYUK6SIP/Kidger and Garcia - 2021 - Equinox neural networks in JAX via callable PyTre.pdf;/Users/rcotton/Zotero/storage/B238MQ5P/2111.html}
}

@misc{liu_towards_2023,
  title = {Towards {{Better Few-Shot}} and {{Finetuning Performance}} with {{Forgetful Causal Language Models}}},
  author = {Liu, Hao and Geng, Xinyang and Lee, Lisa and Mordatch, Igor and Levine, Sergey and Narang, Sharan and Abbeel, Pieter},
  year = {2023},
  month = jan,
  number = {arXiv:2210.13432},
  eprint = {2210.13432},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-26},
  abstract = {Large language models (LLM) trained using the next-token-prediction objective, such as GPT3 and PaLM, have revolutionized natural language processing in recent years by showing impressive zero-shot and few-shot capabilities across a wide range of tasks. In this work, we propose a simple technique that significantly boosts the performance of LLMs without adding computational cost. Our key observation is that, by performing the next token prediction task with randomly selected past tokens masked out, we can improve the quality of the learned representations for downstream language understanding tasks. We hypothesize that randomly masking past tokens prevents over-attending to recent tokens and encourages attention to tokens in the distant past. We find that our method, Forgetful Causal Masking (FCM), significantly improves both few-shot and finetuning performance of PaLM. We further consider a simple extension, T-FCM, which introduces bidirectional context to causal language model without altering the sequence order, and further improves finetuning performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/rcotton/Zotero/storage/GNMSM3NV/Liu et al. - 2023 - Towards Better Few-Shot and Finetuning Performance.pdf;/Users/rcotton/Zotero/storage/EC2D9HK9/2210.html}
}

@misc{loshchilov_decoupled_2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2019},
  month = jan,
  number = {arXiv:1711.05101},
  eprint = {1711.05101},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1711.05101},
  urldate = {2023-07-02},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/rcotton/Zotero/storage/G8CLB4ZB/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf;/Users/rcotton/Zotero/storage/VIIRTENK/1711.html}
}

@article{markus_role_2021,
  title = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care: {{A}} Comprehensive Survey of the Terminology, Design Choices, and Evaluation Strategies},
  shorttitle = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care},
  author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
  year = {2021},
  month = jan,
  journal = {Journal of Biomedical Informatics},
  volume = {113},
  pages = {103655},
  issn = {1532-0464},
  doi = {10.1016/j.jbi.2020.103655},
  urldate = {2022-07-30},
  abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
  langid = {english},
  keywords = {Explainable artificial intelligence,Explainable modelling,Interpretability,Post-hoc explanation,Trustworthy artificial intelligence},
  file = {/Users/rcotton/Zotero/storage/RMKK4P2V/Markus et al. - 2021 - The role of explainability in creating trustworthy.pdf;/Users/rcotton/Zotero/storage/TQIV5SMS/S1532046420302835.html}
}

@article{mcdonough_validity_2001,
  title = {The Validity and Reliability of the {{GAITRite}} System's Measurements: {{A}} Preliminary Evaluation},
  shorttitle = {The Validity and Reliability of the {{GAITRite}} System's Measurements},
  author = {McDonough, A. L. and Batavia, M. and Chen, F. C. and Kwon, S. and Ziai, J.},
  year = {2001},
  month = mar,
  journal = {Archives of Physical Medicine and Rehabilitation},
  volume = {82},
  number = {3},
  pages = {419--425},
  issn = {0003-9993},
  doi = {10.1053/apmr.2001.19778},
  abstract = {OBJECTIVE: To compare the concurrent validity and reliability of the GAITRite computerized gait analysis system with validated paper-and-pencil and video-based methods. DESIGN: Within-groups, repeated-measures design. SETTING: Research laboratory in a physical therapy education program. PARTICIPANT: One healthy woman, age 27 years. INTERVENTIONS: A subject walked across the walkway of the GAITRite system at various walking rates and degrees of step symmetry for 2 of the 3 analyses. Paper placed over the walkway enabled concurrent paper-and-pencil analysis. The subject was concurrently videotaped from the side. For the other analysis, a stride simulator with known step and stride lengths was applied to the walkway to simulate 2 steps and 1 stride. MAIN OUTCOME MEASURES: Cadence, walking speed, right and left step and stride lengths, and right and left step times. RESULTS: Excellent paper-and-pencil and GAITRite correlations (intraclass correlation coefficient [ICC] {$>$} 95) for spatial measures and excellent video-based and GAITRite correlations (ICC {$>$} 93) for temporal measures were found. GAITRite measures of step lengths and times were reliable in both walkway center and left-of-center measurements. CONCLUSIONS: Based on this data, GAITRite is a valid and reliable tool for measuring selected spatial and temporal parameters of gait.},
  langid = {english},
  pmid = {11245768},
  keywords = {Adult,{Diagnosis, Computer-Assisted},Disability Evaluation,Equipment Design,Female,Gait,Humans,Reproducibility of Results,Software,Time Factors,Walking}
}

@article{oord_representation_2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  month = jul,
  urldate = {2018-12-30},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  file = {/Users/rcotton/Zotero/storage/5Q8HGL4J/Oord, Li, Vinyals - 2018 - Representation Learning with Contrastive Predictive Coding.pdf}
}

@book{pearl_book_2018,
  title = {The {{Book}} of {{Why}}: {{The New Science}} of {{Cause}} and {{Effect}}},
  shorttitle = {The {{Book}} of {{Why}}},
  author = {Pearl, Judea and Mackenzie, Dana},
  year = {2018},
  month = may,
  publisher = {{Basic Books}},
  abstract = {A Turing Award-winning computer scientist and statistician shows how understanding causality has revolutionized science and will revolutionize artificial intelligence "Correlation is not causation." This mantra, chanted by scientists for more than a century, has led to a virtual prohibition on causal talk. Today, that taboo is dead. The causal revolution, instigated by Judea Pearl and his colleagues, has cut through a century of confusion and established causality -- the study of cause and effect -- on a firm scientific basis. His work explains how we can know easy things, like whether it was rain or a sprinkler that made a sidewalk wet; and how to answer hard questions, like whether a drug cured an illness. Pearl's work enables us to know not just whether one thing causes another: it lets us explore the world that is and the worlds that could have been. It shows us the essence of human thought and key to artificial intelligence. Anyone who wants to understand either needs The Book of Why.},
  googlebooks = {BzM0DwAAQBAJ},
  isbn = {978-0-465-09761-6},
  langid = {english},
  keywords = {Business \& Economics / Statistics,Computers / Artificial Intelligence / General,Computers / Computer Science,Mathematics / Probability \& Statistics / General,Science / Applied Sciences}
}

@misc{peng_rwkv_2023,
  title = {{{RWKV}}: {{Reinventing RNNs}} for the {{Transformer Era}}},
  shorttitle = {{{RWKV}}},
  author = {Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and GV, Kranthi Kiran and He, Xuzheng and Hou, Haowen and Kazienko, Przemyslaw and Kocon, Jan and Kong, Jiaming and Koptyra, Bartlomiej and Lau, Hayden and Mantri, Krishna Sri Ipsit and Mom, Ferdinand and Saito, Atsushi and Tang, Xiangru and Wang, Bolun and Wind, Johan S. and Wozniak, Stansilaw and Zhang, Ruichong and Zhang, Zhenyuan and Zhao, Qihang and Zhou, Peng and Zhu, Jian and Zhu, Rui-Jie},
  year = {2023},
  month = may,
  number = {arXiv:2305.13048},
  eprint = {2305.13048},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-05-23},
  abstract = {Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/rcotton/Zotero/storage/CZU23DGP/Peng et al. - 2023 - RWKV Reinventing RNNs for the Transformer Era.pdf;/Users/rcotton/Zotero/storage/KWYQTJIU/2305.html}
}

@inproceedings{radford_language_2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  urldate = {2023-07-02},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/Users/rcotton/Zotero/storage/MUCXCZJJ/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learner.pdf}
}

@misc{sarandi_learning_2022,
  title = {Learning {{3D Human Pose Estimation}} from {{Dozens}} of {{Datasets}} Using a {{Geometry-Aware Autoencoder}} to {{Bridge Between Skeleton Formats}}},
  author = {S{\'a}r{\'a}ndi, Istv{\'a}n and Hermans, Alexander and Leibe, Bastian},
  year = {2022},
  month = dec,
  number = {arXiv:2212.14474},
  eprint = {2212.14474},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.14474},
  urldate = {2023-01-02},
  abstract = {Deep learning-based 3D human pose estimation performs best when trained on large amounts of labeled data, making combined learning from many datasets an important research direction. One obstacle to this endeavor are the different skeleton formats provided by different datasets, i.e., they do not label the same set of anatomical landmarks. There is little prior research on how to best supervise one model with such discrepant labels. We show that simply using separate output heads for different skeletons results in inconsistent depth estimates and insufficient information sharing across skeletons. As a remedy, we propose a novel affine-combining autoencoder (ACAE) method to perform dimensionality reduction on the number of landmarks. The discovered latent 3D points capture the redundancy among skeletons, enabling enhanced information sharing when used for consistency regularization. Our approach scales to an extreme multi-dataset regime, where we use 28 3D human pose datasets to supervise one model, which outperforms prior work on a range of benchmarks, including the challenging 3D Poses in the Wild (3DPW) dataset. Our code and models are available for research purposes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.2.10,I.4.8,read},
  file = {/Users/rcotton/Zotero/storage/R8BUFFTW/Srndi et al. - 2022 - Learning 3D Human Pose Estimation from Dozens of D.pdf;/Users/rcotton/Zotero/storage/VGGTP6I5/2212.html}
}

@misc{su_roformer_2022,
  title = {{{RoFormer}}: {{Enhanced Transformer}} with {{Rotary Position Embedding}}},
  shorttitle = {{{RoFormer}}},
  author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  year = {2022},
  month = aug,
  number = {arXiv:2104.09864},
  eprint = {2104.09864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.09864},
  urldate = {2023-07-02},
  abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \textbackslash url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/rcotton/Zotero/storage/XDAHKVES/Su et al. - 2022 - RoFormer Enhanced Transformer with Rotary Positio.pdf;/Users/rcotton/Zotero/storage/XX7LFPVW/2104.html}
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  number = {Nips},
  issn = {0140-525X},
  doi = {10.1017/S0140525X16001837},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  isbn = {9781577357384},
  pmid = {1000303116},
  file = {/Users/rcotton/Zotero/storage/6Y5BMQP3/arXiv 2017 Vaswani - attention is all you need.pdf}
}

@misc{werling_rapid_2022,
  title = {Rapid Bilevel Optimization to Concurrently Solve Musculoskeletal Scaling, Marker Registration, and Inverse Kinematic Problems for Human Motion Reconstruction},
  author = {Werling, Keenon and Raitor, Michael and Stingel, Jon and Hicks, Jennifer L. and Collins, Steve and Delp, Scott L. and Liu, C. Karen},
  year = {2022},
  month = aug,
  pages = {2022.08.22.504896},
  institution = {{bioRxiv}},
  doi = {10.1101/2022.08.22.504896},
  urldate = {2022-09-29},
  abstract = {Creating large-scale public datasets of human motion biomechanics could unlock data-driven breakthroughs in our understanding of human motion, neuromuscular diseases, and assistive devices. However, the manual effort currently required to process motion capture data is costly and limits the collection and sharing of large-scale biomechanical datasets. We present a method to automate and standardize motion capture data processing: bilevel optimization that is able to scale the body segments of a musculoskeletal model, register the locations of optical markers placed on an experimental subject to the markers on a musculoskeletal model, and compute body segment kinematics given trajectories of experimental markers during a motion. The optimization requires less than five minutes of computation to process a subject's motion capture data, compared with about one day of manual work for a human expert. On a sample of 34 trials of experimental data, the root-mean-square marker reconstruction error (RMSE) was 1.38 cm, approximately 40\% lower than the 2.58 cm achieved manually by 3 experts. Optimization solutions reconstructed known joint angle trajectories from four diverse motion trials of synthetic data to an average of 0.79 degrees RMSE. We have published an open source cloud service at AddBiomechanics.org to process experimental motion capture data, which is available at no cost and asks that users agree to share processed and de-identified data with the community. Reducing the barriers to processing and sharing high-quality human motion biomechanics data will enable more people to engage in state-of-the-art biomechanical analysis in their work, do so at lower cost, and share larger and more accurate datasets. Author summary Creating large-scale public datasets of human motion could unlock data-driven breakthroughs in our understanding of neuromuscular diseases, assistive devices, and human motion more broadly. The manual effort currently required to process these motion datasets is costly and limits the collection and sharing of large-scale datasets. Our cloud-based software tool, called AddBiomechanics, uses state-of-the-art optimization techniques to automatically scale the body segments of a musculoskeletal model to match the subject of interest, and then compute body segment kinematics during a motion. The optimization requires less than five minutes of computation to process a subject's motion capture data, compared with about one day of manual work for a human expert. The accuracy of the approach in quantifying the body segment kinematics is as good or better than the results achieved manually by experts. Reducing the barriers to processing and sharing high-quality human motion biomechanics data will enable more people to engage in state-of-the-art biomechanical analysis, do so at lower cost, and share larger and more accurate datasets.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/rcotton/Zotero/storage/ALDG5HX3/Werling et al. - 2022 - Rapid bilevel optimization to concurrently solve m.pdf;/Users/rcotton/Zotero/storage/88J5GYIW/2022.08.22.504896v1.html}
}

@techreport{winner_discovering_2022,
  type = {Preprint},
  title = {Discovering Individual-Specific Gait Signatures from Data-Driven Models of Neuromechanical Dynamics},
  author = {Winner, Taniel S. and Rosenberg, Michael C. and Kesar, Trisha M. and Ting, Lena H. and Berman, Gordon J.},
  year = {2022},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/2022.12.22.521665},
  urldate = {2022-12-30},
  abstract = {Abstract           Locomotion results from the interactions of highly nonlinear neural and biomechanical dynamics. Accordingly, understanding gait dynamics across behavioral conditions and individuals based on detailed modeling of the underlying neuromechanical system has proven difficult. Here, we develop a data-driven and generative modeling approach that recapitulates the dynamical features of gait behaviors to enable more holistic and interpretable characterizations and comparisons of gait dynamics. Specifically, gait dynamics of multiple individuals are predicted by a dynamical model that defines a common, low-dimensional, latent space to compare group and individual differences. We find that highly individualized dynamics \textendash{} i.e., gait signatures \textendash{} for healthy older adults and stroke survivors during treadmill walking are conserved across gait speed. Gait signatures further reveal individual differences in gait dynamics, even in individuals with similar functional deficits. Moreover, components of gait signatures can be biomechanically interpreted and manipulated to reveal their relationships to observed spatiotemporal joint coordination patterns. Lastly, the gait dynamics model can predict the time evolution of joint coordination based on an initial static posture. Our gait signatures framework thus provides a generalizable, holistic method for characterizing and predicting cyclic, dynamical motor behavior that may generalize across species, pathologies, and gait perturbations.},
  langid = {english},
  keywords = {code,read},
  file = {/Users/rcotton/Zotero/storage/RI5S8BEM/Winner et al. - 2022 - Discovering individual-specific gait signatures fr.pdf}
}

@article{winters_generalizability_2015,
  title = {Generalizability of the {{Proportional Recovery Model}} for the {{Upper Extremity After}} an {{Ischemic Stroke}}},
  author = {Winters, Caroline and {van Wegen}, Erwin E. H. and Daffertshofer, Andreas and Kwakkel, Gert},
  year = {2015},
  month = aug,
  journal = {Neurorehabilitation and Neural Repair},
  volume = {29},
  number = {7},
  pages = {614--622},
  issn = {1552-6844},
  doi = {10.1177/1545968314562115},
  abstract = {BACKGROUND AND OBJECTIVE: Spontaneous neurological recovery after stroke is a poorly understood process. The aim of the present article was to test the proportional recovery model for the upper extremity poststroke and to identify clinical characteristics of patients who do not fit this model. METHODS: A change in the Fugl-Meyer Assessment Upper Extremity score (FMA-UE) measured within 72 hours and at 6 months poststroke served to define motor recovery. Recovery on FMA-UE was predicted using the proportional recovery model: {$\Delta$}FMA-UEpredicted = 0.7{$\cdot$}(66 - FMA-UEinitial) + 0.4. Hierarchical cluster analysis on 211 patients was used to separate nonfitters (outliers) from fitters, and differences between these groups were studied using clinical determinants measured within 72 hours poststroke. Subsequent logistic regression analysis served to predict patients who may not fit the model. RESULTS: The majority of patients (\textasciitilde 70\%; n = 146) showed a fixed proportional upper extremity motor recovery of about 78\%; 65 patients had substantially less improvement than predicted. These nonfitters had more severe neurological impairments within 72 hours poststroke (P values {$<$}.01). Logistic regression analysis revealed that absence of finger extension, presence of facial palsy, more severe lower extremity paresis, and more severe type of stroke as defined by the Bamford classification were significant predictors of not fitting the proportional recovery model. CONCLUSIONS: These results confirm in an independent sample that stroke patients with mild to moderate initial impairments show an almost fixed proportional upper extremity motor recovery. Patients who will most likely not achieve the predicted amount of recovery were identified using clinical determinants measured within 72 hours poststroke.},
  langid = {english},
  pmid = {25505223},
  keywords = {Brain Ischemia,Cohort Studies,Disability Evaluation,Female,Humans,Male,Motor Activity,prognosis,Recovery of Function,stroke,Stroke,Time Factors,upper extremity,Upper Extremity}
}

@misc{zhang_motiongpt_2023,
  title = {{{MotionGPT}}: {{Finetuned LLMs}} Are {{General-Purpose Motion Generators}}},
  shorttitle = {{{MotionGPT}}},
  author = {Zhang, Yaqi and Huang, Di and Liu, Bin and Tang, Shixiang and Lu, Yan and Chen, Lu and Bai, Lei and Chu, Qi and Yu, Nenghai and Ouyang, Wanli},
  year = {2023},
  month = jun,
  number = {arXiv:2306.10900},
  eprint = {2306.10900},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.10900},
  urldate = {2023-06-21},
  abstract = {Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4\% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Codes shall be released upon acceptance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/rcotton/Zotero/storage/559RZIZT/Zhang et al. - 2023 - MotionGPT Finetuned LLMs are General-Purpose Moti.pdf;/Users/rcotton/Zotero/storage/94BSAIU4/2306.html}
}

@inproceedings{mahmood_amass_2019,
  title = {{{AMASS}}: {{Archive}} of {{Motion Capture As Surface Shapes}}},
  shorttitle = {{{AMASS}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Mahmood, Naureen and Ghorbani, Nima and Troje, Nikolaus F. and Pons-Moll, Gerard and Black, Michael},
  date = {2019-10},
  pages = {5441--5450},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2019.00554},
  abstract = {Large datasets are the cornerstone of recent advances in computer vision using deep learning. In contrast, existing human motion capture (mocap) datasets are small and the motions limited, hampering progress on learning models of human motion. While there are many different datasets available, they each use a different parameterization of the body, making it difficult to integrate them into a single meta dataset. To address this, we introduce AMASS, a large and varied database of human motion that unifies 15 different optical marker-based mocap datasets by representing them within a common framework and parameterization. We achieve this using a new method, MoSh++, that converts mocap data into realistic 3D human meshes represented by a rigged body model. Here we use SMPL [Loper et al., 2015], which is widely used and provides a standard skeletal representation as well as a fully rigged surface mesh. The method works for arbitrary marker sets, while recovering soft-tissue dynamics and realistic hand motion. We evaluate MoSh++ and tune its hyperparameters using a new dataset of 4D body scans that are jointly recorded with marker-based mocap. The consistent representation of AMASS makes it readily useful for animation, visualization, and generating training data for deep learning. Our dataset is significantly richer than previous human motion collections, having more than 40 hours of motion data, spanning over 300 subjects, more than 11000 motions, and will be publicly available to the research community.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {AMASS,Archive of Motion Capture as Surface Shapes,Computational modeling,computer vision,Computer vision,datasets,deep learning,Joints,Machine learning,Max Planck Institut,MPG,MPI,Shape,Three-dimensional displays},
  file = {/Users/rcotton/Zotero/storage/9YD8N76Q/full-text.pdf;/Users/rcotton/Zotero/storage/JGT3BSKY/Mahmood et al. - 2019 - AMASS Archive of Motion Capture As Surface Shapes.pdf;/Users/rcotton/Zotero/storage/RMSUDQIP/9009460.html}
}
