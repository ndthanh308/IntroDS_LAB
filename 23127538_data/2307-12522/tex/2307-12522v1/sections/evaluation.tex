\section{Accuracy Evaluation}
\label{sec:autoEvaluation}

%\todo{Why is this a 'performance' evaluation - is that the right word to describe?}
%\fix{fixed}

\fix{
We carry out experiments to evaluate the accuracy of the GUI grouping and the converted TV GUI effects in our pipeline.
}

\subsection{Accuracy of GUI Grouping}
\label{sec:groupEva}
The accuracy of GUI component grouping serves as the foundation for succeeding techniques.
Hence, we first evaluate the accuracy of our proposed grouping algorithm.


\subsubsection{Procedure}
To confirm the generalizability of our grouping algorithm, we randomly select another 10 apps that are not in our dataset.
To ensure the quality of selected apps, we only select apps with at least one million Google Play installations.
A total of 100 GUI pages are selected as the test dataset, with 10 GUI pages from each app being randomly selected.
Next, we perform the grouping algorithm on the selected GUI pages and collect grouping results. 
The grouping results are then manually checked to ensure their reasonableness. 

\subsubsection{Metric}
A fair result for a GUI group must contain only UI elements with strong logical connections.
When manually verifying the rationality of grouping, a group fails if it contains UI components that are logically unrelated to other UI components inside the group.
\refix{When we refer to `logically related UI components', we mean an intuitive association of GUI elements based on interactivity and role. Interactivity pertains to the direct communicative relationship between components, such as an input field and a 'submit' button. The 'role' encapsulates the components' collective function within the broader UI context. Hence, `logically related UI groups' signify clusters of UI elements intuitively assembled based on their interaction synergy and shared role in the user interface.}

We use the exact match rate to illustrate the percentage of reasonable GUI groups.
The exact match is a binary metric, with a value of 1 if correct and 0 otherwise.
If there are $m$ GUI groups on this GUI page, and $n$ of them are reasonable, then the grouping accuracy of this GUI page is $n/m$.
Figure~\ref{fig:exampleGroup} demonstrates an example of correct and wrong GUI grouping on one GUI page.
Groups 1, 3, and 4 in Figure~\ref{fig:exampleGroup} are considered to be correctly grouped since all UI components within the group are logically related.
But group 2 is considered to be grouped incorrectly. 
This is because, in addition to the search-related UI components in group 2 (the UI components boxed by the blue dashed line in Figure~\ref{fig:exampleGroup}), there are two logically unrelated icons, which should not be classified into this search-related UI group.
The grouping accuracy of Figure~\ref{fig:exampleGroup} is 0.75.
\refix{Considering the subjective nature of determining 'logically related UI components', we employ three individuals with a minimum of one year's experience in GUI development to independently conduct the exact match evaluation. Thereafter, the Fleiss Kappa value~\cite{fleiss1971measuring} is used to measure the level of agreement among these three evaluators. Fleiss Kappa values are interpreted as follows: [0.01, 0.20] signifies slight agreement, (0.20, 0.40] indicates fair agreement, (0.40, 0.60] represents a moderate agreement, (0.60, 0.80] suggests substantial agreement, and (0.8, 1] signifies near perfect agreement. Instances where the Fleiss Kappa value falls below 0.8 prompts a discussion, analysis, and re-evaluation by the three evaluators until a Fleiss Kappa value exceeding 0.8 is achieved.}
To evaluate our method's ability to minimize the isolated UI components, we follow related works~\cite{zhang2021screen, chen2019code} to select the proportion of reduced UI components as the second metric.
Suppose there are $J$ UI components on the original GUI pages, and $K$ UI components and UI groups are left after grouping, then the ratio of the reduced components is $(j-k)/j$.


\subsubsection{Baseline}

Xiaoyi et al.~\cite{zhang2021screen} propose a similar approach to group UI components for efficient navigation.
They develop multiple heuristics that group UI components based on their UI types, sizes, and spatial relationships on the rendered phone screen.
Considering similar application scenarios, we choose their method as the baseline for our experiments.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[]
\setlength{\abovecaptionskip}{0pt}
\setlength{\belowcaptionskip}{0pt}
  \caption{Evaluation Results of Accuracy of GUI Grouping}
  \label{tab:groupAcc}
\begin{tabular}{|l|l|l|l|ll|ll|}
\hline
\multirow{2}{*}{ID} &
  \multirow{2}{*}{App} &
  \multirow{2}{*}{Category} &
  \multirow{2}{*}{\#Installation (Million)} &
  \multicolumn{2}{c|}{ExactMatch} &
  \multicolumn{2}{c|}{ReducedUI} \\ 
 &             &                   &     & Ours & BL~\cite{zhang2021screen} & Ours & BL~\cite{zhang2021screen} \\ 

\midrule

1                    & iQIYI Video & Entertainment     & 5   & 0.86 & 0.71   & 0.62 & 0.49   \\
2                    & Coursera    & Educational       & 10  & 0.72 & 0.61   & 0.55 & 0.37   \\
3                    & Evernote    & Productivity      & 100 & 0.80 & 0.77   & 0.61 & 0.43   \\
4                    & Kodi        & Tool              & 50  & 0.85 & 0.81   & 0.62 & 0.52   \\
5                    & Pinterest   & Lifestyle         & 500 & 0.74 & 0.62   & 0.56 & 0.40   \\
6                    & Wonder      & Art \& Design     & 5   & 0.79 & 0.72   & 0.53 & 0.47   \\
7                    & Fiverr      & Bussiness         & 10  & 0.83 & 0.69   & 0.55 & 0.51   \\
8                    & ABC listen  & Music \& Audio    & 1   & 0.91 & 0.81   & 0.64 & 0.56   \\
9                    & Fitbit      & Health \& Fitness & 50  & 0.84 & 0.78   & 0.56 & 0.59   \\
10                   & Kik         & Communication     & 100 & 0.76 & 0.73   & 0.52 & 0.45   \\ 

\midrule

\multicolumn{4}{|c|}{Average}                                  & \textbf{0.81} & 0.73   & \textbf{0.58} & 0.48  \\

\bottomrule

\end{tabular}
\end{table}


\subsubsection{Results}
Table~\ref{tab:groupAcc} summarizes the information of the selected 10 apps and the accuracy results of the GUI grouping.
The column \emph{\#Installation} demonstrates the number of app installations. 
\refix{The Fleiss Kappa value of the first grouping results for three evaluators of our 10 apps are all between 0.91 and 1. Three evaluators discuss the different parts and finally agree on a unified final result.} 
The subcolumn \emph{Ours} and \emph{BL} show our and the baseline' experimental results in the exact match rate and reduced UI components rates, respectively. 
Our approach achieves 0.81 in average exact match, which is 10.96\% higher than the baseline \emph{BL}~\cite{zhang2021screen} (0.73).
Our approach reduces isolated GUI components in GUI pages by an average of 58\%, which is 20.83\% higher than the baseline (48\%).
Both results demonstrate the effectiveness of our GUI grouping algorithms.

Figure~\ref{fig:exampleGroup} depicts the results of a pair of our method and baseline's method after grouping, with the first subplot representing our method and the second representing the baseline's.
Our approach splits the GUI page into 5 GUI groups and other GUI components. 
Correspondingly, the baseline separates the page into 8 distinct groupings and components.
Clearly, our grouping results are more precise and overlook fewer individual components. 
For example, in our group 2 and the corresponding groups 2 and 3 of the baseline, our method considers the possible related information surrounding the text, successfully groups the related images on the right side together, and merges the adjacent groups with the same structure, whereas the grouping result of the baseline method omits the related image data. 
In more complex scenarios, such as group 3 and group 4 in subfigure (a), corresponding to groups 4, 5, 6, and 7 in subfigure (b), the results of groups 4 and 5 in subfigure (b) omit the majority of the information since baseline's approach cannot handle the case of numerous lines of text. 
Our approach is built with more general atomic, row, and multi-row grouping methods, so that the group results contain as much important data as feasible, and reduces the number of groups on a GUI page to expedite the subsequent conversion operation.


% Figure environment removed
\setlength{\textfloatsep}{10pt}


\subsection{Accuracy of GUI Conversion}
Once the TV GUI DSLs have been generated, we convert them into source code and then run them to get the rendered run-time GUI pages.
On the one hand, the impact of the same run-time GUI can be accomplished through a variety of code. 
On the other hand, there is a gap between the GUI source code and the rendered GUI effect, and the GUI source code does not reflect the rendered GUI effect in its entirety~\cite{rountev2014static}. 
To demonstrate the efficacy of the approach, we choose to evaluate the rendered effect of apps running through the translated DSLs.
We perform both an automatic evaluation and a user study to evaluate the performance of the whole automated GUI conversion approach.
% Note that we do not have any automated evaluation as there exist very few TV-phone pairs with strict alignment as mentioned in Section~\ref{sec:empiricalStudy}.
All 589 pairs in Section~\ref{sec:rq2} are used in the automated evaluation to objectively evaluate the overall effect of the method.
The quality of visual transformation is strongly dependent on human perception, so we also include a user study to assess our approach's performance.
Due to the great efforts of the user study, we randomly sample 42 (7\%) GUI pairs among 589 pairs from 8 apps in 6 categories.
%There is some defect with this experiment, as since we have the ground truth

\subsubsection{Evaluation Metrics}

%\todo{This metric is a tad unclear if not familiar with it - any chance using a simple TV GUI example and showing how its calculated??}
%\fix{fixed}

According to recent studies~\cite{chen2018ui, moran2018automated}, when using phones and TVs, users largely rely on the layout of images and texts to understand GUIs. \refix{Uniform GUIs facilitate user adaptability from mobile to TV app. Additionally, a consistent GUI promotes a reduction in developmental cost and time by enabling the reuse of code and design elements~\cite{mendoza2013mobile, lowdermilk2013user}.
Therefore, we substantiate the accuracy of GUI conversion by quantifying the similarity between TV GUIs that are automatically generated and those manually designed, serving as the ground truth in this study.}
The mIOU~\cite{jaccard1912distribution}, which could effectively evaluate the layout gap between each type of the component in one GUI page, has been  widely used in GUI evaluation~\cite{kumar2011bricolage,laine2021responsive,zheng2019content,chen2018ui,xie2020uied,zhao2021guigan,moran2018detecting}.
\refix{Based on its characteristics and suitability for our specific study, we select mIoU as the metric to evaluate the layout similarity of generated TV GUIs and ground truths.}
The TV versions in phone-TV GUI pairs have been redesigned and optimized with a more logical layout of text and images in the GUI to accommodate the features of the TV.
So we select these redesigned TV GUIs as the ground truth of the corresponding phone GUI in the automated evaluation.

The mIoU (Mean Intersection Over Union), also known as the Jaccard Index, is a prominent image segmentation assessment measure that computes the IOU for each class before calculating the average over classes.
The IoU is calculated by dividing the overlap area between predicted class positions and ground truth by the area of union between predicted position and ground truth.
This is computed by:
\begin{equation}
    mIoU = \frac{1}{k}\sum_{i=0}^k\frac{TP(i)}{TP(i) + FP(i) + FN(i)}
\end{equation}
where $k$ means $k$ classes in both images, $TP(i)$, $FP(i)$ and $FN(i)$ represent the distribution of true positive, false positive, and false negative of $i_{th}$ class between two compared images. 

Our empirical study in section~\ref{sec:rq2} shows that the current GUIs of paired phones and TVs often do not have strict correspondence.
Besides, whether a GUI design makes sense depends significantly on the user's subjective perception. 
A GUI with a low mIOU in the automatic evaluation may be deemed acceptable by some users. 
For example, some GUI may have a more reasonable GUI design than the ground truth.
These reasonable GUI designs got low scores because their layouts are not in line with the ground truth.
To eliminate the bias, we adopted two metrics~\cite{zhao2021guigan}, structure rationality and overall satisfaction for participants in the user study to rate the quality of the generated TV GUI by considering the characteristics of the TV apps. 
These metrics were inspired by the web GUI evaluation~\cite{kumar2011bricolage, laine2021responsive, zheng2019content} and image evaluation~\cite{hore2010image, ponomarenko2008color}.
First, structure rationality is used to evaluate component layout rationality, which refers to the placement of components in the GUI as well as the reasoning behind their combination and sorting.
Second, overall satisfaction is to evaluate the overall design's pleasing qualities. 
For each metric, the participants will give a score ranging from 1 to 5, with 1 being the least satisfactoriness and 5 representing the highest satisfactoriness.


\subsubsection{Baselines}
Desktop mode~\cite{huaweiDesk, Dex} is widely used in various smartphones, such as Samsung, OnePlus, Huawei, and Oppo.
It allows users to connect an external display to an Android smartphone or tablet to make content easier to view, just like on a TV or computer.
The desktop mode is optimized for larger displays with resizable windows and a different layout for GUIs.
HDMI adapter or WiFi is required to use the desktop mode.
In this experiment, considering the current use range and maturity of computer mode, we first choose Huawei EMUI desktop mode as the baseline, which is one of the earliest phone models to support desktop mode.

Currently, Google provides the big-screen responsive layout component for Android system~\cite{AndroidTV, AndroidTVApp, AndroidTVDe, adaptiveUI, googleTV}. 
When an Android app runs on different-sized screens, adaptive and responsive Android GUI components will adjust their positions and sizes to fit the screen size of each device.
In our empirical study, we found that part of the apps using these technologies automatically adapts to different screens.
Even though these technologies often offer a worse user experience than a fully hand-optimized GUI, it is nevertheless worthwhile to compare them to semi-automatic methods.
Thus,  our second baseline is the result of directly mapping adaptive Android GUI from phone to TV.
Unlike the desktop mode, no external equipment is required for direct mapping.

In the user study, we also compare our method to the redesigned TV GUI, which serves as the ground truth for the automated evaluation.
The comparison with the ground truth will provide a clear image of the efficacy gap between our method and the redesigned GUI, which requires extra work.


% \todo{are these 'fair' baselines?  Need to say why chosen... What about compare generated TV app to one manually done i.e. select some of the apps from section 2.1. and take their mobile app GUI and generate TV one and compare it to the developer-implemented one???  Expect automated technique to not be as good as manually developed special purpose app, but be interested to see how it compares.  Does this make sense???}\fix{Yes, fixed}




% We just directly run these apps on smart TV devices.

% By default, if the app is not specifically designed to support TV display, is will be directed mapped to TV~\cite{NativeAndroid}, \chen{Why using adaptive GUI components? You are not working on source code generation, but runtime GUI mapping? I cannot understand what you mean here!} which always uses adaptive GUI components in developing the GUI, such as \textit{ConstraintLayout}, \textit{RelativeLayout} and \textit{SlidingPaneLayout}.
% These adaptive GUI components automatically adjust their position and size according to the device screen sizes.


\subsubsection{Procedures}
In the automatic evaluation, we select redesigned TV GUIs in 589 pairs as our ground truths.
We use our proposed approach to convert every phone GUI to DSLs of TV GUI.
Then we use the client app on TV to generate code and render GUIs on the Android TV emulators~\cite{androidStudio}.
The emulator is configured with 4 CPU processors, 4 GiB of RAM, and 1 GiB of SD card. 
The API level version of the system image is 26.
When GUIs are rendered, we get the metadata of GUIs to generate their corresponding wireframes.
The content of pictures and text in the GUI is not taken into consideration since we are comparing the structure of the rendered GUI, not the pixels of the produced GUI, therefore we convert images to red blocks and texts to green blocks in wireframes.
In the same way, we generate wireframes of baselines and our approach's generated TV GUI.
Finally, we evaluate the mIoU between wireframes of our generated TV GUIs and ground truths.

% Also because of the poor performance and no complete transformation technique of current, we only take Desktop and Native Android as the baselines.
% We recruit 20 developers/designers with at least 3-year Android usage experience. 
%we try to diverse participants' background with gender balance (?? male vs ?? female).
% Within this cohort, ?? of them are professional GUI designers with several-year (??-?? years) Android GUI design experience.  
In the user study, we recruit 20 participants who are professional designers and developers with more than 3-year Android development experiment for this user study.
We first introduce them with a detailed explanation of tasks and the two GUI evaluation metrics structure rationality and overall satisfaction. 
Meanwhile, we provide participants with all generated GUI designs from different methods and ask them to give the score of each GUI design in two metrics of the user study. 
Note that they do not know which TV GUI is from which method and all of them will evaluate the TV GUI design individually without any discussion.
For each test case, participants are asked to choose one GUI they think works best.
% After the experiment, we tell the participants which GUI are generated by our model and ask them to leave some general comments about our approach.

\subsubsection{Results}

%\todo{Ad noted above - it would be good to compare the generated TV GUI to custom TV app i.e. designed specifially for TV to see if (i) how score compares and (ii) how the developers rate automatic vs hand designed for same app. Is this possible?
%\huComment{manually-designed TV GUIs are used as the ground truth. An app has a lot of pages, and it takes a lot of time to redesign each one. Therefore, the current app only redesigned the main page, and abandoned the optimization of the rest pages. Our tool targets the remaining pages that are strategically abandoned, so there is no comparison with the redesigned main page. Manually redesigned pages are definitely better than automatically and semi-automatically generated pages, but is is time-consuming.}
%I feel the current evaluation sets up a straw-person for failure vs compare to manually designed and build TV apps. Make sense???
%}

Table~\ref{tab:whole} demonstrates average results on the testing set of our approach and three baselines.
The ground truth GUI, which requires extra engineering effort to redesign one-to-one, is undoubtedly more favorable than the other three automatically generated approaches.
In 19 cases out of 42, \emph{Ground Truth} is deemed the most efficient, followed by our method (15), \emph{Desktop mode (7)}, and Direct mapping (1).
It reaches the highest structure rationality (4.27) and overall satisfaction (4.20).
However, this approach requires customization for each GUI page, adding significantly to the engineering expenses and making it non-generalizable. 
Compared to ground truth's approach, ours performs marginally worse (0.17 lower on \emph{Structure Rationality} and 1.2 lower on Overall Satisfaction). 
Our approach produces the best results across 15 cases, which is quite similar to the ground truth (19), and we do not require the additional engineering costs associated with tailoring each GUI page.
Our approach outperforms the other two baselines in mIoU, overall satisfaction, and structure rationality by significant margins in both metrics.
Compared with the baseline \emph{desktop mode}, our approach achieves 21.05\%, 10.42\%, and 21.31\% improvement in mIoU, overall satisfaction, and structure rationality.
According to the experimental findings, our method outperforms the other two automated methods (\emph{Direct mapping} and \emph{Desktop mode}). 
Our approach can also produce comparable outcomes when compared to the ground truth without incurring additional expenses.


\begin{table}
  \caption{Average score of evaluation. * denote the statistical significance $p-value < 0.01$ with other two metrics.}
  \label{tab:whole}
  \scalebox{0.9}{
\begin{tabular}{|c|c|c|c|c|}
    \toprule
Approach &
   mIoU &
  \begin{tabular}[c]{@{}l@{}}Structure \\ Rationality\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}Overall \\ Satisfaction\end{tabular} & \begin{tabular}[c]{@{}l@{}}Vote \\ Best\end{tabular}\\
\midrule
Direct mapping & 0.07 &  2.44          & 2.67         & 1\\
Desktop mode   & 0.19 & 3.38          & 3.55           & 7\\
Ground truth & 1 & \textbf{4.27}          & \textbf{4.20}           & \textbf{19} \\ 
Ours           & \textbf{0.23} & 4.10* & 3.92*   & 15\\
\bottomrule
\end{tabular}
}
\end{table}

To further analyze our method and the other two automatic methods (\emph{Direct mapping} and \emph{Desktop mode}), we plot the boxplots of the scores of these three methods over \emph{Structure Rationality} and \emph{Overall Satisfaction} in user study in Figure~\ref{fig:boxPlot}.
In both box plots, the gap between our first and third quartiles was lower than in \emph{Desktop mode}, indicating that our ratings are more concentrated and stable.
% In the color harmony box plot, the first quartile and median of Desk mode is almost the same (3.45 and 3.5), which are both smaller than our approach's median (3.61).
The maximums of \emph{Desktop mode} in both box plots are higher than ours.
% While there are a few cases with extremely high scores, resulting in a higher average score in color harmony than our method.
There are a few cases that have been individually optimized for \emph{Desktop mode} display, so the scores are particularly high, which increases the maximum score of the whole. 
However, if the page is not individually optimized, the effect of \emph{Desktop mode} is significantly lower than that of our method.
The scores of \emph{Direct mapping} are all significantly lower than the other two, indicating that users generally do not accept this conversion.
% Therefore, Desktop Mode relies more on the number of individually optimized pages, which requires a lot of work.

To understand the significance of the differences between the user study results of baselines and our approach, we carry out the Mann-Whitney U test~\cite{fay2010wilcoxon} on the overall satisfaction and structure rationality with \emph{Direct mapping} and \emph{Desktop mode}, respectively.
Since there are two baselines, we carry out the test between our approach with each baseline separately.
The test results in Table~\ref{tab:whole} show that our tool can significantly create better GUI conversion in overall satisfaction and structural rationality from phone to TV than \emph{Desktop mode} (both $p-value < 0.01$) and \emph{Direct mapping} (both $p-value < 0.01$).


% Figure environment removed


To better illustrate our results, Figure~\ref{fig:examples} lists examples of generated TV GUI by different approaches.
For the case above in Figure~\ref{fig:examples}, the average structure rationality for \emph{Direct mapping}, our tool and \emph{Desktop mode} are 1.8, 4.4, and 3.2.
The average overall satisfaction for \emph{Direct mapping}, our tool and \emph{Desktop mode} are 1.7, 4.0, and 3.6.
For the case below in Figure~\ref{fig:examples}, the average structure rationality for \emph{Direct mapping}, our tool and \emph{Desktop mode} are 1.4, 4.6, and 1.5.
The average overall satisfaction for \emph{Direct mapping}, our tool and \emph{Desktop mode} are 2.0, 4.3, and 1.9.
Generally, for \emph{Desktop model}, only a few specific apps and pages are manually optimized, so when testing with various types of pages, uncustomized pages are original phone pages without optimization, resulting in low scores.
Faced with large TV screens, most cases are very blunt without considering the design criteria of the current platform's widgets and users' usage habits.
\emph{Direct mapping} places all components as where they are on the phone, resulting in a very poor user experience for TV users.
Besides, it can only convert part of GUI components, which has poor universality.
This it is generally not accepted by users.

Our approach sometimes may also generate inappropriate TV mapping, especially for those non-standard GUI inputs.
For the phone GUI in Figure~\ref{fig:error}, promotional images of films in the second row do not show completely due to the size of the mobile screen.
In this case, UI Automator incorrectly provides us with the name of these films with details in \textit{Bottom Tab Layout}.
Thus, our approach fails to get the correct name of films due to the limitation of UI Automator, just like the example shown in red boxes.
As our approach gains confusing hierarchies from UI Automator, it does not correctly identify the \textit{Bottom Tab Layout}.
Therefore, the approach fails \textit{Bottom Tab Layout} to convert it into a TV \textit{Channel}.


% Figure environment removed

% Figure environment removed

\begin{comment}
\chen{These comments are not that good, and please find some positive comments. The negative comments should be suggestions about the future improvement.}

Beyond the scores, many experienced testers give comments about our tool and some cases.
We select some comment:
(1)\textit{This tool is a great help for unpopular apps.}
Most popular apps have carried adaption measures for their phone GUI, so our tool have definitely promotion for unpopular apps.
(2)\textit{Approach B is better than approach C in structure, and Approach A does not work at all.}
In the blind test, method B is our method, C is Desk mode, and A is android native adaptive.
(3) \textit{'Being able to render 60 frames is the most important, not GUI.'}.
The tester thinks lag is the most deadly for the user experience. 
We should consider GUI design in terms of techniques to ensure algorithmic rendering speed.
This comment is a reminder that we should focus on algorithmic efficiency in the future.
\end{comment}

\section{Usefulness Evaluation}
According to the findings of our empirical study in Section~\ref{sec:supportTV}, more developers prefer to redevelop the GUI pages of new TV apps. 
Even while experienced developers can produce the matching TV GUI pages quickly, it still affects the development efficiency and wastes some valuable resources in the phone GUI.
We carry out a user study to evaluate the usefulness of our generated TV GUI for bootstrapping corresponding TV GUI design and implementation by real-world developers.

\subsection{Procedures}
We recruit 6 participants who are all working in software companies and have at least one-year Android GUI developing experience.
Participants are required to design and implement the corresponding conversion TV GUI by referring to the given Phone GUI.
We provide 6 phone GUIs which have covered main 12 phone GUI groups.
%\todo{So - why not compare these 'official' ones in previous section to the generates ones too? See my notes above...}
%\huComment{official ones are used as the ground truth}
The official corresponding designed TV GUIs of 6 phone GUIs are also collected for the subsequent satisfaction evaluation. 
% In 6 testing phone GUIs, we classify two relatively simple GUIs, two moderately complex ones, and two highly complex one.
Participants are required to design and implement the layout skeleton and set component properties, including the view type, size, order, and padding.
Note that participants are allowed to replace some component types with placeholders without affecting the rendering and overall design.

%\todo{Need to explain training for both groups especially the one using the DSL and compiler??}\fix{fixed}
The study consists of two groups of three participants: the experimental group, consisting of three participants, is asked to proceed on the basis of our tool, and the control group, which starts work from scratch on a TV app design.
The experiment group is allowed to use our tool to automatically generate a draft TV GUI and update the generated source code to re-design TV GUI directly.
Each member in the experimental group learns in advance how to use our tool to generate source code and render it on TV. 
Participants in both groups have comparable development
experience by pre-study their developing background to ensure both groups have similar expertise in total.
% The experiment does not require participants to complete all the experiments of the 6 GUIs at once, and there is no requirement for the order of the 6 GUI.
All participants are only allowed to use Android Studio and Java to avoid bias and have up to 20 minutes for each implementation.
Three academic Ph.D. students who are not involved in the study are asked to evaluate 6 participants' results and rate their satisfaction on a five-point scale, with 5 being the highest and 1 the lowest.
The satisfaction metric\cite{kumar2011bricolage, laine2021responsive, zheng2019content} is to evaluate the overall pleasing qualities of this GUI page.
The evaluators must determine if the layout, content, and UI type of all UI components on this GUI page are appropriate.
When rating, raters do not know which TV GUI is developed by which group and the manual-designed TV GUI by companies are used for the reference.
Similar to the accuracy evaluation, we use the Fleiss Kappa value~\cite{fleiss1971measuring} to measure the agreement among the three raters.
Fleiss Kappa values in the range of [0.01, 0.20], (0.20, 0.40], (0.40, 0.60], (0.60, 0.80], and (0.8, 1] correspond to the slight, fair, moderate, substantial, and almost perfect agreement, respectively.
If the Fleiss Kappa value is less than 0.8, the divergent cases will be discussed, analyzed, and re-scored by the three raters until the Fleiss Kappa value is greater than 0.8.
We provide 6 participants with the same development environment and resources and record the time it takes participants to complete each TV GUI.

% After the experiment, the experiment group is asked to give feedback about how satisfied they are with our tool for each GUI in five-point scale.

% Figure environment removed


\subsection{Results}
The box plot in Figure~\ref{fig:boxPlot} and the average score in Table~\ref{tab:aveUsefulness} show that the experimental group implements TV GUI conversion faster (average of 11.56 minutes) with a higher satisfaction score (average of 3.35) than the control group (average of 16.28 minutes and 1.66 satisfaction score).
Members of the experimental group use the DSL to design the GUI, but members of the control group write actual code to design the GUI.
Therefore, their working implementations are different, and the outcomes of the control group members are closer to the actual app implementation.
Theoretically, different working implementations could lead to bias in the results, but all invited professionals have at least one year of Android development experience. 
They master the Android GUI development procedure. 
Some developers use placeholders to replace some UI components, allowing them to focus on developing the GUI rather than completing the code logic and syntax. 
While the working implementations of the two groups may not be equal, the efforts of redesigning and constructing an adequate TV GUI are the decisive factor in determining the outcomes of the two groups experiments.
Our tool successfully and effectively assists developers in developing more suitable TV GUIs faster, taking into account the apparent time difference between 11.56 minutes and 15.6 minutes on average.
In experiments, two participants in the control group fail to finish at least one GUI conversion within 20 minutes.
However, everyone in the experimental group completed the conversion within 20 minutes and had time to personalize the GUIs. This results in significantly higher satisfaction scores than the control group (an average score increase of 1.64).



%\todo{It would be good to discuss the results in more detail e.g.

%-what aspects easy/hard to use e.g. DSL/compiler?  specific TV GU elements?

%-what feedback on the tool the respondents give esp limitations, future work ideas

%}
%\fix{fixed}

Figure~\ref{fig:control} shows an example of two TV GUI conversions from the experimental and control group.
Note that we only evaluate the structure and overall soundness of the GUI, so we do not evaluate the content in the participant's GUI, allowing placeholders.
When designing GUI, members in the experimental group can refer to the output layout of our tool, and can customize the TV GUI on this basis.
Our tool is an excellent inspiration for the experimental group, saving them time in designing and developing the TV GUI and giving them a bottom line for their work.
We also carry the Mann-Whitney U test on the cost time and satisfactory scores. 
The test results suggest that our tool can significantly help the experimental group convert phone GUI faster($p-value < 0.01$) and create better TV GUI ($p-value < 0.01$).

A participant in the experimental group commented on our tool that ``\textit{The automatic conversion results provided by the tool can give me good development design guidance and hints, greatly improving my development efficiency}''.
Overall, the user study results provide preliminary evidence of the usefulness of our tool.
The majority of participants think that the GUI DSL we built is simple to comprehend and use.
They said that our GUI DSL is a useful solution to address the present issue of GUI code reuse caused by the version differences between mobile phone systems and television systems.
Our GUI DSL also makes it simple for designers without any programming background to collaborate on projects.
Participants also pointed out some flaws in our existing approach. 
It is not appropriate for converting real-time demanding apps like games since the generated GUI needs to be re-rendered by the client app on the TV.
Because the image on the mobile GUI isn't designed with a large-screen TV, sometimes the image will seem stretched on the TV GUI after conversion, which can negatively impact the user experience.
To address these issues, we will design more efficient GUI rendering technology solutions and design the display of image resources after screen size conversion in future work.

% Figure environment removed
