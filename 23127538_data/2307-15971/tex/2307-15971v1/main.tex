\UseRawInputEncoding
%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word{}
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[sigconf,review,anonymous]{acmart}
\documentclass[sigconf]{acmart}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algpseudocode}
\newcommand{\ytd}[1]{{\color{red}[#1 --ytd]}}
\newcommand{\cc}[1]{{\color{brown}[#1 --cc]}}
\newcommand{\li}[1]{{\color{blue}[#1 --lx]}}

\newcommand{\algo}[1]{{BapFL}}
\newcommand{\algoplus}[1]{{BapFL+}}
\newcommand{\galgo}[1]{{Gen-BapFL}}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation emai}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{48}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{You Can Backdoor Personalized Federated Learning}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \streetaddress{P.O. Box 1212}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
%   \postcode{43017-6221}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \streetaddress{Rono-Hills}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \streetaddress{30 Shuangqing Rd}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \streetaddress{8600 Datapoint Drive}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}
%   \postcode{78229}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \streetaddress{1 Th{\o}rv{\"a}ld Circle}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

\author{Tiandi Ye}
\affiliation{%
 \institution{East China Normal University,}
 \city{}
 \state{Shanghai}
 \country{China}
}
\email{52205903002@stu.ecnu.edu.cn}

\author{Cen Chen}
% \authornotemark[1]
\affiliation{%
 \institution{East China Normal University,}
 \city{}
 \state{Shanghai}
 \country{China}
}
\email{cenchen@dase.ecnu.edu.cn}

\author{Yinggui Wang}
\affiliation{%
 \institution{Ant Group}
 \city{}
 \state{Beijing}
 \country{China}
}
\email{wyinggui@gmail.com}

\author{Xiang Li}
\affiliation{%
 \institution{East China Normal University,}
 \city{}
 \state{Shanghai}
 \country{China}
}
\email{xiangli@dase.ecnu.edu.cn}

\author{Ming Gao}
\affiliation{%
 \institution{East China Normal University,}
 \city{}
 \state{Shanghai}
 \country{China}
}
\email{mgao@dase.ecnu.edu.cn}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. Th  is command allows
%% the author to define a more concise list 
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Tiandi Ye, Cen Chen, Yinggui Wang, Xiang Li and Ming Gao.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In federated learning (FL), 
malicious clients can manipulate the predictions of the trained model through backdoor attacks, 
posing a significant threat to the security of FL systems. 
However, existing research primarily focuses on backdoor attacks and defenses within the generic FL scenario, 
where all clients collaborate to train a single global model. 
\citet{qin2023revisiting} conduct the first study of backdoor attacks in the personalized federated learning (pFL) scenario, 
where each client constructs a personalized model based on its local data. 
Notably, the study demonstrates that pFL methods with \textit{partial model-sharing} can significantly boost robustness against backdoor attacks. 
In this paper, 
we whistleblow that pFL methods with partial model-sharing are still vulnerable to backdoor attacks in the absence of any defense. 
We propose three backdoor attack methods: BapFL, BapFL+, and Gen-BapFL, and we empirically demonstrate that they can effectively attack the pFL methods. 
Specifically, the key principle of BapFL lies in maintaining clean local parameters while implanting the backdoor into the global parameters. 
BapFL+ generalizes the attack success to benign clients by introducing Gaussian noise to the local parameters. 
Furthermore, we assume the collaboration of malicious clients and propose Gen-BapFL, which leverages meta-learning techniques to further enhances attack generalization. 
We evaluate our proposed attack methods against two classic pFL methods with partial model-sharing, FedPer~\cite{arivazhagan2019federated} and LG-FedAvg~\cite{liang2020think}. 
Extensive experiments on four FL benchmark datasets demonstrate the effectiveness of our proposed attack methods. 
Additionally, we assess the defense efficacy of various defense strategies against our proposed attacks and find that Gradient Norm-Clipping is particularly effective. 
It is crucial to note that pFL method is not always secure in the presence of backdoor attacks, and we hope to inspire further research on attack and defense in pFL scenarios. 
The code is available at: https://github.com/BapFL/code. 
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}
{}
\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{backdoor attack, personalized federated learning, model security}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Federated learning (FL) has gained significant popularity, 
which facilitates collaborative model training among multiple clients while preserving their data privacy~\cite{kairouz2021advances,li2020federated}. 
The non-IID (non-independent and non-identically distributed) nature of the data may significantly impact the model performance. 
To address this challenge, personalized federated learning (pFL) has emerged as a promising solution, 
as it allows personalized models for each participating client~\cite{mansour2020three,arivazhagan2019federated,fallah2020personalized,shamsian2021personalized}. 
A class of methods adopts partial model-sharing to achieve personalization, which involves splitting the client model into two parts: global parameters $\theta^{g}$ shared among clients and local parameters specific $\theta^{l}$ to each client. 

Backdoor attack is a malicious attack method against machine learning models. 
Its primary objective 
is to secretly introduce a backdoor 
into the model during training 
while maintaining its overall performance. 
So, the targeted model can be controlled 
or exploited by attackers in specific situations. 
In detail, attackers poison a subset of training data by implanting specific patterns, i.e., backdoor triggers, into clean samples and replacing their corresponding labels with a predetermined target label. 
The model trained on the polluted training set 
will make wrong predictions 
when trigger patterns appear 
during the testing phase: 
classifying any samples with the trigger as the target label. 
Backdoor attacks in FL can be classified into two categories: white-box attacks and black-box attacks~\cite{lyu2020threats}. In white-box attacks, adversarial clients have full access to the local training process, whereas, 
in black-box attacks, adversarial clients are limited to manipulating local training datasets. 
Recent work~\cite{qin2023revisiting} shows that 
pFL methods with partial model-sharing 
can effectively alleviate black-box backdoor attacks. 
Under the black-box assumption, backdoor implanting 
is performed by updating global and local parameters simultaneously 
to minimize the classification loss w.r.t. the poisoned samples. 

In this paper, we show that 
pFL methods with partial model-sharing 
remain vulnerable to backdoor attacks. 
In this paper, we introduce \algo~ under the white-box assumption, 
a novel and effective \underline{b}ackdoor \underline{a}ttack 
against \underline{pFL} with partial model-sharing. 
A key insight in the design of \algo~ is a more principled selection of neurons for implanting the backdoor. 
\algo~ implants the backdoor into the global parameters while keeping the local part unaffected. 
The motivation behind this design is to maintain the malicious client's local parameters as similar as possible to those of the benign clients, 
thereby enabling the generalization of the backdoor attack success to benign clients. 

We further improve the backdoor attack effectiveness 
by incorporating Gaussian noise into the weights of the local parameters during the poisoning process. 
More specifically, 
before each local iteration, 
we add a noise sampled from a standard multivariate Gaussian distribution 
to the weights of the local parameters 
and poison the global parameters 
with the local parameters fixed. 
We denote the enhanced method as \algoplus~. 
By introducing the noise, 
we increase the variability 
in the local parameters, 
enabling the backdoor to 
effectively target a wider range of clients 
with similar local parameters. 
This enhances the attack's generalization capability. 

In scenarios with severe data heterogeneity, 
the local parameters might vary significantly across clients, 
which poses challenges in generalizing the success of backdoor attacks on malicious clients to benign clients. 
To address this challenge, 
we leverage the idea of 
learning to generalize via meta-learning, and further enhance the generalization capability of \algoplus~\cite{li2018learning}. 
We assume multiple malicious clients collaborate to launch backdoor attacks. These malicious clients can share their models and some data among themselves. 
During local training, 
each malicious client meta-trains its global parameters 
and then meta-tests and optimizes its attack performance 
on local parameters from other malicious clients. 
We expect the well-trained global shared parameters 
can backdoor attack those unseen benign clients with different local parameters. 

We summarize our contributions as follows: 
\begin{itemize}
  \item We issue a warning to the community that even pFL methods with partial model-sharing are still highly vulnerable to backdoor attacks. 
  \item We propose a base attack, named \algo, against partial model-sharing pFL methods, which only modifies the training procedures yet obtains remarkable backdoor attack performance. 
Additionally, we enhance \algo~ by introducing Gaussian noise to the local parameters, allowing for effective attacks on benign clients with similar local parameters. Notably, both attacks incur negligible computation and storage costs.
  \item Furthermore, we introduce an advanced backdoor attack called \galgo~ based on meta-learning. This method generalizes the attack success from malicious clients to other benign clients. 
  \item We conduct comprehensive experiments targeting {two} representative partial model-sharing pFL methods on {four} benchmark datasets to evaluate the effectiveness of our proposed attack methods. The results of these experiments validate the efficacy of our proposed methods. 
  \item Finally, we evaluate three defense strategies: Gradient Norm-Clipping, Fine-Tuning, and Simple-Tuning. Among these, we find that Gradient Norm-Clipping is particularly effective in countering backdoor attacks on partial model-sharing pFL methods without significantly degrading the overall model performance. 
\end{itemize}





\section{Related Work}
\subsection{Personalized Federated Learning}
FL is a distributed machine learning approach, 
which enables training models on decentralized data sources 
and ensuring data privacy. 
We consider an FL system with $N$ clients, 
where each client has a private dataset 
$\mathcal{D}_{i}=\{(x_{j}, y_{j})\}_{j=1}^{|\mathcal{D}_{i}|}$. 
The optimization objective of the generic FL can be formulated as 
\begin{equation}
\begin{gathered}
\underset{\theta}{min} 
\sum_{i=1}^{N} \frac{|\mathcal{D}_{i}|}{|\mathcal{D}|}\mathcal{L}_{i}(\theta), \\ 
where ~\mathcal{L}_{i} = \frac{1}{|\mathcal{D}_{i}|} \sum_{j=1}\ell(x_{j}, y_{j}; \theta^{g}, \theta_{i}^{l})
\end{gathered}
\label{eq:fl-obj}
\end{equation}
Here, $\mathcal{D}=\cup_{i}\mathcal{D}_{i}$ is the aggregated dataset from all clients, 
$\mathcal{L}_{i}(\theta)$ is the empirical risk of client $c_{i}$, 
and $\ell$ refers to the loss function applied to each data instance, 
commonly the cross-entropy loss for classification tasks. 
FedAvg~\cite{mcmahan2017communication} is the first proposed solution to Eq.\ref{eq:fl-obj}, which iterates between local training and global aggregation for multiple rounds of communication. 

Besides FedAvg, other generic FL methods, such as FedProx\cite{li2020federated} and SCAFFOLD~\cite{karimireddy2020scaffold}, output a universal model for all clients. 
In terms of the test accuracy of each client, personalized federated learning (pFL) proves to be a more effective learning paradigm, especially in the presence of data heterogeneity. 

Based on the form of model sharing among clients, 
existing pFL methods can be broadly categorized into two classes: 
full model-sharing and partial model-sharing~\cite{qin2023revisiting}. 
In partial model-sharing pFL methods, 
each client's local model $\theta_{i}$ 
is divided into two groups of parameters, 
the global shared part $\theta^{g}$ and the local private part $\theta_{i}^{l}$~\cite{arivazhagan2019federated,collins2021exploiting,li2021fedbn,liang2020think}. 
The optimization objective can be formulated as 
\begin{equation}
\begin{gathered}
\underset{\{\theta^{g}, \theta_{1}^{l}, \cdots, \theta_{N}^{l}\}}{min} 
\sum_{i=1}^{N} \frac{|\mathcal{D}_{i}|}{|\mathcal{D}|}\mathcal{L}_{i}(\theta^{g}, \theta_{i}^{l}), \\ 
where ~\mathcal{L}_{i} = \frac{1}{|\mathcal{D}_{i}|} \sum_{j=1}\ell(x_{j}, y_{j}; \theta^{g}, \theta_{i}^{l})
\end{gathered}
\end{equation}
Here, $\mathcal{L}_{i}(\theta^{g}, \theta_{i}^{l})$ is the empirical risk of client $i$. 
In full model-sharing pFL, each client synchronizes its full model with the server~\cite{fallah2020personalized,t2020personalized,li2021ditto,shamsian2021personalized}. 
And in this paper, we focus on backdoor attack partial model-sharing pFL methods, including FedPer~\cite{arivazhagan2019federated}, FedRep~\cite{collins2021exploiting}, FedBN~\cite{li2021fedbn}, FedRod~\cite{chen2021bridging} and LG-FedAvg~\cite{liang2020think}. 
FedPer~\cite{arivazhagan2019federated} and FedRep~\cite{collins2021exploiting} learn a global feature representation across clients 
and unique local heads for each client. 
In FedPer~\cite{arivazhagan2019federated}, each client jointly updates the representation and the head for the same number of epochs. 
On the other hand, in FedRep~\cite{collins2021exploiting}, each client first updates the head and then updates the representation for different epochs. 
In FedBN~\cite{li2021fedbn}, each client keeps the BN~\cite{ioffe2015batch} layers local and synchronizes other parameters with the server. 
LG-FedAvg~\cite{liang2020think} learns compact and unique data representation and shares the top layers of the model across clients. 

\subsection{Backdoor Attack and Defense}
Recently, 
backdoor attacks have gained significant attention 
as they can compromise the security and trustworthiness of ML models. 
In a backdoor attack, 
the malicious clients 
aim to implant backdoors into the target model, 
and during inference, a trigger embedded in the input could trick the backdoored model 
to exhibit a predetermined behaviour 
as desired by the attacker, 
while ensuring that the model functions normally on clean inputs~\cite{li2022backdoor,gu2019badnets}. 

Due to the decentralized nature of FL, 
malicious clients may manipulate their local models 
and inject backdoors into the global model during training~\cite{bagdasaryan2020backdoor,wang2020attack,zhang2022flip,qin2023revisiting,chen2020backdoor,zhang2022neurotoxin,xie2020dba}. 
For example, 
under the assumption that multiple malicious clients collaborate and collude, 
DBA~\cite{xie2020dba} launch a distributed-trigger backdoor attack, which decomposes the global trigger into multiple parts and distributes them among multiple clients. 
Experiment results have demonstrated that DBA outperforms the single-trigger approach in terms of both attack effectiveness and stealth. 
Neurotoxin~\cite{zhang2022neurotoxin} extends the duration and persistence of backdoor attacks by projecting the adversarial gradient onto the subspace that remains unused by benign users during retraining.

To defend against backdoor attacks, many strategies have been proposed, such as median~\cite{yin2018byzantine}, Krum~\cite{blanchard2017machine}, norm clipping and differential privacy~\cite{sun2019can}. 
Recent work~\cite{qin2023revisiting} conducts the first study of backdoor attacks in the pFL framework 
and shows that 
pFL methods with partial model-sharing 
can effectively alleviate black-box backdoor attacks. 
Inspired by the defense mechanism of pFL methods of partial model-sharing, they provide a defense method that reinitializes the private parameters and retrains them on clean samples with the global parameters fixed. 

\section{Backdoor Attack against pFL}
\subsection{Threat Model} 
In this paper, 
we backdoor attack against pFL with partial model-sharing under the white-box assumption, 
where malicious clients can manipulate the local training procedures. 
Similar to previous studies~\cite{xie2020dba,qin2023revisiting},
we focus on the image classification task, which has been widely used in research on backdoor attacks.

\subsubsection{Attacker’s Capacities}
We assume that 
attackers have full access to their local training. 
They can poison a subset of local data and change the training components, e.g. training loss and training schedule. 
We further assume multiple malicious clients collaborate to launch backdoor attacks. 
These malicious clients can share local parameters and some data among themselves. 
\subsubsection{Attacker’s Goals} 
Malicious clients in partial model-sharing pFL settings aim to implant backdoors in the global parameters. 
During testing on images that contain the trigger, the predictions of the benign clients' personalized models will be arbitrarily altered to the target class. 
In addition, a successful attack should ensure stealthiness, i.e., the performance of the personalized model on clean samples should not be significantly reduced. 

\subsubsection{Targeted pFL Methods}
FedPer~\cite{arivazhagan2019federated} 
and LG-FedAvg~\cite{liang2020think} 
are two representative pFL methods with partial model-sharing.
The two methods cut the model into two parts, 
with FedPer sharing the bottom layers near the data sources, 
and LG-FedAvg sharing the top layers closer to the output. 
We illustrate the difference between these two paradigms in Fig.~\ref{fig:fedper-vs-fedavg}. 
The orange represents the client-specific layers that remain local throughout the training, while the green represents the shared layers across clients. 

% Figure environment removed

FedPer aims to establish a shared data representation across clients, with each client learning a local head to adapt to its personalized data distribution. 
FedRep~\cite{collins2021exploiting}, FedRod~{chen2021bridging} and FedPer employ the same strategy of model-sharing for personalization. 
While FedRep and FedRod introduce more sophisticated optimization techniques to achieve enhanced personalized results, 
our study primarily focuses on attacking the partial model-sharing architecture, 
thus emphasizing attacking FedPer. 
In contrast to FedPer, 
LG-FedAvg adopts a different personalization strategy 
where each client learns a local data representation 
and shares the information from the top layers of the model. 

\subsection{The Proposed Attacks} 
\subsubsection{\underline{\algo~}}
The key to the resistance of pFL methods with partial model-sharing to black-box backdoor attacks lies in the significant differences between the local parameters of malicious and benign clients. 
The discrepancy inactivates the embedded backdoors among benign clients. 
There are two factors contributing to the discrepancy in local parameters: 
(1) malicious clients update the entire model, including local parameters, to minimize the model's loss on poisoned samples, while benign clients have never encountered such poisoned data, 
and 
(2) there exists data heterogeneity between malicious and benign clients. 

The key to generalizing the backdoor attack to benign clients 
is to implant the backdoor into the global parameters 
and ensure alignment between the local parameters of malicious and benign clients. 
To achieve this, we propose modifying the training procedure of malicious clients. 
Specifically, we optimize the parameters of the entire model to minimize the model's loss on clean samples with the local parameters fixed and update only the global parameters to minimize the model's loss on poisoned samples. Consequently, the optimization of local parameters for malicious clients aligns with that of benign clients, utilizing only the information from clean samples. 
We refer to this attack method as BapFL, which breaks the first factor leading to the discrepancy in local parameters. 
Notably, 
compared to the black-box attack, \algo~ almost doesn't introduce any computational and storage overhead.

\subsubsection{\underline{\algoplus~}}
Due to the data heterogeneity among clients, 
the local parameters of clients optimize in different directions. 
To align the local parameters of malicious clients with those of benign clients, 
we introduce noise to the local parameters of malicious clients to simulate the local parameters of benign clients. 
Specifically, we sample noise from an isotropic multivariate Gaussian distribution with a mean of 0 and then add it to the local parameters of malicious clients. 
By continuously sampling and injecting noise, 
malicious clients are exposed to various local parameter configurations, 
enabling them to launch attacks on clients with different data distributions. 
Additionally, 
we control the variance of the Gaussian distribution to simulate the disparity in data distribution among benign clients, 
where a larger variance corresponds to clients with a greater difference in data distribution, and vice versa. 
We refer to this enhanced attack method as \algoplus~. 
Compared to \algo~, \algoplus~ introduces minimal overhead in terms of sampling. 

\subsubsection{\underline{\galgo~}}
To further enhance the generalization of backdoor attacks, 
we assume multiple colluding malicious clients in the federated system, 
engaging in a coordinated poisoning attack~\cite{xie2020dba}. 
We further assume that during training, malicious clients can exchange their local parameters and partial local data with each other. 
Let us denote a malicious client as the $c_{A}$, with its local parameters and dataset represented as $\theta_{A}^{l}$ and $\mathcal{D}_{A}$, respectively. 
At the beginning of each training round, 
client $c_{A}$ receives the local parameters $\theta_{B}^{l}$ and partial data $\mathcal{D}_{B}$ from another malicious client $c_{B}$. 

Inspired by meta-learning for domain generalization~\cite{li2018learning}, 
based on \algoplus~, 
we meta-train the global parameters $\theta^{g}$ for attack generalization. 
Specifically, the optimization objective is defined as 
\begin{equation}
\begin{gathered}
\underset{\{\substack{\theta^{g}\}}}{min}~
\mathcal{L}_{A}(\theta^{g}, \theta_{A}^{l}) 
+ \beta
\mathcal{L}_{B}(\theta^{g}-\mu\nabla_{\theta^{g}}\mathcal{L}_{A}(\theta^{g}, \theta_{A}^{l}), \theta_{B}^{l}), \\
where~\mathcal{L}_{A/B}(\theta^{g}, \theta_{A/B}^{l}) = \frac{1}{|\mathcal{D}_{A}|} \sum_{j}\ell(\varphi(x_{j}), \tau(y_{j}); \theta^{g}, \theta_{A/B}^{l}). \\
\end{gathered}
\end{equation}
Here, $\varphi$ represents the stamping-trigger function, and $\tau$ outputs the target class regardless of the input. 
The meta-optimization objective of client $c_{A}$ requires that updates to improve the attack performance of the model $M(\theta^{g}, \theta_{A}^{l})$ on client $c_{A}$ 
should also improves the attack performance of the model $M(\theta^{g}, \theta_{B}^{l})$ on client $c_{B}$. 
Here, $M(\theta^{g}, \theta^{l})$ represents a model parameterized by global parameters $\theta^{g}$ and local parameters $\theta^{l}$. 
We introduce this meta-optimization to enhance the generalization of the global parameters, which means that the backdoor can be activated by a model with new local parameters $M(\theta^{g}, \theta_{new}^{l})$. 
To train for generalization, 
at the beginning of each round, 
each malicious client randomly selects another malicious client as the partner and engages in a collaborative backdoor attack. 

We describe the algorithm of \algoplus~ and \galgo~ in detail in Algorithm~\ref{alg:algorithm}. 

\begin{algorithm}[h!]
\SetAlgoLined
\caption{Training of \algoplus~ and \galgo~}
\label{alg:algorithm}
\textbf{Input}: initial global shared parameters $\theta^{g,0}$ and local private parameters $\{\theta_{i}^{l}\}_{i=1}^{N}$, 
total communication rounds $R$, 
local iterations $\tau$, 
local minibatch size $B$, 
learning rate $\eta$ and $\mu$, 
variance of the normal distribution $\sigma$, 
number of participating clients per round $M$, and 
number of poisoned samples in a batch $b$ \\
\textbf{Output}: $\theta^{g,R}$ and $\{\theta_{i}^{l}\}_{i=1}^{N}$ \\
\textbf{[Training]} \\
\For{{each round} $r$ : 0 {to} $R-1$} {
    Select a set of clients $\mathcal{S}_{r}$ of size $M$ \\
    \For{{each client} $c_{i} \in \mathcal{S}_{r}$} {
        // Local Training \\
        $\theta_{i}^{g} \leftarrow \theta^{g,r}$ \\
        $\theta_{i} := \{\theta_{i}^{g}, \theta_{i}^{l}\}$ \\
        $\mathcal{B}_{i} \leftarrow$ Split local dataset $\mathcal{D}_i$ into batches of size $B$ \\
        Select a colluding malicious client $c_j$, and request its local parameters $\theta_{j}^{l}$ and dataset $\mathcal{D}_{j}$ \\
        \For{{each iteration} $t$ : 1 {to} $\tau$} {
            Sample a batch $(X_{t}, Y_{t}) \sim \mathcal{B}_{i}$ \\
            \If{$c_{i}$ is an attack} {
                $(X_{t,clean}, Y_{t,clean}), (X_{t,poison}, Y_{t,poison}) \leftarrow$ Randomly poison $b$ samples in the batch $(X_{t}, Y_{t})$\\
                $\theta_{i} \leftarrow \theta_{i} - \eta \nabla_{\theta_{i}} \mathcal{L}_{i}(X_{t,clean}, Y_{t,clean})$ \\
                $\epsilon \leftarrow$ Sample a noise from a multivariate gaussian distribution $\mathcal{N}(0,\sigma)$ \\
                $\theta_{i}^{l} \leftarrow \theta_{i}^{l} + \epsilon$ \\ 
                $\theta_{i}^{g} \leftarrow \theta_{i}^{g} - \eta \nabla_{\theta_{i}^{g}} \mathcal{L}_{i}(X_{t,poison}, Y_{t,poison})$ \\ 
                \Comment{Optimization of \algoplus~} \\
                $\theta_{i}^{g} \leftarrow \theta_{i}^{g} - \eta \nabla_{\theta_{i}^{g}} \mathcal{L}_{i}(\theta^{g}, \theta_{i}^{l}) + \beta \mathcal{L}_{j}(\theta^{g}-\mu \nabla_{\theta^{g}} \mathcal{L}_{i}(\theta^{g}, \theta_{i}^{l}), \theta_{j}^{l})$ \\ 
                \Comment{Optimization of \galgo~} \\
            } \Else {
                $\theta_{i} \leftarrow \theta_{i} - \eta \nabla_{\theta_{i}} \mathcal{L}_{i}(X_{t}, Y_{t})$
            }
        }
        $\theta_{i}^{g,r+1} \leftarrow \theta_{i}^{g}$ \\
        Client $c_{i}$ sends $\theta_{i}^{g, r+1}$ back to server \\
    }
    // Server Aggregation \\
    $\theta^{g,r+1} \leftarrow \sum_{c_{i} \in \mathcal{S}_{r}} \frac{|\mathcal{D}_{i}|}{\sum_{c_{i} \in \mathcal{S}_{r}}|\mathcal{D}_{i}|} \theta_{i}^{g,r+1}$ \\
}
\end{algorithm}


\subsection{Discussion}
We have presented our attack methods in a more general framework, 
where we divide the client model into two parts: the local parameter $\theta^{l}$ and the global parameter $\theta^{g}$. 
In this section, 
we will examine two specific instances of pFL with partial model-sharing: FedPer and LG-FedAvg, to demonstrate how our methods can effectively attack benign clients in a federated system. 

\subsubsection{Attack on FedPer}
In the case of FedPer, 
both benign clients and malicious clients share the bottom layers of the model, 
and each client performs local classification. 
To attack a benign client, 
the malicious client needs to 
align the local feature space 
generated by its top classification layers 
for the target label 
with that of the benign client. 
In \galgo~, 
the malicious client 
adjusts its own feature subspace 
corresponding to the target label 
to approach the intersection 
of that across all clients.

\subsubsection{Attack on LG-FedAvg}
In the case of LG-FedAvg, 
both benign clients and malicious clients share the top layers of the model, 
and each client performs local encoding. 
To attack a benign client, 
the malicious client needs to align the local representation 
encoded by its bottom encoder layers 
for the target label 
with that of the benign client. 
In \galgo~, 
the malicious client 
adjusts its own representation 
corresponding to the target label 
to approach the intersection 
of that across all clients. 

% Figure environment removed

\begin{table*}[htbp!]
    \centering
    \caption{The optimal hyperparameters for \algo~ and \galgo~.}
    \label{tab:best-hyper-parameters}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{@{}l c ccc c c c ccc c ccc c ccc@{}}
    \toprule
    \multirow{2}{*}{Dataset}                 && \multicolumn{3}{c}{MNIST} && \multicolumn{1}{c}{FEMNIST} && \multicolumn{3}{c}{Fashion-MNIST}&& \multicolumn{3}{c}{CIFAR-10} \\
                    \cmidrule{3-5} \cmidrule{7-7} \cmidrule{9-11} \cmidrule{13-15}
                                && 0.5          & 1             & 5             && $\backslash$ && 0.5 & 1 & 5 && 0.5 & 1 & 5        \\
    \midrule
    % FedPer
    \algoplus~@FedPer ($\sigma$)            && 3e-5             & 3e-4          & 3e-4              && 3e-4             && 3e-3         & 3e-4          & 3e-5          &&  3e-3            & 3e-5         & 3e-3            \\
    \galgo~@FedPer ($\sigma, \beta$)       && (3e-4, 0.8)      & (3e-4, 0.8)   & (3e-4, 0.8)       && 0.8              && (3e-3, 0.4)  & (3e-4, 0.4)   & (3e-5, 0.8)   &&  (3e-3, {0.2})     & (3e-4, 0.4)    &  (3e-3, 0.2)           \\
    \midrule
    % LG-FedAvg
    \algoplus~@LG-FedAvg ($\sigma$)         && 3e-3             & 3e-3          & 3e-3              && 3e-5             && 3e-3         & 3e-4          & 3e-4          &&  3e-5            & 3e-4          & 3e-6              \\
    \galgo~@LG-FedAvg ($\sigma, \beta$)    && (3e-3, 0.4)      & (3e-3, 0.4)   & (3e-3, 0.2)       && (3e-5, 0.2)      && (3e-3, 0.6)  & (3e-6, 0.6)   & (3e-6, 0.2)    &&  (3e-5, 0.2)     & (3e-4, 0.2)   & (3e-6, 0.8)      \\
    \bottomrule{}
    \end{tabular}
    }
\end{table*}

\section{Experiments}
\begin{table*}[]
\caption{
ASR and MTA of all attack methods targeting FedPer and LG-FedAvg on MNIST, FEMNIST, Fashion-MNIST and CIFAR-10 datasets. 
}
\centering

\resizebox{\linewidth}{!}{



\begin{tabular}{|ll|cccccl|cl|llllll|llllll|}
\hline
\multicolumn{2}{|c|}{\multirow{3}{*}{Attack Settings}}                                & \multicolumn{6}{c|}{MNIST}                                                                                                                                                  & \multicolumn{2}{c|}{FEMNIST}                            & \multicolumn{6}{c|}{Fashion-MNIST}                                                                                                                                        & \multicolumn{6}{c|}{CIFAR-10}                                                                                                                                   \\ \cline{3-22} 
\multicolumn{2}{|l|}{}                                                                & \multicolumn{2}{c|}{$\alpha=$0.5}                                & \multicolumn{2}{c|}{$\alpha=$1}                                  & \multicolumn{2}{c|}{$\alpha=$5}                                  & \multicolumn{2}{c|}{\textbackslash{}}                   & \multicolumn{2}{c|}{$\alpha=$0.5}                                & \multicolumn{2}{c|}{$\alpha=$1}                                  & \multicolumn{2}{c|}{$\alpha=$5}                                & \multicolumn{2}{c|}{$\alpha=$0.5}                            & \multicolumn{2}{c|}{$\alpha=$1}                              & \multicolumn{2}{c|}{$\alpha=$5}                              \\ \cline{3-22} 
\multicolumn{2}{|l|}{}                                                                & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA} & \multicolumn{1}{c|}{ASR} & \multicolumn{1}{c|}{MTA} & \multicolumn{1}{c|}{ASR} & \multicolumn{1}{c|}{MTA} & \multicolumn{1}{c|}{ASR} & \multicolumn{1}{c|}{MTA} \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{No-Attack}}        
& \multicolumn{1}{c|}{FedPer}                      
& \multicolumn{1}{c|}{0.43}  & \multicolumn{1}{c|}{97.35} 
& \multicolumn{1}{c|}{0.58}  & \multicolumn{1}{c|}{96.67} 
& \multicolumn{1}{c|}{0.68}  & \multicolumn{1}{c|}{95.80} 

& \multicolumn{1}{c|}{1.06}  & \multicolumn{1}{c|}{71.05} 

& \multicolumn{1}{c|}{2.42}  & \multicolumn{1}{c|}{89.36} 
& \multicolumn{1}{c|}{3.71}  & \multicolumn{1}{c|}{85.80} 
& \multicolumn{1}{c|}{3.54}  & 83.01                    

& \multicolumn{1}{c|}{6.96}  & \multicolumn{1}{c|}{76.58}    
& \multicolumn{1}{c|}{5.70}  & \multicolumn{1}{c|}{69.21}    
& \multicolumn{1}{c|}{5.09}  & \multicolumn{1}{c|}{68.88}                         
\\ \cline{2-22} 
\multicolumn{1}{|l|}{}                                  
& \multicolumn{1}{c|}{LG-FedAvg}                   
& \multicolumn{1}{c|}{0.41}  & \multicolumn{1}{c|}{97.54}      
& \multicolumn{1}{c|}{0.51}  & \multicolumn{1}{c|}{96.99}      
& \multicolumn{1}{c|}{0.54}  & \multicolumn{1}{c|}{96.73}                                

& \multicolumn{1}{c|}{0.73}  & \multicolumn{1}{c|}{79.35}                           

& \multicolumn{1}{c|}{2.86}  & \multicolumn{1}{c|}{89.58} 
& \multicolumn{1}{c|}{4.24}  & \multicolumn{1}{c|}{86.02} 
& \multicolumn{1}{c|}{3.47}  & \multicolumn{1}{c|}{83.74}                    

& \multicolumn{1}{c|}{4.73}  & \multicolumn{1}{c|}{61.18}    
& \multicolumn{1}{c|}{4.44}  & \multicolumn{1}{c|}{54.23}    
& \multicolumn{1}{c|}{6.14}  & \multicolumn{1}{c|}{44.34}                         \\ \hline



\multicolumn{1}{|c|}{\multirow{2}{*}{Black-box Attack}} 
& \multicolumn{1}{c|}{FedPer}                      
& \multicolumn{1}{c|}{4.10}  & \multicolumn{1}{c|}{97.41} 
& \multicolumn{1}{c|}{3.98}  & \multicolumn{1}{c|}{96.59} 
& \multicolumn{1}{c|}{4.91}  & \multicolumn{1}{c|}{95.95} 

& \multicolumn{1}{c|}{4.64}  & \multicolumn{1}{c|}{71.15} 

& \multicolumn{1}{c|}{6.97}  & \multicolumn{1}{c|}{89.48} 
& \multicolumn{1}{c|}{5.83}  & \multicolumn{1}{c|}{86.01} 
& \multicolumn{1}{c|}{7.87}  & \multicolumn{1}{c|}{83.32}                    

& \multicolumn{1}{c|}{57.35} & \multicolumn{1}{c|}{76.38}    
& \multicolumn{1}{c|}{26.18} & \multicolumn{1}{c|}{68.97}    
& \multicolumn{1}{c|}{14.20} & \multicolumn{1}{c|}{68.60}                         
\\ \cline{2-22} 
\multicolumn{1}{|c|}{}                                  
& \multicolumn{1}{c|}{LG-FedAvg}
& \multicolumn{1}{c|}{3.98}      & \multicolumn{1}{c|}{97.48}      
& \multicolumn{1}{c|}{3.79}      & \multicolumn{1}{c|}{96.97}      
& \multicolumn{1}{c|}{4.74}      & \multicolumn{1}{c|}{96.71}                           

& \multicolumn{1}{c|}{4.32}  & \multicolumn{1}{c|}{79.40}                           

& \multicolumn{1}{c|}{7.05}  & \multicolumn{1}{c|}{89.44} 
& \multicolumn{1}{c|}{6.32}  & \multicolumn{1}{c|}{86.16} 
& \multicolumn{1}{c|}{7.15}  & \multicolumn{1}{c|}{83.53}                    

& \multicolumn{1}{c|}{7.04}  & \multicolumn{1}{c|}{60.87}    
& \multicolumn{1}{c|}{7.97}  & \multicolumn{1}{c|}{54.07}    
& \multicolumn{1}{c|}{8.94}  & \multicolumn{1}{c|}{44.82}                         \\ \hline



\multicolumn{1}{|c|}{\multirow{2}{*}{BapFL}}            
& \multicolumn{1}{c|}{FedPer}
& \multicolumn{1}{c|}{50.17} & \multicolumn{1}{c|}{96.60} 
& \multicolumn{1}{c|}{64.30} & \multicolumn{1}{c|}{95.63} 
& \multicolumn{1}{c|}{69.35} & \multicolumn{1}{c|}{93.92} 

& \multicolumn{1}{c|}{13.96} & \multicolumn{1}{c|}{68.62} 

& \multicolumn{1}{c|}{29.58} & \multicolumn{1}{c|}{89.35} 
& \multicolumn{1}{c|}{31.38} & \multicolumn{1}{c|}{85.41} 
& \multicolumn{1}{c|}{70.80} & \multicolumn{1}{c|}{83.13}                    

& \multicolumn{1}{c|}{67.87} & \multicolumn{1}{c|}{75.69}    
& \multicolumn{1}{c|}{67.70} & \multicolumn{1}{c|}{69.28}    
& \multicolumn{1}{c|}{88.97} & \multicolumn{1}{c|}{68.30}                         
\\ \cline{2-22} 
\multicolumn{1}{|c|}{}                                  
& \multicolumn{1}{c|}{LG-FedAvg}
& \multicolumn{1}{c|}{6.53}      & \multicolumn{1}{c|}{94.46}      
& \multicolumn{1}{c|}{4.96}      & \multicolumn{1}{c|}{95.92}      
& \multicolumn{1}{c|}{10.61}     & \multicolumn{1}{c|}{91.89}                             

& \multicolumn{1}{c|}{32.08} & \multicolumn{1}{c|}{50.53}                           

& \multicolumn{1}{c|}{38.32} & \multicolumn{1}{c|}{67.40} 
& \multicolumn{1}{c|}{44.57} & \multicolumn{1}{c|}{71.04} 
& \multicolumn{1}{c|}{50.53} & \multicolumn{1}{c|}{66.51}

& \multicolumn{1}{c|}{9.32}  & \multicolumn{1}{c|}{59.28}    
& \multicolumn{1}{c|}{15.30} & \multicolumn{1}{c|}{50.44}    
& \multicolumn{1}{c|}{15.69} & \multicolumn{1}{c|}{42.06}                         \\ \hline



\multicolumn{1}{|c|}{\multirow{2}{*}{BapFL+}}           
& \multicolumn{1}{c|}{FedPer} 
& \multicolumn{1}{c|}{59.91} & \multicolumn{1}{c|}{96.17} 
& \multicolumn{1}{c|}{65.03} & \multicolumn{1}{c|}{95.18} 
& \multicolumn{1}{c|}{79.18} & \multicolumn{1}{c|}{94.67}                      

& \multicolumn{1}{c|}{16.80} & \multicolumn{1}{c|}{68.23}                      

& \multicolumn{1}{c|}{39.72} & \multicolumn{1}{c|}{89.39} 
& \multicolumn{1}{c|}{37.93} & \multicolumn{1}{c|}{85.90} 
& \multicolumn{1}{c|}{70.86} & \multicolumn{1}{c|}{83.00}                    

& \multicolumn{1}{c|}{76.19} & \multicolumn{1}{c|}{76.24}    
& \multicolumn{1}{c|}{74.92} & \multicolumn{1}{c|}{69.38}    
& \multicolumn{1}{c|}{91.74} & \multicolumn{1}{c|}{68.49}                          
\\ \cline{2-22} 
\multicolumn{1}{|l|}{}                                  
& \multicolumn{1}{c|}{LG-FedAvg}
& \multicolumn{1}{c|}{17.25}      & \multicolumn{1}{c|}{85.95}      
& \multicolumn{1}{c|}{15.59}      & \multicolumn{1}{c|}{88.31}      
& \multicolumn{1}{c|}{29.08}      & \multicolumn{1}{c|}{78.24}                                 

& \multicolumn{1}{c|}{46.04} & \multicolumn{1}{c|}{41.68}                            

& \multicolumn{1}{c|}{40.18} & \multicolumn{1}{c|}{65.42} 
& \multicolumn{1}{c|}{48.85} & \multicolumn{1}{c|}{69.81} 
& \multicolumn{1}{c|}{59.55} & \multicolumn{1}{c|}{63.89}                    

& \multicolumn{1}{c|}{9.55}  & \multicolumn{1}{c|}{58.90}    
& \multicolumn{1}{c|}{18.46} & \multicolumn{1}{c|}{51.12}    
& \multicolumn{1}{c|}{14.36} & \multicolumn{1}{c|}{42.17}                         \\ \hline



\multicolumn{1}{|c|}{\multirow{2}{*}{Gen-BapFL}}        
& \multicolumn{1}{c|}{FedPer} 
& \multicolumn{1}{c|}{67.15} & \multicolumn{1}{c|}{95.50} 
& \multicolumn{1}{c|}{68.20} & \multicolumn{1}{c|}{95.68} 
& \multicolumn{1}{c|}{85.72} & \multicolumn{1}{c|}{94.16} 

& \multicolumn{1}{c|}{19.75} & \multicolumn{1}{c|}{68.39} 

& \multicolumn{1}{c|}{48.72} & \multicolumn{1}{c|}{89.44} 
& \multicolumn{1}{c|}{49.53} & \multicolumn{1}{c|}{84.66} 
& \multicolumn{1}{c|}{85.42} & \multicolumn{1}{c|}{83.39}                    

& \multicolumn{1}{c|}{67.48} & \multicolumn{1}{c|}{76.09}       
& \multicolumn{1}{c|}{67.60} & \multicolumn{1}{c|}{69.20}    
& \multicolumn{1}{c|}{85.41} & \multicolumn{1}{c|}{68.20}    
\\ \cline{2-22} 
\multicolumn{1}{|l|}{}                                  
& \multicolumn{1}{c|}{LG-FedAvg}                   
& \multicolumn{1}{c|}{26.50}      & \multicolumn{1}{c|}{78.97}      
& \multicolumn{1}{c|}{20.33}      & \multicolumn{1}{c|}{83.24}      
& \multicolumn{1}{c|}{30.24}      & \multicolumn{1}{c|}{75.95}        

& \multicolumn{1}{c|}{71.19} & \multicolumn{1}{c|}{25.40}                            

& \multicolumn{1}{c|}{46.93} & \multicolumn{1}{c|}{61.00} 
& \multicolumn{1}{c|}{51.95} & \multicolumn{1}{c|}{65.11} 
& \multicolumn{1}{c|}{63.22} & \multicolumn{1}{c|}{60.62}                    

& \multicolumn{1}{c|}{9.18}  & \multicolumn{1}{c|}{59.74}    
& \multicolumn{1}{c|}{16.74} & \multicolumn{1}{c|}{51.96}    
& \multicolumn{1}{c|}{15.46} & \multicolumn{1}{c|}{43.07}                         
\\ \hline
\end{tabular}



}
\label{tab:asr-mta}

\end{table*}

\subsection{Experimental Settings}
\subsubsection{Datasets and Models}
We conduct experiments on four widely-used datasets: 
MNIST~\cite{lecun1998gradient}, 
FEMNIST~\cite{caldas2018leaf}, 
Fashion-MNIST~\cite{xiao2017fashion}, 
and CIFAR-10~\cite{cifar10}. 
MNIST, Fashion-MNIST, and CIFAR-10 are image classification datasets with ten categories. 
To simulate heterogeneous environments, 
we generate 50 clients with different levels of heterogeneity 
using Dirichlet allocation 
by varying the concentration coefficient $\alpha \in \{0.5, 1, 5\}$. 
A smaller value of $\alpha$ indicates greater data heterogeneity. 
FEMNIST is a naturally heterogeneous dataset, which consists of handwritten images from 3,550 users, 
each with a unique writing style. 
For our experiments, we randomly select a subset of 185 users from the FEMNIST dataset. 

We employ the same model architecture for all experiments conducted on the same dataset. 
Specifically, for MNIST, FEMNIST, and Fashion-MNIST, 
we utilize ConvNet~\cite{lecun1998gradient}, 
which consists of two convolutional layers and two fully connected layers. 
For CIFAR-10, we utilize VGG11~\cite{simonyan2014very}, 
which comprises eight convolutional layers and three fully connected layers. 
In our experiments, 
for a 4-layer ConvNet, 
all clients share the first two layers in FedPer, 
while in LG-FedAvg, 
they share the last two layers. 
Similarly, for VGG11, 
all clients share the first seven layers in FedPer, 
while in LG-FedAvg, they share the last four layers. 

\subsubsection{Training Details} 
We perform 200, 400, and 1000 communication rounds for MNIST, Fashion-MNIST, and CIFAR-10, respectively. 
For FEMNIST, we run 300 communication rounds for FedPer and 400 communication rounds for LG-FedAvg. 
On all datasets, we set the number of local iterations $\tau$ to 20 and the local minibatch size $B$ to 64. 
The learning rate for all methods is set to 0.1 with a learning rate decay of 0.99 per 10 rounds. 
The weight decay is set to 1e-4. 
For \galgo~, the learning rate of the inner loop, i.e., $\mu$, is also set to 0.1. 
We implement \algo~ and its variants using PyTorch and train them with the SGD optimizer. 
In each round, 10\% of the clients (5 clients) are selected for training and aggregation on MNIST, Fashion-MNIST, and CIFAR-10. 
On FEMNIST, we select 10 clients each round. 
For \algoplus~ and \galgo~, we search for the best $\sigma$ values from \{3e-3, 3e-4, 3e-5, 3e-6\} and the best $\beta$ from \{0.2, 0.4, 0.6, 0.8, 1.0\}. 
The resulting optimal hyperparameters are reported in Table~\ref{tab:best-hyper-parameters}. 
\subsubsection{Baseline} 
We compare our proposed methods with the black-box backdoor attack that poisons the training data and updates both the global and local parameters simultaneously to minimize the classification loss on both clean and poisoned samples~\cite{qin2023revisiting}. 


\subsubsection{Attack Setup}
As our contribution in this paper lies in 
effectively attacking pFL methods with partial model-sharing rather than designing more sophisticated and stealthy triggers, 
we employ the grid pattern as the trigger, which has been extensively utilized in federated learning backdoor attacks~\cite{xie2020dba,gu2019badnets}. 
Fig.\ref{fig:triggers} illustrates the poisoned images with the grid pattern as the trigger.
We adopt an all-to-one attack strategy, where all poisoned samples are assigned the same target label regardless of their original ground-truth labels. 
Specifically, for the FEMNIST dataset, 
the target label is set as 0, 
while for the other three datasets (MNIST, Fashion-MNIST, and CIFAR-10), 
the target label is set as 0. 
For MNIST, Fashion-MNIST, and CIFAR-10, we randomly designate 2 clients out of a total of 50 as malicious clients. 
Similarly, for FEMNIST, we randomly designate 4 clients out of a pool of 185 as malicious clients. 
Following the attack setting in~\cite{xie2020dba}, 
the malicious clients begin to attack when the main task accuracy converges, 
which is round {50} for MNIST, round 100 for Fashion-MNIST and round 500 for CIFAR-10. 
For FEMNIST, malicious clients initiate backdoor attacks at round 100 for FedPer and round 200 for LG-FedAvg. 
Subsequently, the malicious clients actively participate in the training process and launch backdoor attacks in every round.
Following~\cite{zhang2022flip}, in each local iteration, 
the malicious client poisons 20 samples of a batch on MNIST, FEMNIST and Fashion-MNIST and 5 samples on CIFAR-10. 

\subsubsection{Evaluation Details}
We employ two evaluation metrics, namely, 
the attack success rate (ASR) 
and the main task accuracy (MTA), 
to assess the efficacy of our proposed backdoor attacks. 
The ASR is computed as the ratio of successfully attacked poisoned samples to the total number of poisoned samples. 
On the other hand, the MTA measures the accuracy of the model when tested on clean samples. 
A successful attack should achieve a high ASR while maintaining a high MTA, 
indicating effective manipulation of the model's outputs 
without compromising its performance on the main task. 
Moreover, to ensure unbiased evaluation, 
we calculate the ASR only on data 
where the true label differs from the target label~\cite{xie2020dba}. 
We evaluate the model's performance every 2 rounds for MNIST and FEMNIST, 
every 4 rounds for Fashion-MNIST, 
and every 10 rounds for CIFAR-10. 
All experiments are conducted on six Tesla V100 GPUs. 
Each experiment is run three times with different random seeds. 
For each run, we record the average performance over the last 50 rounds, 
and then report the mean results from the three runs. 

\subsection{Main Results}
\subsubsection{Attacks on FedPer}
We present the ASR and MTA curves of all attack methods targeting FedPer in 
Fig.\ref{fig:fedper-mnist} (FedPer@MNIST), 
Fig.\ref{fig:fedper-femnist} (FedPer@FEMNIST), 
Fig.\ref{fig:fedper-fashionmnist} (FedPer@Fashion-MNIST), 
and Fig.\ref{fig:fedper-cifar10} (FedPer@CIFAR-10). 
From the figures, 
we can observe that initially, the model exhibits random prediction behavior. 
As the training progresses, the model gradually learns classification abilities and accurately identifies poisoned samples as their original ground-truth labels. 
The ASR corresponding to No-Attack steadily decreases as the training proceeds. 
Upon launching the black-box attack, the ASR shows a slight increase but eventually converges to values below 5\% on MNIST, 5\% on FEMNIST, and 8\% on Fashion-MNIST. 
The black-box attack exhibits unstable attack capabilities. 
On the CIFAR-10 dataset, when $\alpha=0.5$, the black-box attack achieves an ASR of 57.35\%. 
However, as $\alpha$ increases to 1 and 5, the ASR rapidly drops to 26.18\% and 14.20\% as the training progresses. 
Stamping the trigger onto the samples 
can sometimes act as data augmentation 
and improve the model's generalization ability on clean test samples. 
For example, under the black-box attack setting on MNIST, 
the model achieves a better MTA than the No-Attack setting when $\alpha$ is set to 0.5 and 5.


In contrast, our proposed attack methods demonstrate effectiveness in targeting FedPer under settings with varying degrees of data heterogeneity. 
First, our proposed attacks are stable. 
Generally, the smaller the degree of data heterogeneity, 
the smaller the difference between the local parameters of benign clients and adversarial clients, 
leading to a higher ASR of our proposed attacks. 
In general, \algoplus~ achieves better attack performance than \algo~, 
and \galgo~ further enhances the attack performance of \algoplus~. 
Moreover, all three attack methods maintain the model's performance on clean samples. 
For example, on MNIST, compared to the No-Attack setting, the average MTA of \algo~, \algoplus~, and \galgo~ across all $\alpha$ candidates is only slightly lower by 1.22\%, 1.27\%, and 1.49\%, respectively. 
On the CIFAR-10 dataset, compared to the No-Attack setting, the average MTA of \algo~, \algoplus~, and \galgo~ across all $\alpha$ candidates is only slightly lower by 0.47\%, 0.19\%, and 0.39\%, respectively.
On the CIFAR-10 dataset, \galgo~ performs worse than \algoplus~. 
We attribute it to the significant difference in data distributions between the two randomly selected clients, 
making it challenging to learn a global parameter with strong generalization. 
Conversely, blindly pursuing generalization leads to a low attack success rate on the malicious client itself. 
% Figure environment removed

% Figure environment removed

% Figure environment removed 

% Figure environment removed 






















\subsubsection{Attacks on LG-FedAvg}
We present the ASR and MTA curves 
of all attack methods 
targeting LG-FedAvg in 
Fig.~\ref{fig:lgfedavg-mnist} (LG-FedAvg@MNIST), 
Fig.~\ref{fig:lgfedavg-femnist} (LG-FedAvg@FEMNIST), 
Fig.~\ref{fig:lgfedavg-fashionmnist} (LG-FedAvg@Fashion-MNIST), and 
Fig.~\ref{fig:lgfedavg-cifar10} (LG-FedAvg@CIFAR-10), respectively. 
From these figures, we can observe that black-box attacks are ineffective against LG-FedAvg on all datasets. 
In contrast, Gen-BapFL demonstrates better attack performance. 
When $\alpha=0.5$, it achieves an ASR of 26.6\% on MNIST, 71.19\% on FEMNIST, and 46.93\% on Fashion-MNIST. 
However, these successful attacks come at a significant cost to the model's performance on clean samples, which results in poor stealthiness of these attacks. 
% Figure environment removed



% Figure environment removed





% Figure environment removed 



% Figure environment removed 
% \usepackage{multirow}
\begin{table*}[]

\caption{
The performance of three defense methods 
for FedPer 
against black-box attack and our proposed attacks on the CIFAR-10 dataset with $\alpha=0.5$. 
A desirable defense method exhibits lower ASR ($\downarrow$), 
indicating better robustness, 
and higher MTA ($\uparrow$). 
} 
% \resizebox{\linewidth}{!}{{}

\begin{tabular}{|c|cc|cccccc|cc|cc|}
\hline
\multirow{3}{*}{Defenses} & \multicolumn{2}{c|}{No-Defense}       & \multicolumn{6}{c|}{Gradient Norm-Clipping}                                                                                                            & \multicolumn{2}{c|}{Fine-Tuning}      & \multicolumn{2}{c|}{Simple-Tuning}    \\ \cline{2-13} 
                          & \multicolumn{2}{c|}{\textbackslash{}} & \multicolumn{2}{c|}{$M$=3}                              & \multicolumn{2}{c|}{$M$=5}                              & \multicolumn{2}{c|}{$M$=10}        & \multicolumn{2}{c|}{\textbackslash{}} & \multicolumn{2}{c|}{\textbackslash{}} \\ \cline{2-13} 
                          & \multicolumn{1}{c|}{ASR}     & MTA    & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & \multicolumn{1}{c|}{MTA}   & \multicolumn{1}{c|}{ASR}   & MTA   & \multicolumn{1}{c|}{ASR}     & MTA    & \multicolumn{1}{c|}{ASR}     & MTA    \\ \hline
Black-box Attack          & \multicolumn{1}{c|}{57.35}   & 76.38  & \multicolumn{1}{c|}{6.65}  & \multicolumn{1}{c|}{64.27} & \multicolumn{1}{c|}{7.06}  & \multicolumn{1}{c|}{67.62} & \multicolumn{1}{c|}{12.75} & 72.03 & \multicolumn{1}{c|}{56.53}   & 76.08  & \multicolumn{1}{c|}{51.55}   & 76.64  \\ \hline
BapFL                     & \multicolumn{1}{c|}{67.87}   & 75.69  & \multicolumn{1}{c|}{21.60} & \multicolumn{1}{c|}{59.69} & \multicolumn{1}{c|}{22.33} & \multicolumn{1}{c|}{65.71} & \multicolumn{1}{c|}{27.25} & 71.34 & \multicolumn{1}{c|}{63.72}   & 75.92  & \multicolumn{1}{c|}{62.05}   & 76.35  \\ \hline
BapFL+                    & \multicolumn{1}{c|}{76.19}   & 76.24  & \multicolumn{1}{c|}{7.28}  & \multicolumn{1}{c|}{62.60} & \multicolumn{1}{c|}{7.31}  & \multicolumn{1}{c|}{67.39} & \multicolumn{1}{c|}{8.04}  & 72.72 & \multicolumn{1}{c|}{75.18}   & 76.39  & \multicolumn{1}{c|}{56.43}   & 76.25  \\ \hline
Gen-BapFL                 & \multicolumn{1}{c|}{67.48}   & 76.09  & \multicolumn{1}{c|}{8.95}  & \multicolumn{1}{c|}{62.70} & \multicolumn{1}{c|}{16.74} & \multicolumn{1}{c|}{74.92} & \multicolumn{1}{c|}{19.34} & 75.09 & \multicolumn{1}{c|}{62.85}   & 76.86  & \multicolumn{1}{c|}{27.44}   & 67.70  \\ \hline
\end{tabular}
\label{tab:defense-results}
% }
\end{table*}

\subsection{Defense against Backdoor Attacks}
In previous sections, 
we have demonstrated that 
even the pFL methods with partial model-sharing, 
especially FedPer, 
are still vulnerable to backdoor attacks 
in the absence of any defense. 
In this section, 
we focus on evaluating 
the effectiveness of baseline defense methods 
\textit{for FedPer} 
against the black-box attack and our proposed backdoor attacks. 
We employ three defense methods, 
namely Gradient Norm-Clipping~\cite{sun2019can}, 
Fine-Tuning, 
and Simple-Tuning~\cite{qin2023revisiting}. 
Gradient Norm-Clipping normalizes clients' model updates on the server that exceed a threshold of $M$. 
In terms of Fine-Tuning, 
after obtaining the global parameters, 
benign clients perform fine-tuning on the entire model on their local clean dataset. 
\citet{qin2023revisiting} proposed an improved defense method, namely Simple-Tuning, 
which reinitializes the local parameters on benign clients 
and then retrains them on the local clean datasets 
while keeping the global parameters fixed. 
We evaluate the defense performance of these methods on CIFAR-10 dataset with $\alpha = 0.5$. 
Specifically, we evaluate the effectiveness of Gradient Norm-Clipping across different values of $M \in \{3, 5, 10\}$. 
For Fine-Tuning, we fine-tune the entire model for 20 iterations, 
while for Simple-Tuning, 
we retrain the local parameters for 200 iterations. 

\subsubsection{Results} 
Based on the results presented in Tab.~\ref{tab:defense-results}, 
it can be observed that Fine-Tuning and Simple-Tuning 
generally enhance the model's performance on the main task. 
However, Fine-Tuning fails to provide effective defense against backdoor attacks. 
Although Simple-Tuning demonstrates improved resistance compared to Fine-Tuning, 
the ASR of black-box attack, \algo~, and \algoplus~ still exceeds 50\%. 
Surprisingly, Simple-Tuning manages to reduce the ASR of \galgo~ from 67.48\% to 27.44\% with only an about 8\% decrease in MTA. 
\textit{Gradient Norm-Clipping proves to be a more suitable defense strategy for FedPer against backdoor attacks}. 
The impact of Gradient Norm-Clipping defense on the main task diminishes as the threshold $M$ increases; however, this also results in a decline in defense capability. 
When $M$ equals 10, the ASR of black-box attack, \algo~, \algoplus~, and \galgo~ decreases by approximately 44\%, 40\%, 68\%, and 48\%, respectively, with only a loss of about 4\%, 4\%, 3\%, and 1\% in MTA, respectively, which can be considered acceptable.


\section{Conclusion}
In this paper, 
we have shed light on the vulnerability of 
personalized federated learning (pFL) methods 
with partial model-sharing to backdoor attacks, 
emphasizing the need for effective defense strategies. 
While previous research has demonstrated the potential of pFL methods with partial model-sharing 
to enhance robustness against backdoor attacks, 
our study reveals that these methods are still susceptible to such attacks. 
We have introduced three novel backdoor attack methods, namely BapFL, BapFL+, and Gen-BapFL, and validated their efficacy across four datasets and two representative pFL methods with partial model-sharing, FedPer and LG-FedAvg. 
Furthermore, we have evaluated the efficacy of different defense strategies and found that Gradient Norm-Clipping proves is particularly effective. 
We hope that our work stimulates further investigations into attack and defense mechanisms tailored specifically for the pFL scenario, ultimately enhancing the security and trustworthiness of pFL in real-world applications.
\clearpage
\balance
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
