Recent work in Machine Learning (ML) and Computer Vision (CV) has highlighted various types of systematic flaws in the development of object recognition benchmark datasets. See, for instance, \cite{2019-CVPR,2021-NeurIPS}.
%
Our basic tenet is that these flaws are grounded in the way perception and language interact. As a matter of fact, an early version of this problem was already identified in \cite{SGP-2000} as the \textit{Semantic Gap Problem} (SGP), which was described 
as the ``\emph{lack of coincidence between the information that one can extract from the visual data and the interpretation that the same data have for a user in a given situation}". This problem, still unsolved, was formalized in \cite{SNCS-2021} as the fact that there is a \emph{many-to-many mapping} between the \textit{visual information} encoded in an image and the \textit{intended semantics} of the corresponding linguistic descriptions. Thus, for instance, in Fig.\ref{I1}-(II), Image \#251 is labeled \textit{Guitar} but, just because it gives only a partial view of the instrument, it could have also been labeled \textit{Bass} or \textit{Ukelele}, while, at the same time, \textit{Guitar} is also the label of images \#154, \#257.
The net consequence is that the current annotation process is largely under-specified, thus leaving too much freedom to the annotator's judgment, who can then subjectively select one among the many SGP mappings.

% Figure environment removed

In this paper, we propose \texttt{vTelos}\footnote{From the Greek word \texttt{Telos} which means the end; the object aimed at in an effort; \textit{purpose}, with \texttt{v} standing for \texttt{visual}, as in \textit{visual purpose}.}, an integrated Natural Language Processing (NLP), Knowledge Representation (KR) and CV methodology whose main goal is to generate high-quality datasets. There are two main underlying intuitions. The first is that the \textit{purpose} of an annotation effort, i.e., what the ML model trained by the dataset should be used for, should be made explicit via precise guidelines, that we organize around \textit{four main Design Choices}, as follows:
\begin{itemize}
\vspace{-0.1cm}
    \item C1: for each \textit{image}, which object(s) should be considered?
    \item C2: for each \textit{object}, which set of properties should be considered?
    \item C3: for each \textit{set of properties}, which label should be selected to properly describe them?
    \item C4: for each \textit{label}, if \textit{polysemic},\footnote{\label{f1} For instance, in WordNet, the average number of meanings per noun and per polysemous noun are 1.25 and 4, respectively, where the more common a noun, the higher the polysemy \cite{Freihat2014AnOA}.} how do we unambiguously represent its meaning?
    \vspace{-0.1cm}
\end{itemize} 
\noindent
The second underlying intuition is that the role of the annotator should be split in two, i.e., that of the \textit{Classificationist} and that of the \textit{Classifier}, each associated with different tasks (this terminology being adopted from KR and Knowledge Organization (KO) \cite{SRR-67}). Thus, the classificationist, will first define the space of labels to be used (Design Choices C3,C4) and, for each of them, the (visual) properties they describe. Then, in a second phase, the classifier will classify images using the labels defined by the classificationist (Design Choices C1,C2), where Choice C2 is based on the output of Choice C3. Notice how the proposed decision process is  reversed with respect to how image annotation is usually performed, as also described above.  The key idea is that we should first select the space of labels to be used, with a clear understanding of their intended meaning, in terms of the visual properties they are taken to describe, and then exploit this clarity when selecting, for each input image, the label which most properly describes its content. The proposed approach actually follows the standard practice in KR, for instance when classifying products in an online catalogue \cite{get-specific}.

In this process, \textit{the intended natural language  semantics of natural language terms} (as selected during C3, C4) \textit{and the relevant visual information of images} (i.e., objects and their visual properties, as selected during C1) \textit{are natively aligned} (during C2), this being the reason why the SGP does not appear. The annotation bias is dealt with not by building an all-encompassing dataset, which would be impossible \cite{KD-1995-bouquet,giunchiglia2021transparency} but, rather, by making explicit the annotation choices, as in \cite{erculiani2023egocentric}. 
%
This advantage is further amplified by the fact that, instead of performing choices C3, C4 from scratch, we take, as a predefined source of natural language labels, the \textit{WordNet Genus-Differentia lexico-semantic hierarchy} \cite{PWN}. In WordNet, each label is associated with a \textit{gloss}, which, in fact, is an \textit{unambiguous linguistic definition of the properties of the objects it describes}. This opens up the possibility of full and general alignment between natural language lexical semantics and object recognition, thus enabling a general purpose solution to the SGP many-to-many mapping problem.


There are three main contributions of this paper:

\begin{enumerate}
 \vspace{-0.1cm}
    \item A classification of the main sources of annotation mistakes;
    \item A general purpose annotation methodology, i.e., \texttt{vTelos}, for dealing with the SGP many-to-many mapping problem;
    \item The application of \texttt{vTelos} to the alignment of the linguistic semantics of labels to the visual semantics of images. Notice how multi-lingual lexical resources, see, e.g., 
  \cite{L-navigli2012babelnet},
    allows to go beyond the current English-only main practice and apply \texttt{vTelos} to any natural language.
 \vspace{-0.2cm}
\end{enumerate} 
%
The paper is organized as follows. Section \ref{S2} introduces the sources of the annotation mistakes. Section \ref{S3} describes the \texttt{vTelos} four design choices. Section \ref{S4} provides the main algorithm which must be followed by annotators in order to implement Choice C2, when taking in input the WordNet hierarchy. Section \ref{S5} provides an evaluation of \texttt{vTelos} on  (i) inter-annotator agreement, as a proxy of the role of subjective choices, (ii) annotation cost, and (iii) impact on quality of the ML models it generates. The evaluation is carried on a substantial subset of \textit{ImageNet} \cite{IMAGENET-2009}. The choice of ImageNet is not by chance. In fact, despite the fact that this information has hardly been used so far, ImageNet has been generated by populating WordNet, and, therefore, it natively contains the definitions encoding the meaning of labels. Furthermore, ImageNet provides a very good testbed for evaluation, having been analyzed in many studies, also focusing on annotation mistakes, see, e.g., \cite{2020-AnntPipelineError,salari2022object,northcutt2103pervasive,raji2021ai}.
%
 Related work (Section \ref{S6}) and conclusion (Section \ref{S7}) complete the paper.