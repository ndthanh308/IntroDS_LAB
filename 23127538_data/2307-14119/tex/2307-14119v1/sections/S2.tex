\begin{table*}[!t]
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{|l|l|l|l|l|}%{ l p{10cm} }
\hline
\textit{\textbf{Role}} & \textit{\textbf{Annotation Choice}} & \textit{\textbf{Purpose}}  & \textit{\textbf{Source of Subjectivity}} & \textit{\textbf{Solution}}  \\
\hline
Classificationist & C3: Label generation & Choose label and its properties  & MI, SOII & Linguistic Genus-Differentia \\
Classificationist & C4: Label disambiguation & Choose identifier  & SOIL & Alinguistic Identifiers \\
Annotator & C1: Object localization          & Choose object  & MOI & Bounding Polygons \\
Annotator & C2: Visual classification     & Choose label via its properties & MI, SOII, SOIL, SOIA & Visual Genus-Differentia   \\
\hline
\end{tabular}}%{ l p{10cm} }
\caption{The \texttt{vTelos} four choices. MOI and MI stand for Multi-Object and Mislabelled Images, respectively.  SOII, SOIL, SOIA stand for Single-Object Images where the source of subjectivity is the Image, the Label and the Annotator background knowledge, respectively.}
\label{T1}
% \vspace{-0.3cm}
\end{table*}

Let us take an \textit{annotation mistake} to be a label that does not correspond to the (classificationist) \textit{intended semantics} of the image. The sources of annotation mistakes are the cause of the SGP many-to-many mapping as they raise the possibility of choices that are different from those of the classificationist. 
%
Annotation mistakes have three main recurring sources, as follows (see, e.g., \cite{ICML-2020}\footnote{ \url{https://arxiv.org/abs/2005.11295} is an extended version.} for an extensive evaluation-based analysis of the problem):

\begin{enumerate}
 \vspace{-0.1cm}
\item \textit{Mislabelled Images} (MI), where the flaw arises because of mislabelling. 

\item \textit{Multi-Object Images} (MOI), where the flaw arises from the occurrence of multiple objects in the same image and the consequent possible non-correspondence between the classificationist and the classifier assigned labels.

\item \textit{Single-Object Images} (SOI), where the flaw arises from an ambiguity in how to label the single object in the image.
 \vspace{-0.1cm}
\end{enumerate}
%
\textit{Mislabelled images} are an extreme case of SGP many-to-many mapping, and are motivated by factors such as carelessness, missing knowledge or excessive speed, that is, all well-known crowdsourcing problems, see for instance \cite{daniel2018quality}. A concrete example is in Fig.\ref{I1}-(III), wherein Image \#383, labelled as \emph{Acoustic Guitar}, actually depicts a birthday cake. 

\vspace{0.1cm}
\noindent
\textit{Multi-Object Images} (Fig.\ref{I1}-(I)) constitute more than one-fifth of ImageNet's total images. Here the source of subjectivity is  the occurrence of multiple objects, coupled with the fact that the objects selected often do not correspond to the most likely main object. One such example is the image of a \emph{Stage} (Image \#193), having ImageNet label \emph{Electric Guitar}. Thus, on one side, empirical evidence from cognitive psychology \cite{ROSCH-1976} makes clear that the main object chosen by humans possesses the highest \emph{`cue validity'}, i.e., the one which is visually the most salient, while, on the other side, many ImageNet labels correspond to \emph{random features} which don't generalize to object recognition in the wild (see \cite{ICML-2020} for more details). 

\vspace{0.1cm}
\noindent
 \textit{Single-Object Images} (Fig.\ref{I1}-(II)), are by far the most important source of annotation mistakes. The main reason is in the underlying interaction between \emph{visual polysemy} \cite{SGP-2000} and \emph{linguistic polysemy} \cite{PWN}. Single-Object images present three types of ambiguity.

\vspace{0.1cm}
\noindent
 \textit{In the first case} (SOII), \textit{the source of subjectivity is intrinsic to the Image itself.} It occurs when an object in an image is \emph{visually polysemic}\footnote{See the definition in \url{https://gradientscience.org/benchmarks/}}, namely when its visual \emph{``semantics is described only partially"} \cite{SGP-2000} and, consequently, its linguistic description is not unique. An example is Image \#251 representing a dreadnought-shaped instrument, which is labelled \textit{Guitar} in Fig.\ref{I1}-(II), but could also be labelled, e.g., \emph{Bass} or \emph{Ukelele}. Notice that images of this type usually encode \textit{a typical partial view} with no possibility of discrimination among different objects. \cite{ICML-2020} observes that not only do humans assign an alternative label for 40\% of the images, but they also assign up to 10 different labels. 

\vspace{0.1cm}
\noindent
\textit{The second source of subjectivity} in single-object images (SOIL) \textit{is intrinsic to the meaning of Labels}. There are two cases. In the case of \textit{polysemic} labels (i.e., of one-to-many mapping between labels and images), different objects, each corresponding to one of the possible meanings, are associated with the same label\cite{giunchiglia2023incremental}. 
One such example is the two images labelled as \textit{Dulcimer} (Image \#262, \#266) in Fig.\ref{I1}-(II)) where the two objects are essentially two different musical instruments, i.e., the first \textit{Dulcimer} being native to the American and the second to the Chinese culture, and are visually very different. The study in \cite{ICML-2020} highlights how these ambiguous classes generate overlaps in the image distributions with a negative impact on performance. The second case labels which are \textit{synonyms} (i.e., of many-to-one mapping between labels and images) wherein a single set of images, will wrongly be split into two sets. One such example is that of \emph{Koto} which can also be synonymously labelled as, for instance, a \emph{Kin} or a \emph{Jusangen}\cite{2023-iConf}.

\vspace{0.1cm}
\noindent
\textit{The third source} of subjectivity in single-object images (SOIA) \textit{relates to the Annotator incomplete background knowledge}. Thus, for instance, Image \#257 is labelled as \textit{Guitar} in Fig.\ref{I1}-(II), while the image depicts an \emph{Acoustic Guitar}.
This type of generic label appears whenever correct labelling requires deep domain specific knowledge. \cite{2019-CVPR} discusses the bias which arises because of this problem in the case of minority \textit{languages}, \textit{cultures}, and also with images about \textit{low income} people. 
Notice that this is not mislabelling. The issue is that the label captures some but not all of the visual properties of the object. 


Overall, the sources of annotation mistakes can be summarized by saying that MOI images generate a one-to-many mapping from images to labels, those of type SOIA generate a many-to-one mapping, while those of type MI, SOII, and SOIL generate both types of mappings. These four types of mistakes are independent of one another and can, therefore, occur together in the same image, thus generating multiple occurrences of the SGP many-to-many mapping problem. 
 