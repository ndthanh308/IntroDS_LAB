

As from Section \ref{I1}, all the experiments focus on ImageNet.   We focus only on the annotator choices C1, C2, where choices C3, C4 are automatically enforced by the fact that we use ImageNet as background knowledge. To this extent, in Choice C2, we assume the visual differentia to be as defined by the Imagenet gloss.
%
We perform four evaluations.  In the first, we evaluate  the inter-annotator agreement, as a proxy of the decrease of the role of subjective choices. We consider both expert and non-expert annotators. In the second we evaluate the annotation cost.
In the third, we evaluate the increase of accuracy generated on various state-of-the-art ML model. The last experiment is an ablation study.


\subsection{Inter-annotator agreement}
\label{label.2}

We selected 450 of the 3660 images populating the ImageNet categories in Fig.\ref{I0}, 50 images per category. To minimize data bias, these 450 labelled image dataset, let us call GT1 (where GT stands for \textit{Ground Truth}) was randomly generated. 
Then, a second dataset GT2 was generated by an expert annotator EA$_1$ by relabelling the 450 images of GT1 using  Choice C2 only, as described in Section \ref{S4}. We then selected 16 volunteer \textit{non-expert annotators}, with no previous annotation experience. The annotators were university students and collaborators, with an average age of 29 years. To maximize diversity, we selected people from different disciplines and from 3 continents and 7 countries. The annotators were organised in 2 groups of 8 people, each annotator relabelling GT1 and generating a dataset as follows:

\begin{itemize}
\item GT3, as created by each member of Group 1 The annotators were 
asked to perform the annotation in two steps. First, they had to annotate images based on Choice C2.  Here they were allowed to create a new category ``\textit{Discharged}" for all the images they could not classify. Then, in the second step, they were asked to label the categories generated in the first step S2 with their ``most suitable" label, not necessarily an ImageNet label (which was not provided). 


\item GT4, as created by each member of Group 2. This group was instructed to annotate images using the ImageNet labels plus the label ``Discharged".
\end{itemize}
%
The motivation for focusing EA$_1$ and Group 1 on Choice C2 was that of highlighting the use of the differentia in place of the label.
The results are reported in Table \ref{tab:3}. Notice how this table reports the number of images associated with each label for all four datasets and for each annotator working on GT3 and GT4. As it can be seen, the number of images (and the images) associated with the same label are different. We measure the inter-annotator agreement of GT3 and GT4 using \textit{Krippendorff's alpha}  \cite{k-alpha,artstein2017inter} (last row).
%
 Krippendorff's alpha is $0.5973$ for GT3 and $0.5047$ for GT4, that is, GT3 is around 18\% higher than GT4. This is an important improvement but still not good enough.\footnote{As from \cite{hughes2021krippendorffsalpha}[Table 2, p. 417], a value of alpha in the range $(0.6, 0.8]$  indicates substantial agreement while an index in the range $(0.8, 1]$  indicates near-perfect agreement.} The cause of this, not fully satisfactory, result was the Multi-object images which are more than 58\% of the total (2125 out of 3660 images). 
%
We have therefore performed a second experiment where Choice C1, C2 were enforced and where MOI images were proposed multiple times with a bounding box around the object to be annotated. This experiment was performed by Group 1 (generating eight datasets GT2*) and two expert annotators EA$_1$, EA$_2$ (generating two datasets GT3*).
Table \ref{tab:6} reports the results obtained. Here, Krippendorff's alpha goes up to $0.7595$ among non-expert annotators and $0.9877$ between the two experts, namely up to near-perfect agreement. 

An interesting  final observation is that in Group 2, but not in Group 1, 
some annotators were unable to annotate some images (see, e.g., the ``0"s in Table \ref{tab:3}). Two such examples are   ``Dulcimer" and ``Koto".\footnote{Koto is a Japanese instrument.}
%
This observation is also confirmed in Table \ref{tab:4}, which reports some example labels provided during the second part of the Group 1 experiment. The observation here is that, despite properly labeling images via differentia, some annotators did not know the names of some categories, or used wrong or more generic labels, or in one case used a more specific label. 




\begin{table*}[h]
\centering
\setlength{\abovecaptionskip}{0pt}%    
\setlength{\belowcaptionskip}{0pt}%
	\resizebox{1\linewidth}{!}{%
\begin{tabular}{|l|cccccccccccccccc|}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{AlexNet}} & \multicolumn{2}{c|}{\textbf{ZFNet}} & \multicolumn{2}{c|}{\textbf{VGG}}      & \multicolumn{2}{c|}{\textbf{GoogleNet}} & \multicolumn{2}{c|}{\textbf{ResNet}}   & \multicolumn{2}{c|}{\textbf{DenseNet}} & \multicolumn{2}{c|}{\textbf{RAN}}     & \multicolumn{2}{c|}{\textbf{SENets}} 
\\ \cline{2-17} 
  & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}}  & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}} & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}}     & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro(\%)}} & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}}   & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}} & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}}   & \textbf{Acc.} & \multicolumn{1}{c|}{\textbf{Impro.(\%)}} \\ \hline
                                  
\textbf{ImageNet}                 & 0.543   & \multicolumn{1}{c|}{-}  & 0.612  & \multicolumn{1}{c|}{-} & 0.655    & \multicolumn{1}{c|}{-}  & 0.734     & \multicolumn{1}{c|}{-}  & 0.593    & \multicolumn{1}{c|}{-}  & 0.724   & \multicolumn{1}{c|}{-}   & 0.713    & \multicolumn{1}{c|}{-} & 0.728              & -           \\
\textbf{only Chioce C1}     & 0.551   & \multicolumn{1}{c|}{1.47\%}  & 0.623  & \multicolumn{1}{c|}{1.80\%} & 0.672 & \multicolumn{1}{c|}{2.55\%}  & 0.745  & \multicolumn{1}{c|}{1.55\%}  & 0.610 & \multicolumn{1}{c|}{2.86\%}  & 0.733   & \multicolumn{1}{c|}{1.24\%}   & 0.718  & \multicolumn{1}{c|}{0.74\%} & 0.746          & 2.54\%           \\
\textbf{only Chioce C2}       & 0.583   & \multicolumn{1}{c|}{7.37\%}  & 0.644  & \multicolumn{1}{c|}{5.23\%} & 0.732 & \multicolumn{1}{c|}{11.82\%} & 0.829     & \multicolumn{1}{c|}{12.94\%} & 0.729    & \multicolumn{1}{c|}{22.93\%} & 0.782   & \multicolumn{1}{c|}{8.01\%}   & 0.770 & \multicolumn{1}{c|}{8.04\%} & 0.807           & 10.87\%          \\
\textbf{Choice C1 \& C2 (vTelos)}                   & 0.596   & \multicolumn{1}{c|}{9.76\%}  & 0.657  & \multicolumn{1}{c|}{7.35\%} & 0.743    & \multicolumn{1}{c|}{13.44\%} & 0.835     & \multicolumn{1}{c|}{13.76\%} & 0.732    & \multicolumn{1}{c|}{23.44\%} & 0.793   & \multicolumn{1}{c|}{9.53\%}   & 0.784    & \multicolumn{1}{c|}{9.96\%} & 0.811              & 11.40\%          \\ \hline
\end{tabular}}
\vspace{0.2cm}
\caption{Ablation study. ``Acc."  is the accuracy, ``Impro.(\%)"  is the improvement in performance over ImageNet.} 
\label{tab:7}
\end{table*}



\begin{table}
\centering
\setlength{\abovecaptionskip}{2pt}%    
\setlength{\belowcaptionskip}{3pt}%
\resizebox{1\linewidth}{!}{%
\begin{tabular}{|l|cc|c|}
\hline
\multirow{2}{*}{\textbf{Methods}} & \multicolumn{2}{c|}{\textbf{Accuracy}}     &   \multirow{2}{*}{\textbf{Improvement}}            \\ \cline{2-3} 
                                 & \textbf{ImageNet} & \multicolumn{1}{c|}{\textbf{vTelos}}  & \\ \hline
AlexNet \cite{alexnet}           & 0.543             & 0.596            & 9.76\%   \\
ZFNet \cite{zfnet}               & 0.612             & 0.657            & 7.35\%   \\
VGG \cite{vgg}                   & 0.655             & 0.743            & 13.44\%   \\
GoogleNet \cite{googlenet}       & 0.734             & 0.835            & 13.76\%   \\
ResNet \cite{resnet}             & 0.593             & 0.732            & 23.44\%  \\
DenseNet \cite{densenet}         & 0.724             & 0.793            & 9.53\%  \\
RAN \cite{ran}                   & 0.713             & 0.784            & 9.96\%   \\
SENets \cite{senet}              & 0.728             & 0.811            & 11.40\%   \\ \hline
\end{tabular}}
\vspace{0.2cm}
\caption{Classification results for ImageNet and  \texttt{vTelos}.}
\label{tab:5}
\end{table}

 
\subsection{Annotation cost}

As from Section \ref{S4}, \texttt{vTelos} requires that each image is recursively compared with the node one level down in the lexico-semantic hierarchy till it meets a differentia which it does not satisfy. So the question arises of what is the cost of the multiple checks. The preliminary observation is that, while containing an excess of 100K concepts, WordNet is a broad and flat hierarchy whose paths have a depth in the range of around 7 to 15 nodes (with different depths for different versions) and where the nodes higher in the hierarchy are very abstract and have no visual content. %In particular, the number of nodes in the top three layers is less than 20. 
A 4-node deep hierarchy, like the one in Fig. \ref{I0}, is therefore a good proxy for evaluating the annotation cost.
%
We have therefore compared the time consumed by two non-expert annotators in the generation of two GT3 and GT4-like datasets restricted to contain one hundred images. The results show that the average annotation time per image is 5.565 seconds for GT3 and 3.473 seconds for GT4, that is, a time increase for \texttt{vTelos} of around 60\% which one would guess it would become a little more than double with 8-node level deep hierarchies. The time increase correlates to the images classified in intermediate nodes. The fact that it is a low increase seems to correlate to the fact that each single choice is simpler given that: (i) the same image is seen more than once, (ii) the choice is restricted within a precise set of labels, (iii) which have similar meanings, because selected following the pattern induced by the tree structure, thus ultimately improving the automation of the process. 

\subsection{Machine Learning Accuracy}
\label{label.3}

We applied eight state-of-the-art ML methods trained on two datasets, called below ``ImageNet"  ``\texttt{vTelos}, the first containing the original  3660 images, the second containing the same 4605 
images where MOI images are transformed in SOI images. 80\% of the images were used as training set and the remaining 20\% were the test set.

All the experiments have been implemented with PyTorch with identical settings. During the  training,  we performed the same data augmentation (horizontal flipping and random scaling with multiple times) on each dataset and randomly sampled $224 \times 224$ crops from augmented images. All experiments have been optimized using Adaptive Moment Estimation %\cite{kingma2014adam}, 
with learning rate 
%\cite{zulkifli2018understanding} 
initialized to $0.0002$, momentum initialized %\cite{sutskever2013importance} 
to $0.9$, and weight decay 
%\cite{loshchilov2018fixing} 
to $10^{-8}$. All models have been pre-trained on ImageNet with ResNet101 \cite{resnet}. 
The results are reported in Table~\ref{tab:5}. It can be noticed that all models trained on \texttt{vTelos} get significant improvement in accuracy, especially ResNet which achieves up to 23.44\% improvement.


Finally, if we look at the heat maps in 
Fig.~\ref{heat-map}, we can see how the models  pay attention to the visual regions described by the linguistic differentia, see for instance the keyboard in the case of the label \textit{Keyboard Instrument}.


\subsection{Ablation study}

We conducted two ablation studies, the first focusing on Choice C1, the second of Choice C2 for all the eight systems tested in the previous experiment. The results are reported in Table \ref{tab:7} where the first and the last row replicate the results in Table \ref{tab:5}.
%
The key observation is that, in all cases, the improvement in performance due to Choice C2 with respect to that due to Choice C1 is substantial (e.g., 5 times with AlexNet, 2.9 times with ZFNet, 13.5 times with RAN, 8 times with ResNet) thus providing further confirmation of the intuition suggested by the heat maps in Fig. \ref{heat-map}. Taking into account  the relevance of Choice C1 which transforms Multi-Object Images into Single-Object images, we take this as an important indicator of the fact that \textit{labeling images using differentia labels is the way to go.}


% Figure environment removed



