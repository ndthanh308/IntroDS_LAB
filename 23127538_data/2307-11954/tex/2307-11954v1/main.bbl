\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@rmstyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand\BIBentrySTDinterwordspacing{\spaceskip=0pt\relax}
\providecommand\BIBentryALTinterwordstretchfactor{4}
\providecommand\BIBentryALTinterwordspacing{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand\BIBforeignlanguage[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu, ``Asynchronous methods for deep reinforcement learning,''
  in \emph{International conference on machine learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2016, pp. 1928--1937.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{yu2017preparing}
W.~Yu, J.~Tan, C.~K. Liu, and G.~Turk, ``Preparing for the unknown: Learning a
  universal policy with online system identification,'' \emph{arXiv preprint
  arXiv:1702.02453}, 2017.

\bibitem{peng2018sim}
X.~B. Peng, M.~Andrychowicz, W.~Zaremba, and P.~Abbeel, ``Sim-to-real transfer
  of robotic control with dynamics randomization,'' in \emph{2018 IEEE
  international conference on robotics and automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2018, pp. 3803--3810.

\bibitem{katt2022baddr}
S.~Katt, H.~Nguyen, F.~A. Oliehoek, and C.~Amato, ``Baddr: Bayes-adaptive deep
  dropout rl for pomdps,'' in \emph{Proceedings of the 21st International
  Conference on Autonomous Agents and Multiagent Systems}, 2022, pp. 723--731.

\bibitem{ross2011bayesian}
S.~Ross, J.~Pineau, B.~Chaib-draa, and P.~Kreitmann, ``A bayesian approach for
  learning and planning in partially observable markov decision processes.''
  \emph{Journal of Machine Learning Research}, vol.~12, no.~5, 2011.

\bibitem{ong2009pomdps}
S.~C. Ong, S.~W. Png, D.~Hsu, and W.~S. Lee, ``Pomdps for robotic tasks with
  mixed observability.'' in \emph{Robotics: Science and systems}, vol.~5, 2009,
  p.~4.

\bibitem{katt2017learning}
S.~Katt, F.~A. Oliehoek, and C.~Amato, ``Learning in pomdps with monte carlo
  tree search,'' in \emph{International Conference on Machine Learning}.\hskip
  1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 1819--1827.

\bibitem{katt2019bayesian}
------, ``Bayesian reinforcement learning in factored pomdps,'' in
  \emph{Proceedings of the 18th International Conference on Autonomous Agents
  and MultiAgent Systems}, 2019, pp. 7--15.

\bibitem{silver2010monte}
D.~Silver and J.~Veness, ``Monte-carlo planning in large pomdps,''
  \emph{Advances in neural information processing systems}, vol.~23, 2010.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting,''
  \emph{The journal of machine learning research}, vol.~15, no.~1, pp.
  1929--1958, 2014.

\bibitem{gu2017deep}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine, ``Deep reinforcement learning for
  robotic manipulation with asynchronous off-policy updates,'' in \emph{2017
  IEEE international conference on robotics and automation (ICRA)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2017, pp. 3389--3396.

\bibitem{singh2019end}
A.~Singh, L.~Yang, K.~Hartikainen, C.~Finn, and S.~Levine, ``End-to-end robotic
  reinforcement learning without reward engineering,'' \emph{arXiv preprint
  arXiv:1904.07854}, 2019.

\bibitem{zhan2020framework}
A.~Zhan, P.~Zhao, L.~Pinto, P.~Abbeel, and M.~Laskin, ``A framework for
  efficient robotic manipulation,'' \emph{arXiv preprint arXiv:2012.07975},
  2020.

\bibitem{oord2018representation}
A.~v.~d. Oord, Y.~Li, and O.~Vinyals, ``Representation learning with
  contrastive predictive coding,'' \emph{arXiv preprint arXiv:1807.03748},
  2018.

\bibitem{wang2022robot}
D.~Wang, M.~Jia, X.~Zhu, R.~Walters, and R.~Platt, ``On-robot learning with
  equivariant models,'' in \emph{Conference on robot learning}, 2022.

\bibitem{zhu2022sample}
X.~Zhu, D.~Wang, O.~Biza, G.~Su, R.~Walters, and R.~Platt, ``Sample efficient
  grasp learning using equivariant models,'' \emph{arXiv preprint
  arXiv:2202.09468}, 2022.

\bibitem{smith2022walk}
L.~Smith, I.~Kostrikov, and S.~Levine, ``A walk in the park: Learning to walk
  in 20 minutes with model-free reinforcement learning,'' \emph{arXiv preprint
  arXiv:2208.07860}, 2022.

\bibitem{wu2022daydreamer}
P.~Wu, A.~Escontrela, D.~Hafner, K.~Goldberg, and P.~Abbeel, ``Daydreamer:
  World models for physical robot learning,'' \emph{arXiv preprint
  arXiv:2206.14176}, 2022.

\bibitem{kaelbling1998planning}
L.~P. Kaelbling, M.~L. Littman, and A.~R. Cassandra, ``Planning and acting in
  partially observable stochastic domains,'' \emph{Artificial Intelligence},
  vol. 101, no. 1-2, pp. 99--134, 1998.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement Learning: An
  Introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{robbins1951stochastic}
H.~Robbins and S.~Monro, ``A stochastic approximation method,'' \emph{The
  annals of mathematical statistics}, pp. 400--407, 1951.

\bibitem{gal_dropout_2016}
Y.~Gal and Z.~Ghahramani, ``Dropout as a {B}ayesian approximation:
  {Representing} model uncertainty in deep learning,'' in \emph{International
  conference on machine learning}, 2016, pp. 1050--1059.

\bibitem{diankov2008openrave}
R.~Diankov and J.~Kuffner, ``Openrave: A planning architecture for autonomous
  robotics,'' \emph{Robotics Institute, Pittsburgh, PA, Tech. Rep.
  CMU-RI-TR-08-34}, vol.~79, 2008.

\bibitem{sucan2012open}
I.~A. Sucan, M.~Moll, and L.~E. Kavraki, ``The open motion planning library,''
  \emph{IEEE Robotics \& Automation Magazine}, vol.~19, no.~4, pp. 72--82,
  2012.

\bibitem{quigley2009ros}
M.~Quigley, K.~Conley, B.~Gerkey, J.~Faust, T.~Foote, J.~Leibs, R.~Wheeler,
  A.~Y. Ng, \emph{et~al.}, ``Ros: an open-source robot operating system,'' in
  \emph{ICRA workshop on open source software}, vol.~3, no. 3.2.\hskip 1em plus
  0.5em minus 0.4em\relax Kobe, Japan, 2009, p.~5.

\bibitem{hausknecht2015deep}
M.~Hausknecht and P.~Stone, ``Deep recurrent q-learning for partially
  observable mdps,'' in \emph{2015 aaai fall symposium series}, 2015.

\bibitem{ma2020discriminative}
X.~Ma, P.~Karkus, D.~Hsu, W.~S. Lee, and N.~Ye, ``Discriminative particle
  filter reinforcement learning for complex partial observations,'' \emph{arXiv
  preprint arXiv:2002.09884}, 2020.

\bibitem{hafner2020mastering}
D.~Hafner, T.~Lillicrap, M.~Norouzi, and J.~Ba, ``Mastering atari with discrete
  world models,'' \emph{arXiv preprint arXiv:2010.02193}, 2020.

\bibitem{hessel2018rainbow}
M.~Hessel, J.~Modayil, H.~Van~Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver, ``Rainbow: Combining improvements
  in deep reinforcement learning,'' in \emph{Proceedings of the AAAI conference
  on artificial intelligence}, vol.~32, no.~1, 2018.

\bibitem{guo2014deep}
X.~Guo, S.~Singh, H.~Lee, R.~L. Lewis, and X.~Wang, ``Deep learning for
  real-time atari game play using offline monte-carlo tree search planning,''
  \emph{Advances in neural information processing systems}, vol.~27, 2014.

\end{thebibliography}
