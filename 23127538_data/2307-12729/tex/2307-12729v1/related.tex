\textbf{Human-object interaction} in videos is traditionally formulated
for predictions of human action and object affordance labels \cite{liu2020forecasting,nagarajan2019grounded,ji2021detecting,escorcia2013spatio}.
Approaches include modeling the activity continuity with CNNs \cite{liu2020forecasting}
and RNNs \cite{nagarajan2019grounded}, or modeling the interactions
using GNNs \cite{jain2016structural,ghosh2020stacked}, which can
be improved with dynamic topology \cite{morais2021learning,qi2018learning}.
Recent works also leverage the power of transformers \cite{vaswani2017attention}
to model the spatio-temporal relationship between humans and objects
in the videos \cite{ji2021detecting}. In this paper, we consider
human-object interaction motion (HOI-M) prediction, the task of forecasting
the concrete future locations of humans and objects in an activity
which is more well-defined and easier to evaluate than the vague affordance
labels traditionally used in HOI. Also, this task is more complex
and requires more precise modeling structures, such as the duality
proposed in this work.

\textbf{Human body motion} predictions are done with MLPs \cite{bouazizi2022motionmixer,guo2023back},
RNNs \cite{fragkiadaki2015recurrent,ghosh2017learning,pavllo2018quaternet,aksan2019structured,zhang2021we}
or GNNs \cite{jain2016structural,cui2021towards,li2020dynamic,cui2020learning,dang2021msr,mao2019learning,sofianos2021space,ma2022progressively,liu2021motion}
and can be embedded in generative models \cite{gui2018adversarial,hernandez2019human,cui2021efficient,kundu2019bihmp}.
More recent works started to consider relations with surroundings
in the form of intention toward destinations \cite{cao2020long} or
interaction with other entities \cite{corona2020context,adeli2021tripod}.
We advance this line by considering the directional relations between
human and objects beyond the simplistic homogenous graphs such as
in \cite{corona2020context} through a new multi-mechanism adaptive
structural of the persistent-transient duality. 


