% Figure environment removed


\subsection{Preliminaries}

We consider the problem of modeling the sequential motions of $N$
entities of humans and objects, where each entity $i$ is represented
by a class label $c_{i}$ and sequential visual features $X_{i}=\left\{ x_{i}^{t}\right\} _{t=1}^{T}$.
Given the class labels and the sequential features, we want to predict
the future motions in the next $L$ steps, $Y_{i}=\left\{ y_{i}^{t}\right\} _{t=T+1}^{T+L}$.
Each time instance of features $x_{i}^{t}$ of humans contains skeleton
positions; while those of objects include their object types and bounding
boxes. 

This problem is traditionally approached under the single-mechanism
assumption that the inference dynamics stay the same during the whole
session \cite{corona2020context}. In this section, we present our
new multi-mechanism paradigm, modeled with the persistent-transient
duality, to capture how humans maintain their global operation mode
while swiftly adapt to emerging local interaction situations. 

\subsection{The Persistent-Transient Duality }

We consider the motions of the human bodies and the interacted objects
to be caused by two mechanisms: 1. the navigation according to the
global activity progress; and 2. the individual interactions with
objects, which happens in the local contexts. This multi-mechanism
factorization can be viewed as a persistent-transient process duality:
The \emph{persistent process} operates in large activity temporal
scale and with a global spatial perspective. Its counterpart, the
\emph{transient process,} is local in time and egocentric to each
human subject's spatial viewpoint.

We model this duality by a hierarchical neural network called \emph{Persistent-Transient
Duality Networks (PTD)} whose architecture is drawn in \ref{fig:overal_architecture}.
Within PTD, the \emph{Persistent process} is modeled as a single \emph{Persistent
channel}, which operates recurrently along the whole activity sequence.
Its children, the \emph{Transient channel} instances, are initiated
with personalized structures and representations whenever a human
has a new interaction with the surrounding. When the interaction is
over, the outdated transient channel is terminated, and the control
is returned to the persistent channel waiting in the background. These
transient life cycles are managed by the neural modules called \emph{Transient
Switches}. 

\subsection{Persistent Channel\label{subsec:Persistent-Channel}}

The Persistent channel models the common global view of all humans
and objects in the scene. It takes the form of a heterogeneous graph
attention network \cite{wang2019heterogeneous} with two node types
corresponding to human and object entities and \emph{dense spatial
edges} connecting entities at the same time step. We extend this model
by adding \emph{recurrent temporal edges} along the temporal dimension
for each entity:
\begin{equation}
z_{i}^{t}=\left[x_{i}^{t},m_{i}^{t},m_{i,\mathcal{T}\rightarrow\mathcal{P}}^{t}\right],\hspace{3mm}h_{i}^{t}=\text{RNN}_{c_{i}}\left(z_{i}^{t},h_{i}^{t-1}\right),\label{eq:persistent_rnn}
\end{equation}
where $\text{RNN}_{c_{i}}$ is a recurrent unit (e.g., GRU) that corresponds
to the class of the $i^{\text{th}}$ entity. The input $z_{i}^{t}$
of $\text{RNN}_{c_{i}}$ is formed using the the entity's intrinsic
features $x_{i}^{t}$ (skeleton for humans and bounding boxes for
objects), the spatial messages $m_{i}^{t}$ gathered from spatial
edges, and a message from the active transient channel $m_{i,\mathcal{T}\rightarrow\mathcal{P}}^{t}$
(\ref{subsec:transient_process}). 

The spatial message $m_{i}^{t}$ in \ref{eq:persistent_rnn} is aggregated
from the \emph{spatial edges} of the graph via attention: $m_{i}^{t}=\text{Attn}\left(u_{i}^{t},\left\{ u_{j}^{t}\right\} _{j\neq i}\right)$
with $u\_^{t}=\left[x\_^{t};h\_^{t-1}\right]$. Here and throughout
this work, we use the GAT-based attention function \cite{velivckovic2017graph},
$\text{Attn}\left(q,V\right)$, defined over the query $q$ and the
identical key/value pairs $V=\{v_{j}\}_{j=1}^{N}$ by:\vspace{-2mm}
\begin{equation}
\text{Attn}\left(q,V\right)\coloneqq\sum_{j=1}^{N}\text{softmax}_{j}\left(\sigma\left(\text{MLP}\left(\left[\boldsymbol{W_{q}}q;\boldsymbol{W}_{v}v_{j}\right]\right)\right)\right)\boldsymbol{W}_{v}v_{j},\label{eq:attention_function}
\end{equation}
where $\boldsymbol{W_{q}}$ and $\boldsymbol{W}_{v}$ are the learnable
embedding weights for the query and key/value, $\left[\cdot;\cdot\right]$
is the concatenation, $\sigma\left(\cdot\right)$ is a non-linear
activation.

The Persistent channel generates two outputs from its hidden state:
the message to the transient channel of each human $m_{i,\mathcal{P}\rightarrow\mathcal{T}}^{t}$
and the future position of each entity $\hat{y}_{i,\mathcal{P}}^{t}$:\vspace{-2mm}

\begin{equation}
m_{i,\mathcal{P}\rightarrow\mathcal{T}}^{t}=\text{MLP}\left(h_{i}^{t}\right),\qquad\hat{y}_{i,\mathcal{P}}^{t}=\text{MLP}\left(h_{i}^{t}\right).\label{eq:persistent_output}
\end{equation}
These prediction outputs are later combined with those from the Transient
channel as detailed in \ref{subsec:Future-prediction}.

\subsection{Transient Channel \label{subsec:transient_process}}

Operating with a consistent graph, the Persistent process alone cannot
adapt quickly enough to emerging events that require different perspectives,
particularly when the human interacts with an object. When these cases
are detected, PTD initiates a \emph{Transient process} that (1) zooms
in to the relevant context and (2) take the viewpoint of the subject. 

In our framework, the Transient process is implemented by a neural
\emph{Transient channel}, available one for each \emph{human} entity\emph{.}
We propose an \emph{egocentric recurrent graph networks} for the Transient
channel. The egocentric property of this model is the most important
aspect that separates it from the global view of its parent persistent
channel. This egocentric design reflects in three aspects of \emph{computational
structure, feature representation}, and \emph{inference logic}. 

\paragraph{Egocentric computational structure}

In switching from the global to personalized view, we start by forming
an\emph{ egocentric Transient graph} $\mathcal{G}_{i}^{t}=\left(\mathcal{V}_{i}^{t},\mathcal{E}_{i}^{t}\right)$
at time $t$ of the human $i$ and their relations with the objects
interacted with. For a particular human, subscript $i$ will be omitted
for conciseness. The egocentric characteristic of $\mathcal{G}^{t}$
reflects in its star-like structure: the nodes $\mathcal{V}^{t}$
includes a single \emph{center node $r$} for the considering human,
and \emph{leaf nodes} of indices $\left\{ l\right\} _{l\neq r}$ for
objects. The dynamic edges $\mathcal{E}^{t}$ connect the center with
the leaves in two directions: \emph{inward edges }$e_{l\rightarrow r}^{t}$
reflect which objects the human may consider to interact with, and
the \emph{outward edges} $e_{r\rightarrow l}^{t}$ represents the
objects are being manipulated by the human. In our implementation,
the existences of the edges are determined by thresholding the center-leaf
distances $d_{lr}^{t}$:
\begin{equation}
\begin{cases}
e^{t}\_\in\mathcal{E}^{t} & \text{if }d_{lr}^{t}\leq\beta\_\\
e^{t}\_\notin\mathcal{E}^{t} & \text{otherwise}
\end{cases},\label{eq:ego_graph_structure}
\end{equation}
where $\_$ indicates either the inward or onward, $\beta\_$ is a
pair of threshold hyper-parameters. This thresholding effectively
thins out the neighbors, making the graph more efficient and localized
around the center node. Inward threshold is commonly smaller than
outward one, because humans subject's attention, hence motion, are
affected by objects before and after objects get directly manipulated.
The edges are estimated dynamically for each time step, allowing the
graph's topology to evolve within one single Transient session. 

% Figure environment removed


\paragraph{Egocentric representation.}

% Figure environment removed

Paired with the egocentric graph structure, the geometrical features
of the entities are also transformed into the egocentric coordinate
system corresponding to the viewpoint of the human center node $r$:
\begin{align}
\bar{x}\_^{t} & =f_{\text{ego}}\left(x\_^{t},x_{r}^{t}\right)=x\_^{t}-\text{centroid}(x_{r}^{t}),\label{eq:egocentric_leaf}
\end{align}
where $\_$ indicate both center or leaf nodes. Conceptually, this
change of system puts various patterns of the human's motion into
the same aligned space. In the global view, features of two similar
actions can be vastly different if they are far away. After this transformation,
they are aligned into the common egocentric view of the center human
subject. This alignment filters out the irrelevant global information,
and facilitates efficient inference of the egocentric model.

\paragraph{Egocentric inference.}

Operating on the transient graph structure, the RNN hidden state of
each node $s_{-}^{t}$ is updated with the input aggregated through
attention-based message passing along the edges in $\mathcal{E}^{t}$.
See \ref{fig:transient-channel} for an illustration.

In detail, for the \emph{center node}, the inward messages from its
leaves are aggregated into: $m_{r}^{t}=\text{Attn}\left(\begin{array}{l}
u_{r}^{t},\left\{ u_{l}^{t}\right\} _{e_{l\rightarrow r}^{t}\in\mathcal{E}^{t}}\end{array}\right)$, where $u^{t}\_=\left[\bar{x}\_^{t};s\_^{t-1}\right]$ and $\text{Attn}\left(q,V\right)$
is defined in \ref{eq:attention_function}. This message is combined
with the egocentric features $\bar{x}_{r}^{t}$ and the persistent
channel's message $m_{\mathcal{P}\rightarrow\mathcal{T}}^{t}$ (\ref{eq:persistent_output})
to update the recurrent state $s_{r}^{t}$:
\begin{equation}
s_{r}^{t}=\text{RNN}_{r}\left(\left[\bar{x}_{r},m_{r}^{t},m_{\mathcal{P}\rightarrow\mathcal{T}}^{t}\right],s_{r}^{t-1}\right).\label{eq:center_gru}
\end{equation}

For\emph{ leaf nodes}, when they are interacted by the center node
(indicated via $e_{r\rightarrow l}^{t}$), they receive a center-broadcasted
message $m_{l}^{t}=\text{MLP}\left(\left[\bar{x}_{r}^{t};s_{r}^{t-1}\right]\right)$
and update their state $s_{l}^{t}$:

\begin{equation}
s_{l}^{t}=\begin{cases}
\text{\text{RNN}}_{l}\left(\left[\bar{x}_{l}^{t};m_{l}^{t}\right],s_{l}^{t-1}\right) & \textrm{if }e_{r\rightarrow l}^{t}\in\mathcal{E}^{t}\\
s_{l}^{t-1} & \text{otherwise}
\end{cases},\label{eq:local_leaf_hidden_states}
\end{equation}

The updated hidden states are used to generate the messages sent to
the persistent channel $m_{\mathcal{T}\rightarrow\mathcal{P}}^{t}$
and the transient predictions about the future position $\hat{y}_{-}^{\mathcal{T},t}$
of the entities:\vspace{-2mm}

\begin{align}
m_{\mathcal{T}\rightarrow\mathcal{P}}^{t}=\text{MLP}\left(s_{r}^{t}\right),\quad\hat{y}_{-,\mathcal{T}}^{t} & =f_{\text{ego}}^{-1}\left(\text{MLP}\left(s_{-}^{t}\right)\right),\label{eq:transient-readout}
\end{align}
where $f_{\text{ego}}^{-1}$ is the inverse function of \ref{eq:egocentric_leaf}
converting the egocentric back to global coordinates. The Transient
predictions $\hat{y}_{-}^{\mathcal{T},t}$ are then combined with
the Persistent counterparts as described in \ref{subsec:Future-prediction}.
As a key modeling feature, the egocentric design plays a crucial goal
in the power of the PTD which will be demonstrated later in the Ablation
studies (\ref{subsec:HOI-Ablation}).

\subsection{Switching Transient Processes \label{subsec:switch}}

The life cycles of the Transient processes are managed based on the
situation of human's activity. These decisions are made by a neural
\emph{Transient Switch} (See \ref{fig:transient-switch}) that makes
switch-on and switch-off decisions of the Transient process for each
human entity. The switch-on probability $p_{r}^{t}$ is computed by
considering the current hidden state $h_{r}^{t}$ and the input $z_{r}^{t}$
of the persistent RNN in \ref{eq:persistent_rnn}:

\begin{equation}
\hat{p}_{r}^{t}=\gamma_{r}^{t}\cdot\text{sigmoid}\left(\text{MLP}\left(\left[h_{r}^{t};z_{r}^{t}\right]\right)\right),\label{eq:switch_score}
\end{equation}
where $h_{r}^{t}$ and $z_{r}^{t}$ is the current hidden state and
the input of the persistent RNN. The discount factor $\gamma_{r}^{t}\in[0,1]$
responds to subject's distance to the nearest object $d_{r,min}^{t}$:
\begin{equation}
\gamma_{r}^{t}=\exp\left(-\eta\cdot d_{r,min}^{t}\right),\label{eq:discount_factor}
\end{equation}
where $\eta$ is a learnable decay rate. This factor acts as a disruptive
shortcut gate that modulates the switching decision based on the spatial
evidence of the interaction. 

Finally, the binary switch decision is made by thresholding the score
with a learnable threshold $\theta$: the switch is $on$ when $\hat{p}_{r}^{t}>=\theta$,
and is \emph{off} otherwise. When the switch changes from \emph{off}
to \emph{on}, a new Transient process is created for the subject.
It will run until the switch turns \emph{off}, then the persistent
process again becomes the single operator.

\subsection{Future prediction\label{subsec:Future-prediction}}

In PTD, future motions are predicted autoregressively: After running
through the observed sequence $X^{1:T}$, PTD keeps unrolling to predict
the requested $L$ time steps and feeds its prediction back as input
to keep unrolling. 

At each future time step $t$, the predictions from Persistent and
Transient channels $\hat{Y}_{\mathcal{P}}^{t}$, $\hat{Y}_{\mathcal{T}}^{t}$
(\ref{eq:persistent_output}, \ref{eq:transient-readout}) are combined
with the priority on the Transient predictions. For a human entity,
if its Transient channel is activated, the Transient prediction will
be chosen; otherwise, the Persistent prediction will be used. For
an object entity, if it receives an active outward Transient edge,
it will take that channel's prediction. If it receives multiple outward
edges, it uses the prediction from the channel with the highest transient
score $\hat{p}_{r}^{t}$. Otherwise, it uses the persistent prediction
by default.

\subsection{Model Training \label{subsec:losses}}

The model is trained end-to-end with three losses: prediction loss
of humans and objects and switch loss:\vspace{-2mm}

\begin{align}
\mathcal{L} & =\lambda_{h}\mathcal{L}_{pred,h}+\lambda_{o}\mathcal{L}_{pred,o}+\lambda_{switch}\mathcal{L}_{switch}.\label{eq:losses}
\end{align}

The \textbf{Prediction loss} $\mathcal{L}_{\text{pred}}$ measures
the mismatch between predicted values $\hat{Y}$ and ground truth
$Y$, implemented as their Euclidean distance:

\begin{equation}
\mathcal{L}_{\text{pred}}=\left\Vert \hat{Y}^{T+1:T+L}-Y^{T+1:T+L}\right\Vert _{2}.\label{eq:pred_loss}
\end{equation}
Even though the Transient switch can be implicitly trained with the
prediction loss, the gradient flowing through this binary gate can
be weak. To directly supervise the Transient switch, we further introduce
the \textbf{Switch loss}: 

\begin{equation}
\mathcal{L}_{switch}=\text{BCE}\left(\hat{P}^{1:T+L},P^{1:T+L}\right),\label{eq:switch_loss}
\end{equation}
where $\hat{P}^{t}$ and $P^{t}$ are the predicted and ground truth
switch scores (\ref{eq:switch_score}) of all human entities at time
step $t$. 

\paragraph{Setting switch ground truth label}

$P^{t}$ from data is an interesting modeling topic. The term represents
the true moment where the human $r$ turns on their ``Transient mode''.
A simple way to learn it is through self-supervision using a binary
label $q^{t}$ on whether an interaction happens at that time. In
particular, it is determined by the outward edges $e_{r\rightarrow l}^{t}$
in the Transient graph (see \ref{subsec:transient_process}): $q^{t}=1$
if $\exists l:e_{r\rightarrow l}^{t}\in\mathcal{E}^{t}$, and $q^{t}=0$
otherwise.

However, is this the true ground truth to human behavior switch? Humans
usually foresee their interaction and change their behavior before
the observable interaction occurs; therefore, $q^{t}$ is actually
too late to turn on the Transient channel. We resolve this mismatch
by using the future ground truth labels of deviations for the current
label of Transient switch: 

\begin{equation}
p^{t}=q^{t}\vee q^{t+1}\vee...\vee q^{t+\omega},\label{eq:groundtruth_switch_label}
\end{equation}
where $\vee$ indicates 'bit-wise or' operator, and $\omega$ is the
postdating window meta-parameter, whose values are examined in the
Supp, Sec. 5.

\paragraph{Multistage training}

We train our model in two stages: we first use teacher-forcing \cite{lecun2015deep}
to use the ground truth position as the input in the prediction stage,
preventing the model from accumulating errors and facilitate faster
training. Then, in the second stage, we fine-tune the model with the
unrolling mechanism introduced in \ref{subsec:Future-prediction}. 

This way of training can be thought of as a curriculum learning technique
\cite{bengio2009curriculum} used in previous works \cite{adeli2021tripod,jain2016structural}
where we initially train the model with an easy problem, then increase
the difficulty of the problem in the later epochs. 

\subsection{Modeling scope and limitations}

The modeling of the PTD in this paper makes an assumption that the
Transient processes of different persons are independent of each other.
Extensions can be made to break this assumption and support more complex
cases, such as collaborative and competitive multi-agent systems. 

The model also assumes that there is a hard temporal border between
the persistent and transient processes, which may be an approximation
as humans sometimes have a mixed-up thinking and acting mechanisms.
Future works may involve allowing the two processes to take over each
other more softly, hence can cover these cases.

The PTD formulation in this section is done particularly for HOI-M
forecasting. Modifications may be necessary for other applications.
Example adaptation for \emph{pedestrian trajectory prediction} is
given in Supp. Sec 9.
