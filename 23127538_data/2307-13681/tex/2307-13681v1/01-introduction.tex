\section{Introduction}

The recent quality surge in multimodal natural language processing (NLP) and vision-language models has enabled new interaction possibilities between images and text~\cite{CLIP,BLIP,ramesh2022hierarchical, saharia2022photorealistic,poole2022dreamfusion,chen2022tango}. 
%
However, the underlying text-to-image models are trained on hundreds of millions of data points collected online, with a bias towards natural images and pictures typically found on the internet, and with general descriptions that do not capture the fine details and rich subtleties that domain-specific applications may require. 
%


In this paper we explore the use of natural language to convey \textit{fine-grained} material appearance. We pose three open key questions: (i) Is there an underlying common lexicon and structure when people describe material appearance using natural language? (ii) Can we communicate material appearance precisely enough with natural language only? (iii) Are language concepts relevant to material appearance well understood by large foundational models~\cite{CLIP, BLIP}?


Although these questions are relevant for any type of vision-language model dealing with material appearance, to make the task tractable we focus on the particular class of \emph{fabrics}. We choose this class since fabrics exhibit a wide variety of looks, patterns and reflectivity properties at different scales, are familiar to everyone, and are  ubiquitous, widely present in many daily scenarios. 


We first build a large dataset, text2fabric, relating photorealistic renderings of digital fabrics covering a wide range of appearance to natural language descriptions provided through crowdsourcing. We then perform a thorough analysis of our data, aimed at improving our understanding of how fabric appearance is described, and find that: (i) there is a common lexicon (ca. 500 words are enough to cover 95\% of the 15,461 valid descriptions gathered); (ii) common properties (attributes) of appearance emerge from the descriptions; (iii) users do follow a certain structure when describing fabric appearance; and (iv) despite the infinite description space provided by natural language, there is a high similarity between descriptions of the same fabric. 
%
All these findings suggest that we have a shared understanding of language as it relates to material (fabric) appearance, which is key to \new{communicating} appearance precisely.

In addition, we leverage two successful and widely used vision-language models ---CLIP~\cite{CLIP} and BLIP~\cite{BLIP}--- 
and show how their performance improves significantly when fine-tuned on our dataset. 
Last, we demonstrate applications of our model for various tasks (see Figure \ref{fig:teaser}), including fine-grained text-based retrieval, image-based search, and automatic description or captioning of fabrics, % material editing
all of them robust in the presence of light and geometry variations.

In summary, we present the following contributions:
\begin{itemize}
    \item A text2fabric dataset including 15,000+ descriptions associated with 3,000 different fabric materials. The dataset is further augmented with 42,000 additional images featuring different geometries and lighting.
    \item A \new{general methodology to collect and analyze natural language data describing images of fabrics, which is} applicable to other domains. 
    \item The identification of a common lexicon, structure and curated set of attributes that are relevant when describing fabrics.
    \item Fine-tuned models demonstrating the benefit of our dataset on several tasks.
\end{itemize}

Our full text2fabric dataset, as well as the fine-tuned models, are made publicly available to facilitate future research\footnote{Project website: \url{https://valentin.deschaintre.fr/text2fabric}}. 
