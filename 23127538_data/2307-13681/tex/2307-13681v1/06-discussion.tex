\vspace{-0.45em}
\section{Discussion and Future Work}
\label{sec:discussion}

We have presented text2fabric, a comprehensive, large-scale public 
dataset relating the visual appearance of fabrics to natural language. We have analyzed and curated a rich lexicon, classifying it into eleven attributes and highlighting key concepts used by humans when describing fabrics. We have further proposed several applications including fine-grained retrieval, image-based search, and caption generation, and shown how foundational, state-of-the-art vision-language models such as CLIP~\cite{CLIP} or BLIP~\cite{BLIP} struggle to represent fine-grained concepts of appearance, unless fine-tuned on our dataset. 


Our work is not free of limitations. First, as in all studies, our results are only strictly valid for our particular set of stimuli; for example, in the fabric samples used to generate our data, military characteristics and some weathering features  are highly correlated, such as camouflage and dirt, as shown in Figure~\ref{fig:dataset:attributes} (right). This leaves the door open for future extensions of our dataset to explore these correlations further. 
%
\new{Second, we decided to choose non-expert describers (albeit familiar with fashion or design), to target a wider audience for our applications, given that experts usually rely on highly specialized concepts, difficult to understand by the general public. This might lead to the descriptions including some inaccuracies (e.g., due to uncertainty in the fabric type), or common misunderstandings about cloth (e.g., confusing ``stitching'' with ``weaving''). While these are a reflection of assumptions and biases from common users, it could be a limitation in certain scenarios, e.g., involving experts.}
%Indeed, the nature of our describers reflects on the type of the provided descriptions, which might be a limitation in certain scenarios.
\new{Additionally,} as expected, 
%our dataset reflects how people describe fabrics with natural language, and our fine-tuned models learn from the data given. As a result, 
some text-based queries are not fully understood by our models, as shown in Figure~\ref{fig:limitations} (top row). Fabric samples are typically not described in terms of \emph{not} having a certain characteristic (people do not say ``this is not a red fabric'' or ``this fabric is not red''); as a result, our fine-tuned models (and their native counterparts) struggle with such queries. Another limitation regarding caption generation occurs in the presence of very complex, intricate designs, where the descriptions produced may fail to capture certain aspects that would be prominent for a human. An example can be seen in Figure~\ref{fig:limitations} (bottom row), where despite the richness of the generated caption, it fails to mention the presence of leopards in the fabric. 

%
% Figure environment removed

Our work opens up exciting avenues for future research, which we describe in the following paragraphs. 

\paragraph{Generalization} An interesting question from our work is the exploration of how well our methodology generalizes beyond fabrics, maintaining a similar intra-class variation description quality. We argue that our methodology can be readily applied to other material datasets and classes, including both data gathering and analysis, which could in turn enable similar applications to the ones described in Section~\ref{sec:applications}. As a proof-of-concept, and without incurring in the cost of gathering a whole new corpus of descriptions, we resort to Adobe Stock~\shortcite{AdobeStock}, a popular service where assets can be searched by class, and are tagged with keywords provided by artists. We gather the keywords corresponding to four material classes (``wood'', ``stone'', ``brick'' and ``metal''), quite different in nature from fabrics; while Adobe Stock does not provide free text descriptions, 
we aim to assess 
to what extent the keywords are well represented by our attributes found in Section~\ref{subsec:data-analysis-attributes}. 

\begin{table}
\caption{Precision (ratio between true positives and predicted positives) of the automatic classification of keywords from other material categories into our generic attributes. 
}
\scriptsize
\begin{tabular}{l|c|c|c|c|c|c|c|c|}
\cline{2-9}
                            & color & lightness & metallic & pattern & touch & use   & weath. & mat\_type \\ \hline
\multicolumn{1}{|l|}{wood}  & 0.71  & 1.00      & 1.00     & 0.68    & 0.75  & 0.84  & 1.00   & 0.89      \\
\multicolumn{1}{|l|}{brick} & 0.72  & 0.80      & 0.67     & 0.59    & 0.68  & 0.83  & 1.00   & 0.75      \\
\multicolumn{1}{|l|}{stone} & 0.52  & 0.95      & 0.65     & 0.52    & 0.75  & 0.85  & 0.83   & 0.95      \\
\multicolumn{1}{|l|}{metal} & 0.53  & 0.47      & 0.70     & 0.68    & 0.75  & 0.88  & 1.00   & 0.67      \\ \hline
\multicolumn{1}{|l|}{avg}   & 0.62  & 0.80      & 0.76     & 0.62    & 0.73  & 0.85  & 0.96   & 0.82      \\ \hline
\end{tabular}
\label{tab:dataset:generalization}
\end{table}

We remove the attributes that are specific to fabrics, namely: \emph{sewing}, \emph{weight} and \emph{military}, and rename \emph{fabric\_type} with the corresponding material category (e.g., \emph{wood\_type}).
%
We then automatically classify keywords from all four classes into the attributes (see Section~\ref{subsec:data-analysis-attributes}). Table~\ref{tab:dataset:generalization} shows precision values for each attribute and class, i.e., how many keywords assigned to the attribute truly belong to it (we obtain the ground truth by manual classification). We see how precision values are reasonably high, suggesting generality of our attributes. The exceptions are \emph{color}, whose low precision is due to the presence in our lexicon of common objects used as colors (e.g., olive), and \emph{pattern}, probably due to the very general nature of this attribute. While this is a very preliminary analysis, we believe it hints at the generalization capabilities of our methodology and derived attributes, and may inspire future work in this direction. 

Another interesting avenue of research is exploring generalization \textit{beyond} material categories, such as video or meshes. Besides, since our methodology lets us relate synthetic graphics primitives to natural language, we are then free to use our primitives under arbitrary conditions, for example adapting them to a specialized context such as garments, or specific environments. An interesting line of future work would be to exponentially augment datasets by combining geometries and materials descriptions into new complete descriptions of the combination, enabling virtually infinite geometry, environment and material combinations for downstream natural language and visual tasks.


\vspace{-1.2em}
\new{\paragraph{Dataset extension} 
We used rendered images instead of photographs due to the large size of our dataset, since capturing 45,000 samples of different fabrics under controlled, professional conditions would impose non-negligible costs. On the other hand, using existing photographs would introduce uncontrolled variations in geometry and lighting, which may hamper the task of describing material appearance. Nevertheless, carefully augmenting our dataset with real images could enhance the performance of some applications. 
%
Additionally, our dataset could be further extended by adding expert terminology to the textual data, and used for instance to investigate social associations typically derived from clothing, such as occupation, personality, or socioeconomic status.} 
%

\paragraph{Generative models} While the challenging task of material generation is out of the scope of this study, recent material generation models have used different images as conditions~\cite{Zhou22, Guo20}. As shown, our dataset enables better visual correspondence between appearance and natural language. Combined with the strong prior of a fabric material generation model, our dataset could significantly improve text-conditioned material generation and editing.

%
\paragraph{Physical properties} 
Our dataset consists only of static stimuli. Although it has been shown that visual appearance dominates over dynamics when describing most fabrics, certain characteristics may be better conveyed by simulating the physics of such fabrics in motion~\cite{aliaga2015sackcloth}. Exploring the relative weights of appearance and dynamics on the perception of fabrics is an interesting research topic, although requiring a significant amount of work to model and simulate the physics of the fabrics.  
%



We hope that text2fabric helps enable these and other studies, which in turn may lead to the creation of novel applications. 