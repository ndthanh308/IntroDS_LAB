\section{Related work}

\paragraph{Description of visual attributes} Describing the appearance of objects, scenes or situations through language is a common task for humans. It allows to transmit richer information than simpler labeling and categorization approaches. Descriptions are not only more natural, but they also allow to focus on key or unusual aspects, or to add comparisons or semantics~\cite{farhadi2009describing}. This information would then be leveraged by means of natural language processing (NLP) to enable new, more user-friendly computer graphics and vision algorithms. While providing a complete, general method for \textit{any} object is still a daunting task, several methods have been proposed for people~\cite{bourdev2011describing}, faces~\cite{kumar2011describable}, or scenes~\cite{patterson2012sun}. Other authors have focused on the particular problem of texture description. Bhushan et al.~\shortcite{bhushan1997texture} came up with a limited 98-word lexicon which could describe 82\% of their experimental data, formed by textures; instead of using text descriptions, participants had to cluster words based on perceived similarity. Inspired by this work, Cipoi and colleagues~\shortcite{cimpoi2014describing} introduced the Describable Textures Dataset (DTD), composed of more 5,000 images labeled with one or more adjectives in a simple lexicon of 47 texture terms. The work was later extended into the Describable Textures in Detail Dataset (DTD$^2$)~\cite{wu2020describing}, including natural language descriptions. Recently, Xu et al.~\shortcite{xu2022texture} presented Texture BERT, a learning-based architecture that minimizes distances between texture and text features, optimized for image retrieval. While the domain of texture descriptions is rich and varied, our notion of appearance goes beyond 2D RGB maps, including aspects like reflectance, glossiness, touch, use, weight, etc. 
Our methodology further has the potential to generalize to other material classes.


\paragraph{Perceptually-meaningful material spaces}
The role of perception in computer graphics has been extensively researched \cite{bartz2008role,mcnamara2011perception,fleming2015perception,thompson2011visual}. In particular, finding perceptually-meaningful material spaces has many applications in graphics, including editing \cite{serrano2016intuitive,shi2021}, gamut mapping \cite{sun2017attribute}, image-space manipulations \cite{delanoy2022generative,khan2006image,boyadzhiev2015band} or material similarity \cite{lagunas2019similarity}.

Pellacini et al.~\shortcite{pellacini2000toward} derived a two-dimensional perceptually uniform space for gloss, correlated with the parameteres of the Ward BRDF model \cite{ward1992measuring}; the concept was later extended by Wills and colleagues~\shortcite{wills2009toward} to include different reflectance models. Focusing on the problem of optimal reflectance acquisition, Nielsen et al. presented a perceptual scaling and decomposition of BRDF data, which allowed to reduce PCA dimensionality; the authors further showed how the first few dimensions roughly correlate with the specular and diffuse components of appearance~\cite{nielsen2015optimal}. In computer graphics, the joint effect of geometry and illumination on appearance has also been thoroughly studied ~\cite{lagunas2021joint,bousseau2011optimizing,vangorp2007influence,dror2001estimating,storrs2021unsupervised}. Recently, Serrano and colleagues~\shortcite{serrano2021effect} trained a deep learning architecture using over 40,000 combinations or shape, material and illumination, to predict perceptual attributes of materials that correlate with human judgements. 


\paragraph{Distilling human-centered knowledge}
Understanding how people perform certain tasks and interact with different concepts is an important aspect of many human-centered computer graphics applications. Gathering rich, annotated datasets allows to distill this knowledge and apply it to the design of intuitive interfaces and workflows, help the development of novel algorithms, and automate time-consuming tasks. 
%
For instance, Cole et al.~\shortcite{Cole2008} and Gryaditskaya et al.~\shortcite{gryaditskaya2019opensketch} gathered a dataset of sketches and carefully analysed the practices of artists in terms of line types and how they are used. Garces et al. \shortcite{ garces2014similarity} provided a measure of style similarity for clip art by gathering and analyzing human responses in a dataset of a thousand elements. The \textit{OpenSurfaces} dataset \cite{li2018learning} contains crowdsourced pairwise comparisons of material properties, to improve the performance on difficult tasks such as intrinsic image decomposition. Jarabo and colleagues \shortcite{jarabo2014people} tackled the problem of navigating the four-dimensional structure of light fields to provide an intuitive interface for editing them. Last, data from over 800 participants exploring VR scenes has been used to devise novel compression or video synopsis algorithms \cite{sitzmann2018saliency}.

We frame our data gathering and analysis in a similar fashion to these works. Our goal is to distill important knowledge about how people describe fabrics, show applications like automatic captioning and retrieval, and suggest a generalization of our methodology to a larger set of material classes. 



