\section{Large Vision-Language Model Coupling}
\label{sec:applications}


Our dataset links the appearance of fabrics with natural language, helping to better understand how people describe such materials despite their semantic proximity. 
Besides, it provides high quality image and associated text data, in large albeit lower amounts than those present in very large-scale datasets used to train recent, very successful vision-language models (see Section~\ref{subsec:apps:clip_blip}). 
In this section, 
%
we explore applications of our dataset with such models, and to what extent a relatively low amount of high-quality, specialized data improves over their native versions for specific areas such as material appearance. Specifically, we demonstrate text-based fine-grained retrieval, image-based search, and description generation, as well as an improvement of invariance of the image latent representations to light and geometry changes, contributing to, e.g., a more robust notion of appearance similarity. \fillin{While we show here varied results and evaluations, please also refer to the supplemental material for additional examples.}


\subsection{Large Vision-Language Models}
\label{subsec:apps:clip_blip}

Recent progress in joint text and image encoding has been enabled by large vision-language models. In this section in particular, we fine-tune and compare to two of the most widely-used models: CLIP~\cite{CLIP} and BLIP~\cite{BLIP}. 

CLIP is a neural model composed of two encoders, one for each modality (text and image), which are trained using pairs of text and images. The method relies on contrastive learning~\cite{chen2020simple} to encourage encodings of texts and images to lie close to one another in latent space. This has been shown to draw very interesting connections based on the data it is trained on~\cite{goh2021multimodal}. CLIP is particularly powerful thanks to the vast LAION dataset on which it is trained, containing 400 million image-text pairs gathered from the internet. Different encoder architectures have been published, but in this paper we use the ViT-B/16 version, which relies on a visual transformer~\cite{dosovitskiy2020image} with a patch size of $16\times16$.

BLIP is a combination of networks trained jointly, including a pair of encoders, similar to CLIP. Moreover, BLIP also contains a generative head, trained jointly with the rest of the network, enabling it to generate captions corresponding to an image. It is also trained on hundreds of millions of images, including a self-supervised augmentation mechanism called ``CapFilt''. Similarly to CLIP, we use the ViT-B version of the network. 

While both CLIP and BLIP are trained on very large-scale datasets, the text data to which they are exposed is limited to low quality online captions of images. We will show, in the remainder of this section, that a small amount of high quality data is sufficient to significantly improve the networks' sensitivities to specialised concepts.
%
\new{In the following experiments, we use the models published by SalesForce and OpenAI.
}

\subsection{Text-Based Fine-Grained Retrieval}
\label{subsec:apps:retrieval}

Given a query in the form of a text description, the goal of this first application is to retrieve fabric samples that match the query. 
Improving search performance in large datasets is increasingly important as the number and size of libraries and datasets increase~\cite{Substance3DAssets,megascan}.
Different from generic text-based retrieval, which aims at finding images of objects in different classes such as ``chairs'', ``cars'', or ``people'', we target the more difficult case of \textit{fine-grained} retrieval~\cite{qi2021Retrieval}, i.e., finding the right instance despite significant semantic similarities within the dataset. 


\subsubsection{Implementation}
\label{subsubsec:implementation-clip}
We fine-tune CLIP~\cite{CLIP}, 
%
starting from the VIT-B/16 pre-trained model (we term this pre-trained model \emph{native} CLIP). Our dataset is split in 12,334 training and 3,129 test descriptions, ensuring that no procedural variation of a given material in the training data is used for testing. 
%
During training we split the descriptions by sentence,  
%
resulting in 45,871 (36,565 train/9,306 test) individual sentence descriptions for 3,000 (2,393/607) materials.
%
We train the network for 12 epochs \new{(at which point the performance levels off, with small gains until epoch 19)}, using renderings of the training materials on four geometries (\emph{baseline}, \emph{sphere}, \emph{sphere\_draped}, \emph{plane}) and a single illumination (\emph{baseline}). We use a learning rate of $1e^{-6}$ with a linear schedule with $200$ warm-up steps for the Adam optimizer, with $\beta_1 = 0.9$, $\beta_2 = 0.99$ and batch size $128$. This takes five hours to train on a single Nvidia RTX3090 GPU. \new{For inference, execution time is 0.46 seconds for a batch of 64 images.}

%%%%%%%
\subsubsection{Results}
%%%%%%%
Since we have ground-truth data (image-description pairs in our test dataset), we can evaluate retrieval of the \emph{correct} material given an input description. Additionally, for any given generic query, the retrieval application should return relevant results. 
%
The search operation presented in this section is made over the entire 3,000 materials on our \emph{baseline} geometry and illumination unless specified otherwise; in all cases, neither the descriptions used as queries nor the correct images or materials have been seen during training.

\paragraph{Quantitative analysis}
%
Given our test set descriptions, we evaluate retrieval in the complete material database and report the top-K recall results of our fine-tuned CLIP, with $K \in \{1,5,10,20,100\}$, in Table~\ref{tab:application:retrieval}. We also include results for \emph{native} CLIP, \emph{native} BLIP\footnote{~For details on the implementation of native BLIP please refer to Section~\ref{subsubsec:implementation-blip}.}, \new{and BLIP trained on our data only (BLIP \emph{no pretrain}).} 
%
Compared to native CLIP/BLIP, we achieve $4.8$/$4$ times better top-1 retrieval rate and maintain at least $2.12$ times better results for all top-K results, showing that our dataset makes CLIP more sensitive to fabric-specific concepts. This also shows that our fine-tuned model is capable of retrieving a fabric sample from its description alone, which requires strong feature discrimination in a semantically similar dataset. 
\new{The comparison to BLIP trained on our data only (no pretrain) shows that our model significantly benefits from the original model training, leveraging the priors provided by its large corpus of text.}

\begin{table}
 \caption{Top-K retrieval results on the \emph{baseline} geometry for native CLIP, native BLIP, \new{BLIP trained on our data only (BLIP no pretrain)} and our fine-tuned model.
 }
%
\small
\begin{tabular}{r|c|c|c|c|}
 & Native CLIP & Native BLIP & \new{BLIP no pretrain} & Ours \\ \hline
 Top-1 & 2.94\% & 3.42\% & 1.60\% & \textbf{13.81\%} \\
 Top-5 & 8.31\% & 9.94\% & 5.98\% & \textbf{33.91\%} \\
 Top-10 & 12.59\% & 14.60\% & 10.64\% & \textbf{46.76\%} \\
 Top-20 & 18.37\% & 20.17\% & 17.00\% & \textbf{59.76\%} \\
 Top-100 & 41.29\% & 34.26\% & 34.36\% & \textbf{87.63\%} \\ \hline
\end{tabular}
 \label{tab:application:retrieval}
\end{table}

\begin{table}
 \caption{Top-K retrieval results on the \emph{plane\_draped} geometry, unseen during training, for native CLIP, native BLIP, our model fine-tuned on only one geometry (\emph{baseline}), and our model (which is fine-tuned on four geometries, not including \emph{plane\_draped}). 
 %
 }
 \small
 \begin{tabular}{r|c|c|c|c|}
 & Native CLIP & Native BLIP & Ours (1 gm.) & Ours (4 gm.) \\ \hline
 Top-1 & 1.34\% & 2.27\% & 5.98\% & \textbf{7.38\%}  \\
 Top-5 & 5.02\% & 7.03\% & 16.55\% & \textbf{22.95\%} \\
 Top-10 & 7.93\% & 10.9\% & 23.94\% & \textbf{31.77\%} \\
 Top-20 & 12.66\% & 15.05\% & 34.16\% & \textbf{43.72\%} \\
 Top-100 & 32.18\% & 29.05\% & 64.59\% & \textbf{75.93\%} \\ \hline
\end{tabular}
 \label{tab:application:retrieval_unseen}
\end{table}

% Figure environment removed


To evaluate the ability of our model to generalize to other geometries, Table~\ref{tab:application:retrieval_unseen} reports retrieval recall results on a geometry unseen during training (\emph{plane\_draped}). 
%
We see that, despite the unseen geometry being challenging (all methods have lower retrieval results than with the \emph{baseline} geometry), our fine-tuned model still significantly benefits from our dataset. Furthermore, we also include a comparison to our model fine-tuned on images from just one geometry (\emph{baseline}), showing that the training on different geometries improves the generalization of the method.


% Figure environment removed

% Figure environment removed

We also assess the required size of a specialized dataset such as ours for fine-tuning general purpose models. To do so, we plot the top-K retrieval results as we vary the number of descriptions used in the training in Figure~\ref{fig:application:retrievalAblationAdescriptionsnb}. We see how the model significantly benefits from the first 2,000 descriptions, and how the marginal improvement rate then starts diminishing. At constant number of descriptions, we also evaluate whether more images with \new{fewer} descriptions is preferable to \new{fewer} images with more descriptions. We find that using 1,500 images with 5 descriptions per image is equivalent in retrieval recall to using 2,500 images with 3 descriptions per image: Results in both cases are close, indicating a similar impact between image and description diversity. 
More precisely, 1,500 images with 5 descriptions each yield top-1/5/10 retrieval results of 12.56/31.38/44.1\%; in comparison, 2,500 images with 3 descriptions each yield top-1/5/10 retrieval results of 12.66/31.16/43.69\%.
\new{In addition, we also include a quantitative evaluation on negative queries in the supplemental material}.

\vspace{-0.5pt}
 
\paragraph{Qualitative analysis}
%
  % Figure environment removed

 % Figure environment removed

To qualitatively evaluate the performance of our fine-tuned model, 
%
we provide results retrieved from natural language queries in Figure~\ref{fig:application:retrieval}, showing that the retrieved materials exhibit the desired properties, not only in a geometry seen during training\footnote{~Note that the geometry has been seen during training, but the materials and queries have not.} (\emph{baseline}), but also in unseen geometry (\emph{plane\_draped}). 
\new{As expected, diversity increases as we look at more returned samples. This can be seen in our ``Asian looking'' prompt; while the top-3 results in Figure~\ref{fig:application:retrieval} contain similar results due to our space being partly organized with respect to visual features, more diverse results appear when visualizing the top-10 results, as shown in Figure~\ref{fig:application:top10}.}

In Figure~\ref{fig:application:retrieval_sensitivity}, we show a more systematic evaluation with positive and negative queries, corresponding to prominent fabrics concepts extracted from the dataset (see Section~\ref{sec:dataset-analysis}), and include a comparison to native CLIP. Results confirm that our fine-tuned model is effective in the retrieval, and more sensitive to fine-grained descriptions, while native CLIP struggles with specialized concepts (e.g., stitching) and negative wording. 
%
Interestingly, despite the relatively small amount of data used in our fine-tuning (compared to the hundreds of millions of image-text pairs required to train CLIP and BLIP), we observe significant improvement in material retrieval for the class of interest (fabrics). Furthermore, these experiments highlight the limitations existing in the representations of Large Vision-Language Models for fine-grained appearance concepts.

\new{Finally, we evaluate the limits of modeling out-of-distribution queries, containing concepts that do not appear in our dataset descriptions. As shown in Figure~\ref{fig:application:out_of_distrib}, our model finds reasonable results for these queries (e.g., for the case of ``Thanksgiving-themed'' we obtain a variety of autumnal brown and orange fabrics), suggesting that our model preserves its broader priors without overfitting to our dataset.}


   % Figure environment removed


\subsection{Image-Based Search}
\label{subsec:apps:search}

We continue our evaluation by studying image-based search using real images as input. 
We do it by leveraging our fine-tuned CLIP model (see Section~\ref{subsec:apps:retrieval}), as well as native CLIP for comparison.
%
Specifically, we compute the normalized embedding---using either native CLIP or our fine-tuned model---of the input image, and compute its cosine distance to the normalized embeddings of the candidates from our dataset. These candidates are the 3,000 materials in our dataset, rendered on a certain geometry (or set of geometries in Section~\ref{subsec:apps:invariance}).
Figure~\ref{fig:application:imageRetrieval} shows results on the \emph{plane\_draped} geometry (unseen during training) for both our fine-tuned model and native CLIP.
%
We can observe that native CLIP is strongly influenced by the geometrical macrostructure present in the input image, and fails at guiding the retrieval process by the material mesostructure, patterns and reflectivity properties expressed in the input. On the contrary, our fine-tuned model succeeds at fetching results with similarities existing at material scale, bypassing the strong features stemming from the supporting 3D shape.  

\subsection{Caption Generation}
\label{subsec:apps:caption}

Caption generation aims at creating accurate descriptions of a fabric material given an image of it. Similarly to the retrieval application, we target fine-grained description, with precise properties described, which are not limited to high-level semantics. This further allows us to explicitly observe the ingestion of the concepts stemming from our dataset by Large Language Models.

\subsubsection{Implementation}
\label{subsubsec:implementation-blip}

We leverage and fine-tune BLIP~\cite{BLIP} for caption generation, and process our data as described in Section~\ref{subsubsec:implementation-clip}; however, in this case we do not split the sentences, ensuring full descriptions are seen by the model. We fine-tune the generative head of BLIP, starting from 
what we term \emph{native} BLIP: the VIT-B/16 model pre-trained on 129M images from LAION + CapFilt-L (model\_base\_capfilt\_large). We train the network for 12 epochs using the Adam optimizer with weight decay regularization using a decay parameter of $0.05$, an initial learning rate of $1e^{-5}$ and a batch size of $24$. The minimum number of generated tokens is set to $5$, and the maximum to $80$. This takes approximately 9 hours to train on a single RTX3090.
Once fine-tuned, we use nucleus sampling for tokens\new{~\cite{holtzman2020nucleus}}, letting us generate varied descriptions for each image.

 \subsubsection{Keyword Extraction}
 
 While we mainly focus on generating natural language descriptions (using the model described in Section~\ref{subsubsec:implementation-blip}), simple keywords can also be convenient in several search or classification scenarios. 
 %
 Therefore, using our understanding of fabric descriptions in terms of common lexicon, main attributes and structure (Section~\ref{sec:dataset-analysis}), we automatically extract keywords for an image based on the generated descriptions. For that, we use the first five descriptions of an image, post-process the text as explained in Section~\ref{subsubsec:postpro}, extract a set of keywords per attribute from our lexicon, and order them by importance (number of descriptions in which they appear) and the rank product of their attribute. Resulting keywords for real images are shown in Figure~\ref{fig:application:descriptiongen_real}, and \fillin{we include automatically extracted keywords for all material samples as part of our text2fabric dataset}.
 %

\subsubsection{Results}

We show here description results from our fine-tuned BLIP model, 
%
together with comparisons to native BLIP.
%

Figure~\ref{fig:application:descriptiongen_synth} shows captioning results on synthetic images, with two different geometries, and materials from the test set (unseen during training). 
%
For each fabric sample, we include: descriptions from our dataset, provided by humans; descriptions generated by our fine-tuned model; and descriptions generated by native BLIP.
%
We observe how our fine-tuned model generates descriptions that better convey fine-grained material appearance, are more accurate and with more attention to detail, and match more closely the style of human descriptions.
 
We further demonstrate our results on real images 
%
containing fabrics in Figure~\ref{fig:application:descriptiongen_real}. We crop the fabric area of interest (marked by a red square), and generate descriptions for it. The descriptions generated using our fine-tuned model contain significantly richer information than the native BLIP results, trained only on general internet images and high-level descriptions. Additionally, our keyword extraction method is capable of automatically extracting relevant keywords from our generated sentences, which can be useful for, e.g., automatic tagging. These results further show that the fine-tuning on our high quality renderings generalizes well to real photographs. 


% Figure environment removed


  % Figure environment removed


\subsection{Invariance of the Latent Space to Geometry and Illumination}
\label{subsec:apps:invariance}
%
Our dataset significantly helps to improve the invariance to lighting and geometry of large vision-language models representations. We illustrate this by fine-tuning CLIP (as described in Section~\ref{subsubsec:implementation-clip}) using all the renderings of our materials, with five different geometries and three environment illuminations (\emph{baseline}, \emph{outdoor}, \emph{studio}) associated to our \nbvaliddescription descriptions. While we could use contrastive learning to try to learn an invariant representation instead, the only available supervision would be whether or not two images show the same material, making the creation of a perceptually smooth representation challenging.
Using our descriptions as anchor contributes to a smooth representation space, 
allowing the model to learn a more robust notion of material appearance than that of native CLIP, as shown by our evaluation, described next.


\begin{table}
 \caption{
 Average cosine similarity (and associated standard deviation) between pairs of images exhibiting: 
 the same material (and lighting conditions) and different geometries (\emph{varying geometry}); and 
 the same material (and geometry) and different lighting conditions (\emph{varying lighting}).
 Our fine-tuned representation (\emph{first row}) finds images of the same material to be more similar, despite geometry or lighting variation, than native CLIP space (\emph{second row}). \new{We include results of the Wilcoxon signed-rank test showing effect size | p-value (\emph{bottom row}). Differences are statistically significant and with large effect sizes for both experiments.}}
\begin{tabular}{r|c|c|}
 & Varying Geometry & Varying Lighting \\ \hline
 Ours & $0.951 \pm 0.012$ & $0.973 \pm 0.007$ \\
 Native CLIP & $0.835 \pm 0.042$ & $0.945 \pm 0.011$ \\ \hline
 \new{Wilcoxon signed-rank} & 0.866 | <0.0001 & 0.846 | <0.0001 \\ \hline
\end{tabular}
 \label{tab:application:cross_geom_light}
\end{table}

% Figure environment removed

% Figure environment removed

In Table~\ref{tab:application:cross_geom_light} (first column, \emph{varying geometry}) we evaluate the average cosine similarity between pairs of images rendered with the same material and lighting but different geometries, computed both in native CLIP space and in the latent space of our fine-tuned model. Our representation is, on average, more invariant to geometry than the original CLIP features.
The same evaluation for pairs of images rendered with the same material and geometry but different lighting conditions (Table~\ref{tab:application:cross_geom_light}, \emph{varying lighting}), shows a similar trend, although less pronounced. In both cases, the lower standard deviation between pairwise similarities using our representation suggests a greater stability across variations. \new{A Wilcoxon signed-rank test shows that these differences are statistically significant (p-value<0.0001), with effect sizes considered large for both geometry and lighting variations~\cite{rosenthal1994parametric}.}


We qualitatively evaluate this property in Figure~\ref{fig:application:realImageRetrievalDifferentGeometry}: we assess whether, given a real photograph as input to an image-based search (see Section~\ref{subsec:apps:search}), the results change depending on the geometry present in the database we search in (for this test, each database we search in has all materials rendered with a single geometry). The figure shows results for the search in the \emph{sphere\_draped} and \emph{plane} geometries databases, and we display all results rendered on \emph{sphere\_draped} for easier comparison. We can see that our representation is significantly more consistent than native CLIP on varying geometries, and better at learning features at material scale. 


We further pursue this evaluation in Figure~\ref{fig:application:crossGeometryRetrieval}.
Here, we seek at retrieving a given \emph{test} (i.e., unseen during training) material rendered on a given geometry, performing image-based search in a database containing renderings of all 3,000 materials and five geometries. As expected, with both native CLIP and our fine-tuned model, the first result is the same material and geometry. However, it is clearly apparent that the native CLIP representation is heavily biased by the geometry in the input image, while our representation better identifies the same material across geometries.
