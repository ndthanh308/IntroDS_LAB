\section{Our text2fabric Dataset}
\label{sec:dataset-creation}

This section describes how we designed and created our large-scale text2fabric dataset. It consists of 45,000 rendered images depicting samples of 3,000 different fabrics, 
together with \nbvaliddescription associated descriptions in natural language. 
The full dataset, with a web-based browser to explore it, can be downloaded from the project website: \url{https://valentin.deschaintre.fr/text2fabric}.

% Figure environment removed



\subsection{Rendered Images}
\label{subsec:dataset:images}

The 45,000 images of our dataset are generated using the Substance Stager renderer, and span 3,000 different fabric materials with a wide range of appearance. 
\new{These fabric models can be found on the Substance 3D website\footnote{~\url{https://substance3d.adobe.com/assets/allassets?assetType=substanceMaterial\&category=Fabric}}, and consist of both procedural materials generated by artists, as well as high-quality scans. Procedural materials come in the form of directed acyclic graphs, made of nodes of three different types: generators, which typically define the global bidimensional structure of the material (e.g., tiles); filters, which alter their input (e.g., colorization); and data stores, which point to external resources (e.g., raster content). Once executed by a material graph engine, these procedural models provide the material channels in the form of 2D maps, at a chosen resolution. A few parameters of the nodes of a graph are exposed as hyperparameters, so that changing them yields meaningful variations of the so-defined material. In the case of fabrics, these hyperparameters are carefully chosen so that the bounded variation they produce maps realistically to patterns and textile types encountered in the fabric industry.
In addition, each procedural material originally includes a title and some tags describing its main appearance (e.g., sportswear, upholstery, mesh). We choose not to rely on these, as they describe what the artist wanted to represent rather than how people perceive it, do not follow any particular convention, and may introduce bias in the descriptions. 
Nonetheless, this information may be used to complement our collected descriptions.
}


\new{For our task,} we first rendered images of all 3,000 different materials at 4K resolution on a \emph{baseline} geometry, carefully chosen to faithfully convey the appearance of the fabric: it contains both draped and flat areas, covering a wide range of orientations. We then selected a \emph{baseline} indoor illumination, featuring soft lighting through multiple windows.
A representative sample of the resulting fabrics can be seen in Figure~\ref{fig:dataset:images} (top row).

Additionally, to ensure robustness and help future applications (see Section~\ref{sec:applications}), we rendered the same materials using four other geometries---a sphere and a plane, in both draped and non-draped versions---, and two other illuminations---\emph{outdoor}, with direct outdoor illumination, and \emph{studio}, with strong studio indoor lighting, yielding 42,000 more images. Figure~\ref{fig:dataset:images} (bottom row) shows some examples.

Different from other existing general-purpose datasets like ImageNet or LAION\footnote{~LAION~\cite{laion400m} is the large-scale dataset on which the vision-language model CLIP~\cite{CLIP} is trained.}, our fabrics dataset constitutes a quite specific subset of images. This is illustrated, e.g., by the GLCM entropy~\cite{Haralick1973}, a measure of randomness of the images, as shown in Figure~\ref{fig:dataset:entropy}; our data yields a narrower histogram (narrower range of entropy), compared to the same number of randomly selected images from LAION \fillin{(other image statistics can be found in the supplemental material)}. The specific characteristics of our image data are relevant to its use in learning-based models, such as the ones employed in Section~\ref{sec:applications}.


\subsection{Natural Language Descriptions}
\label{subsec:dataset:text}

In the garment manufacturing industry, the description of fabrics involves specialized concepts and words such as \textit{permeability} (how much air or water it allows through), \textit{absorbency} (the ability of a fabric to take in moisture), or \textit{colorfastness} (the ability of a fabric to maintain its color and resist fading), to name a few \cite{glossary}. This specialized vocabulary is different from the one used by digital artists and practitioners in general. We thus gather our own text data to describe fabrics. 

We collected \nbvaliddescription valid descriptions of fabric appearance as free text using natural language, through a carefully controlled crowdsourcing framework (Section~\ref{subsubsec:annotation}), followed by a process of data verification and auditing (Section~\ref{subsubsec:verification}). Finally, we post-processed the resulting data (Section~\ref{subsubsec:postpro}) in preparation for the analysis described in Section~\ref{sec:dataset-analysis}.

\subsubsection{Annotation Procedure and Participants}
\label{subsubsec:annotation}
We conducted a crowdsourced user study in which participants (which we term \emph{describers}) had to provide free-text descriptions for our high-quality fabric renderings. Specifically, describers were shown one image at a time, along with three zoomed-in areas (highlighted in green in the top-left image of Figure~\ref{fig:dataset:images}), and were asked to describe the appearance of the material as precisely as possible using their own words in natural language. Describers were free to use one or several sentences for the descriptions (1-3 was recommended), and word count was limited to the range $[20..100]$ words, to prevent excessively short or long descriptions. To keep the task tractable, we gathered descriptions for our \emph{baseline} set of 3,000 images of different materials. This also encouraged describers to focus on the only changing aspect between images --the material--, familiarizing themselves with the geometry and illumination. This decision is further justified by the nature of the task, the goal of our study, and the ability of the human visual system to achieve perceptual stability and extract constant properties of materials from varying viewing conditions~\cite{tsuda2020material,fleming2017material,fleming2015perception}. 
%
Describers were required to do a minimum of ten descriptions, and we ensured that no describer contributed more than 9\% of the whole text data. We also ensured that, for each image, we gathered at least five \emph{valid} descriptions from different participants.


Given the specifics of the task, we required that the describers be native English speakers, had normal color vision, and were familiar with fashion or design. While we are aware that this may introduce some bias in the responses, it allows to gather a rich and accurate vocabulary. 
Prior to taking part in the study, participants underwent a short training and a qualification test. The training consisted of 
% 
a set of instructions along with example descriptions gathered from a smaller pilot study. For the qualification test, each participant had to describe ten test fabric renderings; participants offering overly simple, poor descriptions, such as ``this is a nice fabric'', were discarded. Approximately one in four did not pass this qualification test. After this process, a total of \nbvaliddescribers describers (ages 18 through 65) went on to provide descriptions for our dataset: 45\% identified as female, 12.3\% as male, none as other gender identities, and 42.7\% preferred not to reply.


% Figure environment removed

\subsubsection{Data Verification}
\label{subsubsec:verification}

We gathered a total of 19,167 descriptions from the \nbvaliddescribers qualified describers. However, ensuring quality in free text description is a difficult task. We therefore established an additional continuous data verification protocol in which we manually audited descriptions. 
%
For each description, we first labeled them manually as one of four options: \emph{accepted}, or rejected due to the description being \emph{too generic}, being \emph{wrong}, or using \emph{poor grammar} to the point of hindering understandability. In addition, we also rated each description using a 5-point scale (1=totally unacceptable, 2=unacceptable, 3=acceptable, 4=very good, 5=excellent). 

Manual auditing of the full set of almost 20,000 descriptions is \new{an arduous} task. However, we found that the quality of the descriptions was highly \new{dependent} on the describer, and data quality (as given by the ratings) was fairly uniform within a describer. Therefore, auditing a randomly-selected subset of the descriptions of a participant provided a good estimate for the rest of their descriptions; for example, for participants with a rejection rate $>35\%$, we rejected all their remaining, non-audited descriptions. \fillin{More details of this process can be found in the supplemental material.} 
%
The data gathering process was iterative, to ensure that we had at least five descriptions for each fabric. After this process we ended up with \nbvaliddescription valid descriptions and \nbinvaliddescription invalid ones (a $19.3\%$ rejection rate). 

\subsubsection{Post-processing} 
\label{subsubsec:postpro}

Following standard natural language processing techniques, we post-process our text data by removing non-alphabetic characters, applying a spell checker, and filtering stop words. Moreover, to carry out a proper analysis we extract tokens, types, and lemmas from the descriptions~\cite{Brezina2018}. 
%
A \textit{token} is each occurrence of a word in a text, while a \textit{type} is each unique occurrence of a word in a text. A \textit{lexeme} corresponds to the set of alternating forms from a common root word (such as ``colors'', ``colored'' or ``coloring''), while a \textit{lemma} refers to the particular form chosen to represent a lexeme (such as ``color'' in our previous example). 
%
\fillin{Further details, including the spell checker and the lemmatizer we use, can be found in the supplemental material.}

The statistics of our textual data after this post-processing can be found in Figure~\ref{fig:dataset:table}. When looking at values per description, we can see that the mean and median are close, indicating that the distributions are not too skewed; we show this distribution for the case of tokens in Figure~\ref{fig:dataset:length_words}, both before and after post-processing. The table in Figure~\ref{fig:dataset:table} (bottom) further shows that the difference between the number of tokens, types and lemmas per description is not large, suggesting that our descriptions are diverse in the sense that there are not many repeated words in them.
%
Finally, we also perform part-of-speech tagging and classify tokens into nouns, adjectives, verbs, etc. (see Figure~\ref{fig:dataset:pos_tagging}). Our data contains mainly nouns and adjectives, as expected in texts of a descriptive nature. 




