% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{pifont}

% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}

% ACLab commands
\renewcommand*\ttdefault{txtt}
\newcommand{\com}[1]{\textcolor{red}{#1}}
\newcommand{\eqnref}[1]{Eq.~(\ref{eqn:#1})}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\algref}[1]{Algorithm.~\ref{alg:#1}}
\newcommand{\thmref}[1]{Theorem~\ref{thm:#1}}
\newcommand{\defref}[1]{Definition~\ref{definition:#1}}
\newcommand{\lemref}[1]{Lemma~\ref{lem:#1}}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}} % For booktabs table formatting
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}

\input{figs}
\input{tab}


\begin{document}

\title{Automatic Infant Respiration Estimation from Video: A Deep Flow-based Algorithm and a Novel Public Benchmark}
%
\titlerunning{Automatic Infant Respiration Estimation}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}
%
%\authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%\institute{Princeton University, Princeton NJ 08544, USA \and
%Springer Heidelberg, Tiergartenstr. 17, 69121 Heidelberg, Germany
%\email{lncs@springer.com}\\
%\url{http://www.springer.com/gp/computer-science/lncs} \and
%ABC Institute, Rupert-Karls-University Heidelberg, Heidelberg, Germany\\
%\email{\{abc,lncs\}@uni-heidelberg.de}}
%

\author{Sai Kumar Reddy Manne \inst{1,2}, Shaotong Zhu\inst{2},\\ Sarah Ostadabbas\inst{2}, Michael Wan\inst{1,2}$^*$}
%index{Manne, Sai Kumar Reddy}
%index{Zhu, Shaotong}
%index{Wan, Michael}
%index{Ostadabbas, Sarah}

\authorrunning{Sai Kumar Reddy Manne et al.}
\institute{Roux Institute, Northeastern University, Portland ME, USA\\
\and
Augmented Cognition Lab, Department of Electrical \& Computer Engineering\\Northeastern University, Boston MA, USA\\
$^*$Corresponding author: \email{mi.wan@northeastern.edu}
}
\maketitle              % typeset the header of the contribution
%


%%%%%%%%% ABSTRACT
\begin{abstract}
Respiration is a critical vital sign for infants, and continuous respiratory monitoring is particularly important for newborns. However, neonates are sensitive and contact-based sensors present challenges in comfort, hygiene, and skin health, especially for preterm babies. As a step toward fully automatic, continuous, and contactless respiratory monitoring, we develop a deep-learning method for estimating respiratory rate and waveform from plain video footage in natural settings. Our automated infant respiration flow-based network (AIRFlowNet) combines video-extracted optical flow input and spatiotemporal convolutional processing tuned to the infant domain. We support our model with the first public annotated infant respiration dataset with 125 videos (AIR-125), drawn from eight infant subjects, set varied pose, lighting, and camera conditions. We include manual respiration annotations and optimize AIRFlowNet training on them using a novel spectral bandpass loss function. When trained and tested on the AIR-125 infant data, our method significantly outperforms other state-of-the-art methods in respiratory rate estimation, achieving a mean absolute error of $\sim$2.9 breaths per minute, compared to $\sim$4.7--6.2 for other public models designed for adult subjects and more uniform environments\footnote{Our code and the manually annotated NNS in-the-wild dataset can be found at \url{https://github.com/ostadabbas/Infant-Respiration-Estimation}. Supported by MathWorks and NSF-CAREER Grant \#2143882.}.



% \com{MW: In progress. To be rewritten with more of a focus on infant respiration.} Remote physiological measurement has amassed interest in the recent years with the onset of the COVID-19 pandemic. Prior research studied heart rate, respiration rate, and remote photoplethysmography (rPPG) estimation for adult subjects mainly on privately owned datasets. While contact-based measurement is not severely restrictive for adults, it can cause discomfort for infants and may interfere with care providers. However, several recent studies focus on adult subjects, and the current measurement systems designed for infants are not reproducible. In this paper, we bridge this gap by establishing a benchmark for infant respiration estimation that is robust to camera type, lighting, pose, and noise. We present a publicly available dataset, an optical flow-based deep convolutional network, and a novel loss function to train the model using manually annotated labels. Furthermore, we compare our model against existing color-based and motion-based methods and set the state-of-the-art in infant respiration estimation. 

%In contrast, contact-based measurement of physiological signals is not desired for infants as they may interfere while providing care for infants and can cause discomfort or irritation with prolonged use. Although recent research is focused on developing non-contact methods for vital sign measurement, most datasets are private due to the sensitive nature of the problem. In this paper, we establish a benchmark for infant respiration research by proposing a new public dataset, and provide a detailed analysis of color- and motion-based methods, and demonstrate the domain gap between adult and infant physiological measurement. Combining the merits of deep learning and motion-based models, we set the state-of-the-art in remote respiration measurement for infants.
%We also develop and test baseline models, including deep-learning models and classical computer vision techniques, to estimate respiration rates using videos from the proposed dataset and demonstrate the domain gap between infant and adult physiological measurement. Finally, we implement and benchmark two late fusion approaches to combine the color- and motion-based methods, with the waveform-based fusion achieving the highest Pearson correlation with ground truth of $r=0.8$, among all methods tested on the infant dataset.
\keywords{Infant respiration measurement \and Spatio-temporal neural network \and Spectral loss \and Vital sign monitoring}
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{sec:intro}
\demo
From an infant's first breath in the seconds after birth, respiration becomes a critical vital sign in early life, with irregularities revealing complications ranging from apnea and respiratory distress in neonates \cite{reuter2014respiratory}, to  respiratory syncytial virus (RSV) infection in months-old infants leading in severe cases to bronchiolitis or pneumonia \cite{hall2009rsv}, or even potentially to sudden infant death syndrome (SIDS) \cite{THACH2005343}. Continuous respiratory monitoring is particularly important for preterm infants during \textit{ex utero} development in neonatal intensive care units (NICUs), where contactless sensors are desirable for comfort and hygiene, and to prevent skin damage, during this sensitive period \cite{villarroel_non-contact_2019}. We present a novel vision-based deep learning method for detecting an infant's respiration waveform and respiratory rate (see \figref{dmem}), as a step toward automated monitoring of infant breathing for both everyday and clinical diagnostic use. Only a few papers have explored deep learning-based infant respiration estimation (see \tabref{dataset-table}), and due to logistical and privacy constraints on infant data collection, none of them publish their data or models and many draw data from just one or two infant subjects \cite{tveit2016motion,kyrollos2021noncontact,lorato2021towards,foldesy2020reference}. As part of this work, we publish the \textit{\textbf{a}nnotated \textbf{i}nfant \textbf{r}espiration dataset of \textbf{125} videos}, \textbf{AIR-125}, with ground truth respiratory rates and waveforms from eight infant subjects, to support public and reproducible research. AIR-125 features minute-long videos sourced from baby monitors and smartphone cameras in natural infant settings, with varying illumination, infant poses, age groups, and respiratory rates. We use manual respiration annotations rather than sensor-captured ground truth to enable data collection from various sources, but also include synthetically-generated respiration waveforms to maintain compatibility with existing models and benchmarks. 

% Remote estimation of physiological signals has garnered much recent attention, spurred on by the COVID-19 pandemic, leading to successes in camera-based heart rate \cite{liu2023efficientphys}, pulse transit time \cite{shao2014noncontact}, heart-rate variability \cite{di2022estimation}, and respiration rate \cite{chen2018deepphys} estimation from deep-learning models. %While pulse rate estimation is heavily studied, fewer attempts have been made to estimate respiration signals from videos. 
% %Among these vital signs, remote heart-rate estimation is heavily studied, while little attention is paid to respiration monitoring. 
% However, most models are 
% %focused on adult subjects, with models 
% trained on adult data captured in controlled environments, and perform poorly on infant subjects in typical infant settings such as cribs \cite{liu2021metaphys}.
% %do not generalize well to settings with different illumination or subject variations , and fail when tested directly on infant videos. 
% A growing body of recent research has emphasized methods specifically targeting the infant domain, including for face \cite{wan2022infanface} and body \cite{huang2021invariant} pose estimation and detection of infant oral sucking patterns \cite{zhu2023videobased}. In the domain of infant respiration, we face unique challenges from the rapid and shallow breathing of infants. As mentioned, existing infant respiration datasets are small and private, so for training data, we are limited to our own public dataset and a handful of larger datasets featuring adults only and annotated with a range of physiological signals \cite{mcduff2022scamps,heusch2017reproducible,soleymani2011multimodal,estepp2014recovering}.

% The general gap between the adult and infant domains is exemplified by other problems such as facial landmark detection \cite{wan2022infanface} and pose estimation \cite{huang2021invariant}, and respiration estimation faces further challenges in the infant domain due to significant differences in rate and depth of breaths. Recent work on deep learning estimation of infant oral sucking patterns \cite{zhu2023videobased} highlights importance of infant data for training and testing. %Although there have been recent studies on closely related areas of infant state monitoring such as \cite{zhu2023videobased} they do not directly study respiration signal estimation for infants.

%Several existing approaches for respiration estimation are validated on private datasets due to the sensitive nature of the physiological information. This leaves very few public datasets open for the research community to work with \cite{mcduff2022scamps,heusch2017reproducible,soleymani2011multimodal,estepp2014recovering}. This problem of public dataset scarcity is even critical in the infant domain as all the datasets used in the literature \cite{tveit2016motion,kyrollos2021noncontact,lorato2021towards} are not open to the public for research. A comprehensive list of available datasets for infant and adult physiological measurement is presented in \tabref{dataset-table}. Hence, the need for a benchmark dataset to evaluate infant respiration rate estimation systems arises.

\publicdata
%\tabstats

Existing approaches for respiration measurement \cite{guo2021remote,liu2023efficientphys} track motion using optical flow or track subtle color changes in skin pixels. The flow-based methods are prone to errors from noise or subject motion and the color-based methods rely on visible skin pixels in the video, which may be scarce for infants who are heavily covered or sleeping in an awkward pose like those in AIR-125. Hence, we also propose a new model, the \textit{\textbf{a}utomated \textbf{i}nfant \textbf{r}espiration \textbf{flow}-based network}, \textbf{AIRFlowNet}, which learns to isolate the periodic respiratory motion in a noisy environment without the need for visible skin in the video. Current respiration models are trained with ground truth obtained from contact sensors perfectly synchronized with videos \cite{chen2018deepphys,liu2020multi,liu2023efficientphys}. We introduce a novel \textit{spectral bandpass loss function} which encourages alignment in the frequency domain while forgiving absolute temporal shifts, enabling more effective training with our manual annotations. When trained and tested on AIR-125 infant data, AIRFlowNet significantly outperforms other state-of-the-art respiration models. %AIRFlowNet also comes out on top when trained on adult data and tested on infant data, and it is competitive with the best models when trained and tested on adult data alone.

% Moving the dataset details and explanation to the dataset section.
%To this end, we create the first ever public dataset for infant respiration monitoring, sourcing videos with varying illumination, infant poses, age groups, and respiration rates (see \tabref{stats}). This dataset will be open to the public for research, promoting reproducibility, and it can serve as an initial benchmark for further research on this critical problem. We collect baby monitor data from people with their consent as our primary source for the dataset. We also use public video hosting websites such as YouTube to create the proposed dataset. Gathering a diverse set of infants in a variety of poses is a time-consuming and difficult task. Hence, we collect existing videos from video hosting websites to create a dataset that can be used to determine the generalizability of existing methods.

In sum, our key contributions include (1) the first public annotated infant respiration dataset (AIR-125), (2) a motion-based infant respiration estimation model (AIRFlowNet) with a novel spectral bandpass loss achieving best-in-class performance, and (3) performance comparison of public color- and motion-based respiration models on infant and adult datasets.

% \begin{enumerate}
%     \item creation of the first public annotated infant respiration dataset, AIR-125,
%     %\item We establish a benchmark for infant respiration using two methods---one color-based and one motion-based---on the proposed dataset.
%     \item development of a novel motion-based deep learning model, AIRFlowNet, which outperforms state-of-the-art models in infant respiration estimation,
%     \item introduction of the spectral bandpass loss function for optimal training with manual respiration annotations, and
%     \item a performance comparison of public color- and motion-based respiration models on infant and adult datasets.
% \end{enumerate}

% The rest of the paper is organized as follows. First, we review the related work in \secref{related_work}, followed by the proposed dataset details in \secref{dataset}. We describe our proposed method and loss function in \secref{motion_dl}, and present our experimental results in \secref{exp-res}.
%followed by a brief description of two common methods implemented for respiration measurement in \secref{rr}. Description of the motion-based deep learning model is presented in \secref{motion_dl} . We then present the dataset generation details in \secref{infant-data}, our experiment results in \secref{exp-res}, and our conclusion in \secref{conc}. 
%Motivated by the lack of infant datasets and realizing that annotation of public videos can generate a diverse dataset that can be a good benchmark, we propose a novel infant respiration estimation dataset.

%-------------------------------------------------------------------------

%%%%%%%%% Related Work
\section{Related Work}
\label{sec:related_work}
% In this section, we present a brief overview of recent work on respiration estimation, including approaches using motion-tracking, color tracking, and thermal imaging.

%\com{[MW: This section is well written overall. We need to include context throughout about respiration estimation in infants, or in particular, the lack thereof. Here in the intro, you should mention that while in recent years researchers have started to develop infant-specific computer vision tools for monitoring faces and states (and anonymously cite our ICPR InfAnFace and preprint NNS papers), nothing exists specifically for infant respiration---existing models do not work well and there is no training/test data. Then in the individual subsections for each method, you can add one sentence reiterating that that method does not work off the shelf for infants.]}

Respiration induces cyclical expansion and contraction in the chest and abdomen regions. % during inhalation and exhalation. %Contact based respiration measurement systems such as respiration belts track the expansion and contraction to generate the respiration signal. %In \cite{koolen2015automated}, the authors use Eulerian video magnification \cite{wu2012eulerian} to magnify the subtle motion around the chest to detect respiration. However, Eulerian magnification imposes severe constraints on the subject motion and the background noise to allow magnification while supressing noise.
%In the case of infants lying in a crib captured using standard baby monitors, these conditions are less likely to be met. 
%Chatterji \textit{et al.} \cite{chatterjee2016real} use optical flow to estimate respiration rate from front and side poses of adults. 
Motion-based methods track this subtle motion in videos to estimate the respiration signal. Tveit \textit{et al.} \cite{tveit2016motion} use Rietz transform in a phase-based algorithm to track respiratory patterns and test their model on infant and adult subjects.
%s with manual annotation and on adults with ground truth waveforms from a pneumograph. 
Shao \textit{et al.} \cite{shao2014noncontact} estimate both heart rate and breathing rate simultaneously by tracking shoulder motion and color changes in a subject's face. Guo \textit{et al.} \cite{guo2021remote} improve the motion tracking using optical flow and human segmentation from pretrained deep learning models. Lorato \textit{et al.} \cite{lorato2021towards} use a two-stage approach to detect and reject video clips with severe motion, followed by a handcrafted feature-based rate estimation. Kyrollos \textit{et al.} \cite{kyrollos2021noncontact} use depth information along with RGB videos to improve the accuracy. FÃ¶ldesy \textit{et al.} \cite{foldesy2020reference} propose an incremental learning model to extract accurate frequency from a noisy estimate.
%also use a similar approach, they identify positive and negative samples based on the respiratory rate extracted in local windows from dense optical flow. For the negative samples, they incrementally train a classifier to extract respiratory rate from a noisy estimate. 

Another common approach for respiration estimation is based on the complex photoplethysmography (PPG) signal, a superposition of the slowly changing DC respiration component and the rapidly changing AC pulse component. Respiration waveform estimation based on color tracking was first introduced in DeepPhys \cite{chen2018deepphys}. 
%They argue that estimating respiration signal from motion tracking is prone to errors from extraneous movements of the subject and occlusions as it relies on accurate tracking of subtle motions in a noisy environment. 
%Instead of estimating respiration signal from tracking the chest motion, remote-PPG signal is used as the base for estimating heart-rate or respiratory rate from the videos directly. 
%In \cite{liu2020multi}, a multi-task model simultaneously estimates respiration and pulse signals using a single network branching out only in the final layer. %arguing that similar processing is needed for both the physiological signals. 
Temporal shift modules were introduced in \cite{liu2020multi}, in place of computationally expensive 3D convolutions, to improve efficiency of the model. %Although the trained models estimate the respiration signal with reasonable accuracy when tested on data from the same source as the training data, they fail to generalize well to subjects of different skin tones and extra subject motion. 
Villarroel \textit{et al.} \cite{villarroel_non-contact_2019} present a PPG signal extraction method for continuous infant monitoring in NICU setting. They use a multi-task CNN for segmenting skin pixels and detecting the presence of a subject in the camera, followed by simple pulse and breathing estimation.
In \cite{liu2021metaphys}, a few-shot adaptation of the base temporal shift deep learning model is used to improve results for individual subjects. In \cite{yu2019remote}, a novel loss based on Pearson correlation is used to train the model, improving the estimation accuracy compared to a model trained with $L^1$ or $L^2$ losses. To the best of our knowledge, our work provides the first comparison study between color- and motion-based approaches for infant subjects, complementing the one existing study for adult subjects \cite{wang2022camera}.
%only one study in the literature compares the color- and motion-based approaches for respiration estimation \cite{wang2022camera}.
%\tscan

%Thermal imaging is another popular area of research for non-contact respiratory rate estimation. 
Finally, thermal imaging can be used to track the alternating cold and warm air flowing from the nose during inhalation and exhalation. Such methods \cite{hochhausen2018estimating,jakkaew2020non,lorato2021towards} usually track a region of interest (ROI) in the nasal area across the video. 
%Respiration measurement is then done by calculating the peaks in the average brightness. 
Thermal cameras can be used in complete darkness, making them a good alternative to RGB cameras, but their setup cost prevents ubiquitous deployment. %Another drawback of thermal imagers is the reliance on ROI detection and tracking, which requires accurate frontal pose estimation of the subjects throughout the video.

% \iffalse
% \section{Existing Methods}
%To be covered:
%\begin{itemize}
    %\item Infant-Adult domain gap.
    %\item PPG based approaches are robust to extraneous motion, lighting changes thus preferred over motion-based approaches.
%\end{itemize}
% \subsection{Infant Respiration}
% \label{sec:rr}
% In this section we describe two baseline approaches studied in the literature for infant respiratory rate estimation: a color-based approach in \secref{ppg}, and a motion-based approach in \secref{opt-flow}.

% \subsubsection{Color-based Approach}
% \label{sec:ppg}
% Imaging photoplethysmography (iPPG) is a technique for estimating a physiological signal called blood volume pulse (BVP). The principle of iPPG is based on tracking subtle changes in the light reflected from skin due to the blood flow underneath. BVP is composed of several vital signs such as heart rate, respiration rate, and heart rate variability. Several methods have been proposed to estimate PPG signals from videos that extract the underlying heart rate and other vital information \cite{bian2020respiratory}. Alternatively, heart rate \cite{poh2010non, osman2015supervised} and breathing rate \cite{poh2010advancements} can be directly estimated by using the color changes.

% \flowrr
% The relation between skin pixel's color and the pulse and respiration signals is based on Shafer's dichromatic reflection model (DRM) \cite{chen2018deepphys, liu2020multi}:

% \begin{align}
% \begin{split}
%     C_k(t) \approx u_c.I_0 & + u_c.I_0.c_0.\psi(m(t), \theta(b(t), r(t))) \\ & + u_s.I_0.\phi(m(t), \theta(b(t), r(t))) 
%     \\ & + u_p.I_0.\theta(b(t), r(t)) + v_n(t), 
% \label{eqn:ppg}
% \end{split}
% \end{align}
% where $C_k(t)$ denotes the RGB vector at pixel $k$ and time $t$, $u_c$ denotes unit color of skin reflection, $I_0$ is the stationary part of luminance, $c_0$ denotes reflection strength, $m(t)$ denotes denotes variations in luminance caused due to extraneous effects such as flickering, camera or subject motion, etc., $b(t)$ and $r(t)$ denote blood volume pulse and respiration signals respectively, $\theta(\cdot)$ is a complex function of $b(t)$ and $r(t)$, $\psi(\cdot)$ models time-varying luminance due to physiological and non-physiological sources, $u_s$ denotes unit color vector of the light source, $\phi(\cdot)$ models the specular reflection variations, $u_p$ denotes relative pulsatile strengths, and $v_n(t)$ denotes the camera sensor quantization noise. A detailed derivation of the above equation can be found in \cite{liu2020multi}.

% As can be seen from \eqnref{ppg} the color variations in an image depend on the blood volume pulse signal, $b(t)$, and the respiration signal, $r(t)$. An end-to-end deep learning model based on the light reflection model defined above is introduced in DeepPhys \cite{chen2018deepphys}. We experiment with recent deep learning models \cite{chen2018deepphys, liu2023efficientphys, liu2020multi} that use the color-tracking principle and compare them against a motion-based approach for completion.

% %We use TS-CAN to baseline the infant respiration rate on the proposed dataset. This model uses a two-branch network with one branch to the model the motion and the other branch to generate an attention mask to refine the features extracted from the motion branch \figref{tscan}. They introduce temporal shift modules in the motion branch \cite{lin2019tsm} that efficiently mimic 3D convolutions without large computational requirements. The appearance branch generates attention masks that can help the motion branch to generate useful features from skin pixels compared to background. This helps in suppressing noise from the background and motion. The model outputs a signal with $N$ values for $N$ frames that represents the respiration signal for an $N$-frame video clip. %The signal is post-processed using a bandpass filter with $0.1$ Hz and $1.0$ Hz lower and upper cutoff frequencies to eliminate any signals out of the normal breathing range of infants. Final respiration rate for a video is computed using power spectral density analysis in the frequency domain and choosing the frequency with the maximal power.


% \subsubsection{Motion-based Approach}
% \label{sec:opt-flow}
% %As an alternative approach we estimate respiration rate using motion between consecutive frames in the video. Our approach closely follows \cite{guo2021remote}. First we estimate optical flow \cite{liu2009beyond} with a sampling rate of 6 fps to minimize the error from background noise and improve computational efficiency. We get flow vectors in horizontal and vertical directions and average these per frame. The resultant signals are filtered using a bandpass filter with [0.5, 1.0] lower and upper cutoff frequencies respectively matching the normal infant breathing rate of 30 to 60 breaths per minute. This filtering ensures only frequency components of interest remain in the signal. The filtered signals are used for the PSD analysis and we get the frequency with maximum power as the final output. This frequency is the estimated respiration rate for the input video clip.
% Respiration induces periodic expansion and contraction in the chest and abdomen regions in humans due to inhalation and exhalation. Contact based respiration measurement systems such as respiration belts are based on this principle, i.e., they mimic the motion of the chest to generate the respiration signal. Following a similar approach, the chest and abdomen motion can be tracked from the videos recorded by conventional cameras to estimate the respiration signal. Optical flow, first introduced by Horn and Schunck \cite{horn1981determining}, determines the brightness variation over time assuming brightness constancy and locally rigid motion. Optical flow estimation constrained by the brightness constancy assumption leads to a simple formulation in terms of spatial and temporal partial derivatives:
% \begin{equation}
%     I_x u + I_y v + I_t = 0
%     \label{eq:opt-flow}
% \end{equation}

% where, $I(x,y,t)$ is a spatio-temporal signal (RGB video in our case), $I_x$ is partial derivative of $I$ in $x$ direction, $I_y$ is partial derivative of $I$ in $y$ direction, $I_t$ is partial derivative of $I$ along time, $u$ and $v$ are the flow components along $x$ and $y$ directions respectively. Equation \eqref{eq:opt-flow} can be solved to determine the flow components in multiple ways \cite{horn1981determining, lucas1981iterative, dosovitskiy2015flownet}. We use a simple yet accurate implementation of coarse-to-fine optical flow \cite{liu2009beyond} for our experiments. To improve the efficiency and reduce the effects of noise, we sample frames at a rate of 6 Hz to compute the optical flow. The calculated flow vectors are averaged per frame to generate our displacement signals in horizontal and vertical directions. These signals are used to estimate the respiration rate as shown in \figref{flow-rr}.
% \fi

% %Although these signals contain the periodic motion information from respiration, they are contaminated by noise and extraneous motion in the video. Hence, the displacement signals are filtered using a bandpass filter with a lower and upper cutoff frequencies of $[0.1, 1.0]$ respectively matching the normal infant breathing rate of $6$ to $60$ breaths per minute. This filtering ensures only frequency components of interest remain in the signal. Respiration rate can be estimated from the constructed signal by either by zero-crossing or power spectral density (PSD) analysis. We perform fourier transform to convert the signal into frequency domain and perform PSD to get the frequency with maximum power. We also eliminate frequencies below and above the lower and upper cutoff frequencies after converting the signal to frequency domain.

% \iffalse
% \subsubsection{Color and Motion Fusion}

% %\com{[MW: In response to your question, yes, please add a very short section (about one paragraph) on the two fusion methods, basically just moving the description from the experiments section to here. You can call them waveform-based and rate-based and then just refer to them as such later on.]}
% Since the color-based and motion-based approaches over-estimate and under-estimate alternatively for different samples of the dataset, we experiment with two late fusion approaches to improve the accuracy of the fused respiration estimation.
% \begin{description}
%     \item[Rate-based Fusion] A straightforward fusion technique where the rates from the color-based and motion-based approaches are averaged.
%     \item[Waveform-based Fusion] A deeper fusion technique which amalgamates the underlying estimated waveforms. Since the motion and color based methods generate signals of different amplitude, the two signals should be brought to a similar scale before adding them. Consequently, we scale the motion-based approach signal to the color-based signal by first normalizing it using its mean and standard deviation, and then scaling the normalized signal with the mean and standard deviation of the output from color-based model; the resulting waveforms from both methods are then summed. 
% \end{description}
% \fi

%\datasets

\datasample

%\section{Datasets}
%label{sec:dataset}
%\com{[MW: I've moved the infant dataset from the methods section to here for conceptual uniformity. I think it's OK if the description for our dataset includes some light methodology, such as annotation details or waveform generation. Please update the introduction to give a very brief overview of the datasets: a public infant dataset created by us an intended as a benchmark, and adult datasets from the literature used for both training and testing.]}

%In the absence of public infant respiration datasets, we use the following two adult datasets for model training for comparative purposes. While these datasets contain subjects of different ages, genders, and head poses, the first is synthetic only, and both have highly controlled environments.

%\com{[MW: Please check the data usage agreements with SCAMPS and COHFACE and make sure we are citing/acknowledging as required.]}

%\begin{description}
%\item[SCAMPS \cite{mcduff2022scamps}] A synthetic dataset with diverse backgrounds, subject motion, and physiological states. The dataset contains 2800 video clips of 20 seconds at $320\times240$ resolution and 30 frames per second. Videos are synthesized using 3D face scans for each subject and the breathing waveforms are generated by choosing a breathing frequency between $8$ to $24$ breaths per minute at random (normal breathing rate in adults). The synthetic waveforms are smoothed and normalized. % to limit the maximum amplitude to $1.0$. Along with the respiration signal, this dataset contains ground-truth labels for PPG, inter-beat intervals, action units, and dense segmentation masks for beard, eyelashes, eyebrows, glasses, hair, skin, and clothing. 
%\item[COHFACE \cite{heusch2017reproducible}] 160 webcam clips from 40 subjects, each approximately 60 seconds long. The videos are recorded at $640\times480$ resolution and 20 frames per second, under both ambient and normal lighting conditions. The respiration signals are extracted from the thoracic stretch using a respiration belt readout 32 times per second. %Although this was the first publicly available dataset open for physiological measurement research, the videos in this dataset are severely compressed and require additional processing \cite{7961724}. Along with the breathing waveforms, this dataset also contains ground-truth signals for blood volume pulse.
%\end{description}

\section{AIR-125: An Annotated Infant Respiration Dataset}
\label{sec:infant-data}

%\com{make sure to reference the figure with dataset stats here. Also, summarize the respiration rate range (in breath per min) as part of this description. }

%\com{[MW: I think this was written while the dataset was still being finalized so it's understandable that there are details missing. Of course, later you should include the number of clips and number of subjects. You could also note here (or perhaps later in the experimental section) that while you take some steps to generate a realistic output waveform, only the annotated peaks are used in the evaluation (i.e., you compare the breath count, but not any other characteristics of the waveforms).]}

%The datasets introduced above depict two different approaches to create a physiological signal measurement dataset. SCAMPS generates synthetic 3D avatars and modulates the skin pixels according to the generated physiological signal. COHACE, on the other hand, captures videos and corresponding physiological signals using a contact-based measurement system. 

%Aside from featuring only adult data, the aforementioned datasets are limited in their usefulness for training generalizable models: SCAMPS, due to its use of synthetic rather than real data, and COHFACE, because its poses and backgrounds are identical across all samples. 
%However, both approaches have drawbacks: SCAMPS being a synthetic dataset exhibits a domain gap when testing on real videos and COHFACE dataset contains videos captured in a controlled environment with no variations in pose and background. 
Available physiological measurement datasets are created synthetically \cite{mcduff2022scamps} or extract reference physiological signals from contact-based systems and require the subjects situated in a controlled environment \cite{heusch2017reproducible,villarroel_non-contact_2019}. Unlike the existing datasets, AIR-125 features infant videos collected from a range of sources and settings to enable training and testing of flexible models useful for monitoring in everyday settings outside of the lab; our manual annotation process makes broad collection easier by eliminating equipment and recruitment constraints.
%We manually annotate videos for respiration, which, while labor intensive, opens up a more natural and representative set of videos to train and test a method intended for continuous monitoring of infants.
Our primary source is baby monitor footage from five infants, captured during daylight and nighttime sleep sessions in-crib by our clinical team under Institutional Review Board (IRB \#22-11-32) approval, with no other constraints on pose, lighting, clothing, and face visibility. The monitor switches between RGB and infrared (IR) modes depending on the light. For further diversity, we also source clips from three infant subjects on YouTube. From both sources, we extract a combined dataset of 125 videos, each approximately 60 seconds long. For respiration annotations, we parse through video frames, focusing on thoracic or abdominal motion to determine start times of exhalation cycles aided by the VGG Image Annotator \cite{dutta2019vgg}. % VIA allows users to run through a video frame-by-frame and annotate segments of the video that indicate an event of interest. %Since for respiration rate estimation the duration of each respiration cycle is inconsequential, we only annotate the exhalation start points in the video reducing the manual effort to create a large dataset. 
Annotated respiratory rates range from 18--42 breaths per minute; see \figref{datadist} for distributions by subject, pose, camera type, and respiratory rate.

%We created a dataset of $20$ videos from $3$ subjects with corresponding respiration signals for each video. A sample set of video frames from the dataset is shown in \figref{dataset-sample}.  We describe our dataset creation pipeline, depicted in \figref{dataset-gen}, in this section. 

%We collect publicly available infant videos from video hosting websites such as YouTube. Each video is then trimmed to create non-overlapping clips longer than 30 seconds with a maximum length of 60 seconds similar to COHFACE \cite{heusch2017reproducible}.  %Since we only use the start points of these annotated windows, that are carefully annotated, the default length does not create any ambiguity in the resulting ground-truth signal for our dataset. %\figref{data-dist} shows the respiration rate distribution in the dataset ranging from $23.5$ to $42$ breaths per minute.
\datadist

The annotations from each video clip are converted to an impulse sequence, with one pulse per exhalation start time label. To create smooth waveforms that are analogous to  signals from a contact-based respiration system, the impulse sequence is Gaussian filtered with an empirically determined radius of $4$ frames. The smoothed waveform is used as ground truth signal for our respiratory rate estimation methods. The video resolutions range from $854\times480$ to $1920\times1080$ and frame rates from 10--30 Hz. % These videos are stored in \texttt{mp4} format, while the ground truth signals are provided \texttt{hdf5} files.
%The videos from different sources are of different resolution and frame rates ranging from $10 - 30$ frames per second are stored in mp4 format and the ground truth signals are provided as hdf5 files. 
%The video clip and the waveform corresponding to each video from the different subjects constitute the proposed dataset. 
%\datagen

\section{Methodology}
\label{sec:motion_dl}

\subsection{AirFlowNet Architecture}
Color-based approaches track imperceptible color changes to estimate the remote photoplethysmography (rPPG) signal and isolate the breathing signal from a complex superposition of other vital signals \cite{chen2018deepphys}. Hence, these methods are prone to errors unless severe restrictions are imposed on the environment such as constant illumination, still subjects, and no camera motion. On the other hand, existing motion-based approaches use hand-crafted features, classical computer vision techniques, or pretrained deep learning models to track specific regions of interest to estimate breathing signals \cite{koolen2015automated,tveit2016motion,shao2014noncontact}. To alleviate the shortcomings of these two approaches, we propose our annotated infant respiration flow-based network (AIRFlowNet), depicted in \figref{dmem}, which processes optical flow input with a spatio-temporal convolutional network and isolates a clean respiration signal from a noisy video with possible subject or camera motion. Using optical flow input also eliminates the need to retrain a model when testing on videos from different camera types such as RGB and IR cameras. 

% Removing basic optical flow details, can be added back later based on space
%Optical flow, first introduced by Horn and Schunck \cite{horn1981determining}, determines the displacement vector field assuming brightness constancy and locally rigid motion. The brightness constancy assumption leads to a simple formulation of optical flow in terms of spatial and temporal partial derivatives:
%\begin{equation}
%    I_x u + I_y v + I_t = 0
%    \label{eq:opt-flow}
%\end{equation}

%where, $I(x,y,t)$ is a spatio-temporal signal (RGB video in our case), $I_x$ is partial derivative of $I$ in $x$ direction, $I_y$ is partial derivative of $I$ in $y$ direction, $I_t$ is partial derivative of $I$ along time, $u$ and $v$ are the flow components along $x$ and $y$ directions respectively. Equation \eqref{eq:opt-flow} can be solved to determine the flow components in multiple ways \cite{horn1981determining,lucas1981iterative,dosovitskiy2015flownet}. 

We use a simple yet accurate implementation of coarse-to-fine optical flow \cite{liu2009beyond} for our experiments. The optical flow is generated at $96\times 96$ resolution to preserve the subtle motion induced by respiration and reduce the effects of spatial noise in the flow calculation. The calculated flow vectors are stored in HSV color space at a frame rate of $5$ Hz. 

%\airflow

We base our convolutional network on EfficientPhys \cite{liu2023efficientphys}, adapting it to optical flow inputs. We replace the difference layer in EfficientPhys with a convolution layer followed by a batch-normalization layer as our inputs are Z-score normalized in the preprocessing stage. The first convolution layer follows a series of temporal shift modules \cite{liu2020multi} and convolution layers that efficiently compute temporal features by shifting the channels across time dimension. Self-attention blocks following the temporal shift modules refine the features to appropriately weigh different spatial locations that correspond to respiration motion. A dense layer is used at the end of the network to estimate a 1D respiration signal. Unlike EfficientPhys, which estimates the first order derivative of the signal, our model estimates the respiration signal directly.

%The convolution layer follows temporal shift modules similar to TS-CAN \cite{liu2020multi} to efficiently compute feature vectors across the time dimension. Attention layers in the model are not computed separately but calculated in the same branch using a different set of convolution layers and an attention block that computes a spatial attention mask to focus on flow values that represent the respiration motion while neglecting any other motion in the frame. Final layers include dense layers that compute the final respiration signal directly. Unlike the color-based approaches, the motion-based deep learning model estimates the respiration signal and not the first derivative of the signal. We train this network using $L^2$ loss on the computed and the ground-truth signal.
    
\subsection{Spectral Bandpass Loss}
\label{sec:psd_mse}
Current respiration estimation models train the networks using the $L^2$ loss between the ground truth signal and the predicted respiration waveform. While $L^2$ loss is useful for training with a ground truth signal that is precisely synchronized with the video, such as that obtained from electronic sensors, any temporal misalignment can lead to erroneous results. Since our manual annotations do not enjoy near-perfect alignment, we employ a new loss function that imposes a penalty entirely in the frequency domain, to prevent slight temporal misalignments from impeding effective learning of respiratory rate.

For any waveform $x=x(t)$, we use the fast Fourier transform $\mathcal{F}$ to define its corresponding power spectral density $X_\text{PSD} := |\mathcal{F}(x-x_0)|^2$, where $x_0$ is the temporal mean of $x$. After computing power spectral densities $Y_\text{PSD}$ and $\hat{Y}_\text{PSD}$ for the predicted ($y$) and the reference waveform ($\hat{y}$) respectively, we filter out the power from frequencies outside the normal infant breathing range of 0.3--1.0 Hz using a bandpass filter, $B(\cdot)$. The filtered power spectral densities are normalized to have a unit cumulative power. 
%For any frequency domain function $X=X(\xi)$ obtained from sampling, we define its norm $\norm{X}:=\sum_{\xi\in \Xi} X(\xi)$, where the sum is taken over the set of frequencies $\Xi$ less than the Nyquist frequency; $\frac{X}{\norm{X}}$ is the normalized $X$. Let $y=y(t)$ and $\hat{y}=\hat{y}(t)$ be the ground truth and predicted waveforms with temporal variable $t$, respectively. We define the \textbf{spectral bandpass loss $L_\text{sb}$} between $y$ and $\hat{y}$ by 
%\begin{equation}
%    L_\text{sb}(y,\hat{y}) = \norm{\frac{B(Y_\text{PSD})}{\|B(Y_\text{PSD})\|} - %\frac{B(\hat{Y}_\text{PSD})}{\|B(\hat{Y}_\text{PSD})\|}}_2
%\end{equation}
%with the outer norm being the $L^2$ norm.
We define the \textbf{spectral bandpass loss $L_\text{sb}$} between $y$ and $\hat{y}$ by 
\begin{equation}
    L_\text{sb}(y,\hat{y}) = \norm{\frac{B(Y_\text{PSD})}{\sum_{\xi \in \Xi}B(Y_\text{PSD})} - \frac{B(\hat{Y}_\text{PSD})}{\sum_{\xi \in \Xi}B(\hat{Y}_\text{PSD})}}_2,
\end{equation}
with the outer norm being the $L^2$ norm, and $\Xi$ constituting the set of frequencies in the power spectrum.

%We first subtract the mean from both the predicted and the ground truth signals resulting in $y$ and $\hat{y}$ respectively. Power spectral density (PSD) is then calculated by transforming the two signals into frequency domain using Fast Fourier Transform (FFT). We zero out the power in frequencies other than normal infant breathing frequency range (0.3 - 1.0 Hz). The normalized densities are used for calculating the loss function. Since this loss does not rely on the sign amplitude and accurately synchronized input and ground truth, we use the proposed loss function to train on our dataset. 
%\begin{align*}
%    Y_{PSD} &= |F(y)|^2 \\
%    \hat{Y}_{PSD} &= |F(\hat{y})|^2 \\
%    Loss &= \norm{\frac{BPass(Y_{PSD})}{\sum_{n=1}^N BPass(Y_{PSD})} - \frac{BPass(\hat{Y}_{PSD})}{\sum_{n=1}^N %BPass(\hat{Y}_{PSD})}} \\
%    \label{eq:psd_mse}
%\end{align*}
%where $F(.)$ denotes FFT, $Y_{PSD} and \hat{Y}_{PSD}$ denote PSD of output and ground truth signals, $BPass(.)$ denotes band pass filtering.

%\resultplot
\section{Evaluation and results}
\label{sec:exp-res}
%\com{[MW: Highlighting from my in-line comments: I think we need to have a clearer separation between the experimental setup on the one hand and the results and analysis on the other. I have set up new sections to reflect this.]}

\subsection{Experimental Setup}
\subsubsection{Datasets}
We evaluate our model on a public adult dataset, COHFACE \cite{heusch2017reproducible}, along with our infant dataset. COHFACE contains 160 webcam clips from 40 subjects, each approximately 60 seconds long. The videos are recorded at $640\times480$ resolution and 20 Hz, under both ambient and normal lighting conditions. Reference respiration signals come from a respiration belt readout at 32 Hz. 

\subsubsection{Training}
To train and evaluate our models on COHFACE, we use the rPPG-toolbox \cite{liu2022deep}, which provides a training framework for several physiological signal estimation models designed for adult subjects. We use the toolbox to train current state-of-the-art physiological measurement models: DeepPhys \cite{chen2018deepphys}, TS-CAN \cite{liu2020multi}, and EfficientPhys \cite{liu2023efficientphys}. For the color-based model training, the dataset is preprocessed to detect a face in each frame using Haar cascade classifier. The frames are then cropped around the face bounding box, and resized to a lower resolution of $96\times96$. All the models are trained with $L^2$ loss to generate a continuous signal for each clip. To train AIRFlowNet, we estimate the optical flow for each video and do not perform any face-based preprocessing. The rest of the training methodology is identical between all the trained models.

\subsubsection{Post-processing}
The estimated respiration signals are first filtered using a bandpass filter to remove noise from external sources. The lower and upper cut-off frequencies for the bandpass filter are $0.3$ Hz and $1.0$ Hz, covering the normal infant respiratory rates of 18--60 breaths per minute. The filtered signal is then transformed to frequency domain through a fast Fourier transform. We perform power spectral density analysis to determine the frequency with the maximal power as the predicted respiratory rate. We calculate three metrics that are commonly used in the literature to compare the different approaches: mean absolute error (MAE), root mean squared error (RMSE), and Pearson's correlation coefficient ($\rho$).

%\tabcomp
%\tabfuse

%Using FFT based post-processing. COHFACE validation results from three different configurations.

\subsection{Results and Analysis}
\label{sec:results}

%\com{MW: I changed the table titles (``Adult$\to$Infant'') but we still have the old section titles here, so we should discuss this.}

%To establish a baseline for the performance for the respiration models on infants, we train TS-CAN model on different synthetic and real dataset configurations to determine the best model for adults:
%\begin{description}
%\item[SCAMPS] Since the SCAMPS dataset includes a large variety of avatars, illumination, and backgrounds we train a model on this dataset and test its generalizability on real infant and adult videos. We train the model for 20 epochs with a learning rate of $9\times10^{-3}$ and choose the model with the lowest validation loss for further tests.

%\item[COHFACE] We randomly split the COHFACE dataset into train, validation, and test splits with the ratio: 0.5, 0.2, and 0.3 respectively, ensuring different subjects in each split. We train the model on the train split with a learning rate of $9\times10^{-3}$.

%\item[SCAMPS-COHFACE] While SCAMPS has large variations in the dataset and more videos, COHFACE has videos collected from real subjects. To utilize both the datasets' capabilities we finetune the model trained on the SCAMPS using the COHFACE dataset for $10$ epochs with a lower learning rate of $10^{-4}$ and choose the model with best performance on the COHFACE validation set. 
%\end{description}

%We present the results on the infant dataset and COHFACE test sets in \tabref{eval}.
%Among the three configurations, pretraining on SCAMPS and finetuning on COHFACE shows the best MAE and RMSE, while maintaining the Pearson correlation coefficient close to training on COHFACE data directly. This improvement is due to the large number of videos and variations in the synthetic data. However, when testing on the infant dataset, we observe that the model trained on COHFACE shows better results compared to the model pretrained on synthetic data. While the color-based models show better results when trained on real data, these models are still inferior to the unsupervised motion-based approach in all the metrics on the COHFACE test data. This performance difference stems from the fact that COHFACE data is collected in a controlled environment and does not include any external motion, thus making it suitable for a motion-based respiration estimation.

\tabcohface
\tabablation

We tabulate results from the following three experimental configurations (training dataset $\to$ testing dataset) in \tabref{cohface}.

\subsubsection{Adult $\to$ Adult}
We compare our model with color-based methods \cite{chen2018deepphys,liu2020multi,liu2023efficientphys} trained on COHFACE and the motion-based method from \cite{guo2021remote}. Since COHFACE has very still subjects with no external motion, motion-based approaches perform better than color-based models. %Since our approach prioritizes filtering out a clean respiration signal from noisy optical flow estimations, it outperforms Guo \textit{et al.} \cite{guo2021remote} in MAE and RMSE metrics.

%Although our model uses optical flow computed from a simple classical approach, we outperform the model from Guo \textit{et al.} \cite{guo2021remote}, in both MAE and RMSE, as they prioritize optical flow estimation over learning from noisy flow signals. 

\subsubsection{Adult $\to$ Infant}
We quantify the domain generalizability of all approaches by training on COHFACE and testing on AIR-125. The AIR-125 dataset is divided into train and test splits, with 50 clips from 3 subjects in the training split, and 75 videos from the remaining 5 subjects in the test split. %The test split from AIR-125 is preprocessed through the same pipeline used above for color-based and motion-based approaches appropriately. 
Our model demonstrates better generalizability as it is agnostic to camera type and brightness changes owing to the optical flow input.

\subsubsection{Infant $\to$ Infant}
For a fair comparison, we train and test both AIRFlowNet and the other models designed for adult subjects purely on AIR-125 data. Even when trained on infant data, the other models struggle to attain acceptable performance, exhibiting high mean absolute error and low Pearson's correlation. Our model achieves the best infant-domain performance, and the quantitative results even rival the performances of adult models tested on adult data.


% We train our model on the AIR-125 infant dataset, splitting the dataset into train, validation and test splits and training with the spectral bandpass loss. Our train split consists of 50 video clips from 3 subjects and the test split contains the rest 75 videos from 5 subjects. To perform a fair comparison with the other models, we also train the comparing models on the infant dataset and present the results in \tabref{cohface}. Our model trained on the infant dataset achieves the lowest MAE among the compared models lowering it from $4.16$ to $\textbf{2.91}$ breaths per minute. 
%Training with a small amount of data reduces the mean error by $30\%$, this proves the need for larger, more diverse real-datasets to boost the accuracy of the end-to-end deep learning models trained for infant respiration estimation. We hope our dataset serves as a good benchmark for infant respiration estimation techniques and encourages researchers to bridge the gap between infant and adult domains.



\subsubsection{Ablation Study}
To demonstrate the effectiveness of our spectral bandpass loss $L_\text{sb}$, we compare results of AIRFlowNet trained and tested on AIR-125 data under $L_\text{sb}$ and three other common loss functions---$L^1$, $L^2$, and negative Pearson loss ($-\rho$) \cite{yu2019remote}---in \tabref{ablation}. The $L_\text{sb}$ loss performs best, but $-\rho$ is also effective, likely because it also relaxes constraints on strictly matching the ground-truth signal, compared to the $L^1$ and $L^2$ losses. Note, however, that training with $-\rho$ requires ground truth waveforms (synthetically generated in AIR-125), whereas $L_\text{sb}$ can be trained with respiration exhalation timestamps alone.

%While the motion-based approach performs well on the COHFACE data, it does not generalize to the infant dataset collected in-the-wild with extraneous motion and background noise. We plot the output and ground truth rates in \figref{result-plot} and observe that color and motion approaches either overestimate or underestimate the respiration rates for different samples. Hence, we experiment with two variations of late-fusion: averaging respiration rates from the two approaches, and averaging respiration signals before calculating the frequency from the fused signal. We present the results from both the fusion approaches in \tabref{fuse}. Rate-based fusion results show better metrics compared to the color based method, as expected. Although the waveform-based fusion generates better Pearson correlation metric, it does not improve the MAE and RMSE. This is primarily due to the asynchrony of the two outputs creating multiple peaks when added out of phase.



\section{Conclusion}
\label{sec:conc}
We have presented the first public annotated infant respiration dataset, AIR-125, together with a novel deep learning model, AIRFlowNet, tuned for infant subjects and achieving state-of-the-art performance on AIR-125. Our model uses optical flow, spatio-temporal learning, and a new spectral bandpass loss function to optimize performance across varied lighting and camera settings, toward the eventual goal of automated, continuous, and purely video-based infant respiratory monitoring, both in NICU and at-home settings. Fruitful work in the future could include expanding the dataset scope and model performance, or achieving similar performance without using dense optical flow to enable immediate real-time monitoring in critical care situations.

% We have presented a novel annotated infant respiration dataset together with an new optical flow-based model with improved generalizability. We show that our model combined with our novel loss function adapts to varied lighting and camera type settings and improves the state-of-the-art respiration estimation in infant domain. The proposed dataset serves as an initial benchmark for future research on infant physiological signal estimation.
% %using end-to-end supervised and unsupervised deep learning models. 
% Although our model improves the accuracy of respiratory rate estimation, the need for optical flow inputs restricts real-time monitoring. Future work in this direction can be towards eliminating the need for dense optical flow estimation and directly learning the respiration signals from raw videos.

\bibliographystyle{splncs04}
\bibliography{biblio}

\end{document}
