\section{Conclusion}
\label{sec:conclusion}
% We introduced a weakly supervised framework to learn a rich embedding space between geolocation and fine-grained text. To carry out this work, we introduced a new large-scale cross-view dataset with 6.1M samples. Our method does not require any text labels and rather learns about the textual space by using ground-level images and CLIP embedding space. We then presented an efficient method to create zero-shot maps from fine-grained text prompts using our model. Our method offers higher flexibility for mapping tasks compared to traditional approaches and can be beneficial across many domains. A limitation of our current approach is that we learn the textual space for a location through CLIP and therefore, are limited by the expressive power of CLIP. Methods that do not rely on preexisting multimodal spaces can thus, be explored on a future work.

We presented a weakly supervised framework for learning a semantically rich embedding space between geolocation and fine-grained text. For this task, we introduced a new large-scale cross-view dataset with 6.1M samples. Our approach does not depend on text supervision, and learns textual representations of a location using only ground-level images and the CLIP embedding space. In addition to higher retrieval performance, we demonstrated that our framework can efficiently generate high-quality zero-shot maps from fine-grained text prompts. This ability to create maps from fine-grained text prompts offer greater flexibility when compared to traditional methods. Finally, we demonstrated that Sat2Cap embeddings can be used to generate dynamic captions that align with the ground-level scene. 

% One limitation of our current approach is its dependence on the pretrained CLIP model to learn textual concepts. This prompts the exploration of methods that do not depend on preexisting multimodal spaces in future work.