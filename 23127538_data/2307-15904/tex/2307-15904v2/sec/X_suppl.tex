\clearpage
\setcounter{page}{1}
\maketitlesupplementary


\section{Implementations Details}
We use a ViT-32B as the CLIP image encoder. This image encoder is kept frozen throughout the training. We use a ViT-32B architecture as the backbone for our Sat2Cap model. The Sat2Cap backbone is initialized using CLIP weights. Following~\cite{radford2021learning} we use an AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1e-05$ with $\beta_1=0.9$ and $\beta_2=0.98$. We use a learnable temperature parameter initialized at $\tau=0.07$. We use Cosine Annealing with Warm Restarts~\cite{loshchilov2016sgdr} as the learning rate scheduler. We augment the overhead images using RandomResizedCrop and RandAugment~\cite{cubuk2020randaugment}. The overhead images are normalized using the mean and standard deviation of the training set. The training was carried out using Nvidia A100 40GB GPU. Since a larger number of negative samples is beneficial for contrastive learning, we simulate a large batch size using a memory bank approach. We initialize a queue of size 9600 and fill it with precomputed ground-level image CLIP embeddings which are used as negative samples for computing the loss.  

\section{Text to Overhead Image Retrieval}
Our framework uses ground-level images as pseudo-labels to learn the textual concepts of geolocation. Although Sat2Cap does not require any text labels during training, it effectively learns an embedding space where geolocation and their fine-grained descriptions are well aligned. To show this, we randomly selected 1000 overhead images from our training set, and compute their Sat2Cap embeddings. For a given text query, we generate the CLIP\cite{radford2021learning} text embedding and compute its similarity with all images in the test set. Figure~\ref{fig:text2ov} shows examples of 4 closest overhead images retrieved for a given query.

We experiment with small perturbations of prompts to analyze how our retrieval results change with minute variations of query. We see in Figure~\ref{fig:text2ov}, the prompt ``people driving cars" retrieves city or residential areas. However, replacing the phrase ``driving cars" with ``riding horses" retrieves locations with farmland. Similarly, the prompt ``person on a long hike" exclusively retrieves mountainous regions, while the prompt ``person on a long run" retrieves images that looks like trails nearby residential areas. Hence, Sat2Cap embeddings demonstrate a good understanding of fine-grained variations of textual concepts.

\section{Learning Dynamic Concepts}
The dynamic encoder allows our model to learn temporally varying concepts over a location. Here we show more qualitative results showcasing the dynamic properties of our model. Figure~\ref{fig:dynamic_retrieval} shows the retrieval results at two different time settings (11:00 p.m. and 08:00 a.m.). Figure~\ref{fig:dynamic_caption} shows the captions generated for the same location over long (order of months) and short (order of hours) term variations. Finally, Figure~\ref{fig:dynamic_map} shows how our model can generate maps that adapt to temporal variations for a given prompt.


\section{Country Level Map of US}
We also create a zero-shot map of the US. However, due to the massively large area, we had to downsample our overhead image acquisition by 10x. Although we are predicting at a much coarser resolution, we still achieve a reasonable zero-shot map as seen in Figure~\ref{fig:usa_map}.

\section{Geolocalizing Textual Queries}
Our model can be used to localize textual queries at a finer resolution. For this experiment, we draw a $24 km^2$ bounding box over a region. We compute the Sat2Cap similarity for all the overhead images in that box with a given text query. We, then, normalize the similarities between $0$ and $1$ and clip the values below $0.5$. 
Figure~\ref{fig:localize} shows the results of this experiment. The red spot indicates the location with the highest activations for the given query. For each query, the left figure shows the total area of inference, and the right figure shows a fine-grained image at the location with the highest activation, obtained from Google. We see that our model makes reasonable localization for the given queries. For example: in (a) our model activates over a soccer stadium. Figure (b) shows that when we compose the concept of people with animals, our model shows very high activation in farm-like areas which is where these two concepts would most likely co-occur. 
These results show that our model can reasonably localize the most plausible point within a given area, where one might observe a given query. This property can be beneficial in solving visual search problems in the geospatial domain.


\section{Dataset}
We introduced a cross-view dataset with overhead images and co-located ground-level images taken from the YFCC100M~\cite{thomee2016yfcc100m} dataset. Figure~\ref{fig:dataset} shows a few samples from our dataset. The ground-level images provide us with detailed fine-grained concepts of a location that cannot be directly inferred when looking at the overhead imagery.  


% \section{Rationale}
% \label{sec:rationale}
% % 
% Having the supplementary compiled together with the main paper means that:
% % 
% \begin{itemize}
% \item The supplementary can back-reference sections of the main paper, for example, we can refer to \cref{sec:intro};
% \item The main paper can forward reference sub-sections within the supplementary explicitly (e.g. referring to a particular experiment); 
% \item When submitted to arXiv, the supplementary will already included at the end of the paper.
% \end{itemize}
% % 
% To split the supplementary pages from the main paper, you can use \href{https://support.apple.com/en-ca/guide/preview/prvw11793/mac#:~:text=Delete%20a%20page%20from%20a,or%20choose%20Edit%20%3E%20Delete).}{Preview (on macOS)}, \href{https://www.adobe.com/acrobat/how-to/delete-pages-from-pdf.html#:~:text=Choose%20%E2%80%9CTools%E2%80%9D%20%3E%20%E2%80%9COrganize,or%20pages%20from%20the%20file.}{Adobe Acrobat} (on all OSs), as well as \href{https://superuser.com/questions/517986/is-it-possible-to-delete-some-pages-of-a-pdf-document}{command line tools}.

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed

% Figure environment removed



% Figure environment removed

% Figure environment removed

% Figure environment removed


