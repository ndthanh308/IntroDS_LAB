% Figure environment removed

\section{Method}
Our objective is to learn an embedding space that describes the expected ground-level scene given a geographic location and an overhead image. We have ground-level images $\{g_1, g_2, ...g_n\}$, corresponding overhead images $\{o_1, o_2, ...o_n\}$, and respective metadata for the ground-level images $\{e_1, e_2, ...e_n\}$. Each $e_i$ contains the latitude and longitude information of the sample, as well as the date and time for when the ground-level image was captured. We also have a CLIP image encoder $f_{\theta}$ that generates CLIP embeddings for a given ground-level image.

\subsection{Dataset}
\label{dataset}
We created a large-scale cross-view dataset to train our model. The ground-level images in the dataset are taken from the YFCC100M~\cite{thomee2016yfcc100m} dataset. The YFCC100M dataset contains 99.3 million images, which are collected from Flickr. Our cross-view dataset uses a smaller sample from this collection which excludes all US imagery. We filter out all images that are not geotagged. The resulting dataset contains 6.1M images. Each of these images has a geolocation, timestamp, and other meta-information. For each image, we download an overhead image centered at its location. Specifically, we use the Bing Maps API to download $800$x$800$ patch satellite images at $0.6m/px$ resolution. We randomly sampled 100k images to be used for testing.

\subsection{Approach}
We initialize our Sat2Cap image encoder $g_\theta$ with the weights of $f_\theta$. A batch of ground-level images $\{g_1, g_2, ..., g_k\}$ is passed through the CLIP encoder to get the ground-level CLIP embeddings. These embeddings serve as the target for alignment. A batch of corresponding overhead images $\{o_1, o_2, ..., o_k\}$ is passed through the Sat2Cap image encoder to obtain the respective embeddings:  
%
\begin{equation}
    G_i = f_\theta{(g_i)}
\end{equation}
\begin{equation}
    O_i = g_\theta{(o_i)}
\end{equation}    

To align the overhead image embeddings with the ground-level CLIP embeddings, we contrastively train our model using the InfoNCE~\cite{oord2018representation} loss as follows:
%
\begin{equation}
    L = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(O_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(O_i \cdot G_j / \tau) }
\end{equation}

We also keep a queue $Q$ and fill it with CLIP embeddings of ground images. Here, $|Q|>>k$ and the embeddings from the queue are used as additional negative samples for our contrastive objective. As in MoCo~\cite{he2020momentum}, the queue is continuously updated during the training with the most recent batch. Minimizing this objective minimizes the distance between co-located overhead and ground-level images in the CLIP space. It is worth noting that throughout the training process, the CLIP image encoder is frozen. Hence, with our training procedure, we allow the overhead images to move close to images from their respective ground-level scene in the CLIP space. As a byproduct, the overhead images also move closer to the textual descriptions of ground-level images, which we are ultimately interested in mapping.


\subsection{Learning Dynamic Concepts of Places}

Many ground-level concepts are temporally dependent. Concepts like ``crowded street", or ``snowy place" can dramatically vary based on the exact time we query about them. To model such dynamic concepts, we condition Sat2Cap on the timestamps of the ground-level images.

For each sample, we extract the year, month, day, and hour in which the ground image was taken. We also add the geolocation information to provide a stronger signal to the model. We encode this meta-information using sin-cos encoding and pass it through a shallow, fully connected network, which we call the Dynamic Encoder $(h_\theta)$. The output from $h_\theta$ is added element-wise to the output from the Sat2Cap encoder before computing the objective.
% \begin{equation}
% \label{eq:finaleq}
%    L_{dynamic} = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(S_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(S_i \cdot G_j / \tau) } 
% \end{equation}
% % where $S_i$ is the sum of outputs from Sat2Cap encoder and Dynamic encoder.
% \begin{equation}
%     E_i = h_\theta{(e_i)}  
% \end{equation}
% where $e_i$ is the output of sin-cos encoding of the date, time, and location information for sample $i$.
\begin{equation}
    S_i = O_i + E_i
\end{equation}
where $O_i$ is the output from the Dynamic Encoder. Now the objective function changes to:
\begin{equation}
\label{eq:finaleq}
   L_{dynamic} = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(S_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(S_i \cdot G_j / \tau) } 
\end{equation}
where $S_i$ is the sum of outputs from the Image Encoder and Dynamic Encoder

% % \begin{equation}
% % O_i = g_\theta{(o_i)}
% % \end{equation}
% \begin{equation}
%     E_i = h_\theta{(e_i)}  
% \end{equation}
% where $e_i$ is the output of sin-cos encoding of the date, time, and location information for sample $i$.
% \begin{equation}
%     S_i = O_i + E_i
% \end{equation}
% Now the objective function changes to:
% \begin{equation}
% \label{eq:finaleq}
%    L_{dynamic} = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(S_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(S_i \cdot G_j / \tau) } 
% \end{equation}
% where $S_i$ is the sum of outputs from Sat2Cap encoder and Dynamic encoder.

% Our training dataset captures the ground-level scenes at various times. If the model is only allowed to learn using the overhead image of a location, it will be forced to learn an average concept for all temporal settings. By conditioning the problem on additional temporal data, our model learns different ground-level concepts for different times of the day and year. This ultimately allows Sat2Cap to dynamically adapt to temporal variations for the same geolocation. 

Our complete framework is shown in Figure~\ref{fig:Sat2Cap}. To prevent overfitting to the meta-information, we implement random dropout of the Dynamic Encoder during training. Our experiments from Section~\ref{retrieval_section} show that this addition significantly improves the performance of our model.
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\begin{table*}
 \begin{center}
 \small
    \begin{tabular}{c c c c |c c c|c c c}
 \hline
 \multicolumn{4}{c|}{Method} &
 \multicolumn{3}{c|}{Overhead2Ground (10K)} &
 \multicolumn{3}{c}{Ground2Overhead (10K)} \\
 
 \hline
 Model &Meta/Training &Dropout & Meta/Inference & R@5$\uparrow$ & R@10$\uparrow$ & Median-R$\downarrow$ & R@5$\uparrow$ & R@10$\uparrow$ & Median-R$\downarrow$\\
 \hline
 CLIP & - &- & - & 0.007 & 0.013 & 1700 & 0.108 & 0.019 & 2857 \\
 \hline
 \multirow{4}{2em}{ours} & \xmark & \xmark & \xmark & 0.398 & 0.493 & 15 & 0.356 & 0.450 & 11\\
 & \cmark & \xmark & \xmark & 0.322 & 0.413 & 34 & 0.254 & 0.343 & 20\\
 & \cmark& \xmark & \cmark & 0.368 & 0.467 & 23 & 0.298 & 0.398 & 13\\
 & \cmark& \cmark & \xmark & 0.467 & 0.564 & 13.5 & 0.366 & 0.462 & 7\\
 & \cmark& \cmark & \cmark & \textbf{0.493} & \textbf{0.591} & \textbf{12} & \textbf{0.390} & \textbf{0.482} & \textbf{6}\\


\end{tabular}
\end{center}
  \caption{\textbf{Cross-view retrieval performance of Sat2Cap model:} The table shows that CLIP performs poorly for the task of cross-view retrieval. Moreover, we study the performance of our model under various settings. The \textbf{Meta/Training} column ablates the impact of adding meta-information in training. The \textbf{Dropout} column ablates the impact of randomly dropping out the Dynamic Encoder in training. The \textbf{Meta/Inference} column ablates the impact of adding meta-information in inference. Our experiments show that using all three achieves the best performance.}
\label{table:retrieval}
\end{table*}
% Figure environment removed

% Figure environment removed

% Figure environment removed



