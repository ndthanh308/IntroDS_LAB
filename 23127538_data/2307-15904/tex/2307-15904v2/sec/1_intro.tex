
% Figure environment removed

\section{Introduction}
\label{sec:intro}

% Creating maps of different attributes is an important task in many domains. Traditionally, methods of mapping involve exhaustive data collection across vast regions, which is both time-consuming and labor-intensive. To address this issue, recent studies have explored the use of Deep Learning models, with their strong visual learning capabilities, to directly predict attributes of interest through overhead imagery.
Maps are a fundamental data product for a wide variety of domains. Traditional map-making involved extensive ground-based surveys. However, such methods are extremely time-consuming, expensive, and labor-intensive. As a result, overhead remote-sensing imagery has emerged as an important data modality for map creation. Machine learning methods have enabled scalable, accurate mapping of attributes using overhead imagery. The current paradigm of prior methods is to learn models that leverage the visual cues from overhead images to predict specific pre-defined attributes (e.g., a fixed set of land cover classes). Persello et al~\cite{persello2022deep} describe the numerous applications of deep learning in addressing the Sustainable Development Goals (SDGs), including crop monitoring, deforestation mapping, wildfire monitoring, and more. Salem et al.~\cite{salem2020learning} used overhead images to map transient attributes~\cite{laffont2014transient} and scene categories~\cite{zhou2017places} across large regions, while Streltsov et al.~\cite{streltsov2020estimating} predicted residential building energy consumption using overhead imagery. Similarly, Bency et al.~\cite{bency2017beyond} also used satellite images to map housing prices. 

All these prior methods focused on learning some specific pre-defined attributes. These attribute-specific models are highly restrictive as they cannot map anything beyond their preset list of variables. To overcome this limitation, we introduce a framework that enables the mapping of fine-grained textual descriptions of concepts that are observable only on the ground. Our approach allows the mapping of any concept that can be expressed in natural language and, thus, serves as a general framework for zero-shot mapping. For example: using our model, one can create a map of concepts like ``harvesting crops" or ``busy streets" without training any task-specific models.

% Numerous recent papers~\cite{radford2021learning, li2021align, yao2021filip} have delved into modeling the relationship between images and text. Models such as CLIP~\cite{radford2021learning} and ALBEF~\cite{li2021align} are trained on a large database of captioned images to learn a multimodal embedding space that unifies vision and text space. These embedding spaces can be utilized to learn the textual descriptors of a given image. However, in the context of overhead imagery, it often has a tendency to provide coarse and generic features of a given area. We observe that this property of overhead images also holds true in the CLIP embedding space, where overhead images collapse to a few coarse concepts like city, beach, or property. These images capture a broad perspective from above, offering limited insight into the intricate concepts and dynamics within the location. On the other hand, ground-level images and their respective CLIP embeddings provide detailed and fine-grained concepts of a location. Yet, several challenges hinder the direct utilization of ground-level imagery for mapping tasks. Firstly, ground-level images are sparsely available, i.e., obtaining a ground-level image for every location on Earth is not feasible. Secondly, the coverage and quality of a ground-level image for the same location can have large variations, which could introduce unwanted variations during inference.

% A textual description of any geolocation contains diverse concepts that are correlated with the visual cues present in an image captured at that location.
% Learning correspondence between geographical textual description and imagery is an important problem with its application to text-driven mapping. 
Modeling the relationship between text and images is a well-studied problem in deep learning. Numerous  methods~\cite{radford2021learning, li2021align, yao2021filip} have been proposed to learn the relationship between these two modalities. Models such as CLIP~\cite{radford2021learning} and ALBEF~\cite{li2021align} are trained on a large database of captioned images to learn a multimodal embedding space that unifies vision and text. However, we observe that directly using these models on overhead imagery leads to the collapse of representations to a few coarse textual concepts like city, beach, island, etc. Overhead images capture a broad perspective of a geolocation but offer limited insight into the intricate concepts and dynamics within the location. Hence, directly using overhead images with such models for text-driven mapping would only allow us to map coarse-level textual concepts. On the other hand, ground-level images and their respective CLIP embeddings provide detailed and fine-grained concepts of a location. Yet, several challenges hinder the direct utilization of ground-level imagery for mapping tasks. Firstly, ground-level images are sparsely available, i.e., obtaining a ground-level image for every location on Earth is not feasible. Secondly, the coverage and quality of a ground-level image for the same location can have large variations, which could introduce unwanted variations during inference.

To address these issues, we present a weakly-supervised cross-view approach for learning fine-grained textual concepts for geographic locations. In our work, ``fine-grained" refers to concepts that are observable in ground-level images but are hard to infer from the low-resolution view of satellite images. To do this, we first create a large-scale dataset with paired overhead and ground-level images. Our dataset uses a subset of the YFCC100M~\cite{thomee2016yfcc100m}. More details about the dataset are presented in Section~\ref{dataset}. Using this paired dataset, we learn the CLIP embeddings of the ground-level images at a given location. CLIP embeddings of ground-level images can describe detailed textual concepts of that location. Using the overhead image, our Sat2Cap model learns to predict the expected CLIP embeddings of the ground-level scene. Compared to the CLIP embeddings, Sat2Cap embeddings tend to capture more fine-grained textual concepts for a given geolocation.

To account for the temporal associations between various concepts and locations, our model is conditioned on temporal data, specifically, the date and time stamps from the Flickr imagery. This allows our model to learn concepts that can be dynamically adapted to different date and time settings. Our method is also weakly-supervised and thus does not require any text labels. To summarize, these are the primary contributions of our work:
\begin{itemize}
  \item A weakly-supervised approach for learning fine-grained textual concepts of geographic locations
  % \item A model for effective cross-view image retrieval between overhead images and ground-level images taken in the wild
  \item A zero-shot approach for creating large-scale maps from textual queries as seen in Figure~\ref{fig:generic_maps}
  \item A new large-scale cross-view dataset 
  \end{itemize}
