% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}


\section{Experiments and Results}
Our model learns a powerful geo-text embedding space that can be used for a variety of applications. We describe 4 experiments to demonstrate the efficacy of our model.

\subsection{Cross-View Image Retrieval}
\label{retrieval_section}
In this experiment, we show that our model learns a strong relationship between co-located overhead images and ground images in the CLIP space. We randomly sample 10,000 image pairs from the test set for this experiment. First, we predict the Sat2Cap embeddings for all overhead images and the corresponding CLIP embeddings for all the ground-level images in the test set. We then compute top-k (or R@k) and median rank metrics between the Sat2Cap overhead embeddings and the CLIP ground embeddings. The top-k metric measures how often the ground truth falls within the top-k closest images for a given query image. Table~\ref{table:retrieval} shows all of the cross-view retrieval results.

As a baseline, we use the retrieval results using CLIP overhead embeddings and CLIP ground embeddings. The cross-view retrieval for CLIP model is extremely low with R@10 score of 0.013 and a median rank of 1700. These low scores suggest that the overhead image CLIP embeddings do not contain high-frequency information about the ground-level scene. For our model, we first experiment without using the Dynamic Encoder. Just by contrastively training the Sat2Cap image encoder with ground-level CLIP embeddings, we achieve a high R@10 score of 0.493 and a median rank of 15, as seen in Table~\ref{table:retrieval}. 

All remaining experiments are conducted on models trained using the Dynamic Encoder. Table~\ref{table:retrieval} shows that initially, the retrieval scores drop when using the Dynamic Encoder. This happens because the model starts to overfit on the meta-information, ignoring important cues from the overhead images. We see a further 5.4\% drop in R@10 metrics when we use meta-information in training but remove it during inference. To reduce the possibility of overfitting, we randomly drop the Dynamic Encoder during training. Simply adding dropout during training increases the R@10 score by 12.4\%. Furthermore, removing meta-information during inference is less severe (2.7\%) when using dropout. Hence, our model achieves good cross-view retrieval scores even if meta-information is not provided during inference. 

Figure~\ref{fig:retrieval_results} shows the 9 closest images retrieved from a given overhead image. We see that our model is able to retrieve ground-level images by relating concepts rather than direct visual matching. For example, in (d), our model relates farmland with cattle and livestock, which are concepts that semantically match the location but are not visible in the overhead image.
% For example, in (a), our model retrieves images of people playing golf for an overhead image of a golf course. Similarly, in (d), our query image seems to be located over a farm. Here, Sat2Cap learns to associate the concept of farmland with cattle and livestock. It retrieves images of horses and goats which are concepts that likely reside in the location but are not visible in the overhead image.
% This suggests that our model maps fine-grained concepts of the ground-level scene to a given geolocation.
Sat2Cap is also capable of encoding temporal information in the embeddings whose results are shown in the supplementary material. 

Our results highlight that CLIP space, in itself, is remarkably poor at learning the relationship between a location and the corresponding ground-level scene. The low cross-view retrieval scores imply that we cannot accurately reason about ground-level scene using the CLIP embeddings of co-located overhead images. On the other hand,  the high cross-view retrieval scores of Sat2Cap suggest that the Sat2Cap embedding of a location approximates the CLIP embeddings of the images at the ground-level. This ultimately results in an emergent alignment between location and textual descriptions of the ground level.

\subsection{Embedding Space Comparison: CLIP vs.\ Sat2Cap}
In Section~\ref{retrieval_section}, we used retrieval scores to draw conclusions about the Sat2Cap and CLIP space. Here, we highlight the differences in Sat2Cap and CLIP space more explicitly. In our work, the term ``fine-grained" refers to the concepts that are easily visible at the ground level but are hard to infer from overhead images. For example, an overhead image over a location might tell us that it is a city, but ground-level images of that location capture detailed information such as how crowded the place is, if there are many restaurants around that area, does the location regularly hosts street festivals, and so on. Our method learns a model that takes an overhead view of a location and predicts a representation of ground-level images at that location in CLIP space.

We hypothesize that, for given overhead images, Sat2Cap learns diverse concepts while CLIP collapses to a few coarse concepts. To examine this, we run k-means clustering for both CLIP and Sat2Cap overhead image embeddings. We then compute the silhouette value for the resulting clusters, which tells us the quality of these clusters. This value ranges from -1 to +1, and it tells us how well the embeddings are separated/clustered for a given value of $k$; higher values indicate better clusters. Figure~\ref{fig:sill} shows the plot of silhouette value for different values of $k$. CLIP embeddings are well clustered for small values of $k$ but gradually worsen as we increase the number of clusters. This suggests that CLIP embeddings collapse to only a few concepts and are not further separable to reason about more diverse concepts. On the contrary, Sat2Cap embeddings perform poorly for low values of $k$ but outperform CLIP as the value of $k$ increases. The result indicates that the overhead embeddings from the Sat2Cap model are more diverse, allowing them to learn a variety of fine-grained concepts. The Sat2Cap embeddings can be well separated into a large number of clusters (concepts) and do not suffer the same issue of collapse as CLIP, making them suitable for the task of fine-grained mapping. 
% Figure environment removed

\begin{table*}
\begin{center}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{tabular}{c|ccc|cc|c}
\hline
\multirow{2}{*}{} & Meta   & \multirow{2}{*}{Dropout} & Meta   & \multicolumn{2}{c|}{Cosine Similarity} & \multirow{2}{*}{BERT Score} \\ \cline{5-6}
     & Training &        & Inference & MPNET\_Base\_v2 & E5\_Small &        \\ \hline
CLIP & -        & -      & -         & 0.1560 & 0.7519 & 0.7135 \\
\hline
ours              & \xmark & \xmark                   & \xmark & \textbf{0.3334}    & \textbf{0.7969}   & \textbf{0.7692}             \\
     & \cmark   & \xmark & \xmark    & 0.2727 & 0.7806 & 0.7476 \\
     & \cmark   & \xmark & \cmark    & 0.2888 & 0.7857 & 0.7572 \\
     & \cmark   & \cmark & \xmark    & 0.2755 & 0.7831 & 0.7533 \\
     & \cmark   & \cmark & \cmark    & 0.2755 & 0.7833 & 0.7537
\end{tabular}
\end{center}
\caption{\textbf{Caption generation alignment:} The table shows the alignment between captions generated from co-located overhead and ground-level images. Captions generated using Sat2Cap have better alignment with the descriptions of co-located ground-level scenery.}
\label{table:text_metrics}
\end{table*}
\subsection{Zero-Shot Map of Fine-grained Concepts}
\label{sec:zero_shot}
We use Sat2Cap embeddings to create zero-shot maps using fine-grained prompts. To do this, we choose a region and download high-resolution satellite images $(0.6m/px)$ over the region. We precompute the Sat2Cap embeddings for all the images.
%and save them on disc.
At inference time, for a given text query, we predict the CLIP embedding and compute its similarity with the overhead image embeddings. The process of computing the similarities only takes about 4 seconds. 

Figure~\ref{fig:fg_maps} shows the zero-shot map of Amsterdam for two prompts: ``Heavy trucks transporting goods", and ``People walking in the streets of downtown." Column 1 shows the maps generated using CLIP embeddings.
For the first prompt, the Sat2Cap activations are localized to the highway network of the city, whereas CLIP has many false positives over general built-up areas. This shows that Sat2Cap can pick the subtle nuances of fine-grained text that CLIP fails to do. Similarly, for the second prompt, CLIP shows high activations throughout the city, while Sat2Cap's activations are more localized to the downtown area.  

We also create country-level zero-shot maps for two countries: the Netherlands and England. We query the images with 3 prompts, each related to a distinct land cover class. Figure~\ref{fig:generic_maps} shows the comparison of our zero-shot maps with the land cover maps obtained from ESRI. We see that the zero-shot maps highly correlate with the ESRI land cover maps of the respective countries. 

%We also show a zero-shot map of the continental United States. However, due to the massive area to cover, we predicted for a 10x coarser grid. The results from this experiment are in the supplement.   

% We use the rich Sat2Cap embedding space to create zero-shot maps using textual prompts. Firstly, we choose three countries to create maps: Netherlands, England,and USA. Then, we download satellite imagery over covers these regions. Specifically, we download 800x800 patches of Bing Map Images at $0.6m/px$ resolution. We precompute the Sat2Cap embeddings for all the images and save them on a disc. Now for any given text query, we compute the similarity of the CLIP text embedding with all overhead images of the region. The process of computing similarities for an entire country takes only about 4 seconds. Hence, our framework is quite efficient for mapping large regions and thus could be easily extended to map the entire world. 

% Figure~\ref{fig:generic_maps} shows the maps for two prompts: ``Kids playing in the sand" and ``A busy street in downtown". We added the phrase ``a photo of" at the beginning of each prompt. For the first prompt, we see that our model activates locations around the ocean and beaches. Activations in both countries are high in areas where you might observe a kid playing in the sand. The second prompt activates locations with major cities in both countries. For England, we see high activations around London, Oxford, Birmingham, Manchester, Liverpool, etc. For the Netherlands, we see high activations in Amsterdam, Rotterdam, The Hague, Maastricht, Groningen, etc as well as other smaller cities. For evaluation, we use ESRI's Sentinel-2 landcover maps as ground-truth. From the landcover maps, we see that our model correctly activates the fine-grained prompt ``A busy street in downtown" in the urban areas. We show more examples of zero-shot mapping in the supplementary material. 

\subsection{Fine-grained and Dynamic Caption Generation}
% Our embedding space captures detailed and fine-grained textual concepts for geographic locations. While the CLIP space can only provide coarse-level generic descriptions from an overhead image, our model learns more fine-grained visual concepts that someone on the ground might observe. 
To generate captions from our embeddings, we use the CLIPCap~\cite{mokady2021clipcap} model. CLIPCap allows us to generate captions by learning a mapping from the CLIP space to the text space. Figure~\ref{fig:img1} shows qualitative examples of captions generated by passing CLIP embedings vs Sat2Cap embeddings as input for a given overhead image. We observe that when using CLIP embeddings of the overhead images as input, the captions generated by CLIPCap mostly describe generic concepts of a location like a beach, island, property, etc. In contrast, our Sat2Cap embeddings produce more fine-grained and aesthetically pleasing captions. Furthermore, we use the dynamic encoder to generate captions in two different months, for the same location. Figure~\ref{fig:img1} shows that Sat2Cap accurately models the seasonal variations and aligns the captions towards respective temporal inputs such as capturing winter concepts for January. However, in Figure (d), we see that the model does not add any winter-specific information for the January input. This is expected behavior since the image is from Australia, where the month of January falls in the middle of summer. The example further demonstrates that Sat2Cap learns a joint model of time and location.
% For example, in figure (a), CLIP generates the caption ``aerial view of a beach," missing out on other important details of the area. Our model, on the other hand, generates the caption ``sea facing apartment with swimming pool, terrace in a quiet residential area", capturing many detailed features of that location.
Fine-grained captions for a location should be well-aligned with the captions describing the ground-level scene. We quantitatively evaluate the quality of the captions by looking at their alignment with the text descriptions for respective ground-level images. First, we use the CLIPCap model to generate captions for ground-level images and use that as our ground truth. We then use the CLIP embeddings and Sat2Cap embeddings of the overhead images to generate the respective captions for that location. Table~\ref{table:text_metrics} shows the similarity metrics between the ground-truth text and generated text. To compute the cosine similarity, we use two different sentence transformers from HuggingFace, \textit{MPNET\_Base\_v2}~\cite{song2020mpnet} and \textit{E5\_Small}~\cite{wang2022text}. The captions generated using Sat2Cap embeddings demonstrate significantly better alignment with the ground-image captions than their CLIP counterparts. The results show that Sat2Cap captions better describe the ground-level characteristics of a given location. Table~\ref{table:text_metrics} also shows that the model trained without metadata has the highest cosine similarity and BERTScore with ground-level descriptions. We suspect this happens because of the uncertainty introduced by the use of a pretrained caption generator, i.e., small noise in the metadata introduces big deviations in the generated text. 
% However, including the metadata allows us to generate captions conditioned on both location and time, which is demonstrated further in the supplementary. 

% Using the metadata allows us to explicitly model temporal concepts in the caption generation process. Figure~\ref{fig:img1} shows the captions generated for two different months, May vs. January. The model reasonably accounts for the seasonal variations for different months. However, in Figure (d), we see that the model does not add any cold/winter-specific information for the January input. This is expected behavior since the image is from Australia, where the month of January falls right in the middle of summer. This example demonstrates that Sat2Cap learns of a joint model of time and location.
