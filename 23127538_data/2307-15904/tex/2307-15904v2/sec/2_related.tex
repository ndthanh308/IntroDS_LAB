\section{Related Works}
\label{sec:related}
\subsection{Deep Learning Based Mapping}
The ability to map attributes of interest is a fundamental task in Remote Sensing and has wide-ranging implications for achieving SDGs. Deep Learning methods have been used extensively~\cite{persello2022deep,kavvada2020towards,behrens2018multi,zong2019deepdpm,onishi2021explainable,greenwell2018goes} in recent years to make mapping efficient and scalable. Alhassan et al.~\cite{alhassan2020deep} finetuned imagenet pretrained models to make landcover predictions. Similarly~\cite{9553499,feizizadeh2023machine} leveraged large-scale annotated data from different sensors to improve land use and landcover classification using deep learning methods. Other works specifically focus on mapping visual attributes. For instance,~\cite{workman2017understanding} used features from both overhead and ground-level imagery and introduced a cross-view approach to map scenic-ness. Later works focused on creating dynamic maps: ~\cite{workman2020dynamic,salem2020learning} conditioned their model on temporal information along with overhead images to learn dynamic concepts for a given location. However, across this huge research area, the prevailing paradigm is to create task-specific models over a fixed set of attributes. We attempt to generalize the mapping process to \textit{any} attribute by introducing a framework to create maps of free-form textual prompts. 

\subsection{Vision-Language Pretraining}
Vision-Language (VL) models have shown great promise in their ability to model complex relationships between the vision and text space. ConVIRT~\cite{zhang2022contrastive} and VirTex~\cite{desai2021virtex} both introduced methods that used image-text pairs to learn rich visual representations. CLIP~\cite{radford2021learning} demonstrated the results of VL pretraining on a large-scale dataset (400M pairs) and validated the efficacy of large-scale VL pretraining for several downstream tasks. Florence~\cite{yuan2021florence} and ALIGN~\cite{jia2021scaling} further increased the scale of data by training on 900M and 1.8B pairs, respectively. Other works~\cite{li2021align,yao2021filip,yang2022unified,yang2022vision} have since focused on learning better VL embedding space. With the existence of these powerful pretrained VL models, many researchers have utilized their embedding spaces to solve specific downstream tasks. CLIPCap~\cite{mokady2021clipcap} and~\cite{cho2022fine} used CLIP space to generate image captions. Models like~\cite{ramesh2022hierarchical,nichol2021glide,wang2022clip} utilized the CLIP space for text-to-image generation. 
% In our work, we utilize the rich CLIP space to bridge the gap between geolocations and their fine-grained textual descriptions.
% Several works~\cite{hendriksen2022extending,sain2023clip,baldrati2022effective} have also used the CLIP space for image retrieval tasks. 

Recently, there has been work in creating image-text datasets of overhead images and captions for VL pertaining of geospatial models. ChatEarthNet~\cite{yuan2024chatearthnet} created a dataset with paired image and text captions using ChatGPT. Similarly, SkyScript~\cite{wang2023skyscript} used OpenStreetMap (OSM) data to create overhead image and text-paired datasets for VL pretraining. However, both approaches only gather coarse textual information for a given location using either the low-resolution visual cues from an overhead image or fixed preset tags from OSM data.

Our approach fundamentally differs from prior work as we try to capture the more intricate concepts occurring at the ground-level of a given location by leveraging the corresponding crowdsourced images. Utilizing the ground-level images uploaded from a location allows us to access extremely fine-grained information of that location that is not distillable solely from satellite images or OSM data.  


  
