\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacvrebuttal}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage[export]{adjustbox}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}
\wacvfinalcopy
%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\wacvPaperID{776} % *** Enter the WACV Paper ID here

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images \\
(Supplementary Material)}  % **** Enter the paper title here

\maketitle
%\thispagestyle{empty}
\appendix

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
\section{Text to Overhead Image Retrieval}
Our framework uses ground-level images as pseudo-labels to learn the textual concepts of geolocation. Although Sat2Cap does not require any text labels during training, it effectively learns an embedding space where geolocation and their fine-grained descriptions are well aligned. To show this, we randomly selected 1000 overhead images from our training set, and compute their Sat2Cap embeddings. For a given text query, we generate the CLIP\cite{radford2021learning} text embedding and compute its similarity with all images in the test set. Figure~\ref{fig:text2ov} shows examples of 4 closest overhead images retrieved for a given query.

We experiment with small perturbations of prompts to analyze how our retrieval results change with minute variations of query. We see in Figure~\ref{fig:text2ov}, the prompt ``people driving cars" retrieves city or residential areas. However, replacing the phrase ``driving cars" with ``riding horses" retrieves locations with farmland. Similarly, the prompt ``person on a long hike" exclusively retrieves mountainous regions, while the prompt ``person on a long run" retrieves images that looks like trails nearby residential areas. Hence, Sat2Cap embeddings demonstrate a good understanding of fine-grained variations of textual concepts.

\section{More Large-scale Textual Maps}
We create country-level maps of England and Netherlands for four different prompts: a) Farmers harvesting crops, b) Cars stuck in traffic, c) Animals grazing in the fields, and d) People fishing on a boat. To generate the textual maps for each prompt, we compute the Sat2Cap embeddings for all images of the country and compute its similarity with the CLIP text embeddings of the given prompt. We normalize the similarities and plot those to create the textual maps. Figure~\ref{fig:moremaps} shows textual maps of Netherlands and England for each prompt. We also placed a landcover map on the bottom of each text map as a reference of places with likely activations for the given prompt.

By comparing with the respective landcover maps, we see that the Sat2Cap embeddings activates reasonable locations on a map for a given prompt. For example, the prompt ``Farmers harvesting crops" gets activated mostly in cropland, while the prompt ``Cars stuck in traffic" is activated in urban areas. Similarly, the textual maps of the prompts ``Animals grazing in the fields", and ``People fishing on a boat" look similar to the rangeland and water landcover respectively. In (d), we see high activations in the top-left corner for England, which does not match the water landcover. This region is the Lake District, which has numerous beautiful lakes.


\section{Dynamic Caption Generation}
We take a single overhead image and show the dynamic captions Sat2Cap embedding can generate. Figure~\ref{fig:dynamiccaption} shows our results on a test image at four different temporal settings. The generated captions capture both the semantic concepts from the given image as well as the temporal concepts that are added to it.  As you move from May to December, the concepts of winter become more prominent in the captions. Similarly, as you move from 10:00 am to 11:00 pm, we see the concepts associated with night are better highlighted. One interesting observation is that we are not getting trivial changes, such as simply adding \textit{in winter} or \textit{at night} to the captions. Rather, the entire concept that the captions describe also changes.

\section{Dataset}
We introduced a cross-view dataset with overhead images and co-located ground-level images taken from the YFCC100M~\cite{thomee2016yfcc100m} dataset. Figure~\ref{fig:dataset} shows a few samples from our dataset. The ground-level images provide us with detailed fine-grained concepts of a location that cannot be directly inferred when looking at the overhead imagery.  

\section{Overhead to Ground Image Retrieval}
In figure~\ref{fig:ov2gr}, we show additional results of overhead-to-ground image retrieval. We see that Sat2Cap embeddings accurately relate overhead imagery with fine-grained ground-level concepts. An interesting thing to note is that the relationship is not based primarily on visual feature matching but rather based on agreement of concepts. For example, the overhead image of a running track retrieves images of people playing different sports, while an image over the ocean retrieves images of people enjoying various water sports.

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed


%-------------------------------------------------------------------------


{\small
\bibliographystyle{ieee}
\bibliography{supplement}
}

\end{document}
