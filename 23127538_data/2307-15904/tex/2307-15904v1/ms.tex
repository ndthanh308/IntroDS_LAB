% \documentclass[10pt,twocolumn,letterpaper]{article}

% \usepackage{wacv}
% \usepackage{times}
% \usepackage{epsfig}
% \usepackage{graphicx}
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{booktabs}
% \usepackage{float}
% \usepackage{caption}
% \usepackage{pifont}
% \usepackage{stfloats}
% \usepackage{multirow}
% \usepackage{tabularx}
% \usepackage{authblk}

% % Include other packages here, before hyperref.



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %
% %%% IMPORTANT - These next three lines are CRUCIAL.
% %               (1) PLEASE enter your paper ID (given by CMT) replacing the
% %                   '****' right below here with the ID from CMT.
% %               (2) If you are submitting to the Algorithms track, then uncomment
% %                   the \wacvalgorithmstrack line below. If you are submitting to the
% %                   Applications track, then uncomment the \wacvapplicationstrack line.
% %                   If you do not update this properly, we cannot guarantee
% %                   that your paper will be evaluated according to the correct
% %                   review criteria!
% %               (3) Leave the \wacvfinalcopy commented out for the submission
% %                   version, but UNCOMMENT it for your CAMERA-READY upload.
 
% %(1)
% % \def\wacvPaperID{776} % Enter the WACV Paper ID here

% %(2)
% %\wacvalgorithmstrack   % Uncomment this line if you are submitting to the Algorithms Track.
% % \wacvapplicationstrack % Uncomment this line if you are submitting to the Applications Track.

% %(3)
% \wacvfinalcopy % *** Uncomment this line for the final submission


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% % If you comment hyperref and then uncomment it, you should delete
% % egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% % run, let it finish, and you should be clear).
% % \ifwacvfinal
% % \usepackage[breaklinks=true,bookmarks=false]{hyperref}
% % \else
% % \usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
% % \fi



% %%% PUT INSTITUTIONS ON THE SAME LINE
% \makeatletter
% \renewcommand\AB@affilsepx{\hspace{1cm}  \protect\Affilfont}
% \makeatother
% %%% END PUT INSTITUTIONS ON THE SAME LINE

% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% \title{Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images }

% \author[1]{Aayush Dhakal}
% \author[1,2]{Adeel Ahmad}
% \author[1]{Subash Khanal}
% \author[1]{Srikumar Sastry}
% \author[1]{Nathan Jacobs}

% \affil[1]{\textit{Washington University in St. Louis}}
% \affil[2]{\textit{Taylor Geospatial Institute}}

% % \author{Aayush Dhakal^{1}\\
% % {\tt\small a.dhakal@wustl.edu}
% % % For a paper whose authors are all at the same institution,
% % % omit the following lines up until the closing ``}''.
% % % Additional authors and addresses can be added with ``\and'',
% % % just like the second author.
% % % To save space, use either the email address or home page, not both
% % \and
% % Adeel Ahmad^{2}\\
% % {\tt\small adeel.geog@pu.edu.pk}
% % \and
% % Subash Khanal^{1}\\
% % {\tt\small k.subash@wustl.edu}
% % \and
% % Srikumar Sastry^{1}\\
% % {\tt\small s.sastry@wustl.edu}
% % \and
% % Nathan Jacobs^{1}\\
% % {\tt\small jacobsn@wustl.edu} }

% \begin{document}
% % Pages are numbered in submission mode, and unnumbered in camera-ready
% \pagestyle{empty}
% %%%%%%%%% TITLE

% \maketitle
% \thispagestyle{empty}

\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{wacv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{pifont}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{anyfontsize}


% Include other packages here, before hyperref.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%%% IMPORTANT - These next three lines are CRUCIAL.
%               (1) PLEASE enter your paper ID (given by CMT) replacing the
%                   '****' right below here with the ID from CMT.
%               (2) If you are submitting to the Algorithms track, then uncomment
%                   the \wacvalgorithmstrack line below. If you are submitting to the
%                   Applications track, then uncomment the \wacvapplicationstrack line.
%                   If you do not update this properly, we cannot guarantee
%                   that your paper will be evaluated according to the correct
%                   review criteria!
%               (3) Leave the \wacvfinalcopy commented out for the submission
%                   version, but UNCOMMENT it for your CAMERA-READY upload.
 
%(1)
\def\wacvPaperID{776} % Enter the WACV Paper ID here

%(2)
%\wacvalgorithmstrack   % Uncomment this line if you are submitting to the Algorithms Track.
\wacvapplicationstrack % Uncomment this line if you are submitting to the Applications Track.

%(3)
\wacvfinalcopy % *** Uncomment this line for the final submission


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\ifwacvfinal
\usepackage[breaklinks=true,bookmarks=false]{hyperref}
\else
\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}
\fi

% Pages are numbered in submission mode, and unnumbered in camera-ready
\pagestyle{plain}

\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\hypersetup{
  colorlinks,
  citecolor=green,
  linkcolor=red,
  urlcolor=blue}

\begin{document}

%%%%%%%%% TITLE
\title{Sat2Cap: Mapping Fine-Grained Textual Descriptions from Satellite Images }

\author{\fontsize{11}{12}\selectfont Aayush Dhakal$^{1}$\\
%{\fontsize{9}{10}\selectfont a.dhakal@wustl.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
 \fontsize{11}{12}\selectfont Adeel Ahmad$^{1,2}$\\
% {\fontsize{9}{10}\selectfont adeel.geog@pu.edu.pk}
\and
\fontsize{11}{12}\selectfont Subash Khanal$^{1}$\\
%{\fontsize{9}{10}\selectfont k.subash@wustl.edu}
\and
\fontsize{11}{12}\selectfont Srikumar Sastry$^{1}$\\
%{\fontsize{9}{10}\selectfont s.sastry@wustl.edu}
\and
\fontsize{11}{12}\selectfont Nathan Jacobs$^{1}$\\
%{\fontsize{9}{10}\selectfont jacobsn@wustl.edu}
\and
\fontsize{11}{12} $^{1}$\textit{Washington University in St. Louis}\\
\and
\fontsize{11}{12} $^{2}$\textit{Taylor Geospatial Institute}\\
}

\maketitle

% Figure environment removed



%%%%%%%%% ABSTRACT
\begin{abstract}
% Automatic scalable mapping of a geographic region is an important task with a wide range of applications. Previous studies have approached this task by developing geospatial models that predict over a fixed set of attributes using overhead imagery. However, these models have a major limitation; once trained, they can only map their predefined attributes. This property leads to very constrained models that can only solve highly specific tasks. 
We propose a novel weakly supervised approach for creating maps using free-form textual descriptions (or captions). We refer to this new line of work of creating textual maps as zero-shot mapping. Prior works have approached mapping tasks by developing models that predict over a fixed set of attributes using overhead imagery. However, these models are very restrictive as they can only solve highly specific tasks for which they were trained. Mapping text, on the other hand, allows us to solve a large variety of mapping problems with minimal restrictions. To achieve this, we train a contrastive learning framework called Sat2Cap on a new large-scale dataset of paired overhead and ground-level images. For a given location, our model predicts the expected CLIP embedding of the ground-level scenery. Sat2Cap is also conditioned on temporal information,  enabling it to learn dynamic concepts that vary over time. Our experimental results demonstrate that our models successfully capture fine-grained concepts and effectively adapt to temporal variations. Our approach does not require any text-labeled data making the training easily scalable. The code, dataset, and models will be made publicly available.
\end{abstract}
%%%%%%%%% BODY TEXT
\section{Introduction}

Creating maps of different attributes is an important task in many domains. Traditionally, methods of mapping involve exhaustive data collection across vast regions, which is both time-consuming and labor-intensive. To address this issue, recent studies have explored the use of Deep Learning models, with their strong visual learning capabilities, to directly predict attributes of interest through overhead imagery. Salem et al.~\cite{salem2020learning} used overhead images to map transient attributes~\cite{laffont2014transient} and scene categories~\cite{zhou2017places} across large regions, while Streltsov et al.~\cite{streltsov2020estimating} predicted residential building energy consumption using overhead imagery. Similarly, Bency et al.~\cite{bency2017beyond} also used satellite images to map housing prices. However, all these prior methods focused on learning some specific pre-defined attributes. These attribute-specific models are quite restrictive as they cannot map anything beyond their preset list of variables. To overcome this limitation, we created a novel framework that enables us to map fine-grained textual descriptions (captions). Our approach allows us to theoretically map anything that can be expressed in natural language, and thus, serves as a general framework for zero-shot mapping.

Recently, several works~\cite{radford2021learning, li2021align, yao2021filip} have delved into modeling the relationship between images and text. Models such as CLIP~\cite{radford2021learning} and ALBEF~\cite{li2021align} are trained on large database of captioned-images to learn a multimodal embedding space that unifies vision and text space. These embedding spaces can be utilized to learn the textual descriptors of a given image. However, a limitation of using overhead imagery is its tendency to provide coarse and generic features of a given area. We observe that this property of overhead images holds true in the CLIP embedding space as well, where these images are related to coarse textual concepts like city, beach, or property. These images capture a broad perspective from above, offering limited insight into the intricate concepts and dynamics within the location. Ground-level images, on the other hand, provide more detailed information about a place. The CLIP embedding space has a better understanding of fine-grained concepts for ground-level images since it was primarily trained on them and their descriptive captions. Yet, several challenges hinder the direct utilization of ground-level imagery for mapping tasks. Firstly, ground-level images are sparsely available i.e., obtaining a ground-level image for every location on Earth is not feasible. Secondly, the coverage and quality of a ground-level image for the same location can have large variations which could introduce unwanted variations during inference.

To address these issues, we present a novel weakly-supervised cross-view approach for learning fine-grained, and dynamic textual concepts for geographic locations. First, we create a large-scale dataset with paired overhead and ground-level images. Our dataset uses a subset of the YFCC100M~\cite{thomee2016yfcc100m}. More details about the dataset are presented in Section~\ref{dataset}. Using this paired dataset, we learn the CLIP distribution of the ground-level scene for a given location. CLIP embeddings of ground-level images can describe detailed textual concepts of that location. Our Sat2Cap model learns to predict the expected CLIP embedding of the ground-level scene using the overhead image. Compared to the CLIP embeddings, Sat2Cap embeddings tend to capture more fine-grained textual concepts for a given geolocation as seen in Figure~\ref{fig:img1}.

To account for the temporal associations between various concepts and a location, our model is conditioned on temporal data, specifically, the date and time stamps from the Flickr imagery. This allows our model to learn fine-grained concepts that can be dynamically adapted to different date and time settings. Figure~\ref{fig:img1} shows an example of CLIP-generated coarse captions vs.~Sat2Cap-generated fine-grained dynamic captions.

Our method is also weakly-supervised and thus does not require any text-labeled data. Creating a large-scale dataset of fine-grained captions and geolocation can be challenging. However, our approach only requires geotagged and timestamped ground-level images which are easily accessible and scalable. Additionally, our framework is designed to learn high-resolution information of a location. This rich information can be used as an additional signal to solve a number of other downstream tasks. The following points summarize the primary contributions of our work:
\begin{itemize}
  \item A novel weakly-supervised approach for learning fine-grained dynamic textual concepts of geographic locations
  \item A model for effective cross-view image retrieval between overhead images and ground-level images taken in the wild
  \item A zero-shot approach for creating large-scale textual maps
  \item A new large-scale cross-view dataset 
  \end{itemize}

%-------------------------------------------------------------------------
\section{Related Works}
\subsection{Deep Learning Based Mapping}
Creating maps of attributes of interest is an important task in many domains. Deep Learning methods have been used extensively~\cite{ilic2019deep,behrens2018multi,zong2019deepdpm,onishi2021explainable,greenwell2018goes} in recent years to make mapping efficient and scalable. Alhassan et al.~\cite{alhassan2020deep} finetuned imagenet pretrained models to make landcover predictions. Similarly~\cite{9553499,feizizadeh2023machine} leveraged large-scale annotated data from different sensors to improve landuse and landcover classification using deep learning methods. 
Apart from remote sensing, other areas have also leveraged deep learning for their own mapping tasks. Using high-resolution images,~\cite{bickel2020deep} trained several deep learning architectures for automated CNN-based mapping of Martian rockfalls. On the other hand,~\cite{zhang2021data} used an unsupervised approach to map regions with high ``Au" deposits.

There have been other works that specifically focus on mapping visual attributes. For instance,~\cite{workman2017understanding} used features from both overhead and ground-level imagery, and introduced a cross-view approach to map scenicness. Later works focused on creating dynamic maps. Both ~\cite{workman2020dynamic,salem2020learning} conditioned their model on temporal information along with overhead images to learn dynamic concepts for a given location. While the prior works mostly focus on mapping some specific attribute, we attempt to generalize the mapping process. Hence, in our work, we introduce a framework to create maps of free-form textual prompts, which we call Textual Maps. 

\subsection{Vision-Language Pretraining}
Recently, Vision-Language (VL) models have shown great promise in their ability to model complex relationships between the vision and text space. ConVIRT~\cite{zhang2022contrastive} and VirTex~\cite{desai2021virtex} both introduced methods that used image-text pairs to learn rich visual representations. CLIP~\cite{radford2021learning} demonstrated the results of VL pretraining on a large-scale dataset (400M pairs) and validated the efficacy of large-scale VL pretraining for several downstream tasks. Florence~\cite{yuan2021florence} and ALIGN~\cite{jia2021scaling} further increased the scale of data by training on 900M and 1.8B pairs respectively. Other works~\cite{li2021align,yao2021filip,yang2022unified,yang2022vision} have since focused on learning better VL embedding space. With the existence of these powerful pretrained VL models, many researchers have utilized their embedding spaces to solve specific downstream tasks. CLIPCap~\cite{mokady2021clipcap}, and~\cite{cho2022fine} used CLIP space to generate image captions. Other models like~\cite{ramesh2022hierarchical,nichol2021glide,wang2022clip} utilized the CLIP space for text-to-image generation. Several works~\cite{hendriksen2022extending,sain2023clip,baldrati2022effective} have also used the CLIP space for image retrieval tasks. In our work, we utilize the rich CLIP space to bridge the gap between geolocations and their fine-grained textual descriptions.


  
% Figure environment removed
\section{Method}
Our objective is to learn an embedding space that describes the expected ground-level scene given a geographic location and an overhead image. Secondly, our embedding space needs to dynamically adapt to temporal manipulations for the same location. We have ground-level images $\{g_1, g_2, ...g_n\}$, corresponding overhead images $\{o_1, o_2, ...o_n\}$, and respective metadata for the ground-level images $\{e_1, e_2, ...e_n\}$. Each $e_i$ contains the latitude and longitude information of the sample, as well as the date and time when the ground-level image was captured. We also have a CLIP image encoder $f_{\theta}$ that generates CLIP embeddings for a given ground-level image.

\subsection{Dataset}
\label{dataset}
We created a large-scale cross-view dataset to train our model. The ground-level images in the dataset are taken from the YFCC100M~\cite{thomee2016yfcc100m} dataset. The YFCC100M dataset contains 99.3 million images, collected from Flickr. Our cross-view dataset uses a smaller sample from this collection which excludes all US imagery. Our dataset contains close to 6M images. Each of these images has a geolocation, timestamp, and other meta information such as tags, description, camera type, etc. For each Flickr image, we download an overhead image centered at its location. We use the Bing Maps API to download $800$x$800$ patch satellite images at $0.6m/px$ resolution.  

\subsection{Approach}
We initialize our Sat2Cap image encoder $g_\theta$ with the weights of $f_\theta$. A batch of ground-level images $\{g_1, g_2, ..., g_k\}$ is passed through the CLIP encoder to get the ground-level CLIP embeddings. These embeddings serve as the target for alignment. A batch of corresponding overhead images $\{o_1, o_2, ..., o_k\}$ is passed through the Sat2Cap image encoder to obtain the embeddings, as follows:  
%
\begin{equation}
    G_i = f_\theta{(g_i)}
\end{equation}
\begin{equation}
    O_i = g_\theta{(o_i)}
\end{equation}    

To align the overhead image embeddings with the ground-level CLIP embeddings, we contrastively train our model using the InfoNCE~\cite{oord2018representation} loss as follows:
%
\begin{equation}
    L = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(O_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(O_i \cdot G_j / \tau) }
\end{equation}

We optimize this loss to minimize the distance between co-located overhead and ground-level images in the CLIP space. It is worth noting that throughout the training process, the CLIP image encoder remains frozen. Hence, with our training procedure, we essentially allow the overhead images to move close to images from their respective ground-level scene in the CLIP space. Our results from Section~\ref{retrieval_section} show that Sat2Cap learns a strong correlation between co-located overhead and ground-level images.

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\begin{table*}
 \begin{center}
 \small
    \begin{tabular}{c c c c |c c c|c c c}
 \hline
 \multicolumn{4}{c|}{Method} &
 \multicolumn{3}{c|}{Overhead2Ground (10K)} &
 \multicolumn{3}{c}{Ground2Overhead (10K)} \\
 
 \hline
 Model &Dynamic Encoder &Dropout & Meta Information & R@5$\uparrow$ & R@10$\uparrow$ & Median-R$\downarrow$ & R@5$\uparrow$ & R@10$\uparrow$ & Median-R$\downarrow$\\
 \hline
 CLIP & - &- & - & 0.007 & 0.013 & 1700 & 0.108 & 0.019 & 2857 \\
 \hline
 \multirow{4}{2em}{ours} & \xmark & \xmark & \xmark & 0.398 & 0.493 & 15 & 0.356 & 0.450 & 11\\
 & \cmark & \xmark & \xmark & 0.322 & 0.413 & 34 & 0.254 & 0.343 & 20\\
 & \cmark& \xmark & \cmark & 0.368 & 0.467 & 23 & 0.298 & 0.398 & 13\\
 & \cmark& \cmark & \xmark & 0.467 & 0.564 & 13.5 & 0.366 & 0.462 & 7\\
 & \cmark& \cmark & \cmark & \textbf{0.493} & \textbf{0.591} & \textbf{12} & \textbf{0.390} & \textbf{0.482} & \textbf{6}\\


\end{tabular}
\end{center}
  \caption{\textbf{Cross-view retrieval performance of Sat2Cap model:} We experiment with three different settings in our model. First, we experiment with the effect of using the Dynamic Encoder. Secondly, we look at the performance degradation in scenarios where meta-information is not available in inference. Finally, we experiment with randomly dropping out the Dynamic Encoder during training.}
\label{table:retrieval}
\end{table*}
% Figure environment removed

% Figure environment removed


\subsection{Learning Dynamic Concepts of Places}

Many ground-level concepts are temporally dependent. Concepts like `crowded street', `snowy place' etc can dramatically vary based on the exact time we query about them. In order to model such dynamic concepts, we condition Sat2Cap on the timestamps of the ground-level images.

For each sample, we extract the year, month, day, and hour in which the ground-level image was taken. We also add the geolocation information to provide a stronger signal to the model. We encode this meta-information using sin-cos encoding. We use a very shallow fully connected layer which we call the Dynamic Encoder represented by $h_\theta$. The encoded meta information is passed through the Dynamic Encoder whose output is added element-wise to the unnormalized output from the Sat2Cap model. We then normalize the final sum to compute our objective. Our framework, shown in Figure~\ref{fig:Sat2Cap}, is defined as:
\begin{equation}
O_i = g_\theta{(o_i)}
\end{equation}
\begin{equation}
    E_i = h_\theta{(e_i)}  
\end{equation}
where $e_i$ is the output of sin-cos encoding of the date, time, and location information for sample $i$.
\begin{equation}
    S_i = O_i + E_i
\end{equation}
Now the objective function is updated to:
\begin{equation}
\label{eq:finaleq}
   L_{dynamic} = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(S_i \cdot G_i / \tau)}{\sum_{j=0}^{k} exp(S_i \cdot G_j / \tau) } 
\end{equation}

Our training dataset captures the ground-level scenes at various times. If the model is only allowed to learn using the overhead image of a location, it will be forced to learn an average concept for all temporal settings. By conditioning the problem on additional temporal data, our model learns different ground-level concepts for different times of the day and year. This ultimately allows Sat2Cap to dynamically adapt to temporal variations for the same geolocation. 

To prevent overfitting to the meta-information, we implement random dropout of the Dynamic Encoder during training. This improves retrieval performance but can decrease the model's sensitivity to temporal variations. The dropout causes the model to learn to disregard the meta-information as it is frequently dropped out during training. Therefore, we view dropout as a hyperparameter that can be adjusted to control the dynamic sensitivity of our model.


\subsection{Implementations Details}
We use a ViT-32B as the CLIP image encoder. This image encoder is kept frozen throughout our training procedure. We use a ViT-32B architecture as the backbone for our Sat2Cap model. The Sat2Cap backbone is initialized using CLIP weights. Following~\cite{radford2021learning} we use an AdamW optimizer~\cite{loshchilov2017decoupled} with a learning rate of $1e-05$ with $\beta_1=0.9$ and $\beta_2=0.98$. We also use a learnable temperature parameter which is initialized at $\tau=0.07$. We use Cosine Annealing with Warm Restarts~\cite{loshchilov2016sgdr} as the learning rate scheduler.

We augment the overhead images using RandomResizedCrop and RandAugment~\cite{cubuk2020randaugment}. The overhead images are normalized using the mean and standard deviation of the dataset. The training was carried out using Nvidia A100 40GB GPU. Since a larger number of negative samples is beneficial for contrastive learning, we simulate a large batch size using a memory bank approach. We initialize a queue of size 9600 and fill it with precomputed ground-level image CLIP embeddings which are used as negative samples for computing the loss. 

% Figure environment removed

\section{Experiments and Results}
Our model learns a powerful geo-text embedding space that can be used for a variety of applications. We experimented with four tasks and show the quantitative and qualitative results on them.
\subsection{Application 1: Cross-View Image Retrieval}
\label{retrieval_section}
In this experiment, we show that our model learns a strong relationship between co-located overhead images and ground-level images in the CLIP space. We randomly sample 10,000 image pairs from the test set for this experiment. First, we compute the Sat2Cap embeddings for all overhead images in the 10k testset. Then we compute the CLIP embeddings for all ground-level images in the set. We then compute top-k and median rank metrics between the Sat2Cap overhead embeddings and the CLIP ground embeddings. Table~\ref{table:retrieval} shows all retrieval results.

As a baseline, we use the distance between CLIP overhead embeddings and CLIP ground embeddings. We get an extremely low R@10 score of 0.013 and a median rank of 1700. These low scores essentially tell us that the overhead images and corresponding ground-level images lie far apart in the CLIP space. For Sat2Cap, we first experiment without using the Dynamic Encoder. Just by contrastively training the Sat2Cap image encoder with ground-level CLIP embeddings, we achieve a high R@10 score of 0.493 and a median rank of 15. 

All remaining experiments are conducted on models that were trained using the Dynamic Encoder. Table~\ref{table:retrieval} shows that initially, the retrieval scores drop when using the Dynamic Encoder. We suspect this happens because the model starts to overfit on the meta-information, ignoring important cues from the overhead images. We also see a 5.4\% drop in R@10 metrics when we remove the meta-information during inference. To reduce the possibility of overfitting, we randomly drop the Dynamic Encoder during training. We see that simply adding dropout during training increases the R@10 score by 12.4\%. Another important observation is that the effect of removing meta-information during inference is less severe when using dropout. Hence, our model achieves good cross-view retrieval scores even if meta-information is not available during inference. 


Figure~\ref{fig:retrieval_results} shows the top 9 closest images retrieved from a given overhead image. We see that our model is able to retrieve ground-level images by relating concepts rather than direct visual matching. For example, in (a), our model retrieves images of people playing golf for an overhead image of a golf course. Similarly, in (d), our query image seems to be located over a farm. Here, Sat2Cap has learned to associate the concept of farmland with cattle and livestock. It retrieves images of horses and goats which are concepts that likely reside in the location but are not visible in the overhead image. This suggests that our model can map fine-grained concepts of the ground-level scene to a given geolocation. Sat2Cap is also capable of dynamic image retrieval. Figure~\ref{fig:dynamic_retrieval} shows the top 9 images retrieved at two different time settings (11:00 p.m.\ vs 08:00 a.m.).  


\subsection{Application 2: Fine-grained and Dynamic Caption Generation}
Our embedding space captures detailed and fine-grained textual concepts for geographic locations. While the CLIP space can only provide coarse-level generic descriptions from an overhead image, our model learns more fine-grained visual concepts that someone on the ground might observe. To generate captions from our embeddings, we use the CLIPCAP~\cite{mokady2021clipcap} model, which maps CLIP space to text space.

Figure~\ref{fig:img1} shows that using CLIP embeddings of the overhead images, the model can only describe generic concepts of a location like a beach, island, property, etc. Our Sat2Cap embeddings on the other hand produce much more fine-grained, as well as, aesthetically pleasing captions. For example: in figure (a) CLIP generates the caption ``aerial view of a beach" missing out on other important details of the area. Our model on the other hand generates the caption ``sea facing apartment with swimming pool, terrace in a quiet residential area", capturing many intricate concepts that reside within that location. 

Sat2Cap also models temporal variations allowing us to generate different captions for different times. Figure~\ref{fig:img1} shows the captions generated for two different months, May vs. January. We see that the model reasonably accounts for the seasonal variations for different months of the year. However, in figure (d), we see that the model does not add any cold/winter-specific information for the January input. This is expected behavior since the image is from Australia, where the month of January falls right in the middle of summer.

% Figure environment removed
\subsection{Application 3: Zero-Shot Map of Fine-grained Concepts}
We use the rich Sat2Cap embedding space to create country-level maps of fine-grained textual prompts in a zero-shot manner. Firstly, we choose two countries to create maps: England and Netherlands. Then, we download satellite imagery that covers these regions. Specifically, we download 800x800 patches of Bing Map Images at 0.6m/px resolution. We precompute the Sat2Cap embeddings for all the images and save them on a disc. Now for any given text query, we compute the similarity of the CLIP text embedding with all overhead images of the region. Then we normalize these similarities between $0$ and $1$ and use the normalized similarities to create textual maps. The process of computing similarities for an entire country took only about 4-5 seconds. Hence, our framework is quite efficient for mapping large regions and thus could be easily extended to map the entire world. 

Figure~\ref{fig:generic_maps} shows the maps for two prompts: ``Kids playing in the sand" and ``A busy street in downtown". We added the phrase ``a photo of" at the beginning of each prompt. For the first prompt, we see that our model activates locations around the ocean and beaches. Activations in both countries are high in areas where you might observe a kid playing in the sand. The second prompt activates locations with major cities in both countries. For England, we see high activations around London, Oxford, Birmingham, Manchester, Liverpool, etc. For the Netherlands, we see high activations in Amsterdam, Rotterdam, The Hague, Maastricht, Groningen, etc as well as other smaller cities. We compare this map with ESRI's Sentinel-2 landcover map. From the landcover maps, we see that our model correctly activates the fine-grained prompt ``A busy street in downtown" in the urban areas. Thus, we introduce to a novel way to create large-scale maps in a zero-shot setting.

\subsection{Application 4: Geolocalizing Textual Queries}
Our model can be used to localize textual queries at a finer resolution. For this experiment, we draw a $24 km^2$ bounding box over a region. We compute the Sat2Cap similarity for all the overhead images in that box with a given text query. We, then, normalize the similarities between $0$ and $1$ and clip the values below $0.5$. 
Figure~\ref{fig:localize} shows the results of this experiment. The red spot indicates the location with the highest activations for the given query. For each query, the left figure shows the total area of inference, and the right figure shows a fine-grained image at the location with the highest activation, obtained from Google. We see that our model is capable of making reasonable localization for the given queries. For example: in (a) our model activates over a soccer stadium. Similarly, for (c) and (d), our model has high activations over an amusement park and the ``National Railway Museum" respectively. Figure (b) shows that when we compose the concept of people with animals, our model shows very high activation in farm-like areas which is where these two concepts would most likely co-occur. 
These results show that our model can reasonably localize the most plausible point within a given area, where one might observe a given query. This property can be beneficial in solving visual search problems in the geospatial domain.

\section{Conclusion}
We introduced a novel weakly supervised framework to learn a rich embedding space between geolocation and fine-grained captions. Our method does not require any text-labeled data making it easy to train and scale. We demonstrated 4 interesting applications of our model. First, we showed that our model can be used for cross-view image retrieval even when using uncurated ground-level images. Secondly, we showed that our model can be used for generating fine-grained and dynamic captions for geolocations. Third, we showed that our model can effectively localize textual concepts within a given geospatial region. Finally, we demonstrated how Sat2Cap embeddings can be used for the newly defined task of large-scale zero-shot mapping.  

{\small
\bibliographystyle{ieeetr}
\bibliography{ms}
}

\clearpage

\end{document}

