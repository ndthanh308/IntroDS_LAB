\textbf{Toy trade execution task.}
The reward in the toy trade execution task is $r=(\sum_{t=31}^{60} \gamma^{t-31} a_t p_t) - (\frac{1}{30} \sum_{t=31}^{60} \gamma^{t-31} p_t)$, where $a_t$ is the action, $p_t$ is the price, and we set $\gamma=\exp(\log(\frac{1}{2}) / 30)= 0.9772$.
We present more results on the learned strategy by ``RL'' and ``Fit parameters'' for the toy trade execution task to illustrate how an overfitted/generalizable policy should perform.
We present their performance under different market environments in Figure \ref{fig:toy}.
In general, when the price is rising (see the left two columns in the figure), a good strategy should sell more at the end of the horizon; otherwise (see the right two columns) a good strategy needs to liquidate as soon as possible. 
Moreover, ``RL'' presents sharp peaks which indicates that it suffers from overfitting the training data while ``Fit parameters'' liquidates the inventory smoothly and resembles the analytical solution in previous papers \cite{almgren2001optimal}.

% Figure environment removed

\textbf{Ablation study on the model designs for trade execution.}
We conduct experiments on the combination of the designs on the observation space, reward function, learning rate, model architecture, etc.
We perform a grid search over all the possible combinations and list the corresponding performance impacts in Table \ref{tab:ablation}.
We can observe that the most influential factor is the design in the reward function.
Tuned DQN/PPO and Algo1/Algo2 use the design that is the best in each group.
\input{inputs/tab_ablation.tex}

We conduct a grid search over all possible designs listed in the table and run each combination five times. 
We show the differences between the average trading cost when the model adopts the design and the average trading cost over the runs in the whole set.
For the first group, the whole set contains all runs for tuned PPO.
For the second to the fifth group, the whole set only contains runs for tuned PPO that use \emph{cash inflow} as the revenue term.
For the last group, the whole set contains all runs for Algo1 (PPO).
\begin{itemize}
\item Equally distributed quoted volumes ranges from 0 to 2TWAP as is in \cite{daberius2019deep}.
\item The quoted price is selected from a non-uniformly distributed set $P$=[[-50, -40, -30, -25, -20, -15], linspace(-10, 10, 21), [15, 20, 25, 30, 40, 50]] bp.
\item The quoted price and volume is selected from $[\frac{1}{2}\text{TWAP}, \text{TWAP}, \frac{3}{2}\text{TWAP}, 2 \text{TWAP}] \times P$.
\end{itemize}





\textbf{Increasing the data volume.}
We argue that simply increasing the data volume does not address the overfitting problem since the RL agents are trained in the low-data region in practice.
Although there is a large amount of historical data available in finance, the high noise and high dimensionality of the context (i.e., the indicators) calls for far more samples than we have.
Moreover, in real trading, although using far data augments the dataset, this may also induce the distribution shift due to the fast changing market environment.
To validate our claim, we conduct new experiments using more data (6 months $\approx 3$M samples) and compare them with our main experiment. 
We present the result in Table \ref{tab:more_data}.
We observe that simply using more data does not alleviate overfitting and our algorithm achieves better performance when more data is consumed.

\begin{table}[htbp]
\tiny
\centering
\begin{tabular}{ r r r r}
Algorithm & Training & Testing & Gap \\
\hline
tuned DQN (2 months) & 1.6078 (2.1974) & 6.7199 (2.7776) & 5.1121 \\ 
tuned DQN (6 months) & 2.0545 (2.8511) & 6.1737 (2.3555) & 4.1192 \\ 
CATE+DQN (2 months) & -2.7526 (1.3319) & 0.0749 (1.5233) & 2.8275 \\ 
CATE+DQN (6 months) & -3.0444 (1.8510) & 0.7940 (0.5523) & 3.8384 \\ 
\end{tabular}
\caption{Performance of the RL agents trained using more data. The experiment setting is the same as the setting in Table \ref{tab:main_exp}.}
\label{tab:more_data}
\end{table}


\textbf{Hardware.}
Our experiments are conducted on a server with the following configurations:
\begin{itemize}
    \item System: Ubuntu 18.04.5 LTS
    \item CPU: 24 x Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz
    \item GPU: 4 x Tesla V100
    \item Memory: 441G
\end{itemize} 

\textbf{Limitation and social impact of our work.}
Generalization in trade execution or other tasks in quantitative investment is important. This paper only studies on how to improve generalization from the representation learning perspective. There are other aspects that are worth investing such as feature engineering, neural architecture design, and traditional deep learning techniques that can prevent overfitting.
Trade execution is an important application in the financial industry.
With a good trade execution strategy, the firms can allocate the resources more efficiently.

% We argue that overfitting in trade execution could happen ``robustly`` even if we use a reasonably large offline dataset.
% Notice that overfitting happens in our main experiments (cf. Table \ref{tab:main_exp}) where the trading frequency is high and the training already consumes a large amount of data (with approximately 1 million samples).
% To compare with

% We conduct new experiments using more data (6 months $\approx 3$M samples) and present the results as follows. We can see that simply using more data does not alleviate overfitting. 
% 股票数量特别多，高频的数据，数据量已经很大了。datasize 超出了 daily based 的二十年的数据。


