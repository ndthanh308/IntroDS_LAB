In the following proof, we use the lower case $p$ to represent the transition probability and the upper case $P$ to represent the corresponding matrix form, which is slightly different from the notation in the main text.
With access to $\phi$, the underlying transition model can be written as $p(x',s'|x,s,a) = p_x(x'|x) p_s(s'|x,s,a)$.
With a generative model, we can collect $N$ context transition samples from the context that corresponds to each latent context $x\in\mathcal{X}$.
Therefore, we consume a total number of $N|\mathcal{X}|$ samples.
Our algorithm estimates $p_x$ with these samples as follows: 
$\hat{p}_x(x'|x) = \text{count}(x,x') / \text{count}(x)$,
where $\text{count}(x,x')$ is the number of transitions from $x$ to $x'$ in the samples and $\text{count}(x)$ is the number of transitions starting from $x$ in the samples.

We denote $\hat{p}(x',s'|x,s,a) = \hat{p}_x(x'|x) p_s(s'|x,s,a)$ and the matrix form as $\hat{P}\in\mathbb{R}^{ |\mathcal{X}||\mathcal{S}||\mathcal{A}| \times |\mathcal{X}||\mathcal{S}| }$.
We can link the estimation error of $\hat{p}_x$ to that of $\hat{p}$ with the following lemma:
\begin{lemma}
\label{lemma:link}
\textit{
For any $x\in\mathcal{X}, s\in\mathcal{S}, a\in\mathcal{A}$, we have
$$
\| p(\cdot|x,s,a) - \hat{p}(\cdot|x,s,a) \|_1 = \| p_x(\cdot| x) - \hat{p}_x(\cdot|x) \|_1
$$
}
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lemma:link}]
$$
\begin{aligned}
& \| p(\cdot|x,s,a) - \hat{p}(\cdot|x,s,a) \|_1 \\
= & \sum_{x',s'} |p(x',s'|x,s,a) - \hat{p}(x',s'|x,s,a)|  \\
= & \sum_{x',s'} |p_x(x'|x) p_s(s'|x,s,a) - \hat{p}_x(x'|x) p_s(s'|x,s,a) |  \\
= & \sum_{x',s'} |p_x(x'|x) - \hat{p}_x(x'|x) | p_s(s'|x,s,a)  \\
= & \Big( \sum_{x'} |p_x(x'|x) - \hat{p}_x(x'|x) | \Big) \Big( \sum_{s'} p_s(s'|x,s,a) \Big)  \\
= & \| p_x(\cdot| x) - \hat{p}_x(\cdot|x) \|_1
\end{aligned}
$$
\end{proof}

Next, we introduce several lemmas for MDP. 
Notice that they also apply to our setting by treating the state in MDP as $(x,s)$ in the ORDC model.
\begin{lemma}[Lemma 2.2 in \cite{agarwal2019reinforcement}]
\label{lemma:22}
$$
Q^\pi - \hat{Q}^\pi = \gamma (I - \gamma \hat{P}^\pi)^{-1} (P - \hat{P}) V^\pi, \forall \pi
$$
\end{lemma}
\begin{lemma}[Lemma 2.3 in \cite{agarwal2019reinforcement}]
\label{lemma:23}
\textit{
For any policy $\pi$, and vector $v\in\mathbb{R}^{|\mathcal{X}||\mathcal{S}||\mathcal{A}|}$, we have
$$
\| (I - \gamma P^\pi)^{-1} v \|_\infty \le \|v\|_\infty / (1-\gamma)
$$
}
\end{lemma}
\begin{lemma}[Lemma A.8 in \cite{agarwal2019reinforcement}]
\label{lemma:A8}
\textit{
Let $z$ be a discrete random variable that takes values in $\{1, \cdots, d\}$ distributed according to $q\in\Delta^d$ with $q_i = Pr(z=i)$. 
Assume we have $N$ iid samples, and the empirical estimate $\hat{q}\in\Delta^d$ with $\hat{q}_i = \sum_{i=1}^N {\bf 1}[z_i=i] / N$.
For some constant $c>0$ and w.p. at least $1-\delta$, we have
$$
\| q - \hat{q} \|_1 \le c \sqrt{\dfrac{d \log(1/\delta)}{N}}.
$$
}
\end{lemma}

\begin{proof}[Proof of Theorem \ref{theorem:upper_bound}]
For any policy $\pi$, we have
$$
\begin{aligned}
& \| Q^\pi - \hat{Q}^\pi \|_\infty \\
= & \| \gamma (I - \gamma \hat{P}^\pi)^{-1} (P - \hat{P}) V^\pi \|_\infty \\
\le & \frac{\gamma}{1-\gamma}  \| (P - \hat{P}) V^\pi \|_\infty \\
\le & \frac{\gamma}{1-\gamma} \Big( \max_{x,s,a} \| p(\cdot|x,s,a) - \hat{p} (\cdot|x,s,a) \|_1  \Big)  \| V^\pi \|_\infty \\
= & \frac{\gamma}{1-\gamma} \Big( \max_{x} \| p_x(\cdot|x) - \hat{p}_x (\cdot|x) \|_1  \Big)  \| V^\pi \|_\infty \\
\le & \frac{\gamma}{(1-\gamma)^2} \Big( \max_{x} \| p_x(\cdot|x) - \hat{p}_x (\cdot|x) \|_1  \Big) \\
\le & c \frac{\gamma}{(1-\gamma)^2} \sqrt{\dfrac{|\mathcal{X}| \log(|\mathcal{X}|/\delta)}{N}} \quad \text{w.p. at least } 1-\delta. \\
\end{aligned}
$$
The first line uses Lemma \ref{lemma:22}. 
The second line uses Lemma \ref{lemma:23}.
The third line uses Holder's inequality.
The fourth line uses Lemma \ref{lemma:link}.
The fifth line uses Assumption \ref{assumption:MDP_regularity}.
The sixth line uses Lemma \ref{lemma:A8} with a union bound.
Equivalently, with $|\mathcal{X}| N = \dfrac{c^2 \gamma^2}{(1-\gamma)^4} \dfrac{|\mathcal{X}|^2 \log(|\mathcal{X}|/\delta)}{\epsilon^2}$ samples, we have $\| Q^\pi - \hat{Q}^\pi \|_\infty \le \epsilon$ w.p. at least $1-\delta$.

Then, for all $(x,s,a)\in\mathcal{X}\times\mathcal{S}\times\mathcal{A}$, we have
$$
\begin{aligned}
& |Q^*(x,s,a) - \hat{Q}^*(x,s,a)| \\
= & |\sup_\pi Q^\pi(x,s,a) - \sup_\pi \hat{Q}^\pi(x,s,a) | \\
\le & \sup_\pi |Q^\pi(x,s,a) - \hat{Q}^\pi(x,s,a)| \\
\le & \epsilon,
\end{aligned}
$$
which completes the proof.
\end{proof}

\textbf{An illustrative example.}
In addition to the proof, we also provide an illustrative view on how context aggregation can help generalization.
We consider a policy evaluation setting where we aim to estimate the value function for some context.
We will later show that, by aggregating the context properly, we can obtain a more accurate estimate of the value.

% Figure environment removed

Let us consider to estimate a value function $Q(c,s,a)$ for some $(c, s, a)\in\mathcal{C}\times\mathcal{S}\times\mathcal{A}$.
We illustrate the procedure under the simplified trade execution task (described in Section \ref{sec:toy_example}) in Figure \ref{fig:theorem2_illustration}.

When it is possible to reset the environment to $(c,s)$ and then take the action $a$ to rerun unlimited number of times, an accurate estimation of $Q(c,s,a)$ is $\hat{Q}'(c,s,a) = \frac{1}{N} (R'_1 + R'_2 + R'_3 + R'_4 + \cdots)$, where $R'_i$ is the cumulative return from $(c,s,a)$ in the $i$-th run (cf. the gray dashed lines in Figure \ref{fig:theorem2_illustration}a) and $N$ is the total number of runs.
When the number of samples is sufficient, $\hat{Q}'(c,s,a)$ approaches the true value $Q(c,s,a)$.

However, a resettable environment is not provided in most realistic scenarios, and therefore it is hard to obtain $R'$s.
For example, in the trade execution task, it is hard to reset to a specific market context and perform a counterfactual simulation on the complex market dynamics.
Without context aggregation (via either implicit function approximation or explicit context encoder), a straightforward way to estimate $Q(c,s,a)$ is to use the sample mean.
Suppose the context $c$ only appears once in the dataset (which is quite common when the context is high-dimensional).
In this scenario, the best estimate should be $\hat{Q}_1(c,s,a) = R_0$, where $R_0$ is the expected cumulative return calculated based on the only future context sequence (cf. the blue dashed lines in Figure \ref{fig:theorem2_illustration}a).
Notice that $\hat{Q}'(c,s,a)$ is calculated based on multiple context sequences but $\hat{Q}_1(c,s,a)$ is based on only one context sequence provided in the dataset.
Accordingly, $\hat{Q}_1(c,s,a)$ can suffer from a large variance.

An alternative method is to resort to the mapping between the high-dimensional context and the compact latent context.
When such a mapping is known, we can select the contexts that share a similar latent context to the interested context $c$ in the dataset (cf. the orange lines in Figure \ref{fig:theorem2_illustration}b).
Then, we can estimate the value function using $\hat{Q}_2(c,s,a) = \frac{1}{M} (R_0 + R_1 + R_2 + R_3 + R_4 + \cdots)$.
This estimate is the average of the returns collected based on multiple context sequences.
If these context sequences resemble the fictitious sequences starting from $(c,s,a)$ (cf. the gray dashed lines in Figure \ref{fig:theorem2_illustration}a),
we can consider $R_i\approx R'_i$ and therefore
$\hat{Q}_2(c,s,a)$ is an accurate estimate with reduced variance compared with $\hat{Q}_1$.

\iffalse

Based on the observation in the previous simplified trade execution task, we first introduce our motivation for a generalizable algorithm.
Then, we design an algorithm that is based on several hand-crafted target features for the real trade execution task.
Next, we propose an end-to-end algorithm that does not require any prior knowledge.

\textbf{Policy Evaluation}
In ORDC, the agent may overfit to the limited context sequences in the dataset.
We observe that, by resolving to a context encoding function $\phi:\mathcal{C} \to \mathcal{X}$ that maps the high-dimensional context into the latent context, we can obtain a more generalizable Q function.
Given a policy $\pi$, we learn a tabular Q function $q: \mathcal{X}\times\mathcal{S}\times\mathcal{A} \to \mathbb{R}$ by optimizing
\begin{equation}
    \min_q \frac{1}{N} \sum_{i=1}^N \Big( q(\phi(c_i), s_i, a_i) - \hat{Q}^\pi(c_i, s_i, a_i) \Big)^2,
\end{equation}
where $N$ is the number of training samples.
The corresponding solution is 
\begin{equation}
\begin{aligned}
    & q(x_t,s_t,a_t) = \\
    % & \frac{1}{M_{x_t}} \sum_{j=1}^{M_{x_t}} \frac{1}{N(c_j)} \sum_{i=1}^{N(c_j)} [R(c_j, s_t, a_t, \epsilon_c^{(j)}, \epsilon_c^{(i)}, \epsilon_\pi^{(i)})], 
    & \sum_{c\in\mathcal{D}} I(\phi(c)=x_t) \hat{Q}^\pi(c,s_t,a_t) / \Big[ \sum_{c\in\mathcal{D}} I(\phi(c)=x_t) \Big]
\end{aligned}
\end{equation}
where $\mathcal{D}$ is the offline context dataset.
By averaging over the contexts that share the same latent context, this estimator has smaller variance and avoid overfitting to the only context sequence in the dataset.

\subsection{Why generalization is difficult?}

Next, we explain why generalization is hard in the ORDC setting.
Here, we assume the latent context space $\mathcal{X}$ is discrete, and each context $c\in\mathcal{C}$ uniquely determines its generating latent context $x\in\mathcal{X}$ (i.e., block structure).

\textbf{Policy Evaluation} We consider the value-based RL method whose core is to estimate the Q function with samples collected from the simulator based on historical context data. 
Given a policy $\pi$, we aim to estimate
\begin{equation}
Q^\pi(c_t, s_t, a_t) = \mathbb{E}_{\epsilon_c} \mathbb{E}_{\epsilon_e, \epsilon_\pi}
[R(c_t, s_t, a_t, \epsilon_c, \epsilon_e, \epsilon_\pi)]
% [R({\bf s}_{t:T}, {\bf a}_{t:T}, {\bf c}_{t:T})],
\end{equation}
where $\epsilon_c, \epsilon_e$ and $\epsilon_\pi$ denote the randomness in context dynamics (i.e., in $s_{\tau-1} \to s_{\tau}$ and $s_{\tau} \to c_{\tau}, \forall \tau \in[t+1, T]$), in the environment (i.e., in state dynamics and the reward function), and in action selection.
The above randomness determines a complete trajectory and therefore the return after the $t$-th time step which is denoted as $R(c_t, s_t, a_t, \epsilon_c, \epsilon_e, \epsilon_\pi)$.
% $R$ is the cumulative reward when the state and action sequences are given.
% are calculated over all the possible action sequences sampled from the policy $\pi$, and $R$ is the cumulative reward when the state and action sequences are given\footnote{For ease of notation, we omit the randomness in state transitions and the reward function which should be averaged over in the second expectation.}.
While the expectation over $\epsilon_e$ and $\epsilon_\pi$ can be estimated with samples collected from the simulator, 
the expectation over $\epsilon_c$ cannot be accurately estimated due to limited context sequences provided in offline dataset.
For example, we consider the scenario where each context only appears once in the dataset which is common since the context is high-dimensional and the dataset is limited.
Therefore, the estimated Q value becomes
\begin{equation}
\hat{Q}^\pi(c_t, s_t, a_t) = \frac{1}{N(c_t)} \sum_{i=1}^{N(c_t)} [R(c_t, s_t, a_t, {\epsilon}_c^{(0)}, \epsilon_e^{(i)}, \epsilon_\pi^{(i)})],
\end{equation}
where $\epsilon_c^{(0)}$ indicates the only context sequence containing $c_t$ in the dataset, and $N(c)$ is the number of collected trajectory that contains $c$.
When the agent is trained with infinite number of samples collected from the simulator, the estimated Q value becomes $\mathbb{E}_{\epsilon_e,\epsilon_\pi}[R(c_t, s_t, a_t, \epsilon_c^{(0)}, \epsilon_e, \epsilon_\pi]$.
The estimated value function is prone to overfit to that context sequence.
For example, the agent may memorize the highest price in that context sequence and learn an aggressive policy that liquidates all the stocks on that price.
Indeed, the function approximation of neural networks may alleviate data scarcity of context sequences, but we observe overfitting in deep models empirically (see the next subsection).

\fi