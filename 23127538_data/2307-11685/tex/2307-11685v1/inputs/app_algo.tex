\begin{algorithm}[H]
\caption{CASH: Context Aggregation with Handcrafted Statistics}
\label{algo:CASH_full}
\begin{algorithmic}[1]
\STATE Initialize context encoder $\phi_{\theta}(\cdot)$, PolicyNet/ValueNet $\text{RL}_{w}(\cdot,\cdot)$;
\STATE Given hand-crafted statistics function $\phi_{\text{hand}}(\cdot)$, Simulated Environment $E$;

\STATE \textit{\# Pre-train the context encoder}
\FOR{$i = 1 $ \textbf{to} $ n$}
    \STATE Sample minibatch $\{(c_{j}, {c'}_{j})\}$ from simulator $E$ with arbitrary actions;
    \STATE Using gradient descent to minimize the loss $ \mathcal{L}_\text{stat}(\theta)= \sum_{j} (\phi_{\theta}(c_{j}) - \phi_{\text{hand}}({c'}_{j}))^{2}$;
\ENDFOR
\STATE \textit{\# Train the RL model}
\STATE Fix the parameters $\theta$ in the context encoder $\phi_{\theta}(\cdot)$;
\FOR{$i = 1 $ \textbf{to} $m$}
    \STATE Sample minibatch $\{(c_{j}, {s}_{j}, r_{j})\}$ from simulator $E$ by rolling the current policy;
    \STATE Train $\text{RL}_{w}(\phi_{\theta}(c_{j}), s_{j})$ by minimizing loss $\mathcal{L}(w)=\sum_{j} \mathcal{L}_\text{RL}(w)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{CATE}
\label{algo:CATE_full}
\begin{algorithmic}[1]
\STATE Initialize context encoder $\phi_{\theta}(\cdot)$, future encoder $\phi_{\vartheta}(\cdot)$, PolicyNet/ValueNet $\text{RL}_{w}(\cdot,\cdot)$;
\STATE Given simulator $E$;

\STATE \textit{\# Train model simultaneously in an end-to-end manner}
\FOR{$i = 1 $ \textbf{to} $ m$}
    \STATE Sample minibatch $\{(c_{j}, {c}_{j}')\}$ from simulator $E$ by rolling the current policy;
    \STATE Using gradient descent to minimize the loss 
    \[
    \mathcal{L}(\theta, \vartheta, w):= \sum_{j} \mathcal{L}_{\text{Enc}}(\theta, \vartheta) + \mathcal{L}_{\text{RL1}}(\theta, w) + \mathcal{L}_{\text{RL2}}(\vartheta, w)
    \]
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Simulator}
\label{algo:simulation}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Policy $\pi$, 
\STATE \textbf{Given:} Historical dataset $D$, transition dynamics $P(s'|c,s,a)$, and reward function $r(c,s,a)$, execution period $T$, and execution volume $V$;
\STATE Pick a context trajectory $\{c_1, c_2, \cdots, c_T\}$ from $D$;
\STATE Initialize $s_1$ and output $(c_1,s_1)$
\FOR{$t = 1 $ \textbf{to} $ T$}
    \STATE Receive an action $a_t$ from $\pi$;
    \STATE Calculate $s_{t+1} \sim P(\cdot|c_t, s_t, a_t)$ and $r_{t} = r(c_t, s_t, a_t)$;
    \STATE Return $(r_t, c_{t+1}, s_{t+1})$;
\ENDFOR
\end{algorithmic}
\end{algorithm}