\textbf{Dataset.}
Compared with the simulators based on the preset stochastic process \cite{daberius2019deep} or a collection of preset interactive agents \cite{byrd2019abides}, the simulator driven by real market data can capture the complex market more accurately \cite{vyetrenko2020get}.
Moreover, compared with the previous work where the simulator is based on bar-level data, our simulator relies on the LOB-level data of the market which records an LOB snapshot every 3 seconds.
With finer-grained data, we are able to learn more practical trading agents.
For example, we can evaluate how an agent that trades using only MOs suffers from a large trading cost, which is the scheme adopted in many existing papers \cite{fang2021universal}.
The interested time period of trade execution tasks in the industry is typically from 10 to 120 minutes, which is configurable in our simulator.
% For higher fidelity, we extract information from the simulation is carried our snapshot by snapshot
% To avoid a long planning horizon, the agent can interact with our simulator in a lower frequency (one minute per step in our experiment), but the simulation is carried out snapshot by snapshot for accuracy.
Our simulator is based on the dataset that records an LOB snapshot every 3 seconds from the real market.
The time period of trade execution tasks in our experiments is set to 30 minutes.
To avoid a long planning horizon, the agent interacts with the simulator at a lower frequency (i.e., one minute per step).
Nevertheless, simulation is carried out snapshot by snapshot for higher accuracy.

\textbf{Observations.}
The observation in our simulator consists of the private variable (i.e., the state) and the market variable (i.e., the context).
The private variable consists of the remaining time and executed quantity.
The market variable can be the stacked features (including order-book-related features, technical indicators, raw snapshots, etc.) over several past steps. 
Our simulator implements a wide range of features including the features that appear in the previous papers as far as we know.
For different designs on the observations space, the agent can choose from these features.
To eliminate the differences in the features on different stocks,
the simulator normalizes the features as follows:
The price (or the feature whose dimension is price) is normalized using z-score with the open price on that trading day as the mean and the volatility on the previous trading day as the standard deviation.
The volume (or the feature whose dimension is volume) is normalized by dividing by the total volume of the last trading day.
In specific algorithms, we may perform another normalization on these features to fit them into a proper value range.

% The observation in our simulator consists of the market variables (context) and the private variables (state). 
% The market variables includes order-book related features, technical indicators, and are stacked over several past steps.
% Our simulator implements a wide range of features that appear in previous papers to choose from.
% The private variables consist of the remaining time and executed quantity.
% \blue{Normalization.}

% encode the information of the market (such as the price and volume) and the private variables include the remaining time and executed quantity.
% Different methods use different variables as the market variables such as order-book related features \cite{nevmyvaka2006reinforcement}, technical indicators \cite{ning2018double} or a stack of features of several past steps \cite{fang2021universal}.
% Our simulator implements a wide range of hand-crafted features and can easily generate different forms of observation variables through a configuration file.
% all the state variables in the previous papers as far as we know.

\textbf{Actions.}
On each time step, our simulator receives a list of orders, each of which can be an MO or LO that specifies the direction, the quantity, and the price (only for LO).
On top of this, we provide a series of wrappers to fit different designs on the action space (e.g., discrete/continuous/combinatorial action spaces).
% If the price is lower than the best bid price, the order will be executed as an MO. 
Our simulator will provide the best possible execution for each order.
For example, if the quoted price is lower than some bid price level, the simulator will automatically place an MO to fill the outstanding bid orders whose prices are higher than the quoted price, and an LO for the remaining quantity.
In our algorithms, the agent places an order on each step by choosing a quoted volume and a quoted price from discretized sets.
The quoted volume is selected from $\{\frac{1}{2}\text{TWAP}, \text{TWAP}, \frac{3}{2}\text{TWAP}, 2 \text{TWAP} \}$ where TWAP is the volume executed on each step by a TWAP strategy (i.e., selling an equal amount on each step).
The quoted price is specified by a price difference w.r.t. the best ask price. 
If the quoted price is lower than the best bid price, the agent actually places an MO; otherwise, it is an LO.
Outstanding orders at the end of each step will be withdrawn.
% However, different models use different action spaces such as placing orders with discrete prices with fixed quantity, only MOs with varying quantities, or even a combination of LOs and MOs \cite{karpe2020multi}.
% In our simulator, we provide a series of wrappers to fit different designs on the action space.

\textbf{Reward.}
The reward function may consist of a basic revenue term (e.g., negative trading cost or average execution price) and several regularization terms (e.g., approximating the permanent market impact or enforcing a TWAP-like strategy).
The revenue term reflects the overall objective of trade execution that minimizes the trading cost or the average execution price for a sell program.
Moreover, various of regularizers are adopted to model the permanent market impact or the prior knowledge of a good execution program (e.g., enforcing a TWAP-like program).
Our simulator provides various choices on the reward function design and can benchmark different designs with a uniform set of metrics (such as the trading cost or implementation shortfall \cite{perold1988implementation}).
In our algorithms, the reward function consists of a revenue term $r_1$ and a regularization term $r_2$, i.e., $r_t= r_1 + \beta r_2$ where $\beta$ is a coefficient.
The revenue term is $r_1 = n_t \bar{p}_t$ where
$n_t$ is the executed volume in the last step, and $\bar{p}_t$ is the corresponding average execution price.
The regularization term is $r_2 =  (v_t - v_{t,\text{TWAP}})^2$ where $v_t$ is the remaining inventory, and $v_{t,\text{TWAP}}$ is the remaining inventory if we follow the TWAP strategy.

% Although different algorithms use different reward functions, they are evaluated using a uniform set of  metrics.
% The basic reward function reflects the overall objective of trade execution which minimizes the trading cost or the average execution price for a sell program.
% Moreover, various of regularizers are adopted to model permanent market impact or the prior knowledge of a good execution program (e.g., enforcing a TWAP-like program).
% Our simulator provides various choices on the reward function design and can benchmark different design with a uniform set of metrics (such as trading cost or implementation shortfall \cite{perold1988implementation}).

\textbf{Transition dynamics.}
Given a list of orders on the $t$-th time step, our simulator will determine the reward and the state on the next time step.
For MOs, we consider the temporary market impact and the time delay.
For example, when the decision of the agent is based on observation generated on time $\tau$, the execution of an MO is based on the snapshot on time $\tau+\Delta\tau$ where $\Delta\tau$ is a preset time delay.
Typically, we use $\Delta\tau=3s$.
For LOs, we determine whether the order can be executed snapshot by snapshot till the ($t+1$)-th time step.
If the highest market price (i.e., a transaction occurs on this price) in one snapshot exceeds the price quoted in the LO, we consider the order is fully executed.
If the highest market price exactly equals the quoted price, the order may be partially filled and the ratio is calculated by reconstructing the transactions between snapshots.
However, considering that 1) the quantity may be too large to be fully executed or 2) the LO may be at the end of the queue of the quoted price level, we impose additional trading limits on the above matching mechanism to encourage conservative simulation.

\textbf{Discussion.}
To improve fidelity, our simulator considers the temporary market impact of MOs, the time delay, and determines the execution of LOs based on reconstructing the transactions between snapshots.
However, there are still components that we do not consider.
First, the permanent market impact is the change of the equilibrium price during at least our planning horizon when we place an order.
Here, we assume the permanent market impact is linear w.r.t. the order quantity and therefore considering this factor does not change the optimal solution of our strategy \cite{almgren2001optimal}.
Then, MOs in our simulation not only change the current LOB but also the LOB of the next time step, possibly resulting in degenerated fidelity.
Therefore, we also rely on the assumption that the limit orders are resilient within a short period of time (which should be smaller or comparable to the time interval between two simulation steps).
Fortunately, this is verified by empirical studies such as \cite{degryse2005aggressive,cummings2010further,gomber2015liquidity}.
