@inproceedings{suhr2017corpus,
  title={A corpus of natural language for visual reasoning},
  author={Suhr, Alane and Lewis, Mike and Yeh, James and Artzi, Yoav},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={217--223},
  year={2017}
}


@article{suhr2018corpus,
  title={A corpus for reasoning about natural language grounded in photographs},
  author={Suhr, Alane and Zhou, Stephanie and Zhang, Ally and Zhang, Iris and Bai, Huajun and Artzi, Yoav},
  journal={arXiv preprint arXiv:1811.00491},
  year={2018}
}


@inproceedings{suhr-etal-2017-corpus,
    title = "A Corpus of Natural Language for Visual Reasoning",
    author = "Suhr, Alane  and
      Lewis, Mike  and
      Yeh, James  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2034",
    doi = "10.18653/v1/P17-2034",
    pages = "217--223",
    abstract = "We present a new visual reasoning language dataset, containing 92,244 pairs of examples of natural statements grounded in synthetic images with 3,962 unique sentences. We describe a method of crowdsourcing linguistically-diverse data, and present an analysis of our data. The data demonstrates a broad set of linguistic phenomena, requiring visual and set-theoretic reasoning. We experiment with various models, and show the data presents a strong challenge for future research.",
}
@article{nlvr2,
  author    = {Alane Suhr and
               Stephanie Zhou and
               Iris Zhang and
               Huajun Bai and
               Yoav Artzi},
  title     = {A Corpus for Reasoning About Natural Language Grounded in Photographs},
  journal   = {CoRR},
  volume    = {abs/1811.00491},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.00491},
  eprinttype = {arXiv},
  eprint    = {1811.00491},
  timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-00491.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{marvl,
  author    = {Fangyu Liu and
               Emanuele Bugliarello and
               Edoardo Maria Ponti and
               Siva Reddy and
               Nigel Collier and
               Desmond Elliott},
  title     = {Visually Grounded Reasoning across Languages and Cultures},
  journal   = {CoRR},
  volume    = {abs/2109.13238},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.13238},
  eprinttype = {arXiv},
  eprint    = {2109.13238},
  timestamp = {Sun, 02 Oct 2022 15:32:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-13238.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}   
@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}
@article{GoyalKSBP16,
  author    = {Yash Goyal and
               Tejas Khot and
               Douglas Summers{-}Stay and
               Dhruv Batra and
               Devi Parikh},
  title     = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding
               in Visual Question Answering},
  journal   = {CoRR},
  volume    = {abs/1612.00837},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.00837},
  eprinttype = {arXiv},
  eprint    = {1612.00837},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GoyalKSBP16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Sarthak Jain at 2016-09-29 17:28:05 +0530 


%% Saved with string encoding Unicode (UTF-8) 


@inproceedings{villa,
  title={Large-Scale Adversarial Training for Vision-and-Language Representation Learning},
  author={Gan, Zhe and Chen, Yen-Chun and Li, Linjie and Zhu, Chen and Cheng, Yu and Liu, Jingjing},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{rocl,
author = {Kim, Minseon and Tack, Jihoon and Hwang, Sung Ju},
title = {Adversarial Self-Supervised Contrastive Learning},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Existing adversarial learning approaches mostly use class labels to generate adversarial samples that lead to incorrect predictions, which are then used to augment the training of the model for improved robustness. While some recent works propose semi-supervised adversarial learning methods that utilize unlabeled data, they still require class labels. However, do we really need class labels at all, for adversarially robust training of deep neural networks? In this paper, we propose a novel adversarial attack for unlabeled data, which makes the model confuse the instance-level identities of the perturbed data samples. Further, we present a self-supervised con-trastive learning framework to adversarially train a robust neural network without labeled data, which aims to maximize the similarity between a random augmentation of a data sample and its instance-wise adversarial perturbation. We validate our method, Robust Contrastive Learning (RoCL), on multiple benchmark datasets, on which it obtains comparable robust accuracy over state-of-the-art supervised adversarial learning methods, and significantly improved robustness against the black box and unseen types of attacks. Moreover, with further joint fine-tuning with supervised adversarial loss, RoCL obtains even higher robust accuracy over using self-supervised learning alone. Notably, RoCL also demonstrate impressive results in robust transfer learning.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {251},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}
@inproceedings{adv_1,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}
@misc{pgd_train,
  doi = {10.48550/ARXIV.1706.06083},
  
  url = {https://arxiv.org/abs/1706.06083},
  
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{adv_2,
        title = {Adversarial Image Perturbation for Privacy Protection -- A Game Theory Perspective},
        author = {Oh, Seong Joon and Fritz, Mario and Schiele, Bernt},
        year = {2017},
        booktitle = {International Conference on Computer Vision (ICCV)},
        note = {to appear},
        pubstate = {published},
        tppubtype = {inproceedings}
    }

@INPROCEEDINGS{white_box_2,  author={Carlini, Nicholas and Wagner, David},  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},   title={Towards Evaluating the Robustness of Neural Networks},   year={2017},  volume={},  number={},  pages={39-57},  doi={10.1109/SP.2017.49}}

@misc{https://doi.org/10.48550/arxiv.1706.06083,
  doi = {10.48550/ARXIV.1706.06083},
  
  url = {https://arxiv.org/abs/1706.06083},
  
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{black_box_1,
  doi = {10.48550/ARXIV.1712.04248},
  
  url = {https://arxiv.org/abs/1712.04248},
  
  author = {Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  
  keywords = {Machine Learning (stat.ML), Cryptography and Security (cs.CR), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{simba,
  doi = {10.48550/ARXIV.1905.07121},
  
  url = {https://arxiv.org/abs/1905.07121},
  
  author = {Guo, Chuan and Gardner, Jacob R. and You, Yurong and Wilson, Andrew Gordon and Weinberger, Kilian Q.},
  
  keywords = {Machine Learning (cs.LG), Cryptography and Security (cs.CR), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Simple Black-box Adversarial Attacks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{textfooler,
  doi = {10.48550/ARXIV.1907.11932},
  
  url = {https://arxiv.org/abs/1907.11932},
  
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@inproceedings{adv_3,
  title={Adversarial Camouflage: Hiding Physical-World Attacks with Natural Styles},
  author={Duan, Ranjie and Ma, Xingjun and Wang, Yisen and Bailey, James and Qin, A Kai and Yang, Yun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1000--1008},
  year={2020}
}

@article{vilio,
  author    = {Niklas Muennighoff},
  title     = {Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful
               Memes},
  journal   = {CoRR},
  volume    = {abs/2012.07788},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.07788},
  eprinttype = {arXiv},
  eprint    = {2012.07788},
  timestamp = {Sat, 02 Jan 2021 15:43:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-07788.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{harmeme,
    title = "Detecting Harmful Memes and Their Targets",
    author = "Pramanick, Shraman  and
      Dimitrov, Dimitar  and
      Mukherjee, Rituparna  and
      Sharma, Shivam  and
      Akhtar, Md. Shad  and
      Nakov, Preslav  and
      Chakraborty, Tanmoy",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.246",
    doi = "10.18653/v1/2021.findings-acl.246",
    pages = "2783--2796",
}

@article{two_face,
  title={Two-Face: Adversarial Audit of Commercial Face Recognition Systems},
  author={Siddhartha Jaiswal and Karthikeya Duggirala and Abhisek Dash and Animesh Mukherjee},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.09137}
}

@misc{augly,
  author        = {Zoe Papakipos and Joanna Bitton},
  title         = {AugLy: Data Augmentations for Robustness},
  year          = {2022},
  eprint        = {2201.06494},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI}}
}

@article{simclr,
  title={A Simple Framework for Contrastive Learning of Visual Representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}
@software{gimp,
  author = {{The GIMP Development Team}},
  title = {GIMP},
  url = {https://www.gimp.org},
  version = {2.10.12},
  date = {2019-06-12},
}
@inproceedings{vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}
@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
}
@inproceedings{lxmert,
  title={LXMERT: Learning Cross-Modality Encoder Representations from Transformers},
  author={Tan, Hao and Bansal, Mohit},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing},
  year={2019}
}


@misc{frozen,
  doi = {10.48550/ARXIV.2106.13884},
  
  url = {https://arxiv.org/abs/2106.13884},
  
  author = {Tsimpoukelli, Maria and Menick, Jacob and Cabi, Serkan and Eslami, S. M. Ali and Vinyals, Oriol and Hill, Felix},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multimodal Few-Shot Learning with Frozen Language Models},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{transvg,
  doi = {10.48550/ARXIV.2104.08541},
  
  url = {https://arxiv.org/abs/2104.08541},
  
  author = {Deng, Jiajun and Yang, Zhengyuan and Chen, Tianlang and Zhou, Wengang and Li, Houqiang},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {TransVG: End-to-End Visual Grounding with Transformers},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{mdetr,
  doi = {10.48550/ARXIV.2104.12763},
  
  url = {https://arxiv.org/abs/2104.12763},
  
  author = {Kamath, Aishwarya and Singh, Mannat and LeCun, Yann and Synnaeve, Gabriel and Misra, Ishan and Carion, Nicolas},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@misc{fromage,
  doi = {10.48550/ARXIV.2301.13823},
  
  url = {https://arxiv.org/abs/2301.13823},
  
  author = {Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Grounding Language Models to Images for Multimodal Generation},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@misc{limber,
  doi = {10.48550/ARXIV.2209.15162},
  
  url = {https://arxiv.org/abs/2209.15162},
  
  author = {Merullo, Jack and Castricato, Louis and Eickhoff, Carsten and Pavlick, Ellie},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Linearly Mapping from Image to Text Space},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{magma,
  doi = {10.48550/ARXIV.2112.05253},
  
  url = {https://arxiv.org/abs/2112.05253},
  
  author = {Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7; I.4.8; I.5.1},
  
  title = {MAGMA -- Multimodal Augmentation of Generative Models through Adapter-based Finetuning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{edit,
  doi = {10.48550/ARXIV.2210.07229},
  
  url = {https://arxiv.org/abs/2210.07229},
  
  author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mass-Editing Memory in a Transformer},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{chain_thought,
  doi = {10.48550/ARXIV.2201.11903},
  
  url = {https://arxiv.org/abs/2201.11903},
  
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}





@misc{opt,
  doi = {10.48550/ARXIV.2205.01068},
  
  url = {https://arxiv.org/abs/2205.01068},
  
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OPT: Open Pre-trained Transformer Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{gpt,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@misc{llama,
  doi = {10.48550/ARXIV.2302.13971},
  
  url = {https://arxiv.org/abs/2302.13971},
  
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {LLaMA: Open and Efficient Foundation Language Models},
  
  publisher = {arXiv},
  
  year = {2023},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{uniter,
  title={Uniter: Universal image-text representation learning},
  author={Chen, Yen-Chun and Li, Linjie and Yu, Licheng and Kholy, Ahmed El and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
  booktitle={ECCV},
  year={2020}
}
@article{visbert,
  author    = {Liunian Harold Li and
               Mark Yatskar and
               Da Yin and
               Cho{-}Jui Hsieh and
               Kai{-}Wei Chang},
  title     = {VisualBERT: {A} Simple and Performant Baseline for Vision and Language},
  journal   = {CoRR},
  volume    = {abs/1908.03557},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.03557},
  eprinttype = {arXiv},
  eprint    = {1908.03557},
  timestamp = {Mon, 19 Aug 2019 13:21:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-03557.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{text1,
  title={Deep learning for hate speech detection in tweets},
  author={Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
  booktitle={Proceedings of the 26th international conference on World Wide Web companion},
  pages={759--760},
  year={2017}
}
@article{mmbt,
  author    = {Douwe Kiela and
               Suvrat Bhooshan and
               Hamed Firooz and
               Davide Testuggine},
  title     = {Supervised Multimodal Bitransformers for Classifying Images and Text},
  journal   = {CoRR},
  volume    = {abs/1909.02950},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.02950},
  eprinttype = {arXiv},
  eprint    = {1909.02950},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-02950.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{text3,
  title={Hierarchical CVAE for fine-grained hate speech classification},
  author={Qian, Jing and ElSherief, Mai and Belding, Elizabeth and Wang, William Yang},
  journal={arXiv preprint arXiv:1809.00088},
  year={2018}
}
@article{explain,
  title={HateXplain: A Benchmark Dataset for Explainable Hate Speech Detection},
  author={Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh},
  journal={arXiv preprint arXiv:2012.10289},
  year={2020}
}

@article{facebook,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  journal={arXiv preprint arXiv:2005.04790},
  year={2020}
}

@InProceedings{text2,
author="Zhang, Ziqi
and Robinson, David
and Tepper, Jonathan",
editor="Gangemi, Aldo
and Navigli, Roberto
and Vidal, Maria-Esther
and Hitzler, Pascal
and Troncy, Rapha{\"e}l
and Hollink, Laura
and Tordai, Anna
and Alam, Mehwish",
title="Detecting Hate Speech on Twitter Using a Convolution-GRU Based Deep Neural Network",
booktitle="The Semantic Web",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="745--760",
abstract="In recent years, the increasing propagation of hate speech on social media and the urgent need for effective counter-measures have drawn significant investment from governments, companies, and empirical research. Despite a large number of emerging scientific studies to address the problem, a major limitation of existing work is the lack of comparative evaluations, which makes it difficult to assess the contribution of individual works. This paper introduces a new method based on a deep neural network combining convolutional and gated recurrent networks. We conduct an extensive evaluation of the method against several baselines and state of the art on the largest collection of publicly available Twitter datasets to date, and show that compared to previously reported results on these datasets, our proposed method is able to capture both word sequence and order information in short texts, and it sets new benchmark by outperforming on 6 out of 7 datasets by between 1 and 13{\%} in F1. We also extend the existing dataset collection on this task by creating a new dataset covering different topics.",
isbn="978-3-319-93417-4"
}

@article{clip,
  author    = {Alec Radford and
               Jong Wook Kim and
               Chris Hallacy and
               Aditya Ramesh and
               Gabriel Goh and
               Sandhini Agarwal and
               Girish Sastry and
               Amanda Askell and
               Pamela Mishkin and
               Jack Clark and
               Gretchen Krueger and
               Ilya Sutskever},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  journal   = {CoRR},
  volume    = {abs/2103.00020},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  eprinttype = {arXiv},
  eprint    = {2103.00020},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-00020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{composeAE,
  title={Compositional Learning of Image-Text Query for Image Retrieval},
  author={Anwaar, Muhammad Umer and Labintcev, Egor and Kleinsteuber, Martin},
  journal={arXiv preprint arXiv:2006.11149},
  year={2020}
}

@misc{retailmarket,
Author = {DigitalCommerce},
Year = {2020},
Eprint = {https://www.digitalcommerce360.com/article/global-ecommerce-sales/},
}

@misc{forbes,
Author = {Forbes},
Year = {2020},
Eprint = {https://www.forbes.com/sites/johnkoetsier/2020/06/12/covid-19-accelerated-e-commerce-growth-4-to-6-years/#3a60a82600fa},
}

@misc{deloitte,
Author = {Deloitte},
Year = {2020},
Eprint = {https://www2.deloitte.com/content/dam/Deloitte/fr/Documents/consumer-business/Publications/deloitte_global-powers-of-retailing-2020.pdf},
}


@inproceedings{TIRG,
  title={Composing text and image for image retrieval-an empirical odyssey},
  author={Vo, Nam and Jiang, Lu and Sun, Chen and Murphy, Kevin and Li, Li-Jia and Fei-Fei, Li and Hays, James},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6439--6448},
  year={2019}
}


@inproceedings{VALpaper,
  title={Image Search with Text Feedback by Visiolinguistic Attention Learning},
  author={Chen, Yanbei and Gong, S. and Bazzani, L},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3001--3011},
  year={2020}
}

@article{MAAF,
  title={Modality-Agnostic Attention Fusion for visual search with text feedback},
  author={Dodds, Eric and Culpepper, Jack and Herdade, Simao and Zhang, Yang and Boakye, Kofi},
  journal={arXiv preprint arXiv:2007.00145},
  year={2020}
}


@incollection{Relationship,
title = {A simple neural network module for relational reasoning},
author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {4967--4976},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf}
}

@incollection{MRN,
title = {Multimodal Residual Learning for Visual QA},
author = {Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {361--369},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6446-multimodal-residual-learning-for-visual-qa.pdf}
}


%% Datasets
@article{fashionIQdataset,
  title={Fashion IQ: A New Dataset towards Retrieving Images by Natural Language Feedback},
  author={Guo, Xiaoxiao and Wu, Hui and Gao, Yupeng and Rennie, Steven and Feris, Rogerio},
  journal={arXiv preprint arXiv:1905.12794},
  year={2019}
}
@inproceedings{fashion200Kdataset,
  title={Automatic spatially-aware fashion concept discovery},
  author={Han, Xintong and Wu, Zuxuan and Huang, Phoenix X and Zhang, Xiao and Zhu, Menglong and Li, Yuan and Zhao, Yang and Davis, Larry S},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1463--1471},
  year={2017}
}
@inproceedings{showsdataset,
  title={Automatic attribute discovery and characterization from noisy web data},
  author={Berg, Tamara L and Berg, Alexander C and Shih, Jonathan},
  booktitle={European Conference on Computer Vision},
  pages={663--676},
  year={2010},
  organization={Springer}
}


@inproceedings{guo2018dialog,
  title={Dialog-based interactive image retrieval},
  author={Guo, Xiaoxiao and Wu, Hui and Cheng, Yu and Rennie, Steven and Tesauro, Gerald and Feris, Rogerio},
  booktitle={Advances in neural information processing systems},
  pages={678--688},
  year={2018}
}

@misc{qair,
  doi = {10.48550/ARXIV.2103.02927},
  
  url = {https://arxiv.org/abs/2103.02927},
  
  author = {Li, Xiaodan and Li, Jinfeng and Chen, Yuefeng and Ye, Shaokai and He, Yuan and Wang, Shuhui and Su, Hang and Xue, Hui},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {QAIR: Practical Query-efficient Black-Box Attacks for Image Retrieval},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{textbugger,
	doi = {10.14722/ndss.2019.23138},
  
	url = {https://doi.org/10.14722%2Fndss.2019.23138},
  
	year = 2019,
	publisher = {Internet Society},
  
	author = {Jinfeng Li and Shouling Ji and Tianyu Du and Bo Li and Ting Wang},
  
	title = {{TextBugger}: Generating Adversarial Text Against Real-world Applications},
  
	booktitle = {Proceedings 2019 Network and Distributed System Security Symposium}
}

@InProceedings{AdvTextMatch,
author = {Sarafianos, Nikolaos and Xu, Xiang and Kakadiaris, Ioannis A.},
title = {Adversarial Representation Learning for Text-to-Image Matching},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
} 

@conference{birds2words,
    title = {Neural Naturalist: Generating Fine-Grained Image Comparisons},
    author = {Maxwell Forbes and Christine Kaeser-Chen and Piyush Sharma and Serge Belongie},
    year = {2019},
    date = {2019-11-03},
    booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    address = {Hong Kong},
}

@inproceedings{AoA,
  title={Attention on Attention for Image Captioning},
  author={Huang, Lun and Wang, Wenmin and Chen, Jie and Wei, Xiao-Yong},
  booktitle={International Conference on Computer Vision},
  year={2019}
}

@inproceedings{GRU,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@inproceedings{yu2017multi,
  title={Multi-level attention networks for visual question answering},
  author={Yu, Dongfei and Fu, Jianlong and Mei, Tao and Rui, Yong},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4709--4717},
  year={2017}
}

@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={8317--8326},
  year={2019}
}
@misc{mmimdb,
  doi = {10.48550/ARXIV.1702.01992},
  
  url = {https://arxiv.org/abs/1702.01992},
  
  author = {Arevalo, John and Solorio, Thamar and Montes-y-Gómez, Manuel and González, Fabio A.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gated Multimodal Units for Information Fusion},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{food101,
	abstract = {In this paper we address the problem of automatically recognizing pictured dishes. To this end, we introduce a novel method to mine discriminative parts using Random Forests (rf), which allows us to mine for parts simultaneously for all classes and to share knowledge among them. To improve efficiency of mining and classification, we only consider patches that are aligned with image superpixels, which we call components. To measure the performance of our rf component mining for food recognition, we introduce a novel and challenging dataset of 101 food categories, with 101'000 images. With an average accuracy of 50.76{\%}, our model outperforms alternative classification methods except for cnn, including svm classification on Improved Fisher Vectors and existing discriminative part-mining algorithms by 11.88{\%} and 8.13{\%}, respectively. On the challenging mit-Indoor dataset, our method compares nicely to other s-o-a component-based classification methods.},
	address = {Cham},
	author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
	booktitle = {Computer Vision -- ECCV 2014},
	editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
	isbn = {978-3-319-10599-4},
	pages = {446--461},
	publisher = {Springer International Publishing},
	title = {Food-101 -- Mining Discriminative Components with Random Forests},
	year = {2014}}
@INPROCEEDINGS{image_transfer_learning,
  author={Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks}, 
  year={2014},
  volume={},
  number={},
  pages={1717-1724},
  doi={10.1109/CVPR.2014.222}}
@InProceedings{resnet,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
} 

@inproceedings{ICSA,
Author = {Quanzeng You and Hailin Jin and Zhaowen Wang and Chen Fang and Jiebo Luo},
Title = {Image Captioning with Semantic Attention},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
Year = {2016},
}

@inproceedings{TATT,
  title={Text-guided Attention Model for Image Captioning},
  author={Mun, Jonghwan and Cho, Minsu and Han, Bohyung},
  booktitle={AAAI},
  year={2017}
}

@InProceedings{patro_question,
author = {Patro, Badri and Namboodiri, Vinay P.},
title = {Differential Attention for Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@InProceedings{Compatibility_MDSR,
author = {Singhal, Anirudh and Chopra, Ayush and Ayush, Kumar and Govind, Utkarsh Patel and Krishnamurthy, Balaji},
title = {Towards a Unified Framework for Visual Compatibility Prediction},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
} 

@InProceedings{GSN,
author = {Chopra, Ayush and Sinha, Abhishek and Gupta, Hiresh and Sarkar, Mausoom and Ayush, Kumar and Krishnamurthy, Balaji},
title = {Powering Robust Fashion Retrieval With Information Rich Feature Embeddings},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
month = {June},
year = {2019}
} 

@InProceedings{visualizing_dnn,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}


@article{deep,
author = {LeCun, Yann and Bengio, Y. and Hinton, Geoffrey},
year = {2015},
month = {05},
pages = {436-44},
title = {Deep Learning},
volume = {521},
journal = {Nature},
doi = {10.1038/nature14539}
}

 @InProceedings{cvpr20_matching,
author = {Zhang, Qi and Lei, Zhen and Zhang, Zhaoxiang and Li, Stan Z.},
title = {Context-Aware Attention Network for Image-Text Retrieval},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{cross_attn_matching,

  author={X. {Xu} and T. {Wang} and Y. {Yang} and L. {Zuo} and F. {Shen} and H. T. {Shen}},

  journal={IEEE Transactions on Neural Networks and Learning Systems}, 

  title={Cross-Modal Attention With Semantic Consistence for Image-Text Matching}, 

  year={2020},

  volume={},

  number={},

  pages={1-14},}

@inproceedings{DFAF,
author = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven and Wang, Xiaogang and Li, Hongsheng},
title = {Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2019},
pages = {6632-6641},

doi = {10.1109/CVPR.2019.00680}
}
@inProceedings{mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}


@Misc{BertForQuestionAnswering,
title = {BertForQuestionAnswering},
key = {Bert For QA},
howpublished = {\url{https://huggingface.co/transformers/model\_doc/bert.html\#bertforquestionanswering}},
note = {Accessed June 1, 2020},
author = {huggingface.co}
}


% to ignore this citation for the main paper
@article{yahooresearch,
  title={Modality-Agnostic Attention Fusion for visual search with text feedback},
  author={Dodds, Eric and Culpepper, Jack and Herdade, Simao and Zhang, Yang},
  journal={arXiv preprint arXiv:2007.00145},
  year={2020}
}



@article{applications_image_retrieval,
author = {Halawani, Alaa and Teynor, Alexandra and Setia, Lokesh and Brunner, Gerd and Burkhardt, Hans},
year = {2006},
month = {01},
pages = {14-23},
title = {Fundamentals and Applications of Image Retrieval: An Overview.},
volume = {18},
journal = {Datenbank-Spektrum}
}

@article{similar_search,
author = {Bell, Sean and Bala, Kavita},
title = {Learning Visual Similarity for Product Design with Convolutional Neural Networks},
year = {2015},
issue_date = {August 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2766959},
doi = {10.1145/2766959},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {98},
numpages = {10},
keywords = {search, visual similarity, deep learning, interior design}
}
  

@INPROCEEDINGS{VQA,  author={S. {Antol} and A. {Agrawal} and J. {Lu} and M. {Mitchell} and D. {Batra} and C. L. {Zitnick} and D. {Parikh}},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={VQA: Visual Question Answering},   year={2015},  volume={},  number={},  pages={2425-2433},}

@inproceedings{grounding1,
  title={Understanding natural language commands for robotic navigation and mobile manipulation},
  author={Tellex, Stefanie and Kollar, Thomas and Dickerson, Steven and Walter, Matthew R and Banerjee, Ashis Gopal and Teller, Seth and Roy, Nicholas},
  booktitle={Twenty-fifth AAAI conference on artificial intelligence},
  year={2011}
}

@article{grounding2,
author = {Misra, Dipendra and Sung, Jaeyong and Lee, Kevin and Saxena, Ashutosh},
year = {2015},
month = {11},
pages = {},
title = {Tell Me Dave: Context-Sensitive Grounding of Natural Language to Manipulation Instructions},
volume = {35},
journal = {The International Journal of Robotics Research},
doi = {10.1177/0278364915602060}
}

@INPROCEEDINGS{spatial,  author={L. {Mai} and H. {Jin} and Z. {Lin} and C. {Fang} and J. {Brandt} and F. {Liu}},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Spatial-Semantic Image Search by Visual Feature Synthesis},   year={2017},  volume={},  number={},  pages={1121-1130}}


@inproceedings{cnc,
  title={Contrast and Classify: Training Robust VQA Models},
  author={Yash Kant and Abhinav Moudgil and Dhruv Batra  
          and Devi Parikh and Harsh Agrawal},
  year={2021},
  eprint={2010.06087},
  archivePrefix={arXiv},
  primaryClass={cs.CV}
  }



@INPROCEEDINGS{gradcam,
  author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}, 
  year={2017},
  volume={},
  number={},
  pages={618-626},
  doi={10.1109/ICCV.2017.74}}

@inproceedings{cam,
    author    = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
    title     = {Learning Deep Features for Discriminative Localization},
    booktitle = {Computer Vision and Pattern Recognition},
    year      = {2016}
}

@inproceedings{fairface,
      title={FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age for Bias Measurement and Mitigation},
      author={Karkkainen, Kimmo and Joo, Jungseock},
      booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
      year={2021},
      pages={1548--1558}
    }
    
@inproceedings{mmhs,
  title={Exploring hate speech detection in multimodal publications},
  author={Gomez, Raul and Gibert, Jaume and Gomez, Lluis and Karatzas, Dimosthenis},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1470--1478},
  year={2020}
}    

@article{dogwhistle,
  author    = {Austin Botelho and
               Bertie Vidgen and
               Scott A. Hale},
  title     = {Deciphering Implicit Hate: Evaluating Automated Detection Algorithms
               for Multimodal Hate},
  journal   = {CoRR},
  volume    = {abs/2106.05903},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.05903},
  eprinttype = {arXiv},
  eprint    = {2106.05903},
  timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-05903.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}    


@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@INPROCEEDINGS{mausoom_ground,

  author={A. {Sinha} and B. {Akilesh} and M. {Sarkar} and B. {Krishnamurthy}},

  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 

  title={Attention Based Natural Language Grounding by Navigating Virtual Environment}, 

  year={2019},

  volume={},
  number={},
  pages={236-244},}
@ARTICLE{fasterrcnn,
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
  year={2017},
  volume={39},
  number={6},
  pages={1137-1149},
  doi={10.1109/TPAMI.2016.2577031}}
@INPROCEEDINGS{fast_rcnn,

  author={R. {Girshick}},

  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Fast R-CNN}, 

  year={2015},

  volume={},

  number={},

  pages={1440-1448},}


@ARTICLE{spatial_TPAMI,

  author={A. {Barman} and S. K. {Shah}},

  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 

  title={A Graph-based Approach for Making Consensus-based Decisions in Image Search and Person Re-identification}, 

  year={2019},

  volume={},

  number={},

  pages={1-1},}


 @InProceedings{spatial_WACV,
author = {Ma, Jin and Pang, Shanmin and Yang, Bo and Zhu, Jihua and Li, Yaochen},
title = {Spatial-Content Image Search in Complex Scenes},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
}

@inproceedings{sketch1,
  title={Sketch me that shoe},
  author={Yu, Qian and Liu, Feng and Song, Yi-Zhe and Xiang, Tao and Hospedales, Timothy M and Loy, Chen-Change},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={799--807},
  year={2016}
}

@article{sketch2,
author = {Sangkloy, Patsorn and Burnell, Nathan and Ham, Cusuh and Hays, James},
title = {The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies},
year = {2016},
issue_date = {July 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/2897824.2925954},
doi = {10.1145/2897824.2925954},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {119},
numpages = {12},
keywords = {deep learning, image synthesis, sketch-based image retrieval, triplet network, siamese network}
}
  @InProceedings{Sketch_3,
author = {Dutta, Titir and Biswas, Soma},
title = {s-SBIR: Style Augmented Sketch based Image Retrieval},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
} 

@inproceedings{applications_2,
  title={Large-scale image retrieval with attentive deep local features},
  author={Noh, Hyeonwoo and Araujo, Andre and Sim, Jack and Weyand, Tobias and Han, Bohyung},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3456--3465},
  year={2017}
}

@article{shin2020fashion,
  title={Fashion-IQ 2020 Challenge 2nd Place Team's Solution},
  author={Shin, Minchul and Cho, Yoonjae and Hong, Seongwuk},
  journal={arXiv e-prints},
  pages={arXiv--2007},
  year={2020}
}


@INPROCEEDINGS{attribute2,  author={B. {Zhao} and J. {Feng} and X. {Wu} and S. {Yan}},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search},   year={2017},  volume={},  number={},  pages={6156-6164},}

@INPROCEEDINGS{attribute,  
author={K. E. {Ak} and A. A. {Kassim} and J. H. {Lim} and J. Y. {Tham}},  
booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},   title={Learning Attribute Representations with Localization for Flexible Fashion Search},   year={2018},  volume={},  number={},  pages={7708-7717},}

@article{yu2020curlingnet,
  title={CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data},
  author={Yu, Youngjae and Lee, Seunghwan and Choi, Yuncheol and Kim, Gunhee},
  journal={arXiv e-prints},
  pages={arXiv--2003},
  year={2020}
}

@incollection{shoes_guo,
title = {Dialog-based Interactive Image Retrieval},
author = {Guo, Xiaoxiao and Wu, Hui and Cheng, Yu and Rennie, Steven and Tesauro, Gerald and Feris, Rogerio},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {678--688},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7348-dialog-based-interactive-image-retrieval.pdf}
}


@InProceedings{Scene_4,
author = {Wang, Sijin and Wang, Ruiping and Yao, Ziwei and Shan, Shiguang and Chen, Xilin},
title = {Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval},
booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
month = {March},
year = {2020}
} 
@inproceedings{attribute4,
  title={Fine-Grained Fashion Similarity Learning by Attribute-Specific Embedding Network},
  author={Ma, Zhe and Dong, Jianfeng and Long, Zhongzi and Zhang, Yao and He, Yuan and Xue, Hui and Ji, Shouling},
  booktitle={Thirty-fourth AAAI Conference on Artificial Intelligence},
  year = {2020}
}
@article{attribute3,
  title={Explainable fashion recommendation: A semantic attribute region guided approach},
  author={Hou, Min and Wu, Le and Chen, Enhong and Li, Zhi and Zheng, Vincent W and Liu, Qi},
  journal={arXiv preprint arXiv:1905.12862},
  year={2019}
}

@misc{flamingo,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@INPROCEEDINGS{scene_graph,  author={J. {Johnson} and R. {Krishna} and M. {Stark} and L. {Li} and D. A. {Shamma} and M. S. {Bernstein} and L. {Fei-Fei}},  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Image retrieval using scene graphs},   year={2015},  volume={},  number={},  pages={3668-3678},}

@misc{bias,
      title={Challenging the appearance of machine intelligence: Cognitive bias in LLMs}, 
      author={Alaina N. Talboy and Elizabeth Fuller},
      year={2023},
      eprint={2304.01358},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}
@misc{mscoco,
      title={Microsoft COCO: Common Objects in Context}, 
      author={Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Dollár},
      year={2015},
      eprint={1405.0312},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{cc12m,
  title = {{Conceptual 12M}: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts},
  author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle = {CVPR},
  year = {2021},
}

@misc{scene2,
Author = {Sahana Ramnath and Amrita Saha and Soumen Chakrabarti and Mitesh M. Khapra},
Title = {Scene Graph based Image Retrieval -- A case study on the CLEVR Dataset},
Year = {2019},
Eprint = {arXiv:1911.00850},
}
@inproceedings{intermodal,
  title={Stacked cross attention for image-text matching},
  author={Lee, Kuang-Huei and Chen, Xi and Hua, Gang and Hu, Houdong and He, Xiaodong},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={201--216},
  year={2018}
}

@inproceedings{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  booktitle={Advances in neural information processing systems},
  pages={91--99},
  year={2015}
}

@inproceedings{sagan,
  title={Self-attention generative adversarial networks},
  author={Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  booktitle={International Conference on Machine Learning},
  pages={7354--7363},
  year={2019}
}

@inproceedings{danet,
  title={Dual attention network for scene segmentation},
  author={Fu, Jun and Liu, Jing and Tian, Haijie and Li, Yong and Bao, Yongjun and Fang, Zhiwei and Lu, Hanqing},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3146--3154},
  year={2019}
}

@InProceedings{Mult_Scale,
author = {Jandial, Surgan and Chopra, Ayush and Ayush, Kumar  and Hemani, Mayur and Krishnamurthy, Balaji},
title = {Robust Cloth Warping via Multi-Scale Patch Adversarial Loss for Virtual Try-On Framework},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2019}
} 
@InProceedings{Aux_Ton,
author = {Jandial, Surgan and Chopra, Ayush and Ayush, Kumar  and Krishnamurthy, Balaji},
title = {Powering Virtual Try-On via Auxiliary Human Segmentation Learning},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
month = {Oct},
year = {2019}
} 

@inproceedings{film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}
@misc{vilt,
  doi = {10.48550/ARXIV.2102.03334},
  
  url = {https://arxiv.org/abs/2102.03334},
  
  author = {Kim, Wonjae and Son, Bokyung and Kim, Ildoo},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@ARTICLE{gem,
  author={F. {Radenović} and G. {Tolias} and O. {Chum}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Fine-Tuning CNN Image Retrieval with No Human Annotation}, 
  year={2019},
  volume={41},
  number={7},
  pages={1655-1668}
}



@inproceedings{showandtell,
  title={Show and tell: A neural image caption generator},
  author={Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3156--3164},
  year={2015}
}



@inproceedings{finegrained2,
  title={Fine-grained image classification by exploring bipartite-graph labels},
  author={Zhou, Feng and Lin, Yuanqing},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1124--1133},
  year={2016}
}

@inproceedings{sketch_example,
  title={Interactive sketch \& fill: Multiclass sketch-to-image translation},
  author={Ghosh, Arnab and Zhang, Richard and Dokania, Puneet K and Wang, Oliver and Efros, Alexei A and Torr, Philip HS and Shechtman, Eli},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1171--1180},
  year={2019}
}

@inproceedings{tryon1,
  title={SieveNet: A Unified Framework for Robust Image-Based Virtual Try-On},
  author={Jandial, Surgan and Chopra, Ayush and Ayush, Kumar and Hemani, Mayur and Krishnamurthy, Balaji and Halwai, Abhijeet},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={2182--2190},
  year={2020}
}

@INPROCEEDINGS{tryon2,
  author={X. {Han} and Z. {Wu} and Z. {Wu} and R. {Yu} and L. S. {Davis}},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={VITON: An Image-Based Virtual Try-on Network}, 
  year={2018},
  volume={},
  number={},
  pages={7543-7552},}
  
  @inproceedings{imagesearch,
  title={Deep image retrieval: Learning global representations for image search},
  author={Gordo, Albert and Almaz{\'a}n, Jon and Revaud, Jerome and Larlus, Diane},
  booktitle={European conference on computer vision},
  pages={241--257},
  year={2016},
  organization={Springer}
}

@inproceedings{relative_attribute,
  title={Thinking outside the pool: Active training image creation for relative attributes},
  author={Yu, Aron and Grauman, Kristen},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={708--718},
  year={2019}
}

@INPROCEEDINGS{certain_attribute,
  author={B. {Zhao} and J. {Feng} and X. {Wu} and S. {Yan}},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Memory-Augmented Attribute Manipulation Networks for Interactive Fashion Search}, 
  year={2017},
  volume={},
  number={},
  pages={6156-6164}
}
  
  
@article{adv_text,
   title={A Geometry-Inspired Attack for Generating Natural Language Adversarial Examples},
   url={http://dx.doi.org/10.18653/V1/2020.COLING-MAIN.585},
   DOI={10.18653/v1/2020.coling-main.585},
   journal={Proceedings of the 28th International Conference on Computational Linguistics},
   publisher={International Committee on Computational Linguistics},
   author={Meng, Zhao and Wattenhofer, Roger},
   year={2020} }
@inproceedings{textattack,
  title={TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP},
  author={Morris, John and Lifland, Eli and Yoo, Jin Yong and Grigsby, Jake and Jin, Di and Qi, Yanjun},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={119--126},
  year={2020}
}
@misc{flava,
  doi = {10.48550/ARXIV.2112.04482},
  
  url = {https://arxiv.org/abs/2112.04482},
  
  author = {Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {FLAVA: A Foundational Language And Vision Alignment Model},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@inproceedings{adv_text2,
author = {Sato, Motoki and Suzuki, Jun and Shindo, Hiroyuki and Matsumoto, Yuji},
title = {Interpretable Adversarial Perturbation in Input Embedding Space for Text},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Following great success in the image processing field, the idea of adversarial training has been applied to tasks in the natural language processing (NLP) field. One promising approach directly applies adversarial training developed in the image processing field to the input word embedding space instead of the discrete input space of texts. However, this approach abandons such interpretability as generating adversarial texts to significantly improve the performance of NLP tasks. This paper restores interpretability to such methods by restricting the directions of perturbations toward the existing words in the input embedding space. As a result, we can straightforwardly reconstruct each input with perturbations to an actual text by considering the perturbations to be the replacement of words in the sentence while maintaining or even improving the task performance.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4323–4330},
numpages = {8},
location = {Stockholm, Sweden},
series = {IJCAI'18}
}

@inproceedings{imagesearch2,
  title={Large-scale image retrieval with attentive deep local features},
  author={Noh, Hyeonwoo and Araujo, Andre and Sim, Jack and Weyand, Tobias and Han, Bohyung},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3456--3465},
  year={2017}
}

@inproceedings{recommend,
  title={Image-based recommendations on styles and substitutes},
  author={McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Van Den Hengel, Anton},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={43--52},
  year={2015}
}

@inproceedings{recommend2,
  title={Augmented reality based recommendations based on perceptual shape style compatibility with objects in the viewpoint and color compatibility with the background},
  author={Tanmay, Kumar and Ayush, Kumar},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@inproceedings{caption1,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3128--3137},
  year={2015}
}

@inproceedings{
infobert,
title={Info{\{}BERT{\}}: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Boxin Wang and Shuohang Wang and Yu Cheng and Zhe Gan and Ruoxi Jia and Bo Li and Jingjing Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=hpH98mK5Puk}
}

@inproceedings{
FreeLB:,
title={FreeLB: Enhanced Adversarial Training for Natural Language Understanding},
author={Chen Zhu and Yu Cheng and Zhe Gan and Siqi Sun and Tom Goldstein and Jingjing Liu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BygzbyHFvB}
}

@article{image_defense,
  title={Attacking adversarial attacks as a defense},
  author={Wu, Boxi and Pan, Heng and Shen, Li and Gu, Jindong and Zhao, Shuai and Li, Zhifeng and Cai, Deng and He, Xiaofei and Liu, Wei},
  journal={arXiv preprint arXiv:2106.04938},
  year={2021}
}

@inproceedings{zhuang-etal-2021-robustly,
    title = "A Robustly Optimized {BERT} Pre-training Approach with Post-training",
    author = "Zhuang, Liu  and
      Wayne, Lin  and
      Ya, Shi  and
      Jun, Zhao",
    booktitle = "Proceedings of the 20th Chinese National Conference on Computational Linguistics",
    month = aug,
    year = "2021",
    address = "Huhhot, China",
    publisher = "Chinese Information Processing Society of China",
    url = "https://aclanthology.org/2021.ccl-1.108",
    pages = "1218--1227",
    abstract = "In the paper we present a {`}pre-training{'}+{`}post-training{'}+{`}fine-tuning{'} three-stage paradigm which is a supplementary framework for the standard {`}pre-training{'}+{`}fine-tuning{'} languagemodel approach. Furthermore based on three-stage paradigm we present a language modelnamed PPBERT. Compared with original BERT architecture that is based on the standard two-stage paradigm we do not fine-tune pre-trained model directly but rather post-train it on the domain or task related dataset first which helps to better incorporate task-awareness knowl-edge and domain-awareness knowledge within pre-trained model also from the training datasetreduce bias. Extensive experimental results indicate that proposed model improves the perfor-mance of the baselines on 24 NLP tasks which includes eight GLUE benchmarks eight Su-perGLUE benchmarks six extractive question answering benchmarks. More remarkably our proposed model is a more flexible and pluggable model where post-training approach is able to be plugged into other PLMs that are based on BERT. Extensive ablations further validate the effectiveness and its state-of-the-art (SOTA) performance. The open source code pre-trained models and post-trained models are available publicly.",
    language = "English",
}

@inproceedings{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}


@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@inproceedings{
vit,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@article{pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}


@article{Wang2021SimVLMSV,
  title={SimVLM: Simple Visual Language Model Pretraining with Weak Supervision},
  author={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.10904}
}

@InProceedings{10.1007/978-3-030-58577-8_7,
author="Chen, Yen-Chun
and Li, Linjie
and Yu, Licheng
and El Kholy, Ahmed
and Ahmed, Faisal
and Gan, Zhe
and Cheng, Yu
and Liu, Jingjing",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="UNITER: UNiversal Image-TExt Representation Learning",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="104--120",
abstract="Joint image-text embedding is the bedrock for most Vision-and-Language (V+L) tasks, where multimodality inputs are simultaneously processed for joint visual and textual understanding. In this paper, we introduce UNITER, a UNiversal Image-TExt Representation, learned through large-scale pre-training over four image-text datasets (COCO, Visual Genome, Conceptual Captions, and SBU Captions), which can power heterogeneous downstream V+L tasks with joint multimodal embeddings. We design four pre-training tasks: Masked Language Modeling (MLM), Masked Region Modeling (MRM, with three variants), Image-Text Matching (ITM), and Word-Region Alignment (WRA). Different from previous work that applies joint random masking to both modalities, we use conditional masking on pre-training tasks (i.e., masked language/region modeling is conditioned on full observation of image/text). In addition to ITM for global image-text alignment, we also propose WRA via the use of Optimal Transport (OT) to explicitly encourage fine-grained alignment between words and image regions during pre-training. Comprehensive analysis shows that both conditional masking and OT-based WRA contribute to better pre-training. We also conduct a thorough ablation study to find an optimal combination of pre-training tasks. Extensive experiments show that UNITER achieves new state of the art across six V+L tasks (over nine datasets), including Visual Question Answering, Image-Text Retrieval, Referring Expression Comprehension, Visual Commonsense Reasoning, Visual Entailment, and NLVR{\$}{\$}^2{\$}{\$}2(Code is available at https://github.com/ChenRocks/UNITER.).",
isbn="978-3-030-58577-8"
}



@inproceedings{NEURIPS2021_50525975,
 author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {9694--9705},
 publisher = {Curran Associates, Inc.},
 title = {Align before Fuse: Vision and Language Representation Learning with Momentum Distillation},
 url = {https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf},
 volume = {34},
 year = {2021}
}


@InProceedings{Huang_2021_CVPR,
    author    = {Huang, Zhicheng and Zeng, Zhaoyang and Huang, Yupan and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
    title     = {Seeing Out of the Box: End-to-End Pre-Training for Vision-Language Representation Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12976-12985}
}


@InProceedings{pmlr-v139-brock21a,
  title = 	 {High-Performance Large-Scale Image Recognition Without Normalization},
  author =       {Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {1059--1071},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/brock21a/brock21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/brock21a.html},
  abstract = 	 {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when fine-tuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%.}
}
@article{clipadapter,
  title={Clip-adapter: Better vision-language models with feature adapters},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2110.04544},
  year={2021}
}
@article{vlmo,
  author    = {Wenhui Wang and
               Hangbo Bao and
               Li Dong and
               Furu Wei},
  title     = {VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts},
  journal   = {CoRR},
  volume    = {abs/2111.02358},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.02358},
  eprinttype = {arXiv},
  eprint    = {2111.02358},
  timestamp = {Fri, 05 Nov 2021 15:25:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-02358.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{xvlm,
  author    = {Yan Zeng and
               Xinsong Zhang and
               Hang Li},
  title     = {Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual
               Concepts},
  journal   = {CoRR},
  volume    = {abs/2111.08276},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.08276},
  eprinttype = {arXiv},
  eprint    = {2111.08276},
  timestamp = {Tue, 12 Jul 2022 09:19:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-08276.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{sam,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}
@inproceedings{winoground,
  title={Winoground: Probing vision and language models for visio-linguistic compositionality},
  author={Thrush, Tristan and Jiang, Ryan and Bartolo, Max and Singh, Amanpreet and Williams, Adina and Kiela, Douwe and Ross, Candace},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5238--5248},
  year={2022}
}
@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={arXiv preprint arXiv:2304.08485},
  year={2023}
}

@article{cot,
  author       = {Jason Wei and
                  Xuezhi Wang and
                  Dale Schuurmans and
                  Maarten Bosma and
                  Ed H. Chi and
                  Quoc Le and
                  Denny Zhou},
  title        = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  journal      = {CoRR},
  volume       = {abs/2201.11903},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  eprinttype    = {arXiv},
  eprint       = {2201.11903},
  timestamp    = {Fri, 22 Apr 2022 16:06:31 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2201-11903.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{selfimprove,
  title={Large language models can self-improve},
  author={Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2210.11610},
  year={2022}
}
@inproceedings{cc3m,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}
@article{open-flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andy Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
  journal={ArXiv},
  year={2022},
  volume={abs/2204.14198}
}
@article{laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={arXiv preprint arXiv:2210.08402},
  year={2022}
}
@article{sachin,
  title={Finetune like you pretrain: Improved finetuning of zero-shot vision models},
  author={Goyal, Sachin and Kumar, Ananya and Garg, Sankalp and Kolter, Zico and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2212.00638},
  year={2022}
}