\begin{thebibliography}{58}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei,
  Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski,
  Barreira, Vinyals, Zisserman, and Simonyan}]{open-flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
  Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock,
  Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
  Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{ArXiv}, abs/2204.14198.

\bibitem[{{Antol} et~al.(2015){Antol}, {Agrawal}, {Lu}, {Mitchell}, {Batra},
  {Zitnick}, and {Parikh}}]{VQA}
S.~{Antol}, A.~{Agrawal}, J.~{Lu}, M.~{Mitchell}, D.~{Batra}, C.~L. {Zitnick},
  and D.~{Parikh}. 2015.
\newblock Vqa: Visual question answering.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pages 2425--2433.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{gpt}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc.

\bibitem[{Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and Soricut}]{cc12m}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021.
\newblock {Conceptual 12M}: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{CVPR}.

\bibitem[{Chen et~al.(2020{\natexlab{a}})Chen, Li, Yu, El~Kholy, Ahmed, Gan,
  Cheng, and Liu}]{10.1007/978-3-030-58577-8_7}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu. 2020{\natexlab{a}}.
\newblock Uniter: Universal image-text representation learning.
\newblock In \emph{Computer Vision -- ECCV 2020}, pages 104--120, Cham.
  Springer International Publishing.

\bibitem[{Chen et~al.(2020{\natexlab{b}})Chen, Li, Yu, Kholy, Ahmed, Gan,
  Cheng, and Liu}]{uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed~El Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu. 2020{\natexlab{b}}.
\newblock Uniter: Universal image-text representation learning.
\newblock In \emph{ECCV}.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}.

\bibitem[{Deng et~al.(2021)Deng, Yang, Chen, Zhou, and Li}]{transvg}
Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li.
  2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2104.08541} {Transvg:
  End-to-end visual grounding with transformers}.

\bibitem[{Devlin et~al.(2019{\natexlab{a}})Devlin, Chang, Lee, and
  Toutanova}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
  2019{\natexlab{a}}.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Devlin et~al.(2019{\natexlab{b}})Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
  2019{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby}]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.
\newblock \href {https://openreview.net/forum?id=YicbFdNTTy} {An image is worth
  16x16 words: Transformers for image recognition at scale}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Eichenberg et~al.(2021)Eichenberg, Black, Weinbach, Parcalabescu, and
  Frank}]{magma}
Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and
  Anette Frank. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2112.05253} {Magma --
  multimodal augmentation of generative models through adapter-based
  finetuning}.

\bibitem[{Gao et~al.(2021)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and
  Qiao}]{clipadapter}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu~Qiao. 2021.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{arXiv preprint arXiv:2110.04544}.

\bibitem[{{Girshick}(2015)}]{fast_rcnn}
R.~{Girshick}. 2015.
\newblock Fast r-cnn.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pages 1440--1448.

\bibitem[{Goyal et~al.(2022)Goyal, Kumar, Garg, Kolter, and
  Raghunathan}]{sachin}
Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan.
  2022.
\newblock Finetune like you pretrain: Improved finetuning of zero-shot vision
  models.
\newblock \emph{arXiv preprint arXiv:2212.00638}.

\bibitem[{Goyal et~al.(2016)Goyal, Khot, Summers{-}Stay, Batra, and
  Parikh}]{GoyalKSBP16}
Yash Goyal, Tejas Khot, Douglas Summers{-}Stay, Dhruv Batra, and Devi Parikh.
  2016.
\newblock \href {http://arxiv.org/abs/1612.00837} {Making the {V} in {VQA}
  matter: Elevating the role of image understanding in visual question
  answering}.
\newblock \emph{CoRR}, abs/1612.00837.

\bibitem[{Halawani et~al.(2006)Halawani, Teynor, Setia, Brunner, and
  Burkhardt}]{applications_image_retrieval}
Alaa Halawani, Alexandra Teynor, Lokesh Setia, Gerd Brunner, and Hans
  Burkhardt. 2006.
\newblock Fundamentals and applications of image retrieval: An overview.
\newblock \emph{Datenbank-Spektrum}, 18:14--23.

\bibitem[{He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick}]{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick. 2022.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16000--16009.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}.

\bibitem[{Huang et~al.(2022)Huang, Gu, Hou, Wu, Wang, Yu, and
  Han}]{selfimprove}
Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,
  and Jiawei Han. 2022.
\newblock Large language models can self-improve.
\newblock \emph{arXiv preprint arXiv:2210.11610}.

\bibitem[{Huang et~al.(2021)Huang, Zeng, Huang, Liu, Fu, and
  Fu}]{Huang_2021_CVPR}
Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong
  Fu. 2021.
\newblock Seeing out of the box: End-to-end pre-training for vision-language
  representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 12976--12985.

\bibitem[{Johnson et~al.(2017)Johnson, Hariharan, Van Der~Maaten, Fei-Fei,
  Lawrence~Zitnick, and Girshick}]{johnson2017clevr}
Justin Johnson, Bharath Hariharan, Laurens Van Der~Maaten, Li~Fei-Fei,
  C~Lawrence~Zitnick, and Ross Girshick. 2017.
\newblock Clevr: A diagnostic dataset for compositional language and elementary
  visual reasoning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2901--2910.

\bibitem[{Kamath et~al.(2021)Kamath, Singh, LeCun, Synnaeve, Misra, and
  Carion}]{mdetr}
Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and
  Nicolas Carion. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2104.12763} {Mdetr -- modulated
  detection for end-to-end multi-modal understanding}.

\bibitem[{Karpathy and Fei-Fei(2015)}]{caption1}
Andrej Karpathy and Li~Fei-Fei. 2015.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3128--3137.

\bibitem[{Kim et~al.(2021)Kim, Son, and Kim}]{vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2102.03334} {Vilt:
  Vision-and-language transformer without convolution or region supervision}.

\bibitem[{Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo et~al.}]{sam}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
  2023.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}.

\bibitem[{Koh et~al.(2023)Koh, Salakhutdinov, and Fried}]{fromage}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2301.13823} {Grounding language
  models to images for multimodal generation}.

\bibitem[{Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and
  Hoi}]{NEURIPS2021_50525975}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
  and Steven Chu~Hong Hoi. 2021.
\newblock \href
  {https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf}
  {Align before fuse: Vision and language representation learning with momentum
  distillation}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 9694--9705. Curran Associates, Inc.

\bibitem[{Li et~al.(2019)Li, Yatskar, Yin, Hsieh, and Chang}]{visbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho{-}Jui Hsieh, and Kai{-}Wei Chang.
  2019.
\newblock \href {http://arxiv.org/abs/1908.03557} {Visualbert: {A} simple and
  performant baseline for vision and language}.
\newblock \emph{CoRR}, abs/1908.03557.

\bibitem[{Lin et~al.(2015)Lin, Maire, Belongie, Bourdev, Girshick, Hays,
  Perona, Ramanan, Zitnick, and Dollár}]{mscoco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,
  James Hays, Pietro Perona, Deva Ramanan, C.~Lawrence Zitnick, and Piotr
  Dollár. 2015.
\newblock \href {http://arxiv.org/abs/1405.0312} {Microsoft coco: Common
  objects in context}.

\bibitem[{Liu et~al.(2021)Liu, Bugliarello, Ponti, Reddy, Collier, and
  Elliott}]{marvl}
Fangyu Liu, Emanuele Bugliarello, Edoardo~Maria Ponti, Siva Reddy, Nigel
  Collier, and Desmond Elliott. 2021.
\newblock \href {http://arxiv.org/abs/2109.13238} {Visually grounded reasoning
  across languages and cultures}.
\newblock \emph{CoRR}, abs/2109.13238.

\bibitem[{Liu et~al.(2023)Liu, Li, Wu, and Lee}]{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee. 2023.
\newblock Visual instruction tuning.
\newblock \emph{arXiv preprint arXiv:2304.08485}.

\bibitem[{Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov}]{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}, abs/1907.11692.

\bibitem[{Lu et~al.(2019)Lu, Batra, Parikh, and Lee}]{vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13--23.

\bibitem[{Meng et~al.(2022)Meng, Sharma, Andonian, Belinkov, and Bau}]{edit}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
  2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2210.07229} {Mass-editing
  memory in a transformer}.

\bibitem[{Merullo et~al.(2022)Merullo, Castricato, Eickhoff, and
  Pavlick}]{limber}
Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2209.15162} {Linearly mapping
  from image to text space}.

\bibitem[{Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever}]{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever. 2021.
\newblock \href {http://arxiv.org/abs/2103.00020} {Learning transferable visual
  models from natural language supervision}.
\newblock \emph{CoRR}, abs/2103.00020.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever}]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21(1):5485--5551.

\bibitem[{Ren et~al.(2017)Ren, He, Girshick, and Sun}]{fasterrcnn}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2017.
\newblock \href {https://doi.org/10.1109/TPAMI.2016.2577031} {Faster r-cnn:
  Towards real-time object detection with region proposal networks}.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 39(6):1137--1149.

\bibitem[{Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman et~al.}]{laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al. 2022.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock \emph{arXiv preprint arXiv:2210.08402}.

\bibitem[{Sharma et~al.(2018)Sharma, Ding, Goodman, and Soricut}]{cc3m}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.
\newblock \href {https://doi.org/10.18653/v1/P18-1238} {Conceptual captions: A
  cleaned, hypernymed, image alt-text dataset for automatic image captioning}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565,
  Melbourne, Australia. Association for Computational Linguistics.

\bibitem[{Suhr et~al.(2017{\natexlab{a}})Suhr, Lewis, Yeh, and
  Artzi}]{suhr2017corpus}
Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017{\natexlab{a}}.
\newblock A corpus of natural language for visual reasoning.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 217--223.

\bibitem[{Suhr et~al.(2017{\natexlab{b}})Suhr, Lewis, Yeh, and
  Artzi}]{suhr-etal-2017-corpus}
Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/P17-2034} {A corpus of natural
  language for visual reasoning}.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pages 217--223,
  Vancouver, Canada. Association for Computational Linguistics.

\bibitem[{Suhr et~al.(2018{\natexlab{a}})Suhr, Zhou, Zhang, Zhang, Bai, and
  Artzi}]{suhr2018corpus}
Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
  2018{\natexlab{a}}.
\newblock A corpus for reasoning about natural language grounded in
  photographs.
\newblock \emph{arXiv preprint arXiv:1811.00491}.

\bibitem[{Suhr et~al.(2018{\natexlab{b}})Suhr, Zhou, Zhang, Bai, and
  Artzi}]{nlvr2}
Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai, and Yoav Artzi.
  2018{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/1811.00491} {A corpus for reasoning about
  natural language grounded in photographs}.
\newblock \emph{CoRR}, abs/1811.00491.

\bibitem[{Talboy and Fuller(2023)}]{bias}
Alaina~N. Talboy and Elizabeth Fuller. 2023.
\newblock \href {http://arxiv.org/abs/2304.01358} {Challenging the appearance
  of machine intelligence: Cognitive bias in llms}.

\bibitem[{Tan and Bansal(2019)}]{lxmert}
Hao Tan and Mohit Bansal. 2019.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing}.

\bibitem[{Thrush et~al.(2022)Thrush, Jiang, Bartolo, Singh, Williams, Kiela,
  and Ross}]{winoground}
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe
  Kiela, and Candace Ross. 2022.
\newblock Winoground: Probing vision and language models for visio-linguistic
  compositionality.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5238--5248.

\bibitem[{Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample}]{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample. 2023.
\newblock \href {https://doi.org/10.48550/ARXIV.2302.13971} {Llama: Open and
  efficient foundation language models}.

\bibitem[{Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals,
  and Hill}]{frozen}
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, S.~M.~Ali Eslami, Oriol Vinyals,
  and Felix Hill. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2106.13884} {Multimodal
  few-shot learning with frozen language models}.

\bibitem[{Wang et~al.(2021{\natexlab{a}})Wang, Bao, Dong, and Wei}]{vlmo}
Wenhui Wang, Hangbo Bao, Li~Dong, and Furu Wei. 2021{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2111.02358} {Vlmo: Unified
  vision-language pre-training with mixture-of-modality-experts}.
\newblock \emph{CoRR}, abs/2111.02358.

\bibitem[{Wang et~al.(2021{\natexlab{b}})Wang, Yu, Yu, Dai, Tsvetkov, and
  Cao}]{Wang2021SimVLMSV}
Zirui Wang, Jiahui Yu, Adams~Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
  2021{\natexlab{b}}.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock \emph{ArXiv}, abs/2108.10904.

\bibitem[{Wei et~al.(2022{\natexlab{a}})Wei, Wang, Schuurmans, Bosma, Chi, Le,
  and Zhou}]{cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~H. Chi, Quoc Le, and
  Denny Zhou. 2022{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2201.11903} {Chain of thought prompting
  elicits reasoning in large language models}.
\newblock \emph{CoRR}, abs/2201.11903.

\bibitem[{Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Ichter,
  Xia, Chi, Le, and Zhou}]{chain_thought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc Le, and Denny Zhou. 2022{\natexlab{b}}.
\newblock \href {https://doi.org/10.48550/ARXIV.2201.11903} {Chain-of-thought
  prompting elicits reasoning in large language models}.

\bibitem[{Zeng et~al.(2021)Zeng, Zhang, and Li}]{xvlm}
Yan Zeng, Xinsong Zhang, and Hang Li. 2021.
\newblock \href {http://arxiv.org/abs/2111.08276} {Multi-grained vision
  language pre-training: Aligning texts with visual concepts}.
\newblock \emph{CoRR}, abs/2111.08276.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer}]{opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2205.01068} {Opt: Open
  pre-trained transformer language models}.

\bibitem[{Zhuang et~al.(2021)Zhuang, Wayne, Ya, and
  Jun}]{zhuang-etal-2021-robustly}
Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021.
\newblock \href {https://aclanthology.org/2021.ccl-1.108} {A robustly optimized
  {BERT} pre-training approach with post-training}.
\newblock In \emph{Proceedings of the 20th Chinese National Conference on
  Computational Linguistics}, pages 1218--1227, Huhhot, China. Chinese
  Information Processing Society of China.

\end{thebibliography}
