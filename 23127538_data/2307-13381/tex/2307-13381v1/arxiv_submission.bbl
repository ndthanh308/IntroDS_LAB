\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abay et~al.(2020)Abay, Zhou, Baracaldo, Rajamoni, Chuba, and
  Ludwig]{abay2020mitigating}
Annie Abay, Yi~Zhou, Nathalie Baracaldo, Shashank Rajamoni, Ebube Chuba, and
  Heiko Ludwig.
\newblock Mitigating bias in federated learning.
\newblock \emph{arXiv preprint arXiv:2012.02447}, 2020.

\bibitem[Albain et~al.(2009)Albain, Unger, Crowley, Coltman, and
  Hershman]{albain2009racial}
Kathy~S Albain, Joseph~M Unger, John~J Crowley, Charles~A Coltman, and Dawn~L
  Hershman.
\newblock Racial disparities in cancer survival among randomized clinical
  trials patients of the southwest oncology group.
\newblock \emph{JNCI: Journal of the National Cancer Institute}, 101\penalty0
  (14):\penalty0 984--992, 2009.

\bibitem[Beznosikov et~al.(2022)Beznosikov, Dvurechenskii, Koloskova, Samokhin,
  Stich, and Gasnikov]{beznosikov2022decentralized}
Aleksandr Beznosikov, Pavel Dvurechenskii, Anastasiia Koloskova, Valentin
  Samokhin, Sebastian~U Stich, and Alexander Gasnikov.
\newblock Decentralized local stochastic extra-gradient for variational
  inequalities.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 38116--38133, 2022.

\bibitem[Blum et~al.(2021)Blum, Haghtalab, Phillips, and Shao]{blum2021one}
Avrim Blum, Nika Haghtalab, Richard~Lanas Phillips, and Han Shao.
\newblock One for one, or all for all: Equilibria and optimality of
  collaboration in federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1005--1014. PMLR, 2021.

\bibitem[Chambolle and Pock(2011)]{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40:\penalty0
  120--145, 2011.

\bibitem[Chambolle and Pock(2016)]{chambolle2016ergodic}
Antonin Chambolle and Thomas Pock.
\newblock On the ergodic convergence rates of a first-order primal--dual
  algorithm.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  253--287, 2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255. Ieee, 2009.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020distributionally}
Yuyang Deng, Mohammad~Mahdi Kamani, and Mehrdad Mahdavi.
\newblock Distributionally robust federated averaging.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 15111--15122, 2020.

\bibitem[du~Terrail et~al.(2022)du~Terrail, Ayed, Cyffers, Grimberg, He, Loeb,
  Mangold, Marchand, Marfoq, Mushtaq, et~al.]{du2022flamby}
Jean~Ogier du~Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg,
  Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum
  Mushtaq, et~al.
\newblock Flamby: Datasets and benchmarks for cross-silo federated learning in
  realistic settings.
\newblock 2022.

\bibitem[Duchi et~al.(2023)Duchi, Hashimoto, and
  Namkoong]{duchi2023distributionally}
John Duchi, Tatsunori Hashimoto, and Hongseok Namkoong.
\newblock Distributionally robust losses for latent covariate mixtures.
\newblock \emph{Operations Research}, 71\penalty0 (2):\penalty0 649--664, 2023.

\bibitem[Duchi and Namkoong(2021)]{duchi2021learning}
John~C Duchi and Hongseok Namkoong.
\newblock Learning models with uniform performance via distributionally robust
  optimization.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (3):\penalty0
  1378--1406, 2021.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and
  Zemel]{dwork2012fairness}
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
\newblock Fairness through awareness.
\newblock In \emph{Proceedings of the 3rd Innovations in Theoretical Computer
  Science Conference}, pages 214--226, 2012.

\bibitem[Gao and Kleywegt(2022)]{gao2022distributionally}
Rui Gao and Anton Kleywegt.
\newblock Distributionally robust stochastic optimization with wasserstein
  distance.
\newblock \emph{Mathematics of Operations Research}, 2022.

\bibitem[Graham(2015)]{graham2015disparities}
Garth Graham.
\newblock Disparities in cardiovascular disease risk in the united states.
\newblock \emph{Current Cardiology Reviews}, 11\penalty0 (3):\penalty0
  238--245, 2015.

\bibitem[Hamedani and Aybat(2021)]{hamedani2021primal}
Erfan~Yazdandoost Hamedani and Necdet~Serhat Aybat.
\newblock A primal-dual algorithm with line search for general convex-concave
  saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0
  1299--1329, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hou et~al.(2021)Hou, Thekumparampil, Fanti, and Oh]{hou2021efficient}
Charlie Hou, Kiran~K Thekumparampil, Giulia Fanti, and Sewoong Oh.
\newblock Efficient algorithms for federated saddle point optimization.
\newblock \emph{arXiv preprint arXiv:2102.06333}, 2021.

\bibitem[Hsieh et~al.(2020)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2020non}
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4387--4398. PMLR, 2020.

\bibitem[Izmailov et~al.(2022)Izmailov, Kirichenko, Gruver, and
  Wilson]{izmailov2022feature}
Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew~G Wilson.
\newblock On feature learning in the presence of spurious correlations.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 38516--38532, 2022.

\bibitem[Jones and Tonetti(2020)]{jones2020nonrivalry}
Charles~I Jones and Christopher Tonetti.
\newblock Nonrivalry and the economics of data.
\newblock \emph{American Economic Review}, 110\penalty0 (9):\penalty0 2819--58,
  2020.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, et~al.]{kairouz2019federated}
Peter Kairouz, H.~Brendan McMahan, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  14\penalty0 (1--2):\penalty0 1--210, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Karimireddy et~al.(2022)Karimireddy, Guo, and
  Jordan]{karimireddy2022mechanisms}
Sai~Praneeth Karimireddy, Wenshuo Guo, and Michael~I Jordan.
\newblock Mechanisms that incentivize data sharing in federated learning.
\newblock \emph{arXiv preprint arXiv:2207.04557}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kulynych et~al.(2020)Kulynych, Madras, Milli, Raji, Zhou, and
  Zemel]{paml2020}
Bogdan Kulynych, David Madras, Smitha Milli, Inioluwa~Deborah Raji, Angela
  Zhou, and Richard Zemel.
\newblock Participatory approaches to machine learning.
\newblock International Conference on Machine Learning Workshop, 2020.

\bibitem[Le and Yang(2015)]{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Levy et~al.(2020)Levy, Carmon, Duchi, and Sidford]{levy2020large}
Daniel Levy, Yair Carmon, John~C Duchi, and Aaron Sidford.
\newblock Large-scale methods for distributionally robust optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8847--8860, 2020.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Schmidt, Alstr{\o}m, and
  Stich]{li2022partial}
Bo~Li, Mikkel~N Schmidt, Tommy~S Alstr{\o}m, and Sebastian~U Stich.
\newblock Partial variance reduction improves non-convex federated learning on
  heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2212.02191}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Diao, Chen, and He]{li2022federated}
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.
\newblock Federated learning on non-iid data silos: An experimental study.
\newblock In \emph{2022 IEEE 38th International Conference on Data Engineering
  (ICDE)}, pages 965--978. IEEE, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Sanjabi, Beirami, and Smith]{li2019fair}
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith.
\newblock Fair resource allocation in federated learning.
\newblock \emph{arXiv preprint arXiv:1905.10497}, 2019.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Beirami, Sanjabi, and
  Smith]{li2020tilted}
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith.
\newblock Tilted empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:2007.01162}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 2:\penalty0
  429--450, 2020{\natexlab{b}}.

\bibitem[Martinez et~al.(2021)Martinez, Bertran, Papadaki, Rodrigues, and
  Sapiro]{martinez2021blind}
Natalia~L Martinez, Martin~A Bertran, Afroditi Papadaki, Miguel Rodrigues, and
  Guillermo Sapiro.
\newblock Blind pareto fairness and subgroup robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  7492--7501. PMLR, 2021.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and
  Richt{\'a}rik]{mishchenko2022proxskip}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter
  Richt{\'a}rik.
\newblock Proxskip: Yes! local gradient steps provably lead to communication
  acceleration! finally!
\newblock \emph{arXiv preprint arXiv:2202.09357}, 2022.

\bibitem[Mohajerin~Esfahani and Kuhn(2018)]{mohajerin2018data}
Peyman Mohajerin~Esfahani and Daniel Kuhn.
\newblock Data-driven distributionally robust optimization using the
  wasserstein metric: Performance guarantees and tractable reformulations.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1-2):\penalty0
  115--166, 2018.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4615--4625. PMLR, 2019.

\bibitem[Namkoong and Duchi(2016)]{namkoong2016stochastic}
Hongseok Namkoong and John~C Duchi.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Nana-Sinkam et~al.(2021)Nana-Sinkam, Kraschnewski, Sacco, Chavez,
  Fouad, Gal, AuYoung, Namoos, Winn, Sheppard, et~al.]{nana2021health}
Patrick Nana-Sinkam, Jennifer Kraschnewski, Ralph Sacco, Jennifer Chavez, Mona
  Fouad, Tamas Gal, Mona AuYoung, Asmaa Namoos, Robert Winn, Vanessa Sheppard,
  et~al.
\newblock Health disparities and equity in the era of covid-19.
\newblock \emph{Journal of Clinical and Translational Science}, 5\penalty0
  (1):\penalty0 e99, 2021.

\bibitem[Nash~Jr(1950)]{nash1950bargaining}
John~F Nash~Jr.
\newblock The bargaining problem.
\newblock \emph{Econometrica}, pages 155--162, 1950.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Pentland et~al.(2021)Pentland, Lipton, and
  Hardjono]{pentland2021building}
Alex Pentland, Alexander Lipton, and Thomas Hardjono.
\newblock \emph{Building the New Economy: Data as Capital}.
\newblock MIT Press, 2021.

\bibitem[Pillutla et~al.(2021)Pillutla, Laguel, Malick, and
  Harchaoui]{pillutla2021federated}
Krishna Pillutla, Yassine Laguel, J{\'e}r{\^o}me Malick, and Zaid Harchaoui.
\newblock Federated learning with superquantile aggregation for heterogeneous
  data.
\newblock \emph{arXiv e-prints}, pages arXiv--2112, 2021.

\bibitem[Piratla et~al.(2021)Piratla, Netrapalli, and
  Sarawagi]{piratla2021focus}
Vihari Piratla, Praneeth Netrapalli, and Sunita Sarawagi.
\newblock Focus on the common good: Group distributional robustness follows.
\newblock \emph{arXiv preprint arXiv:2110.02619}, 2021.

\bibitem[Quinonero-Candela et~al.(2008)Quinonero-Candela, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2008dataset}
Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset Shift in Machine Learning}.
\newblock MIT Press, 2008.

\bibitem[Rahimian and Mehrotra(2019)]{rahimian2019distributionally}
Hamed Rahimian and Sanjay Mehrotra.
\newblock Distributionally robust optimization: A review.
\newblock \emph{arXiv preprint arXiv:1908.05659}, 2019.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LkFG3lB13U5}.

\bibitem[Ro et~al.(2021)Ro, Chen, Mathews, Mohri, and
  Suresh]{ro2021communication}
Jae Ro, Mingqing Chen, Rajiv Mathews, Mehryar Mohri, and Ananda~Theertha
  Suresh.
\newblock Communication-efficient agnostic federated averaging.
\newblock \emph{arXiv preprint arXiv:2104.02748}, 2021.

\bibitem[Santurkar et~al.(2020)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock \emph{arXiv preprint arXiv:2008.04859}, 2020.

\bibitem[Sim et~al.(2020)Sim, Zhang, Chan, and Low]{sim2020collaborative}
Rachael Hwee~Ling Sim, Yehong Zhang, Mun~Choon Chan, and Bryan Kian~Hsiang Low.
\newblock Collaborative machine learning with incentive-aware model rewards.
\newblock In \emph{International Conference on Machine Learning}, pages
  8927--8936. PMLR, 2020.

\bibitem[Tseng(2008)]{tseng2008accelerated}
Paul Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock \emph{submitted to SIAM Journal on Optimization}, 2\penalty0 (3),
  2008.

\bibitem[Vokinger et~al.(2021)Vokinger, Feuerriegel, and
  Kesselheim]{vokinger2021continual}
Kerstin~N Vokinger, Stefan Feuerriegel, and Aaron~S Kesselheim.
\newblock Continual learning in medical devices: Fda's action plan and beyond.
\newblock \emph{The Lancet Digital Health}, 3\penalty0 (6):\penalty0
  e337--e338, 2021.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and Poor]{wang2020tackling}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7611--7623, 2020.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H~Brendan McMahan, Maruan
  Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
  et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wiesemann et~al.(2014)Wiesemann, Kuhn, and
  Sim]{wiesemann2014distributionally}
Wolfram Wiesemann, Daniel Kuhn, and Melvyn Sim.
\newblock Distributionally robust convex optimization.
\newblock \emph{Operations Research}, 62\penalty0 (6):\penalty0 1358--1376,
  2014.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, and
  Srebro]{woodworth2020minibatch}
Blake~E Woodworth, Kumar~Kshitij Patel, and Nati Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6281--6292, 2020.

\bibitem[Xu et~al.(2021)Xu, Lyu, Ma, Miao, Foo, and Low]{xu2021gradient}
Xinyi Xu, Lingjuan Lyu, Xingjun Ma, Chenglin Miao, Chuan~Sheng Foo, and Bryan
  Kian~Hsiang Low.
\newblock Gradient driven rewards to guarantee fairness in collaborative
  machine learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16104--16117, 2021.

\bibitem[Yu et~al.(2020)Yu, Bagdasaryan, and Shmatikov]{yu2020salvaging}
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov.
\newblock Salvaging federated learning by local adaptation.
\newblock \emph{arXiv preprint arXiv:2002.04758}, 2020.

\bibitem[Yu et~al.(2022)Yu, Wei, Karimireddy, Ma, and Jordan]{yu2022tct}
Yaodong Yu, Alexander Wei, Sai~Praneeth Karimireddy, Yi~Ma, and Michael Jordan.
\newblock Tct: Convexifying federated learning using bootstrapped neural
  tangent kernels.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30882--30897, 2022.

\bibitem[Zecchin et~al.(2022)Zecchin, Kountouris, and
  Gesbert]{zecchin2022communication}
Matteo Zecchin, Marios Kountouris, and David Gesbert.
\newblock Communication-efficient distributionally robust decentralized
  learning.
\newblock \emph{arXiv preprint arXiv:2205.15614}, 2022.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Malekmohammadi, Chen, and
  Yu]{zhang2022proportional}
Guojun Zhang, Saber Malekmohammadi, Xi~Chen, and Yaoliang Yu.
\newblock Proportional fairness in federated learning.
\newblock \emph{arXiv preprint arXiv:2202.01666}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Aybat, and
  G{\"u}rb{\"u}zbalaban]{zhang2021robust}
Xuan Zhang, Necdet~Serhat Aybat, and Mert G{\"u}rb{\"u}zbalaban.
\newblock Robust accelerated primal-dual methods for computing saddle points.
\newblock \emph{arXiv preprint arXiv:2111.12743}, 2021.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Aybat, and
  Gurbuzbalaban]{zhang2022sapd}
Xuan Zhang, Necdet Aybat, and Mert Gurbuzbalaban.
\newblock {SAPD}+: An accelerated stochastic method for nonconvex-concave
  minimax problems.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=GiUpEVQmNx8}.

\bibitem[Zhang and Lin(2015)]{zhang2015stochastic}
Yuchen Zhang and Xiao Lin.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  353--361. PMLR, 2015.

\end{thebibliography}
