\section{Theoretical Analysis}\label{sec:theory}
We now present the convergence results for {\algname} in solving the min-max optimization problem described in Eq.~\eqref{eq:dro-def}. 
Firstly, in Section~\ref{sec:theory-sc-c}, we introduce the results for the strongly-convex-concave setting. Subsequently, in Section~\ref{sec:theory-sc-sc}, we present the results for the strongly-convex-convex setting. 
\vspace{-0.1in}
\subsection{Strongly-convex-concave Setting}\label{sec:theory-sc-c}
We first introduce how to choice the parameters for {\algname} in when $\psi$ is convex and $\{f_i\}_{i\in[N]}$ are strongly convex in Condition~\ref{condition:stepsize-scc}.

% \vspace{-0.1in}
\begin{condition}\label{condition:stepsize-scc}
The parameters of Algorithm~\ref{Algorithm:ScaffoldAPD} are defined as
\begin{equation}
\sigma_{-1} = \gamma_0 \bar{\tau},\quad \sigma_r = \gamma_r \tau_r, \quad
\theta_r = \sigma_{r-1}/\sigma_r,\quad
\gamma_{r+1} = \gamma_r (1 + \mu_{\bx}\tau_{r}).
\end{equation}
\end{condition}
\vspace{-0.05in}
Next we present our convergence results in this setting.

% \vspace{-0.1in}
\begin{theorem}\label{thm:main-sc-c}
Supppose $\{f_i\}_{i\in[N]}$ are $\mu_{\bx}$-strongly convex. If  Assumption~\ref{assumption:L_yx-smooth} and Assumption\ref{assumption:local-noise} hold,  and we let the parameters $\{\tau_{r}, \sigma_{r},  \gamma_{r}, \theta_{r}\}$ of Algorithm~\ref{Algorithm:ScaffoldAPD} satisfy Condition~\ref{condition:stepsize-scc}, then the $R$-th iterate $(\bx^{R}, \blambda^{R})$ satisfies
\begin{equation}
    \E\left[\|\bx^{R} - \bx^{\star}\|^{2} \right] 
    \leq \frac{C_1}{R^{2}}\left[\|\bx^{\star} - \bx^{0}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}\right] + \frac{C_2}{R}\zeta^{2},
\end{equation}
where $C_1, C_2 \geq 0$ are non-negative constants.
\end{theorem}

\vspace{-0.1in}
\begin{corollary}\label{corollary:sc-c}
Under the assumptions in Theorem~\ref{thm:main-sc-c},
\begin{itemize}[leftmargin=0.7cm]
    \item (deterministic local gradient): If the local gradient satisfies $g_i(\bx)=\nabla f_i(\bx)$ for $i\in[N]$, then after
    $        O\left(\frac{\|\bx^{\star} - \bx^{0}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}}{\sqrt{\varepsilon}}\right)$
    rounds, we have $\|\bx^{R} - \bx^{\star}\|^{2} \leq \varepsilon$.
    \item (stochastic local gradient): If the local gradient satisfies Assumption~\ref{assumption:local-noise} with $\sigma > 0$, then after $O\left(\frac{\|\bx^{\star} - \bx^{0}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}}{\sqrt{\varepsilon}} + \frac{\zeta^{2}}{\varepsilon}\right)$
    rounds, we have $\E\left[\|\bx^{R} - \bx^{\star}\|^{2}\right] \leq \varepsilon$.
\end{itemize}
\end{corollary}
\vspace{-0.15in}
\begin{remark}
As suggested by the Corollary~\ref{corollary:sc-c}, in the deterministic setting ($\zeta=0$, when applying {\algname} for solving the min-max problems in the vanilla AFL and the super-quantile approach, {\algname} achieves the convergence rate of $O(1/R^2)$.
The rate of {\algname} is faster than existing algorithms --
the convergence rate  is $O(1/R)$ in both \citet{mohri2019agnostic, pillutla2021federated}. 
In addition, the algorithm with theoretical convergence guarantees introduced in \citet{mohri2019agnostic} does not apply local steps (i.e., number of local updates $J=1$), resulting in inferior performance in practical applications.
\end{remark}
\vspace{-0.15in}
\begin{remark}
{\algname} matches the rates ($O(1/R^{2})$) of the centralized accelerated primal-dual algorithm~\citep{hamedani2021primal} when $\zeta=0$. Meanwhile, our proposed algorithm converges faster compared to directly applying centralized gradient descent ascent (GDA) and extra-gradient method (EG) for solving Eq.~\eqref{eq:dro-def}, which achieve a rate of $O(1/R)$. 
\end{remark}




\subsection{Strongly-convex-strongly-concave Setting}\label{sec:theory-sc-sc}
We next present results for the strongly-convex-strongly-concave setting. Differing from the strongly-convex-concave setting, the parameters of Algorithm~\ref{Algorithm:ScaffoldAPD} are fixed across different rounds, as follows.
\begin{condition}\label{condition:stepsize-scsc}
The parameters of Algorithm~\ref{Algorithm:ScaffoldAPD} are defined as
\begin{small}
\begin{equation}
\begin{aligned}
    \mu_{\bx}\tau = O\left(\frac{1-\theta}{\theta}\right),\quad \mu_{\blambda}\sigma = O\left(\frac{1-\theta}{\theta}\right),\quad \frac{1}{1-\theta} = O\left(\Bigg(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\,\Bigg) \vee \frac{\zeta^{2}}{\mu_{\bx}\varepsilon}\right).
\end{aligned}
\end{equation}
\end{small}
\end{condition}

\begin{theorem}\label{thm:main-sc-sc}
Suppose $\{f_i\}_{i\in[N]}$ are $\mu_{\bx}$-strongly convex and $\psi$ is $\mu_{\by}$-strongly convex. 
If  Assumption~\ref{assumption:L_yx-smooth} and Assumption\ref{assumption:local-noise} hold,  and we let the parameters $\{\tau, \sigma, \theta\}$ of Algorithm~\ref{Algorithm:ScaffoldAPD} satisfy Condition~\ref{condition:stepsize-scsc}, then the $R$-th iterate $(\bx^{R}, \blambda^{R})$ satisfies 
\begin{equation}
\E\left[\mu_{\bx}\|\bx^{r} - \bx^{\star}\|^{2}\right] \leq C_1\theta^{R}\left[\|\bx^{0} - \bx^{\star}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}\right]+ C_2(1-\theta)\frac{\zeta^{2}}{\mu_{\bx}},
\end{equation}
where $C_1, C_2 \geq 0$ are non-negative constants.
\end{theorem}
\begin{corollary}
Under the assumptions in Theorem~\ref{thm:main-sc-sc},
\begin{itemize}[leftmargin=0.7cm]
    \item (deterministic local gradient): If the local gradient satisfies $g_i(\bx)=\nabla f_i(\bx)$ for $i\in[N]$, then after 
    $O\left(\Bigg(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\Bigg)\log\left(\frac{\|\bx^{0} - \bx^{\star}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}}{\varepsilon}\right)\right)$ 
    rounds, $\mu_{\bx}\|\bx^{R} - \bx^{\star}\|^{2} \leq \varepsilon$.
    \item (stochastic local gradient): If the local gradient satisfies Assumption~\ref{assumption:local-noise} with $\zeta > 0$, then after 
    $O\left(\Bigg(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}+\frac{\zeta^{2}}{\mu_{\bx}\varepsilon}\Bigg)\log\left(\frac{\|\bx^{0} - \bx^{\star}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}}{\varepsilon} \right)\right)$ 
    rounds, $\E\left[\mu_{\bx}\|\bx^{R} - \bx^{\star}\|^{2}\right] \leq \varepsilon$.
\end{itemize}
\end{corollary}

% \vspace{-0.1in}
\begin{remark}
Our algorithm converges linearly to the global saddle point when each client applies a noiseless gradient for local updates (i.e., $\zeta=0$) in the presence of data heterogeneity and client-drift in federated learning. 
In contrast, previous approaches exhibit only sub-linear convergence. In the strongly-convex-strongly-concave setting, DRFA~\citep{deng2020distributionally} converges to the saddle-point solution with rate $O(1/R)$ when there is no data heterogeneity and $\zeta=0$.
\end{remark}
% \vspace{-0.15in}

\begin{remark}
By applying bias correction in local updates, the convergence rates of our algorithm match those of the centralized accelerated primal-dual algorithm~\citep{zhang2021robust} in both deterministic and stochastic settings. 
\end{remark}


\begin{remark}
Compared to the standard minimization in federated learning, the DRO objective results in a slightly worse condition number in terms of convergence rate. 
In comparison to the standard minimization objective in federated learning, the DRO objective yields a slightly worse condition number. Solving DRO with {\algname} requires $(\sqrt{L_{\bx\bx}/\mu_{\bx}} + \sqrt{L_{\blambda\bx}^{2}/(L_{\bx\bx} \mu_{\blambda})})$ times more communication rounds compared to solving minimization problems with ProxSkip~\citep{mishchenko2022proxskip}.
\end{remark}



