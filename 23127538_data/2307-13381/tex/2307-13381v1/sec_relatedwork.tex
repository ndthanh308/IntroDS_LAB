% \vspace{-0.1in}
\section{Related Work}\label{sec:relatedwork}

\vspace{-0.05in}
\paragraph{Cross-silo FL.} Federated learning (FL) is a distributed machine learning paradigm that enables model training without exchanging raw data. 
In cross-silo FL (which is our focus), valuable data is split across different organizations, and each organization is either protected by privacy regulations or unwilling to share their raw data. Such organizations are referred to as ``data islands'' and can be found in hospital networks, financial institutions, autonomous-vehicle companies, etc. Thus, cross-silo FL involves a few highly reliable clients who potentially have extremely diverse data.

\vspace{0.05in}
The most widely used federated optimization algorithm is Federated Averaging (FedAvg) \citep{mcmahan2017communication}, which averages the local model updates to produce a global model. However, FedAvg is known to suffer from poor convergence when the local datasets are heterogeneous ~\cite[etc.]{hsieh2020non,li2020federated,karimireddy2020scaffold,reddi2021adaptive,wang2021field,du2022flamby}. Scaffold~\citep{karimireddy2020scaffold} corrects for this heterogeneity, leading to more accurate updates and faster convergence~\citep{mishchenko2022proxskip,li2022partial,yu2022tct}. However, all of these methods are restricted to optimizing the average of the client objectives.

\vspace{-0.1in}
\paragraph{Distributionally Robust Optimization.} DRO is a framework for optimization under uncertainty, where the goal is to optimize the worst-case performance over a set of probability distributions. See \citet{rahimian2019distributionally} for a review and its history in risk management, economics, and finance. Fast centralized optimization methods have been developed when uncertainity is represented by $f$-divergences \citep{wiesemann2014distributionally,namkoong2016stochastic,levy2020large} or Wasserstein distances \citep{mohajerin2018data,gao2022distributionally}. The former approach accounts for changing proportions of subpopulations, relating it to notions of subpopulation fairness~\citep{duchi2023distributionally,santurkar2020breeds,piratla2021focus,martinez2021blind}. Our work also implicitly focuses on $f$-divergences. \citet{deng2020distributionally} and \citet{zecchin2022communication} adapt the gradient-descent-ascent (GDA) algorithm to solve the federated and decentralized DRO problems respectively. However, these methods inherit the slowness of both the GDA and FedAvg algorithms, making their performance trail the state of the art for the average objective~\citep{mishchenko2022proxskip}.



\vspace{-0.1in}
\paragraph{Fairness in FL.} While fairness is an extremely multi-faceted concept, here we are concerned with the distribution of model performance across clients. \citet{mohri2019agnostic} noted that minimizing the average of the client losses may lead to unfair distribution of errors, and instead proposed an \emph{agnostic FL} (AFL) framework which minimizes a worst-case mixture of the client losses. Alternatives and extensions to AFL have also been  proposed subsequently~\citet{li2019fair,li2020tilted,pillutla2021federated}. Again, the convergence of optimization methods for these losses (when analyzed) is significantly slower than their centralized counterparts.

\vspace{0.05in}
While all of these works demand equitable performance across all clients, others propose to scale a client's accuracy in proportion to their contribution~\citep{sim2020collaborative,blum2021one,xu2021gradient,zhang2022proportional,karimireddy2022mechanisms}. These methods are motivated by game-theoretic considerations to incentivize clients and improve the quality of the data contributions. Our framework \eqref{eq:dro-def} can be applied to such mechanisms by an appropriate choice of $\{f_i\}, \Lambda, \text{and } \psi$. For example, \citet{zhang2022proportional} show how to set these to recover the Nash bargaining solution~\citep{nash1950bargaining}. Thus, our work can be seen as a practical optimization algorithm to implement many of the mechanisms studied in FL.

\vspace{0.05in}
Finally, personalization---serving a separate model to each client---has also been proposed as a method to improve the distribution of client performance~\citep{yu2020salvaging}. However, personalized models are sometimes not feasible either due to regulations~\citep{vokinger2021continual} or because the client may not have additional data. Further, personalization does not remove the differences in performance (though it does reduce it)~\citep{yu2020salvaging}, nor does it solve the game-theoretic considerations described above. Extending our work to this setting is an important question we leave for future work.

