\section{Experiments}\label{sec:exp}


We now study the performance of {\algname} for solving  federated DRO problems on both synthetic datasets and real-world datasets. 
Our primary objective when working with synthetic datasets is to validate the convergence analysis of {\algname}. 
On real-world datasets, we compare with existing federated optimization algorithms for learning robust and fair models (DRFA~\citep{deng2020distributionally}, AFL~\citep{mohri2019agnostic}, and $q$-FFL~\citep{li2019fair}) as well as widely used federated algorithms for solving minimization problems including FedAvg~\citep{mcmahan2017communication} and SCAFFOLD~\citep{karimireddy2020scaffold}. 
After conducting thorough evaluations, we have observed that  our proposed accelerated algorithms achieve fast convergence rates and strong empirical performance on real-world datasets. 
We have provided supplementary experimental results in Appendix~\ref{sec:appendix-exp}, which includes additional baseline methods, ablations on our algorithm, and other relevant findings.



\vspace{-0.1in}
\subsection{Results on Synthetic Datasets}
To construct the synthetic datasets, we follow the setup described in Eq.~\eqref{eq:dro-def} and consider a simple robust regression problem. 
Specifically, for the $i$-th client, the local function $f_i$ is defined as $f_i(\bx) =  \frac{1}{m_i}\sum_{j=1}^{m_i}(\langle \ba_i^{j}, \bx\rangle - y_i^j)^{2} + \frac{\mu_{\bx}}{2}\|\bx\|^{2}$, where $j$ is sample index on this client and there are $m_i$ training samples on client-$i$. 
We apply the $\chi^{2}$ penalty for regularizing the weight vector $\blambda$. 
To generate the data, each input ${\ba_i^{j}}$ is sampled from a Gaussian distribution $\ba_i^{j} \sim \mathcal{N}(\bm{0}, \bm{I}_{d\times d})$. Then we random generate $\widehat{\bx} \sim \mathcal{N}(\bm{0}, c^{2}\bm{I}_{d\times d})$, and $\delta_{i}^{\bx} \sim \mathcal{N}(\bm{0}, \sigma^{2}\bm{I}_{d\times d})$. Based on $(\widehat{\bx}, \delta_{i}^{\bx})$,  we generate $y_i^{i}$ as $y_i^{i} = \langle \ba_i^j, \widehat{\bx} + \delta_{i}^{\bx}\rangle$. 
Therefore, there exist distribution shifts across different clients (i.e., concept shifts). 
We set $N=5$, $d=10$, and $m_i=100$ for $i\in[N]$. 
To measure the algorithm performance, we evaluate the distance between $\bx^{R}$ and the optimal solution $\bx^{\star}$: $\|\bx^{R} - \bx^{\star}\|^{2}$.

\vspace{0.05in}
We compare {\algname} with DRFA~\citep{deng2020distributionally} on this synthetic dataset. 
The regularization parameter $\rho$ for $\psi$ is varied from $0.01$ to $0.1$. 
For both algorithms, we set the number of local steps to be 100 and select the algorithm parameters through grid search. 
The comparison results are summarized in Fig~\ref{fig:exp-syn}. 
As shown in Fig~\ref{fig:exp-syn}, we observe that our proposed algorithm {\algname} achieves linear convergence rates in all three settings. 
In contrast, DRFA converges much more slowly compared to {\algname}. 
We have included more experimental results under this synthetic setup in Appendix~\ref{sec:appendix-exp}, including results on the effect of local steps and data heterogeneity. 



% Figure environment removed



\subsection{Results on Real-world Datasets}


\textbf{Dataset setup.} 
We evaluate the performance of various federated learning algorithms on CIFAR100~\citep{krizhevsky2009learning} and TinyImageNet~\citep{le2015tiny}. 
We follow the setup used in \citet{li2022federated}: we consider different degrees of data heterogeneity by applying Dirichlet allocation, denoted by $\text{Dir}(\alpha)$, to partition the dataset into different clients. 
Smaller $\alpha$ values in $\text{Dir}(\alpha)$ leads to higher data heterogeneity. 
Additionally, after the data partition through the Dirichlet allocation, we randomly sample $30\%$ of the clients and remove $70\%$ training samples from those clients. 
Such a sub-sampling procedure can better model real-world data-imbalance scenarios.  
We consider the number of clients $N=20$ for both datasets. Results on larger number of clients and other real-world datasets can be found in Appendix~\ref{sec:appendix-exp}.

\vspace{0.05in}
\textbf{Model setup.} 
We consider learning a linear classifier by using representations extracted from pre-trained deep neural networks. 
Previous studies have demonstrated the efficacy of this approach, particularly in the context of data heterogeneity~\citep{yu2022tct} as well as sub-group robustness~\citep{izmailov2022feature}. 
For both datasets, we apply the ResNet-18~\citep{he2016deep} pre-trained on ImageNet-1k~\citep{deng2009imagenet} as the backbone for extracting feature representations of the image samples. 
To apply the pre-trained ResNet-18, we resize the images from CIFAR100 and TinyImageNet to 3$\times$224$\times$224. 



\begin{table*}[t]
	\centering
	\caption{The average and worst-20\% top-1 accuracy of our algorithm ({\algname}) vs.\ state-of-the-art federated learning algorithms evaluated on CIFAR100 and Tiny-ImageNet. 
 The highest top-1 accuracy in each setting is highlighted in \textbf{bold}.
	}
	% \vspace{0.25em}
	\label{tab:main_table}
	    \footnotesize
     \resizebox{0.95\textwidth}{!}{
		\begin{tabular}{cccc|cc|ccc}
			\toprule
			\multirow{1}{*}{\bf Datasets}  &   \multirow{1}{*}{\bf Methods}                 & \multicolumn{6}{c}{\bf Non-i.i.d. degree}   
			  \\
			  \midrule
			 &                   & \multicolumn{2}{c}{$\alpha=0.01$}    & \multicolumn{2}{c}{$\alpha=0.05$}    & \multicolumn{2}{c}{$\alpha=0.1$}   
			  \\
		\midrule
			\multirow{11}{*}{CIFAR-100}   &  &    
			\texttt{average} & \texttt{worst-20\%} & \texttt{average} & \texttt{worst-20\%} &\texttt{average} & \texttt{worst-20\%} & 
			\\ 
			\cmidrule(lr){1-9} 
			   & {FedAvg} & 38.77 & 15.93 & 35.96 & 24.43 & 36.57 & 26.50     \\ 
			\tablewhitespace
			   & {SCAFFOLD} & 37.38 & 14.65 & 35.28 & 24.77 & 35.63 & 25.61      \\
			  \tablewhitespace
			   & {$q$-FFL} & 26.39 & 5.43 & 29.60 & 18.62 & 30.38 & 21.98      \\
			  \tablewhitespace
			   & {AFL} & 47.38 & 18.04 & \textbf{44.73} & 22.06 & \textbf{44.89} & 27.27      \\
			  \tablewhitespace
			   & {DRFA} & 46.47 & 26.77 & 41.61 & 27.66 & 43.20 & 32.04      \\
			  \tablewhitespace
                 & \textit{SCAFF-PD}  & \textbf{49.03} & \textbf{29.30} & 42.06 & \textbf{28.37} & 43.69 & \textbf{32.77}     \\
                \midrule
   \multirow{11}{*}{TinyImageNet}   &  &    
			\texttt{average} & \texttt{worst-20\%} & \texttt{average} & \texttt{worst-20\%} &\texttt{average} & \texttt{worst-20\%} & 
			\\ 
			\cmidrule(lr){1-9} 
			   & {FedAvg} & 33.66 & 18.18 &  31.53 & 23.46   & 35.08 & 27.61     \\ 
			\tablewhitespace
			   & {SCAFFOLD} & 31.79 & 15.85  &  30.43 &  22.57 & 34.58 & 27.33      \\
			  \tablewhitespace
			   & {$q$-FFL} & 25.50 & 9.70 &  27.45 &  19.38  & 32.90 & 26.24      \\
			  \tablewhitespace
			   & {AFL} & \textbf{45.32} & 18.65 & \textbf{45.54}  & 28.02  & \textbf{46.11} & 29.50      \\
			  \tablewhitespace
			   & {DRFA} & 36.80 & 22.32 &  37.39  & 28.38 & 37.39 & 28.38      \\
			  \tablewhitespace
                 & \textit{SCAFF-PD} & 41.26 & \textbf{25.32}    & 39.32 & \textbf{30.27}  & 41.23 &  \textbf{29.78}  \\
			\bottomrule
		\end{tabular}
    }
	\vspace{-.75em}
\end{table*}

\vspace{0.05in}
\textbf{Comparisons with existing approaches.} 
We consider three data heterogeneity settings for both datasets. 
To measure the performance of different algorithms, beside the average classification accuracy across clients, we also evaluate the \texttt{worst-20\%} accuracy\footnote{First sort the clients by test  accuracy, then select the lower $20\%$ of clients and compute the mean from this subset.} for comparing fairness and robustness  of different federated learning algorithms. 
Previous studies have employed this metric for comparing different model in federated learning~\cite{li2019fair}.
The comparative results are summarized in Table~\ref{tab:main_table}. We find that our proposed algorithm outperforms existing methods in most settings, especially under higher heterogeneity. 
For example, when the level of data heterogeneity is low ($\alpha=0.1$), applying {\algname} does not yield very large improvements compared to the existing algorithms. 
In the case of high data heterogeneity ($\alpha=0.01$), our proposed algorithm largely improves the worst-20\% accuracy performance on both datasets.


\vspace{0.05in}
\textbf{Effect of $\rho$ in DRO.} 
To gain a better understanding of the empirical performance of our algorithm, we investigate the role of $\rho$ in DRO when applying our algorithm. We consider $\rho \in \{0.1, 0.2, 0.5\}$ and measure both the average and worst-20\% accuracy during training. 
We present the results in Fig~\ref{fig:exp-real-effect-rho}. 
We find that when $\rho$ is small, {\algname} can achieve better fairness/robustness---the worst-20\% accuracy significantly improves when we decrease the $\rho$ in {\algname}. 
Meanwhile, the experimental results suggest that smaller $\rho$ leads to faster convergence w.r.t. worst-20\% accuracy for our algorithm. 
On the other hand, when applying smaller $\rho$, the condition number of the min-max optimization problem becomes worse. 
Fortunately, our algorithm is guaranteed to  achieve accelerated rates, making it particularly beneficial in scenarios where $\mu_{\blambda}$ is small.
As we have demonstrated in Fig~\ref{fig:exp-syn}, our proposed algorithm still converges relatively fast when $\rho$ is small.

\vspace{0.05in}
In addition, we study the trade-off between average accuracy vs.\ worst-20\% accuracy vs.\ best-20\% accuracy for different algorithms. 
The results are summarized in Fig~\ref{fig:exp-real-scatter} (in Appendix~\ref{sec:appendix-exp}). 
Without sacrificing much on average accuracy and best-20\% accuracy, our algorithm largely improves the worst-20\% accuracy. 




% Figure environment removed


