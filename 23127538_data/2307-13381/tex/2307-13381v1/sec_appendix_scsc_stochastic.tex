\subsection{Proofs -- strongly-convex-strongly-concave (SC-SC) setting}\label{subsec:appendix-proof-sc-sc}
In this subsection, we first present the technical lemma in Section~\ref{subsec:appendix-proof-sc-sc-lemmas}. 
Next, we analyze how to set the step size related parameters in Section~\ref{subsec:appendix-proof-sc-sc-parameters}.
Finally, we prove Theorem~\ref{thm:main-sc-sc} in Section~\ref{subsec:appendix-proof-sc-sc-theorem-proof}.

\subsubsection{Technical  Lemmas}\label{subsec:appendix-proof-sc-sc-lemmas}


\begin{lemma}\label{lemma:sc-sc-stochastic}
If we set the step size in Alg~\ref{Algorithm:ScaffoldAPD} as $\tau \cdot L_{\bx\bx} \leq 1$, 
then for any $\bx, \blambda$ we have
\begin{equation}\label{eq:lemma-sc-sc-0}
\begin{aligned}
\E\left[{F(\bx^{r+1}, \blambda)} -  {F(\bx, \blambda^{r+1})}\right] \leq -Z_{r+1} + V_r + \Delta_r + C\tau\zeta^{2},
\end{aligned}
\end{equation}
where $Z_{r}, V_{r}, \Delta_r$ are defined as
\begin{small}
\begin{align}
Z_{r+1} &= \E\left[\langle \bq^{r+1}, \blambda^{r+1} - \blambda \rangle + \left(\frac{1}{2\sigma}+\frac{\mu_{\blambda}}{2}\right)\|\blambda^{r+1} - \blambda\|^{2} + \left(\frac{1}{2\tau}+\frac{\mu_{\bx}}{8}\right)\|\bx^{r+1} -\bx\|^{2}\right], \nonumber\\
V_{r} &=   \E\left[\theta^{r}\langle\bq^{r}, \blambda^{r} - \blambda\rangle +  \frac{1}{2\sigma}\|\blambda^{r} - \blambda\|^{2} + \frac{1}{2\tau}\|\bx^{r} - \bx\|^{2}\right], \\
\Delta_r &= \E\left[\left(\frac{\theta L_{\blambda\bx}}{2\pi} - \frac{1}{2\sigma}\right)\|\blambda^{r+1}-\blambda^{r}\|^{2} + \left(\frac{\pi \theta L_{\blambda\bx}}{2} + 3L_{\bx\bx} - \frac{1}{4\tau}\right)\|\bx^{r+1}-\bx^{r}\|^{2}\right],\nonumber
\end{align}
\end{small}
where $\pi>0$ is a parameter and $C \geq 0$ is a constant.

\end{lemma}


\begin{proof} 
Most of the steps are the same as in the Lemma~\ref{lemma:sc-c-stochastic}. To start with, based on the condition that $\psi(\blambda)$ is strongly convex in $\blambda$, we apply Lemma~\ref{Lemma:lamba-update-optimal-confidtion},
\begin{equation}
\begin{aligned}
    &\psi(\blambda^{r+1}) - \langle\bs^{r}, \blambda^{r+1} - \blambda\rangle \\
    \leq &\psi(\blambda^r) + {\frac{1}{\sigma}\left[\mathrm{D}(\blambda, \blambda^{r})-\mathrm{D}(\blambda, \blambda^{r+1})-\mathrm{D}(\blambda^{r+1}, \blambda^{r})\right]} - \frac{\mu_{\blambda}}{2}\|\blambda^{r+1}-\blambda\|^{2}.
\end{aligned}
\end{equation}
Next, we change the way we upper bound $\theta\langle\bq^{r}, \blambda^{r+1} - \blambda^{r}\rangle $ in the strongly-convex-concave setting, and we upper bound this term as follows,
\begin{equation}\label{eq:apd-bound-q-sc-sc}
\begin{aligned}
    \theta\langle\bq^{r}, \blambda^{r+1} - \blambda^{r}\rangle 
    &= \theta \langle\nabla_{\blambda}\Phi(\bx^{r}, \blambda^{r}) - \nabla_{\blambda}\Phi(\bx^{r-1}, \blambda^{r-1}), \blambda^{r+1} - \blambda^{r}\rangle \\
    &=  \theta \langle\nabla_{\blambda}\Phi(\bx^{r}, \blambda^{r}) - \nabla_{\blambda}\Phi(\bx^{r-1}, \blambda^{r}), \blambda^{r+1} - \blambda^{r}\rangle \\
    &\leq  {\theta}\|\nabla_{\blambda}\Phi(\bx^{r}, \blambda^{r}) - \nabla_{\blambda}\Phi(\bx^{r-1}, \blambda^{r})\|\|\blambda^{r+1} - \blambda^{r}\| \\
    &\leq  \frac{\pi\theta L_{\blambda\bx}}{2}\|\bx^{r} - \bx^{r-1}\|^{2} + 
    \frac{\theta L_{\blambda\bx}}{2\pi }\|\blambda^{r+1} - \blambda^{r}\|^{2},
\end{aligned}
\end{equation}
where $\pi > 0$ is a constant. Then we have
\begin{small}
\begin{equation*}
\begin{aligned}
&  {F(\bx^{r+1}, \blambda)} -  {F(\bx, \blambda^{r+1})}  \\ 
\leq & -\underbrace{\E\left[\langle \bq^{r+1}, \blambda^{r+1} - \blambda \rangle  + \frac{1}{\tau}\mathrm{D}(\bx, \bx^{r+1}) + \frac{\mu_{\bx}}{8}\|\bx^{r+1} - \bx\|^{2} + \frac{1}{\sigma}\mathrm{D}(\blambda, \blambda^{r+1}) + \frac{\mu_{\blambda}}{2}\|\blambda^{r+1} - \blambda\|^{2}  \right]}_{Z_{r+1}} \\ 
&+\underbrace{\E\left[  \theta\langle\bq^{r}, \blambda^{r} - \blambda\rangle +  \frac{1}{\sigma}\mathrm{D}(\blambda, \blambda^{r}) + \frac{1}{\tau}\mathrm{D}(\bx, \bx^{r})  \right]}_{V_r}   +\frac{\pi\theta L_{\blambda\bx}}{2}\E\left[\|\bx^{r} - \bx^{r-1}\|^{2}\right] + 
    \frac{\theta L_{\blambda\bx}}{2\pi }\E\left[\|\blambda^{r+1} - \blambda^{r}\|^{2}\right] \\
& + 3L_{\bx\bx}\E\left[\|\bx^{r+1}-\bx^{r}\|^{2}\right] + 3L_{\bx\bx}{\mathcal{E}_{r}} - \frac{1}{2\tau}\E\left[\mathrm{D}(\bx^{r+1}, \bx^{r})\right] - \frac{1}{\sigma}\E\left[\mathrm{D}(\blambda^{r+1}, \blambda^{r})\right] + \frac{2\chi\,\tau}{J}\zeta^{2}\\
= & -Z_{r+1} + V_r + \left(\frac{\theta L_{\blambda\bx}}{2\pi} - \frac{1}{2\sigma}\right)\E\left[\|\blambda^{r+1}-\blambda^{r}\|^{2}\right] + \left(\frac{\pi \theta L_{\blambda\bx}}{2} + 3L_{\bx\bx} - \frac{1}{4\tau}\right)\E\left[\|\bx^{r+1}-\bx^{r}\|^{2}\right] \\
& + 3L_{\bx\bx}{\mathcal{E}_{r}} - \frac{1}{8\tau}\E\left[\|\bx^{r+1}-\bx^{r}\|^{2} \right]  + \frac{2\chi\,\tau}{J}\zeta^{2} \\
\geq & -Z_{r+1} + V_r + \underbrace{\left(\frac{\theta L_{\blambda\bx}}{2\pi} - \frac{1}{2\sigma}\right)\E\left[\|\blambda^{r+1}-\blambda^{r}\|^{2} \right]+ \left(\frac{\pi \theta L_{\blambda\bx}}{2} + 3L_{\bx\bx} - \frac{1}{4\tau}\right)\E\left[\|\bx^{r+1}-\bx^{r}\|^{2}\right]}_{\Delta_r} \\
&+ C\tau\zeta^{2},
\end{aligned}
\end{equation*}
\end{small}

where the last inequality is because Eq.~\eqref{eq:apd-scaffold-err-control-stoc}.  This completes our proof.
\end{proof}


\subsubsection{How to Set Parameters in Strongly-convex-strongly-concave (SC-SC) Setting?}\label{subsec:appendix-proof-sc-sc-parameters}
\begin{lemma}\label{lemma:appendix-scsc-stepsize-1}
For Algorithm~\ref{Algorithm:ScaffoldAPD}, if we set the parameters as 
\begin{equation}
        \mu_{\bx}\tau = O\left(\frac{1-\theta}{\theta}\right),\quad \mu_{\blambda}\sigma = O\left(\frac{1-\theta}{\theta}\right),\quad \frac{1}{1-\theta} = O\left(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\right),
\end{equation}
then we have
\begin{equation}\label{eq:lemma:appendix-scsc-stepsize-eq1}
    \frac{1}{2\tau} + \frac{\mu_{\bx}}{8} \geq \frac{1}{2\tau\theta}, \quad \frac{1}{2\sigma} + \frac{\mu_{\blambda}}{2} \geq \frac{1}{2\sigma\theta}, \quad \frac{1}{\tau} \geq 12L_{\bx\bx} + 2\pi \theta L_{\blambda\bx}, \quad \frac{1}{\sigma} \geq \frac{\theta L_{\blambda\bx}}{\pi}.
\end{equation}
\end{lemma}

\begin{proof}
The conditions in Eq.~\eqref{eq:lemma:appendix-scsc-stepsize-eq1} can be reformulated as follows,
\begin{equation}\label{eq:apd-step-size-condition-sc-sc}
\begin{aligned}
    \frac{1}{2\tau} + \frac{\mu_{\bx}}{8} \geq \frac{1}{2\tau\theta} \quad &\Leftrightarrow \quad \mu_{\bx}\tau \geq 4\frac{1-\theta}{\theta}, \\ 
    \frac{1}{2\sigma} + \frac{\mu_{\blambda}}{2} \geq \frac{1}{2\sigma\theta} \quad &\Leftrightarrow \quad \mu_{\blambda}\sigma \geq \frac{1-\theta}{\theta}, \\ 
    \frac{1}{\tau} \geq 12L_{\bx\bx} + 2\pi \theta L_{\blambda\bx} \quad &\Leftarrow  \quad \frac{1}{\tau} \geq 12L_{\bx\bx} + 2\pi L_{\blambda\bx}, \\ 
    \frac{1}{\sigma} \geq \frac{\theta L_{\blambda\bx}}{\pi} \quad &\Leftarrow  \quad \frac{c}{\sigma} \geq \frac{\theta L_{\blambda\bx}}{\pi}, 
\end{aligned}
\end{equation}
where $c \in (0, 1]$. 

Next we study how to set $\{\tau, \sigma, \theta\}$ such that Eq.~\eqref{eq:apd-step-size-condition-sc-sc} holds, we could set 
\begin{equation}\label{eq:apd-step-size-condition-sc-sc-2}
\begin{aligned}
    \tau &\geq \frac{4}{\mu_{\bx}}\frac{1-\theta}{\theta},\quad
    \sigma \geq \frac{1}{\mu_{\blambda}}\frac{1-\theta}{\theta}, \\
    \pi &= \frac{\theta \sigma L_{\blambda\bx}}{c}, \\
    \frac{\mu_{\bx} \theta}{4(1-\theta)} - 12 L_{\bx\bx} &\geq \frac{2\theta \sigma L_{\blambda\bx}^{2}}{c}\geq (1-\theta)\frac{2  L_{\blambda\bx}^{2}}{c\mu_{\blambda}},
\end{aligned}
\end{equation}
therefore, once $\theta$ satisfy the following condition
\begin{equation}\label{eq:apd-step-size-condition-sc-sc-3}
    \frac{\mu_{\bx} \theta}{4(1-\theta)} - 12 L_{\bx\bx} \geq (1-\theta)\frac{2  L_{\blambda\bx}^{2}}{c\mu_{\blambda}},
\end{equation}
and then we can set $\tau$ and $\sigma$ based on the value of $\theta$ according to Eq.~\eqref{eq:apd-step-size-condition-sc-sc-2}. 
Then if we let 
\begin{equation}
    \omega = \frac{1}{1-\theta},
\end{equation}
therefore, based on Eq.~\eqref{eq:apd-step-size-condition-sc-sc-3}, by setting $c=1$, we have
\begin{equation}\label{eq:appendix-omega-sc-sc}
\begin{aligned}
    &\quad \frac{\omega-1}{\omega} \frac{\omega \mu_{\bx}}{4} - 12 L_{\bx\bx} \geq \frac{1}{\omega} \frac{2  L_{\blambda\bx}^{2}}{\mu_{\blambda}},\\
\Leftrightarrow&\quad    
\mu_{\bx}\mu_{\blambda}\omega^{2} - (\mu_{\bx}\mu_{\blambda} + 48\mu_{\blambda}L_{\bx\bx})\omega  - 8 L_{\blambda\bx}^{2}\geq 0, \\
\Leftarrow&\quad \omega= C_{\omega}\frac{(\mu_{\bx}\mu_{\blambda} + 48 \mu_{\blambda}L_{\bx\bx}) + \sqrt{(\mu_{\bx}\mu_{\blambda} + 48 \mu_{\blambda}L_{\bx\bx})^{2} + 32 \mu_{\bx}\mu_{\blambda}L_{\blambda\bx}^{2}}}{2\mu_{\bx}\mu_{\blambda}},\\
\Leftrightarrow&\quad \omega= C_{\omega}\left(\frac{1}{2} + \frac{24 L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\left(\frac{1}{2} + \frac{24 L_{\bx\bx}}{\mu_{\bx}}\right)^{2} + \frac{16 L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\right), \\
\Leftrightarrow&\quad \omega= O\left(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\right),
\end{aligned}
\end{equation}
where $C_{\omega}\geq 1$ is a constant. This completes our proof.





\end{proof}



\subsubsection{Convergence Analysis}\label{subsec:appendix-proof-sc-sc-theorem-proof}

\begin{theorem}
Under the assumptions in Theorem~\ref{thm:main-sc-sc}, Algorithm~\ref{Algorithm:ScaffoldAPD} will converge to $\bx^{\star}$, and
\begin{equation}
\E\left[\|\bx^{r} - \bx^{\star}\|^{2}\right] \leq C_1\theta^{R}\left[\|\bx^{0} - \bx^{\star}\|^{2} + \| \blambda^{0} - \blambda^{\star}\|^{2}\right]+ C_2(1-\theta)\frac{\zeta^{2}}{\mu_{\bx}^{2}},
\end{equation}
where $C_1, C_2 \geq 0$ are non-negative constants.
\end{theorem}
\begin{proof}
The last two conditions in Eq.~\eqref{eq:lemma:appendix-scsc-stepsize-eq1} ensure 
\begin{equation*}
    \Delta_r = \E\left[\left(\frac{\theta L_{\blambda\bx}}{2\pi} - \frac{1}{2\sigma}\right)\|\blambda^{r+1}-\blambda^{r}\|^{2} + \left(\frac{\pi \theta L_{\blambda\bx}}{2} + 3L_{\bx\bx} - \frac{1}{4\tau}\right)\|\bx^{r+1}-\bx^{r}\|^{2}\right] \leq 0,
\end{equation*}
for $r=0,\dots, R$. The first two conditions in Eq.~\eqref{eq:lemma:appendix-scsc-stepsize-eq1} ensure 
\begin{equation*}
\begin{aligned}
Z_{r+1} \geq \frac{1}{\theta} V_{r+1}.
\end{aligned}
\end{equation*}    
Therefore, by applying Lemma~\ref{lemma:sc-sc-stochastic}, we have
\begin{equation}
    \E\left[{F(\bx^{r+1}, \blambda)} -  {F(\bx, \blambda^{r+1})}\right] + \frac{1}{\theta}V_{r+1} \leq  V_r + \Delta_r + C\tau\zeta^{2},
\end{equation}

Then we plug $\bx=\bx^{\star}, \blambda=\blambda^{\star}$ in Eq.~\eqref{eq:lemma-sc-sc-0}, and we have ${F(\bx^{r+1}, \blambda^{\star})} -  {F(\bx^{\star}, \blambda^{r+1})} \geq 0$, 
then we have
\begin{equation}
    V_{r+1} \leq \theta V_r + \theta\Delta_{r} + C\theta\tau\zeta^{2} ,
\end{equation}
therefore, we can derive that
\begin{equation}
    V_{R} \leq {\theta}^{R} V_0 + \theta\Delta_{R} + \frac{C\tau\theta\zeta^{2}}{1-{\theta}},
\end{equation}
meanwhile, we can set the parameters $\{\tau, \sigma, \theta\}$ (according to Eq.~\eqref{eq:apd-step-size-condition-sc-sc}) such that
\begin{equation}
    \E\left[\|\bx^{r} - \bx\|^{2}\right] \leq 4\tau\theta^{R}V_0 + \frac{C\tau^{2}\theta\zeta^{2}}{1-{\theta}}
\end{equation}
since $\tau = 2(1-\theta)/(\theta \mu_{\bx})$,
\begin{equation}
    \E\left[\|\bx^{r} - \bx\|^{2}\right] \leq 4\tau\theta^{R}V_0 + \frac{4C(1-\theta)\zeta^{2}}{\theta\mu_{\bx}^{2}}
\end{equation}




We need to run at least $N_\varepsilon$ rounds such that $4\tau\theta^{R}V_0 + \frac{4C(1-\theta)\zeta^{2}}{\theta\mu_{\bx}^{2}} \leq 2\varepsilon$. 

Suppose $N_\varepsilon$ satisfies
\begin{equation}
    N_\varepsilon = O\left(\ln\left(\frac{V_0}{\varepsilon}\right)\Big/\ln\left(\frac{1}{\theta}\right)\right),
\end{equation}
then we have $4\tau\theta^{R}V_0  \leq \varepsilon$. 
Because $\ln(1/\theta)$ is convex in $\theta\in\mathbb{R}_{+}$, then we have 
\begin{equation}
    \ln\left(\frac{1}{\theta}\right) \leq \frac{1}{1 - \theta}, \quad \theta \in (0, 1),
\end{equation}
therefore, to get an upper bound for $N_\varepsilon$, we only need to get the upper bound for $\frac{1}{1 - \theta}$. Then if we set $\omega = \frac{1}{1-\theta}$,
then based on Eq.~\eqref{eq:appendix-omega-sc-sc}, 
we have
\begin{equation}
\begin{aligned}
\omega= O\left(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}}\right),
\end{aligned}
\end{equation}
meanwhile, we need to ensure $\frac{4C(1-\theta)\zeta^{2}}{\theta\mu_{\bx}^{2}}$ is small, i.e.,
\begin{equation}
    \frac{4C(1-\theta)\zeta^{2}}{\theta\mu_{\bx}^{2}} = \varepsilon \quad \Leftrightarrow \quad \frac{1}{1-\theta} = \frac{4C\zeta^{2}}{\theta\mu_{\bx}^{2}\varepsilon}.
\end{equation}

therefore, in ensure $\E\left[\|\bx^{r} - \bx\|^{2}\right] \leq 2\varepsilon$, the number of communication rounds satisfies
\begin{equation}
    N_\varepsilon = \widetilde{O}\left(\frac{L_{\bx\bx}}{\mu_{\bx}} + \sqrt{\frac{L_{\blambda\bx}^{2}}{\mu_{\bx}\mu_{\blambda}}} + \frac{\zeta^{2}}{\mu_{\bx}^{2} \varepsilon}\right),
\end{equation}
which completes our proof.
\end{proof}



\newpage
\section{Additional Implementation Details and Experimental Results}\label{sec:appendix-exp}
In this section, we provide further details for algorithm implementations (Section~\ref{subsec:appendix-exp-details}) as well as additional experimental results -- trade-off between worst-20\% and average accuracy (Section~\ref{subsec:appendix-additional-exp}), convergence performance on synthetic datasets (Section~\ref{subsec:appendix-exp-synthetic}), and comparison with existing methods (Section~\ref{subsec:appendix-exp-comparison}).

\subsection{Additional Experimental Details}\label{subsec:appendix-exp-details}
In order to enhance the performance of baseline methods, we incorporate local steps into the AFL~\citep{mohri2019agnostic} method. We find that employing local steps yields significantly better performance compared to taking a single gradient step. 
To ensure a fair comparison, we employ identical feature extraction procedures across all methods. Following the setup outlined in \citet{yu2022tct}, we first compute the empirical neural tangent kernel (eNTK) representations of the input samples. Then, we randomly select 50,000 features from the eNTK representation through subsampling. 
For the (local) objective function, we utilize the mean squared error (MSE) loss, which has been used for classification tasks as described in \citet{yu2022tct}. To calculate the average accuracy, we begin by computing the test accuracy of each client. Then, we compute the average accuracy by averaging the results from all clients.


\subsection{Trade-off between Worst-20\% Accuracy and Average Accuracy}\label{subsec:appendix-additional-exp}
We present the trade-off between worst-20\% accuracy and average/best-20\% accuracy through a scatter plot, as illustrated in Figure~\ref{fig:exp-real-scatter}.
We consider the TinyImageNet dataset with the Non-i.i.d. degree parameter $\alpha=0.01$. 
Our proposed algorithm, as illustrated in Figure~\ref{fig:exp-real-scatter}, showcases a compelling trade-off between accuracy in the worst-20\% and the average/best-20\% scenarios.

% Figure environment removed



\newpage
\subsection{Additional Experiments on Synthetic Datasets}\label{subsec:appendix-exp-synthetic}
We vary the level of data heterogeneity by changing the parameter $\sigma$ from 0.01 to 0.1, where $\sigma$ is used for generating $\delta_{i}^{\bx}$ ($\delta_{i}^{\bx} \sim \mathcal{N}(\bm{0}, \sigma^{2}\bm{I}_{d\times d})$). 
Figure~\ref{fig:appendix-exp-synthetic-heterogeneity} illustrates the fast convergence of {\algname} to the optimal solution across various data heterogeneity settings. 
We also explore the effect of varying the number of local steps. Figure~\ref{fig:appendix-exp-synthetic-local-steps} demonstrates that increasing the number of local steps results in faster convergence towards the optimal solution.

% Figure environment removed



\subsection{Additional Experiments on Comparison with Existing Methods}\label{subsec:appendix-exp-comparison}
\paragraph{More clients.} 
On the CIFAR100 dataset, we conduct a comparison of different algorithms in the 50 clients setting, following the configuration outlined in Table~\ref{tab:main_table}. 
The summarized results are presented in Table~\ref{tab:appendix-table-more-clients}. 
Consistent with our previous findings, {\algname} exhibits superior robustness when compared to existing methods.




\begin{table*}[ht]
	\centering
	\caption{The average and worst-20\% top-1 accuracy of our algorithm ({\algname}) vs.\ state-of-the-art federated learning algorithms evaluated on CIFAR100 with 50 clients. 
 The highest top-1 accuracy in each setting is highlighted in \textbf{bold}.
	}
	\label{tab:appendix-table-more-clients}
	    \footnotesize
     \resizebox{0.5\textwidth}{!}{
		\begin{tabular}{cccccc}
			\toprule
			\multirow{1}{*}{\bf Datasets}  &   \multirow{1}{*}{\bf Methods}                 & \multicolumn{2}{c}{\bf Non-i.i.d. degree}   
			  \\
			  \midrule
			 &                   & \multicolumn{2}{c}{$\alpha=0.01$}    
			  \\
		\midrule
			\multirow{11}{*}{CIFAR-100}   &  &    
			\texttt{average} & \texttt{worst-20\%} 
			\\ 
			\cmidrule(lr){1-4} 
			   & {FedAvg} & 45.45  &  20.64  \\ 
			\tablewhitespace
			   & {SCAFFOLD} & 43.73 & 18.33  \\
			  \tablewhitespace
			   & {$q$-FFL} &  33.42 & 8.13    \\
			  \tablewhitespace
			   & {AFL} & 49.93  &  31.87     \\
			  \tablewhitespace
			   & {DRFA} & \textbf{51.07}  &  31.23  \\
			  \tablewhitespace
                 & \textit{SCAFF-PD}  & {50.43} & \textbf{33.03}     \\
                 \bottomrule
		\end{tabular}
    }
	\vspace{-.75em}
\end{table*}

\paragraph{Additional dataset.} 
We consider another dataset -- CIFAR10 dataset, the setup mostly follows the configuration outlined in Table~\ref{tab:main_table}. 
We  summarize the results  in Table~\ref{tab:appendix-table-more-dataset}. 
We observe that {\algname} outperforms existing methods.


\begin{table*}[ht]
	\centering
	\caption{The average and worst-20\% top-1 accuracy of our algorithm ({\algname}) vs.\ state-of-the-art federated learning algorithms evaluated on CIFAR10 with 20 clients and $\alpha=0.05$. 
 The highest top-1 accuracy in each setting is highlighted in \textbf{bold}.
	}
	\label{tab:appendix-table-more-dataset}
	    \footnotesize
     \resizebox{0.5\textwidth}{!}{
		\begin{tabular}{cccccc}
			\toprule
			\multirow{1}{*}{\bf Datasets}  &   \multirow{1}{*}{\bf Methods}                 & \multicolumn{2}{c}{\bf Non-i.i.d. degree}   
			  \\
			  \midrule
			 &                   & \multicolumn{2}{c}{$\alpha=0.05$}    
			  \\
		\midrule
			\multirow{11}{*}{CIFAR-100}   &  &    
			\texttt{average} & \texttt{worst-20\%} 
			\\ 
			\cmidrule(lr){1-4} 
			   & {FedAvg} & 77.42  &  60.63  \\ 
			\tablewhitespace
			   & {SCAFFOLD} & 77.75 & 62.89  \\
			  \tablewhitespace
			   & {$q$-FFL} &  68.52 & 41.26    \\
			  \tablewhitespace
			   & {AFL} & 78.89  &  65.07     \\
			  \tablewhitespace
			   & {DRFA} & {79.04}  &  65.02  \\
			  \tablewhitespace
                 & \textit{SCAFF-PD}  & \textbf{79.71} & \textbf{69.59}     \\
                 \bottomrule
		\end{tabular}
    }
	\vspace{-.75em}
\end{table*}


\paragraph{Additional baselines.} 
In addition to the baseline methods listed in Table~\ref{tab:main_table}, we include $\Delta$-FL~\citep{pillutla2021federated} and FedProx~\citep{li2020federated} in our evaluation. 
We adopt a similar setup as presented in Table~\ref{tab:main_table} to assess the performance of these two methods. 
The summarized results are presented in Table~\ref{tab:appendix-table-more-baseline}, indicating that our proposed algorithm surpasses both $\Delta$-FL and FedProx in terms of worst-20\% accuracy and average accuracy.

\begin{table*}[ht]
	\centering
	\caption{The average and worst-20\% top-1 accuracy of our algorithm ({\algname}) vs.\ state-of-the-art federated learning algorithms evaluated on CIFAR100 and Tiny-ImageNet. 
 The highest top-1 accuracy in each setting is highlighted in \textbf{bold}.
	}
	\label{tab:appendix-table-more-baseline}
	    \footnotesize
     \resizebox{0.95\textwidth}{!}{
		\begin{tabular}{cccc|cc|ccc}
			\toprule
			\multirow{1}{*}{\bf Datasets}  &   \multirow{1}{*}{\bf Methods}                 & \multicolumn{6}{c}{\bf Non-i.i.d. degree}   
			  \\
			  \midrule
			 &                   & \multicolumn{2}{c}{$\alpha=0.01$}    & \multicolumn{2}{c}{$\alpha=0.05$}    & \multicolumn{2}{c}{$\alpha=0.1$}   
			  \\
		\midrule
			\multirow{6.2}{*}{CIFAR-100}   &  &    
			\texttt{average} & \texttt{worst-20\%} & \texttt{average} & \texttt{worst-20\%} &\texttt{average} & \texttt{worst-20\%} & 
			\\ 
			\cmidrule(lr){1-9} 
			   & {FedProx} & 38.76 & 15.58 & 35.91 & 24.57 & 36.49 & 26.45    \\ 
			\tablewhitespace
			   & {$\Delta$-FL} & 30.09 & 7.26 & 33.18 & 15.82 & 31.69 & 16.63     \\ 
			  \tablewhitespace
                 & \textit{SCAFF-PD}  & \textbf{49.03} & \textbf{29.30} & \textbf{42.06} & \textbf{28.37} & \textbf{43.69} & \textbf{32.77}     \\
                \midrule
   \multirow{6.2}{*}{TinyImageNet}   &  &    
			\texttt{average} & \texttt{worst-20\%} & \texttt{average} & \texttt{worst-20\%} &\texttt{average} & \texttt{worst-20\%} & 
			\\ 
			\cmidrule(lr){1-9} 
			   & {FedProx} & 33.65 & 18.09 & 31.52 & 23.62 & 34.98 & 27.59     \\ 
			\tablewhitespace
			   & {$\Delta$-FL} & 29.06 & 11.94 & 36.77 & 22.24 & 36.47 & 20.13     \\ 
			  \tablewhitespace
                 & \textit{SCAFF-PD} & \textbf{41.26} & \textbf{25.32}    & \textbf{39.32} & \textbf{30.27}  & \textbf{41.23} &  \textbf{29.78}  \\
			\bottomrule
		\end{tabular}
    }
	\vspace{-.75em}
\end{table*}




    
