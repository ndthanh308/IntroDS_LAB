\subsection{Service layer}

The service layer enables the infrastructure for ML inference queries and is responsible for committing service information to the proof layer. This layer is equivalent to a Web2 server-client architecture with some modifications to support the proof framework. An instantiation of this layer creates a connection between a client and a server to exchange data and makes the server’s compute available through agreed-upon Inference APIs. The service layer works in conjunction with other layers in the infrastructure as depicted in Figure \ref{fig:ServiceLayer} 
and described below:\\

% Figure environment removed

\noindent {\bf Server Assignment:} The client requests the control layer to assign a server for an AI model, and the control layer notifies the client of the server’s ID and address. It also notifies the server of an incoming connection from the client. \\

\noindent {\bf Service exchange:}  The client establishes a connection with the server using the address provided by the control layer. Both server and client verify through the transaction layer if an SLA path exists between them through the common aggregator; if such a path exists, both parties implicitly agree on the trade. The client sends inference requests using the server’s API endpoint; the client signs the request for use in dispute resolution if the need arises. The server processes the requests and sends the output data back to the client as the response; the server might submit a commitment to the delivered response on a DA layer at a later stage if the need arises for dispute resolution. Per service of a single unit of inference - a single API request, the server anticipates a micropayment as dictated by its SLA. A request is made to the transaction layer, which then sends payments from the client to the aggregator and from the aggregator to the server. The server proceeds to serve the subsequent request from the client only if the payment for the previous request is processed.\\

\noindent {\bf Service dispute witnesses:} The data exchanged in the service layer is used as a witness in case a payment dispute arises, such as a client not paying for the AI inference service delivered. The signed inference requests, output data committed to a DA layer, and the previous exchanged micropayment will be used for dispute resolution, as discussed in detail in the following sections on the Transaction and Proof layers. 
