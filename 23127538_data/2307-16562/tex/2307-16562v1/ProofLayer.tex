\subsection{Proof Layer}

The proof layer, operating outside the data and transaction paths, provides a way to resolve various disputes in \sakshi, utilizing blockchains as a immutable and trusted medium to read and write service states. A variety of disputes can arise in the AI service and ``proof" systems to provide cryptographic resolution mechanisms address the corresponding issues. In this paper, we focus on two  categories of proofs, each responding to different types of disputes. 
\begin{itemize}
    \item Proof of Inference, a proof of correct computation on a prescribed (and open) AI model, mediates disputes of correct inference;
    \item Proof of Model-ownership, a proof of how closely two AI models are related to each other and whether one AI model is a clone or a fine-tuned version of the other, mediates potential disputes related to intellectual property held by the owner of an AI model. 
\end{itemize}

Figure \ref{fig:proofLayer} depicts the interaction of the dispute resolution contract in the proof layer with the rest of the platform layers. A detailed description of the individual proof follows. 

% Figure environment removed

\subsubsection{Proof of Inference}
A crucial aspect of decentralized inference platforms is the presence of incentives that encourage honest participation in the protocol while discouraging malicious actors. An essential component of this incentive design is addressing the problem of provably verifying computations executed by untrusted servers. Various design choices are available to enable such proof of inference, with several emerging research directions.

One such line of research involves the application of zero-knowledge proofs (ZKP) to verify AI model execution \cite{kang2022scaling}. However, this approach is extremely computationally intensive, necessitating concessions such as quantization, which leads to lower accuracy. Furthermore, generating ZKPs for modern, large-scale generative AI models is currently impractical. 

An alternative strategy is to adopt an optimistic approach. In this scheme, the server commits the hash of the generated output, and the system assumes the off-chain inference to be accurate. If a participant (``challenger") doubts the inference's correctness, they can contest its validity by submitting a fraud proof. This proof can be generated using a verification oracle that can re-run the model and determine the accuracy of the server's or challenger's claim. However, since these oracle nodes may have limited computational capabilities, recomputing the entire neural network forward pass is prohibitively expensive and inefficient.

To address this issue, we propose a method inspired by the bisection scheme employed in the optimistic rollup Arbitrum~\cite{kalodner2018arbitrum}. A key observation is that AI models can be viewed as a sequence of functions, such as layers in a neural network.
$$  f(x) = y \quad \rightarrow \quad f_n(f_{n-1}(f_{n-2}(...f_2(f_1(x))...))) = y$$
When there is a discrepancy between the outputs of a server and a challenger, we can employ an interactive bisection scheme to identify a single function—the first layer in the AI model where the outputs of the two parties differ. By implementing this system, oracle nodes only need to compute and verify a single layer of the network, significantly reducing costs and making the verification of extremely large models feasible. Indeed, deterministic AI inference is a prerequisite for such schemes, which is attainable by fixing the random state.

% Figure environment removed

We illustrate our ModelBisection algorithm in Figure \ref{fig:modelbisection}, that identifies the earliest layer of the AI model where the inputs align for both parties, but the resulting outputs diverge, while minimizing the number of interactive steps involved. In case of a sequential model (left), one can use a form of binary search - if the output of a queried layer (typically the midpoint) is inconsistent between the parties, we recursively bisect the first half of the node sequence. Otherwise, we eliminate the first half, and recursively bisect the second half of the sequence. Each bisection step eliminates half of the remaining candidates for the faulty layer. After a logarithmic number of iterations, we locate a layer whose input is consistent, yet the parties produce differing outputs. 

However, the computations within an AI model are not simply sequential but rather form a Directed Acyclic Graph (DAG) structure. Consequently, the bisection mechanism used for sequential networks cannot be directly applied to AI models. We demonstrate our approach, \emph{ModelBisection}, on an Inception block of GoogLeNet \cite{szegedy2015going} as depicted in Figure \ref{fig:modelbisection} (right). Suppose we select the node $n_1 = L_{2.2}$ in the DAG for output verification. Both parties compute and share the intermediate output of layer $L_{2.2}$. If the outputs are equal, we prune all ancestor nodes of this node in the DAG from consideration (as their outputs would have to be consistent). If, however, the outputs differ, we eliminate all non-ancestor nodes of this node in the DAG (since one of outputs among ancestors must be inconsistent). We keep track of the identified consistent and inconsistent nodes, and continue this process until we reach a single layer where the inputs are consistent between the parties, but the outputs differ. We employ a greedy strategy to select the node in the digraph such that it is split in the most balanced way. We choose the node which maximizes $\min\{|x|, n - |x|\}$, where $|x|$ is the number of ancestors of node $x$, and n is the total number of nodes in the current digraph. This score can be interpreted as the least number of nodes that would be eliminated as potential candidates for the first point of divergence, when $x$ is queried, thus minimizing the number of ModelBisection rounds. It's noteworthy that even in large foundation models, the ModelBisection approach can pinpoint a single layer of divergence in a very small number of iterations. For example, in the case of the 13 billion parameter LLaMA model \cite{touvron2023llama},  fewer than ten iterations suffice. Finally we observe that the bisection subroutine bears similarity to the one utilized by GitHub in \emph{git bisect}, which aids in identifying the first faulty entry in the DAG of commits and merges.

\subsubsection{Proof of Model ownership}

A decentralized AI marketplace comprises three main entities - model owners who collect datasets and train or finetune AI models, compute-rich servers, and end-users. As opposed to current open-source model hosting solutions, decentralized marketplaces can allow incentivizing model creators by rewarding them a percentage of the inference fee when their models are utilized.
However, such an incentive design is susceptible to model copying attacks, where a malicious actor can copy, slightly modify, and profit from the hosted models at the cost of the model creators. Therefore, a robust mechanism for model ownership resolution becomes a crucial prerequisite for decentralized AI marketplaces.

One promising solution for a proof of model ownership is by embedding a watermark in the neural networks during the training phase. To be effective, a DNN watermarking scheme must fulfill several criteria: it should be functionality-preserving, meaning the watermark embedding must not impact model performance. The watermark must be robust, and be extractable from any transformed model (e.g., through weight scaling or finetuning). Additionally, a watermarked model should remain indistinguishable from a non-watermarked model to potential adversaries. Moreover, a watermark must be resistant to ambiguity attacks - false claims of existence of a different watermark.

Various watermarking schemes have been proposed in research literature. Parameter encoding methods \cite{uchida2017embedding, darvish2019deepsigns, fan2019rethinking}, integrate a watermark directly into the model's parameters. For classification models, an alternate method involves backdooring, which involves assigning incorrect labels to examples in a trigger set, and this can be used as a watermark \cite{adi2018turning, szyller2021dawn}. Additionally, task-specific and model-specific watermarking methods have been proposed \cite{fernandez2023stable, zhao2023recipe, christ2023undetectable, kirchenbauer2023watermark}. Nonetheless, the robustness of existing methods against model copying has been questioned by recent attacks \cite{lukas2022sok, yan2023rethinking, liu2023false}, highlighting an unresolved research challenge.

Notably, in most watermark extraction algorithms, information about the watermark location or the trigger examples are revealed during the verification process. This knowledge facilitates easier watermark removal and ambiguity attacks. Therefore, in our system a trusted judge is required to resolve model ownership disputes. Model creators must embed watermarks in their models, and commit a commitment of the watermark on the blockchain. The judge must be able to verify the existence of watermarks using the extraction algorithm, which may be task and model-specific. Such a proof of model ownership can ensure the non-feasibility of profiting from stolen models within the decentralized marketplace. However, it does not prevent an adversary from copying a model and using it outside this system (eg - via a black-box api). Such acts can be deterred by licensing the model’s use only in this marketplace, and resorting to legal means if necessary.

\subsection{Summary} Proofs of inference and ownership are two examples of a broader family of protocols providing Byzantine resistance in \sakshi. Even here, we have worked more to describe the problems rather than the solutions -- a call to arms from the scientific community. As the platform evolves and participation rises, the attack space could also expand opening the door for new and different kinds of proof systems (e.g., proof of custody; proof of infrastructure hosting the AI models). 