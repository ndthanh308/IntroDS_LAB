\section{Evaluation\label{evaluation}}

In this section, we evaluate \sys.
We first describe our experiment methodology.
Next, we present experimental results with various workloads and system conditions.

\subsection{Methodology}
\subsubsection{Testbed setup}
To evaluate \sys, we use a cluster consisting of 8 commodity servers, which are connected by an APS Networks BF6064X-T switch.
The switch data plane is based on a 6.5 Tbps Intel Tofino switch ASIC~\cite{tofinonodate}.
The servers are equipped with a 10-core CPU (Intel i5-12600K @ 3.7 Ghz, 12 hyperthreads and 4 non-hyperthreads), 32 GB of DDR5 memory, and a single-port 100GbE RDMA-capable NIC.
The servers run Ubuntu 20.04 LTS with Linux kernel 5.15.0.
Unless specified, 2 servers act as clients to generate requests and the remaining 6 servers are used as worker servers.
The performance bottleneck is at worker servers.


\subsubsection{Workloads}
We use a variety of synthetic and real-world application workloads similar to recent works~\cite{racksched,shinjuku,zygos}.
The workloads use one-packet requests and responses with UDP like RackSched~\cite{racksched}.

With a synthetic workload, a worker server processes a dummy RPC for a duration that we specify. 
The synthetic workload allows us to evaluate the performance of \sys with various applications by emulating any target distribution of services and variability.
Unless specified, we consider an exponential distribution with mean = 25 $\mu s$ by default, which can represent common short-lasting RPCs.
We also consider a bimodal distribution where 90\% are 25 $\mu s$ and 10\% are 250 $\mu s$, which represents a mix of simple and complex RPCs.
To inspect the impact of RPC duration, we use 50 $\mu s$ and 500 $\mu s$ as well.
To emulate the service-time variability, we follow the observations from \sota~\cite{laedge}.
We consider $p=0.01$ and $p=0.001$ to represent a high variability and a low variability, where $p$ denotes the jitter probability to experience excessive long latency.
We basically consider that workloads have high service-time variability, and the runtime of an RPC experiencing the unexpected jitter can take 15 times more than the normal case.
For the real-world application workload, we use Redis~\cite{redis}, a widely deployed in-memory key-value store in many production systems.

\subsubsection{Compared Schemes}
We compare our work against the baseline, \cc, \sota~\cite{laedge}.
The baseline sends requests to workers randomly without cloning.
\cc is the client-based cloning mechanism that always sends duplicate requests to two random worker servers.
\sota performs dynamic cloning using the coordinator.
In most experiments, we compare \sys to the baseline and \cc because \sota has significantly lower throughput than \sys.


% Figure environment removed


\subsection{Synthetic Workloads Results}
%In this subsection, we present our main experimental results, which include throughput vs. latency with different workloads and service-time variability, scalability, performance with Redis, and comparison with other solutions.
%\subsubsection{Throughput vs. Latency}
We plot the performance of three schemes, the baseline, \cc, and \sys in Figure~\ref{fig:main} for different workloads.
Note that Y-axis is in log scale for better visibility.
\cc shows limited throughput in all figures due to its static request cloning, which overloads worker servers beyond a certain point.
Thanks to the dynamic cloning and response filtering, \sys achieves low tail latency while maintaining similar throughput to the baseline.
In Figure~\ref{fig:main} (a) and (b), we can find that \sys achieves better latency than the baseline across almost all loads.
The average improvement is 1.48$\times$ and 1.27$\times$ for Exp(25) and Bimodal(90\%-25,10\%-250), respectively.
Since the work servers become busier as throughput grows, \sys clones requests less as well. 
Therefore, the degree of improvement decreases as the system load grows.
Meanwhile, at low loads, \sys experiences worse latency than \cc.
This occurs because \sys does not replicate requests when the tracked queue length is not zero.
Note that the queue can build up occasionally, even at low loads.
In Figure~\ref{fig:main} (c) and (d), \sys provides low tail latency at low loads, similar to Figure~\ref{fig:main} (a) and (b).
However, the performance improvement at high loads is negligible due to the longer processing time of RPCs that keeps the queue length non-zero at high loads.



% Figure environment removed

\subsection{Scalability}
\subsubsection{Comparison with the existing solutions}
In this experiment, we compare \sys with \cc and \sota to show that \sys has better throughput and scalability.
We use five worker servers because one server should be dedicated to the \sota coordinator.
The results, shown in Figure~\ref{fig:laedge}, indicate that \sys provides high throughput, while \sota and \cc exhibit low throughput.
\cc does not incur latency overhead to clone requests but its static cloning limits system throughput.
\sota performs even worse than \cc since it relies on a CPU-based coordinator to clone requests.
The coordinator server easily becomes a performance bottleneck, making it difficult to support high request rates with multiple worker servers.
Even with a highly optimized coordinator, \sota would be still behind \sys, as switches can process billions of packets per second, while optimized servers can handle only a few million packets per second.
This result demonstrates that performing request cloning in the switch is a desirable approach to achieve high performance.

% Figure environment removed

\subsubsection{Impact of the number of servers}
We now evaluate the scalability of \sys by varying the number of worker servers.
As \sys performs request cloning in switches, it can scale out to multiple servers while maintaining low tail latency. Figure~\ref{fig:scalability} shows the results for 2, 4, and 6 worker servers.
We did not conduct an experiment with one server as \sys requires a minimum of two servers for redundancy.
As the number of worker servers increases, both \sys and the baseline show improved throughput.
\sys maintains lower tail latency than the baseline regardless of the number of servers.
One observation worth mentioning is that when the number of worker servers is two or four, \sys shows worse latency at very high loads.
This can be attributed to two reasons.
First, \sys sends cloned requests only when the server is idle, but the server may be busy in fact.
We drop cloned requests if the actual state is busy, but the processing cost can be harmful if the number of redundant requests is large at very high loads.
Second, with a small number of servers, there may not be enough idle servers available.
Therefore, many cloned requests are forwarded to actually overloaded servers for a short time with herding effects, resulting in high tail latency at very high loads.
However, when the number of servers is large, the probability of performance degradation decreases as \sys has a larger pool of servers to choose.



% Figure environment removed


\subsection{Performance with RackSched}

We now show how \sys can make synergy with Racksched~\cite{racksched}.
\sys contributes to reducing latency and RackSched is effective to improve throughput.
Figure~\ref{fig:homo} is experimental results for Exp(25) and Bimodal(90\%-25,10\%-250) workloads with a different number of workers.
The homogeneous workloads assume that each worker server has an equal number of worker threads (15 worker threads and 1 dispatcher thread).
In the heterogeneous workloads, three of the worker servers have 15 worker threads, while the other three have 8 worker threads.
We see that \sys with RackSched achieves the best performance, thanks to RackSched's ability to handle possible load imbalances between worker servers.
\sys with RackSched performs better with heterogeneous workloads than with homogeneous workloads because the latter workloads result in more imbalance loads.
Meanwhile, in homogeneous workloads, \sys with RackSched is worse than \sys at very high loads, and we suspect that this is because the cases when the tracked state and the actual state are unmatched increase as RackSched makes the request queue empty more often.





% Figure environment removed


\subsection{Applications: Redis and Memcached}
We now show that \sys is effective with real-world applications using Redis~\cite{redis} and Memcached~\cite{memcached}, which are popular in-memory key-value stores, commonly used in production services.
We conduct experiments using 1 million objects with 16-byte keys and 64-byte values~\cite{mica} by considering replicated key-value storage.
Unlike previous in-network solutions for key-value stores~\cite{jin17,netlr,jin18,harmonia}, \sys does not impose any limitations on the key or value sizes, as it does not store keys or values in the switch data plane.
In this experiment, clients generate read requests, and worker servers return values with a skewed key access pattern with Zipf-0.99.
Note that \sys does not clone write requests because the write coordination should be handled by replication protocols.
We use 8 worker threads in each worker server.
We vary the portion of GET and SCAN requests to 99\%-GET,1\%-SCAN and 90\%-GET,10\%-SCAN where GET reads a single object and SCAN reads 100 objects.

Figure~\ref{fig:redis} and Figure~\ref{fig:memcached} show results, which have similar trends.
We can see that, like the result with the synthetic workload, \sys improves tail latency by masking service-time variability.
The performance gap is the biggest at low loads, and the gap becomes small as throughput grows.
\cc shows similar tail latency to \sys, but its throughput is limited to half of \sys as expected.
In the Redis experiment, \sys is better than the baseline by up to 22.59$\times$ and 1.77$\times$ for 99\%-GET,1\%-SCAN and 90\%-GET,10\%-SCAN, respectively.
In Memcached, the largest improvement degree is 22.00$\times$ and the smallest one is 1.06$\times$ for 99\%-GET,1\%-SCAN.
For 90\%-GET,10\%-SCAN in Memcached, \sys achieves better tail latency than the baseline by 1.24$\times$ on average.



% Figure environment removed

% Figure environment removed

\subsection{Deep Dive}
\subsubsection{Confidence of State Signals}
\sys considers the server as idle if the queue length of the server is zero.
Therefore, we investigate the portion of empty queues by varying loads.
We make a server record its current queue length when sending a response.
In Figure~\ref{fig:qlen} (a), we can see that the portion of empty queues decreases as the load grows, as expected.
We see two important observations as follows.
First, even at low loads, the queue may not be empty.
This explains why \sys shows higher latency than \cc at low loads in Figure~\ref{fig:main}.
Second, likewise, queues do not always build up even under very high loads.
This is the reason why cloning happens at not only low loads but also high loads.
To check the efficiency of cloning at high loads, we run experiments with the baseline and \sys 10 times at 0.9 of load and get the average tail latency and their standard deviations.
Figure~\ref{fig:qlen} (b) shows the results.
As expected, we see that \sys may cause worse latency than the baseline occasionally.
However, by considering the average and the standard deviation, we can conclude that \sys generally provides better latency than the baseline even at very high loads.


\subsubsection{Impact of Service-Time Variability}
Figure~\ref{fig:main-001} shows the experimental results for synthetic workloads with a low variability of $p=0.001$.
The Y-axis of Figure~\ref{fig:main-001} (a) and (b) is in the log-scale.
We can see that \sys can decrease tail latency even if the service-time variability is low.
The trend of experimental results is similar to Figure~\ref{fig:main}.
One difference is that performance improvement slightly decreases.
However, it is not surprising since the benefit of request cloning comes from masking service-time variability.


% Figure environment removed


% Figure environment removed


\subsubsection{Impact of Redundant Response Filtering}
We now inspect the impact of redundant response filtering.
To do this, we turn off the response filtering function and compare its performance against the baseline and \sys.
Figure~\ref{fig:overhead} plots the result.
We have the following observations.
First, at low loads, redundant responses barely harm performance since the client has enough capability to handle redundancy.
However, as the system load grows, the latency gets worse.
The performance is even worse than the baseline at high loads if \sys does not use response filtering.
This means that filtering redundant responses plays an important role to optimize the performance of \sys.

\subsubsection{Performance under Switch Failures}
In this experiment, we evaluate the resilience of \sys to switch failures.
Figure~\ref{fig:failure} shows the throughput for 25 seconds.
The switch was stopped at 5 seconds and manually reactivated at 7 seconds.
The throughput recovers after approximately 10 seconds.
The downtime is not a result of \sys, but rather depends on the switch architecture.
Thus, we can say that \sys is robust to switch failures.
Note that \sys does not incur permanent misbehavior since the switch stores only soft states.




