% Figure environment removed
\section{Implementation \label{implementation}}
\subsection{Switch Data Plane}
Our switch data plane is written in P4$_{16}$~\cite{bosshart14} and is compiled with Intel P4 Studio SDE 9.7.0 for Intel Tofino~\cite{tofinonodate}.
We implement our data plane modules in the ingress pipeline because the switch should finish cloning decisions before packet forwarding.
\sys consumes 7 match-action stages when using two filter tables.
We use 18.04\% SRAM, 12.28\% Match Input Crossbar, 26.79\% Hash Unit, and 21.43\% ALUs of the switch ASIC.
Most memory space is used to keep track of request IDs in the filter tables where each table has $2^{17}$ hash slots.
We can calculate how much throughput can be supported by the tables using a back-of-the-envelope calculation~\cite{racksched}.
When the average request latency is 50$\mu s$, each slot can handle 20 KRPS. 
As we have a total of $2^{18}$ slots, the current \sys prototype can support roughly 5.24 BRPS throughput.
Since we use a 32-bit slot, our hash tables use roughly 1.05 MB, which is 4.77\% of the switch memory.



\subsection{Client-Server Application}
We implement an open-loop multi-threaded application in C like prior work~\cite{racksched,breakwater,shinjuku}.
We use the NVIDIA Messaging Accelerator library (VMA)~\cite{vmanodate} for high-performance packet processing.
The VMA allows applications to process packets in userspace with RDMA-like kernel-bypass networking, minimizing the packet processing delay in hosts.
The client measures the throughput and latency by generating requests at a given target sending rate.
It consists of two threads, one is the sender thread and the other is the receiver thread.
The inter-arrival time between two consecutive requests is exponentially distributed.
The server consists of a single dispatcher thread and multiple worker threads.
The dispatcher enqueues received requests into a global request queue with FCFS policy.
Worker threads dequeue requests and process them in parallel.
