\documentclass{article}
\usepackage{changes}

\title{Decoding Algorithms for the Surface Code: changes and letter to the reviewers}
\author{Antonio deMarti iOlius \and Patricio Fuentes \and Rom\'an Or\'us \and Pedro M. Crespo \and Josu Etxezarreta Martinez}
\date{\today}


\begin{document}

\maketitle

\tableofcontents
\newpage
\section{Introduction}
% Introduction to the change log document.
Dear Editor and Reviewers,
\\
\\
Thank you very much for your comments regarding our manuscript entitled \emph{Decoding Algorithms for surface codes}, submitted for publication to \textit{Quantum}. We strongly believe that the referee comments have allowed us to improve the manuscript considerably. We have addressed all the comments of the reviewers and made substantial changes to the article. In what follows, we include our responses to the comments made by the referees and we detail the changes that have been made to the manuscript. Added text appears highlighted in red while removed text appears with a strikethrough in the revised version of the paper. We are aware this format may be hard to read or follow, therefore, by the time we resubmit the review, it will be also updated to the arXiv in a clean version. We have organized this response letter in three sections. The first one is addressed to the comments of Reviewer 1, the second one refers to those of Reviewer 2 and the last one are additional changes made by the authors, which believe will improve the presentation of the article. For the reviewers sections we will quote the reviewers' comments and answer each of them with the changes we have conducted to the article.




\newpage
\section{Reviewer 1}

We thank the reviewer for his interest in our work. While their comments towards the work are overall positive, they ask for two different types of changes, the first one refers to correcting typos:

\begin{enumerate}
    \item \textbf{page 2 right column, an extra space: Many of the errors arise due to similar limits in the equipment being used for preparing( ,) controlling or measuring such  mechanical systems, and it…}\\
    
    The typo has been corrected.\\
    
    \item \textbf{page 6 right column, an extra “this”: In this (this) tutorial we will consider the rotated planar code in a square}\\
    
    The typo has been corrected.\\
    
    \item \textbf{page 12 right column, an extra “that”: a minimum weight perfect matching can be found is that (that) all correctable error chains within the code have two non-trivial endpoint c}\\
    
    The typo has been corrected.\\
    
    \item \textbf{page 14 left column, an extra “an”: decoding rule has been observed to be (an) efficient for low physical error probabilities}\\ 

    The typo has been corrected by rewriting the sentence as: \textit{Applying the suboptimal decoding rule has been observed to be an efficient approach for low physical error probabilities}.\\

    \item \textbf{page 21 left column, an extra “that”: This cannot be better exemplified than by the fact that (that) the}\\

    The typo has been corrected.\\

    \item \textbf{page 25 right column, a typo: discuss the fact that when a (measure met) is noisy, the input in- formation of the Tanner graph regarding the inf}\\

    The typo has been corrected, we meant to say a ``measurement".
\end{enumerate}

The second change requested by the reviewer relates with the figures of the article:
\begin{enumerate}
    \item \textbf{Besides the figures uses color that are not very distinguishable from each other, e.g., figure 4 orange and red. It will be better if some more distinguishable colors are used.}
    
    We have made extensive changes to the figures throughout the article for more clear presentation. Also as requested by Referee 2.
\end{enumerate}

    



\newpage

\section{Reviewer 2}

We thank the referee for the detailed analysis of our manuscript. They recommend a thorough rewriting of several sections of the article so as to make it more understandable and suitable for the readers. We have made a substantial effort in revising the article based on their comments. In what follows, we answer all the points raised by the referee.

The comments of Reviewer 2 can be split in 4 sections:

    
The \textbf{first section} comment focuses on the fact that the readability and utility of the article can be improved. "\textit{I was specifically interested to understand the belief-propagation decoder, but this paper did not help me understand it, giving only a basic algorithm with unclear notation, no useful figures, and no insights as to why it works.}".

Based on this feedback, we have completely rewritten and added further explanation for Belief Propagation and every other decoding methods. We are confident that it is now up to the standard expected by the reviewer. 

The remaining comments with regard to that section are:

\begin{itemize}

    \item \textbf{Title: Already the title says “surface codes”,  the paper alternates between “code” singular and “codes” plural}

    This comment has been addressed by establishing a classification of the definitions which were previously named as \textit{surface codes}:

    \begin{itemize}
        \item \textbf{Surface codes}: family of quantum error correction codes in which data and check qubits are displayed in a lattice. Within this family we can include more specific codes like the toric code, the planar code and the rotated planar code.
        \item \textbf{A surface code}: an arbitrary code from the surface codes class.
        \item \textbf{The rotated planar code or rotated code}: a type of code within the surface codes family.
    \end{itemize}

    Before, we used to name the three terms interchangeably, their definitions depending in the context. We understand this may have made it confusing to the reader and have changed it.

    \item \textbf{Abstract: “intractable” - not clear what this means here. Also used later on p19, also not clear there.}

    By intractable we meant problems that cannot be solved in a reasonable amount of time by classical computers. We have changed the wording in the modified version with the aim of clarifying what we mean. 
    
    In the abstract we have written: \textit{Quantum technologies have the potential to solve certain computationally hard problems with polynomial or super-polynomial speedups when compared to classical methods}.

    In the introduction: `computational processes that are deemed as not efficiently solvable by means of classical machines", and to clarify the meaning of efficient, we added the following footnote: \textit{Efficient in this context means that the problem can be solved in a reasonable amount of time.}

    \item \textbf{p4: The paper claims that computations are “noiseless” or protected qubits operate “as if they were noiseless” – I think this oversimplification has no place in a paper on error correction.}

    We agree with the reviewer that this wording is an oversimplification. Our intention there was to use a rahter simplified and top view definition of the intention of QEC, that's why we used the quotation marks for noiseless. We changed the wording to something more technical as: \textit{the main objective of QEC is to obtain qubits and computations that experience error rates that are arbitrarily low. For example, it is considered that a logical error probability in the order of $10^{-14}-10^{-15}$ is required to run Shor's algorithm fault-tolerantly [25]}.  

    \item \textbf{p5: “complex conjugate” of a guess of the error}

    We have changed this sentence to only saying that the recovered error will be operated to the system. Since we are working with Pauli operators, which are self-inverse, applying the same operator twice will produce a trivial interaction on the qubit. 

    \item \textbf{p6: No numbered formulas for the stabilizers of the surface code – This is certainly needed for an introduction-level paper, unlike numbered formulas (1), (2), and (3).}

    We have included a formula covering all the stabilizer generators for the rotated planar code of distance 3 in section 3.1. Moreover, Figure 2 has been changed so as to also be a distance 3 rotated planar code to refer to and better understand the stabilizer generators.

    \item \textbf{p7: initialized with all qubits in |0>, “corresponds to the logical |0> state” – certainly not, only after the measurement of the stabilizers. Maybe a small difference, but again, in a scientific paper, necessary}

    We thank the reviewer for bringing this up. We have addressed this issue in the manuscript and now explicitly mention the fact that the stabilizers need to be measured to project the physical qubits into the codespace. This was not clear in our original description.

    \item \textbf{p8: “momentous result..: the performance of a QECC improves as its distance increases”. This is NOT true in general, see the Bacon-Shor code for a counterexample.}

    We thank the reviewer for pointing this issue out. Our explanation here was somewhat vague. We were not claiming that every code should have a threshold. Our point was that if a code does have a threshold, then the scaling should result in reducing the logical error rate. We have rewritten this part, first we explain the general threshold theorem that states that by means of QEC the logical error rate can be suppressed to arbitrarily low levels for some noise model (we do not fix code here). We then specify that the code threshold refers to the maximum physical error rate for which increasing the distance of a code results in suppression of the logical error rate. We refer to the Bacon-Shor code as a code with no threshold (equivalently a threshold equal to zero) for reference. Finally we point out that surface codes do present code thresholds implying the desired distance vs logical error rate scaling. We hope that this makes this issue clearer. 
    
    The modified paragraph is: \textit{Although it may seem intuitive that a larger surface code would perform better, this is not always the case. The momentous result in quantum computing named the threshold theorem states that, by means of quantum error correction, the error logical error rate experienced by a quantum computer can be suppressed to an arbitrarily low level provided that the physical error rate of the data qubits is below a certain value [24,31,68,69]. This closely relates with the concept of code threshold ($p_{th}$), which refers to the maximum physical error rate for which increasing the distance of a code in consideration results in a better performance considering a certain error model. Note that not all QECCs present a code threshold, e.g. the Bacon-Shor code [70], implying that increasing distance does not always result in lowering the logical error rate. Note that not having threshold is equivalent to saying that the threshold is zero. The considered rotated planar surface code presents non-trivial code thresholds for the noise models considered in theoretical QEC. Thus, as long as the physical error rate is below $p_{th}$, increasing the size of such code will lead to a better performance.} 
    
    \item \textbf{p8: “unavoidable, in principle, interaction with their … environment” – I don’t see what principle makes this unavoidable}

    We have removed \textit{unavoidable} with the aim of reducing the strength of the sentence. We understand that the reviewer is referring to passive quantum error correction approaches that rely on encoding quantum information in physical systems that are protected from decoherence, e.g. topological order. 

    We have explained that such proposals exist, but that we are focusing on active error correction. We are talking about decoders so we are actually detecting and correcting errors actively. The added comment reads as:
    \textit{It is noteworthy to state that there are approaches to construct qubits that are naturally protected from decoherence by means of topological order or physical symmetries [43]. These methods are usually referred to as passive quantum error correction. However, in this review we are focusing on active error correction, in the sense that errors are corrected by actively detecting and correcting them.}

    We have also specified that we are describing \textit{ conventional qubit-based quantum computation, i.e. discrete two-level coherent quantum systems.}
    
    

    \item \textbf{p8: decoherence’s “existence does not depend on imperfect implementations” – I don’t see why that would be the case}
    
    Note that the complete sentence is \textit{decoherence does not depend on \textbf{imperfect implementations of qubit manipulations or measurements}}. We are not stating that decoherence can not be treated by construction (see the added comment on passive error correction), we are saying that even if the gate application and measurement were done to arbitrary precision, there would still be coupling to the environment. We can rephrase this though if the reviewer feels that it is still unclear.

    \item \textbf{p9: Giving 3 examples for decoherence without giving the explicit channels is quite unnecessary. Also on this page, introducing GAPD without giving the channel?}
    
    Our intention for this section is to provide to explain the motivation of the models that are usually considered in QEC and decoding. Most of the literature deals with amplitude damping and dephasing channels that are then mapped to Pauli noise as a result of twirling them. The origin of those channels comes from the decoherence processes that we have presented. Thus the need for introducing them, so that the readers can understand the motivation of the noise usually considered in theoretical QEC. The more experienced readers may know this motivation, but we consider it to be helpful for newcomers to this field. There are many other noise types/sources in the reality, most of them usually not considered when designing codes and decoders (and not considered in our review to introduce the decoding algorithms). However, we have commented such thing for completeness (see following point).

    Moreover, we added the Kraus operators of the channels.

    \item \textbf{p9: “GAPD channel is a complete mathematical description of the evolution that a qubit undergoes” – VERY NOT TRUE. This channel is extremely far from complete, does not cover non-Markovian effects, correlations, etc.}

    We also added that we are introducing Markovian noise by adding the following after relaxation, dephasing and thermal excitations are introduced: \textit{For Markovian noise, there.} We also add the following footnote to introduce what it is meant by Markovian approximation: \textit{Markovian noise assumes that the system-environment interaction is memoryless [43].}
    
    We agree with the reviewer that the sentence was an overstatement. We therefore have relaxed the sentence to: \textit{Therefore, the GAPD channel describes the evolution that a qubit undergoes when the considered decoherence processes are considered under the Markovian approximation.}

    We also agree that there exist many other noise sources that are relevant for quantum systems. Our intention for this section was to provide an understanding of the usual noise models considered in theoretical error correction and for constructing decoding methods, not to provide the full landscape of noise in quantum information. We do this for motivating many noise model in the literature such as biased noise. However, we agree that it is important to clarify this for correctness and thus we added the following:  \textit{The aim of this section is to describe the usual noise sources considered in theoretical quantum error correction as well as the motivation of certain error models, e.g. biased noise. In this sense, the usual error models are the code capacity noise, phenomenological noise and circuit-level noise. However, it is important to state that many important error sources are usually neglected by this models such as non-Markovian noise or correlated errors arising from crosstalk or cosmic rays (in superconducting qubits) [45,51,71,72].}

    We have also added the definition of phenomenological noise in section 4.4. The sentence reads as: \textit{If only imperfect measurements are considered, the model is named phenomenological noise.}

    \item \textbf{p9: “correctable code” – not clear what this means here}
    
   By correctable code it is meant that the Knill-Laflamme conditions for quantum error correction are fulfilled. We added the following footnote to specify this:\textit{Correctable codes refers to the fact that the Knill-Lafalamme conditions for quantum error correction are fulfilled [59]}.

    \item \textbf{p12: long 2nd paragraph of 5.1 without clear definitions or plots is hard to follow. Here the introduction of “virtual checks” is also vague here, and should definitely precede the discussion of “non-trivial syndrome elements” or else that part is misleading.}

    The second paragraph of 5.1 has been changed entirely into four separate paragraphs, so as to improve the clarity. On the first of these three paragraphs the concept of chain is defined as a set of same type non-trivial Pauli operators with non-trivial checks on their endpoints. Since there can be the instance of chains ending on a boundary where no checks can act as a non-trivial endpoint, we introduce the necessity of considering virtual qubits, and define them. On the second paragraph we introduce how to compute the syndrome graph of the rotated planar code, that is, a graph in which  checks and virtual checks are nodes and data qubits can be regarded as edges. This subgraph is necessary for decoding any syndrome using MWPM. The third paragraph explains the elaboration of the decoding graph for an arbitrary syndrome. In this case, one needs to consider the aforementioned syndrome graph in order to construct a second graph specific to the syndrome where nodes represent  non-trivial checks. On the last paragraph, we describe the process of finding the minimum weight perfect match within the decoding graph and recovering the Pauli error from the result.

    Moreover, FIG. 5 has been changed so as to contemplate the necessity of virtual checks, and is used for illustrating the definitions on the first three paragraphs.

    \item \textbf{p13: “the data qubits of the code are then considered to be the edges of such graphs connecting the checks nodes with their nearest ones” – ?}

    The sentence has been rewritten as: \textit{The edges connecting nodes in the syndrome graph represent the data qubits which, when changing parity, trigger a change in the parity of the check nodes they are incident to.} We mean to say that the edges connecting the nodes within the syndrome graph can be regarded as data qubits in the sense that, were they to be affected by a non-trivial Pauli operator, the two checks which are represented through the nodes the specific edge connects within the syndrome graph will change their parity.

    \item \textbf{p13: “matching the endpoints of an error chain to the observed syndrome” - not clear}

    The sentence has been changed to: \textit{In addition to always matching the syndrome, if the recovered minimum weight perfect matching from the decoding graph succeeds, the recovered error will be correct independently of the path chosen within the syndrome graph. In other words, if the error can be represented as a set of chains which relate the same non-trivial checks as the minimum weight perfect matching, the recovered error will correct it even if they are not the same Pauli operator.} We meant to say that if the minimum weight perfect matching step is successful, the recovered error will always be correct, even if the recovered Pauli operator is not the same. 

    \item \textbf{p14: “Blossom’s” is incorrect, blossom here is not a name but a noun}

    This typo has been addressed and changed, altogether with "sparse blossom", which is now written in lowercase. Nonetheless, "Fusion Blossom" has retained the capital B, since the authors of that work do it in their articles and repository.
    
    \item \textbf{p14: “performs better when considering noise channels closer to the depolarizing channel” - quite surprising, given that it neglects correlations}

    The explanation of the decrease in performance of the MWPM decoder when considering biased noise is provided within the same paragraph, and has been expanded so as to better capture the phenomenon: \textit{This performance decrease when the channel is biased can be explained by the fact that both subgraphs are considered independently. Considering a bias towards $Z$-noise results in the $Z$ decoding subgraph being more dense, i.e. more non-trivial syndromes are triggered, as opposed to the $X$-checks one. Having more non-trivial checks within the $Z$ decoding subgraph makes it more probable for the recovered minimum weight perfect matching not to capture the error that the code underwent, as the performance of the MWPM decreases as the physical error rate $p$ increases. This results in the $Z$-subgraph reaching the probability threshold before the total physical error probability reaches the threshold of the depolarizing channel.}

    \item \textbf{p14: “experimentally implemented qubits have shown lower dephasing times” – that depends on the implementation, not necessarily true for superconducting qubits}

    It was not our intention to state that every qubit platform presents biased noise, but that some implementations do and consequently biased noise is studied in QEC and decoding. We have specified this as: \textit{Biased channels are important since for some experimentally implemented qubits, such as ion traps or spin qubits, have shown lower dephasing times ...}.


    \item \textbf{p15: “performance of the code will improve when compared to single-round decoding” - not clear how this improvement is measured here}

    We have attempted to clarify the sentence by changing, expanding it and introducing an example: \textit{Measurement errors highly compromise the decoding process under single-round syndrome extraction, due to the fact that they are not accounted for and will most likely cause a logical error. For example, consider an error consisting of a single measurement error in a check, which becomes non-trivial. As has been seen in this section, the MWPM decoder will consider this check altogether with its closest virtual check and return the minimum weight perfect matching of the decoding graph as the recovered error, which will be non-trivial. By considering this space-time decoding, the performance of the code will improve when compared to single-round decoding due to the fact that measurement errors will be efficiently considered.}  

    \item \textbf{p16: peeling is not described, it should be given a proper description}

    The peeling decoder is the decoder proposed for the Union-Find erasure decoder step, and it is the one defined in section 5.2.2. Other decoders could be applied for this step of the Union-Find decoding process, but for the work of Dr. Delfosse and Dr. Zemor [140], the peeling is the only one considered due to its low complexity and good performance. We have emphasized this by changing the beginning of section 5.2.2 into: \textit{Once the syndrome validation has been computed, the Pauli error within a surface code can be treated as an erasure error (due to its known location) and, thus, can be decoded with an erasure decoder. Due to its good performance and linear complexity, the erasure decoder of choice for this step is the peeling decoder, which will be covered in continuation.}

    \item \textbf{p20: Error is a “length n-k binary vector” – not clear why not length n}

    We have rewritten this section in the revised version of the manuscript. Originally, what we meant to express was that the syndrome is generally a length $n-k$ binary vector (the error is a length $2n$ binary vector). 

    \item \textbf{p21: “capacity-achieving” is not defined}

    We have removed the use of the term ``capacity-achieving”. We have substituted it by ``achieve theoretical performance limits" so as to not define complex concepts of classical and quantum information therory that are not central to the present review (capacity).

    \item \textbf{p21: “quantum codes manifest a phenomenon known as degeneracy” – this is already clear to the reader at this point, no need for this reminder and citations, especially not for a whole paragraph}

    We have removed this part of the text.
    

    \item \textbf{p25: “vainglorious” ??}

    This term has been removed.
    

    \item \textbf{p25: “Tanner graph” not introduce}

    We have addressed this in the revised version of the manuscript. We discuss factor graphs and Tanner graphs directly in section 5.3. 
    

    \item \textbf{p26: The paragraph beginning with “In broad terms” restates the maximum likelihood problem again, covered already in this paper - it feels unnecessary}

   This paragraph has been removed.


    \item \textbf{p26: The “introduction to TNs” section does not have numbered equations, cannot be used as an introduction.}

    The equations in sections 5.4.1 and 5.4.2 have been numbered.

    \item \textbf{p27: beginning 5.4.2 with restating the maximum likelihood problem again, unnecessary}

    The first two sentences from section 5.4.2 have been removed so as not to reiterate on the QMLD vs DQMLD conception.    

    \item \textbf{p28: “chi, the maximum value of which scales in an exponential manner as larger configurations are considered” – not clear why}

    That paragraph has been extended so as to clarify the pointed statement: \textit{It is worth mentioning that this exceptional performance is highly related to the value of $\chi$. Recall that $\chi$ is the truncation is the maximum number of Schmidt tensors that we will be considered when contracting the code. A larger distance will increase the number of Schmidt tensor exponentially, thus, the maximum value of $\chi$ scales in an exponential manner as larger code distances are considered.}  

    \item \textbf{p30: “Complexity” is not defined}

    We do not understand this point raised by the referee. We have discussed the complexity of the TN decoder as a function of the code length, $n$, and the bond dimension, $\chi$, used in the truncation used in the tensor network contraction. The complexity of the algorithm is then $\mathcal{O}(n^2 \chi^3)$ as written in the article. 

    \item \textbf{p30: The cellular-automaton, RG group, and NN decoder sections contains no numbered equations, I found it impossible to understand from the paper how they work.}

    Our intention with this section was not to go as in depth as we went for the main surface code decoders (MWPM,UF,BPOSD,TN), but to introduce those and discuss their capabilities. This also not to completely discard those approaches that are still valid decoding methods. However, we made an effort to improve their presentation and discuss a bit more in depth how they operate. We made the following:

    The cellular-automaton decoder section has been changed to the core so as to be more understandable. We have begun by introducing the general definition of a cellular automaton altogether with its more famous example, the Game of Life. This is done to give an intuition to the reader on what a CA refers to in the general context. Afterwards we have moved to an introductory description to the first ever cellular-automaton decoder, where the equation of the update rule of the field within the auxiliary cellular automaton is provided. We have concluded covering the advancements in the research of cellular-automaton decoders, contemplating other decoders and updating rules. Moreover, more didactic figures have been introduced so as to improve the clarity of the section.

    Regarding the renormalization group decoder, we have included an introduction to concatenated coding and how the structure of those codes can be exploited for decoding. We have included top view figures for showing this process and it decoding pictorically. We also included an equation to discuss concatenation. We then complemented our discussion on how the toric code can be approximated to a concatenated code. We have updated the unit cell figure so that it is consistent with the other plots of the article and to give a better understanding of the process. We also discuss the intricacy of sharing qubits between unit cells and what is the problem with this. We comment that the inconsistency between shared qubits is solved by using BP (we do not show the whole process for this since it is not the objective of this chapter, but readers can refer to the provided references for it). We also added an equation showing this as well as a graphical depiction of two unit cells and which of their qubits require this inconsistency management.

    Finally, for the neural network decoders we have further explained the logic behind the splitting of the Pauli errors into their stabilizer, pure and logical components. We added a numbered equation for this, as well as a figure showing and example of this splitting. We then discuss how this splitting can be used for doing the decoding problem by means of a NN. We added a numbered equation for the activation function used for the artificial neurons in the network. We also discussed that the estimated error will match the syndrome by construction, but that those might not be of minimum weight due to the heuristic nature of the decoding approach. We also added a footnote explaining that the used NN is a feeforward NN, but that instances where message passing in the graph is done forward and backwards exist with the name of recurrent neural networks.
    
    \item \textbf{p34: The MaxSAT decoder contains a single equation, but it is not well described how this helps the decoding.}

    The main point of the MaxSAT decoder is to make an analogy between the problem of decoding the color code with the LightsOut, which is known to be solvable by means of a MaxSAT problem. The mapping is given in the bullet points of such section. In this sense, the problem of decoding the color code is translated to equation (43) that can be treated by known MaxSAT solvers, as discussed in the paragraph. Thus, the way in which this helps decoding is that the problem can be solved by known methods for combinatorial problems. We have then proven numerically that, by means of treating the decoding problem in this way, the color code can be decoded with a performance equivalent to the TN decoder but with less complexity for an specific noise model. This is also explained in the text. 

    We have added the following sentence after the MaxSAT problem equation to clarify that solving this problem with the mapping provided gives a minimum weight estimation of the error that satisfies the syndrome (which is the actual target for decoding the color code for i.i.d. noise): \textit{Thus, as a result of the analogy presented before (bullet points), solving such problem leads to an error of minimum weight (from the soft constraints) that matches the syndrome (from the satisfiability problem).}

    \item \textbf{p40: “This feature complicates the probability of a logical phase-flip” - not sure what “complicates” refers to}

    We meant to say that the fact that there is only a possible set of physical $Z$-errors which produce a logical error instead lowers the probability of a logical phase-flip. The wording has been changed accordingly.

\end{itemize}

On the \textbf{second section:} 

\textbf{Figures hard to read, have captions not informative enough, and print very slowly. All of the figures that have layouts of the surface code contain too much graphical detail (shading, etc), which makes them hard to interpret and even harder to print. This is shown in other papers using less fancy graphics in a fashion that is much easier to read. The captions are not informative enough.}

All the pictures illustrating surface codes have had the design changed so as to address this problem. Moreover several captions have been changed so as to be more informative. 

\textbf{An example: Fig. 4 has “thick red line represents the shape of two logical operators” – NO, it does not, it shows the single logical operator Y. It would be useful to show two logical operators separately, as is done in all the papers introducing the surface code. I could not find the pink dashed line, which
shows “the changed path of one of them after interacting with a stabilizer” – there are no interactions with a stabilizer here.}

The image has been addressed and several changes have been made. Now, FIG. 4 contains three figures with every non-trivial logical operator: $Y_L, X_L$ and $Z_L$. The dashed pink line representing the change of the Pauli operator upon the application of a stabilizer is now denoted with a stronger dashed red line. Moreover, the two paragraphs covering the figure have been changed so as to contemplate the features of the new figure and to be more clear.


On the \textbf{third section:} 

\textbf{Too many acronyms. There are many abbreviations that make the text hard to read, and some of these are terms that are not familiar to me from other papers on quantum error correcting codes (QMLD and DQMLD: I am more familiar with minimum weight vs maximum likelihood decoding, would be easier to read if these names were used instead of the acronyms). GAPD, GAD, APD, PTA, CTA - these are only used around pages 10 a couple of times and never later - it is an unnecessary burden to the reader to introduce them. Similarly, “llr” is used a couple of times on and around page 20, but not later - does not warrant the introduction of a new acronym.}

We have removed the acronyms for QSC, DSZNE, GAPD, GAD, APD, PTA, CTA since, as pointed out by the reviewer, those are not used extensively. We have also removed CPTP and GKLS. 

Regarding the quantum maximum likelihood decoding (QMLD) and degenerate quantum maximum likelihood decoding (DQMLD), it was introduced by Iyer and Poulin in [42], where they discuss the hardness of the decoding problem. The motivation for doing so is that both decoding problems are maximum-likelihood problems with different likelihood functions, the former neglects degeneracy while the latter considers it. In this sense, non-degenerate decoding is analogous to the classical decoding problem of finding the most likely error that satisfies the syndrome, while the degenerate maximum likelihood problem considers the error classes that arise due to degeneracy. We are aware of the minimum weight vs maximum likelihood terminology in the literature, but we consider that it does not capture the nature of the decoding problem as accurately as this formulation. First of all, as discussed, both decoding problems are maximum likelihood estimation problems, so we think that referring to a single one of them as ``maximum likelihood" is misleading. Second, we feel that referring to the non-degenerate problem as ``minimum weight decoding" does not capture the problem accurately as it just refers to the reduction of the problem for an independent, identically distributed noise model. This occurs because when considering independent, identically distributed noise, the probability of having $n$ errors equals to $p^n$, where $p$ is the error probability. Hence, the weight of the error quantifies the probability of such event and, thus, finding the error with minimum weight is equivalent to finding the error with maximum probability. However, note that if the error model is not the one described, the minimum weight rule does not have to apply. As a naive example, if a weight-two error is more probable than a weight one error (due to, for example, very correlated noise), then a minimum weight decoder will not target the adequate problem, and it will be less accurate. It is due to such reason that we would like to stick with the terminology proposed by Iyer and Poulin. However, we introduce this explanation in the text with comment \textit{For an independent, identically distributed noise model, this decoding rule results in a minimum-weight decoding rule, i.e. looking for the error with minimum weight that satisfies the measured syndrome.} after the definition of QMLD; and with footnote \textit{Note that the literature sometimes names this decoding rule as ``maximum likelihood decoding" [60]. However, both rules are actually maximum likelihood estimation problems. Their difference is the likelihood function to maximize. Therefore, we stick to the terminology in [42]} in the definition of DQMLD.

Note also that when we introduce the tensor network decoder we had discussed this subtlety in footnote 35: \textit{Note that this nomenclature might be somewhat confusing as it does not explicitly represent the fact that it is a degenerated decoder.}

Finally, regarding the term ``llr" for log-likelihood ration, it is standard in the belief propagation terminology and ubiquitous through the literature. That's why we consider it important to be defined in this article even if it is not very much used through it. Newcomers to BP should benefit from knowing what it is meant by llr in the literature.

On the \textbf{last section: }

\textbf{Too many references, too many self-citations This is a minor point, but in my view the paper contains too many references to be really useful, some of which, 217-224 don’t really qualify as references. Also, I found that there are too many self-citations also when standard things are introduced (an example: p4, citing ref 30 for the Pauli group).}

It was not our intention to include self citations for any reason and, thus, we have thoroughly checked those citations in order to remove the ``potential" skewness towards our own work claimed by the reviewer. We therefore have removed the following references:
\begin{itemize}
\item \textbf{[50]} : qldpc not central to work, we only leave foundational work and results for BPOSD that mostly come from the QLPDC field 
\item \textbf{[53]} : qtc and EAQEC not central to the work, we only leave foundational work.
\item \textbf{[70] }: superadditivity not central to the work, we only leave foundational work.
\item \textbf{[174], [175],[176]} : TNs are not central to the work, we only leave necessary references.
\end{itemize}


We understand that using some of our literature to present some simple concepts does look strange. As said, it was not our intention to skew the references to our work, it probably occurred because we know that those things appear in such references and, thus, we committed the mistake of referring to that. We have corrected this.

Additionally, we distilled the references that where not directly related to the review and left only foundational work. Specifically, we removed (references to our own work removed have been pointed out before):
\begin{itemize}
\item \textbf{[8],[9] }: leave a single reference for optimization
\item \textbf{[11],[12]} : leave single reference for macromolecule design
\item \textbf{[14]} : leave single reference for basic science application.
\item \textbf{[31] }: single use, changed by reference [32] of the new version.
\item \textbf{[36],[37],[38]} : references to classical error correction not necessary.
\item \textbf{[47]}: QLDPCs not central, enough with other refs.
\item \textbf{[52]} : QTCs not central, leave only foundational reference.
\item \textbf{[65],[66]} : single citation, not relevant for this work.
\item \textbf{[68],[69]} : superadditivity not central to this work, leave single foundational reference that is also referenced for other reasons (degeneracy).
\item \textbf{[73],[75]} : QEM not central to the work, remove references not used anymore. Leave QEM and surface code merge paper, review and PEC paper that is referenced later for gate errors.
\item \textbf{[80],[81],[82]} : remove other works on quantum decoding complexity other than Iyer and Poulin.
\item \textbf{[94]} : single citation.
\item \textbf{[99]} : redundant, keep other references.
\item \textbf{[114],[115] }: single citations, not necessary for the purposes of this work.
\item \textbf{[118]} : software, now footnote.
\item \textbf{[129]} : single citation, not necessary for this work.
\item \textbf{[143]} : single citation, not necessary for this work.
\item \textbf{[149]} : single citation, not necessary for this work.
\item \textbf{[151] }: keep single reference for trapping sets.
\item \textbf{[155]} : single citation, not necessary for this work.
\item \textbf{[164],[165]} : single shot decoding not central for this work, remove extra references.
\item \textbf{[168],[169]} : TN not central to the work, keep only one reference for introduction to TNs in general.
\item \textbf{[172]- [179]} : TN not central to the work, keep only one reference for introduction to TNs in general.
\item \textbf{[216] - [225]} : software, now footnote.
\end{itemize}

Moreover, as a result of the modification of some of the sections we have added the following references:
\begin{itemize}
\item \textbf{[52]} : reference to the surface code experiments on a neutral atoms processor by Harvard and QuEra.
\item \textbf{[71],[72]} : references on correlated and non-Markovian noise.
\item \textbf{[70] }: Bacon-Shor codes reference.
\item \textbf{[124]}: reference for QLDPC memory proposals.
\item \textbf{[129], [130]} : references to complexity of Belief propagations
\item \textbf{[146], [147]} : references to general concepts of cellular automata.
\item \textbf{[192]} : reference to the Solovay-Kitaev theorem for universal quantum computing (for the new appendix on the Backlog problem).

\end{itemize}

We have modified the references to the software packages by adding the links to the webs as footnotes so that the interested reader can access those. 
\newpage
\section{Authors' changes}

The changes we have conducted. These were not required by the reviewers, but we considered to be relevant to be included in the new version of the article.

\begin{enumerate}
    \item Sorted the citations.
    \item Changed the bibliography style to ``unsrt".
    \item Changed complexity "$O$" for $\mathcal{O}$.
    \item Changed a type ``weitghs" to ``weights".
    \item We took out the second paragraph of section 5.4. since we thought its contribution to the overall work is limited and wanted to avoid making it too long.
    \item Figure 28 caption has been changed.
    \item We comment recent experiments by Harvard and QuEra on implementing surface codes in neutral atom platforms in the intro and discuss it a bit in the discussion when we discuss the other experiments.
    \item We updated the BP+OSD package by Joschka Roffe to the LDPC software package by the same author. This the code is now hosted there. We have been told by the author that they are planning to update the package to include other decoders, we comment this too.
    \item We introduce the Backlog problem in a new appendix (new Appendix A). This is done for readers to understand why real-time decoding is required for fault-tolerant quantum computing. We comment this necessity through the text, so an introduction to why it is the case is important.
    \item Several changes have been made into the mathematical notiation in 5.3 so as to make them more clear and concise.
    \item Introduced BPOSD for circuit-level noise in 5.3.6, due to its recent discovery and great results.
\end{enumerate}

\end{document}

