

\section{Decoders for the surface code}\label{secdec}

\subsection{MWPM}
\subsection{UF}
\subsection{BP}

Belief Propagation (BP) is a message-passing algorithm that can be used to solve inference problems on probabilistic graphical models [cite Pearl]. BP has been shown to be a specific instance of the Sum-Product Algorithm (SPA) [], a general-purpose algorithm that computes marginal functions associated with a global function. Although the terms BP and SPA are essentially interchangeable, throughout this paper we will refer to the algorithm used to decode error correction codes as BP to stay consistent with the literature.

Error correction codes, irrespective of being quantum or classical, can be represented by bipartite graphs known as factor graphs. A factor graph $G = (N, E)$ is defined by a set of nodes $N = V\cup C$, where $V, C$ represent two distinct types of nodes known as variable and check nodes\footnote{In the context of classical codes, variable nodes represent bits (the columns of the PCM) and check nodes represent parity check operations (the rows of the PCM). The same holds true for quantum codes, but instead of representing bits and parity checks, variable nodes represent qubits and check nodes represent the action of stabilizer generators. In both paradigms, edges between variable and check nodes exist if the associated entry in the PCM is non-zero.}, respectively, and a set of edges $E$. Against this backdrop, BP can be used to approximate the problem of Maximum Likelihood Decoding (MLD) by exchanging messages over the factor graph representation of an error correction code. If BP runs over a graph that is a tree, it will converge to the exact MLD solution in a time bounded by the tree’s depth. In scenarios where this does not hold, 
i.e., when the algorithm is run over a loopy factor graph, BP has proven to be a good heuristic method in cases where it is not known to converge, especially when the typical size of the loops in the graph is large. 

The excellent performance of BP as a decoding algorithm for classical random-like codes, such as LDPC codes and turbo codes [44], is well-documented. In fact, classical LDPC codes are essentially capacity-achieving for a very reasonable complexity when decoded via BP []. For these reasons, along with with their finite-rate guarantees, the design of so-called 'good'\footnote{By 'good' error correction codes we refer to codes whose number of encoded logical qubits and distance increase in a polynomial manner with the number of physical qubits. Essentially, $k, d \approx O(n)$.} quantum LDPC codes has been a long-pursued topic in the field of QEC. Although the existence of sparse codes exhibiting such favorable parameter scaling remained unproven for the past two decades, groundbreaking results by Panteleev et. al as well as [] have finally shown that quantum analogues of robust ldpc codes do actually exist. However, this only addresses half of the problem, as challenges on the decoding side of things also arise in the context of QEC. Quantum codes manifest a phenomenon known as
degeneracy [51]–[54], which has no classical equivalent. This 
poses a quandary that the classical version of BP cannot resolve, as it is designed for a classical environment in which degeneracy is not present.

Degeneracy refers to the fact that errors $E \in \overbar{\mathcal{G}}_n$ that differ by a stabilizer element (errors that belong to the same logical coset) have the same effect on the code and, thus, are correctable by the same recovery operation. How degenerate a quantum code is depends on the difference between the weight of its stabilizer generators and its distance. If $w(S_k) << d \forall k \in {0,\ldots, n-k}$, then each logical coset (equivalence class) will contain many operators of the same weight and the code will be highly degenerate, whereas if $w(S_k) = d$ the code will be non-degenerate.

An optimal decoding strategy for degenerate codes should weigh the probability of each logical coset and pick an operator from the most probable one (the operator itself is irrelevant, what matters is picking the right equivalence class). This nuance is critical, as it highlights the differences between optimal decoding for degenerate quantum codes: DQMLD and non-degenerate quantum codes: QMLD. Applying the QMLD rule to a degenerate quantum code is suboptimal, as the operator with highest probability need not belong to the coset with highest probability. This performance difference is further aggravated if a BP decoder is employed to solve QMLD\footnote{Recall that BP is an approximation of QMLD that relies on marginalization: it optimizes the error probability individually for each qubit rather than jointly as is called for by QMLD.}. This is due to the fact that, degenerate quantum codes can simultaneously exhibit an error probability that is sharply peaked over a logical coset (which would lead to great performance under DQMLD) and that has a broad marginal distribution over individual qubits, which given the large number of low and similarly weighted operators of such codes, would throw off a BP decoder. The existence of the aforementioned operators and their equivalence under BP decoding lead to the peculiar phenomenon first introduced as the symmetric degeneracy error and now known as split-beliefs or quantum trapping sets. Quite frankly, if codes are degenerate enough, split beliefs put the proverbial nail-in-the-coffin for BP decoding of degenerate quantum codes. This can not be better exemplified than by the fact that that the surface/toric code\footnote{The planar code can be thought of a highly degenerate (it's distance is much higher than the weight of its stabilizers) qLDPC code of vanishing rate.} exhibits no threshold under BP decoding []. Split-beliefs have been analyzed in the literature [] and their impact has been successfully alleviated through myriads of post-processing routines added to general BP decoding. The most successful of these modified BP-based decoding techniques is known as BP-OSD. The performance increments provided by OSD post-processing have made it the forerunner in the conversation of a general purpose decoder for qLDPC codes. In fact, the toric code actually exhibits a threshold when decoded via this more sophisticated algorithm.

In consequence, this begs the question of whether BPOSD can compete with the other decoding strategies for the planar code that we have seen thus far.

\subsection{Enhanced Belief Propagation: Quantum Ordered Statistics Decoding}

The post-processing algorithm known as Ordered Statistics Decoding (OSD) [] was originally designed to improve the performance of small classical codes, as well as to lower the error floors of certain LDPC codes. It was later adapted to the quantum paradigm by Panteleev et al. in [], where the authors successfully devised the so-called qOSD routine via specific modifications to the classical OSD algorithm. In a similar fashion to other post-processing routines for sparse quantum codes, qOSD only works in conjunction with a BP decoder; i.e, it requires the outputs of a BP decoder\footnote{Research if it can work with the outputs of other decoders.} in order to function. For this reason, the decoding routine that combines both BP and qOSD is generally referred to as BPOSD. In later work, BPOSD was shown to work well for Toric codes and a novel class of semi-topological codes []. The authors also made their implementation of BPOSD public, which marked the first open-source demonstration of the decoding algorithm. 

As is done in [], for the sake of notational simplicity, we describe OSD post-processing as applied to the classical decoding problem $s = H\cdot e$. It is obvious that this framework is equally applicable to decoding the $X$ and $Z$ components of a CSS code or directly decoding over the entire parity check matrix of a non-CSS code.

The OSD post-processing algorithm is called upon whenever BP fails to produce an estimate $\hat{e}$ that matches the measured syndrome. Hence, our starting point is the following: we have a measured syndrome $s$ and an incorrect estimate of the error $\hat{e}$. The fact that $s \neq H\cdot \hat{e}$ does not imply that all components of $\hat{e}$ are wrong (in fact most will be correct), an outcome that OSD will exploit to find a valid solution to the syndrome problem. We will call the set of indices $[I]$ for which $H\cdot e_I = H\cdot \hat{e}_I$ the most reliable information set. The complement of this set $[\bar{I}]$, all those indices for which $H\cdot e_I \neq H\cdot \hat{e}_I$, will thus be the least reliable information set. The parity check matrix $H$ is a rectangular matrix that does not have full column-rank, making it impossible to solve $s \neq H\cdot \hat{e}$ via matrix inversion. However, this becomes possible if an appropriate set of linearly independent columns $[S]$ of $H$ can be found: $H^{-1}_{[S]}s = e_{[S]}$ (note that $H^{-1}_{[S]}$ is a full-rank matrix). For every choice of $[S]$, the basis of linearly independent columns, we will obtain a unique solution $e_{[S]}$, which means that the symmetry effect by which degeneracy harms BP is broken. Against this backdrop, it is clear that OSD post-processing will always yield a syndrome-matching solution $e_{[S]}$. The question then becomes how do we pick the basis $[S]$ in a way that guarantees that this solution is actually 'good', i.e, that it is the lowest weight operator associated to the measured syndrome. It is at this point that the previously introduced concept of reliable sets is applied. More precisely, we can use the soft-values (a-posteriori llrs) produced in the final BP decoding iteration to rank the bits from most likely to least likely of being flipped (lowest to highest llr values). We can then apply this order to re-arrange the parity check matrix of the code into a new matrix $\Lambda$. Against this backdrop, it is clear that the basis $[J]$ defined by the columns of the first full column rank submatrix $\Lambda_{[S]}$ (this matrix is found by simply taking the first $\text{rank}(H)$ columns of $\Lambda$) of the rearranged matrix is the least reliable basis, as it is obtained from the indices of the linearly independent columns associated to the least reliable set of bits. By picking the OSD submatrix in this way, we are guaranteed a low weight solution to the syndrome equation. At this point, instances of the OSD algorithm with varying degrees of complexity, denoted as order-$w$ OSD or OSD-$w$, where $w \in [0,\ldots, k]$, can be applied to produce search for the lowest weight solution to the syndrome equation. In what follows we detail the functioning of OSD post-processing and the differences between the lowest order version of OSD, OSD-$0$, and higher order intances known as OSD-$w$.

Assume the following, after measuring a given syndrome $s$ for a specific code with parity check matrix $H$, decoding via BP, following a fixed number of iterations, yields $\hat{e}$, which unfortunately does not produce a matching syndrome: $s \neq H\cdot \hat{e}$. In this context, we would execute OSD, which would operate as follows:

\begin{enumerate}

\item Take the soft-outputs\footnote{The a-posteriori llrs estimated in the final decoding iteration.} of the BP decoder given by $l_{i,\text{ap}} = \frac{P(e_i=0|\mathbf{s})}{P(e_i=1|\mathbf{s})},\ \forall i \in {0,\ldots,n}$, and order them from most-likely to least-likely to have been flipped (increasing order of magnitude). Store the list of bit-indices $[J]$, as this defines the least reliable information set of bits.

\item Re-arrange the columns of the parity check matrix of the code according to the ranking defined by $[J]$. We will denote this new matrix by $\Lambda$.

\item Select the first $\text{rank}(H)$ columns of $\Lambda$ to obtain the submatrix $\Lambda_{[J]}$.

\item Invert $\Lambda_{[J]}$ into $\Lambda^{-1}_{[J]}$. Calculate the solution $e_{[J]}$ to the OSD syndrome equation as $\Lambda^{-1}_{[J]}s = e_{[J]}$. Refer to the appendix for more details on how this actually works.

\item The complete solution to the decoding problem is $\mathbf{e} = [e_{[J]}, e_{[\bar{J}]}]$, where $J$ and $\bar{J}$ denote the least and most reliable information set of bits, respectively. Knowing that $e_{[J]}$ satisfies $\Lambda_{[J]}e_{[J]} = s$ then it is easy to see that $\Lambda e_\Lambda = \Lambda [e_{[J]}, \mathbf{0}] = s$.

\item The last step is to take the solution $e_\Lambda = [e_{[J]}, \mathbf{0}]$, and map it to the original bit-index order. We call the rearranged vector $e_\text{OSD-0}$ the OSD-0 solution. 

\end{enumerate}








\subsection{MPS}