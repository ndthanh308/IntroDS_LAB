%------------------------------------------------------------------------------
\section{Challenges with L\lowercase{i}DAR Point Cloud Compression}
\label{sec:motivation}
%------------------------------------------------------------------------------

\noindent{\bf Reducing discrepancy latency. }
For online perception, the end-to-end latency of the processing pipeline has a major impact on performance as it affects the application responsiveness to changes in the real-world environment~\cite{li2020towards}.
For instance, if the perception result of the data captured at $t_{0}$ is available at $t_{1}$, there would be a discrepancy between the result and real world with the changes during the time from $t_{0}$ to $t_{1}$.
Figure~\ref{fig:odres} is the screenshot of the simulated online perception results with and without 300 ms of the discrepancy latency in the object detection task.
Without the discrepancy latency, all real-world objects are aligned with the detection results as Figure~\ref{fig:odgt}.
However, with the latency, the perception results are not correctly corresponding to the real-world objects because of the discrepancies as Figure~\ref{fig:od300ms}.


In Figure~\ref{fig:sap}, we show the impact of different discrepancy latencies between the real world and the perception results on the performance of object detection.
We use the metrics for streaming perceptions~\cite{li2020towards} -- the average precision (AP) with intersection over union (IoU) threshold 0.5, and the number of mismatched objects between the results with and without discrepancy latencies.
For object detection, we use Mask R-CNN~\cite{he2017mask} pre-trained with the dataset of Microsoft COCO~\cite{lin2014microsoft}.
Then, the detection model runs with an autonomous driving dataset, Argoverse~\cite{chang2019argoverse}, and we measure the metrics with different discrepancy latencies.
Without the discrepancy latency, the perception model detects $\sim$36\% of objects to the ground truth.
As the latency increases, the AP result starts to decrease with the increased number of mismatched objects.
These results show the latency governs the perception performance and there is the latency-performance tradeoff of online perceptions.

% Figure environment removed


% Figure environment removed


In addition to the perception algorithm, there are additional components in the processing pipeline which introduce latency when offloading the LiDAR perceptions on the edge: data compression and network transportation.
The overheads from these steps contribute to the discrepancy latency, and the benefit of the reduced processing time of a perception can be compromised by them.
As shown in Figure~\ref{fig:sap}, even with 30 ms of discrepancy latency, AP decreases by $\sim$28\% of the result without the latency.
With 100 ms delay, it becomes half of the result without delay.
Furthermore, the number of mismatched objects soars with the higher latencies.
So, along with the application of edge computing to reduce the network latency, a lightweight and low-latency LiDAR point cloud compression method is essential to enable edge-assisted online LiDAR perceptions for resource-constrained mobile users.


\begin{table*}[]
  \caption{\label{tab:expcc} The benchmark results of the existing compression methods for 3D point clouds. The results in parentheses are on the Jetson AGX.}
  \begin{tabularx}{\textwidth}{{|>{\raggedright}X
                                |>{\centering}c
                                |>{\centering}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c|}}
  \hhline{=======}
    \diagbox[width=18.6em]{Metrics}{Methods}      & RLE          & Dict Coding~\cite{ziv1977universal}  & Google Draco~\cite{draco}    & MPEG G-PCC~\cite{mammou2019g}  & PCL~\cite{rusu20113d}    & RT-ST~\cite{feng2020real}  \\ \hline
    Compression Ratio                             & 0.54         & 1.67                                 & \textbf{17.05}               & 8.76                           & 5.72                     & 15.96                      \\ \hline
    PSNR (dB)                                     &\textbf{Lossless}      & \textbf{Lossless}           & 67.29                        & 78.43                          & 89.77                    & 63.18                      \\ \hline
    CD (cm)                                       &\textbf{Lossless}      & \textbf{Lossless}           & 0.267                        & 0.184                          & 0.001                    & 3.07                       \\ \hline
    Enc Time (ms)                                 & 40.1 (42.4)           & 40.5 (75.4)                 & \textbf{21.1} (\textbf{48.4})& 598 (741)                      & 72.1 (198)               & 97.7 (240)                 \\ \hline
    Dec Time (ms)                                 & 17.9 (15.8)           & 13.8 (33.7)                 & \textbf{9.44} (\textbf{18.6})& 204 (265)                      & 55.4 (153)               & 15.2 (34.8)                \\ \hline
    Enc Energy Usage (J)                          & 1.18 (\textbf{0.11})  & 1.19 (0.23)                 & \textbf{0.83} (0.14)         & 15.35 (2.41)                   & 2.05 (0.46)              & 2.63 (0.59)                \\ \hline
    Dec Energy Usage (J)                          & 0.51 (\textbf{0.04})  & 0.39 (0.08)                 & \textbf{0.36} (0.05)         & 5.56 (0.66)                    & 1.54 (0.36)              & 0.57 (0.12)                \\
  \hhline{=======}
  \end{tabularx}
\end{table*}


\noindent{\bf Limitations of existing compression methods. }
Given the popularity of 3D point cloud data, there are existing technologies for LiDAR point cloud compression: Google Draco~\cite{draco}, MPEG Geometry based point cloud compression (G-PCC)~\cite{mammou2019g}, Point Cloud Library (PCL)~\cite{rusu20113d} octree compression, and the real-time spatio-temporal (RT-ST) compression by Feng \emph{et al.}~\cite{feng2020real}.
Google Draco is based on k-d tree, PCL and G-PCC are on octree, and
 RT-ST compresses range images (RIs) of 3D point clouds.
Moreover, with these methods, it is possible to apply bytestream compressions of run-length encoding (RLE) and dictionary coding (LZ77)~\cite{ziv1977universal} to the point clouds directly; they treat point clouds as raw byte arrays and deflate them losslessly.
We compare these methods based on performance, quality, and efficiency metrics on  a desktop and on NVIDIA Jetson AGX of our experimental testbed, as described in Section~\ref{sec:subsec_exp}.
As the quality metrics of the point clouds, we use peak signal-to-noise ratio (PSNR) and Chamfer Distance (CD) as defined in Section~\ref{sec:metric}.
For each method, we encode and decode 100 point clouds from the KITTI dataset~\cite{geiger2013vision}.
The averaged results are in Table~\ref{tab:expcc}, showing the results of the desktop, and in parentheses the results from the Jetson.

While every method has its advantages for different metrics, we focus on the compression ratio, energy usage and latency, because these metrics show how well a compression method meets the requirements of point cloud compression for edge-assisted online perceptions.
For our target use case, there are three requirements of a compression method.

\begin{enumerate}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,label=(\arabic*),leftmargin=16pt]
\item It should be very low-latency because of the latency-performance tradeoff of online perceptions;
\item The compression performance effectiveness in reducing the data size is important since the larger size of compressed data causes higher network cost and energy consumption on the client to transmit;
\item It should be lightweight to run on mobile devices of limited resources while satisfying the other requirements.
\end{enumerate}

For the compression ratio and latency, Google Draco outperforms the other methods and is highly efficient in terms of the compression ratio and energy usage.
Although RLE shows comparable latency and energy usage on Jetson, it shows  increased total size when applying RLE directly to the floating-point values of 3D points.
Except for the lossless methods, PCL's octree compression shows the highest quality metrics.
However, it is at the cost of the lower efficiency and high energy usage versus Google Draco.

Based on the results in Table~\ref{tab:expcc}, Google Draco seems the best option to meet the requirements, but it causes about 60 ms ($\sim$50 ms for encoding and $\sim$10 ms for decoding) of the compression cost when the user device is Jetson and our desktop is a server.
Since there are additional delays from network transmission and algorithm processing, the discrepancy latency of the whole pipeline will be too high given a compression cost of 60 ms.
This would hurt the perception performance and be hard to use,
as illustrated in Figure~\ref{fig:sap}.

In summary, {\em there is a need for a low-latency, lightweight, and efficient LiDAR compression method for edge-assisted online perceptions}, which motivates us to pursue this work.

