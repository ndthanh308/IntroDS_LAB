%------------------------------------------------------------------------------
\section{Evaluation}
\label{sec:eval}
%------------------------------------------------------------------------------
The goal of this section is to demonstrate that our approach appropriately meets the requirements of the LiDAR point cloud compression for enabling edge-assisted online perceptions.
We compare FLiCR with several existing compression method.
Since FLiCR affects the quality of the point clouds, its impact is evaluated with the state-of-the-art LiDAR perception algorithms for 3D objection detection and LiDAR odometry and mapping (LOAM).
We also evaluate ePSNR and demonstrate its effectiveness compared to PSNR and the naive way of combining PSNR and SE.
All experiments are done by using the LiDAR point clouds of the KITTI dataset~\cite{geiger2013vision}, which are captured from Velodyne HDL-64E~\cite{hdl64}.

\subsection{Experimental Testbed}
\label{sec:subsec_exp}
The testbed consists of two machines, an NVIDIA Jetson AGX Xavier and a high-end desktop.
The Jetson has ARMv8 CPU and 32 GB memory, and
we set its power mode as 15W.
The desktop has Intel Core i7-10700, 32 GB memory, and NVIDIA RTX 2070 GPU.
Both run Ubuntu 18.04, and the energy usage is measured with \texttt{perf} on desktop and \texttt{tegrastats} on Jetson.

\subsection{FLiCR Benchmark}
As shown in Table~\ref{tab:expcc}, Google Draco~\cite{draco} is the most suitable compression method for online perceptions in terms of compression ratio, latency, and energy usage.
So, we compare FLiCR with different RI resolutions to Draco.
We benchmark FLiCR and measure the compression ratio, SE, PSNR, ePSNR ($\alpha=-0.15$, $\beta=0.5$), latencies, and energy usage.
The compression level parameter of Draco is set as 10 which is maximum.
For FLiCR, each RI is quantized by 8 bpp.
The benchmark results are shown in Table~\ref{tab:oursvsdraco}.

% Table - Draco vs FLiCR 4500, 4096, 2048, 1024, 512, 256
\begin{table*}[]
  \caption{\label{tab:oursvsdraco} \small The comparison between Google Draco and FLiCR of different RI resolutions.}
  \begin{tabularx}{\textwidth}{{|>{\raggedright}X
                                |>{\centering}c
                                |>{\centering}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c|}}
  \hhline{========}
                         & Google Draco   & \specialcell{FLiCR \\ 4500$\times$64}          & \specialcell{FLiCR \\ 4096$\times$64}         &  \specialcell{FLiCR \\ 2048$\times$64}         & \specialcell{FLiCR \\ 1024$\times$64}         & \specialcell{FLiCR \\ 512$\times$64}          & \specialcell{FLiCR \\ 256$\times$64} \\ \hline
    Compression Ratio    & 17.05          & 21.26                      & \textbf{24.75}                     & 46.18                      & 80.88                     & 131.13                    & 215.85           \\ \hline
    SE                   & \textbf{6.7\%} & 8.4\%                      & 9.1\%                     & 21\%                       & 58.8\%                    & 78.9\%                    & 89.2\%           \\ \hline
    PSNR (dB)            & \textbf{67.29} & 63.18                      & 63.09                     & 62.4                       & 61.41                     & 58.61                     & 53.71            \\ \hline
    ePSNR (dB)           & \textbf{67.27} & 63.13                      & 63.01                     & 61.64                      & 51.38                     & 35.4                      & 22.29            \\ \hline
    Enc Time (ms)        & 21.1 (48.4)    & 10.48 (26.83)              & \textbf{7.69} (\textbf{23.41})              & 4.3 (16.03)                & 3.37 (12.26)              & 2.35 (10.46)              & 1.99 (9.54)      \\ \hline
    Dec Time (ms)        & 9.44 (18.6)    & 12.52 (21.94)              & \textbf{7.44} (\textbf{20.97})              & 4.67 (17.09)               & 2.47 (15.13)              & 2.01 (11.06)              & 1.36 (10.11)     \\ \hline
    Enc Energy Usage (J) & 0.83 (0.14)    & 0.36  (0.09)               & \textbf{0.27} (\textbf{0.07})               & 0.16 (0.04)                & 0.13 (0.03)               & 0.09 (0.03)               & 0.07 (0.02)      \\ \hline
    Dec Energy Usage (J) & 0.36 (0.05)    & 0.48  (0.05)               & \textbf{0.3}  (\textbf{0.05})               & 0.19 (0.04)                & 0.13 (0.04)               & 0.09 (0.03)               & 0.08 (0.02)      \\
  \hhline{========}
  \end{tabularx}
\end{table*}

FLiCR achieves higher compression ratios across all resolutions than Draco, and it is $\sim$25\% higher even with the highest resolution.
As highlighted in Table~\ref{tab:oursvsdraco}, FLiCR with little subsampling starts to outperform Draco in the compression ratio, latency, and energy usage.
One characteristic of Draco is its encoding process takes longer and uses more energy than decoding while with FLiCR it is similar in most cases.
Considering the use case of edge-assisted perceptions, it is the client who encodes and sends data, and the server receives and decodes the encoded data to feed it to perception modules.
When the client is Jetson and the server is the desktop in our testbed, Draco introduces  $\sim$60 ms end-to-end latency, and with FLiCR it ranges from $\sim$11 ms for 256$\times$64 to $\sim$40 ms for 4500$\times$64.
Therefore, FLiCR is more advantageous for the latency-performance tradeoff of online perceptions  for commodity mobile devices, given its higher compression efficiency in terms of compression ratios and energy usage.


To satisfy the aforementioned requirements, we compromise the quality of point clouds with lossy RIs.
Since Draco quantizes each point as 11 bpp, it shows a higher PSNR, and ePSNR is almost same with PSNR as it has  small SE.
One issue about SE is that the highest resolution RIs have $\sim$8\% SE even though its resolution is with the maximum sensor precision of HDL64 specified in the spec sheet~\cite{hdl64}.
We presume this SE is caused by the sensor's measurement error and noise as the LiDAR point clouds are captured by running cars.
In Table~\ref{tab:oursvsdraco}, PSNR barely changes with 1024$\times$64 RIs which have 58.8\% SE while ePSNR reflects it.
Since the reduced quality affects the performance of downstream perceptions, we also evaluate our compression and metric with the state-of-the-art LiDAR perceptions.


\subsection{End-to-end Evaluation}
We evaluate our method and metric with two perception tasks: 3D object detection and LOAM.
For 3D object detection, we use machine learning (ML) models pre-trained with the original point clouds from the KITTI dataset from the Model Zoo of OpenPCDet~\cite{openpcdet2020}. We use the following models: Part-$A^2$ Net~\cite{shi2020points}, PointPillars~\cite{lang2019pointpillars}, PointRCNN~\cite{shi2019pointrcnn}, PV-RCNN~\cite{shi2020pv}, SECOND~\cite{yan2018second}, Voxel R-CNN~\cite{deng2020voxel}.
These models are trained with 7481 samples, and the testset is 7518 LiDAR scans.
For LOAM~\cite{zhang2014loam}, we use the A-LOAM implementation~\cite{aloam}.
For checking the impacts of RI quantization and subsampling, we generate the LiDAR point cloud dataset reconstructed from different resolution RIs.
Then, we feed our dataset to those perception models.
Since the object detection models are trained with the original LiDAR data and A-LOAM is implemented and tested by using the original dataset, we can quantitatively measure the impacts of lossy RIs in FLiCR on the perception performance.


\begin{table*}[]
  \caption{\label{tab:exp3dobj} \small The 3D object detection performances with different IoU threshold and reconstructed point clouds from the RIs. The number in each cell is the recall for the detected objects in the scene.}
  \begin{tabularx}{\textwidth}{{|>{\raggedright}X
                                ||>{\centering}c
                                |>{\centering}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c
                                |>{\centering\arraybackslash}c|}}
  \hhline{=========}
    \multicolumn{2}{|l|}{}                                        & Original     & 4500$\times$64 RI & 4096$\times$64 RI  & 2048$\times$64 RI& 1024$\times$64 RI & 512$\times$64 RI & 256$\times$64 RI   \\ \hline
    \multirow{6}{8em}{IoU Threshold \\ \hfil 0.3}          & Part-$A^2$ Net   & 95.1         & 88.4              & 88.3               & 87.9             & 76.2              & 75.5             & 56.1               \\ \cline{2-9}
                                                           & \textbf{PointPillars} & \textbf{94}           & \textbf{92.6}              & \textbf{92.4}               & \textbf{91.5}             & \textbf{82}                & \textbf{76.1}             & \textbf{54.8}               \\ \cline{2-9}
                                                           & PointRCNN    & 89.8         & 70.7              & 71.2               & 71.7             & 68.8              & 62.9             & 47.5               \\ \cline{2-9}
                                                           & PV-RCNN      & 96.8         & 94.6              & 94.4               & 94               & 89.9              & 82.8             & 70.2               \\ \cline{2-9}
                                                           & SECOND       & 94.9         & 92.7              & 92.6               & 92.1             & 88.4              & 79.4             & 60.2               \\ \cline{2-9}
                                                           & Voxel R-CNN  & 95.4         & 93.6              & 93.6               & 93.5             & 89                & 87.5             & 76.1               \\ \hline
    \multirow{6}{8em}{IoU Threshold \\ \hfil 0.5}       & Part-$A^2$ Net  & 91.2         & 82.3              & 82.2               & 81.1             & 71.4              & 63.7             & 41.3               \\ \cline{2-9}
                                                           & \textbf{PointPillars} & \textbf{88.7}         & \textbf{82.7}              & \textbf{82.5}               & \textbf{81}               & \textbf{69.8}              & \textbf{57.8}             & \textbf{30.8}               \\ \cline{2-9}
                                                           & PointRCNN    & 87.1         & 65.5              & 66                 & 66.2             & 63.8              & 57               & 38.5               \\ \cline{2-9}
                                                           & PV-RCNN      & 93.4         & 88.9              & 88.8               & 87.6             & 81.8              & 71.2             & 53                 \\ \cline{2-9}
                                                           & SECOND       & 89.1         & 83.8              & 83.7               & 82.3             & 76.8              & 61.9             & 38                 \\ \cline{2-9}
                                                           & Voxel R-CNN  & 94.9         & 91.1              & 91.1               & 90.5             & 85.2              & 79.4             & 58.4               \\ \hline
    \multirow{6}{8em}{IoU Threshold \\ \hfil 0.7}       & Part-$A^2$ Net  & 73.6         & 59.9              & 59.8               & 57.4             & 46                & 38.1             & 21.6               \\ \cline{2-9}
                                                           & \textbf{PointPillars} & \textbf{63.9}         & \textbf{49.6}              & \textbf{49.3}               & \textbf{46}               & \textbf{33.4}              & \textbf{18.4}             & \textbf{5.5}               \\ \cline{2-9}
                                                           & PointRCNN    & 73.3         & 46.8              & 47.3               & 46.9             & 43.9              & 36.4             & 20.8               \\ \cline{2-9}
                                                           & PV-RCNN      & 75.9         & 60.6              & 60.4               & 57.4             & 49                & 33.3             & 16.7               \\ \cline{2-9}
                                                           & SECOND       & 66.5         & 52.4              & 52.3               & 49.1             & 41.5              & 26.3             & 10.3               \\ \cline{2-9}
                                                           & Voxel R-CNN  & 84.6         & 67.9              & 67.9               & 64               & 54.1              & 37.1             & 16.4               \\
  \hhline{=========}
  \end{tabularx}
\end{table*}

\noindent{\textbf{3D Object Detection.\quad}}
3D object detection is the task of detecting objects from 3D point clouds.
Each algorithm of the models we use has a different network architecture, but there is a commonality between them: a backbone network extracts features from the point clouds and the extracted features are used by the regional proposal networks (RPN).
As the backbone networks of these models, PointNet++~\cite{qi2017pointnet++} is used to extract the point-level features.
For the voxel-level features, the voxel feature encoder (VFE) layer and 3D sparse convolutional networks~\cite{3DSemanticSegmentationWithSubmanifoldSparseConvNet} are used.
When each model produces the region proposals of the detected objects, they are compared with the region of the ground truth objects.
The result recall is determined by the detected objects corresponding to the ground truth object with the IoU threshold.

Table~\ref{tab:exp3dobj} shows the recall for the detected objects of the models.
With the highlighted example of PointPillars, the performance reductions between the original and 4500$\times$64 RIs show the impact of the quantization error.
In the case of  IoU threshold 0.7, it shows $\sim$23\% performance reduction while it is $\sim$2\% with threshold 0.3.
This shows the results of  higher IoU thresholds are more sensitive to the quantization error, and 3D object detection with a higher IoU threshold requires  input point clouds of almost the same quality as the original training data.

The results across the different resolutions show the impacts of SE.
One noticeable thing is the performance results decrease little with 2048$\times$64 RIs compared to 4500$\times$64 RIs, and this trend is for all IoU thresholds.
These results support our assumption for ePSNR in Section~\ref{sec:metric}; there is a knee of the curve in the entropy loss by SE.
Moreover, Table~\ref{tab:oursvsdraco} shows the ePSNR results drop drastically from 1024$\times$64 RIs as does the performance of the 3D objection detection models.

Figure~\ref{fig:epsnrres} shows the object detection recall values, and PSNR, ePSNR, and the naive way of making PSNR capture entropy, PSNR$\times(1-SE)$, as described in Section~\ref{sec:metric}.
The IoU threshold is 0.5 for all detection models, and the parameters of ePSNR are $\alpha$ (-0.15) and $\beta$ (0.5).
For the changes of SE and recalls, PSNR mildly changes across the RI resolutions, and PSNR$\times(1-SE)$ shows 
more drastic decreases compared to the perception results.
On the other hand, ePSNR shows a similar trend with the performance reduction of the perception models.
These results demonstrate the effectiveness of ePSNR with the probability function estimating the actual entropy by using SE, as a single-number metric for the point-wise and entropy-wise qualities of a point cloud.

% Figure environment removed

\noindent{\bf LiDAR Odometry and Mapping.} LOAM (or LiDAR SLAM) is a 3D mapping technique running the odometry, point matching, and registration (mapping) algorithms simultaneously~\cite{zhang2014loam}.
LOAM, and other SLAM algorithms that use different sensors, are widely used in various use cases, including autonomous vehicle, extended reality, and 3D reconstruction, and are one of the key perception tasks.
We evaluate the quality impacts of point clouds reconstructed from different RI resolutions with A-LOAM~\cite{aloam} and the evaluator~\cite{lidarslamevaluator2021}.
The experiments are with a sequence of 1101 LiDAR point clouds from the KITTI dataset.
We show our evaluation results using two metrics: absolute trajectory error (ATE) and relative error (RE).
While ATE calculates the root mean squared errors (RMSE) of position (ATE$_{pos}$) and rotation (ATE$_{rot}$) to the groundtruth, RE measures the relative relations of sub-trajectories in position (RE$_{pos}$) and rotation (RE$_{rot}$)~\cite{zhang2018tutorial}.

\begin{table}[]
  \caption{\small The LOAM averaged results of the error metrics: Position (m) and Rotation (degree).}
  \begin{center}
  \begin{tabular}{ |c|c|c|c|c| }
    \hline
                      & ATE$_{pos}$ & ATE$_{rot}$ & RE$_{pos}$  & RE$_{rot}$  \\ \hline
    Original          & 0.316       & 0.57        & 0.389       & 0.82        \\ \hline
    4500$\times$64 RI & 0.321       & 0.2         & 0.387       & 0.83        \\ \hline
    4096$\times$64 RI & 0.313       & 0.21        & 0.39        & 0.84        \\ \hline
    2048$\times$64 RI & 0.294       & 0.17        & 0.388       & 0.82        \\ \hline
    1024$\times$64 RI & 0.394       & 0.17        & 0.388       & 0.82        \\ \hline
    512$\times$64  RI &\textbf{0.610}       & 0.17        & 0.388       & 0.82        \\ \hline
    256$\times$64  RI &\textbf{0.596}       & 0.2         & 0.387       & 0.82        \\ \hline
  \end{tabular}
  \end{center}
  \label{tab:loammetric}
\end{table}


Table~\ref{tab:loammetric} shows the evaluation results of A-LOAM with different RIs.
Based on the results, the LOAM algorithm works well even with high quantization and subsampling errors.
Except for the increased ATE$_{pos}$ for 512$\times$64 and 256$\times$64, other results are almost same with the result of the original data.
Moreover, the LOAM paths of all cases are almost identical to each other as shown in Figure~\ref{fig:loam}.
After thorough analysis of the A-LOAM implementation, we find the mapping resolutions of A-LOAM are attributed to these results; the line and plane mapping resolutions of A-LOAM are 0.4 m and 0.8 m~\cite{aloam}.
The increased ATE$_{pos}$ for 512$\times$64 and 256$\times$64 are because the coarser subsampling causes loss of the sparse regions in the scene.
Specifically, in Figure~\ref{fig:subsample}, the points over long distances are lost with coarser subsampling.
The distance errors are reflected in ATE$_{pos}$ because ATE calculates the RMSE over the whole path; there is no global reference in LOAM, and the early small errors can contribute to ATE more than the later errors~\cite{zhang2018tutorial, kummerle2009measuring, burgard2009comparison}.
For RE$_{pos}$, it calculates the averaged errors of separate sub-trajectories, and the distance errors are not accumulated over the whole path.

% Figure environment removed

\noindent{\bf Summary.\quad}
Based on the experiments, we demonstrate that FLiCR is suitable for enabling edge-assisted online perceptions to mobile users.
Compared to the existing LiDAR point cloud compressions, it is {\em fast} in terms of the end-to-end compression/decompression latency, and {\em lightweight and efficient} in terms of energy usage and compression ratio.
%Moreover, it has advantages with low latency for the latency-performance tradeoff of online perceptions.
FLiCR achieves these benefits by affecting the quality of the point clouds using RI quantization and subsampling errors, and the end-to-end experiments of 3D object detection and LOAM show the impacts of the quality degradation %are dependent
on the downstream perception algorithms and their parameters.
Even though the lossy RIs have a different effect on the perception performance based on  each algorithm setting, ePSNR is able to quantify the point-wise and entropy-wise quality of a point cloud effectively.
Thus, when optimizing the compression method, it would be crucial to co-design the compression system with  awareness of the impact on downstream perceptions, and we leave this for future work.

