%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:intro}
%------------------------------------------------------------------------------

% General Context about LiDAR
Light detection and ranging (LiDAR) sensors have been used in robotics and autonomous vehicles for robust and accurate 3D sensing.
The 3D point clouds from LiDAR sensors are used in perception tasks for understanding real-world contexts, such as 3D object detection and tracking, and LiDAR simultaneous localization and mapping (LiDAR SLAM).
A LiDAR sensor has advantages over image sensors as it can provide environmental information in 3D with higher accuracy and is more robust to challenging weather and light conditions.
Previously, despite the benefits of LiDAR sensors, only a few device platforms were equipped with them because the early models of LiDAR were too expensive and large in size~\cite{expensive}.
However, as LiDAR technology advances, the sensors are becoming smaller and affordable, while maintaining their sensing performance even at lower power usage.
Recently, Velodyne Lidar released a palm-size LiDAR sensor, Velabit~\cite{velabit}, for around \$100, Intel RealSense camera~\cite{realsense} has a tiny LiDAR sensor, as do the latest Apple iPhone and iPad~\cite{applelidar} devices.
As LiDAR becomes cost-effective and smaller in size, there would be more opportunities for mobile devices to leverage perceptions of this new sense in diverse use cases.


% General Problem
Although LiDAR technology is becoming available on mobile devices, there remain challenges in leveraging LiDAR perceptions for real-time use cases.
As discussed in prior LiDAR perception research~\cite{simon2019complexer, simony2018complex, ye2020hvnet, yang2018pixor, zhang2020polarnet}, the computational complexities of LiDAR perceptions are very high because these algorithms process unstructured 3D data.
Even with these highly optimized algorithms, high-end processors and GPUs are required to make them run in real-time.
This poses limitations to enabling real-time LiDAR perception on resource-constrained (including by battery lifetime) mobile devices which lack such high-end hardware.

% General Solution
Edge computing is a technology that can relieve such issues and enable computationally intensive perceptions for mobile users with commodity hardware.
The edge (or cloudlet) is located at the edge of the network and close to the end users in the multi-tier cloud.
End users can utilize edge resources effectively for storing and processing data on nearby edge servers accessible via low-latency and high-bandwidth networks such as 5G~\cite{khan2019edge}.
There has been research on offloading image-based real-time perceptions on the edge~\cite{chen2015glimpse, liu2019edge, jin2021flexible}.
When remotely running the perceptions, a sequence of images is transmitted to the edge server in real-time.
In such settings, efficient and low-latency image compression is essential because of the large size of the raw images.
Fortunately, image compression algorithms have been extensively studied by both industry and academia~\cite{richardson2004h, grange2016vp9, sullivan2012overview}, and the accelerators for such standard codecs are broadly available even on mobile devices, given the popularity of video streaming~\cite{intelquicksink, snapd, nvcodec}.
These existing video codecs with accelerators enable the online perceptions on the remote edge servers.

% Specific Problem: LiDAR Compression
While offloading image-based perceptions takes advantage of the available codecs and accelerators for real-time video streaming, there are challenges to enabling  edge-assisted online LiDAR perceptions due to the lack of such infrastructure for LiDAR point cloud compression.
LiDAR sensors generate unstructured 3D point clouds, and their volume is too large to send them as raw data.
As an example, the points per scan of the KITTI dataset~\cite{geiger2013vision} are about 120,000 of 2 MB, and streaming the raw sensor data at 60 frames per second (FPS) needs a bandwidth of 120 Mbps.
In addition to the high bandwidth usage, transmitting a large amount of data not only causes higher network loads on the backend middleboxes, resulting in additional transmission delays, but also reduces the lifetime of a mobile device by consuming its battery~\cite{xiao2013modeling, vergara2013energybox, zhang2012mili}.
In this context, an effective point cloud compression is crucial, but it is required to be low-latency and lightweight. Since the responsiveness of online perceptions is determined by the end-to-end latency, high compression latency can introduce  discrepancy between the real environment and the perception results~\cite{li2020towards}.
The compression time should be sufficiently small not to compromise the benefit of the reduced perception processing time on the server.
Moreover, the compression should be lightweight to run on mobile devices. % compresses the point clouds.

There are prior efforts to improve 3D point cloud compression, but
their primary focus is  on achieving higher compression ratio
while preserving the original content qualities~\cite{mammou2019g, tu2016compressing, tu2019point, ahn2014large, houshiar20153d, huang2020octsqueeze, graziosi2020overview}.
Even with real-time compression methods, the latency ranges of previous methods are too high to enable online perceptions,
or they require  high-end processors with GPUs for low latency~\cite{feng2020real, tu2019real, que2021voxelcontext, sun2020novel, sun2019novel, song2021layer}.
In short, the effectiveness of existing point cloud compression methods is limited when considering mobile devices, which poses a challenge to enabling edge-assisted LiDAR perceptions.

For enabling edge-assisted LiDAR perceptions to real-time applications, we propose \textbf{FLiCR}, a lightweight and low-latency point cloud compression method based on the range-image (RI) representation and a lossless compression algorithm.
While previous research on RI compressions utilizes only the quantization of the point bit precision with lossless RI mapping~\cite{ahn2014large, feng2020real, houshiar20153d, tu2016compressing, tu2019point, tu2019real}, we explore the optimization opportunities of lossy RIs with subsampling for reducing the data size and improving the compression efficiency.
The idea is that compression algorithms such as dictionary coding, which use shorter references to repetitive features, would become more effective since they operate with a more limited data representation space.
Subsampling of mapped points leads to point loss, and it is criticial to understand how this translates to reduction in end-to-end perception quality.
We demonstrate the limitations of existing quality metrics to represent this total information loss because their designs are only concerned with point-to-point distances.
Then, we propose a unified metric, \textbf{ePSNR}, that captures both point-wise and entropy-wise point cloud qualities, by extending the current PSNR with a probability function of entropy estimation.

We evaluate FLiCR and ePSNR with the current compression methods and metrics and different LiDAR perception use cases.
In our results, FLiCR achieves up to 5.3$\times$ improvement in end-to-end compression latency on mobile devices and 12.6$\times$ in compression ratio compared to Google Draco, and ePSNR captures the quality impact of the lossyness introduced by FLiCR, enabling future system support to dynamically exercise the latency-performance tradeoff it exposes.
In summary, we make the following contributions:
\begin{tightitemize}
  \item We identify the requirements of LiDAR point cloud compression methods for edge-assisted online perceptions and conduct a thorough analysis on the limitations of the state-of-the-art technologies.
  \item We propose FLiCR, a lightweight, low-latency, and efficient compression method that combines use of lossy RI and lossless dictionary coder, and compare it to the existing methods.
  \item We point out the limitations of the current quality metrics for point clouds in terms of the entropy loss and propose ePSNR as a new single-number metric reflecting both point-wise and entropy-wise qualities.
  \item We demonstrate the benefits of our compression method and metric on two downstream perception tasks, 3D object detection and LiDAR SLAM.
\end{tightitemize}

