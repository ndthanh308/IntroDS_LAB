\section{Preliminaries}\label{sec:prelim}

\paragraph*{Notation.} 
Throughout, $\log(\cdot)$ and $\ln(\cdot)$ denote logarithms with base $2$ and $e$ respectively. We use $\N=\cbra{0,1,2,\ldots}$ to denote the set of natural numbers including 0.
For $n \in \mathbb{N}$, we write $[n]$ to denote the set $\cbra{1,2,\ldots,n}$. We use the standard $O(\cdot), \Omega(\cdot), \Theta(\cdot)$ notation, and emphasize that in this paper they only hide universal constants that do not depend on any parameter.

We write $\odot$ to denote the entrywise product for vectors and matrices: in particular, for any $x,y\in \Rbb^n$, we define $x\odot y\in \Rbb^n$ to be a vector where $(x\odot y)_i=x_iy_i$ for $i\in[n]$ and similarly for any $X,Y\in\Rbb^{n\times m}$, we define $X\odot Y\in \Rbb^{n\times m}$ to be a matrix where $(X\odot Y)_{ij}=X_{ij}Y_{ij}$ for $i \in [n], j\in [m]$. We use $\tensor$ to denote a tensor with zeros on the diagonal, i.e., for any $x\in\Rbb^n$, $x\tensor x$ is a $n\times n$ matrix where $\pbra{x\tensor x}_{ij}= x_ix_j$ if $i\neq j$ and zero if $i = j$.

For a vector $x\in\Rbb^n$, we use $\vabs{x}$ to denote its Euclidean norm. Similarly, for a matrix $X\in\Rbb^{n\times n}$, we use $\frob{X}$ to denote its Euclidean norm viewing the matrix $X$ as an $n^2$-dimensional vector. For nonzero $x \in \R^n$ or $X \in \R^{n\times n}$, we define $\unit(x) \in \R^n$ or $\unit(X) \in \R^{n\times n}$ as the unit vector along direction $x$ and $X$ respectively: $\unit(x) = x/\vabs{x}$ and $\unit(X) = X/\frob{X}$. 
We write $\mathbb{S}^{n-1}$ for the unit sphere in $\R^n$, and write $\Sbb^{n\times n-1}$ for the unit sphere in $\Rbb^{n\times n}$ where additionally the diagonal entries of the $n \times n$ matrices are zero. 
We use $\ip{x}{y}$ to denote the inner product between vectors $x,y \in \R^n$ and $\ip{X}{Y}$ to denote the inner product between matrices $X, Y \in \R^{n \times n}$ viewing them as $n^2$-dimensional vectors.

\paragraph*{Probability.}
A probability space is a triple $(\Omega, \F, \xi)$ where $\Omega$ is the sample space, $\F$ is a $\sigma$-algebra which describes the measurable sets (or events) in the probability space, and $\xi$ is a probability measure. We use $\lX\sim \xi$ to denote a random sample distributed according to $\xi$ and $\E_{\lX\sim \xi}[f(\lX)]$ to denote the expectation of a function $f$ under the measure $\xi$. For any event $S\in \F$, we use $\xi(S)$ to denote the measure of $S$ under $\xi$. We say an event $S$ holds \emph{almost surely} if $\xi(S)=1$, i.e., the exceptions to the event have measure zero. For a measurable event $\Ecal \in \F$, we write $\F \cap \{\Ecal\}$ to denote the intersection of the sigma-algebra $\F$ and the sigma-algebra generated by $\Ecal$.

We use $\Ucal_n$ to denote the uniform probability measure over $\binpm^n$ and $\gamma_n$ to denote the $n$-dimensional standard Gaussian measure in $\R^n$. We say a random variable $\lX \in \R^n$ is a  standard Gaussian in $\R^n$ if its probability distribution is $\gamma_n$. We will drop the subscript if the dimension is clear from context. We will also need lower dimensional Gaussian measures: given a linear subspace $V$ of dimension $k$, there is a $k$-dimensional standard Gaussian measure on it, which we denote by $\gamma_V$. For any measurable subset $S \subseteq \R^n$, we define its ambient space to be the smallest affine subspace $V+t$ that contains it where $V$ is a linear subspace of $\R^n$ and $t \in \R^n$. The relative Gaussian measure of $S$ denoted by $\gamma_{\rel}(S)$ is then defined to be the Gaussian measure of the set $S-t$ under $\gamma_V$.

\paragraph*{Martingales.} 
Given a sequence of real-valued random variables $\lX_1, \lX_2, \ldots, \lX_n$ in a probability space $(\Omega, \F, \xi)$ and a function $f(\lX_1,\ldots, \lX_n)$ satisfying $\BE\sbra{|f(\lX_1,\ldots,\lX_n)|} < \infty$, the sequence of random variables $\supZ{t} = \BE\sbra{f(\lX_1, \ldots, \lX_n) \mid \supF{t-1}}$ is called the \emph{Doob martingale} where $\supF{t-1}$ is the $\sigma$-algebra generated by $\lX_1,\ldots, \lX_{t-1}$ which should be viewed as a record of the randomness of the process until time $t-1$. The sequence $(\supF{t})_t$ is called a \emph{filtration}. A sequence of random variables $(\supZ{t})_t$ is called \emph{predictable} (or \emph{adapted}) with respect to $\supF{t}$ if $\supZ{t}$ is $\supF{t}$-measurable for every $t$, meaning that it is determined by the randomness in $\supF{t}$.

A discrete random variable $\btau \in \N$ is called a \emph{stopping time} with respect to the filtration $(\supF{t})_t$ if the event $\{\btau = t\} \in \supF{t}$ for all $t \in \Nbb$, or in words, whether the event $\btau=t$ occurs is determined by the history of the process until time $t$. All stopping times considered in this paper will be finite. The sigma-algebra $\supF{\btau}$ which contains all events that imply the stopping condition is defined as the set of all events $\Ecal$ such that $\Ecal \cap \{\btau = t\} \in \supF{t}$ for all $t \in \N$. We also note if one takes an increasing sequence of stopping times $(\btau_m)_m$ then the process defined by $(\supZ{\btau_m})_m$ is also a martingale.

Let $\Delta \supZ{t} := \supZ{t} - \supZ{t-1}$ be the martingale differences.
Note that $\BE\sbra{\Delta \supZ{t} \mid \supF{t-1}} = 0$ and thus
\begin{equation}\label{eqn:martingale-orthogonality}
    \BE\left[\left(\supZ{t}\right)^2\right] = \BE\left[\left(\sum_{t=1}^n \Delta \supZ{t}\right)^2\right] = \BE\left[\sum_{t=1}^n \left(\Delta \supZ{t}\right)^2\right], 
\end{equation}
where the cross terms disappear upon taking expectation. 
In other words, the martingale differences are orthogonal under taking expectations. The right hand side above is the \emph{expected quadratic variation} of the martingale $\pbra{\supZ{t}}_t$. If the sequence $(\supZ{t})_t$ is vector-valued (resp., matrix-valued) and satisfies $\BE\sbra{\Delta \supZ{t} \mid \supF{t-1}} = 0$ where $0$ is zero vector (resp., matrix), then we say it is a vector-valued (resp., matrix-valued) martingale with respect to $(\supF{t})_t$. Since each coordinate of a vector or matrix-valued martingale is itself a real-valued martingale, vector-valued or matrix-valued martingale differences are also orthogonal under Euclidean norms:
\begin{equation}\label{eqn:martingale-orthogonality-vec}
    \BE\left[\frob{\supZ{t}}^2\right] = \BE\left[\frob{\sum_{t=1}^n \Delta \supZ{t}}^2\right] = \BE\left[\sum_{t=1}^n \frob{\Delta \supZ{t}}^2\right]. 
\end{equation}

\paragraph*{Useful Inequalities.} 
We will use the well-known level-$k$ inequality \cite{DBLP:journals/combinatorica/Talagrand96, DBLP:conf/focs/KahnKL88} (see e.g., \cite[Level-$k$ Inequalities]{DBLP:books/daglib/0033652}). A statement in the Gaussian setting can be found in, e.g., \cite[Lemma 2.2]{eldan2022reduction}. We remark that we will only use the case for $k=1$ and $k=2$ here which we state below.\footnote{Our \Cref{thm:level_k_ineq} is slightly different from the references, where they additionally require $\mu\le1/e$. By Parseval's identity, the left hand side is always at most one. Therefore we use a slightly worse bound for the right hand side to allow for the whole range of $\mu$.}

Below we write $\ind_A$ for the indicator function of a set and $x_S = \prod_{i\in S} x_i$ for a monomial.

\begin{theorem}[Level-$k$ Inequality]\label{thm:level_k_ineq}
Let $k \in \{1,2\}$. Assume $A \subseteq \Rbb^n$ is measurable and $\mu:=\E_{\lX\sim\gamma}[\ind_A(\lX)]$. Then, we have
$$
\sum_{|S|=k}\pbra{\E_{\lX\sim\gamma}\sbra{\ind_A(\lX)\lX_S}}^2\le 2e^2\mu^2 \cdot \ln^k(e/\mu).
$$
\end{theorem}

In particular, if $\mu$ is non-zero, dividing both sides by $\mu^2$, we get the following more convenient form for $k \in \{1,2\}$: 
\[ \sum_{|S|=k}\pbra{\E_{\lX\sim\gamma}\sbra{\lX_S \mid \lx \in A}}^2 \le 2e^2 \cdot \ln^k(e/\mu). \]
We also make use of the following standard concentration inequality for sums of squares of independent standard Gaussians (see \cite{vershynin2018high}).

\begin{fact}\label{thm:chi_squared_concentration}
Let $m\in\N$ be arbitrary.
For any $r\ge 2m$, we have $\Pr_{\lX \sim \gamma_m} \sbra{\sum_{i=1}^m \lX_i^2 \ge r}\le e^{-{r}/{4}}$.
\end{fact}

We also need a concentration inequality for sums of squares of orthogonal quadratic forms over Gaussian random variables. 
In particular, we prove the following inequality which follows from a generalization of the Hanson-Wright inequality to a Banach space-valued setting~\cite[Theorem 6]{A20}. 
Since, we only need a special case that is easier to prove, we include a self-contained proof using the Gaussian isoperimetric inequality in \Cref{app:thm:quadratic_concentration} following~\cite[Proposition 23]{A20}.

\begin{theorem}\label{thm:quadratic_concentration}
Let $m\in\N$ be arbitrary.
Let $M_1,\ldots,M_m$ be $n \times n$ real matrices where each $M_i$ has zero diagonal, $\abra{M_i,M_i}=1$ and $\abra{M_i,M_j}=0$ for $i\neq j$. Then for any $r\ge98m$, we have
$$
\Pr_{\lX\sim\gamma_n}\sbra{\sum_{i=1}^m\abra{\lX\tensor \lX,M_i}^2\ge r}\le\exp\cbra{-\Omega\pbra{\frac r{m+\sqrt r}}}.
$$
\end{theorem} 

We remark that the tail bound above holds more generally for sub-Gaussian random variables $\lx$ (see~\cite{A20}). 