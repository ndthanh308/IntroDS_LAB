\section{Proof Overview}\label{sec:overview}

We first briefly outline the proof strategy, which consists of three main components:  

\begin{itemize}
    \item First, we show that the level-one bound can be characterized as the expected absolute value of a martingale defined as follows: Consider the random walk induced on the protocol tree when Alice and Bob are given inputs $\lx$ and $\ly$ uniformly from $\pmones$.
    Let $\X^{(t)} \times \Y^{(t)}$ be the rectangle associated with the random walk at time $t$. The martingale process tracks the inner product $\ip{\com(\X^{(t)})}{\com(\Y^{(t)})}$ where $\com(\X^{(t)}) = \BE\sbra{\lX \mid \lX \in \X^{(t)}}$ and $\com(\Y^{(t)}) = \BE\sbra{\lY \mid \lY \in \Y^{(t)}}$ are Alice's and Bob's center of masses. 
    \item Second, to bound the value of the martingale, it is necessary to ensure that neither $\X^{(t)}$ nor $\Y^{(t)}$ become excessively elongated in any direction during the protocol execution. To measure the length of $\X^{(t)}$ in a particular direction $\theta \in \mathbb{S}^{n-1}$, we calculate the variance $\mathbb{V}\mathrm{ar}\sbra{\abra{\xbm,\theta} \mid \lx\in\X^{(t)}}$, i.e. the variance of a uniformly random $\lx \in \X^{(t)}$ in the direction $\theta$. If the set is not elongated in any direction, this can be thought of as a spectral notion of almost pairwise independence. Such a notion also generalizes to almost $k$-wise independence by considering higher moments. %\mnote{Define $\lx$...} \kewen{$\lx$ is from the first item. I think it follows...}
    %If $\X^{(t)}$ is not elongated, it follows that $\lx'$ is nearly pairwise independent in any two orthogonal directions, resembling a spectral version of pairwise independence.
 
    To achieve the property that the sets are not elongated, one of the main novel ideas in our paper is to modify the original protocol to a new one that incorporates additional cleanup steps where the parties communicate \emph{real values} $\abra{\xbm,\theta}$. Through these communication steps, the sets $\X^{(t)}$ and $\Y^{(t)}$ are recursively divided into affine slices along problematic directions. %\mnote{Should we say something more?}
    
    %Second, to bound the value of this martingale, one needs to ensure that throughout the execution of the protocol both sets $\X^{(t)}$ and $\Y^{(t)}$ are not too elongated in any direction. Here we measure the length of the set $\X^{(t)}$ in a direction $\theta \in \mathbb{S}^{n-1}$ as the variance of the random vector $\lx'$ uniformly sampled from $\X^{(t)}$ in the direction $\theta$. \textcolor{red}{If the set $\X^{(t)}$ is not elongated, this implies that the random vector $\lx'$ is almost pairwise independent along any two orthogonal directions --- a sort of spectral analog of the notion of pairwise independence.}
    
  %  To ensure the above property, one of the main new ideas in our paper is to transform the original protocol into a new protocol by introducing additional cleanup steps where the parties communicate \emph{real values} (instead of bits). Such communication steps recursively partition the sets $\X^{(t)}$ and $\Y^{(t)}$ into affine slices along bad directions.  
    \item Last, one needs to show that the number of cleanup steps are small in order to bound the value of the martingale for the new protocol. This is the most involved part of our proof and requires considerable effort because the cleanup steps are real-valued and adaptively depend on the entire history, including the previous real values communicated.
\end{itemize}

The strategy outlined above also generalizes to level-two Fourier growth by considering higher moments and sending values of quadratic forms in the inputs. We also remark that since we view the sets $\X^{(t)}$ and $\Y^{(t)}$ above as embedded in $\R^n$ and allow the protocol to send real values, it is more natural for us to work in Gaussian space by doing a standard transformation. The rotational invariance of the Gaussian space also seems to be essential for us to obtain optimal level-one bound without losing additional polylogarithmic factors. %\mnote{Added a blurb about Gaussian space here...}
%\mnote{Need some sort of comment about Gaussian space here...}
%Such cleanup steps partition the space according to the level-sets defined by the value of the quadratic form. 
%We also remark that unless we limit the precision of the real communication, Alice can send her input to Bob with one message. This can be handled with truncation, although such an approach seems to lose polylogarithmic factors, and thus we shall eventually opt to work with protocols in Gaussian space.

We now elaborate on the above components in detail and also highlight the differences between the level-one and level-two settings. For conciseness, in the following overview we use $f\lesssim g$ to denote $f=O(g)$ and $f\gtrsim g$ to denote $f=\Omega(g)$ where $O$ and $\Omega$ only hide absolute constants.

\subsection{Level-One Fourier Growth}\label{sec:overview_level_one}

The level-one Fourier growth of the XOR-fiber $h$ is given by 
\[ 
L_{1,1}(h) = \sum_{i=1}^n \abs{\hat{h}(\{i\})} 
= \sum_{i=1}^n \abs{\BE_{\lZ \sim \unif}[h(\lZ) \lZ_i]} 
= \sum_{i=1}^n \abs{\BE_{\lX,\lY \sim \unif}[\Ccal(\lX,\lY)\lX_i\lY_i]}.
\]

To bound the above, it suffices to bound $\sum_{i=1}^n \eta_i \cdot \BE [\Ccal(\lX,\lY)\lX_i\lY_i]$ for any sign vector $\eta \in \pmones$. 
Here for simplicity we assume $\eta_i\equiv1$ and the probability of reaching every leaf is $\approx2^{-d}$.

\paragraph*{A Martingale Perspective.} 
To evaluate the quantity $\sum_{i=1}^n \BE [\Ccal(\lX,\lY)\lX_i\lY_i]$, consider a random leaf $\bell$ of the protocol and let $\Xell \times \Yell$ be the corresponding rectangle. Since the leaf determines the answer of the protocol, denoted by $\Ccal(\bell)$, the quantity above equals
\[ 
\sum_{i=1}^n \BE_{\bell}\sbra{\Ccal(\bell) \cdot \BE [\lX_i \mid \lX \in \Xell] \cdot \BE[\lY_i \mid \lY \in \Yell]} 
= \BE_{\bell}[ \Ccal(\bell) \cdot \ip{\com(\Xell)}{\com(\Yell)}] 
\le \BE_{\bell}[ |\ip{\com(\Xell)}{\com(\Yell)}|],
\]
where $\com(\Xell) = \BE\sbra{\lX \mid \lX \in \Xell}$ and $\com(\Yell) = \BE\sbra{\lY \mid \lY \in \Yell}$ are the center of masses of the rectangle. 
Our goal is to bound the magnitude of the random variable $\lZ = \ip{\com(\Xell)}{\com(\Yell)}$. 

We shall show that $\BE_{\bell}[|\lZ|] \lesssim\sqrt{d}$. Note that $|\lZ|$ can be as large as $d$ in the worst case --- for instance if the first $d$ coordinates of $\Xell$ and $\Yell$ are fixed to the same value --- thus we cannot argue for each leaf separately.  

To analyze it for a random leaf, we first characterize the above as a martingale process using the tree structure of the protocol. 
The martingale process is defined as $\pbra{\supZ{t}}_t$ where $\supZ{t}:=\abra{\com(\supX{t}),\com(\supY{t})}$ tracks the inner product between the center of masses $\com(\supX{t})$ and $\com(\supY{t})$ of the current rectangle $\supX{t} \times \supY{t}$ at step $t$. 
Denote the martingale differences by $\Delta \supZ{t+1} = \supZ{t+1} - \supZ{t}$ and note that if in the $t^{\text{th}}$ step Alice sends a message, then 
\[ \Delta \supZ{t+1} = \ip{\Delta \com(\supX{t+1})}{ \com(\supY{t+1})},\]
where $\Delta \com(\supX{t+1}) = \com(\supX{t+1}) - \com(\supX{t})$ is the change in Alice's center of mass. A similar expression holds if Bob sends a message.
Then it suffices to bound the expected quadratic variation (see \Cref{sec:prelim}) since
\begin{equation}\label{eqn:martingale}
     \pbra{\BE\sbra{\abs{\supZ{d}}}}^2 \le \BE\sbra{\pbra{\supZ{d}}^2} =  \BE\sbra{\sum_{t = 0}^{d-1}\pbra{\Delta \supZ{t+1}}^2 },
\end{equation}
where the equality holds due to the martingale property: $\BE\sbra{\Delta \supZ{t+1} \mid 
\supZ{1}, \ldots \supZ{t}} = 0$.

To obtain the desired bound, we need to bound the expected quadratic variation by $O(d)$. Note that it could be the case that a single $\Delta \supZ{t+1}$  scales like $\sqrt{d}$. For instance, if Bob first announces his first $d$ coordinates, $y_1, \ldots, y_d$, and then Alice sends a majority of $x_1 \cdot y_1, \ldots, x_d \cdot y_d$, then in the last step Alice's center of mass $\com(\supX{t+1})$ changes by $\approx1/\sqrt{d}$ in each of the first $d$ coordinates, and the inner product with Bob's center of mass changes by $\approx \sqrt{d}$ in a single step.

%It turns out that such examples are ruled out if Alice and Bob's sets are not elongated in any direction. 
%Therefore 
%by introducing real communication steps that slice the elongated directions
Such cases make it difficult to directly control the individual step sizes of the martingale and we will only be able to obtain an amortized bound. It turns out, as we explain later, that such an amortized bound on the martingale can be obtained if Alice and Bob's sets are not elongated in any direction. Therefore, we will transform the original protocol into a \emph{clean} protocol by introducing real communication steps that slice the elongated directions. For this, it will be convenient to work in Gaussian space which also turns out to be essential in proving the optimal $O(\sqrt{d})$ bound.

\paragraph*{Protocols in Gaussian Space.} 
A communication protocol in Gaussian space takes as inputs $\lx, \ly \in \Rbb^n$ where $\lx, \ly$ are independently sampled from the Gaussian distribution $\gamma_n$. One can embed the original Boolean protocol in the Gaussian space by running the protocol on the uniformly distributed Boolean inputs $\sgn(\lx)$ and $\sgn(\ly)$ where $\sgn(\cdot)$ takes the sign of each coordinate. Note that any node of the protocol tree in the Gaussian space corresponds to a rectangle $X \times Y$ where $X, Y \subseteq \Rbb^n$. 
Abusing the notation and defining their \emph{Gaussian} centers of masses as $\com(X) = \BE_{\lx \sim \gamma_n}\sbra{\lX \mid \lX \in X}$ and $\com(Y) = \BE_{\ly \sim \gamma_n}\sbra{\lY \mid \lY \in Y}$, one can associate the same martingale $(\supZ{t})_t$ with the protocol in the Gaussian space:
\[ 
\supZ{t} = \ip{\com(\supX{t})}{ \com(\supY{t})}.
\] 
It turns out that bounding the quadratic variation of this martingale suffices to give a bound on $L_{1,2}(h)$ (see \Cref{sec:fourier_via_martingale}), so we will stick to the Gaussian setting.
We now describe the ideas behind the cleanup process so that the step sizes can be controlled more easily.  

\paragraph*{Cleanup with Real Communication.} 
The cleanup protocol runs the original protocol interspersed with some cleanup steps where Alice and Bob send real values. As outlined before, one of the goals of these cleanup steps is to ensure that the sets are not elongated in any direction, in order to control the martingale steps.
In more detail, recall that we want to control
\[
\BE\sbra{(\Delta \supZ{t+1})^2 \mid \supZ{1},\ldots,\supZ{t}} = \BE\sbra{\abra{\Delta \com(\supX{t+1}), \com(\supY{t+1})}^2 \mid \supZ{1},\ldots,\supZ{t}}
\]
in the $t^{\text{th}}$ step where Alice speaks. There are two key underlying ideas for the cleanup steps:

\begin{itemize}
    \item \textbf{Gram-Schmidt Orthogonalization:} 
    At each round, if the current rectangle is $\X \times \Y$, before Alice sends the actual message, she sends the inner product $\ip{x}{\com({\Y})}$ between her input and Bob's current center of mass $\com({\Y})$. This partitions Alice's set $\X$ into affine slices orthogonal to Bob's current center of mass $\com(\Y)$. 
    Thus the change in Alice's center of mass in later rounds is orthogonal to $\com(\Y)$ since it only takes place inside the affine slice.
    
    Recall that the martingale $\supZ{t}$ is the inner product of Alice and Bob's center of masses, and Bob's center of mass does not change when Alice speaks.
    The original communication steps now do not contribute to the martingale and only the steps where the inner products are revealed do. In particular, if $t_{\mathrm{prev}} < t$ are two consecutive times where Alice revealed the inner product, then the change in Alice's center of mass is orthogonal to change in Bob's center of mass between time $t_{\mathrm{prev}}$ and $t$. Thus, conditioned on the rectangle $\supX{t} \times \supY{t}$ fixed by the messages until time $t$, we have, by Jensen's inequality,
    \begin{align}\label{eqn:overview}
    \BE\sbra{(\Delta \supZ{t+1})^2 \mid \supX{t},\supY{t}} 
    &= \BE\sbra{\ip{\Delta \com(\supX{t+1})}{ \com(\supY{t})- \com(\supY{t_\mathrm{prev}})}^2 \mid\supX{t}, \supY{t}}\notag\\
    &\le \BE\sbra{\ip{\lX - \com(\supX{t})}{ \com(\supY{t})- \com(\supY{t_{\mathrm{prev}}})}^2 \mid\supX{t}, \supY{t}}.
    \end{align}

    Note that the quantity on the right-hand side above is of the form $\ip{\lX - \BE[\lX]}{v}$. In other words, it is the variance of the random vector $\lX$ along direction $v$. To maintain a bound on this quantity, we introduce the notion of ``not being elongated in any direction''.    
    %second moment of a linear polynomial of the form $\ip{\lX - \BE[\lX]}{v}$ where $\lX-\BE[\lX]$ is some arbitrary mean-zero random vector. 
     %spectral notion of almost pairwise uncorrelated random variables (and $k$-wise uncorrelated random variables eventually). 
    
    \item \textbf{Not elongated in any direction:}  We define the following notion to capture the fact that the random vector is not elongated in any direction: we say that a mean-zero random vector $\lX' = \lX - \BE[\lX]$ in $\R^n$ is $\lambda$-\emph{pairwise clean}, if for every $v \in \R^n$,
    \begin{equation}\label{eq:clean_definition}
    \BE\sbra{\abra{\lX' ,v}^2} \le \lambda\cdot \|v\|^2, 
    \end{equation}
    or equivalently, the operator norm of the covariance matrix $\BE[\lX'\lX'^\top]$ is at most $\lambda$. This can be considered a spectral notion of almost pairwise independence, since the pairwise moments are well-behaved in every direction. % the coordinates of $\lX'$ were in fact pairwise independent with unit variance, i.e.,  $\BE[\lX'_i \lX'_j] = 0$ for all distinct $i,j \in [n]$ and $\BE[{\lX'_i}^2] = 1$ for each $i$, then $\BE\sbra{\abra{\lX' ,v}^2} = \|v\|^2$. \mnote{Update uncorrelated to indpendent in the proofs}

    
     %Observe that if a random vector is not elongated in any direction, meaning that, its variance is small in every direction, then the operator norm of the covariance matrix of $\lX'$ is small and equivalently, $\BE\sbra{\abra{\lX' ,v}^2} \lesssim \|v\|^2$ for any vector $v \in \R^n$. 
     
    
    
    % then random vector $\lX \in \pmones$ (i.e., supported over the hypercube), has zero mean and is also pairwise uncorrelated, then $\lX$ is also pairwise independent with uniform marginals.
    
    % Note that this is equivalent to saying that for any linear polynomial $\abra{\lX',v}$,
    % % \begin{equation}\label{eq:clean_definition}\BE\sbra{\abra{\lX' ,v}^2} \le \lambda \|v\|^2. \end{equation}
    % % This notion is precisely what we need to bound second moment of any linear polynomial. 
    
    % Recall that a random vector $\lX' = \lX - \BE[\lX]$ in $\R^n$ is pairwise uncorrelated\footnote{Note that if $\lX \in \pmones$ (i.e., supported over the hypercube), has zero mean and is also pairwise uncorrelated, then $\lX$ is also pairwise independent with uniform marginals. This fact also generalizes to $k$-wise independence.} if $\BE[\lX'_i \lX'_j] = 0$ for all distinct $i,j \in [n]$. One very helpful property of pairwise uncorrelated distributions (where additionally each coordinate has bounded variance) is that we can bound the second moment of any linear polynomial by utilizing cancellations: $\E\sbra{\abra{\lX',v}^2}\lesssim\vabs{v}^2$ where $\|\cdot\|$ denotes the Euclidean norm.
%         \avishay{I think for that you also need to assume that for all $i\in[n]$, $\E[{\lX'}_i^2] = O(1)$ right?}
%         \mnote{Yes, that is why "where additionally each coordinate has bounded variance" is needed. Maybe, it is better to be more concrete?}
% In a communication protocol, both parties could reveal partial information about their inputs in very complicated ways. We will not be able to guarantee that $\lX'$ is almost pairwise independent. We will need a different notion of almost pairwise independence that allows us to control the second moment of linear polynomials.
    
    % Motivated by this, we say a mean-zero random vector $\lX'$ is $\lambda$-spectrally pairwise uncorrelated if the operator norm of the covariance matrix $\BE[\lX'\lX'^\top]$ is at most $\lambda$.
    % Note that this is equivalent to saying that for any linear polynomial $\abra{\lX',v}$,
    % \begin{equation}\label{eq:clean_definition}\BE\sbra{\abra{\lX' ,v}^2} \le \lambda \|v\|^2. \end{equation}
    % This notion is precisely what we need to bound second moment of any linear polynomial. 
\end{itemize}

%\kewen{Why do we need ``not elongated in any direction'', ``spectrally pairwise independent'', ``pairwise clean'' for the same definition? I think we can just keep ``elongated'' and ``clean'' as a pair of antonym.} \mnote{Better antonym would be "well-rounded" or "isotropic" but I don't want to introduce more terms. Ideally we can just use elongated informally to give intuition and say clean otherwise.}

If the input distribution conditioned on Alice's set $\supX{t}$ is $O(1)$-pairwise clean, we say that her set is \emph{pairwise clean}. Based on the above ideas, after Alice sends the initial message, if her set is not yet clean, she partitions it recursively by taking affine slices and transmitting real values. More precisely, while there is direction $\theta\in \Sbb^{n-1}$ violating \Cref{eq:clean_definition}, Alice does a cleanup of her set by sending the inner product $\ip{x}{\theta}$. 
This direction is known to Bob as it only depends on Alice's current space. In addition, this cleanup does not contribute to the martingale \emph{in the future} because the inner product along this direction is fixed now.

%Based on the above ideas, after Alice sends the original message, while her current set is not clean, she recursively partitions her set by taking affine slices (sending real values). 

The resulting protocol is pairwise clean in the sense that at each step\footnote{We remark that the sets are only clean at intermediate steps where a cleanup phase ends, but we show that because of the orthogonalization step, the other steps do not contribute to the value of the martingale.}, Alice's current set is pairwise clean.
Similar arguments work for Bob.
    
Let $\D$ be the total number of communication rounds including all the cleanup steps. Then, by the above argument, and denoting by $(\btau_m)_m$ and $(\btau'_m)_m$ the indices of the inner product steps for Alice and Bob, we can ultimately bound
\begin{align}\label{eqn:qv}
\BE\sbra{(\supZ{\D})^2} & \lesssim \BE\sbra{\sum_{m} \vabs{\com(\supX{\btau_m})- \com(\supX{\btau_{m-1}})}^2 + \vabs{\com(\supY{\btau'_m})- \com(\supY{\btau'_m-1})}^2}\notag\\
&= \BE\sbra{\vabs{\com(\supX{\D})}^2 + \vabs{\com(\supY{\D})}^2},
\end{align}
where again, the last equality follows from the martingale property. 
The right hand side above can be bounded by the expected number of communication rounds $\BE[\D]$ using the level-one inequality (see \Cref{thm:level_k_ineq}) --- this inequality bounds the Euclidean norm of the center of mass of a set in terms of its Gaussian measure.
%\kewen{I commented out the intuition part since it's confusing that cleanup steps are real values but the example/information is still for Boolean.} \mnote{I added a short phrase...}
%For intuition, consider the protocol where Alice and Bob reveal one new coordinate of their input each time --- in this case, $\|\Delta \com(\supX{t})\|$ and $\|\Delta \com(\supY{t})\|$ change by one in each step, so the final bound is $O(d)$. 
%More generally, each communication essentially only gives $O(1)$ bits of information about the sets $\supX{t}$ and $\supY{t}$, and hence after $\D$ steps, the centers of mass  $\com(\supX{t})$ and $\com(\supY{t})$ are not too far from the origin. This is formally proved using the level-one inequality (see \Cref{thm:level_k_ineq}). 
    
\paragraph*{Expected Number of Cleanup steps.} 
Since the original communication only consists of $d$ rounds, the analysis essentially reduces to bounding the expected number of cleanup steps by $O(d)$, which is technically the most involved part of the proof.

%\mnote{This is not really the non-adaptive version since they are not cleaning and this undermines what we do... I rephrased it}

It is implicit in the previous works on the Gap-Hamming Problem \cite{DBLP:journals/siamcomp/ChakrabartiR12,DBLP:journals/cjtcs/Vidick12} that large sets are not elongated in many directions: if a set $X \subseteq \Rbb^n$ has Gaussian measure $\approx2^{-d}$, then for a random vector $\lx$ sampled from $X$, there are at most $m\lesssim d$ orthogonal directions $\theta_1, \ldots, \theta_m$ such that $\BE[\ip{\lX'}{\theta_i}^2]\gtrsim1$ where $\lX' = \lX - \BE[\lX]$.
This is a consequence of the fact that the expectation of $\lQ = \sum_{i=1}^m \ip{\lX'}{\theta_i}^2$ can be bounded by $O(d)$ provided that $X$ has measure $\approx2^{-d}$.
%\kewen{Simplified things here. Introducing tail bounds is a bit distracting since anyway we're using expectation of $\lQ$ as the potential function.}

The above argument suggests that maybe we can clean up the set $X$ along these $O(d)$ bad orthogonal directions.
However this is not enough for our purposes: after taking an affine slice, the set may not be clean in a direction where it was clean before.
Moreover, since the parties take turns to send messages and clean up, the bad directions will also depend on the entire history of the protocol, including the previous real and Boolean communication.
This adaptivity makes the analysis more delicate and to prove the optimal bound we crucially utilize the rotational symmetry of the Gaussian distribution.
Indeed, the fact that a large set is not elongated in many directions also holds even when we replace the Gaussian distribution with the uniform distribution on $\pmones$, but it is unclear how to obtain an optimal level-one bound using the latter.

In the final protocol, since the parties only send Boolean bits and linear forms of their inputs, conditioned on the history of the martingale, one can still say what the distribution of the next cleanup $\abra{\lx, \theta}$ looks like, as the Gaussian distribution is well-behaved under linear projections. We then use martingale concentration and stopping time arguments to show that the expected number of cleanup steps is indeed bounded by $O(d)$ even if the cleanup is adaptive.

We make two remarks in passing: 
First, we can also prove the optimal level-one bound using information-theoretic ideas but they do not seem to generalize to the level-two setting, so we adopt the alternative concentration-based approach here and they are similar in spirit. 
Second, it is possible from our proof approach (in particular, the approach for level two described next) to derive a weaker upper bound of $\sqrt{d}\cdot\polylog(n)$ for the level one while directly working with the uniform distribution on the hypercube.

\subsection{Level-Two Fourier Growth}\label{sec:overview_level_two}

%We start with Boolean inputs again for simplicity --- eventually we will carry out the argument in Gaussian space similar to the level-one case. Our techniques for level two can be directly made to work with Boolean inputs with additional $\polylog(n)$ factors that we do not want to lose for the level-one bound.

We start by noting that the level-two Fourier growth of the XOR-fiber $h$ is given by 
\[ 
L_{1,2}(h) = \sum_{i\neq j} \abs{\hat{h}(\{i,j\})} = \sum_{i\neq j} \abs{\BE_{\lZ \sim \unif}[h(\lZ) \lZ_i\lZ_j]} = \sum_{i\neq j} \abs{\BE_{\lX,\lY \sim \unif}[\Ccal(\lX,\lY)\lX_i\lX_j\lY_i\lY_j]}.
\]

To bound the above, it suffices to bound $\sum_{i\neq j} \eta_{ij} \cdot \BE [\Ccal(\lX,\lY)\lX_i\lX_j \lY_i\lY_j]$ for any symmetric sign matrix $(\eta_{ij})$. For this proof overview, we assume for simplicity that $\eta_{ij}\equiv1$.

\paragraph*{Martingales and Gram-Schmidt Orthogonalization.} 
Similar to the case of level one, the level-two Fourier growth also has a martingale formulation.
In particular, let $\supX{t}$ and $\supY{t}$ be Alice and Bob's sets at time $t$ as before and define $\comtwo(\supX{t}) = \BE\sbra{\lx \tensor \lx \mid \lx \in \supX{t}},\comtwo(\supY{t}) = \BE\sbra{\lY \tensor \lY \mid \lY \in \supY{t}}$ to be the $n\times n$ matrices that represent the \emph{level-two center of masses} of the two sets. 
Here $\lx \tensor \ly$ %=\lx\otimes\ly-\mathrm{Id}_n$ 
denotes the tensor product $\lx\otimes\ly$ with the diagonal zeroed out.%
\footnote{Here $x \tensor y$ is an $n \times n$ matrix. We will  also interchangeably view $n \times n$ matrices as $n^2$-length vectors.}
To bound the level-two Fourier growth, it suffices to bound the expected quadratic variation of the martingale $\pbra{\supZ{t}}_t$ defined by taking the inner product of the level-two center of masses $\supZ{t} := \ip{\comtwo(\supX{t})}{ \comtwo(\supY{t})}$ where $\ip{\cdot}{\cdot}$ is the inner product of two matrices viewed as vectors.

To this end, we again move to Gaussian space where the inputs $x, y \in \Rbb^n$ and transform the protocol to a clean protocol. First, we need an analog of the \emph{Gram-Schmidt orthogonalization} step --- this is achieved in a natural way by Alice sending inner product $\ip{x \tensor x}{\comtwo(\supY{t})}$ with Bob's level-two center of mass, and Bob does the same. 
Note that Alice and Bob are now exchanging values of quadratic polynomials in their inputs. Thus, to control the step sizes, we now need to control the second moment of quadratic forms which naturally motivates the following spectral analogue of $4$-wise independence.

\paragraph*{4-wise Cleanup with Quadratic Forms.} 
We say a random vector $\lX$ is $4$-wise clean with parameter $\lambda$ if the operator norm of the $n^2 \times n^2$ covariance matrix 
$$
\BE\sbra{\pbra{\lX\tensor \lX - \BE\sbra{\lX \tensor \lX}}\pbra{\lX\tensor \lX - \BE\sbra{\lX \tensor \lX}}^\top}
$$
is at most $\lambda$ where we view $\lX\tensor \lX - \BE[\lX \tensor \lX]$ as an $n^2$-dimensional vector.
This is equivalent to saying that for any quadratic form $\abra{M,\lX\tensor \lX}$,
\begin{equation}\label{eq:overview_level_two}
\E\sbra{\abra{M,\lX\tensor \lX - \E\sbra{\lX\tensor \lX} }^2} \le \lambda \frob{M}^2, 
\end{equation}
where $\frob{M}$ denotes the Euclidean norm of $M$ when viewed as a vector.
Thus, this allows us to control the second moment of any quadratic polynomial (and in particular, fourth moments of linear functions). 
We note that one can generalize the above spectral notion to $k$-wise independence in the natural way by looking at the covariance matrix of the tensor $\lX^{\tensor k}$.

We say a set is \emph{$4$-wise clean} with parameter $\lambda$ if \Cref{eq:overview_level_two} is preserved for all $M$ with zero diagonal\footnote{The requirement of zero diagonal is for analysis purposes only and can be assumed without loss of generality since $\lx\tensor\lx$ is zero diagonal anyway.}.
Combined with this notion, one can define the cleanup in an analogous way to the level-one cleanup: While there exists some $M \in \R^{n \times n}$ violating \Cref{eq:overview_level_two}, Alice sends the quadratic form $\ip{x \tensor x}{M}$ to Bob until her set is 4-wise clean with parameter $\lambda$. 

\paragraph*{Cleanup Analysis via Hanson-Wright Inequalities.} 
The crux of the proof is to bound the number of cleanup steps which, together with a similar analysis as in the level-one case, gives us the desired bound. 
We show that $m\lesssim d$ cleanup steps suffice in expectation to make the sets $4$-wise clean for $\lambda \le d \cdot \polylog(n)$. Analogous to \Cref{eqn:martingale} and \Cref{eqn:qv}, this gives a bound of $d^3 \cdot \polylog(n)$ on the expected quadratic variation and implies $L_{1,2}(h) \le d^{3/2}\cdot \polylog(n)$.

Since the parties send values of quadratic forms now, the analysis here is significantly more involved compared to the level-one case, even after moving to the Gaussian setting, where one could previously use the fact that the Gaussian distribution behaves nicely under linear projections. 
We rely on a powerful generalization of the Hanson-Wright inequality to a Banach-space-valued setting due to Adamczak, LataÅ‚a, and Meller~\cite{A20}. 
This inequality gives a tail bound for sum of squares of quadratic forms: 
In particular if $M_1, \ldots, M_m$ are matrices with zero diagonal which form an orthonormal set when viewed as $n^2$ dimensional vectors,
then the random variable $\lQ = \sum_{i=1}^m \ip{\lX \tensor \lX}{M_i}^2$ satisfies $\Pr_{\lx \sim \gamma_n}[\lQ \ge t] \le e^{-\Omega(\sqrt{t})}$ for any  $t\gtrsim m^2$ (see \Cref{thm:quadratic_concentration} for a precise statement).
We remark that this tail bound relies on the orthogonality of the quadratic forms and is much sharper than, for example, the bound obtained from hypercontractivity or other standard polynomial concentration inequalities.

In our setting, the matrices are being chosen adaptively.
In addition, the parties are sending quadratic forms in their inputs, and the distribution of the next $\ip{\lX \tensor \lX}{M}$ conditioned on the history is hard to determine, unlike the level-one case. 
To handle this, we replace the real communication with Boolean communication of finite precision $\pm 1/\poly(n)$. 
This means that whenever Alice wants to perform cleanup $\abra{\lx\otimes\lx,M}$ for some $M$ known to both parties, she sends only $O(\log(n))$ bits. 
On the one hand, this modification is similar enough to the cleanup protocol with real messages so that most of the argument carries through. On the other hand, now the protocol is completely discrete, which allows us to condition on any particular transcript.

For intuition, assume we fix a transcript of $L=d + O(m\log(n))$ bits which has gone through $m$ cleanups.
Typically, this transcript should capture $\approx 2^{-L}$ of the probability mass. 
More crucially, the matrices $M_1, \ldots, M_m$ for the cleanups are also fixed along the transcript, and one can apply the aforementioned Hanson-Wright inequality on $\lQ = \sum_{i=1}^m \ip{\lX \tensor \lX}{M_i}^2$. 
Combining the two facts together, we can apply the non-adaptive tail bound above and then condition on obtaining such typical transcript. 
This shows $\E[\lQ]\le d^2\cdot\polylog(n)$.
However, each quadratic form comes from a violation of \Cref{eq:overview_level_two} and contributes at least $\lambda$ to $\lQ$ in expectation.
This implies that $\E[\lQ]\ge \lambda\cdot m$ and by taking $\lambda=d\cdot\polylog(n)$, we derive that the number of cleanup steps $m\lesssim d$. This shows that the level-two Fourier growth is $O((m+d)\cdot\sqrt\lambda)=d^{3/2}\cdot\polylog(n)$ completing the proof. 

Note that if we could take $\lambda = \polylog(n)$ while having the same number of cleanup steps $m=d\cdot\polylog(n)$, then we would obtain an optimal level-two bound of $d \cdot \polylog(n)$.
However, it is not clear how to use current approach to show this.
In \Cref{sec:improved-hw}, we identify examples showing the tightness of our current analysis and also discuss potential ways to circumvent the obstacles within.

We remark that by replacing the Hanson-Wright inequality with its higher-degree variants and performing level-$k$ cleanups, we can analyze level-$k$ Fourier growth in the similar way.
However, since the first two levels already suffice for our applications and we believe that our level-two bound can be further improved, we do not make the effort of generalizing it to higher levels here.