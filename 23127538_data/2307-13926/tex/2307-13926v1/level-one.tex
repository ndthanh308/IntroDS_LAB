\section{Level-One Fourier Growth}\label{sec:proof_of_level_one}

In this section, we will give a proof of
\Cref{thm:boolean_bound_level_one} that $L_{1,1}(h) = O(\sqrt{d})$. We start with a $d$-round communication protocol $\tilde{\Ccal}$ over the Gaussian space as defined in \Cref{sec:boolean_to_real}. 
Given the discussion in the previous section and \Cref{prop:fwt-to-qv}, our task ultimately reduces to bounding the expected quadratic variation of the martingale that results from the protocol $\bar{\Ccal}$. For example, one can simply take $\bar\Ccal=\tilde\Ccal$, but, as discussed in \Cref{sec:overview}, the individual step sizes of this martingale can be quite large in the worst-case and it is not so easy to leverage cancellations here to bound the quadratic variation by $O(d)$. 

So, we first define a \emph{generalized} communication protocol $\bar{\Ccal}$ that is equivalent to the original protocol $\tilde\Ccal$ but has additional ``cleanup'' rounds where Alice and Bob reveal certain linear forms of their inputs so that their sets are pairwise clean in the sense described in the overview. These cleanup steps allow us to keep track of the quadratic variation more easily.

\subsection{Pairwise Clean Protocols}\label{sec:pairwise_clean_protocols}

To define a clean protocol, we first define the notion of a pairwise clean set.
Let $X\subseteq \Rbb^n$. We say that the set $X$ is \emph{pairwise clean in a direction $a \in \mathbb{S}^{n-1}$} with parameter $\lambda$ if 
\begin{equation}\label{eqn:pairwiseclean}
 \E_{\lX\sim \gamma}\sbra{ \abra{\lX-\com(X),a}^2 \mid \lX\in X }\le \lambda,
\end{equation}
where we recall that $\com(X) = \E_{\lX\sim \gamma}\sbra{\lX\mid \lX \in X}$ is the level-one center of mass of $X$. 

The above condition implies that for a random vector $\lX$ sampled from $\gamma$ conditioned on $X$, its variance along the direction $a$ is bounded by $\lambda$. We say that the set $X$ is \emph{pairwise clean} (with parameter $\lambda$) if it is clean in \emph{every direction $a \in \mathbb{S}^{n-1}$}. Equivalently, the operator norm of the covariance matrix of the random vector $\lX$ is bounded by $\lambda$.

We call a generalized communication protocol pairwise clean with parameter $\lambda$ if at the start of a new ``phase'' of the protocol, the corresponding rectangle $X \times Y$ satisfies that both $X$ and $Y$ are pairwise clean. Starting from a communication protocol $\tilde{\Ccal}$ in the Gaussian space, we will transform it into a pairwise clean protocol $\bar \Ccal$ by proceeding from top to bottom and adding certain Gram-Schmidt orthogonalization and cleanup steps.

In particular, consider an intermediate node in the protocol tree of $\tilde{\Ccal}$. Before Alice sends her bit as in the original protocol $\tilde\Ccal$, she first performs an orthogonalization step by revealing the inner-product between her input and Bob's current level-one center of mass. After this, she sends her bit according to the original protocol and afterwards she repeatedly cleans her current set $X$ by revealing $\abra{x,a}\in \Rbb$ while $X$ is not clean along the direction $a$ orthogonal to previous directions. 
Once $X$ becomes clean, they proceed to the next round. 
We now describe this formally.

\paragraph*{Construction of pairwise clean protocol $\bar \Ccal$ from $\tilde \Ccal$.}

We set $\lambda = 100$. The construction of the new protocol is recursive and we first define some notation. Consider an intermediate node of the new protocol $\bar \Ccal$ at depth $t$. We use the random variable $\supX{t}\subseteq\Rbb^n$ (resp., $\supY{t}\subseteq \Rbb^n$) to denote the set of inputs of Alice (resp., Bob) reaching the node. 
If Alice reveals a linear form in this step, we use $\supa{t}\in \Rbb^n$ to denote the vector of the linear form; otherwise, we set $\supa{t}$ to be the all-zeroes vector. 
We define $\supb{t}$ similarly for Bob. Throughout the protocol, we will abbreviate $\supu{t} = \com(\supX{t})$ and $\supv{t} = \com(\supY{t})$ for Alice's and Bob's current center of mass respectively. 
\begin{enumerate}
	\item At the beginning, Alice receives an input $x\in\Rbb^n$ and Bob receives an input $y\in\Rbb^n$.
	\item We initialize $t\gets0$, $\supX{0},\supY{0}\gets\Rbb^n$, and $\supa{0},\supb{0}\gets0^{n}$. 
	\item For each phase $i=1,2,\ldots,d$: suppose we are starting the cleanup for a node at depth $i$ in the original protocol $\tilde\Ccal$ and suppose we are at a node of depth $t$ in the new protocol $\bar\Ccal$. If it is Alice's turn to speak in $\tilde{\Ccal}$:
	\begin{enumerate}
		\item \textbf{Orthogonalization by revealing the correlation with Bob's center of mass.}\\
		Alice begins by revealing the inner product of her input $x$ with Bob's current (signed) center of mass $\Lambda\odot \supv{t}$. Since in the previous steps, she has already revealed the inner product with Bob's previous centers of mass, for technical reasons, we will only have Alice announce the inner product with the component of $\Lambda\odot \supv{t}$ that is orthogonal to the previous directions along which Alice announced the inner product. More formally, let $\la^{(t+1)}$ be the component of $\Lambda\odot \supv{t}$ that is orthonormal to all previous directions $\supa{1},\dots, \supa{t}$, i.e.,
		$$\textstyle
		\la^{(t+1)}=\unit\pbra{ 
		\Lambda\odot \supv{t}
		- \sum_{\tau=1}^{t}	\abra{\Lambda \odot \supv{t},\supa{\tau}} \cdot \supa{\tau}}.$$
		
		Alice computes $\bar \lc^{(t+1)}\gets \abra{x,\la^{(t+1)}}$ and sends $\bar \lc^{(t+1)}$ to Bob. Set $\lb^{(t+1)}\gets 0^ n$. Increment $t$ by 1 and go to step (b). 
		\item \textbf{Original communication.} Alice sends the bit $\supcbar{t+1}$ that she was supposed to send in $\tilde\Ccal$ based on previous messages and the input $x$. Set $\la^{(t+1)},\lb^{(t+1)}\gets 0^n$. Increment $t$ by 1 and go to step (c). 
		\item \textbf{Cleanup steps.} While there exists some direction $a\in\mathbb{S}^{n-1}$ orthogonal to the previous directions (i.e., satisfying $\abra{a,\la^{(\tau)}}=0$ for all $\tau\in [t]$) such that $\X^{(t)}$ is \emph{not pairwise clean} in direction $a$, Alice computes $\bar \lc^{(t+1)}\gets\abra{x,a}$ and sends this to Bob. 
		Set $\la^{(t+1)}\gets a$ and $\lb^{(t+1)}\gets0^n$. Increment $t$ by 1. Repeat step (c) as long as $\X^{(t)}$ is not pairwise clean; otherwise increment $i$ by 1 and go back to the for-loop in step 3 which starts the new phase.
	\end{enumerate}
	If it is Bob's turn to speak, we define everything similarly with the role of $x,\la,\X,\V$ switched with $y,\lb,\Y,\U$.
	\item Finally at the end of the protocol, the value $\bar\Ccal(x,y)$ is determined based on all the previous communication and the corresponding output it defines in $\tilde\Ccal$.
\end{enumerate}

We note some basic properties that directly follow from the description. First we note that the steps 3(a), 3(b), and 3(c) always occur in sequence for each party and we refer to such a sequence of steps as a \emph{phase} for that party. Note that there are at most $d$ phases. If a new phase starts at time $t$, then the current rectangle $\supX{t} \times \supY{t}$ is pairwise clean for both parties by construction. Also, note that the non-zero vectors in the sequence $(\supa{t})_t$ (resp., $(\supb{t})_t$) form an orthonormal set. We also note that the Boolean communication in step 3(b) is solely determined by the original protocol and hence only depends on the previous Boolean messages.

Lastly, each phase has one 3(a) and 3(b) step, followed by potentially many 3(c) steps. However, the following claim shows that it is always finite.
\begin{claim}\label{clm:finite_steps_level_one}
Let $\ell$ be an arbitrary leaf of the protocol $\bar\Ccal$ and $D(\ell)$ be its depth. Then $D(\ell) \le 2n + 2d$. 
Moreover, along this path there are at most $2d$ many steps 3(a) and 3(b).
\end{claim}
\begin{proof}
	We count the number of communication steps separately:
	\begin{itemize}
		\item \textbf{Steps 3(a) and 3(b).} Steps 3(a) and 3(b)  occur once in every phase, thus at most $d$ times.
		\item \textbf{Step 3(c).} For Alice, each time she communicates at step 3(c) $a\in\Rbb^n$, the direction is orthogonal to all previous $\supa{t}$'s. Since the dimension of $\Rbb^n$ is $n$, this happens at most $n$ times. Similar argument works for Bob.
	\end{itemize}
	Thus in total we have at most $2n+2d$ steps.
\end{proof}
We will eventually show that the \emph{expected} depth of the protocol $\bar \Ccal$ is $O(d)$ when $\lX, \lY \sim \gamma_n$.

\subsection{Bounding the Expected Quadratic Variation}

Consider a random walk on the protocol tree generated by the new protocol $\bar \Ccal$ when the parties are given independent inputs $\lx, \ly \sim \gamma_n$.
Consider the corresponding level-one martingale process defined in \Cref{eqn:def-martingale}. Formally, at time $t$ the process is defined by
\[ \supZ{t}_1 = \ip{\supu{t}}{\eta \odot \supv{t}},\]
where we recall that $\supu{t} = \com(\supX{t})$ and $\supv{t} = \com(\supY{t})$ and $\eta \in \pmones$ is a fixed sign vector. 

The martingale process stops once it hits a leaf of the protocol $\bar \Ccal$. Let $\D$ denote the (stopping) time when this happens. Note that $\BE[\D]$ is exactly the expected depth of the protocol $\bar \Ccal$. Then, in light of \Cref{prop:fwt-to-qv}, to prove \Cref{thm:boolean_bound_level_one}, it suffices to prove the following.

\begin{lemma}\label{lem:qv-level-one}
$\E\sbra{\sum_{t=1}^{\D} \pbra{\Delta\supZ{t}_1}^2} = O(d)$.
\end{lemma}

We will prove this in two steps. We first show that the only change in the value of the martingale occurs during the orthogonalization step 3(a). 
This is because in each phase, Alice's change of center of mass in steps 3(b) and 3(c) is always orthogonal to $\eta \odot \supv{t}$ so they do not change the value of the martingale $\supZ{t}_1$ as discussed in \Cref{sec:overview}. 
Moreover, recalling \Cref{eqn:overview}, since Alice's node was pairwise clean just before Alice sent the message in step 3(a), the expected change $\BE\left[\left(\Delta\supZ{t+1}_1 \right)^2\right]$ can be bounded in terms of the squared norm of the change that occurred in $\supu{t}$ between the current round and the last round where Alice was in step 3(a). 
A similar argument works for Bob.

Formally, this is encapsulated by the next lemma for which we need some additional definition.
Let $(\supF{t})_t$ be the natural filtration induced by the random walk on the generalized protocol tree with respect to which $\supZ{t}_1$ is a Doob martingale and also $\supu{t}, \supv{t}$ form vector-valued martingales (recall \Cref{prop:vec-martingale}). 
Note that $\supF{t}$ fixes all the rectangles encountered during times $0,\ldots, t$ and thus for $\tau \le t$, the random variables $\supu{\tau},\supv{\tau},\supZ{\tau}_1$ are determined, in particular, they are $\supF{t}$-measurable. Recalling that $\lambda = 100$ is the cleanup parameter, we then have the following. Below we assume without any loss of generality that Alice speaks first and, in particular, we note that Alice speaks in step 3(a) for the first time at time zero. 

\begin{lemma}[Step Size]\label{lem:step_size_square_level_one}
Let $0=\btau_1 < \btau_2 < \cdots \le \D$ be a sequence of stopping times with $\btau_m$ being the index of the round where Alice speaks in step 3(a) for the $m^\text{th}$ time or $\D$ if there is no such round. 
Then, for any integer $m \ge 2$, 
$$
\BE\left[ \left(\Delta\supZ{\btau_m+1}_1\right)^2 ~\bigg|~ \supF{\btau_m}\right] \le \lambda \cdot \vabs{\supv{\btau_m} - \supv{\btau_{m-1}}}^2,
$$
and moreover, for any $t \in \N$, we have that
$$
\BE\left[ \left(\Delta\supZ{t+1}_1\right)^2 ~\bigg|~ \supF{t}, \btau_{m-1} < t <\btau_{m}, \text{Alice speaks at time }t \right] = 0.
$$
A similar statement also holds if Bob speaks where $\V$ is replaced by $\U$ and the sequence $(\btau_m)$ is replaced by $(\btau'_m)$ where $\btau'_m$ is the index of the round where Bob speaks in step 3(a) for the $m^\text{th}$ time or $\D$ if there is no such round. 
\end{lemma}
In particular, we see that the steps 3(b) and 3(c) do not contribute to the quadratic variation and only the steps 3(a) do. Also, since the first time Alice and Bob speak, they start in step 3(a), we also note that $\supu{\btau_1}$ and $\supv{\btau'_1}$ are their initial centers of mass which are both zero.  

We shall prove the above lemma in \Cref{sec:step_size_level_one} and continue with the bound on the quadratic variation here. Using \Cref{lem:step_size_square_level_one}, we have
\begin{align*}
\E\sbra{\sum_{t=1}^{\D} \pbra{\Delta\supZ{t}_1}^2} \le \lambda \cdot\E\sbra{\sum_{m\ge 2} \vabs{\V^{(\btau_m)}-\V^{(\btau_{m-1})}}^2+\vabs{\U^{(\btau'_m)}-\U^{(\btau'_{m-1})}}^2}.
\end{align*}
On the other hand, by the orthogonality of vector-valued martingale differences from \Cref{eqn:martingale-orthogonality-vec}, we have
\begin{align*}
	\E\sbra{\sum_{m \ge 2} \vabs{\V^{(\btau_m)}-\V^{(\btau_{m-1})}}^2} = \E\sbra{\vabs{\V^{(\D)}}^2}.
\end{align*}
A similar statement holds for $(\supu{t})_t$. Therefore, 
\begin{align}\label{eqn:qv-upper-bound}
	 \E\sbra{\sum_{t=1}^{\D} \pbra{\Delta\supZ{t}_1}^2} \le \lambda \cdot\pbra{\E\sbra{\frob{\U^{(\D)}}^2}+\E\sbra{\frob{\V^{(\D)}}^2}}.
\end{align}

We prove the following in \Cref{sec:expected_cleanup_depth} to upper bound the quantity on the right hand side above. Loosely speaking, by an application of level-one inequalities (see \Cref{thm:level_k_ineq}), the lemma below ultimately boils down to a bound on the expected number of cleanup steps. 

\begin{lemma}[Final Center of Mass]\label{lem:expected_norm_level_one}
$
\E\sbra{\vabs{\supu{\D}}^2+\vabs{\supv{\D}}^2} = O(d).
$
\end{lemma}

Since $\lambda = 100$, plugging in the bounds from the above into \Cref{eqn:qv-upper-bound} readily implies \Cref{lem:qv-level-one}. Together with \Cref{prop:fwt-to-qv}, this completes the proof of \Cref{thm:boolean_bound_level_one}.

\subsection[Bounds on Step Sizes]{Bounds on Step Sizes (Proof of \Cref{lem:step_size_square_level_one})}\label{sec:step_size_level_one}

Let us abbreviate $\btau = \btau_m$. Observe that
\begin{align}
\E\sbra{\pbra{\Delta\lZ^{(\btau+1)}_1}^2\mid \Fcal^{(\btau)}}&=\E\sbra{\abra{\U^{(\btau+1)}-\U^{(\btau)}, \Lambda\odot \V^{(\btau)}}^2\mid \Fcal^{(\btau)}}\notag\\
&=\E\sbra{ \abra{\U^{(\btau+1)}, \Lambda\odot \V^{(\btau)} } ^2-\abra{\U^{(\btau)},\Lambda\odot \V^{(\btau)}}^2\mid \Fcal^{(\btau)}},
\label{step_size_level_one_alpha_3}
\end{align}
where the second line is due to $(\supu{t})_t$ being a vector-valued martingale and thus $\E\sbra{\U^{(\btau+1)}\mid \Fcal^{(\btau)}}=\U^{(\btau)}$.  

We first consider the case that at time $\btau$ a new phase starts for Alice. By construction, this means that the current rectangle $\supX{\btau} \times \supY{\btau}$ determined by $\supF{\btau}$ is pairwise clean with parameter $\lambda$, and since Alice is in step 3(a) at the start of a new phase, $\supa{\btau+1}$ is chosen to be the (normalized) component of $\Lambda\odot \V^{(\btau)}$ that is orthogonal to previous directions $\supa{0}, \ldots, \supa{\btau}$. Let $\balpha^{(\btau+1)}:= \abra{\Lambda\odot \V^{(\btau)},\la^{(\btau+1)}}$ be the length of this component before normalization. Note that $\balpha^{(\btau+1)}$ is $\Fcal^{(\btau)}$-measurable (i.e., it is determined by $\Fcal^{(\btau)}$). 

We now claim that components of $\supu{\btau+1}$ and $\supu{\btau}$ are the same along any of the previous directions $\supa{0}, \ldots, \supa{\btau}$. So in \Cref{step_size_level_one_alpha_3}, they cancel out and the only relevant quantity is the component in the direction $\supa{\btau+1}$. 
This follows since, in all the previous steps $t \le \btau$, Alice has already fixed $\sabra{x,\la^{(t)}}$. 
This implies that for any $\supX{\btau}$ and $\supX{\btau+1}$ that are determined by $\supF{\btau+1}$, the inner product with all the previous $\la^{(0)}, \ldots, \la^{(\btau)}$ is fixed over the choice of $x$ from both rectangles. 
Formally, we have that for any $x\in \X^{(\btau)}$ and $x'\in \X^{(\btau+1)} $, it holds that $\sabra{x,\la^{(t)}}=\sabra{x',\la^{(t)}}$ for any $t \le \btau$. In particular, since $\U^{(\btau)}=\com(\X^{(\btau)})$ and $\U^{(\btau+1)}=\com(\X^{(\btau+1)})$ are the corresponding centers of mass, we have that
\begin{equation}\label{step_size_level_one}
\abra{\U^{(\btau+1)}, \la^{(t)}}=\abra{\U^{(\btau)},\la^{(t)}} \text{ for all } t\le \btau.
\end{equation}

This, together with \Cref{step_size_level_one_alpha_3} and recalling that $\balpha^{(\btau+1)}$ is determined by $\Fcal^{(\btau)}$, implies that 
\begin{align}\label{eqn:step_size_level_one_alpha_4}
 \E\sbra{\pbra{\Delta\lZ^{(\btau+1)}_1}^2\mid \Fcal^{(\btau)}} &=\pbra{\balpha^{(\btau+1)}}^2\cdot \E\sbra{ \abra{\U^{(\btau+1)},\la^{(\btau+1)} } ^2-\abra{\U^{(\btau)}, \la^{(\btau+1)}}^2\mid \Fcal^{(\btau)}}.
\end{align}

We now bound the term outside the expectation by the change in the center of mass $\supv{\cdot}$ and the term inside the expectation by the fact that the set is pairwise clean.

\paragraph*{Term Outside the Expectation.} 
Recall that $\supa{\btau+1}$ is chosen to be the (normalized) component of $\Lambda\odot \V^{(\btau)}$ that is orthogonal to the span of $\supa{0}, \ldots, \supa{\btau}$. Since $\Lambda\odot \V^{(\btau_{m-1})}$ is in the span of $\supa{1}, \ldots, \supa{\btau_{m-1}+1}$ and $\btau_{m-1} + 1 \le \btau=\btau_m$, it is orthogonal to $\la^{(\btau+1)}$. Hence,
	\[ 
	\balpha^{(\btau+1)} = \abra{\Lambda\odot \V^{(\btau)},\la^{(\btau+1)}}= \abra{\Lambda\odot\pbra{ \V^{(\btau)}-\V^{(\btau_{m-1})}},\la^{(\btau+1)}}.
	\]
Since $\la^{(\btau+1)}$ is a unit vector and each entry of $\Lambda$ is in $\binpm$, this implies that
\begin{equation}\label{step_size_level_one_alpha}
	\pbra{\balpha^{(\btau+1)}}^2\le \vabs{\V^{(\btau)}-\V^{(\btau_{m-1})}}^2.
	\end{equation}
	
\paragraph*{Term Inside the Expectation.}
Since $(\supu{\tau})$ is a vector-valued martingale with respect to $\supF{\tau}$, and $\supa{\tau+1}$ is $\supF{\tau}$-measurable (determined by $\supF{\tau}$), we have that
\begin{align*}
    \E\sbra{ \abra{\U^{(\btau+1)},\la^{(\btau+1)} } ^2-\abra{\U^{(\btau)}, \la^{(\btau+1)}}^2\mid \Fcal^{(\btau)}}
 = \E\sbra{ \abra{\supu{\tau+1} - \supu{\tau} ,\la^{(\btau+1)} } ^2\mid \supF{\tau}}.
\end{align*}

Since Alice is in step 3(a), her message fixes $\abra{x,\la^{(\btau+1)}}$ at time $\btau$ for every $x \in \supX{\btau+1}$. Thus,
\begin{align}\label{eqn:step_size_level_one_alpha_2}
\E\sbra{\abra{\U^{(\btau+1)} - \U^{(\btau)}, \la^{(\btau+1)}}^2 \mid \supF{\btau}}
&= \E\sbra{\abra{\E_{\lx\sim \gamma}\sbra{\lx\mid \lx\in \X^{(\btau+1)}} - \supu{\tau},\la^{(\btau+1)}}^2 \mid \supF{\btau}} 
\notag\\
&= \E\sbra{\E_{\lx\sim \gamma}\sbra{\abra{\lx-\supu{\btau},\la^{(\btau+1)}}^2\mid \lx\in \X^{(\btau+1)}} \mid \supF{\btau}} \notag \\
&= \E\sbra{\abra{\lx-\supu{\btau},\la^{(\btau+1)}}^2\mid \supF{\btau}},
\end{align}
where the last line follows from the tower property of conditional expectation.

Recall that $\supu{\btau} = \mu(\supX{\btau})$ is the center of mass. Moreover, the unit vector $\supa{\tau+1}$ is determined by $\supF{\tau}$ and also the conditional distribution of $\lx$ conditioned on $\supF{\tau}$ is that of $\lx \sim \gamma$ conditioned on $\lx \in \supX{\tau}$. Thus, using the fact that $\X^{(\btau)}$ is pairwise clean since Alice is in step 3(a), the right hand side in \Cref{eqn:step_size_level_one_alpha_2} is at most $\lambda$.

\paragraph*{Final Bound.} 
Substituting the above in \Cref{eqn:step_size_level_one_alpha_4}, we have 
\begin{align*} 
\E\sbra{\pbra{\Delta\supZ{\btau+1}_1}^2\mid \Fcal^{(\btau)}}
&\le \lambda\cdot \pbra{\balpha^{(\btau+1)}}^2 \le \lambda \cdot \vabs{\V^{(\btau)}-\V^{(\btau_{m-1})}}^2,
\end{align*}
where the second inequality follows from \Cref{step_size_level_one_alpha}. This completes the proof of the first statement.

For the moreover part, let us condition on the event $\btau_{m-1} < t <\btau_{m}$ where Alice speaks at time $t$. Note that such $t$ must all lie in the same phase of the protocol where Alice is the only one speaking. 
So, Bob's center of mass does not change from the time $\btau_{m-1}$ till $t$, i.e., $\V^{(t+1)}=\V^{(\btau_{m-1})}$. 
Thus we have $\Delta\supZ{t+1}_1=\abra{\U^{(t+1)}-\U^{(t)}, \Lambda\odot \V^{(\btau_{m-1})}}$. 
Analogous to \Cref{step_size_level_one},
the component of Alice's center of mass along the previous directions are fixed.
Thus $\abra{\U^{(t+1)}, \la^{(r)}}=\abra{\U^{(t)},\la^{(r)}}$ for all $r \le t$. Furthermore, by construction, $\Lambda \odot \V^{(\btau_{m-1})}$ lies in the linear subspace spanned by $\la^{(0)},\ldots,\la^{(\btau_{m-1} +1)}$. Therefore, since $\btau_{m-1}+1\le t$, it follows that $\Delta\supZ{t+1}_1=0$. 

\subsection[Expected Norm of Final Center of Mass]{Expected Norm of Final Center of Mass (Proof of \Cref{lem:expected_norm_level_one})}
\label{sec:expected_cleanup_depth}

Let $\BH_A = \BH_A^{(\D)}$ be the (random) linear subspace spanned by the vectors $\supa{0}, \ldots, \supa{\D}$ and similarly, let $\BH_B = \BH_B^{(\D)}$ be the linear subspace spanned by the vectors $\supb{0}, \ldots, \supb{\D}$. 
For any linear subspace $V$ of $\Rbb^n$, we denote by $\bPi_V$ and $\bPi_{V^\bot}$ the projectors on the subspace $V$ and its orthogonal complement $V^\bot$ respectively. 
Then, we have that
\[
\vabs{\supu{\D}}^2 = \vabs{\bPi_{H_A} \supu{\D}}^2 + \vabs{\bPi_{H_A^\bot} \supu{\D}}^2 
\text{ and } 
\vabs{\supv{\D}}^2 = \vabs{\bPi_{H_B} \supv{\D}}^2 + \vabs{\bPi_{H_B^\bot} \supv{\D}}^2.
\]

Note that the non-zero vectors in $(\supa{t})_t$ and $(\supb{t})_t$ form an orthonormal basis for the subspaces $\BH_A$ and $\BH_B$ respectively. Moreover, for each $t \le \D$, the inner product $\ip{x}{\supa{t}}$ is fixed for every $x \in \supX{\D}$ and the inner product $\ip{y}{\supb{t}}$ is also fixed for every $y \in \supY{\D}$ where $\supX{\D} \times \supY{\D}$ is the current rectangle determined by $\supF{\D}$. In particular, since $\supu{\D}$ is the center of mass of $\supX{\D}$, this implies that 
\begin{align*}
\vabs{\bPi_{H_A} \supu{\D}}^2 = \sum_{t=1}^{\D} \abra{\supu{\D},\la^{(t)}}^2 &= \sum_{t=1}^{\D} \pbra{\E_{\lx\sim \gamma} \sbra{\abra{\lx,\la^{(t)}}\mid \lx\in \X^{(\D)}}}^2\\
& = \sum_{t=1}^{\D} \E_{\lx\sim \gamma} \sbra{\abra{\lx,\la^{(t)}}^2\mid \lx\in \X^{(\D)}},
\end{align*}
where the second line follows from the inner product being fixed in $\X^{(\D)}$. 
Therefore, we have 
\[ 
\vabs{\supu{\D}}^2 = \underbrace{\sum_{t=1}^{\D} {\E_{\lx\sim \gamma} \sbra{\abra{\lx,\la^{(t)}}^2\mid \lx\in \X^{(\D)}}}}_{\bP_A} + \underbrace{\vabs{\bPi_{H_A^\bot} \supu{\D}}^2}_{\bQ_A}.
\]
In an analogous fashion, 
\[ 
\vabs{\supv{\D}}^2 = \underbrace{\sum_{t=1}^{\D} {\E_{\ly\sim \gamma} \sbra{\abra{\ly,\lb^{(t)}}^2\mid \ly\in \Y^{(\D)}}}}_{\bP_B} + \underbrace{\vabs{\bPi_{H_B^\bot} \supv{\D}}^2}_{\bQ_B}.
\]

We next show that both $\E[\bP_A+\bP_B]$ and $\E[\Q_A+\Q_B]$ are at most $O(d)$. 
The former follows from stopping time and concentration arguments laid out in the overview that there cannot be too many orthogonal directions where $\BE\sbra{\ip{\lx}{\supa{t}}^2}$ is large. 
The latter follows from an application of level-one inequalities.

We will bound the norm of the projection on the subspaces $\BH_A$ and $\BH_B$, which corresponds to the quantity $\BE[\bP_A+\bP_B]$, in \Cref{sec:level_one_inside_subspace} and bound the norm of the projection on the orthogonal subspaces $\BH_A^\bot$ and $\BH_B^\bot$,  which corresponds to the quantity $\BE[\Q_A+\Q_B]$, in \Cref{sec:level_one_inside_complement_subspace}.

\subsubsection{Projection on the Subspaces \texorpdfstring{$\BH_A$}{H\textunderscore A} and \texorpdfstring{$\BH_B$}{H\textunderscore B}}\label{sec:level_one_inside_subspace}

We shall show that the expected norm of the final center of mass when projected on the subspaces $\BH_A$ and $\BH_B$ is 
\[ \E[\bP_A+\bP_B] = O(d).\]

Towards this end, define the random variable $\K_t = \K_t(\lx,\ly) =\abra{\lx,\la^{(t)}}^2+ \abra{\ly,\lb^{(t)}}^2$ for each $t \in \Nbb$. 
Note that the vectors $\la^{(t)}$'s are being chosen adaptively depending on the previous inner products $\ip{\lx}{\la^{(\tau)}}$ for $\tau < t$, as well as the Boolean communication bits from step 3(b), thus they are functions of $\lx$ and $\ly$ as well here. Observe that
\[
\E\sbra{\bP_A+\bP_B}= \E\sbra{\sum_{t=1}^{\D} \E\sbra{\K_t \mid \supF{\D}}}= \E_{\lx,\ly\sim \gamma} \sbra{ \sum_{t=1}^{\D} \K_t}.
\]

We now divide the time sequence into successive intervals of different lengths $r\cdot 4d$ for $r=1,2, \ldots$.
Then we bound the expected sum of $\K_t$ within each time interval by $O(r d)$. 
We further argue that the probability that the stopping time $\D$ lies in the $r$-th interval is at most $2\cdot 2^{-r}$. In particular, for $r\in\Nbb$, letting interval 
$I_r=\cbra{\binom{r}{2}\cdot 4d+1,\ldots,\binom{r+1}{2}\cdot 4d}$,
which is of length $4dr$, we show the following.
\begin{claim}\label{lem:depth_tail_bound_level_one}
For any $r\in \Nbb$, we have 
\[ \E_{\lx,\ly\sim \gamma}
\sbra{\sum_{t\in I_r} \K_t \mid \D>\binom{r}{2}\cdot 4d}
\le 20d r
+4 \ln\pbra{\dfrac{1}{\Pr\sbra{\D>\binom{r}{2}\cdot 4d}}}.\]
\end{claim}

We shall prove the above claim later since it is the most involved part of the proof. The previous claim readily implies the following probability bounds.
\begin{claim}\label{lem:depth_tail_bound_level_one_part_two}
For any $r\in \Nbb$, we have $\Pr\sbra{\D >\binom{r}{2}\cdot 4d}\le 2\cdot 2^{-r}$.
\end{claim}

\begin{proof}[Proof of \Cref{lem:depth_tail_bound_level_one_part_two}] 
We bound $\Pr\sbra{\D > \binom{r}{2}\cdot 4d}$ by induction on $r$. The claim trivially holds for $r=1$. 

Now we proceed to analyze the event $\D\ge\tbinom{r+1}2\cdot4d$.
Observe that \Cref{clm:finite_steps_level_one} implies that there are at most $2d$ many step 3(a) and 3(b) throughout the protocol.
Thus if the event above occurs, there are at least $4dr-2d\ge 2dr$ many time steps $t \in I_r$ where the process is in step 3(c).

By the definition of the cleanup step, if $X \times Y$ is a rectangle determined\footnote{It suffices to consider such events since we have a product measure on $\supX{t} \times \supY{t}$ conditioned on $\supF{t}$ and $\D$ is a stopping time and is $\supF{t}$-measurable (i.e., determined by the randomness in $\supF{t}$).} by $\supF{t-1} \cap \{\D > \binom{r}{2}\cdot 4d\}$ where the process is in step 3(c) and Alice speaks, then
\[ 
\BE_{\lx \sim \gamma}\sbra{\K_t \mid (\lx,\ly) \in X \times Y} = \BE_{\lx \sim \gamma}\sbra{\ip{\lx}{\supa{t}}^2 \mid \lx\in X} \ge \BE_{\lx \sim \gamma}\sbra{\ip{\lx - \com(X)}{\supa{t}}^2 \mid \lx\in X} \ge \lambda, 
\]
where $\lambda=100$ is the cleanup parameter and $\com(X) = \BE_{\lx \sim \gamma}[\lx \midd \lx \in X]$ is the center of mass. This is because $\supa{t}$ is chosen to be a unit vector in a direction where the current set (conditioned on the history) is not pairwise clean. 
A similar statement holds if Bob speaks in step 3(c) for the random variable $\ip{\ly}{\supb{t}}^2$ where $\ly$ is sampled from $\gamma$ conditioned on $Y$. 

By the tower property of conditional expectation, the above implies that
\[
100\cdot2dr\cdot \Pr\sbra{\D > {\textstyle\binom{r+1}{2}}\cdot 4d \mid \D > {\textstyle\binom{r}{2}}\cdot 4d} \le \E\sbra{\sum_{t\in I_r} \K_t \mid \D > {\textstyle\binom{r}{2}}\cdot 4d}.
\]
Recall that \Cref{lem:depth_tail_bound_level_one} implies that the right hand side is at most $\le 20d r + 4 \ln\pbra{\frac1{\Pr[\D>\tbinom{r}{2} \cdot 4d]}}$.
We consider two cases:
\begin{enumerate}
\item[(i)] if $\Pr[\D>\binom{r}{2} \cdot 4d] \le 2^{-r}$, then clearly $\Pr[\D>\binom{r+1}{2} \cdot 4d] \le 2^{-r}$ as well as required;
\item[(ii)] otherwise $\Pr[\D>\binom{r}{2} \cdot 4d] \ge 2^{-r}$ and $20d r + 4\pbra{\frac1{\Pr[\D>\tbinom{r}{2} \cdot 4d]}} \le 20dr + 4r$, then it follows that 
\[ 
\Pr\sbra{\D > \textstyle\binom{r+1}{2}\cdot 4d \mid \D > \textstyle\binom{r}{2}\cdot 4d}\le 1/2,
\]
and by induction this implies that $\Pr\sbra{\D > \textstyle\binom{r+1}{2}\cdot 4d} \le 1/2\cdot\Pr\sbra{\D > \textstyle\binom{r}{2}\cdot 4d}\le2^{-r}$.\qedhere
\end{enumerate} 
\end{proof}

These claims imply that 
\begin{align*} 
\E[\bP_A+\bP_B] & \le \E\sbra{\sum_{r=0}^\infty 1\sbra{\D>{\textstyle\binom{r}{2}}\cdot 4d}\cdot \sum_{t\in I_r} \K_t}
\\&= \sum_{r=0}^\infty \Pr[ \D> {\textstyle\binom{r}{2}}\cdot 4d] \cdot\E\sbra{\sum_{t\in I_r} \K_t\mid \D > {\textstyle\binom{r}{2}}\cdot 4d} \\
&\le \sum_{r=0}^\infty\pbra{2^{1-r}\cdot O(r d)  +   4 \cdot\Pr[ \D> {\textstyle\binom{r}{2}}\cdot 4d] \cdot \ln\pbra{\tfrac{1}{ \Pr\sbra{\D> {\textstyle\binom{r}{2}}\cdot 4d}}}}\\
&\le \sum_{r=0}^\infty\pbra{2^{1-r}\cdot O(r d)  +  O\pbra{(r+1)2^{-r}}}\le O(d),
\end{align*}
where the last line uses the fact that $x\ln(1/x)\le O((r+1)2^{-r})$ for $0\le x\le2\cdot2^{-r}$ and $r\in\N$.
This proves the desired bound on $\E[\bP_A+\bP_B]$ assuming \Cref{lem:depth_tail_bound_level_one}  which we prove next.

\begin{proof}[Proof of \Cref{lem:depth_tail_bound_level_one}] 
To prove the claim, we need to analyze the expectation of $\sum_{t \in I_r} \K_t$ under $\lx, \ly$ sampled from $\gamma$ conditioned on the event $\D \ge \binom{r}{2} \cdot 4d$. 

We first describe an equivalent way of sampling from this distribution which will be easier for analysis. 
First, we recall that the definition of the cleanup protocol implies that the Boolean communication in $\bar \Ccal$ is solely determined by the previous Boolean communication, since it is specified by the original protocol $\tilde{\Ccal}$ (and thus $\Ccal$) before the cleanup. 

Let us fix any Boolean string $c\in\{0,1\}^*$ that is a valid Boolean transcript in the original communication protocol $\tilde \Ccal$.
This defines a rectangle $X_c\times Y_c\subseteq\Rbb^n\times\Rbb^n$ consisting of all pairs of inputs to Alice and Bob that result in the Boolean transcript $c$ in $\tilde\Ccal$.
If we sample $\lx,\ly\sim \gamma$ conditioned on $\D>\binom{r}{2}\cdot 4d$ and output the unique $(\X_c,\Y_c)$ such that $(\lx,\ly)\in \X_c\times \Y_c$, 
we obtain a distribution on rectangles. We use $\gamma(X_c\times Y_c\,|\,\D >\binom{r}{2}\cdot 4d)$ 
to denote the probability of obtaining $X_c\times Y_c$ by this sampling process so that $\sum_c \gamma(X_c\times Y_c\,|\,\D >\binom{r}{2}\cdot 4d)=1$. 

Now consider the following two-stage sampling process. First, we sample a rectangle $X_c \times Y_c$ according to the above distribution, and then we sample  the inputs $\lx, \ly$ sampled from $\gamma_n$ conditioned on the event that $\{(\lx,\ly)\in X_c \times Y_c\} \wedge \{\D> \binom{r}{2}\cdot 4d\}$. We shall show the following claim for any rectangle $X_c \times Y_c$ that could be sampled in the first step. 

\begin{claim}\label{eq:depth_tail_bound_level_one_eq1}
$\E_{\lx,\ly\sim \gamma}\sbra{\sum_{t\in I_r} \K_t \mid \D > 4d\tbinom{r}{2},(\lx,\ly)\in X_c\times Y_c } \le 12dr + 4\ln
\pbra{\tfrac{1}{\Pr[\D > 4d\tbinom{r}{2},(\lx,\ly)\in X_c\times Y_c]}}$.
\end{claim}

Assuming the above, and taking an expectation over $X_c\times Y_c$ drawn with probability $\gamma(X_c\times Y_c\,|\,\D >\binom{r}{2}\cdot 4d)$, we immediately obtain \Cref{lem:depth_tail_bound_level_one}:
\begin{align*} 
&\E_{\lx,\ly\sim \gamma}\sbra{\sum_{t\in I_r} \K_t \mid \D> {\textstyle\binom{r}{2}}\cdot 4d}\\
&\le 12dr + 4\cdot\sum_{\substack{c\in\{0,1\}^*,|c|\le d}} \gamma(X_c\times Y_c|\D >{\textstyle\binom{r}{2}}\cdot 4d)\cdot 
\pbra{\ln\pbra{\tfrac{1}{\gamma(X_c\times Y_c|\D >\binom{r}{2}\cdot 4d)}}+
\ln\pbra{\tfrac{1}{\Pr[\D >\binom{r}{2}\cdot 4d]}}}\\
&\le 12dr +  4\cdot \ln(3^d) + 4\cdot \ln\pbra{\tfrac{1}{\Pr[\D >\binom{r}{2}\cdot 4d]}} 
\tag{by concavity of $\ln(\cdot)$}
\\
&\le 20dr + 4\cdot \ln\pbra{\tfrac{1}{\Pr[\D >\binom{r}{2}\cdot  4d]}}.
\tag*{\qedhere}
\end{align*}
\end{proof}

To complete the proof, we now prove \Cref{eq:depth_tail_bound_level_one_eq1}.

\begin{proof}[Proof of \cref{eq:depth_tail_bound_level_one_eq1}]
Fix any $c$ such that $\gamma(X_c\times Y_c\,|\,\D >\binom{r}{2}\cdot 4d)>0$. 
We will bound the expectation of the quantity $\sum_{t\in I_r} \K_t = \sum_{t\in I_r} \abra{\lx,\la^{(t)}}^2 +\abra{\ly,\lb^{(t)}}^2$ where $\lx, \ly$ are sampled from $\gamma_n$ conditioned on the event that $\{(\lx,\ly)\in X_c \times Y_c\} \wedge \{\D > \binom{r}{2}\cdot 4d\}$.
Note that $\supa{t}, \supb{t},\D$ are functions of the previous messages of the protocol and hence also the inputs $\lx, \ly$. Once we condition on the above event, the Boolean communication is also fixed to be $c$.

To analyze the above conditioning, we first do a thought experiment and consider a different protocol that takes standard Gaussian inputs (without any conditioning) and show a tail bound for the random variable $\sum_{t \in I_r} \K_t$ for this new protocol. In the last step, we will use it to compute the expectation we ultimately want.

\paragraph*{Protocol $\Ccal_c$.} 
The protocol $\Ccal_c$ always communicates according to the fixed transcript $c$ in a Boolean communication step and otherwise according to the cleanup protocol $\bar\Ccal$ on any input $x,y$. Consider the random walk on this new protocol tree where the inputs $\lx, \ly \sim \gamma$ (without any conditioning). Let $(\Gcal^{(t)})_t$ be the associated filtration of the new protocol $\Ccal_c$ which can be identified with the collection of all partial transcripts till time $t$. Note that the vectors $\supa{t}$ and $\supb{t}$ in this new protocol are determined only by the previous real communication since the Boolean communication is fixed to $c$. This also implies that the vectors $\supa{t}$ and $\supb{t}$ form a predictable sequence with respect to the filtration $(\Gcal^{(t)})_t$. Moreover, by the definition of the protocol the next non-zero vector $\supa{\cdot}$ is chosen to be a unit vector orthogonal to the previously chosen $\supa{\cdot}$'s and the same holds for the vectors $\supb{\cdot}$.

We denote by $\K_t^{(c)}$ the random variable that captures $\K_t$ for the protocol $\Ccal_c$, i.e., $\K_t^{(c)} = \abra{\lx,\la^{(t)}}^2 +\abra{\ly,\lb^{(t)}}^2$ for $\lx, \ly \sim \gamma$ and $\la^{(t)}, \lb^{(t)}$ defined by the protocol $\Ccal_c$.
Observe that if $(\lx, \ly) \in X_c \times Y_c$ then $\K_t^{(c)} = \K_t$.

Consider the behavior of the protocol $\Ccal_c$ at some fixed time $t$. The nice thing about the protocol $\Ccal_c$ is that conditioned on all previous real messages for $\tau < t$, both $\lx$ and $\ly$ are standard Gaussian distributions on an affine subspace of $\R^n$ (defined by the previous messages).
Then, at time $t$, since $\supa{t}$ is orthogonal to the directions used in all previous real messages, it follows that the distribution of $\abra{\lx,\la^{(t)}}$ conditioned on any event in $\Gcal^{(t-1)}$ is an independent standard Gaussian for every $t$ if $\supa{t}$ is non-zero. The same holds for $\abra{\ly,\lb^{(t)}}$ as well. This last fact uses that the projection of a multi-variate standard Gaussian $\gamma_n$ in orthonormal directions yields independent real-valued standard Gaussians.

This implies that each new $\abra{\lx,\la^{(t)}}^2$ and $\abra{\ly,\lb^{(t)}}^2$ is an independent chi-squared random variable conditioned on the history (up to depth $\binom r2\cdot4d$) of the random walk. Therefore, \Cref{thm:chi_squared_concentration} implies that
\[ 
\Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t^{(c)}(\lx,\ly) \ge 2|I_r|+ s \mid \Gcal^{(\binom{r}{2}\cdot 4d)}} \le e^{-s/4}. 
\]
Since $|I_r|\le 4dr$, we have 
$\Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K^{(c)}_t(\lx,\ly) \ge 8dr+ s } \le e^{-s/4}.$

\paragraph*{Computing the Original Expectation.} 
Let us compare the probability of the above tail event in the original protocol $\bar\Ccal$ 
where inputs $\lx, \ly$ are sampled from  $\gamma$ conditioned on the event that $\{(\lx,\ly)\in X_c \times Y_c\} \wedge \{\D > \binom{r}{2}\cdot 4d\}$. 
We can write
\begin{align}\label{eq:tail_of_kt}
&\phantom{\le}
\Pr_{(\lx,\ly)\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t(\lx,\ly)\ge 8dr + s \mid \D> {\textstyle\binom{r}{2}}\cdot 4d, (\lx,\ly) \in X_c\times Y_c}\\
&= \frac{\Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t(\lx,\ly)\ge 8dr+ s,(\lx,\ly) \in X_c\times Y_c, \D > \binom{r}{2}\cdot 4d}}{ \Pr_{\lx,\ly\sim \gamma}\sbra{ (\lx,\ly)\in X_c\times Y_c, \D >\binom{r}{2}\cdot 4d}}.
\notag
\end{align}
We then bound the numerator by
\begin{align*}
&\phantom{\le}
\Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t(\lx,\ly)\ge 8dr+ s, (\lx,\ly) \in X_c\times Y_c, \D > {\textstyle\binom{r}{2}}\cdot 4d}\\
&= \Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t^{(c)}(\lx,\ly)\ge 8dr+ s, (\lx,\ly) \in X_c\times Y_c, \D > {\textstyle\binom{r}{2}}\cdot 4d}
\tag{if $(\lx,\ly)\in X_c \times Y_c$ then $\K_t^{(c)} = \K_t$}
\\
&\le  \Pr_{\lx,\ly\sim \gamma}\sbra{ \sum_{t\in I_r} \K_t^{(c)}(\lx,\ly)\ge 8dr+ s}  \le e^{-s/4}.
\end{align*}

Note that the inequality gives us an exponential tail on \Cref{eq:tail_of_kt}:
$$
\Cref{eq:tail_of_kt}
\le e^{-s/4}\cdot\pbra{\Pr_{\lx,\ly\sim \gamma}\sbra{ (\lx,\ly)\in X_c\times Y_c, \D >\binom{r}{2}\cdot 4d}}^{-1}.
$$
We can now integrate the above inequality to get an upper bound on the expected value of $\sum_{t \in I_r} \K_t$ under the distribution of interest. In particular, since for any non-negative random variable $\lw$, the following holds for any parameter $\alpha\ge0$:
\[ 
\BE[\lw] = \int_0^{+\infty} \Pr[\lw \ge z]\sd{z} 
\le \alpha + \int_\alpha^{+\infty} \Pr[\lw \ge z]\sd{z}
=\alpha + \int_0^{+\infty} \Pr[\lw \ge\alpha+z]\sd{z},
\]
we derive the following by taking $\alpha = 8dr+4\ln\pbra{\frac{1}{\Pr_{\lx,\ly\sim \gamma}\sbra{(\lx,\ly)\in X_c\times Y_c, \D >\binom{r}{2}\cdot 4d}}}$:
\begin{align*}
&\E_{(\lx,\ly)\sim \gamma}\sbra{ \sum_{i\in I_r} \K_t(\lx,\ly) \mid \D> {\textstyle\binom{r}{2}}\cdot 4d, (\lx,\ly)\in X_c\times Y_c } \\
&\qquad\le\alpha+\int_0^{+\infty}e^{-z/4}\sd z
=\alpha + 4\\
&\qquad \le 12dr + 4\ln\pbra{\dfrac{1}{\Pr_{\lx,\ly\sim \gamma}\sbra{ (\lx,\ly)\in X_c\times Y_c, \D >\binom{r}{2}\cdot 4d}}}.
\end{align*}
This completes the proof of \Cref{eq:depth_tail_bound_level_one_eq1}.
\end{proof}

\subsubsection{Projection on the Orthogonal Subspaces \texorpdfstring{$\BH_A^\bot$}{of H\textunderscore A} and \texorpdfstring{$\BH_B^\bot$}{of H\textunderscore B}}\label{sec:level_one_inside_complement_subspace}

We shall show that the expected norm of the final center of mass when projected on the subspaces $\BH_A^\bot$ and $\BH_B^\bot$ is 
\[ \E[\Q_A+\Q_B] = O(d).\]

Recall that $\bQ_{A} = \vabs{\bPi_{\BH_A^\bot} \supu{\D}}^2$ where $\BH_A$ is the (random) linear subspace spanned by the orthonormal set of vectors $\supa{0}, \ldots, \supa{\D}$ and $\BH_A^\bot$ its orthogonal complement. 
Moreover, the vectors $\supa{t}$ are determined by the previous Boolean and real communication. A similar statement holds for $\bQ_B$ and the vectors $\supb{t}$ as well.

The proof will follow in two steps. We will first show that one can bound the norm of the projection $\bPi_{\BH_A^\bot} \supu{d}$, which turns out to be the Gaussian center of mass of a set that lives in the subspace $\BH_A^\bot$, in terms of the logarithm of the inverse relative measure with respect to the subspace. Note that the Gaussian measure here is the Gaussian measure $\gamma_{\BH_A^\bot}$ on the subspace $\BH_A^\bot$. The case for $\bPi_{\BH_B^\bot} \supu{d}$ will be similar. The second step will use information theory-esque convexity argument to show that on average the logarithm of the inverse relative measure is small.

For the first part, we observe that if we sample $\lx,\ly \sim \gamma$ and take a random walk on this protocol tree, we obtain a probability measure over transcripts which includes both real and Boolean values. Recall that the Boolean transcript is determined by the original protocol and only depends on the previous Boolean communication and the real transcript is sandwiched between the Boolean communication. 
Let $\bell = (\bc, \br)$ denote the random variable representing the full transcript of the generalized protocol where $\bc$ is the Boolean communication and $\br$ is the real communication. For any given transcript $\bell$, let $\X_{\bell} \times \Y_{\bell}$ denote the corresponding rectangle consists of inputs reaching the leaf, and let $\X_{\bc}\times \Y_{\bc}$ (for $\X_{\bc},\Y_{\bc} \subseteq\Rbb^n$) denote the rectangle consisting of all pairs of inputs to Alice and Bob that result in the Boolean transcript $\bc$. Note that the real communication $\br$ together with $\bc$ fixes the subspaces $\BH_A$ and $\BH_B$ and particular affine shifts $\bs_A$ and $\bs_B$ of those subspaces depending on the value of the inner products determined by the full transcript. In particular, the rectangle $\X_{\bell} \times \Y_{\bell}$ consistent with the full transcript $\bell = (\bc,\br)$ is given by $\X_{\bell} = \X_{\bc} \cap (\BH_A + \bs_A)$ and $\Y_{\bell} = \Y_{\bc} \cap (\BH_B + \bs_B)$, i.e., taking (random) affine slices of the original sets.

Note that $\supu{\D}$ and $\supv{\D}$ are distributed as the center of masses of the final rectangle $\X_{\bell} \times \Y_{\bell}$, and thus is suffices to look at the rectangles for the rest of the argument. Since $\X_{\bell}$ (resp., $\Y_{\bell}$) lies in some affine shift of $\BH_A^\bot$ (resp., $\BH_B^\bot$), defining the relative center of mass for a set $A$ that lives in the ambient linear subspace $V$, as $\com_V(A) = \BE_{\lx \sim \gamma_V}[\lx \midd \lx \in A]$ where the Gaussian measure $\gamma_V$ is on the subspace $V$, it follows that
\begin{align*}
	 \E\sbra{\bQ_A +\bQ_B} &= \BE\left[\vabs{\bPi_{\BH_A^\bot} \supu{\D}}^2 + \vabs{\bPi_{\BH_A^\bot} \supu{\D}}^2\right] =  \BE_{\bell}\left[\|\com_{\BH_A^\perp}(\bPi_{\BH_A^\bot}\X_{\bell})\|^2 + \|\com_{\BH_B^\perp}(\bPi_{\BH_B^\bot}\Y_{\bell})\|^2\right].
\end{align*}

Recalling that $\gamma_\rel$ is the Gaussian measure of a set relative to its ambient space, we will show: 

\begin{claim}\label{eqn:relative-measure}
   $\|\com_{\BH_A^\perp}(\bPi_{\BH_A^\bot}\X_{\bell})\|^2 \le 2e^2 \ln\pbra{\dfrac{e}{\gamma_\rel\pbra{\X_{\bell}}}}$ and $\|\com_{\BH_B^\perp}(\bPi_{\BH_B^\bot}\Y_{\bell})\|^2 \le 2e^2 \ln\pbra{\dfrac{e}{\gamma_\rel\pbra{\Y_{\bell}}}}$.
\end{claim}
Note that we can ignore the case when $\gamma_\rel(\X_{\bell})$ is zero above, since we will eventually take an expectation over $\bell$ and almost surely this measure is non-zero.  

Using the previous claim, 
\begin{align*}
	 \E\sbra{\bQ_A +\bQ_B} &= \BE\left[\vabs{\bPi_{\BH_A^\bot} \supu{\D}}^2 + \vabs{\bPi_{\BH_A^\bot} \supu{\D}}^2\right] \le2e^2 \cdot \E_{\bell}\sbra{\ln\pbra{\frac{e}{\gamma_\rel\pbra{{\X_{\bell} \times \Y_{\bell}}}}}}.
\end{align*}

For the second step of the proof, we show the next claim which relies on convexity arguments to bound  the right hand side above by $O(d)$. This is similar in spirit to chain-style arguments from information theory.

\begin{claim}\label{claim:convexity}
$\E_{\bell}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel\pbra{{\X_{\bell} \times \Y_{\bell}}}}}} = O(d)$.
\end{claim}

This gives us the final bound $\E\sbra{\bQ_A +\bQ_B} = O(d)$ assuming the claims which we now prove.

\begin{proof}[Proof of \Cref{eqn:relative-measure}]
    
We can bound the norm of the above projection by an application of the Gaussian level-one inequality (\Cref{thm:level_k_ineq}), which, by rotational symmetry, implies that if $A$ is a subset of a linear subspace $V$ with non-zero measure, then 
\begin{align}\label{eqn:level-one-inequality}
    \|\com_V(A)\|^2 \le 2e^2 \ln\left(\frac{e}{\gamma_V(A)}\right),
\end{align}
where recall that $\com_V(A) = \BE_{\lx \sim \gamma_V}[\lx \midd \lx \in A]$ is the center of mass with respect to the Gaussian measure $\gamma_V$ on the subspace $V$.

If we run the generalized protocol on $\lx, \ly \sim \gamma$ and condition on getting the full transcript $\bell$, the conditional probability measure on $\bPi_{\BH_A^\bot}\lx$ is that of the Gaussian measure $\gamma_{\BH_A^\bot}$ conditioned on $\lx \in \X_{\bell} - \bs_A$ and $\bPi_{\BH_A^\bot}\ly$ is that of the Gaussian measure $\gamma_{\BH_B^\bot}$ conditioned on $\ly \in \Y_{\bell} - \bs_B$ and they are independent. 
This follows from the fact that so far the parties have fixed inner products along a basis for the orthogonal subspaces $\BH_A$ and $\BH_B$ and the fact the projection of a standard Gaussian on orthogonal subspaces are independent.

Thus, applying \Cref{eqn:level-one-inequality}, we have 
\begin{align*}
    \|\com_{\BH_A^\bot}(\bPi_{\BH_A^\bot}\X_{\bell})\|^2 \le 2e^2 \ln\left(\frac{e}{\gamma_{\BH_A^\bot}(\X_{\bell}-\bs_A)}\right) = 2e^2 \ln\left(\frac{e}{\gamma_\rel(\X_{\bell})}\right),
\end{align*}
where the last line follows since $\BH_A+\bs_A$ is the ambient space for $\X_{\bell}$ (this holds almost surely) and $\gamma_\rel(S) = \gamma_V(S-t)$ if $V+t$ is the ambient space of $S$. A similar argument proves the bound on $\|\com_{\BH_B^\bot}(\bPi_{\BH_B^\bot}\Y_{\bell})\|^2$.
\end{proof}

\begin{proof}[Proof of \Cref{claim:convexity}]
    For this claim, it will be convenient to consider a different generalized protocol $\Ccal'$ that generates the same distribution on the leaves $\bell$. In particular, since the Boolean messages in the generalized protocol $\bar\Ccal$ only depend on the previous Boolean messages, one can first send all the Boolean messages $\bc$, and then send all the real messages $\br$ choosing them according to the protocol $\bar\Ccal$ depending on the previous real messages and the (partial) Boolean transcript. Note that the protocol $\Ccal'$ generates the same distribution on the leaves $\bell$ when the inputs $\lx, \ly \sim \gamma_n$. In particular, the real communication only partitions \footnote{We remark that this protocol $\Ccal'$ suffices for proving this claim since we are looking only at the leaves. However, unlike \Cref{lem:step_size_square_level_one}, directly bounding the expected quadratic variation of the martingale corresponding to the protocol $\Ccal'$ is difficult.} each rectangles $X_c \times Y_c$ that corresponds to the Boolean transcript $c$ into affine slices.

    For rest of the claim, we now work with the protocol $\Ccal'$ where the Boolean communication happens first. To prove the claim, we condition on a Boolean transcript $\bc=c$ and by induction show that 
    \begin{align}\label{eqn:rectangle-measure}
          \BE_{\br}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{(c,\br)} \times \Y_{(c,\br)})}} \mid \bc=c} \le \ln\pbra{\dfrac{e}{\gamma_\rel(X_c \times Y_c)}},     
    \end{align}
     where $(c, r)$ is the full transcript and  $X_c \times Y_c$ is the rectangle containing all the inputs such that Boolean transcript is $c$. Note that $\gamma_\rel(X_c \times Y_c)$ is the probability of obtaining the Boolean transcript $c$ since the ambient space of $X_c$ and $Y_c$ is $\Rbb^n$.

    Then, taking expectation over the Boolean transcript $c$,
    \begin{align*}
         \BE_{\bell}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{\bell} \times \Y_{\bell})}}} &\le \BE_{\bc}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{\bc} \times \Y_{\bc})}}}  \\
         &= \sum_{\substack{c\in\{0,1\}^*,|c|\le d}} \Pr[\bc=c] \ln\pbra{\dfrac{e}{\Pr[\bc=c]}} \\
          & \le \ln(2e\cdot 2^d) = O(d),  
    \end{align*}
    where the last line follows from concavity.

    \paragraph*{Induction.} 
    To complete the proof, we now show \Cref{eqn:rectangle-measure} by induction. For this, let us look at an intermediate step $t$ in $\Ccal'$ where the Boolean communication is fixed to $c$ and Alice and Bob have exchanged some real messages $r_{<t} := r_1,\ldots, r_{t-1}$. Let the current rectangle be $X_{(c,r_{<t})} \times Y_{(c,r_{<t})}$ and it is Alice's turn to speak. Note that $X_{(c,r_{<t})}$ and $Y_{(c,r_{<t})}$ live in some affine subspaces at this point and in the current round, Alice sends the inner product of her input $x$ with a vector $a^{(t)}$ that is determined by the previous messages and orthogonal to the ambient space of $X_{(c,r_{<t})}$. At this step, Bob's set $Y_{(c,r_{<t})}$ does not change at all. 
    We shall show that in each step, the log of the inverse of the relative measure of the current rectangle does not increase on average over the next message:
    \begin{align}\label{eqn:rectangle-measure-one-step}
          \BE_{\br_{\le t}}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{(c,\br_{\le t})})}} \mid \bc=c, \br_{<t}=r_{<t}} \le \ln\pbra{\dfrac{e}{\gamma_\rel(X_{(c,r_{<t})})}},   
    \end{align}
   and an analogous statement holds when Bob speaks. 
   Taking an expectation over $\br_{<t}$, the above directly applies \eqref{eqn:rectangle-measure} by a straightforward backward induction:
    \begin{align*}
          \BE_{\br_{\le t}}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{(c,\br_{\le t})} \times \Y_{(c,{\br_{\le t})}})}} \mid \bc=c} &\le \BE_{\br_{<t }}\sbra{\ln\pbra{\dfrac{e}{\gamma_\rel(\X_{(c,\br_{< t})} \times \Y_{(c,{\br_{< t})}})}} \mid \bc=c}\\
          & \le \cdots \le  \ln\pbra{\dfrac{e}{\gamma_\rel(X_c \times Y_c)}}.   
    \end{align*}
    
   To see \Cref{eqn:rectangle-measure-one-step}, let us write $X := X_{(c,r_{<t})}$ for Alice's current set.  Recall that since we have fixed the history, Alice has fixed inner product with some orthogonal directions $a^{(1)}, \ldots, a^{(t-1)}$ and she has decided on the next direction $a := a^{(t)}$ along which she will send the next inner product.
   Thus, $X$ lives in some fixed affine subspace $V^\bot+s$ where $V$  is the span of $a^{(1)},\ldots,a^{(t-1)}$ and the next message $r := r_t = \ip{x}{a}$.
   Moreover, conditioned on the history till this point, the conditional probability distribution on Alice's input $\lx \in \Rbb^n$ can be described as follows: the projections corresponding to the non-zero vectors in the sequence $a^{(1)}, \ldots, a^{(t-1)}$, i.e., the inner products $\abra{\lx,a^{(\tau)}}$ where $a^{(\tau)}\neq0$ for $\tau < t$, are fixed according to the shift $s$, while the distribution on the orthogonal complement $V^\bot$ is that of the Gaussian measure $\gamma_{V^\bot}$ on the subspace $V^\bot$ after conditioning on the event that $\lx \in X - s$ (which lives in $V^\bot$). This uses that projections of a standard $n$-dimensional Gaussian in orthogonal directions are independent.

Let $k$ be the dimension of $V$ where $k<n$. Then, by doing a linear transformation, we may assume that $V^\bot= \Rbb^{n-k}$ (and thus $X \subseteq \Rbb^{n-k}$ and the shift $s$ fixes the coordinates $n-k+1$ through $n$) and $a = e_1$, i.e., in the current message Alice reveals the first coordinate of $\lx \in \Rbb^{n-k}$ where $\lx$ is sampled from $\gamma_{n-k}$ conditioned on $\lx \in X$. In this case, $\gamma_\rel$ in the left hand side of \Cref{eqn:rectangle-measure-one-step} is exactly $\gamma_\rel(X \cap \{x_1=r\})$ if Alice sends $r$ as the message, while for the right hand side of  \Cref{eqn:rectangle-measure-one-step}, we have $\gamma_\rel(X) = \gamma_{n-k}(X)$. Denoting by  $\sd\mu_{x_1}$ the probability density function of $\lx_1$, our statement boils down to showing
   \begin{align*}
       \int_{\Rbb} \ln\pbra{\dfrac{e}{\gamma_\rel(X \cap \{x_1=r\})}} \sd\mu_{x_1}(r) &\le  \ln\pbra{\dfrac{e}{\gamma_{n-k}(X)}}.  
   \end{align*}
   
    We show the above by explicitly writing the probability density function $\sd\mu_{x_1}$. Denote by $\sd\gamma_{n-k}(x_1,\ldots,x_{n-k})$ the standard Gaussian density function\footnote{Explicitly $\sd\gamma_m(x_1,\ldots,x_m)=\prod_{i=1}^m\sd\gamma_1(x_i)$ where $\sd\gamma_1(r) = \frac{1}{\sqrt{2\pi}} e^{-r^2/2}$ is the density function for one-dimensional standard Gaussian.} in $\Rbb^{n-k}$.
    The density function of the random vector $\lx$ sampled from $\gamma_{n-k}$ conditioned on $x \in X$, is given ${\gamma_{n-k}(X)}^{-1} \cdot {\sd\gamma_{n-k}(x_1,\ldots,x_{n-k})}$ for $x\in X$ and zero outside. Thus, we have 
   \begin{align*}
        \sd\mu_{x_1}(r) &= \frac{\int_{X \cap \{x_1=r\}} \sd\gamma_{n-k}(x_1,\ldots,x_{n-k})}{\gamma_{n-k}(X)}\\
        & = \sd\gamma_1(r) \cdot \frac{\int_{X \cap \{x_1=r\}} \sd\gamma_{n-k-1}(x_2,\ldots,x_{n-k})}{\gamma_{n-k}(X)}  = \sd\gamma_1(r) \cdot \frac{\gamma_\rel(X \cap \{x_1=r\})}{\gamma_{n-k}(X)}.   
   \end{align*}

   Then, by concavity, the left hand side of \Cref{eqn:rectangle-measure-one-step} is exactly given by
   \begin{align*}
       \int_{\Rbb} \ln\pbra{\dfrac{e}{\gamma_\rel(X \cap \{x_1=r\})}} \sd\mu_{x_1}(r) &\le  \ln\pbra{\int_{\Rbb} \dfrac{e}{\gamma_\rel(X \cap \{x_1=r\})} \sd\mu_{x_1}(r)}  \\
       &=  \ln\pbra{\dfrac{e}{\gamma_{n-k}(X)} \int_{\Rbb}  \sd\gamma_1(r)} = \ln\pbra{\dfrac{e}{\gamma_{n-k}(X)}}.  
       \qedhere
   \end{align*}

\end{proof}

