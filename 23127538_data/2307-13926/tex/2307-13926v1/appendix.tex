\appendix
\section{Gap-Hamming Lower Bounds}\label{app:thm:gap_hamming}

As an immediate consequence of~\Cref{thm:coin_problem}, we can derive optimal lower bounds against the Gap-Hamming problem as in~\Cref{thm:gap_hamming}.
\begin{proof}[Proof of~\Cref{thm:gap_hamming}]
Set $\rho=10/\sqrt{n}$. Fix the randomness to be any $r\in\bin^*$ and let $\Ccal_r$ refer to the deterministic protocol $\Ccal$ with randomness fixed to $r$. Suppose $d\le \tau \cdot n$ for a sufficiently small constant $\tau$, we apply \Cref{thm:coin_problem} on $\rho$ as well as $-\rho$, and apply triangle inequality to conclude that 
\[ 
\abs{\E_{\lz\sim \biased{\rho}}[h_r(\lz)]-\E_{\lz\sim \biased{-\rho}}[h_r(\lz)]}\le 2\cdot O\pbra{\sqrt{d/n}}< 1/9.
\]
Let $\sigma_\rho$ be the distribution of $(\lx,\ly)$ induced by sampling $\lx\sim \biased0$ and $\lz\sim \biased{\rho}$ and letting $\ly=\lx\odot \lz$, similarly define $\sigma_{-\rho}$ but with $\lz\sim \biased{-\rho}$. We now expand $h_r(z)$ in terms of $\Ccal(x,y)$, take an expectation over $r$ and apply triangle inequality to conclude that
\begin{equation}\label{eq:gap_hamming} 
\abs{\E_{(\lx,\ly)\sim \sigma_\rho}[\Ccal(\lx,\ly)]-\E_{(\lx,\ly)\sim \sigma_{-\rho}}[\Ccal(\lx,\ly)]}<1/9.
\end{equation}

Hoeffding's inequality implies that for $\lz\sim \biased{\rho}$, we have
\[ 
\Pr\sbra{\abs{\sum_i \lz_i - 10 \sqrt{n}}\ge 5\sqrt{n}}\le 2\exp\cbra{\tfrac{-2\cdot (5\sqrt{n})^2}{4n}} < 1/18.
\]
This implies that a random $(\lx,\ly)\sim \sigma_\rho$ is a \textsc{yes} instance of the Gap-Hamming problem with probability larger than $17/18$. Let $\tilde \sigma_\rho$ denote $\sigma_\rho$ conditioned on \textsc{Yes} instances of the Gap-Hamming problem. Similarly define $\tilde \sigma_{-\rho}$ to be $\sigma_{-\rho}$ conditioned on \textsc{No} instances of the Gap-Hamming problem. Since $\Ccal(x,y)$ has outputs in $[-1,1]$, we have 
\[ 
\abs{\E_{(\lx,\ly)\sim \sigma_\rho}[\Ccal(\lx,\ly)]-\E_{(\lx,\ly)\sim \tilde \sigma_\rho}[\Ccal(\lx,\ly)]}<1/9
\]
and
\[
\abs{\E_{(\lx,\ly)\sim \sigma_{-\rho}}[\Ccal(\lx,\ly)]-\E_{(\lx,\ly)\sim \tilde \sigma_{-\rho}}[\Ccal(\lx,\ly)]}<1/9. 
\]
This, along with \Cref{eq:gap_hamming} and triangle inequality, implies that 
\begin{equation*}
\abs{\E_{(\lx,\ly)\sim \tilde \sigma_\rho}[\Ccal(\lx,\ly)]-\E_{(\lx,\ly)\sim\tilde \sigma_{-\rho}}[\Ccal(\lx,\ly)]}< 1/3.
\end{equation*}
However, this contradicts the assumption that the protocol $\Ccal$ solves the Gap-Hamming problem with advantage at least $2/3$. 
\end{proof}

\section{Concentration for Sum of Squares of Quadratic Forms}\label{app:thm:quadratic_concentration}

Here we prove \Cref{thm:quadratic_concentration}.
While it follows from \cite[Theorem 6]{A20} which is a Banach space-valued version of the Hanson-Wright inequality, in our setting a weaker statement suffices, for which we give a self-contained proof following \cite{A20}.

For any integer $n\ge1$, we use $\Bcal^n=\cbra{x\in\Rbb^n\mid\vabs{x}\le1}$ to denote the unit Euclidean ball in $\Rbb^n$.
For any two sets $A,B\subseteq\Rbb^n$, we define $A+B=\cbra{x+y\mid x\in A,y\in B}$.
For any set $A\in\Rbb^n$ and any number $t\in\Rbb$, we define $tA=\cbra{t\cdot x\mid x\in A}$.
Let $\Phi\colon\Rbb\to[0,1]$ be the cumulative distribution function of the standard Gaussian distribution, i.e., $\Phi(a)=\frac1{\sqrt{2\pi}}\int_{-\infty}^ae^{-u^2/2}\sd u$.

Now we cite the famous Gaussian isoperimetric inequality \cite{borell1975brunn,sudakov1978extremal}.
\begin{theorem}[Gaussian Isoperimetric Inequality]\label{thm:gaussian_iso_ineq}
Let $A\subseteq\Rbb^n$ be a measurable set and assume $\gamma_n(A)\ge\Phi(a)$ for some $a\in\Rbb$.
Then for any $t\ge0$, we have $\gamma_n(A+t\Bcal^n)\ge\Phi(a+t)$.
\end{theorem}

In particular, if $\gamma_n(A)\ge1/2$, then we can pick $a=0$ in \Cref{thm:gaussian_iso_ineq} and have 
\begin{equation}\label{eq:gaussian_iso_ineq_1/2}
\gamma_n(A+t\Bcal^n)\ge\Phi(t)\ge1-e^{-t^2/2}.
\end{equation}

Now we are ready to prove \Cref{thm:quadratic_concentration}.

\begin{proof}[Proof of \Cref{thm:quadratic_concentration}]
Note that the bound is trivial when $m=0$. Thus from now on we assume without loss of generality $m\ge1$.

For each $x\in\Rbb^n$, let $K_x=\sum_{i=1}^m\abra{x\tensor x,M_i}^2$. 
We first write $K_x$ as a squared Euclidean norm of a vector:
\begin{itemize}
\item For $i\in[m]$, we view $M_i$ as a length-$n^2$ row vector.
\item Let $M\in\Rbb^{m\times n^2}$ be a matrix where the $i$-th row is $M_i$.
\end{itemize}
Therefore we have
\begin{equation}\label{eq:quadratic_concentration_k}
K_x=\vabs{M(x\tensor x)}^2=\vabs{M(x\otimes x)}^2,
\end{equation}
where $\otimes$ is the standard tensor product and the second equality follows since each $M_i$ has zero diagonal.

Define $f(y)=\vabs{M(y\otimes y)}$, $g(y)=\sup_{z\in\Sbb^{n-1}}\vabs{M(z\otimes y)}$, and $h(y)=\sup_{z\in\Sbb^{n-1}}\vabs{M(y\otimes z)}$.
Let $F=\E_{\ly\sim\gamma_n}[f(\ly)]$, $G=\E_{\ly\sim\gamma_n}[g(\ly)]$, and $H=\E_{\ly\sim\gamma_n}[h(\ly)]$ be their mean.
Define the set 
$$
A=\cbra{y\in\Rbb^n\mid f(y)<6F,\ g(y)<6G,\text{ and }h(y)<6H}.
$$
By Markov's inequality and union bound, we have the Gaussian measure of $A$ is $\gamma_n(A)\ge1/2$.
Then by \Cref{eq:gaussian_iso_ineq_1/2}, we have
\begin{equation}\label{eq:quadratic_concentration_1}
\gamma_n(A+t\Bcal^n)\ge1-e^{-t^2/2}
\quad\text{holds for all $t\ge0$.}
\end{equation}
Now for an arbitrary $x\in A+t\Bcal^n$, we write $x=y+tz$ where $y\in A$ and $z\in\Bcal^n$.
Then 
\begin{align*}
\vabs{M(x\otimes x)}
&\le\vabs{M(y\otimes y)}+t\cdot\vabs{M(y\otimes z)}+t\cdot\vabs{M(z\otimes y)}+t^2\cdot\vabs{M(z\otimes z)}\\
&<6F+6t(G+H)+t^2V,
\end{align*}
where $V=\sup_{z\in\Sbb^{n-1}}\vabs{M(z\otimes z)}$.
This, together with \Cref{eq:quadratic_concentration_k} and \Cref{eq:quadratic_concentration_1}, implies
\begin{equation}\label{eq:quadratic_concentration_2}
\Pr_{\lx\sim\gamma_n}\sbra{K_{\lx}\ge\pbra{6F+6t(G+H)+t^2V}^2}
\le\Pr_{\lx\sim\gamma_n}\sbra{\lx\notin A+t\Bcal^n}
=1-\gamma_n(A+t\Bcal^n)
\le e^{-t^2/2}.
\end{equation}
Now we calculate $F,G,H,V$ in the following claim, the proof of which will be presented later.
\begin{claim}\label{clm:fghv}
$F\le\sqrt{2m}$, $G,H\le\sqrt m$, and $V\le1$.
\end{claim}
Plugging \Cref{clm:fghv} into \Cref{eq:quadratic_concentration_2}, we have
$$
\Pr_{\lx\sim\gamma_n}\sbra{K_{\lx}\ge\pbra{6\sqrt{2m}+12t\sqrt m+t^2}^2}\le e^{-t^2/2}
\quad\text{holds for any $t\ge0$.}
$$
Now we set 
$$
t=\frac1{168}\sqrt{\frac r{m+\sqrt r}}\ge0
$$
and assume $r\ge98m$.
Then $6\sqrt{2m}\le\frac67\sqrt r$, $12t\sqrt m\le\frac1{14}\sqrt r$, and $t^2\le\frac1{14}\sqrt r$.
Therefore
\begin{equation*}
\Pr_{\lX\sim\gamma_n}\sbra{\sum_{i=1}^m\abra{\lX\tensor \lX,M_i}^2\ge r}
=\Pr_{\lx\sim\gamma_n}\sbra{K_{\lx}\ge r}
\le e^{-t^2/2}
=\exp\cbra{-\frac1{56448}\cdot\frac r{m+\sqrt r}}.
\tag*{\qedhere}
\end{equation*}
\end{proof}

Finally we present the missing proof of \Cref{clm:fghv}.
\begin{proof}[Proof of \Cref{clm:fghv}]
First we observe that rows of $M$ are unit vectors, therefore
\begin{equation}\label{eq:clm:fghv_1}
\frob{M}=\sqrt m.
\end{equation}
In addition, rows of $M$ are orthogonal to each other, therefore the operator norm of $M$ is 
\begin{equation}\label{eq:clm:fghv_2}
\opnorm{M}\le1.
\end{equation}

We index the columns of $M$ by $[n]^2$ and let the column vectors of $M$ be $\pbra{b_{i,j}}_{i,j\in[n]}$. 
Since rows of $M$ are flattened matrices with zero diagonal, we have
\begin{equation}\label{eq:clm:fghv_3}
b_{i,i}=0^m\quad\text{for all $i\in[n]$.}
\end{equation}
Now we bound $F,G,H,V$ separately.

\paragraph*{Bounding $F$.}
Observe that
\begin{align*}
F^2
&=\pbra{\E_{\ly\sim\gamma_n}\sbra{\vabs{M(\ly\otimes \ly)}}}^2
\le\E_{\ly\sim\gamma_n}\sbra{\vabs{M(\ly\otimes \ly)}^2}
=\E_{\ly\sim\gamma_n}\sbra{\vabs{\sum_{i,j\in[n]}b_{i,j}\ly_i\ly_j}^2}
\tag{by convexity}\\
&=\E_{\ly\sim\gamma_n}\sbra{\sum_{i,j,i',j'\in[n]}\abra{b_{i,j},b_{i',j'}}\ly_i\ly_j\ly_{i'}\ly_{j'}}
=\sum_{i,j\in[n]}\pbra{\vabs{b_{i,j}}^2+\abra{b_{i,j},b_{j,i}}}
\tag{by \Cref{eq:clm:fghv_3}}\\
&\le\sum_{i,j\in[n]}\pbra{\vabs{b_{i,j}}^2+\frac12\pbra{\vabs{b_{i,j}}^2+\vabs{b_{j,i}}^2}}
=2\sum_{i,j\in[n]}\vabs{b_{i,j}}^2\\
&=2\frob{M}^2=2m.
\tag{by \Cref{eq:clm:fghv_1}}
\end{align*}

\paragraph*{Bounding $G$ and $H$.}
Fix an arbitrary $y\in\Rbb^n$ and we first simplify $g(y)$.
For each $i\in[n]$, define vector $b_i=\sum_{j\in[n]}b_{i,j}y_j$ and let $B$ be the matrix with $b_i$'s as column vectors.
Then
\begin{equation}\label{eq:clm:fghv_4}
g(y)
=\sup_{z\in\Sbb^{n-1}}\vabs{\sum_{i,j\in[n]}b_{i,j}z_iy_j}
=\sup_{z\in\Sbb^{n-1}}\vabs{\sum_{i\in[n]}b_iz_i}
=\opnorm{B}
\le\frob{B}
=\sqrt{\sum_{i\in[n]}\vabs{\sum_{j\in[n]}b_{i,j}y_j}^2}.
\end{equation}
Now we bound $G$:
\begin{align*}
G^2
&=\pbra{\E_{\ly\sim\gamma_n}\sbra{g(\ly)}}^2
\le\E_{\ly\sim\gamma_n}\sbra{g(\ly)^2}
\tag{by convexity}\\
&\le\E_{\ly\sim\gamma_n}\sbra{\sum_{i\in[n]}\vabs{\sum_{j\in[n]}b_{i,j}\ly_j}^2}
=\E_{\ly\sim\gamma_n}\sbra{\sum_{i\in[n]}\sum_{j,j'\in[n]}\abra{b_{i,j},b_{i,j'}}\ly_j\ly_{j'}}
\tag{by \Cref{eq:clm:fghv_4}}\\
&=\sum_{i,j\in[n]}\vabs{b_{i,j}}^2=\frob{M}^2=m.
\tag{by \Cref{eq:clm:fghv_1}}
\end{align*}
Similar argument works for $H$.

\paragraph*{Bounding $V$.}
Note that for any $z\in\Sbb^{n-1}$, we have $\vabs{z\otimes z}=\vabs{z}^2=1$.
Thus, by \Cref{eq:clm:fghv_2}, we have
\begin{equation*}
V=\sup_{z\in\Sbb^{n-1}}\vabs{M(z\otimes z)}\le\opnorm{M}\le1.
\tag*{\qedhere}
\end{equation*}
\end{proof}

