\section{Directions Towards Further Improvements}\label{sec:future}

In this section we propose potential directions for further improving our second level bounds.
In \Cref{sec:lift}, we show that better Fourier growth bounds can be obtained from strong lifting theorems in a black-box way. This relies on the Fourier growth reductions in \Cref{sec:gadget}.
In \Cref{sec:improved-hw}, we examine the bottleneck in our analysis and identify major obstacles within.


\subsection{Better Lifting Theorems Imply Better Fourier Growth}\label{sec:lift}

Let $f:\pmone^n \to \pmone$ be a Boolean function. Let $g: \pmone^{m_1} \times \pmone^{m_2} \to \pmone$ be a gadget.
A lifting theorem connects the communication complexity of $f \circ g$ with the query complexity of $f$.
Some lifting theorems show that a low-cost communication protocol can be simulated by a low-cost query algorithm.

To be more precise, let $\Ccal: (\pmone^{m_1})^n \times (\pmone^{m_2})^n \to [-1,1]$ be a randomized two-party protocol.
Recall \Cref{def:g-fiber}, the $g$-fiber of $\Ccal$, denoted $\Ccal_{\downarrow g}(z): \pmone^{n} \to [-1,1]$, is defined by
$$
\Ccal_{\downarrow g}(z) = \E_{\bm{x}\sim\bar\unif_1, \bm{y}\sim \bar\unif_2}\left[ \Ccal(\bm{x},\bm{y})\mid g(\bm{x}_i, \bm{y}_i)=z_i,~\forall i\right].
$$
We say that $g$ satisfies a strong lifting theorem if for all randomized protocols $\Ccal$ of small communication bits, there is a randomized decision tree of small depth that approximates $\Ccal_{\downarrow g}$ on each input with error $1/\poly(n)$ (see e.g., \cite{GPW20}).

\begin{theorem}\label{thm:gadget}
Assume gadget $g\colon\binpm^{m_1}\times\binpm^{m_2}\to\binpm$ satisfies \Cref{as:balance}.
Assume for any randomized protocol $\Ccal\colon(\binpm^{m_1})^n\times(\binpm^{m_2})^n\to[-1,1]$ with at most $d$ bits of communication, there exists a randomized decision tree $\Tcal$ of depth at most $D$ that approximates $\Ccal_{\downarrow g}$ with pointwise error at most $1/n^k$, i.e.,
$$
\abs{\Tcal(z)-\Ccal_{\downarrow g}(z)}\le n^{-k}
\quad\forall z\in\binpm^n.
$$

Then, for any randomized protocol $\Ccal'\colon\binpm^n\times\binpm^n\to[-1,1]$ with at most $d$ bits of communication, its XOR-fiber $\Ccal'_{\downarrow\mathrm{XOR}}$ has level-$k$ Fourier growth
\begin{align*}
L_{1,k}(\Ccal'_{\downarrow\mathrm{XOR}})
&\le\pbra{\max_{S,T}|\hat g(S,T)|}^{-k}\cdot\sqrt{D^k\cdot O\pbra{\log(n)}^{k-1}}\\
&\le2^{(m_1+m_2)\cdot k/2}\cdot\sqrt{D^k\cdot O\pbra{\log(n)}^{k-1}}.
\end{align*}
\end{theorem}

As a simple corollary, we see that if the assumption of \Cref{thm:gadget} holds with $k=2$, $D= d \cdot \polylog(n)$, and a polylogarithmic-sized gadget $g$ (i.e., $2^{m_1},2^{m_2}\le\polylog(n)$), then the second level Fourier growth of the XOR-fiber of any randomized protocol of cost $d$ is at most $d\cdot\polylog(n)$ as desired.

We also remark that state-of-the-art lifting results hold with the gadget $g$ being either:
\begin{itemize}
\item The inner product on $m_1 = m_2 =  O(\log(n))$ bits~\cite{CFKMP19}. 
However, for such $g$ the largest Fourier coefficient squared is $1/\poly(n)$, which yields a trivial bound in Theorem~\ref{thm:gadget}.
\item The index function with $m_1 = \poly(n)$, $m_2 = \log(m_1)$~\cite{GPW20}.\footnote{For deterministic lifting, a better bound $m_1=O(n\log(n))$ is known \cite{lovett2022lifting}, but it doesn't suffice for our reduction.} 
In this case the largest Fourier coefficient squared is $1/m_1^2$, which again yields a trivial bound in Theorem~\ref{thm:gadget}.
Nonetheless, even a polynomial improvement on $m_1$, say $m_1 = n^{0.01}$, would give new non-trivial bounds in Theorem~\ref{thm:gadget} and in turn improves our lower bound on the XOR-lift of Forrelation.
\end{itemize}

\begin{proof}[Proof of \Cref{thm:gadget}]
Let $\Ccal\colon(\binpm^{m_1})^n\times(\binpm^{m_2})^n\to[-1,1]$ be a randomized protocol of cost at most $d$.
Then by assumption, $\Ccal_{\downarrow g}$ can be approximated up to error $1/n^k$ by a randomized decision tree $\Tcal$ of depth at most $D$.
Thus any Fourier coefficient of $\Ccal_{\downarrow g}$ and $\Tcal$ differs by at most $1/n^k$.
Therefore by the level-$k$ Fourier growth bounds on randomized decision trees \cite{Tal20,SSW21}, we have
$$
L_{1,k}(\Ccal_{\downarrow g})
\le \sum_{S\subseteq[n]:|S|=k}\pbra{n^{-k} + \abs{\hat{\Tcal}(S)}}
\le \sqrt{D^k\cdot O(\log(n))^{k-1}}.
$$
Since $\Ccal$ is arbitrary, the claimed bound for $\Ccal'_{\downarrow\mathrm{XOR}}$ follows from \Cref{thm:xor_to_g}.
 \end{proof}

\subsection{Sums of Squares of Quadratic Forms for Pairwise Clean Sets}
\label{sec:improved-hw}

In our analysis for the level-two bound, we showed that one can transform a general protocol to a $4$-wise clean protocol with parameter $\lambda = d\cdot\polylog(n)$ by adding $O(d)$ additional cleanup steps in expectation. If one could show that with essentially the same number of steps, one could take $\lambda = \polylog(n)$, then we would obtain the optimal level-two bound of $d \cdot \polylog(n)$.

We recall that to bound the number of cleanup steps, we rely on a concentration inequality for sums of squares of orthonormal quadratic forms (\Cref{thm:quadratic_concentration}), which says that if $M_1, \ldots, M_m$ are matrices with zero diagonal and form an orthonormal set when viewed as $n^2$ dimensional vectors,
then the random variable $\lQ = \sum_{i=1}^m \ip{\lX \tensor \lX}{M_i}^2$ satisfies $\Pr_{\lx \sim \gamma_n}[\lQ \ge t] \le e^{-\Omega(\sqrt{t})}$ for any  $t\gtrsim m^2$. 
Using this tail bound for $m= \Theta(d)$ and conditioning on $\lx \in X$ where $X$ is an arbitrary subset of $\Rbb^n$ with Gaussian measure $\approx 2^{-d}$, we obtained a bound $\BE_{\lx \sim \gamma}[\lQ \midd \lx \in X] \lesssim d^2$. 
This shows that there can be at most $O(d)$ such quadratic forms $M_i$'s where the value $\BE_{\lx \sim \gamma}\sbra{\ip{\lX \tensor \lX}{M_i}^2 \mid \lx \in X}$ can be larger than $d$ and hence, the reason we can only take $\lambda \approx d$. We note that the argument just described is for the non-adaptive setting, while in our case the $M_i$'s are also being chosen adaptively, so additional work is needed. 

The next example shows that the aforementioned statement is tight even in the non-adaptive setting where the $M_i$'s are fixed: in particular, there is a set $X$ of large measure and $\approx d$ such orthonormal quadratic forms where the above expectation after conditioning on $\lx \in X$ is $\Theta(d^2)$.

\begin{example} 
For $1\le i< j\le\sqrt d$, let $M_{ij} = E_{ij}$ for $i < j$ where $E_{ij}$ denotes the $n \times n$ matrix where only the $(i,j)$ entry is one. Note that the matrices $M_{ij}$ form an orthonormal set and they all have a zero diagonal. Let $X = \cbra{x \in \Rbb^n \mid |x_i| \gtrsim d^{1/4} \text{ for all $i \le d^{1/2}$}}$. Then, the Gaussian measure $\gamma(X) = 2^{-\Theta(d)}$ but 
\[ 
\BE_{\lx \sim \gamma}\sbra{ \sum_{1\le i< j\le \sqrt d} \ip{\lX \tensor \lX}{M_{ij}}^2 \mid \lx \in X} = \Theta(d^2).
\]
\end{example}

Note that the set $X$ in the example above is not pairwise clean and for our application, one can get around it by first ensuring that the protocol is pairwise clean and then proceeding with the 4-wise cleanup process. Motivated by this, we speculate that  when the set is pairwise clean, then the expected value of the sum of squares of orthonormal quadratic forms is much smaller unlike the example above.
Assuming such a statement and combining it with our ideas for handling the adaptivity suggests a potential way of improving the level-two bounds.