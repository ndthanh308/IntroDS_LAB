{
  "title": "A Model for Every User and Budget: Label-Free and Personalized Mixed-Precision Quantization",
  "authors": [
    "Edward Fish",
    "Umberto Michieli",
    "Mete Ozay"
  ],
  "submission_date": "2023-07-24T10:03:28+00:00",
  "revised_dates": [
    "2024-02-13T01:26:04+00:00"
  ],
  "abstract": "Recent advancement in Automatic Speech Recognition (ASR) has produced large AI models, which become impractical for deployment in mobile devices. Model quantization is effective to produce compressed general-purpose models, however such models may only be deployed to a restricted sub-domain of interest. We show that ASR models can be personalized during quantization while relying on just a small set of unlabelled samples from the target domain. To this end, we propose myQASR, a mixed-precision quantization method that generates tailored quantization schemes for diverse users under any memory requirement with no fine-tuning. myQASR automatically evaluates the quantization sensitivity of network layers by analysing the full-precision activation values. We are then able to generate a personalised mixed-precision quantization scheme for any pre-determined memory budget. Results for large-scale ASR models show how myQASR improves performance for specific genders, languages, and speakers.",
  "categories": [
    "cs.SD",
    "cs.CL",
    "eess.AS"
  ],
  "primary_category": "cs.SD",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12659",
  "pdf_url": null,
  "comment": "INTERSPEECH 2023. Code is available at https://github.com/SamsungLabs/myQASR",
  "num_versions": null,
  "size_before_bytes": 2335155,
  "size_after_bytes": 276019
}