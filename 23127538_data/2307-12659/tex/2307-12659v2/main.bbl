% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{yu2016automatic}
D.~Yu and L.~Deng, \emph{Automatic speech recognition}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2016, vol.~1.

\bibitem{malik2021automatic}
M.~Malik, M.~K. Malik, K.~Mehmood, and I.~Makhdoom, ``Automatic speech
  recognition: a survey,'' \emph{MTA}, vol.~80, no.~6, pp. 9411--9457, 2021.

\bibitem{baevski2020wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{NeurIPS},
  vol.~33, pp. 12\,449--12\,460, 2020.

\bibitem{radford2022robust}
A.~Radford, J.~W. Kim, T.~Xu, G.~Brockman, C.~McLeavey, and I.~Sutskever,
  ``Robust speech recognition via large-scale weak supervision,''
  \emph{arXiv:2212.04356}, 2022.

\bibitem{gholami}
A.~Gholami, S.~Kim, Z.~Dong, Z.~Yao, M.~W. Mahoney, and K.~Keutzer, ``A survey
  of quantization methods for efficient neural network inference,''
  \emph{arXiv:2103.13630}, 2021.

\bibitem{liang-survey}
T.~Liang, J.~Glossner, L.~Wang, S.~Shi, and X.~Zhang, ``Pruning and
  quantization for deep neural network acceleration: A survey,''
  \emph{Neurocomputing}, vol. 461, pp. 370--403, 2021.

\bibitem{nguyen}
H.~D. Nguyen, A.~Alexandridis, and A.~Mouchtaris, ``Quantization aware training
  with absolute-cosine regularization for automatic speech recognition.'' in
  \emph{Interspeech}, 2020, pp. 3366--3370.

\bibitem{mishchenko2019low}
Y.~Mishchenko, Y.~Goren, M.~Sun, C.~Beauchene, S.~Matsoukas, O.~Rybakov, and
  S.~N.~P. Vitaladevuni, ``Low-bit quantization and quantization-aware training
  for small-footprint keyword spotting,'' in \emph{ICMLA}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2019, pp. 706--711.

\bibitem{allenet2022disentangled}
T.~Allenet, D.~Briand, O.~Bichler, and O.~Sentieys, ``Disentangled loss for
  low-bit quantization-aware training,'' in \emph{CVPR}, 2022, pp. 2788--2792.

\bibitem{ding20224}
S.~Ding, P.~Meadowlark, Y.~He, L.~Lew, S.~Agrawal, and O.~Rybakov, ``4-bit
  conformer with native quantization aware training for speech recognition,''
  \emph{arXiv:2203.15952}, 2022.

\bibitem{bie2019simplified}
A.~Bie, B.~Venkitesh, J.~Monteiro, M.~Haidar, M.~Rezagholizadeh \emph{et~al.},
  ``A simplified fully quantized transformer for end-to-end speech
  recognition,'' \emph{arXiv:1911.03604}, 2019.

\bibitem{zhen22_interspeech}
K.~Zhen, H.~D. Nguyen, R.~Chinta, N.~Susanj, A.~Mouchtaris, T.~Afzal, and
  A.~Rastrow, ``{Sub-8-Bit Quantization Aware Training for 8-Bit Neural Network
  Accelerator with On-Device Speech Recognition},'' in \emph{Proc. Interspeech
  2022}, 2022, pp. 3033--3037.

\bibitem{fasoli22_interspeech}
A.~Fasoli, C.-Y. Chen, M.~Serrano, S.~Venkataramani, G.~Saon, X.~Cui,
  B.~Kingsbury, and K.~Gopalakrishnan, ``{Accelerating Inference and Language
  Model Fusion of Recurrent Neural Network Transducers via End-to-End 4-bit
  Quantization},'' in \emph{Proc. Interspeech 2022}, 2022, pp. 2038--2042.

\bibitem{liu2021post}
Z.~Liu, Y.~Wang, K.~Han, W.~Zhang, S.~Ma, and W.~Gao, ``Post-training
  quantization for vision transformer,'' \emph{NeurIPS}, vol.~34, pp.
  28\,092--28\,103, 2021.

\bibitem{nagel2020up}
M.~Nagel, R.~A. Amjad, M.~Van~Baalen, C.~Louizos, and T.~Blankevoort, ``Up or
  down? adaptive rounding for post-training quantization,'' in \emph{ICML},
  2020, pp. 7197--7206.

\bibitem{hubara2021accurate}
I.~Hubara, Y.~Nahshan, Y.~Hanani, R.~Banner, and D.~Soudry, ``Accurate post
  training quantization with small calibration sets,'' in \emph{ICML}, 2021,
  pp. 4466--4475.

\bibitem{nagel2019data}
M.~Nagel, M.~v. Baalen, T.~Blankevoort, and M.~Welling, ``Data-free
  quantization through weight equalization and bias correction,'' in
  \emph{ICCV}, 2019, pp. 1325--1334.

\bibitem{choi2021qimera}
K.~Choi, D.~Hong, N.~Park, Y.~Kim, and J.~Lee, ``Qimera: Data-free quantization
  with synthetic boundary supporting samples,'' \emph{NeurIPS}, vol.~34, pp.
  14\,835--14\,847, 2021.

\bibitem{aghli2021combining}
N.~Aghli and E.~Ribeiro, ``Combining weight pruning and knowledge distillation
  for cnn compression,'' in \emph{CVPR}, 2021, pp. 3191--3198.

\bibitem{gou2021knowledge}
J.~Gou, B.~Yu, S.~J. Maybank, and D.~Tao, ``Knowledge distillation: A survey,''
  \emph{IJCV}, vol. 129, no.~6, pp. 1789--1819, 2021.

\bibitem{jin2021kdlsq}
J.~Jin, C.~Liang, T.~Wu, L.~Zou, and Z.~Gan, ``{KDLSQ-BERT: A quantized bert
  combining knowledge distillation with learned step size quantization},''
  \emph{arXiv:2101.05938}, 2021.

\bibitem{yuan2022ptq4vit}
Z.~Yuan, C.~Xue, Y.~Chen, Q.~Wu, and G.~Sun, ``Ptq4vit: Post-training
  quantization for vision transformers with twin uniform quantization,'' in
  \emph{ECCV}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp.
  191--207.

\bibitem{li2022vit}
Z.~Li and Q.~Gu, ``I-vit: integer-only quantization for efficient vision
  transformer inference,'' \emph{arXiv:2207.01405}, 2022.

\bibitem{lin2022fq}
Y.~Lin, T.~Zhang, P.~Sun, Z.~Li, and S.~Zhou, ``{FQ-ViT: Post-training
  quantization for fully quantized vision transformer},'' in \emph{IJCAI},
  2022, pp. 1173--1179.

\bibitem{zharikov2022low}
I.~Zharikov, I.~Krivorotov, V.~Alexeev, A.~Alexeev, and G.~Odinokikh, ``Low-bit
  quantization of transformer for audio speech recognition,'' in
  \emph{Neuroinformatics}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2022, pp. 107--120.

\bibitem{wang2022deep}
N.~Wang, C.-C. Liu, S.~Venkataramani, S.~Sen, C.-Y. Chen, K.~El~Maghraoui,
  V.~Srinivasan, and L.~Chang, ``Deep compression of pre-trained transformer
  models,'' in \emph{NeurIPS}, 2022.

\bibitem{odinokikh2022low}
G.~Odinokikh, ``Low-bit quantization of transformer,'' in
  \emph{Neuroinformatics}, vol. 1064.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer Nature, 2022, p. 107.

\bibitem{li2022q}
Y.~Li, S.~Xu, B.~Zhang, X.~Cao, P.~Gao, and G.~Guo, ``{Q-ViT: Accurate and
  Fully Quantized Low-bit Vision Transformer},'' \emph{arXiv:2210.06707}, 2022.

\bibitem{kim2022integer}
S.~Kim, A.~Gholami, Z.~Yao, N.~Lee, P.~Wang, A.~Nrusimha, B.~Zhai, T.~Gao,
  M.~W. Mahoney, and K.~Keutzer, ``Integer-only zero-shot quantization for
  efficient speech recognition,'' in \emph{ICASSP}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2022, pp. 4288--4292.

\bibitem{yao2022zeroquant}
Z.~Yao, R.~Y. Aminabadi, M.~Zhang, X.~Wu, C.~Li, and Y.~He, ``Zeroquant:
  Efficient and affordable post-training quantization for large-scale
  transformers,'' \emph{arXiv:2206.01861}, 2022.

\bibitem{generative-diverse}
X.~Zhang, H.~Qin, Y.~Ding, R.~Gong, Q.~Yan, R.~Tao, Y.~Li, F.~Yu, and X.~Liu,
  ``Diversifying sample generation for accurate data-free quantization,'' in
  \emph{CVPR}, 2021, pp. 15\,658--15\,667.

\bibitem{dong2019hawq}
Z.~Dong, Z.~Yao, A.~Gholami, M.~W. Mahoney, and K.~Keutzer, ``Hawq: Hessian
  aware quantization of neural networks with mixed-precision,'' in \emph{ICCV},
  2019, pp. 293--302.

\bibitem{zhao2021automatic}
C.~Zhao, T.~Hua, Y.~Shen, Q.~Lou, and H.~Jin, ``{Automatic Mixed-Precision
  Quantization Search of BERT},'' in \emph{IJCAI}, 2021.

\bibitem{cai2020zeroq}
Y.~Cai, Z.~Yao, Z.~Dong, A.~Gholami, M.~W. Mahoney, and K.~Keutzer, ``{ZeroQ: A
  novel zero shot quantization framework},'' in \emph{CVPR}, 2020, pp.
  13\,169--13\,178.

\bibitem{yu2020low}
H.~Yu, T.~Wen, G.~Cheng, J.~Sun, Q.~Han, and J.~Shi, ``Low-bit quantization
  needs good distribution,'' pp. 680--681, 2020.

\bibitem{9772057}
S.~B. Eryilmaz and A.~Dundar, ``Understanding how orthogonality of parameters
  improves quantization of neural networks,'' \emph{IEEE TNNLS}, pp. 1--10,
  2022.

\bibitem{chen2021towards}
W.~Chen, P.~Wang, and J.~Cheng, ``Towards mixed-precision quantization of
  neural networks via constrained optimization,'' in \emph{ICCV}, 2021, pp.
  5350--5359.

\bibitem{dong2020}
Z.~Dong, Z.~Yao, D.~Arfeen, A.~Gholami, M.~W. Mahoney, and K.~Keutzer,
  ``Hawq-v2: Hessian aware trace-weighted quantization of neural networks,''
  \emph{NeurIPS}, vol.~33, pp. 18\,518--18\,529, 2020.

\bibitem{easyquant}
D.~Wu, Q.~Tang, Y.~Zhao, M.~Zhang, Y.~Fu, and D.~Zhang, ``{EasyQuant:
  Post-training quantization via scale optimization},''
  \emph{arXiv:2006.16669}, 2020.

\bibitem{panayotov2015librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``{Librispeech: An ASR
  corpus based on public domain audio books},'' in \emph{ICASSP}, 2015, pp.
  5206--5210.

\bibitem{fleurs}
A.~Conneau, M.~Ma, S.~Khanuja, Y.~Zhang, V.~Axelrod, S.~Dalmia, J.~Riesa,
  C.~Rivera, and A.~Bapna, ``Fleurs: Few-shot learning evaluation of universal
  representations of speech,'' \emph{arXiv:2205.12446}, 2022.

\bibitem{speech-commands}
P.~Warden, ``Speech commands: A dataset for limited-vocabulary speech
  recognition,'' \emph{arXiv:1804.03209}, 2018.

\bibitem{fairseq}
C.~Wang, Y.~Tang, X.~Ma, A.~Wu, D.~Okhonko, and J.~Pino, ``{FAIRSEQ S2T: Fast
  speech-to-text modeling with FAIRSEQ},'' \emph{arXiv:2010.05171}, 2020.

\bibitem{nni_link_observer_num_samples}
``{NNI Documentation},''
  \url{https://nni.readthedocs.io/en/latest/_modules/nni/compression/pytorch/quantization/observer_quantizer.html},
  {Accessed: 2023-01-25}.

\end{thebibliography}
