\documentclass{article}

\usepackage{arxiv}
\usepackage{graphicx}
\usepackage{bbold}
\usepackage{optidef}
%\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,titling,url,array}
\usepackage{multirow}
\let\proof\relax
\let\endproof\relax
\let\example\relax
\let\endexample\relax
\usepackage{amsthm}
%\usepackage[tbtags]{amsmath}
%\usepackage[ruled,vlined]{algorithm2e}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem*{cor}{Corollary}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{optidef}
\usepackage{hyperref}
\usepackage{nicematrix}
\usepackage{xfrac} 
\usepackage{caption}
\usepackage{subcaption}
\usepackage{comment}
\usepackage[ruled,linesnumbered]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}
%\NiceMatrixOptions{transparent,nullify-dots}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=black,
}
\usepackage[numbers]{natbib}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsmath}
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}		% Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{doi}

\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
\title{Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems}

%\date{September 9, 1985}	% Here you can change the date presented in the paper title
%\date{} 					% Or removing it

\author{ \href{https://orcid.org/0000-0003-0380-4197}{% Figure removed\hspace{1mm}Dai Hai Nguyen}\\%\thanks{Use footnote for providing further
	%	information about author (webpage, alternative
	%	address)---\emph{not} for acknowledging funding agencies.} \\
	Department of Computer Science\\
	University of Tsukuba, Japan\\
	\texttt{hai@cs.tsukuba.ac.jp} \\
	%% examples of more authors
    	\And
	\href{https://orcid.org/0000-0002-5789-7547}{% Figure removed\hspace{1mm}Tetsuya Sakurai} \\
    Department of Computer Science\\
	University of Tsukuba, Japan\\
	\texttt{sakurai@cs.tsukuba.ac.jp} \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
	%% \And
	%% Coauthor \\
	%% Affiliation \\
	%% Address \\
	%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{MYVT: A Framework For Solving Regularized Distributional Optimization Problems}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
pdftitle={A template for the arxiv style},
pdfsubject={q-bio.NC, q-bio.QM},
pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
\maketitle


\begin{abstract}
We consider a general optimization problem of minimizing a composite objective functional defined over a class of probability distributions. The objective is composed of two functionals: one is assumed to possess the variational representation and the other is expressed in terms of the expectation operator of a possibly nonsmooth convex regularizer function. Such a regularized distributional optimization problem widely appears in machine learning and statistics, such as proximal Monte-Carlo sampling, Bayesian inference and generative modeling, for regularized estimation and generation.


We propose a novel method, dubbed as \textbf{M}oreau-\textbf{Y}oshida \textbf{V}ariational \textbf{T}ransport (\textbf{MYVT}), for solving the regularized distributional optimization problem. First, as the name suggests, our method employs the Moreau-Yoshida envelope for a smooth approximation of the nonsmooth function in the objective. Second, we reformulate the approximate problem as a concave-convex saddle point problem by leveraging the variational representation, and then develope an efficient primal-dual algorithm to approximate the saddle point. Furthermore, we provide theoretical analyses and report experimental results to demonstrate the effectiveness of the proposed method.
%Such as distributional optimization problem widely appears in machine learning and statistics, such as proximal Monte-Carlo sampling, variational inference, synthetic sample generation, where we wish the generated samples to be regularized.  
\end{abstract}

\section{Introduction}
Many tasks in machine learning and computational statistics are posed as distributional optimization problems, where the goal is to optimize a functional $F:\mathcal{P}_{2}(\mathcal{X})\rightarrow \mathbb{R}$ of probability distributions: $\min_{q\in \mathcal{P}_{2}(\mathcal{X})}F(q)$, where $\mathcal{P}_{2}(\mathcal{X})$ denotes the set of probability distributions defined on the domain $\mathcal{X}$ ($\subset \mathbb{R}^{d}$) with finite second-order moment. Examples of this formulation include many well-known problems such as Bayesian inference (e.g. variational autoencoder \cite{kingma2014stochastic}) and synthetic sample generation (e.g. generative adversarial networks \cite{goodfellow2020generative}). Basically, these models aim to approximate a target distribution by generating samples (or also called particles) in a manner that minimizes the dissimilarity between the empirical probability distribution obtained from the samples and the target distribution. The dissimilarity function typically involves measures such as Kullback-Leiber (KL) divergence, Jensen-Shanon (JS) divergence or Wasserstein distance in the optimal transport \cite{villani2009optimal}.

In this paper, we consider a general class of distributional optimization problems with a composite objective functional of the following form:
\begin{align}
\label{eqn:regDP}
    \min_{q\in \mathcal{P}_{2}(\mathcal{X})}\left\{ G(q)\coloneqq F(q) + \alpha\mathbb{E}_{\textbf{x}\sim q}\left[g(\textbf{x})\right] \right\}
\end{align}
where $\mathbb{E}$ denotes the expectation operator and $g:\mathbb{R}^{d}\rightarrow \mathbb{R}$ denotes a possibly nonsmooth convex regularizer function for $\textbf{x}\in \mathcal{X}$ and $\alpha$ is a constant. Two choices of the function $g$ are the $l_{1}$-norm function $g(\textbf{x})=|\textbf{x}|$ which encourages sparse samples (with many elements equal 0), and one-dimensional total variation semi-norm $g(\textbf{x})=\sum_{i=2}^{d}|\textbf{x}_{i}-\textbf{x}_{i-1}|$ which encourages sparsity of the difference between nearby elements (i.e. local constancy of elements). Solving problem (\ref{eqn:regDP}) could be challenging due to the non-smoothness of the function $g$.

An example of the above formulation is the proximal Markov Chain Monte Carlo (MCMC) sampling \cite{pereyra2016proximal,durmus2018efficient}, which exploits convex analysis to obtain samples efficiently from log-concave density of the following form: $\text{exp}(-U(\textbf{x}))$, where $U(\textbf{x})=f(\textbf{x})+g(\textbf{x})$, $f:\mathbb{R}^{d}\rightarrow \mathbb{R}$ is a smooth convex function while $g$ is the nonsmooth convex function. By employing KL divergence to quantify the difference between the empirical probability distribution $q$  obtained from the current samples and the target distribution, the proximal MCMC sampling can be regarded as a specific instance of problem (\ref{eqn:regDP}).

We are particularly focused on devising an efficient algorithm for solving problem (\ref{eqn:regDP}) when the functional $F$ has a variational represenation in the following form:
%We are especially interested in developing efficient algorithm for solving problem (\ref{eqn:regDP}) when the functional $F$ admits a variational representation of the following form:
\begin{align}
\label{eqn:varform}
    F(q) = \sup_{h\in\mathcal{H}}{\left\{ \mathbb{E}_{\textbf{x}\sim q}\left[ h(\textbf{x})\right]-F^{*}(h)\right\}}
\end{align}
where $\mathcal{H}$ is a class of square-integrable functions on $\mathcal{X}$ with respect to the Lebesgue measure and $F^{*}:\mathcal{H}\rightarrow \mathbb{R}$ is a convex conjugate functional of $F$. For instance, when $F$ is the KL divergence, its variational representation is defined as:
\begin{align}
\label{eqn:varformKL}
    KL(q,\pi) = \sup_{h\in\mathcal{H}}{\left\{\mathbb{E}_{\textbf{x}\sim q}\left[ h(\textbf{x})\right]-\log\mathbb{E}_{\textbf{x}\sim \pi} \left[e^{h(\textbf{x})}\right]\right\}}
\end{align}
where $\pi$ denotes the target distribution. The solution to this problem can be estimated using samples from $q$ and $\pi$. In general, directly optimizing $F$ (problem (\ref{eqn:regDP}) with $\alpha=0$) can be achieved through an iterative algorithm called Wasserstein Gradient Descent \cite{santambrogio2015optimal}. This algorithm involves two steps in each iteration: 1) computing Wasserstein gradient of $F$ based on the current probability distribution and 2) performing an exponential mapping on $\mathcal{P}_{2}(\mathcal{X})$, the space of probability distributions. Variational Transport (VT) \cite{liu2021infinite} is introduced as a method to minimize $F$ by approximating a probability distribution using a set of particles. It leverages the variational representation of $F$ to update particles. Specifically, VT solves problem (\ref{eqn:varform}) by utilizing particles to approximate the Wasserstein gradient of $F$. The obtained solution is then used to push each particle in a specified direction. This process can be considered as a forward discretization of the Wasserstein gradient flow. 

However, when $\mathcal{X}$ is a constrained domain, VT may push the particles outside of the domain when following the direction given by the solution of (\ref{eqn:varform}). To address this issue, MirrorVT \cite{nguyen2023mirror} is introduced. The main idea behind mirrorVT is to map the particles from constrained domain (primal space) to an unconstrained domain (dual space), induced by a mirror map. Then, an approximate Wassrestein gradient descent is performed on the space of probability distributions defined over the dual space to update each particle, similar to VT. At the end of each iteration, the particles are mapped back to the original constrained domain.\\



\noindent
\textbf{Contributions}. We propose a novel method, named \textbf{M}oreau-\textbf{Y}oshida \textbf{V}ariational \textbf{T}ransport (\textbf{MYVT})\footnote{The code can be found at \url{https://github.com/haidnguyen0909/MYVT} after the acceptance of the paper.}, for solving problem (\ref{eqn:regDP}). 
Our method tackles the non-smoothness of $g$ in the regularization term by employing the Moreau-Yoshida envelope \cite{rockafellar2009variational} to obtain a smooth approximation of $g$. By leveraging the variational representation of $F$, we reformulate the original problem as a concave-convex saddle point problem and develop an efficient primal-dual algorithm to approximate the saddle point. In contrast to the particle-based methods \cite{liu2021infinite, nguyen2023mirror}, MYVT employs a neural network to represent the probability distribution $q$ and generate the particles. The network parameters are trained to optimize the objective of problem (\ref{eqn:regDP}). This approach addresses the issue of limited approximation capacity and the need for significant memory resources to store a large number of particles in particle-based methods. Furthermore, we provide theoretical analyses and conduct experiments on synthetic datasets to verify the effectiveness of the proposed MYVT method. 

\section{Related Works}
Our work is related to the research on methods for sampling distributions with nonsmooth convex composite potentials, which involve the sum of a continuously differentiable function and a possibly nonsmooth function. In particular, \cite{pereyra2016proximal,durmus2018efficient} introduced the proximal MCMC algorithm for quantifying uncertainty in Bayesian imaging application. They focused on the total-variation semi-norm \cite{rudin1992nonlinear} and $l_{1}$-norm \cite{park2008bayesian} which encourage the generation of parameter estimates with specific structural properties. To handle the non-smoothness of the penalties, they utilized the Moreau-Yoshida envelope to obtain a smooth approximation to the total-variation semi-norm and $l_{1}$-norm. The Langevin algorithm was then employed to generate samples from the smoothed posterior distribution.

Furthermore, our method builds on VT \cite{liu2021infinite}, which leverages the optimal transport framework and variational representation (\ref{eqn:varform}) of $F$ to solve problem (\ref{eqn:regDP}) without regularization via particle approximation. In each iteration, VT estimated the Wasserstein gradient of $F$ by solving a variational maximization problem associated with $F$ and the current particles. It then performed Wasserstein gradient descent by moving particles in a direction specified by the estimated Wasserstein gradient. The advantage of VT was its ability to optimize $F$ beyond commonly targeted KL divergence in MCMC sampling methods.

When $g$ represents the indicator function of a set, problem (\ref{eqn:regDP}) transforms into a constrained distributional optimization problem, where the probability distributions are defined over a constrained domain. Applying VT to this problem may lead to particles moving outside the constrained domain. MirrorVT \cite{nguyen2023mirror} addressed this issue by mapping particles from the constrained domain to the unconstrained one through a mirror map, which is inspired by the Mirror Descent algorithm originally designed for constrained convex optimization \cite{beck2003mirror}. 

\section{Preliminaries}
%In this section, we describe definitions of important notions of our %proposed method and state some related properties of such notions.
We first review notions from convex analysis essential for our proposed method, specifically Moreau-Yoshida envelopes and proximal operators. Then, we review some concepts in optimal transport and Wasserstein space, and also state some related properties. We lastly summarize VT.
\subsection{Moreau-Yoshida Envelopes and Proximal Operators}
\begin{defn}
    (Moreau-Yoshida envelope) Given a convex function $g:\mathbb{R}^{d}\rightarrow \mathbb{R}$ and a positive scaling parameter $\lambda$, the Moreau-Yoshida envelope of $g$, denoted by $g^{\lambda}$, is given by:
    \begin{align}
    \label{def:envelope}
    g^{\lambda}(\textbf{x})=\inf_{\textbf{y}\in \mathbb{R}^{d}}\left\{ g(\textbf{y})+\frac{1}{2\lambda}\lVert \textbf{x}-\textbf{y} \rVert^{2}\right\}
    \end{align}
    
\end{defn}
\noindent
The infimum of (\ref{def:envelope}) is always uniquely attained and the minimizer defines the proximal map of $g$:
\begin{defn}
    \label{def:proxmap}
    (Proximal Map) The proximal map of $g$, denoted by $\texttt{prox}^{\lambda}_{g}$, is given by:
    \begin{align}
    \texttt{prox}^{\lambda}_{g}(\textbf{x})=\argmin_{\textbf{y}\in \mathbb{R}^{d}}\left\{ g(\textbf{y})+\frac{1}{2\lambda}\lVert \textbf{x}-\textbf{y} \rVert^{2}\right\}
    \end{align}
\end{defn}
\noindent
The approximation $g^{\lambda}$ inherits the convexity of $g$ and is always continuously differentiable. In particular, $g^{\lambda}$ is gradient Lipschitz: for $x,y\in \mathbb{R}^{d}$,
\begin{align*}
    \lVert \nabla g^{\lambda}(\textbf{x})- \nabla g^{\lambda}(\textbf{y}) \rVert \leq \frac{1}{\lambda}\lVert \textbf{x}-\textbf{y} \rVert
\end{align*}
where the gradient $\nabla g^{\lambda}(\textbf{x})$ is given by:
\begin{align}
    \nabla g^{\lambda}(\textbf{x})=\frac{1}{\lambda}\left( \textbf{x} - \texttt{prox}^{\lambda}_{g}(\textbf{x})\right)
\end{align}
Most importantly, $g^{\lambda}(\textbf{x})$ converges pointwise to $g(\textbf{x})$ as $\lambda$ tends to zero \cite{rockafellar2009variational}.
\subsection{Optimal transport and Wasserstein space}
Optimal Transport \cite{villani2009optimal} has received much attention in the machine learning community and has been shown to be an effective tool for comparing probability distributions in many applications \cite{petric2019got,nguyen2021learning,nguyen2023linear}.
Formally, given a measurable map $T:\mathcal{X}\rightarrow \mathcal{X}$ and $p\in \mathcal{P}_{2}(\mathcal{X})$, we say that $q$ is the \textit{push-forward measure} of $p$ under $T$, denoted by $q=T\sharp p$, if for every Borel set $E\subseteq \mathcal{X}$, $q(E)=p(T^{-1}(E))$. For any $p,q\in \mathcal{P}_{2}(\mathcal{X})$, the $2$-Wasserstein distance $\mathcal{W}_{2}(p,q)$ is defined as:
\begin{align*}
   \mathcal{W}_{2}^{2}(p, q)= \inf_{\pi \in \Pi(p,q)} \int_{\mathcal{X}\times \mathcal{X}} \lVert \textbf{x} - \textbf{x}^\prime\rVert_{2}^{2}\mathrm{d}\pi(\textbf{x},\textbf{x}^\prime)
\end{align*}
where $\Pi(p,q)$ is all probability measures on $\mathcal{X}\times \mathcal{X}$ whose two marginals are equal to $p$ and $q$, $\lVert\cdot \rVert_{2}$ denotes the Euclidean norm. It is known that the metric space $(\mathcal{P}_{2}(\mathcal{X}), \mathcal{W}_{2})$, also known as Wasserstein space, is an infinite-dimensional geodesic space \cite{villani2009optimal}. 

\begin{defn}
    \label{def:firstvariational}
    (The first variation of functional) 
    the first variation of $F$ evaluated at $p$, denoted by $\partial F(p)/\partial p:\mathcal{X}\rightarrow \mathbb{R}$, is  given as follows:
\begin{align*}
    \lim_{\epsilon\rightarrow 0}\frac{1}{\epsilon} \left( F(p+\epsilon \chi) - F(p) \right)=
    \int_{\mathcal{X}} \frac{\partial F(p)}{\partial p}(\textbf{x})\chi(\textbf{x})\mathrm{d}\textbf{x}
\end{align*}
for all $\chi=q-p$, where $q\in \mathcal{P}_{2}(\mathcal{X})$.
\end{defn}
With mild regularity assumptions, the Wasserstein gradient of $F$, denoted by $\texttt{grad}F$, relates to the gradient of the first variation of $F$ via the following continuity equation:
\begin{align}
    \texttt{grad}F(p)(\textbf{x})=-\texttt{div}\left(p(\textbf{x})\nabla\frac{\partial F(p)}{\partial p}(\textbf{x})\right)
\end{align}
for all $\textbf{x}\in \mathcal{X}$, where $\texttt{div}$ denotes the divergence operator.  

\begin{defn}
    \label{def:stronglyconvex}
    (Geodesically strong convexity) 
    If $F$ is geodesically $\mu$-strongly convex with respect to the $2$-Wasserstein distance, then for $\forall p$, $p^\prime \in \mathcal{P}_{2}(\mathcal{X})$, we have: 
\begin{align*}
    %\label{assumption:strongconvexity}
     F(p^\prime) &\geq F(p) + \langle \texttt{grad}F(p), \texttt{Exp}_{p}^{-1}(p^\prime)\rangle_{p} + \frac{\mu}{2}\cdot \mathcal{W}^{2}_{2}(p^\prime,p)
\end{align*}
\end{defn}
\noindent
where $\texttt{Exp}_{p}$ denotes the exponential mapping, which specifies how to move $p$ along a tangent vector on $\mathcal{P}_{2}(\mathcal{X})$ and $\texttt{Exp}_{p}^{-1}$ denotes its inversion mapping, which maps a point on $\mathcal{P}_{2}(\mathcal{X})$ to a tangent vector. We refer the readers to \cite{santambrogio2015optimal} for more details.

%For every vector field $v:\mathcal{X}\rightarrow\mathcal{X}$, we have:
%\begin{align}
%\label{eqn:div}
%    \int_{\mathcal{X}}\langle \nabla\frac{\partial F(p)}{\partial p}(\textbf{x}),v(\textbf{x}) \rangle p(\textbf{x})\mathrm{d}\textbf{x}=\\
%    -\int_{\mathcal{X}}\frac{\partial F(p)}{\partial p}(\textbf{x})\texttt{div}(p(\textbf{x})v(\textbf{x}))\mathrm{d}\textbf{x}
%\end{align}


\subsection{Variational Transport}
To optimize $F$ defined on the unconstrained domain, we can utilize functional gradient descent with respect to the geodesic distance. This approach involves constructing a sequence of probability distributions $\left\{ q_{t}\right\}_{t\geq 1}$ in $\mathcal{P}_{2}(\mathcal{X})$ as follows:
\begin{align}
\label{eqn:functionalgd}
    q_{t+1}\leftarrow \texttt{Exp}_{q_{t}}[ -\eta_{t}\cdot \texttt{grad}F(q_{t})]
\end{align}
where $\eta_{t}$ is the step size. The VP algorithm \cite{liu2021infinite} is introduced to solve the distributional optimization problem by approximating $q_{t}$ with an empirical measure $\Tilde{q}_{t}$ obtained from $N$ particles $\left\{\textbf{x} _{t,i} \right\}_{i\in \left[N\right]}$. VP assumes that $F$ can be expressed in the variational representation (\ref{eqn:varform}). One advantage of this formulation is that the Wasserstein gradient can be computed based on the solution $h^{*}_{t}$ of problem (\ref{eqn:varform}), which can be estimated using samples drawn from $q_{t}$. Specifically, it is shown that $h^{*}_{t}=\partial F/\partial q_{t}$, representing the first variation of $F$ (refer to Proposition 3.1 in \cite{liu2021infinite}). Furthermore,  under the assumption that $\nabla h^{*}_{t}$ is $h$-Lipschitz continuous, it is demonstrated that for any $\eta_{t}\in \left[ 0,1/h\right)$, the exponential mapping in (\ref{eqn:functionalgd}) is equivalent to the push-forward mapping defined by $h^{*}_{t}$: for $\textbf{x}_{t,i}\sim q_{t}$:

\begin{align}
    \textbf{x}_{t+1,i}\leftarrow \texttt{Exp}_{\textbf{x}_{t,i}}[ -\eta_{t}\cdot \nabla h^{*}_{t}(\textbf{x}_{t,i})]
\end{align}
where $\textbf{x}_{t+1,i}$ is the updated particle which is drawn from $q_{t+1}$, $\texttt{Exp}_{\textbf{x}}[\eta\cdot \nabla u]$ denotes the transportation map which sends $\textbf{x}\in  \mathcal{X}$ to a point $\textbf{x}+\eta\cdot \nabla u \in \mathcal{X}$ (see Proposition 3.2 in \cite{liu2021infinite}).
In addition, VP estimates the solution $h^{*}_{t}$ by solving problem (\ref{eqn:varform}) using finite samples drawn from $q_{t}$. This is achieved through stochastic gradient descent on the domain $\mathcal{X}$:

\begin{align}
\label{eqn:varformmax}
     \Tilde{h_{t}^{*}}= \argmax_{h\in\Tilde{\mathcal{H}}}{\left\{\frac{1}{N}\sum_{i=1}^{N}h(\textbf{x}_{t,i})-F^{*}(h)\right\}}
\end{align}
where $\Tilde{\mathcal{H}}$ is a function class, which can be specified to be the following class of deep neural networks:
\begin{align}
    \label{eqn:nnclass}
    \Tilde{\mathcal{H}}=\left\{\Tilde{h}| \Tilde{h}(\textbf{x})=\frac{1}{\sqrt{n_{w}}}\sum_{i=1}^{n_{w}}b_{i}\sigma([\textbf{w}]_{i}^{T}\textbf{x})\right\}
\end{align}
where $n_{w}$ is the width of the neural networks, $[\textbf{w}]_{i}\in \mathbb{R}^{d}$, $\textbf{w}=([\textbf{w}]_{1},...,[\textbf{w}]_{n_{w}})^{T}\in \mathbb{R}^{n_{w}\times d}$ is the input weight, $\sigma$ denotes a smooth activation function, and $b_{i}\in \left\{ -1,1\right\}$. In each iteration, the weights $\textbf{w}$ is guaranteed to lie in the $l_{2}$-ball centered at the initial weights $\textbf{w}(0)$ with radius $r_{h}$ defined as $\mathcal{B}^{0}(r_{h})=\left\{\textbf{w}: \lVert\textbf{w} - \textbf{w}(0) \rVert_{2}\leq r_{h} \right\}$. This choice of neural network class facilitates the analysis of the gradient error induced by the difference between $h^{*}_{t}$ and $ \Tilde{h_{t}^{*}}$ \cite{liu2021infinite}.


\section{Moreau-Yoshida Variational Transport}
In this section, we present our method, MYVT, for solving problem (\ref{eqn:regDP}). 
\subsection{Moreau-Yoshida approximation of problem (\ref{eqn:regDP})}
To address the non-smoothness of function $g$, our approach is to replace $g$ with its envelope $g^{\lambda}$, which leads to the following smooth approximate distributional optimization problem:
\begin{align}
\label{eqn:smoothregDP}
    \min_{q\in \mathcal{P}_{2}(\mathcal{X})}\left\{ G^{\lambda}(q)\coloneqq F(q) + \alpha\mathbb{E}_{\textbf{x}\sim q}[g^{\lambda}(\textbf{x})] \right\}
\end{align}
We denote $\pi$ and $\pi^{\lambda}$ as the optimal solutions of problems (\ref{eqn:regDP}) and (\ref{eqn:smoothregDP}), respectively.  The following theorem establishes a connection between the two solutions.

\begin{thm}
\label{thm:twosolutions} Given that $F(q)$ is geodesically $\mu$-strongly convex ($\mu$>0), the solution $\pi^{\lambda}$ converges to $\pi$ as $\lambda$ goes to 0 with respect to the 2-Wasserstein distance, i.e.
\begin{equation}
    \label{eqn:limW2}
    \lim_{\lambda\to 0}\mathcal{W}^{2}_{2}(\pi^{\lambda}, \pi)=0
\end{equation}
\end{thm}
\noindent
The proof of Theorem \ref{thm:twosolutions} is given in Appendix A.1. 

\subsection{Primal-Dual Approach to problem (\ref{eqn:smoothregDP})}
%In the same vein as VT \cite{liu2021infinite} and mirrorVT \cite{nguyen2023mirror}, our method is based on the assumption that $F$ admits the variational representation (\ref{eqn:varform}). Thus we replace $F(q)$ with its variational representation in problem (\ref{eqn:smoothregDP}), leading to the following concave-convex saddle point problem:

%\begin{align}
%\label{eqn:saddle}
%    \min_{ q\in \mathcal{P}_{2}(\mathcal{X})} \max_{h\in\mathcal{F}}\left\{ \int{h(\textbf{x})q(\textbf{x})\mathrm{d}\textbf{x}}-F^{*}(h) + \alpha\int{g^{\lambda}(\textbf{x})q(\textbf{x})\mathrm{d}\textbf{x}}\right\}
%\end{align}


% Figure environment removed
\noindent
%\subsection{Local Variable Optimization}
The objective in problem (\ref{eqn:smoothregDP}) still poses a challenge as it covers the entire space of probability distributions, making it generally computationally intractable. To tackle this problem, a common approach is to employ specific parameterization forms for the distribution $q$ and optimize its parameters or approximate it using a set of particles, as discussed in \cite{liu2021infinite, nguyen2023mirror}. However, these approaches often have limitations in terms of approximation ability and memory resources they require. Taking inspiration from \cite{wang2016learning}, we propose an alternative method of implicitly representing $q$ using a neural network. In this approach, we generate $\textbf{x}_{\epsilon} \sim q$ by passing $\epsilon$ drawn from a simple distribution $p_{\epsilon}$ through a network, i.e. $\textbf{x}_{\epsilon}=V(\epsilon, \theta)$, where $\theta$ denotes the network parameters, which are iteratively adjusted to minimize the objective in problem(\ref{eqn:smoothregDP}). 
In the following theorem, we present an equivalent primal-dual view of problem (\ref{eqn:smoothregDP}) by utilizing the variational representation of $F$:
%Based on these considerations, we propose an efficient algorithm for solving the saddle point problem (\ref{eqn:saddle}), as suggested by the following theorem:


\begin{thm}
\label{thm:saddle} We can formulate problem (\ref{eqn:smoothregDP}) equivalently as:

\begin{equation}
    \label{eqn:saddle1}
    \max_{h\in\mathcal{H}} \mathbb{E}_{\epsilon\sim p_{\epsilon}}\left[ \min_{\textbf{x}_{\epsilon}\in\mathcal{X}} \left\{ h(\textbf{x}_{\epsilon})+ \alpha g^{\lambda}(\textbf{x}_{\epsilon}) \right\}  \right] -F^{*}(h)
\end{equation}
\end{thm}
\noindent
The primal-dual formulation (\ref{eqn:saddle1}) is derived by applying the variational representation of $F$ and interchangeability principle introduced in \cite{dai2017learning}. 
The detailed proof of Theorem \ref{thm:saddle} can be found in Appendix A.2.

Based on the finding of Theorem \ref{thm:saddle}, we can transition to handling the distribution $q$ for each local variable. Specifically, given $\epsilon\sim p_{\epsilon}$ and a fixed $h$, we can solve the following local optimization problem for $\textbf{x}_{\epsilon}$:

\begin{equation}
    \label{eqn:localopt}
    x^{*}_{\epsilon}=\argmin_{x_{\epsilon}\in\mathcal{X}}\left\{ h(x_{\epsilon}) + \alpha g^{\lambda}(x_{\epsilon})\right\}
\end{equation}
To efficiently solve problem (\ref{eqn:localopt}), we can take advantage of the advancements in optimization literature. In this work, we will focus on utilizing gradient descent for its simplicity and effectiveness.
%Assume we perform $T$ iterations of gradient updates, i.e. for %$t=1,2...,T-1$
%\begin{align*}
%    & \textbf{x}_{\epsilon}^{(0)}= \textbf{x}_{\theta}(\epsilon)\\
%    & \textbf{x}^{(t+1)}_{\epsilon} = \textbf{x}^{(t)}_{\epsilon} - \eta \left[ \nabla h(\textbf{x}^{(t)}_{\epsilon})  + \frac{1}{\lambda} (\textbf{x}^{(t)}_{\epsilon} - \texttt{prox}^{\lambda}_{g}(\textbf{x}^{(t)}_{\epsilon}))\right]\\
%    & \textbf{x}^{*}_{\epsilon} = \textbf{x}^{(T)}_{\epsilon}
%\end{align*}
%\noindent
Specifically, in each iteration of our method, we draw a batch of random inputs $\left\{ \epsilon_{i}\right\}_{i=1}^{m}$, where $m$ is mini-batch size. We then calculate the corresponding outputs for these inputs, which are subsequently used as the initial particles:
\begin{equation*}
    \textbf{x}^{(0)}_{i}=V(\epsilon_{i}, \theta)
\end{equation*}
We perform $T$ steps of gradient updates to optimize problem (\ref{eqn:localopt}) for each particle, i.e., for $t=0,...,T-1$:
\begin{align}
\begin{split}
\label{eqn:updatex}
    &\Delta^{(t)}_{i}= \nabla_{\textbf{x}} h(\textbf{x}^{(t)}_{i})  + \frac{\alpha}{\lambda} (\textbf{x}^{(t)}_{i} - \texttt{prox}^{\lambda}_{g}(\textbf{x}^{(t)}_{i}))\\
    &\textbf{x}^{(t+1)}_{i} = \textbf{x}^{(t)}_{i} - \eta \Delta^{(t)}_{i}
\end{split}
\end{align}
The particles obtained from the last update, denoted as $\textbf{x}^{(T)}_{i}$ ($i=1,...,m$), approximate the solutions of the local optimization problems and are utilized to estimate $h$ in problem (\ref{eqn:saddle1}).
Furthermore, the particles undergo updates over multiple steps to converge towards the minimizers of local optimization problems. Therefore, the parameters $\theta$ of $V$ need to be updated such that it outputs $\{\textbf{x}^{(t+1)}_{i} \}_{i=1}^{m}$ instead of $\{\textbf{x}^{(t)}_{i} \}_{i=1}^{m}$. In other words, we aim to update $\theta$ as follows:
\begin{equation}
\label{eqn:updatetheta}
    \theta^{(t+1)}\leftarrow \argmin_{\theta} \sum_{i=1}^{m}\lVert V(\epsilon_{i}, \theta) - \textbf{x}^{(t+1)}_{i}\rVert^{2}
\end{equation}
As suggested in \cite{feng2017learning}, we can perform only one step of gradient descent as follows:
\begin{equation}
\label{eqn:fastupdatetheta}
    \theta^{(t+1)}\leftarrow \theta^{(t)} - \eta \sum_{i=1}^{m}\nabla_{\theta}V(\epsilon_{i}, \theta^{(t)})\Delta^{(t)}_{i}
\end{equation}
While the update (\ref{eqn:fastupdatetheta}) is an approximation of (\ref{eqn:updatetheta}), it offers computational efficiency and has shown promising performance in our experiments. Additionally, the following theorem establishes a connection between the optimization of the local variables and the gradient flow for optimizing the regularized functional in (\ref{eqn:smoothregDP}). This connection holds in the limit case when utilizing a specific form of $h(\textbf{x}_{\epsilon})$:


\begin{thm}
\label{thm:gradflow} For a continuous time $t=\eta T$ and step size $\eta \to 0$, the distribution of particles $\textbf{x}^{(t)}_{\epsilon}$, denoted as $q_{t}$, follows the following Fokker-Planck equation:
\begin{equation}
\label{eqn:continuityequation}
    \frac{\partial q_{t}}{\partial t}=-\texttt{div}\left(  q_{t} v_{t}\right)
\end{equation}
where $v_{t}(\textbf{x})\coloneqq \nabla h^{*}_{t}(\textbf{x})+\frac{\alpha}{\lambda}\left( \textbf{x}- \texttt{prox}^{\lambda}_{g}(\textbf{x})\right)$, for all $\textbf{x}\in\mathcal{X}$,
and $h^{*}_{t} = \argmax_{h\in\mathcal{H}}{\left\{\mathbb{E}_{\textbf{x}\sim q_{t}}\left[h(\textbf{x})\right]-F^{*}(h)\right\}}=\frac{\partial F}{\partial q}\left( q_{t}\right)$.

\noindent
This is the Wasserstein gradient flow of $G^{\lambda}(q)$ in the space of probability distributions with 2-Wasserstein metric. Suppose $F$ is geodesically $\mu$-strongly convex, the convergence of $G^{\lambda}(q_{t})$ is as follows, for $t\geq 0$:
%\begin{align}
%    \frac{\mathrm{d}G^{\lambda}(q_{t})}{\mathrm{d}t}=-\mathbb{E}_{\textbf{x}\sim q_{t}} \lVert v_{t}(x) \rVert^{2}_{2} \leq 0
%\end{align}
\begin{align}
    G^{\lambda}(q_{t})-G^{\lambda}(\pi^{\lambda})\leq 
    \exp(-2\mu t)( G^{\lambda}(q_{0})-G^{\lambda}(\pi^{\lambda}))
\end{align}
\end{thm}
\noindent
The proof of Theorem \ref{thm:gradflow} is given in Appendix A.3. We observe that in limit case, when $F$ is geodesically convex, $q_{t}$ exponentially converges to the minimizer of $G^{\lambda}(q)$ as $t\rightarrow \infty$.

\subsection{Practical Implementation}
We are now ready to present our algorithm for solving problem (\ref{eqn:smoothregDP}). The components of our method can be easily parameterized by deep neural networks and their parameters can be estimated by stochastic gradient descent. For instance, our method requires the initialization $\textbf{x}^{(0)}_{i}$, which is the output of the network $V(\epsilon_{i}, \theta)$, where $\theta$ is the parameters. The function $h$ is parameterized by another neural network with parameters $W$. Taking into account of above parameters, we have our proposed algorithm illustrated in Algorithm 1. We perform $K$ iterations. For each iteration, initial particles are obtained by drawing a mini-batch of $m$ random inputs and calculating their corresponding outputs through $V$, then we perform $T$ steps of updates for each  particle. The particles obtained from the last steps are used for estimating $h$ by performing $T^\prime$ steps of updates to optimize its parameters $W$.


\begin{algorithm}[H]
\label{alg:rvt}
\SetAlgoLined
\KwIn{Functional $F$, mini-batch size $m$, number of iterations $K$, number of steps $T$ to optimize (\ref{eqn:updatex}), number of steps $T^\prime$ to optimize $h$ in (\ref{eqn:localopt}), step size $\eta$ and $\lambda$,$\alpha$.}
\KwOut{Networks $V(\cdot, \theta)$, $h_{W}(\cdot)$}
Randomly initialize $\theta$, $W$ (parameters of $V$ and $h$) \\
$k\leftarrow 0$\\
\While{$k< K$}{
    sample mini-batch $\left\{ \epsilon_{i}\right\}_{i=1}^{m}\sim p_{\epsilon}$ 
    
    compute $\textbf{x}^{(0)}_{i}=V(\epsilon_{i}, \theta)$, for $i=1,...,m$

    $t\leftarrow 0$

    \While{$t<T$}{
     $\Delta^{(t)}_{i}= \nabla_{x} h(\textbf{x}^{(t)}_{i})  + \frac{\alpha}{\lambda} (\textbf{x}^{(t)}_{i} - \texttt{prox}^{\lambda}_{g}(\textbf{x}^{(t)}_{i}))$
     
    update $\textbf{x}^{(t+1)}_{i} = \textbf{x}^{(t)}_{i} - \eta \Delta^{(t)}_{i}$, for $i=1,...,m$

    update $\theta\leftarrow \theta - \eta \sum_{i=1}^{m}\nabla_{\theta}V(\epsilon_{i}, \theta)\Delta^{(t)}_{i}$

    $t\leftarrow t+1$
    }
    $\textbf{x}^{*}_{i}\leftarrow \textbf{x}^{(T)}_{i}$, for $i=1,...,m$

     $t^\prime\leftarrow 0$

    \While{$t^\prime<T^\prime$}{

    update $W=W+\eta \left( \frac{1}{m}\sum_{i=1}^{m}\nabla_{W}h(\textbf{x}^{*}_{i}) - \nabla _{W}F^{*}(h_{W}) \right)$\\
    $t^\prime\leftarrow t^\prime+1$
    }
    
    $k\leftarrow k+1$
    }
 \caption{Moreau-Yoshida variational transport (MYVT)}
\end{algorithm}



\section{Numerical Experiments}
In this section, we report numerical experiments on synthetic data sets to demonstrate the effectiveness of MYVT. %Other additional numerial experiments are given in Appendix A.4. %The code can be found at \url{https://github.com/haidnguyen0909/MYVT}.

% Figure environment removed


\subsection{Synthetic Experiments}

\noindent
\textbf{Experimental settings}. We consider two case studies corresponding to two common choices
of the nonsmooth function $g$: (a) $l_{1}$-norm and (b) total variation (TV) semi-norm, which promote sparsity of samples (i.e. few 
non-zero elements) and local constancy of elements (i.e. sparsity of the difference of nearby element), respectively. For the $l_{1}$-norm, the proximal map of $g(\textbf{x})=\lVert \textbf{x}\rVert_{1}$ is the well-known soft-thresholding operator $S_{\alpha}(\textbf{x})$ \cite{tibshirani1996regression}. For TV semi-norm, the proximal map does not have a closed-form solution, but it can be efficiently estimated using the alternating direction method of multipliers (ADMM) \cite{wahlberg2012admm}. In our experiments, we perform 20 iterations of ADMM to estimate the proximal map. For the first case study, we design the truth $\textbf{z}\in \mathbb{R}^{100}$ to be sparse vector with only a few non-zero elements. For the second case study, we design $\textbf{z}$ to be a locally smooth vector. For each case study, we generate 500 examples $\left\{\textbf{y}_{i} \right\}_{i=1}^{500}$ by adding Gaussian noise with mean 0 and variance $0.2^{2}$ to the truth.
These settings allow to evaluate the performance of methods in recovering the underlying structure of the data in the presence of noise.

The generated examples are used to represent the target empirical distributions $\pi$ we aim to approximate. We set $F(q)=D(q, \pi)$, where $D$ represents a dissimilarity measure (KL or JS divergences in our experiments) between two probability distributions. In the following, we report results when utilizing the KL divergence for the dissimilarity $D$. For experiments with the JS divergence, refer to Appendix A.4.\\



% Figure environment removed

\noindent
\textbf{Comparing methods}. We compare MYVT and VT in the experiments. For VT, we represent $q$ using a set of particles and updating particles. For MYVT, we set $\alpha=0.1$ for both case studies. We evaluate and compare the quality of particles generated by the two methods using the following measures: a) mean squared error (MSE): the average squared difference between the generated samples and the truth $\textbf{z}$, (b) the average $l_{1}$-norm of generated samples for the first study case and (c) the average TV semi-norm of generated samples for the second study case. We parameterize $V$ using a neural network with four layers, each of which consists of a linear layer with 100 neurons, followed by an activation function.  We parameterize $h$ using another neural network with two layers, each of which has 100 neurons. The step sizes for VT and MYVT are fine-tuned and set to 0.01 and 0.0001, respectively. We run $K=2000$ and  $K=4000$ iterations for the first and the second case studies. respectively. For MYVT, we set $\lambda=0.0001$, $T=5$ and $T^\prime=2$ for all of experiments.\\


% Figure environment removed

\noindent
\textbf{Results}. In the first case study, we compare MYVT and VT in terms of MSE and average $l_{1}$-norm. As illustrated in Figure \ref{fig:sparsity}, both methods are able to generate samples that are similar to the truth, as indicated by the decreasing MSE values over 2000 iterations. However, MYVT keeps the average $l_{1}$-norm much lower than that of VT over iterations. In particular, MYVT consistently produce samples with much lower average $l_{1}$-norm (around 15.81) compared to VT (around 30.68). This can be attributed to the effect of $g(\textbf{x})= \lVert \textbf{x} \rVert_{1}$ in problem (\ref{eqn:regDP}), which promotes the sparsity in the generated samples. Visually, samples generated by MYVT appear considerably sparser than those generated by VT (see Figures \ref{fig:sparsity}c and \ref{fig:sparsity}d).


In the second case study, we compare MYVT and VT in terms of MSE and average TV semi-norm, as shown in Figure \ref{fig:tv}. Again both methods are able to generate samples that are closely resemble the truth, as evidenced by the significant decrease in MSE values over 4000 iterations (see Figure \ref{fig:tv}a), while MYVT keeps the average TV semi-norm much lower than VT over iterations (see Figure \ref{fig:tv}b). In particular, the average TV semi-norm of samples generated by MYVT (18.52) is significantly smaller than that of samples generated by VT (31.02). Visually, samples generated by MYVT appear significantly smoother than those generated by VT (see Figures \ref{fig:tv}c and \ref{fig:tv}d). These results in both case studies demonstrate the regularization effect of problem (\ref{eqn:regDP}) on the generated samples and highlight the effectiveness of MYVT.




\subsection{Image Denoising}

In the third case study, we focus on the task of denoising an image that has been degraded by  Gaussian noise, using two-dimensional TV. Given an image $\textbf{x}\in \mathbb{R}^{w\times h}$, its two-dimensional TV is defined as follows:
\begin{equation*}
TV(\textbf{x})=\sum_{i=1}^{h}\sum_{j=2}^{w}|\textbf{x}[i,j]-\textbf{x}[i,j-1]| +\sum_{j=1}^{w}\sum_{i=2}^{h}|\textbf{x}[i,j]-\textbf{x}[i-1,j]| 
\end{equation*}
We consider a truth image: $\textbf{z}\in\mathbb{R}^{80\times 80}$ (see Figure \ref{fig:truth}a) and its noisy version: $\textbf{y}=\textbf{z}+\mathcal{N}(0, 100^{2}\textbf{I})$ (see Figure \ref{fig:truth}b), and aim to apply VT and MYVT to generate image samples $\{ \textbf{x}_{i}\}$, which are resemble $\textbf{y}$ (i.e. the Frobenius norm $\lVert \textbf{x}_{i}-\textbf{y}\rVert_{F}$ is small) and smooth (i.e. $TV(\textbf{x}_{i})$ is small). This task can be formulated as follows:
\begin{align*}
\label{eqn:regKL}
    \min_{q\in \mathcal{P}_{2}(\mathcal{X})} KL(q,\pi) + \alpha \mathbb{E}_{\textbf{x}\sim q}[\text{TV}(\textbf{x})] 
\end{align*}
where $\pi(\textbf{x})\propto e^{-\lVert \textbf{x}-\textbf{y} \rVert_{F}^{2}}$. In the case where we only have access to un-normalized density of $\pi$, it becomes challenging to approximate the variational formulation of KL diveregence using samples from $\pi$ (see Equation (\ref{eqn:varformKL})). However, we can overcome this issue by introducing the following change of variable (for the case of using KL divergence): $h^{\prime}(\textbf{x})=e^{h(\textbf{x})}\pi(\textbf{x})/p(\textbf{x})$,
%\begin{equation*}
%    h^{\prime}(\textbf{x})=\frac{e^{h(\textbf{x})}\pi(\textbf{x})}{p(\textbf{x})}
%\end{equation*}
where $p$ is a probability distribution which is easy to sample from, e.g. $p(\textbf{x})=\mathcal{N}(0, \textbf{I})$. Then the variational formulation of KL divergence can be rewritten as:
\begin{align*}
    KL(q,\pi)&=\max_{h^\prime\in \mathcal{H}^{+}}\{ \mathbb{E}_{\textbf{x}\sim q}\left[\log h^\prime(\textbf{x}) \right]+\mathbb{E}_{\textbf{x}\sim q}\left[ \log p(\textbf{x})\right] \\
    &- \mathbb{E}_{\textbf{x}\sim q}\left[ \log \pi(\textbf{x})\right] - \log\mathbb{E}_{\textbf{x}\sim p}\left[ h^\prime(\textbf{x})\right] \}
\end{align*}
where $\mathcal{H}^{+}$ is the space of positive functions. As we can use $\log$ of un-normalized density of $\pi$ in the above optimization problem, $h^{\prime}$ can be estimated using samples drawn from $q$ and $p$. Therefore, $h$ can be estimated from $h^{\prime}$ using: $h(\textbf{x})=\log h^{\prime}(\textbf{x})+\log p(\textbf{x})-\log \pi(\textbf{x})$.\\

\noindent
\textbf{Experiment setting}. For MYVT, we set $\alpha=100.0$. We parameterize $V$ using a neural network with five layers, each of which has 200 neurons. We parameterize $h^\prime$ using another neural network with two layers, each of which has 200 neurons. We use the ReLU activation function in the last layer to guarantee the output of $h^\prime$ to be positive. We set the number of iterations $K$, mini-batch size, step sizes, $T$ and $T^\prime$ for both VT and MYVT as 3000, 200, 0.01, 5 and 2, respectively.\\

\noindent
\textbf{Results}. Figure \ref{fig:imagedenoising_samples}a and \ref{fig:imagedenoising_samples}b display the evolution of example samples over 3000 iterations of VT and MYVT, repsectively, for the denoising task applied to the given image. It is evident that the samples generated by MYVT become increasingly smooth over iterations. This behavior is a direct result of the inclusion of the TV semi-norm in problem (\ref{eqn:regDP}).

\section{Conclusion}
We have addressed the regularized distributional optimization problem with a composite objective composed of two functionals. The first one has the variational representation while the second one is expressed in terms of the expectation operator of a non-smooth convex regularizer function. We have introduced MYVT as a solution to this problem. Its key idea is to approximate the original problem using Moreau-Yoshida approximation and reformulate it as a concave-convex saddle point problem by leveraging the variational representation. In our future work we aim to develop more efficient algorithms for estimating the solutions of problem (\ref{eqn:varform}). Additionally, we plan to extend MYVT to handle other forms of objective functionals which do not possess the variational representation. By exploring these directions, we aim to enhance the versatility and efficiency of MYVT and further advance the field of regularized distributional optimization.
\bibliographystyle{plainnat}
\bibliography{references}

\newpage
\appendix
\section{Supplemetary Materials of Moreau-Yoshida Variational Transport: A General Framework For Solving Regularized Distributional Optimization Problems}
\subsection{Proof of Theorem \ref{thm:twosolutions}}
\label{appendix:twosolutions}
\begin{proof}
By the definitions of the functionals $G(q)$ and $G^{\lambda}(q)$, we have:
\begin{equation}
    \label{appendix:eqn1}
    G^{\lambda}(\pi)-G(\pi)= \alpha\mathbb{E}_{\textbf{x}\sim\pi}[g^{\lambda}(\textbf{x})-g(\textbf{x})]
\end{equation}
We can decompose the left-hand side of (\ref{appendix:eqn1}) into three terms as follows:
\begin{align}
\begin{split}
    \label{appendix:eqn2}
    G^{\lambda}(\pi)-G(\pi)= \underbrace{G^{\lambda}(\pi)-G^{\lambda}(\pi^{\lambda})}_{(a)} + \underbrace{G^{\lambda}(\pi^{\lambda})-G(\pi^{\lambda})}_{(b)}
    + \underbrace{G(\pi^{\lambda})  - G(\pi)}_{(c)}
\end{split}
\end{align}
As $F(q)$ is geodesically $\mu$-strongly convex ($\mu$>0), it is easy to verify that $G(q)$ and $G^{\lambda}(q)$ are also geodesically $\mu$-strongly convex. Therefore, we obtain the following inequalities:
\begin{align}
\label{ineq:a}
\begin{split}
    \text{(a)} \geq \langle \texttt{grad}G^{\lambda}(\pi^{\lambda}), \texttt{Exp}_{\pi^{\lambda}}^{-1}(\pi)\rangle_{\pi^{\lambda}} + \frac{\mu}{2}\mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)
    = \frac{\mu}{2}\mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)
\end{split}
\end{align}
\noindent
\begin{align}
\label{ineq:c}
\begin{split}
    \text{(c)} \geq \langle \texttt{grad}G(\pi), \texttt{Exp}_{\pi}^{-1}(\pi^{\lambda})\rangle_{\pi} + \frac{\mu}{2}\mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)
    = \frac{\mu}{2}\mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)
\end{split}
\end{align}
These inequalities are obtained by the assumption that $\pi$ and $\pi^{\lambda}$ are the minimizers of $G(q)$ and $G^{\lambda}(q)$, respectively. Again, by the definition of $G(q)$ and $G^{\lambda}(q)$, we have:
\begin{equation}
\label{ineq:b}
    \text{(b)}= \alpha\mathbb{E}_{\textbf{x}\sim\pi^{\lambda}}[g^{\lambda}(\textbf{x})-g(\textbf{x})]
\end{equation}
Combining (\ref{ineq:a}), (\ref{ineq:b}), (\ref{ineq:c}) and (\ref{appendix:eqn1}), we have:
\begin{equation}
\label{ineq:e}
    \alpha\mathbb{E}_{\textbf{x}\sim\pi}[g^{\lambda}(\textbf{x})-g(\textbf{x})] - \alpha\mathbb{E}_{\textbf{x}\sim\pi^{\lambda}}[g^{\lambda}(\textbf{x})-g(\textbf{x})]\geq \mu \mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)
\end{equation}
Most importantly, it is known that $g^{\lambda}(\textbf{x})$ converges pointwise to $g(\textbf{x})$ as $\lambda$ tends to zero \cite{rockafellar2009variational}. Therefore, induced by (\ref{ineq:e}), $\mathcal{W}^{2}_{2}(\pi^{\lambda},\pi)$ tends to zero as $\lambda$ tends to zero, which concludes the proof.
\end{proof}
\subsection{Proof of Theorem \ref{thm:saddle}}
\begin{proof}
In the same vein as VT \cite{liu2021infinite} and mirrorVT \cite{nguyen2023mirror}, our method is based on the assumption that $F$ admits the variational representation (\ref{eqn:varform}). Thus we replace $F(q)$ with its variational representation in problem (\ref{eqn:smoothregDP}), leading to the following concave-convex saddle point problem:

\begin{align}
\label{eqn:saddle}
    \min_{ q\in \mathcal{P}_{2}(\mathcal{X})} \max_{h\in\mathcal{H}}\left\{ \mathbb{E}_{\textbf{x}\sim q}\left[ h(\textbf{x})\right]-F^{*}(h) + \alpha \mathbb{E}_{\textbf{x}\sim q}\left[g^{\lambda}(\textbf{x}) \right]\right\}
\end{align}

It is easy to verify that problem (\ref{eqn:saddle}) is concave-convex, so the strong duality holds, which indicates that problem (\ref{eqn:saddle}) is equivalent to the following problem:

\begin{align*}
\label{eqn:saddle2}
    \max_{h\in\mathcal{H}}\min_{ q\in \mathcal{P}_{2}(\mathcal{X})} \left\{ \mathbb{E}_{\textbf{x}\sim q}\left[ h(\textbf{x})+\alpha g^{\alpha}(\textbf{x}) \right]-F^{*}(h) \right\}\\
    =\max_{h\in\mathcal{F}}\min_{\textbf{x}_{\epsilon}=V(\epsilon, \theta)\in \mathcal{F} } \left\{ \mathbb{E}_{\epsilon\sim p_{\epsilon}}\left[ h(\textbf{x}_{\epsilon})+\alpha g^{\alpha}(\textbf{x}_{\epsilon}) \right]-F^{*}(h) \right\}
\end{align*}
where the equality  is obtained from the reparameterization: $\epsilon\sim p_{\epsilon},\textbf{x}_{\epsilon}=V(\epsilon, \theta)$, and holds if $V(\epsilon, \theta)$ is flexible enough to contain the optimal function that transform $p_{\epsilon}$ to $q^{*}$ (the optimal solution of problem (\ref{eqn:saddle})). Furthermore, because $h(\textbf{x})$ and $g^{\alpha}(\textbf{x})$ are continuous, we can apply the interchangeable principle in \cite{dai2017learning} to obtain the following inequality:
\begin{align*}
    \min_{\textbf{x}_{\epsilon}=V(\epsilon, \theta)\in \mathcal{F} } \left\{ \mathbb{E}_{\epsilon\sim p_{\epsilon}}\left[ h(\textbf{x}_{\epsilon})+\alpha g^{\alpha}(\textbf{x}_{\epsilon}) \right]\right\}=
    \mathbb{E}_{\epsilon\sim p_{\epsilon}}\left[ \min_{\textbf{x}_{\epsilon}\in\mathcal{X}} \left\{ h(\textbf{x}_{\epsilon})+ \alpha g^{\lambda}(\textbf{x}_{\epsilon}) \right\}  \right] 
\end{align*}
which concludes the proof.
\end{proof}

% Figure environment removed


\subsection{Proof of Theorem \ref{thm:gradflow}}
We need the following lemma, known as the gradient dominance of the geodesically strongly convex functional, in order to show the convergence of $G^{\lambda}(q)$.
\begin{lem}
    \label{lem:gradientdominance}
    (Gradient Dominance) Let $F$ be a geodesically $\mu$-strongly convex and $F^{*}=\min_{q\in\mathcal{P}_{2}(\mathcal{X})}F(q)$, we have:
    \begin{align*}
        2\mu \left[ F(q)-F^{*}\right]\leq \langle \texttt{grad}F(q), \texttt{grad}F(q) \rangle_{q}
    \end{align*}
\end{lem}
\begin{proof}
    Let $\pi=\argmin_{q\in\mathcal{P}_{2}(\mathcal{X})}F(q)$, by the definition of geodesically strong convexity, we have:
    \begin{align*}
        &F(\pi)\geq F(q)+\langle \texttt{grad}F(q), \texttt{Exp}_{q}^{-1}(\pi)\rangle_{q} + \frac{\mu}{2}\mathcal{W}^{2}_{2}(q,\pi)\\
        & =F(q)+\langle \texttt{grad}F(q), \texttt{Exp}_{q}^{-1}(\pi)\rangle_{q} 
        + \frac{\mu}{2}\langle \texttt{Exp}_{q}^{-1}(\pi), \texttt{Exp}_{q}^{-1}(\pi)\rangle_{q}
    \end{align*}
    where the equality is obtained by the definition of 2-Wasserstein distance: $\mathcal{W}^{2}_{2}(q,\pi)=\langle \texttt{Exp}_{q}^{-1}(\pi), \texttt{Exp}_{q}^{-1}(\pi)\rangle_{q}$. We continue as follows:
    \begin{align*}
        &F(\pi)\geq F(q)+\frac{\mu}{2}\langle \texttt{Exp}_{q}^{-1}(\pi) +\frac{1}{\mu}\texttt{grad}F(q), \texttt{Exp}_{q}^{-1}(\pi) 
        +\frac{1}{\mu}\texttt{grad}F(q)\rangle_{q} - \frac{1}{2\mu}\\
        &\langle \texttt{grad}F(q), \texttt{grad}F(q) \rangle_{q}
        \geq F(q) - \frac{1}{2\mu}\langle \texttt{grad}F(q), \texttt{grad}F(q) \rangle_{q}
    \end{align*}
    The lemma is proved.
\end{proof}



Now we prove Therem \ref{thm:gradflow}. 
\begin{proof}
When step size $\eta$ is small ($\eta\rightarrow 0$), we consider the following gradient flow:
\begin{equation*}
    \textbf{x}^{\prime}_{t}=-v_{t}(\textbf{x}_{t})
\end{equation*}
and assume that $\textbf{x}_{0}$ is random with density $q_{0}$. As $q_{t}$ is the density of $\textbf{x}_{t}$, by directly applying the Fokker-Planck equation, we obtain:
\begin{align*}
    &\frac{\partial q_{t}(\textbf{x})}{\partial t}=-\texttt{div}\left(  q_{t}(\textbf{x}) v_{t}(\textbf{x})\right)\\
    &=-\texttt{div}\left(  q_{t}(\textbf{x})  \left[ \nabla h^{*}_{t}(\textbf{x})+\frac{\alpha}{\lambda}\left( \textbf{x}- \texttt{prox}^{\lambda}_{g}(\textbf{x})\right) \right]\right)\\
    &=-\texttt{div}\left( q_{t}(\textbf{x}) \nabla \frac{\partial F}{\partial q_{t}}(\textbf{x}) + \frac{\partial \mathbb{E}_{q_{t}}\left[g^{\lambda}\right]}{\partial q_{t}}(\textbf{x})\right)\\
    &=-\texttt{div}\left(q_{t}(\textbf{x}) \nabla \frac{\partial G^{\lambda}}{\partial q_{t}}(\textbf{x}) \right)
\end{align*}
where the second equality is obtained by the definition of $v_{t}$, the third equality is obtained by the facts $h_{t}^{*}=\partial F / \partial q_{t}$ and $g^{\lambda}=\partial \mathbb{E}_{q_{t}}\left[ g^{\lambda}\right] / \partial q_{t}$. The last equation shows that the continuity equation (\ref{eqn:continuityequation}) is the Wasserstein gradient flow of $G^{\lambda}(q)$ in the space of probability distributions with 2-Wasserstein metric. Lastly, we can easily show that:

\begin{align*}
&\frac{\mathrm{d}}{\mathrm{d}t}\left(G^{\lambda}(q_{t})-G^{\lambda}(\pi^{\lambda}) \right)=\int \frac{\partial G^{\lambda}}{\partial q}(q_{t})(\textbf{x})\frac{\mathrm{d} q_{t}}{\mathrm{d} t}\mathrm{d}(\textbf{x})\textbf{x}\\
&=\int \frac{\partial G^{\lambda}}{\partial q_{t}}(q_{t})(\textbf{x})\texttt{div}\left(q_{t}(\textbf{x})\nabla \frac{\partial G^{\lambda}}{\partial q}(q_{t})(\textbf{x})\right)\mathrm{d}\textbf{x}\\
&= - \int \langle \nabla \frac{\partial G^{\lambda}}{\partial q}(q_{t})(\textbf{x}),\nabla \frac{\partial G^{\lambda}}{\partial q}(q_{t})(\textbf{x})\rangle q_{t}(\textbf{x})\mathrm{d}(\textbf{x})\\
&=- \langle \texttt{grad}G^{\lambda}(q_{t}), \texttt{grad}G^{\lambda}(q_{t})  \rangle_{q_{t}}
\end{align*}
Since $G^{\lambda}$ is composed of two geodesically strongly convex functionals $F$ and $\mathbb{E}_{q}\left[g^{\lambda}\right]$, it is easy to verify that $G^{\lambda}$ is geodesically $\mu$-strongly convex. Using Lemma \ref{lem:gradientdominance}, we have:
\begin{align*}
    \frac{\mathrm{d}}{\mathrm{d}t}\left(G^{\lambda}(q_{t})-G^{\lambda}(\pi^{\lambda}) \right) \leq -2\mu \left( G^{\lambda}(q_{t})-G^{\lambda}(\pi^{\lambda} \right)
\end{align*}
From a straight forward application of the Gronwall's inequality, it follows that:
\begin{align*}
    G^{\lambda}(q_{t})-G^{\lambda}(\pi^{\lambda})\leq 
    \exp(-2\mu t)( G^{\lambda}(q_{0})-G^{\lambda}(\pi^{\lambda}))
\end{align*}
which concludes the proof.
\end{proof}


% Figure environment removed

\subsection{Additional Experiments on Synthetic Data}
We consider the case $F(q)=JS(q, \pi)$, where $\pi$ is represented by 500 noisy examples $\left\{\textbf{y}_{i} \right\}_{i=1}^{500}$. As shown in \cite{nguyen2023mirror}, the variational representation of the JS divergence is as follows:
\begin{align*}
    JS(q, \pi) = \sup_{h\in\mathcal{H}^{c}}{\left\{ \mathbb{E}_{\textbf{x}\sim q}\left[ h(\textbf{x}) \right]   -JS^{*}(h)\right\}}
\end{align*}
where $JS^{*}(h)=-\frac{1}{2}\mathbb{E}_{\textbf{x}\sim\pi}\left[ \log\left( 1-2e^{2h(\textbf{x})}\right) \right]$, and $\mathcal{H}^{c}$ is the space of function $h$ that satisfies: $h(\textbf{x})< 1/2\log(1/2)$ for all $\textbf{x}\in \mathcal{X}$. We introduce the following change of variable: $h^\prime(\textbf{x})=1-2e^{2h(\textbf{x})}$. It is easy to verify that $0<h^\prime(\textbf{x})<1$ for all $\textbf{x}\in \mathcal{X}$. Then, the variational representation of $JS$ can be rewritten as:
\begin{align*}
     \sup_{h^\prime\in\mathcal{H}^\prime}{\left\{ \mathbb{E}_{\textbf{x}\sim q}\left[ \log\left( 1-h^\prime(\textbf{x}) \right) \right]  + \mathbb{E}_{\textbf{y}\sim \pi}\left[ \log(h^\prime(\textbf{x})) \right]\right\}}
\end{align*}
where $\mathcal{H}^\prime$ is the space of functions whose outputs are in between 0 and 1. As we have access to samples drawn from $q$ and $\pi$, we can estimate $h^\prime$ and then estimate $h$ using: $h(\textbf{x})=\frac{1}{2}\log\left( \frac{1-h^\prime(\textbf{x})}{2} \right)$ for all $\textbf{x}\in\mathcal{X}$.\\




\noindent
\textbf{Experiment settings}. Similar to the experiments for the KL divergence, we compare MYVT and VT in terms of MSE and average $l_{1}$-norm for the first case study, and MSE and average TV semi-norm for the second case study. We set $\alpha=0.01$ and $\alpha=0.1$ for MYVT in the first and second case studies, respectively. We parameterize the function $V$ with a neural network with four layers, each of which has 100 neurons. For the function $h^\prime$, we use another neural network with two layers, each of which has 100 neurons, to parameterize it. To guarantee the outputs of $h^\prime$ to be in $(0,1)$, we use the sigmoid activation function in the last layer. The step sizes for VT and MYVT are set to 0.01 and 0.001, respectively. We run $K=2000$ iterations for the first case study and $K=4000$ iterations for the second case study.\\



\noindent
\textbf{Results}. The results for the first case study are illustrated in Figure \ref{fig:sparsity_JS}. Both methods MYVT and VT are able to generate samples that are similar to the truth, as indicated by the decreasing MSE over 2000 iterations (see Figure \ref{fig:sparsity_JS}a). However, MYVT keeps the average $l_{1}$-norm much lower than that of VT over iterations (see Figure \ref{fig:sparsity_JS}b). In particular, MYVT consistently produce samples with much lower average $l_{1}$-norm (around 16.35) compared to VT (around 31.81). Visually, samples generated by MYVT appear much sparser than those generated by VT (see Figure \ref{fig:sparsity_JS}c and \ref{fig:sparsity_JS}d). 

The results for the second case study are shown in Figure \ref{fig:TV_JS}. The methods are again able to produce samples that are closely resemble the truth, as evidenced by the significant decrease in MSE values over 4000 iterations (see Figure \ref{fig:TV_JS}a). However, MYVT produces samples with a lower average TV semi-norm due to the effect of the TV regularization. Figure \ref{fig:TV_JS}c and \ref{fig:TV_JS}d show example samples generated by VT and MYVT, respectively. The average TV semi-norm of samples generated by MYVT (19.35) is significantly lower than that of samples generated by VT (36.93). These results in both case studies confirm the regularization effects of problem (\ref{eqn:regDP}) on the generated samples and effectiveness of MYVT in the case of JS divergence.





\end{document}







