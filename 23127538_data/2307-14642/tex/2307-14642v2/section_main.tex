
\vspace{-1ex}
\section{MAIN RESULTS}
\vspace{-1ex}
\subsection{Theoretical Analysis of the STL Estimator}\label{section:gradient_variance}
\vspace{-1ex}
\subsubsection{Proof Sketch}
\vspace{-1ex}
Before presenting our analysis on BBVI gradient estimators, we will discuss a notable aspect of our strategy and the key step in our proof.

\vspace{-1ex}
\paragraph{Adaptive Bounds with the Peter-Paul Inequality}
Unlike the QV bounds obtained by \citet{domke_provable_2023}, our bounds involve a free parameter \(\delta \geq 0\).
We call these bounds \textit{adaptive} QV bounds.
%
\begin{assumption}[\textbf{Adaptive QV}]\label{assumption:adaptiveqvc}
  The gradient estimator \(\rvvg\) satisfies the bound
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \[
    \norm{\rvvg\left(\vlambda\right)}_2^2 \leq (1 + C \delta) \, \widetilde{\alpha} \, \norm{\vlambda - \vlambda^*}_2^2 + (1 + C^{-1} \delta^{-1}) \,\widetilde{\beta},
  \]
  }%
  for any \(\delta > 0\), any \(\vlambda \in \Lambda_S\), and some \(0 < \widetilde{\alpha}, \widetilde{\beta} < \infty\), where \(\vlambda^*\) is a stationary point.
\end{assumption}
%
This is a consequence of the use of the ``Peter-Paul'' inequality such that
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{equation}
  {\left( a + b \right)}^2 \leq \left(1 + \delta\right) \,a^2 + (1 + \delta^{-1}) \,b^2,
  \label{eq:peterpaul}
\end{equation}
}%
and can be seen as a generalization of the usual inequality \({\left(a + b\right)}^2 \leq 2 a^2 + 2 b^2\).
As mentioned in \citet[Remark 6]{kim_practical_2023}, adjusting \(\delta\) can occasionally tighten the analysis.
In fact, \(\delta\) can be optimized to become \textit{adaptive} to the downstream analysis.
Indeed, in our complexity analysis, \(\delta\) automatically trades-off the influence of \(\widetilde{\alpha}\) and \(\widetilde{\beta}\) according to the accuracy budget \(\epsilon\).

\vspace{-1ex}
\paragraph{Key Lemma}
The key first step in all of our analysis is the following decomposition:
\input{thm_stl_general_decomposition}

Here, \(J_{\mathcal{T}}\) is a term that stems from the Jacobian of \(\mathcal{T}\).
Thus, \(J_{\mathcal{T}}\) contains the properties unique to the chosen variational family.
\(T_{\text{\ding{182}}}\) and \(T_{\text{\ding{183}}}\) measure how far the current variational approximation \(q_{\vlambda}\) is from a stationary point \({\vlambda^*}\).
Thus, both terms will eventually reach 0 as BBVI converges, regardless of family specification.
The key is \(T_{\text{\ding{184}}}\), which captures the amount of mismatch between the score of the true posterior \(\pi\) and variational posterior \(q_{\vlambda^*}\).
Establishing the ``interpolation condition'' amounts to analyzing when \(T_{\text{\ding{184}}}\) becomes 0.

\vspace{-1ex}
\subsubsection{Upper Bounds}
\vspace{-1ex}
We now present our upper bound on the expected-squared norm of the STL gradient estimator.
%
\input{thm_stl_upperbound}

\begin{remark}[\textbf{Mean-Field Variational Family}]
  We prove an equivalent result for the mean-field variational family, \cref{thm:stl_upperbound_mf} in \cref{section:stl_meanfield}, which has an \(\mathcal{O}\left(\sqrt{d}\right)\) dimensional dependence.
\end{remark}

\begin{remark}[\textbf{Interpolation Condition}]
  The encompasses both settings where the variational family is well-specified and misspecified.
  That is, when the variational family is well specified, \textit{i.e.}, \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) = 0 \), we obtain interpolation such that \(\beta_{\mathrm{STL}} = 0\).
\end{remark}

\begin{remark}[\textbf{Adaptivity of Bound}]
  When the variational family is well specified such that \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) = 0 \), we can adaptively tighten the bound by setting \(\delta = 0\), where \(\alpha_{\mathrm{STL}}\) is reduced by a constant factor.
\end{remark}

\vspace{-1ex}
\subsubsection{Lower Bounds}
\vspace{-1ex}
We also obtain lower bounds on the expected-squared norm of the STL estimator to analyze its best-case behavior and the tightness of the bound.

\vspace{-1ex}
\paragraph{Necessary Conditions for Interpolation}
First, we obtain lower bounds that generally hold for all \(\vlambda \in \Lambda_L\) and any \(\pi\).
Our analysis relates the gradient variance with the Fisher-Hyv\"arinen divergence.
This can be related back to the KL divergence through an assumption on the posterior \(\pi\) known as the log-Sobolev inequality.
The general form of the log-Sobolev inequality was originally proposed by \citet{gross_logarithmic_1975} to study diffusion processes.
In this work, we use the form used by \citet{otto_generalization_2000}:
%
\begin{assumption}[\textbf{Log-Sobolev Inequality; LSI}]
\(\pi\) is said to satisfy the log-Sobolev inequality if, for any variational family \(\mathcal{Q}\) and all \(q_{\vlambda} \in \mathcal{Q}\), the following inequality holds:
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{0ex} \setlength{\abovedisplayshortskip}{0ex}
\[
  \DKL{q}{\pi}
  \leq
  \frac{C_{\mathrm{LSI}}}{2} \, \DHF{q}{\pi}.
\]
}%
\end{assumption}
\vspace{-1ex}
%
Strongly log-concave distributions are known to satisfy the LSI, where the strong log-concavity constant becomes the (inverse) LSI constant. 
This is known as the Bakry-\'Emery Theorem~\citep{bakry_diffusions_1985}. (See also~\citealp[Theorem 9.9]{villani_topics_2016}).
\begin{remark}[\citealp{bakry_diffusions_1985}]
    Let \(\pi\) be \(\mu\)-strongly log-concave.
    Then, it satisfies the LSI with \(C_{\mathrm{LSI}}^{-1} = \mu\).
\end{remark}

We now present our lower bound which holds for all \(\vlambda \in \Lambda_S\) and any differentiable \(\pi\):
%
\input{thm_stl_lowerbound}

\begin{corollary}[\textbf{Necessary Conditions for Interpolation}]
For the STL estimator, the interpolation condition does not hold if
\begin{enumerate}[label=\textbf{(\roman*)}]
  \vspace{-1.5ex}
  \setlength\itemsep{0.ex}
    \item \(\DHF{q_{\vlambda^*_{\mathrm{F}}}}{\pi} > 0\), or,
    \item when \(\pi\) is LSI, \(\DKL{q_{\vlambda^*_{\mathrm{KL}}}}{\pi} > 0\),
  \vspace{-1.5ex}
\end{enumerate}
%
  \begin{center}
   {\begingroup
    \setlength\tabcolsep{10pt} 
  \begin{tabular}{ll}
    \text{where }
    &
    \(\vlambda_{\mathrm{F}}^* \in \argmin_{\vlambda \in \Lambda_S} \DHF{q_{\vlambda}}{\pi} \),\; 
    \text{and} \\
    & \(\vlambda_{\mathrm{KL}}^* \in \argmin_{\vlambda \in \Lambda_S} \DKL{q_{\vlambda}}{\pi} \),
  \end{tabular}
  \endgroup}
  \end{center}
  \vspace{-1.5ex}
  for any \(0 < S < \infty\).
\end{corollary}
\vspace{-1ex}

\vspace{-1.ex}
\paragraph{Tightness Analysis}
The bound in \cref{thm:stl_lowerbound} is unfortunately not tight regarding the constants.
It, however, holds for all \(\vlambda\) and \(\pi\).
Instead, we establish an alternative lower bound that holds for a subset of \(\vlambda\) and \(\pi\) but is tight regarding the constants.
%
\input{thm_stl_lowerbound_unimprovability}
%
\begin{remark}\label{remark:stl_tightness}
  \cref{thm:stl_lowerbound_unimprovability} implies that \cref{thm:stl_upperbound_corollary} is tight with respect to the dimension dependence \(d\) and the log-smoothness \(L\) except for a factor of 4.
\end{remark}

\begin{remark}[\textbf{Room for Improvement}]
  Part of the factor of \(4\) looseness is due to the extreme worst case: when \(\nabla \log \pi\) and \(\nabla \log q_{\vlambda}\) are perfectly anti-correlated.
  This worst case is unlikely to appear in practice, thus making a tighter lower bound challenging to obtain.
  But at the same time, we were unsuccessful at seeking a general assumption that would rule out these worst cases in the upper bound.
  Specifically, we tried very hard to apply coercivity/gradient monotonicity of log-concave distributions, but to no avail, leaving this to future works.
\end{remark}

\vspace{-1.ex}
\subsection{Theoretical Analysis of the CFE Estimator}
\vspace{-1.ex}
We now present the analysis of the CFE estimator.
While the CFE estimator has been studied in-depth by \citet{domke_provable_2019,kim_practical_2023,domke_provable_2023}, we slightly improve the latest analysis of \citet[Theorem 3]{domke_provable_2023}.
Specifically, we improve the constants and obtain an adaptive bound.
This ensures that we have a fair comparison with the STL estimator.
%
\input{thm_cfe_upperbound}

\begin{corollary}
  Let \(S = L\).
  Then, the expected-squared norm of the CFE estimator with the full-rank parameterization satisfies the QVC for any \(\vlambda \in \Lambda_{L}\) with the constants
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \begin{align*}
    \alpha_{\mathrm{CFE}} &= L^2 \left( d + k_{\varphi} + 4 \right) \left(1 + \delta\right),\\
    \beta_{\mathrm{CFE}}  &= L^2 \left( d + k_{\varphi} \right) \left( 1 + \delta^{-1}  \right) {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2,
  \end{align*}
  }%
  for any \(\delta \geq 0\).
\end{corollary}

\begin{remark}[\textbf{Comparison with STL}]\label{remark:variance_comparison}
    Compared to the STL estimator, the constant \(\alpha\) of the CFE estimator is tighter by a factor of \(4\).
    Considering \cref{thm:stl_lowerbound_unimprovability}, the constant factor difference should be marginal in practice.
    %This means that the STL estimator will perform similarly when \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) large.
    % When \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) is small, it might have a smaller variance when \(\mathrm{D}_{F^4}\left(q_{\vlambda^*}, \pi\right)\) is small.
\end{remark}

\begin{remark}[\textbf{Intuitions on \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\)}]
  The quantity \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) can be expressed in the Wasserstein-2 distance as
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \[
    \mathrm{d}_{\mathcal{W}_2}\left(q_{\vlambda^*}, \delta_{\bar{\vz}}\right) = \sqrt{ {\lVert \vm^* - \bar{\vz}\rVert}_2^2 + \norm{\mC^*}_{\mathrm{F}}^2} =  {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2,
  \]
}
  where \(\delta_{\bar{\vz}}\) is a delta measure centered on the posterior mode \(\bar{\vz}\).
   Also, when the variational posterior mean \(\vm^*\) is close to \(\bar{\vz}\) such that  \({\lVert \vm^* - \bar{\vz}\rVert}_2^2 \approx 0\), \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) corresponds to the variational posterior variance as
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \[
     {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2 \approx \norm{\mC^*}_{\mathrm{F}}^2 = \mathrm{tr}\, \Vsub{\rvvz \sim q_{\vlambda^*}}{\rvvz}.
  \]
}
\end{remark}

% \vspace{-1.ex}
% \subsection{Non-Asymptotic Complexity of SGD with the Quadratic Variance Condition}

\input{thm_projsgd_stronglyconvex_fixedstepsize}
\input{thm_projsgd_stronglyconvex_decstepsize}
% \input{thm_projsgd_convex_fixedstepsize}

%% \begin{remark}
%%   The bound for \cref{thm:projsgd_convex_fixedstepsize} is inconveniently non-linear with respect to \(\beta\) and \(\alpha\).
%%   Under ``interpolation'' such that \(\beta = 0\), it clearly reduces to a \(\mathcal{O}\left(1/\epsilon\right)\).
%%   When \(\beta > 0\) is non-negligible, a series expansion does suggest that the bound behaves as \(\mathcal{O}\left(1/\epsilon^2\right)\), which is the expected complexity guarantee~\citep{garrigos_handbook_2023}.
%% \end{remark}

%% \begin{remark}
%%   Note that \cref{thm:projsgd_convex_fixedstepsize} is quite loose due to the use of Bernoulli's inequality for simplifying the any-time convergence result by \citet[Theorem 7]{domke_provable_2023}. (This inequality is also used by \citeauthor{domke_provable_2023}.) 
%%   Tightening the any-time convergence statement would be an important future direction.
%% \end{remark}

% \begin{remark}
%   Lastly, obtaining a \(\mathcal{O}\left(1/\sqrt{T}\right)\) convergence guarantee with a decreasing stepsize schedule independent of the number of steps \(T\) is an open problem.
%   (\citet[Theorem 7]{domke_provable_2023} use a fixed stepsize dependent on \(T\). Thus, one must fix the number of steps before running projected SGD.) 
% \end{remark}

\vspace{-2.ex}
\subsection{Non-Asymptotic Complexity of Black-Box Variational Inference}\label{section:bbvicomplexity}
\vspace{-1.ex}
We now apply the general complexity results obtained in the previous section to BBVI.
We will focus on 
\begin{enumerate*}[label=\textbf{(\roman*)}]
    \item strongly log-concave posteriors,
    \item SGD run with fixed stepsizes, and 
    \item the full-rank variational family.
\end{enumerate*}
This is because the convergence analyses for \textbf{(ii)} \(\cap\) \textbf{(iii)} are the tightest.
Although the bounds for the mean-field parameterization have better dependences on \(d\), they have not been shown to be tight and empirically appear loose~\citep{kim_practical_2023}. (See also \citealp[Conjecture 1]{kim_blackbox_2023}.)

\vspace{-1.ex}
\paragraph{Complexity with Adaptive QV Estimators}
As mentioned in \S, we established \textit{adaptive} QV bounds.
For the complexity guarantees for strongly convex objectives (\cref{thm:projsgd_stronglyconvex_fixedstepsize,thm:projsgd_stronglyconvex_decstepsize}), it is possible to optimize the free parameter \(\delta\) in the bounds, such that they automatically adapt to other problem-specific constants.
The following generic lemma does this:
%
\input{thm_projsgd_stronglyconvex_adaptive_complexity}

\begin{remark}
  The analogous result for the decreasing stepsize schedule, \cref{thm:projsgd_stronglyconvex_decstepsize}, is unfortunately quite ugly and had to be omitted from the main text.
  It can be found in \cref{section:complexity_adaptiveqvc}.
\end{remark}

\vspace{-1.ex}
\paragraph{Complexity of BBVI on Strongly-Log-Concave \(\pi\)}
We can now plug in the constants obtained in \cref{section:gradient_variance}.
This immediately establishes the iteration complexity resulting from the use of different gradient estimators.
%
\input{thm_projsgd_bbvicfe_stronglyconvex_complexity}

And in particular, the following theorem establishes that BBVI with the STL estimator can achieve linear convergence under perfect variational family specification.

\input{thm_projsgd_bbvistl_stronglyconvex_complexity}

\begin{corollary}[\textbf{Linear Convergence of BBVI with STL}]
  If the variational family is perfectly specified such that \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda}^*, \pi\right) = 0\) for \(\vlambda^* = \argmin_{\vlambda \in \Lambda_L} F\left(\vlambda\right)\), then BBVI with the STL estimator converges linearly with a complexity of \(\mathcal{O}\left(d \kappa^2 \log \left( 1 / \epsilon \right) \right)\).
\end{corollary}

% \begin{remark}
%   The overall complexity of the STL estimator is larger by a constant factor of 4 compared to that of CFE.
%   This means that the STL estimator will converge slowly for a large target \(\epsilon\).
%   As mentioned in \cref{remark:stl_tightness,remark:variance_comparison}, a factor of 2 difference would be more realistic and is closer to what is observed in our simulations.
% \end{remark}

\begin{remark}
  Convergence is slowed when using a decreasing step size schedule, as shown in \cref{thm:projsgd_bbvistl_decstepsize_complexity}.
  Thus, one does not achieve a linear convergence rate under this schedule even if the variational family is perfectly specified. 
  However, when the variational family is misspecified, this achieves a better rate of \(\mathcal{O}\left(1/\epsilon\right)\) compared to the \(\mathcal{O}\left(1/\epsilon \log 1/\epsilon\right)\) of \cref{thm:projsgd_bbvistl_complexity}.
\end{remark}

\begin{remark}[\textbf{Variational Family Misspecification}]\label{remark:misspecification}
  Under variational family misspecification, STL has an \(\mathcal{O}\left(1/\epsilon\right)\) dependence on the 4th order Fisher divergence \(\mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) > 0\).
  To compare the computational performance of CFE and STL in this setting, one needs to compare \( L^{-2} \sqrt{\mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right)}\) versus \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\).
\end{remark}

\begin{remark}
  \cref{thm:stl_upperbound_mf} also implies that the mean-field parameterization improves the dimension dependence to a complexity of \(\mathcal{O}\left( \sqrt{d} \kappa^2 \log \left( 1 / \epsilon \right) \right)\).
\end{remark}

%% \paragraph{Complexity of BBVI on Log-Concave Posteriors}
%% Lastly, we present the complexity of BBVI with the STL estimator when \(\pi\) is only log-concave.
%% Since the complexity guarantee of \cref{thm:projsgd_convex_fixedstepsize} is too nonlinear with respect to \(\beta\), we present the case where the variational family is well specified (\(\beta_{\mathrm{STL}} = 0\)).
%% In this setting, the STL estimator achieves a better complexity than that of the CFE estimator.

%% \input{thm_projsgd_bbvistl_convex_complexity}

%% \subsection{Variational Family Misspecification and the STL Estimator}

%% As mentioned in \cref{remark:misspecification}, comparing the performance of the STL estimator against the CFE estimator involves the 4th-order Fisher divergence and \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\).
%% Unfortunately, the relationship between the two quantities is not obvious.
%% Because of this, we further reduce our scope to Gaussians in order to gather more intuition.

%% Recall that term \(T_{\ding{184}}\) in \cref{thm:stl_decomposition} captures the effect of variational family specification on the STL estimator.
%% Assuming both the posterior and variational approximation are Gaussians, we can obtain a simpler result on \(T_{\ding{184}}\).

\vspace{-1ex}
\subsection{Should we stick the landing?}
\vspace{-1ex}
When the variational family is misspecified, it is hard to tell \textit{when} STL would be superior to CFE; the Fisher-Hyv\"arinen divergence and the posterior variance are fundamentally unrelated quantities.
Furthermore, the Fisher-Hyv\"arinen divergence is hard to interpret apart from some relationships with other divergences~\citep{huggins_practical_2018}.
Thus, we lastly provide some characterization of the Fisher-Hyv\"arinen divergence.

Our final analysis will focus on Gaussian posteriors and the mean-field Gaussian family.
In practice, the STL estimator becomes infeasible to use with full-rank variational families as each evaluation of the log-density \(\log q_{\vlambda}\) involves a back-substitution with a \(\mathcal{O}\left(d^3\right)\) cost and numerical stability becomes a concern.
Therefore, studying the effect of misspecification due to the mean-field approximation is particularly relevant.

\input{thm_stl_gaussian}

\begin{remark}
For Gaussians, the 4th-order Fisher-Hyv\"arinen divergence term in \cref{thm:stl_upperbound} can be replaced by its 2nd-order counterpart.
Thus, combined with \cref{thm:stl_lowerbound}, the 2nd-order Fisher-Hyv\"arinen divergence fully characterizes the variance of STL.
\end{remark}

\begin{remark}
  \cref{thm:fisher_bound} implies that, when approximating a full-rank Gaussian with a mean-field Gaussian, the value of the Fisher-Hyv\"arinen divergence is tightly characterized by the degree of correlation in the posterior; it will increase indefinitely as the posterior correlation matrix becomes singular.
\end{remark}

\begin{remark}
  We have provided a sufficient condition for the STL estimator to perform poorly compared to the CFE estimator. 
  It is foreseeable that alternative types of model misspecification abundant in practice should yield additional sufficient conditions, \textit{i.e.}, tail mismatch, but we leave this to future works.
\end{remark}

