
% \begin{theorem}[Proj. SGD for convex \(f\) with a decreasing stepsize]
%   For a convex \(f\),
%   \begin{alignat*}{2}
%     \mathbb{E}
%     F \left(\bar{\vlambda}_T\right)
%     -
%     F^*
%     \leq
%     \frac{
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     }
%     +
%     \frac{
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t,
%     }
%   \end{alignat*}
%   where the sequence \(w_{0}, \ldots, w_{T}\) with the initial condition \(w_0 = 1\) follows the recursion
%   \[
%     w_{t+1} = \prod^{t}_{t'=0} {\left( 1 + \gamma_{t'}^2 \alpha \right)}^{-1}
%   \]
%   and \(\bar{\vlambda}_{T}\) is the weighted average
%   \[
%     \bar{\vlambda}_{T} = \sum^{T-1}_{t=1} w_{t+1} \gamma_t \vlambda_t \,\big/\, \sum^{T-1}_{t=1} w_{t+1} \gamma_t.
%   \]
% \end{theorem}
% \begin{proof}
%   \begin{alignat*}{2}
%     \norm{ \vlambda_{t+1} - \vlambda^* }_2^2
%     &=
%     {\lVert\,
%       \mathrm{proj}_{\Lambda}\left( \vlambda_{t} - \gamma_t \rvvg\left(\vlambda_t\right) \right)  
%       -
%       \mathrm{proj}_{\Lambda}\left( \vlambda^* \right)  
%     \,\rVert}_2^2
%     \\
%     &\leq
%     \norm{ 
%       \vlambda_{t} - \gamma_t \rvvg\left(\vlambda_t\right)
%       - \vlambda^*
%     }_2^2
%     \\
%     &=
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     - 
%     2 \gamma_t 
%     \inner{
%       \rvvg\left(\vlambda_t\right)
%     }{
%       \vlambda_{t} - \vlambda^*
%     }
%     +
%     \gamma_t^2 \norm{ \rvvg\left(\vlambda_t\right) }_2^2
%   \end{alignat*}
%   Taking expectation,
%   \begin{alignat*}{2}
%     \E{ \norm{ \vlambda_{t+1} - \vlambda^* }_2^2 \mid \vlambda_{t} }
%     &\leq
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     - 
%     2 \gamma_t 
%     \inner{
%       \nabla F \left(\vlambda_t\right)
%     }{
%       \vlambda_{t} - \vlambda^*
%     }
%     +
%      \gamma_t^2 \E{ \norm{ \rvvg\left(\vlambda_t\right) }_2^2 \mid \vlambda_{t} }
%     \\
%     &\leq
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     - 
%     2 \gamma_t 
%     \left(
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     +
%     \E{ \norm{ \rvvg\left(\vlambda_t\right) }_2^2 \mid \vlambda_{t} }
%     \\
%     &\leq
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     - 
%     2 \gamma_t 
%     \left(
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     +
%     \gamma_t^2 \alpha \norm{ \vlambda_t - \vlambda^* }_2^2 
%     +
%     \gamma_t^2 \beta
%     \\
%     &=
%     \left( 1 + \gamma_t^2 \alpha \right)
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     - 
%     2 \gamma_t 
%     \left(
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     +
%     \gamma_t^2 \beta.
%   \end{alignat*}

%   Re-ordering and taking full expectation, 
%   \begin{alignat}{2}
%     2 \gamma_t 
%     \left(
%        \mathbb{E}
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     \leq
%     \left( 1 + \gamma_t^2 \alpha \right)
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     -
%     \mathbb{E} \norm{ \vlambda_{t+1} - \vlambda^* }_2^2
%     +
%     \gamma_t^2 \beta.
%     \label{eq:convex_decstep_eq1}
%   \end{alignat}
%   The fundamental contribution of \citet{domke_provable_2023} is that one can obtain a telescoping sum from here, which is based on the strategy of \citet{stich_unified_2019}.
%   It suffices to multiply a constant \(w_{t+1}\) to \cref{eq:convex_decstep_eq1}, where the series \(w_1, \ldots, w_{T+1}\) is subject to the constraint that \(w_{t} = w_{t+1} \left(1 + \gamma_t^2 \alpha\right)\).
%   Then, by adding summing the terms over \(t = 0, \ldots, T-1\),
%   \begin{alignat*}{2}
%     &\sum_{t=0}^{T-1}
%     2 w_{t+1}
%     \gamma_t 
%     \left(
%        \mathbb{E}
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     \\
%     &\;\leq
%     \sum_{t=0}^{T-1}
%     w_{t+1}
%     \left( 1 + \gamma_t^2 \alpha \right)
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     -
%     w_{t+1} \mathbb{E} \norm{ \vlambda_{t+1} - \vlambda^* }_2^2
%     +
%     w_{t+1} \gamma_t^2 \beta
%     \\
%     &\;\leq
%     \sum_{t=0}^{T-1}
%     w_{t+1}
%     \left( 1 + \gamma_t^2 \alpha \right)
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     -
%     w_{t+1} \mathbb{E} \norm{ \vlambda_{t+1} - \vlambda^* }_2^2
%     +
%     w_{t+1} \gamma_t^2 \beta
%     \\
%     &\;=
%     \sum_{t=0}^{T-1}
%     w_t
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{t} - \vlambda^*
%     }_2^2
%     -
%     w_{t+1} \mathbb{E} \norm{ \vlambda_{t+1} - \vlambda^* }_2^2
%     +
%     w_{t+1} \gamma_t^2 \beta
%     \\
%     &\;=
%     w_0
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     -
%     w_{T} \mathbb{E} \norm{ \vlambda_{T} - \vlambda^* }_2^2
%     +
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta 
%     \\
%     &\;\leq
%     w_0
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     +
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta
%     \\
%     &\;=
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     +
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta
%   \end{alignat*}
%   From here, \citet{domke_provable_2023} only considered the case where \(\gamma_t\) is fixed.
%   We generalize this result to the case where \(\gamma_t\) is chosen to be a proper sequence.
%   \begin{alignat*}{2}
%     &
%     \frac{
%     \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     \left(
%        \mathbb{E}
%        F \left(\vlambda_t\right)
%        -
%        F^*
%     \right)
%     }{
%     \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     }
%     \leq
%     \frac{
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     }
%     +
%     \frac{
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     }
%   \end{alignat*}
%   Then, the result follows from Jensen's inequality by regarding the weighted sum on the right-hand side as an average.
% \end{proof}

% \begin{corollary}[Proj. SGD for convex \(f\) with a decreasing stepsize]
%   For a fixed stepsize \(\gamma_t = 1/\sqrt{t+1}\) and a convex \(f\),
%   \begin{alignat*}{2}
%     \mathbb{E}
%     F \left(\bar{\vlambda}_T\right)
%     -
%     F^*
%     \leq
%     \frac{
%     \mathbb{E}
%     \norm{ 
%       \vlambda_{0} - \vlambda^*
%     }_2^2
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t 
%     }
%     +
%     \frac{
%     \sum_{t=0}^{T-1} w_{t+1} \gamma_t^2 \beta
%     }{
%     2 \sum_{t=0}^{T-1}
%     w_{t+1}
%     \gamma_t,
%     }
%   \end{alignat*}
% \end{corollary}
% \begin{proof}
%   The results amounts to analyzing the convergence of the sequence \(w_{1} \gamma_0, \ldots, w_{T} \gamma_{T-1},\) for our specific stepsize choice.

%   First, notice that the values of \(w_{t+1}\) can be denoted as
%   \begin{align*}
%     w_{t+1}
%     &=
%     \prod^{t}_{t'=0} {\left( 1 + \gamma_{t'}^2 \alpha \right)}^{-1}
%     \\
%     &=
%     \exp\left(
%       \sum^{t}_{t'=0} \log\left( {\left( 1 + \alpha \gamma^2_{t'} \right)}^{-1} \right) 
%     \right)
%     \\
%     &=
%     \exp\left(
%       \sum^{t}_{t'=0} \log\left( {\left( 1 + \frac{\alpha \gamma^2}{t+1} \right)}^{-1} \right) 
%     \right)
%     \\
%     &=
%     \exp\left(
%       \sum^{t}_{t'=0} \log\left( \frac{t+1}{t + 1 + \alpha \gamma^2} \right) 
%     \right).
%   \end{align*}
  
%   \(\nu = \alpha \gamma^2\) 

%   Since \(\exp\left(\cdot\right)\) is monotonic, we can use the Riemann integral bounds 
%   \[
%     \int_{1}^{t} 
%       \log\left( \frac{t' + 1}{t' + 1 + \nu} \right) 
%     \mathrm{d}t'
%     \leq
%     \sum^{t}_{t'=0}  \log\left( \frac{t' + 1}{t' + 1 + \nu} \right)
%     \leq
%     \int_{0}^{t} 
%       \log\left( \frac{t' + 1}{t' + 1 + \nu} \right).
%     \mathrm{d}t'
%   \]
%   For the integrals, we have
%   \begin{align*}
%     \exp\left(
%     \int_{1}^{t} 
%       \log\left( \frac{t' + 1}{t' + 1 + \nu} \right) 
%     \mathrm{d}t'
%     \right)
%     &=
%     {\left(1 + \nu \right)}^{(1 + \nu)}
%     {\left(1 + t \right)}^{(1 + t)}
%     {\left(1 + \nu + t \right)}^{-(1 + \nu + t)}
%     \\
%     &\geq
%     {\left(1 + \nu \right)}^{(1 + \nu)}
%     {\left(1 + t \right)}^{(1 + t)}
%     {\left(1 + \nu + t \right)}^{-(1 + \nu + t)}
%     \\
%     \exp\left(
%     \int_{0}^{t} 
%       \log\left( \frac{t' + 1}{t' + 1 + \nu} \right)
%     \mathrm{d}t'
%     \right)
%     &=
%     \frac{1}{4}
%     {\left(2 + \nu \right)}^{(2 + \nu)}
%     {\left(1 + t \right)}^{(1 + t)}
%     {\left(1 + \nu + t \right)}^{-(1 + \nu + t)}
%     \\
%     &\leq
%     \frac{1}{4}
%     {\left(2 + \nu \right)}^{(2 + \nu)}
%     {\left(1 + \nu + t \right)}^{(1 + t)}
%     {\left(1 + \nu + t \right)}^{-(1 + \nu + t)}
%     \\
%     &=
%     \frac{1}{4}
%     {\left(2 + \nu \right)}^{(2 + \nu)}
%     {\left(1 + \nu + t \right)}^{- \nu}
%   \end{align*}
% \end{proof}