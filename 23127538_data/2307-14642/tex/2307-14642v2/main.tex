
\documentclass[twoside]{article}

\usepackage[accepted]{aistats2024}
% If your paper is accepted, change the options for the package
% aistats2024 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

\usepackage[sort&compress,round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{setspace}
\usepackage{nicefrac}
\usepackage{pgfplots}
\usepackage{float}
\usepackage[inline]{enumitem}
\usepackage{pifont}
\usepackage{tabularx,booktabs,threeparttable,makecell,colortbl}
% \usepackage[13-]{pagesel}

\usepackage{calc}

\input{krkmath}

\usepackage{etoc}
%\etocmulticolstyle{\noindent\bfseries\footnotesize
%\leaders\hrule height1pt\hfill
%\MakeUppercase{Contents}}
\etocframedstyle[1]{\textbf{\textsc{Table of Contents}}}
\etocsettocdepth{3}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
\runningtitle{Linear Convergence of Black-Box Variational Inference}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Linear Convergence of Black-Box Variational Inference: \\ Should We Stick the Landing?}

\aistatsauthor{ Kyurae Kim \And Yi-An Ma \And Jacob R. Gardner }

\aistatsaddress{ University of Pennsylvania \And University of California San Diego \And  University of Pennsylvania }
]

\begin{abstract}
  We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called ``linear'') rate under perfect variational family specification.
  In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families.
  Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent.
  We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator, and provide explicit non-asymptotic complexity guarantees for both.
\end{abstract}

\input{section_introduction}
\input{section_background}
\input{section_main}
%\input{section_experiments}
\input{section_discussions}

\subsubsection*{Acknowledgements}
The authors would like to sincerely thank Jisu Oh for thoroughly proofreading the paper and Kaiwen Wu for helpful discussions.

%\clearpage
\bibliographystyle{rss}
\bibliography{references}

\clearpage
\input{section_checklist}

\newpage
\appendix

% \etocdepthtag.toc{mtappendix}
% \etocsettagdepth{mtchapter}{none}
% \etocsettagdepth{mtappendix}{part}

\onecolumn

\aistatstitle{Linear Convergence of Black-Box Variational Inference: \\ Should We Stick the Landing?\\
\textit{Supplementary Material}}

\vspace{-4ex}
\section{OVERVIEW OF THEOREMS}
\input{table_theorems}

\newpage
{\hypersetup{linkcolor=black}
\tableofcontents
% \localtableofcontents
}

\newpage
\input{section_related_works}

\onecolumn

\newpage
\section{PROOFS}\label{section:proofs}

\input{section_definitions}

% \newpage
% \subsection{Miscellaneous Propositions}
% \vspace{1ex}
% \input{thm_peterpaul}
% \printProofs[logsobolev]
% \printProofs[misc]

\subsection{Auxiliary Lemmas}
\vspace{1ex}
\input{section_external_lemmas}
\printProofs[external]
\printProofs[gradvarlemmas]

\clearpage
\subsection{Upper Bound on Gradient Variance of STL}

\subsubsection{General Decomposition}
\vspace{1ex}
\printProofs[stlupperboundlemma]

\newpage
\subsubsection{Full-Rank Parameterization}
\vspace{1ex}
\printProofs[stlupperboundfr]

\newpage
\subsubsection{Mean-Field Parameterization}\label{section:stl_meanfield}
\vspace{1ex}
\printProofs[stlupperboundmf]

\clearpage
\subsection{Lower Bound on Gradient Variance of STL}
\subsubsection{General Lower Bound}
\vspace{1ex}
\printProofs[stllowerbound]

\newpage
\subsubsection{Unimprovability}
\vspace{1ex}
\printProofs[stllowerboundunimprovability]

\clearpage
\subsection{Upper Bound on Gradient Variance of CFE}
\subsubsection{Full-Rank Parameterization}
\vspace{1ex}
\printProofs[cfeupperbound]

\newpage
\subsubsection{Mean-Field Parameterization}
\vspace{1ex}
\printProofs[cfeupperboundmf]

\clearpage
\subsection{Non-Asymptotic Complexity of Projected SGD}

To precisely compare the computational complexity resulting from different estimators, we refine the convergence analyses of~\citet{domke_provable_2023}.
Specifically, we obtain precise complexity guarantees from their ``anytime convergence'' statements.
This type of convergence analysis, which has been popular in the ERM sample selection strategy literature~\citep[\S 1.1]{csiba_importance_2018}, is convenient for comparing the lower-order or even constant factor improvements of different gradient estimators.

\subsubsection{QVC Gradient Estimator}
\vspace{1ex}
\printProofs[complexityprojsgdqvcfixed]
\newpage
\printProofs[complexityprojsgdqvcdec]

\clearpage
\subsubsection{Adaptive QVC Gradient Estimator}\label{section:complexity_adaptiveqvc}
\vspace{1ex}
\printProofs[complexityprojsgdadaptiveqvcfixed]

\newpage
\printProofs[complexityprojsgdadaptiveqvcdec]

\clearpage
\subsection{Non-Asymptotic Complexity of BBVI}
\subsubsection{CFE Gradient Estimator}
\vspace{1ex}
\printProofs[complexitybbvicfefixed]
\newpage
\printProofs[complexitybbvicfedec]

\newpage
\subsubsection{STL Gradient Estimator}
\vspace{1ex}
\printProofs[complexitybbvistl]
\newpage
\printProofs[complexitybbvistldec]

\newpage
\subsection{Fisher-Hyv\"arinen Divergence of Gaussians}
\subsubsection{Bounds on the Gradient Variance of STL}
\vspace{1ex}
\printProofs[stlgaussianfisher]
\newpage
\printProofs[gaussianklmeanfield]
\newpage
\printProofs[stlgaussian]


%\newpage
%\subsection{Connections with Other Gradient Variance Conditions}
%\vspace{1ex}
%\printProofs[gradientconditions]


%\printProofs

\end{document}
