
\begin{theoremEnd}[category=complexityprojsgdqvc]{theorem}[\textbf{Convex \(F\) with a fixed stepsize}]\label{thm:projsgd_convex_fixedstepsize}
  For a convex \(F : \Lambda \to \mathbb{R}\) on a convex set \(\Lambda\), the last weighted average of the iterates generated by projected SGD, \(\bar{\vlambda}_T = \sum^{T}_{t=0}  w^{t+1} \vlambda_t \big/ \sum^T_{t=0} w^{t+1}\), where \(w = 1 / \left(1 + \alpha \gamma^2 \right)\), satisfies \( F\left(\bar{\vlambda}_T\right) - F\left(\vlambda^*\right) \leq \epsilon \), where \(\vlambda^* \in \argmin_{\vlambda \in \Lambda} F\left(\vlambda\right)\), if
  \[
  \gamma =
  \frac{
    -\beta + \sqrt{
      \beta^2 + 8 \epsilon^2 \alpha 
    } 
  }{
    4 \epsilon \alpha
  }
  \quad\text{and}\quad
  T
  \geq
  \frac{2 \alpha}{ -\beta + \sqrt{ \beta^2 + 8 \epsilon^2 \alpha } }
  \, \norm{\vlambda_0 - \vlambda^*}_2^2.
  \]
\end{theoremEnd}
\begin{proofEnd}
  Theorem 7 of \citet{domke_provable_2023} states that
  \[
     F\left(\bar{\vlambda}_T\right) - F\left(\vlambda^*\right)
     \leq
     \frac{ \gamma \alpha  }{ 2 \left( 1 - \theta^{T} \right) } \norm{\vlambda_0 - \vlambda^*}_2^2 + \frac{\gamma \beta}{2},
  \]
  where \(\theta = 1 / (1 + 2 \alpha \gamma^2)\).
  For the first term, we generalize of Lemma 28 of \citet{domke_provable_2023} as
  \begin{align}
    \frac{\gamma \alpha }{2 \left( 1 - \theta^{T} \right) } 
    &\leq \frac{\gamma \alpha}{2 \left( 1 - \theta \right) {T}}
    =    \frac{\gamma \alpha}{2 \left( 1 - \frac{1}{1 + 2 \alpha \gamma^2} \right) {T}}
    =    \frac{\gamma \alpha}{2 \left( \frac{2 \alpha \gamma^2 }{1 + 2 \alpha \gamma^2} \right) {T}}
    =    \frac{ 1 + 2 \alpha \gamma^2 }{4 \gamma} \frac{1}{T}
    \nonumber
    \\
    &=    \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) \frac{1}{T}.
    \label{eq:stl_convex_fixedstepsize_optterm}
  \end{align}
  Note that the use of Bernoulli's inequality here is quite loose.
  Originally, \citeauthor{domke_provable_2023} chose \(\gamma = 1/\sqrt{T}\), which means one needs to fix the number of SGD iterations before actually running the algorithm.
  We instead prove convergence with a stepsize independent of \(T\).

  Based on \cref{eq:stl_convex_fixedstepsize_optterm}, the fixed-stepsize any-time convergence result becomes
  \[
     F\left(\bar{\vlambda}_T\right) - F\left(\vlambda^*\right)
     \leq
     \underbrace{
       \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) \frac{1}{T} \, \norm{\vlambda_0 - \vlambda^*}_2^2
     }_{\text{optimization term}}
     +
     \underbrace{
       \frac{\gamma \beta}{2}.
     }_{\text{statistical term}}
  \]

  Define
  \begin{align*}
    \epsilon_{\mathrm{opt.}}
    \triangleq
    \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) \frac{1}{T} \, \norm{\vlambda_0 - \vlambda^*}_2^2,
    %
    \quad\text{and}\quad
    %
    \epsilon_{\mathrm{stat.}}
    \triangleq
    \frac{\gamma \beta}{2}.
  \end{align*}
  We aim to solve the convex program:
  \begin{alignat*}{3}
    &\minimize_{T, \gamma}\quad  &&T \\
    &\text{subject to}\quad && \epsilon_{\text{opt.}}\left(T, \gamma\right) + \epsilon_{\text{stat.}}\left(\gamma\right) = \epsilon \\
    & && \gamma > 0,\quad T > 0.
  \end{alignat*}
  The Lagrangian is given as
  \begin{alignat*}{2}
    \mathcal{L}\left(T, \gamma, \lambda\right)
    =
    T + \lambda \left(\epsilon_{\text{opt.}}\left(T, \gamma\right) + \epsilon_{\text{stat.}}\left(\gamma\right) - \epsilon \right)
  \end{alignat*}
  with respect to the Lagrangian multiplier \(\lambda\).

  The stationary point of \(\mathcal{L}\) is found by solving the system of equations
  \begin{alignat*}{4}
    \frac{\partial \mathcal{L}}{\partial T}
    &=
    \;
    1 + \lambda \frac{\partial \epsilon_{\text{opt.}}}{\partial T}
    \quad&&=
    0
    &&\qquad\Leftrightarrow\qquad
    -\lambda \frac{\partial \epsilon_{\text{opt.}}}{\partial T}
    &&=
    1 
    \\
    %
    \frac{\partial \mathcal{L}}{\partial \lambda}
    &=
    \;
    \epsilon_{\text{opt.}}\left(T, \gamma\right)
    +
    \epsilon_{\text{stat.}}\left(\gamma\right)
    - 
    \epsilon
    &&=
    0
    &&\qquad\Leftrightarrow\qquad
    \epsilon_{\text{opt.}}\left(T, \gamma\right)
    +
    \epsilon_{\text{stat.}}\left(\gamma\right)
    &&=
    \epsilon
    \\
    %
    \frac{\partial \mathcal{L}}{\partial \gamma}
    &=
    \;
    \lambda
    \left(
    \frac{\partial \epsilon_{\text{opt.}}}{\partial \gamma}
    +
    \frac{\partial \epsilon_{\text{stat.}}}{\partial \gamma}
    \right)
    &&=
    0
    &&\qquad\Leftrightarrow\qquad
    \frac{\partial \epsilon_{\text{opt.}}}{\partial \gamma}
    &&=
    -\frac{\partial \epsilon_{\text{stat.}}}{\partial \gamma}
  \end{alignat*}
  The partial derivatives are given as
  \begin{align*}
    \frac{ \partial \epsilon_{\mathrm{opt.}} }{ \partial T }
    &=
    - \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma}\right) \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T^2}
    \\
    \frac{ \partial \epsilon_{\mathrm{opt.}} }{ \partial \gamma }
    &=
    \frac{1}{2} \left(\alpha - \frac{1}{2 \gamma^2}\right) \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T}
    \\
    \frac{ \partial \epsilon_{\mathrm{stat.}} }{ \partial \gamma }
    &=
    \frac{\beta}{2}.
  \end{align*}
  Applying these to the Lagrangian system, we now need to solve the system:
  \begin{alignat}{2}
    \frac{\lambda}{2} \left(\alpha \gamma + \frac{1}{2 \gamma}\right) \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T^2} &= 1
    \label{eq:lagrangian1}
    \\
    \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) \frac{1}{T} \, \norm{\vlambda_0 - \vlambda^*}_2^2
    +
    \frac{\gamma \beta}{2}
    &=
    \epsilon
    \label{eq:lagrangian2}
    \\
    \frac{1}{2} \left(\alpha - \frac{1}{2 \gamma^2}\right) \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T}
    &=
    -\frac{\beta}{2}.
    \label{eq:lagrangian3}
  \end{alignat}
  Notice that \(\lambda\) in \cref{eq:lagrangian1} is a free variable.
  Thus the last two equations \cref{eq:lagrangian2,eq:lagrangian3} are the only relevant equations.
  In particular, from \cref{eq:lagrangian3}, we obtain the identity
  \begin{alignat}{2}
    \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T}
    =
    \frac{ 2 \beta \gamma^2 }{ 1 - 2 \gamma^2 \alpha }.
    \label{eq:projsgd_convex_keyeq}
  \end{alignat}
  Applying this to \cref{eq:lagrangian2}, we can decouple \(T\) and \(\gamma\), obtaining the quadratic equation
  \begin{alignat*}{3}
    & &\quad
    \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) \frac{1}{T} \, \norm{\vlambda_0 - \vlambda^*}_2^2
    +
    \frac{\gamma \beta}{2}
    &=
    \epsilon
    \\
    &\Leftrightarrow&\quad
    \frac{1}{2} \left(\alpha \gamma + \frac{1}{2 \gamma} \right) 
    \left(
    \frac{ 2 \beta \gamma^2 }{ 1 - 2 \gamma^2 \alpha }
    \right)
    +
    \frac{\gamma \beta}{2}
    &=
    \epsilon
    \\
    &\Leftrightarrow&\quad
    \left( 2 \epsilon \alpha \right) \gamma^2 
    +
    \beta \gamma 
    -
    \epsilon
    &=
    0.
  \end{alignat*}
  Since \(\gamma > 0\), this quadratic has the unique solution
  \begin{alignat}{2}
    \gamma = \frac{
      -\beta + \sqrt{
        \beta^2 + 8 \epsilon^2 \alpha 
      } 
    }{
      4 \epsilon \alpha
    } > 0.
    \label{eq:projsgd_convex_optgamma}
  \end{alignat}
  as long as \(0 < \alpha < \infty\).

  Note that \cref{eq:projsgd_convex_keyeq} can be represented as
  \begin{alignat*}{3}
    & &\quad
    \norm{\vlambda_0 - \vlambda^*}_2^2 \frac{1}{T}
    &=
    \frac{ 2 \beta \gamma^2 }{ 1 - 2 \gamma^2 \alpha }
    \\
    &\Leftrightarrow&\quad
    T
    &=
    \left( \frac{1}{\gamma^2} - 2 \alpha \right) \frac{1}{2 \beta}
    \norm{\vlambda_0 - \vlambda^*}_2^2.
  \end{alignat*}
  Plugging \cref{eq:projsgd_convex_optgamma},
  \begin{alignat*}{3}
    T
    &=
    \left(
      \frac{
        16 \epsilon^2 \alpha^2
      }{
        {\left( -\beta + \sqrt{
          \beta^2 + 8 \epsilon^2 \alpha 
        }
        \right)}^2
      }
    - 2 \alpha \right) \frac{1}{2 \beta}
    \norm{\vlambda_0 - \vlambda^*}_2^2
    \\
    &=
    \frac{2 \alpha}{ -\beta + \sqrt{ \beta^2 + 8 \epsilon^2 \alpha } }
    \, \norm{\vlambda_0 - \vlambda^*}_2^2.
  \end{alignat*}
\end{proofEnd}
