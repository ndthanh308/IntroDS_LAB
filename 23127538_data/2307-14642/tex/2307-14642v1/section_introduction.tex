

\section{Introduction}
Despite the massive success of black-box variational inference (BBVI; \citealp{ranganath_black_2014,titsias_doubly_2014,kucukelbir_automatic_2017}), our understanding of its computational properties has only recently started to make progress \citep{domke_provable_2019,hoffman_blackbox_2020,domke_provable_2020,domke_provable_2023,kim_blackbox_2023,kim_practical_2023}. 
Notably, \citet{domke_provable_2023,kim_blackbox_2023} have independently established the convergence of ``full'' BBVI.
This is a significant advance from the previous results where simplified versions of BBVI were analyzed \citep{hoffman_blackbox_2020,bhatia_statistical_2022} and results that \textit{a-priori} assumed regularity of the ELBO~\citep{alquier_concentration_2020,cherief-abdellatif_generalization_2019,regier_fast_2017,khan_faster_2016,khan_kullbackleibler_2015a,fujisawa_multilevel_2021,buchholz_quasimonte_2018,liu_quasimonte_2021}.
We now have rigorous convergence guarantees that, for certain well-behaved posteriors, BBVI achieves a convergence rate of \(\mathcal{O}\left(1/T\right)\), corresponding to a computational complexity of \(\mathcal{O}\left(1/\epsilon\right)\)~\citep{domke_provable_2023,kim_blackbox_2023}.
A remaining theoretical question is whether BBVI can achieve better rates, in particular geometric convergence rates, which is traditionally called ``linear'' convergence in the optimization literature (see the textbook by \citealt[\S 1.2.3]{nesterov_introductory_2004}), corresponding to a complexity of \(\mathcal{O}\left(\log\left( 1/\epsilon \right) \right)\).

For stochastic gradient descent (SGD; \citealp{robbins_stochastic_1951,bottou_online_1999,nemirovski_robust_2009}), it is known that improving the \(\mathcal{O}\left(1/T\right)\) convergence rate is challenging~\citep{rakhlin_making_2012,harvey_tight_2019}.
This is because, once in the stationary regime, it is necessary to either decrease the stepsize or average the iterates, where the latter reduces SGD to Markov chain Monte Carlo~\citep{dieuleveut_bridging_2020}.
Not surprisingly, both cases result in a significant slowdown compared to deterministic gradient descent.
Overall, SGD is known to achieve \(\mathcal{O}\left(1/\sqrt{T}\right)\) for general convex functions and \(\mathcal{O}\left(1/T\right)\) for strongly convex functions (\citealp{nemirovski_robust_2009}; for more modern analysis techniques, see~\citealp{garrigos_handbook_2023,gower_sgd_2019}).

Meanwhile, under a condition known as ``interpolation,'' which assumes that the gradient variance becomes zero at the optimum, SGD is known to achieve a linear convergence rate~\citep{schmidt_fast_2013}.
This can automatically hold for certain problems, such as empirical risk minimization (ERM) with overparameterized models, explaining the fast empirical convergence of modern machine learning models~\citep{vaswani_fast_2019, ma_power_2018}.
Also, control variate methods such as ``variance-reduced'' gradients \citep{schmidt_minimizing_2017,johnson_accelerating_2013,gower_variancereduced_2020} algorithmically achieve the same effect and have been successful both in theory and practice.
Unfortunately, variance-reduced gradient methods are strictly restricted to the finite-sum setting, which BBVI is not part of (See \S 2.4 by \citealp{kim_blackbox_2023}).
Thus it is yet unclear how BBVI could benefit from the advances in variance-reduced gradients.

Fortunately, other types of control variates have been actively pursued in BBVI \citep{geffner_using_2018,miller_reducing_2017, ranganath_black_2014, paisley_variational_2012, geffner_approximation_2020}.
One of which, called the \textit{sticking-the-landing} (STL; \citealp{roeder_sticking_2017}) estimator, has been known to satisfy the interpolation condition under realizable assumptions\footnote{Although the term interpolation does not 
literally make sense outside of the ERM context, we will stick to this term to stay in line with the SGD literature.}.
When using BBVI with perfect variational family specification (the posterior \(\pi\) is in the variational family \(\mathcal{Q}\) such that \(\pi \in \mathcal{Q}\)), the STL estimator achieves zero variance at the optimum.
It is thus natural to ask whether existing control variate approaches such as STL are sufficient to achieve linear convergence under appropriate conditions.
In fact, this possibility has been mentioned by~\citet[\S 5]{hoffman_blackbox_2020} and \citet[\S 5]{domke_provable_2023}.

\input{table_relatedworks}

In this work, we confirm these previous comments by establishing a linear convergence rate of BBVI with STL when the variational family is perfectly specified.
For a \(d\)-dimensional strongly log-concave posterior with a condition number of \(\kappa\) and a location-scale variational family with a full rank scale, BBVI with the STL estimator finds an \(\epsilon\)-accurate variational parameters at a rate of \(\mathcal{O}\left(d \kappa^2 \log\left(1/\epsilon\right) \right)\).
Not only this, our theoretical results generally encompass the behavior of the STL estimator in the misspecified setting, which is closer to practical usage.
This provides some intuition as why the comparisons between the STL and ``standard'' closed-form entropy (CFE;~\citealp{titsias_doubly_2014,kucukelbir_automatic_2017}) estimators appear mixed in practice~\citep{geffner_rule_2020,agrawal_advances_2020}.

\vspace{-1ex}
\paragraph{Contributions} Overall, our contributions are summarized in the following list.
An overview of the theorems is provided in \cref{table:theorems}.
We also provide an overview of previous rigorous complexity analyses on BBVI in \cref{table:relatedworks}.
\begin{enumerate}
    \item[\ding{182}] \textbf{We prove that BBVI with the STL estimators can converge at a linear rate.} \\
      When the variational family is perfectly specified such that the posterior is contained in the variational family, \cref{thm:projsgd_bbvistl_complexity} establishes this through \cref{thm:stl_upperbound}.
      This is the first result for ``full'' BBVI without algorithmic simplification. 
      
    \item[\ding{183}] \textbf{Our analysis encompasses the case where the variational family is misspecified.}
      When the variational family is misspecified, the Fisher divergence between the variational posterior and the true posterior captures the behavior of the STL estimator.
    
    \item[\ding{184}] \textbf{We improve previously obtained gradient variance bounds for the CFE estimator} \\
      In \cref{thm:cfe_upperbound}, we tighten the constants of the bounds previously obtained by \citet{domke_provable_2023}.
      This makes the theoretical results for the CFE and STL estimators comparable.
      
    \item[\ding{185}] \textbf{We prove precise complexity guarantees for SGD with QV gradient estimators.}
      Specifically, we prove precise complexity guarantees from the ``anytime convergence'' results of \citet{domke_provable_2023}.
\end{enumerate}

