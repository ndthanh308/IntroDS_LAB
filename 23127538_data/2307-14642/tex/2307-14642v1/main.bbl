\begin{thebibliography}{58}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL: }\fi

\bibitem[{Agrawal et~al.(2020)Agrawal, Sheldon and
  Domke}]{agrawal_advances_2020}
Agrawal, A., Sheldon, D.~R. and Domke, J. (2020) Advances in black-box {{VI}}:
  {{Normalizing}} flows, importance weighting, and optimization.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~33, 17358--17369. {Curran Associates, Inc.}

\bibitem[{Alquier and Ridgway(2020)}]{alquier_concentration_2020}
Alquier, P. and Ridgway, J. (2020) Concentration of tempered posteriors and of
  their variational approximations.
\newblock \textit{The Annals of Statistics}, \textbf{48}, 1475--1497.

\bibitem[{Bakry and {\'E}mery(1985)}]{bakry_diffusions_1985}
Bakry, D. and {\'E}mery, M. (1985) Diffusions hypercontractives.
\newblock In \textit{S\'eminaire de {{Probabilit\'es XIX}} 1983/84}, vol. 1123,
  177--206. {Berlin, Heidelberg}: {Springer Berlin Heidelberg}.

\bibitem[{Bhatia et~al.(2022)Bhatia, Kuang, Ma and
  Wang}]{bhatia_statistical_2022}
Bhatia, K., Kuang, N.~L., Ma, Y.-A. and Wang, Y. (2022) Statistical and
  computational trade-offs in variational inference: A case study in
  inferential model selection.

\bibitem[{Blei et~al.(2017)Blei, Kucukelbir and
  McAuliffe}]{blei_variational_2017}
Blei, D.~M., Kucukelbir, A. and McAuliffe, J.~D. (2017) Variational inference:
  {{A}} review for statisticians.
\newblock \textit{Journal of the American Statistical Association},
  \textbf{112}, 859--877.

\bibitem[{Bottou(1999)}]{bottou_online_1999}
Bottou, L. (1999) On-line learning and stochastic approximations.
\newblock In \textit{On-{{Line Learning}} in {{Neural Networks}}}, 9--42.
  {Cambridge University Press}, first edn.

\bibitem[{Buchholz et~al.(2018)Buchholz, Wenzel and
  Mandt}]{buchholz_quasimonte_2018}
Buchholz, A., Wenzel, F. and Mandt, S. (2018) Quasi-{{Monte Carlo}} variational
  inference.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Machine Learning}}}, vol.~80 of \textit{{{PMLR}}}, 668--677. {ML Research
  Press}.

\bibitem[{{Ch{\'e}rief-Abdellatif} et~al.(2019){Ch{\'e}rief-Abdellatif},
  Alquier and Khan}]{cherief-abdellatif_generalization_2019}
{Ch{\'e}rief-Abdellatif}, B.-E., Alquier, P. and Khan, M.~E. (2019) A
  generalization bound for online variational inference.
\newblock In \textit{Proceedings of the {{Asian Conference}} on {{Machine
  Learning}}}, vol. 101 of \textit{{{PMLR}}}, 662--677. {ML Research Press}.

\bibitem[{Csiba and Richt{\'a}rik(2018)}]{csiba_importance_2018}
Csiba, D. and Richt{\'a}rik, P. (2018) Importance sampling for minibatches.
\newblock \textit{Journal of Machine Learning Research}, \textbf{19}, 1--21.

\bibitem[{Dieuleveut et~al.(2020)Dieuleveut, Durmus and
  Bach}]{dieuleveut_bridging_2020}
Dieuleveut, A., Durmus, A. and Bach, F. (2020) Bridging the gap between
  constant step size stochastic gradient descent and {{Markov}} chains.
\newblock \textit{The Annals of Statistics}, \textbf{48}.

\bibitem[{Domke(2019)}]{domke_provable_2019}
Domke, J. (2019) Provable gradient variance guarantees for black-box
  variational inference.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~32. {Curran Associates, Inc.}

\bibitem[{Domke(2020)}]{domke_provable_2020}
--- (2020) Provable smoothness guarantees for black-box variational inference.
\newblock In \textit{Proceedings of the International Conference on Machine
  Learning}, vol. 119 of \textit{{{PMLR}}}, 2587--2596. {ML Research Press}.

\bibitem[{Domke et~al.(2023)Domke, Garrigos and Gower}]{domke_provable_2023}
Domke, J., Garrigos, G. and Gower, R. (2023) Provable convergence guarantees
  for black-box variational inference.

\bibitem[{Fujisawa and Sato(2021)}]{fujisawa_multilevel_2021}
Fujisawa, M. and Sato, I. (2021) Multilevel {{Monte Carlo}} variational
  inference.
\newblock \textit{Journal of Machine Learning Research}, \textbf{22}, 1--44.

\bibitem[{Garrigos and Gower(2023)}]{garrigos_handbook_2023}
Garrigos, G. and Gower, R.~M. (2023) Handbook of convergence theorems for
  (stochastic) gradient methods.

\bibitem[{Geffner and Domke(2018)}]{geffner_using_2018}
Geffner, T. and Domke, J. (2018) Using large ensembles of control variates for
  variational inference.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~31. {Curran Associates, Inc.}

\bibitem[{Geffner and Domke(2020{\natexlab{a}})}]{geffner_approximation_2020}
--- (2020{\natexlab{a}}) Approximation {{Based Variance Reduction}} for
  {{Reparameterization Gradients}}.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~33, 2397--2407. {Curran Associates, Inc.}

\bibitem[{Geffner and Domke(2020{\natexlab{b}})}]{geffner_rule_2020}
--- (2020{\natexlab{b}}) A rule for gradient estimator selection, with an
  application to variational inference.
\newblock In \textit{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, vol. 108 of \textit{{{PMLR}}}, 1803--1812. {ML
  Research Press}.

\bibitem[{Geffner and Domke(2021)}]{geffner_difficulty_2021}
--- (2021) On the difficulty of unbiased alpha divergence minimization.
\newblock In \textit{Proceedings of the International Conference on Machine
  Learning}, vol. 139 of \textit{{{PMLR}}}, 3650--3659. {ML Research Press}.

\bibitem[{Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin and
  Richt{\'a}rik}]{gower_sgd_2019}
Gower, R.~M., Loizou, N., Qian, X., Sailanbayev, A., Shulgin, E. and
  Richt{\'a}rik, P. (2019) {{SGD}}: {{General}} analysis and improved rates.
\newblock In \textit{Proceedings of the International Conference on Machine
  Learning}, vol.~97 of \textit{{{PMLR}}}, 5200--5209. {ML Research Press}.

\bibitem[{Gower et~al.(2020)Gower, Schmidt, Bach and
  Richtarik}]{gower_variancereduced_2020}
Gower, R.~M., Schmidt, M., Bach, F. and Richtarik, P. (2020) Variance-reduced
  methods for machine learning.
\newblock \textit{Proceedings of the IEEE}, \textbf{108}, 1968--1983.

\bibitem[{Gross(1975)}]{gross_logarithmic_1975}
Gross, L. (1975) Logarithmic {{Sobolev}} inequalities.
\newblock \textit{American Journal of Mathematics}, \textbf{97}, 1061.

\bibitem[{Harvey et~al.(2019)Harvey, Liaw, Plan and
  Randhawa}]{harvey_tight_2019}
Harvey, N. J.~A., Liaw, C., Plan, Y. and Randhawa, S. (2019) Tight analyses for
  non-smooth stochastic gradient descent.
\newblock In \textit{Proceedings of the {{Thirty}}-{{Second Conference}} on
  {{Learning Theory}}}, 1579--1613. {PMLR}.

\bibitem[{Hoffman and Ma(2020)}]{hoffman_blackbox_2020}
Hoffman, M. and Ma, Y. (2020) Black-box variational inference as a parametric
  approximation to {{Langevin}} dynamics.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Machine Learning}}}, {{PMLR}}, 4324--4341. {ML Research Press}.

\bibitem[{Huggins et~al.(2018)Huggins, Campbell, Kasprzak and
  Broderick}]{huggins_practical_2018}
Huggins, J.~H., Campbell, T., Kasprzak, M. and Broderick, T. (2018) Practical
  bounds on the error of {{Bayesian}} posterior approximations: {{A}}
  nonasymptotic approach.
\newblock \textit{{{arXiv}} Preprint arXiv:1809.09505}.

\bibitem[{Hyv{\"a}rinen(2005)}]{hyvarinen_estimation_2005}
Hyv{\"a}rinen, A. (2005) Estimation of non-normalized statistical models by
  score matching.
\newblock \textit{Journal of Machine Learning Research}, \textbf{6}, 695--709.

\bibitem[{Johnson and Zhang(2013)}]{johnson_accelerating_2013}
Johnson, R. and Zhang, T. (2013) Accelerating stochastic gradient descent using
  predictive variance reduction.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~26. {Curran Associates, Inc.}

\bibitem[{Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola and
  Saul}]{jordan_introduction_1999}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S. and Saul, L.~K. (1999) An
  introduction to variational methods for graphical models.
\newblock \textit{Machine Learning}, \textbf{37}, 183--233.

\bibitem[{Khan et~al.(2016)Khan, Babanezhad, Lin, Schmidt and
  Sugiyama}]{khan_faster_2016}
Khan, M.~E., Babanezhad, R., Lin, W., Schmidt, M. and Sugiyama, M. (2016)
  Faster stochastic variational inference using proximal-gradient methods with
  general divergence functions.
\newblock In \textit{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, {{UAI}}'16, 319--328. {Arlington, Virginia, USA}: {AUAI
  Press}.

\bibitem[{Khan et~al.(2015)Khan, Baque, Fleuret and
  Fua}]{khan_kullbackleibler_2015a}
Khan, M. E.~E., Baque, P., Fleuret, F. and Fua, P. (2015) Kullback-{{Leibler
  Proximal Variational Inference}}.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~28. {Curran Associates, Inc.}

\bibitem[{Kim et~al.(2023{\natexlab{a}})Kim, Wu, Oh and
  Gardner}]{kim_practical_2023}
Kim, K., Wu, K., Oh, J. and Gardner, J.~R. (2023{\natexlab{a}}) Practical and
  matching gradient variance bounds for black-box variational {{Bayesian}}
  inference.
\newblock \textit{Proceedings of the International Conference on Machine
  Learning (to appear)}.

\bibitem[{Kim et~al.(2023{\natexlab{b}})Kim, Wu, Oh, Ma and
  Gardner}]{kim_blackbox_2023}
Kim, K., Wu, K., Oh, J., Ma, Y. and Gardner, J.~R. (2023{\natexlab{b}})
  Black-box variational inference converges.

\bibitem[{Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman and
  Blei}]{kucukelbir_automatic_2017}
Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A. and Blei, D.~M. (2017)
  Automatic differentiation variational inference.
\newblock \textit{Journal of Machine Learning Research}, \textbf{18}, 1--45.

\bibitem[{Liu and Owen(2021)}]{liu_quasimonte_2021}
Liu, S. and Owen, A.~B. (2021) Quasi-{{Monte Carlo}} quasi-{{Newton}} in
  {{Variational Bayes}}.
\newblock \textit{Journal of Machine Learning Research}, \textbf{22}, 1--23.

\bibitem[{Ma et~al.(2018)Ma, Bassily and Belkin}]{ma_power_2018}
Ma, S., Bassily, R. and Belkin, M. (2018) The power of interpolation:
  {{Understanding}} the effectiveness of {{SGD}} in modern over-parametrized
  learning.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Machine Learning}}}, vol.~80 of \textit{{{PMLR}}}, 3325--3334. {ML Research
  Press}.

\bibitem[{Miller et~al.(2017)Miller, Foti, D'~Amour and
  Adams}]{miller_reducing_2017}
Miller, A., Foti, N., D'~Amour, A. and Adams, R.~P. (2017) Reducing
  reparameterization gradient variance.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~30. {Curran Associates, Inc.}

\bibitem[{Mohamed et~al.(2020)Mohamed, Rosca, Figurnov and
  Mnih}]{mohamed_monte_2020}
Mohamed, S., Rosca, M., Figurnov, M. and Mnih, A. (2020) Monte {{Carlo}}
  gradient estimation in machine learning.
\newblock \textit{Journal of Machine Learning Research}, \textbf{21}, 1--62.

\bibitem[{Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan and
  Shapiro}]{nemirovski_robust_2009}
Nemirovski, A., Juditsky, A., Lan, G. and Shapiro, A. (2009) Robust stochastic
  approximation approach to stochastic programming.
\newblock \textit{SIAM Journal on Optimization}, \textbf{19}, 1574--1609.

\bibitem[{Nesterov(2004)}]{nesterov_introductory_2004}
Nesterov, Y. (2004) \textit{Introductory Lectures on Convex Optimization},
  vol.~87 of \textit{Applied {{Optimization}}}.
\newblock {Boston, MA}: {Springer US}.

\bibitem[{Otto and Villani(2000)}]{otto_generalization_2000}
Otto, F. and Villani, C. (2000) Generalization of an inequality by talagrand
  and links with the logarithmic sobolev inequality.
\newblock \textit{Journal of Functional Analysis}, \textbf{173}, 361--400.

\bibitem[{Paisley et~al.(2012)Paisley, Blei and
  Jordan}]{paisley_variational_2012}
Paisley, J., Blei, D.~M. and Jordan, M.~I. (2012) Variational bayesian
  inference with stochastic search.
\newblock In \textit{Proceedings of the International Conference on Machine
  Learning}, {{ICML}}'12, 1363--1370. {Madison, WI, USA}: {Omnipress}.

\bibitem[{Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood
  and Teh}]{rainforth_tighter_2018}
Rainforth, T., Kosiorek, A., Le, T.~A., Maddison, C., Igl, M., Wood, F. and
  Teh, Y.~W. (2018) Tighter variational bounds are not necessarily better.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Machine Learning}}}, {{PMLR}}, 4277--4285. {ML Research Press}.

\bibitem[{Rakhlin et~al.(2012)Rakhlin, Shamir and
  Sridharan}]{rakhlin_making_2012}
Rakhlin, A., Shamir, O. and Sridharan, K. (2012) Making gradient descent
  optimal for strongly convex stochastic optimization.

\bibitem[{Ranganath et~al.(2014)Ranganath, Gerrish and
  Blei}]{ranganath_black_2014}
Ranganath, R., Gerrish, S. and Blei, D. (2014) Black box variational inference.
\newblock In \textit{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, vol.~33 of \textit{{{PMLR}}}, 814--822. {ML
  Research Press}.

\bibitem[{Regier et~al.(2017)Regier, Jordan and McAuliffe}]{regier_fast_2017}
Regier, J., Jordan, M.~I. and McAuliffe, J. (2017) Fast black-box variational
  inference through stochastic trust-region optimization.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~30. {Curran Associates, Inc.}

\bibitem[{Robbins and Monro(1951)}]{robbins_stochastic_1951}
Robbins, H. and Monro, S. (1951) A stochastic approximation method.
\newblock \textit{The Annals of Mathematical Statistics}, \textbf{22},
  400--407.

\bibitem[{Roeder et~al.(2017)Roeder, Wu and Duvenaud}]{roeder_sticking_2017}
Roeder, G., Wu, Y. and Duvenaud, D.~K. (2017) Sticking the landing: {{Simple}},
  lower-variance gradient estimators for variational inference.
\newblock In \textit{Advances in {{Neural Information Processing Systems}}},
  vol.~30. {Curran Associates, Inc.}

\bibitem[{Schmidt et~al.(2017)Schmidt, Le~Roux and
  Bach}]{schmidt_minimizing_2017}
Schmidt, M., Le~Roux, N. and Bach, F. (2017) Minimizing finite sums with the
  stochastic average gradient.
\newblock \textit{Mathematical Programming}, \textbf{162}, 83--112.

\bibitem[{Schmidt and Roux(2013)}]{schmidt_fast_2013}
Schmidt, M. and Roux, N.~L. (2013) Fast convergence of stochastic gradient
  descent under a strong growth condition.

\bibitem[{Solodov(1998)}]{solodov_incremental_1998}
Solodov, M. (1998) Incremental gradient algorithms with stepsizes bounded away
  from zero.
\newblock \textit{Computational Optimization and Applications}, \textbf{11},
  23--35.

\bibitem[{Titsias and {L{\'a}zaro-Gredilla}(2014)}]{titsias_doubly_2014}
Titsias, M. and {L{\'a}zaro-Gredilla}, M. (2014) Doubly stochastic variational
  {{Bayes}} for non-conjugate inference.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Machine Learning}}}, vol.~32 of \textit{{{PMLR}}}, 1971--1979. {ML Research
  Press}.

\bibitem[{Vaswani et~al.(2019)Vaswani, Bach and Schmidt}]{vaswani_fast_2019}
Vaswani, S., Bach, F. and Schmidt, M. (2019) Fast and faster convergence of
  {{SGD}} for over-parameterized models and an accelerated perceptron.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, {{PMLR}}, 1195--1204. {ML
  Research Press}.

\bibitem[{Villani(2016)}]{villani_topics_2016}
Villani, C. (2016) \textit{Topics in Optimal Transportation}.
\newblock No.~58 in Graduate Studies in Mathematics. {Providence, Rhode
  Island}: {American Mathematical Society}.

\bibitem[{Wang et~al.(2023)Wang, Geffner and Domke}]{wang_dual_2023}
Wang, X., Geffner, T. and Domke, J. (2023) Dual control variate for faster
  black-box variational inference.

\bibitem[{Wright and Recht(2021)}]{wright_optimization_2021}
Wright, S.~J. and Recht, B. (2021) \textit{Optimization for Data Analysis}.
\newblock {New York}: {Cambridge University Press}.

\bibitem[{Xu et~al.(2019)Xu, Quiroz, Kohn and Sisson}]{xu_variance_2019}
Xu, M., Quiroz, M., Kohn, R. and Sisson, S.~A. (2019) Variance reduction
  properties of the reparameterization trick.
\newblock In \textit{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, vol.~89 of
  \textit{{{PMLR}}}, 2711--2720. {ML Research Press}.

\bibitem[{Zegers(2015)}]{zegers_fisher_2015}
Zegers, P. (2015) Fisher information properties.
\newblock \textit{Entropy}, \textbf{17}, 4918--4939.

\bibitem[{Zhang et~al.(2019)Zhang, Butepage, Kjellstrom and
  Mandt}]{zhang_advances_2019}
Zhang, C., Butepage, J., Kjellstrom, H. and Mandt, S. (2019) Advances in
  variational inference.
\newblock \textit{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, \textbf{41}, 2008--2026.

\end{thebibliography}
