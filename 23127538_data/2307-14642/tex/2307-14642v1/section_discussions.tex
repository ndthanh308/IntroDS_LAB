
\section{Discussion}

% \paragraph{QV and Other Conditions}
% We would like to mention some connections of the QV condition with other known gradient variance conditions.
% Notably, when the Polyak-\L{}ojasiewicz (PL) condition holds, QV immediately implies the convex expected smoothness (CES; \citealp{gower_sgd_2019}) and relaxed growth (RG; \citealp{bottou_optimization_2018}) conditions. 
% (See \cref{section:definitions} for definitions and also the overview by~\citealt{khaled_better_2023}.)
% Furthermore, when ``interpolation'' holds, we obtain the stronger expected strong growth (ESG;~\citealp{solodov_incremental_1998, vaswani_fast_2019}) condition.

%
%% \begin{theoremEnd}{proposition}[category=gradientconditions]
%%     Let a gradient estimator \(g\) satisfy the QV condition.
%%     Then, for a \(\mu\)-PL objective \(F\), it also satisfies CES and RG as
%%     \[
%%       \mathbb{E}\norm{\rvvg}_2^2 
%%       \leq
%%       \frac{2 \alpha }{\mu} \left( F\left(\vlambda\right) - F\left(\vlambda^*\right) \right) + \beta
%%       \leq 
%%       \frac{\alpha}{\mu^2} \norm{\nabla F}_2^2 + \beta,
%%     \]
%%     where \(F^* = \inf_{\vlambda \in \Lambda} F\left(\vlambda\right)\).
%% \end{theoremEnd}
%% \begin{proofEnd}
%%   First, the \(\mu\)-PL condition is 
%%   \begin{align*}
%%      \mu \left( F\left(\vlambda\right) - F\left(\vlambda^*\right) \right)
%%      \leq
%%      \frac{1}{2} \norm{\nabla F}_2^2.
%%   \end{align*}
%%   As shown by \citet{karimi_linear_2016}, this directly implies the quadratic growth condition~\citep{anitescu_degenerate_2000}
%%   \begin{align*}
%%      \frac{\mu}{2} \norm{\vlambda - \vlambda^*}_2^2
%%      \leq F\left(\vlambda\right) - F^*.
%%   \end{align*}

%%   The proof follows from applying the previous equalities one after another as
%%   \begin{align*}
%%     \mathbb{E}\norm{\rvvg}_2^2
%%     \leq
%%     \alpha \norm{ \vlambda - \vlambda^* }_2^2 + \beta
%%     \leq
%%     \frac{2 \alpha }{\mu} \left( F\left(\vlambda\right) - F^* \right) + \beta
%%     \leq
%%     \frac{\alpha}{\mu^2} \norm{\nabla F\left(\vlambda\right)}_2^2 + \beta.
%%   \end{align*}
%% \end{proofEnd}
%
%
%% \begin{corollary}
%%   If the a gradient estimator \(g\) satisfies the QVC condition with \(\beta = 0\), for a \(\mu\)-PL objective \(F\), it also satisfies the SG condition as
%%   \[
%%     \mathbb{E}\norm{\rvvg}_2^2 \leq \frac{\alpha}{\mu^2} \norm{ \nabla F }_2^2
%%   \]
%% \end{corollary}

% Naturally, we can combine these facts with our results in \cref{section:gradient_variance} about the STL estimator.
% %
% \begin{proposition}
%   For a \(\mu\)-log-PL posterior, 
%   \begin{enumerate*}[label=\textbf{(\roman*)}]
%       \item the STL and CFE gradient estimators satisfy the RSG condition. And,
%       \item if the variational family is perfectly specified, the STL gradient estimator also satisfies the SG condition.
%   \end{enumerate*}
% \end{proposition}
% This implies that other analysis results utilizing the CES, RSG, and SG conditions could be used here.
% For example, \citep{meng_fast_2020} proves the convergence of local quadratic convergence of a stochastic Newton algorithm.
% This would improve the previous analysis of \citep{liu_quasimonte_2021}, which had to \textit{a-priori} assume the regularity of BBVI.
% We leave the analysis of such second-order and quasi-second-order BBVI algorithms~\citep{liu_quasimonte_2021,fan_fast_2015} to future works.

\paragraph{STL Estimator for Proximal SGD}
In this work, we have focused on BBVI with projected SGD, which is known to converge slower than BBVI with proximal SGD \citep{kim_blackbox_2023,domke_provable_2020,domke_provable_2023}.
To apply the STL estimator in proximal SGD, we need a gradient estimator that only targets the energy term, which can be devised as follows:
\begin{definition}[\textbf{STL Estimator for the Energy}]\label{def:stl_prox}
The sticking-the-landing gradient estimator for only estimating the energy is given as
\[
  \rvvg_{\mathrm{STL}}\left(\vlambda\right) 
  \triangleq
  \nabla_{\vlambda} \log \pi \left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right)
  - 
  \nabla_{\vlambda} \log q_{\vnu}\left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) \Big\lvert_{\vnu = \vlambda}
  -
  \nabla_{\vlambda} \mathbb{H}\left(q_{\vlambda}\right).
\]
\end{definition}
%
While \cref{def:stl_prox} can be readily used, there is a catch; the variance of the STL estimator is bounded only if the entropy resulting from \(\mC\) is bounded, such as in \(\Lambda_{S}\).
This means a projection operator needs to be employed, defeating the purpose of using proximal SGD.
Thus, our theoretical analysis has focused entirely on projected SGD.

\paragraph{Empirically Comparing Estimators}
From our analysis and that of \citet{domke_provable_2023}, it is apparent that for a QV gradient estimator, \(\alpha\) and \(\beta\) sufficiently characterize its behavior on strongly log-concave posteriors: \(\alpha\) characterizes the convergence speed, while \(\beta\) determines the complexity with respect to \(\epsilon\).
It is conceivable that \textit{estimating} these quantities in practical settings would provide a principled way to compare and evaluate different estimators.
Previously, the signal-to-noise (SNR) ratio have been popularized by~\citet{rainforth_tighter_2018}, and since been used by, for example, by \cite{geffner_difficulty_2021,fujisawa_multilevel_2021}.
In contrast to the QV coefficients, a \textit{constant} SNR relates with convergence only through the expected strong growth condition~\citep{solodov_incremental_1998,vaswani_fast_2019,schmidt_fast_2013}, which is valid only under the strongest condition (perfect variational family specification, strong log-concavity)
The QV coefficients, \(\alpha\) and \(\beta\), on the other hand, apply to a wider range of settings.
We also note that measuring the ELBO of the last iterates, as done by \citet{agrawal_advances_2020}, only estimates the radius of the stationary region (related to \(\beta\)).
To have the full picture, it would also be useful to measure the convergence \textit{speed} (related to \(\alpha\)).

\paragraph{Conclusions and Future Works}
In this work, we have analyzed the sticking-the-landing (STL) estimator by~\citet{roeder_sticking_2017}.
When the variational family is perfectly specified, our complexity guarantees automatically guarantee a logarithmic complexity, known as linear convergence in the optimization literature.
This demonstrates that it is possible to rigorously show that control variates can accelerate the convergence of BBVI.
It will be interesting to analyze and compare the existing control variates by~\citet{miller_reducing_2017, geffner_approximation_2020, wang_dual_2023} and those that will be revealed in the future.
Note that, for the STL estimator, our convergence guarantees suggest that the convergence \textit{speed} is slower than CFE.
That is, on larger \(\epsilon\)-accuracy levels, STL is slower to converge.
It would be interesting to see if there exists a control variate that is capable of accelerating the overall speed of convergence, not just better complexity.


