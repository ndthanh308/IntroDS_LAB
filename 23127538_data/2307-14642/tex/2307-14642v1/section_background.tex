
\section{Preliminaries}

{%\footnotesize
\paragraph{Notation}
Random variables are denoted in serif (\textit{e.g.}, \(\rvx\), \(\rvvx\)), vectors are in bold (\textit{e.g.}, \(\vx\), \(\rvvx\)), and matrices are in bold capitals (\textit{e.g.} \(\mA\)).
For a vector \(\vx \in \mathbb{R}^d\), we denote the inner product as \(\vx^{\top}\vx\) and \(\inner{\vx}{\vx}\), the \(\ell_2\) norm as \(\norm{\vx}_2 = \sqrt{\vx^{\top}\vx}\).
For a matrix {\footnotesize\(\mA\), \(\norm{\mA}_{\mathrm{F}} = \sqrt{\mathrm{tr}\left(\mA^{\top} \mA\right)}\)} denotes the Frobenius norm, and for some matrix \(\mB\), \(\mA \succeq \mB\) is the Loewner order implying that \(\mA - \mB\) is a positive semi-definite matrix.
\(\mathbb{S}^d\), \(\mathbb{S}^d_{++}\), \(\mathrm{GL}_d\left(\mathbb{R}\right)\) are the set of symmetric, positive definite, and lower triangular matrices. 
}

\subsection{Variational Inference}

Variational inference (VI,~\citealp{zhang_advances_2019, blei_variational_2017, jordan_introduction_1999}) aims to minimize the exclusive (or backward/reverse) Kullback-Leibler (KL) divergence as:
%
{\begingroup
%\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
%\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\begin{align*}
    \minimize_{\vlambda \in \Lambda}\; \mathrm{D}_{\mathrm{KL}}\left(q_{\vlambda}, \pi\right)
    \triangleq
    \mathbb{E}_{\rvvz \sim q_{\vlambda}} -\log \pi \left(\rvvz\right) -\mathbb{H}\left(q_{\vlambda}\right),
\end{align*}
\endgroup}
%
\begin{center}
  %\vspace{-2ex}
  {\begingroup
  %  \setlength\tabcolsep{1.0ex} 
  \begin{tabular}{lll}
    where 
    & \(\mathrm{D}_{\mathrm{KL}}\) & is the KL divergence, \\
    & \(\mathbb{H}\)     & is the differential entropy, \\
    & \(\pi\) & is the (target) posterior distribution, and  \\
    & \(q_{\vlambda}\) & is the variational approximation. \\
  \end{tabular}
  \endgroup}
\end{center}

For Bayesian posterior inference, the KL divergence is, unfortunately intractable.
Instead, one equivalently minimizes the negative \textit{evidence lower bound} (ELBO,~\citealp{jordan_introduction_1999}) \(F\) such that:
{%
%\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
%\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\[
  \minimize_{\vlambda \in \Lambda}\; F\left(\vlambda\right)
  \triangleq
  \mathbb{E}_{\rvvz \sim q_{\vlambda}} -\log p \left(\rvvz, \vx\right) - \mathbb{H}\left(q_{\vlambda}\right),
\]
}%
where \(p\left(\vz, \vx\right)\) is the \textit{joint likelihood}, which is proportional to the posterior \(\pi\left(\vz\right)\) up to a multiplicative constant.

\paragraph{Black-Box Variational Inference}
Black-box variational inference (BBVI; \citealp{ranganath_black_2014,titsias_doubly_2014,kucukelbir_automatic_2017}) minimizes \(F\) by leveraging stochastic gradient descent (SGD; \citealp{robbins_stochastic_1951,bottou_online_1999,nemirovski_robust_2009}).
By obtaining a stochastic estimate \(\rvvg\left(\vlambda\right)\) which is \textit{unbiased}  as \(\mathbb{E} \rvvg\left(\vlambda\right) = \nabla F\left(\vlambda\right)\), BBVI repeats the update:
\[
  \vlambda_{t+1} = \mathrm{proj}\left( \vlambda_t - \gamma_t \rvvg \right), 
\]
where \(\gamma_t\) is called the stepsize.
The use of the projection operator \(\mathrm{proj}\left(\cdot\right)\) forms a subset of the broad SGD framework called \textit{projected} SGD.
The convergence of BBVI with projected SGD has recently been established by \citet{domke_provable_2023}.

\paragraph{Fisher Divergence}
In addition to the KL divergence, our analysis involves the Hyv\"arinen-Fisher divergence~\citep{otto_generalization_2000,hyvarinen_estimation_2005}.
\begin{definition}[\textbf{Fisher-Hyv\"arinen Divergence}]
\[
  \mathrm{D}_{\mathrm{F}^p}\left(q, \pi\right)
  \triangleq 
  \mathbb{E}_{\rvvz \sim q} 
  \norm{\nabla \log \pi\left(\rvvz\right) - \nabla \log q\left(\rvvz\right) }_2^p
\]
\end{definition}
Here, we use the \(p\)th order generalization~\citep{huggins_practical_2018} of the original Hyv\"arinen-Fisher divergence. 
We denote the standard 2nd order Hyv\"arinen-Fisher divergence as \(\mathrm{D}_{\mathrm{F}}\left(q, \pi\right) \triangleq \mathrm{D}_{\mathrm{F}^2}\left(q, \pi\right)\).
The Hyv\"arinen-Fisher divergence was first defined by~\citep{otto_generalization_2000} (attributed by \citealp{zegers_fisher_2015}) as the \textit{relative Fisher information} in the context of optimal transport.
It was later introduced to the machine learning community by \citet{hyvarinen_estimation_2005} for score-matching variational inference.

\subsection{Variational Family}
Throughout this paper, we restrict our interest to the location-scale variational family.
The location-scale variational family has been successfully used by \citet{kim_practical_2023,kim_blackbox_2023,domke_provable_2019,domke_provable_2020,domke_provable_2023, fujisawa_multilevel_2021} for analyzing the properties of BBVI.
It encompasses many practically used variational families such as the Gaussian, Student-t, and other elliptical distributions.
In particular, the location-scale family is part of the broader reparameterized family:
%
\begin{definition}[\textbf{Reparameterized Family}]\label{def:family}
  Let \(\varphi\) be some \(d\)-variate distribution.
  Then, \(q_{\vlambda}\) that can be equivalently represented as
{%
\setlength{\belowdisplayskip}{-.5ex} \setlength{\belowdisplayshortskip}{-.5ex}
\setlength{\abovedisplayskip}{-.5ex} \setlength{\abovedisplayshortskip}{-.5ex}
  \begin{alignat*}{2}
    \rvvz \sim q_{\vlambda}  \quad\Leftrightarrow\quad &\rvvz \stackrel{d}{=} \mathcal{T}_{\vlambda}\left(\rvvu\right); \quad \rvvu \sim  \varphi,
  \end{alignat*}
  }%
  where \(\stackrel{d}{=}\) is equivalence in distribution, is said to be part of a reparameterized family generated by the base distribution \(\varphi\) and the reparameterization function \(\mathcal{T}_{\vlambda}\).
\end{definition}
%
Naturally, this means we focus on the \textit{reparameterization gradient estimator}, often observed to achieve lower variance than alternatives~\citep{xu_variance_2019}.  
(See the overview of \citealt{mohamed_monte_2020} on other estimators.)
From this, we obtain the location-scale family by defining the location-scale reparameterization function:
%
\begin{definition}[\textbf{Location-Scale Reparameterization Function}]\label{def:reparam}
  A mapping \(\mathcal{T}_{\vlambda} : \mathbb{R}^p \times \mathbb{R}^d \rightarrow \mathbb{R}^d\) defined as
{
\setlength{\belowdisplayskip}{.5ex} \setlength{\belowdisplayshortskip}{.5ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \begin{align*}
    &\mathcal{T}_{\vlambda}\left(\vu\right) \triangleq \mC \vu + \vm
  \end{align*}
}%
  with \(\vlambda \in \mathbb{R}^p\) containing the parameters for forming the location \(\vm \in \mathbb{R}^d\) and scale \(\mC \in \mathbb{R}^{d \times d}\) is called the location-scale reparameterization function.
\end{definition}
%
For the scale matrix \(\mC\), various parameterizations are used in practice, as shown by \citet[Table 1]{kim_practical_2023}.
We will discuss our scale parameterizations of choice in \cref{section:scale_parameterization}.

The choice for the base distribution \(\varphi\) completes the specifics of the variational family.
For example, choosing \(\varphi\) to be a univariate Gaussian result in the Gaussian variational family.
We impose the following general assumptions on the base distribution:
\begin{assumption}[\textbf{Base Distribution}]\label{assumption:symmetric_standard}
  \(\varphi\) is a \(d\)-dimensional distribution such that \(\rvvu \sim \varphi\) and \(\rvvu = \left(\rvu_1, \ldots, \rvu_d \right)\) with indepedently and identically distributed components.
  Furthermore, \(\varphi\) is
  \begin{enumerate*}[label=\textbf{(\roman*)}]
      \item symmetric and standardized such that \(\mathbb{E}\rvu_i = 0\), \(\mathbb{E}\rvu_i^2 = 1\), \(\mathbb{E}\rvu_i^3 = 0\), and 
      \item has finite kurtosis \(\mathbb{E}\rvu_i^4 = \kappa < \infty\).
  \end{enumerate*}
\end{assumption}

Overall, the assumptions on the variational family are collected as follows:
\begin{assumption}\label{assumption:variation_family}
  The variational family is the location-scale family formed by \cref{def:family,def:reparam} with the base distribution \(\varphi\) satisfying \cref{assumption:symmetric_standard}.
\end{assumption}

\subsection{Scale Parameterization}\label{section:scale_parameterization}
For the scale parameterization \(\vlambda \mapsto \mC\), in principle, any choice that results in a positive-definite covariance matrix is valid.
However, recently, \citet{kim_blackbox_2023} have shown that a seemingly innocent choice of parameterization can have a massive impact on computational performance.
For example, nonlinear parameterizations can easily break the strong convexity of the ELBO~\citep{kim_blackbox_2023}, which could have been otherwise obtained~\citep{domke_provable_2020}.
Therefore, the scale parameterization is subject to the constraints:
\begin{enumerate}
  \item[\ding{182}] \textbf{Positive Definiteness}: \(\mC\mC^{\top} \succ 0\).  \\
    This is needed to ensure that \(\mC\mC^{\top}\) forms a valid covariance in \(\mathbb{S}_{++}^d\).
    %
  \item[\ding{183}] \textbf{Linearity}: \(\norm{\vlambda - \vlambda'}_2^2 = \norm{\vm - \vm'}_2^2 + \norm{ \mC - \mC'}_{\mathrm{F}}^2 \).  \\
    As shown by \citet{kim_blackbox_2023}, this constraint is necessary to form a strongly-convex ELBO from a strongly log-concave posterior.
    
  \item[\ding{184}] \textbf{Convexity}: \textit{The mapping \(\vlambda \to \mC\mC^{\top}\) is convex on \(\Lambda_{S}\)}.\\
    This is needed to ensure that the ELBO is convex whenever the target posterior is log-concave \citep{domke_provable_2023,kim_blackbox_2023}.
\end{enumerate}
These constraints can be met by the following choices that have been commonly used:
\begin{alignat*}{4}
  \mC &= \mB, 
  &&\qquad
  \text{(full-rank)}
  &&\qquad
  \\
  \mC &= \mathrm{diag}\left(L_{11}, \ldots, L_{dd}\right)
  &&\qquad
  \text{(mean-field)}
\end{alignat*}
where \(\mB \in \mathbb{S}^d\) is an invertible symmetric matrix.
Under the full-rank parameterization, \(\mB\) is a proper matrix square root of the covariance \(\mSigma = \mC \mC^{\top}\) such that \(\mB = \mB^{\top} = \mSigma^{\nicefrac{1}{2}}\).

\paragraph{Inapplicability of Triangular Scale}
Notice that the full-rank scale is chosen to be dense instead of triangular.
In practice, setting \(\mC\left(\vlambda\right) = \mL\), where 
\(\mL \in \mathrm{GL}_{d}\left(\mathbb{R}\right)\) is a lower triangular matrix, is also a valid option, and in fact results in lower gradient variance \citep{kim_practical_2023}.
Furthermore, using the projection operator
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\[
  \mathrm{proj}\left(\vlambda\right)
  =
  \left(\vm, \widetilde{\mL}\right), 
  \quad\text{where}\quad
  \widetilde{L}_{ii} = \max\left(L_{ii}, \epsilon\right), 
\]
}%
for some constant \(0 < \epsilon < \infty\) ensures that \(\mC \mC^{\top}\) is always strictly positive definite with only \(\mathcal{O}\left(d\right)\) computational cost.
Unfortunately, this parameterization cannot be used to obtain a theoretical guarantee.
This is because, to enforce that the ELBO is smooth in combination with constraint \ding{183}, we need the additional constraint:
%
\begin{enumerate}
  \item[\ding{185}] \textbf{Bounded entropy}: \(\mC\mC^{\top} \succ S^{-1} \boldupright{I}\),
\end{enumerate}
which was originally proposed by \citet{domke_provable_2020,domke_provable_2023}.
%
For this, the domain of \(\vlambda\) is restricted to 
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
  \Lambda_{S} \triangleq \left\{\, \left(\vm, \mC\right) \mid \vm \in \mathbb{R}^{d}, \mC \mC^{\top} \in \mathbb{S}_{++}^d\; \text{such that}\; \mC \mC^{\top} \succeq 
 S^{-1} \boldupright{I} \,\right\}.
\]
}%
\citeauthor{domke_provable_2023} (\citeyear{domke_provable_2020,domke_provable_2023}) went to chose \(S = L\), where \(L\) is the log-smoothness constant of the posterior.
The problem with this domain constraint is that a projection operator for \(\Lambda_{S}\) that preserves the triangular structure appears challenging to devise.
Thus, we are restricted to the less efficient matrix square root parameterization.

\paragraph{Projection Operator}
For the projection operator, \citet{domke_provable_2023} propose multiple operators.
But for the purpose of this paper, any projection operator to \(\Lambda_{S}\) suffices. 
We will mention here the operator based on the singular value decomposition (SVD), which is the one we used for the experiments:
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
  \mathrm{proj}_{\Lambda_S}\left(\vlambda\right)
  =
  \left(\vm, \mU \widetilde{\mD} \mU^{\top}\right), 
  \quad\text{where}\quad
  \widetilde{D}_{ii} = \max\left(D_{ii}, \sqrt{1/S}\right),
\]
}%
and \(\mC = \mU \mD \mV^{\top}\) is the singular value decomposition of \(\mC\).

\subsection{Gradient Estimators}\label{section:gradient_estimators}
The gradient estimators considered in this work are the closed-form entropy (CFE;~\citealp{kucukelbir_automatic_2017}) and sticking the landing (STL;~\citealp{roeder_sticking_2017}) estimators.

\paragraph{Closed-From Entropy Estimator}
The CFE estimator is the ``standard'' estimator used for BBVI, defined as the following:
%
\begin{definition}[\textbf{Closed-Form Entropy Estimator}]\label{def:cfe}
The closed-form entropy gradient estimator is
\[
  \rvvg\left(\vlambda\right) 
  \triangleq
  \nabla_{\vlambda} \log \pi \left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) + \nabla_{\vlambda} \mathbb{H}\left(q_{\vlambda}\right),
\]
where the gradient of the entropy term is computed deterministically.
\end{definition}
%
It can be used whenever the entropy \(\mathbb{H}\left(q_{\vlambda}\right)\) is available in a closed form.

\paragraph{Sticking-the-Landing Estimator}
On the other hand, the STL estimator estimates the entropy term stochastically.
%
\begin{definition}[\textbf{Sticking-the-Landing Estimator; STL}]\label{def:stl}
The sticking-the-landing gradient estimator 
\[
  \rvvg_{\mathrm{STL}}\left(\vlambda\right) 
  \triangleq
  \nabla_{\vlambda} \log \pi \left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) - \nabla_{\vlambda} \log q_{\vnu}\left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) \Big\lvert_{\vnu = \vlambda}
\]
is given by stopping the gradient from propagating through \(\log q_{\vlambda}\).
\end{definition}
%
Notice that, the gradient of \(\log q\) is ``stopped'' by the assignment \(\vnu = \vlambda\).
This essentially creates a control variate effect, where the control variate \(\mathrm{cv}\left(\vlambda\right)\) is implicitly formed as
\[
  \mathrm{cv}\left(\vlambda\right)
  =
  \nabla_{\vlambda} \mathbb{H}\left(\vlambda\right)
  +
  \nabla_{\vlambda} \log q_{\vnu}\left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) \Big\lvert_{\vnu = \vlambda}.
\]
Subtracting this to the CFE estimator leads to the STL estimator.

\subsection{Quadratic Variance Condition}
The convergence of BBVI has recently been established concurrently by \citet{kim_blackbox_2023,domke_provable_2023}.
However, the analysis of \citeauthor{domke_provable_2023} presents a broadly applicable framework based on the \textit{quadratic variance} (QV) condition.
%
\begin{definition}[\textbf{Quadratic Variance; QV}]
  A gradient estimator \(\rvvg\) is said to satisfy the quadratic variance condition if the following bound holds:
  \[
    \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2 \leq \alpha \norm{\vlambda - \vlambda^*}_2^2 + \beta
  \] 
  for any \(\vlambda \in \Lambda_S\) and some \(0 \leq \alpha, \beta < \infty\).
\end{definition}
%
This basically assumes that the gradient variance grows no more than a quadratic plus a constant.
For the analysis of SGD, this bound was first proposed by~\citet[p. 85]{wright_optimization_2021}, but a more comprehensive convergence analysis, including proximal SGD, was conducted by \citet{domke_provable_2023}.
This work will connect with their analysis by establishing the QV condition of the considered gradient estimators.

\subsection{Interpolation Condition}
To establish the linear, or more intuitively ``exponential'', convergence of SGD, \citet{schmidt_fast_2013} have relied on the interpolation condition:
\begin{definition}[\textbf{Interpolation}]
  A gradient estimator \(\rvvg\) is said to satisfy the interpolation condition if  \(\mathbb{E}\norm{\rvvg\left(\vlambda^*\right)}_2^2 = 0\) for \(\vlambda^* = \argmin_{\vlambda \in \Lambda} F\left(\vlambda\right)\).
\end{definition}
This assumes that the gradient variance vanishes at the optimum, gradually retrieving the convergence behavior of deterministic gradient descent.
For the QV condition, this corresponds to \(\beta = 0\).

\paragraph{Achieving ``Interpolation''}
Currently, there are two ways where the interpolation condition can be achieved.
The first case is when interpolation is achieved \textit{naturally}.
That is, in ERM, when the model is so overparameterized that certain parameters can ``interpolate'' all of the data points in the train data~\citep{ma_power_2018,vaswani_fast_2019}, the gradient becomes 0 at such point.
Otherwise, a control variate approach such as stochastic average gradient (SAG; \citealp{schmidt_minimizing_2017}) or stochastic variance-reduced gradient (SVRG; \citealp{johnson_accelerating_2013}), and their many variants~\citep{gower_variancereduced_2020} can be used.
We will collectively refer to these methods as variance-reduced SGD methods.

\paragraph{Does STL ``Interpolate?''}
As we previously discussed, the STL estimator is essentially a control variate method.
Thus, an important question is whether it can achieve the same effect, notably linear convergence, as the variance-reduced SGD methods. 
While \citet{roeder_sticking_2017} have already shown that the STL estimator achieves interpolation when \(q_{\vlambda} = \pi\), our research question is whether this fact can be rigorously used to establish linear convergence of SGD.
