
\begin{theoremEnd}[all end, category=stlgaussianfisher]{lemma}\label{thm:gaussian_fisher_divergence}
  For \(\pi = \mathcal{N}\left(\vmu, \mSigma\right)\) and \(q = \mathcal{N}\left(\vm, \mC \mC^{\top} \right)\), the Fisher-Hyv\"arinen divergence is
  \[
    \DHF{q}{\pi}
    =
    {\lVert \mSigma^{-1} \mC - \mC^{-\top} \rVert}_{\mathrm{F}}^2
    +
    {\lVert \mSigma^{-1}\left(\vm  - \vmu\right) \rVert}_{2}^2.
  \]
\end{theoremEnd}
\begin{proofEnd}
The result is straightforward using the reparameterization representation of the Gaussian.
That is,
\[
  \nabla \log \pi\left(\rvvz\right)
  =
  \nabla \log \pi\left(\mathcal{T}_{\vlambda}\left(\rvvu\right) \right)
  =
  \mSigma^{-1}\left(\mathcal{T}_{\vlambda}\left(\rvvu\right) - \vmu\right).
\]
Using this, we have
  \begin{align*}
    \DHF{q}{\pi}
    &=
    \mathbb{E}_{\rvvz \sim q} \norm{ \nabla \log \pi\left(\rvvz\right) - \nabla \log q\left(\rvvz\right) }_{2}^2
    \\
    &=
    \mathbb{E} \norm{ \mSigma^{-1} \left(\mC \rvvu + \vm  - \vmu\right) - {\left( \mC\mC^{\top} \right)}^{-1} \left(\mC \rvvu + \vm - \vm\right) }_{2}^2
    \\
    &=
    \mathbb{E} \norm{ \mSigma^{-1} \left(\mC \rvvu + \vm  - \vmu\right) - {\left( \mC\mC^{\top} \right)}^{-1} \mC \rvvu }_{2}^2
    \\
    &=
    \mathbb{E} \norm{ \mSigma^{-1} \left(\mC \rvvu + \vm  - \vmu\right) - \mC^{-\top} \rvvu }_{2}^2,
\shortintertext{grouping the terms involving \(\mC\),} 
    &=
    \mathbb{E} \norm{ \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \rvvu  + \mSigma^{-1}\left(\vm  - \vmu\right) }_{2}^2,
\shortintertext{expanding the quadratic,} 
    &=
    \mathbb{E} \norm{ \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \rvvu }_2^2
    +
    2 \inner{ \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \mathbb{E} \rvvu }{ \mSigma^{-1}\left(\vm  - \vmu\right) }
    +
    {\lVert \mSigma^{-1}\left(\vm  - \vmu\right) \rVert}_{2}^2,
\shortintertext{applying \cref{assumption:symmetric_standard},} 
    &=
    \mathbb{E} \norm{ \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \rvvu }_2^2
    +
    {\lVert \mSigma^{-1}\left(\vm  - \vmu\right) \rVert}_{2}^2.
  \end{align*}

The expectation term can be simplified as
  \begin{align*}
    \mathbb{E} \norm{ \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \rvvu }_2^2
    &=
    \mathbb{E} \mathrm{tr} \left( \rvvu^{\top} {\left( \mSigma^{-1} \mC - \mC^{-\top} \right)}^{\top} \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \rvvu \right),
\shortintertext{rotating the elements of the trace,} 
    &=
    \mathrm{tr} \left( {\left( \mSigma^{-1} \mC - \mC^{-\top} \right)}^{\top} \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \mathbb{E} \rvvu \rvvu^{\top} \right),
\shortintertext{applying \cref{assumption:symmetric_standard},} 
    &=
    \mathrm{tr} \left( {\left( \mSigma^{-1} \mC - \mC^{-\top} \right)}^{\top} \left( \mSigma^{-1} \mC - \mC^{-\top} \right) \right)
    \\
    &=
    \norm{ \mSigma^{-1} \mC - \mC^{-\top} }_{\mathrm{F}}^2.
  \end{align*}
\end{proofEnd}

\begin{theoremEnd}[all end, category=gaussianklmeanfield]{lemma}\label{thm:gaussian_kl_meanfield}
  Let \(\pi = \mathcal{N}\left(\vmu, \mSigma\right)\) and \(\mathcal{Q}\) be the mean-field Gaussian variational family.
  Then, the solution of the KL divergence minimization problem
  \[
     q_* = \argmin_{q \in \mathcal{Q}}  \;  \DKL{q}{\pi}, 
  \]
  where \(q_* = \mathcal{N}\left(\vm_*, \mC_* \mC_*^{\top}\right)\) is given as
  \[
    \vm_* = \vmu, \qquad \mC_* = {\mathrm{diag}\left(\mSigma\right)}^{\nicefrac{1}{2}}.
  \]
\end{theoremEnd}
\begin{proofEnd}
  Consider that the KL divergence between Gaussian distributions is given as 
  \begin{align*}
    \mathcal{L}\left(\vm, \mC\right)
    = \DKL{q}{\pi} 
    = \frac{1}{2} \left( 
      {\left(\vm - \vmu\right)} \mSigma^{-1} {\left(\vm - \vmu\right)}
      +
      \log\frac{\abs{\mSigma}}{\abs{\mC\mC^{\top}}}
      +
      \operatorname{tr}\left( \mSigma^{-1} \mC\mC^{\top} \right)
      -
      d
    \right).
  \end{align*}
  Firstly, it is clear that \(\vm = \vm_* = \vmu\) minimizes \(\DKL{q}{\pi}\) with respect to \(\vm\) regardless of \(\mC\).
  Then, we have
  \begin{align*}
    \mathcal{L}\left(\vm_*, \mC\right)
    &=
    \frac{1}{2} \left( 
      \log\frac{\abs{\mSigma}}{\abs{\mC\mC^{\top}}}
      +
      \operatorname{tr}\left( \mSigma^{-1} \mC\mC^{\top} \right)
      -
      d
    \right)
    \propto
    -
    \log \abs{\mC\mC^{\top}}
    +
    \operatorname{tr}\left( \mSigma^{-1} \mC\mC^{\top} \right).
  \end{align*}
  When \(\mC\) is a diagonal matrix, taking the partial derivative with respect to \(\mC\) yields
  \begin{align*}
    \frac{\partial\mathcal{L}}{\partial \mC}  \Big\lvert_{\vm = \vm_*}
    =
    -2 \, \mC^{-1}
    +
    2 \operatorname{diag}\left(\mSigma^{-1}\right) \mC.
  \end{align*}
  The first-order optimality condition with respect to \(\mC\) is then
  \[
    {\left( \mC\mC \right)}^{-1} = \operatorname{diag}\left(\mSigma^{-1}\right).
  \]
  Since \(\mSigma\) is always positive definite, its diagonal elements are always strictly positive.
  Therefore, the unique solution \(\mC^*\) is
  \[
    \mC_* = {\operatorname{diag}\left(\mSigma\right)}^{\nicefrac{1}{2}}.
  \]
\end{proofEnd}
\begin{theoremEnd}[category=stlgaussian]{proposition}\label{thm:fisher_bound}
  Let \(\pi = \mathcal{N}\left(\vmu, \mSigma\right)\) and \(\mathcal{Q}\) be the mean-field Gaussian variational family.
  Then, the Fisher-Hyv\"arinen divergence of the KL minimizer
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{-.5ex} \setlength{\belowdisplayshortskip}{-.5ex}
  \[
    q_* = \argmin_{q \in \mathcal{Q}} \DKL{q}{\pi}
  \] 
}%
  is bounded as
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \begin{align*}
    &{\lambda_{\mathrm{max}}\left(\mD\right)}^{-1} 
    {\lVert \mR^{-1}  - \boldupright{I} \rVert}_{\mathrm{F}}^2
    \\
    &\qquad\leq
    \DHF{q_*}{\pi}
    \leq
    {\lambda_{\mathrm{min}}\left(\mD\right)}^{-1} 
    {\lVert \mR^{-1}  - \boldupright{I} \rVert}_{\mathrm{F}}^2,
  \end{align*}
}%
  where \(\mD = \mathrm{diag}\left(\mSigma\right)\) and \(\mR\) is the correlation matrix of \(\pi\) such that \(\mSigma = \mD \mR \mD\).
\end{theoremEnd}
\vspace{-1ex}
\begin{proofEnd}
  First, the Fisher-Hyv\"arinen divergence between Gaussians is given in \cref{thm:gaussian_fisher_divergence} as
  \begin{align*}
    \DHF{q}{\pi}
    =
    {\lVert \mSigma^{-1} \mC - \mC^{-\top} \rVert}_{\mathrm{F}}^2
    +
    {\lVert \mSigma^{-1}\left(\vm  - \vmu\right) \rVert}_{2}^2.
  \end{align*}
  Plugging the KL minimizer \(q_{*}\) given in \cref{thm:gaussian_kl_meanfield}, 
  \begin{align}
    \DHF{q_{*}}{\pi}
    &=
    {\lVert \mSigma^{-1} \mC_* - \mC_*^{-\top} \rVert}_{\mathrm{F}}^2
    +
    {\lVert \mSigma^{-1}\left(\vm_* - \vmu\right) \rVert}_{2}^2
    \nonumber
    \\
    &=
    {\lVert \mSigma^{-1} \mC_* - \mC_*^{-1} \rVert}_{\mathrm{F}}^2.
    \label{eq:stl_gaussian_sandwich_1}
  \end{align}

  From here, we can pull out a \(\mC_*^{-1}\) factor as
  \begin{align}
    {\lVert \mSigma^{-1} \mC_* - \mC_*^{-1} \rVert}_{\mathrm{F}}^2
    =
    {\lVert \mC_*^{-1} \left( \mC_* \mSigma^{-1} \mC - \boldupright{I} \right) \rVert}_{\mathrm{F}}^2.
    \label{eq:stl_gaussian_sandwich_2}
  \end{align}
  And from the property of the Frobenius norm,
  \begin{alignat*}{6}
    & &\quad
    {\lambda_{\mathrm{min}}\left(\mC_*^{-1}\right)}^2 {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
    \quad&\leq&\quad
    \, {\lVert \mC_*^{-1} \left( \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \right) \rVert}_{\mathrm{F}}^2 \;
    \quad&\leq&\quad
    {\lambda_{\mathrm{max}}\left(\mC_*^{-1}\right)}^2 {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2,
\shortintertext{inverting the singular values,}
    &\Leftrightarrow&\quad
    {\lambda_{\mathrm{max}}\left(\mC_*\right)}^{-2} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
    \quad&\leq&\quad
    \,{\lVert \mC_*^{-1} \left( \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \right) \rVert}_{\mathrm{F}}^2 \,
    \quad&\leq&\quad
    {\lambda_{\mathrm{min}}\left(\mC_*\right)}^{-2} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2,
\shortintertext{by \cref{eq:stl_gaussian_sandwich_1,eq:stl_gaussian_sandwich_2},}
    &\Leftrightarrow&\quad
    {\lambda_{\mathrm{max}}\left(\mC_*\right)}^{-2} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
    \quad&\leq&\quad
    \,\DHF{q_*}{\pi}\,
    \quad&\leq&\quad
    {\lambda_{\mathrm{min}}\left(\mC_*\right)}^{-2} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2.
  \end{alignat*}
    
Denoting \(\mD = \operatorname{diag}\left(\mSigma\right)\), we know that \(\mC_* = \mD^{\nicefrac{1}{2}}\). Then,
  \begin{alignat*}{2}
    {\lambda_{\mathrm{max}}\left(\mD\right)}^{-1} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
    \quad\leq\quad
    \DHF{q_*}{\pi}
    \quad\leq\quad
    {\lambda_{\mathrm{min}}\left(\mD\right)}^{-1} {\lVert \mC_* \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2.
  \end{alignat*}
  
  Clearly, the behavior of the Fisher divergence is fully determined by the term
  \begin{align*}
      {\lVert \mC_{*} \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2.
  \end{align*}
  To further analyze this quantity, notice that the correlation matrix \(\mR\) is related with the covariance \(\mSigma\) as
  \[
     \mSigma 
     = {\operatorname{diag}\left(\mSigma\right)}^{\nicefrac{1}{2}} \, \mR \, {\operatorname{diag}\left(\mSigma\right)}^{\nicefrac{1}{2}}
     = \mC_* \mR \mC_*.
  \]
  Then, it immediately follows that 
  \begin{align*}
      {\lVert \mC_{*} \mSigma^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
      =
      {\lVert \mC_{*} {\left( \mC_* \mR \mC_* \right)}^{-1} \mC_* - \boldupright{I} \rVert}_{\mathrm{F}}^2
      =
      {\lVert \mR^{-1}  - \boldupright{I} \rVert}_{\mathrm{F}}^2.
  \end{align*}
\end{proofEnd}
