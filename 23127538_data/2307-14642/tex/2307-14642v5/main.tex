
\documentclass[twoside]{article}

\usepackage[accepted]{aistats2024}
% If your paper is accepted, change the options for the package
% aistats2024 as follows:
%
%\usepackage[accepted]{aistats2024}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

\usepackage[sort&compress,round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{setspace}
\usepackage{nicefrac}
\usepackage{pgfplots}
\usepackage{float}
\usepackage[inline]{enumitem}
\usepackage{pifont}
\usepackage{tabularx,booktabs,threeparttable,makecell,colortbl}
% \usepackage[1-20]{pagesel}

\usepackage{calc}

\input{krkmath}

\usepackage{etoc}
%\etocmulticolstyle{\noindent\bfseries\footnotesize
%\leaders\hrule height1pt\hfill
%\MakeUppercase{Contents}}
\etocframedstyle[1]{\textbf{\textsc{Table of Contents}}}
\etocsettocdepth{3}

% If you use BibTeX in apalike style, activate the following line:
%\bibliographystyle{apalike}

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
\runningtitle{Linear Convergence of Black-Box Variational Inference}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Linear Convergence of Black-Box Variational Inference: \\ Should We Stick the Landing?}

\aistatsauthor{ Kyurae Kim \And Yi-An Ma \And Jacob R. Gardner }

\aistatsaddress{ University of Pennsylvania \And University of California San Diego \And  University of Pennsylvania }
]

\begin{abstract}
  We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called ``linear'') rate under perfect variational family specification.
  In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families.
  Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent.
  For the projection operator, we consider a domain with triangular scale matrices, which the projection onto is computable in \(\Theta(d)\) time, where \(d\) is the dimensionality of the target posterior.
  We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator, providing explicit non-asymptotic complexity guarantees for both.
\end{abstract}

\input{section_introduction}
\input{section_background}
\input{section_main}
%\input{section_experiments}
\input{section_discussions}

\clearpage
\subsubsection*{Acknowledgements}
The authors sincerely thank Jisu Oh (NCSU) for thoroughly proofreading the paper, Justin Domke (UMass Amherst) for discussions on the concurrent results, Kaiwen Wu (UPenn) for helpful discussions, Xi Wang (UMass Amherst) for pointing out a typo, and the anonymous reviewers for comments that improved the readability of the work.

K. Kim was supported by a gift from AWS AI to Penn Engineering's ASSET Center for Trustworthy AI; Y.-A. Ma was funded by the NSF Grants [NSF-SCALE MoDL-2134209] and [NSF-CCF-2112665 (TILOS)], the U.S. Department Of Energy, Office of Science, as well as CDC-RFA-FT-23-0069 from the CDCâ€™s Center for Forecasting and Outbreak Analytics; J. R. Gardner was supported by NSF award [IIS-2145644].

\bibliographystyle{rss}
\bibliography{references}

\clearpage
\input{section_checklist}

\newpage
\appendix

% \etocdepthtag.toc{mtappendix}
% \etocsettagdepth{mtchapter}{none}
% \etocsettagdepth{mtappendix}{part}

\onecolumn

\newpage
{\hypersetup{linkcolor=black}
\tableofcontents
}

\newpage
\section{OVERVIEW OF THEOREMS}\label{section:overviewtheorems}
\input{table_theorems}

\newpage
\input{section_related_works}

\onecolumn

\newpage
\section{PROOFS}\label{section:proofs}

\input{section_definitions}

% \newpage
% \subsection{Miscellaneous Propositions}
% \vspace{1ex}
% \input{thm_peterpaul}
% \printProofs[logsobolev]
% \printProofs[misc]

\newpage
\subsection{Auxiliary Lemmas}
\vspace{1ex}
\input{section_external_lemmas}
\printProofs[external]
\printProofs[gradvarlemmas]

\clearpage
\subsection{Smoothness Under Triangular Scale Parameterization}\label{proof:entropy_smoothness}
\vspace{1ex}
\printProofs[entropysmooth]

\clearpage
\subsection{Upper Bound on Gradient Variance of STL}

\subsubsection{General Decomposition}
\vspace{1ex}
\printProofs[stlupperboundlemma]

\newpage
\subsubsection{Full-Rank Parameterization}
\vspace{1ex}
\printProofs[stlupperboundfr]

\newpage
\subsubsection{Mean-Field Parameterization}\label{section:stl_meanfield}
\vspace{1ex}
\printProofs[stlupperboundmf]

\clearpage
\subsection{Lower Bound on Gradient Variance of STL}
\subsubsection{General Lower Bound}
\vspace{1ex}
\printProofs[stllowerbound]

\newpage
\subsubsection{Unimprovability}
\vspace{1ex}
\printProofs[stllowerboundunimprovability]

\clearpage
\subsection{Upper Bound on Gradient Variance of CFE}
\subsubsection{Full-Rank Parameterization}
\vspace{1ex}
\printProofs[cfeupperbound]

\newpage
\subsubsection{Mean-Field Parameterization}
\vspace{1ex}
\printProofs[cfeupperboundmf]

\clearpage
\subsection{Non-Asymptotic Complexity of Projected SGD}\label{section:projsgdcomplexity}

To precisely compare the computational complexity resulting from different estimators, we refine the convergence analyses of~\citet{domke_provable_2023}.
Specifically, we obtain precise complexity guarantees from their ``anytime convergence'' statements.
This type of convergence analysis, which has been popular in the ERM sample selection strategy literature~\citep[\S 1.1]{csiba_importance_2018}, is convenient for comparing the lower-order and constant factor improvements of different gradient estimators.

\subsubsection{QVC Gradient Estimator}
\vspace{1ex}
\printProofs[complexityprojsgdqvcfixed]
\newpage
\printProofs[complexityprojsgdqvcdec]

\clearpage
\subsubsection{Adaptive QVC Gradient Estimator}\label{section:complexity_adaptiveqvc}
\vspace{1ex}

As mentioned at the beginning of \cref{section:gradient_variance}, we established \textit{adaptive} QV bounds.
For the complexity guarantees for strongly convex objectives (\cref{thm:projsgd_stronglyconvex_fixedstepsize,thm:projsgd_stronglyconvex_decstepsize}), it is possible to optimize the free parameter \(\delta\) in the bounds, such that they automatically adapt to other problem-specific constants.
In this section, we do this for both SGD with fixed stepsize (\cref{thm:projsgd_stronglyconvex_adaptive_complexity}) and a decreasing (\cref{thm:projsgd_stronglyconvex_decstepsize_adaptive_complexity}) stepsize schedule.
%
\vspace{2ex}
\printProofs[complexityprojsgdadaptiveqvcfixed]

\newpage
\printProofs[complexityprojsgdadaptiveqvcdec]

\clearpage
\subsection{Non-Asymptotic Complexity of BBVI}
\subsubsection{CFE Gradient Estimator}
\vspace{1ex}
\printProofs[complexitybbvicfefixed]
\newpage
\printProofs[complexitybbvicfedec]

\newpage
\subsubsection{STL Gradient Estimator}
\vspace{1ex}
\printProofs[complexitybbvistl]
\newpage
\printProofs[complexitybbvistldec]

\newpage
\subsection{Fisher-Hyv\"arinen Divergence Between Gaussians}
%\subsubsection{Bounds on the Gradient Variance of STL}
\vspace{1ex}
\printProofs[stlgaussianfisher]
\newpage
\printProofs[gaussianklmeanfield]
\newpage
\printProofs[stlgaussian]


%\newpage
%\subsection{Connections with Other Gradient Variance Conditions}
%\vspace{1ex}
%\printProofs[gradientconditions]


%\printProofs

\end{document}
