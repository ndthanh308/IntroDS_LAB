
\section{MAIN RESULTS}
\vspace{-1ex}
\subsection{Triangular Scale Parameterization}\label{section:triangular_scale}
\vspace{-1ex}

First, we will demonstrate a parameterization that is more computationally efficient than the matrix square-root parameterization considered in \cref{section:scale_parameterization}, while satisfying the constraints \labelcref{item:positive_definite,item:linearity,item:convexity,item:smooth_entropy}.
We first turn our attention to the following domain for the variational parameters:
{%
\setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}
\setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
\begin{align*}
  \Lambda_{S} \triangleq
  \left\{\, \left(\vm, \mC\right) \in \mathbb{R}^{d} \times \mathbb{L}_{++}^d \mid \sigma_{\mathrm{min}}\left(\mC\right) \geq
   1/\sqrt{S} \,\right\},
\end{align*}
}%
where \(\mathbb{L}_{++}^d\) is the set of Cholesky factors.
A key special case is the mean-field variational family, which is a strict subset of \(\Lambda_S\), where we restrict \(\mC\) to be diagonal matrices.
With that said, we consider the two following parameterizations:
{
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{alignat*}{4}
  \mC &= \mL, 
  &&\qquad
  \text{(full-rank)}
  &&\qquad
  \\
  \mC &= \mathrm{diag}\left(L_{11}, \ldots, L_{dd}\right),
  &&\qquad
  \text{(mean-field)}
\end{alignat*}
}%
where \(\mL\) is a Cholesky factor.
In practice, the triangular matrix parameterization is most commonly used~\citep{kucukelbir_automatic_2017}, and results in lower gradient variance than the square root parameterization \citep{kim_practical_2023}.

\vspace{-1ex}
\paragraph{Projection Operator}
The key advantage of operating with triangular scale matrices is that the entropy is the log-sum of their eigenvalues, which turns out to be their diagonal elements.
This implies that the gradient of the entropy term \(\vlambda \mapsto \mathbb{H}\left(q_{\vlambda}\right)\) only resides on the diagonal subspace of \(\mC\).
Therefore, the ``smoothness'' of \(\vlambda \mapsto \mathbb{H}\left(q_{\vlambda}\right)\) can be achieved by only controlling the eigenvalues (or diagonal elements) of \(\mC\).
This sharply contrasts with the square-root parameterization where the constraint is on the \textit{singular values}, which are much harder to control.
Nevertheless, the canonical Euclidean projection operator is:
\begin{proposition}
    The Euclidean projection operator onto \(\Lambda_S\), 
    \(\mathrm{proj}_{\Lambda_S} : \mathbb{R}^d \times \mathbb{L}^d \to \Lambda_{S} \), is given as
{%
\setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}
\setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
    \begin{align*}
       \mathrm{proj}_{\Lambda_S}\left(\vlambda\right) 
       = \argmin_{\vlambda' \in \Lambda_S} \norm{\vlambda - \vlambda'}_2^2
       = \left(\vm, \widetilde{\mC}\right),
    \end{align*}
}
    where \(\widetilde{\mC}\) is the projection of \(\mC\) such that
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
    \[
      \widetilde{C}_{ij} = \begin{cases}
        \; \max\left(C_{ii}, \; 1/\sqrt{S}\right) &\text{for } i = j \\
        \; C_{ij} &\text{for } i \neq j.
      \end{cases}
    \]
}%
\end{proposition}
\vspace{-2ex}
\begin{proof}
    Since the eigenvalues of a triangular matrix are its diagonal elements, we notice that \(\Lambda_{S}\) is a constraint only on the diagonal elements of \(\mC\) such that \(C_{ii} \geq 1/\sqrt{S}\).
    Conveniently, this is an element-wise half-space constraint for which the projection follows as
{%
\setlength{\belowdisplayskip}{0ex} \setlength{\belowdisplayshortskip}{0ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
    \[
       \widetilde{C}_{ii} 
       = \argmin_{c \geq 1/\sqrt{S}} \norm{C_{ii} - c}_2^2
       = \max\left(C_{ii}, 1/\sqrt{S}\right).
    \]
}%    
\vspace{-1ex}
\end{proof}

\vspace{-1ex}
\paragraph{Theoretical Properties}
We will now prove that our construction is valid.
It is trivial that \labelcref{item:positive_definite,item:linearity,item:convexity} are satisfied.
We formally prove that \(\Lambda_S\) satisfies \labelcref{item:smooth_entropy}:

\begin{theoremEnd}[category=entropysmooth]{proposition}\label{thm:entropy_smoothness}
    The entropy \(\vlambda \mapsto \mathbb{H}\left(q_{\vlambda}\right)\) is \(S\)-Lipschitz smooth on \(\Lambda_S\).
\end{theoremEnd}
\begin{proofEnd}
    From the definition of the entropy of location-scale variational families, we have
    \begin{align*}
        \norm{ 
          \nabla_{\vlambda} \mathbb{H}\left(q_{\vlambda}\right) - \nabla_{\vlambda'} \mathbb{H}\left(q_{\vlambda'}\right) 
        }_2^2
        &=
        \norm{ 
          \nabla_{\mC} \log \mathrm{det} \,\mC - \nabla_{\mC'} \log \mathrm{det} \, \mC'
        }_2^2,
\shortintertext{since \(\mC, \mC' \in \mathbb{L}^d_{++}\),} 
        &=
        \norm{ 
          \nabla_{\mC} \log \mathrm{det} \left(\mathrm{diag}\left(\mC\right)\right) - \nabla_{\mC'} \log \mathrm{det} \left(\mathrm{diag}\left(\mC'\right)\right)
        }_2^2,
\shortintertext{since the determinant of triangular matrices is the product of the diagonal,} 
        &=
        \sum^d_{i=1} \abs{ \frac{\partial \log C_{ii}}{\partial C_{ii}} - \frac{\partial \log C_{ii}'}{\partial C_{ii}'}}^2 
        \\
        &=
        \sum^d_{i=1} \abs{ \frac{1}{C_{ii}} - \frac{1}{C_{ii}'}}^2 
        \\
        &=
        \sum^d_{i=1} C_{ii}^{-2} \abs{ C_{ii}' - C_{ii} }^2 {\left(C_{ii}'\right)}^{-2},
\shortintertext{and since \(\sigma_{\mathrm{min}}\left(\mC\right) \geq S^{\nicefrac{-1}{2}} \Leftrightarrow C_{ii}^{-2} \leq S\) for all \(i=1, \ldots, d\),} 
        &\leq
        S^2 \sum^d_{i=1} \abs{ C_{ii}' - C_{ii} }^2
        \\
        &=
        S^2 \norm{ \mathrm{diag}\left(\mC\right) - \mathrm{diag}\left(\mC'\right) }^2_2
        \\
        &\leq
        S^2 \norm{ \vlambda - \vlambda' }^2_2.
    \end{align*}
\end{proofEnd}

Given these results, we will hereafter assume projected SGD is run on \(\Lambda_S\) with the projection operator \(\mathrm{proj}_{\Lambda_S}\).

\subsection{Theoretical Analysis of the STL Estimator}\label{section:gradient_variance}
Before presenting our analysis on BBVI gradient estimators, we will discuss a notable aspect of our strategy and the key step in our proof.

Our main assumption on the target posterior is that it is \(L\)-log(-Lipschitz) smooth:
\begin{definition}
    \(\pi\) is said to be \(L\)-log-(Lipschitz) smooth if its log-density \(\log \pi : \mathbb{R}^d \to \mathbb{R}\) is \(L\)-Lipschitz smooth such that
    \[
      \norm{ \nabla \log \pi\left(\vz\right) - \nabla \log \pi\left(\vz'\right) }_2 \leq L \norm{\vz - \vz'}_2,
    \]
    for all \(\vz, \vz' \in \mathbb{R}^d\) and some \(0 < L < \infty\).
\end{definition}
If this holds for \(\pi\), the same bound holds for \(\ell\) as well since they are proportional up to a constant such that \(\nabla \log \ell = \nabla \log \pi\).
This assumption has been used by \citet{domke_provable_2019} to establish similar results for the CFE estimator and is also widely used in the analysis of sampling algorithms based on log-concave analysis. 
(See \citet[\S 2.3]{dwivedi_logconcave_2019} for such example.)
For probability measures, log-smoothness implies that the density of \(\pi\) can be upper bounded by some Gaussian.
Naturally, this essentially corresponds to assuming \(\pi\) has sub-Gaussian tails.

\vspace{-1ex}
\paragraph{Adaptive Bounds with the Peter-Paul Inequality}
Unlike the QV bounds obtained by \citet{domke_provable_2023}, our bounds involve a free parameter \(\delta \geq 0\).
We call these bounds \textit{adaptive} QV bounds.
%
\begin{assumption}[\textbf{Adaptive QV}]\label{assumption:adaptiveqvc}
  The gradient estimator \(\rvvg\) satisfies the bound
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \[
    \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2 \leq (1 + C \delta) \, \widetilde{\alpha} \, \norm{\vlambda - \vlambda^*}_2^2 + (1 + C^{-1} \delta^{-1}) \,\widetilde{\beta},
  \]
  }%
  for any \(\delta > 0\), any \(\vlambda \in \Lambda_S\), and some \(0 < \widetilde{\alpha}, \widetilde{\beta} < \infty\), where \(\vlambda^*\) is a stationary point.
\end{assumption}
%
This is a consequence of the use of the ``Peter-Paul'' inequality such that
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\begin{equation}
  {\left( a + b \right)}^2 \leq \left(1 + \delta\right) \,a^2 + (1 + \delta^{-1}) \,b^2,
  \label{eq:peterpaul}
\end{equation}
}%
and can be seen as a generalization of the usual inequality \({\left(a + b\right)}^2 \leq 2 a^2 + 2 b^2\).
Adjusting \(\delta\) can occasionally tighten the analysis.
In fact, \(\delta\) can be optimized to become \textit{adaptive} to the downstream analysis.
Indeed, in our complexity analysis, \(\delta\) automatically trades-off the influence of \(\widetilde{\alpha}\) and \(\widetilde{\beta}\) according to the accuracy budget \(\epsilon\).

\vspace{-1ex}
\paragraph{Key Lemma}
The key first step in all of our analysis is the following decomposition:
\input{thm_stl_general_decomposition}

Here, \(J_{\mathcal{T}}\) is a term that stems from the Jacobian of \(\mathcal{T}\).
Thus, \(J_{\mathcal{T}}\) contains the properties unique to the chosen variational family.
\(V_{1}\) and \(V_{2}\) measure how far the current variational approximation \(q_{\vlambda}\) is from a stationary point \({\vlambda^*}\).
Thus, both terms will eventually reach 0 as BBVI converges, regardless of family specification.
The key is \(V_{3}\), which captures the amount of mismatch between the score of the true posterior \(\pi\) and variational posterior \(q_{\vlambda^*}\).
Establishing the ``interpolation condition'' amounts to analyzing when \(V_{3}\) becomes 0.

\subsubsection{Upper Bounds}
We now present our upper bound on the expected-squared norm of the STL gradient estimator.
%
\input{thm_stl_upperbound}

\newpage
\begin{remark}[\textbf{Mean-Field Variational Family}]
  We prove an equivalent result for the mean-field variational family, \cref{thm:stl_upperbound_mf} in \cref{section:stl_meanfield}, which has an \(\mathcal{O}\left(\sqrt{d}\right)\) dimensional dependence.
\end{remark}
\vspace{0.5ex}
\begin{remark}[\textbf{Interpolation Condition}]
  \cref{thm:stl_upperbound} encompasses both settings where the variational family is well-specified and misspecified.
  That is, when the variational family is well specified, \textit{i.e.}, \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) = 0 \), we obtain interpolation such that \(\beta_{\mathrm{STL}} = 0\).
\end{remark}
\vspace{0.5ex}
\begin{remark}[\textbf{Adaptivity of Bound}]
  When the variational family is well specified such that \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) = 0 \), we can adaptively tighten the bound by setting \(\delta = 0\), where \(\alpha_{\mathrm{STL}}\) is reduced by a constant factor.
\end{remark}

\subsubsection{Lower Bounds}
We also obtain lower bounds on the expected-squared norm of the STL estimator to analyze its best-case behavior and the tightness of the bound.

\vspace{-1ex}
\paragraph{Necessary Conditions for Interpolation}
First, we obtain lower bounds that generally hold for all \(\vlambda \in \Lambda_L\) and any \(\pi\).
Our analysis relates the gradient variance with the Fisher-Hyv\"arinen divergence.
This can be related back to the KL divergence through an assumption on the posterior \(\pi\) known as the log-Sobolev inequality.
The general form of the log-Sobolev inequality was originally proposed by \citet{gross_logarithmic_1975} to study diffusion processes.
In this work, we use the form used by \citet{otto_generalization_2000}:
%
\begin{assumption}[\textbf{Log-Sobolev Inequality; LSI}]
\(\pi\) is said to satisfy the log-Sobolev inequality if, for any variational family \(\mathcal{Q}\) and all \(q_{\vlambda} \in \mathcal{Q}\), the following inequality holds:
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{0ex} \setlength{\abovedisplayshortskip}{0ex}
\[
  \DKL{q}{\pi}
  \leq
  \frac{C_{\mathrm{LSI}}}{2} \, \DHF{q}{\pi}.
\]
}%
\end{assumption}
\vspace{-1ex}
%
Strongly log-concave distributions are known to satisfy the LSI, where the strong log-concavity constant becomes the (inverse) LSI constant (see also~\citealp[Theorem 9.9]{villani_topics_2016}):

\vspace{1ex}
\begin{remark}[\citealp{bakry_diffusions_1985}]
    Let \(\pi\) be \(\mu\)-strongly log-concave.
    Then, LSI holds with \(C_{\mathrm{LSI}}^{-1} = \mu\).
\end{remark}

We now present a lower bound which holds for all \(\vlambda \in \Lambda_S\) and any log-differentiable \(\pi\):
%
\input{thm_stl_lowerbound}

\begin{corollary}[\textbf{Necessary Conditions for Interpolation}]
For the STL estimator, the interpolation condition does not hold if
\begin{enumerate}[label=\textbf{(\roman*)}]
  \vspace{-1.5ex}
  \setlength\itemsep{0.ex}
    \item \(\DHF{q_{\vlambda^*_{\mathrm{F}}}}{\pi} > 0\), or,
    \item when \(\pi\) is LSI, \(\DKL{q_{\vlambda^*_{\mathrm{KL}}}}{\pi} > 0\),
  \vspace{-1.5ex}
\end{enumerate}
%
  \begin{center}
   {\begingroup
    \setlength\tabcolsep{10pt} 
  \begin{tabular}{ll}
    \text{where }
    &
    \(\vlambda_{\mathrm{F}}^* \in \argmin_{\vlambda \in \Lambda_S} \DHF{q_{\vlambda}}{\pi} \),\; 
    \text{and} \\
    & \(\vlambda_{\mathrm{KL}}^* \in \argmin_{\vlambda \in \Lambda_S} \DKL{q_{\vlambda}}{\pi} \),
  \end{tabular}
  \endgroup}
  \end{center}
  \vspace{-1.5ex}
  for any \(0 < S < \infty\).
\end{corollary}
\vspace{-1ex}

\vspace{-1.ex}
\paragraph{Tightness Analysis}
The bound in \cref{thm:stl_lowerbound} is unfortunately not tight regarding the constants.
It, however, holds for all \(\vlambda\) and \(\pi\).
Instead, we establish an alternative lower bound that holds for some \(\vlambda\) and \(\pi\) but is tight regarding the dependence on \(d\) and \(L\).
\vspace{1ex}
\input{thm_stl_lowerbound_unimprovability}
%
\vspace{0.5ex}
\begin{remark}\label{remark:stl_tightness}
  \cref{thm:stl_lowerbound_unimprovability} implies that \cref{thm:stl_upperbound}  with \(S = L\) is tight with respect to the dimension dependence \(d\) and the log-smoothness \(L\) except for a factor of 4.
\end{remark}
\vspace{1ex}
\begin{remark}[\textbf{Room for Improvement}]
  Part of the factor of \(4\) looseness is due to the extreme worst case: when \(\nabla \log \pi\) and \(\nabla \log q_{\vlambda}\) are anti-correlated.
  This worst case is unlikely to appear in practice, thus making a tighter lower bound challenging to obtain.
  But at the same time, we were unsuccessful at seeking a general assumption that would rule out these worst cases in the upper bound.
  Specifically, we tried very hard to apply coercivity/gradient monotonicity of log-concave distributions, but to no avail, leaving this to future works.
\end{remark}

\subsection{Theoretical Analysis of the CFE Estimator}
We now present the analysis of the CFE estimator.
While the CFE estimator has been studied in-depth by \citet{domke_provable_2019,kim_practical_2023,domke_provable_2023}, we slightly improve the latest analysis of \citet[Theorem 3]{domke_provable_2023}.
Specifically, we improve the constants and obtain an adaptive bound.
This ensures that we have a fair comparison with the STL estimator.
%
\input{thm_cfe_upperbound}

% \begin{corollary}
%   Let \(S = L\).
%   Then, the expected-squared norm of the CFE estimator with the full-rank parameterization satisfies the QVC for any \(\vlambda \in \Lambda_{L}\) with the constants
% {%
% \setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
% \setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
%   \begin{align*}
%     \alpha_{\mathrm{CFE}} &= L^2 \left( d + k_{\varphi} + 4 \right) \left(1 + \delta\right),\\
%     \beta_{\mathrm{CFE}}  &= L^2 \left( d + k_{\varphi} \right) \left( 1 + \delta^{-1}  \right) {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2,
%   \end{align*}
%   }%
%   for any \(\delta \geq 0\).
% \end{corollary}

\vspace{1ex}
\begin{remark}[\textbf{Comparison with STL}]\label{remark:variance_comparison}
    Compared to the STL estimator, the constant \(\alpha\) of the CFE estimator is tighter by a factor of \(4\).
    Considering \cref{thm:stl_lowerbound_unimprovability}, the constant factor difference should be marginal in practice.
    %This means that the STL estimator will perform similarly when \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) large.
    % When \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) is small, it might have a smaller variance when \(\mathrm{D}_{F^4}\left(q_{\vlambda^*}, \pi\right)\) is small.
\end{remark}

\vspace{1ex}
\begin{remark}[\textbf{Intuitions on \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\)}]
  The quantity \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) can be expressed in the Wasserstein-2 distance as
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \[
    \mathrm{d}_{\mathcal{W}_2}\left(q_{\vlambda^*}, \delta_{\bar{\vz}}\right) = \sqrt{ {\lVert \vm^* - \bar{\vz}\rVert}_2^2 + \norm{\mC^*}_{\mathrm{F}}^2} =  {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2,
  \]
}
  where \(\delta_{\bar{\vz}}\) is a delta measure centered on the posterior mode \(\bar{\vz}\).
   Also, when the variational posterior mean \(\vm^*\) is close to \(\bar{\vz}\) such that  \({\lVert \vm^* - \bar{\vz}\rVert}_2^2 \approx 0\), \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\) corresponds to the variational posterior variance as
{%
\setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
  \[
     {\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2 \approx \norm{\mC^*}_{\mathrm{F}}^2 = \mathrm{tr}\, \Vsub{\rvvz \sim q_{\vlambda^*}}{\rvvz}.
  \]
}
\end{remark}

% \vspace{-1.ex}
% \subsection{Non-Asymptotic Complexity of SGD with the Quadratic Variance Condition}

\input{thm_projsgd_stronglyconvex_fixedstepsize}
\input{thm_projsgd_stronglyconvex_decstepsize}
% \input{thm_projsgd_convex_fixedstepsize}

%% \begin{remark}
%%   The bound for \cref{thm:projsgd_convex_fixedstepsize} is inconveniently non-linear with respect to \(\beta\) and \(\alpha\).
%%   Under ``interpolation'' such that \(\beta = 0\), it clearly reduces to a \(\mathcal{O}\left(1/\epsilon\right)\).
%%   When \(\beta > 0\) is non-negligible, a series expansion does suggest that the bound behaves as \(\mathcal{O}\left(1/\epsilon^2\right)\), which is the expected complexity guarantee~\citep{garrigos_handbook_2023}.
%% \end{remark}

%% \begin{remark}
%%   Note that \cref{thm:projsgd_convex_fixedstepsize} is quite loose due to the use of Bernoulli's inequality for simplifying the any-time convergence result by \citet[Theorem 7]{domke_provable_2023}. (This inequality is also used by \citeauthor{domke_provable_2023}.) 
%%   Tightening the any-time convergence statement would be an important future direction.
%% \end{remark}

% \begin{remark}
%   Lastly, obtaining a \(\mathcal{O}\left(1/\sqrt{T}\right)\) convergence guarantee with a decreasing stepsize schedule independent of the number of steps \(T\) is an open problem.
%   (\citet[Theorem 7]{domke_provable_2023} use a fixed stepsize dependent on \(T\). Thus, one must fix the number of steps before running projected SGD.) 
% \end{remark}

\subsection{Non-Asymptotic Complexity of Black-Box Variational Inference}\label{section:bbvicomplexity}
\paragraph{Strongly Log-Concave Posteriors}
First, let us define the following:
\begin{definition}
    \(\pi\) is said to be \(\mu\)-strongly log-concave if its log-density \(\log \pi : \mathbb{R}^d \to \mathbb{R}\) (equivalently \(\log \ell\)) satisfies the inequality
{%
\setlength{\abovedisplayskip}{.5ex} \setlength{\abovedisplayshortskip}{.5ex}
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
    \[
      \inner{ \nabla \log \pi\left(\vz\right) }{ \vz - \vz' }
      \geq \log \pi\left(\vz\right) - \log \pi\left(\vz'\right)
      +
      \frac{\mu}{2} \norm{ \vz - \vz' }^2_2
    \]
}%
    for all \(\vz, \vz' \in \mathbb{R}^d\) and some \(\mu > 0\).
\end{definition}
Essentially, this assumes that the log-density of \(\pi\) is \(\mu\)-strongly convex.
It also implies that the density of \(\pi\) is lower bounded by some Gaussian.
A consequence of \(\mu\)-strong log-concavity is that, combined with the constraints on the variational parameterization in \cref{section:scale_parameterization}, the ELBO is also \(\mu\)-strongly convex \citep{domke_provable_2023,kim_convergence_2023,challis_gaussian_2013}.
\(\mu\)-strongly log-concave posteriors can easily be constructed by combining a log-concave likelihood with a Gaussian prior, and are popularly used to analyze BBVI and sampling algorithms.

\vspace{-1.ex}
\paragraph{Theoretical Setup}
We now apply the general complexity results for projected SGD established in \cref{section:projsgdcomplexity} to BBVI.
\begin{enumerate*}[label=\textbf{(\roman*)}]
    \item strongly log-concave posteriors,
    \item SGD run with fixed stepsizes, and 
    \item the full-rank variational family.
\end{enumerate*}
This is because the convergence analyses for \textbf{(ii)} \(\cap\) \textbf{(iii)} are the tightest.
Although the bounds for the mean-field parameterization have better dependences on \(d\), so far, it is unknown whether they are tight~\citep{kim_practical_2023}. (See also \citealp[Conjecture 1]{kim_convergence_2023}.)

\input{thm_projsgd_stronglyconvex_adaptive_complexity}

\paragraph{Complexity of BBVI on Strongly-Log-Concave \(\pi\)}
We can now plug in the constants obtained in \cref{section:gradient_variance}.
This immediately establishes the iteration complexity resulting from the use of different gradient estimators.
\vspace{0ex}
%
\input{thm_projsgd_bbvicfe_stronglyconvex_complexity}
\vspace{1.ex}

In particular, the following theorem establishes that BBVI with the STL estimator can achieve linear convergence under perfect variational family specification.
\vspace{0ex}
\input{thm_projsgd_bbvistl_stronglyconvex_complexity}

\vspace{1ex}
\begin{corollary}[\textbf{Linear Convergence of BBVI with STL}]
  If the variational family is perfectly specified such that \( \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda}^*, \pi\right) = 0\) for \(\vlambda^* = \argmin_{\vlambda \in \Lambda_L} F\left(\vlambda\right)\), then BBVI with the STL estimator converges linearly with a complexity of \(\mathcal{O}\left(d \kappa^2 \log \left( 1 / \epsilon \right) \right)\).
\end{corollary}

% \begin{remark}
%   The overall complexity of the STL estimator is larger by a constant factor of 4 compared to that of CFE.
%   This means that the STL estimator will converge slowly for a large target \(\epsilon\).
%   As mentioned in \cref{remark:stl_tightness,remark:variance_comparison}, a factor of 2 difference would be more realistic and is closer to what is observed in our simulations.
% \end{remark}

\vspace{1ex}
\begin{remark}
  Convergence is slowed when using a decreasing step size schedule, as shown in \cref{thm:projsgd_bbvistl_decstepsize_complexity}.
  Thus, one does not achieve a linear convergence rate under this schedule even if the variational family is perfectly specified. 
  However, when the variational family is misspecified, this achieves a better rate of \(\mathcal{O}\left(1/\epsilon\right)\) compared to the \(\mathcal{O}\left(1/\epsilon \log 1/\epsilon\right)\) of \cref{thm:projsgd_bbvistl_complexity}.
\end{remark}

\vspace{1ex}
\begin{remark}[\textbf{Variational Family Misspecification}]\label{remark:misspecification}
  Under variational family misspecification, STL has an \(\mathcal{O}\left(1/\epsilon\right)\) dependence on the 4th order Fisher divergence \(\mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) > 0\).
  To compare the computational performance of CFE and STL in this setting, one needs to compare \( L^{-2} \sqrt{\mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right)}\) versus \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\).
\end{remark}

\vspace{1ex}
\begin{remark}
  \cref{thm:stl_upperbound_mf} also implies that the mean-field parameterization improves the dimension dependence to a complexity of \(\mathcal{O}\left( \sqrt{d} \kappa^2 \log \left( 1 / \epsilon \right) \right)\).
\end{remark}

%% \paragraph{Complexity of BBVI on Log-Concave Posteriors}
%% Lastly, we present the complexity of BBVI with the STL estimator when \(\pi\) is only log-concave.
%% Since the complexity guarantee of \cref{thm:projsgd_convex_fixedstepsize} is too nonlinear with respect to \(\beta\), we present the case where the variational family is well specified (\(\beta_{\mathrm{STL}} = 0\)).
%% In this setting, the STL estimator achieves a better complexity than that of the CFE estimator.

%% \input{thm_projsgd_bbvistl_convex_complexity}

%% \subsection{Variational Family Misspecification and the STL Estimator}

%% As mentioned in \cref{remark:misspecification}, comparing the performance of the STL estimator against the CFE estimator involves the 4th-order Fisher divergence and \({\lVert \bar{\vlambda} - \vlambda^* \rVert}_2^2\).
%% Unfortunately, the relationship between the two quantities is not obvious.
%% Because of this, we further reduce our scope to Gaussians in order to gather more intuition.

%% Recall that term \(T_{\ding{184}}\) in \cref{thm:stl_decomposition} captures the effect of variational family specification on the STL estimator.
%% Assuming both the posterior and variational approximation are Gaussians, we can obtain a simpler result on \(T_{\ding{184}}\).

\subsection{Should we stick the landing?}
When the variational family is misspecified, it is hard to tell \textit{when} STL would be superior to CFE; the Fisher-Hyv\"arinen divergence and the posterior variance are fundamentally unrelated quantities.
Furthermore, the Fisher-Hyv\"arinen divergence is hard to interpret apart from some relationships with other divergences~\citep{huggins_practical_2018}.
Thus, we conclude by providing a characterization of the Fisher-Hyv\"arinen divergence.

Our final analysis will focus on Gaussian posteriors and the mean-field Gaussian family.
In practice, the STL estimator becomes infeasible to use with full-rank variational families as each evaluation of the log-density \(\log q_{\vlambda}\) involves a back-substitution with a \(\mathcal{O}\left(d^3\right)\) cost and numerical stability becomes a concern.
Therefore, studying the effect of misspecification of mean-field is particularly relevant.

\vspace{1ex}
\input{thm_stl_gaussian}
\vspace{1ex}

\begin{remark}
For Gaussians, the 4th-order Fisher-Hyv\"arinen divergence term in \cref{thm:stl_upperbound} can be replaced by its 2nd-order counterpart.
Thus, combined with \cref{thm:stl_lowerbound}, the 2nd-order Fisher-Hyv\"arinen divergence fully characterizes the variance of STL.
\end{remark}

\vspace{1ex}
\begin{remark}
  \cref{thm:fisher_bound} implies that, when approximating a full-rank Gaussian with a mean-field Gaussian, the value of the Fisher-Hyv\"arinen divergence is tightly characterized by the degree of correlation in the posterior; it will increase indefinitely as the posterior correlation matrix becomes singular.
\end{remark}

\vspace{1ex}
\begin{remark}
  We have provided a sufficient condition for the STL estimator to perform poorly compared to the CFE estimator. 
  It is foreseeable that alternative types of model misspecification abundant in practice should yield additional sufficient conditions, \textit{i.e.}, tail mismatch, but we leave this to future works.
\end{remark}

