\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2020)Agrawal, Sheldon, and
  Domke]{agrawal_advances_2020}
Abhinav Agrawal, Daniel~R Sheldon, and Justin Domke.
\newblock Advances in black-box {{VI}}: {{Normalizing}} flows, importance
  weighting, and optimization.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~33, pages 17358--17369. {Curran Associates, Inc.}, 2020.

\bibitem[Alquier(2021)]{alquier_nonexponentially_2021}
Pierre Alquier.
\newblock Non-{{Exponentially Weighted Aggregation}}: {{Regret Bounds}} for
  {{Unbounded Loss Functions}}.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume 193 of \emph{{{PMLR}}}, pages 207--218. {JMLR}, July
  2021.

\bibitem[Alquier and Ridgway(2020)]{alquier_concentration_2020}
Pierre Alquier and James Ridgway.
\newblock Concentration of tempered posteriors and of their variational
  approximations.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (3):\penalty0
  1475--1497, June 2020.

\bibitem[Bakry and {\'E}mery(1985)]{bakry_diffusions_1985}
D.~Bakry and M.~{\'E}mery.
\newblock Diffusions hypercontractives.
\newblock In \emph{S{\'e}minaire de {{Probabilit{\'e}s XIX}} 1983/84}, volume
  1123, pages 177--206. {Springer Berlin Heidelberg}, {Berlin, Heidelberg},
  1985.

\bibitem[Bhatia et~al.(2022)Bhatia, Kuang, Ma, and
  Wang]{bhatia_statistical_2022}
Kush Bhatia, Nikki~Lijing Kuang, Yi-An Ma, and Yixin Wang.
\newblock Statistical and computational trade-offs in variational inference: A
  case study in inferential model selection.
\newblock {{arXiv}} Preprint arXiv:2207.11208, {arXiv}, July 2022.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and
  McAuliffe]{blei_variational_2017}
David~M. Blei, Alp Kucukelbir, and Jon~D. McAuliffe.
\newblock Variational inference: {{A}} review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (518):\penalty0 859--877, April 2017.

\bibitem[Bottou(1999)]{bottou_online_1999}
L{\'e}on Bottou.
\newblock On-line learning and stochastic approximations.
\newblock In \emph{On-{{Line Learning}} in {{Neural Networks}}}, pages 9--42.
  {Cambridge University Press}, 1 edition, January 1999.

\bibitem[Buchholz et~al.(2018)Buchholz, Wenzel, and
  Mandt]{buchholz_quasimonte_2018}
Alexander Buchholz, Florian Wenzel, and Stephan Mandt.
\newblock Quasi-{{Monte Carlo}} variational inference.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~80 of \emph{{{PMLR}}}, pages 668--677. {JMLR}, July 2018.

\bibitem[Challis and Barber(2013)]{challis_gaussian_2013}
Edward Challis and David Barber.
\newblock Gaussian {{Kullback-Leibler}} approximate inference.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (68):\penalty0 2239--2286, 2013.

\bibitem[{Ch{\'e}rief-Abdellatif} et~al.(2019){Ch{\'e}rief-Abdellatif},
  Alquier, and Khan]{cherief-abdellatif_generalization_2019}
Badr-Eddine {Ch{\'e}rief-Abdellatif}, Pierre Alquier, and Mohammad~Emtiyaz
  Khan.
\newblock A generalization bound for online variational inference.
\newblock In \emph{Proceedings of the {{Asian Conference}} on {{Machine
  Learning}}}, volume 101 of \emph{{{PMLR}}}, pages 662--677. {JMLR}, October
  2019.

\bibitem[Csiba and Richt{\'a}rik(2018)]{csiba_importance_2018}
Dominik Csiba and Peter Richt{\'a}rik.
\newblock Importance sampling for minibatches.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (27):\penalty0 1--21, 2018.

\bibitem[Dieuleveut et~al.(2020)Dieuleveut, Durmus, and
  Bach]{dieuleveut_bridging_2020}
Aymeric Dieuleveut, Alain Durmus, and Francis Bach.
\newblock Bridging the gap between constant step size stochastic gradient
  descent and {{Markov}} chains.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (3):\penalty0
  1348--1382, June 2020.

\bibitem[Domke(2019)]{domke_provable_2019}
Justin Domke.
\newblock Provable gradient variance guarantees for black-box variational
  inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~32, pages 329--338. {Curran Associates, Inc.}, 2019.

\bibitem[Domke(2020)]{domke_provable_2020}
Justin Domke.
\newblock Provable smoothness guarantees for black-box variational inference.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume 119 of \emph{{{PMLR}}}, pages 2587--2596. {JMLR}, July
  2020.

\bibitem[Domke et~al.(2023{\natexlab{a}})Domke, Garrigos, and
  Gower]{domke_provable_2023}
Justin Domke, Guillaume Garrigos, and Robert Gower.
\newblock Provable convergence guarantees for black-box variational inference.
\newblock {{arXiv}} Preprint arXiv:2306.03638v1, {arXiv}, June
  2023{\natexlab{a}}.

\bibitem[Domke et~al.(2023{\natexlab{b}})Domke, Garrigos, and
  Gower]{domke_provable_2023a}
Justin Domke, Guillaume Garrigos, and Robert Gower.
\newblock Provable convergence guarantees for black-box variational inference.
\newblock {{arXiv}} Preprint arXiv:2306.03638v2, {arXiv}, August
  2023{\natexlab{b}}.

\bibitem[Dwivedi et~al.(2019)Dwivedi, Chen, Wainwright, and
  Yu]{dwivedi_logconcave_2019}
Raaz Dwivedi, Yuansi Chen, Martin~J. Wainwright, and Bin Yu.
\newblock Log-concave sampling: {{Metropolis-Hastings}} algorithms are fast.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (183):\penalty0 1--42, 2019.

\bibitem[Fan et~al.(2015)Fan, Wang, Beck, Kwok, and Heller]{fan_fast_2015}
Kai Fan, Ziteng Wang, Jeff Beck, James Kwok, and Katherine~A Heller.
\newblock Fast second order stochastic backpropagation for variational
  inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~28, pages 1387--1395. {Curran Associates, Inc.}, 2015.

\bibitem[Fujisawa and Sato(2021)]{fujisawa_multilevel_2021}
Masahiro Fujisawa and Issei Sato.
\newblock Multilevel {{Monte Carlo}} variational inference.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (278):\penalty0 1--44, 2021.

\bibitem[Garrigos and Gower(2023)]{garrigos_handbook_2023}
Guillaume Garrigos and Robert~M. Gower.
\newblock Handbook of convergence theorems for (stochastic) gradient methods.
\newblock Preprint arXiv:2301.11235, {arXiv}, February 2023.

\bibitem[Geffner and Domke(2018)]{geffner_using_2018}
Tomas Geffner and Justin Domke.
\newblock Using large ensembles of control variates for variational inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~31, pages 9960--9970. {Curran Associates, Inc.}, 2018.

\bibitem[Geffner and Domke(2020{\natexlab{a}})]{geffner_approximation_2020}
Tomas Geffner and Justin Domke.
\newblock Approximation {{Based Variance Reduction}} for {{Reparameterization
  Gradients}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~33, pages 2397--2407. {Curran Associates, Inc.}, 2020{\natexlab{a}}.

\bibitem[Geffner and Domke(2020{\natexlab{b}})]{geffner_rule_2020}
Tomas Geffner and Justin Domke.
\newblock A rule for gradient estimator selection, with an application to
  variational inference.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, volume 108 of \emph{{{PMLR}}}, pages
  1803--1812. {JMLR}, August 2020{\natexlab{b}}.

\bibitem[Geffner and Domke(2021)]{geffner_difficulty_2021}
Tomas Geffner and Justin Domke.
\newblock On the difficulty of unbiased alpha divergence minimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume 139 of \emph{{{PMLR}}}, pages 3650--3659. {JMLR}, July
  2021.

\bibitem[Gower et~al.(2020)Gower, Schmidt, Bach, and
  Richtarik]{gower_variancereduced_2020}
Robert~M. Gower, Mark Schmidt, Francis Bach, and Peter Richtarik.
\newblock Variance-reduced methods for machine learning.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (11):\penalty0
  1968--1983, November 2020.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower_sgd_2019}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richt{\'a}rik.
\newblock {{SGD}}: {{General}} analysis and improved rates.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~97 of \emph{{{PMLR}}}, pages 5200--5209. {JMLR}, June 2019.

\bibitem[Gross(1975)]{gross_logarithmic_1975}
Leonard Gross.
\newblock Logarithmic {{Sobolev}} inequalities.
\newblock \emph{American Journal of Mathematics}, 97\penalty0 (4):\penalty0
  1061--1083, 1975.

\bibitem[Harvey et~al.(2019)Harvey, Liaw, Plan, and
  Randhawa]{harvey_tight_2019}
Nicholas J.~A. Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa.
\newblock Tight analyses for non-smooth stochastic gradient descent.
\newblock In \emph{Proceedings of the {{Conference}} on {{Learning Theory}}},
  volume~99 of \emph{{{PMLR}}}, pages 1579--1613. {JMLR}, June 2019.

\bibitem[Hoffman and Ma(2020)]{hoffman_blackbox_2020}
Matthew Hoffman and Yian Ma.
\newblock Black-box variational inference as a parametric approximation to
  {{Langevin}} dynamics.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume 119 of \emph{{{PMLR}}}, pages 4324--4341. {JMLR},
  November 2020.

\bibitem[Huggins et~al.(2018)Huggins, Campbell, Kasprzak, and
  Broderick]{huggins_practical_2018}
Jonathan~H. Huggins, Trevor Campbell, Miko{\l}aj Kasprzak, and Tamara
  Broderick.
\newblock Practical bounds on the error of {{Bayesian}} posterior
  approximations: {{A}} nonasymptotic approach.
\newblock {{arXiv}} Preprint arXiv:1809.09505, 2018.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen_estimation_2005}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (24):\penalty0 695--709, 2005.

\bibitem[Johnson and Zhang(2013)]{johnson_accelerating_2013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~26, pages 315--323. {Curran Associates, Inc.}, 2013.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{jordan_introduction_1999}
Michael~I. Jordan, Zoubin Ghahramani, Tommi~S. Jaakkola, and Lawrence~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine Learning}, 37\penalty0 (2):\penalty0 183--233, 1999.

\bibitem[Khan et~al.(2016)Khan, Babanezhad, Lin, Schmidt, and
  Sugiyama]{khan_faster_2016}
Mohammad~Emtiyaz Khan, Reza Babanezhad, Wu~Lin, Mark Schmidt, and Masashi
  Sugiyama.
\newblock Faster stochastic variational inference using proximal-gradient
  methods with general divergence functions.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, {{UAI}}'16, pages 319--328, {Arlington, Virginia, USA}, 2016.
  {AUAI Press}.

\bibitem[Khan et~al.(2015)Khan, Baque, Fleuret, and
  Fua]{khan_kullbackleibler_2015}
Mohammad Emtiyaz~E Khan, Pierre Baque, Fran{\c c}ois Fleuret, and Pascal Fua.
\newblock Kullback-{{Leibler}} proximal variational inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~28, pages 3402--3410. {Curran Associates, Inc.}, 2015.

\bibitem[Kim et~al.(2023{\natexlab{a}})Kim, Oh, Wu, Ma, and
  Gardner]{kim_convergence_2023}
Kyurae Kim, Jisu Oh, Kaiwen Wu, Yian Ma, and Jacob~R. Gardner.
\newblock On the convergence of black-box variational inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} (to
  {{Appear}})}, volume~36, {New Orleans, LA, USA}, December 2023{\natexlab{a}}.
  {Curran Associates Inc.}

\bibitem[Kim et~al.(2023{\natexlab{b}})Kim, Wu, Oh, and
  Gardner]{kim_practical_2023}
Kyurae Kim, Kaiwen Wu, Jisu Oh, and Jacob~R. Gardner.
\newblock Practical and matching gradient variance bounds for black-box
  variational {{Bayesian}} inference.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume 202 of \emph{{{PMLR}}}, pages 16853--16876, {Honolulu,
  HI, USA}, July 2023{\natexlab{b}}. {JMLR}.

\bibitem[Kingma and Welling(2014)]{kingma_autoencoding_2014}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational {{Bayes}}.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Learning Representations}}}, {Banff, AB, Canada}, April 2014.

\bibitem[Kucukelbir et~al.(2017)Kucukelbir, Tran, Ranganath, Gelman, and
  Blei]{kucukelbir_automatic_2017}
Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, and David~M.
  Blei.
\newblock Automatic differentiation variational inference.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (14):\penalty0 1--45, 2017.

\bibitem[Kullback and Leibler(1951)]{kullback_information_1951}
S.~Kullback and R.~A. Leibler.
\newblock On information and sufficiency.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (1):\penalty0 79--86, March 1951.

\bibitem[Liu and Owen(2021)]{liu_quasimonte_2021}
Sifan Liu and Art~B. Owen.
\newblock Quasi-{{Monte Carlo}} quasi-{{Newton}} in {{Variational Bayes}}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (243):\penalty0 1--23, 2021.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{ma_power_2018}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: {{Understanding}} the effectiveness of
  {{SGD}} in modern over-parametrized learning.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~80 of \emph{{{PMLR}}}, pages 3325--3334. {JMLR}, July
  2018.

\bibitem[Miller et~al.(2017)Miller, Foti, D'~Amour, and
  Adams]{miller_reducing_2017}
Andrew Miller, Nick Foti, Alexander D'~Amour, and Ryan~P Adams.
\newblock Reducing reparameterization gradient variance.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~30, pages 3708--3718. {Curran Associates, Inc.}, 2017.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and
  Mnih]{mohamed_monte_2020}
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
\newblock Monte {{Carlo}} gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (132):\penalty0 1--62, 2020.

\bibitem[Moulines and Bach(2011)]{moulines_nonasymptotic_2011}
Eric Moulines and Francis Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~24, pages 451--459. {Curran Associates, Inc.}, 2011.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski_robust_2009}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, January 2009.

\bibitem[Nesterov(2004)]{nesterov_introductory_2004}
Yurii Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization}, volume~87 of
  \emph{Applied {{Optimization}}}.
\newblock {Springer US}, {Boston, MA}, 2004.

\bibitem[Otto and Villani(2000)]{otto_generalization_2000}
F.~Otto and C.~Villani.
\newblock Generalization of an inequality by talagrand and links with the
  logarithmic sobolev inequality.
\newblock \emph{Journal of Functional Analysis}, 173\penalty0 (2):\penalty0
  361--400, June 2000.

\bibitem[Paisley et~al.(2012)Paisley, Blei, and
  Jordan]{paisley_variational_2012}
John Paisley, David~M. Blei, and Michael~I. Jordan.
\newblock Variational bayesian inference with stochastic search.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, {{ICML}}'12, pages 1363--1370, {Madison, WI, USA}, 2012.
  {Omnipress}.

\bibitem[Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood,
  and Teh]{rainforth_tighter_2018}
Tom Rainforth, Adam Kosiorek, Tuan~Anh Le, Chris Maddison, Maximilian Igl,
  Frank Wood, and Yee~Whye Teh.
\newblock Tighter variational bounds are not necessarily better.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~80 of \emph{{{PMLR}}}, pages 4277--4285. {JMLR}, July
  2018.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and
  Sridharan]{rakhlin_making_2012}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, {{ICML}}'12, pages 1571--1578, {Madison, WI, USA}, 2012.
  {Omnipress}.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath_black_2014}
Rajesh Ranganath, Sean Gerrish, and David Blei.
\newblock Black box variational inference.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, volume~33 of \emph{{{PMLR}}}, pages 814--822.
  {JMLR}, April 2014.

\bibitem[Regier et~al.(2017)Regier, Jordan, and McAuliffe]{regier_fast_2017}
Jeffrey Regier, Michael~I Jordan, and Jon McAuliffe.
\newblock Fast black-box variational inference through stochastic trust-region
  optimization.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~30, pages 2399--2408. {Curran Associates, Inc.}, 2017.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende_stochastic_2014}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~32 of \emph{{{PMLR}}}, pages 1278--1286. {JMLR}, June
  2014.

\bibitem[Robbins and Monro(1951)]{robbins_stochastic_1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, September 1951.

\bibitem[Roeder et~al.(2017)Roeder, Wu, and Duvenaud]{roeder_sticking_2017}
Geoffrey Roeder, Yuhuai Wu, and David~K Duvenaud.
\newblock Sticking the landing: {{Simple}}, lower-variance gradient estimators
  for variational inference.
\newblock In \emph{Advances in {{Neural Information Processing Systems}}},
  volume~30, pages 6928--6937. {Curran Associates, Inc.}, 2017.

\bibitem[Rudner et~al.(2021)Rudner, Key, Gal, and
  Rainforth]{rudner_signaltonoise_2021}
Tim G.~J. Rudner, Oscar Key, Yarin Gal, and Tom Rainforth.
\newblock On {{Signal-to-Noise Ratio Issues}} in {{Variational Inference}} for
  {{Deep Gaussian Processes}}.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, {{PMLR}}, pages 9148--9156. {JMLR}, July 2021.

\bibitem[Schmidt and Roux(2013)]{schmidt_fast_2013}
Mark Schmidt and Nicolas~Le Roux.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock {{arXiv}} Preprint arXiv:1308.6370, {arXiv}, August 2013.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and
  Bach]{schmidt_minimizing_2017}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  March 2017.

\bibitem[{Shalev-Shwartz} et~al.(2011){Shalev-Shwartz}, Singer, Srebro, and
  Cotter]{shalev-shwartz_pegasos_2011}
Shai {Shalev-Shwartz}, Yoram Singer, Nathan Srebro, and Andrew Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for {{SVM}}.
\newblock \emph{Mathematical Programming}, 127\penalty0 (1):\penalty0 3--30,
  March 2011.

\bibitem[Solodov(1998)]{solodov_incremental_1998}
M.V. Solodov.
\newblock Incremental gradient algorithms with stepsizes bounded away from
  zero.
\newblock \emph{Computational Optimization and Applications}, 11\penalty0
  (1):\penalty0 23--35, October 1998.

\bibitem[Titsias and {L{\'a}zaro-Gredilla}(2014)]{titsias_doubly_2014}
Michalis Titsias and Miguel {L{\'a}zaro-Gredilla}.
\newblock Doubly stochastic variational {{Bayes}} for non-conjugate inference.
\newblock In \emph{Proceedings of the {{International Conference}} on {{Machine
  Learning}}}, volume~32 of \emph{{{PMLR}}}, pages 1971--1979. {JMLR}, June
  2014.

\bibitem[Vaswani et~al.(2019)Vaswani, Bach, and Schmidt]{vaswani_fast_2019}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of {{SGD}} for over-parameterized models
  and an accelerated perceptron.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume~89 of
  \emph{{{PMLR}}}, pages 1195--1204. {JMLR}, April 2019.

\bibitem[Villani(2016)]{villani_topics_2016}
C{\'e}dric Villani.
\newblock \emph{Topics in Optimal Transportation}.
\newblock Number~58 in Graduate Studies in Mathematics. {American Mathematical
  Society}, {Providence, Rhode Island}, 2016.

\bibitem[Wang et~al.(2024)Wang, Geffner, and Domke]{wang_joint_2024}
Xi~Wang, Tomas Geffner, and Justin Domke.
\newblock Joint control variate for faster black-box variational inference.
\newblock In \emph{Proceedings of {{The International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}} (to Appear)}, {{PMLR}}.
  {JMLR}, May 2024.

\bibitem[Wright and Recht(2021)]{wright_optimization_2021}
Stephen~J. Wright and Benjamin Recht.
\newblock \emph{Optimization for Data Analysis}.
\newblock {Cambridge University Press}, {New York}, 2021.

\bibitem[Xu et~al.(2019)Xu, Quiroz, Kohn, and Sisson]{xu_variance_2019}
Ming Xu, Matias Quiroz, Robert Kohn, and Scott~A. Sisson.
\newblock Variance reduction properties of the reparameterization trick.
\newblock In \emph{Proceedings of the {{International Conference}} on
  {{Artificial Intelligence}} and {{Statistics}}}, volume~89 of
  \emph{{{PMLR}}}, pages 2711--2720. {JMLR}, April 2019.

\bibitem[Xu and Campbell(2022)]{xu_computational_2022}
Zuheng Xu and Trevor Campbell.
\newblock The computational asymptotics of {{Gaussian}} variational inference
  and the {{Laplace}} approximation.
\newblock \emph{Statistics and Computing}, 32\penalty0 (4), August 2022.

\bibitem[Zegers(2015)]{zegers_fisher_2015}
Pablo Zegers.
\newblock Fisher information properties.
\newblock \emph{Entropy}, 17\penalty0 (12):\penalty0 4918--4939, July 2015.

\bibitem[Zhang et~al.(2019)Zhang, Butepage, Kjellstrom, and
  Mandt]{zhang_advances_2019}
Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt.
\newblock Advances in variational inference.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 41\penalty0 (8):\penalty0 2008--2026, August 2019.

\end{thebibliography}
