
\section{PRELIMINARIES}\label{section:preliminaries}

{%\small %\footnotesize
\paragraph{Notation}
Random variables are denoted in sans-serif (\textit{e.g.}, \(\rvx\), \(\rvvx\)), vectors are in bold (\textit{e.g.}, \(\vx\), \(\rvvx\)), and matrices are in bold capitals (\textit{e.g.} \(\mA\)).
For a vector \(\vx \in \mathbb{R}^d\), we denote the inner product as \(\vx^{\top}\vx\) and \(\inner{\vx}{\vx}\), the \(\ell_2\) norm as \(\norm{\vx}_2 = \sqrt{\vx^{\top}\vx}\).
For a matrix {\footnotesize\(\mA\), \(\norm{\mA}_{\mathrm{F}} = \sqrt{\mathrm{tr}\left(\mA^{\top} \mA\right)}\)} denotes the Frobenius norm, and for some matrix \(\mB\), \(\mA \succeq \mB\) is the Loewner order implying that \(\mA - \mB\) is a positive semi-definite matrix.
\(\mathbb{S}^d\), \(\mathbb{S}^d_{++}\), \(\mathbb{L}^d\), \(\mathbb{L}^d_{++}\) are the set of symmetric, positive definite, triangular, and triangular matrices with strictly positive eigenvalues (Cholesky factors).
\(\sigma_{\mathrm{min}}\left(\mA\right)\) is the smallest eigenvalue of \(\mA\).
}

\subsection{Variational Inference}

Variational inference (VI,~\citealp{zhang_advances_2019, blei_variational_2017, jordan_introduction_1999}) aims to minimize the exclusive (or reverse) Kullback-Leibler (KL; \citealp{kullback_information_1951}) divergence as:
%
{\begingroup
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\begin{align*}
    \minimize_{\vlambda \in \Lambda}\; \mathrm{D}_{\mathrm{KL}}\left(q_{\vlambda}, \pi\right)
    \triangleq
    \mathbb{E}_{\rvvz \sim q_{\vlambda}} -\log \pi \left(\rvvz\right) -\mathbb{H}\left(q_{\vlambda}\right),
\end{align*}
\endgroup}
%
\begin{center}
  \vspace{-2ex}
  {\begingroup
  %  \setlength\tabcolsep{1.0ex} 
  \begin{tabular}{lll}
    where 
    & \(\mathrm{D}_{\mathrm{KL}}\) & is the KL divergence, \\
    & \(\mathbb{H}\)     & is the differential entropy, \\
    & \(\pi\) & is the (target) posterior distribution,  \\
    & \(q_{\vlambda}\) & is the variational approximation. \\
  \end{tabular}
  \vspace{-2ex}
  \endgroup}
\end{center}

For Bayesian posterior inference, the KL divergence is, unfortunately, intractable.
Instead, one equivalently minimizes the negative \textit{evidence lower bound} (ELBO;~\citealp{jordan_introduction_1999}) \(F\) such that:
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\[
  \minimize_{\vlambda \in \Lambda}\; F\left(\vlambda\right)
  \triangleq
  \mathbb{E}_{\rvvz \sim q_{\vlambda}} -\log \ell \left(\rvvz\right) - \mathbb{H}\left(q_{\vlambda}\right),
\]
}%
where \(\ell\left(\vz\right) \propto \pi\left(\vz\right)\) is the unnormalized posterior proportional up to a multiplicative constant.
In typical use cases of VI, we only have access to \(\ell\) but not \(\pi\), and the normalizing constant is intractable.

\vspace{-1ex}
\paragraph{Black-Box Variational Inference}
Black-box variational inference (BBVI; \citealp{ranganath_black_2014,titsias_doubly_2014}) minimizes \(F\) by leveraging stochastic gradient descent (SGD; \citealp{robbins_stochastic_1951,bottou_online_1999,nemirovski_robust_2009}).
By obtaining a stochastic estimate \(\rvvg\left(\vlambda\right)\) which is \textit{unbiased}  as \(\mathbb{E} \rvvg\left(\vlambda\right) = \nabla F\left(\vlambda\right)\), BBVI repeats:
{
\setlength{\belowdisplayskip}{1ex} \setlength{\belowdisplayshortskip}{1ex}
\setlength{\abovedisplayskip}{1ex} \setlength{\abovedisplayshortskip}{1ex}
\[
  \vlambda_{t+1} = \mathrm{proj}\left( \vlambda_t - \gamma_t \rvvg \right), 
\]
}%
where \(\gamma_t\) is called the stepsize.
The use of the projection operator \(\mathrm{proj}\left(\cdot\right)\) forms a subset of the broad SGD framework called \textit{projected} SGD.
The convergence of BBVI with projected SGD has recently been established by \citet{domke_provable_2023}.

%\vspace{-1ex}
%\paragraph{Fisher Divergence}
In addition to the KL divergence, our analysis invokes the Fisher-Hyv\"arinen divergence~\citep{otto_generalization_2000,hyvarinen_estimation_2005}:
\begin{definition}[\textbf{Fisher-Hyv\"arinen Divergence}]
The \(p\)th order Fisher-Hyv\"arinen divergence between two distributions \(\pi\) and \(q\) is given as
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\[
  \mathrm{D}_{\mathrm{F}^p}\left(q, \pi\right)
  \triangleq 
  \mathbb{E}_{\rvvz \sim q} 
  \norm{\nabla \log \pi\left(\rvvz\right) - \nabla \log q\left(\rvvz\right) }_2^p.
\]
}%
\end{definition}
Here, we use the \(p\)th order generalization~\citep{huggins_practical_2018} of the original Fisher-Hyv\"arinen divergence. 
We denote the standard 2nd order Fisher-Hyv\"arinen divergence as \(\mathrm{D}_{\mathrm{F}}\left(q, \pi\right) \triangleq \mathrm{D}_{\mathrm{F}^2}\left(q, \pi\right)\).
This divergence was first defined by~\citet{otto_generalization_2000} (attributed by \citealp{zegers_fisher_2015}) as the ``relative Fisher information'' in the context of optimal transport.
It was later introduced to the machine learning community by \citet{hyvarinen_estimation_2005} for score matching.

\subsection{Variational Family}
Throughout this paper, we restrict our interest to the location-scale variational family, which has been successfully used by \citet{kim_practical_2023,kim_convergence_2023,domke_provable_2019,domke_provable_2020,domke_provable_2023, fujisawa_multilevel_2021, titsias_doubly_2014} for analyzing the properties of BBVI.
It encompasses many practical families such as the Gaussian, Student-t, and elliptical distributions.
In particular, the location-scale family is part of the broader reparameterized family:
%
\begin{definition}[\textbf{Reparameterized Family}]\label{def:family}
  Let \(\varphi\) be some \(d\)-variate distribution.
  Then, \(q_{\vlambda}\) that can be equivalently represented as
{%
\setlength{\belowdisplayskip}{-.5ex} \setlength{\belowdisplayshortskip}{-.5ex}
\setlength{\abovedisplayskip}{-.5ex} \setlength{\abovedisplayshortskip}{-.5ex}
  \begin{alignat*}{2}
    \rvvz \sim q_{\vlambda}  \quad\Leftrightarrow\quad &\rvvz \stackrel{d}{=} \mathcal{T}_{\vlambda}\left(\rvvu\right); \quad \rvvu \sim  \varphi,
  \end{alignat*}
  }%
  where \(\stackrel{d}{=}\) is equivalence in distribution, is said to be part of a reparameterized family generated by the base distribution \(\varphi\) and the reparameterization function \(\mathcal{T}_{\vlambda}\).
\end{definition}
%
Naturally, this means we focus on the \textit{reparameterization gradient estimator}~\citep{titsias_doubly_2014,kingma_autoencoding_2014,rezende_stochastic_2014}, often observed to achieve lower variance than alternatives~\citep{xu_variance_2019}.  
(See \citealt{mohamed_monte_2020} for a comprehensive overview.)
From this, we obtain the location-scale family through the following reparameterization function:
%
\begin{definition}[\textbf{Location-Scale Reparameterization Function}]\label{def:reparam}
  A mapping \(\mathcal{T}_{\vlambda} : \mathbb{R}^p \times \mathbb{R}^d \rightarrow \mathbb{R}^d\) defined as
{
\setlength{\belowdisplayskip}{.5ex} \setlength{\belowdisplayshortskip}{.5ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \begin{align*}
    &\mathcal{T}_{\vlambda}\left(\vu\right) \triangleq \mC \vu + \vm
  \end{align*}
}%
  with \(\vlambda \in \mathbb{R}^p\) containing the parameters for forming the location \(\vm \in \mathbb{R}^d\) and scale \(\mC \in \mathbb{R}^{d \times d}\) is called the location-scale reparameterization function.
\end{definition}
%
For the scale matrix \(\mC\), various parameterizations are used in practice, as shown by \citet[Table 1]{kim_practical_2023}.
We discuss our parameterization of choice in \cref{section:scale_parameterization}.

The choice for the base distribution \(\varphi\) completes the specifics of the variational family.
For example, choosing \(\varphi\) to be a univariate Gaussian result in the Gaussian variational family.
We impose the following general assumptions on the base distribution:
\begin{assumption}[\textbf{Base Distribution}]\label{assumption:symmetric_standard}
  \(\varphi\) is a \(d\)-dimensional distribution such that \(\rvvu \sim \varphi\) and \(\rvvu = \left(\rvu_1, \ldots, \rvu_d \right)\) with indepedently and identically distributed components.
  Furthermore, \(\varphi\) is
  \begin{enumerate*}[label=\textbf{(\roman*)}]
      \item symmetric and standardized such that \(\mathbb{E}\rvu_i = 0\), \(\mathbb{E}\rvu_i^2 = 1\), \(\mathbb{E}\rvu_i^3 = 0\), and 
      \item has finite kurtosis \(\mathbb{E}\rvu_i^4 = \kappa < \infty\).
  \end{enumerate*}
\end{assumption}

Overall, the assumptions on the variational family are collected as follows:
\vspace{0.5ex}
\begin{assumption}\label{assumption:variation_family}
  The variational family is the location-scale family formed by \cref{def:family,def:reparam} with the base distribution \(\varphi\) satisfying \cref{assumption:symmetric_standard}.
\end{assumption}

\vspace{-1ex}
\subsection{Scale Parameterization}\label{section:scale_parameterization}
\vspace{-1ex}
For the scale parameterization \(\vlambda \mapsto \mC\), in principle, any choice that results in a positive-definite covariance matrix is valid.
However, recently, \citet{kim_convergence_2023} have shown that a seemingly innocent choice of parameterization can have a massive impact on computational performance.
For example, nonlinear parameterizations can easily break the strong convexity of the ELBO~\citep{kim_convergence_2023}, which could have been otherwise obtained~\citep{domke_provable_2020}.
Therefore, the scale parameterization is subject to the constraints:
\vspace{-1ex}
\begin{enumerate}[label=\textbf{(\roman*)}]
    \setlength\itemsep{0ex}
  \item \textbf{Positive Definiteness}: \(\mC\mC^{\top} \succ 0\).  \\
    This is needed to ensure that \(\mC\mC^{\top}\) forms a valid covariance in \(\mathbb{S}_{++}^d\).
    \label{item:positive_definite}
    %
  \item \textbf{Linearity}: \(\norm{\vlambda - \vlambda'}_2^2 = \norm{\vm - \vm'}_2^2 + \norm{ \mC - \mC'}_{\mathrm{F}}^2 \).  \\
    As shown by \citet{kim_convergence_2023}, this constraint is sufficient to form a \(\mu\)-strongly convex ELBO from a \(\mu\)-strongly log-concave posterior.
    \label{item:linearity}
    
  \item \textbf{Convexity}: The mapping \(\vlambda \mapsto \mC\mC^{\top}\) is convex on \(\Lambda_{S}\).\\
    This is needed to ensure that the ELBO is convex whenever the target posterior is log-concave \citep{domke_provable_2023,kim_convergence_2023}.
    \label{item:convexity}
    
  \item \textbf{Smooth Log-Determinant}: \(\vlambda \mapsto \log \operatorname{det} \mC\) is Lipschitz smooth on \(\Lambda_{S}\). \label{item:smooth_entropy}
  
  This condition is only required by projected SGD so that the ELBO is Lipschitz smooth on \(\Lambda_S\).
  
\end{enumerate}
\vspace{-1ex}
\citet{domke_provable_2020,domke_provable_2023} guaranteed \labelcref{item:smooth_entropy} by setting the domain of \(\vlambda\) to be
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\begin{align*}
  \big\{\, \left(\vm, \mC\right) \in \mathbb{R}^{d} \times \mathbb{S}_{++}^d \mid \mC \mC^{\top} \succeq 
   S^{-1} \boldupright{I} \,\big\},
\end{align*}
}%
with \(S = L\), where \(L\) is the log-smoothness constant of the posterior.
That is, \(\mC\) is chosen to be a proper matrix square root of the covariance \(\mSigma = \mC \mC^{\top}\) such that \(\mC = \mC^{\top} = \mSigma^{\nicefrac{1}{2}}\).
This parameterization ensures that a proper projection operator exists onto \(\Lambda_S\), where they proposed to use the singular value decomposition.
This projection operator is quite costly as it imposes a \(\mathcal{O}(d^3)\) complexity.
We will later propose a different parameterization based on triangular matrices, where the projection operator only costs \(\Theta(d)\) while obtaining the same convergence guarantees. 

\subsection{Gradient Estimators}\label{section:gradient_estimators}
The gradient estimators considered in this work are the closed-form entropy (CFE;~\citealp{kucukelbir_automatic_2017,titsias_doubly_2014}) and sticking the landing (STL;~\citealp{roeder_sticking_2017}) estimators.

\vspace{-1ex}
\paragraph{Closed-From Entropy Estimator}
The CFE estimator is the ``standard'' estimator used for BBVI.
%
\begin{definition}[\textbf{Closed-Form Entropy Estimator}]\label{def:cfe}
The closed-form entropy gradient estimator is
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.5ex} \setlength{\abovedisplayshortskip}{1.5ex}
\[
  \vg\left(\vlambda\right) 
  \triangleq
  \nabla_{\vlambda} \log \ell \left(\mathcal{T}_{\vlambda}\left(\vu\right)\right) + \nabla_{\vlambda} \mathbb{H}\left(q_{\vlambda}\right),
\]
}%
where the gradient of the entropy term is computed deterministically.
\end{definition}
%
It can be used whenever the entropy \(\mathbb{H}\left(q_{\vlambda}\right)\) is available in a closed form.
For location-scale families, this is always the case up to an additive constant.

\vspace{-1ex}
\paragraph{Sticking-the-Landing Estimator}
On the other hand, the STL estimator estimates the entropy through a
 special Monte Carlo strategy:
%
\begin{definition}[\textbf{Sticking-the-Landing Estimator; STL}]\label{def:stl}
The sticking-the-landing gradient estimator 
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
  \rvvg_{\mathrm{STL}}\left(\vlambda\right) 
  \triangleq
  \nabla_{\vlambda} \log \ell \left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) - \nabla_{\vlambda} \log q_{\vnu}\left(\mathcal{T}_{\vlambda}\left(\rvvu\right)\right) \Big\lvert_{\vnu = \vlambda}
\]
}%
is given by stopping the gradient from propagating through \(\log q_{\vlambda}\).
\end{definition}
%
Notice that, the gradient of \(\log q\) is ``stopped'' by the assignment \(\vnu = \vlambda\).
This creates a control variate effect, where the control variate \(\mathrm{cv}\) is implicitly formed as
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
  \mathrm{cv}\left(\vlambda; \vu\right)
  =
  \nabla_{\vlambda} \mathbb{H}\left(\vlambda\right)
  +
  \nabla_{\vlambda} \log q_{\vnu}\left(\mathcal{T}_{\vlambda}\left(\vu\right)\right) \Big\lvert_{\vnu = \vlambda}.
\]
}%
Subtracting this to the CFE estimator leads to the STL estimator.

\subsection{Quadratic Variance Condition}
The convergence of BBVI has recently been established concurrently by \citet{kim_convergence_2023,domke_provable_2023}.
However, the analysis of \citeauthor{domke_provable_2023} presents a broadly applicable framework based on the \textit{quadratic variance} (QV) condition.
%
\begin{definition}[\textbf{Quadratic Variance; QV}]
  A gradient estimator \(\rvvg\) is said to satisfy the quadratic variance condition if the following bound holds:
{%
\setlength{\belowdisplayskip}{1.ex} \setlength{\belowdisplayshortskip}{1.ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
  \[
    \mathbb{E}\norm{\rvvg\left(\vlambda\right)}_2^2 \leq \alpha \norm{\vlambda - \vlambda^*}_2^2 + \beta,
  \] 
  }%
  for any \(\vlambda \in \Lambda_S\) and some \(0 \leq \alpha, \beta < \infty\), where \(\vlambda^*\) is a stationary point.
\end{definition}
%
This basically assumes that the gradient variance grows no more than a quadratic plus a constant.
For non-asymptotic analysis of SGD, this bound was first used by \citet{moulines_nonasymptotic_2011} as an intermediate step implied by the condition:
{%
\setlength{\belowdisplayskip}{1.5ex} \setlength{\belowdisplayshortskip}{1.5ex}
\setlength{\abovedisplayskip}{1.ex} \setlength{\abovedisplayshortskip}{1.ex}
\[
    \mathbb{E}\norm{ \rvvg\left(\vlambda\right) - \rvvg\left(\vlambda'\right) }_2^2 \leq \mathcal{L} \norm{\vlambda - \vlambda'}_2^2,
\]
}%
for all \(\vlambda, \vlambda' \in \Lambda\) and some \(0 < \mathcal{L} < \infty\).
This, combined with the assumption \(\mathbb{E}\norm{\rvvg\left(\vlambda^*\right)}_2^2 < \infty\), implies the QV condition.
They used this strategy to prove the convergence of SGD on strongly convex functions.
Later on,~\citet[p. 85]{wright_optimization_2021} directly assumed the QV condition to obtain similar results.
A comprehensive convergence analysis of projected and proximal SGD with the QV condition was conducted by \citet{domke_provable_2023}, where they also prove the convergence on general convex functions.
This work will invoke the analysis of \citeauthor{domke_provable_2023} by establishing the QV condition of the considered gradient estimators.

\subsection{Interpolation Condition}
To establish the linear, or more intuitively ``exponential,'' convergence of SGD, \citet{schmidt_fast_2013} have relied on the interpolation condition:
\begin{definition}[\textbf{Interpolation}]
  A gradient estimator \(\rvvg\) is said to satisfy the interpolation condition if  \(\mathbb{E}\norm{\rvvg\left(\vlambda^*\right)}^2 = 0\) for \(\vlambda^* \in \Lambda\) such that \(\norm{\nabla F\left(\vlambda^*\right)} = 0\).
\end{definition}
This assumes that the gradient variance vanishes at a stationary point, gradually retrieving the convergence behavior of deterministic gradient descent.
For the QV condition, this corresponds to \(\beta = 0\).

\vspace{-1ex}
\paragraph{Achieving ``Interpolation''}
Currently, there are two ways where the interpolation condition can be achieved.
The first case is when interpolation is achieved \textit{naturally}.
That is, in ERM, when the model is so overparameterized that certain parameters can ``interpolate'' all of the data points in the train data~\citep{ma_power_2018,vaswani_fast_2019}, the gradient becomes 0.
Otherwise, a control variate approach such as stochastic average gradient (SAG; \citealp{schmidt_minimizing_2017}) or stochastic variance-reduced gradient (SVRG; \citealp{johnson_accelerating_2013}), and their many variants~\citep{gower_variancereduced_2020} can be used.

\vspace{-1ex}
\paragraph{Does STL ``Interpolate?''}
As we previously discussed, the STL estimator is essentially a control variate method.
Thus, an important question is whether it can achieve the same effect, notably linear convergence, as variance-reduced SGD methods. 
While \citet{roeder_sticking_2017} have already shown that the STL estimator achieves interpolation when \(q_{\vlambda^*} = \pi\), our research question is whether this fact can be rigorously used to establish linear convergence of SGD.
