

\section{INTRODUCTION}
Despite the massive success of black-box variational inference (BBVI; \citealp{ranganath_black_2014,titsias_doubly_2014,kucukelbir_automatic_2017}), our understanding of its computational properties has only recently started to make progress \citep{domke_provable_2019,hoffman_blackbox_2020,domke_provable_2020,domke_provable_2023,kim_convergence_2023,kim_practical_2023}. 
Notably, \citet{domke_provable_2023,kim_convergence_2023} have independently established the convergence of ``full'' BBVI.
This is a significant advance from the previous results where simplified versions of BBVI were analyzed \citep{hoffman_blackbox_2020,bhatia_statistical_2022} and results that \textit{a-priori} assumed regularity of the ELBO~\citep{alquier_concentration_2020,cherief-abdellatif_generalization_2019,regier_fast_2017,khan_faster_2016,khan_kullbackleibler_2015,fujisawa_multilevel_2021,buchholz_quasimonte_2018,liu_quasimonte_2021}.
We now have rigorous convergence guarantees that, for certain well-behaved posteriors, BBVI achieves a convergence rate of \(\mathcal{O}\left(1/T\right)\), corresponding to a computational complexity of \(\mathcal{O}\left(1/\epsilon\right)\)~\citep{domke_provable_2023,kim_convergence_2023}.
A remaining theoretical question is whether BBVI can achieve better rates, in particular geometric convergence rates, which is traditionally called ``linear'' convergence in the optimization literature (see the textbook by \citealt[\S 1.2.3]{nesterov_introductory_2004}), corresponding to a complexity of \(\mathcal{O}\left(\log\left( 1/\epsilon \right) \right)\).

For stochastic gradient descent (SGD; \citealp{robbins_stochastic_1951,bottou_online_1999,nemirovski_robust_2009}), it is known that improving the \(\mathcal{O}\left(1/T\right)\) convergence rate is challenging~\citep{rakhlin_making_2012,harvey_tight_2019}.
This is because, once in the stationary regime, it is necessary to either decrease the stepsize or average the iterates, where the latter reduces SGD to Markov chain Monte Carlo~\citep{dieuleveut_bridging_2020}.
Not surprisingly, both cases result in a significant slowdown compared to deterministic gradient descent.
Overall, SGD is known to achieve \(\mathcal{O}\left(1/\sqrt{T}\right)\) for general convex functions and \(\mathcal{O}\left(1/T\right)\) for strongly convex functions (\citealp{nemirovski_robust_2009}; for more modern analysis techniques, see~\citealp{garrigos_handbook_2023,gower_sgd_2019}).

Meanwhile, under a condition known as ``interpolation,'' which assumes that the gradient variance becomes zero at the optimum, SGD is known to achieve a linear convergence rate~\citep{schmidt_fast_2013}.
This can automatically hold for certain problems, such as empirical risk minimization (ERM) with overparameterized models, explaining the fast empirical convergence of modern machine learning models~\citep{vaswani_fast_2019, ma_power_2018}.
Also, control variate methods such as ``variance-reduced'' gradients \citep{schmidt_minimizing_2017,johnson_accelerating_2013,gower_variancereduced_2020} algorithmically achieve the same effect and have been successful both in theory and practice.
Unfortunately, variance-reduced gradient methods are strictly restricted to the finite-sum setting, which BBVI is not part of (See \S 2.4 by \citealp{kim_convergence_2023}).
Thus, it is yet unclear how BBVI could benefit from the advances in variance-reduced gradients.

Fortunately, other types of control variates have been actively pursued in BBVI \citep{geffner_using_2018,miller_reducing_2017, ranganath_black_2014, paisley_variational_2012, geffner_approximation_2020,wang_joint_2024}. In particular, the \textit{sticking-the-landing} (STL; \citealp{roeder_sticking_2017}) estimator satisfies the interpolation condition (e.g., achieves zero gradient variance at the optimum) when the variational family $\mathcal{Q}$ contains the true posterior $\pi$~\footnote{Although the term interpolation does not literally make sense outside of the ERM context, we will stick to this term to stay in line with the SGD literature.}. It is thus natural to ask whether existing control variate approaches such as STL are sufficient to achieve linear convergence under realizable conditions.
In fact, this possibility has been mentioned by~\citet[\S 5]{hoffman_blackbox_2020}.

In this work, we confirm these previous comments by establishing a linear convergence rate of BBVI with STL when the variational family contains the true posterior--\textit{i.e.}, is perfectly specified.
For a \(d\)-dimensional strongly log-concave posterior with a condition number of \(\kappa\) and a location-scale variational family with a full rank scale, BBVI with the STL estimator finds \(\epsilon\)-accurate variational parameters at a rate of \(\mathcal{O}\left(d \kappa^2 \log\left(1/\epsilon\right) \right)\). Even beyond the perfectly specified setting, our theoretical results characterize the behavior of the STL estimator in the misspecified setting, which is closer to practical usage.
This provides some intuition as why the comparisons between the STL and the ``standard'' closed-form entropy (CFE;~\citealp{titsias_doubly_2014,kucukelbir_automatic_2017}) estimators appear mixed in practice~\citep{geffner_rule_2020,agrawal_advances_2020}.

While our results are built on top of the theoretical framework of \citet{domke_provable_2023}, a similar convergence result on the STL estimator appeared in a later, recent preprint version \citep{domke_provable_2023a} concurrently with this work.
The details of the result differ, and we provide additional contributions specific to the STL estimator.
We discuss the differences in more detail in \cref{section:related} along with other related works.

\vspace{-1ex}
\paragraph{Contributions} Our contributions are summarized in the following list.
An overview of the theorems is provided in \cref{table:theorems} of \cref{section:overviewtheorems}.
We also provide an overview of previous rigorous complexity analyses on BBVI in \cref{table:relatedworks} of \cref{section:related}.
\begin{enumerate}[label=\ding{228}]
  \vspace{-1ex}
    \setlength\itemsep{1ex}
    \item \textbf{We prove that BBVI with the STL estimators can converge at a linear rate.} \\
      When the variational family is perfectly specified such that the posterior is contained in the variational family, \cref{thm:projsgd_bbvistl_complexity} establishes this through \cref{thm:stl_upperbound}.
      This is the first result for ``full'' BBVI without algorithmic simplification. 
      
    \item \textbf{Our analysis encompasses variational family misspecification.}
      When the variational family is misspecified, the Fisher-Hyv\"arinen divergence between the variational posterior and the true posterior captures the behavior of STL.
      
    \item \textbf{We establish a matching lower bound on the gradient variance.}
      Our upper bound in \cref{thm:stl_upperbound} and the concurrent result by \citet{domke_provable_2023a} are proven to be tight by a constant factor through \cref{thm:stl_lowerbound_unimprovability}.
    
    \item \textbf{We improve previously obtained gradient variance bounds for the CFE estimator.} \\
      In \cref{thm:cfe_upperbound}, we tighten the constants of the bounds previously obtained by \citet{domke_provable_2023}.
      This makes the theoretical results for the CFE and STL estimators comparable.
      
    \item \textbf{We provide a parameterization with a projection operator with \(\Theta(d)\) complexity.} \\
      In \cref{section:triangular_scale}, we propose a triangular scale parameterization with a corresponding projection operator that can be computed in \(\Theta(d)\) time.
      This improves over the matrix square-root parameterization used by \citet{domke_provable_2023}, which involved a \(\mathcal{O}(d^3)\) projection operator based on the singular value decomposition.
      
    \item \textbf{We prove precise quantitative complexity guarantees for SGD with QV gradient estimators.}
      We obtain quantatitive non-asymptotic complexity guarantees from the ``anytime convergence'' results of \citet{domke_provable_2023}.
\end{enumerate}

