

\prAtEndRestatexvii*

\makeatletter\Hy@SaveLastskip\label{proofsection:prAtEndxvii}\ifdefined\pratend@current@sectionlike@label\immediate\write\@auxout{\string\gdef\string\pratend@section@for@proofxvii{\pratend@current@sectionlike@label}}\fi\Hy@RestoreLastskip\makeatother\begin{proof}[Proof]\phantomsection\label{proof:prAtEndxvii}\label {proof:projsgd_bbvicfe_complexity} From \cref {thm:cfe_upperbound} with \(S = L\), the CFE estimator satisfies adaptive QV with the constants \begin {alignat*}{2} \alpha _{\mathrm {CFE}} = L^2 \left ( d + k_{\varphi } + 4 \right ) \left (1 + \delta \right ) \qquad \text {and}\qquad \beta _{\mathrm {CFE}} = L^2 \left ( d + k_{\varphi } \right ) \left ( 1 + \delta ^{-1} \right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2. \end {alignat*} Furthermore, for a \(\mu \)-strongly log-concave posterior and our variational parameterization,~\citet [Theorem 9]{domke_provable_2020} show that the ELBO is \(\mu \)-strongly convex. \par We can thus invoke \cref {thm:projsgd_stronglyconvex_adaptive_complexity} with \begin {align*} \widetilde {\alpha } = L^2 \left (d + k_{\varphi } + 4\right ),\qquad \widetilde {\beta } = L^2 \left (d + k_{\varphi }\right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2,\quad \text {and}\quad C = 1. \end {align*} This yields a lower bound on the number of iteration \begin {align*} &\frac {2}{\mu ^2} \max \left (\widetilde {\alpha } + 2 \widetilde {\beta } \frac {1}{\epsilon }, \; \frac {\mu ^2}{4} \right ) \log \left ( 2 \norm {\vlambda _0 - \vlambda ^*}_2^2 \, \frac {1}{\epsilon } \right ) \\ &\;= \frac {2}{\mu ^2} \max \left (L^2 \left (d + k_{\varphi } + 4\right ) + 2 L^2 \left (d + k_{\varphi }\right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 \frac {1}{\epsilon },\; \frac {\mu ^2}{4}\right ) \log \left ( 2 {\lVert \vlambda _0 - \vlambda ^*\rVert }_2^2 \, \frac {1}{\epsilon } \right ), \shortintertext {pulling out \(L\),} &\;= \frac {2 L^2}{\mu ^2} \max \left ( \left (d + k_{\varphi } + 4\right ) + 2 \left (d + k_{\varphi }\right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 \frac {1}{\epsilon },\; \frac {\mu ^2}{4 L^2}\right ) \log \left ( 2 {\lVert \vlambda _0 - \vlambda ^*\rVert }_2^2 \, \frac {1}{\epsilon } \right ), \shortintertext {and since \(\frac {\mu ^2}{4 L^2} < \frac {1}{4}\) and the first argument is larger than 1, the max operation is redundant that} &\;= \frac {2 L^2}{\mu ^2} \left ( \left (d + k_{\varphi } + 4\right ) + 2 \left (d + k_{\varphi }\right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 \frac {1}{\epsilon }\right ) \log \left ( 2 {\lVert \vlambda _0 - \vlambda ^*\rVert }_2^2 \, \frac {1}{\epsilon } \right ). \shortintertext {Now, using the trivial fact \(d + k_{\varphi } < d + k_{\varphi } + 4\) simplifies the bound as,} &\;< \frac {2 L^2}{\mu ^2} \left (d + k_{\varphi } + 4\right ) \left (1 + 2 {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 \frac {1}{\epsilon } \right ) \log \left ( 2 {\lVert \vlambda _0 - \vlambda ^*\rVert }_2^2 \, \frac {1}{\epsilon } \right ) \\ &\;= 2 \kappa ^2 \left (d + k_{\varphi } + 4\right ) \left (1 + 2 {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 \frac {1}{\epsilon }\right ) \log \left ( 2 {\lVert \vlambda _0 - \vlambda ^*\rVert }_2^2 \, \frac {1}{\epsilon } \right ). \end {align*} The optimal \(\delta \) is given as \begin {align*} \delta = \frac {2}{\epsilon } \frac {\widetilde {\beta }}{\widetilde {\alpha }} C^{-1} = \frac {2}{\epsilon } \frac { L^2 \left (d + k_{\varphi }\right ) {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 }{ L^2 \left (d + k_{\varphi } + 4\right ) } C^{-1} = \frac {2}{\epsilon } \, \frac {d + k_{\varphi }}{d + k_{\varphi } + 4} \, {\lVert \bar {\vlambda } - \vlambda ^* \rVert }_2^2 . \end {align*}\end{proof}
