
\begin{theoremEnd}[category=complexitybbvistl]{theorem}[\textbf{Complexity of Fixed Stepsize BBVI with STL}]\label{thm:projsgd_bbvistl_decstepsize_complexity_logconcave}
  For some weighted iterate averaging scheme, the last average \(\bar{\vlambda}_{T}\) of BBVI with the STL estimator and projected SGD with a fixed stepsize schedule applied to a log-concave and \(L\)-log-smooth posterior under perfect variational family specification satisfy \(F\left(\bar{\vlambda}_T\right) - F\left(\vlambda^*\right) \leq \epsilon\), where \(\vlambda^* \in \argmin_{\vlambda \in \Lambda_L} F\left(\vlambda\right) \) if
  \begin{align*}
    T
    \geq
    \sqrt{2 \left(d + k_{\varphi}\right)} \, L 
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    \frac{1}{\epsilon},
  \end{align*}
  for some fixed stepsize \(\gamma\).
\end{theoremEnd}
\begin{proofEnd}
  As shown by \cref{thm:stl_upperbound}, the STL estimator satisfies an adative QV bound with the constants
  \begin{align*}
    \alpha_{\mathrm{STL}} &= 2 \left(d + k_{\varphi}\right) \left(2 + \delta\right)  L^2 \\
    \beta_{\mathrm{STL}}  &= \sqrt{3} \left(d + k_{\varphi}\right) \left(1 + 2 \delta^{-1}\right)  \sqrt{\mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right)}.
  \end{align*}
  Also, when the variational family is perfectly specified, the variational posterior \(q_{\vlambda^*}\) is equal to \(\pi\) such that 
  \[
  \mathrm{D}_{\mathrm{F}^4}\left(q_{\vlambda^*}, \pi\right) = 0.
  \]
  Then, we immediately have \(\beta_{\text{STL}} = 0\).
  Therefore, the optimal \(\delta\) is \(\delta = 0\), resulting in 
  \begin{align*}
    \alpha_{\mathrm{STL}} &= 4 \left(d + k_{\varphi}\right) L^2 \\
    \beta_{\mathrm{STL}}  &= 0.
  \end{align*}
  Furthermore, for a log-concave posterior and our variational parameterization,~\citet[Theorem 9]{domke_provable_2020} show that the ELBO is convex.

  Thus, we can invoke \cref{thm:projsgd_convex_fixedstepsize}, for which we have a lower bound on the required number of iteration:
  \begin{align*}
    \frac{2 \alpha}{ -\beta + \sqrt{ \beta^2 + 8 \epsilon^2 \alpha } }
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    &=
    \frac{2 \alpha}{ \sqrt{ 8 \epsilon^2 \alpha } }
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    \\
    &=
    \frac{\sqrt{\alpha}}{\sqrt{2}}
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    \frac{1}{\epsilon}
    \\
    &=
    \frac{\sqrt{4 \left(d + k_{\varphi}\right) L^2}}{\sqrt{2}}
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    \frac{1}{\epsilon}
    \\
    &=
    \sqrt{2 \left(d + k_{\varphi}\right)} \, L 
    \, \norm{\vlambda_0 - \vlambda^*}_2^2
    \frac{1}{\epsilon}.
  \end{align*}
\end{proofEnd}
