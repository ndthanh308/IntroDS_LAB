\section{Method}

% \subsection{Overview}
In this section, we present an efficient Depth Completion network for Transparent Objects (FDCT), as illustrated in Figure \ref{fig:figure2}. The network is designed to deliver real-time predictions of rectified depth maps from RGB-D cameras.
% , even on low-power platforms.
We begin by giving an overview of the proposed network, followed by the introduction of the compact design of the encoder and decoder components. Next, we demonstrate the effective utilization of low-level features through the use of our fusion branch and cross-layer shortcut. Finally, we detail the loss functions that are employed during the training of the network.

% Figure environment removed
\vspace{-0.4cm}
\subsection{Overview of Network}
We follow the common practice in \cite{fang2022transcg} to build our network similar to UNet \cite{ronneberger2015u}. The network can predict rectified depth directly from RGB and depth images. The transparent mask is involved only in the training of the network.
 As shown in \fref{fig:figure2}, our network contains an encoder and decoder with skip connection added and a fusion branch serving as a sub-encoder. 
However, the original skip connection in UNet only transmit features from encoder to decoder. As is known,  low-level features contains sufficient position information, which plays an important role in depth completion task \cite{zuo2016explicit,tao2021dilated}. In observing that deeper layer contains less positional information \cite{olah2017feature,peng2021dgfau}, we additionally add shortcuts to every layer (shown as orange connection in Figure \ref{fig:figure2}) to make low-level features available for deeper layers, preserving positional information and leading to more accurate results.  

The encoder of FDCT consists of two parts: the main part is built with Feature Fusion and Extraction Blocks (FFEB), while the subpart is called Fusion Branch and is built with a series of Shortcut Fusion Modules (SFM).  
% SFM integrates features from the encoder layer by layer, preserving the representative features for subsequent decoding and ensuring the integration of multi-layer and multi-scale features. 
The main and sub encoders work together to provide aggregated features to the decoder, preserving low-level information such as pixel position.
Besides, the decoder is constructed with Depth Fusion and Completion Blocks (DFCB).
% , while avoiding harm to high-level information through the use of a new loss function and max pooling-based downsampling method.
In our work, the original depth image is efficiently and accurately utilized by providing it to every FFEB and DFCB through a domain adaptation operation. This is a departure from previous works, which either used a network to predict a transparent mask based on raw depth information \cite{sajjan2020clear,tang2021depthgrasp} (leading to time-consuming two-stage processing) or directly concatenated raw depth to features \cite{fang2022transcg} (leading to sensitivity to noise and inaccuracies).

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% To be efficient, we design FDCT as a four-layered U-Net-based \cite{ronneberger2015u} image-to-image network.
% Shortcuts are added to every layer in both encoder and decoder as shown with the orange connections in \fref{fig:figure2}.  
% Position information plays an important role in depth completion task to predict depth value pixel by pixel. However, compared with the shallower layers, deeper layers usually contain less positional information to reconstruct the image at the same resolution \cite{olah2017feature,peng2021dgfau}.
% Adding shortcuts can make low-level features available for deeper layers \cite{zhou2019unet++}. Therefore, we add shortcuts from low to high resolution in decoder to take more account of positional information when recovering pixel depth values and achieve more accurate results.

% The encoder of {FDCT} consists of two parts, the main part is built with Feature Fusion and Extracting Blocks (FFEB), the sub part is called Fusion Branch and built by series of shortcut fusion modules (SFM). The decoder of FDCT is built with Depth Fusion and Completion Blocks (DFCB). To preserve the representative features of different layers for subsequent decoding, SFM integrates features from encoder layer by layer and implements the fusion of multi-layer and multi-scale features. Main and sub encoders are working together to provide aggregated features to decoder, and it ensures that the architecture can keep abundant low-level features such as position information of each pixel. Meanwhile, to avoid it being harmful for the learning of high-level information, we use a new loss function and max pooling based downsampling method.

% Original depth image is usually used in two ways in previous works. One way is using a network to predict a transparent mask based on the raw depth information and setting the value within it to zero for the downstream processing\cite{sajjan2020clear,tang2021depthgrasp}, then the two-stage processing could be time-consuming. The other way is straightly concatenating the raw depth to features\cite{fang2022transcg}, which could be sensitivity to the noise and often lead to additional inaccuracy. In our work, to make full use of raw depth information efficiently and accurately, the original depth image is provided to every FFEB and DFCB with a domain adaption operation.

% Figure environment removed

\subsection{Lightweight Encoder and Decoder}
\subsubsection{\bf Feature Fusion and Extraction Block}
% \textcolor{red}{Emphasize the effectiveness and light-weight desgin of our blocks. Give conmparision with dense block.}
% \paragraph{Feature Fusion and Extracting Block}
The backbone plays a crucial role in determining the efficiency of an architecture. Although DenseNet \cite{huang2017densely} has been proven to be effective in previous work \cite{fang2022transcg}, it can be resource-intensive and its shallow layers may not contribute much to the results \cite{lee2019energy}, making it unsuitable for depth completion tasks. To address these issues, we adopt the One-Shot Aggregation module (OSA) \cite{lee2019energy} as the backbone  to effectively extract features.
The Encoder consists of four Feature Fusion and Extraction Blocks (FFEBs), depicted in the left part of Figure \ref{fig:figure3}. Each FFEB takes the original depth map, features from the previous block, and a shortcut from one layer apart as input. The features are concatenated and processed through a data-level fusion. Specifically, we use a convolution operation to force them fusing in a lower dimensional space. This fusion method can reduce the domain gap between deep features and raw depth image \cite{csurka2017comprehensive}. We will further show that it can work better than direct concatenation in Section \ref{sec:analysis}.

% It is a method recommended for raw data fusion\cite{khaleghi2013multisensor} and can reduce the domain gap while fusing them with depth information.


% The features are then concatenated and processed through a convolution operation, which reduces the domain gap while fusing them with depth information. We perform data-level fusion by fusing data in a lower dimensional space. This is a method recommended for raw data\cite{khaleghi2013multisensor}.

% This has been shown to be more effective than concatenating the raw depth directly{\color{blue}\cite{csurka2017comprehensive}}.

Downsampling is performed through max pooling, which not only speeds up the network but also enhances its robustness. To show the effectiveness of the simple max pooling, please refer to our experiments for a thorough analysis of different downsampling methods.
% Backbone is important to the efficiency of the architecture. DenseNet\cite{huang2017densely} is used in the SOTA model DFNet and proved to perform well\cite{fang2022transcg}, however, it is memory and time consuming, and its shallow layers contributes less to the result\cite{lee2019energy}, which might be harmful to depth completion task. Therefore, We use One-Shot Aggregation module (OSA)\cite{lee2019energy} as the backbone of the Encoder for extracting features efficiently. 

% Encoder is formulated by four Feature Fusion and Extracting Blocks (FFEB, the left part in Figure \ref{fig:figure3}). Each FFEB takes the original depth, features from previous block and shortcut from one layer apart as input. Features are concatenated together and subjected to a convolution operation to reduce the domain gap while fusing them with depth information. It is proved to be better than concatenating raw depth directly. 
% Downsampling is achieved by max pooling, which can accelerate network and enable more robust performance. We make comparative analysis of the downsampling methods in ablation study.
% \hspace*{\fill}
\subsubsection{\bf Depth Fusion and Completion Block}
The Decoder is comprised of four Depth Fusion and Completion Blocks (DFCBs), as depicted in the right part of Figure \ref{fig:figure3}. The architecture and the inputs of the DFCBs are similar to those of the FFEBs. A residual connection is established between the corresponding DFCB and FFEB (as indicated by the "FFEB residual" in Figure \ref{fig:figure3}). To restore the resolution, we employ an accurate and efficient technique, pixel shuffle \cite{shi2016real}, which is used as upsampling to reduce computational complexity while taking channel information into consideration.
% Pixel shuffle do upsampling by sub-pixel convolution, reduce computational complexity and take channels into consideration.

% \textcolor{blue}{why efficient and why accurate?}

% Decoder is formulated by four Depth Fusion and Completion Blocks (DFCB, the right part in Figure \ref{fig:figure3}). The architecture and inputs of DFCB are similar to those of FFEB. The residual connection from corresponding DFCB to FFEB is preserved (FFEB residual in Figure \ref{fig:figure3}). We use pixel shuffle\cite{shi2016real}, an efficient and accurate super resolution method, to recover the resolution and channels.

\subsection{Fusion Branch and Cross-layer Shortcuts}
We introduce a novel approach for combining multi-scale and multi-layer features in our model, depicted in Figure \ref{fig:figure4}. Our approach involves the use of Cross-layer Shortcuts and a Fusion Branch, where the Fusion Branch collects the outputs of each encoder layer and integrates them layer by layer using a series of Shortcut Fusion modules (SFMs). Essentially, the Fusion Branch can be thought of as a sub-encoder that contributes to the final output of our model.

% We proposed new Cross-layer shortcut and Fusion Branch to integrate multi-scale and multi-layer features (Figure \ref{fig:figure4}). Fusion branch takes the output of each encoder layer and fuses them layer by layer. It is built by a series of Shortcut Fusion modules (SFM), and can be regarded as a sub-encoder.
%Shortcut Fusion Module (SFM)
% Figure environment removed

% The Fusion Branch is designed to ensure that low-level features remain accessible throughout the decoding process. 
% Many low-level features of the original depth map, such as original pixel values (depth values), edge positioning, etc., are important for depth completion tasks. Adding skip connections allows these features to be transferred to the decoder more directly \cite{hou2021divide}.

% Unlike segmentation, which emphasizes semantic information for class prediction, depth completion focuses on object boundaries and depth values to fill in regions. 
The purpose of the Fusion Branch is to maintain accessibility to low-level features during the coding process. In depth completion tasks, low-level features of the original depth map, such as original pixel values and edge positioning, play a crucial role. 
%The value of some pixels is actually the value that should be predicted, and most of the edge is helpful for rebuild the depth. 
By incorporating skip connections, these features can be directly transferred to the decoder for more effective processing \cite{hou2021divide}.
Therefore, rather than extracting deep semantic information, the Fusion Branch acts like a selector by aggregating features from all levels and exchanging information channel-wise. With this simple structure, it preserves low-level features while fusing multi-level features.


\subsection{Loss Function}
\label{section:Loss}


% In summary, the Fusion Branch and shortcuts provide sufficient low-level features to the network while avoiding overfitting through a carefully designed loss function.
% Fusion branch is designed to keep the low-level features being available to decoding process. Compared to segmentation, where semantic information is important for predicting class, depth completion focus on the boundary of an object and the depth value to fill the region. Instead of deepening digging meaningful semantic information, fusion branch is more like a selector to find the most suitable information by aggregating features from all levels and exchanging information channels-wise. With simple structure, fusion branch preserves the low-level features and fuse multi-level features.

% In general, fusion branch together with shortcuts provide abundant low-level features to the network. But at the same time, too many local features may also lead to overfitting in the training process, affecting the performance of the model. Therefore, we design a loss function to avoid it and experiments show that it is beneficial for depth completion tasks.




% \textcolor{blue}{what is the commonly used loss? (mse+cosine distance of surface normals) What are their problems? Our benefits?}
Too many local features can result in overfitting during the training process, which can negatively impact the performance of the model. To address this, we develop a loss function to prevent overfitting. 
Moreover, having a well-defined optimization objective is critical for deep learning-based methods, while many recent approaches for depth completion neglect the careful design of a proper loss function. 
% Specifically, these methods rely on simple mean squared error (MSE) loss and cosine distance of surface normal (Smooth Loss) as the primary loss functions. 
Specifically, these methods mainly rely on mean squared error (MSE) loss as the primary loss functions. 
This lack of attention to the design of the loss function could potentially lead to suboptimal performance in depth completion tasks.

This observation motivates us to incorporate appropriate losses in model training. Specifically, we note that certain pixels, such as those along edges or affected by sensor noise, often have unstable depth values that would be over penalized by the commonly used mean squared error (MSE) loss. Additionally, using the raw depth image throughout the network with a low-level aware structure can lead to overfitting to these pixels, resulting in poor performance.

To address these issues, we introduce the following loss function that primarily utilizes the Huber Loss, with SSIM loss and smooth loss serving as penalty terms. We demonstrate the necessity of the proposed loss function in the analysis section. All losses are calculated on the transparent regions.
% In the analysis section, we conduct experiments where we modify the weight of these pixels and demonstrate the necessity of our proposed loss function.
% The underlying idea is that some pixels, such as those along edges or affected by sensor noise, often have unstable depth values. Commonly used MSE loss would impose an excessive penalty on these points. Moreover, using the raw depth image throughout the network with a low-level aware structure, overfitting to these pixels can lead to poor performance. To mitigate this issue, we construct a loss function that primarily uses the Huber Loss, with SSIM loss and smooth loss serving as penalty terms. Me make an experiment in the analysis part, modifying the weight of these pixels, and demonstrate The necessity of the proposed loss function.
% The intuition is that, there are usually some pixels whose depth values are unstable e.g., edge, sensor noise. When the raw depth image being used throughout the network with designed low-level awareness structure, overfitting to these pixels could lead to bad performance. To overcome it, we build a loss function with Huber Loss as the main loss, SSIM loss and smooth loss as the penalty terms.

% \noindent\textbf{Huber Loss.}
\hspace*{\fill}
\subsubsection{\bf Huber Loss}

Huber Loss, as described in \eqref{equation:L-huber}, is a well-established loss function in deep learning. Unlike MSE and MAE, Huber Loss is quadratic for small differences below a threshold $\delta$ and linear for larger values, making it less sensitive to outliers in the data. During the training process, $\delta$ is empirically set to a value of 0.1. 
% and will be discussed further in the Limitations section. 
Conceptually, $\delta$ can be viewed as a threshold of error tolerance, where differences below $\delta$ are considered ``acceptable''.
% \begin{equation}
%     L_{\rm Huber} = \left\{ 
%     \begin{matrix}
%         \frac{1}{2}\left( {D_{\rm p} - D_{\rm gt}} \right)^{2} & \left| {D_{\rm p} - D_{\rm gt}} \right| \leq \delta \\
%         \delta\left( {D_{\rm p} - D_{\rm gt}} \right) - \frac{\delta^{2}}{2} & \left| {D_{\rm p} - D_{\rm gt}} \right| > \delta
%     \end{matrix} \right.
%     \label{equation:L-huber}
% \end{equation}
\begin{equation}
    {L_{\rm \rm Huber}} = \begin{cases}
    {\frac{1}{2}( D_{\rm \rm p} - D_{\rm \rm gt})^{2}},&|D_{\rm p} - D_{\rm gt}| \leq \delta, \\
    \delta(D_{\rm p} - D_{\rm gt}) - \frac{\delta^{2}}{2},&|D_{\rm p} - D_{\rm gt}|>\delta,
    \label{equation:L-huber}
\end{cases}
\end{equation}
where, $D_{\rm p}$ and $D_{\rm gt}$ are the predicted and ground-truth depth.

% \noindent\textbf{SSIM Loss.}
\hspace*{\fill}
\vspace{-0.2cm}
\subsubsection{\bf SSIM Loss}
The SSIM (Structural Similarity Index Measure) is a method for assessing the quality of an image, first introduced in \cite{wang2004image}. Instead of computing loss on a pixel-by-pixel basis, SSIM calculates the loss on a global scale, focusing more on the overall structure and distribution of the image, rather than its individual details.
% SSIM (Structural Similarity Index Measure) is an image quality assessment first introduced in 2004\cite{wang2004image}. Rather than computing loss pixel-wise, SSIM computes loss in graph scale. SSIM can draw network's attention more to structure/distribution rather than detail.
\begin{equation}
    L_{\rm SSIM} = \frac{\left( {{2\sigma}_{\rm gt,p} + C_{\rm 2}} \right)\left( {2\mu_{\rm gt}\mu_{\rm p} + C_{\rm 1}} \right)}{\left( \sigma_{\rm gt}^{2} + \sigma_{\rm p}^{2} + C_{\rm 2} \right)\left( \mu_{\rm gt}^{2}{+ \mu}_{\rm p}^{2} + C_{\rm 1} \right)},
    \label{equation:L-ssim}
\end{equation}
where, $\mu$, $\sigma$ are the mean and variance of the depth image, $\sigma_{\rm gt,p}$ is the covariance of ground-truth depth and predicted depth, $C_1$, $C_2$ are constants related to pixel range. 
\hspace*{\fill}
\subsubsection{\bf Smooth Loss}
The smooth loss $L_{\rm smooth}$ is determined by the cosine similarity between the surface normals $S_{\rm pre}$ and $S_{\rm gt}$, which are calculated from the predicted depth and ground-truth depth, respectively. A small value $\epsilon$ is added to prevent division by zero. The smooth loss $L_{\rm smooth}$ models the image edges and is performed at the pixel level \cite{zhu2021rgb,fang2022transcg}: 

% It has been widely adopted by previous research studies \cite{zhu2021rgb,fang2022transcg}:

% $L_{\rm smooth}$ is the cosine similarity between surface normal $S_{\rm pre}$ and $S_{\rm gt}$ computed from predicted depth and ground-truth depth, $\epsilon$ is a small value to avoid division by zero. Smooth loss $L_{\rm smooth}$ also computes at the image scale and is widely used in previous work \cite{zhu2021rgb,fang2022transcg}.
\begin{equation}
L_{\rm smooth} = 1-\frac{\rm S_{pre}\cdot S_{\rm gt}}{\max({||S_{\rm pre}||_{2}\cdot ||S_{\rm gt}||_{2}},\epsilon )}.
\label{equation:L-S}
\end{equation}

Our training loss is described as follow:
\begin{equation}
    L = L_{\rm Huber} + \alpha L_{\rm SSIM} + {\beta L}_{\rm smooth},
    \label{equation:L}
\end{equation}
where $L_{\rm Huber}$ penalizes depth inaccuracy, $L_{\rm SSIM}$ penalizes structural similarity and $L_{\rm smooth}$ penalizes unsmoothness.  $\alpha$ and $\beta$ are the weight parameters.
% Pixels with depth out of range [0.3, 1.5] are regarded as invalid value and do not participate loss calculation.
Pixels with intensity values outside the range of $[0.3, 1.5]$ are considered invalid and are excluded from the loss calculation.