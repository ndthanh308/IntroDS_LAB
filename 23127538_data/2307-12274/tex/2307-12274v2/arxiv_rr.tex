\documentclass[letterpaper, 10 pt, journal, twoside]{IEEEtran}

\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{setspace}
\pdfoutput=1 
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{cite}
\usepackage{indentfirst}
\usepackage{multirow}
\IEEEoverridecommandlockouts 
\usepackage[margincaption,outercaption,ragged,wide]{sidecap}
\sidecaptionvpos{figure}{t} 
\sidecaptionvpos{table}{t}
\usepackage[amssymb]{SIunits}
\usepackage{booktabs}
%\usepackage[misc,geometry]{ifsym}
% \setlength {\marginparwidth }{1.35cm}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{marvosym}
\usepackage{ifsym}
% \usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{caption}
% \usepackage[letterpaper,top=60pt,bottom=43pt,left=48pt,right=48pt]{geometry}
\usepackage{threeparttable}
% \usepackage{tablefootnote}
%  for type3 font
% \usepackage[T1]{fontenc}
% \usepackage{aecompl}
\newcommand{\fref}[1]{Fig. \ref{#1}}
\newcommand{\sref}[1]{Section \ref{#1}}
\newcommand{\tref}[1]{Table \ref{#1}}
% arxiv
% \usepackage{fancyhdr}
% \renewcommand{\headrulewidth}{0pt}
% \renewcommand{\footrulewidth}{0pt}
\begin{document}

% \let\cleardoublepage\clearpage
\title{FDCT: Fast Depth Completion for Transparent Objects}


\author{Tianan Li$^{1}$, Zhehan Chen$^{1,\textrm{\Letter}}$,~\IEEEmembership{Member,~IEEE}, Huan Liu$^{2}$,~\IEEEmembership{Member,~IEEE}, and Chen Wang$^{3}$

\thanks{Manuscript received: March, 18, 2023; Revised June, 13, 2023; Accepted July, 17, 2023.}% Use only for final RAL version
\thanks{This paper was recommended for publication by Editor Cesar Cadena Lerma upon evaluation of the Associate Editor and Reviewers' comments.}
\thanks{This work was supported by the National Key R\&D
Program of China: [grant number 2021YFB3401500] and the
Fundamental Research Funds for the Central Universities: [grant number FRF-TP-20-009A3].}%  %Use only for final RAL version</span>}%
\thanks{$^{1}$The authors are with the School of Mechanical Engineering, University of Science and Technology Beijing, Beijing 100083, China. $^{\textrm{\Letter}}$Corresponding author: {\tt chenzh\_ustb@163.com}}% <-this % stops a space}}%
\thanks{$^{2}$Huan Liu is with the Department of Electrical and Computer Engineering, McMaster University, Canada. Email: {\tt liuh127@outlook.com}}%
\thanks{$^{3}$Chen Wang is with the Spatial AI \& Robotics Lab at The Department of Computer Science and Engineering, State University of New York at Buffalo, NY 14260, USA. {\tt\small chenw@sairlab.org}}% <-this % stops a space
\thanks{Digital Object Identifier (DOI): see top of this page.}}

% \markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted July, 2023}
% {Li \MakeLowercase{\textit{et al.}}: FDCT} 


\markboth{IEEE Robotics and Automation Letters. Preprint Version. Accepted July, 2023}
{Li \MakeLowercase{\textit{et al.}}: FDCT}

\maketitle
% % for arvix
% \thispagestyle{fancy}

% \setlength{\footskip}{5mm}
% \fancyhead{}
% \lhead{}
% \lfoot{\small

% 两倍行距的文字
% \copyright~2023 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

% }
% \cfoot{}
% \rfoot{}

\begin{abstract}
Depth completion is crucial for many robotic tasks such as autonomous driving, 3-D reconstruction, and manipulation.
Despite the significant progress, existing methods remain computationally intensive and often fail to meet the real-time requirements of low-power robotic platforms.
Additionally, most methods are designed for opaque objects and struggle with transparent objects due to the special properties of reflection and refraction.
To address these challenges, we propose a Fast Depth Completion framework for Transparent objects (FDCT), which also benefits downstream tasks like object pose estimation.
To leverage local information and avoid overfitting issues when integrating it with global information, we design a new fusion branch and shortcuts to exploit low-level features and a loss function to suppress overfitting.
This results in an accurate and user-friendly depth rectification framework which can recover dense depth estimation from RGB-D images alone.
Extensive experiments demonstrate that FDCT can run about 70 FPS with a higher accuracy than the state-of-the-art methods.
We also demonstrate that FDCT can improve pose estimation in object grasping tasks.
The source code is available at \url{https://github.com/Nonmy/FDCT}.
\end{abstract}


\begin{IEEEkeywords}
Deep Learning Methods,Computer Vision for Manufacturing, Depth Completion
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\input{Final_Submission/introduction}
\input{Final_Submission/related_work.tex}

\input{Final_Submission/method.tex}
\input{Final_Submission/experiment.tex}
\input{Final_Submission/limitation_conclusion.tex}

\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}