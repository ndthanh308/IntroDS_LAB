\section{Related Works}

\subsection{Depth Completion of Transparent Objects}
Depth completion is a long-standing problem in computer vision. Considerable works\cite{eigen2015predicting,chen2016single} infer depth directly from single RGB image. However, these methods can hardly be used for transparent objects. RGB-D-based methods are demonstrated to outperform monocular methods by a large margin in previous work. 
Zhang et al. \cite{zhang2018deep} firstly use a deep learning-based method for depth completion from RGB-D images. They use neural network to predict surface normal and occlusion boundary from RGB image respectively, and then solve the output depths with a global linear optimization regularized by raw depth information. Huang et al.\cite{huang2019indoor} and Tang et al. \cite{tang2021depthgrasp} replace the global optimization part with a neural network, resulting in an end-to-end network.
% , making it an end-to-end network. 

The first work focus on transparent objects depth completion is ClearGrasp\cite{sajjan2020clear}, which employs transparent mask to modify raw depth image. They also constructed the first transparent objects dataset for depth completion and pose estimation. DepthGrasp \cite{tang2021depthgrasp} replaces the global optimization part with adversarial neural network and introduces spectral residual blocks for stability. Both methods are computatioally heavy and require normal information, which is inconvenient to acquire, making them impractical for real-time applications. 
Local Implicit \cite{zhu2021rgb} builds local implicit neural representation on ray-voxel pairs and iteratively refines the depth. It also introduces a large-scale synthetic dataset Omniverse. Their work significantly improves the speed for depth completion. TranspareNet \cite{xu2021seeing} combines depth completion and point cloud completion, using distorted depth to construct point clouds and complete it as a rough representation of depth. 

Recently, Fang et al.\cite{fang2022transcg} proposed a larger-scale real-world dataset TrasCG and a depth completion network for transparent objects (DFNet), it adopts U-Net architecture and uses a lightweight backbone called DenseBlock\cite{huang2017densely}, and achieves SOTA performance with small memory use and model size.

\subsection{Pose Estimation of Transparent Objects}
RGB-D-based methods focusing on opaque rigid objects 6 DoF pose estimation mostly failed to transparent objects due to the inaccuracy of depth value. Early works focusing on transparent objects use traditional features to estimate pose \cite{lysenkov2013recognition,lysenkov2013pose,guo2019transparent}. However, low-level traditional features are not as discriminative as high-level deep features. The RGB-based method KeyPose \cite{liu2020keypose} captures and labels 3D key points from stereo RGB images and estimates pose from these key points. GhostPose* \cite{chang2021ghostpose} further improves this by defining four virtual 3D key points from RGB images and reconstructing 3D coordinates of the key points based on triangulation. Xu et al. \cite{xu20206dof} estimate 6DoF transparent objects pose from a single RGB-D image. Recently, Chen et al. proposed ClearPose \cite{chen2022clearpose}, a large-scale real-world benchmark and dataset for transparent objects, and ProgressLabeller \cite{chen2022progresslabeller}, a semi-automatic method for labeling the 6D pose of transparent objects.

There are works that utilize light field cameras \cite{zhou2019glassloc}, single pixel cameras \cite{mathai20203d}, microscope-camera systems \cite{grammatikopoulou2019three}, and polarization cameras \cite{kalra2020deep} for pose estimation or grasping on transparent objects. However, these methods differ significantly from RGB-D based methods and are less accessible compared to commercial depth cameras like RealSense or Kinect. One potential research direction is the use of neural radiation fields for transparent scenes \cite{ichnowski2021dex,zhou2020lit,mildenhall2021nerf}.
Pose estimation methods using only RGB information will lose multimodal information at the initial stage, and there are only a few methods that can directly estimate the pose of transparent objects from RGB-D information. Thus, we believe that, at the current stage, combining depth completion and specially designed pose estimation methods can bring benefits to the pose estimation of transparent objects.