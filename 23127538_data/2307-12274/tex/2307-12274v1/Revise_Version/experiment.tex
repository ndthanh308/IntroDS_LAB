
\section{Experiment}

\subsection{Datasets and metrics}

% \noindent\textbf{Dataset.}
\subsubsection{Dataset}
% We adopt three datasets in our experiments, i.e., ClearGrasp \cite{sajjan2020clear}, TransCG \cite{fang2022transcg} and ClearPose \cite{chen2022clearpose}. The ClearGrasp dataset is the pioneering large-scale synthetic dataset that specifically focused on transparent objects. It provids a large-scale synthetic dataset as well as a real-world benchmark. The TransCG dataset comprises 57K RGB-D images from 130 different real-world scenes. 
% ClearPose dataset contains 350K RGB-D images of 63 household objects in real-world settings. Depth completion experiments and generalization verification (reported respectively in Section \ref{sec:depth} and \ref{sec:generalization}) are conducted on ClearGrasp, TransCG and ClearPose. Ablation study (reported in Section \ref{sec:ablation}) is performed on TransCG.
We use three datasets including ClearGrasp \cite{sajjan2020clear}, TransCG \cite{fang2022transcg}, and ClearPose \cite{chen2022clearpose}. The ClearGrasp dataset is a pioneering large-scale synthetic dataset that specifically focuses on transparent objects. It provides a large-scale synthetic dataset as well as a real-world benchmark. The TransCG dataset comprises 57K RGB-D images from 130 different real-world scenes. The ClearPose dataset contains 350K RGB-D images of 63 household objects in real-world settings. 
% We conducted depth completion experiments and generalization verification on ClearGrasp, TransCG, and ClearPose, reported respectively in Section \ref{sec:depth} and \ref{sec:generalization}. We performed an ablation study on TransCG, which is reported in Section \ref{sec:ablation}.

% ClearGrasp\cite{sajjan2020clear} is the first large-scale synthetic dataset as well as a real-world test benchmark focusing on transparent objects. TransCG\cite{fang2022transcg} is a large-scale real-world dataset, which contains 57K RGB-D images from 130 different scenes. ClearPose\cite{chen2022clearpose} is a recentily proposed real-world dataset, containing 350K RGB-D images covering 63 household objects.

% \newgeometry{letterpaper,top=60pt,bottom=43pt,left=48pt,right=48pt}
% \begin{table*}[!t]
% \caption{Ablation study. We show the impact of progressively substituting the components of the DFNet with ours. \label{tab:table1}
% }
% \centering
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{cccccccccc}
% \toprule
% Model/Metric    & RMSE  & REL   & MAE   & $\delta$1.05 & $\delta$1.10 & $\delta$1.25          & Inference time (s)& Parameters & Size (MB)   \\ \midrule
% DFNet\cite{fang2022transcg}          & 0.018 & 0.027 & 0.012 & 83.76 & 95.67 & 99.71          & 0.0244s        & 1.25M & 4.819MB \\ \midrule
% New Loss        & 0.017 & 0.026 & 0.012 & 84.42 & 96.30 & \textbf{99.81} & 0.0244s        & 1.25M & 4.819MB \\ \midrule
% Shortcut Fusion & 0.017 & 0.024 & 0.011 & 86.18 & 96.67 & 99.79          & 0.0218s        & 1.02M & 3.919MB \\ \midrule
% Ours(slim) & 0.016          & 0.024          & 0.011          & 86.22          & 96.64          & \textbf{99.81} & \textbf{0.0143s} & \textbf{0.39M} & \textbf{1.518MB} \\ \midrule
% Ours       & \textbf{0.015} & \textbf{0.022} & \textbf{0.010} & \textbf{88.18} & \textbf{97.15} & \textbf{99.81} & 0.0153s          & 1.25M          & 4.803MB          \\
% \bottomrule
% \end{tabular}%
% }
% \end{table*}
\begin{table}[!t]
\renewcommand{\arraystretch}{1.05}
\setlength{\tabcolsep}{5pt}
\caption{Ablation study. We show the impact of progressively substituting the components of the DFNet with ours. \label{tab:table1}
}
\centering
\resizebox{\linewidth}{!}{%
\begin{threeparttable}
\begin{tabular}{cccccccccc}
\toprule
Model   & RMSE  & REL   & MAE   & $\delta$1.05 & $\delta$1.10 & $\delta$1.25          & Time(s)& Para(M) & Size (MB)   \\ \midrule
DFNet\cite{fang2022transcg}          & 0.018 & 0.027 & 0.012 & 83.76 & 95.67 & 99.71          & 0.0244        & 1.25 & 4.819 \\ \midrule
Huber Loss &0.017   &0.027  &0.012  &84.10  &95.82  &99.74 &0.0244  &1.25   &4.819  \\ \midrule
New Loss        & 0.017 & 0.026 & 0.012 & 84.42 & 96.30 & \textbf{99.81} & 0.0244        & 1.25 & 4.819 \\ \midrule
SF* & 0.017 & 0.024 & 0.011 & 86.18 & 96.67 & 99.79          & 0.0218        & 1.02 & 3.919 \\ \midrule
Ours(s)* & 0.016          & 0.024          & 0.011          & 86.22          & 96.64          & \textbf{99.81} & \textbf{0.0143} & \textbf{0.39} & \textbf{1.518} \\ \midrule
Ours       & \textbf{0.015} & \textbf{0.022} & \textbf{0.010} & \textbf{88.18} & \textbf{97.15} & \textbf{99.81} & 0.0153          & 1.25          & 4.803          \\
\bottomrule
\end{tabular}%
% \multicolumn{10}{l}{Note: NL* represents New Loss, SF* represents Shortcut Fusion and Ours(s)* represents Ours(slim).}
\begin{tablenotes}
\footnotesize
\item Note: SF* represents Shortcut Fusion and Ours(s)* represents Ours(slim).
\end{tablenotes}

\end{threeparttable}
}


\end{table}
% \vspace{-0.5cm}
\subsubsection{Metrics}
For evaluating the performance of our depth completion model, we employ four common metrics: RMSE, REL, MAE and Threshold $\delta$ (where $\delta$ is set to 1.05, 1.10, and 1.25). These metrics are calculated only on the transparent areas, as determined by transparent masks.
% Me use common metrics RMSE, REL, MAE and Threshold $\delta$ ($\delta$ is set to 1.05, 1.10 and 1.25) to evaluate our model. All metrics are calculated on the transparent areas according to transparent masks.


% We use three metrics to evaluate performance on pose estimation task. The average closest point distance (ADD-S)\cite{xiang2017posecnn} calculates the mean distance from each 3D model point to its closest neighbor on the target model. Followed DenseFusion\cite{wang2019densefusion} we report the area under the ADD-S curve (AUC) and the percentage of ADD-S smaller than 2cm ($<$2cm).

\subsection{Implementation Details}
% \noindent
% \textbf{Network configuration.}
\subsubsection{\bf Network Configuration}
% \textcolor{blue}{
In the network architecture, the number of hidden channels, \textbf{$C$}, is set to 64. Each FFEB/DFCB contains a single OSA module. Each OSA module is composed of 5 layers with stage channels of 20. The SFM module maintains \textbf{$C$} channels throughout the pipeline, while cross-layer shortcuts have only 1 channel. Residual connections between the encoder and decoder retain only \textbf{$C$} channels. The input head module and output head module use $3\times3$ convolution to adjust the number of channels and resolution (with resolution changes only occurring in the input head module). For the slim version, \textbf{$C$} is set to 32, and the OSA block contains 4 layers with stage channels of 16.
% }
% The hidden channels \textbf{$C$} in the network is set to 64. Each FFEB/DFCB contains one OSA module, in which, we use 5 layers per block and set stage channels \textbf{$C'$} to 20. SFM keeps \textbf{$C$} channels throughout the pipeline while cross-layer shortcuts take 1 channel only. Residual connections between encoder and decoder just keep channel \textbf{$C$}. $3\times3$ convolution is used in the input head module and the output head module to modify channels and resolution (resolution modified in the input head module only). For slim version, \textbf{$C$} is set to 32, \textbf{$C'$} is set to 16 and uses 4 layers per OSA block.

\subsubsection{\bf Training Details}
% \noindent
% \textbf{Training details.}
All experiments are carried out using the AdamW optimizer with an initial learning rate of $10^{-3}$. The learning rate is reduced by half after 5, 15, 25, and 35 epochs, and training continues for a total of 40 epochs with a batch size of 32. The threshold $\delta$ is kept constant at 0.1 during the training process. The weights $\alpha$ and $\beta$ for the loss function are set to 0.1 and 0.001, respectively. The images are resized to $320\times240$ for both training and testing. The experiments were conducted using an NVIDIA GeForce RTX 3090 GPU.
% We use AdamW optimizer with initial learning rate of $10^{-3}$ and multi-step learning rate scheduler which decays the learning rate by half after 5, 15, 25, 35 epochs. We train the model for 40 epochs with the batch size of 32. Threshold $\delta$ keeps 0.1 during training. Considering loss, we set $\alpha=0.1$, $\beta=0.001$. For all methods, we scale the images to $320\times240$ during training and testing. We use NVIDIA GeForce RTX 3090 for training and testing. 

 % Depth completion task and generalization ability are tested on ClearGrasp, TransCG and ClearPose. Pose estimation task is carried out on the set1 of ClearPose, since Clearpose has an accurate pose annotation without sticker. We use typical network DenseFusion\cite{wang2019densefusion} as pose estimation network. Following the learning strategy of DenseFusion, we train the network on 12G NVIDIA TITAN Xp GPU for 5 epochs with batch size of 128. The margin of refinement is set to 0.03. For fair comparison, we evaluate others works using their released source codes and optimal hyper-parameters or statistics reported in their paper.

\subsection{Ablation study} \label{sec:ablation}
We conduct an ablation study to investigate the effectiveness of our proposed components, including  new loss function, fusion branch, cross-layer shortcut and backbone structure. We take DFNet as baseline method since it is constructed following UNet structure. We  gradually replace its original components by our proposed ones and show the influence of using our proposed components. All the experiments of the ablation study are conducted on TransCG dataset.

% In view that DFNet is also constructed based on UNet, We here gradually replace its original components by our proposed. This study is conducted on TransCG dataset.
% To study the impact of each component in our proposed method, we perform experiments with different configurations of loss functions, network architecture, and backbones. Our method is compared against the recent transparent object depth completion work DFNet, which serves as our baseline. The ablation study experiments are all performed on the TransCG dataset.
% To verify the effectiveness of each component in our method, we evaluate the performance w.r.t. different configurations of loss functions, network architecture, and backbones. We use recently proposed transparent objects depth completion work DFNet as baseline. Ablation study is carried out on TransCG.




\subsubsection{\bf Loss Function}
The training of DFNet employs the mean squared error (MSE) and smooth loss as its loss function. However, these simple loss functions can lead to overfitting to local features, which makes the model more sensitive to the noise from low-level features such as edges and positions, negatively impacting its accuracy. To validate our proposed loss function, we first replaced the MSE loss with Huber loss in DFNet and termed it as Huber Loss. And then we replaced the loss function of DFNet with ours, leaving all other aspects unchanged and termed it as New Loss in Table \ref{tab:table1}. It can be observed by comparing New Loss with DFNet that all metrics showed improvement without requiring any additional parameters. 

% Qualitatively, the use of our proposed loss function can let the network to concentrate on the global structure rather than local details. By comparing the rows 3 and 4 of Figure \ref{fig:figure5}, the boundaries become smoother and even less distinct.
% The training of DFNET uses MSE and cosine distance. The simple loss function may lead to overfit to local features during training. This makes the model more sensitive to the noise of low-level features such as edge and position, which in turn affects its accuracy. So we propose a loss function consisting of Huber loss, SSIM loss and Smooth loss to suppress it. To verify its validity, we replaced the loss function of DFNet with ours and remain its other parts unchanged, then compared the results output by the mixed model (New Loss in Table \ref{tab:table1}) with the original one.
% All metrics are improved without extra parameters. Furthermore, we manually designed a feature to describe those pixels by computing the gradient of depth image and doing Gaussian blur to form an 'edge mask'. As their wights drop, the performance of the model is improved (Edge weight modified in Table \ref{tab:table2}), suggesting that it is necessary to treat pixels differently.
%and lower their weight during training. Specifically, we compute the gradient of depth image and do gaussian blur to form an 'edge mask'. Result (Edge weight modified in Table \ref{tab:table2}) supports our idea and shows it is necessary to treat pixels differently. 

\subsubsection{\bf Fusion Branch and Cross-layer Shortcuts}
In order to evaluate the impact of our proposed fusion branch and cross-layer shortcuts, we make changes to DFNet's architecture. First, we remove the redundant CDC blocks in DFNet from its skip connections, in line with our insight of preserving low-level features and the purpose of light weighting. Then, we added cross-layer shortcuts and a fusion branch to the modified network. It can be seen in Table \ref{tab:table1} that adopting this new architecture (referred to as Shortcut Fusion), almost all metrics show improvement with fewer parameters. 

\subsubsection{\bf Backbone}
We finally replace the denseblock in DFNet with our OSA module and utilized max pooling as the downsampling method. This final modification has transformed DFNet into our network. As shown in Table \ref{tab:table1}, our network outperforms the previous state-of-the-art (SOTA) by at least 16\% on difference-based metrics and improves ratio-based metrics by up to 4.42\%, resulting in a new SOTA performance. To make it practical for low-power robots, we created a slim version to balance speed and accuracy. 


% Qualitatively, figure \ref{fig:figure5} shows our method predicts clearer edges and is better handling crowded area.

% The fusion branch in our proposed network introduces a rich collection of low-level features, while the OSA module promotes feature reuse. Additionally, raw depth information is provided throughout the network, which enhances the representation of low-level features but may also hinder the learning of high-level semantic information. Our hypothesis is that the use of max pooling as a less aggressive downsampling method can mitigate these side effects while also reducing the number of parameters. The results in Table \ref{tab:table2} support our viewpoint.
% We fianlly relace the denseblock in DFNet by our used OSA module, and use max pooling as downsampling method. After this final modification, DFNet is tranformed to our proposed network. We thus show the performance by :Our"  in Table \ref{tab:table1}. It can be observed that ours outperforms previous SOTA by at least 16\% on difference-based metrics and improves ratio-based metrics by 0.1\% to 4.42\%, achieving the new state-of-the-art performance. In order to be capable in real applications, we also construct a slim version for speed/accuracy trade-off. 
% As we mentioned above, fusion branch introduces abundant low-level features and OSA encourages feature reuse. Furthermore, Raw depth is provided throughout the network. They enrich the representation of low-level features but may also harm to the learning of high-level semantic information. We suppose that using maxpooling to loosely downsampling may reduce their side effects as well as parameters saving. Result in Table \ref{tab:table2} proved our point of view.

% For summary, with our loss function, network tend to learn high-level features, with fusion branch, raw depth image and shortcuts, network can take advantage of low-level features. These components working together gives the network ability to take into account both local details and global structures. OSA module and max-pooling downsampling accelerate inference speed and reduce side effects.




% To intuitively show the impact of the proposed components, we visualize the predicted depth on TransCG and CleargGrasp dataset in Figure \ref{fig:figure5}. All networks are trained on TransCG dataset. Qualitatively, with our loss function, network is likely to focus on global structure rather than local detail. Red rectangle in row 3 and 4 show that with our loss function, boundaries become smoothy and even ambiguous, and outliers in the bottom right corner of the second column are suppressed. 



% FDCT performs domain adaption to the concatenation of raw depth and deep features and adopts maxpooling to lossly downsampling. It is supposed to reduce the disadvantage of the inaccuracy of raw depth. Our method predicts more accuracy and smooth edge as shown by the red circle on the left and the black square on the right. And even correct the ground truth as depicted in black circle on the right. The light spot reflected on the apple significantly affects the performance in row 2,3,5, but has little impact on row 4,6. Our methods successfully overcome the side effect of the raw depth information.

\subsection{Depth Completion Experiments} \label{sec:depth}

We compare our method with others on synthetic dataset ClearGrasp and real-world dataset TransCG. The quantitative results are respectively reported in Table \ref{tab:table2} and Table \ref{tab:table3}. Our proposed network surpasses others in almost every metric on these datasets which contain  synthetic and real-world scenes. Our method achieves a new state-of-the-art performance with a smaller model size and faster inference time, making it a highly competitive solution in this field.
%except on ClearGrasp synthetic validation set. It may be result of that the local implicit depth function which is environment-dependent, as well as the extra training data. 

% {\color{blue}
Specifically, our method outperforms the other methods by a larger margin in terms of REL and $\delta1.05$ metrics. This indicates its robustness to noise in the raw depth information, as these metrics are computed based on relative values and are sensitive to noise. Additionally, the gap between our method and others is larger in tests involving novel objects in ClearGrasp (CG Syn-novel in Table \ref{tab:table4} and the ClearGrasp column in Figure \ref{fig:figure5}), indicating that our method has a better ability to generalize to unseen objects. The qualitative results is reported in Figure \ref{fig:figure5}. The prediction of our method exhibits a clearer boundary and finer details than DFNet.
% }
% Specifically, our method has a bigger gap in REL and $\delta1.05$ to others most of the time. It demonstrates that our method is more stable to the noise in raw depth information of pixels, because these metrics are computed by relative value and significantly affected by noise. Noteworthy, the gap between our method and others getting bigger in the test of novel objects in most cases, indicates our method is able to generalize better to unseen objects.

\begin{table}[!t]
\caption{Depth Completion Result on TransCG dataset.}
\label{tab:table2}

\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccccc}
\toprule
Model & RMSE  & REL   & MAE   & $\delta1.05$ & $\delta1.10$ & $\delta1.25$ & Time ($\second$)   & Size ($\mega$B)    \\ \midrule
ClearGrasp\cite{sajjan2020clear}   & 0.054 & 0.083 & 0.037 & 50.48 & 68.68 & 95.28 & 2.281          & 934          \\
LIDF-Refine\cite{zhou2021pr}  & 0.019 & 0.034 & 0.015 & 78.22 & 94.26 & 99.80 & 0.018          & 251          \\
DFNet\cite{fang2022transcg}        & 0.018 & 0.027 & 0.012 & 83.76 & 95.67 & 99.71 & 0.024          & 4.8          \\
Ours (slim)   & 0.017 & 0.025 & 0.011 & 85.53 & 96.46 & 99.79 & \textbf{0.014} & \textbf{1.6} \\
Ours & \textbf{0.015} & \textbf{0.022} & \textbf{0.010} & \textbf{88.18} & \textbf{97.15} & \textbf{99.81} & 0.015 & 4.8 \\ \bottomrule
\end{tabular}}
% \vspace{-0.5cm}
\end{table}


\begin{table}[!t]
\renewcommand{\arraystretch}{0.9}
\caption{Depth Completion Results on ClearGrasp dataset\label{tab:table3}}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{Model/Metric} &
  \multicolumn{1}{c}{RMSE} &
  \multicolumn{1}{c}{REL} &
  \multicolumn{1}{c}{MAE} &
  \multicolumn{1}{c}{$\delta$1.05} &
  \multicolumn{1}{c}{$\delta$1.10} &
  $\delta$1.25 \\ \midrule
\multicolumn{7}{c}{Train CG Test CG Syn-novel} \\ \midrule
\multicolumn{1}{c}{ClearGrasp} &
  \multicolumn{1}{c}{0.040} &
  \multicolumn{1}{c}{0.071} &
  \multicolumn{1}{c}{0.035} &
  \multicolumn{1}{c}{42.95} &
  \multicolumn{1}{c}{80.04} &
  98.10 \\ 
\multicolumn{1}{c}{Local Implicit} &
  \multicolumn{1}{c}{\underline{0.028}} &
  \multicolumn{1}{c}{\underline{0.045}} &
  \multicolumn{1}{c}{\underline{0.023}} &
  \multicolumn{1}{c}{\underline{68.62}} &
  \multicolumn{1}{c}{\underline{89.10}} &
  \underline{99.20} \\ 
\multicolumn{1}{c}{DFNet} &
  \multicolumn{1}{c}{0.032} &
  \multicolumn{1}{c}{0.051} &
  \multicolumn{1}{c}{0.027} &
  \multicolumn{1}{c}{62.59} &
  \multicolumn{1}{c}{84.37} &
  98.39 \\ 
\multicolumn{1}{c}{FDCT (Ours)} &
  \multicolumn{1}{c}{\textbf{0.025}} &
  \multicolumn{1}{c}{\textbf{0.040}} &
  \multicolumn{1}{c}{\textbf{0.021}} &
  \multicolumn{1}{c}{\textbf{71.66}} &
  \multicolumn{1}{c}{\textbf{92.95}} &
  \textbf{99.64} \\ \midrule
\multicolumn{7}{c}{Train CG Test CG Syn-known} \\ \midrule
\multicolumn{1}{c}{Local Implicit} &
  \multicolumn{1}{c}{\textbf{0.012}} &
  \multicolumn{1}{c}{\textbf{0.017}} &
  \multicolumn{1}{c}{\textbf{0.009}} &
  \multicolumn{1}{c}{\textbf{94.79}} &
  \multicolumn{1}{c}{\textbf{98.52}} &
  99.67 \\ 
\multicolumn{1}{c}{ClearGrasp} &
  \multicolumn{1}{c}{0.044} &
  \multicolumn{1}{c}{0.047} &
  \multicolumn{1}{c}{0.033} &
  \multicolumn{1}{c}{71.23} &
  \multicolumn{1}{c}{92.60} &
  98.24 \\ 
\multicolumn{1}{c}{DFNet} &
  \multicolumn{1}{c}{0.018} &
  \multicolumn{1}{c}{0.023} &
  \multicolumn{1}{c}{0.013} &
  \multicolumn{1}{c}{88.85} &
  \multicolumn{1}{c}{97.57} &
  \underline{99.92} \\ 
\multicolumn{1}{c}{FDCT (Ours)} &
  \multicolumn{1}{c}{\underline{0.015}} &
  \multicolumn{1}{c}{\underline{0.020}} &
  \multicolumn{1}{c}{\underline{0.012}} &
  \multicolumn{1}{c}{\underline{90.53}} &
  \multicolumn{1}{c}{\underline{98.21}} &
  \textbf{99.99} \\ \bottomrule

\end{tabular}%
% \tablen}
}
\end{table}



\subsection{Generalization Experiment} \label{sec:generalization}
% The generalization capability of a network is essential for practical applications. We evaluated the generalization ability of our proposed method from two perspectives: from synthetic images to real-world images and from one real-world dataset to another. The results of our experiments, shown in Table \ref{tab:table6}, indicate that our method (FDCT) has a comparable generalization capability to the state-of-the-art methods in cross-dataset evaluations, and it outperforms similar works in the synthetic-to-real test. However, it lags behind methods that focus solely on sim-to-real (noted as "local implicit*").
% The generalization ability of a network is critical for real-world application. The proposed method has a generalization ability that can be trained on synthetic data and aply to real world scene (syn-to-real) or trained on one real world dataset TransCG and adap to ClearGrasp (real-to-real). Comparison result is reported in Table \ref{tab:table4}. It shows that although there is still a certain gap compared with the method Local Implicit designed for syn-to-real; compared with the similar method DFNet, our method achieves a better result in the syn-to-real setting, and a competitive result in the syn-to-syn setting.
The generalization ability of a network is critical for real-world application. Our proposed method exhibits a high degree of generalization, being able to be trained on synthetic data and applied to real-world scenes (syn-to-real), or trained on one real-world dataset TransCG and adapted to the other real-world dataset (real-to-real), such as ClearGrasp. Comparison results are reported in Table \ref{tab:table4}, which show that while there is still a certain gap compared to the syn-to-real method (Local Implicit \cite{zhu2021rgb}), our method achieves better results in the syn-to-real setting when compared to the similar method DFNet, and competitive results in the real-to-real setting.

% We inspect the generalization ability of our proposed method from two aspects, from synthetic image to real-world image and from one real-world dataset to another. Experiment results in Table \ref{tab:table5} show that FDCT has a similar generalization ability to previous SOTA in cross-dataset and get better result in synthetic-to-real test compared to similar work, but is far below to methods focusing on sim-to-real.

% Since both datasets comprise real-world image, we train models on TransCG and test it on ClearGrasp real-world set for cross-dataset test. DFNet outperformed other method with a huge gap in generalization test and is chosen to be compared with ours. Comparison result is reported in Table \ref{tab:table5}. Our method outperforms the closest work in all metrics both for known and novel objects in synthetic-to-real test. There is a bigger gap between DFNet and ours in terms of novel objects. It might owe to a better utilization of RGB cues. Our method gets similar results to DFNet in cross dataset test, showing that our method has the ability to generalize from real-world dataset to another. With a series of real-world transparent objects datasets being proposed, we believe that the generalization ability in real-world is more important than sim-to-real.



% {\color{blue}
% Figure environment removed

\begin{table}[!t]
\caption{
% Result of Synthetic to Real and Cross Dataset Generalization Experiment
Generalization test on syn-to-real and real-to-real.}
\label{tab:table4}
\renewcommand{\arraystretch}{0.95}
\centering
\resizebox{\linewidth}{!}{%
% \begin{threeparttable}
\begin{tabular}{ccclclclclcl}
\toprule
\multicolumn{1}{c}{Model/Metric} &
  \multicolumn{1}{c}{RMSE} &
  \multicolumn{2}{c}{REL} &
  \multicolumn{2}{c}{MAE} &
  \multicolumn{2}{c}{$\delta$1.05} &
  \multicolumn{2}{c}{$\delta$1.10} &
  \multicolumn{2}{c}{$\delta$1.25} \\ \midrule
\multicolumn{12}{c}{Train CG Test CG Real-known (syn-to-real)} \\ \midrule
\multicolumn{1}{c}{Local Implicit\cite{zhu2021rgb}} &
  \multicolumn{1}{c}{\textbf{0.028}} &
  \multicolumn{2}{c}{\textbf{0.033}} &
  \multicolumn{2}{c}{\textbf{0.020}} &
  \multicolumn{2}{c}{\textbf{82.37}} &
  \multicolumn{2}{c}{\textbf{92.98}} &
  \multicolumn{2}{c}{\textbf{98.63}} \\ 
\multicolumn{1}{c}{DFNet} &
  \multicolumn{1}{c}{0.068} &
  \multicolumn{2}{c}{0.107} &
  \multicolumn{2}{c}{0.059} &
  \multicolumn{2}{c}{32.42} &
  \multicolumn{2}{c}{56.88} &
  \multicolumn{2}{c}{91.47} \\ 
\multicolumn{1}{c}{FDCT (Ours)} &
  \multicolumn{1}{c}{\underline{0.065}} &
  \multicolumn{2}{c}{\underline{0.103}} &
  \multicolumn{2}{c}{\underline{0.057}} &
  \multicolumn{2}{c}{\underline{33.08}} &
  \multicolumn{2}{c}{\underline{59.81}} &
  \multicolumn{2}{c}{\underline{91.70}} \\ \midrule
\multicolumn{12}{c}{Train CG Test CG Real-novel (syn-to-real)} \\ \midrule
\multicolumn{1}{c}{Local Implicit\cite{zhu2021rgb}} &
  \multicolumn{1}{c}{\textbf{0.025}} &
  \multicolumn{2}{c}{\textbf{0.036}} &
  \multicolumn{2}{c}{\textbf{0.020}} &
  \multicolumn{2}{c}{\textbf{76.21}} &
  \multicolumn{2}{c}{\textbf{94.01}} &
  \multicolumn{2}{c}{\textbf{99.35}} \\ 
\multicolumn{1}{c}{DFNet} &
  \multicolumn{1}{c}{0.051} &
  \multicolumn{2}{c}{0.088} &
  \multicolumn{2}{c}{0.046} &
  \multicolumn{2}{c}{31.23} &
  \multicolumn{2}{c}{64.66} &
  \multicolumn{2}{c}{97.77} \\ 
\multicolumn{1}{c}{FDCT (Ours)} &
  \multicolumn{1}{c}{\underline{0.043}} &
  \multicolumn{2}{c}{\underline{0.073}} &
  \multicolumn{2}{c}{\underline{0.038}} &
  \multicolumn{2}{c}{\underline{39.42}} &
  \multicolumn{2}{c}{\underline{75.54}} &
  \multicolumn{2}{c}{\underline{99.09}} \\ \midrule
\multicolumn{12}{c}{Train TCG Test CG Real-novel (real-to-real)} \\ \midrule
\multicolumn{1}{c}{Local Implicit\cite{zhu2021rgb}} &
  \multicolumn{1}{c}{0.152} &
  \multicolumn{2}{c}{0.225} &
  \multicolumn{2}{c}{0.139} &
  \multicolumn{2}{c}{9.86} &
  \multicolumn{2}{c}{20.63} &
  \multicolumn{2}{c}{46.02} \\ 
\multicolumn{1}{c}{DFNet} &
  \multicolumn{1}{c}{\textbf{0.041}} &
  \multicolumn{2}{c}{\textbf{0.054}} &
  \multicolumn{2}{c}{\textbf{0.031}} &
  \multicolumn{2}{c}{\textbf{62.74}} &
  \multicolumn{2}{c}{\textbf{83.31}} &
  \multicolumn{2}{c}{\textbf{97.33}} \\ 
\multicolumn{1}{c}{FDCT (Ours)} &
  \multicolumn{1}{c}{\textbf{0.041}} &
  \multicolumn{2}{c}{\underline{0.055}} &
  \multicolumn{2}{c}{\underline{0.032}} &
  \multicolumn{2}{c}{\underline{61.23}} &
  \multicolumn{2}{c}{\underline{82.84}} &
  \multicolumn{2}{c}{\underline{97.28}} \\ \bottomrule
\end{tabular}
%     \begin{tablenote}
%         \footnotesize
%         \item [*]Local Implicit is method aiming at sim-to-real.
%     \end{tablenote}
% \end{threeparttable}
}
%\vspace{-0.5cm}
\end{table}
% Figure environment removed
\subsection{Analysis} \label{sec:analysis}
In our proposed method, the loss function plays a crucial role in enabling the network to focus on structural information and alleviate the effects of unstable pixels. However, this focus on structural information may come at the expense of some details. On the other hand, the fusion branch and shortcuts draw attention to the details, which can introduce extra redundancy. Nonetheless, the use of maxpooling facilitates lossy and aggressive downsampling, which can reduce redundancy and improve robustness. The convolution based fusion method make better use of the raw depth image. All components work together and complement each other to achieve the best possible balance between structural information and details. In this section, we analyze the four critical components of our method and demonstrate their effectiveness.

\subsubsection{Influence of loss term}
% As we mentioned above, some unstable pixels can unwantedly make big penalty to the loss. By computing the gradient of the depth image and applying Gaussian blur, we manually created a feature to represent these pixels. As the weights of these pixels were reduced, the model's performance improved (as seen in Experiment of weight in Table \ref{tab:table5}), indicating the importance of treating pixels differently and pointing out the necessity of the so designed loss function. However, the side effect of such loss function is that the network pays too much attention to the structure and ignores some details. The highlighted area of the feature map changes from dotted to regional in the Loss column in Figure \ref{fig:figure6}.
As mentioned in \ref{section:Loss}, unstable pixels can have a significant negative influence on the calculation of the training loss. To illustrate this issue, we manually created a feature to represent these pixels by computing the gradient of the depth image and applying a Gaussian blur. By reducing the weights of these pixels, we observed an improvement in the model's performance (as seen in the Experiment of weight in Table \ref{tab:table5}), highlighting the importance of treating pixels differently and emphasizing the necessity of the used loss functions (especially the Huber Loss). Qualitatively, as shown in Figure \ref{fig:figure6}, the New Loss model places greater emphasis on the overall structure of transparent objects, as compared to DFNet, which primarily focuses on local information. The downside of such a loss function is that the network may ignore some details.
% Figure environment removed

\subsubsection{Low-level feature preservation}
% Fusion branch and cross-layer shortcuts alleviate the indistinct boundaries and perceptual details by taking more low-level cues into consideration. The highlighted area of the feature map changes from regional to scattered in the Fusion column in Figure \ref{fig:figure6}. Loss function and low-level feature awareness components together make a good trade-off between detail and structure information.
The fusion branch and cross-layer shortcuts help alleviate the issue of blurry boundaries and low perceptual details by incorporating more low-level cues. As a result, more low-level features such as object edges and holes are preserved in the feature map of Fusion model in Figure \ref{fig:figure6}. The combination of the loss function and low-level feature awareness components strikes a good balance between detail and structural information.

\subsubsection{Influence of downsampling}
Our hypothesis is that the use of max pooling as a lossy downsampling method can mitigate the side effects of the low-level awareness components while reducing the number of parameters. The results in Table \ref{tab:table5} that are noted as ``Experiment of downsampling'' support our viewpoint. It can be observed that the performance of using convolutional downsampling and average pooling is slightly worse than that of using max pooling.

% The loss function makes the network focus on structural information and alleviating the affects of unstable pixels, but may harming to the details. The fusion branch and shortcuts draws the attention to details, but may introduce extra redundancy. Maxpooling is used to lossy and aggressively downsampling. It can reduce redundancy and improve robustness. These components work together and complement each other.
% }

\subsubsection{Fusion method of depth image}
To demonstrate that fusing the raw depth image with feature map via convolution is better than directly concatenation. We removed the convolution layers used for fusion in the model Ours and named it Ours(concat). The result labeled Table ``Experiment on fusion method'' in Table \ref{tab:table5} support our viewpoint.

\begin{table}[!ht]
\centering
\caption{Experiment Result on Weight Modification, Downsampling Implementation and Fusion Method\label{tab:table5}}

\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc}
\toprule
\multicolumn{1}{c}{Model/Metric} &
  \multicolumn{1}{c}{RMSE} &
  \multicolumn{1}{c}{REL} &
  \multicolumn{1}{c}{MAE} &
  \multicolumn{1}{c}{$\delta$1.05} &
  \multicolumn{1}{c}{$\delta$1.10} &
  $\delta$1.25 \\ \midrule
\multicolumn{7}{c}{Experiment on weight} \\ \midrule
\multicolumn{1}{c}{Baseline} &
  \multicolumn{1}{c}{0.018} &
  \multicolumn{1}{c}{0.027} &
  \multicolumn{1}{c}{0.012} &
  \multicolumn{1}{c}{83.76} &
  \multicolumn{1}{c}{95.67} &
  99.71 \\ 
\multicolumn{1}{c}{Edge Weight Modified} &
  \multicolumn{1}{c}{\textbf{0.017}} &
  \multicolumn{1}{c}{\textbf{0.025}} &
  \multicolumn{1}{c}{\textbf{0.011}} &
  \multicolumn{1}{c}{\textbf{85.34}} &
  \multicolumn{1}{c}{\textbf{96.26}} &
  \textbf{99.75} \\ \midrule
\multicolumn{7}{c}{Experiment on downsampling} \\ \midrule
\multicolumn{1}{c}{Conv Down} &
  \multicolumn{1}{c}{0.016} &
  \multicolumn{1}{c}{0.023} &
  \multicolumn{1}{c}{0.011} &
  \multicolumn{1}{c}{87.16} &
  \multicolumn{1}{c}{96.83} &
  99.80 \\ 
\multicolumn{1}{c}{AvgPooling Down} &
  \multicolumn{1}{c}{0.016} &
  \multicolumn{1}{c}{0.024} &
  \multicolumn{1}{c}{0.011} &
  \multicolumn{1}{c}{87.16} &
  \multicolumn{1}{c}{96.93} &
  99.80 \\ 
\multicolumn{1}{c}{MaxPooling Down} &
  \multicolumn{1}{c}{\textbf{0.015}} &
  \multicolumn{1}{c}{\textbf{0.022}} &
  \multicolumn{1}{c}{\textbf{0.010}} &
  \multicolumn{1}{c}{\textbf{88.18}} &
  \multicolumn{1}{c}{\textbf{97.15}} &
  \textbf{99.81} \\ \midrule
  \multicolumn{7}{c}{Experiment on fusion method} \\ \midrule
  \multicolumn{1}{c}{Ours(concat)} &
  \multicolumn{1}{c}{\textbf{0.015}} &
  \multicolumn{1}{c}{0.023} &
  \multicolumn{1}{c}{0.011} &
  \multicolumn{1}{c}{87.90} &
  \multicolumn{1}{c}{96.68} &
  99.80 \\ 
\multicolumn{1}{c}{Ours} &
  \multicolumn{1}{c}{\textbf{0.015}} &
  \multicolumn{1}{c}{\textbf{0.022}} &
  \multicolumn{1}{c}{\textbf{0.010}} &
  \multicolumn{1}{c}{\textbf{88.18}} &
  \multicolumn{1}{c}{\textbf{97.15}} &
  \textbf{99.81} \\ 
\bottomrule
\end{tabular}%
}
%\vspace{-0.5cm}
\end{table}
\vspace{-0.2cm}


\subsection{Pose Estimation Experiment}
In this experiment, we aim to demonstrate the applicability of our network for downstream tasks and to show that it can improve the accuracy of pose estimate.
To evaluate the performance of pose estimation, we use three evaluation metrics, i.e, the average closest point distance (ADD-S), the area under the ADD-S curve (AUC), and the percentage of ADD-S values that are smaller than 2 \centi\meter.
%\cite{xiang2017posecnn}
% The higher the metrics the stronger the performance.

% This experiment is carried out on the set1 of ClearPose, since Clearpose has an accurate pose annotation without sticker. We use typical network DenseFusion \cite{wang2019densefusion} as pose estimation network. Following the learning strategy of DenseFusion, we train the network on 12G NVIDIA TITAN Xp GPU for 5 epochs with batch size of 128. The margin of refinement is set to 0.03. For fair comparison, we evaluate others works using their released source codes and optimal hyper-parameters or statistics reported in their paper.
Both our method and DFNet are trained on the ClearPose Set 1 and are used to predict the depth of Set 1-Scene 5 for pose estimation purposes. The depth completion result is reported in Table \ref{tab:table6} and a screenshot of the live demonstration is reported in Figure \ref{fig:figure7}. In our experiments, we use DenseFusion \cite{wang2019densefusion}  as the pose estimation method. We trained DenseFusion with the restored depth and tested it on 3,000 randomly selected images. Ideally, a more accurate depth prediction can lead to improved performance in pose estimation. The results of our evaluations, presented in Table \ref{tab:table7}, indicate that the depth restored by our method outperforms DFNet in almost every object in the pose estimation task. This results validate that the depth map given by our method is more appropriate for addressing the downstream task, i.e., pose estimation.
% Depth completion models are trained on ClearPose set 1 and predict the depth of set 1-scene 5 for pose estimation. We train DenseFusion with the restored depth and test on 3k randomly chosen images. Metrics for each object are reported in Table \ref{tab:table7}. Result shows that the depth restored by FDCT outperforms DFNet's in almost every object in pose estimation task.
% \todo{format of tablehead!!}
\begin{table}[!t]
\caption{Depth Completion Results on ClearPose dataset.}
\label{tab:table6}
\centering
\begin{tabular}{ccccccc}
\toprule
Model & RMSE           & REL            & MAE            & $\delta$1.05          & $\delta$1.10          & $\delta$1.25          \\ \midrule
DFNet        & 0.048          & 0.038          & 0.033          & 76.36          & 94.22          & \textbf{99.40} \\
Ours         & \textbf{0.045} & \textbf{0.033} & \textbf{0.028} & \textbf{82.15} & \textbf{94.43} & 99.25          \\
\bottomrule
\end{tabular}%
\end{table}



\begin{table}[!t]
\caption{Pose Estimation Results on ClearPose dataset\label{tab:table7}}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc}
\toprule
Models &
  \multicolumn{3}{c}{DFNet} &
  \multicolumn{3}{c}{Ours} \\ \midrule
Object/Metirc &
  \multicolumn{1}{c}{AUC} &
  \multicolumn{1}{c}{\textless{}2cm} &
  ADD-S(10\%) &
  \multicolumn{1}{c}{AUC} &
  \multicolumn{1}{c}{\textless{}2cm} &
  ADD-S(10\%) \\ 
beaker\_1 &
  \multicolumn{1}{c}{79.07} &
  \multicolumn{1}{c}{\textbf{0.00}} &
  0.68 &
  \multicolumn{1}{c}{\textbf{80.44}} &
  \multicolumn{1}{c}{\textbf{0.00}} &
  \textbf{7.53} \\ 
dropper\_1 &
  \multicolumn{1}{c}{\textbf{67.76}} &
  \multicolumn{1}{c}{61.00} &
  \textbf{48.00} &
  \multicolumn{1}{c}{31.70} &
  \multicolumn{1}{c}{\textbf{65.33}} &
  0.00 \\ 
dropper\_2 &
  \multicolumn{1}{c}{81.09} &
  \multicolumn{1}{c}{\textbf{33.10}} &
  1.78 &
  \multicolumn{1}{c}{\textbf{84.24}} &
  \multicolumn{1}{c}{0.00} &
  \textbf{9.61} \\ 
flask\_1 &
  \multicolumn{1}{c}{84.96} &
  \multicolumn{1}{c}{60.33} &
  42.33 &
  \multicolumn{1}{c}{\textbf{86.71}} &
  \multicolumn{1}{c}{\textbf{68.33}} &
  \textbf{68.00} \\ 
funnel\_1 &
  \multicolumn{1}{c}{78.85} &
  \multicolumn{1}{c}{91.33} &
  0.00 &
  \multicolumn{1}{c}{\textbf{82.91}} &
  \multicolumn{1}{c}{\textbf{98.33}} &
  \textbf{12.33} \\ 
cylinder\_1 &
  \multicolumn{1}{c}{78.77} &
  \multicolumn{1}{c}{48.33} &
  28.67 &
  \multicolumn{1}{c}{\textbf{79.83}} &
  \multicolumn{1}{c}{\textbf{77.00}} &
  \textbf{33.33} \\ 
cylinder\_2 &
  \multicolumn{1}{c}{62.75} &
  \multicolumn{1}{c}{54.67} &
  3.33 &
  \multicolumn{1}{c}{\textbf{75.68}} &
  \multicolumn{1}{c}{\textbf{58.67}} &
  \textbf{29.33} \\ 
pan\_1 &
  \multicolumn{1}{c}{86.76} &
  \multicolumn{1}{c}{13.67} &
  33.33 &
  \multicolumn{1}{c}{\textbf{89.37}} &
  \multicolumn{1}{c}{\textbf{53.67}} &
  \textbf{50.00} \\ 
pan\_2 &
  \multicolumn{1}{c}{88.71} &
  \multicolumn{1}{c}{84.67} &
  44.00 &
  \multicolumn{1}{c}{\textbf{89.73}} &
  \multicolumn{1}{c}{\textbf{90.33}} &
  \textbf{56.00} \\ 
pan\_3 &
  \multicolumn{1}{c}{\textbf{88.90}} &
  \multicolumn{1}{c}{87.67} &
  \textbf{53.33} &
  \multicolumn{1}{c}{88.10} &
  \multicolumn{1}{c}{\textbf{91.00}} &
  48.00 \\ 
bottle\_1 &
  \multicolumn{1}{c}{86.05} &
  \multicolumn{1}{c}{91.53} &
  24.41 &
  \multicolumn{1}{c}{\textbf{88.71}} &
  \multicolumn{1}{c}{\textbf{93.22}} &
  \textbf{31.53} \\ 
bottle\_2 &
  \multicolumn{1}{c}{71.81} &
  \multicolumn{1}{c}{83.16} &
  4.04 &
  \multicolumn{1}{c}{\textbf{77.01}} &
  \multicolumn{1}{c}{\textbf{88.22}} &
  \textbf{13.47} \\ 
stick\_1 &
  \multicolumn{1}{c}{69.53} &
  \multicolumn{1}{c}{32.32} &
  32.66 &
  \multicolumn{1}{c}{\textbf{79.60}} &
  \multicolumn{1}{c}{\textbf{57.58}} &
  \textbf{58.92} \\ 
syringe\_1 &
  \multicolumn{1}{c}{73.03} &
  \multicolumn{1}{c}{31.67} &
  25.67 &
  \multicolumn{1}{c}{\textbf{80.15}} &
  \multicolumn{1}{c}{\textbf{57.00}} &
  \textbf{47.00} \\ 
MEAN &
  \multicolumn{1}{c}{78.43} &
  \multicolumn{1}{c}{55.25} &
  24.45 &
  \multicolumn{1}{c}{\textbf{79.58}} &
  \multicolumn{1}{c}{\textbf{64.19}} &
  \textbf{33.22} \\


  \bottomrule
  \end{tabular}%
}
\vspace{-0.5cm}
\end{table}