% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{hu2021penet}
M.~Hu, S.~Wang, B.~Li, S.~Ning, L.~Fan, and X.~Gong, ``Penet: Towards precise
  and efficient image guided depth completion,'' in \emph{2021 IEEE
  International Conference on Robotics and Automation (ICRA)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2021, pp. 13\,656--13\,662.

\bibitem{nazir2022semattnet}
D.~Nazir, A.~Pagani, M.~Liwicki, D.~Stricker, and M.~Z. Afzal, ``Semattnet:
  Towards attention-based semantic aware guided depth completion,'' \emph{IEEE
  Access}, 2022.

\bibitem{yan2022rignet}
Z.~Yan, K.~Wang, X.~Li, Z.~Zhang, J.~Li, and J.~Yang, ``Rignet: Repetitive
  image guided network for depth completion,'' in \emph{European Conference on
  Computer Vision}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2022, pp.
  214--230.

\bibitem{ichnowski2021dex}
J.~Ichnowski, Y.~Avigal, J.~Kerr, and K.~Goldberg, ``Dex-nerf: Using a neural
  radiance field to grasp transparent objects,'' \emph{arXiv preprint
  arXiv:2110.14217}, 2021.

\bibitem{zhang2018deep}
Y.~Zhang and T.~Funkhouser, ``Deep depth completion of a single rgb-d image,''
  in \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, 2018, pp. 175--185.

\bibitem{jaritz2018sparse}
M.~Jaritz, R.~De~Charette, E.~Wirbel, X.~Perrotton, and F.~Nashashibi, ``Sparse
  and dense data with cnns: Depth completion and semantic segmentation,'' in
  \emph{2018 International Conference on 3D Vision (3DV)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2018, pp. 52--60.

\bibitem{sajjan2020clear}
S.~Sajjan, M.~Moore, M.~Pan, G.~Nagaraja, J.~Lee, A.~Zeng, and S.~Song, ``Clear
  grasp: 3d shape estimation of transparent objects for manipulation,'' in
  \emph{2020 IEEE International Conference on Robotics and Automation
  (ICRA)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2020, pp. 3634--3642.

\bibitem{tang2021depthgrasp}
Y.~Tang, J.~Chen, Z.~Yang, Z.~Lin, Q.~Li, and W.~Liu, ``Depthgrasp: Depth
  completion of transparent objects using self-attentive adversarial network
  with spectral residual for grasping,'' in \emph{2021 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2021, pp. 5710--5716.

\bibitem{fang2022transcg}
H.~Fang, H.-S. Fang, S.~Xu, and C.~Lu, ``Transcg: A large-scale real-world
  dataset for transparent object depth completion and grasping,'' \emph{arXiv
  preprint arXiv:2202.08471}, 2022.

\bibitem{tanaka2016recovering}
K.~Tanaka, Y.~Mukaigawa, H.~Kubo, Y.~Matsushita, and Y.~Yagi, ``Recovering
  transparent shape from time-of-flight distortion,'' in \emph{Proceedings of
  the IEEE Conference on Computer Vision and Pattern Recognition}, 2016, pp.
  4387--4395.

\bibitem{zhu2021rgb}
L.~Zhu, A.~Mousavian, Y.~Xiang, H.~Mazhar, J.~van Eenbergen, S.~Debnath, and
  D.~Fox, ``Rgb-d local implicit function for depth completion of transparent
  objects,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2021, pp. 4649--4658.

\bibitem{zuo2016explicit}
Y.~Zuo, Q.~Wu, J.~Zhang, and P.~An, ``Explicit edge inconsistency evaluation
  model for color-guided depth map enhancement,'' \emph{IEEE Transactions on
  Circuits and Systems for Video Technology}, vol.~28, no.~2, pp. 439--453,
  2016.

\bibitem{tao2021dilated}
Z.~Tao, P.~Shuguo, Z.~Hui, and S.~Yingchun, ``Dilated u-block for lightweight
  indoor depth completion with sobel edge,'' \emph{IEEE Signal Processing
  Letters}, vol.~28, pp. 1615--1619, 2021.

\bibitem{huang2019indoor}
Y.-K. Huang, T.-H. Wu, Y.-C. Liu, and W.~H. Hsu, ``Indoor depth completion with
  boundary consistency and self-attention,'' in \emph{Proceedings of the
  IEEE/CVF International Conference on Computer Vision Workshops}, 2019, pp.
  0--0.

\bibitem{ying2019overview}
X.~Ying, ``An overview of overfitting and its solutions,'' in \emph{Journal of
  physics: Conference series}, vol. 1168.\hskip 1em plus 0.5em minus
  0.4em\relax IOP Publishing, 2019, p. 022022.

\bibitem{eigen2015predicting}
D.~Eigen and R.~Fergus, ``Predicting depth, surface normals and semantic labels
  with a common multi-scale convolutional architecture,'' in \emph{Proceedings
  of the IEEE international conference on computer vision}, 2015, pp.
  2650--2658.

\bibitem{chen2016single}
W.~Chen, Z.~Fu, D.~Yang, and J.~Deng, ``Single-image depth perception in the
  wild,'' \emph{Advances in neural information processing systems}, vol.~29,
  2016.

\bibitem{xu2021seeing}
H.~Xu, Y.~R. Wang, S.~Eppel, A.~Aspuru-Guzik, F.~Shkurti, and A.~Garg, ``Seeing
  glass: Joint point cloud and depth completion for transparent objects,''
  \emph{arXiv preprint arXiv:2110.00087}, 2021.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected
  convolutional networks,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition}, 2017, pp. 4700--4708.

\bibitem{lysenkov2013recognition}
I.~Lysenkov, V.~Eruhimov, and G.~Bradski, ``Recognition and pose estimation of
  rigid transparent objects with a kinect sensor,'' \emph{Robotics}, vol. 273,
  no. 273-280, p.~2, 2013.

\bibitem{lysenkov2013pose}
I.~Lysenkov and V.~Rabaud, ``Pose estimation of rigid transparent objects in
  transparent clutter,'' in \emph{2013 IEEE International Conference on
  Robotics and Automation}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2013,
  pp. 162--169.

\bibitem{guo2019transparent}
C.~Guo-Hua, W.~Jun-Yi, and Z.~Ai-Jun, ``Transparent object detection and
  location based on rgb-d camera,'' \emph{Journal of Physics: Conference
  Series}, vol. 1183, no.~1, p. 012011, 2019.

\bibitem{liu2020keypose}
X.~Liu, R.~Jonschkowski, A.~Angelova, and K.~Konolige, ``Keypose: Multi-view 3d
  labeling and keypoint estimation for transparent objects,'' in
  \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern
  recognition}, 2020, pp. 11\,602--11\,610.

\bibitem{chang2021ghostpose}
J.~Chang, M.~Kim, S.~Kang, H.~Han, S.~Hong, K.~Jang, and S.~Kang, ``Ghostpose*:
  Multi-view pose estimation of transparent objects for robot hand grasping,''
  in \emph{2021 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2021, pp.
  5749--5755.

\bibitem{xu20206dof}
C.~Xu, J.~Chen, M.~Yao, J.~Zhou, L.~Zhang, and Y.~Liu, ``6dof pose estimation
  of transparent object from a single rgb-d image,'' \emph{Sensors}, vol.~20,
  no.~23, p. 6790, 2020.

\bibitem{chen2022clearpose}
X.~Chen, H.~Zhang, Z.~Yu, A.~Opipari, and O.~C. Jenkins, ``Clearpose:
  Large-scale transparent object dataset and benchmark,'' \emph{arXiv preprint
  arXiv:2203.03890}, 2022.

\bibitem{chen2022progresslabeller}
X.~Chen, H.~Zhang, Z.~Yu, S.~Lewis, and O.~C. Jenkins, ``Progresslabeller:
  Visual data stream annotation for training object-centric 3d perception,''
  \emph{arXiv preprint arXiv:2203.00283}, 2022.

\bibitem{zhou2019glassloc}
Z.~Zhou, T.~Pan, S.~Wu, H.~Chang, and O.~C. Jenkins, ``Glassloc: plenoptic
  grasp pose detection in transparent clutter,'' in \emph{2019 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 4776--4783.

\bibitem{mathai20203d}
A.~Mathai, N.~Guo, D.~Liu, and X.~Wang, ``3d transparent object detection and
  reconstruction based on passive mode single-pixel imaging,'' \emph{Sensors},
  vol.~20, no.~15, p. 4211, 2020.

\bibitem{grammatikopoulou2019three}
M.~Grammatikopoulou and G.-Z. Yang, ``Three-dimensional pose estimation of
  optically transparent microrobots,'' \emph{IEEE Robotics and Automation
  Letters}, vol.~5, no.~1, pp. 72--79, 2019.

\bibitem{kalra2020deep}
A.~Kalra, V.~Taamazyan, S.~K. Rao, K.~Venkataraman, R.~Raskar, and A.~Kadambi,
  ``Deep polarization cues for transparent object segmentation,'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2020, pp. 8602--8611.

\bibitem{zhou2020lit}
Z.~Zhou, X.~Chen, and O.~C. Jenkins, ``Lit: Light-field inference of
  transparency for refractive object localization,'' \emph{IEEE Robotics and
  Automation Letters}, vol.~5, no.~3, pp. 4548--4555, 2020.

\bibitem{mildenhall2021nerf}
B.~Mildenhall, P.~P. Srinivasan, M.~Tancik, J.~T. Barron, R.~Ramamoorthi, and
  R.~Ng, ``Nerf: Representing scenes as neural radiance fields for view
  synthesis,'' \emph{Communications of the ACM}, vol.~65, no.~1, pp. 99--106,
  2021.

\bibitem{ronneberger2015u}
O.~Ronneberger, P.~Fischer, and T.~Brox, ``U-net: Convolutional networks for
  biomedical image segmentation,'' in \emph{International Conference on Medical
  image computing and computer-assisted intervention}.\hskip 1em plus 0.5em
  minus 0.4em\relax Springer, 2015, pp. 234--241.

\bibitem{olah2017feature}
C.~Olah, A.~Mordvintsev, and L.~Schubert, ``Feature visualization: How neural
  networks build up their understanding of images. distill,'' 2017.

\bibitem{peng2021dgfau}
D.~Peng, X.~Yu, W.~Peng, and J.~Lu, ``Dgfau-net: Global feature attention
  upsampling network for medical image segmentation,'' \emph{Neural Computing
  and Applications}, vol.~33, no.~18, pp. 12\,023--12\,037, 2021.

\bibitem{lee2019energy}
Y.~Lee, J.-w. Hwang, S.~Lee, Y.~Bae, and J.~Park, ``An energy and
  gpu-computation efficient backbone network for real-time object detection,''
  in \emph{Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition workshops}, 2019, pp. 0--0.

\bibitem{shi2016real}
W.~Shi, J.~Caballero, F.~Husz{\'a}r, J.~Totz, A.~P. Aitken, R.~Bishop,
  D.~Rueckert, and Z.~Wang, ``Real-time single image and video super-resolution
  using an efficient sub-pixel convolutional neural network,'' in
  \emph{Proceedings of the IEEE conference on computer vision and pattern
  recognition}, 2016, pp. 1874--1883.

\bibitem{hou2021divide}
J.~Hou, Y.~Zhang, Q.~Zhong, D.~Xie, S.~Pu, and H.~Zhou, ``Divide-and-assemble:
  Learning block-wise memory for unsupervised anomaly detection,'' in
  \emph{Proceedings of the IEEE/CVF International Conference on Computer
  Vision}, 2021, pp. 8791--8800.

\bibitem{wang2004image}
Z.~Wang, A.~C. Bovik, H.~R. Sheikh, and E.~P. Simoncelli, ``Image quality
  assessment: from error visibility to structural similarity,'' \emph{IEEE
  transactions on image processing}, vol.~13, no.~4, pp. 600--612, 2004.

\bibitem{zhou2021pr}
G.~Zhou, H.~Wang, J.~Chen, and D.~Huang, ``Pr-gcn: A deep graph convolutional
  network with point refinement for 6d pose estimation,'' in \emph{Proceedings
  of the IEEE/CVF International Conference on Computer Vision}, 2021, pp.
  2793--2802.

\bibitem{wang2019densefusion}
C.~Wang, D.~Xu, Y.~Zhu, R.~Mart{\'\i}n-Mart{\'\i}n, C.~Lu, L.~Fei-Fei, and
  S.~Savarese, ``Densefusion: 6d object pose estimation by iterative dense
  fusion,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, 2019, pp. 3343--3352.

\end{thebibliography}
