% \documentclass[lettersize,journal]{IEEEtran}
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}
\overrideIEEEmargins   
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{cite}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\usepackage{indentfirst}
\usepackage{multirow}
\IEEEoverridecommandlockouts 
\usepackage[margincaption,outercaption,ragged,wide]{sidecap}
\sidecaptionvpos{figure}{t} 
\sidecaptionvpos{table}{t}
\usepackage[amssymb]{SIunits}
\usepackage{booktabs}
%\usepackage[misc,geometry]{ifsym}
\setlength {\marginparwidth }{1.35cm}
\usepackage[colorinlistoftodos,prependcaption,textsize=tiny]{todonotes}
\usepackage{marvosym}
\usepackage{ifsym}
% \usepackage{subfigure}
% \usepackage{subcaption}
\usepackage{caption}
% \usepackage[letterpaper,top=57pt,bottom=43pt,left=48pt,right=48pt]{geometry}
\usepackage{threeparttable}
% \usepackage{tablefootnote}
%  for type3 font
% \usepackage[T1]{fontenc}
% \usepackage{aecompl}
\newcommand{\fref}[1]{Fig. \ref{#1}}
\newcommand{\sref}[1]{Section \ref{#1}}
\newcommand{\tref}[1]{Table \ref{#1}}
% \usepackage{lineno}
% % \linenumbers
% \switchlinenumbers
% \pagewiselinenumbers

\title{\LARGE \bf FDCT: Fast Depth Completion for Transparent Objects}
\author{Tianan Li$^{1}$, Zhehan Chen$^{1,\textrm{\Letter}}$,~\IEEEmembership{Member,~IEEE}, Huan Liu$^{2}$,~\IEEEmembership{Member,~IEEE}, and Chen Wang$^{3}$
\thanks{$^{\textrm{\Letter}}$Corresponding author: {\tt chenzh\_ustb@163.com}}% <-this % stops a space
\thanks{$^{1}$The authors are with the School of Mechanical Engineering, University of Science and Technology Beijing, Beijing 100083, China.}% <-this % stops a space
\thanks{$^{2}$Huan Liu is with the Department of Electrical and Computer Engineering, McMaster University, Canada. Email: {\tt liuh127@mcmaster.ca}}
\thanks{$^{3}$Chen Wang is with the Spatial AI \& Robotics Lab at The Department of Computer Science and Engineering, State University of New York at Buffalo, NY 14260, USA. {\tt\small chenwang@dr.com}}% <-this % stops a space
%\thanks{Manuscript received April 19, 2021; revised August 16, 2021.}
}

% The paper headers
\markboth{IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. X, NO. X, XXX 2022 }%
{Li \MakeLowercase{\textit{et al.}}: FDCT: Fast Depth Completion for Transparent Objects}

\usepackage{fancyhdr}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\usepackage{setspace}

\begin{document}

\maketitle
\thispagestyle{fancy}

\setlength{\footskip}{5mm}
\fancyhead{}
\lhead{}
\lfoot{
\small
% 两倍行距的文字
\copyright~2023 IEEE.  Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.}
\cfoot{}
\rfoot{}
\begin{abstract}
Depth completion is crucial for many robotic tasks such as autonomous driving, 3-D reconstruction, and manipulation.
Despite the significant progress, existing methods remain computationally intensive and often fail to meet the real-time requirements of low-power robotic platforms.
Additionally, most methods are designed for opaque objects and struggle with transparent objects due to the special properties of reflection and refraction.
To address these challenges, we propose a Fast Depth Completion framework for Transparent objects (FDCT), which also benefits downstream tasks like object pose estimation.
To leverage local information and avoid overfitting issues when integrating it with global information, we design a new fusion branch and shortcuts to exploit low-level features and a loss function to suppress overfitting.
This results in an accurate and user-friendly depth rectification framework which can recover dense depth estimation from RGB-D image alone.
Extensive experiments demonstrate that FDCT can run about 70 FPS with a higher accuracy than the state-of-the-art methods.
We also demonstrate that FDCT can improve pose estimation in object grasping tasks.
To benefit to the robotics community, we release the source code at \url{https://github.com/Nonmy/FDCT}.
\end{abstract}

% \begin{IEEEkeywords}
% Deep Learning for Visual Perception, deep learning in grasping and manipulation.
% \end{IEEEkeywords}

\input{Revise_Version/introduction}
\input{Revise_Version/related_work.tex}
\input{Revise_Version/method.tex}
\input{Revise_Version/experiment.tex}
\input{Revise_Version/limitation_conclusion.tex}


\section*{Acknowledgments}
The authors thank the supports by the National Key R\&D Program of
China: [grant number 2020YFF0304004] and the Fundamental
Research Funds for the Central Universities: [grant
number FRF-TP-20-009A3].


\bibliographystyle{IEEEtran}
\bibliography{ref}

\end{document}
