{
  "title": "FDCT: Fast Depth Completion for Transparent Objects",
  "authors": [
    "Tianan Li",
    "Zhehan Chen",
    "Huan Liu",
    "Chen Wang"
  ],
  "submission_date": "2023-07-23T09:34:13+00:00",
  "revised_dates": [
    "2023-08-01T00:07:58+00:00"
  ],
  "abstract": "Depth completion is crucial for many robotic tasks such as autonomous driving, 3-D reconstruction, and manipulation. Despite the significant progress, existing methods remain computationally intensive and often fail to meet the real-time requirements of low-power robotic platforms. Additionally, most methods are designed for opaque objects and struggle with transparent objects due to the special properties of reflection and refraction. To address these challenges, we propose a Fast Depth Completion framework for Transparent objects (FDCT), which also benefits downstream tasks like object pose estimation. To leverage local information and avoid overfitting issues when integrating it with global information, we design a new fusion branch and shortcuts to exploit low-level features and a loss function to suppress overfitting. This results in an accurate and user-friendly depth rectification framework which can recover dense depth estimation from RGB-D images alone. Extensive experiments demonstrate that FDCT can run about 70 FPS with a higher accuracy than the state-of-the-art methods. We also demonstrate that FDCT can improve pose estimation in object grasping tasks. The source code is available at https://github.com/Nonmy/FDCT",
  "categories": [
    "cs.CV"
  ],
  "primary_category": "cs.CV",
  "doi": "10.1109/LRA.2023.3300544",
  "journal_ref": "IEEE Robotics and Automation Letters (RA-L), 2023",
  "arxiv_id": "2307.12274",
  "pdf_url": null,
  "comment": "9pages,7figures",
  "num_versions": null,
  "size_before_bytes": 6440340,
  "size_after_bytes": 626176
}