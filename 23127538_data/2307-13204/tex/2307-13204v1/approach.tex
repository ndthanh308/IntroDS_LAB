\section{Problem Formulation}\label{prob_def}
% We assume access to a set of object classes $\mathcal{C} = \{c_i\}_{i=1}^{K_c}$ and a set of tasks $\mathcal{T} = \{t_j\}_{j=1}^{K_t}$, where $K_c$ and $K_t$ are numbers of object classes and tasks, respectively. We consider the problem of predicting task-oriented grasp poses of a parallel-jaw gripper given the partial point cloud of an object $X_o \in \mathbb{R}^{N \times 3}$ and a language instruction $I$ specifying an object class $c_i$ and a task $t_j$  (e.g., ``\textit{Grasp the \textbf{knife} to \textbf{cut}}"), where $N$ is the number of points, and $c_i$ and $t_j$ may be novel concepts to the robot (i.e., out of the training set). To incorporate semantic knowledge about object classes and tasks, we prompt an LLM to generate a set of object class description paragraphs $L_{\mathcal{C}, i}$ for $c_i$ and a set of task description paragraphs $L_{\mathcal{T}, j}$ for $t_j$, where $L_\mathcal{C}$ and $L_\mathcal{T}$ are all possible sets of object class and task description paragraphs, respectively.

We assume access to an object class set $\mathcal{C} = \{c_i\}_{i=1}^{K_c}$ and a task set $\mathcal{T} = \{t_j\}_{j=1}^{K_t}$, where $K_c$ and $K_t$ are numbers of object classes and tasks, respectively. Based on $\mathcal{C}$ and $\mathcal{T}$, we consider the problem of learning task-oriented grasp pose prediction for a parallel-jaw gripper given the partial point cloud of an object $X_o \in \mathbb{R}^{N \times 3}$ and a natural language instruction $I$ specifying an object class $c$ and a task $t$, where $N$ is the number of points. During training, we have $c  = c_i \in \mathcal{C}$ and $t = t_j \in \mathcal{T}$. The challenge for real-world robotic applications is that $c$ or $t$ can be novel concepts (i.e., out of the training set) from open-world concept sets $\mathcal{C}_{ow}$ and $\mathcal{T}_{ow}$ during inference, where $\mathcal{C} \subset \mathcal{C}_{ow}$ and $\mathcal{T} \subset \mathcal{T}_{ow}$. To enable the generalization of TOG skills from known to novel concepts, GraspGPT incorporates open-end semantic knowledge by prompting an LLM to generate a set of object class description paragraphs $L_{c}$ for $c$ and a set of task description paragraphs $L_{t}$ for $t$.

Mathematically, we aim to estimate the posterior distribution $P(G|X_o, I, L_c, L_t)$, where $G$ represents the space of all task-oriented grasp poses. Following the convention in prior work \cite{murali2021same, mousavian20196}, the estimation process is factorized into two steps: (1) task-agnostic grasp sampling $P(G | X_o)$ and (2) task-oriented grasp evaluation $P(S|X_o, I, L_c, L_t, g)$, where $S$ is the score (probability of success) for each $g \in G$. Each grasp pose $g$ is represented by $(R, T) \in SE(3)$, where $R \in SO(3)$ represents the 3D orientation and $T \in \mathbb{R}^3$ represents the 3D translation. Since the first step is well-studied by previous works, we directly apply off-the-shelf task-agnostic grasp sampler from \cite{sundermeyer2021contact} to obtain a set of grasp pose candidates, and focus on solving the second step.

% from an LLM, \textcolor{red}{DESCRIPTION PARAGRPH SET!!!}the robot prompts the LLM to acquire language descriptions $\mathcal{D}$ of the concept(s). In this letter, we define two types of descriptions: object class description $\mathcal{OD}$ and task description $\mathcal{TD}$. 


% \textcolor{red}{Following the convention in prior work \cite{murali2021same, mousavian20196}, the estimation process is factorized into two steps: (1) task-agnostic grasp sampling $P(G | X_o)$ and (2) task-oriented grasp evaluation $P(S|X_o, I, L_{\mathcal{C}}, L_{\mathcal{T}}, g)$, where $S$ is the probability of success for each $g \in G$. Since the first step is well-studied by previous works, we directly apply off-the-shelf task-agnostic grasp sampler from \cite{ten2018using} to obtain a set of grasp pose candidates, and focus on solving the second step.}

% Figure environment removed 

\section{GraspGPT} \label{approach}

\subsection{Overview}
An overview of the proposed GraspGPT framework is presented in Figure \ref{fig:pipeline} (a). We begin by outlining the methodology for data generation in Section \ref{data+llm}. We then detail the strategy for obtaining feature representations of multi-modal inputs in Section \ref{feat}. Finally, we describe how to perform task-oriented grasp evaluation in Section \ref{tog}. 


\subsection{Data Generation with an LLM}\label{data+llm}
The data generation pipeline is designed based on TaskGrasp dataset that comprises $K_c$ household object classes and $K_t$ everyday tasks. To construct the Language Augmented TaskGrasp (LA-TaskGrasp) dataset, we employ an LLM to generate language descriptions for each object class $c_i$ and task $t_j$, which we will describe first. We then present the procedure for generating language instructions. \\

% , involving language description generation and language instruction generation, 

% \begin{table*}[th]
% \centering
% \renewcommand\arraystretch{1.1}
% \setlength\tabcolsep{10pt}%调列距
% \begin{tabular}{ccc}
% \hline
% \textbf{Class} & \textbf{Property}                                                                                                                                                                                  & \textbf{Similarity}                                                                                                                                                                                                              \\ \hline \specialrule{0em}{3pt}{3pt}
% Spoon & \begin{tabular}[c]{@{}c@{}} \textit{``A spoon is a unit of measurement for dry and} \\ \textit{liquid ingredients, and is used for serving,} \\ \textit{measuring, and mixing foods."}\end{tabular}                      & \begin{tabular}[c]{@{}c@{}}\textit{A spoon is a utensil commonly used for measuring, stirring, and} \\ \textit{serving food. Household objects similar in function include a} \\ \textit{measuring cup, cereal bowl, and soup ladle."}\end{tabular} \\ \specialrule{0em}{3pt}{3pt}
% Mug   & \begin{tabular}[c]{@{}c@{}}\textit{``The mug is cylindrical in shape, with a slightly rounded} \\ \textit{base leading up to straight walls which eventually taper} \\ \textit{slightly towards the rim."}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{``Mugs typically have a cylindrical shape with a slightly tapered} \\ \textit{``top and a curved handle; objects with similar shapes include} \\ \textit{bottles and vases."}\end{tabular}                           \\ \specialrule{0em}{3pt}{3pt} \hline
% \textbf{Task}  & \textbf{Affordance}                                                                                                                                                                                & \textbf{Relevance}                                                                                                                                                                                                               \\ \hline \specialrule{0em}{3pt}{3pt}
% Sweep & \begin{tabular}[c]{@{}c@{}}\textit{``Household objects that can be used to sweep include} \\ \textit{brooms, dustpans and mops."}\end{tabular}                                                                 & \begin{tabular}[c]{@{}c@{}}\textit{``Verbs that are semantically close to sweep include} \\ \textit{cleanse, purify, and eradicate."}\end{tabular}                                                                                           \\ \specialrule{0em}{3pt}{3pt}
% Slice & \begin{tabular}[c]{@{}c@{}}\textit{``Household objects that can be used to slice include} \\ \textit{knives, graters, mandolines, and vegetable peelers."}\end{tabular}                                        & \begin{tabular}[c]{@{}c@{}}\textit{``Verbs such as cleave, cut, and divide can also achieve} \\ \textit{the same effect as slicing an object."}\end{tabular}                                                                                 \\ \specialrule{0em}{3pt}{3pt} \hline
% \end{tabular}
% \caption{Examples of object class and task descriptions from an LLM.}
% \label{tab:desc_examples}
% \end{table*}

\begin{table*}[th]
\centering
\renewcommand\arraystretch{1.3}
\setlength\tabcolsep{10pt}%调列距
  \vspace*{-0.1in}
\begin{tabular}{ccc}
\hline
\textbf{Class} & \textbf{Property Description}                                                                                                                                                                                  & \textbf{Similarity Description}                                                                                                                                                                                                              \\ \hline \specialrule{0em}{3pt}{3pt}
% Spoon & \begin{tabular}[c]{@{}c@{}} \textit{``A spoon is a unit of measurement for dry and} \\ \textit{liquid ingredients, and is used for serving,} \\ \textit{measuring, and mixing foods."}\end{tabular}                      & \begin{tabular}[c]{@{}c@{}}\textit{A spoon is a utensil commonly used for measuring, stirring, and} \\ \textit{serving food. Household objects similar in function include a} \\ \textit{measuring cup, cereal bowl, and soup ladle."}\end{tabular} \\ \specialrule{0em}{3pt}{3pt}
\textit{Mug}   & \begin{tabular}[c]{@{}c@{}}\textit{``The mug is cylindrical in shape, with a slightly rounded} \\ \textit{base leading up to straight walls which eventually taper} \\ \textit{slightly towards the rim."}\end{tabular} & \begin{tabular}[c]{@{}c@{}}\textit{``Mugs typically have a cylindrical shape with a slightly tapered} \\ \textit{``top and a curved handle; objects with similar shapes include} \\ \textit{bottles and vases."}\end{tabular}                           \\ \specialrule{0em}{3pt}{3pt} \hline
\textbf{Task}  & \textbf{Affordance Description}                                                                                                                                                                                & \textbf{Relevance Description}                                                                                                                                                                                                               \\ \hline \specialrule{0em}{3pt}{3pt}
\textit{Sweep} & \begin{tabular}[c]{@{}c@{}}\textit{``Household objects that can be used to sweep include} \\ \textit{brooms, dustpans and mops."}\end{tabular}                                                                 & \begin{tabular}[c]{@{}c@{}}\textit{``Verbs that are semantically close to sweep include} \\ \textit{cleane, purify, and eradicate."}\end{tabular}                                                                                           \\ \specialrule{0em}{3pt}{3pt} \hline
% Slice & \begin{tabular}[c]{@{}c@{}}\textit{``Household objects that can be used to slice include} \\ \textit{knives, graters, mandolines, and vegetable peelers."}\end{tabular}                                        & \begin{tabular}[c]{@{}c@{}}\textit{``Verbs such as cleave, cut, and divide can also achieve} \\ \textit{the same effect as slicing an object."}\end{tabular}                                                                                 \\ \specialrule{0em}{3pt}{3pt} \hline
\end{tabular}

\caption{examples of object class and task descriptions in LA-TaskGrasp dataset.}
  \vspace*{-0.3in}
\label{tab:desc_examples}
\end{table*}

% , enabling the generalization of learned TOG skills to novel object classes or tasks out of the training set.
\noindent \textbf{Language Description Generation} \ The key idea behind GraspGPT is to leverage the language descriptions from an LLM to establish connections between novel and known concepts. According to Rosch's theory \cite{rosch1975cognitive} of cognitive representations of semantic categories, a concept shares similar geometry, function, or effect descriptions with its related concepts. Inspired by \cite{murali2021same}, the similarity between concepts can be described by 
(1) directly prompting the LLM (e.g., \textit{``Describe what verbs are similar to cut:"}) and (2) comparing the descriptions of two concepts (e.g., \textit{``Describe the geometry of a cup/bowl:"}). In essence, the ability to relate concepts in this way is guaranteed by the fact that LLMs are trained on massive amounts of internet-scale data, enabling them to capture a broad range of linguistic patterns and semantic information.

% \textcolor{red}{Table \ref{tab:prompt_tab} presents two sets of prompts for object class and task descriptions.} 
% As mentioned earlier, we define two sets of description paragraphs: object class description set $L_{\mathcal{C}}$ and task description set $L_{\mathcal{T}}$. 
% establishes connections between the target object class and other relevant classes. 

To obtain $L_{c_i}$ for object class $c_i$, we design two prompt sets: (1) property prompt set, each of which asks a property of $c_i$, and (2) similarity prompt set, each of which asks classes sharing a similar property with $c_i$. Here, properties can be shape, geometry, function, etc. Similarly, to obtain $L_{t_j}$ for task $t_j$, we design: (1) affordance prompt set, each of which asks object classes that afford $t_j$, and (2) relevance prompt set, each of which asks semantically or physically relevant tasks to $t_j$. For each prompt set, we equally define $N_p$ prompts. To generate language descriptions about concepts, we recursively query the LLM to generate $N_a$ different answers per prompt. Examples of generated language descriptions are presented in Table \ref{tab:desc_examples}. We then orderly combine answers from each prompt to obtain complete description paragraphs of a concept, resulting in ${N_a}^{2N_p}$ description paragraphs. However, we have empirically observed that using only a subset of these paragraphs is sufficient for training due to the information redundancy. Each paragraph has an approximate length of 4-6 sentences. A complete list of prompts used to construct LA-TaskGrasp dataset and more examples of language descriptions can be found in the appendix. GraspGPT is not restricted to concepts defined in LA-TaskGrasp dataset, as it can incorporate open-end semantic knowledge from an LLM about any concept. It is a primary advantage over existing methods.\\

% \textit{[obj]} and \textit{[task]} are substituted with $\mathcal{C}$ and $\mathcal{T}$ when prompting the LLM. \textcolor{blue}{Please refer to the supplementary material for a full list of prompts.}

% \textcolor{red}{To resolve the ambiguity in object classes and tasks defined by TaskGrasp (e.g., object class \textit{control} and \textit{server}), WORDNET XXXXX.}

\noindent \textbf{Language Instruction Generation} \ To efficiently generate language instructions during each training loop, we employ a template-based generation strategy. Following our prior work \cite{tang2023task}, we begin with $M$ templates from \cite{nguyen2020robot}, such as \textit{``Use the [obj] to [task]"}. Each template requires an object class label $c_i$ and a task label $t_j$. To further enrich the vocabulary and grammatical diversities, we perform template augmentation using an LLM (e.g., \textit{``rewrite the following sentence in a different grammatical format:"}) to generate $M^+$ additional templates, such as \textit{``hold the [obj] in your hand and [task]"} and \textit{``grip the [obj] in a [task]ing-friendly manner"}. For a complete list of $M+M^+$ templates used in LA-TaskGrasp dataset, please refer to the appendix. During each training loop, we randomly sample $c_i$ and $t_j$ from $\mathcal{C}$ and $\mathcal{T}$, and select a template to generate a natural language instruction without human effort. 
% \noindent \textbf{LA-TaskGrasp Dataset} \ xxxx
    
\subsection{Multi-Modal Feature Representation}\label{feat}
To incorporate semantic knowledge about concepts in the form of language descriptions into GraspGPT framework, we transform them along with other sensory inputs into their feature representations. Therefore, two encoders are introduced: one for embedding point cloud data and the other for embedding language data. \\

\noindent \textbf{Object and Grasp Encoder} \ To model the relative spatial relationship between a 6 DoF (Degree of Freedom) grasp pose and $X_o$, we adopt a joint embedding strategy. Following Mousavian et al. \cite{mousavian20196}, we first approximate the robot gripper with six control points $X_g$ defined in the object frame and concatenate them to the object point cloud $X_o$ to form a joint point cloud. A binary feature vector is then added to the joint point cloud, indicating that each point belongs to the object or the gripper. Finally, the joint point cloud is encoded with PointNet++\cite{qi2017pointnet++}, which consists of three set abstraction layers:
\begin{align*}
    F_X = \textnormal{PN++}(\textnormal{Concat}([X_g, X_o], \textnormal{dim}=0))
\end{align*}
The resulting point cloud embedding $F_X \in \mathbb{R}^{1024}$ is later fused with language embeddings. \\

% Other point cloud encoders, such as PCT\cite{guo2021pct} is also applicable here.\\

% : Our input observations are object point clouds and we want to reason
% about SE(3) grasps. Qi et al. [42] proposed the PointNet++ architecture to efficiently represent 3D
% data which we use to learn a representation for the object point cloud and 6-DOF grasp poses. The
% grasp g is defined in the object frame and six control points are selected on the gripper to form a
% gripper point cloud Xg. Similar to Mousavian et al. [7], Xg is concatenated with the object point
% cloud X with an extra latent indicator vector to represent whether a point is part of the gripper or the
% object. The PointNet layer reasons about the relative spatial information between the grasp and the
% object. It outputs a embedding which is used initialize the grasp node (orange in Fig 3) in the graph.

\noindent \textbf{Language Encoder} \ In order to relate novel and known concepts, GraspGPT necessitates the ability to digest a large variety of linguistic elements in language descriptions. For instance, in the case of an affordance description of the task \textit{``pour"}:
\begin{center}

``\textit{Household objects that support the function of pouring include utensils such as pitchers, cups, and ladles, as well as containers with pouring spouts, to aid in the transfer of liquid or other items from one vessel to another.}"
    
\end{center}
GraspGPT must be able to comprehend object classes (e.g., \textit{pitchers}, \textit{cups}, \textit{ladles}), entity taxonomy (e.g., \textit{utensil}, \textit{container}, \textit{vessel}, \textit{liquid}), actions (e.g., \textit{transfer}), and relations (e.g., \textit{from ... to ...}). While training a dedicated language encoder from scratch is a common choice, it would require a significant amount of training data and is meanwhile time-consuming. We, therefore, opt for a BERT\cite{devlin2018bert} pre-trained on a large corpus of text data to encode both language descriptions and instructions. The pre-trained BERT outputs word embeddings for a task description paragraph $F_{td} \in \mathbb{R}^{T_{td} \times 768}$, an object class description paragraph $F_{od} \in \mathbb{R}^{T_{od} \times 768}$, and a language instruction $F_{I} \in \mathbb{R}^{T_{I} \times 768}$, where $T_{td}$, $T_{od}$, and $T_{I}$ denote the maximum lengths (with zero-padding) for each language sequence, respectively. The language encoder is frozen during training. 

% With the collected language descriptions, we now incorporate them into policy training. One common choice is to train a language module (e.g., long short-term memory (LSTM) [42]) from scratch
% to embed features of the language input, which can take substantial time and effort to tune. Instead,
% we use a pre-trained LLM to distill the language descriptions into feature representations. Since
% LLMs are trained with vast amounts of data, they can better interpret and generalize to the diverse set
% of long descriptions. Since the GPT-3 model is not publicly available, we opt for the Google BERTBase [43] model on HuggingFace, which has 110.1M parameters and outputs a 768-dimensional
% vector representation for each description input. T-SNE analysis shown in Fig. 3 demonstrates that,
% without any fine-tuning, the model already captures semantic meanings of the descriptions among
% tools (e.g., hammer and mallet are close to each other).
% Figure 3: T-SNE results of BERT output for
% multiple language descriptions of each tool.
% Fig. 2 shows the overall neural network architecture. We
% first prompt GPT-3 to obtain a set Li of text descriptions for tool τi via the procedure in Sec. 4.1. During
% meta-training, we randomly sample lij from Li for each
% episode to ensure that the policy sees a diverse set of descriptions. We then freeze the BERT model during policy
% training. The output from BERT is fed into a single fullyconnected layer with ReLU (language head, θl). The image observations (possibly from two camera angles–one
% from overhead and one from the wrist) are passed through
% convolutional layers (image encoder, θo), whose output is
% then concatenated with that from the language head. The
% actor head (θa) and critic head (θc) then output the action at and the corresponding value for the Q function
% Q(ot, at). See Appendix A3 for more details of the neural network setup.


\subsection{Task-Oriented Grasp Evaluation}\label{tog}
% GraspGPT-MLP GraspGPT-ATN
% an object point cloud, a candidate grasp pose, a language instruction, and language description paragraphs,
After obtaining the feature representations of all elements, we next present a multi-modal fusion module for the task-oriented grasp evaluation, which can be represented below:
\begin{align*}
    S = \textnormal{TGE}(F_X, F_I, F_{td}, F_{od})
\end{align*}
where TGE is the task-oriented grasp evaluator, and $S$ is the score for the candidate grasp pose $g$. \\

% To fuse multi-modal features and perform task-oriented grasp evaluation, 
% Since the architecture of GraspGPT is highly adaptable, existing models can be readily integrated into the framework. Next, we introduce 
% two versions of GraspGPT architecture: \textbf{GraspGPT-MLP} and \textbf{GraspGPT-ATN}. The former uses a lightweight Multi-Layer Perceptron (MLP) to fuse multi-modal features, and the latter uses a customized attention-based Transformer decoder\cite{vaswani2017attention} to build cross-modal correspondences. \\

% \noindent \textbf{Fusion} \ \textcolor{red}{The architecture of GraspGPT-MLP is depicted in Figure X}. It is a straightforward yet effective pipeline which directly processes concatenated multi-modal embeddings with an MLP. Specifically, $F_X$ is first projected to $\Tilde{F}_{X} \in \mathbb{R}^{300}$ via a fully connected layer. $F_{td}$, $F_{od}$, and $F_I$ are also projected and mean pooled to obtain sentence embeddings $\Tilde{F}_{td} \in \mathbb{R}^{128}$, $\Tilde{F}_{od}\in \mathbb{R}^{128}$, and $\Tilde{F}_{I} \in \mathbb{R}^{128}$. To fuse multi-modal embeddings, the three language embeddings are concatenated with the point cloud embedding channel-wise, and passed through an MLP:
% \begin{align*}
%     logits &= MLP(Concat([\Tilde{F}_{X}, \Tilde{F}_{I}, \Tilde{F}_{td}, \Tilde{F}_{od}], dim=-1))
% \end{align*}
% The MLP comprises of three fully connected layers with 1D batch normalization, ReLU activation, and dropout. \\

\noindent \textbf{Task-Oriented Grasp Evaluator} \ TGE is implemented as a customized Transformer decoder\cite{vaswani2017attention}. It is analogous to a sequence-to-sequence model commonly used in machine translation, which converts sequences from one domain to another. In our problem, the robot is unable to comprehend novel concepts out of the training set. We utilize TGE to translate a novel concept using its description paragraphs, and connect the novel concept to its related concepts described during training.

The architecture of TGE is depicted in Figure \ref{fig:pipeline} (b). The translation process incorporates contextual information from language descriptions into their corresponding concept in $I$. Both training and inference follow the same computational procedure. Specifically, we begin by transforming word embeddings from the pre-trained language encoder to a lower dimension space and obtain $\Tilde{F}_{td} \in \mathbb{R}^{T_{td} \times 128}$, $\Tilde{F}_{od}\in \mathbb{R}^{T_{od} \times  128}$, and $\Tilde{F}_{I} \in \mathbb{R}^{T_I \times 128}$. TGE consists of two layers, one for incorporating object class knowledge $\Tilde{F}_{od}$ and the other for incorporating task knowledge $\Tilde{F}_{td}$. Each decoder layer aims to learn a function as below:
\begin{align*}
    \phi_{td}: \mathbb{R}^{T_{I} \times 128} \times \mathbb{R}^{T_{td} \times 128} \rightarrow \mathbb{R}^{T_{I} \times 128} \\
    \phi_{od}: \mathbb{R}^{T_{I} \times 128} \times \mathbb{R}^{T_{od} \times 128} \rightarrow \mathbb{R}^{T_{I} \times 128}
\end{align*}
where the outputs of $\phi_{td}$ and $\phi_{od}$ are language instruction word embeddings augmented with contextual knowledge. Two decoder layers share a similar design. The computational procedure of $\phi_{*d}$ can be represented as follows:
\begin{align*}
    \Tilde{F}_{I} &= \text{LN}(\Tilde{F}_{I} + \text{MHA}(\Tilde{F}_{I}, \Tilde{F}_{*d})) \\
    \Tilde{F}_{I} &= \text{LN}(\Tilde{F}_{I} + \text{FFN}(\Tilde{F}_{I}))
 \end{align*}
where $*d$ can be either $td$ or $od$; LN, MHA, and FFN denote layer normalization, multi-head attention, and feedforward network, respectively; MHA consists of eight cross-attention heads in our implementation. The computation of each cross-attention head can be represented as:
\begin{align*}
    A = \text{Softmax}(\frac{Q_IK_{*d}^{T}}{\sqrt{128}})V_{*d}
\end{align*}
where $A$ is the attended word embeddings. $Q_I$, $K_{*d}$, and $V_{*d}$ are transformed from $\Tilde{F}_{I}$ and $\Tilde{F}_{*d}$ as follows:
\begin{align*}
    Q_I = \textnormal{Q}_{\textnormal{proj}}(\Tilde{F}_{I}), K_{*d} = \textnormal{K}_{\textnormal{proj}}(\Tilde{F}_{*d}), V_{*d} = \textnormal{V}_{\textnormal{proj}}(\Tilde{F}_{*d}) 
\end{align*}
where $\textnormal{Q}_{\textnormal{proj}}$, $\textnormal{K}_{\textnormal{proj}}$, and $\textnormal{V}_{\textnormal{proj}}$ are projection matrices. The intuition is to reconstruct $\Tilde{F}_{I}$ by all elements in $\Tilde{F}_{*d}$ weighted by their normalized correspondence. Since cross-attention mechanism can dynamically assign weights to each input token, it learns to attend to concept tokens in $I$ while ignore irrelevant tokens. 

Finally, $\Tilde{F}_{I}$ is mean pooled to output a sentence embedding $\overline{F}_{I}\in \mathbb{R}^{128}$. It is then concatenated with the shape embedding $\Tilde{F}_{X} \in \mathbb{R}^{300}$, which is obtained by projecting $F_X$ via a fully connected layer. We compute $S$ using an MLP (Multi-Layer Perceptron) with a sigmoid activation:
\begin{align*}
    S = \textnormal{Sigmoid}(\textnormal{MLP}(\textnormal{Concat}([\Tilde{F}_{X}, \overline{F}_{I}], \textnormal{dim}=-1)))
\end{align*} 
The MLP comprises three fully connected layers with 1D batch normalization, ReLU activation, and dropout.\\


\noindent \textbf{Loss Function} \ We compute the binary cross-entropy loss between $S$ and the corresponding ground truth label $S_{gt}$:
\begin{align*}
    \mathcal{L}_{bce} = -\frac{1}{N} \sum_{i=1}^{N} S_{gt, i} \cdot \text{log}(S_{i}) + \\ (1 - S_{gt, i}) \cdot \text{log}(1-S_{i})
\end{align*}
where $N$ is the total number of samples, and $S_{gt, i}$ is set to one if the $i_{th}$ grasp pose is successful and zero otherwise.









% loss function


% \begin{table*}[t]
% \centering
% \renewcommand\arraystretch{1.5}
% \begin{tabular}{l}
% \toprule
% \multicolumn{1}{c}{\textbf{Object Class Description Prompts}}                                                                  \\ \toprule
% \multicolumn{1}{c}{``\textit{Describe the shape/geometry of household object [obj] in a detailed and scientific response:}"}                   \\
% \multicolumn{1}{c}{``\textit{Describe the common use/function of household object [obj] in a detailed and scientific response:}"}              \\
% \multicolumn{1}{c}{``\textit{Describe what household objects have similar shapes/geometries to [obj] in a detailed and scientific response:}"} \\
% \multicolumn{1}{c}{``\textit{Describe what household objects have similar uses/functions to [obj] in a detailed and scientific response:}"}      \\ \toprule
% \multicolumn{1}{c}{\textbf{Task Description Prompts}}                                                                     \\ \toprule
% \multicolumn{1}{c}{``\textit{Describe what household objects can be used to [task] in a detailed and scientific response:}"}                 \\
% \multicolumn{1}{c}{``\textit{Describe what household objects support the function of [task] in a detailed and scientific response:}" }         \\ 
% \multicolumn{1}{c}{``\textit{Describe what verbs are similar/semantically close to [task] in a detailed and scientific response:}"}            \\
% \multicolumn{1}{c}{``\textit{Describe what verbs achieve similar effects to '[task] an object' in a detailed and scientific response:}"}     \\
% \bottomrule
% \end{tabular}
% \caption{sss}
% \label{tab:prompt_tab}
% \end{table*}


