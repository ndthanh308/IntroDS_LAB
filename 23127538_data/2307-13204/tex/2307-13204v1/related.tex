\section{Related Work} \label{related_work}

\subsection{Task-Oriented Grasping}
The ability to perform task-oriented grasping is essential for household robots as it is the first step towards tool manipulation. Data-driven approaches have achieved success in solving TOG problems to some extent. Dang et al.\cite{dang2012semantic} and Liu et al.\cite{liu2020cage} propose novel semantic representations of grasp contexts for task-oriented grasp pose prediction. These methods learn class-task-grasp relationships purely from data without any external knowledge sources, thus achieving unsatisfying performance.
% To predict successful task-oriented grasps, the robot needs to model the complex relationships between objects, tasks, and grasps. 
% These methods require labor-intensive pixel-wise affordance segmentation, and only demonstrate their effectiveness on a small set of known object or affordance categories. To alleviate the burden of manual annotation, Fang et al.\cite{fang2020learning} and Qin et al.\cite{qin2020keto} learn single-task grasping policies from simulated self-supervision. 

% Another line of works focuses on affordance detection \cite{do2018affordancenet, chu2019recognizing, xu2021affordance} of object parts to support task-oriented grasping.

More recent works \cite{song2010learning, ardon2019learning, antanas2019semantic, murali2021same} have proposed incorporating semantic knowledge as priors into TOG pipelines. Song et al. \cite{song2010learning} construct a semantic KB with a set of tasks, object classes, actions, constraints, and reason over the KB using Bayesian Networks. Similarly, Ard{\'o}n et al. \cite{ardon2019learning} and Antanas et al. \cite{antanas2019semantic} build KGs relating pre-defined semantic attributes using probabilistic logic approaches. Despite these advancements, a significant bottleneck that hinders the generalization to a broader range of object classes and tasks is the need for large-scale TOG datasets. Motivated by this dilemma, Murali et al.\cite{murali2021same} contribute the largest and the most diverse TOG dataset, named TaskGrasp dataset, and build a KG based on the concepts collected in the dataset. More importantly, they propose the state-of-the-art TOG algorithm GCNGrasp, which uses the semantic knowledge of object classes and tasks encoded in the graph to generalize to concepts within the graph. However, the major limitation of GCNGrasp is its inability to directly handle novel concepts out of the graph. This limitation is critical as a household robot needs to deal with open-end object classes and tasks in real-world applications. In this letter, we address this problem by leveraging the open-end semantic knowledge from an LLM to generalize learned TOG skills to novel concepts.

% Early work in analytic approaches \cite{li1988task}\cite{borst2004grasp} develop several quality metrics for optimal task-oriented grasp evaluation. However, these approaches assume the access to the full object and robot hand models, limiting their generalization to unseen objects and tasks. 







% contributes the largest and the most diverse TOG dataset
% named TaskGrasp dataset, and builds a knowledge graph
% based on the concepts collected in the dataset. 











% Our method, on the contrary, is able to generalize to novel object classes and tasks with the open-end semantic knowledge from an LLM.





% Our proposed method leverages the autoregressive sequence generation nature of an LLM to acquire semantic knowledge in an automatic manner. 
% cage

% affordance (do, chu, xu, limited categories of affordance)

% fang, keto, self-supervision, dedicated 

% external semantic knowledge song ardon antana same 








\subsection{LLMs in Robotics}
Recent advances in LLMs have motivated the robotics community to harness the semantic knowledge embedded in these models for a wide range of robotic applications, such as tabletop manipulation\cite{liang2022code}, navigation\cite{shah2023lm, huang2022visual}, and mobile manipulation \cite{ahn2022can, wu2023tidybot}.

Huang et al.\cite{huang2022language} first propose to decompose high-level tasks into mid-level plans with LLMs for robot decision making. To enable LLM-based robots to act properly in real-world applications, Ahn et al.\cite{ahn2022can} ground LLMs through affordance functions of pre-trained skills. Meanwhile, Huang et al.\cite{huang2022inner} extend previous work to include closed-loop feedback for both mobile and tabletop manipulation with a collection of perception models. Liang et al.\cite{liang2022code} re-purpose LLMs to directly generate policy code running on real-world robots. More recent works \cite{jiang2022vima, zeng2022socratic} combine multi-modal reasoning with LLMs for object rearrangement tasks. While aforementioned methods primarily consider LLMs for high-level task and motion planning, GraspGPT directly grounds the semantic knowledge from an LLM to grasping actions, which opens up the potential for other low-level policies (e.g., manipulation, navigation) optimization with an LLM.


% we assume that there exists a low-level policy or planner that
% can translate these decisions into low-level actions.

% L -> A; S -> detector -> L + ins; S -> A with L




% While aforementioned methods primarily utilize LLMs as high-level task planners for robots, our work focuses specifically on leveraging an LLM for task-oriented grasp pose prediction. 
% The aforementioned methods primarily consider LLMs as high-level task planners for robots. Differently, our work focuses on using an LLM for task-oriented grasp pose prediction.




% Jiang et al. \cite{jiang2022vima} and Zeng et al.\cite{zeng2022socratic} utilize LLMs for multi-modal prompting and multi-modal reasoning, respectively.



 % Following work continues to explore the potential of LLMs for advanced robot tasks. Jiang et al. \cite{jiang2022vima} and Zeng et al.\cite{zeng2022socratic} utilize LLMs for multi-modal prompting and multi-modal reasoning, respectively. 
 
 


% multimodal
% VIMA: GENERAL ROBOT MANIPULATION WITH MULTIMODAL PROMPTS
% we can express a wide spectrum of robot manipulation tasks with multimodal prompts, interleaving
% textual and visual tokens.

% Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language
% (LLMs, Audio LMs, Visual LMs) these model differences are complementary and can be jointly leveraged to compose (via prompting) new multimodal capabilities out-of-the-box

% code completion
% Code as Policies: Language Model Programs for Embodied Control (2023 march)
% writing LLMs can be re-purposed to write robot policy code, given natural language commands
% Our approach differs in that it uses an LLM to directly generate policy code (plans nested within) to run on the robot and avoids the requirement of having predefined policies to map every step in the plan

% task planning, different grasp synthesis







































