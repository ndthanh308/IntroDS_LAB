\section{Experimental Setup} \label{exp_setup}
% To demonstrate the superiority of GraspGPT over existing methods, we conduct both perception experiments and real-robot experiments. The goal is to evaluate the performance of generalizing learned TOG skills to novel object classes and tasks out of the training set. 

\subsection{Perception Experiments} \label{perception_exp_setup}
\noindent \textbf{Baselines} \  We compare GraspGPT to the following methods: (1) \textbf{Random}, which represents the method in \cite{sundermeyer2021contact} that focuses on grasp stability only and ignores task constraints (i.e., task-agnostic grasping method). (2) \textbf{Semantic Grasp Network (SGN)} \cite{liu2020cage}, which learns class-task-grasp relations without incorporating external semantic knowledge. (3) \textbf{GCNGrasp} \cite{murali2021same}, which is the state-of-the-art TOG algorithm introduced earlier and whose main limitation is its inability to generalize to novel concepts out of the graph. During inference, we connect the novel concept node to its nearest neighbor in the KG. The nearest neighbor search is based on the cosine similarity between the concepts' pre-trained word embeddings provided by ConceptNet \cite{liu2004conceptnet}. \\

% To evaluate its generalization ability to novel concepts, 



\noindent \textbf{Dataset} \ GraspGPT and three baselines are evaluated on the LA-TaskGrasp dataset, which augments the TaskGrasp dataset with language data. The original TaskGrasp dataset contains 250K task-oriented grasp pose annotations for 56 tasks, 75 object classes, and 191 object instances. Each instance is a partial point cloud of a real household object \textbf{with multi-view RGB-D fusion}. TaskGrasp provides three types of held-out settings: held-out (object) class, held-out task, and held-out instance. We focus on the former two settings in this letter. For language data, LA-TaskGrasp contains 80 language description records for each object class and 40 records for each task, resulting in 6000 object class description records and 2240 task description records. We combine these descriptions to generate 750 object class description paragraphs and 560 task description paragraphs. LA-TaskGrasp dataset also includes 53 language instruction templates, resulting in 222600 possible language instruction sentences. \\

% % Figure environment removed 

% Figure environment removed 

\noindent \textbf{Metrics} \  We use the same set of evaluation metrics used by GCNGrasp. Specifically, we compute the Average Precision (AP) score for each object class, task, and instance, and then compute the mean AP (mAP) averaged over all object classes, tasks, and instances (i.e., class mAP, task mAP, and instance mAP).

\subsection{Real-Robot Experiments}
% Real-robot experiments evaluate the performance of physical grasping subject to the task constraints. 
The real-robot experiment platform comprises a 7 DoF Kinova Gen3 robotic arm with a parallel jaw gripper and an Intel RealSense D435 RGB-D camera with eye-in-hand calibration. For each test object, we first apply SAM\cite{kirillov2023segment} to extract the object point cloud \textbf{captured from a single view} and then apply Contact-GraspNet\cite{sundermeyer2021contact} to generate 50 grasp pose candidates. Single-view setup is used here because it is more practical for real-world robotic applications. We collect test objects from our laboratory and YCB dataset. More details on the experimental setup can be found in the appendix.

% Finally, all the candidates are evaluated and the one with the highest score are executed. 
The physical grasping pipeline is divided into three stages: Perception, Planning, and Action, and the statistics of each stage is reported separately for clarity. A trial succeeds if the test object is grasped subject to the natural language instruction and lifted stably by the robot. We additionally combine GraspGPT with three pre-defined skills (pouring, handover, and scooping) in the form of motion primitive to showcase its practicality in tool manipulation. 

% We place each object (without
% clutter) on a table in front of the robot. After table plane segmentation to obtain the object point
% cloud, 600 stable grasps are sampled and 50 candidates are selected using farthest point sampling
% for evaluation. We evaluate the grasps on our best performing GCNGrasp model from the held-out
% task ablations (Table 2). Our hardware setup comprises of a 7-DOF Sawyer Robot with a 2-fingered
% Robotiq gripper and a Intel Realsense D415 RGB-D camera mounted on the gripper wrist. Inference
% for the 50 grasps takes around 3s on a desktop with an NVIDIA GTX 1080 Ti GPU and the grasp with
% the best score is executed on the robot. Fig 4 shows the executed task-oriented grasps on unknown
% objects. Even though our dataset objects were collected only in one canonical pose, our approach
% is able to generalize to new grasps and in unknown poses due to data augmentation during training.
% Based on the grasp evaluator scores from Fig 4, our model is also able to interpolate between modes
% in the continuous SE(3) space to reason about task-oriented grasping. One failure mode of our work
% is that it does not generalize to categories (like the spray bottle in Fig 4 in the bottom right) with
% limited training data. A future work is to balance the dataset in terms of object categories.

% \noindent \textbf{Baselines} \  xxx \\

% \noindent \textbf{Metrics} \  xxx \\

\subsection{Implementation Details}\label{detail}
All the experiments are conducted on a desktop PC with a single Nvidia RTX 3090 GPU. GraspGPT is optimized with an Adam optimizer\cite{kingma2014adam} with a weight decay of 0.0001. The learning rate is set to 0.0001 initially and decays subject to a customized function as in GCNGrasp. We train GraspGPT for 50 epochs with a batch size of 32. Each point cloud is downsampled to 4096 points before being fed into the model. 

% We provide language descriptions to each concept during both training and testing. 

For the choice of an LLM, we select the OpenAI GPT-3 model, specifically the \textit{text-davinci-003} version. GraspGPT is capable of incorporating any current LLM, such as OpenAI GPT-4 and Google Bard, or using an ensemble of LLMs. We leave this ensemble approach for our future work. For the language encoder, we choose the Google pre-trained \textit{BERT-Base} model provided by Hugging Face.

% This ensemble approach is expected to provide complementary semantic knowledge from each LLM, further enhancing the overall performance of GraspGPT. 

% learning rate 1e-4, LambdaLR, Adam optimizer, weight decay 0.0001, 50 epochs, batch size 32, , 4096 points, loss

% GPT-3 (text-davinci-003), GPT-3.5/4, PalM, bard, subset of 10 each concept, pre-embedding encoded description and instruction



