\pdfoutput=1 
\documentclass[preprint,3p,12pt,authoryear]{elsarticle} %preprint

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{physics}
\usepackage{amsmath}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algorithmic}

%\usepackage{todonotes}

\journal{arxiv}

%\usepackage{setspace}
%\doublespacing

\begin{document}

\begin{frontmatter}

\title{Fading memory as inductive bias in residual recurrent networks}

\author[inst1,inst2]{Igor Dubinin}%\corref{cor1}}
\ead{igor.dubinin@esi-frankfurt.de}

\author[inst1]{Felix Effenberger}
\ead{felix.effenberger@esi-frankfurt.de}

%\cortext[cor1]{Corresponding author}

\affiliation[inst1]{organization={Ernst Strüngmann Institute},%Department and Organization
            addressline={Deutschordenstraße 46}, 
            city={Frankfurt am Main},
            postcode={60528}, 
            country={Germany}}

\affiliation[inst2]{organization={Frankfurt Institute for Advanced Studies},%Department and Organization
            addressline={Ruth-Moufang-Straße 1}, 
            city={Frankfurt am Main},
            postcode={60438}, 
            country={Germany}}

\begin{abstract}
%
Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm.
%
Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties.
%
Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory.
%
We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks.
%
We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity.
%
In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties are shown to increase practical expressivity.
%
In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs.

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
recurrent neural network \sep inductive bias \sep residual connection \sep memory
%% PACS codes here, in the form: \PACS code \sep code
%\PACS 0000 \sep 1111
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
%\MSC 0000 \sep 1111
\end{keyword}

\end{frontmatter}

\section{Introduction}
\noindent
The power of artificial neural networks in solving tasks lies in their universal approximation abilities, which is commonly referred to as \emph{theoretical expressivity}~\citep{hornik1989multilayer,cybenko1989approximation,funahashi1989approximate,barron1994approximation}. 
%
However, the practical solutions to which networks can converge in a reasonable number of training iterations of a typically gradient-based learning scheme such as backpropagation, the \emph{practical expressivity} of a network, have been shown to lag behind the theoretical expressivity~\citep{hanin2019complexity}.
% 
Practical expressivity is determined by a set of inductive biases, which take the form of (i) network architecture, (ii) weight initialization methods, (iii) convergence properties and other specifics of the training procedure, and (iv) more generally, comprise anything that influences the space of mappings learnable by a given network~\citep{battaglia2018relational,goyal2022inductive}.
%
As the bias-variance trade-off suggests, the right choice of inductive biases plays a crucial role because properly informed biases can improve the efficiency of learning, a serious constraint on network expressivity in practice~\citep{kearns1994introduction}.

A celebrated example of feed-forward networks with an effective inductive bias in the form of a constrained network architecture are convolutional neural networks~\citep{lecun1995convolutional}.
%
Another popular form of an architectural inductive bias are so-called \emph{residual connections} (or skip connections) of deep feed-forward architectures.
%
These have been shown to strongly increase performance for many architectures and are used, for example, in the U-Net~\citep{ronneberger2015u}, ResNet~\citep{he2016deep} or Transformer~\citep{vaswani2017attention} architectures.
%
Such residual connections have been shown to prevent gradients from vanishing in deep feed-forward networks, one aspect of the well-studied exploding and vanishing gradients problem (EVGP) that appears in practice when training deep networks with the backpropagation algorithm~\citep{glorot2010understanding}.

When considering inductive biases in recurrent neural networks (RNNs), the crucial question is how these biases influence network dynamics and the resulting memory properties of the network.
%
Training of RNNs with the backpropagation through time (BPTT) algorithm involves unrolling the RNN into a deep feed-forward network, so that networks trained on longer time series also face the EVGP~\citep{pascanu2013difficulty}.
%
Historically, RNNs have been studied from a dynamical systems perspective~\citep{bengio1994learning,hochreiter1997long} and many ideas have been proposed to address the EVGP~\citep{schoenholz2016deep, chang2019antisymmetricRNN, miller2018stable, erichson2020lipschitz}.
%
Importantly, the dynamical systems approach showed that the RNNs' dynamics which are close to the point of a transition between stability and instability (the edge of chaos) are characterized by long-term fading memory and therefore efficient gradient propagation~\citep{vogt2020lyapunov,engelken2020lyapunov}. 

Although the influence of residual connections on RNN performance has been studied previously~\citep{wang2016recurrent, yue2018residual}, dynamics and memory properties of RNNs with residual connections have not been studied in detail.
%
Here, we fill this gap and explore how the residual connections in RNNs result in inductive biases which influence the networks' dynamics and the properties of their fading memory.
%
The main contributions of this work are as follows.

\begin{itemize} 

\item We establish a connection between network dynamics, fading memory, and learning dynamics in RNNs. In particular, it is shown that the fading memory properties of network dynamics produce temporally modulated learning rates.

\item
A new architecture, \emph{weakly coupled residual recurrent networks} (WCRNN), is introduced and proven to have stable and easily controllable memory properties by showing the existence of Lyapunov exponents of network dynamics. We demonstrate how the eigenvalues of the residual matrix control the fading memory of WCRNNs. We study WCRNNs with dynamics close to the edge of chaos and confirm the theoretically predicted trade-off between the speed and the stability of learning.

\item We show for WCRNNs that informed residual connections and corresponding inductive biases result in higher practical expressivity on a set of benchmark problems, assessed by convergence speed and best test accuracy achieved. In particular, we show how residuals resulting in weakly subcritical network dynamics allow the networks to benefit from long memory timescales, how residuals with rotational dynamics allow the networks to utilize spectral properties of the data samples, and how heterogeneous residuals allow the networks to capitalize on the resulting diversity of informed memory timescales.

\item We show how results from WCRNNs can be generalized to non-linear residuals and to the general case of a standard Elman RNN in the form of a weakly coupled residual initialization scheme.

\end{itemize}

%
\noindent
%
Our work demonstrates how Lyapunov exponents can be used to characterize fading memory resulting from residual connections and shows how informed residual connections can be used to achieve superior practical expressivity in RNNs.
%

\section{Related work}
%
\noindent
%
Over the years, many approaches have been proposed to overcome the EVGP encountered when training RNNs with BPTT.
%
Among those are gated architectures, such as long-short-term memory networks (LSTM)~\citep{hochreiter1997long} and gated recurrent units (GRU)~\citep{cho2014learning}, which in many cases still require gradient clipping to achieve high practical expressivity~\citep{pascanu2013difficulty}. 
%
Furthermore, models that place constraints on weight matrices such as orthogonal~\citep{helfrich2018orthogonal}, unitary~\citep{arjovsky2016unitary} or antisymmetric weight matrices~\citep{chang2019antisymmetricRNN} were proposed to mitigate the EVGP.
%
In contrast to these models, the WCRNNs proposed here do not impose strict conditions on weight matrices, as this can limit the practical expressivity of networks~\citep{vorontsov2017orthogonality}. 
%
In that sense, WCRNNs are similar to RNNs with a specific initialization of their recurrent weights (for example, using the identity or orthogonal matrices~\citep{le2015simple,mishkin2015all}), but differ in that the weak coupling in WCRNNs ensures the stability of the gradient properties throughout the learning process.

Although residual networks were first introduced without dynamical systems theory in mind, later it was shown in the mean-field approximation that they possess efficient gradient propagation properties when their dynamics is close to the edge of chaos ~\citep{yang2017mean}. 
%
The presence of weak coupling in WCRNNs resembles this form of approximation, with the important distinction that our analysis is focused on recurrent architectures and their temporal dynamics.

Residual networks also sparked new interest in a dynamical systems approach due to the fact that in the limit of an infinite-depth approximation they can be understood as a system of differential equations, the NeuralODE approach~\citep{chen2018neural}.
%
In contrast to WCRNNs, architectures based on the NeuralODE approach are limited to describe continuous-like dynamics.

%
Recent studies on recurrent networks show that oscillatory properties and heterogeneity of neural dynamics provide computational advantages~\citep{norcliffe2020second, rusch2020coupled, zeldenrust2021efficient, perez2021neural, effenberger2022biology, orvieto2023resurrecting}.
%
Here, we provide an extended explanation of how different memory properties of RNNs can benefit their practical expressivity.  

\section{Background and motivation}

\noindent
In this section, we discuss the connection between network dynamics, memory, and learning dynamics resulting from the training by BPTT. In Section \ref{ss:analysis} we introduce the concept of Lyapunov exponents and show how they determine memory timescales. In Section \ref{ss:learning} we show how learning dynamics, mediated by backpropagation through time, is influenced by the properties of fading memory.


\subsection{Dynamical systems analysis}
\label{ss:analysis}
%
The memory of a system, commonly called fading memory in the context of recurrent neural networks, is a well-defined and thoroughly studied concept in the field of dynamical systems. 
%
From a dynamical system's perspective, a recurrent network is a non-autonomous, non-linear recurrent discrete map, the dynamics of which can be defined as
%
\begin{equation}
\mathbf{x}_{t+1}=\mathbf{f}(\mathbf{x}_{t}, \mathbf{S}_{t}),
\label{eq:disc_map}
\end{equation}
%
where $\mathbf{x}_{t} \in \mathbb{R}^N$ is the network state at time $t$, $\mathbf{f}: \mathbb{R}^N\rightarrow \mathbb{R}^N$  is non-linear function, $\mathbf{S}_{t} \in \mathbb{R}^N$ is the input to the network at time $t$ and $N$ is the dimensionality of the network state. 

First, we consider the autonomous case where the memory properties of network dynamics can be studied by means of perturbation theory and Lyapunov exponents. 
%
If we evolve an infinitesimal perturbation of the $P$-dimensional volume of the tangent space $\delta \mathbf{P}$, linearize it along this perturbation and apply the chain rule over $t$ time steps, we obtain
%
\begin{equation}
\delta \mathbf{P}(t+1) = \mathbf{V}_{\mathbf{x}}(f^t(\mathbf{x}_0)) \delta \mathbf{P}(0),
\label{eq:var_general_vol}
\end{equation}
%
where $\mathbf{x}_0$ is the initial state, $\mathbf{f}^t=\mathbf{f} \circ \dots \circ \mathbf{f}$ denotes the $t$-fold iteration of the map $\mathbf{f}$, and $\mathbf{V}_{x}(f^t(x_0))$ denotes a variational term. 
%
In our case of a discrete map $\mathbf{f}$, the variational term $\mathbf{V}_{x}(f^t(x_0))$ takes the form of the product of the instantaneous Jacobians $\mathbf{J}_x$ of $\mathbf{f}$ over the course of the system trajectory~\citep{eckmann1985ergodic,sandri1996numerical} and writes as 
%
\begin{equation}
\mathbf{V}_{\mathbf{x}}(f^t(x_0)) = \mathbf{J}_{\mathbf{x}}(f^{t}(x_0))\mathbf{J}_{\mathbf{x}}(f^{t-1}(\mathbf{x}_0))...\mathbf{J}_{\mathbf{x}}(f(\mathbf{x}_0)).
\label{eq:var_disc_lin_eq}
\end{equation}
%
Finally, according to the Oseledets theorem~\citep{oseledets1968multiplicative}, the Lyapunov exponents for the system (\ref{eq:disc_map}) are given by the eigenvalues of the matrix
\begin{equation}
\mathbf{M}=\lim_{t \to \infty} \frac{1}{2t}\log(\mathbf{V}_x(f^t(\mathbf{x}_0)) \mathbf{V}_x(f^t(\mathbf{x}_0))^T),
\label{eq:ose}
\end{equation}
%
where $\cdot^T$ indicates the matrix transpose. 

Essentially, the Jacobians $\mathbf{J}_{x}(f^{k})$ define the space of instantaneous local volume transformations that are accessible to the system. 
%
The variational term $\mathbf{V}_{x}(f^t)$ determines the memory timescales of network dynamics, where every Lyapunov exponent defines the direction with an asymptotically stable rate of memory change. 
%
The number of Lyapunov exponents is equal to the dimensionality of the system $N$.
%
The largest Lyapunov exponent determines the stability of the network dynamics, with a negative (positive) Lyapunov exponent indicating its stability (instability). 
%
In particular, the largest Lyapunov exponent changes sign when the system undergoes a transition between chaos and order. 
 
However, in the general case (\ref{eq:disc_map}), the external input makes the system non-autonomous, meaning that the existence of the limit (\ref{eq:ose}) is not guaranteed. 
%
Thus, the described analysis cannot be easily performed for most input-driven recurrent networks.
%
In this study, we show how introducing weak coupling can mitigate this issue; see Section \ref{s:resid}.  
 
\subsection{Learning dynamics}
\label{ss:learning}

%
Through the lens of the dynamical systems perspective, training a network (\ref{eq:disc_map}) with backpropagation through time creates the following learning dynamics $\mathbf{g}$ on the weights $\mathbf{w}$, 
%
\begin{equation}
\mathbf{w}_{\tau+1} = \mathbf{g}(\mathbf{w}_{\tau}) = \mathbf{w}_{\tau} - \eta \frac{1}{M} \sum_{M}\nabla_{w}L,
\label{eq:learn_dyn}
\end{equation}
%
where $\tau$ denotes the training step, $\eta$ is the learning rate hyperparameter, $\nabla_{w}L$ is the gradient of the loss function $L$  with respect to the weight $w$, and $M$ is the batch size. 
%
Here, we assume a deterministic version of gradient descent without loss of generality. 
%
The instantaneous Jacobian $\mathbf{J}_{w}$ of the learning dynamics (\ref{eq:learn_dyn}) is then given by
%
\begin{equation}
\mathbf{J}_{w}(\mathbf{g})=\mathbf{I}-\eta\frac{1}{M}\sum_{M}\mathbf{H}_{w}(L), 
\label{eq:jac_learn}
\end{equation}
%
where $\mathbf{H}_{w}(L)$ denotes the Hessian of loss function $L$ and $\mathbf{I}$ is the identity matrix.
%
Importantly, in the same way as for network dynamics (\ref{eq:disc_map}), the asymptotic behavior of the product of instantaneous Jacobians (\ref{eq:jac_learn}) determines the convergence or divergence of the learning dynamics.
% 
This agrees with previous studies that have shown that the curvature of the loss landscape defined by the Hessian plays a crucial role in gradient-based learning~\citep{dauphin2014identifying}.

%Equation (\ref{eq:jac_learn}) shows that every eigenvalue of the Hessian of the loss function has an associated eigenvector in weight space.
%
%We will refer to these eigenvectors of the Hessian as \emph{learning directions}.
%
%The \emph{effective learning rate} for a particular learning direction is equal to $(1-\eta \lambda_{\mathbf{H}}^{M})^{-1}$, where $\lambda_{\mathbf{H}}^{M}$ is the corresponding eigenvalue of the Hessian averaged over a given batch.
%

From Equation (\ref{eq:jac_learn}), it also follows that every eigenvalue of the Hessian $\mathbf{H}_{w}(L)$ defines an \emph{effective learning rate} in the direction of an associated eigenvector in the weight space, modulating the base learning rate $\eta$ in the direction of the eigenvector.
%
This effective learning rate is equal to $(1-\eta \lambda_{\mathbf{H}}^{M})^{-1}$, where $\lambda_{\mathbf{H}}^{M}$ is the corresponding eigenvalue of the Hessian averaged over a given batch.
%
In order to further investigate these modulated learning rates, we can rewrite the Hessian as
%
\begin{equation}
\begin{split}\mathbf{H}_{w}(L) = (\mathbf{J}_{w}(f^{t}))^{T} \mathbf{H}_{f^{t}}(L) (\mathbf{J}_{w}(f^{t})) + \sum_{N}(\nabla_{f^{t}}(L))_{n}(\mathbf{H}_{w}{f^{t}_{n}}),
\label{eq:hessian_gen}
\end{split}
\end{equation}
%
see~\citep{schraudolph2002fast}.
%
In statistical terms, this means that the Hessian can be split into a first term corresponding to the variance and a second term corresponding to the bias.
%
The first term in (\ref{eq:hessian_gen}) is known as the Generalized Gauss-Newton (GGN) matrix, which is a popular approximation for the curvature matrix in second-order optimization methods~\citep{thomas2020interplay}.
%
The second term is proportional to the value of the loss and vanishes as the learning approaches a local minimum, meaning that the Hessian converges to the GGN matrix when training by backpropagation.
%
Taken together, this shows that the asymptotic behavior of learning dynamics (\ref{eq:learn_dyn}) is predominately influenced by the properties of the GGN matrix.
%
Moreover, the GGN matrix is proportional to the Jacobians $\mathbf{J}_{w}(f^{t})$, which can be computed by applying the chain rule as 
%
\begin{equation}
\mathbf{J}_{w}(f^{t}) = \sum_{1\leq k \leq t}
\mathbf{V}_{x}^{k}(f^t(x_0))
\mathbf{J}_{w}(f^{k}),
\label{eq:grad_prop}
\end{equation}
%
where $\mathbf{V}_{x}^{k}(f^t(x_0)) = \mathbf{J}_{x}(f^{t}(x_0))\mathbf{J}_{x}(f^{t-1}(x_0))...\mathbf{J}_{x}(f^{k}(x_0)) $
%
is a truncated version of the variational term in (\ref{eq:var_disc_lin_eq}). 
%
This shows that the contribution of long-term derivatives to effective learning rates is proportional to the variational term $\mathbf{V}_{x}^{k}$.
%
Essentially, this variational term not only defines the memory properties of the network dynamics, but also temporally modulates effective learning rates, establishing the connection between fading memory and learning dynamics.

To the best of our knowledge, this connection between fading memory and learning dynamics has not been explicitly shown before, and it will be important for the further analysis of weakly coupled residual recurrent networks as defined below.

\section{Weakly coupled residual recurrent networks} 
\label{s:resid}

\noindent
Residual connections in deep feed-forward networks have been shown to allow for a better backpropagation of errors and are usually implemented by an identity map between subsequent layers~\citep{he2016deep}. 

Here, we consider a more general case of \emph{weakly coupled residual recurrent neural network} (WCRNN) equipped with an arbitrary fixed residual map $\mathbf{R}: \mathbb{R}^N\rightarrow \mathbb{R}^N$. 
%
The update equation for such networks takes the form
%
\begin{equation}
\mathbf{x}_{t+1} = \mathbf{R}(\mathbf{x}_t) + \gamma \cdot \sigma(\mathbf{W}\mathbf{x}_t + \mathbf{S}_t),
\label{eq:general_residual}
\end{equation}
%
where  $\gamma\ll1$ denotes a weak coupling constant and $\sigma$ a non-linearity (typically $\tanh$). 

The introduction of weak coupling allows us to simplify memory properties of such networks because in this case the variational term (\ref{eq:var_disc_lin_eq}) is given by
%
\begin{equation}
\begin{aligned}
\mathbf{V}_x (f^t(x_0)) = \mathbf{J}_R(x_t)\mathbf{J}_R(x_{t-1})...\mathbf{J}_R(x_0)+O(\gamma),
\label{eq:var_gen_residual}
\end{aligned}
\end{equation}
%
where $\mathbf{J}_R(x_t)$ denotes the instantaneous Jacobian of the residual at the time step $t$ and $O(\cdot)$ is the Landau O. 
%
Equation (\ref{eq:var_gen_residual}) shows that the properties of the dynamics of WCRNNs are predominately determined by the instantaneous Jacobians of the residual. 
%
Thus, the WCRNN architecture allows to control the fading memory properties of the entire network by choosing an appropriate residual.

In the general case (\ref{eq:general_residual}), the Lyapunov exponents depend on $x_t$ due to the non-linearity in the residual, which can complicate proving the existence of the limit in (\ref{eq:ose}).
%
To simplify the analysis, we consider networks with linear residuals $\mathbf{R}({x}_t) =  \mathbf{R} \mathbf{x}_t$, where $\mathbf{R}$ is a $N \times N$ matrix. 
%
For the linear case, the instantaneous eigenvalues of the residual are independent of the trajectory of the system, and the variational term simplifies to $\mathbf{V}(f^t(x_0)) = \mathbf{R}^t+\mathcal{O}(\gamma)$, where $\cdot^{t}$ denotes matrix exponentiation.
%
Furthermore, we can easily derive the Lyapunov exponents in this case, because the Oseledets equation (\ref{eq:ose}) in the limit of weak coupling yields $\mathbf{M}=\log{\mathbf{D}} + \mathcal{O}(\gamma)$,
%
where $\mathbf{D}=\operatorname{diag}(\lambda_{1},\lambda_{2},...,\lambda_{N})$ is the diagonal matrix of eigenvalues of $\mathbf{R}$.
%
This means that, for weakly coupled linear residual networks, the logarithms of the eigenvalues of the residuals $\lambda_{\text{residual}}$ approximate the Lyapunov exponents $\text{LE}_{\text{net}}$ of the entire network 
%
\begin{equation}
\text{LE}_{\text{net}} \approx \log{\lambda_{\text{residual}}},
\label{eq:ll_res}
\end{equation}
%
and the weak coupling limits the range of their finite-size fluctuations.
%
Based on their dynamical stability (distance to the edge of chaos~\citep{bertschinger2004real}) as determined by the magnitude of the largest eigenvalue $\lambda_{\text{max}}$ of the residual matrix, we can distinguish three classes of WCRNNs: (i) subcritical ($\lambda_{\text{max}}<1$), 
 (ii) critical($\lambda_{\text{max}} = 1$), and (iii) supercritical ($\lambda_{\text{max}}>1$) networks.
%
Moreover, it follows from Section \ref{ss:learning} that for subcritical (supercritical) networks, the contribution to effective learning rates decays (increases) over time with a constant rate defined by $\lambda_{\text{max}}$ and that for critical networks, all time steps contribute equally.
%
In addition, due to the stability of the Jacobians of networks with linear residuals, the expression for their gradients takes the form 
%
\begin{equation}
\nabla_{w}L= \sum_{1\leq k \leq t}
( \mathbf{R}^{t-k}\mathbf{J}_{w}(f^{k}))^{T}
\nabla_{f^{t}}L+\mathcal{O}(\gamma^2),
\label{eq:res_grad_prop}
\end{equation}
%
which results in a monotonic exponential decay of gradient norms over time.

Taken together, the discussed properties of WCRNNs makes us hypothesize that there exists a trade-off between the stability of learning dynamics and \emph{convergence speed}, the number of iterations required to achieve a certain value of the loss function.
%
For more subcritical networks, we expect problems with slow convergence speed and vanishing gradients.
%
For more supercritical networks, we expect faster convergence, but at the same time problems with unstable learning trajectories and exploding gradients.
%
For critical WCRNNs, characterized by network dynamics in proximity to the edge of chaos, we anticipate an optimal balance in learning dynamics and, therefore, the best performance and convergence speed.

In summary, we design weakly coupled residual recurrent networks, where the introduction of weak coupling allows us to obtain stable memory timescales as defined by Lyapunov exponents.
%
We show that the properties of fading memory are mainly determined by properties of the residuals and, in the case of linear residuals, by the eigenvalues of the residual matrix.
%
On the basis of this, we predict a trade-off between convergence speed and stability of learning dynamics and divide the networks into three categories based on their dynamics: subcritical, critical, and supercritical networks. 

\section{Experiments}

\noindent
To empirically validate our theoretical predictions, we performed experiments on several datasets: 
\begin{itemize}
\item
Sequential MNIST (sMNIST), where the 28x28 pixels of a sample~\citep{lecun1998gradient} are presented to the network sequentially in the form of a time series. MNIST digits are turned into time series of length 784 by collecting intensity values in scan-line order from top left to bottom right. We also consider permuted sequential MNIST (psMNIST), where a random but fixed permutation is applied to sMNIST samples. The permutation removes low-frequency components present in sMNIST samples and increases the difficulty of the classification problem.
\item
The adding problem (ADD) of lengths 100, 200, 400, 800, where every sample is given by a two-dimensional time series of the specified length~\citep{hochreiter1997long}. The first coordinate is given by random numbers drawn from a uniform distribution $[0, 1]$, and the second coordinate constitutes a cue signal taking values $0$ (no cue) and $1$ (cue). The task of the network is to compute the sum of the input values presented in the first coordinate for two cue points randomly placed in the first half and the second half of the signal, respectively.
\end{itemize}
%
The experiments were carried out in PyTorch~\citep{paszke2019pytorch} for network sizes of 50, 100, and 200 units.
%
Network training was performed for 200 epochs for the sMNIST and psMNIST datasets and for 150 epochs for the ADD datasets.
%
Stochastic gradient descent (SGD) with a momentum of $0.9$ was used as an optimizer for BPTT and update steps were performed according to a minibatch scheme, using batch sizes of 64, 128, and 256 samples.
%
Qualitatively, results were found to be mostly independent of network and batch size. 
%
Thus, we present results for networks of 100 units, trained with a batch size of 128 samples in the following.
%
Results were collected over 5 network instances with random weight initialization.
%
During initialization, weights and biases were randomly sampled from a uniform distribution according to the default implementation of the PyTorch \texttt{torch.\allowbreak nn.\allowbreak Linear} layer ($U(-1/\sqrt{n_{\text{in}}}, 1/\sqrt{n_{\text{in}}})$, where $n_{\text{in}}$ denotes the input dimension of a given layer).  
%
The $\tanh$ activation function was used as the non-linearity $\sigma$ in all our experiments.

In Section \ref{ss:transition} we present simulation results of WCRNNs with dynamics close to the edge of chaos, showing the validity of our theoretical predictions about their performance for all datasets. 
%
In Sections \ref{ss:rot} and \ref{ss:hete} we show how rotational and heterogeneous residuals can be beneficial to the performance of WCRNNs. 
%
Lastly, in Section \ref{ss:init} we show that our results can also be generalized to Elman RNNs by an initialization scheme.
%
The results presented were found to be consistent across all datasets, inputs, and networks with different random weight initializations.

\subsection{Critical residuals}
\label{ss:transition}
%
First, we studied different WCRNNs with network dynamics in proximity to the edge of chaos.
To place networks in this dynamical regime, we introduced linear diagonal residuals of type $\mathbf{R}=r\mathbf{I}$, where $r$ is a scalar parameter and $\mathbf{I}$ denotes the identity matrix.
%
According to (\ref{eq:ll_res}), these networks have $N$ Lyapunov exponents with identical values equal to $\log{r}$, where $N$ is the number of units in the network. 
%
By varying $r$ we can control the Lyapunov exponents, and thus the distance of the network dynamics to the edge of chaos.
%
We varied the value of $r$ in the interval $r \in [0.91,1.02]$ to explore the range of subcritical, critical, and supercritical dynamics (see Figure \ref{fig:grads_all}). 
%
To numerically compute Lyapunov exponents from network dynamics, we used a method based on QR-decomposition~\citep{sandri1996numerical} that ensures the numerical convergence of the eigenvalues of the orthonormalized variational term even for unstable dynamics (see \ref{a:algo}).     
%
Gradient propagation was measured by the L1 gradient norms $\pdv{L}{x_i}$ in the training dataset~\citep{arjovsky2016unitary}. 
%
Due to the computational complexity of calculating the Hessian, all Hessians shown below were computed for the same randomly chosen but fixed input.
%
We note that the results were found to be consistent across different choices of the fixed input chosen.
%
The findings were found to be qualitatively consistent in the range $\gamma \in [0.1, 0.001]$ and here we present results for the value $\gamma = 0.01$.
%
In particular, the coupling constant $\gamma$ had an effect on the stability and convergence speed, which is similar to the global learning rate; see \ref{a:weak} for more details.
%

% Figure environment removed
%
As expected, we found that the eigenvalues of the variational term had converged to the values defined by the residual according to equation (\ref{eq:ll_res}), which confirms the existence of Lyapunov exponents for the WCRNNs (Figure \ref{fig:grads_all}B). 
%
Furthermore, we observed that the magnitude of the highest eigenvalue of the Hessian was proportional to $r$, as predicted by (\ref{eq:hessian_gen}).
%
This supports the theoretical results that the proximity of network dynamics to the edge of chaos affects the effective learning rates, see Figure \ref{fig:grads_all}C.
%
The gradient norms of subcritical and supercritical networks were observed to decrease or increase exponentially with constant rate over time, as predicted by (\ref{eq:res_grad_prop}), see Figure \ref{fig:grads_all}D.
%
We compare the eigenvalues of the variational term, the eigenvalues of the Hessian, and the gradient propagation before training WCRNNs, because the initial differences between the networks are attributed to the differences in their residuals.
%
In contrast to the eigenvalues of the Hessian and the gradients, the eigenvalues of the variational term remained in the same range during the training period, by design, see \ref{a:memory}.

%
To evaluate the networks' practical expressivity, we measure not only the best accuracy achieved on a test set during the training period (overall performance), but also the convergence speed.
%
We assess the convergence speed by the number of training iterations required for the network to reach a given threshold of minimal performance, chosen differently for each dataset.
%
The threshold was set to $50\%$ for the classification accuracy for the sMNIST and psMNIST datasets and to $0.05$ for the root mean square error (RMS) for the ADD datasets.
%
The threshold values were chosen to capture a non-negligible deviation from chance-level performance.
%
For networks that were unable to reach this threshold, the convergence speed was set to the maximum number of iterations in the training period.
%
The practical expressivity of WCRNNs was found to be strongly dependent on the parameter $r$, showing that although residual connections do not limit the theoretical expressivity of the network, they play an important role for network expressivity in practice.
%
% Figure environment removed

We observed that an increase in distance from the edge of chaos in the subcritical regime resulted in a decrease in convergence speed for all datasets, see Figure \ref{fig:perf_speed_all}.
%
This is in good agreement with our theoretical predictions presented in Section \ref{s:resid}. 
%
The observed decay in convergence speed is caused by vanishing gradients, showing that EVGP poses an important practical limitation for subcritical networks.
%
We observed that supercritical networks showed unstable learning trajectories, see Figure \ref{fig:perf_speed_all}A, again in line with our theoretical predictions.
%
The fact that the gradients of supercritical networks were informative but exploded in magnitude was supported by the finding that clipping of the gradients resulted in more stable learning trajectories (data not shown). 
%
Overall, these results support our hypothesis that proximity to the edge of chaos enables a faster convergence speed at the expense of the stability of learning dynamics.

Interestingly, we observed that supercritical networks showed better performance for the sMNIST and psMNIST datasets, while subcritical networks performed better for the ADD datasets.
%
This can be explained by the fact that the inputs of ADD datasets are dense in the first dimension (input values in the first dimension are rarely zero), while the inputs of MNIST-based datasets are more sparse (many input values are zero or close to it).
%
These different characteristics of the input favor exploding and vanishing gradients, respectively. 
%
Importantly, we observed that the best performing networks were closer to the edge of chaos for longer ADD datasets.
%
This agrees with the general intuition that longer inputs require longer memory timescales and shows that the best inductive biases are dependent on the characteristics of the inputs as defined by the dataset.
%
We saw that networks with dynamics close to the edge of chaos showed both higher overall performance and faster convergence speed compared to strongly supercritical and strongly subcritical networks. 
%
Overall, weakly subcritical networks performed best with a dataset-specific optimal distance to the edge of chaos.
%

In summary, we assessed the practical expressivity of subcritical, critical, and supercritical WCRNNs by means of their overall performance and convergence speed on a set of benchmark tasks.
%
We validated the existence of Lyapunov exponents by numerical calculations and confirmed our theoretical predictions about the trade-off between convergence speed and the stability of learning dynamics.
% 
Consistent with the previous literature~\citep{schoenholz2016deep}, we found that residual networks with dynamics close to the edge of chaos possess higher practical expressivity compared to strongly subcritical or supercritical networks.
%
Importantly, we observed that the weakly subcritical networks showed the best overall performance, and found that the optimal distance to the edge of chaos was indicative of memory timescales beneficial for the task at hand.

\subsection{Rotational residuals}
\label{ss:rot}
%
The WCRNNs considered so far only had residual matrices with real eigenvalues, so they were limited to scaling linear transformations and reflections. 
%
To study all geometrical transformations represented by the group of square matrices, we have to introduce rotations, which correspond to matrices with eigenvalues having non-vanishing imaginary part. 
%
For this purpose, we consider residual matrices $\mathbf{R}$ taking the form of orthonormal diagonal block matrices
%
\begin{equation}
\mathbf{R}
 =
  \begin{bmatrix}
   \mathbf{T} _{1} &
  \mathbf{0} &
  \cdots &
   \mathbf{0}  \\
   \mathbf{0}  &
  \mathbf{T}_{2} &
  \cdots &
   \mathbf{0}   \\
   \vdots &
   \vdots &
   \ddots &
   \vdots  \\
   \mathbf{0}  &
   \mathbf{0}  &
    \cdots &
  \mathbf{T}_{N/2}  \\
   \end{bmatrix},
   \quad\text{with}\quad 
   \mathbf{T}_{i}
 =
  \begin{bmatrix}
   \cos{\phi_{i}} &
  -\sin{\phi_{i}}\\
   \sin{\phi_{i}}  &
  \cos{\phi_{i}}
   \end{bmatrix},
   \label{eq:rot}
\end{equation}
%
where $N$ is the number of units in the network
%
and $\phi_{i} \in [0,2\pi]$ denotes an angular frequency of rotation.
%
This type of residual represents rotation matrices with arbitrary combinations of angular frequencies $\phi_{i}$. 

% Figure environment removed
%
First, we studied WCRNNs with rotational residuals of the form (\ref{eq:rot}) for which all $\phi_{i}$ have a constant value $\phi_0$, referred to as \emph{homogeneous rotational residuals} from now on.
%
We trained WCRNNs with homogeneous rotational residuals for different values of $\phi_0$ on sMNIST and psMNIST for 50 epochs, see Figure \ref{fig:rot}.
%
Here, we chose a shorter training period to better demonstrate the differences in performance between such networks that arise due to differences in their convergence speed.
%
We found that these networks converged fastest and expressed highest performance on sMNIST when their angular frequency $\phi_0$ was close to $2\pi/28 \approx 0.22$.
%
Interestingly, this frequency coincides with the maximal peak of the average power spectra of samples in sMNIST, thus we call it the \emph{characteristic frequency} $\phi_c$ of the dataset. 
%
We found that homogeneous rotational residuals with angular frequencies that were close to integer multiplies of $\phi_c$ (i.e., $\phi_c$ and its harmonics) result in networks that show the highest performance and the fastest convergence speed, see Figure \ref{fig:rot}.
%
As follows from Section \ref{ss:learning}, we attribute this enhanced performance to the fact that instantaneous Jacobians of networks with rotational residuals modulate effective learning rates in an oscillatory manner.
%
Such instantaneous Jacobians allow the networks to accumulate derivatives that are in-phase and cancel out derivatives that are anti-phasic, see \ref{a:init}. 
%
Taken together, this shows that oscillatory learning rates can be beneficial for a network's practical expressivity when they align with characteristic spectral properties of the dataset. 

In contrast, we did not find a strong effect on practical expressivity when varying the angular frequencies of the rotational residuals for the psMNIST, see Figure \ref{fig:rot}.
%
This comes at no surprise, as compared to the sMNIST dataset, the samples of the psMNIST dataset do not possess a prominent characteristic frequency.

In addition, we also studied the case where residuals are given by random orthonormal matrices with a uniform distribution of angular frequencies of rotations (constructed with the function \texttt{scipy.stats.ortho\_group.rvs} from the SciPy package~\citep{2020SciPy-NMeth}).
%
The latter were found to result in networks with a performance similar to that of those with residuals with heterogeneous angular frequencies discussed below, see \ref{a:init} for more details.

Overall, we found that choosing residuals informed by characteristic spectral properties of the samples in the dataset can result in WCRNNs with higher practical expressivity.
%
In particular, we show for sMNIST that rotational residuals can significantly accelerate the convergence speed during learning. 
%
This agrees with the previous findings of increased performance of networks composed of oscillatory units~\citep{norcliffe2020second, rusch2020coupled,effenberger2022biology}.

\subsection{Heterogeneous residuals}
\label{ss:hete}
%
In this section, we study how introducing heterogeneous residuals influences practical expressivity in WCRNNs. 
%
We first considered networks for which the residual matrix takes the form 
%
\begin{equation}
\mathbf{R}=\operatorname{diag}(\mathbf{r}),
\label{eq:het}
\end{equation}
%
with $r_{i}$ sampled from $\operatorname{U}(r_{0}-\delta r/2,r_{0}+\delta r/2)$. 
%
For our experiments, we fix $r_{0}$ which defines a baseline distance to edge of chaos and gradually increase the level of heterogeneity controlled by the term $\delta r$.
%
We found that subcritical networks $r_{0}<1$ with moderate heterogeneity ($\delta r/2$ is less than the distance to the edge of chaos) showed better performance and faster convergence speed, and only strong heterogeneity ($\delta r/2$ is greater than the distance to the edge of chaos) yield WCRNNs with unstable learning dynamics, see Figure \ref{fig:hete_crit}A.
%
This can be explained by the fact that the heterogeneity of the residual matrix increased the diversity of memory timescales in the Lyapunov spectrum and brought the network dynamics closer to the edge of chaos, resulting in longer memory timescales, see Figure \ref{fig:hete_crit}B.
%
Moreover, the heterogeneity in subcritical networks improved the gradient propagation and increased the number of non-vanishing eigenvalues of the Hessian, making the learning dynamics richer, see Figure \ref{fig:hete_crit}C and D.
%
In contrast, for networks with dynamics close to the edge of chaos, the benefit of having heterogeneous residuals reduced, because heterogeneity increased the risk of exploding gradients (data not shown).
%
% Figure environment removed

%
\begin{table}
  \caption{Practical expressivity of informed heterogeneous WCRNNs compared to homogeneous WCRNNs with highest performance, assessed by best test accuracy and convergence speed}
  \label{t:het}
  \centering
  \begin{tabular}{ p{2cm} p{3cm} p{3cm} p{3cm} p{3cm}  }
    \toprule
     & Homogeneous
    & Heterogeneous
    & Homogeneous
    & Heterogeneous \\
    &accuracy 
    &accuracy
    &speed
    &speed \\
     &\%/RMS 
    &\%/RMS
    &tr. iterations
    &tr. iterations \\ [0.5ex]
    \midrule
    sMNIST & 96.05   & 98.22 & 200 & 200  \\
    spMNIST  & 92.66  & 95.37 & 2076 & 1038  \\
    ADD100 & $2\text{e-}5$   & $4.6\text{e-}5$ & 9298 & 8698    \\
    ADD200  & $5.1\text{e-}5$   & $3.3\text{e-}5$ & 2.4\text{e+}4 & 3.1\text{e+}4     \\
    ADD400  & $1.5\text{e-}4$   & $3.1\text{e-}4$ & 7.4\text{e+}4 & 9.5\text{e+}4   \\
    ADD800  & $9.1\text{e-}4$   & $19.1\text{e-}4$ & 2.9\text{e+}5 & 4.1\text{e+}5   \\
    \bottomrule
  \end{tabular}
\end{table}
%
Next, we considered the WCRNNs with residual matrices that are a product of an orthonormal matrix of Equation (\ref{eq:rot}) and the heterogeneous matrix (\ref{eq:het}).
%
It is important to note that any square matrix can be presented as a product of such matrices with an appropriate coordinate transformation, making the generalization of our results to other matrices straightforward.
%
Furthermore, we allowed for unit-specific heterogeneity by substituting the scalar coupling constant from (\ref{eq:general_residual}) by a vector $\boldsymbol{\gamma}$.
%
Informed by previous experiments, we sampled $\mathbf{r}_{i}$ from $U([0.99,1])$, $\phi_{i}$ from $U([0,\pi/4])$, and $\boldsymbol{\gamma}_i$ from $U([0.005, 0.05])$
%
We found that WCRNNs with such informed heterogeneity performed on par with the best homogeneous configuration of WCRNNs on all datasets considered, see Table \ref{t:het}.
%
Our results show that the variety of memory timescales present in WCRNNs with heterogeneous residuals allows them to generalize well over different datasets and therefore obtain increased practical expressivity compared to networks with homogeneous residuals.
%
In particular, it follows that informed heterogeneity can be used to avoid a computationally expensive search for the best performing residual configuration.

\subsection{Non-linear residuals}
\label{ss:init}
%
So far, we have only studied WCRNNs for which the residual was given by a linear map.
%
In this section, we study two non-linear variants of WCRNNs of the form
%
\begin{equation}
\mathbf{x}_{t+1} =  \sigma(\mathbf{R}\mathbf{x}_t)  + \gamma \sigma(\mathbf{W}\mathbf{x}_t + \mathbf{S}_t).
\label{eq:non_res}
\end{equation}
%
and
%
\begin{equation}
\mathbf{x}_{t+1} =  \sigma(\mathbf{R}\mathbf{x}_t  + \gamma (\mathbf{W}\mathbf{x}_t + \mathbf{S}_t)).
\label{eq:weak_init}
\end{equation}
%
For ease of notation, we will refer to networks defined by (\ref{eq:non_res}) as type A and to networks defined by (\ref{eq:weak_init}) as type B.
%
Note that the observations from Section \ref{s:resid} also hold for the non-linear case presented here, meaning that the weak coupling ensures that the memory properties of the networks (\ref{eq:non_res}) and (\ref{eq:weak_init}) are still mostly determined by the residual connections.
%
Moreover, when the non-linearity is not in a strongly saturating regime, the eigenvalues of the residuals matrices of both types A,B still influence network dynamics to a large extent. 
%
This is why we continue to classify nonlinear WCRNNs into subcritical, critical, and supercritical networks as before.
%
We note that due to the presence of the non-linearity $\sigma$, this classification is less strict than for WCRNNs with linear residuals. 
%
To investigate these networks, we performed the same set of experiments on the MNIST-based datasets as in Sections \ref{ss:transition}, \ref{ss:rot}, and \ref{ss:hete}. 

% Figure environment removed 
%
When training networks of both types, we found the same trade-off between convergence speed and stability of learning dynamics as observed in networks with linear residuals.
%
Similarly to linear networks, the non-linear WCRNNs with the highest performance and fastest convergence speed were the networks that have eigenvalues of residual matrices close to critical value $1$, see Figure \ref{fig:weak_init} A.
%
Subcritical networks of type A showed the best performance and the fastest convergence, in contrast to type B networks, for which the best practical expressivity was achieved by weakly supercritical networks.
 %
For both studied non-linear variants of WCRNNs, we also found peaks in the convergence speed and performance when trained with rotational residuals on sMNIST, see in Figure \ref{fig:weak_init} B.
%
Interestingly, we found that performance of type B showed stronger sensitivity to angular frequencies of rotational residuals compared to type A.
%
These differences can be explained by the the fact that one common non-linearity better prevents chaotic dynamics, but makes biases from initialization have a stronger effect on performance.
%
We also observed that both network types were able to achieve similar performance levels as their highest performing homogeneous variants when equipped with informed heterogeneity from Section \ref{ss:hete} (data not shown).
%

Notably, the network of type B can be considered as a weakly coupled residual initialization of an Elman RNN, because the residual matrix and other weight matrices are subject to the same non-linearity.
%
In our experiments, the classic Elman network did not achieve high performance on sMNIST in agreement with previous studies~\citep{arjovsky2016unitary}. 
%
The sMNIST dataset remained challenging for such Elman networks even after applying standard techniques such as identity residuals, gradient clipping, and orthogonal initialization (data not shown), emphasizing the significant advantage of weakly coupled residual initialization (\ref{eq:weak_init}).
%
We also want to note that as Elman networks are part of many recurrent architectures, our results can be used to introduce informed biases in memory of other recurrent architectures.

\section{Conclusion}

\noindent
In this work we introduced weakly coupled residual recurrent networks (WCRNNs) and studied how their recurrent residual connections influence network dynamics, properties of fading memory, and practical expressivity.
%
A dynamical systems analysis of WCRNNs allowed us to uncover a connection between network dynamics and the weight dynamics resulting from BPTT training.
%
On the basis of this analysis, we predicted a trade-off between the stability of learning dynamics and convergence speed of backpropagation algorithm.
%
In line with this prediction, simulation results showed that networks with dynamics close to the edge of chaos achieve greater practical expressivity than more subcritical or supercritical networks.
%
Moreover, we found that several classes of informed residual connections could yield effective inductive biases for WCRNNs.
%
In particular, rotational residuals were found to be beneficial when they matched the characteristic spectral properties of the data, residuals resulting in subcritical fading memory were found to be favorable when long-term dependencies were present in the data, and heterogeneous residuals were found to increase the networks' practical expressivity and provide an informed range of memory properties.
%
In addition, heterogeneity can help to avoid the computationally expensive search required to find the best performing configuration for a homogeneous network.

Our results are consistent with previous studies on the benefits of residual networks with dynamics at the edge of chaos~\citep{schoenholz2016deep}, and furthermore provide an explanation for the previously shown computational advantage of recurrent networks consisting of oscillatory units over non-oscillating architectures~\citep{ norcliffe2020second, rusch2020coupled,effenberger2022biology}. 
%
Our findings also agree with studies in the field of neuroscience, indicating that the brain seems to operate close to criticality but in a slightly subcritical regime, as this has computational advantages~\citep{wilting2018operating, wilting201925}.
%
In addition, our results also agree with recent studies that emphasize the functional role of neural heterogeneity~\citep{perez2021neural, effenberger2022biology}.

We also anticipate that our results will be relevant in the context of continual learning, where effective memory is known to be essential to avoid catastrophic forgetting~\citep{hadsell2020embracing}.
%
Furthermore, we hypothesize that our approach to incorporate informed biases into residuals could find its application in feedforward architectures, because residual connections are widely used in a range of modern architectures~\citep{ronneberger2015u,he2016deep,vaswani2017attention}.

In a broader context, we believe that the findings presented here are in line with a recent interest in RNNs, seeking to overcome the EVGP and training inefficiencies~\citep{orvieto2023resurrecting}.
%
These developments show that the careful design of RNNs can result in state-of-art performance on long range memory tasks.
%
In some cases, RNNs were shown to surpass the performance of feed-forward Transformer-based architectures~\citep{tay2022efficient}, while overcoming the drawbacks of dot-product attention as implemented in Transformer architectures, where memory and computational complexity exhibit quadratic scaling with sequence length~\cite{peng2023rwkv}.
%
Here, we show how the practical expressivity and training stability of RNNs are influenced by their dynamics, and how heterogeneous and informed residuals can increase further practical expressivity without increasing system size.
%
This is an important step forward in the understanding of residual RNNs on both a theoretical and practical level. 

In summary, our results show how the properties of fading memory resulting from RNN dynamics play a crucial role in shaping the backpropagation-induced learning dynamics, and how the implementation of informed memory properties by means of residual connections can improve the expressivity of RNNs in practice.


\bibliographystyle{elsarticle-harv} 
\bibliography{references}

\appendix
\clearpage

\section{Algorithm for computation of Lyapunov exponents}
\label{a:algo}

Here, we explain how we compute the Lyapunov exponents for WCRNNs defined by equation (\ref{eq:disc_map}). In theory, Laypunov exponents can be computed directly from the Oseledts equation (\ref{eq:ose}), but in practice the $P$-dimensional volume of the tangent space tends to align with the eigenvector associated with the largest eigenvalue, resulting in degenerate matrices and therefore numerical problems. To overcome such problems, an orthonormalization procedure was introduced; see, for example,~\citep{sandri1996numerical}. This procedure orthonormalizes the tangent space along a system trajectory that gives more stable estimates of the scaling of the $P$ dimensional volume. For the case of a discrete system (\ref{eq:disc_map}), we iterate it alongside with its variational equation
%
\begin{equation}
\delta \mathbf{P}(t+1) = \mathbf{J}_{\mathbf{x}}(f^t(\mathbf{x}_0)) \delta \mathbf{P}(t).
\label{eq:a_var_general}
\end{equation}
%
If we apply the chain rule for $t$ time steps, we obtain Equation (\ref{eq:var_general_vol}) from the main text

\begin{equation}
\delta \mathbf{P}(t+1) = \mathbf{V}_{\mathbf{x}}(f^t(\mathbf{x}_0)) \delta \mathbf{P}(0), 
\label{eq:a_var_main_text}
\end{equation}

where $\mathbf{V}_{\mathbf{x}}(f^t(x_0)) = \mathbf{J}_{\mathbf{x}}(f^{t}(x_0))\mathbf{J}_{\mathbf{x}}(f^{t-1}(\mathbf{x}_0))...\mathbf{J}_{\mathbf{x}}(f(\mathbf{x}_0))$. To compute the Lyapunov exponents, we need to estimate the eigenvalues of the variational term $\mathbf{V}_{\mathbf{x}}(f^t(x_0))$ with the initial condition of $\delta \mathbf{P}(0)=\mathbf{I}$.  

The orthonormalization procedure is performed as follows. After every iteration of the system (\ref{eq:disc_map}), we compute its Jacobian and evolve the variational equation (\ref{eq:a_var_general}), where the $P$ dimensional volume of the tangent space is initialized with the identity matrix. Next, we perform a QR decomposition on the resulting volume of the tangent space $\mathbf{Q}_0$. The QR decomposition of $\mathbf{Q}_0$ produces two matrices $\mathbf{Q}_1$ and $\mathbf{R}_1$, where the orthonormal matrix $\mathbf{Q}_1$ defines the rotational component, and the diagonal elements of $\mathbf{R}_1$ define the volume scaling. Finally, we collect the eigenvalues of $\mathbf{R}$, which describe the instantaneous transformation of the volume, and perform the next iteration with a new volume from the tangent space $\mathbf{Q}_1$. Furthermore, if we decompose every variational term into $\mathbf{V}_{\mathbf{x}}(f^k(\mathbf{x}_0))=\mathbf{Q}_k\mathbf{R}_k$ and evolve the volume $\mathbf{Q}_k$ for $k+1$ iterations, the final variational term is 

\begin{equation}
\mathbf{V}_{\mathbf{x}}(f^t(\mathbf{x}_0)) =  \mathbf{Q}_t\mathbf{R}_t \dots \mathbf{R}_1 .
\label{eq:a_normalization}
\end{equation}

From (\ref{eq:a_normalization}) it follows that if the logarithms of the eigenvalues $\mathbf{R}_t$ converge, they define unique Lyapunov exponents according to the Oseledets equation (\ref{eq:ose}). Therefore, we show the convergence of eigenvalues of the variational term in all of our figures. We also want to note that this method is usually applied to autonomous systems, but the design of the residuals of the system (\ref{eq:general_residual}) allows us to study non-autonomous dynamics. 

The pseudocode for the orthonormalization algorithm is as follows:

\begin{algorithm}
\caption{Collect eigenvalues of the variational term}
\begin{algorithmic} 
\STATE \textbf{Initialize:} $\mathbf{x}, \mathbf{Q}, \mathbf{I}$
\WHILE{$t \leq L$}
\STATE $\mathbf{x} \leftarrow \mathbf{f}(\mathbf{x},\mathbf{I})$
\STATE $\mathbf{J} \leftarrow \dv{\mathbf{f}}{\mathbf{x}}$
\STATE $\mathbf{Q} \leftarrow \mathbf{J}\mathbf{Q}$
\STATE $\mathbf{Q},\mathbf{R} \leftarrow qr(\mathbf{Q})$
\STATE $\lambda \gets \text{append}(\operatorname{diag}(R))$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

\clearpage
\section{Memory properties}
\label{a:memory}
\setcounter{figure}{0}
%
% Figure environment removed
%
\clearpage
\section{Weak coupling}
\label{a:weak}
\setcounter{figure}{0}

Weak coupling plays a crucial role in the design of WCRNNs. 
%
As shown in Equation (\ref{s:resid}), the weak coupling defines limits for fluctuations in the variational term. 
%
This way the weak coupling ensures that memory properties of network dynamics have only weak dependence on external input.
%
Therefore, such stability allows us to study the influence of the memory properties on the learning dynamics of WCRNNs.
%
Importantly, from Equation (\ref{eq:res_grad_prop}), it follows that
%
\begin{equation}
\nabla_{w}L= \mathcal{O}(\gamma),
\label{eq:a_grads}
\end{equation}
%
meaning that the weak coupling acts as non-selective global multiplier for gradients up to higher order terms of $\gamma$.
%
Thus,  the coupling parameter $\gamma$ resembles the learning rate hyperparameter $\eta$, for small values.
%
Here, we present a comparison of 4 configurations, where $\gamma$ equals 0.01 and 0.001, and $\eta$ equals 0.1 and 0.01, see Figure \ref{fig:a_gamma}.
%
We show that the learning dynamics is unstable for the configuration $\gamma=0.01$ and $\eta=0.1$, and if we decrease $\gamma$ (Figure \ref{fig:a_gamma} B) or $\eta$ (Figure \ref{fig:a_gamma} C), the convergence of learning trajectories becomes more stable but slower.
%
We also show that the cases of $\gamma=0.001$ and $\eta=0.01$ exhibit very slow and stable convergence, see Figure \ref{fig:a_gamma} D.
%
% Figure environment removed

\clearpage
\section{Supplementary figures for rotational residuals}
\label{a:init}
\setcounter{figure}{0}

% Figure environment removed

\end{document}