\documentclass[a4paper,10pt]{scrartcl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{a4wide}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\pgfplotsset{compat=newest}
\newlength{\fwidth}
\newlength{\fheight}

\usepackage{todonotes} 

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}[2]{\mathbb{P}_{#1}\left[#2\right]}
\newcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\I}{\mathbb{I}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\V}[2]{\mathbb{V}_{#1}\left[#2\right]}
\newcommand{\OP}{\mathcal O_{\mathbb P_0}}
\newcommand{\toP}{\stackrel{\mathbb P}{\rightarrow}}
\newcommand{\toD}{\stackrel{\mathcal D}{\rightarrow}}
\newcommand{\p}{\mathrm{pen}}
\newcommand{\pen}[2]{\p_{#1}\left(#2\right)}
\newcommand{\1}{\mathbf{1}}
\newcommand{\ms}{\mu \mathrm{s}}
\newcommand{\nm}{\mathrm{nm}}
\newcommand{\LINK}{\url{https://go.uniwue.de/math-ams}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{examples}[theorem]{Examples}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}[theorem]{Definition}

%\renewcommand{\thefootnote}{\arabic{footnote}}

%\allowdisplaybreaks

%preamble

\begin{document}

	\begin{center}
	\begin{minipage}{.8\textwidth}
		\centering 
		\LARGE Multiscale scanning with nuisance parameters\\[0.5cm]
		
		\normalsize
		\textsc{Claudia K\"onig}\\[0.1cm]
		\verb+claudia-juliane.koenig@mathematik.uni-goettingen.de+\\
		Institute for Mathematical Stochastics, University of G\"ottingen\\[0.3cm]
		
		\textsc{Axel Munk}\\[0.1cm]
		\verb+munk@math.uni-goettingen.de+\\
		Institute for Mathematical Stochastics, University of G\"ottingen\\
		and\\
		Felix Bernstein Institute for Mathematical Statistics in the Bioscience, University of G\"ottingen\\
		and\\
		DFG Cluster of Excellence "Multiscale Bioimaging - From Molecular Machines to Networks of Excitable Cells", University of G\"ottingen Medical Center\\[0.3cm]
		
		\textsc{Frank Werner}\footnotemark[1]\\[0.1cm]
		\verb+frank.werner@uni-wuerzburg.de+\\
		Julius-Maximilians-Universität Würzburg (JMU), Institute of Mathematics, Würzburg, Germany
	\end{minipage}
\end{center}

\footnotetext[1]{Corresponding author}	

\begin{abstract}
We {develop a multiscale scanning method} to find anomalies in a $d$-dimensional random field in the presence of nuisance parameters. This covers the common situation that either the baseline-level or additional parameters such as the variance are unknown and have to be estimated from the data. We argue that state of the art approaches to determine asymptotically correct critical values for multiscale scanning statistic{s} will in general fail when such parameters are naively replaced by plug-in estimators. Opposed to this, we suggest to estimate the nuisance parameters on the largest scale and to use {(only) smaller} scales for multiscale scanning. We prove a uniform invariance principle for the resulting adjusted multiscale statistic (AMS), which is widely applicable and provides a computationally feasible way to simulate asymptotically correct critical values. We illustrate the implications of our theoretical results in a simulation study and in a real data example from super-resolution STED microscopy. This allows us to identify interesting regions inside a specimen in a pre-scan with controlled family-wise error rate.
\end{abstract}

\textit{Keywords:} scan statistic, invariance principle, limit theorem, multiscale analysis, super-resolution microscopy\\[0.1cm]

\textit{AMS classification numbers:} 60F17, 62H10, 60G50, 62F03.  \\[0.3cm]
\date{\today}

\section{Introduction}


Scanning a $d$-dimensional random field $Y = \left(Y_i\right)_{i\in \left\{1,...,n\right\}^d}$ of random variables (r.v.'s) $Y_i$ for anomalies in their distribution is a prominent topic in statistical {research} which is of great practical use. Applications range from the detection of anomalous clusters in social and biological networks and graphs \citep{accd11,cn14,srs16,bamh20} {to} detection of structural changes in imaging \citep{s18,cmv02,adh06,kkm23}{,} genomics \citep{rdj14,fhms14}, brain research \citep{lpstv21} and cell microscopy \citep{pwm18}, to mention a few.

\subsection{Setup}

In this paper, we consider a parametric model of the form
\begin{equation}\label{eq:model}
	Y_i \sim F_{\theta_i, \xi},\qquad i \in I_n^d := \left\{1,...,n\right\}^d,
\end{equation}
where the $Y_i$ are independent, real valued r.v.'s distributed to some unknown $F_{\theta_i, \xi}$ from a given, known class of distributions $\mathcal F$ indexed in $\theta$ and $\xi$. {Here, $i$ denotes a multi-index, which we assume to index a $d$-dimensional grid $I_n^d = \{1,...,n\}^d$. In what follows}, $\theta_i \in \Theta$ denotes the parameter of interest, $\xi \in \Xi$ a collection of nuisance parameters, and $\Theta \subset \R^{d_1}$ and $\Xi\subset \R^{d_2}$ are the corresponding parameter spaces. {Note, that $\theta_i$ may vary between observations $Y_i$, whereas the nuisance parameter $\xi$ is assumed to be the same for all $i \in I_n^d$}, but we stress that our analysis can also be transferred to varying $\xi$. An exemplary case for model \eqref{eq:model} would be a Gaussian field $Y_i\sim \mathcal N \left(\theta_i, \Sigma\right)$, where anomalies $\theta_i$ occur in the mean as deviation from a ground truth $\theta_0 \in\Theta$ {and} the unknown covariance $\Sigma > 0$ is the nuisance parameter. For the moment, we do not pose structural assumptions on $F_{\theta, \xi}$, but require some smoothness and tail properties later on (see Assumption \ref{ass:model} below).

\subsection{Anomaly detection}

Given a baseline or reference intensity $\theta_0 \in \Theta$ and a region $R \subset I_n^d$, the problem to detect an anomaly inside $R$ can be formulated as the testing problem
\begin{subequations}\label{eq:single_testing}
	\begin{equation}\label{eq:single_testing_hypothesis_pois}
		H_{R,n}:~\forall~i \in R: \theta_i = \theta_0
	\end{equation}
	vs.
	\begin{equation}\label{eq:single_testing_alternative_pois}
		K_{R,n}:~\exists~i \in R \text{ s.t. }\theta_i \neq \theta_0.
	\end{equation}
\end{subequations}
Note, that in practice often $d_1 = 1$, {i.e. $\theta_i \in \R$,} and it might be more relevant to consider alternatives of the form $\theta_i > \theta_0$ or $\theta_i < \theta_0$. For the sake of brevity, we consider only two-sided {alternatives as in \eqref{eq:single_testing_alternative_pois}} here, but we emphasize that the one-sided situation can be developed completely analogous (see Remark \ref{rem:one_sided} below).

Depending on the structure of $R$, the likelihood ratio test (LRT) is known to have {favourable} properties, see e.g. \cite{lr05}. Therefore, throughout this paper, we consider the corresponding log LRT statistic
\begin{equation}\label{eq:TR}
	T_R \left(Y, \theta_0, \xi\right) := \sqrt{2 \log \left(\frac{\sup_{\theta \in\Theta} \prod_{i \in R} f_{\theta, \xi} \left(Y_i\right)}{\prod_{i \in R} f_{\theta_0, \xi} \left(Y_i\right)}\right)}
\end{equation}
for \eqref{eq:single_testing}, where $f_{\theta, \xi}$ denotes the density or probability distribution function of $F_{\theta, \xi}$. Whenever this statistic exceeds a suitably chosen threshold (depending on $R$ and the prescribed level $\alpha \in \left(0,1\right)$), the corresponding LRT will reject.

\subsection{Multiscale scanning}\label{subsec:scanning}

However, as position and size of anomalies will in general be unknown, the region of interest $R$ is not known in advance. Therefore, we aim to \textit{scan} over $\mathcal{R}_n \subset 2^{I_n^d}$ (with $2^X $ being the power set of $X$), a set of regions of interest (e.g. the system of all discrete cubes inside $I_n^d$). {If the system $\mathcal R_n$ covers a range of spatial scales, then this approach is denoted as} \textit{multiscale scanning}{, which} has a long history in statistics, see e.g. \citet{k97} for an early reference and \citet{adh06,w10,accd11,n12,mbs13,cw13,sac16,pwm18,kmw20} for more recent works. A particular instance is multiscale change point detection, see e.g. \citet{fms14}, where the field is of spatial dimension $d = 1$, or \citet{cws22} for arbitrary dimension $d$, and the references given there. 

Outgoing from \eqref{eq:single_testing}, {multiscale scanning} can be treated as a multiple testing problem
\begin{equation}\label{eq:multiple_testing}
	H_{R,n} \text{ vs. }K_{R,n} \qquad\mathbf{simultaneously}\text{ over } \mathcal R_n.
\end{equation}
The resulting scanning test then has to combine the local test statistics $T_R$, $R \in \mathcal R_n$, in \eqref{eq:TR} {in a suitable way}, such that the number of false rejections over $\mathcal R_n$ can be controlled. To this end, we aim to bound (asymptotically) the family wise error rate (FWER) by $\alpha \in \left(0,1\right)$, i.e. \citep[see e.g.][]{m81,d14} 
\begin{equation}\label{eq:FWER}
	\sup\limits_{R \in \mathcal R_n} \P{H_{R,n}}{\text{``any false rejection in }R\text{''}} \leq \alpha + o(1) \qquad\text{as}\qquad n\to \infty.
\end{equation}
To combine the local test statistics {in a multiscale fashion}, it is pertinent to employ a scale calibration $\widetilde{\omega}{_n}, \omega{_n} : {(0,\infty)} \to \left(0,\infty\right)$ to avoid small regions $R \in \mathcal R_n$ from dominating the overall statistic. Here, and in what follows, the cardinality $\left|R\right| \in \mathbb N$ of a region $R \in \mathcal R_n$ is denoted as \textit{scale}. This leads to
\begin{align}\label{eq:Tn}
	T_n \equiv T_n (Y, \mathcal R_n, \theta_0, \xi):=\max_{R\in\mathcal R_n} \widetilde{\omega}{_n}\left(\left|R\right|\right)\left[ T_R\left(Y, \theta_0, \xi\right)-\omega{_n}\left(\left|R\right|\right)\right],
\end{align}
which we then call a (calibrated) \textit{multiscale test statistic} whenever a range of scales is covered by $\mathcal R_n$. An appropriate choice of the calibration $\widetilde{\omega}{_n}$ and $\omega{_n}$ is of high importance for the overall method and will determine its detection properties. Different choices of $\widetilde{\omega}{_n}$ and $\omega{_n}$ have been proposed in the literature, mostly to achieve a non-degenerate limiting behaviour of $T_n$ or to optimize the detection power of the multiple test on all or specific scales. Besides uncalibrated scan statistics used e.g. in \citet{k97,a97,a98,km08,n12,pywr22}, we mention \citet{ds01}, who proposed a calibration ensuring non-degenerateness of the limiting distribution, and a more recent multivariate analogue by \citet{ds18}. Building on this, \citet{dw08} proposed a calibration, which was later shown to be superior over the uncalibrated scan statistic in \citet{cw13}, and which is since then widely used in the community, see e.g. \citet{rw13,smd13,fms14,kmw20}. More recently, other calibrations have been suggested, e.g. leading to Gumbel limits \citep{sac16,pwm18}, see also \citet{w20} for a recent review on this topic. In this study, we mostly leave the choice of $\widetilde{\omega}{_n}$ and $\omega{_n}$ to the user up to some mild condition (cf. Assumption \ref{ass:Omega}) on its growth posed below. This choice may e.g. reflect a preference for detecting signals of certain size, which is driven by the application at hand.

The multiscale scanning methodology is now as follows. Given $Y$, the values $T_R \left(Y, \theta_0, \xi\right)$, $R \in \mathcal R_n$ are computed, and all hypotheses $H_{R,n}$ such that
\begin{equation}\label{eq:reject}
	T_R \left(Y, \theta_0, \xi\right) \geq c_{|R|}(\eta) := \frac{\eta}{\widetilde{\omega}{_n}(|R|)} + \omega{_n}(|R|)
\end{equation}
are rejected. Here, $\eta \in \R$ is a global threshold to be determined appropriately. Hence, the scale calibration $\widetilde{\omega}{_n}, \omega{_n}$ leads to \textit{scale dependent} local critical values in general, see also \citet{w10,psm17}. The global threshold $\eta$ is now universal, however, and to be chosen such that 
\begin{equation}\label{eq:threshold}
	\P{0}{T_n \left(Y, \mathcal R_n, \theta_0, \xi\right)>\eta} \leq \alpha + o(1)\qquad\text{as}\qquad n \to \infty,
\end{equation}
for some $\alpha \in \left(0,1\right)$. {Here and in what follows, $\mathbb P_0$ denotes the law of $Y$ under the global null hypothesis, i.e. $\mathbb P_0 := \mathbb P_{H_{I_n^d},n}$. Since controlling the false positives over $H_{I_n^d}$ implies control over any system of sub-hypotheses $H_{R,n}$, $R \in \mathcal R_N$, it follows that}
\[
\sup\limits_{R \in \mathcal R_n} \P{H_{R,n}}{\text{``any false rejection in }R\text{''}}  \leq \P{0}{\text{``any false rejection in }I_n^d\text{''}},
\]
{i.e. \eqref{eq:threshold}} ensures \eqref{eq:FWER} to hold true, and hence any of the local rejections is (asymptotically) correct with probability $\geq 1-\alpha$. 

{This principle to control the FWER underlies (often implicitly) many modern multiscale scanning methods}. One central example is to scan an independent Gaussian field $Y$ for anomalies (elevated means), where $\theta_ 0 =\mu_0 = 0$ and $\xi = \Sigma_0$ is the known covariance. 
%Multiscale scanning has e.g. been used by \citet{ds01,dw08} to obtain uniform confidence statements about qualitative properties of a density, in \citet{adh06} to detect filamentary structures, or in \cite{pwm18} to infer on the support of an indirectly observed quantity. 
In this case, optimality properties of this method, depending on the chosen calibration $\widetilde \omega{_n}$ and $\omega{_n}$, are known, see e.g. \citet{cw13} for a statement about minimax optimality when using just a single scale, or \citet{sac16} and \citet{kmw20} for an oracle property revealing that scanning over several scales is asymptotically as efficient as scanning over the correct scale only. 

\subsection{Choosing the global threshold $\eta$}

{The remaining and potentially difficult question is how to choose the threshold $\eta$ such that \eqref{eq:threshold} is valid. In principle, this requires knowledge of the distribution of $T_n$ under $\mathbb P_0$, which is not easy to obtain, in general. If the distributions $F_{\theta,\xi} = F_{\mu, \sigma^2}$ are Gaussians with known nuisance parameter $\xi = \sigma^2$ (variance), and if also the reference intensity $\mu = \theta_0$ (mean) is known, then it readily turns out that all local test statistics $T_R$ are constituted by Gaussians again, as
	\[
	T_R\left(Y, \mu,\sigma\right) = \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} \frac{Y_i - \mu}{\sigma} \right|
	\]
	in this case. Consequently, $T_n$ can be simulated as the maximum over finitely many (dependent) Gaussian random variables. This way, it is possible to obtain empirical quantiles for $T_n$ under $H_0$ and to choose $\eta$ accordingly.}

For many other distributional settings such as Binomial or Poisson distributions, the situation is slightly more delicate. First steps have been taken by \citet{a97,a98,asn16}, who proposed distributional approximations of $T_n$ in a Poissonian setting {under knowledge of a baseline intensity $\theta_0$}. However, those results are limited to one fixed scale. Another approach is based on tail bounds for $T_R$ as proven in \citet{w22}, which can be combined with a Bonferroni correction to ensure \eqref{eq:threshold} also in a multiscale setting. Complete multiscale approximations of $T_n$ in \eqref{eq:Tn} have been proposed in case of Bernoulli and Binomial fields by \citet{w10} or in a {more} general exponential family setting by \citet{kmw20} under knowledge of $\theta_0$ and $\xi$. The latter result provides an invariance principle, which allows (given the baseline parameter $\theta_0$ and the nuisance parameters $\xi$) to approximate the distribution of $T_n$ by {the distribution in the Gaussian case as above, i.e.}
\begin{equation}\label{eq:Mn}
	M_n \equiv M_n\left(\mathcal R_n\right) := \max_{R\in\mathcal R_n} \widetilde{\omega}{_n}(|R|)\left[ \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right| - \omega{_n}(|R|) \right]
\end{equation}
with i.i.d. standard normal r.v.'s $X_i$, $i \in I_n^d$. Consequently, it is possible to simulate an appropriate threshold $\eta$ in \eqref{eq:threshold} in practice as soon as $\theta_0$ and $\xi$ are known. The rationale behind approximating $T_n$ in \eqref{eq:Tn} by $M_n$ in \eqref{eq:Mn} is, that for sufficiently large scales, a CLT implies that $T_R \approx \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right|$ in distribution. However, to ensure that $M_n$ in fact approximates $T_n$, this CLT has to hold true uniformly over all sets in $\mathcal R_n$, i.e. it is required to couple the process {(or at least its maximum)} $\left(T_R\right)_{R \in \mathcal R_n}$ with $\left(\left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i\right|\right)_{R \in \mathcal R_n}$ {(or the maximum, respectively)} and to account for the calibration. Based on classical {strong approximation} results \citep{kmt75}, this approach has also been used in \citet{fms14}, and more recently been extended in \citet{pwm18,kmw20} by using explicit couplings for the suprema of empirical processes developed by \citet{cck14}.

\section{Proposed Method: adjusted multiscale scanning (AMS)}

In contrast to existing work, the focus of this study is, however, on the case that both $\theta_0$ and the nuisance parameter $\xi$ in \eqref{eq:model} are unknown, {a typical situation in practice}. Note, that this changes the overall situation completely, as the local LRT statistics $T_R \left(Y, \theta_0, \xi\right)$ are no longer accessible. A common remedy which immediately comes to mind is to pre-estimate both $\theta_0$ and $\xi$ from $Y$ globally and then use the corresponding estimators $\hat \theta_0$, $\hat \xi$ as a plug-in in $T_R$ and consequently in the multiscale extension $T_n$ in \eqref{eq:Tn}. When one aims to \textit{estimate} $\theta_0$ only, then such an approach is known as pseudo maximum likelihood, and has e.g. been proposed in \citet{gs81}. Under certain regularity conditions, they prove asymptotic normality of the resulting estimator, but this result is limited to a single scale and cannot be transferred to the case of multiscale testing. A rigorous analysis of a similar approach in case of \textit{testing} is -- to the best of our knowledge -- missing. In fact, we will show that the distribution of
\[
{\hat T_n := T_n \left(Y,\mathcal R, \hat \mu_0, \hat \sigma_0\right)}
\]
differs {also asymptotically} from that of $T_n \left(Y, \mathcal R_n, \theta_0, \xi\right)$ (and hence also from that of $M_n\left(\mathcal R_n\right)$ in \eqref{eq:Mn}) in general, and this is the case even in the most simple Gaussian example. This then raises the question how to adjust the threshold $\eta$ in \eqref{eq:threshold} properly, an issue which has often been overlooked and has not been discussed in the literature. A notable exception is \citet{wp22}, where the tail bounds for $T_R \left(Y, \hat \theta_0, \hat \xi\right)$ in \cite{w22} are combined with a Bonferroni correction to construct corrected values for $\eta$ in \eqref{eq:threshold}. This approach is, however, limited to relatively small systems $\mathcal R_n$ of regions. In this paper, we will close the aforementioned gap and we will show that is still possible to obtain a uniform invariance principle (see Section \ref{sec:theory}) for the distribution of $\hat T_n$ as long as the scales in $\mathcal R_n$ are not too small or too large compared to the sample size $n^{d}$. More precisely, {under this restriction,} we will show that $\hat T_n$ can be approximated in distribution by $M_n \left(\mathcal R_n\right)$ as in \eqref{eq:Mn}, which can be pre-simulated given $\mathcal R_n$ in advance. From this it becomes apparent that working with thresholds, which do not incorporate the error in estimating $\theta_0$ and $\xi$, does not provide the correct FWER, even asymptotically. The suggested adjusted multiscale scanning (AMS) procedure provides correct thresholds for estimated values of $\theta_0$ and $\xi$.

{Let us briefly compare the invariance principle in the paper at hand with previous results, especially those from \citet{kmw20}. There, an approximation of $T_n$ in \eqref{eq:Tn} by $M_n$ in \eqref{eq:Mn} has been proven in the case that $F_{\theta,\xi}$ is a natural exponential family, both $\theta_0$ and $\xi$ are known, and the calibration functions $\omega{_n}, \widetilde{\omega}{_n}$ are explicitly chosen as in \cite{dw08}, see also Example \ref{ex:omega} below. In the paper at hand, we extend the above result by dropping all three restrictions. First of all, we allow for distributions $F_{\theta,\xi}$ beyond the exponential family setting. We will elaborate in Example \ref{ex:distributions} below that e.g. the Gamma distribution can be handled by our theory but not by that from \citet{kmw20}. However, the major contribution of the paper at hand is the presence of nuisance parameters (including e.g. the reference intensity $\theta_0$), which is of serious importance in practice and serves as the major motivation for our work. This will be illustrated by our data examples in Section \ref{sec:data_analysis}. Finally, we allow for a wide choice of scale calibrations (see Assumption \ref{ass:Omega} below) and do not restrict ourselves to one specific choice.}

\subsection{Failure of naive plug-in in the Gaussian model}\label{subsec:gauss}

{Let us start by explaining why} simply replacing $\theta_0$ and $\xi$ by estimators $\hat\theta_0$ and $\hat \xi$ will lead to invalid thresholds $\eta$ in \eqref{eq:threshold}{. Therefore} consider the homogeneous Gaussian model for $d = 1$ \citep[see e.g.][]{fms14,sac16} as a the most common and best understood example of \eqref{eq:model}. Here, $F_{\theta_i, \xi} = \mathcal N \left(\mu_i, \sigma_0^2\right)$ parametrized by its mean $\mu_i \in\R$ and variance $\sigma_0^2 > 0$. In this case, it can readily be seen that replacing $\mu_0$ and / or $\sigma_0$ by estimators does actually influence the distribution of $T_n$ and requires corrected critical values (thresholds) $c_{|R|}(\eta)$ for the local LRT test statistics. To illustrate this, we have depicted the empirical distributions of $T_n \left(Y,\mathcal R, \mu_0, \sigma_0\right) = M_n(\mathcal R)$ as in \eqref{eq:Mn}, and a corresponding variant with standard estimators $\hat \mu_0 = \frac{1}{n^d} \sum_{i \in I_n^d} Y_i$ and $\hat \sigma_0^2 = \frac{1}{n^d-1} \sum_{i \in I_n^d} \left(X_i - \hat \mu_0\right)^2$ for $n = 128$ and {$\mathcal R = \mathcal D_n^d$ being the set of all rectangles in $2^{I_n^d}$} in Figure \ref{fig:gauss_dist}. As an exemplary scale calibration, we use the common choice $\widetilde \omega{_n} \equiv 1$ and $\omega{_n} \left(|R|\right) = \sqrt{2 \log \left(n^d / |R|\right) + 1}$, see \citet{dw08}.  

% Figure environment removed

It is clearly visible that the distribution of the true scan statistic $T_n$ differs especially for large quantiles substantially from the one involving any of the estimators. {If, e.g., the $80\%$-quantile of $M_n$ is used to calibrate $\hat T_n$, then the corresponding FWER in \eqref{eq:FWER} will be even $\approx 30\%$.}

{The reason for this can already be explained in the easier setting when the} true variance $\sigma_0^2$ is known, i.e. for $T_n \left(Y, \mathcal R, \hat \mu_0, \sigma_0\right)$ with estimated global baseline $\hat \mu_0$. {In this case,} the log LRT statistic $T_R \left(Y,\mu_0,\sigma_0\right)$ compares the local average of $Y$ on $R$ with $\mu_0$, i.e. in case of unknown $\mu_0$ with the global average $\hat \mu_0$ on $I_n^d$. Consequently, for large regions, the value of $T_R \left(Y, \hat \mu_0, \sigma_0\right)$ will be negligible (in particular for $R = I_n^d$ even constantly be $0$), but $T_R \left(Y, \mu_0, \sigma_0\right)$ is not (for $R = I_n^d$, this is proportional to a $\chi^2_{n^d-1}$ distributed random variable). So, as long as the penalization $\widetilde \omega{_n}, \omega{_n}$ does not hinder these large scales from contributing to $T_n \left(Y, \mathcal R, \hat \mu_0, \sigma_0\right)$ (and $T_n \left(Y, \mathcal R, \mu_0, \sigma_0\right)$), the corresponding distributions must be different. On the other hand, a penalization hindering large scales from contributing to $T_n$ is not desired, as this will decrease the overall detection power of the corresponding LRT tests on such scales. It is well known, see e.g. \citet{ds01}, that the penalization chosen here indeed ensures the largest scales to contribute to $T_n \left(Y, \mathcal R, \mu_0, \sigma_0\right)$. Hence, we find that in this case, also the asymptotic distribution (which for this scale penalization is a supremum of a Gaussian process, see \citet{ds01}) should be different. 

Note, that the above observation is not restricted to {a} Gaussian model only, {it effectively} occurs in any model {where the nuisance parameter is estimated and naively plugged into the scan statistic}. The largest scales will in general contribute to the distribution-free statistic $M_n$ in \eqref{eq:Mn}, but will under mild assumptions on $T_R$ (local means) not contribute to $T_n$ in \eqref{eq:Tn} as argued above.

\subsection{Adjusted multiscale scanning -- AMS}

As a consequence, we have to carefully incorporate the influence of estimating global parameters in multiscale scanning in such a way that we allow for a large as possible range of scales. This is the starting point for the subsequent invariance principle in Section \ref{sec:theory}, which is at the heart of AMS. Afterwards, in Section \ref{sec:data_analysis}, we illustrate a different scenario - a base signal, which is not known beforehand, has to be incorporated into the multiscale statistic $T_n$ and hence needs to be estimated from the data.

Whereas in Section \ref{subsec:gauss} the failure of the unadjusted multiscale statistic results from estimating a nuisance parameter, in the subsequent example (Section \ref{sec:data_analysis}) we will face difficulties arising from estimation of the Poisson baseline $\lambda_0$. Both issues can be dealt with the main technical result of this paper, a \textit{uniform} invariance principle for estimated base intensity and nuisance parameters. 

In the following, we will briefly describe a {simplified} version of our main result, for the general result see Theorem \ref{thm:approximation_estimated_intensity} {in Section \ref{sec:theory}}. Suppose for simplicity that the estimators $(\hat{\theta}_n)_{n \in \N}$ and $(\hat{\xi})_{n \in\N}$ are consistent with the parametric rate $n^{-d/2}$ {(as typically achieved by the MLE)}. Furthermore let $\gamma > 0$ an explicitly known exponent depending on $\omega{_n}$ and $\widetilde{\omega}{_n}$, and let $(r_n)_n\subset(0,\infty)$ and $\left(m_n\right)_n \subset \left(0,\infty\right)$ be sequences such that
\[
\log^\gamma\left(n\right) = o \left(r_n\right)\qquad\text{and}\qquad m_n = o \left(n^{-d/2}\right),
\]
as $n \to \infty$. Then, if model \eqref{eq:model} satisfies some mild differentiability properties (see Assumption \ref{ass:model} for details), the distribution of $\hat T_n$ can be approximated by that of $M_n$ in probability as long as $\mathcal R_n$ contains only scales of size between $r_n$ and $m_n$. In other words, we obtain a uniform invariance principle on all not too small (poly-logarithmic in the sample size) and all not too large scales just below the largest ones of order $n^{-d/2}$. {In view of subsection \ref{subsec:gauss}, i}t is obvious that {the latter} condition is necessary to distinguish the baseline signal from local deviations, hence unavoidable in our context. 

If, for a given system of candidate sets $\mathcal R_n$, we introduce the notation
\[
\mathcal R_n (c,d) := \left\{R \in \mathcal R_n ~\big|~ c \leq \left|R\right| \leq d\right\},
\]
then the above result can be formulated as 
\begin{equation}\label{eq:convergence}
	T_n \left(Y, \mathcal R_n\left(r_n,m_n\right), \hat \theta_n, \hat \xi_n\right) - M_n \left(\mathcal R_n\left(r_n,m_n\right)\right) \toP 0,
\end{equation}
as $n \to \infty$. The rate of convergence of this approximation will be made precise in Theorem \ref{thm:approximation_estimated_intensity}, which is (up to log factors) $\OP \left(n^{-d/2} \frac{\sqrt{m_n}}{{r_n}} + r_n^{1/8}\right)$.

Our result holds true for many different calibrations, including the previously mentioned one due to \citet{ds01}, which ensures that $M_n$ converges to a non-degenerate continuous limit (and hence, due to \eqref{eq:convergence}, so does $T_n$). From our result, similar results can be derived for the scale calibrations by \citet{sac16} and \citet{pwm18}, which ensure that $M_n$ converges to a Gumbel distribution.

%We emphasize that $\log^\gamma\left(n\right) = o \left(r_n\right)$ as $n \to \infty$ allows for a (relative to the total number of samples $n^d$) smallest scale size close the sampling rate $1/n$. This can - to the best of our knowledge - not be achieved with classical coupling techniques such as KMT-type constructions \citep[see e.g.][]{kmt75,r93}, but only with recent coupling results for the suprema of processes by \citet{cck14} as exploited in \citet{kmw20,pwm18} in the context of multiscale scanning without nuisance parameters. As mentioned before, we also have to remove the largest scales due to the estimation of $\theta_0, \xi$ in \eqref{eq:Tn}, as our assumptions allow for scale calibrations such that these scales do in fact contribute to $M_n \left(\mathcal R_n\right)$ substantially \citep[see e.g.][]{ds01}.


%
%The computation of $M_n$ and $T_n$ can in principle be carried out under the same way whenever $T_R$ in \eqref{eq:TR} is given by a function of the local mean $\bar Y_R := \sum_{i \in R} Y_i$. In this case, if there is a global shape $B \subset I_n^d$ such that each $R \in \mathcal R_n$ is given as a rescaled and translated version of $B$, i.e. if there exist $t,h \in I_n^d$ with $t_i + h_i \leq n$ for all $1 \leq i \leq d$ such that $\mathbf 1_{R} \left(x\right) = \mathbf 1_B \left(\left(x-t\right)/h\right)$ with $\mathbf 1$ denoting the indicator function, fast computation is possible. Employing the fast Fourier transform, this structure allows for evaluation of $M_n$ in \eqref{eq:Mn} and $\left(T_R\right)_{R \in \mathcal R_n}$ in $\mathcal O \left(d N n^d \log \left(n\right) \right)$ operations as discussed in \cite{kmw20}, where $N$ is the number of scales, i.e. the number of different sizes of rectangles. This will be detailed in Section \ref{sec:implementation}. We provide MATLAB\copyright code for the corresponding algorithms under \todo[inline]{Insert link to code}. 
%

{With the above result, the AMS method is now carried out as depicted in Algorithm \ref{algo:AMS}, which by construction ensures asymptotic control of the FWER, i.e. \eqref{eq:FWER} holds true.
	\begin{algorithm}[H]
		\centering
		\caption{Adjusted multiscale scanning - AMS}
		\label{algo:AMS}
		\begin{algorithmic}[1]
			\Require Sample size $n \in \N$, dimension $d \in \N$, scale system $\mathcal R_n$, penalization $\widetilde \omega_n, \omega_n$, scale bounds $r_n < m_n$, data $Y$, FWER level $\alpha \in (0,1)$
			\State Simulate the distribution of $M_n \left(\mathcal R_n\left(r_n,m_n\right)\right)$
			\State Compute the (empirical) $(1-\alpha)$-quantile $q_{1-\alpha}$ of $M_n$
			\Statex\Comment{Steps 1 and 2 can be precomputed and the results can be stored independent of $Y$}
			\State Set $\mathcal S \leftarrow \emptyset$
			\State Compute the estimators $\hat \theta_0$, $\hat \xi$ based on $Y$ for the baseline intensity $\theta_0$ and the nuisance parameters $\xi$
			\For{$R \in \mathcal R_n \left(r_n, m_n\right)$}
			\State Compute $T \leftarrow T_R\left(Y, \hat \theta_0, \hat \xi\right)$
			\State Compute $c\leftarrow c_{|R|}(q_{1-\alpha})$ as in \eqref{eq:reject}
			\If{$T \geq c$}
			\State $\mathcal S \leftarrow \mathcal S\cup \{R\}$
			\EndIf
			\EndFor
			\Ensure $\mathcal S \subset \mathcal R_n$ such that with (asymptotic) probability $\geq 1-\alpha$ all $R \in \mathcal S$ contain at least one anomaly
		\end{algorithmic}
	\end{algorithm}
	%	Once the values $n$, $r_n$ and $m_n$ are fixed, the distribution of $M_n \left(\mathcal R_n\left(r_n,m_n\right)\right)$ is simulated beforehand and the corresponding $(1-\alpha)$-quantile $q_{1-\alpha}$ of $M_n$ is saved. Given now the data $Y$, we first compute the corresponding estimators $\hat \theta_0$ and $\hat \xi$ for the reference intensity and the nuisance parameters. Afterwards, we evaluate all local LRT statistics $T_R \left(Y, \hat \theta_0, \hat \xi\right)$ for $R \in \mathcal R_n\left(r_n,m_n\right)$ and finally reject all hypotheses $H_{R,n}$ such that
}

{In Section \ref{sec:implementation} we will provide details on a computationally efficient implementation of this procedure, which can be carried out by means of fast Fourier transforms as soon as $T_R$ in \eqref{eq:TR} is given by a function of the local mean $\bar Y_R := \sum_{i \in R} Y_i$. This leads to an algorithm for the evaluation of $M_n$ in \eqref{eq:Mn} and $\left(T_R\right)_{R \in \mathcal R_n}$ in $\mathcal O \left(d N n^d \log \left(n\right) \right)$ operations, where $N$ is the number of scales contained in $\mathcal R_n$. The corresponding MATLAB$^\copyright$-code is available under \LINK.}

%\subsection{Literature review}

%Let us also briefly mention other approaches to the multiplicity issue raised above. Besides controlling the FWER by using a scan statistic, one can also apply a weaker criterion and control the false discovery rate (FDR) by the method proposed in \cite{bh95,by01}, see e.g. \cite{zyxs16}. The FWER criterion can also be ensured by the higher criticism approach proposed in \cite{dj04}, see \cite{kw22} for an application of this to scan statistics.

%\begin{itemize}
%	\item \cite{ds18} Multivariate version of \cite{ds01}
%	\item \cite{gs81}: Replacing the nuisance parameter by its estimate can reduce its impact and offer a simple inference procedure was originally proposed by them.
%\item \cite{k97}
%seen as the fundamental article about spatial scan statistics, \\
%test procedure uses a likelihood ratio test to identify clusters ( critical value via simulations )

%\item \cite{a97}
%one- and two-dimensional homogeneous poisson process, fixed scanning set that gets translated over a rectangular area: rectangular scanning sets and general convex scanning sets like circular and triangular ones $\rightarrow$ approximation of the distribution of the scan statistic
%\item \cite{a98} 
%homogeneous Poisson process, one and two- dimensional repeated and generalized to three-dimensional scan statistic with fixed rectangular scanning set ( remark that it should be possible to generalize this as well)
%\item \cite{flr11}
%test homogeneity of a PP observed on a finite interval
%\item \cite{t13}
%ratchet scan statistic: scanning window is a grid set instead of rectangular box; asymptotic distribution of this statistic; connection between ratchet scan statistic and scan statistic (over rectangular box): ratchet effect can be factored out by a function $\rightarrow$ tail probability converges to that of the scan statistic
%\item \cite{sac16}
%measurement is a signal vector with additive white Gaussian noise, testing for nonzero signal, discrete hyperrectangles, Z-score ( $y[R]\hat{=} \bar{Y}_R$), 
%data is i.i.d. standard normal when no anomaly is present and an anomaly comes in form of a rectangle with increased mean, adaptive scan ( rectangles with shape $h$, which forms an $\epsilon-$net), oracle scan ( knows the shape $h^*$)
%\item \cite{fs16}
%one dimensional, general setting of EF, log likelihood ratio statistic, two scan statistics: one with fixed window size and one with varying one; convergence rates for tail probabilities of these statistics; proof relies on Stein's method
%\item \cite{zyxs16}
%scan statistic for Poisson-type data, $1$-dimensional, likelihood-based framework, FDR-formulas and power
%\item \cite{accd11} minimax detectio thresholds for detecting clustered anomalies in a field or network; calibrated scan statistic is optimal for this case
%\item \cite{cn14}: event detection in social media, non-parametric heterogeneous graph scan
%\item \cite{s18}: anomalous patterns in images; more theoretical (chaining)
%\item \cite{cmv02}: detection in imaging; method to detect man-made objects in images
%\item \cite{rdj14}: genetics, gene-wise scan statistic
%\item \cite{n12}: uncalibrated scan over a subset of all regions
%\item \cite{srs16}: graph fourier scan statistic
%\item \cite{mbs13} Testing for intervals where a regression function exceeds a baseline level, based on p-value scanning and kernel estimation
%\item \cite{cw13} penalized scan (our penalization) is superior to unpenalized scan, scanning system can be approximated by dyadic system for computational efficiency.
%\item \cite{kw22} uses higher criticism to derive the detection boundary for clustered signals (blocks)
%\item \cite{dj04} introduces higher criticism to perform multiplicity adjustment, i.e. maximizing a Z-score (LRT statistic) over a range of significance levels using p-values.
%\item 
%\cite{dw08} Calibrated (our calibration) scan statistic for detecting increases or decreases in a density
%\item \cite{pywr22} uncalibrated scan statistic based on kernel density estimators, scale is either pre-described or bounded from below
%\item \cite{rw13} penalized LRT-scan statistic (like ours) to infer on the intensity of a Poisson process
%\item \cite{w10} bernoulli scanning, scale dependent critical values. 2-dimensional Bernoulli-model (log-likelihood ratio statistic); first time size-dependent critical values to control the domination of the small scales (instead of penalty-term); approximation of the set of all rectangles to get an almost linear algorithm
%\end{itemize}

\section{Assumptions and detailed theory}\label{sec:theory}

In this section we will now discuss our assumptions and the result \eqref{eq:convergence} in detail. Furthermore, we will examine several examples to demonstrate the validity of our assumptions in realistic scenarios. 

To formulate our invariance principle, we will make use of stochastic $\mathcal O$-notation \citep[see Sect. 2.2 in][]{vdV98}: For a sequence $\left(X_n\right)_{n \in \N}$ of real-valued random variables we write $X_n = \mathcal O_{\mathbb P} \left(1\right)$ iff $\left(X_n\right)_{n \in \N}$ is tight, that is for all $\varepsilon > 0$ there exists $M > 0$ such that
\[
\sup_{n \in \N} \P{}{\left|X_n\right| >M} <\varepsilon.
\]
We write $X_n = \mathcal O_{\mathbb P} \left(a_n\right)$ with a sequence $\left(a_n\right)_{n\in\N}$ of real numbers, iff $X_n / a_n = \mathcal O_{\mathbb P}(1)$. {Note, that in our context this means stochastic convergence for one fixed probability distribution $\mathbb P$ only, and not uniform convergence e.g. over all $\theta_0 \in \Theta$.} {Furthermore, we write $X_n = o_{\mathbb P}(1)$ if $X_n$ converges stochastically to $0$.}

{Recall, that we abbreviate $\mathbb P_0 = \mathbb P_{H_{I_n^d,n}}$. Effectively, this means that $\mathbb P_0$ depends on $\theta_0$ and $\xi$.}

\subsection{Assumptions and examples}

We start with our assumptions on the distributions $F_{\theta, \xi}$ in model \eqref{eq:model}. For notional simplicity we introduce
\begin{equation}\label{eq:mean_variance}
	m(\theta, \xi) :=  \E{\theta,\xi}{Y}\qquad\text{and}\qquad  v(\theta, \xi):= \V{\theta,\xi}{Y}.
\end{equation}
Here and in what follows, $Y$ denotes a generic random variable with distribution $Y \sim F_{\theta,\xi}$ according to the parameters mentioned in the subscript.
\begin{assumption}[Model assumptions]\label{ass:model}
	Let $Y \sim F_{\theta, \xi}$, $\left(\theta,\xi\right) \in \Theta \times \Xi \subseteq \R^{d_1}\times \R^{d_2}$ with $d_1, d_2  \in \mathbb N$. For $\theta_0 \in \Theta, \xi \in \Xi$ assume {that there exists a non-empty open neighbourhood} $U \times V \subset \Theta \times \Xi$ of $\left(\theta_0, \xi\right)$.
	\begin{enumerate}
		\item[(a)] \textbf{Uniformly sub-exponential tails:} There exist constants $c_1 > 1, c_2 > 0$ such that
		\begin{align*}
			\sup_{\left(\theta, \xi\right)\in U \times V}\P{\theta_0,\xi}{|Y|>t} \leq c_1 \exp\left(- c_2 t\right) \qquad\text{for all}\qquad t > 0.
		\end{align*}
		\item[(b)] \textbf{Moments exist locally:} The functions $m$ and $v$ in \eqref{eq:mean_variance} exist and are finite on $U \times V$ and there exists a constant $c_v > 0$ such that $v > c_v$ on $U \times V$.
		\item[(c)] \textbf{Bounded derivatives of $m$:} There exists a constant $C_m < \infty$ such that $\|\nabla_\theta m \|_2, \| \nabla_\xi m \|_2\leq C_m$ on $U \times V$.
		\item[(d)] \textbf{Bounded and non-vanishing derivatives of $v$:} There exist constants $0 < c_v < C_v < \infty$ such that $c_v \leq \|\nabla_\theta v\|_2, \|\nabla_\xi v\|_2 \leq C_v$ on $U \times V$.
		\item[(e)] \textbf{Taylor approximation:} There exists a constant $C_T < \infty$ such that the likelihood ratio statistic $T_R$ in \eqref{eq:TR} can be approximated by local means $\bar{Y}_R = |R|^{-1}\sum\limits_{i \in R}Y_i$ in the sense that
		\begin{equation}\label{eq:lrt_approx}
			\left| T_R^2 \left(Y,\theta, \xi\right) - |R|\left(\frac{\bar{Y}_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right)^2 \right|  \leq C_T |R|\left( \frac{\left|\bar{Y}_R - m\left(\theta, \xi\right)\right|}{\sqrt{v\left(\theta, \xi\right)}}\right)^3 \left(1+ o_{\mathbb P_0} \left(1\right)\right)
		\end{equation}
		holds true for all $R \in \mathcal R$ and $\left(\theta, \xi\right) \in U \times V$.
	\end{enumerate}
\end{assumption}
We briefly comment on these assumptions.
\begin{remark}
	\begin{enumerate}
		\item[a)] Assumption \ref{ass:model}(a) allows us to control moments of $Y_i$ and hence of local means $\bar Y_R := \sum_{i \in R} Y_i$ simultaneously over regions $R \in \mathcal R_n$. %, so that we can finally employ a coupling result proven in \citet{kmw20}.
		\item[b)] Assumption \ref{ass:model}(b) seems natural, as otherwise no CLT on the small scales can hold true and hence no distributional limit is to be expected.
		\item[c)] Assumptions \ref{ass:model}(c) and (d) will be used to control the impact caused by replacing $\theta_0$ and $\xi$ in \eqref{eq:Tn} by the estimators $\hat \theta_n$ and $\hat \xi_n$, respectively.
		\item[d)] Assumption \ref{ass:model}(e) is required to link $T_R$ to the local means $\bar Y_R$ sufficiently well.
	\end{enumerate}
\end{remark}

For a better understanding of Assumption \ref{ass:model} let us discuss some examples:
\begin{example}[Complete exponential family model]
	Suppose that 
	\[
	\left\{F_{\theta, \xi}\right\}_{\left(\theta,\xi\right) \in \Theta \times \Xi}
	\]
	is a natural exponential family with natural parameter space $\Theta \times \Xi$. In this case, each $Y_i$ has sub-exponential tails (which implies Assumption \ref{ass:model}(a)) and the differentiability properties from Assumption \ref{ass:model}(c)--(e) hold true \citep[see e.g.][]{b86}. 
	
	This includes both the homogeneous and the heterogeneous Gaussian model as well as the Poisson model, which we employ for our data example in Section \ref{sec:data_analysis} on fluorescence microscopy.
\end{example}

\begin{example}[Partial exponential family model]\label{ex:distributions}
	Suppose that for each fixed value of $\xi \in \Xi$, the collection $\left\{F_{\theta, \xi}\right\}_{\theta \in \Theta}$ is a natural exponential family. In this case, Assumption \ref{ass:model}(a) is again satisfied due to standard theory on exponential families, but the differentiability properties in Assumption \ref{ass:model}(c)--(e) have to be verified case by case. As one particular example we consider the 
	Gamma distribution $\Gamma(\alpha, \beta)$ with shape parameter $\alpha>0$, rate $\beta>0${, and density}
	\begin{align*}
		f(x)&= \frac{1}{\Gamma(\alpha)}  \beta^\alpha x^{\alpha-1} \exp \left( -\beta x \right).
		%\E{}{Y}&=\frac{\alpha}{\beta}\\
		%\V{Y}&= \frac{\alpha}{\beta^2}
	\end{align*}
	If the shape parameter $\alpha$ is a nuisance parameter, we obtain $m(\beta, \alpha)= \alpha/\beta$, $v(\beta, \alpha)= \alpha/\beta^2$ and
	\begin{align*}
		\frac{\partial}{\partial \beta} m(\beta, \alpha) = - \frac{\alpha}{\beta^2},  \qquad \frac{\partial}{\partial \beta} v(\beta, \alpha) = - \frac{2\alpha}{\beta^3}, \qquad \frac{\partial}{\partial \alpha} m(\beta, \alpha) = \frac{1}{\beta}, \qquad \frac{\partial}{\partial \alpha} v(\beta, \alpha) = \frac{1}{\beta^2},
	\end{align*}
	which proves the boundedness conditions in Assumption \ref{ass:model}(c) and (d). Furthermore, \eqref{eq:lrt_approx} follows directly from the exponential family structure \citep[see e.g.][]{kmw20}. 
\end{example}

We also give a counterexample to show the limitations of our Assumptions:
\begin{example}[Weibull distribution]
	Consider the Weibull distribution with shape parameter $k>0$ and rate parameter $\lambda>0$. If $k\geq 1$, then all random variables $Y_i$ will have sub-exponential tails \citep[see e.g.][]{s85,fb97}, but Assumption \ref{ass:model}(e) is not satisfied. Therefore, we compute 
	\begin{align*}
		\log \left(\frac{\sup\limits_{\theta \in \Theta} \prod_{i \in R} f_{(\theta, \xi)}(Y_i)}{\prod_{i \in R} f_{(\theta_0, \xi)} (Y_i)} \right)&= k \log(\lambda_0) - \log \left( \sum\limits_{i \in R} Y_i^k\right) + \frac{\sum\limits_{i \in R} Y_i^k}{\lambda_0^k} -1\\
		|R|\left(\frac{\bar{Y}_R - m(\theta_0, \xi)}{\sqrt{v(\theta_0, \xi)}}\right)^2 &= |R| \left( \frac{\bar{Y}_R^2 - 2 \lambda \Gamma\left(1+ \frac{1}{k} \right)\bar{Y}_R +\lambda^2 \left(\Gamma\left(1+ \frac{1}{k} \right)\right)^2}{\lambda^2 \left[\Gamma\left(1+ \frac{2}{k} \right) - \left(\Gamma\left(1+ \frac{1}{k} \right)\right)^2 \right]}\right),
	\end{align*}
	which shows that \eqref{eq:lrt_approx} cannot hold true.
\end{example}

Next we continue with our assumptions on the multiscale scanning procedure described in terms of $T_n$ in \eqref{eq:Tn}. To approximate $T_n$ by $M_n$ in \eqref{eq:Mn}, we first have to restrict the cardinality of $\mathcal R_n$.
\begin{assumption}[Polynomial growth of $\mathcal R_n$]\label{ass:R}~
	There exists constants $c_1, c_2>0$ such that
	\begin{equation}\label{eq:finite_cardinality}
		\#\left(\mathcal R_n \right) \leq c_1 n^{c_2}.
	\end{equation}
	where $\#$ denotes the number of elements.
\end{assumption}
As discussed in \citet{kmw20}, Assumption \ref{ass:R} is rather mild and allows e.g. for $\mathcal R_n$ being the set of all hyper-rectangles, hyper-cubes or half-spaces in $\left[0,1\right]^d$.

Our next assumption is on the scale calibration $\widetilde{\omega}{_n}, \omega{_n}$ in \eqref{eq:Tn}:
\begin{assumption}\label{ass:Omega}
	Suppose $\omega{_n},\widetilde{\omega}{_n}: \left(0,\infty\right) \to \left(0,\infty\right)$ have the following properties:
	\begin{enumerate}
		\item For each $n \in \mathbb N$, $\omega{_n}$ and $\widetilde \omega{_n}$ are decreasing.
		\item There exist $C_{\omega} > 0$, $\alpha, \widetilde{\alpha}>0$ and $\beta, \widetilde{\beta} \in \R$ such that
		\begin{align*}
			\omega{_n}(r) & \leq C_\omega\left(\log \frac{n^d}{r} \right)^\alpha, \qquad \left|\omega{_n}^\prime(r) \right|  \leq C_\omega\left(\log \frac{n^d}{r} \right)^\beta \frac{1}{r},\\
			\widetilde{\omega}{_n}(r) & \leq C_\omega\left(\log \frac{n^d}{r} \right)^{\widetilde{\alpha}}, \qquad
			\left|\widetilde{\omega}{_n}^\prime(r) \right| \leq C_\omega\left(\log \frac{n^d}{r} \right)^{\widetilde{\beta}} \frac{1}{r}
		\end{align*}
		for all $r > 0$ and $n \in \N$.
	\end{enumerate}
\end{assumption}
Let us briefly discuss this assumption for common choices considered in the literature:
\begin{example}\label{ex:omega}
	\begin{enumerate}
		\item[a)] \citet{dw08,cw13,rw13} consider
		\begin{align}
			\widetilde{\omega}{_n}\equiv 1, \qquad 
			\omega{_n}(|R|)= \sqrt{2 \nu (\log(n^d/|R|))+1},\label{pen_ds01}
		\end{align}
		where $\nu  \geq 1$ depends on the complexity of the candidate region. These terms fulfil Assumption \ref{ass:Omega} with $\alpha=\frac{1}{2}, \beta=-\frac{1}{2}$ and $\widetilde{\alpha}= \widetilde{\beta}=0.$
		\item[b)] \citet{sac16} consider
		\begin{align}
			\widetilde{\omega}{_n}(|R|)& = \sqrt{2(\log(n^d/|R|))},\nonumber\\
			\omega{_n}(|R|)&= \sqrt{2(\log(n^d/|R|))} + \frac{\left(4d-1\right) \log \left(\sqrt{2(\log(n^d/|R|))}\right) - \log\left(\sqrt{2\pi}\right)}{\sqrt{2(\log(n^d/|R|))}} ,\label{pen_sac}
		\end{align}
		and therefore Assumption \ref{ass:Omega} is fulfilled with $\alpha = \widetilde{\alpha} = \frac12$ and $\beta = \widetilde{\beta} = - \frac12$.
		\item[c)] \citet{pwm18} consider 
		\begin{equation}\label{pen_pwm}
			\omega{_n}(|R|) := \widetilde{\omega}{_n}(|R|):= \sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)} + C_d \frac{\log \left(\sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)} \right)}{\sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)}},
		\end{equation}
		with $C>1$ and $C_d$ depending on the dimension and the system of considered scales.
		These fulfil Assumption \ref{ass:Omega} with $\alpha= \widetilde{\alpha}= \frac{1}{2}$ and $\beta= \widetilde{\beta}= -\frac{1}{2}.$
	\end{enumerate}
\end{example}

\subsection{Main result and implications}

Now we are in position to state our uniform invariance principle in detail. Therefore let $(\hat{\theta}_n)_{n \in \N}$ and $(\hat{\xi})_{n \in\N}$ be sequences of estimators for $\theta_0$ and $\xi$ based on $Y$ such that they are $s_n$ (e.g. $s_n = n^{-d/2}$) consistent under the null hypothesis, i.e. 
\begin{equation}\label{eq:estimators}
	\|\hat{\theta}_n - \theta_0\|_2 = \OP \left(s_n^{-1}\right)\qquad\text{and}\qquad \|\hat{\xi}_n - \xi\|_2 = \OP \left(s_n^{-1}\right),
\end{equation}
where $\left(s_n\right)_{n \in \N} \subset \R$ is a monotone sequence tending to $\infty$. Note, that the MLEs for $\theta_0$ and $\xi$ satisfy \eqref{eq:estimators} under Assumption \ref{ass:model} with $s_n = n^{d/2}$. Furthermore let 
\[
\gamma= 12+ 6 \widetilde{\alpha}+ 2 \max\left\{\frac12, \alpha, \widetilde{\alpha}\right\} + 2 \max\left\{\beta, \widetilde{\beta},0\right\}
\]
with the parameters $\alpha, \widetilde{\alpha}, \beta, \widetilde{\beta}$ from Assumption \ref{ass:Omega} and suppose that
\begin{align}
	\log^{\gamma}(n) = o\left(r_n\right) \text{ as }n \to \infty\label{eq:r_n},\qquad m_n =  o \left(s_n\right)  \text{ as }n \to \infty.
\end{align}
In this situation we can prove the following Gaussian approximation. 

\begin{theorem}[Gaussian approximation]\label{thm:approximation_estimated_intensity}
	Let $\mathcal R_n$ be a set of candidate regions and let $\omega{_n}, \widetilde \omega{_n}$ be scale calibrations. Grant Assumptions \ref{ass:model}, \ref{ass:R} and \ref{ass:Omega}. Furthermore let $(r_n)_n\subset(0,\infty)$ and $\left(m_n\right)_n \subset \left(0,\infty\right)$ be sequences such that \eqref{eq:r_n} holds true and let $(\hat{\xi}_n)_{n\in\N}$ and $(\hat{\theta}_n)_{n \in\N}$ are sequences of estimators such that \eqref{eq:estimators} is valid. 
	
	Then, for a field $Y = \left(Y_i\right)_{i \in I_n^d}$ with i.i.d. random variables $Y_i \sim F_{\theta_0, \xi}$, it holds
	%\begin{align}\label{eq:approx_in_prob}
	%\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right)
	%\end{align}
	\begin{align}\label{eq:approx_in_prob}
		\left|T_n \left(Y,  \mathcal R_n\left(r_n,m_n\right), \hat{\theta}, \hat{\xi} \right) - M_n\left(\mathcal R_n\left(r_n,m_n\right)\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \sqrt{\frac{m_n}{s_n}}+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right),
	\end{align}
	as $n \to \infty$.
\end{theorem}

\begin{cor}
	Let the assumptions of Theorem \ref{thm:approximation_estimated_intensity} hold true, $Y = \left(Y_i\right)_{i \in I_n^d}$ as in \eqref{eq:model} (independent, but not necessarily identically distributed), and $\alpha \in \left[0,1\right]$. If we define
	\[
	\mathcal R_n^* := \left\{R \in \mathcal R_n ~\big|~ r_m \leq |R| \leq m_n, T_R \left(Y, \hat \theta_n, \hat \xi_n\right) \geq \frac{q_{1-\alpha}}{\widetilde{\omega}{_n} \left(\left|R\right|\right)} + \omega{_n} \left(\left|R\right|\right)\right\}
	\]
	with the $\left(1-\alpha\right)$-quantile $q_{1-\alpha}$ of $M_n$ as in \eqref{eq:Mn}, then \eqref{eq:FWER} holds true, i.e.
	\[
	\P{}{\forall~R \in \mathcal R_n^* ~\exists~i \in R \text{ s.t. } \theta_i \neq \theta_0} \geq 1-\alpha + o(1) \qquad\text{as}\qquad n \to \infty.
	\]
\end{cor}

\begin{remark}\label{rem:one_sided}
	In the situation that all local alternatives {(and tests)} are one-sided, i.e. $\theta_i > \theta_0$, the above result can readily be generalized by setting
	\[
	\tilde T_R \left(Y,\hat \theta_n, \hat \xi_n\right) = \begin{cases} T_R \left(Y, \hat \theta_n,\hat\xi_n\right) & \text{if }\bar Y_R > \hat \theta_n, \\ 0 & \text{else},\end{cases}
	\]
	as well as
	\[
	\tilde T_n \left(Y,\mathcal R_n,\hat \theta_n, \hat \xi_n\right)  := \max_{R\in\mathcal R_n} \widetilde{\omega}{_n}\left(\left|R\right|\right)\left[ \tilde T_R\left(Y, \theta_0, \xi\right)-\omega{_n}\left(\left|R\right|\right)\right]
	\]
	and
	\[
	\tilde M_n\left(\mathcal R_n\right) := \max_{R\in\mathcal R_n, \bar X_R > 0} \widetilde{\omega}{_n}(|R|)\left[ \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right| - \omega{_n}(|R|) \right].
	\]
	With this notation, it can readily be seen that Theorem \ref{thm:approximation_estimated_intensity} holds also true for those quantities, i.e. under the same conditions we obtain
	\[
	\left|\tilde T_n \left(Y,  \mathcal R_n\left(r_n,m_n\right), \hat{\theta}, \hat{\xi} \right) - \tilde M_n\left(\mathcal R_n\left(r_n,m_n\right)\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \sqrt{\frac{m_n}{s_n}}+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right)
	\]
	as $n \to \infty$.
\end{remark}

\begin{remark}
	As a different approach to the invariance principle in Theorem \ref{thm:approximation_estimated_intensity}, it would be particularly elegant to exploit asymptotic equivalence in the Le Cam sense \citep[see e.g.][]{gn98,rsh18} after a variance stabilizing transformation to a Gaussian model. However, to the best of our knowledge, such results will only be valid for a fixed number of scales.{Theorem \ref{thm:approximation_estimated_intensity}} therefore {gives rise to} interesting questions on how to extend asymptotic equivalence to multiscale frameworks with growing number of tests.
\end{remark}

\section{Implementation and simulations}

In this section, we will first outline details on our implementation and afterwards investigate the {behaviour} of the adaptive multiscale test in a simulation study. 

\subsection{Implementation}\label{sec:implementation}

As already mentioned in the introduction, for a generic set $\mathcal R_n$ all local test statistics $T_R$ in \eqref{eq:TR} have to computed individually. To this end, for example, the approximating rectangular system from \citet{w10} can be used to compute local averages on these. Depending on the structure of $\mathcal R_n$, other efficient computational schemes can be employed. In the following we will focus on the situation that there is a global shape $B \subset I_n^d$ such that every $R \in \mathcal R_n$ is a rescaled and shifted version of $B$. More precisely, for each $R \in \mathcal R_n$ there exist $t, h \in I_n^d$ with $t_i + h_i \leq n$ for all $1 \leq i\leq d$ such that
\begin{equation}\label{eq:R_struct}
	\1_R \left(x\right) = \1_B \left(\frac{x-t}{h}\right), \qquad x \in I_n^d,
\end{equation}
where division is meant component-wise and $\1$ denotes the indicator function. For the set $\mathcal R_n$ of all hyper-rectangles, this is for example the case with $B = \1_{I_n^d}$. 

If $R$ obeys \eqref{eq:R_struct}, we obtain
\[
\bar Y_R = \frac{1}{\left|R\right|} \sum_{i \in R} Y_i = \frac{1}{\left|R\right|} \sum_{i \in I_n^d} Y_i \1_B\left(\frac{i-t}{h}\right) = \frac{1}{\left|R\right|} \left(Y \ast \1_B \left(\frac{\cdot}{h}\right)\right) \left(t\right),
\]
where $\ast$ denotes a discrete convolution. Employing the fast Fourier transform (FFT), {see \cite{ct65},} this allows to compute all $T_R$ with a fixed scale $h$ by means of {three} FFTs via
\[
\left(\bar Y_R\right)_{h} = \frac{1}{h^d \left|B\right|} \text{FFT}^{-1} \left(\text{FFT} \left(Y\right) \cdot \text{FFT}\left( \1\left(\frac{\cdot}{h}\right)\right)\right).
\]
This operation has a computational complexity almost linear in the data, i.e. $\mathcal O \left(d n^d \log\left(n\right)\right)$, and hence if each $R \in \mathcal R_n$ obeys \eqref{eq:R_struct}, then $T_n$ in \eqref{eq:Tn} can be computed within $\mathcal O \left(d N n^d\log\left(n\right)\right)$ where $N$ is the number of different scales. The same holds true for the evaluation of $M_n$ in \eqref{eq:Mn}. {A corresponding implementation in MATLAB$^\copyright$ is available under \LINK.}

\subsection{Validity of the Gaussian approximation of AMS}

{We illustrate \eqref{eq:convergence} in homogeneous the Gaussian setting from} Subsection \ref{subsec:gauss}. Figure \ref{fig:gauss_dist_limited} shows the empirical distribution of $T_n \left(Y, \mathcal R_n\left(r_n,m_n\right), \hat \theta_n, \hat \xi_n\right)$ and $M_n \left(\mathcal R_n\left(r_n,m_n\right)\right)$ with the same $n$ and $\widetilde{\omega}{_n}, \omega{_n}$ as there for  $r_{128} = 4$ and $m_{128} = 64$.

% Figure environment removed

We find the distributional fit to be remarkably accurate in all situations, which supports the applicability of the asymptotic result from Theorem \ref{thm:approximation_estimated_intensity}. The distribution of $M_n$ itself will be shown in Figure \ref{fig:invariance_dist} for the situation considered in our simulations.

\subsection{Simulations of level and power}

In this section, we will study the finite sample properties of the adjusted multiscale test. We therefore consider {two different model situations, namely a heterogeneous Gaussian model and a Poisson model}. In both models, we investigate the empirical level and power of the procedure by means of Monte Carlo simulations {with $M = 1000$ runs. As a default setup, we use $d = 2$, $n = 128$ and} $\mathcal R_n$ to be the set of all rectangles in $I_n^d$ with even side-lengths between $4$ and $14$ pixels. As penalization functions $\widetilde \omega{_n}$ and $\omega{_n}$, we choose the previously mentioned penalization \eqref{pen_ds01} of \cite{dw08}. Empirical quantiles of $M_n$ in \eqref{eq:Mn} for this situation are shown in Table \ref{tab:quantiles_128}, {and} the empirical density and CDF {both for the calibrated and the uncalibrated statistic} are displayed in Figure \ref{fig:invariance_dist}.

\begin{table}
	\caption{\label{tab:quantiles_128}	Empirical quantiles $q_{1-\alpha}$ with different values of $\alpha$ for the distribution of $M_n$ in \eqref{eq:Mn} in the case $n = 128, d = 2$ and $\mathcal R_n$ being the set of all rectangles in $I_n^d$ with side-lengths between $4$ and $14$.
	}
	\centering
	\fbox{%
		\begin{tabular}{*{6}{c}}
			$\alpha$ & $0.2$ & $0.1$ & $0.05$ & $0.025$ & $0.01$ \\
			$q_{1-\alpha}$& $1.2906$ & $1.4677$ & $1.6278$ & $1.7841$ & $1.9768$\\
		\end{tabular}
	}
\end{table}

% Figure environment removed

For power simulations, we consider a $a^2 = 8\times8$ sized anomaly positioned in the {centre} of the $128\times 128$ image. For comparison, we also investigate the corresponding oracle procedure, which arises from evaluating $T_n\left(Y, \mathcal R_n, \theta_0, \xi\right)$ with the true values of $\theta_0$ and $\xi$.

{To investigate the influence of $d$ and $n$, we additionally perform simulations with $d= 1,2$ and $n = 32,64,96,128$. Therein we determine the necessary signal strength (mean) of an anomaly of size $a^d$ ($a \in \{4,5,...,13,14\}$ a fixed number), again positioned in the centre of the domain, such that the empirical power is $\approx 90\%$. The corresponding functions $\theta = \theta(a)$, where $\theta$ is either the mean $\mu$ in the heterogeneous Gaussian model or the parameter $\lambda$ in the Poisson model, are in fact an upper bound for the (finite sample) separation rate between $H_0$ and the prescribed alternative. The (asymptotic) separation rate, i.e. the asymptotic detection boundary, provides a sharp notion of the minimal signal strength necessary for detection, which cannot be overcome by any test. Note, that the asymptotic detection boundary for the identification of cuboid signals with known size $a$ in Gaussian white noise as considered here has been derived in the literature, see e.g. \cite{cw13,emw18,k21}, as, translated to our notation,
	\begin{equation}\label{eq:separation_rate}
		\mu \sim \sqrt{\frac{2}{a^d} \log \left(\frac{n^d}{a^d}\right)}.
	\end{equation}
	We emphasize that AMS treats a more difficult situation (i.e. $\theta_0, \xi$ and $a$ unknown), and will hence not be able to reach this boundary, but as we will show below a certain comparison is still possible.}

{In the simulations of level and power, we always compare the AMS approach with an oracle approach, where the local LRT statistics $T_R$ from \eqref{eq:TR} are evaluated using the true parameters $\theta_0$ and $\xi$.}

%It is to be expected that this procedure will have a larger power, and hence the power loss can (to some extend) be seen as the price for adaptation.

\subsubsection{Homogeneous Gaussian model}

Let us first consider the case already discussed in the introduction that $F_{\theta, \xi}$ is a normal distribution $\mathcal N \left(\mu, \sigma^2\right)$, and the variance $\sigma^2$ is considered as the nuisance parameter and estimated by the sample variance
\[
\hat\sigma^2_n := \frac{1}{n^d-1} \sum_{i \in I_n^d} \left(Y_i - \bar Y_{I_n^d}\right)^2.
\]
Then \eqref{eq:estimators} is clearly satisfied under $\mathbb P_0$ with $s_n = n^{-d/2}$. 

{The results of the simulations as described before are depicted in Figure \ref{fig:gauss}. Sub-panels (a) and (b) display the signal and the corresponding data for the  default setup ($d = 2$, $n = 128$) with an anomaly of size $8 \times 8$ pixels and amplitude $\mu = 0.5$ in the centre, where the variance $\sigma^2 = 1$. Sub-panel (c) shows the empirical level for different variances $\sigma^2$. We find that the level is kept quite stable over a large range of variances. Sub-panel (d) shows the empirical power for $\sigma^2 = 1$ against different means $\mu$. Notably, there is nearly no power loss of the AMS method compared to the oracle caused by adaptation for an unknown $\sigma^2$. Sub-panel (e) is devoted to simulations in dimension $d = 1$ with $\sigma^2 = 1$ and different values of $n \in \{32,64,96,128\}$. The lines depict that value of $\mu$, such that the AMS procedure is able to detect a present anomaly of size $a$ marked on the $x$-axis with amplitude $\mu$ with a power of $\approx 90\%$. We find that a larger anomaly clearly corresponds to a smaller amplitude required for detection. This is in agreement with the asymptotic separation rate in \eqref{eq:separation_rate}, and remarkably, despite the presence of nuisance parameters and the additional difficulty that the length $a$ is not known, the finite sample shapes of all separation lines in sub-panel (e) are of this form. Furthermore, we find that the influence of the sample size $n$ on the separation lines is small. Sub-panel (f) shows the same situation in dimension $d = 2$. Due to the larger anomaly, which now contains $a^2$ pixels, the corresponding values for $\mu$ are smaller. However, the conclusions drawn for the one-dimensional case apply here similarly. Compared with sub-panel (e) we furthermore conclude that the dimension $d$ does not have a big influence.}

% Figure environment removed

\subsubsection{Poisson model}

{As a second setup, we consider a Poisson model where} $F_{\theta, \xi}$ is a Poisson distribution $\text{Poi} \left(\lambda\right)$ with unknown background intensity $\lambda_0$, which will be estimated by the sample mean, i.e $\hat\lambda_n := \bar Y_{I_n^d}$. Then \eqref{eq:estimators} is clearly satisfied with $s_n = n^{-d/2}$. 

{The results of the simulations as described before are depicted in Figure \ref{fig:poisson}. Sub-panels (a) and (b) show the signal and the corresponding data for the default setup ($d = 2$, $n = 128$) with an anomaly of size $8 \times 8$ pixels and amplitude $\lambda = 1.6$ in the centre, where the background intensity $\lambda_0 = 1$. Sub-panel (c) shows the empirical level for different background intensities $\lambda_0$. We again find that the level is kept quite stable, even for considerably small values of $\lambda_0$. The little bump around $\lambda_0 = 0.3$ emphasizes that the quantiles are chosen according to the Gaussian approximation in Theorem \ref{thm:approximation_estimated_intensity}. Sub-panel (d) shows the empirical power for $\lambda_0 = 1$ against different amplitudes $\lambda\geq \lambda_0$. Similarly to the Gaussian case, there is  nearly no power loss caused by adaptation for an unknown $\lambda_0$. Sub-panel (e) is devoted to simulations in dimension $d = 1$ with $\lambda_0 = 1$ and different values of $n \in \{32,64,96,128\}$. The lines depict that value of $\lambda$, such that the AMS procedure is able to detect a present anomaly of size $a$ marked on the $x$-axis with amplitude $\lambda$ with a power of $\approx 90\%$. We find that a larger anomaly clearly corresponds to a smaller amplitude required for detection, again in agreement with the results on separation rates in the Gaussian case. Furthermore, we find that the influence of the sample size $n$ on the separation lines is small, despite for the smallest sample size $n = 32$ where the required value of $\lambda$ is moderately larger. This might be caused by the worse approximation of $T_n$ by $M_n$ in this case. Sub-panel (f) shows the same situation in dimension $d = 2$. Due to the larger anomaly, which now contains $a^2$ pixels, the corresponding values for $\mu$ are smaller. The conclusions drawn for the one-dimensional case apply here for all considered values of $n$. Compared with sub-panel (e) we furthermore conclude that the dimension $d$ does not have a big influence.}

% Figure environment removed

\section{Data Analysis}\label{sec:data_analysis}

{A major motivation for our work comes from super-resolution fluorescence microscopy imaging. Fluorescence microscopy is employed to visualize structures of interest (e.g. proteins or protein complexes) in a sample. Therefore, the structures of interest in a sample are {labelled} biochemically by fluorescent markers, and then scanned spatially along a grid with a diffraction-limited laser spot {centred} at the current grid point. Whenever a marker is hit by the incoming light it excites with a certain probability, and if so, afterwards light of a different wavelength is emitted and recorded by detectors \citep[cf.][for details]{aem15}. The later excitation-emission-recording procedure is repeated several (say $t \in\N$) times, called the measurement time, before the focus is moved to the next grid point.}

{To obtain a resolution below the Abbe diffraction limit (see \citet{kmw21} for an explanation in a statistical context), nowadays different techniques are known. Here we will focus on stimulated emission depletion (STED) microscopy, developed by Stefan Hell (see e.g. \citet{hw94,h07} for details), which has been awarded the Nobel price for Chemistry in 2014. It is based on an additional donut-shaped depletion beam, which suppresses fluorescence in the corresponding region and hence reduces effectively the area in which light is emitted. This way, the effective resolution can be decreased dramatically from roughly $200-300\nm$ for a standard (confocal) microscope to $\leq 50 \nm$, say.}

\subsection{Mathematical setup}

{Mathematically, the nature of photon counting in super-resolution microscopy leads to a Poisson field of independent observations \citep[see e.g.][]{msw20}. Consequently, we will model the corresponding observations as an independent $d-$dimensional (usually $d = 2$) field $Y$ of Poisson random variables
	\begin{align}\label{eq:model_pois}
		Y_i \sim \text{Poi}(t\lambda_i), \quad i \in I_n^d:= \{1, \ldots,n\}^d
	\end{align}
	with parameters $\lambda_i >0$, $i \in I_n^d$.}

{However, $t$ cannot be chosen arbitrary, as each marker is only able to pass through the cycle of excitation and emission a limited number of times before it bleaches. To avoid masking effects by bleaching, the total illumination time per pixel (called pixel dwell time) is hence set to a (not too large) number $T > 0$. It is immediately clear, that a larger value of $T$ will allow for a more accurate imaging of the underling structures of interest. However, even if large areas in the image are mostly empty, these regions have to be scanned and by doing so all markers in the image are hit by light and hence either excited or at least stressed. Therefore, methods which are able to localize interesting structures within a short pre-scan (i.e. with short measurement time $t \ll T$) have achieved great interest recently in many applications and can -- in the present context -- be seen as an instance of \emph{smart} microscopy, see \citet{s20}. Some heuristic methods have been suggested \citep[such as RESCue and others, see][]{vge20}, which stop measuring at a pixel as soon as a predefined number of photons has been detected, but without any statistical guarantees. Such approaches for a real-time spatial control of the illumination are known as spatially-controlled illumination microscopy (SCIM), see e.g. the review by \citet{kvnmh16}. In principle this can be interpreted as scanning each grid point with a substantially smaller pixel dwell time $t\ll T$ and deciding upon the corresponding measurement $Y_i$ if the full pixel dwell time $T$ is applied or if the measurement at the current grid point is stopped. Another approach exploits machine learning to pre-select regions of interest based on shortly measured (i.e. very noisy) data, see e.g. \citet{pwsg19,mszgwm22}. All the previously described methods suffer from the fact that they are not able to provide statistical guarantees for correctness of the selected (or more intensively scanned) regions. As we will demonstrate now, AMS does provide a methodology that allows to identify interesting regions with prescribed statistical error.}

\subsection{Used data sets}

{The data sets to be shown below have been measured in the Institut für Nanophotonik (IFNANO) by Ren\'e Siegmund especially to apply AMS with different initial scan times $t$. The measurement device is a STED super-resolution microscope with an effective resolution of $\sim 60 \nm$. For each $t \in \{1 \ms,2 \ms,...,100\ms \}$ a data set $Y_t$ with corresponding dwell time $t$ (in microseconds $\ms$) is recorded. The corresponding value $t = 100 \ms$ can be seen as the maximal value $T$, which was chosen to avoid bleaching of the fluorescent dyes. In that sense, we will consider $t = T$ as \textit{ground truth}, since no better measurement is available or can realistically be measured. We aim to achieve a localization of all or most regions of interest by means of AMS based on data with $t \ll T$, since this then yields a statistically rigorous method to obtain the spatially-controlled illumination described above with (asymptotically) controlled FWER.}

{The available raw data image is corrupted by background noise resulting from other contributions such as out-of-focus markers or external photon sources, which also can be modelled as Poissonian random variables, see e.g. \citet{aem15,msw20}. Since the the background intensity $\lambda_0 > 0$ varies between experiments, it has to be estimated from the data.}

\subsection{Applying AMS}

{Applying AMS to the microscopy data in \eqref{eq:model_pois} works now as follows. The problem of identifying regions which contain true features can be formulated as detection of regions $R \subset I_n^d$ such that $\lambda_i > \lambda_0$ for some $i \in R$. However, $\lambda_0$ is unknown in practice and will hence be estimated from the available data $Y$ by the global mean $\hat \lambda_n = \bar Y_{I_n^d}$. Below, we have $n = 400$, $d = 2$, and we choose $\mathcal R_n$ to be the subset of all rectangles in $I_n^d$ with scale between $r_n = 4$ and $m_n \in \{10,20\}$, depending on the investigated structure of interest. The details will be explained below. As we are only interested in active regions, we consider the one-sided testing problem to find regions $R$ where $\lambda_i > \lambda_0$, which can be achieved by restricting the corresponding local rejections $R$ with $\bar Y_R = \frac{1}{|R|} \sum_{i \in R} Y_i > \hat \lambda_n$ (recall Remark \ref{rem:one_sided}). Furthermore we simulate the threshold $\eta$ from an asymptotically distribution-free Gaussian approximation $M_n$ of the test statistic (see \eqref{eq:Mn}, with the corresponding adjustment that $\bar X_R > 0$, cf. also Remark \ref{rem:one_sided}). We then choose $\eta$ as the $0.9$-quantile $q_{0.9}^{M_n}$ of this adjusted $M_n$. Our main Theorem \ref{thm:approximation_estimated_intensity} implies that this asymptotically controls the FWER at level $\alpha$. The corresponding results will be shown as \textit{significance (fluorescence) maps}, depicting all significant rectangles with colour indicating their size in square-nanometers $\nm^2$. If two rectangles overlap, the colour is chosen according to the smaller one, which leads to a map indicating the smallest scale of significance. The major result of this paper implies that the AMS method ensures the FWER control in \eqref{eq:FWER}, and hence each coloured region contains an anomaly with uniform probability $\geq 0.9$, i.e. the probability that any of these detections is false is less then $0.1$.}

\subsection{Results}

\subsubsection{Data example 1: Single sized crimson beads}

As a first data example, we investigate $48 \nm$ crimson beads, i.e. carboxylate modified microspheres of $48\nm$ diameter filled with fluorescent markers. {Taking into account the microscopes effective resolution of about $60$nm, we expect to see circular anomalies of size $\sim 110$nm in the data. Therefore we choose $r_n$ and $m_n$ such that the smallest box-size is $80$nm, and the largest box-size is $400$nm. Since each pixel has size $20 \nm \times 20 \nm$, this corresponds to $r_n = 4$ and $m_n = 20$.} The measured data for an illumination time of $t =5 \ms$ and $T = 100 \ms$ are shown in Figure \ref{fig:beads}(a) and (d). The corresponding results for AMS are shown in sub-panels (b) and (e). Surprisingly, {the test detects most beads based on the $5 \mu\mathrm{s}$ data, even though some of them are barely} visible by eye there. Regarding the $T = 100 \ms$ data as ground truth, we depict this together with the regions found in (b) in sub-panel (e), {revealing} that there no false positive detections. Finally, the $T = 100 \ms$ data can be used to derive a segmentation into active and inactive regions depicted in (f).

% Figure environment removed

\subsubsection{Data example 2: mixture of differently sized crimson beads}

As a second data example, we now consider a mixture of $48 \nm$ and $200 \nm$ crimson beads. Consequently, the data shown in Figure \ref{fig:mixed_beads}(a) and (d) consists of two structures of different sizes, namely around $48 \nm$ and $200 \nm$. Due to the number of markers inside the spheres, the larger structures are significantly brighter than the small ones. We investigate $t = 15 \ms$ and $T = 100 \ms$. The AMS procedure is applied with the same parameters as before (i.e. again $m_n = 20$, corresponding to a largest box size of $400 \nm$), and the result is shown in sub-panels (b) and (e). It is immediately visible that it detects again nearly all of the small structures already from the measurements taken with $15 \ms$ dwell time, which is surprising as some of them are not visible by eye in the data. Once again, regarding the $T = 100 \ms$ data as ground truth, we depict this together with the regions found in (b) in sub-panel (e), {revealing} that there no false positive detections. Finally, the $T = 100 \ms$ data can be used to derive a segmentation into active and inactive regions depicted in (f).

% Figure environment removed

\subsection{Data example 3: tubulin in neonatal fibroblast}

{As third data set we finally consider tubulin in neonatal fibroblast. This data set is much more challenging compared to the previous two, since the corresponding structures are filaments and hence, compared to the spheres there, no longer fully contained in single rectangles. To better cover this filamentary structure, which is expected to have width of around $60-80\nm$, we choose $m_n = 10$ here, i.e. we restrict to boxes with maximal length $200 \nm$. Again, we aim to detect active regions inside the field of view based on noisy $t = 25 \ms$ data. It can readily be seen from sub-panel (a) of Figure \ref{fig:fibro}, that the corresponding data is quite noisy, but filamentary structures are clearly visible. Sub-panel (d) of the same figure shows the corresponding $T = 100 \ms$ measurement, revealing a folded filamentary structure in the upper right part of the field of view, which was hardly visible by eye in sub-panel (a). With the previously described setup, AMS is able to detect even the folded filamentary structure in the upper left part with a high accuracy, cf. sub-panels (b) and (c). We furthermore find that the method did not yield any false positives despite the relatively high noise-level in the $t = 25 \ms$ measurement, as the comparison with the full $T = 100 \ms$ ground truth data proves that all significant regions belong to fibroblasts.}

% Figure environment removed

{At this point, let us also compare the AMS methodology with direct pixel-wise thresholding. This is e.g. comparable to the techniques used in the previously mentioned SCIM methods (cf. \cite{vge20,kvnmh16}. Therefore, we consider simply each pixel, for which the corresponding value $Y_i$ in \eqref{eq:model_pois} exceeds the threshold $\eta \in \{2,3\}$, as an anomaly. Note, that these methods do not lead to any sort of statistical error control. In order to do so, we employ the \cite{bh95} procedure to obtain a pixel-wise thresholding with controlled false discovery rate (FDR). The results are shown in Figure \ref{fig:fibro_thresholding}.}

% Figure environment removed

{Apparently, none of the three methods is able to effectively detect the filamentary structures within the specimen. Even though the hard thresholding with $\eta = 3$ seems to work best, it still contains several noise pixels in all parts of the image. The FDR-controlled method seems to suffer severely from the inhomogeneity of the image, namely that the right side appears brighter than the left, and hence the corresponding $p$-values of the pixel-wise tests are smaller there. Overall, we can conclude that due to its multiscale structure the AMS procedure clearly outperforms simple pixel-wise thresholding and hence yields a substantial advantage over stat-of-the-art SCIM methods.}

We also compare the resulting significance maps for different values of $\alpha$, see Figure \ref{fig:fibro3}. It turns out that the AMS methodology is {remarkably} stable w.r.t. the significance level $\alpha$.

% Figure environment removed


\section{Discussion}

{In this paper, we have developed AMS, a multiscale scanning method allowing for unknown baseline and nuisance parameters. Under a minimal condition on the scales, AMS selects all regions deviating from the global null at guaranteed FWER. To this end we have proven a uniform invariance principle which allows to approximate the multiscale statistic by a distribution-free Gaussian version. This can be used for the computation of (asymptotically) correct critical values. The performance of AMS has been investigated both in simulations and on real data examples from super-resolution microscopy, where it can be seen as an instance of smart microscopy equipped with statistical guarantees. In particular, it outperforms pixel-wise thresholding procedures, including FDR-controlled ones.}

{We give an FFT based algorithm to perform AMS which roughly scales as $dNn^d$ where $N$ is the number of scales contained in $\mathcal R_n$. This effectively delimits the practical performance of AMS to small dimensions $d \leq 3$. However, a combination with ideas from \citet{w10} or using random regions $R$ might allow to extend AMS to a larger $d$ in almost linear time. However, it is expected that this comes at the expense of the losing detection power compared to the use of the full grid $I_n^d$.}

{An alternative option to the AMS approach might be based on sample splitting. Therefore, one could separate the spatial observations $Y_i$, $i \in I := I_n^d$ into two subsets $I= I_1 \cup I_2$, $I_1 \cap I_2 = \emptyset$, say e.g. $I_1 = \{1,3,...,n-1\}^d$ and $I_2 = \{2,4,....n\}^d$ for even $n$, and use $\left(Y_i\right)_{i \in I_1}$ for estimation of $\theta_0$ and $\xi$ only. This eases the asymptotic analysis and the statistic $T_n \left(\left(Y_i\right)_{i \in I_2}, \mathcal R_n, \hat \theta, \hat \xi\right)$ will then asymptotically behave exactly like $T_{n/2} \left(Y, \mathcal R_{n/2}, \theta_0,\xi\right)$, and hence previous results on invariance principles might be applicable. However, note that this leads to a severe reduction of the actual resolution of the method (precisely by a factor of $2$ due to omitting every second observation) and provides hence a reduced detection power.}

{A further interesting possible extension of the AMS methodology would be to random fields with spatial dependency. For $d = 1$ we suggest in the case of simple linear time series error models to follow the approach in \citet{fms14}, Section 6.1.1 to ease computational effort. Instead of the full likelihood ratio statistic, simply the local LRT statistics for the independent case have to be reweighed by estimates of the total variance. For general dimensions this is more involved. We believe that it might be possible to extend the weak invariance principle for $m$-dependent fields in Thm. 2.3 in \citet{kkm23} to the multiscale setup in this paper, however, at the expense of a large technical effort.}

{Finally, as mentioned by a referee, it would be of interest to extend AMS to the situation when no parametric family $F_{\theta,\xi}$ is assumed. In this case one aims to scan against an (unknown) baseline $F_0$, say. Consequently, not only certain nuisance parameters are unknown, rather the entire distribution $F_0$ and all possible alternatives $F_i, i \in I_n^d$. Already for $d = 1$ this leads to an entirely different situation, as the local LRT statistics have to be replaced by some \textit{non-parametric analogue}, e.g. local median statistics, see \citet{vbm22}. How to extend this to general $d$ is not obvious at all and an interesting task for future research.}

\section*{Data availability statement}

{The data underlying this article, including all software and scripts to reproduce the simulations and data analysis shown in this work are available under \LINK.}

\section*{Acknowledgment}

Financial support by the German Research Foundation DFG through subprojects A04 and A07 of CRC 755 and the DFG Cluster of Excellence "MBExC Multiscale Bioimaging - from Molecular Machines to Networks of Excitable Cells" is gratefully acknowledged. We also thank Ren\'e Siegmund {and Alexander Egner} from IFNANO for providing data and helpful discussions. {Furthermore, helpful comments and questions raised by the editor, an associate editor and three referees, which led to an improved presentation of the manuscript, are gratefully acknowledged.}

\bibliography{MultiNuisance}
\bibliographystyle{apalike} 

\appendix
	
	\section{Proofs}
	
	We start with some helpful abbreviations. For the whole section, let $X = \left(X_i\right)_{i \in I_n^d}$ be a field of i.i.d. standard Gaussians, $X_i \sim \mathcal N\left(0,1\right)$. As we rely heavily on {centred} and standardized partial sums, we introduce for $R \in \mathcal R_n$ the quantity
	\[
	Y_R\left(\theta, \xi\right):= \left|R\right|^{-1}\sum\limits_{i \in R} \left( \frac{Y_i-m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right) = \frac{\bar Y_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta,\xi\right)}}, \qquad \left(\theta, \xi\right) \in \Theta \times \Xi
	\]
	%\[
	%Y_R\left(\theta, \xi\right):= \left|R\right|^{-\frac12}\sum\limits_{i \in R} \left( \frac{Y_i-m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right) = \left|R\right|^{\frac12} \frac{\bar Y_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta,\xi\right)}}, \qquad \left(\theta, \xi\right) \in \Theta \times \Xi
	%\]
	with the mean and variance functions $m$ and $v$ from Assumption \ref{ass:model}. Also recall our abbreviation
	\[
	\bar X_R := \frac{1}{\left|R\right|} \sum_{i \in R} X_i.
	\]
	%as well as the corresponding Gaussian version
	%\[
	%X_R := \left|R\right|^{-\frac12} \sum\limits_{i \in R}X_i =  \left|R\right|^{\frac12} \bar X_R.
	%\]
	
	\subsection{Preparations}
	
	Next we recall a a series of helpful results taken from \citet{kmw20}.
	
	\begin{lem}[Coupling]\label{lem:coupling}
		Let Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, and let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta, \xi}$ with $\theta \in U, \xi \in V$ and let $\left(r_n\right)_{n \in \N} \subset \mathbb N$ be a sequence. Then, on the same probability space, there exists an array $X = \left(X_i\right)_{i \in I_n^d}$ of i.i.d. standard Gaussians $X_i \sim \mathcal N\left(0,1\right)$ such that
		\[
		\P{}{\left|\max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right|  - \max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)}\left|R\right|^{\frac12} \left|\bar X_R\right| \right| > \delta } \leq C \delta^{-3} \left(\frac{\log^{10}n}{r_n} \right)^{1/2}
		\]
		for all $\delta > 0$ with some universal constant $C> 0$ independent of $\theta$ and $\xi$.
	\end{lem}
	\begin{proof}
		As the distributions $F_{\theta, \xi}$ have uniformly sub-exponential tails, this follows directly from Theorem 4.3 in \citet{kmw20} in combination with a symmetrization argument as employed in the proof of Theorem 2.5 there.
	\end{proof}
	Note, that in terms of $\OP$ notation, Lemma \ref{lem:coupling} yields
	\begin{equation}\label{eq:coupling}
		\max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right|  - \max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)}\left|R\right|^{\frac12}\left|\bar X_R\right|  = \OP \left(\left(\frac{\log^{10} \left(n\right)}{r_n}\right)^{\frac16}\right),
	\end{equation}
	uniformly in $\left(\theta, \xi\right)\in U \times V$.
	%, and that this as well as the statement of Lemma \ref{lem:coupling} stays true if we replace the upper bound $n^d$ for the scale size by $m_n$ with any sequence $\left(m_n\right)_{n \in \N} \subset \N$ satisfying $r_n \leq m_n$ for all $n \in \N$.
	
	\begin{lem}[Taylor expansion, see \protect{Lemma 5.1 in \citet{kmw20}}]\label{lem:taylor_exact}
		Let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta_0, \xi}$ with fixed $\theta_0 \in \Theta, \xi \in \Xi$ and let $\left(r_n\right)_{n \in \N} \subset \mathbb N$ be a sequence satisfying \eqref{eq:r_n}. If Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, then
		
		\[
		\max\limits_{R \in  \mathcal{R}_n \left(r_n, n^d\right)} \left| T_R\left(Y, \theta_0, \xi\right) -  \left|R\right|^{\frac12} Y_R \left(\theta_0, \xi\right)\right| = \OP \left(\left( \frac{\log^3(n)}{r_n}\right)^{1/4}\right).
		\]
	\end{lem}
	Note, that the previous results remain true if we replace the upper bound $n^d$ for the scale size by $m_n$ with any sequence $\left(m_n\right)_{n \in \N} \subset \N$ satisfying $r_n \leq m_n$ for all $n \in \N$.
	
	\begin{lem}\label{lem:bound}
		Let Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, and let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta, \xi}$ with fixed $\theta \in U, \xi \in V$. If  $\left(r_n\right)_{n \in \N} \subset \mathbb N$ is a sequence satisfying \eqref{eq:r_n}, then we obtain	
		\[
		\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|\left| Y_R \left(\theta_0, \xi\right) \right|^3 = \OP\left(\left(\frac{\log^3\left(n\right)}{r_n}\right)^{\frac12} \right)
		\]
		uniformly for $\theta \in U, \xi \in V$.
	\end{lem}
	\begin{proof}
		Note, that due to $\alpha, \widetilde{\alpha} >0$, \eqref{eq:r_n} implies especially that $\gamma \geq 4$ and hence $\log^4\left(n\right) = o \left(r_n\right)$. 
		
		It is well known (see e.g. \citet{k11}), that for a standard Gaussian array $X = \left(X_i\right)_{i \in I_n^d}$, $X_i \stackrel{\text{i.i.d.}}{\sim} \mathcal N \left(0,1\right)$, one has
		\begin{equation}\label{eq:aux3}
			\E{}{\max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12} \left|\bar X_R\right|} \leq C \sqrt{\log\left(\# \mathcal R_n \left(r_n, m_n\right)\right)}
		\end{equation}
		with some constant $C > 0$. Due to Assumption \ref{ass:R} this implies that
		\[
		\frac{1}{\sqrt{\log\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12} \left|\bar X_R\right|  = \OP\left(1\right). 
		\]
		Together with \eqref{eq:coupling} this yields
		\[
		\frac{1}{\sqrt{\log\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right| = \OP\left(1\right)
		\]
		uniformly for $\left(\theta, \xi\right)\in U \times V$, where we used that $\log^4\left(n\right)= o \left(r_n\right)$.  Consequently we find
		\begin{align*}
			\sqrt{\frac{r_n}{\log^3\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)}  \left|R\right|\left| Y_R \left(\theta_0, \xi\right) \right|^3 \leq  \sqrt{\frac{1}{\log^3\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)}\left|R\right|^{\frac32}\left| Y_R \left(\theta_0, \xi\right) \right|^3 =\OP \left(1\right)
		\end{align*}
		uniformly for $\left(\theta, \xi\right)\in U \times V$, which proves the claim.
	\end{proof}
	
	\subsection{An adjusted Taylor expansion}
	
	Now we are in position to derive some preparations for the proof of Theorem \ref{thm:approximation_estimated_intensity}. Therefore we first replace the likelihood ratio statistic $T_R$ by its Taylor expansion according to Assumption \ref{ass:model}(e):
	\begin{lem}\label{lem:taylor_estimated}
		Let $\left(r_n\right)_{n\in\N} \subset \N$ and $\left(m_n\right)_{n\in\N} \subset \N$ be sequences tending to $\infty$ and consider the local statistic $T_R$ from \eqref{eq:TR}. Suppose that $Y$ is as in \eqref{eq:model} and let Assumptions \ref{ass:model} and \ref{ass:R} be fulfilled. If the estimators $\hat \theta_n$ and $\hat \xi_n$ satisfy \eqref{eq:estimators}, then
		\begin{equation}\label{eq:taylor}
			\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \hat \theta_n, \hat \xi_n) -  \left|R\right|^{1/2}\left|Y_R \left(\theta_0, \xi\right)\right|\right| = \OP\left(\left(\frac{\log^7(n)}{r_n}\right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right).
		\end{equation}
	\end{lem}
	
	\begin{proof}
		Let $U \times V$ be the {neighbourhood} of $\left(\theta_0, \xi\right)$ as in Assumption \ref{ass:model}. By \eqref{eq:estimators}, the probability that $\hat \theta_n \in U$ and $\hat \xi_n \in V$ tends to $1$, as $n \to \infty$. Hence, it suffices to prove the result conditional on this event. Consequently we can assume $\hat \theta_n \in U$ and $\hat \xi_n \in V$ in the following.
		
		By the triangle inequality and Lemma \ref{lem:taylor_exact}, we find
		\begin{align*}
			&\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R\left(Y, \hat \theta_n, \hat \xi_n\right) -   \left|R\right|^{\frac12} \left|Y_R\left(\theta_0, \xi\right)\right|\right| \\
			\leq &  \max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \theta_0, \xi) -   |R|^{\frac12}\left|Y_R\left(\theta_0, \xi\right)\right|\right|\\
			&+ \max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \hat \theta_n, \hat \xi_n) -   T_R\left(Y, \theta_0, \xi\right)\right| \\
			\leq &\sqrt{\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R^2(Y, \hat \theta_n, \hat \xi_n) -   T_R^2\left(Y, \theta_0, \xi\right)\right|} + \OP\left(\left( \frac{\log^3(n)}{r_n}\right)^{\frac14}\right),
		\end{align*}
		where we exploited Lemma \ref{lem:taylor_exact} and $\left|a-b\right| \leq \sqrt{\left|a^2 - b^2\right|}$. By the triangle inequality and Assumption \ref{ass:model}(e) we find furthermore that
		\begin{align*}
			&\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2(Y, \hat{\theta}, \hat{\xi}) - T_R^2(Y, \theta_0, \xi) \right|\\
			\leq& \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2(Y, \hat{\theta}, \hat{\xi}) - \left|R\right| \left|Y_R\left(\hat \theta_n, \hat\xi_n\right)\right|^2 \right| + \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2\left(Y, \theta_0, \xi\right)-\left|R\right| \left|Y_R\left(\theta_0, \xi\right)\right|^2\right|\\
			&+ \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2 - Y_R\left(\theta_0, \xi\right)^2 \right|\\
			\leq & C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R \left(\hat \theta_n, \hat \xi_n\right) \right|^3 + C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|  \left|Y_R\left(\theta_0, \xi\right) \right|^3 \\
			&+ \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2-Y_R\left(\theta_0,\xi\right)^2\right| \\
			\leq & 2C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\theta_0, \xi\right) \right|^3  + \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|\left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2-Y_R\left(\theta_0,\xi\right)^2\right|\\ & + C_T\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^3-Y_R\left(\theta_0,\xi\right)^3\right|,
		\end{align*}
		where we used $\left| \left|a\right|^3 - \left|b\right|^3\right| = \left| \left|a^3\right| - \left|b^3\right|\right| \leq \left| a^3 - b^3\right|$ for $a,b \in \R$. In view of Assumptions \ref{ass:model}(a) and \ref{ass:R}, the first term in the last display can be bounded using Lemma \ref{lem:bound}. The second and third term will be handled using the mean value theorem. Therefore, note that all derivatives of
		\[
		\left(\theta, \xi\right)\mapsto Y_R\left(\theta,\xi\right)^k, \qquad k \in \left\{2,3\right\}
		\]
		are a.s. bounded due to Assumption \ref{ass:model}(b)--(d) and the fact that $\bar Y_R$ is a.s. bounded in view of Lemma \ref{lem:bound}. This together with \eqref{eq:estimators} implies that
		\begin{align*}
			\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left| Y_R\left(\hat{\theta}, \hat{\xi}\right)^2 - Y_R\left(\theta_0, \xi\right)^2\right|\leq m_n C \left(\|\hat{\theta}_n-\theta_0 \|_2 + \|\hat{\xi}_n - \xi \|_2\right) = \OP \left(\frac{m_n}{s_n}\right),
		\end{align*}
		where $C$ is some generic constant $C > 0$. As $\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^3-Y_R\left(\theta_0,\xi\right)^3\right|$ can be treated similarly, this yields the claim.
	\end{proof}
	
	\subsection{Proof of Theorem \ref{thm:approximation_estimated_intensity}}
	
	Now we are ready to prove the Gaussian approximation result from Theorem \ref{thm:approximation_estimated_intensity}.
	\begin{proof}[Proof of Theorem \ref{thm:approximation_estimated_intensity}]
		For notional simplicity, we will throughout this proof drop the index of $\widetilde{\omega}{_n}$ and $\omega{_n}$. Furthermore we abbreviate $\widetilde{\mathcal R}_n := \mathcal R_n \left(r_n, m_n\right)$. First of all, we estimate	
		\begin{align*}
			&\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right|\\
			=& \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right| \\
			\leq& \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(T_R \left(Y, \hat \theta_n, \hat \xi_n\right) -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| - \omega \left(\left|R\right|\right) \right) \right| \\
			& + \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right|- \omega \left(\left|R\right|\right) \right) \right| \\
			\leq & \max_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right)  \max _{R \in \widetilde{\mathcal R}_n} \left| T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right|\right| \\
			& + \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|,
		\end{align*}
		where we used $\left|\|x\|_\infty-\|y\|_\infty  \right|\leq \|x-y\|_\infty$. The first term can be controlled using Assumption \ref{ass:Omega} and Lemma \ref{lem:taylor_estimated} by
		\begin{align*}
			\max_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right)  \max _{R \in \widetilde{\mathcal R}_n} \left| T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right|\right| 
			= \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)\right).
		\end{align*}
		So it remains to estimate the second term, i.e. for a suitable sequence $\left(c_n\right)_{n\in\N}$ we want to show that
		\begin{align}\label{eq:aux2}
			\lim_{M \to \infty} \limsup_{n  \to\infty} \mathbb P_0 \left[\left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right)- \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|> c_n M\right]  = 0
		\end{align}
		This is more subtle, as the coupling in Lemma \ref{lem:coupling} only yields a result for the difference of the maxima, and not for the maximum of the differences (as lemma \ref{lem:taylor_estimated} does). We therefore borrow a slicing technique from \citet{pwm18}, dividing the set of scales is into families on which $\widetilde{\omega}$ and $\omega$ are almost constant, i.e. into
		\[
		\mathcal{R}_{n,j}:= \left\lbrace R \in \mathcal{R}_n \left| \right. \exp\left(\epsilon_j\right) \leq \left|R\right| <  \exp\left(\epsilon_{j+1}\right) \right\rbrace.
		\]
		Here and in what follows we choose
		\[
		\epsilon_1 := \log\left(r_n\right), \qquad \epsilon_j = \epsilon_1 + \frac{j-1}{J} \log \left(\frac{m_n}{r_n}\right), \quad j=2, ..., J.
		\]
		With this definition, for any choice $J \in \N_{\geq 2}$ we obtain
		\[
		\widetilde{\mathcal R}_n = \mathcal{R}_n(r_n, m_n) = \left\{ R \in \R_n ~\big|~ r_n \leq \left|R\right| \leq m_n \right\} = \bigcup\limits_{j\in J}\mathcal{R}_{n,j}.
		\]
		The parameter $J \in \N$ will be defined later. On $\mathcal R_{n,j}$ we can approximate $\widetilde{\omega}\left(\left|R\right|\right)$ and $\omega\left(\left|R\right|\right)$ by
		\begin{align*}
			\widetilde{\omega}_{j,n} := \widetilde{\omega}\left(\exp\left(\epsilon_j\right)\right) \qquad \text{and}\qquad \omega_{j,n} := \omega\left(\exp\left(\epsilon_j\right)\right),
		\end{align*}
		respectively. Then it holds
		\[
		\omega_{j+1,n} \leq \omega\left(\left|R\right|\right) \leq \omega_{j,n}\qquad\text{and}\qquad \widetilde{\omega}_{j+1,n} \leq \widetilde{\omega}\left(\left|R\right|\right) \leq \widetilde{\omega}_{j,n}
		\]
		for all $R \in \mathcal{R}_{n,j}$. Now we compute
		\begin{align*}
			&\max\limits_{R \in \mathcal{R}_{n,j}}\widetilde{\omega}(|R|) \left(\left|R\right|^{\frac12} \left|Y_R\left(\theta_0, \xi\right)\right|-\omega(|R|) \right)-\max\limits_{R \in \mathcal{R}_{n,j}}\widetilde{\omega}(|R|) \left(\left|R\right|^{\frac12} \left|\bar X_R\right|-\omega(|R|) \right)\\
			\leq& \left( \widetilde{\omega}_{j,n} \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left|Y_R \left(\theta_0, \xi\right) \right| - \widetilde{\omega}_{j+1,n} \max\limits_{R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right| \right) + \left(\widetilde{\omega}_{j,n} \omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n} \right)\\
			=& \widetilde{\omega}_{j,n} \left( \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left| \bar X_R\right|\right) + \left(\widetilde{\omega}_{j,n} - \widetilde{\omega}_{j+1,n} \right) \max\limits_{ R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right|\\
			& + \left(\widetilde{\omega}_{j,n}\omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n} \right),
		\end{align*}
		and a similar relation holds true if the roles of $\bar X_R$ and $Y_R \left(\theta_0, \xi\right)$ are interchanged. To bound the differences involving the $\widetilde{\omega}_{j,n}$ terms, we exploit the rough bound
		\begin{equation}\label{eq:aux1}
			\widetilde{\omega}_{j,n} = \widetilde{\omega}\left(\exp(\epsilon_j) \right)\leq C_\omega \left(\log \left( \frac{n^d}{\exp\left(\epsilon_j\right)}\right) \right)^{\widetilde{\alpha}}\leq C_\omega\left(\log(n^d) \right)^{\widetilde{\alpha}}=:A_n
		\end{equation}
		for all $1 \leq j \leq J$, and a similar analogue for $\omega_{j,n}$. From the mean value theorem we get
		\begin{align*}
			\widetilde{\omega}_{j,n} - \widetilde{\omega}_{j+1,n} =&\ \widetilde{\omega}\left(\exp\left(\epsilon_{j+1}\right) \right)-\widetilde{\omega}\left(\exp\left(\epsilon_{j}\right) \right)\\
			\leq& \left|\exp\left(\epsilon_j\right)-\exp\left(\epsilon_{j+1}\right) \right| \max_{\zeta \in \left(\epsilon_j, \epsilon_{j+1}\right)} \widetilde{\omega}'\left(\exp\left(\zeta_j\right) \right)\\
			\leq& C_\omega\left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)} \left(\exp(\epsilon_{j+1}-\epsilon_j)-1\right)\\
			=& C_\omega \left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)}\left( \exp\left(\frac{1}{J} \log\left(\frac{m_n}{r_n}\right) \right)-1\right) \\
			= & C_\omega \left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)} \left(\left(\frac{m_n}{r_n}\right)^{\frac1J}-1\right)=:B_n
		\end{align*}
		Similar statements hold for $\omega_{j,n}$ and $\omega_{j,n}-\omega_{j+1,n}$ and hence using \eqref{eq:aux1} we obtain
		\begin{align*}
			\left|\widetilde{\omega}_{j,n} \omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n}\right|
			=& \left|\widetilde{\omega}_{j,n} (\omega_{j,n} - \omega_{j+1,n} ) - \omega_{j+1,n}\left(\widetilde{\omega}_{j,n}-\widetilde{\omega}_{j+1,n}\right)\right|\\
			\leq&C_\omega^2  \left(\left(\frac{m_n}{r_n}\right)^{\frac1J}-1\right) \left( \left(\log(n^d) \right)^{\widetilde{\alpha} + \max\left\{\beta, 0\right\}} +  \left(\log(n^d) \right)^{\alpha + \max\left\{\widetilde{\beta}, 0\right\}} \right)=:C_n
		\end{align*}
		This implies
		\begin{align*}
			&\left|\max\limits_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega}\left(\left|R\right|\right) \left(\left|R\right|^{\frac12}\left|Y_R\left(\theta_0,\xi\right)\right|-\omega\left(\left|R\right|\right) \right)- \max\limits_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega}\left(\left|R\right|\right) \left(\left|R\right|^{\frac12}\left|\bar X_R\right|-\omega\left(\left|R\right|\right) \right) \right|\\
			\leq & A_n \max_{1 \leq j \leq J} \left| \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \widetilde{\mathcal R}_n} \left|R\right|^{\frac12}\left| \bar X_R\right|\right| + B_n \max\limits_{ R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right| + C_n,
		\end{align*}
		and hence
		\begin{align*}
			&\P{0}{\left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right)- \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|> c_n M}\\
			\leq & \P{0}{A_n \max_{1 \leq j \leq J} \left|  \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left| \bar X_R\right|\right| > \frac{c_n M}{3}} \\
			& + \P{0}{B_n \max\limits_{ R \in \mathcal{R}_{n}}\left|R\right|^{\frac12}\left|\bar X_R\right| > \frac{c_n M}{3}} + \P{0}{C_n > \frac{c_n M}{3}} \\
			=:& \text{I} + \text{II} + \text{III}.
		\end{align*}
		To bound I, we use Lemma \ref{lem:coupling} and the union bound to obtain
		\[
		\text{I} \leq C\left|J\right| \frac{3^3A_n^3}{c_n^3M^3} \left(\frac{\log^{10} \left(n\right)}{r_n}\right)^{\frac12}.
		\]
		Here and in what follows, $C >0$ is some generic constant, the value of which can change from line to line. For II, we exploit Markov's inequality as well as \eqref{eq:aux3} and find in view of Assumption \ref{ass:R}, that
		\[
		\text{II} \leq \frac{\E{}{\max_{R \in \mathcal R_n} \left|R\right|^{\frac12} \left|\bar X_R\right|}}{c_n M 3^{-1} B_n^{-1}} \leq C \frac{B_n \sqrt{\log\left(n\right)}}{c_n M}
		\]
		with some constant $C > 0$. This shows that \eqref{eq:aux2} is satisfied as soon as
		\begin{enumerate}
			\item[(A)] $\left|J\right| A_n^3 c_n^{-3} \log^5 \left(n\right) r_n^{-\frac12} = \mathcal O \left(1\right)$, 
			\item[(B)] $B_n c_n^{-1} \sqrt{\log\left(n\right)} = \mathcal O \left(1\right)$, and
			\item[(B)] $C_n = o \left( c_n\right)$
		\end{enumerate}
		hold true. To obtain this, we set
		\[
		J := \lfloor \frac{\log^{\nu+1} \left(n\right)}{c_n}\rfloor
		\]
		with some parameter $\nu > 0$ to be determined. This implies
		\[
		\left|J\right| \frac{A_n^3}{c_n^3} \left(\frac{\log^{10}\left(n\right)}{r_n}\right)^{\frac12} \leq C \frac{\log^{6+3\widetilde{\alpha} + \nu}}{c_n^4 \sqrt{r_n}},
		\]
		which proves (B) for
		\begin{equation}\label{eq:cn}
			c_n := \left(\frac{\log^{12+6\widetilde{\alpha} + 2\nu}\left(n\right)}{r_n}\right)^{\frac18}.
		\end{equation}
		Next we use $m_n / r_n \leq n^d$ and use $\exp\left(x\right) - 1 \leq 2x$ for $x \in \left[0,1\right]$ to find
		\[
		\left(\frac{m_n}{r_n}\right)^{\frac1J} \leq n^{\frac{d}{J}} = \exp\left(\frac{d c_n}{\log^\nu\left(n\right)}\right) \leq 1+C \frac{c_n}{\log^\nu\left(n\right)}
		\]
		for any sufficiently large $n$ supposed that $c_n = o \left(\log^\nu\left(n\right)\right)$. Thus it holds
		\[
		B_n \leq C \frac{c_n}{\log^{\nu-\max\left\{\widetilde{\beta},0\right\}}\left(n\right)},
		\]
		and hence (A) is satisfied if $\nu \geq \max\left\{\widetilde{\beta},0\right\} + \frac12$. It can readily be seen that (C) is now satisfied as soon as we define
		\[
		\nu := \max\left\{\frac12, \alpha, \widetilde{\alpha}\right\} + \max\left\{\beta, \widetilde{\beta},0\right\}.
		\]
		Note that this also ensures the required condition $c_n = o \left(\log^\nu\left(n\right)\right)$ under \eqref{eq:r_n}. Hence \eqref{eq:aux2} is satisfied with $c_n$ as in \eqref{eq:cn}, and thus we have
		\begin{align*}
			\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right|= \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)+\left(\frac{\log^{12+6\widetilde{\alpha} + 2\nu}\left(n\right)}{r_n}\right)^{\frac18}\right),
		\end{align*}
		which together with $\frac{\log^{\tilde \alpha + 3/44}(n)}{r_n^{5/4}} = \mathcal O \left(\frac{\log^{\gamma/8}(n)}{r_n^{1/8}}\right)$ proves the claim.
	\end{proof}

\end{document}