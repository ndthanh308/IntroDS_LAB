\documentclass[a4paper,10pt]{scrartcl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{a4wide}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{natbib}
\pgfplotsset{compat=newest}
\newlength{\fwidth}
\newlength{\fheight}

\usepackage{todonotes} 

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}[2]{\mathbb{P}_{#1}\left[#2\right]}
\newcommand{\E}[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\I}{\mathbb{I}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\V}[2]{\mathbb{V}_{#1}\left[#2\right]}
\newcommand{\OP}{\mathcal O_{\mathbb P_0}}
\newcommand{\toP}{\stackrel{\mathbb P}{\rightarrow}}
\newcommand{\toD}{\stackrel{\mathcal D}{\rightarrow}}
\newcommand{\p}{\mathrm{pen}}
\newcommand{\pen}[2]{\p_{#1}\left(#2\right)}
\newcommand{\1}{\mathbf{1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lem}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollary}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{examples}[theorem]{Examples}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}{Assumption}
\newtheorem{definition}[theorem]{Definition}

%\renewcommand{\thefootnote}{\arabic{footnote}}

%\allowdisplaybreaks

%preamble

\begin{document}

	\begin{center}
	\begin{minipage}{.8\textwidth}
		\centering 
		\LARGE Multiscale scanning with nuisance parameters\\[0.5cm]
		
		\normalsize
		\textsc{Claudia K\"onig}\\[0.1cm]
		\verb+claudia-juliane.koenig@mathematik.uni-goettingen.de+\\
		Institute for Mathematical Stochastics, University of G\"ottingen\\[0.3cm]
		
		\textsc{Axel Munk}\\[0.1cm]
		\verb+munk@math.uni-goettingen.de+\\
		Institute for Mathematical Stochastics, University of G\"ottingen\\
		and\\
		Felix Bernstein Institute for Mathematical Statistics in the Bioscience, University of G\"ottingen\\[0.3cm]
		
		\textsc{Frank Werner}\footnotemark[1]\\[0.1cm]
		\verb+frank.werner@mathematik.uni-wuerzburg.de+\\
		Julius-Maximilians-Universität Würzburg (JMU), Institute of Mathematics, Würzburg, 		Germany
	\end{minipage}
\end{center}

\footnotetext[1]{Corresponding author}	

\begin{abstract}
We investigate the problem to find anomalies in a $d$-dimensional random field via multiscale scanning in the presence of nuisance parameters. This covers the common situation that either the baseline-level or additional parameters such as the variance are unknown and have to be estimated from the data. We argue that state of the art approaches to determine asymptotically correct critical values for the multiscale scanning statistic will in general fail when naively such parameters are replaced by plug-in estimators. Opposed to this, we suggest to estimate the nuisance parameters on the largest scale and to use the remaining scales for multiscale scanning. We prove a uniform invariance principle for the resulting adjusted multiscale statistic (AMS), which is widely applicable and provides a computationally feasible way to simulate asymptotically correct critical values. We illustrate the implications of our theoretical results in a simulation study and in a real data example from super-resolution STED microscopy. This allows us to identify interesting regions inside a specimen in a pre-scan with controlled family-wise error rate.
\end{abstract}

\textit{Keywords:} scan statistic, invariance principle, limit theorem, multiscale analysis, super-resolution microscopy\\[0.1cm]

\textit{AMS classification numbers:} 60F17, 62H10, 60G50, 62F03.  \\[0.3cm]
\date{\today}

\section{Introduction}

Scanning a $d$-dimensional random field $Y = \left(Y_i\right)_{i\in \left\{1,...,n\right\}^d}$ of random variables (r.v.'s) $Y_i$ for anomalies in their distribution is a prominent topic in statistical reaseach which is of great practical use. Applications range from the detection of anomalous clusters in social and biological networks and graphs \citep{accd11,cn14,srs16,bamh20} over detection of structural changes in imaging \citep{s18,cmv02,adh06} to genomics \citep{rdj14,fhms14}, brain research \citep{lpstv21} and cell microscopy \citep{pwm18}, to mention a few.

\subsection{Multiscale scanning}\label{subsec:scanning}

We will find anomalies in the field $Y$ by \textit{multiscale scanning}, which has a long and rich history in statistics. We refer to \citet{k97} for an early reference and to \citet{adh06,w10,accd11,n12,mbs13,cw13,sac16,pwm18,kmw20} for more recent works. A particular instance is multiscale change point detection, see e.g. \citet{fms14}, where the field is of spacial dimension $d = 1$, or \citet{cws22} for arbitrary dimension $d$, and the references given there. 

In this paper, we consider a parametric model of the form
\begin{equation}\label{eq:model}
	Y_i \sim F_{\theta_i, \xi},\qquad i \in I_n^d := \left\{1,...,n\right\}^d,
\end{equation}
where the $Y_i$ are independent, real valued r.v.'s distributed to some unknown $F_{\theta_i, \xi}$ from a given, known class of distributions $\mathcal F$ indexed in $\theta$ and $\xi$. Here, $\theta_i \in \Theta$ denotes the parameter of interest, $\xi \in \Xi$ a collection of nuisance parameters, and $\Theta \subset \R^{d_1}$ and $\Xi\subset \R^{d_2}$ are the corresponding parameter spaces. $i$ denotes a multiindex, which we assume to index a $d$-dimensional grid, but we stress that our analysis can also be transferred to different settings. An exemplary case would be a Gaussian field $Y_i\sim \mathcal N \left(\theta_i, \Sigma\right)$, where anomalies $\theta_i$ occur in the mean as deviation from a ground truth $\theta_0 \in\Theta$, the unknown covariance $\Sigma > 0$ is the nuisance parameter. For the moment, we do not pose structural assumptions on $F_{\theta, \xi}$, but require some smoothness and tail properties later on (see Assumption \ref{ass:model} below).

Given a baseline or reference intensity $\theta_0 \in \Theta$ and a region $R \subset I_n^d$, the problem to detect an anomaly inside $R$ can then be formulated as the testing problem
\begin{subequations}\label{eq:single_testing}
	\begin{equation}\label{eq:single_testing_hypothesis_pois}
		H_{R,n}:~\forall~i \in R: \theta_i = \theta_0
	\end{equation}
	vs.
	\begin{equation}\label{eq:single_testing_alternative_pois}
		K_{R,n}:~\exists~i \in R \text{ s.t. }\theta_i \neq \theta_0.
	\end{equation}
\end{subequations}
Note, that in practice often $d_1 = 1$, and it might be more relevant to consider alternatives of the form $\theta_i > \theta_0$ or $\theta_i < \theta_0$. For the sake of brevity, we consider only two-sided testing here, but we emphasize that the one-sided situation can be developed completely analogously (see Remark \ref{rem:one_sided} below).

Depending on the structure of $R$, the likelihood ratio test (LRT) is known to have favourable properties, see e.g. \cite{lr05}. Therefore, throughout this paper, we consider the corresponding log LRT statistic
\begin{equation}\label{eq:TR}
	T_R \left(Y, \theta_0, \xi\right) := \sqrt{2 \log \left(\frac{\sup_{\theta \in\Theta} \prod_{i \in R} f_{\theta, \xi} \left(Y_i\right)}{\prod_{i \in R} f_{\theta_0, \xi} \left(Y_i\right)}\right)}
\end{equation}
for \eqref{eq:single_testing}, where $f_{\theta, \xi}$ denotes the density or probability distribution function of $F_{\theta, \xi}$. Whenever this statistic exceeds a suitably chosen threshold (depending on $R$ and the prescribed level $\alpha \in \left(0,1\right)$), the corresponding LRT will reject. 

However, as position and size of anomalies will in general be unknown, the region of interest $R$ is not known in advance. Therefore, we aim to \textit{scan} over $\mathcal{R}_n \subset 2^{I_n^d}$ (with $2^X $ being the power set of $X$) a set of regions of interest (e.g. the system of all discrete cubes inside $I_n^d$). This leads to a multiple testing problem
\begin{equation}\label{eq:multiple_testing}
	H_{R,n} \text{ vs. }K_{R,n} \qquad\mathbf{simultaneously}\text{ over } \mathcal R_n.
\end{equation}
The resulting scanning test then has to combine in a suitable way the local test statistics $T_R$, $R \in \mathcal R_n$, in \eqref{eq:TR}, such that the number  of false rejections over $\mathcal R_n$ can be controlled. To this end, we aim to bound (asymptotically) the family wise error rate (FWER) by $\alpha \in \left(0,1\right)$, i.e. \citep[see e.g.][]{m81,d14} 
\begin{equation}\label{eq:FWER}
	\sup\limits_{R \in \mathcal R_n} \P{H_{R,n}}{\text{``any false rejection in }R\text{''}} \leq \alpha + o(1) \qquad\text{as}\qquad n\to \infty.
\end{equation}
To combine the local test statistics to a reasonable scanning-type statistic, it is pertinent to employ a scale calibration $\widetilde{\omega}, \omega : \mathbb N \times \mathbb N \to \left(0,\infty\right)$ to avoid small regions $R \in \mathcal R_n$ from dominating the overall statistic. Here, and in what follows, the cardinality $\left|R\right| \in \mathbb N$ of a region $R \in \mathcal R_n$ is denoted as \textit{scale}. This leads to
\begin{align}\label{eq:Tn}
	T_n \equiv T_n (Y, \mathcal R_n, \theta_0, \xi):=\max_{R\in\mathcal R_n} \widetilde{\omega}\left(\left|R\right|,n\right)\left[ T_R\left(Y, \theta_0, \xi\right)-\omega\left(\left|R\right|,n\right)\right],
\end{align}
which we then call a (calibrated) \textit{multiscale test statistic} whenever a range of scales is covered by $\mathcal R_n$. An appropriate choice of the calibration $\widetilde{\omega}$ and $\omega$ is of high importance for the overall method and will determine its detection properties. Different choices of $\widetilde{\omega}$ and $\omega$ have been proposed in the literature, mostly to achieve a non-degenerate limiting behaviour of $T_n$ or to optimize the detection power of the multiple test on all or specific scales. Besides uncalibrated scan statistics used e.g. in \citet{k97,a97,a98,n12,pywr22}, we mention \citet{ds01}, who proposed a calibration ensuring non-degenerateness of the limiting distribution, and a more recent multivariate analogue by \citet{ds18}. Building on this, \citet{dw08} proposed a calibration, which was later shown to be superior over the uncalibrated scan statistic in \citet{cw13}, and which is since then widely used in the community, see e.g. \citet{rw13,smd13,fms14,kmw20}. More recently, other calibrations have been suggested, e.g. leading to Gumbel limits \citep{sac16,pwm18}, see also \citet{w20} for a recent review on this topic. In this study, we mostly leave the choice of $\widetilde{\omega}$ and $\omega$ to the user up to some mild condition (cf. Assumption \ref{ass:Omega}) on its growth posed below. This choice may e.g. reflect a preference for detecting signals of certain size, which is driven by the application at hand.

The multiscale scanning methodology is now as follows. Given $Y$, the values $T_R \left(Y, \theta_0, \xi\right)$, $R \in \mathcal R_n$ are computed, and all hypotheses $H_{R,n}$ such that
\[
T_R \left(Y, \theta_0, \xi\right) \geq c_{|R|}(\eta) := \frac{\eta}{\widetilde{\omega}(|R|,n)} + \omega(|R|,n)
\]
are rejected. Here, $\eta \in \R$ is a global threshold to be determined appropriately. Hence, the scale calibration $\widetilde{\omega}, \omega$ leads to \textit{scale dependent} local critical values in general, see also \citet{w10,psm17}. The global threshold $\eta$ is now universal, however, and to be chosen such that 
\begin{equation}\label{eq:threshold}
	\P{0}{T_n \left(Y, \mathcal R_n, \theta_0, \xi\right)>\eta} \leq \alpha + o(1)\qquad\text{as}\qquad n \to \infty
\end{equation}
for some $\alpha \in \left(0,1\right)$, as then the estimate
\[
\sup\limits_{R \in \mathcal R_n} \P{H_{R,n}}{\text{``any false rejection in }R\text{''}}  \leq \P{0}{\text{``any false rejection in }I_n^d\text{''}}.
\]
ensures \eqref{eq:FWER} to hold true, i.e. any of the local rejections is (asymptotically) correct with probability $\geq 1-\alpha$. 

As long as $\theta_0$ and the nuisance parameter $\xi$ are known, the above methodology is widely developed. One central example is to scan an independent Gaussian field $Y$ for anomalies (elevated means), where $\theta_ 0 =\mu_0 = 0$ and $\xi = \Sigma_0$ is the known covariance. 
%Multiscale scanning has e.g. been used by \citet{ds01,dw08} to obtain uniform confidence statements about qualitative properties of a density, in \citet{adh06} to detect filamentary structures, or in \cite{pwm18} to infer on the support of an indirectly observed quantity. 
In this case, optimality properties of this method, depending on the chosen calibration $\tilde \omega$ and $\omega$, are known, see e.g. \citet{cw13} for a statement about minimax optimality when using just a single scale, or \citet{sac16} and \citet{kmw20} for an oracle property revealing that scanning over several scales is asymptotically as efficient as scanning over the correct scale only. 

For many other distributional settings such as Binomial or Poisson distributions, the situation is slightly more delicate. First steps to treat this case have been taken by \citet{a97,a98,asn16}, who proposed distributional approximations of $T_n$ in a Poissonian setting. However, those results are limited to one fixed scale. Another approach is based on tail bounds for $T_R$ as proven in \citet{w22}, which can be combined with a Bonferroni correction to ensure \eqref{eq:threshold} also in a multiscale setting. Complete multiscale approximations of $T_n$ in \eqref{eq:Tn} have been proposed in case of Bernoulli and Binomial fields by \citet{w10} or in a general exponential family setting by \citet{kmw20}. The latter result provides an invariance principle, which allows (given the baseline parameter $\theta_0$ and the nuisance parameters $\xi$) to approximate the distribution of $T_n$ by an easy to compute Gaussian approximation
\begin{equation}\label{eq:Mn}
	M_n \equiv M_n\left(\mathcal R_n\right) := \max_{R\in\mathcal R_n} \widetilde{\omega}(|R|,n)\left[ \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right| - \omega(|R|,n) \right]
\end{equation}
with i.i.d. standard normal r.v.'s $X_i$, $i \in I_n^d$. Consequently, it is possible to simulate an appropriate threshold $\eta$ in \eqref{eq:threshold} in practice. 


\subsection{Our contribution - AMS}

In contrast to existing work, the focus of this study, however, is on the case that both $\theta_0$ and the nuisance parameter $\xi$ in \eqref{eq:model} are unknown. Note, that this changes the overall situation completely, as the local LRT statistics $T_R \left(Y, \theta_0, \xi\right)$ are no longer accessible. A common remedy which immediately comes to mind is to pre-estimate both $\theta_0$ and $\xi$ from $Y$ globally and then use the corresponding estimators $\hat \theta_0$, $\hat \xi$ as a plug-in in $T_R$ and consequently in the multiscale extension $T_n$ in \eqref{eq:Tn}. When one aims to estimate $\theta_0$ only, then such an approach is known as pseudo maximum likelihood, and has e.g. been proposed in \citet{gs81}. Under certain regularity conditions, they prove asymptotic normality of the resulting estimator, but this result is limited to a single scale and cannot be transferred to the case of multiscale testing. A rigorous analysis of a similar approach in case of testing is -- to the best of our knowledge -- missing. In fact, we will show that the distribution of $T_n \left(Y,\mathcal R_n, \hat \theta_0, \hat \xi\right)$ differs from that of $T_n \left(Y, \mathcal R_n, \theta_0, \xi\right)$ (and hence also from that of $M_n\left(\mathcal R_n\right)$ in \eqref{eq:Mn}) in general, and this is the case even in the most simple Gaussian example. This then raises the question whether it is possible to extend the multiscale test in \eqref{eq:Tn} to this situation and how to adjust the threshold $\eta$ in \eqref{eq:threshold} properly, an issue which has often been overlooked and has not been discussed in the literature. A notable exception is \citet{wp22}, where the tail bounds for $T_R \left(Y, \hat \theta_0, \hat \xi\right)$ in \cite{w22} are combined with a Bonferroni correction to construct corrected values for $\eta$ in \eqref{eq:threshold}. This approach is, however, limited to relatively small systems $\mathcal R_n$ of regions. In this paper, we will close the aforementioned gap and we will show that is still possible to obtain a uniform invariance principle (see Section \ref{subsec:theory}) for the distribution of $T_n \left(Y,\mathcal R_n, \hat \theta_0, \hat \xi\right)$ as long as the scales in $\mathcal R_n$ are not too small or too large compared to the sample size $n^{-d}$. More precisely, we will show that $T_n \left(Y,\mathcal R_n, \hat \theta_0, \hat \xi\right)$ can be approximated in distribution by $M_n \left(\mathcal R_n\right)$ as in \eqref{eq:Mn}, which can be pre-simulated given $\mathcal R_n$ in advance. From this it becomes apparent that working with thresholds, which do not incorporate the error in estimating $\theta_0$ and $\xi$, does not provide the correct FWER, even asymptotically. The suggested adjusted multiscale scanning (AMS) procedure provides correct thresholds for estimated values of $\theta_0$ and $\xi$. 

\subsection{Failure of naive plug-in in the Gaussian model}\label{subsec:gauss}

To explain why simply replacing $\theta_0$ and $\xi$ by estimators $\hat\theta_0$ and $\hat \xi$ will lead to invalid thresholds $\eta$ in \eqref{eq:threshold}, let us consider the homogeneous Gaussian model for $d = 1$ \citep[see e.g.][]{sac16} as a the most common and best understood example of \eqref{eq:model}. Here, $F_{\theta_i, \xi} = \mathcal N \left(\mu_i, \sigma_0^2\right)$ parametrized by its mean $\mu_i \in\R$ and variance $\sigma_0^2 > 0$. In this case, it can readily be seen that replacing $\mu_0$ and / or $\sigma_0$ by estimators does actually influence the distribution of $T_n$ and requires corrected critical values (thresholds) $c_{|R|}(\eta)$ for the local LRT test statistics. To illustrate this, we have depicted the empirical distributions of $T_n \left(Y,\mathcal R, \mu_0, \sigma_0\right) = M_n(\mathcal R)$ as in \eqref{eq:Mn}, and a corresponding variant with standard estimators $\hat \mu_0 = \frac{1}{n^d} \sum_{i \in I_n^d} Y_i$ and $\hat \sigma_0^2 = \frac{1}{n^d-1} \sum_{i \in I_n^d} \left(X_i - \hat \mu_0\right)^2$ for $n = 128$ and $\mathcal R = 2^{I_n^d}$ in Figure \ref{fig:gauss_dist}. As an exemplary scale calibration, we use the common choice $\tilde \omega \equiv 1$ and $\omega \left(|R|,n\right) = \sqrt{2 \log \left(n^d / |R|\right) + 1}$, see \citet{dw08}.  

% Figure environment removed

It is clearly visible that the distribution of the true scan statistic $T_n$ differs especially for large quantiles substantially from the one involving any of the estimators. A distributional difference already occurs when the true variance $\sigma_0^2$ is known, i.e. for $T_n \left(Y, \mathcal R, \hat \mu_0, \sigma_0\right)$ with estimated global baseline $\hat \mu_0$. Then, the log LRT statistic $T_R \left(Y,\mu_0,\sigma_0\right)$ compares the local average of $Y$ on $R$ with $\mu_0$, i.e. in case of unknown $\mu_0$ with the global average $\hat \mu_0$ on $I_n^d$. Consequently, for large regions, the value of $T_R \left(Y, \hat \mu_0, \sigma_0\right)$ will be negligible (in particular for $R = I_n^d$ even constantly be $0$), but $T_R \left(Y, \mu_0, \sigma_0\right)$ is not (for $R = I_n^d$, this is proportional to a $\chi^2_{n^d-1}$ distributed random variable). So, as long as the penalization $\tilde \omega, \omega$ does not hinder these large scales from contributing to $T_n \left(Y, \mathcal R, \hat \mu_0, \sigma_0\right)$ (and $T_n \left(Y, \mathcal R, \mu_0, \sigma_0\right)$), the corresponding distributions must be different. On the other hand, a penalization hindering large scales from contributing to $T_n$ is not desired, as this will decrease the overall detection power of the corresponding LRT tests on such scales. It is well known, see e.g. \citet{ds01}, that the penalization chosen here indeed ensures the largest scales to contribute to $T_n \left(Y, \mathcal R, \mu_0, \sigma_0\right)$. Hence, we find that in this case, also the asymptotic distribution (which for this scale penalization is a supremum of a Gaussian process, see \citet{ds01}) should be different. 

Note, that the above observation is not restricted to Gaussian model only, but also occurs in more general models. The largest scales will in general contribute to the distribution-free statistic $M_n$ in \eqref{eq:Mn}, but will under mild assumptions on $T_R$ (local means) not contribute to $T_n$ in \eqref{eq:Tn} as argued above. Hence, this problem is universal and not a specifically Gaussian one.

As a consequence, we have to carefully incorporate the influence of estimating global parameters in multiscale scanning in such a way that we allow for a large as possible range of scales. This is the starting point for the subsequent invariance principle in Section \ref{subsec:theory}, which is at the heart of AMS. Afterwards, in Section \ref{subsec:microscopy}, we illustrate a different scenario - a base signal, which is not known beforehand, has to be incorporated into the multiscale statistic $T_n$ and hence needs to be estimated from the data.

\subsection{Theory: A uniform invariance principle}\label{subsec:theory}

Whereas in Section \ref{subsec:gauss} the failure of the unadjusted multiscale statistic results from estimating a nuisance parameter, in the subsequent example (Section \ref{subsec:microscopy}) we will face difficulties arising from estimation of the Poisson baseline $\lambda_0$. Both issues will be related with the main technical result of this paper, a \textit{uniform} invariance principle for estimated base intensity and nuisance parameters. 

%The rationale behind approximating $T_n$ by $M_n$ is, that for sufficiently large scales, a CLT implies that $T_R \approx \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right|$ in distribution. However, to ensure that $M_n$ in fact approximates $T_n$, this CLT has to hold true uniformly over all sets in $\mathcal R_n$, i.e. is required to couple the process $\left(T_R\right)_{R \in \mathcal R_n}$ with $\left(\left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i\right|\right)_{R \in \mathcal R_n}$ and to account for the calibration. Based on classical results \citep{kmt75}, this approach has also been used in \citet{fms14}, and more recently been extended in \citet{pwm18,kmw20} by using explicit couplings for the suprema of processes \citet{cck14}.
%
%Furthermore, for not too large scales, the difference of $T_n$ and $\hat T_n$ is hopefully small, and thus an approximation of $\hat T_n$ by $M_n$ seems reasonable.
%
%On the other hand, without such scale restrictions, an approximation seems unfeasible for general scale calibrations. Therefore, we introduce a restricted set of candidate regions
%\[
%\mathcal R_n (c,d) := \left\{R \in \mathcal R_n ~\big|~ c \leq \left|R\right| \leq d\right\}.
%\]
%with numbers $c,d>0$, and take two sequences $\left(r_n\right)_{n \in \N} \subset \mathbb N, \left(m_n\right)_{n \in \N} \subset \N$ tending to $\infty$.
%
%Then the main result of this paper can be stated as follows (see Theorem \ref{thm:approximation_estimated_intensity} for the detailed formulation). Suppose that the model \eqref{eq:model} satisfies some mild differentiability properties (see Assumption \ref{ass:model} for details). If $(r_n)_n\subset(0,\infty)$ and $\left(m_n\right)_n \subset \left(0,\infty\right)$ are sequences that
%\[
%\log^\gamma\left(n\right) = o \left(r_n\right)\qquad\text{and}\qquad m_n = o \left(s_n\right),
%\]
%where $\gamma > 0$ is an explicitly known exponent depending on $\omega$ and $\widetilde{\omega}$ and $s_n$ is the consistency rate of the $(\hat{\theta}_n)_{n \in \N}$ and $(\hat{\xi})_{n \in\N}$, then 
%\begin{equation}\label{eq:convergence}
%	T_n \left(Y, \mathcal R_n\left(r_n,m_n\right), \hat \theta_n, \hat \xi_n\right) - M_n \left(\mathcal R_n\left(r_n,m_n\right)\right) \toP 0,
%\end{equation}
%as $n \to \infty$, and the rate of convergence in this result can even be made precise.

The rationale behind approximating $T_n$ in \eqref{eq:Tn} by $M_n$ in \eqref{eq:Mn} is, that for sufficiently large scales, a CLT implies that $T_R \approx \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right|$ in distribution. However, to ensure that $M_n$ in fact approximates $T_n$, this CLT has to hold true uniformly over all sets in $\mathcal R_n$, i.e. it is required to couple the process $\left(T_R\right)_{R \in \mathcal R_n}$ with $\left(\left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i\right|\right)_{R \in \mathcal R_n}$ and to account for the calibration. Based on classical results \citep{kmt75}, this approach has also been used in \citet{fms14}, and more recently been extended in \citet{pwm18,kmw20} by using explicit couplings for the suprema of empirical processes developed by \citet{cck14}. Furthermore, for not too large scales, the difference of $T_n$ and $\hat T_n$ will be shown to be small, and thus an approximation of $\hat T_n$ by $M_n$ seems reasonable. In fact, if we do not exclude too large scales in $T_n$, the Gaussian approximation becomes unfeasible in contrast to the case with known baseline intensity and known nuisance parameter. 

In the following, we will briefly describe a special version of our main result, for the general result see Theorem \ref{thm:approximation_estimated_intensity}. Suppose for simplicity that the estimators $(\hat{\theta}_n)_{n \in \N}$ and $(\hat{\xi})_{n \in\N}$ are consistent with the parametric rate $n^{-d/2}$. Furthermore let $\gamma > 0$ an explicitly known exponent depending on $\omega$ and $\widetilde{\omega}$, and let $(r_n)_n\subset(0,\infty)$ and $\left(m_n\right)_n \subset \left(0,\infty\right)$ be sequences such that
\[
\log^\gamma\left(n\right) = o \left(r_n\right)\qquad\text{and}\qquad m_n = o \left(n^{-d/2}\right),
\]
as $n \to \infty$. Then, if model \eqref{eq:model} satisfies some mild differentiability properties (see Assumption \ref{ass:model} for details), the distribution of $T_n\left(Y, \mathcal R_n, \hat \theta_n, \hat \xi_n\right)$ can be approximated by that of $M_n \left(\mathcal R_n\right)$ in probability as long as $\mathcal R_n$ contains only scales of size between $r_n$ and $m_n$. In other words, we obtain a uniform invariance principle on all not too small (poly-logarithmic in the sample size) and all scales just below the largest ones of order $n^{-d/2}$. It is obvious that this condition is necessary to distinguish the base line signal from local deviations, hence unavoidable in our context. 

If, for a given system of candidate sets $\mathcal R_n$, we introduce the notation
\[
\mathcal R_n (c,d) := \left\{R \in \mathcal R_n ~\big|~ c \leq \left|R\right| \leq d\right\},
\]
then the above result can be formulated as 
\begin{equation}\label{eq:convergence}
	T_n \left(Y, \mathcal R_n\left(r_n,m_n\right), \hat \theta_n, \hat \xi_n\right) - M_n \left(\mathcal R_n\left(r_n,m_n\right)\right) \toP 0,
\end{equation}
as $n \to \infty$. The rate of convergence of this approximation will be made precise in Theorem \ref{thm:approximation_estimated_intensity}, which is (up to log factors) $\mathcal O_P \left(n^{-d/2} \frac{\sqrt{m_n}}{r_m} + r_n^{1/8}\right)$.

Note, that \eqref{eq:convergence} might be trivial as both $T_n$ and $M_n$ just tend to $0$. However, our result holds true for many different calibrations, including the previously mentioned one due to \citet{dw08}, which ensures that $M_n$ converges to a non-degenerate continuous limit (and hence, due to \eqref{eq:convergence}, so does $T_n$). From our result, similar results can be derived for the scale calibrations by \citet{sac16} and \citet{pwm18}, which ensure that $M_n$ converges to a Gumbel distribution.

%We emphasize that $\log^\gamma\left(n\right) = o \left(r_n\right)$ as $n \to \infty$ allows for a (relative to the total number of samples $n^d$) smallest scale size close the sampling rate $1/n$. This can - to the best of our knowledge - not be achieved with classical coupling techniques such as KMT-type constructions \citep[see e.g.][]{kmt75,r93}, but only with recent coupling results for the suprema of processes by \citet{cck14} as exploited in \citet{kmw20,pwm18} in the context of multiscale scanning without nuisance parameters. As mentioned before, we also have to remove the largest scales due to the estimation of $\theta_0, \xi$ in \eqref{eq:Tn}, as our assumptions allow for scale calibrations such that these scales do in fact contribute to $M_n \left(\mathcal R_n\right)$ substantially \citep[see e.g.][]{ds01}.

We illustrate \eqref{eq:convergence} in the Gaussian setting already discussed in Subsection \ref{subsec:gauss}. Figure \ref{fig:gauss_dist_limited} shows the empirical distribution of $T_n \left(Y, \mathcal R_n\left(r_n,m_n\right), \hat \theta_n, \hat \xi_n\right)$ and $M_n \left(\mathcal R_n\left(r_n,m_n\right)\right)$ with the same $n$ and $\tilde{\omega}, \omega$ as there for  $r_{128} = 4$ and $m_{128} = 64$.

% Figure environment removed

We find the distributional fit to be remarkably accurate in all situations, which supports the applicability of the asymptotic result from Theorem \ref{thm:approximation_estimated_intensity}. The distribution of $M_n$ itself will be shown in Figure \ref{fig:invariance_dist} for the situation considered in our simulations in Section \ref{sec:simulations}.
%
%The computation of $M_n$ and $T_n$ can in principle be carried out under the same way whenever $T_R$ in \eqref{eq:TR} is given by a function of the local mean $\bar Y_R := \sum_{i \in R} Y_i$. In this case, if there is a global shape $B \subset I_n^d$ such that each $R \in \mathcal R_n$ is given as a rescaled and translated version of $B$, i.e. if there exist $t,h \in I_n^d$ with $t_i + h_i \leq n$ for all $1 \leq i \leq d$ such that $\mathbf 1_{R} \left(x\right) = \mathbf 1_B \left(\left(x-t\right)/h\right)$ with $\mathbf 1$ denoting the indicator function, fast computation is possible. Employing the fast Fourier transform, this structure allows for evaluation of $M_n$ in \eqref{eq:Mn} and $\left(T_R\right)_{R \in \mathcal R_n}$ in $\mathcal O \left(d N n^d \log \left(n\right) \right)$ operations as discussed in \cite{kmw20}, where $N$ is the number of scales, i.e. the number of different sizes of rectangles. This will be detailed in Section \ref{sec:implementation}. We provide MATLAB\copyright code for the corresponding algorithms under \todo[inline]{Insert link to code}. 
%
%In total, the above result allows hence (under the posed assumptions) for a computationally feasible scanning approach for the multiple hypothesis testing problem \eqref{eq:multiple_testing}. Given the simulated quantile $q_{1-\alpha}$ of $M_n$ in \eqref{eq:Mn}, each of the local test statistics $T_R$ in \eqref{eq:TR}, $R \in \mathcal R_n$ is compared to $\frac{q_{1-\alpha}}{\widetilde{\omega}(|R|,n)} + \omega(|R|,n)$. Whenever this threshold is exceeded, we can conclude that $K_{R,n}$ holds true with uniform (FWER controlled) probability $\geq 1-\alpha$. 

%\subsection{Literature review}

%Let us also briefly mention other approaches to the multiplicity issue raised above. Besides controlling the FWER by using a scan statistic, one can also apply a weaker criterion and control the false discovery rate (FDR) by the method proposed in \cite{bh95,by01}, see e.g. \cite{zyxs16}. The FWER criterion can also be ensured by the higher criticism approach proposed in \cite{dj04}, see \cite{kw22} for an application of this to scan statistics.

%\begin{itemize}
%	\item \cite{ds18} Multivariate version of \cite{ds01}
%	\item \cite{gs81}: Replacing the nuisance parameter by its estimate can reduce its impact and offer a simple inference procedure was originally proposed by them.
%\item \cite{k97}
%seen as the fundamental article about spatial scan statistics, \\
%test procedure uses a likelihood ratio test to identify clusters ( critical value via simulations )

%\item \cite{a97}
%one- and two-dimensional homogeneous poisson process, fixed scanning set that gets translated over a rectangular area: rectangular scanning sets and general convex scanning sets like circular and triangular ones $\rightarrow$ approximation of the distribution of the scan statistic
%\item \cite{a98} 
%homogeneous Poisson process, one and two- dimensional repeated and generalized to three-dimensional scan statistic with fixed rectangular scanning set ( remark that it should be possible to generalize this as well)
%\item \cite{flr11}
%test homogeneity of a PP observed on a finite interval
%\item \cite{t13}
%ratchet scan statistic: scanning window is a grid set instead of rectangular box; asymptotic distribution of this statistic; connection between ratchet scan statistic and scan statistic (over rectangular box): ratchet effect can be factored out by a function $\rightarrow$ tail probability converges to that of the scan statistic
%\item \cite{sac16}
%measurement is a signal vector with additive white Gaussian noise, testing for nonzero signal, discrete hyperrectangles, Z-score ( $y[R]\hat{=} \bar{Y}_R$), 
%data is i.i.d. standard normal when no anomaly is present and an anomaly comes in form of a rectangle with increased mean, adaptive scan ( rectangles with shape $h$, which forms an $\epsilon-$net), oracle scan ( knows the shape $h^*$)
%\item \cite{fs16}
%one dimensional, general setting of EF, log likelihood ratio statistic, two scan statistics: one with fixed window size and one with varying one; convergence rates for tail probabilities of these statistics; proof relies on Stein's method
%\item \cite{zyxs16}
%scan statistic for Poisson-type data, $1$-dimensional, likelihood-based framework, FDR-formulas and power
%\item \cite{accd11} minimax detectio thresholds for detecting clustered anomalies in a field or network; calibrated scan statistic is optimal for this case
%\item \cite{cn14}: event detection in social media, non-parametric heterogeneous graph scan
%\item \cite{s18}: anomalous patterns in images; more theoretical (chaining)
%\item \cite{cmv02}: detection in imaging; method to detect man-made objects in images
%\item \cite{rdj14}: genetics, gene-wise scan statistic
%\item \cite{n12}: uncalibrated scan over a subset of all regions
%\item \cite{srs16}: graph fourier scan statistic
%\item \cite{mbs13} Testing for intervals where a regression function exceeds a baseline level, based on p-value scanning and kernel estimation
%\item \cite{cw13} penalized scan (our penalization) is superior to unpenalized scan, scanning system can be approximated by dyadic system for computational efficiency.
%\item \cite{kw22} uses higher criticism to derive the detection boundary for clustered signals (blocks)
%\item \cite{dj04} introduces higher criticism to perform multiplicity adjustment, i.e. maximizing a Z-score (LRT statistic) over a range of significance levels using p-values.
%\item 
%\cite{dw08} Calibrated (our calibration) scan statistic for detecting increases or decreases in a density
%\item \cite{pywr22} uncalibrated scan statistic based on kernel density estimators, scale is either pre-described or bounded from below
%\item \cite{rw13} penalized LRT-scan statistic (like ours) to infer on the intensity of a Poisson process
%\item \cite{w10} bernoulli scanning, scale dependent critical values. 2-dimensional Bernoulli-model (log-likelihood ratio statistic); first time size-dependent critical values to control the domination of the small scales (instead of penalty-term); approximation of the set of all rectangles to get an almost linear algorithm
%\end{itemize}


\subsection{An application from super-resolution fluorescence microscopy}\label{subsec:microscopy}

A major motivation for our work comes from super-resolution fluorescence microscopy imaging, and we refer to Section \ref{sec:application} for details. Fluorescence microscopy is employed to visualize structures of interest (e.g. proteins or protein complexes) in a sample, which are labelled by fluorescent markers and afterwards imaged by the microscope using visible light. More precisely, the specimen is scanned spatially by applying an excitation laser pulse focused at the current grid point and measuring the resulting fluorescence afterwards. The later excitation-emission-recording procedure is repeated several (say $t \in\N$) times, called the measurement time, before the focus is moved to the next grid point. Mathematically, due to the nature of photon counting, this leads to a Poisson field of independent observations \citep[see e.g.][]{msw20}, i.e. $Y_i \sim \text{Poi} \left(t\lambda_i\right)$, where $\lambda_i \geq 0$ denotes the underlying photon intensity at grid point $i$. As fluorescent markers are known to bleach out after a certain number of excitation cycles $T$, one is interested in localizing interesting structures with in a short pre-scan (i.e. with short measurement time $t \ll T$), which then will be detailed investigated in a subsequent scan. This has achieved great interest recently in many applications and can -- in the present context -- be seen as an instance of \emph{smart} microscopy, see \citet{s20}. Some heuristic methods have been suggested \citep[such as RESCue and others, see][]{vge20}, which stop measuring as soon as a predefined number of photons has been detected, but without any statistical guarantees. Another approach exploits machine learning to pre-select regions of interest based on shortly measured (i.e. very noisy) data, see e.g. \citet{pwsg19,mszgwm22}. As we will demonstrate, AMS does provide a methodology that allows to identify interesting regions with prescribed statistical error.

An illustrative example is visualized in Figure \ref{fig:fibro}. Sub-panel (a) shows raw data (photon counts) of tubulin in neonatal fibroblast, imaged by a STED microscope \citep[see e.g.][]{h07} using a measurement time of $t = 25 \mu \mathrm{s}$ (pre-scan). Clearly visible, the image is corrupted by background noise (resulting from other contributions such as out-of-focus markers or external photon sources), which also can be modelled as Poissonian random variables, see e.g. \citet{aem15,msw20}. Here, the background intensity $\lambda_0 > 0$ varies between experiments and has to be estimated from the data. Therefore, we aim to detect active regions inside the field of view based on the noisy $t = 25 \mu \mathrm{s}$ data. To achieve this, we estimate $\lambda_0$ by $\hat \lambda_0 = \bar Y_{I_n^d} = \frac{1}{n^d} \sum_{i \in I_n^d} Y_i$ and apply the multiscale scanning procedure described above. As we are only interested in active regions, we consider the one-sided testing problem to find regions $R$ where $\lambda_i > \lambda_0$, which can be achieved by restricting the corresponding local rejections $R$ with $\bar Y_R = \frac{1}{|R|} \sum_{i \in R} Y_i > \hat \lambda_0$ (cf. Remark \ref{rem:one_sided} below). 
% In principle, it is possible to adjust the procedure described in Section \ref{subsec:scanning} to one-sided testing problems, but this requires a different local test statistic than \eqref{eq:TR} and hence a completely different asymptotic theory.

By applying our invariance principle, we can simulate the threshold $\eta$ from an asymptotically distribution-free Gaussian approximation $M_n$ of the test statistic (see \eqref{eq:Mn}, with the corresponding adjustment that $\bar X_R > 0$, cf. Remark \ref{rem:one_sided}). Our main Theorem \ref{thm:approximation_estimated_intensity} shows that this asymptotically controls the FWER at level $\alpha$. Precisely, we choose $\eta$ as the $0.9$-quantile $q_{0.9}^{M_n}$ of $M_n$. Here, $n = 400$, $d = 2$, and we choose $\mathcal R_n$ to be the subset of all rectangles in $I_n^d$ with scale between $4$ and $10$. The result is shown in sub-panel (b) of Figure \ref{fig:fibro} as a \textit{significance map}, depicting all significant rectangles with colour indicating their size in square-nanometers $\mathrm{nm}^2$. If two rectangles overlap, the colour is chosen according to the smaller one, which leads to a map indicating the smallest scale of significance. The major result of this paper implies that the AMS method ensures the FWER control in \eqref{eq:FWER}, and hence each coloured region contains an anomaly with uniform probability $\geq 0.9$, i.e. the probability that any of these detections is false is less then $0.1$.

Returning to the previous data set, sub-panel (c) of Figure \ref{fig:fibro} shows a $T = 100 \mu \mathrm{s}$ measurement of the same specimen (full scan), which can be regarded as ground truth. Within this image, we have marked the regions (precisely the union of all significant rectangles found based on the $t = 25 \mu \mathrm{s}$ data om sub-panel (b)) in red. We find that the method did not yield any false positives despite the relatively high noise-level in the $t = 25 \mu \mathrm{s}$ measurement.

% Figure environment removed

We find that even on $25 \mu \mathrm{s}$ data, the methodology allows to detect nearly all filamentary structures. The comparison with the full $T = 100 \mu \mathrm{s}$ ground truth data proves that all significant regions belong to fibroblasts.

Furthermore, the AMS methodology can also be applied to the full $T = 100 \mu \mathrm{s}$ data set to obtain a statistically sound segmentation of the image e.g. into active and inactive regions, see Figure \ref{fig:fibro2}. Again, the probability of a false detection is $\leq 0.1$.

% Figure environment removed

We also compare the resulting significance maps for different values of $\alpha$, see Figure \ref{fig:fibro3}. It turns out that the AMS methodology is very stable w.r.t. the significance level $\alpha$.

% Figure environment removed

\section{Assumptions and detailed theory}\label{sec:theory}

In this section we will now discuss our assumptions and the result \eqref{eq:convergence} in detail. Furthermore, we will examine several examples to demonstrate the validity of our assumptions in realistic scenarios. 

To formulate our invariance principle, we will make use of stochastic $\mathcal O$-notation \citep[see Sect. 2.2 in][]{vdV98}: For a sequence $\left(X_n\right)_{n \in \N}$ of real-valued random variables we write $X_n = \OP \left(1\right)$ iff $\left(X_n\right)_{n \in \N}$ is tight, that is for all $\varepsilon > 0$ there exists $M > 0$ such that
\[
\sup_{n \in \N} \P{0}{\left|X_n\right| >M} <\varepsilon.
\]
We write $X_n = \OP \left(a_n\right)$ with a sequence $\left(a_n\right)_{n\in\N}$ of real numbers, iff $X_n / a_n = \OP(1)$. 

We start with our assumptions on the distributions $F_{\theta, \xi}$ in model \eqref{eq:model}. For notional simplicity we introduce
\begin{equation}\label{eq:mean_variance}
m(\theta, \xi) :=  \E{\theta,\xi}{Y}\qquad\text{and}\qquad  v(\theta, \xi):= \V{\theta,\xi}{Y}.
\end{equation}
Here and in what follows, $Y$ denotes a generic random variable with distribution $Y \sim F_{\theta,\xi}$ according to the parameters mentioned in the subscript.
\begin{assumption}[Model assumptions]\label{ass:model}
	Let $Y \sim F_{\theta, \xi}$, $\left(\theta,\xi\right) \in \Theta \times \Xi \subseteq \R^{d_1}\times \R^{d_2}$ with $d_1, d_2  \in \mathbb N$. For $\theta_0 \in \Theta, \xi \in \Xi$ assume $U \times V \subset \Theta \times \Xi$ an open neighborhood of $\left(\theta_0, \xi\right)$.
\begin{enumerate}
\item[(a)] \textbf{Uniformly sub-exponential tails:} There exist constants $c_1 > 1, c_2 > 0$ such that
\begin{align*}
\sup_{\left(\theta, \xi\right)\in U \times V}\P{\theta_0,\xi}{|Y|>t} \leq c_1 \exp\left(- c_2 t\right) \qquad\text{for all}\qquad t > 0.
\end{align*}
\item[(b)] \textbf{Moments exist locally:} The functions $m$ and $v$ in \eqref{eq:mean_variance} exist and are finite on $U \times V$ and there exists a constant $c_v > 0$ such that $v > c_v$ on $U \times V$.
\item[(c)] \textbf{Bounded derivatives of $m$:} There exists a constant $C_m < \infty$ such that \linebreak $\|\nabla_\theta m \|_2, \| \nabla_\xi m \|_2\leq C_m$ on $U \times V$.
\item[(d)] \textbf{Bounded and non-vanishing derivatives of $v$:} There exist constants $0 < c_v < C_v < \infty$ such that $c_v \leq \|\nabla_\theta v\|_2, \|\nabla_\xi v\|_2 \leq C_v$ on $U \times V$.
\item[(e)] \textbf{Taylor approximation:} There exists a constant $C_T < \infty$ such that the likelihood ratio statistic $T_R$ in \eqref{eq:TR} can be approximated by local means $\bar{Y}_R = |R|^{-1}\sum\limits_{i \in R}Y_i$ in the sense that
\begin{equation}\label{eq:lrt_approx}
\left| T_R^2 \left(Y,\theta, \xi\right) - |R|\left(\frac{\bar{Y}_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right)^2 \right|  \leq C_T |R|\left( \frac{\left|\bar{Y}_R - m\left(\theta, \xi\right)\right|}{\sqrt{v\left(\theta, \xi\right)}}\right)^3 \left(1+ o_{\mathbb P_0} \left(1\right)\right)
\end{equation}
holds true for all $R \in \mathcal R$ and $\left(\theta, \xi\right) \in U \times V$.
\end{enumerate}
\end{assumption}
We briefly comment on these assumptions.
\begin{remark}
\begin{enumerate}
\item Assumption \ref{ass:model}(a) allows us to control moments of $Y_i$ and hence of local means $\bar Y_R := \sum_{i \in R} Y_i$ simultaneously over regions $R \in \mathcal R_n$. %, so that we can finally employ a coupling result proven in \citet{kmw20}.
\item Assumption \ref{ass:model}(b) seems natural, as otherwise no CLT on the small scales can hold true and hence no distributional limit is to be expected.
\item Assumptions \ref{ass:model}(c) and (d) will be used to control the impact caused by replacing $\theta_0$ and $\xi$ in \eqref{eq:Tn} by the estimators $\hat \theta_n$ and $\hat \xi_n$, respectively.
\item Assumption \ref{ass:model}(e) is required to link $T_R$ to the local means $\bar Y_R$ sufficiently well.
\end{enumerate}
\end{remark}

For a better understanding of Assumption \ref{ass:model} let us discuss some examples:
\begin{example}[Complete exponential family model]
Suppose that 
\[
\left\{F_{\theta, \xi}\right\}_{\left(\theta,\xi\right) \in \Theta \times \Xi}
\]
is a natural exponential family with natural parameter space $\Theta \times \Xi$. In this case, each $Y_i$ has sub-exponential tails (which implies Assumption \ref{ass:model}(a)) and the differentiability properties from Assumption \ref{ass:model}(c)--(e) hold true \citep[see e.g.][]{b86}. 

This includes both the homogeneous and the heterogeneous Gaussian model as well as the Poisson model, which we employ for our data example in Section \ref{subsec:microscopy} on fluorescence microscopy.
\end{example}

\begin{example}[Partial exponential family model]	
Suppose that for each fixed value of $\xi \in \Xi$, the collection $\left\{F_{\theta, \xi}\right\}_{\theta \in \Theta}$ is a natural exponential family. In this case, Assumption \ref{ass:model}(a) is again satisfied due to standard theory on exponential families, but the differentiability properties in Assumption \ref{ass:model}(c)--(e) have to be verified case by case. As one particular example we consider the 
Gamma distribution $\Gamma(\alpha, \beta)$ with shape parameter $\alpha>0$ and rate $\beta>0$ given by
\begin{align*}
f(x)&= \frac{1}{\Gamma(\alpha)}  \beta^\alpha x^{\alpha-1} \exp \left( -\beta x \right).
%\E{}{Y}&=\frac{\alpha}{\beta}\\
%\V{Y}&= \frac{\alpha}{\beta^2}
\end{align*}
If the shape parameter $\alpha$ is a nuisance parameter, we obtain $m(\beta, \alpha)= \alpha/\beta$, $v(\beta, \alpha)= \alpha/\beta^2$ and
\begin{align*}
\frac{\partial}{\partial \beta} m(\beta, \alpha) = - \frac{\alpha}{\beta^2},  \qquad \frac{\partial}{\partial \beta} v(\beta, \alpha) = - \frac{2\alpha}{\beta^3}, \qquad \frac{\partial}{\partial \alpha} m(\beta, \alpha) = \frac{1}{\beta}, \qquad \frac{\partial}{\partial \alpha} v(\beta, \alpha) = \frac{1}{\beta^2},
\end{align*}
which proves the boundedness conditions in Assumption \ref{ass:model}(c) and (d). Furthermore, \eqref{eq:lrt_approx} follows directly from the exponential family structure \citep[see e.g.][]{kmw20}. 
\end{example}

We also give a counterexample to show the limitations of our Assumptions:
\begin{example}[Weibull distribution]
Consider the Weibull distribution with shape parameter $k>0$ and rate parameter $\lambda>0$. If $k\geq 1$, then all random variables $Y_i$ will have sub-exponential tails \citep[see e.g.][]{s85,fb97}, but Assumption \ref{ass:model}(e) is not satisfied. Therefore, we compute 
\begin{align*}
	\log \left(\frac{\sup\limits_{\theta \in \Theta} \prod_{i \in R} f_{(\theta, \xi)}(Y_i)}{\prod_{i \in R} f_{(\theta_0, \xi)} (Y_i)} \right)&= k \log(\lambda_0) - \log \left( \sum\limits_{i \in R} Y_i^k\right) + \frac{\sum\limits_{i \in R} Y_i^k}{\lambda_0^k} -1\\
	|R|\left(\frac{\bar{Y}_R - m(\theta_0, \xi)}{\sqrt{v(\theta_0, \xi)}}\right)^2 &= |R| \left( \frac{\bar{Y}_R^2 - 2 \lambda \Gamma\left(1+ \frac{1}{k} \right)\bar{Y}_R +\lambda^2 \left(\Gamma\left(1+ \frac{1}{k} \right)\right)^2}{\lambda^2 \left[\Gamma\left(1+ \frac{2}{k} \right) - \left(\Gamma\left(1+ \frac{1}{k} \right)\right)^2 \right]}\right),
\end{align*}
which shows that \eqref{eq:lrt_approx} cannot hold true.
\end{example}

Next we continue with our assumptions on the multiscale scanning procedure described in terms of $T_n$ in \eqref{eq:Tn}. To approximate $T_n$ by $M_n$ in \eqref{eq:Mn}, we first have to restrict the cardinality of $\mathcal R_n$.
\begin{assumption}[Polynomial growth of $\mathcal R_n$]\label{ass:R}~
There exists constants $c_1, c_2>0$ such that
\begin{equation}\label{eq:finite_cardinality}
\#\left(\mathcal R_n \right) \leq c_1 n^{c_2}.
\end{equation}
where $\#$ denotes the number of elements.
\end{assumption}
As discussed in \citet{kmw20}, Assumption \ref{ass:R} is rather mild and allows e.g. for $\mathcal R_n$ being the set of all hyperrectangles, hypercubes or half-spaces in $\left[0,1\right]^d$.

Our next assumption is on the scale calibration $\widetilde{\omega}, \omega$ in \eqref{eq:Tn}:
\begin{assumption}\label{ass:Omega}
Suppose $\omega,\widetilde{\omega}: \left(0,\infty\right) \times \mathbb N \to \left(0,\infty\right)$ have the following properties:
\begin{enumerate}
\item For each $n \in \mathbb N$, $\omega \left(\cdot, n\right)$ and $\widetilde \omega \left(\cdot,n\right)$ are decreasing.
\item There exist $C_{\omega} > 0$, $\alpha, \widetilde{\alpha}>0$ and $\beta, \widetilde{\beta} \in \R$ such that
\begin{align*}
\omega(r,n) & \leq C_\omega\left(\log \frac{n^d}{r} \right)^\alpha, \qquad \left|\omega'(r,n) \right|  \leq C_\omega\left(\log \frac{n^d}{r} \right)^\beta \frac{1}{r},\\
\widetilde{\omega}(r,n) & \leq C_\omega\left(\log \frac{n^d}{r} \right)^{\widetilde{\alpha}}, \qquad
\left|\widetilde{\omega}'(r,n) \right| \leq C_\omega\left(\log \frac{n^d}{r} \right)^{\widetilde{\beta}} \frac{1}{r}
\end{align*}
for all $r > 0$ and $n \in \N$, where derivatives $\omega'$ and $\widetilde{\omega}'$ are always taken w.r.t. the first argument.
\end{enumerate}
\end{assumption}
Let us briefly discuss this assumption for common choices considered in the literature:
\begin{example}
\begin{enumerate}
\item \citet{dw08,cw13,rw13} consider
\begin{align}
\widetilde{\omega}\equiv 1, \qquad 
\omega(|R|,n)= \sqrt{2 \nu (\log(n^d/|R|))+1},\label{pen_ds01}
\end{align}
where $\nu  \geq 1$ depends on the complexity of the candidate region. These terms fulfil Assumption \ref{ass:Omega} with $\alpha=\frac{1}{2}, \beta=-\frac{1}{2}$ and $\widetilde{\alpha}= \widetilde{\beta}=0.$
\item \citet{sac16} consider
\begin{align}
\widetilde{\omega}& = \sqrt{2(\log(n^d/|R|))},\nonumber\\
\omega(|R|,n)&= \sqrt{2(\log(n^d/|R|))} + \frac{\left(4d-1\right) \log \left(\sqrt{2(\log(n^d/|R|))}\right) - \log\left(\sqrt{2\pi}\right)}{\sqrt{2(\log(n^d/|R|))}} ,\label{pen_sac}
\end{align}
and therefore Assumption \ref{ass:Omega} is fulfilled with $\alpha = \widetilde{\alpha} = \frac12$ and $\beta = \widetilde{\beta} = - \frac12$.
\item \citet{pwm18} consider 
\begin{equation}\label{pen_pwm}
\omega(|R|,n) := \widetilde{\omega}(|R|,n):= \sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)} + C_d \frac{\log \left(\sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)} \right)}{\sqrt{2 \log \left( \frac{Cn^d}{|R|}\right)}},
\end{equation}
with $C>1$ and $C_d$ depending on the dimension and the system of considered scales.
These fulfil Assumption \ref{ass:Omega} with $\alpha= \widetilde{\alpha}= \frac{1}{2}$ and $\beta= \widetilde{\beta}= -\frac{1}{2}.$
\end{enumerate}
\end{example}

Now we are in position to state our uniform invariance principle in detail. Therefore let $(\hat{\theta}_n)_{n \in \N}$ and $(\hat{\xi})_{n \in\N}$ be sequences of estimators for $\theta_0$ and $\xi$ based on $Y$ such that they are $s_n$ (e.g. $s_n = n^{-d/2}$) consistent under the global hypothesis $H_{I_n^d,n}$ that there is no anomaly, i.e. 
\begin{equation}\label{eq:estimators}
	\|\hat{\theta}_n - \theta_0\|_2 = \OP \left(s_n^{-1}\right)\qquad\text{and}\qquad \|\hat{\xi}_n - \xi\|_2 = \OP \left(s_n^{-1}\right)
\end{equation}
with $\mathbb P_0 := \mathbb P_{H_{I_n^d},n}$ where $\left(s_n\right)_{n \in \N} \subset \R$ is a monotone sequence tending to $\infty$. Note, that the MLEs for $\theta_0$ and $\xi$ satisfy \eqref{eq:estimators} under Assumption \ref{ass:model} with $s_n = n^{d/2}$. Furthermore let 
\[
\gamma= 12+ 6 \widetilde{\alpha}+ 2 \max\left\{\frac12, \alpha, \widetilde{\alpha}\right\} + 2 \max\left\{\beta, \widetilde{\beta},0\right\}
\]
with the parameters $\alpha, \widetilde{\alpha}, \beta, \widetilde{\beta}$ from Assumption \ref{ass:Omega} and suppose that
\begin{align}
\log^{\gamma}(n) = o\left(r_n\right) \text{ as }n \to \infty\label{eq:r_n},\qquad m_n =  o \left(s_n\right)  \text{ as }n \to \infty.
\end{align}
In this situation we can prove the following Gaussian approximation. 

\begin{theorem}[Gaussian approximation]\label{thm:approximation_estimated_intensity}
Let $\mathcal R_n$ be a set of candidate regions and let $\omega, \widetilde \omega$ be scale calibrations. Grant Assumptions \ref{ass:model}, \ref{ass:R} and \ref{ass:Omega}. Furthermore let $(r_n)_n\subset(0,\infty)$ and $\left(m_n\right)_n \subset \left(0,\infty\right)$ be sequences such that \eqref{eq:r_n} holds true and let $(\hat{\xi}_n)_{n\in\N}$ and $(\hat{\theta}_n)_{n \in\N}$ are sequences of estimators such that \eqref{eq:estimators} is valid. 

Then, for a field $Y = \left(Y_i\right)_{i \in I_n^d}$ with i.i.d. random variables $Y_i \sim F_{\theta_0, \xi}$, it holds
%\begin{align}\label{eq:approx_in_prob}
%\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right)
%\end{align}
\begin{align}\label{eq:approx_in_prob}
	\left|T_n \left(Y,  \mathcal R_n\left(r_n,m_n\right)\hat{\theta}, \hat{\xi} \right) - M_n\left(\mathcal R_n\left(r_n,m_n\right)\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \sqrt{\frac{m_n}{s_n}}+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right)
\end{align}
as $n \to \infty$.
\end{theorem}

\begin{cor}
Let the assumptions of Theorem \ref{thm:approximation_estimated_intensity} hold true, $Y = \left(Y_i\right)_{i \in I_n^d}$ as in \eqref{eq:model} (independent, but not necessarily identically distributed), and $\alpha \in \left[0,1\right]$. If we define
\[
\mathcal R_n^* := \left\{R \in \mathcal R_n ~\big|~ r_m \leq |R| \leq m_n, T_R \left(Y, \hat \theta_n, \hat \xi_n\right) \geq \frac{q_{1-\alpha}}{\widetilde{\omega} \left(\left|R\right|, n\right)} + \omega \left(\left|R\right|, n\right)\right\}
\]
with the $\left(1-\alpha\right)$-quantile $q_{1-\alpha}$ of $M_n$ as in \eqref{eq:Mn}, then \eqref{eq:FWER} holds true, i.e.
\[
\P{}{\forall~R \in \mathcal R_n^* ~\exists~i \in R \text{ s.t. } \theta_i \neq \theta_0} \geq 1-\alpha + o(1) \qquad\text{as}\qquad n \to \infty.
\]
\end{cor}

\begin{remark}\label{rem:one_sided}
In the situation that all local alternatives are one-sided, i.e. $\theta_i > \theta_0$, the above result can readily be generalized by setting
\[
\tilde T_R \left(Y,\hat \theta_n, \hat \xi_n\right) = \begin{cases} T_R \left(Y, \hat \theta_n,\hat\xi_n\right) & \text{if }\bar Y_R > \hat \theta_n, \\ 0 & \text{else},\end{cases}
\]
as well as
\[
\tilde T_n \left(Y,\mathcal R_n,\hat \theta_n, \hat \xi_n\right)  := \max_{R\in\mathcal R_n} \widetilde{\omega}\left(\left|R\right|,n\right)\left[ \tilde T_R\left(Y, \theta_0, \xi\right)-\omega\left(\left|R\right|,n\right)\right]
\]
and
\[
 \tilde M_n\left(\mathcal R_n\right) := \max_{R\in\mathcal R_n, \bar X_R > 0} \widetilde{\omega}(|R|,n)\left[ \left|R\right|^{-1/2} \left| \sum\limits_{i \in R} X_i \right| - \omega(|R|,n) \right].
\]
With this notation, it can readily be seen that Theorem \ref{thm:approximation_estimated_intensity} holds also true for those quantities, i.e. under the same conditions we obtain
\[
	\left|\tilde T_n \left(Y,  \mathcal R_n\left(r_n,m_n\right), \hat{\theta}, \hat{\xi} \right) - \tilde M_n\left(\mathcal R_n\left(r_n,m_n\right)\right) \right| = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \sqrt{\frac{m_n}{s_n}}+\left(\frac{\log^\gamma\left(n\right)}{r_n}\right)^{\frac18}\right)
\]
as $n \to \infty$.
\end{remark}

\begin{remark}
As a different approach to the invariance principle in Theorem \ref{thm:approximation_estimated_intensity}, it would be particularly elegant to exploit asymptotic equivalence in the Le Cam sense \citep[see e.g.][]{gn98,rsh18} after a variance stabilizing transformation to a Gaussian model. However, to the best of our knowledge, such results will only be valid for a fixed number of scales. This alternative approach would therefore rise interesting questions on how to extend asymptotic equivalence to multiscale frameworks with growing number of tests.
\end{remark}

\section{Simulations and real data example}\label{sec:simulations}

In this section, we will first outline details on our implementation and afterwards investigate the behaviour of the adaptive multiscale test in a simulation study. Finally we will elaborate further on real data examples from super-resolution microscopy, including the one from Section \ref{subsec:microscopy}.

\subsection{Implementation}\label{sec:implementation}

As already mentioned in the introduction, for a generic set $\mathcal R_n$ all local test statistics $T_R$ in \eqref{eq:TR} have to computed individually. To this end, for example, the approximating rectangular system from \citet{w10} can be used to compute local averages on these. Depending on the structure of $\mathcal R_n$, other efficient computational schemes can be employed. In the following we will focus on the situation that there is a global shape $B \subset I_n^d$ such that every $R \in \mathcal R_n$ is a rescaled and shifted version of $B$. More precisely, for each $R \in \mathcal R_n$ there exist $t, h \in I_n^d$ with $t_i + h_i \leq n$ for all $1 \leq i\leq d$ such that
\begin{equation}\label{eq:R_struct}
\1_R \left(x\right) = \1_B \left(\frac{x-t}{h}\right), \qquad x \in I_n^d,
\end{equation}
where division is meant component-wise and $\1$ denotes the indicator function. For the set $\mathcal R_n$ of all hyper-rectangles, this is for example the case with $B = \1_{I_n^d}$. 

If $R$ obeys \eqref{eq:R_struct}, we obtain
\[
\bar Y_R = \frac{1}{\left|R\right|} \sum_{i \in R} Y_i = \frac{1}{\left|R\right|} \sum_{i \in I_n^d} Y_i \1_B\left(\frac{i-t}{h}\right) = \frac{1}{\left|R\right|} \left(Y \ast \1_B \left(\frac{\cdot}{h}\right)\right) \left(t\right),
\]
where $\ast$ denotes a discrete convolution. Employing the fast Fourier transform (FFT), this allows to compute all $T_R$ with a fixed scale $h$ by means of 3 FFTs via
\[
\left(\bar Y_R\right)_{h = \bar h} = \frac{1}{h^d \left|B\right|} \text{FFT}^{-1} \left(\text{FFT} \left(Y\right) \cdot \text{FFT}\left( \1\left(\frac{\cdot}{h}\right)\right)\right).
\]
This operation has a computational complexity almost linear in the data, i.e. $\mathcal O \left(d n^d \log\left(n\right)\right)$, and hence if each $R \in \mathcal R_n$ obeys \eqref{eq:R_struct}, then $T_n$ in \eqref{eq:Tn} can be computed within $\mathcal O \left(d N n^d\log\left(n\right)\right)$ where $N$ is the number of different scales. The same holds true for the evaluation of $M_n$ in \eqref{eq:Mn}. %A corresponding implementation in MATLAB\copyright is available under \todo[inline]{Insert link to code here}

\subsection{Simulation study}

In this section, we will study the finite sample properties of the adjusted multiscale test. We therefore consider different model situations already outlined before. In all of these models, we investigate the empirical level and power of the procedure by means of Monte Carlo simulations. We set $n = 128$, $d = 2$, and $\mathcal R_n$ to be the set of all rectangles in $I_n^d$ with even side-lengths between $4$ and $14$ pixels. As penalization functions $\tilde \omega$ and $\omega$, we choose the previously mentioned penalization due to \cite{dw08} as in \eqref{pen_ds01}. Empirical quantiles of $M_n$ in \eqref{eq:Mn} for this situation are shown in Table \ref{tab:quantiles_128}, the empirical density and CDF are displayed in Figure \ref{fig:invariance_dist}.

\begin{table}
\caption{\label{tab:quantiles_128}	Empirical quantiles $q_{1-\alpha}$ with different values of $\alpha$ for the distribution of $M_n$ in \eqref{eq:Mn} in the case $n = 128, d = 2$ and $\mathcal R_n$ being the set of all rectangles in $I_n^d$ with side-lengths between $4$ and $14$.
}
\centering
\fbox{%
\begin{tabular}{*{6}{c}}
$\alpha$ & $0.2$ & $0.1$ & $0.05$ & $0.025$ & $0.01$ \\
$q_{1-\alpha}$& $1.2906$ & $1.4677$ & $1.6278$ & $1.7841$ & $1.9768$\\
\end{tabular}
}
\end{table}

% Figure environment removed

For power simulations, we consider a $8\times8$ sized anomaly positioned in the centre of the $128\times 128$ image. For comparison, we also investigate the corresponding oracle procedure, which arises from evaluating $T_n\left(Y, \mathcal R_n, \theta_0, \xi\right)$ with the true values of $\theta_0$ and $\xi$.

%It is to be expected that this procedure will have a larger power, and hence the power loss can (to some extend) be seen as the price for adaptation.

\subsubsection{Homogeneous Gaussian model}

Let us first consider the case already discussed in the introduction that $F_{\theta, \xi}$ is a normal distribution $\mathcal N \left(\mu, \sigma^2\right)$, and the variance $\sigma>0$ is considered as the nuisance parameter and estimated by the sample variance
\[
\hat\sigma^2_n := \frac{1}{n^d-1} \sum_{i \in I_n^d} \left(Y_i - \bar Y_{I_n^d}\right)^2.
\]
In a first scenario, we assume that $\mu_0 = 0$ is known. Then \eqref{eq:estimators} is clearly satisfied under $\mathbb P_0$ with $s_n = n^{-d/2}$. Figure \ref{fig:gauss}(a) depicts the levels of the oracle and the adaptive procedure for the choice $\alpha = 0.1$. We find that the level is kept quite stable over a large range of variances. In Figure \ref{fig:gauss}(b), the empirical power in case $\mu_0 = 0$ and $\mu_i = 1$ on the anomaly for different values of $\sigma$ is shown. Notably, there is nearly no power loss caused by adaptation for an unknown $\sigma^2$.

% Figure environment removed

\subsubsection{Poisson model}

Next we consider our guiding example where $F_{\theta, \xi}$ is always a Poisson distribution $\text{Poi} \left(\lambda\right)$ with unknown background intensity $\lambda_0$, which will be estimated by the sample mean, i.e $\hat\lambda_n := \bar Y_{I_n^d}$. Then \eqref{eq:estimators} is clearly satisfied with $s_n = n^{-d/2}$. Figure \ref{fig:poisson}(a) depicts the levels of the oracle and the adaptive procedure for the choice $\alpha = 0.1$. We again find that the level is kept quite stable, even for considerably small values of $\lambda_0$. In Figure \ref{fig:poisson}(b), the empirical power in case $\lambda_0 = 1$ and $\lambda_i = \lambda$ on the anomaly for different values of $\lambda$ is shown. Similarly to the Gaussian case, there is  nearly no power loss caused by adaptation for an unknown $\lambda_0^2$.

% Figure environment removed

\subsection{Application to super-resolution microscopy}\label{sec:application}

Recall Section \ref{subsec:microscopy}. In fluorescence microscopy, structures of interest (e.g. proteins) in a sample are labelled by fluorescent markers, and then scanned spatially along a grid with a diffraction-limited laser spot centred at the current grid point. Whenever a marker is hit by the incoming light it excites with a certain probability, and if so, afterwards light of a different wavelength is emitted and recorded by detectors \citep[cf.][for details]{aem15}. However, the measurement procedure cannot be repeated forever, as each marker is only able to pass through the cycle of excitation and emission a limited number of times before the marker bleaches. To avoid masking effects by bleaching, the total illumination time per pixel (called pixel dwell time) is hence set to a (not too large) number $t_{\mathrm{max}} > 0$. 

It is immediately clear, that a larger value of $t_{\mathrm{max}}$ will allow for a more accurate imaging of the underling structures of interest. However, even if large areas in the image are mostly empty, these regions have to be scanned and by doing so all markers in the image are hit by light and hence either excited or at least stressed. Therefore, modern methods in fluorescence microscopy avoid scanning mostly empty areas in the image by a real-time spatial control of the illumination known as spatially-controlled illumination microscopy (SCIM), see e.g. the review by \citet{kvnmh16}. In principle this can be interpreted as scanning each grid point with a substantially smaller pixel dwell time $t\ll t_{\mathrm{max}}$ and deciding upon the corresponding measurement $Y_i$ if the full pixel dwell time $t_{\mathrm{max}}$ is applied or if the measurement at the current grid point is stopped. However, $t$ has to be determined by the user, and no statistical guarantee of not overseeing regions with markers can be given for existing methods.

As the data collected by a fluorescence microscope consists of photon counts, a natural assumption is that the available data follows a Poisson distribution, cf. \citet{aem15}. We will hence model the corresponding observations as an independent $d-$dimensional (usually $d = 2$) field $Y$ of Poisson random variables
\begin{align}\label{eq:model_pois}
Y_i \sim \text{Poi}(t\lambda_i), \quad i \in I_n^d:= \{1, \ldots,n\}^d
\end{align} S
with parameters $\lambda_i >0$, $i \in I_n^d$. Given some background parameter $\lambda_0>0$, corresponding to photons which are always collected at any grid position due to background contributions, the problem of identifying regions which contain true features can be formulate as detection of regions $R \subset I_n^d$ such that $\lambda_i > \lambda_0$ for some $i \in R$.

In addition to the example in Section 1, we will in the following present two further real data examples, where we know exactly which labelled structures are contained in the specimen. 


\subsubsection{Data example 1: Single sized crimson beads}

As a first data example, we investigate $48 \mathrm{nm}$ crimson beads, i.e. carboxylate modified microspheres of $48\mathrm{nm}$ diameter filled with fluorescent markers. Those have been imaged using STED super-resolution microscopy, see e.g. \citet{hw94,h07} for details) with an effective resolution of $\sim 60 \mathrm{nm}$. The corresponding data for an illumination time of $t =1 \mu\mathrm{s}$ and $T = 100 \mu \mathrm{s}$ is shown in Figure \eqref{fig:beads}(a) and (d). Note, that each pixel has size $20 \mathrm{nm} \times 20 \mathrm{nm}$. We then apply the AMS methodology with $\alpha = 0.1$, $n = 400$, $d = 2$, $\mathcal R$ being the set of all rectangles in $I_{400}^2$, and $r_n = 4$, $m_n = 20$, i.e. we scan over all rectangles with side lengths between $4$ and $20$ pixels, corresponding to $80\mathrm{nm}$ and $400\mathrm{nm}$. The result is shown in sub-panels (b) and (e). Surprisingly, the test detects nearly all beads based on the $5 \mu\mathrm{s}$ data, which are barely visible by eye there. Once again, regarding the $T = 100 \mu \mathrm{s}$ data as ground truth, we depict this together with the regions found in (b) in sub-panel (e), reveiling that there no false positive detections. Finally, the $T = 100 \mu \mathrm{s}$ data can be used to derive a segmentation into active and inactive regions depicted in (f).

% Figure environment removed

\subsubsection{Data example 2: mixture of differently sized crimson beads}

As a second data example, we now consider a mixture of $48 \mathrm{nm}$ and $200 \mathrm{nm}$ crimson beads, again imaged by STED with an effective resolution of $\sim 60 \mathrm{nm}$. Consequently, the data shown in Figure \ref{fig:mixed_beads}(a) and (d) consists of two structures of different sizes, namely around $48 \mathrm{nm}$ and $200 \mathrm{nm}$. Due to the number of markers inside the spheres, the larger structures are significantly brighter than the small ones. We investigate $t = 3 \mu \mathrm{s}$ and $T = 100 \mu\mathrm{s}$. The AMS procedure is applied with the same parameters as before, and the result is shown in sub-panels (b) and (e). It is immediately visible that it detects again nearly all of the small structures already from the measurements taken with $15 \mu\mathrm{s}$ dwell time, which is surprising as some of them are not visible by eye in the data. Once again, regarding the $T = 100 \mu \mathrm{s}$ data as ground truth, we depict this together with the regions found in (b) in sub-panel (e), reveiling that there no false positive detections. Finally, the $T = 100 \mu \mathrm{s}$ data can be used to derive a segmentation into active and inactive regions depicted in (f).

% Figure environment removed

\section{Proofs}

We start with some helpful abbreviations. For the whole section, let $X = \left(X_i\right)_{i \in I_n^d}$ be a field of i.i.d. standard Gaussians, $X_i \sim \mathcal N\left(0,1\right)$. As we rely heavily on centred and standardized partial sums, we introduce for $R \in \mathcal R_n$ the quantity
\[
Y_R\left(\theta, \xi\right):= \left|R\right|^{-1}\sum\limits_{i \in R} \left( \frac{Y_i-m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right) = \frac{\bar Y_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta,\xi\right)}}, \qquad \left(\theta, \xi\right) \in \Theta \times \Xi
\]
%\[
%Y_R\left(\theta, \xi\right):= \left|R\right|^{-\frac12}\sum\limits_{i \in R} \left( \frac{Y_i-m\left(\theta, \xi\right)}{\sqrt{v\left(\theta, \xi\right)}}\right) = \left|R\right|^{\frac12} \frac{\bar Y_R - m\left(\theta, \xi\right)}{\sqrt{v\left(\theta,\xi\right)}}, \qquad \left(\theta, \xi\right) \in \Theta \times \Xi
%\]
with the mean and variance functions $m$ and $v$ from Assumption \ref{ass:model}. Also recall our abbreviation
\[
\bar X_R := \frac{1}{\left|R\right|} \sum_{i \in R} X_i.
\]
%as well as the corresponding Gaussian version
%\[
%X_R := \left|R\right|^{-\frac12} \sum\limits_{i \in R}X_i =  \left|R\right|^{\frac12} \bar X_R.
%\]

\subsection{Preparations}

Next we recall a a series of helpful results taken from \citet{kmw20}.

\begin{lem}[Coupling]\label{lem:coupling}
Let Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, and let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta, \xi}$ with $\theta \in U, \xi \in V$ and let $\left(r_n\right)_{n \in \N} \subset \mathbb N$ be a sequence. Then, on the same probability space, there exists an array $X = \left(X_i\right)_{i \in I_n^d}$ of i.i.d. standard Gaussians $X_i \sim \mathcal N\left(0,1\right)$ such that
\[
\P{}{\left|\max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right|  - \max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)}\left|R\right|^{\frac12} \left|\bar X_R\right| \right| > \delta } \leq C \delta^{-3} \left(\frac{\log^{10}n}{r_n} \right)^{1/2}
\]
for all $\delta > 0$ with some universal constant $C> 0$ independent of $\theta$ and $\xi$.
\end{lem}
\begin{proof}
As the distributions $F_{\theta, \xi}$ have uniformly sub-exponential tails, this follows directly from Theorem 4.3 in \citet{kmw20} in combination with a symmetrization argument as employed in the proof of Theorem 2.5 there.
\end{proof}
Note, that in terms of $\OP$ notation, Lemma \ref{lem:coupling} yields
\begin{equation}\label{eq:coupling}
\max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right|  - \max\limits_{ R \in \mathcal{R}_n\left(r_n, n^d\right)}\left|R\right|^{\frac12}\left|\bar X_R\right|  = \OP \left(\left(\frac{\log^{10} \left(n\right)}{r_n}\right)^{\frac16}\right),
\end{equation}
uniformly in $\left(\theta, \xi\right)\in U \times V$.
%, and that this as well as the statement of Lemma \ref{lem:coupling} stays true if we replace the upper bound $n^d$ for the scale size by $m_n$ with any sequence $\left(m_n\right)_{n \in \N} \subset \N$ satisfying $r_n \leq m_n$ for all $n \in \N$.

\begin{lem}[Taylor expansion, see \protect{Lemma 5.1 in \citet{kmw20}}]\label{lem:taylor_exact}
Let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta_0, \xi}$ with fixed $\theta_0 \in \Theta, \xi \in \Xi$ and let $\left(r_n\right)_{n \in \N} \subset \mathbb N$ be a sequence satisfying \eqref{eq:r_n}. If Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, then

\[
\max\limits_{R \in  \mathcal{R}_n \left(r_n, n^d\right)} \left| T_R\left(Y, \theta_0, \xi\right) -  \left|R\right|^{\frac12} Y_R \left(\theta_0, \xi\right)\right| = \OP \left(\left( \frac{\log^3(n)}{r_n}\right)^{1/4}\right).
\]
\end{lem}
Note, that the previous results remain true if we replace the upper bound $n^d$ for the scale size by $m_n$ with any sequence $\left(m_n\right)_{n \in \N} \subset \N$ satisfying $r_n \leq m_n$ for all $n \in \N$.

\begin{lem}\label{lem:bound}
Let Assumptions \ref{ass:model}(a) and \ref{ass:R} hold true, and let $Y = \left(Y_i\right)_{i \in I_n^d}$ be an array of i.i.d. random variables $Y_i \sim F_{\theta, \xi}$ with fixed $\theta \in U, \xi \in V$. If  $\left(r_n\right)_{n \in \N} \subset \mathbb N$ is a sequence satisfying \eqref{eq:r_n}, then we obtain	
\[
\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|\left| Y_R \left(\theta_0, \xi\right) \right|^3 = \OP\left(\left(\frac{\log^3\left(n\right)}{r_n}\right)^{\frac12} \right)
\]
uniformly for $\theta \in U, \xi \in V$.
\end{lem}
\begin{proof}
Note, that due to $\alpha, \widetilde{\alpha} >0$, \eqref{eq:r_n} implies especially that $\gamma \geq 4$ and hence $\log^4\left(n\right) = o \left(r_n\right)$. 
	
It is well known (see e.g. \citet{k11}), that for a standard Gaussian array $X = \left(X_i\right)_{i \in I_n^d}$, $X_i \stackrel{\text{i.i.d.}}{\sim} \mathcal N \left(0,1\right)$, one has
\begin{equation}\label{eq:aux3}
\E{}{\max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12} \left|\bar X_R\right|} \leq C \sqrt{\log\left(\# \mathcal R_n \left(r_n, m_n\right)\right)}
\end{equation}
with some constant $C > 0$. Due to Assumption \ref{ass:R} this implies that
\[
\frac{1}{\sqrt{\log\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12} \left|\bar X_R\right|  = \OP\left(1\right). 
\]
Together with \eqref{eq:coupling} this yields
\[
\frac{1}{\sqrt{\log\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)} \left|R\right|^{\frac12}\left|Y_R\left(\theta, \xi\right)\right| = \OP\left(1\right)
\]
uniformly for $\left(\theta, \xi\right)\in U \times V$, where we used that $\log^4\left(n\right)= o \left(r_n\right)$.  Consequently we find
\begin{align*}
\sqrt{\frac{r_n}{\log^3\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)}  \left|R\right|\left| Y_R \left(\theta_0, \xi\right) \right|^3 \leq  \sqrt{\frac{1}{\log^3\left(n\right)}} \max_{R \in \mathcal R_n \left(r_n, m_n\right)}\left|R\right|^{\frac32}\left| Y_R \left(\theta_0, \xi\right) \right|^3 =\OP \left(1\right)
\end{align*}
uniformly for $\left(\theta, \xi\right)\in U \times V$, which proves the claim.
\end{proof}

\subsection{An adjusted Taylor expansion}

Now we are in position to derive some preparations for the proof of Theorem \ref{thm:approximation_estimated_intensity}. Therefore we first replace the likelihood ratio statistic $T_R$ by its Taylor expansion according to Assumption \ref{ass:model}(e):
\begin{lem}\label{lem:taylor_estimated}
Let $\left(r_n\right)_{n\in\N} \subset \N$ and $\left(m_n\right)_{n\in\N} \subset \N$ be sequences tending to $\infty$ and consider the local statistic $T_R$ from \eqref{eq:TR}. Suppose that $Y$ is as in \eqref{eq:model} and let Assumptions \ref{ass:model} and \ref{ass:R} be fulfilled. If the estimators $\hat \theta_n$ and $\hat \xi_n$ satisfy \eqref{eq:estimators}, then
\begin{equation}\label{eq:taylor}
\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \hat \theta_n, \hat \xi_n) -  \left|R\right|^{1/2}\left|Y_R \left(\theta_0, \xi\right)\right|\right| = \OP\left(\left(\frac{\log^7(n)}{r_n}\right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right).
\end{equation}
\end{lem}

\begin{proof}
Let $U \times V$ be the neighbourhood of $\left(\theta_0, \xi\right)$ as in Assumption \ref{ass:model}. By \eqref{eq:estimators}, the probability that $\hat \theta_n \in U$ and $\hat \xi_n \in V$ tends to $1$, as $n \to \infty$. Hence, it suffices to prove the result conditional on this event. Consequently we can assume $\hat \theta_n \in U$ and $\hat \xi_n \in V$ in the following.

By the triangle inequality and Lemma \ref{lem:taylor_exact}, we find
\begin{align*}
 &\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R\left(Y, \hat \theta_n, \hat \xi_n\right) -   \left|R\right|^{\frac12} \left|Y_R\left(\theta_0, \xi\right)\right|\right| \\
\leq &  \max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \theta_0, \xi) -   |R|^{\frac12}\left|Y_R\left(\theta_0, \xi\right)\right|\right|\\
 &+ \max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R(Y, \hat \theta_n, \hat \xi_n) -   T_R\left(Y, \theta_0, \xi\right)\right| \\
\leq &\sqrt{\max\limits_{R \in  \mathcal{R}_n ( r_n, m_n)} \left| T_R^2(Y, \hat \theta_n, \hat \xi_n) -   T_R^2\left(Y, \theta_0, \xi\right)\right|} + \OP\left(\left( \frac{\log^3(n)}{r_n}\right)^{\frac14}\right),
\end{align*}
where we exploited Lemma \ref{lem:taylor_exact} and $\left|a-b\right| \leq \sqrt{\left|a^2 - b^2\right|}$. By the triangle inequality and Assumption \ref{ass:model}(e) we find furthermore that
\begin{align*}
&\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2(Y, \hat{\theta}, \hat{\xi}) - T_R^2(Y, \theta_0, \xi) \right|\\
\leq& \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2(Y, \hat{\theta}, \hat{\xi}) - \left|R\right| \left|Y_R\left(\hat \theta_n, \hat\xi_n\right)\right|^2 \right| + \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|T_R^2\left(Y, \theta_0, \xi\right)-\left|R\right| \left|Y_R\left(\theta_0, \xi\right)\right|^2\right|\\
&+ \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2 - Y_R\left(\theta_0, \xi\right)^2 \right|\\
\leq & C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R \left(\hat \theta_n, \hat \xi_n\right) \right|^3 + C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|  \left|Y_R\left(\theta_0, \xi\right) \right|^3 \\
&+ \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2-Y_R\left(\theta_0,\xi\right)^2\right| \\
\leq & 2C_T \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\theta_0, \xi\right) \right|^3  + \max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right|\left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^2-Y_R\left(\theta_0,\xi\right)^2\right|\\ & + C_T\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^3-Y_R\left(\theta_0,\xi\right)^3\right|,
\end{align*}
where we used $\left| \left|a\right|^3 - \left|b\right|^3\right| = \left| \left|a^3\right| - \left|b^3\right|\right| \leq \left| a^3 - b^3\right|$ for $a,b \in \R$. In view of Assumptions \ref{ass:model}(a) and \ref{ass:R}, the first term in the last display can be bounded using Lemma \ref{lem:bound}. The second and third term will be handled using the mean value theorem. Therefore, note that all derivatives of
\[
\left(\theta, \xi\right)\mapsto Y_R\left(\theta,\xi\right)^k, \qquad k \in \left\{2,3\right\}
\]
are a.s. bounded due to Assumption \ref{ass:model}(b)--(d) and the fact that $\bar Y_R$ is a.s. bounded in view of Lemma \ref{lem:bound}. This together with \eqref{eq:estimators} implies that
\begin{align*}
\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left| Y_R\left(\hat{\theta}, \hat{\xi}\right)^2 - Y_R\left(\theta_0, \xi\right)^2\right|\leq m_n C \left(\|\hat{\theta}_n-\theta_0 \|_2 + \|\hat{\xi}_n - \xi \|_2\right) = \OP \left(\frac{m_n}{s_n}\right),
\end{align*}
where $C$ is some generic constant $C > 0$. As $\max\limits_{R \in \mathcal{R}_n(r_n, m_n)} \left|R\right| \left|Y_R\left(\hat \theta_n, \hat \xi_n\right)^3-Y_R\left(\theta_0,\xi\right)^3\right|$ can be treated similarly, this yields the claim.
\end{proof}

\subsection{Proof of Theorem \ref{thm:approximation_estimated_intensity}}

Now we are ready to prove the Gaussian approximation result from Theorem \ref{thm:approximation_estimated_intensity}.
\begin{proof}[of Theorem \ref{thm:approximation_estimated_intensity}]
For notional simplicity, we will throughout this proof drop the second argument of $\widetilde{\omega}$ and $\omega$. Furthermore we abbreviate $\widetilde{\mathcal R}_n := \mathcal R_n \left(r_n, m_n\right)$. First of all, we estimate	
\begin{align*}
&\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right|\\
=& \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right| \\
\leq& \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(T_R \left(Y, \hat \theta_n, \hat \xi_n\right) -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| - \omega \left(\left|R\right|\right) \right) \right| \\
& + \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right|- \omega \left(\left|R\right|\right) \right) \right| \\
\leq & \max_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right)  \max _{R \in \widetilde{\mathcal R}_n} \left| T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right|\right| \\
& + \left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right) - \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|,
\end{align*}
where we used $\left|\|x\|_\infty-\|y\|_\infty  \right|\leq \|x-y\|_\infty$. The first term can be controlled using Assumption \ref{ass:Omega} and Lemma \ref{lem:taylor_estimated} by
\begin{multline*}
\max_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right)  \max _{R \in \widetilde{\mathcal R}_n} \left| T_R \left(Y, \hat \theta_n, \hat \xi_n\right) - \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right|\right| \\
= \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)\right).
\end{multline*}
So it remains to estimate the second term, i.e. for a suitable sequence $\left(c_n\right)_{n\in\N}$ we want to show that
\begin{multline}\label{eq:aux2}
\lim_{M \to \infty} \limsup_{n  \to\infty} \mathbb P_0 \left[\left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right)\right.\right. \\\left.\left.- \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|> c_n M\right]  = 0
\end{multline}
This is more subtle, as the coupling in Lemma \ref{lem:coupling} only yields a result for the difference of the maxima, and not for the maximum of the differences (as lemma \ref{lem:taylor_estimated} does). We therefore borrow a slicing technique from \citet{pwm18}, dividing the set of scales is into families on which $\widetilde{\omega}$ and $\omega$ are almost constant, i.e. into
\[
\mathcal{R}_{n,j}:= \left\lbrace R \in \mathcal{R}_n \left| \right. \exp\left(\epsilon_j\right) \leq \left|R\right| <  \exp\left(\epsilon_{j+1}\right) \right\rbrace.
\]
Here and in what follows we choose
\[
\epsilon_1 := \log\left(r_n\right), \qquad \epsilon_j = \epsilon_1 + \frac{j-1}{J} \log \left(\frac{m_n}{r_n}\right), \quad j=2, ..., J.
\]
With this definition, for any choice $J \in \N_{\geq 2}$ we obtain
\[
\widetilde{\mathcal R}_n = \mathcal{R}_n(r_n, m_n) = \left\{ R \in \R_n ~\big|~ r_n \leq \left|R\right| \leq m_n \right\} = \bigcup\limits_{j\in J}\mathcal{R}_{n,j}.
\]
The parameter $J \in \N$ will be defined later. On $\mathcal R_{n,j}$ we can approximate $\widetilde{\omega}\left(\left|R\right|\right)$ and $\omega\left(\left|R\right|\right)$ by
\begin{align*}
\widetilde{\omega}_{j,n} := \widetilde{\omega}\left(\exp\left(\epsilon_j\right)\right) \qquad \text{and}\qquad \omega_{j,n} := \omega\left(\exp\left(\epsilon_j\right)\right),
\end{align*}
respectively. Then it holds
\[
\omega_{j+1,n} \leq \omega\left(\left|R\right|\right) \leq \omega_{j,n}\qquad\text{and}\qquad \widetilde{\omega}_{j+1,n} \leq \widetilde{\omega}\left(\left|R\right|\right) \leq \widetilde{\omega}_{j,n}
\]
for all $R \in \mathcal{R}_{n,j}$. Now we compute
\begin{align*}
&\max\limits_{R \in \mathcal{R}_{n,j}}\widetilde{\omega}(|R|) \left(\left|R\right|^{\frac12} \left|Y_R\left(\theta_0, \xi\right)\right|-\omega(|R|) \right)-\max\limits_{R \in \mathcal{R}_{n,j}}\widetilde{\omega}(|R|) \left(\left|R\right|^{\frac12} \left|\bar X_R\right|-\omega(|R|) \right)\\
\leq& \left( \widetilde{\omega}_{j,n} \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left|Y_R \left(\theta_0, \xi\right) \right| - \widetilde{\omega}_{j+1,n} \max\limits_{R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right| \right) + \left(\widetilde{\omega}_{j,n} \omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n} \right)\\
=& \widetilde{\omega}_{j,n} \left( \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left| \bar X_R\right|\right) + \left(\widetilde{\omega}_{j,n} - \widetilde{\omega}_{j+1,n} \right) \max\limits_{ R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right|\\
& + \left(\widetilde{\omega}_{j,n}\omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n} \right),
\end{align*}
and a similar relation holds true if the roles of $\bar X_R$ and $Y_R \left(\theta_0, \xi\right)$ are interchanged. To bound the differences involving the $\widetilde{\omega}_{j,n}$ terms, we exploit the rough bound
\begin{equation}\label{eq:aux1}
\widetilde{\omega}_{j,n} = \widetilde{\omega}\left(\exp(\epsilon_j) \right)\leq C_\omega \left(\log \left( \frac{n^d}{\exp\left(\epsilon_j\right)}\right) \right)^{\widetilde{\alpha}}\leq C_\omega\left(\log(n^d) \right)^{\widetilde{\alpha}}=:A_n
\end{equation}
for all $1 \leq j \leq J$, and a similar analogue for $\omega_{j,n}$. From the mean value theorem we get
\begin{align*}
\widetilde{\omega}_{j,n} - \widetilde{\omega}_{j+1,n} =&\ \widetilde{\omega}\left(\exp\left(\epsilon_{j+1}\right) \right)-\widetilde{\omega}\left(\exp\left(\epsilon_{j}\right) \right)\\
\leq& \left|\exp\left(\epsilon_j\right)-\exp\left(\epsilon_{j+1}\right) \right| \max_{\zeta \in \left(\epsilon_j, \epsilon_{j+1}\right)} \widetilde{\omega}'\left(\exp\left(\zeta_j\right) \right)\\
\leq& C_\omega\left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)} \left(\exp(\epsilon_{j+1}-\epsilon_j)-1\right)\\
=& C_\omega \left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)}\left( \exp\left(\frac{1}{J} \log\left(\frac{m_n}{r_n}\right) \right)-1\right) \\
 = & C_\omega \left( \log (n^d)\right)^{\max(\widetilde{\beta}, 0)} \left(\left(\frac{m_n}{r_n}\right)^{\frac1J}-1\right)=:B_n
\end{align*}
Similar statements hold for $\omega_{j,n}$ and $\omega_{j,n}-\omega_{j+1,n}$ and hence using \eqref{eq:aux1} we obtain
\begin{align*}
&\left|\widetilde{\omega}_{j,n} \omega_{j,n} - \widetilde{\omega}_{j+1,n}\omega_{j+1,n}\right|\\
=& \left|\widetilde{\omega}_{j,n} (\omega_{j,n} - \omega_{j+1,n} ) - \omega_{j+1,n}\left(\widetilde{\omega}_{j,n}-\widetilde{\omega}_{j+1,n}\right)\right|\\
\leq&C_\omega^2  \left(\left(\frac{m_n}{r_n}\right)^{\frac1J}-1\right) \left( \left(\log(n^d) \right)^{\widetilde{\alpha} + \max\left\{\beta, 0\right\}} +  \left(\log(n^d) \right)^{\alpha + \max\left\{\widetilde{\beta}, 0\right\}} \right)=:C_n
\end{align*}
This implies
\begin{align*}
&\left|\max\limits_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega}\left(\left|R\right|\right) \left(\left|R\right|^{\frac12}\left|Y_R\left(\theta_0,\xi\right)\right|-\omega\left(\left|R\right|\right) \right)- \max\limits_{R \in \widetilde{\mathcal R}_n} \widetilde{\omega}\left(\left|R\right|\right) \left(\left|R\right|^{\frac12}\left|\bar X_R\right|-\omega\left(\left|R\right|\right) \right) \right|\\
\leq & A_n \max_{1 \leq j \leq J} \left| \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \widetilde{\mathcal R}_n} \left|R\right|^{\frac12}\left| \bar X_R\right|\right| + B_n \max\limits_{ R \in \mathcal{R}_{n,j}}\left|R\right|^{\frac12}\left|\bar X_R\right| + C_n,
\end{align*}
and hence
\begin{align*}
&\P{0}{\left|\max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right)\right| -  \omega \left(\left|R\right|\right) \right)- \max_{R\in \widetilde{\mathcal R}_n} \widetilde{\omega} \left(\left|R\right|\right) \left(\left|R\right|^{\frac12} \left|\bar X_R\right| - \omega \left(\left|R\right|\right) \right) \right|> c_n M}\\
\leq & \P{0}{A_n \max_{1 \leq j \leq J} \left|  \max\limits_{R \in \mathcal{R}_{n,j} } \left|R\right|^{\frac12} \left|Y_R \left(\theta_0, \xi\right) \right| - \max\limits_{R \in \mathcal{R}_{n,j}} \left|R\right|^{\frac12}\left| \bar X_R\right|\right| > \frac{c_n M}{3}} \\
& + \P{0}{B_n \max\limits_{ R \in \mathcal{R}_{n}}\left|R\right|^{\frac12}\left|\bar X_R\right| > \frac{c_n M}{3}} + \P{0}{C_n > \frac{c_n M}{3}} \\
=:& \text{I} + \text{II} + \text{III}.
\end{align*}
To bound I, we use Lemma \ref{lem:coupling} and the union bound to obtain
\[
\text{I} \leq C\left|J\right| \frac{3^3A_n^3}{c_n^3M^3} \left(\frac{\log^{10} \left(n\right)}{r_n}\right)^{\frac12}.
\]
Here and in what follows, $C >0$ is some generic constant, the value of which can change from line to line. For II, we exploit Markov's inequality as well as \eqref{eq:aux3} and find in view of Assumption \ref{ass:R}, that
\[
\text{II} \leq \frac{\E{}{\max_{R \in \mathcal R_n} \left|R\right|^{\frac12} \left|\bar X_R\right|}}{c_n M 3^{-1} B_n^{-1}} \leq C \frac{B_n \sqrt{\log\left(n\right)}}{c_n M}
\]
with some constant $C > 0$. This shows that \eqref{eq:aux2} is satisfied as soon as
\begin{enumerate}
\item[(A)] $\left|J\right| A_n^3 c_n^{-3} \log^5 \left(n\right) r_n^{-\frac12} = \mathcal O \left(1\right)$, 
\item[(B)] $B_n c_n^{-1} \sqrt{\log\left(n\right)} = \mathcal O \left(1\right)$, and
\item[(B)] $C_n = o \left( c_n\right)$
\end{enumerate}
hold true. To obtain this, we set
\[
J := \lfloor \frac{\log^{\nu+1} \left(n\right)}{c_n}\rfloor
\]
with some parameter $\nu > 0$ to be determined. This implies
\[
\left|J\right| \frac{A_n^3}{c_n^3} \left(\frac{\log^{10}\left(n\right)}{r_n}\right)^{\frac12} \leq C \frac{\log^{6+3\widetilde{\alpha} + \nu}}{c_n^4 \sqrt{r_n}},
\]
which proves (B) for
\begin{equation}\label{eq:cn}
c_n := \left(\frac{\log^{12+6\widetilde{\alpha} + 2\nu}\left(n\right)}{r_n}\right)^{\frac18}.
\end{equation}
Next we use $m_n / r_n \leq n^d$ and use $\exp\left(x\right) - 1 \leq 2x$ for $x \in \left[0,1\right]$ to find
\[
\left(\frac{m_n}{r_n}\right)^{\frac1J} \leq n^{\frac{d}{J}} = \exp\left(\frac{d c_n}{\log^\nu\left(n\right)}\right) \leq 1+C \frac{c_n}{\log^\nu\left(n\right)}
\]
for any sufficiently large $n$ supposed that $c_n = o \left(\log^\nu\left(n\right)\right)$. Thus it holds
\[
B_n \leq C \frac{c_n}{\log^{\nu-\max\left\{\widetilde{\beta},0\right\}}\left(n\right)},
\]
and hence (A) is satisfied if $\nu \geq \max\left\{\widetilde{\beta},0\right\} + \frac12$. It can readily be seen that (C) is now satisfied as soon as we define
\[
\nu := \max\left\{\frac12, \alpha, \widetilde{\alpha}\right\} + \max\left\{\beta, \widetilde{\beta},0\right\}.
\]
Note that this also ensures the required condition $c_n = o \left(\log^\nu\left(n\right)\right)$ under \eqref{eq:r_n}. Hence \eqref{eq:aux2} is satisfied with $c_n$ as in \eqref{eq:cn}, and thus we have
\begin{multline*}
\left|T_n \left(Y,  \widetilde{\mathcal R}_n, \hat{\theta}, \hat{\xi} \right) - M_n\left(\widetilde{\mathcal R}_n\right) \right|\\ = \OP\left( \frac{\log^{\widetilde{\alpha}} \left(n\right)}{r_n} \left(\left(\frac{\log^3\left(n\right)}{r_n} \right)^{\frac14} + \sqrt{\frac{m_n}{s_n}}\right)+\left(\frac{\log^{12+6\widetilde{\alpha} + 2\nu}\left(n\right)}{r_n}\right)^{\frac18}\right),
\end{multline*}
which together with $\frac{\log^{\tilde \alpha + 3/44}(n)}{r_n^{5/4}} = \mathcal O \left(\frac{\log^{\gamma/8}(n)}{r_n^{1/8}}\right)$ proves the claim.
\end{proof}

\section*{Acknowledgment}

Financial support by the German Research Foundation DFG through subprojects A04 and A07 of CRC 755 and the DFG Cluster of Excellence "MBExC Multiscale Bioimaging - from Molecular Machines to Networks of Excitable Cells" is gratefully acknowledged. We also thank Ren\'e Siegmund from the Laser Lab G\"ottingen e.V. for providing data and helpful discussions.

\small
\bibliography{MultiNuisance}
\bibliographystyle{apalike} 
\end{document}