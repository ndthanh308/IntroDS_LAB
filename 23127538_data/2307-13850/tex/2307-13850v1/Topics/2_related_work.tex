% \section{Preliminaries}
% \paragraph{Input feature attribution methods.}
% Consider a linear model $f(x) = w_1 x_1 + w_2 x_2$. To explain which feature is more important for predicting the value of f(x), we can compare their coefficients. If $w_1 = 1000$ and $w_2 = 0.01$, we can say that $x_1$ would be weighed more than $x_2$. This type of explanation assumes that the values of $x_1$ and $x_2$ are of the same order. This is true in the case of most inputs to the neural network models, for example image pixels. 
% Gradients are the general way of discussing the coefficient with respect to a particular feature to discuss its importance.
% \textbf{Element-wise product of gradient into input} \textsc{grad $\odot$ input} \cite{Shrikumar2016NotJA}, provides global importance about the input feature in the model's output. 
% \cite{} have used it show the feature importance in attention models. 
% , as compared to just the gradient.
% details of computing the attribution with math 
% Assume $\mathbf{x}$ is a real-valued input feature vector (for any modality). For discrete inputs, real-valued vector obtained after passing the feature through a look-up embedding.

% , but there is no clear superior attribution technique over another. 

% Instead of considering attributions over pixels, \textbf{XRAI} \cite{Kapishnikov2019XRAIBA} computes the effective attributions of integrated gradients over overly segmented image. The image is segmented based on similarity such as color, which makes the segment boundaries align with the edges. The segmentation is done at multiple scales to obtain a set of overlapping image segments.
% Assume that attribution mask over an image $I$ of size ${H\times W}$ is $A$ of the same size. 
% Using graph-based segmentations over multiple scale parameters, we obtain a set of segments $\mathcal{S}$. 
% Let a pixel be indexed by $i$ in the original image. For a segment $s$, the gain can be calculated by $g_s = \sum_{i \in s\backslash M} \frac{A_i}{area(s\backslash M)}$. 
% The segment with maximum gain is selected as  attribution to update the XRAI saliency set $\mathcal{M}$.
% The process is repeated with the remaining segments until the area of the mask set is equal to that of the image. 
% While this method seems to produce slightly better visual attributions over other variants of IG, it is sensitive to the size of segmentation scales and dilation factor. We consider  $XRAI(\cdot)$ to denote this attribution method for visual attribution analysis in \S \ref{subsec:visual_attr}.   
% which create grainy regions. 
% However, this method depends on the size of segmentation scales selected for computation. Further, dilation added to the final attribution masks to include edges may depict an inflated version of model's actual feature importance. 
% In this work, $XRAI(\cdot)$ denotes that this attribution method is applied.
%  \vspace{-0.5em}
\section{Related Work}
%  \vspace{-0.3em}
\label{sec:related_work}
\paragraph{Interpretability and explainability } Recent work in multimodal explainability in autonomous vehicles \cite{gilpin-2021-multimodal} uses symbolic explanations to debug and process outputs out of sub-components.
In contrast, we address the challenge of post-hoc multimodal interpretability for any existing end-to-end trained differentiable policies. \textsc{grad $\odot$ input}~\cite{Shrikumar2016NotJA},  a simple and modality-agnostic attribution that works on par with recent methods~\cite{Ancona2017AUV}. We use this method to compute multimodal attribution at inputs to the fusion layer to weigh how each modality contributes to the decision-making. 
% as it has been shown to work at par compared to the recent gradient-based attribution techniques~\cite{Ancona2017AUV}.
While \textsc{grad $\odot$ input} is a modality-agnostic starting point for attributions, 
it is not easy to understand, especially for images. Among recent works to improve visual attribution  \cite{Smilkov2017SmoothGradRN, Simonyan2014DeepIC, ig, sturmfels2020visualizing, Xu_2020_CVPR, Kapishnikov2021GuidedIG, Kapishnikov2019XRAIBA},  we use XRAI~\cite{Kapishnikov2019XRAIBA} for vision-specific analysis as it produces visually intuitive attributions by relying on regions, not individual pixels. 
% \cite{Smilkov2017SmoothGradRN} proposed ways to visually sharpen these vanilla gradient-based attributions. ~\cite{Simonyan2014DeepIC}  applying Gaussian noise perturbations over averaged over a sufficient number of samples.
% describe IG
% IG \cite{ig} and path methods have been studied as a cost-sharing method called Aumann-Shapley. 
% Attribution based on IG preserves axiomatic properties like \textit{sensitivity} and \textit{implementation invariance}.
% While IG aggregate the gradients on sampling inputs on a straight line between the baseline and the input, there are several paths possible in higher dimensional spaces and corresponding different attribution.
% Recent works build on IG to obtain more visually intuitive attributions, like SHAP Deep Explainer~\cite{sturmfels2020visualizing}, Blur IG ~\cite{Xu_2020_CVPR}, Guided IG~\cite{Kapishnikov2021GuidedIG} and XRAI~\cite{Kapishnikov2019XRAIBA}. Qualitatively, XRAI showed visually intuitive attributions by relying on regions and not individual pixels.  
% Interpretability using gradient-based attribution techniques is quite similar to adversarial attacks \cite{Goodfellow2015ExplainingAH} and adversarial training for robustness \cite{Bai2021RecentAI}, as both fundamentally rely on gradient of the input feature with respect to the output. 
% Do we need a figure to show the difference in attributions with just gradient vs gradxinput? 
\vspace{-0.8em}
\paragraph{Language-driven task benchmarks}

There are many benchmarks to study an agentâ€™s ability to follow natural language instructions \cite{ALFRED20, padmakumar2022teach, gu2022vision,  mahmoudieh2022zero}. 
% While most existing settings apply only to either navigation \cite{} or manipulation \cite{}, 
% we conside one of the benchmarks which handles both, that is,
% navigation (Anderson et al., 2018; Chen
% et al., 2019), object manipulation (Misra et al.,
% 2017; Zhu et al., 2017) and embodied reasoning
% (Das et al., 2018a; Gordon et al., 2018). 
ALFRED \cite{ALFRED20} serves as a suitable testbed for this analysis as these tasks require both high reasoning for navigation and manipulation. ALFRED dataset provides visual demonstrations collected through PDDL planning in 3D Unity household environments and natural language description of the high-level goal and low-level instructions annotated by MTurkers. 
The benchmarks provide evaluation metrics for the overall task goal completion success rate (SR) and those weighted by the expert's path length (PLWSR)
% over seen and unseen tasks
and have reported a huge gap in the performance of learning algorithms and humans at these tasks. 
% ALFRED  is a benchmarking environment that provides natural language instructions annotated by MTurkers on egocentric visual sequences of actions taken for everyday household tasks. As ALFRED is a simulated environment on Unity3D game engine, the visual demonstrations are collected based on PDDL planning. 

\vspace{-0.8em}
\paragraph{End-to-end Learned Policies} We investigate the end-to-end learned policies for the task, such that, the gradient can be attributed at a task level. While we do not discuss modular yet differentiable policies like \cite{min2021film} \cite{DBLP:journals/corr/ZhouC15}, tying the gradient across multiple modular learned components is a direction for future work.
% as 
% tying the gradient among modular learned components in future work. 
In our work, we consider the checkpoints of policies trained on the ALFRED dataset. Broadly, these policies are of two types: (a) sequence-to-sequence models, that are, the one proposed with ALFRED dataset (Baseline) \cite{ALFRED20} and Modular Object-Centric Approach (MOCA) \cite{Singh2021FactorizingPA}, (b) transformer-based models, that are Episodic Transformers (ET) \cite{pashevich2021episodic}, and Hierarchical Tasks via Unified Transformers (HiTUT) \cite{Zhang2021HierarchicalTL}. Refer Table~\ref{tab:policiesarch} to compare architectural details \footnote{Previous action is modeled with learned embedding look-up in all these policies.}.
% \textbf{Seq2Seq(Baseline)} \cite{ALFRED20} is a single-stream Seq-to-Seq model with progress monitoring, processing the visual frames through  a frozen ResNet-18 encoder, language through bi-LSTM and soft attention and fusion of the latent visual, language and previous action encodings through an LSTM layer.
%%%% The visual frames are encoded by a frozen ResNet-18 encoder. The language instruction tokens are processed with a bi-LSTM and soft attention. The latent encodings for visual, language and previous action are passed through an LSTM.
% \textbf{MOCA} \cite{Singh2021FactorizingPA} presents a factorized model into two, i.e. interactive perception and action policy. The inputs to the action policy model are language encoding from bi-LSTM, visual embedding from a pretrained ResNet-18, and previous action embedding; all concatenated as input to an LSTM with residual connection.
% \textbf{Episodic Transformers} \cite{pashevich2021episodic} proposes a transformer architecture that encodes the language instructions and the sequence of visual observations and actions to predict subsequent actions per visual frame. Visual observations are processed through pretrained ResNet-50, language tokens passed through a transformer encoder pre-trained with synthetic language targets,  and action are encoded by embedding look-up. 

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[t]
\centering
%  \vspace{-1em}
\caption{Policies trained on ALFRED Dataset and their architectures for each modality}
\label{tab:policiesarch}
\begin{tabular}{@{}llll@{}}
\toprule
Policies & Visual                                                                       & Language                       & Fusion                                                                   \\ \midrule
Baseline \cite{ALFRED20} & Frozen ResNet-18                                                             & Learned Embedding, Bi-LSTM     & LSTM                                                                     \\
MOCA \cite{Singh2021FactorizingPA}    & \begin{tabular}[c]{@{}l@{}}Frozen ResNet-18\\ + Dynamic Filters\end{tabular} & Learned Embedding, Bi-LSTM     & \begin{tabular}[c]{@{}l@{}}LSTM with \\ residual connection\end{tabular} \\
ET \cite{pashevich2021episodic}      & Frozen ResNet-50                                                             & Learned Embedding, Transformer & Transformer Encoder                                                      \\
HiTUT \cite{Zhang2021HierarchicalTL}   & Frozen MaskRCNN                                                              & Learned Embedding, FC, LN      & Transformer Encoder                                                  \\ \bottomrule
\end{tabular}
\vspace{-0.2em}
\end{table}
% EmBERT




 
%  provide spurious 
%  explanations and 
%  may not 
%  In cases where the attribution may 
%  this method depends on the underlying attribution methods such as IG. 
