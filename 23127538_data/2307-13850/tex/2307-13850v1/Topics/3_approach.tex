%  \vspace{-0.3em}
\section{Approach}
% Consider a linear model $f(x) = w_1 x_1 + w_2 x_2$. To explain which feature is more important for predicting the value of $f(x)$, we can compare their coefficients. If $w_1 = 1000$ and $w_2 = 0.01$, we can say that $x_1$ would be weighed more than $x_2$. 
Gradients are the general way of discussing the coefficient of the importance of a particular feature in deciding the output.
Using only weights or gradients as an attribution assumes that the values of $x_1$ and $x_2$ are of the same order, like image pixels. However, this doesn't hold in the case of multimodal policies when $x_1$ is an image embedding and $x_2$ is a language embedding.
\textit{Element-wise product of gradient into input}, also known as \textsc{grad $\odot$ input} \cite{Shrikumar2016NotJA}, provides global importance about the input feature in the dimensionality of the input feature itself. 
% in the model's output. 
We compute the attributions per modality with respect to the predicted action label output.
The differentiable policy $f: (V_t, a_{t-1}, L) \rightarrow a_t$ takes input as current visual frame $V_t$, previous action label embedding $a_{t-1}$, and the language instruction $L$. 
At the penultimate fusion layer, an intermediate representation is typically a vector or matrix. The attribution for each modality can be computed by \textsc{grad $\odot$ input}, where input is the feed-forward features computed at the penultimate fusion layer.
% taking norm of the element-wise product of gradient into . 
% \textsc{Grad $\odot$ input} attribution \cite{Shrikumar2016NotJA} is computed by taking the 
% % (signed)
% partial derivatives of the output with respect to the input and multiplying them with the input itself.
Let the policy neural network be $f$ that outputs a softmax distribution over the action to be taken. 
The input feature attribution $\alpha_i$ for $i^{th}$ dimension of vector $\mathbf{x}$ for the most likely predicted action is computed as 
$
\alpha_i = \frac{\partial (\max f(x))}{\partial x_i} \odot x_i.
$
% The number of dimensions for the norm differs as per the modality. There are different constituent components per modality, such as for visual: pixels, previous action: embedding representation in first layer of the network, and language: word token. 
For each modality represented as a vector, we need to pool the attribution per dimension to compute a scalar value attribution. Here are the implications of different pooling approaches:
\vspace{-0.7em}
\begin{itemize}
\setlength\itemsep{-0.2em}
    \item[{\makebox[2.5em][l]{$\textsc{L}^{\infty}$}}] gives the maximum magnitude value, independent of the dim $d$ (same as $\textsc{L}^\infty/d$).
    \item[{\makebox[2.5em][l]{$\textsc{L}^1$}}] provides the sum of all attribution magnitudes.  
    \item[{\makebox[2.5em][l]{$\textsc{L}^1/d$}}] is same as $\textsc{L}^1$, but invariant of the dim $d$.
    \item[{\makebox[2.5em][l]{$\textsc{L}^2$}}] diminishes the majority insignificant but non-zero attributions.
    \item[{\makebox[2.5em][l]{$\textsc{L}^2/d$}}] has same impact of $\textsc{L}^2$ but with undesirable scaling by dim $d$. 
\end{itemize}
\vspace{-0.5em}
% Different pooling techniques like sum, avergaing, L0, L1 and L2 norm. 
% L0 norm takes the maximum attribution in the vector. L1 takes the sum of the absolute values. L2 takes the squared root of sum of squared attributions. 

% Although L2 norm (euclidean distance) is not an appropriate distance measure between 2 points in higher dimensions,

To compute modality-specific attribution for latent vectors, 
% at inputs to the fusion layer,
$\textsc{L}^2$ is suitable to include the attributions from every dimension in the vector (unlike $\textsc{L}^{\infty}$) and reduce the impact of insignificant close to zero attributions (unlike $\textsc{L}^1$). We do not consider $\textsc{L}^{1}/d$ or $\textsc{L}^{2}/d$ as the attribution of modality should depend on the number of latent features allocated to highlight modeling biases. Note that for global attribution, we treat the feature extraction in each modality as a black box as we capture attributions at the penultimate fusion layer. 
 
% To give attributions per constituent unit per modality, such as attributions to a word from the attributions calculated at their embedding vector, $L0$ captures the notion of sparsity as it ignores the non-significant close to zero attribution of majority of the dimensions. 
% scale gracefully with the different dimension size unlike sum, averaging, or L1. For detailed comparison, refer Appendix \ref{app:norm_comparison}.  

% Comment 
% ---------
% In order to analyze attributions within a modality (modality-specific) analysis, we compute attributions at the first layer where gradients can be computed. Here, the attributions are in terms of the atomic constituents of a modality, for example, pixels in a visual frame (more details in \S\ref{subsec:visual_attr}. 
% % For visual input $V_t \in \mathbb{R}^{H\times W}$ where $H$ is the height and $W$ is the width of the image, we use XRAI (refer \S \ref{sec:related_work}) for computing attributions per pixel.  
% % $$e_{V_t}:=XRAI(V_t).$$
% In case the input is discrete, such as a word or previous action from a discrete set, the attribution is computed for each dimension of the embedding.
% More details on language attribution per word in \S \ref{subsec:lang_attr}.
%. ---- 
% Specifically for language attribution, we compute the gradient of the predicted output with respect to embeddings of goal and instruction word tokens. 
% As an embedding is n-dimensional vector, the gradient is also computed with respect to each dimension. To obtain an attribution value per word token, there are many ways to pool the vector of attribution to a scalar. Some of the pooling methods like averaging, max pooling, L2 norm, etc. resulted in washed out attributions per word. 
% % The 
% One of the ways to reduce the attribution ${a}_w$ of embedding size $d$, where $w$ is a word token, to a scalar is by choosing the value which has the maximum absolute value as the attribution. This value can be positive or negative attribution.
% % $\max abs(a_w)$
% \begin{align*}
%     W &:= \texttt{Embedding(Tokenize}(L_{[0:t]})) \\
%     e_W &:= \frac{\partial f}{\partial W} \odot W  \\
%     e_{W}[i] &:= \texttt{max } \texttt{abs}(e_W[i]),\quad\quad  \forall i \in n. 
% \end{align*}
% Details on visual and previous action attribution are in Appendix \ref{app:attribution_approach}
% | e_W \in \mathbb{R}^{n \times d}$ 
% \texttt{\# Language attribution per word token $w = W[i]$}
% \State $e_{w} = e_W[i][\texttt{argmax } \texttt{ abs}(e_W[i])]$
% \State $$ 

