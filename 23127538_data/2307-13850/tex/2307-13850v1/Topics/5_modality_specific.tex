
 \vspace{-0.1em}
\section{Modality-specific Attribution Analysis}
\begin{wrapfigure}{r}{0.37\textwidth}
    \vspace{-5em}
    \centering
    % Figure removed
    % % Figure removed
    % % Figure removed
    \caption{Focus center of Language attributions per episode length: Baseline~\protect{\cite{ALFRED20}} (yellow), MOCA~\protect{\cite{Singh2021FactorizingPA}} (purple) and ET~\protect{\cite{pashevich2021episodic}} (green) on validation seen dataset.}
    % \caption{Multimodal attributions in ALFRED policies baseline \cite{}: Language attribution is the highest, especially towards the end of the trajectory.}
    \label{fig:focus_center_lang_attr}
    \vspace{-1.5em}
\end{wrapfigure}
\paragraph{Does language attribution change depending on the steps taken in the episode?}
\label{subsec:lang_attr}
For language attribution, we compute the gradient of the predicted output with respect to embeddings of the word tokens in the language instruction, $M:= \texttt{Embedding(Tokenize}(L))$, where $M \in \mathbb{R}^{n\times d}$. 
We compute the attribution for the matrix $M$ by $\alpha_M := \frac{\partial f}{\partial M} \odot M $.
As word embedding is a $d$-dimensional vector, the gradient is also computed with respect to each feature dimension. For pooling, we use the maximum absolute value as the attribution as $\alpha_{w}:= \texttt{max } \texttt{abs}(\alpha_M[i]), \forall i \in n$.
% This value can be positive or negative attribution.
% $\max abs(a_w)$
% \begin{align*}
%     W &:= \texttt{Embedding(Tokenize}(L)) \\
%     \alpha_W &:= \frac{\partial f}{\partial W} \odot W  \\
%     \alpha_{W}[i] &:= \texttt{max } \texttt{abs}(\alpha_W[i]),\quad\quad  \forall i \in n. 
% \end{align*}
% Details on visual and previous action attribution are in Appendix \ref{app:attribution_approach}
\begin{wrapfigure}{l}{0.405\textwidth}
    \vspace{-0.5em}
    \centering
    % Figure removed
    \caption{Visual attributions for an interactive action with an object can indicate the focus of the baseline policy. Top-Left: Expert's interaction mask, Top-Right: Predicted interaction mask, Bottom-Left: visual observation in RGB, Bottom-Right: XRAI attributions.}
    \label{fig:xrai_eg_baseline}
    \vspace{-1em}
\end{wrapfigure}
To see how the focus center of the attribution for language instructions shifts in terms of the percentage of episode completion (Fig. \ref{fig:focus_center_lang_attr}\footnote{HiTUT is not shown in Fig.~\ref{fig:focus_center_lang_attr} because it doesn't consider the previous language instructions to make the current prediction which is different from how the other papers do it.}), we compute the word-index weighted attribution over all the word tokens in instruction per trajectory 
$c_{raw} = [(i+1)*\text{abs}(\alpha_w)] \forall i \in n$. To compare across variable length trajectories, we normalize it as 
    $ c_{scaled} = \sum c_{raw} / (\sum \text{abs}(\alpha_w) * n) $.
In the ideal case, the focus center of attribution on instruction should increase as the episode nears completion. 
 MOCA (blue) follows a linearly increasing trend in Fig.~\ref{fig:focus_center_lang_attr} and has high language attribution in Fig.~\ref{fig:main_results}. Baseline and ET do not show such a trend, which aligns with their low attribution to language in Fig.~\ref{fig:main_results}. 

% % Figure environment removed
\vspace{-1em}
\paragraph{Does visual attribution align with the predicted interaction mask?}
\label{subsec:visual_attr}

% \begin{algorithm}
% \caption{Algorithm to estimate attribution for action mask}
% \label{alg:action_mask_iou}
% \begin{algorithmic}
% \State $e_t \gets$  attribution on the current visual frame of size $H \times W$
% \State $m_t \gets$  expert's interact boolean mask of size $H \times W$
% \While{$i \leq n$}
%     \If{expert's action is an interact type action}
%     \State $e_{V_t} = $XRAI$(V_t)$
%     \State $e'_{V_t} = \frac{e_{V_t} - min(e_{V_t})}{max(e_{V_t}) - min(e_{V_t})}$
%     \State $I = e'_{V_t} * m_t$
%     \State iou $= \sum(I) / \sum(m_t)$
%     \EndIf
% \EndWhile
% \\
% \Return iou
% \end{algorithmic}
% \end{algorithm}
% % Figure environment removed

We analyze how visual attributions for interact action (PickObject) can indicate the focus of the interaction mask. We calculate the visual attribution with XRAI~\cite{Kapishnikov2019XRAIBA} as shown in Fig.~\ref{fig:xrai_eg_baseline} for Baseline policy. 
While the action label is predicted correctly by the policy, the intersection over the union of the mask is small. The simple visualization of the interaction mask reveals that a lot of objects (microwave, mug, and tomato) are selected. With XRAI attribution, we can qualitatively analyze the regions in terms of high (eg. parts of the microwave) and low (eg. tomato) attribution.  
Visual attribution can be used to analyze failure cases where the policy predicts the correct interact action but the wrong action mask. While the action mask is a heatmap on the visual image to determine the IOU with the groundtruth object mask, XRAI attribution provides some insight based on the regions, and not just pixels, about which parts the policy is focusing on while predicting the action label. 
Note that this is an interpretability and analysis tool to debug failure cases, and not a complete solution for mitigating errors in predicting interaction action masks.

%Algorithm \ref{alg:action_mask_iou} shows 


% We see that when the agent performs an interaction correctly, this correlates with a higher average attribution per pixel in the ground truth object mask.  Therefore, we hypothesize that model failures are generally caused by an inability to focus on the correct/relevant object. 
% Fig. \ref{} shows an improved average pixel attribution with the GT mask in newer(better) models.
% \VJ{future work}
% at hand, and plot overall, regardless of if the action prediction was correct, average pixel attribution within the GT mask for $n$ models and see a â€¦ general trend of improved attribution in newer (better) models
