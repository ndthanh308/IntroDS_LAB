Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task.
A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer.
To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. 
We present MAEA, a framework to compute global attributions per modality of any differentiable policy. 
In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
% % Pointers - 
% % what?
% Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. 
% % Multimodal inputs for embodied AI may contain highly complementary as well as redundant information for the task.
% A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer.  

% % Within ALFRED
% % If we understand how each modality contributes to the fusion process, we could leverage it to iteratively correct for dataset biases and modeling choices.
% % Although several gradient-based attribution techniques exist for unimodal data, 
% % most of them utilize structural priors of their data domain and
% % it is unclear how to extend them to multimodal policies.
% To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. 
% We present MAFEA, a framework to compute global attributions per modality of any differentiable policy. 
% In addition, we show how attributions enable lower-level behavior analysis in EAI policies through two example case studies on language and visual attributions.
% % to investigate multimodal EAI.
% % In addition, we provide language and visual modality-specific tools to investigate in multimodal EAI.

% % Depending heavily on one modality over another may not hamper the success in training data distribution but will affect the robustness of the learned policy. 
% % now what?
% % Multimodal inputs like vision and language are increasingly being used to train end-to-end embodied AI policies. However, it is unclear how to make sense of the predictions from such black-box models, thereby making it hard to understand the failure case scenarios. 
% % We investigate how  multimodal the policies are indeed that are trained for complex everyday tasks, in terms of global attribution per input modality and local inspection tool for understanding the failure scenarios. 
% % impact
% % Attention based attribution techniques ; 
% % But simpler models just have concatenation ;
% %  gradient based attribution technique grad x input 
% % Models trained on ALFRED task ;
% % This is not the only application ; 
% % Applied it to visualize attribution to language instruction 
% % and visual inputs.
% % We applied it for the basic cases to analyze language and visual attributions. ;


% % Objective of the work
% % Understand how multimodal the policies are for complex everyday tasks. We work with ALFRED tasks. 


% % Prior work in ALFRED task has shown that learning to navigate and interact for day-to-day household tasks is challenging. The performance of the models is far below the average human performance. To understand the underlying reasons for model's performance,
% % we utilize input feature attribution methods to analyze and compare the pitfalls of these models: where are these models learning the right things and what kind of dataset biases are they susceptible to?
% % % ere are they susceptible to the dataset biases?
% % We compare the policies in terms of which modality - language, visual or previous action taken - do the models tend to focus on. We observe that the state tracking trends of seq2seq models is intuitive to the subgoal progression while the transformer based models tend to focus on the entire instruction set at every timestep (probably to realize where to focus through the attention mechanism).
% % We also visualize the distribution of correctly predicted interaction actions with respect to the expert's interact mask and (related semantic categories).
% % We hope that our insights can enable better training datasets and algorithms for embodied AI tasks. 