% Figure environment removed
% % Figure environment removed
 \vspace{-0.1em}
\section{Global Attribution Analysis}
% We compute global attribution for visual, language and previous action.
% Tasks in ALFRED dataset require the agent's policy to reason over language instructions, visual input, and previous action information to take the optimal action.   
To understand how the multimodal attribution at the fusion layer is, we analyze the \textsc{grad $\odot$ input} with L2 norm as pooling to compute attributions. In Fig. \ref{fig:main_results}, we show the percentage of attribution given to language, vision, and previous action before and after training by different policies on ALFRED dataset. The attributions before training represent the implicit bias in the architecture of the model since it has not seen any data yet. The attributions after training, given the ones before training, represent the bias that is introduced by the data. This decoupling of interpretability of the biases introduced by model architecture and training data provide better equips the user to associate the bias between the two aspects. See Appendix \ref{appsec:global_exp_setup}.

Baseline and HiTUT have a balanced attribution after training over all three modalities, with a preference for visual features. 
While MOCA prefers language slightly over visual and previous action, ET strongly  prefers visual features over previous action and language. 
% MOCA has high attributions over the language instructions as compared to the visual frame embedding and previous actions.
%Unlike E.T. which has the same dimensional embeddings for each modality, MOCA has 100 dim embedding per word, 100 embedding for just previous action, and 512x7x7 embedding for the current frame. 
% to the most common multimodal inputs: language instructions,
% % \footnote{We ignore the goal as MOCA and ET treat the goal separately}
% visual frame and previous action are taken. 
% For example, when a policy predicts an action to pick up the tomato when sub-instruction refers to pick up an bowl, attribution analysis can reveal that the visual attribution is on the tomato whereas the language attribution does not focus on the word `bowl'. 
% , which means that the overall attribution analysis will be biased in random not meaningful states if the policy is stuck, for example facing a wall. 
At initialization, Baseline and MOCA have the majority of the \% attribution on the previous action. ET model starts with the majority of the attribution over previous action and visual frame, and very little focus on the language. HiTUT has a balanced attribution before training, which does not inherently induce modality bias. 
% We compute these attributions based on the expert trajectories to keep the analysis on the more relevant input states within the training distribution. 
More details in Appendix \ref{app:marker}.
%If the likelihood of the expert's action is small, this will result in lower and not meaningful attribution at the latent layer. 
%On the other hand, qualitative attributions with respect to the predicted action allow us to analyze dataset biases. 

% App.~\ref{app:multimodal_attr}. \VJ{Comment on modeling biases}
% and dataset containing .

% Baseline shows a higher \% attribution for visual features, followed by previous action and least attribution to language, but absolute values of all the attributions are close
% We compare the multimodal attribution in terms of interact vs non-interact action \ref{fig:l2_attr}, and how they may change with respect to \ref{fig:multimodal_attr_baseline, fig:multimodal_attr_moca, fig:multimodal_attr_et}.   