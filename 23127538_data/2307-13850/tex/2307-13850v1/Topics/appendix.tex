\section{More Multimodal Attribution Analysis} 
\label{app:multimodal_attr}
We compare the multimodal attribution in terms of interact vs non-interact action (Fig.~\ref{fig:l2_attr}), and how they may change with respect to  \% episode completion (Fig.~\ref{fig:attr_with_episode_comp}). We also compare the biases in attributions to the modalities because of choices in model architecture and the way training is performed, i.e., dataset and learning techniques.

\subsection{Interact vs non-interact action attributions}

% Figure environment removed

We analyze the L2 norm of the attribution over all the possible input-output pairs in valid seen and unseen trajectories and bin them in terms of interact (like pick, place, open, close, etc.) and non-interact (like move ahead, turn left, turn right, etc) actions in Fig. \ref{fig:l2_attr}. This is helpful to visualize how the attributions differ with (i) different types of actions taken and (ii) correct and incorrect predictions. We observe that the attribution patterns do not differ significantly in interact vs non-interact action. 
% \VJ{Discussion on the plots}

\subsection{Attribution with respect to \% episode completion}
To compare the overall attribution over modality with respect to where the action is taken in terms of \% episode length, 
% we take the L2 norm to reduce the attribution vector.
we plot the attributions for the models trained on ALFRED task in Fig \ref{fig:multimodal_attr_baseline}, \ref{fig:multimodal_attr_moca}, \ref{fig:multimodal_attr_et} and 
\ref{fig:multimodal_attr_hitut}
to visualize how the attributions change during the episode for baseline, MOCA, ET and HiTUT respectively. In an ideal case, we would expect more attribution on visual and previous action features in the exploration phase when starting in a new environment, and more on language instruction towards the later part of the episode completion phase. But in current models, we observe that the attribution pattern remains consistent over the episode -- indicating a possible need for improvement in modeling choices and training procedures. 

The baseline has high attribution for visual features, especially initially and toward the end of the episode. Attribution for previous action and language also increases near episode completion steps. 
MOCA has high attributions over the language instructions as compared to the visual frame embedding and previous actions. Previous action attribution closely overlaps with language attribution. Visual attribution increase towards the episode completion. 
ET shows significantly high visual attributions that increase and then plateau towards episode completion. Previous action and language seem to have very low attribution compared to visual features.
HiTUT, which is the best-performing model of all, shows sufficiently balanced attribution among all three modalities, with occasional high peaks.
% E.T. has the same dimensional embeddings for each modality, MOCA has 100 dim embedding per word, 100 embedding for just previous action, and 512x7x7 embedding for the current frame. 
% to the most common multimodal inputs: language instructions,
% Left side plots are for validation seen and right side plots are for
% validation
% unseen episodes. 

% Figure environment removed
% % Figure environment removed


% \label{app:attribution_approach}


% For visual input $V_t \in \mathbb{R}^{H\times W}$ where $H$ is the height and $W$ is the width of the image, we use XRAI (refer \S \ref{sec:related_work}) for computing attributions per pixel.  
% $$e_{V_t}:=XRAI(V_t).$$


% For previous action $a_{t-1} \in \mathbb{R}^{d}$, where $d$ is the embedding dimension of the action look-up table, we compute grad $\times$ input per dimension in its embedding representation at the first layer of the neural network. 
% $$e_{a_{t-1}} :=\frac{\partial f}{\partial a_{t-1}} \odot a_{t-1}.$$ 

\section{What is the ideal target to estimate multimodal attribution?}
\label{app:marker}
We compute the attribution of each modality/modality-specific feature with respect to the action predicted by the agent, that is $f$ is a scalar value for the most likely action. Other values for function $f$ can be other scalar values such as the loss used during training or the action taken by the expert at that particular timestep, or tensors, like the entire action space (since a policy would return logits for all actions in most cases) or the predicted interaction masks. 
% We defend our choice of picking the policy's predicted best action as a good candidate by pointing out the flaws in them or alternate interpretations they would result in.
Taking attributions with respect to the loss tells us what the agent should be looking at to better imitate the expert. But in terms of what features the agent is using to select an action gets conflated with the gradient of the loss with respect to that action. 
% Also, considering any training loss as a reference would be problematic because the gradients backpropagated are usually high in the initial phases and then slowly go down as the training proceeds. This makes the attributions a function of how futher we are into training which is dictated by multiple factors like initializations, learning rate schedules, batch sizes, and model architectures. This makes it almost impossible to make a fair comparison across models.
Taking attributions with respect to the expert action would result in an interpretation that would convey the attributions that would have led to the agent taking the expert action.
Considering all the logits corresponding to the action choices makes sense for our purposes but there is a concern that not all model architectures allow us to backpropagate to inputs from all the possible discrete actions. Predicted interaction masks are a good indication of what the model is looking
at while making an interact action. But this is a vision-biased attribution and also assumes that the model computes an interaction mask in the first place. Therefore, gradient with respect to the action taken is a better choice to interpret the policy decisions and it intuitively translates to the objective of filtering for inputs that gave rise to a certain decision.

\section{Global Attribution Experiment Setup}
\label{appsec:global_exp_setup}
The attributions prior to training are used to analyze modeling biases. 
To get the attributions before training, we use randomly initialized policies over 5 seeds.
For the trained model's attributions, we use pretrained checkpoints. Both attributions are evaluated by a sample of 100 trajectories from 820 in validation-seen data.

We compute these attributions based on the expert trajectories to keep the analysis on the more relevant input states within the training distribution. 
We do not analyze the rollout trajectories because most policies have poor success rates (SR) and are often stuck in irrelevant out-of-distribution states, such as facing a wall.
We take the gradients with respect to the most likely action predicted by the policy, and not the expert's chosen action. 

\section{Limitations of the attribution methods}
% Consider a linear model $f(x) = w_1 x_1 + w_2 x_2$. To explain which feature is more important for predicting the value of $f(x)$, we can compare their coefficients. If $w_1 = 1000$ and $w_2 = 0.01$, we can say that $x_1$ would be weighed more than $x_2$. 
Attribution methods might be sensitive to the choice of pooling used. 
Here we defend our formulation, 
why L2 norm - because we want to compare policies that may use different dimension embeddings to represent an input. For example, policy 1 uses $d_1=128$ dim to represent word token embedding, policy 2 uses $d_2=768$ dim to represent the same. 
There are two factors here: first, in higher dimensions, the L1 norm gives a better distance estimate than the L2 norm. But, by the virtue of how the neural networks are initialized for stable training, the higher dimension will have a lower value per dimension. This means that the absolute attribution value in policy 1 should be scaled by its embedding size for a fair comparison to policy 2. 
