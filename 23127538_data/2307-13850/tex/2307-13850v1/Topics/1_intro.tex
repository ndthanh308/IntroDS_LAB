\begin{wrapfigure}{r}{0.3505\textwidth} % % Figure environment removed
% \vspace{-0.9em}

\section{Introduction}


%--------------------------------
% why attribution for multimodal polcies?
%--------------------------------


%--------------------------------
% introducing embodied agents and all multimodality
%--------------------------------


Embodied AI policies have achieved remarkable success in  simulated 3d environments \cite{chaplot2020learning, chaplot2020object, eqa_modular, Yang2019VisualSN} and physical robots \cite{Sharif2021EndtoendGP, fu2021coupling}. Analogous to the success of end-to-end learning for image classification \cite{krizhevsky2017imagenet} and language modeling \cite{dale2021gpt}, recent works on end-to-end embodied policies \cite{ALFRED20} attempt to solve complex everyday tasks, like `making a cup of coffee' or `throwing a chilled tomato slice in trash' (as shown in Fig.~\ref{fig:xrai_expert_tn_open_vs_rotright}). 
Such policies often fail for reasons that are poorly understood. Interpreting the decision-making in end-to-end policies is important to enable trustworthy deployment and handle failure case scenarios.

Learning an embodied AI policy typically involves function $f$ mapping the observation/state $O_t$ to action $a_t$. In many environments ~\cite{Beattie2016DeepMindL, openaigym}, the observation can just be the visual frame at current timestep $O_t = \{V_t\}$. 
However, complex tasks can hardly be solved with just a single type of observation. 
% Whenever $|O_t| > 1$, multimodal inputs are fused within the policy.
% Here the task success is usually defined by the environment or imitation and is stationary i.e., does not change with time or context. 
In Atari \cite{Mnih2013PlayingAW}, the policy operates on last 4 visual frames: $O_t = \{V_i\}_{t-3}^t$.
% \{V_{t-3}, V_{t-2}, V_{t-1}, V_{t}\}$.  
% Here the most recent frame is usually the most important for the task. 
For long horizon tasks in a large sparse maze environment \cite{gym_minigrid}, inputs may include the previous action: $O_t= \{V_t, a_{t-1}\}$, as it will be likely that the following action is to `move forward' if the previous action was the same. Real-world robotics navigation relies on visual frame and proprioception \cite{fu2021coupling} to successfully navigate undetected obstacles. In this work, we consider a mobile robot that takes natural language instructions and interacts with objects in simulated household environments~\cite{ALFRED20}. 
% such as in ALFRED \cite{ALFRED20} dataset. 
Such a robot needs to predict the sequence of actions that would complete the task, given the previous action $a_{t-1}$, natural language instructions $L$, and ego-centric vision $V_t$, that is the observation $O_t= \{V_t, a_{t-1}, L\}$.
% As physical agents (e.g., robots) start to emerge as
% our assistants and partners, it has become increasingly important to empower these agents with an
% ability to learn new tasks by following human language instructions
While many attempts have been made to train policies for this task \cite{ALFRED20,Singh2021FactorizingPA,pashevich2021episodic,min2021film}, the existing best performance is far below that of an average human~\cite{ALFRED20}. 
% , given the language goal and instructions.
% Recent works \cite{} evaluate these agents on the overall task goal completion success rate (SR) and that weighted by expert's path length (PLWSR) over seen and unseen tasks, and have reported a huge gap in the performance of learning algorithms and humans at these tasks. 
% To understand why policies fail on ALFRED tasks, we 
% compute and visualize the attributions
% % over the visual, language, previous action and state 
% of the policy network for the most likely predicted action, given the expert's trajectory inputs at current timestep. 
To this end, we investigate how multimodal a policy is, in terms of attribution given to visual, language, and previous actions.  
Aggregated analysis per modality provides insights into the effect of modeling choices, such as at what layer the fusion happens, what sub-network is used to process each modality separately, and how the representation per modality affects the fusion process. Further, attributions provide an introspection technique to rank the contributions from each modality in a decision, as well as analyze modeling and dataset biases. 
% and the expert's action.
% % Unnecessary details??
% Expert's action is provided at each instance based on PDDL planning in the simulated environment. 
% % The attributions demonstrate where the policy focuses 
% The input to the policy is word embedding of goal and instructions, the current frame, the previous action and the latent state representation. 
% The output of the agent's policy is one of 13 possible  actions\footnote{Note that two of --pad-- and --seg-- in action vocabulary are dummy values.} with 5 navigation and 7 interaction based actions. 
% The main research questions are: 
% (i) For the action label and mask prediction, what percentage of attribution is per modality?
% (ii) In what contexts, one modality is favored over the other?
Our main contribution is to propose Multimodal Attribution for Embodied AI, MAEA, for (a) global analysis in terms of the percentage of average attribution per modality, as well as (b) modality-specific local case studies. 
Our work does not intend to comment on the kind of attributions useful for understanding multimodal policy but only provides a tool to better understand the modality attributions in any model architecture setting. 
% let the user discern the kind of attribution that works best for their use case.  
% We demonstrate our findings with policies trained for ALFRED task. 
