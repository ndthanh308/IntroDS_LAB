\section{DEFINITION AND PRELIMINARY}\label{sec:def}
In this section, we formulate the problem of multi-interest Multi-round Conversational Recommendation (MMCR) \cite{zhang2022multiple}.

Specifically, we define the set of items $\mathcal{V}$, attributes $\mathcal{P}$, and attributes types $\mathcal{C}$. Each item $v \in \mathcal{V}$ is associated with a set of attributes  $\mathcal{P}_v \subseteq \mathcal{P}$ and each attribute $p$ has its corresponding type $c_p \in \mathcal{C}$. In each episode, there exists an item set $\mathcal{V}_u$ that is acceptable for the user. Then CRS screens out candidate items $\mathcal{V}_{cand} \subseteq \mathcal{V}$ that contains the user-preferred attribute $p_0$ and candidate attributes $\mathcal{P}_{cand} \subseteq \mathcal{P}$ that are associated to the candidate items. Then in each turn $t$ ($t= 1, 2, \cdots, T$; $T$ is the max turn of the session), the CRS can either \emph{ask} $K_p$ attribute $\tilde{\mathcal{P}}_c \in \mathcal{P}_{cand}$ corresponding to the same attribute type $c$, or \emph{recommend} $K_v$ items  $\tilde{\mathcal{V}} \in \mathcal{V}_{cand}$:
\begin{itemize}
    \item If the CRS chooses to \emph{ask}, the user gives feedback according to whether $\mathcal{P}^{*}_c$ is associated with one of the items in the target item set $\mathcal{V}_u$.
    \item If the CRS chooses to \emph{recommend}, the user chooses to accept or not according to whether one of the items in the target item set $\mathcal{V}_u$ is listed in the recommended items $\tilde{\mathcal{V}}$.
\end{itemize}
The session of MMCR terminates if the user accepts the recommended items or leaves impatiently when the max turn accesses. 



% \section{METHODOLOGIES}
% motivation of our technologies


\section{Framework}
In this section, we propose a novel Multi-view Hypergraph Contrastive Policy Learning (MHCPL) illustrated in Figure \ref{fig:model} that learns user preferences from the hypergraph integrating the interactive conversation, item knowledge and social information, and accordingly chooses actions. The Markov Decision Process (MDP) \cite{sutton2018reinforcement} formulation of our framework contains four components: multi-view user preference modeling, action, transition, and reward.

\subsection{Multi-view User Preference Modeling}
We first encode the state $s_t$, which contains the interactive conversation information $\mathcal{I}_u$ between the user and CRS, and the social information $\mathcal{F}_u$ that helps learn user preferences:
    \begin{equation}
    s_t=[\mathcal{I}^{(t)}_u, \mathcal{F}^{(t)}_u],
    \label{eq:state}
    \end{equation}
where $\mathcal{I}^{(t)}_u=[\mathcal{P}^{(t)}_{acc}, \mathcal{P}^{(t)}_{rej}, \mathcal{V}^{(t)}_{rej}, \mathcal{P}^{(t)}_{cand}, \mathcal{V}^{(t)}_{cand}]$ records the interactive history, and $\mathcal{F}^{(t)}_u$ denotes user's friends who have preferred items satisfying the interactive history $\mathcal{I}^{(t)}_u$, which is updated by:
 \begin{equation}
    \begin{aligned}
    \mathcal{F}_{u}^{(t)}=\{f \mid f \in \mathcal{F}_{u} & \text { and } \mathcal{V}^{(t)}_{f} \neq \emptyset\},
    \end{aligned}
    \label{eq:action}
    \end{equation}
where $\mathcal{F}_{u}$ denotes the friends of the user,  $\mathcal{V}^{(t)}_{f}=\mathcal{V}_{f} \cap \mathcal{V}_{cand}^{(t)}$ indicates the set of items that are acceptable for the friend $f$ and satisfy the interactive history. To this end, we build a dynamic hypergraph that integrates the interactive conversation, item knowledge, and social information to learn the user preference representation. Moreover, we develop a hypergraph-based state encoder to learn user preferences with multiplex relations from different views.
\subsection{Action}    
 According to the state $s_t$, the CRS agent chooses an action $a_t$ from the action space $\mathcal{A}_t$. The action space $\mathcal{A}_t$ contains candidate attributes $\mathcal{P}^{(t)}_{cand}$ and candidate items $\mathcal{V}^{(t)}_{cand}$, which are updated by:
    \begin{equation}
    %\mathcal{V}^{(t)}_{cand}=\mathcal{V}_{\mathcal{P}^{(t)}_{acc}}\setminus\mathcal{V}^{(t)}_{rej}
    \begin{aligned}
    \mathcal{V}_{\text {cand }}^{(t)}=\left\{v \mid v \in \mathcal{V}_{p_{0}}-\mathcal{V}_{r e j}^{(t)}\right.& \text { and } \mathcal{P}_{v} \cap \mathcal{P}_{acc}^{(t)} \neq \emptyset \\
    &\text { and } \left.\mathcal{P}_{v} \cap \mathcal{P}_{r e j}^{(t)}=\emptyset\right\},
    \end{aligned}
    \label{eq:action}
    \end{equation}
    
    \begin{equation}
    \mathcal{P}^{(t)}_{cand}=\mathcal{P}_{\mathcal{V}^{(t)}_{cand}}-\mathcal{P}^{(t)}_{acc}\cup\mathcal{P}^{(t)}_{rej},
    \label{eq:action2}
    \end{equation}
    where $\mathcal{V}_{p_{0}}$ denotes the items that satisfy the initial attribute $p_0$ of the user and $\mathcal{P}_{\mathcal{V}^{(t)}_{cand}}$ indicates attributes that belong to at least one of the candidate items $\mathcal{V}^{(t)}_{cand}$. When the CRS agent chooses to recommend, the agent chooses the top-K items $\tilde{\mathcal{V}}^{(t)}$ from $\mathcal{A}_t$. If the CRS agent decides to consult, the agent chooses $K_a$ attributes $\tilde{\mathcal{P}}^{(t)}_c$ that belong to the same attribute type $c$ from $\mathcal{A}_t$.
    
\subsection{Transition}  
After the CRS agent chooses the action $a_t$, the state $s_t$ will transition to the next state $s_{t+1}$. Specifically, if the agent chooses to consult, the attribute the user accepts and rejects in the current turn can be defined as $\mathcal{P}^{(t)}_{cur\_acc}$ and $\mathcal{P}^{(t)}_{cur\_rej}$. Then the state is updated by $\mathcal{P}^{(t+1)}_{acc}=\mathcal{P}^{(t)}_{acc}\cup\mathcal{P}^{(t)}_{cur\_acc}$ and $\mathcal{P}^{(t+1)}_{rej}=\mathcal{P}^{(t)}_{rej}\cup\mathcal{P}^{(t)}_{cur\_rej}$. When the agent chooses to recommend items $\tilde{\mathcal{ V}}^{(t)}$ and the user rejects all the items, the state is updated by $\mathcal{V}^{(t+1)}_{rej}=\mathcal{V}^{(t)}_{rej}\cup\tilde{\mathcal{ V}}^{(t)}$. Otherwise, this session ends with success. 
    
\subsection{Reward}
In this work, we design five kinds of rewards following previous works \cite{zhang2022multiple}: (1) $r_{rec\_suc}$, a strong reward when recommending successfully; (2) $r_{rec\_fail}$, a weak penalty when the user rejects the recommended items; (3) $r_{ask\_suc}$, a weak reward when the user accepts the asked attributes; (4) $r_{ask\_fail}$, a weak penalty when the user rejects the asked attributes; (5) $r_{quit}$, a strong penalty when the session quits without success. The reward on the multi-choice question is designed as $r_t=\sum_{\mathcal{P}_{cur_{-}acc}^{(t)}}r_{ask_{-}suc}+\sum_{\mathcal{P}_{cur_rej}^{(t)}}r_{ask_{-}rej}$.

\section{Multi-view Hypergraph Contrastive Policy Learning}
In this section, we detail the design of the Multi-view Hypergraph Contrastive Policy Learning (MHCPL) \ref{fig:model}. To model the dynamic user preferences, we build a hypergraph with three types of multiplex relations from different views to integrate information from the interactive conversation, item knowledge, and social information. To comprehensively learn user preferences, we develop a hypergraph-based state encoder, that captures the graphical and sequential structure in the dynamic hypergraph and propose a cross-view contrastive learning module to maintain the inherent characteristics and the correlation of user preferences from different views. Moreover, we develop an action decision policy to decide the next action based on the learned dynamic user preferences.

\subsection{Multi-view Hypergraph Construction}
% Figure environment removed
As illustrated in Figure \ref{fig:model}, we model the user preference at timestep $t$ with a multi-view dynamic hypergraph which can be  formulated as $\mathcal{G}^{(t)}_u=(\mathcal{N}^{(t)}, \mathcal{H}^{(t)}, \mathbf{A}^{(t)})$, including: (1) a node set $\mathcal{N}^{(t)}=\{u\}\cup\mathcal{P}^{(t)}_{rej}\cup\mathcal{P}^{(t)}_{acc}\cup\mathcal{F}^{(t)}_{u}\cup\mathcal{V}_{p_0}$, where $\mathcal{V}_{p_0}$ indicates the items satisfying the initial attribute $p_0$ of the user $u$, and $\mathcal{F}^{(t)}_{u}$ denotes the filtered friends that have preferring items that satisfy the interactive history $\mathcal{I}^{(t)}_u$; (2) a hyperedge set $\mathcal{H}^{(t)}=\mathcal{H}^{(t)}_{like}\cup\mathcal{H}^{(t)}_{dis}\cup\mathcal{H}^{(t)}_{f}$,  where $\mathcal{H}^{(t)}_{like}$ denotes the user like items that satisfy the attribute (\emph{Like View}), $\mathcal{H}^{(t)}_{dis}$ indicates the user dislike items that satisfy the attribute (\emph{Dislike View}), and $\mathcal{H}^{(t)}_{f}$ denotes the user shares preferences to the items with the friend (\emph{Social View}). Each hyperedge $h \in \mathcal{H}^{(t)}$ is corresponding to an attribute $p_h$ or friend $f_h$; (3) a $|\mathcal{N}^{(t)}|\times |\mathcal{H}^{(t)}|$ adjacent matrix $\mathbf{A}^{(t)}$ which denotes the weighted edge between each node and hyperedge, with entries denoted as:
    \begin{equation}
    A_{i, j}^{(t)}= \begin{cases}
    1, & \text { if } n_{i}=u, h_j \in \mathcal{H}^{(t)}_{like} \cup \mathcal{H}^{(t)}_{f} \\
    -1, & \text { if } n_{i}=u, h_j \in \mathcal{H}^{(t)}_{dis} \\
    \frac{1}{|\mathcal{V}^{(t)}_{h_j}|}, & \text { if } n_{i}\in \mathcal{V}^{(t)}_{h_j}, h_{j}\in \mathcal{H}^{(t)}_{like} \cup \mathcal{H}^{(t)}_{dis} \\
    \frac{1}{|\mathcal{V}^{(t)}_{h_j}|}, & \text { if } n_{i}\in \mathcal{V}^{(t)}_{h_j}, h_{j}\in \mathcal{H}^{(t)}_{f} \\
    1, & \text { if } h_{j}\in \mathcal{H}^{(t)}_{like} \cup \mathcal{H}^{(t)}_{dis}, n_{i}= p_{h_j} \\ 
    1, & \text { if } h_{j}\in \mathcal{H}^{(t)}_{f}, n_{i}= f_{h_j} \\
    0, & \text { otherwise }\end{cases},
    \label{eq:hyperedge}
    \end{equation}
where $\mathcal{V}^{(t)}_{h_j}$ denotes items connected to the hyperedge $h_j$. Specifically, when $h_j \in \mathcal{H}^{(t)}_{like} \cup \mathcal{H}^{(t)}_{dis}$, $\mathcal{V}^{(t)}_{h_j}$ means items that satisfy the corresponding attribute $p_{h_j}$. And when $h_j \in \mathcal{H}^{(t)}_{f}$, it means the friend $f_{h_j}$'s acceptable items that satisfy the interactive history $\mathcal{I}^{(t)}_u$. We filter out noises in friends' acceptable items with the interactive history to help learn the user's current dynamic preferences. 


\subsection{Hypergraph-based State Encoder}
\subsubsection{Hypergraph Message Passing Paradigm}
Motivated by the strength of hypergraph \cite{feng2019hypergraph, xia2022hypergraph} for generalizing the concept of edge to connect more than two nodes, we endow our MHCPL to capture multiplex relations under a hypergraph message passing architecture, where the hyperedges are treated as intermediate hubs for message passing across different nodes without the hop limitation. The formal representation of our hypergraph message passing is formulated as:
\begin{equation}
    \mathbf{\Gamma}=\text{ReLU}(\mathbf{A} \cdot \mathbf{H})=\text{ReLU}\left(\mathbf{A} \cdot \mathbf{A}^{\top} \cdot \mathbf{E}\right),
    \label{eq:hyperagg1}
    \end{equation}
where $\mathbf{E} \in \mathbb{R}^{|\mathcal{N}^{(t)}|\times d}$ denotes the initial embedding of nodes $\mathcal{N}^{(t)}$ in the hypergraph, $\mathbf{H} \in \mathbb{R}^{|\mathcal{H}^{(t)}|\times d}$ indicates the hyperedge representations aggregated from the node representations, and $\text{ReLU}(\cdot)$ denotes the LeakyReLU mapping. $\mathbf{\Gamma}$ denotes the hyper embedding of the nodes in the hypergraph representation space. With the help of hypergraph message passing, our MHCPL is capable to capture the multiplex collaborative relations that specify the attribute/friend that motivates/discourages the user's interest in the items.

\subsubsection{Hierarchical Hypergraph State Encoder} During the conversation, the hyperedges are successively generated when the user accepts or rejects the asked attribute. Moreover, the higher-level interactions between different hyperedges are also important in learning user preferences. Although the aforementioned hypergraph message passing paradigm is capable to capture the multiplex relations, it fails to model sequential information and hyperedge-wise feature interactions.  Inspired by the success of the Transformer encoder \cite{vaswani2017attention} in capturing sequential information and feature interactions, we employ the Transformer encoder to realize high-level hyperedge-wise message passing. Specifically, with the representation of hyperedges $\boldsymbol{H}$ that aggregate information from neighbor nodes, higher-level hypergraph layers further pass messages through the interactions between hyperedges under the same view:

%To deal with this problem, we further enhance our hypergraph neural architectures with higher-level hyperedge-wise feature interactions. Specifically, with the representation of hyperedges $\boldsymbol{H}$ that aggregate information from neighbor nodes, higher-level hypergraph layers further pass messages through the interactions between hyperedges under the same view as:
\begin{equation}
\begin{aligned}
    \bar{\mathbf{H}}=\psi^l(\boldsymbol{H}), \boldsymbol{H}=\mathbf{A}^{\top} \mathbf{E},
    \end{aligned}
    \label{eq:hyperagg2}
    \end{equation}
where $\mathbf{E} \in \mathbb{R}^{|\mathcal{N}^{(t)}|\times d}$ denotes the initial embedding of nodes $\mathcal{N}^{(t)}$ in the hypergraph. $\psi^l(\cdot)$ indicates the high-level hypergraph layers. $l$ denotes the layer number of high-level hypergraph layers. Hyperedges $\mathcal{H}^{(t)}_{o}$ of the same view $o \in \{like, dis, f\}$ are successively connected according to their generation order in the interactive conversation. To realize this, we apply the Transformer encoder ${MHSA}_o(\cdot)$ on hyperedges $\mathcal{H}^{(t)}_{o}$ of each view $o$ as:
\begin{equation}
\begin{aligned}
    \psi(\boldsymbol{H}^l_o)=\text{MHSA}_o(\boldsymbol{H}^{l-1}_{o}, \boldsymbol{H}^{l-1}_{o}, \boldsymbol{H}^{l-1}_{o}).
    \end{aligned}
    \label{eq:hyperagg3}
    \end{equation}
After the high-level hyperedge message passing, we aggregate the information from hyperedges to refine the node representations as:
\begin{equation}
\begin{aligned}
    \mathbf{\Gamma}_l=\text{ReLU}(\mathbf{A} \cdot \bar{\mathbf{H}})=\text{ReLU}\left(\mathcal{H} \cdot \psi^l\left(\mathbf{A}^{\top} \cdot \mathbf{E}\right)\right),
    \end{aligned}
    \label{eq:hyperagg4}
    \end{equation}
where $\psi^l$ denotes $l$ high-level hypergraph layers.
The hyper representation from different layers of the user is summed to get the representation of state $s_t$:
\begin{equation}
    \mathbf{q}_t=\sum_{l}\mathbf{\Gamma}_l(u)
\label{eq:hyperagg5}
\end{equation}
%In order to incorporate the hyper-connectivity information and sequential information into the representation of the state $s_t$, we employ transformer \cite{vaswani2017attention} and obtain the representation of state $s_t$ as:
%\begin{equation}
%\begin{aligned}
%    f_{\theta_{S}}\left(s_{t}\right)=\boldsymbol{N}^l_u\oplus\text{Mean}(\boldsymbol{H}^*),
%    \end{aligned}
%    \label{eq:hyperstate}
%    \end{equation}
%where $\boldsymbol{H}^{l}_{acc} \in \mathbb{R}^{|\mathcal{P}^{(t)}_{acc}|\times d}$ denotes the representation of the hyperedges corresponding to the accepted attribute instances in the historical interactions between the user and the CRS agent. $MHSA(\cdot)$ denotes the multi-head attention in transformer and $Mean(\cdot)$ denotes the mean pooing over the sequence. The presentation of the state $f_{\theta_{S}}\left(s_{t}\right)$ is obtained by concatenating $\oplus$ the representation of the user $\boldsymbol{N}^l_u$ and the interactive sequence.

\subsection{Cross-view Contrastive Learning}
Different types of multiplex relations present user preferences from various views (\ie \emph{Like View}, \emph{Dislike View}, \emph{Social View}). Actually, it is still non-trivial to sufficiently integrate user preferences from different views, since it might obscure the inherent characteristics of preference distributions from different views and the correlation between them.  Specifically, the user preferences from the same view should be more similar than those from different views. Also, user preferences from \emph{Like View} should be similar to \emph{Social View} while different from \emph{Dislike View}.
To capture these two correlations and better integrate user  preferences from different views, we develop cross-view contrastive learning based on InfoNCE\cite{oord2018representation} as:  
\begin{equation}
\begin{array}{ll}
    \mathcal{L}^{SSL}= \\
    -\sum\limits_{o}\sum\limits_{i \in \mathcal{H}_o }\log \frac{\sum\limits_{i^+ \in \mathcal{H}_o }\text{exp}({\operatorname{s}\left({{\mathbf{H}_i}}, {{\mathbf{H}_{i^+}} }\right) / \tau })}{ \underbrace{\sum\limits_{i^+ \in \mathcal{H}_o }\text{exp}({ \operatorname{s}\left({ {\mathbf{H}_i}}, {{\mathbf{H}_{i^+}}}\right) / \tau })}_{\text {positive pairs }} +
    \underbrace{\sum_{i^- \in \mathcal{H}-\mathcal{H}_o}\text{exp}({\operatorname{s}\left({{\mathbf{H}_i}}, {{\mathbf{H}_{i^-}}} \right)/ \tau }) }_{\text {negative pairs }}}\\
    -\sum\limits_{o}\sum\limits_{i \in \mathcal{H}_o }\log \frac{\sum\limits_{i^+ \in \mathcal{H}_{o^+} }\text{exp}({\operatorname{s}\left({{\mathbf{H}_i}}, {{\mathbf{H}_{i^+}} }\right) / \tau })}{ \underbrace{\sum\limits_{i^+ \in \mathcal{H}_{o^+} }\text{exp}({ \operatorname{s}\left({ {\mathbf{H}_i}}, {{\mathbf{H}_{i^+}}}\right) / \tau })}_{\text {positive pairs }} +
    \underbrace{\sum_{i^- \in \mathcal{H}_{o^-}}\text{exp}({\operatorname{s}\left({{\mathbf{H}_i}}, {{\mathbf{H}_{i^-}}} \right)/ \tau }) }_{\text {negative pairs }}}, 
    \end{array}
\label{eq:loss}
\end{equation}
where $o \in \{like, dis, f\}$ denotes three views, $\mathcal{H}=\mathcal{H}_{like}\cup\mathcal{H}_{dis}\cup\mathcal{H}_{f}$ indicates the set of hyperedges, $\mathbf{H}$ denotes the representations of hyperedges, and $s(\cdot)$ is the cosine similarity function. In Eq.\ref{eq:loss}, the first term is designed to maintain the intrinsic characteristics of user preferences from each view, which treats the hyperedges of the same view as positive pairs, while the different-view hyperedges as negative pairs. The second term of Eq.\ref{eq:loss} is designed to maintain the correlation of user preferences from different views, where the hyperedges in $\mathcal{H}_{like}$ and $\mathcal{H}_{f}$ are treated as positive pairs to each other, while treated as negative pairs with the hyperedges in $\mathcal{H}_{dis}$.


\subsection{Action Decision Policy Learning}
A large action search space reduces the efficiency of policy learning. Following \cite{deng2021unified,zhang2022multiple}, we select top-$\mathcal{K}_v$ candidate items and top-$\mathcal{K}_p$ candidate attributes to form the action space $\mathcal{A}_t$. To this end, we develop a multi-view action selection strategy, which selects items/attributes according to user preferences from three views. Specifically, we rank them as:
\begin{equation}
w_v^{(t)}=\sigma\left(\mathbf{e}_u^T \mathbf{e}_v+\sum_{p \in \mathcal{P}_{acc}^{(t)}} \mathbf{e}_v^T \mathbf{e}_{p}+\sum_{f \in \mathcal{F}_u^{(t)}} \mathbf{e}_v^T \mathbf{\tilde{e}}_f
-\sum_{p \in \mathcal{P}_{rej}^{\prime(t)}} \mathbf{e}_v^T \mathbf{e}_{p}\right),
\label{eq:itemscore}
\end{equation}

\begin{equation}
w_p^{(t)}=\sigma\left(\mathbf{e}_u^T \mathbf{e}_p+\sum_{p^{\prime} \in \mathcal{P}_{acc}^{(t)}} \mathbf{e}_p^T \mathbf{e}_{p^{\prime}}+\sum_{f \in \mathcal{F}_u^{(t)}} \mathbf{e}_p^T \mathbf{\tilde{e}}_f
-\sum_{p^{\prime} \in \mathcal{P}_{rej}^{\prime(t)}} \mathbf{e}_p^T \mathbf{e}_{p^{\prime}}\right),
\label{eq:attrscore}
\end{equation}
where $\mathbf{e}_u$, $\mathbf{e}_v$, $\mathbf{e}_p$ and $\mathbf{e}_f$ are embeddings of the user, item, attribute, and friend. $\mathbf{\tilde{e}}_f=\sum_{v^{\prime} \in \mathcal{V}_{f}^{(t)}} \mathbf{e}_v$ represents friend preferences that satisfy the interactive history, $\sigma(\cdot)$ denotes the sigmoid function.

With the action space $\mathcal{A}_t$ and the state representation $\mathbf{q}_t$, we introduce the dueling Q-networks \cite{wang2016dueling} to determine the next action and calculate the Q-value as:
\begin{equation}
Q\left(s_{t}, a_{t}\right)=f_{\theta_{V}}\left(\mathbf{q}_{t}\right)+f_{\theta_{A}}\left(\mathbf{q}_{t}, a_{t}\right),
\label{eq:DQN}
\end{equation}
where the value function $f_{\theta_{V}}(\cdot)$ and the advantage function $f_{\theta_{A}}(\cdot)$ are two separate multi-layer perceptions with $\theta_{V}$ and $\theta_{A}$ denote the parameters, respectively. The optimal Q-function $Q^{*}(\cdot)$, which has the maximum expected reward achievable by the optimal policy $\pi^*$, follows the Bellman equation \cite{bellman1957role} as:
\begin{equation}
Q^{*}\left(s_{t}, a_{t}\right)=\mathbb{E}_{s_{t+1}}\left[r_{t}+\gamma \max _{a_{t+1} \in \mathcal{A}_{t+1}} Q^{*}\left(s_{t+1}, a_{t+1} \mid s_{t}, a_{t}\right)\right],
\label{eq:DQN2}
\end{equation}
where $\gamma$ denotes the discount factor for the delayed rewards.

In each turn, the CRS agent will get the reward $r_t$, and we can update the state $s_{t+1}$ and the action space $\mathcal{A}_{t+1}$ according to the user's feedback. Following Deng \etal \shortcite{deng2021unified}, we define a replay buffer $\mathcal{D}$ to store the experience $\left(s_{t}, a_{t}, r_{t}, s_{t+1}, \mathcal{A}_{t+1}\right)$. For training of the DQN, we sample mini-batches from the buffer and minimize the following loss:
\begin{equation}
\mathcal{L}^{DQN}=\mathbb{E}_{\left(s_{a}, a_{t}, r_{t}, s_{t+1}, \mathcal{A}_{t+1}\right) \sim \mathcal{D}}\left[\left(y_{t}-Q\left(s_{t}, a_{t} ; \theta_{Q}, \theta_{S}\right)\right)^{2}\right],
\label{eq:DQN3}
\end{equation}
where $\theta_{S}$ is the set of parameters in the module for hypergraph-based representation learning, $\theta_{Q}=\left\{\theta_{V}, \theta_{A}\right\}$, and $y_{t}$ is the target value based on the currently optimal $Q^{*}$:

\begin{equation}
y_{t}=r_{t}+\gamma \max _{a_{t+1} \in \mathcal{A}_{t+1}} Q\left(s_{t+1}, a_{t+1} ; \theta_{Q}, \theta_{S}\right).
\label{eq:DQN4}
\end{equation}
To deal with the overestimation bias in the original DQN, we apply the double DQN \cite{van2016deep}, which copies a target network $Q^{'}$ as a periodic from the online network to train the model. During training, the action decision policy learning in Eq.\ref{eq:DQN3}, and the cross-view contrastive learning in Eq.\ref{eq:loss} are alternatively
optimize.

