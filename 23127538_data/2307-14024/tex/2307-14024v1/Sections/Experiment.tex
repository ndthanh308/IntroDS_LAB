\section{Experiments}
% In this section, we will detail the settings of our experiments and present the experimental results.
To fully demonstrate the superiority of MHCPL,
we conduct experiments\footnote{https://github.com/Snnzhao/MHCPL} on two public datasets to explore the following questions:
\begin{itemize}
    \item \textbf{RQ1:} How does MHCPL perform compared with the state-of-the-art methods?
    \item \textbf{RQ2:} How do different components (social influence, hypergraph based state encoder, and cross-view contrastive learning) affect the results of MHCPL?
    \item \textbf{RQ3:} How do parameters (the layer number of Hypergraph based State Encoder) influence the results of MHCPL?
    \item \textbf{RQ4:} Can our MHCPL effectively leverage the interactive conversation, item knowledge, and social influence to learn the dynamic user preferences?
\end{itemize}

\subsection{Datasets}\label{sec:standalone}
To evaluate the proposed method, we adapt two existing
MCR benchmark datasets, named Yelp and LastFM. The statistics of these datasets are presented in Table \ref{tab:data}.
\begin{itemize}
    \item {\textbf{LastFM}}~\cite{lei2020estimation}: LastFM dataset is the music listening dataset collected from Last.fm online music systems. As Zhang \etal \shortcite{zhang2022multiple}, We define the 33 coarse-grained groups as attribute types for the 8,438 attributes.
    \item{\textbf{Yelp}}~\cite{lei2020estimation}: Yelp dataset is adopted from the 2018 edition of the Yelp challenge. Following Zhang \etal \shortcite{zhang2022multiple}, we define the 29 first-layer categories as attribute types, and 590 second-layer categories as attributes.
\end{itemize}
Following Zhang \etal \shortcite{zhang2022multiple}, we sample two items with partially overlapped attributes as the user's acceptable items for each conversation episode.
\begin{table}[ht]
    \setlength{\tabcolsep}{5pt}
 \centering
 \small
    \begin{tabularx}{0.45\textwidth}{p{3cm}|X|X}
    \toprule
    \makecell[c]{\text{Dataset}}&\makecell[c]{\text{Yelp}}&\makecell[c]{\text{LastFM}}\cr
    \hline
    \hline
    \makecell[c]{\text{Users}}&\makecell[c]{27,675}&\makecell[c]{1,801}\cr

    \makecell[c]{\text{Items}}&\makecell[c]{70,311}&\makecell[c]{7,432}\cr
    \makecell[c]{\text{Attributes}}&\makecell[c]{590}&\makecell[c]{8,438}\cr
    \makecell[c]{\text{Attribute types}}&\makecell[c]{29}& \makecell[c]{34}\cr
    \hline
    \makecell[c]{\text{User-Item}}&\makecell[c]{1,368,606}&\makecell[c]{76,693}\cr
    \makecell[c]{\text{User-User}}&\makecell[c]{688,209}&\makecell[c]{23,958}\cr
    \makecell[c]{\text{Item-Attribute}}&\makecell[c]{477,012}&\makecell[c]{94,446}\cr
    \bottomrule[0.8pt]
    \end{tabularx}
    \caption{Statistics of two utilized datasets}
    \label{tab:data}
\end{table}
\subsection{Experiments Setup}

\subsubsection{User Simulator}
MMCR is a system that is trained and evaluated based on interactive conversations with users. Following the user simulator adopted in \cite{zhang2022multiple}, we simulate a interactive session for each user-item set interaction pair $(u, \mathcal{V}_u)$. Each item in the item set $v \in \mathcal{V}_u$ is treated as an acceptable item for the user. Each session is initialized with a user $u$ specifying an attribute $p_0 \in \mathcal{P}_{joint}$, where $\mathcal{P}_{joint}$ is the set of attributes that are shared by the items in $\mathcal{V}_u$. Then the session follows the process of "System Ask or Recommend, User response" \cite{zhang2022multiple} as described in Section \ref{sec:def}.

\begin{table*}[t]
    \centering
    \begin{tabular}{p{2.0cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{0.01cm}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{0.01cm}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}p{0.01cm}p{1cm}<{\centering}p{1cm}<{\centering}p{1cm}<{\centering}}
    \toprule
    \multirow{2}{*}{\bfseries Models }&\multicolumn{5}{c}{\bfseries Yelp }&&\multicolumn{5}{c}{\bfseries LastFM }\\
    \cline{2-6}
    \cline{8-12}
    &SR@5&SR@10&SR@15&AT&hDCG&&SR@5&SR@10&SR@15&AT&hDCG\\
    \midrule

    Abs Greedy &0.078&0.124&0.150&13.65&0.065&&0.292&0.436&0.512&10.10&0.237\\
    Max Entropy&0.046&0.200&0.390&12.97&0.117&&0.280&0.560&0.680&9.34&0.263\\
    CRM& 0.026&0.100&0.188&13.99&0.059&&0.092&0.240&0.372&12.56&0.130\\
    EAR& 0.120&0.198&0.240&12.91&0.094&&0.298&0.436&0.508&10.08&0.237\\
    SCPR &0.146&0.188&0.436&12.29&0.169&&0.322&0.630&0.764&8.47&0.322\\
    UNICORN &\underline{0.200}&0.338&0.430&11.33&0.175&&0.444&0.774&0.846&7.10&0.348\\
     MCMIPL&{0.162}&{0.366}&{0.522}&{11.25}&{0.184}&&\underline{0.448}&{0.809}&{0.884}&{6.87}&{0.353}\\
    \midrule
    S*-UNICORN &0.120&0.478&0.696&10.59&0.223&&0.412&0.850&0.912&6.69&0.363\\
     S*-MCMIPL&{0.126}&\underline{0.490}&\underline{0.722}&\underline{10.51}&\underline{0.230}&&{0.442}&\underline{0.872}&\underline{0.940}&\underline{6.43}&\underline{0.368}\\
    \midrule
    MHCPL&{0.142}&{\bfseries 0.592}&{\bfseries 0.854}&{\bfseries 9.96}&{\bfseries 0.261}&&{\bfseries 0.470}&{\bfseries 0.938}&{\bfseries 0.982}&{\bfseries 5.87}&{\bfseries 0.427}\\
    % \midrule
     Improv. &-&20.82$\%$&18.28$\%$&5.23$\%$&13.48$\%$&&4.91$\%$&7.57$\%$&4.47$\%$&8.71$\%$&16.03$\%$\\
    \bottomrule
    \end{tabular}
    \caption{Performance comparison of different models on the two datasets. The bold number represents the improvement of our model over baselines is statistically significant with p-value $< 0.01$. hDCG stands for hDCG@($15,10$).}
    \label{tab:results}
\end{table*}

\subsubsection{Baselines}
To demonstrate the effectiveness of the proposed MHCPL, the state-of-the-art methods are chosen for comparison : 
\begin{itemize}
    \item \textbf{Max Entropy.} This method employs a rule-based strategy to ask and recommend. It chooses to select an attribute with maximum entropy based on the current state, or recommends the top-ranked item with certain probabilities \cite{lei2020estimation}.
    \item \textbf{Greedy\cite{christakopoulou2016towards}.} This method only makes item recommendations and updates the model based on the feedback. It keeps recommending items until the successful recommendation is made or the pre-defined round is reached. 

    \item \textbf{CRM\cite{sun2018conversational}.} A reinforcement learning-based method that records the users' preferences into a belief tracker and learns the policy deciding when and which attributes to ask based on the belief tracker.
    \item \textbf{EAR\cite{lei2020estimation}.} This method proposes a three-stage solution to enhance the interaction between the conversational component and the recommendation component.
    \item \textbf{SCPR\cite{lei2020interactive}.} This method learns user preferences by reasoning the path on the user-item-attribute graph via the user’s feedback and accordingly chooses actions.
    \item \textbf{UNICORN\cite{deng2021unified}.} This work builds a weighted graph to model dynamic relationships between the user and the candidate action space, and proposes a graph-based Markov Decision Process (MDP) environment to learn dynamic user preferences and chooses actions from the candidate action space.
    \item \textbf{MCMIPL\cite{zhang2022multiple}.} This approach proposes a multi-interest policy learning framework that captures the multiple interests of the user to decide the next action.
    \item \textbf{S*-UNICORN and S*-MCMIPL.} For a more comprehensive and fair performance comparison, we adapt UNICORN and MCMIPL by timely selecting helpful social information and incorporating it into the weighted graph of the model. We name the two adapted methods S*-UNICORN and S*-MCMIPL.
\end{itemize}

\subsubsection{Parameters Setting}
Following \cite{zhang2022multiple}, we recommend top $K=10$ items or ask $K_a= 2$ attributes in each turn. We employ the Adam optimizer with a learning rate of $1e-4$. Discount factor $\gamma$ is set to be $0.999$. Following \cite{deng2021unified}, we adopt TransE \cite{TransE} via OpenKE \cite{OpenKE} to pretrain the node embeddings with 64 dimensions in the constructed KG with the training set. We make use of Nvidia Titan RTX graphics cards equipped with AMD r9-5900x CPU (32GB Memory).
For the action space, we select $K_p=10$ attributes and $K_v=10$ items.
To maintain a fair comparison, we adopt the same reward settings as previous works  \cite{lei2020estimation, lei2020interactive,deng2021unified,zhang2022multiple}: $r_{rec\_suc}=1, r_{rec\_fail}=-0.1, r_{ask\_suc}=0.01, r_{ask\_fail}=0.1, r_{quit}=-0.3$. For MHCPL, we select the number of layers from {1, 2, 3, 4}.

\subsubsection{Evaluation Metrics}
Following previous works \cite{lei2020estimation, lei2020interactive,deng2021unified}, we adopt success rate (SR@t) to measure the cumulative ratio of successful recommendations by the turn t, average turns (AT) to evaluate the average number of turns for all sessions, and hDCG@(T, K) to additionally evaluate the ranking performance of recommendations. 
Therefore, the higher SR@t and hDCG@(T, K) indicate better performance, while the lower AT means an higher efficiency.


\subsection{Performance Comparison (RQ1)}
\subsubsection{Overall Performance}
The comparison experimental results of the baseline models and our models are shown in Table \ref{tab:results}. 
We can summarize our observations as follows:

\begin{itemize}[leftmargin=*]
    %our对比正常
    \item \textbf{Our proposed MHCPL achieves the best performance.} MHCPL significantly outperforms all the baselines on the metrics of SR@15, AT and hDCG by over 4.47$\%$, 5.23$\%$ and 13.48$\%$, respectively.  We attribute the improvements to the following reasons: 1) The proposed dynamic multi-view hypergraph could effectively capture multiplex relations from three views. And the proposed hierarchical hypergraph neural network is able to well learn dynamic user preferences by integrating the information of graph structure and sequential modeling from the dynamic multi-view hypergraph; 
    2) MHCPL timely selects helpful social information and effectively integrates the interactive conversation, item knowledge, and social influence for better dynamic user preference learning; 3) MHCPL designs a cross-view contrastive learning method to help maintain the inherent characteristics and the correlations of user preferences from different views.
    
    \item \textbf{The learning of the dynamic user preferences is crucial for conversational recommendation.} The graph-based methods (MHCPL, MCMIPL, UNICORN, SCPR) outperforms the factorization-based methods (EAR, CRM) since they learn user preferences from the collaborative information in the graph. MCMIPL achieves the best performance among the graph-base baselines since it further considers the multiple interests of the user preferences. Our proposed MHCPL further outperforms these methods since we leverage multiplex relations to integrate interactive conversation, item knowledge, and social influence to help learn the dynamic user preferences.

    \item \textbf{Social influence is effective in helping learn dynamic user preferences for conversational recommendation when well filtered.} The socially adapted methods (\ie S*-UNICORN and S*-MCMIPL) outperform their original versions in the final performances. We attribute this to the reason that social influence is an important factor that affects user preferences and could help learn dynamic user preferences with friends' preferences that satisfy the interactive conversation. But the socially adapted methods perform worse than their original version in the early turns (\eg SR@5). This happens because the information in the interactive conversation is not sufficient to filter out the noise from the social information in the early turn of the conversation.
    
\end{itemize}

% Figure environment removed

\subsubsection{Comparison at Different Conversation Turns} Besides the performance in the final turn, we also present success rates at different turns in \autoref{fig:overall}.
In order to better observe the differences among different models, we use the relative success rate compared with the most competitive baseline $\text{S*-MCMIPL}$, where the blue line of $\text{S*-MCMIPL}$ is set to zero in the figures. From the \autoref{fig:overall}, we following observations:
\begin{itemize}[leftmargin=*]
    %our对比正achieve
    \item \textbf{} The proposed MHCPL outperforms these baseline methods across all the datasets and almost all the turns in the conversational recommendation. This is because our proposed MHCPL could better learn dynamic user preferences with multiplex relations that integrate interactive conversation, item knowledge, and social influence.
    
    \item \textbf{} The recommendation success rate of the proposed socially-aware methods (\ie MHCPL, S*-MCMIPL, and S*-UNICORN) could not surpass all the baselines in the early turns of the conversational recommendation, especially on the dataset Yelp with a larger candidate space of items and attributes. This is because the information in the interactive conversation is not sufficient to filter out the noise from the social information at the early turn of the conversation.
    Furthermore, socially-aware methods prefer to ask rather than recommend in the early turns when the user's preference is not certain enough. This will effectively reduce the action space and better learn user preferences, but lead to a lower recommendation success rate in the early turns. %This is because they may recommend items with unclear user preferences in the early turns. This can increase early turns' success rates but is not effective since it is helpless in learning user preferences.
\end{itemize}
\begin{table}[t]
    \small
    \centering
    \begin{tabular}{p{2.7cm}<{\centering}p{0.6cm}<{\centering}p{0.5cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.5cm}<{\centering}p{0.6cm}<{\centering}}
    \toprule
    % \hline
    \multirow{2}{*}{\bfseries Models }& \multicolumn{3}{c}{\textbf{Yelp}}& \multicolumn{3}{c}{\textbf{LastFM}}\\
    &SR@15&AT&hDCG&SR@15&AT&hDCG\\
    \midrule
    Ours&{\bfseries0.854}&{\bfseries9.96}&{\bfseries0.261}&{\bfseries0.982}&{\bfseries5.87}&{\bfseries0.427}\\
    \midrule
    -w/o social&0.592&10.80&0.208&0.908&6.63&0.365\\
    -w/o hypergraph&0.726&10.68&0.346&0.938&6.58&0.382\\
    -w/o contrastive&0.762&10.37&0.237&0.962&6.17&0.403\\
    \bottomrule
    \end{tabular}
    \caption{Results of the Ablation Study. }
    \label{tab:ablation_study}
\end{table}
\subsection{Ablation Studies (RQ2)}
To investigate the underline mechanism of MHCPL, we conduct ablation experiments on the Yelp and LastFM datasets with three ablated methods including: $\text{MHCPL}_{\wo social}$ that ablates the social influence, $\text{MHCPL}_{\wo hypergraph}$ that replaces the hypergraph neural networks with graph neural networks, and $\text{MHCPL}_{\wo contrastive}$ that ablates the cross-view contrastive learning. From results shown in Table~\ref{tab:ablation_study}, we have the following observations:
\begin{itemize}
    \item $\text{MHCPL}_{\wo social}$ is the least  competitive. This demonstrates the importance of social influence in alleviating the data sparsity problem and helping learn dynamic user preferences. And it is effective to accordingly choose helpful social information based on interactive conversation. $\text{MHCPL}_{\wo social}$ still outperforms all the baselines that ignore the social information in Table~\ref{tab:results}, which proves the effectiveness of MHCPL in learning dynamic user preferences with multiplex relations.
    
    \item MHCPL outperforms $\text{MHCPL}_{\wo hypergraph}$. 
    We contribute this to the importance of multiplex relations in learning dynamic user preferences. This also proves the effectiveness of our proposed multi-view hypergraph-based state encoder in learning user preferences by integrating the information of graph structure and sequential modeling from the dynamic multi-view hypergraph.
    \item MHCPL outperforms $\text{MHCPL}_{\wo contrastive}$. This demonstrates the effectiveness of the cross-view contrastive learning module in helping maintain the inherent characteristics and correlations of user preferences from different views. 
\end{itemize}

% Figure environment removed

\subsection{Hyper-parameter Sensitivity Analysis (RQ3)}
\subsubsection{Impact of Layer Number} The hypergraph-based state encoder learns dynamic user preferences from the multiplex relations in the hypergraph. By stacking more layers, collaborative information from multi-hop neighbors is distilled. We investigate how the layer number $L$ influences the performance of MHCPL. Specifically, we conduct experiments with $L$ in the range $\{1, 2, 3, 4\}$, and the results are shown in Figure \ref{fig:layers}. There are some observations:
\begin{itemize}
    \item Increasing the number of layers can improve the performance of our model. MHCPL-2 highly outperforms MHCPL-1. The reason is that MHCPL-1 only gains information from the one-hop neighbors and neglects high-order collaborative information. 
    \item When increasing the layer of number, the performance does not always improve. MHCPL-3 outperforms MHCPL-4 on data LastFM. This can be attributed to the noise which increases along with the hop of neighbors.
\end{itemize}

% Figure environment removed

\subsection{Case Study (RQ4)}
To show the effectiveness of our proposed MHCPL in leveraging multiplex relations to integrate interactive conversation, item knowledge, and social influence to learn dynamic user preferences, we present a case of conversational recommendation generated by our framework in \autoref{fig:case}. As illustrated in the figure, by integrating the information from the interactive conversation, item knowledge, and social information with multiplex relations from different views, MHCPL is able to effectively ask attributes and recommend user-preferred items, reaching success in five turns. Furthermore, the social information selected according to the interactive conversation is helpful in learning dynamic user preferences. With the help of selected social information, MHCPL could accurately select the target item when the information from the interactive history is limited in distinguishing user preferences towards the seventy candidate items.