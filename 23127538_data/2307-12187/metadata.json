{
  "title": "Monadic Deep Learning",
  "authors": [
    "Bo Yang",
    "Zhihao Zhang Kirisame Marisa",
    "Kai Shi"
  ],
  "submission_date": "2023-07-23T00:17:37+00:00",
  "revised_dates": [],
  "abstract": "The Java and Scala community has built a very successful big data ecosystem. However, most of neural networks running on it are modeled in dynamically typed programming languages. These dynamically typed deep learning frameworks treat neural networks as differentiable expressions that contain many trainable variable, and perform automatic differentiation on those expressions when training them.\n  Until 2019, none of the learning frameworks in statically typed languages provided the expressive power of traditional frameworks. Their users are not able to use custom algorithms unless creating plenty of boilerplate code for hard-coded back-propagation.\n  We solved this problem in DeepLearning.scala 2. Our contributions are:\n  1. We discovered a novel approach to perform automatic differentiation in reverse mode for statically typed functions that contain multiple trainable variable, and can interoperate freely with the metalanguage.\n  2. We designed a set of monads and monad transformers, which allow users to create monadic expressions that represent dynamic neural networks.\n  3. Along with these monads, we provide some applicative functors, to perform multiple calculations in parallel.\n  With these features, users of DeepLearning.scala were able to create complex neural networks in an intuitive and concise way, and still maintain type safety.",
  "categories": [
    "cs.PL",
    "cs.AI",
    "cs.LG"
  ],
  "primary_category": "cs.PL",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12187",
  "pdf_url": null,
  "comment": "27 pages, 7 figures, 3 tables",
  "num_versions": null,
  "size_before_bytes": 596651,
  "size_after_bytes": 590672
}