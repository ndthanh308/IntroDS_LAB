
In this section, we benchmark Uruv against  (a) previous lock-free variants of the B$^+$Tree for updates and search operations (to our knowledge, there are no existing wait-free implementations of the B$^+$Tree, and lock-free B$^+$Trees do not implement range search), and (b) the lock-free VCAS-BST of \cite{wei+:PPOPP:2021}, which is the best-performing data structure in their benchmark. The code of the benchmarks is available at \url{https://github.com/gaurav-bhardwaj03/Uruv/}.



%\paragaraph*{Our competitors.} Uruv's design is inspired from LFB$^+$Tree \cite{braginsky+:SPAA:2012}, we compare  with their tree. We also compare Uruv with the state-of-the-art lock-free B$^+$Tree, OpenBw-Tree \cite{wang2018building}. Since Uruv supports wait-free ranged search, we compare with the best performing ranged structure in \cite{wei+:PPOPP:2021}, VCAS-BST. We haven’t compared our implementation of Uruv with their implementation of VCAS-CT as we implement ours in C++ and they did it in Java. However, we will make public both our C++ and Java implementations soon.

\paragraph{Experimental Setup.} We conducted our experiments on a system with an IBM Power9 model 2.3 CPU packing 40 cores with a minimum clock speed of 2.30 GHz and a maximum clock speed of 3.8 GHz. There are four logical threads for each core, and each has a private 32KB L1 data cache and L1 instruction cache. Every pair of cores shares a 512KB L2 cache and a 10MB L3 cache. The system has 240GB RAM and a 2TB hard disk. The machine runs Ubuntu 18.04.6 LTS. We implement Uruv in C++. Our code was compiled using g++ 11.1.0 with -std=c++17 and linked the pthread and atomic libraries. We take the average of the last seven runs out of 10 total runs, pre-warming the cache the first three times. Our average excludes outliers by considering results closest to the median.

\input{figs/OpenBwTree_vs_Uruv}

\input{figs/VCAS_BST_vs_Uruv}

\paragraph{Benchmark.} Our benchmark takes 7 parameters - read, insert, delete, range query, range query size, prefilling size, and dataset size. Read, insert, delete, and range queries indicate the percentage of these operations. We use a uniform distribution to choose between these four operations probabilistically. We prefill each data structure with 100 million keys, uniformly at random, from a universe of 500 million keys ranging [1, 500M].

\paragraph{Performance for dictionary operations.} Results of three different workloads - Read-only(Fig. \ref{fig:uruv-anastasia-openbwtree}a), Read-Heavy(Fig. \ref{fig:uruv-anastasia-openbwtree}b), and a Balanced workload(Fig. \ref{fig:uruv-anastasia-openbwtree}c) are shown in Figure \ref{fig:uruv-anastasia-openbwtree}. Across the workloads, at 80 threads, Uruv beats LF$\_$B$^+$Tree\cite{braginsky+:SPAA:2012} by \textbf{95x}, \textbf{76x}, and \textbf{44x} as it replaces the node with a new node for every insert. Uruv beats OpenBwTree\cite{wang2018building} by \textbf{1.7x}, \textbf{1.7x}, and \textbf{1.25x}. 
The performance of LF-URUV and WF-URUV correlates since WF-URUV has a lower possibility of any thread taking a slow path. In all three cases, the gap between Uruv and the rest increases as the number of threads increases. This shows the scalability of the proposed method. As we move from 1 to 80 threads, Uruv scales \textbf{46x} to \textbf{61x} in performance, LFB$^+$Tree scales \textbf{2.4x} to \textbf{5x} and OpenBw-Tree scales \textbf{39x} to \textbf{42x}. These results establish the significantly superior performance of Uruv over its existing counterpart.


% performs much better when compared to its lock-free variants, including the state-of-the-art.
\paragraph{Performance for workloads including range search.} We compare Uruv against VCAS-BST in various workloads in Figure \ref{fig:5d-5f}. Figures \ref{fig:5d-5f}a - \ref{fig:5d-5f}c are read-heavy workloads and \ref{fig:5d-5f}d - \ref{fig:5d-5f}f are update-heavy workloads. Across each type of workload, we vary the range query percentage from 1\% to 10\%. At 80 threads, we beat VCAS-BST by \textbf{1.38x} in update-heavy workloads and \textbf{1.68x} in read-heavy workloads. 
%We haven’t compared Uruv to VCAS-CT, but from \cite{wei+:PPOPP:2021}, we see that VCAS-BST performs similarly to VCAS-CT. 
These set of results demonstrate the efficacy of Uruv's wait-free range search.


%\vspace{-2mm}\paragaraph*{Summary.} Overall, Uruv outperforms the state-of-the-art B$^+$Tree variant and beats the best performing consistent linearizable lock-free ranged search data structure. In addition to this, it is the first wait-free B$^+$Tree variant thereby harnessing much stronger progress guarantees versus its competition.
