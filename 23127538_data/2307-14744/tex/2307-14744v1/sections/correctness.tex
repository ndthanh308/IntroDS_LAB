To prove the correctness of Uruv, we have shown that Uruv is linearizable by describing \textit{linearization points} (LPs) that map any concurrent setting to a sequential order of said operations. We discuss them in detail below.


\subsection{Linearization Points}

As explained earlier, we traverse down Uruv to the correct leaf node and perform all operations on the linked list in that leaf. Therefore, we discuss the LPs of the versioned linked list.
\vspace{-0.2cm}\paragraph*{\textbf{\insertADT:}} There are two cases. If the key does not exist, we insert the key into the linked list. However, the timestamp of the \texttt{vnode} is not set, so the \texttt{LP} for \insertADT \op is when the timestamp of \texttt{vnode} is set to the current timestamp. This can be executed either just after the insertion of the key in the linked list or by some other thread before reading the value from \texttt{vnode}.

If the key already exists, we update its value by atomically replacing a new versioned node by its current $vhead$. After successfully changing the $vhead$, the node’s timestamp is still not set. It can be set just after adding the new versioned node or by some other thread before reading the value from the newly added versioned node. In both the cases the \texttt{LP} is when the timestamp of the versioned node is set to the current timestamp. 

\vspace{-0.3cm}\paragraph*{\textbf{\remove.}} There are two cases. If the key does not exist, then there is no need to delete the key as it does not exist. Therefore, the \texttt{LP} would be where we last read a node from the linked list. Instead, if the key exists, the \texttt{LP} will be same as \insertADT when we set the timestamp of the versioned node.

\vspace{-0.2cm}\paragraph*{\textbf{\search}.} There are two cases, first if the key doesn't exist in the linked list, the \search \texttt{LP} would be when we first read the node whose key is greater than the key we are searching for in the linked list. Second, if the key is present in the linked list it reads the value in the versioned node at \texttt{vhead}. So the \texttt{LP} is when we atomically reads the value from the versioned node. If a concurrent insert/delete leads to a split/merge operation, then there is a chance that the search will end up at a leaf node that is no longer a part of Uruv. In that case, the search’s \texttt{LP} would have happened before insert/delete’s \texttt{LP}. Search’s \texttt{LP} remains the same as above.

\vspace{-0.2cm}\paragraph*{\textbf{\rangeQuery}.} \rangeQuery method reads the global timestamp and increment it by 1. So the \texttt{LP} for range query would be the atomic read of global timestamp. The range query’s \texttt{LP} will remain the same regardless of any other concurrent operation.
\ignore{
\subsection{Correctness of the Wait-Free Mechanism}

The LPs for the wait-free algorithm have already been discussed above. There are two essential aspects of the wait-free mechanism that must be carefully analyzed. The first is that a helping thread needs to help the intended operation. There might be a case that the entry in the \texttt{stateArray} is updated while the thread is helping, and it may perform an incorrect operation. To avoid this situation, every helping thread checks if the phase has been updated in the \texttt{stateArray} entry or not. If it has changed, then the entry in the \texttt{stateArray} has been replaced by a later entry, and the intended operation was already done.

The second consideration is that only one thread should be able to complete an operation successfully. Otherwise, two threads may perform the same operation due to \textit{race conditions} which can lead to inconsistency. This has already been discussed above, wherein we show that only one thread can complete any operation preventing this problematic condition.
}
\subsection{Complexity Analysis} Let there be $t$ threads in the shared-memory system. Let the number of attempts after an operation decides to go the slow path way be $f$ and the number of operations performed by a thread between every check of helping a slow peer be $s$. Notice that, when a \cas fails, before reattempting the operation, a thread necessarily helps one of its concurrent peers. Thus, every help gets counted as a failed attempt. With this, the maximum number of restarts that any operation would do is $m=\min(f+st, I_C)$, where $I_C$ is the interval contention, i.e., total number of concurrent operations during the lifetime of the operation \cite{afek1999long}. 

Now, let the  maximum threshold of an internal node (essentially, its size) be $B$ and the maximum size of the linked-list in a leaf node, after which it is decided to be split, be $L$. Then, using the standard worst-case complexity analysis of the hierarchical search, if an update or a search operation remains unobstructed, it will finish in $O(L+\log_B(n))$ number of steps, where $n$  is the number of keys in the data structure at its invocation. 

Let $n_{max,op}$ be the maximum size of the dataset contained in Uruv during the lifetime of an operation $op$. Then counting the maximum number of attempts, the worst case complexity of an update operation will be $O(m(L+\log_B(n_{max,op})))$. Because, the search and range queries do not restart, it is easy to see that their worst case step complexity will be $O(L+\log_B(n_{max,op}))$ and $O(L+\log_B(n_{max,op})+k)$, respectively, where $k$ is the size of the output of range search operations.

Clearly, all the above expressions of worst-case complexity are finite. Therefore, each of the operations in Uruv are wait-free. 
\label{sec:Analysis}
