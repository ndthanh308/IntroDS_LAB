With the growing size of main memory, the in-memory big-data analytics engines are becoming increasingly popular \cite{zhang2015memory}. Often the analytics tasks are based on retrieving keys from a dataset specified by a given range. Additionally, such applications are deployed in a streaming setting, e.g., Flurry \cite{flurry}, where the dataset ingests real-time updates. Ensuring progress to every update would be attractive for many applications in this setting, such as financial analytics \cite{tian2015latency}. The demand for real-time high-valued analytics, the powerful multicore CPUs, and the availability of large main memory together motivate designing scalable concurrent data structures to utilize parallel resources efficiently.

%supporting both wait-free updates and range search. 
%
%Concurrent data structures have become very important due to their significant performance benefits over sequential data structures in multicore environments. Active research continues in this area resulting in new sophisticated and efficient algorithms developed yearly. 

It is an ever desirable goal to achieve \textit{maximum progress} of the concurrent operations on a data structure. The maximum progress guarantee -- called \textit{wait-freedom} \cite{Herlihy+:OnNatProg:opodis:2011} -- ensures that each concurrent non-faulty thread completes its operation in a finite number of steps. Traditionally, wait-freedom has been known for its high implementation cost and subsided performance. Concomitantly, a weaker guarantee that some non-faulty threads will finitely complete their operations -- known as \textit{lock-freedom} -- has been a more popular approach. However, it has been found that the lock-free data structures can be transformed to \textit{practical} wait-free \cite{kogan2012methodology} ones with some additional implementation and performance overhead. Progress promises of wait-free data structures make their development imperative, to which a practical approach is to co-design them with their efficient lock-free counterpart. While a progress guarantee is desirable, consistency of concurrent operations is a necessity. The most popular consistency framework is \textit{\lbty} \cite{herlihy1990linearizability}, i.e., every concurrent operation emerges taking effect at an atomic step between its invocation and return.  

In the existing literature, the lock-free data structures such as k-ary search trees \cite{brown2012range}, and the lock-based key-value map KiWi \cite{basin2017kiwi} provide range search. In addition, several generic methods of concurrent range search have been proposed. Chatterjee \cite{chatterjee2017lock} presented a lock-free range search algorithm for lock-free linked-lists, skip-lists, and binary search trees. Arbel-Raviv and Brown \cite{arbel2018harnessing} proposed a more generic approach associated with memory reclamation that fits into different concurrency paradigms, including lock-based and software transactional memory-based data structures. Recently, two more approaches -- bundled-reference \cite{nelson2021bundled} and constant time snapshots \cite{wei+:PPOPP:2021} -- were proposed along the same lines of generic design. Both these works derive from similar ideas of expanding the data structure with versioned updates to ensure \lbty of scans. While the former stores pointers with time-stamped updates, the latter adds objects to nodes time-stamped by every new range search. Moreover, bundled-reference~\cite{nelson2021bundled} design requires locks in every node.
 
In most cases, for example \cite{basin2017kiwi, chatterjee2017lock, nelson2021bundled, wei+:PPOPP:2021}, the range scans are unobstructed even if a concurrent modification (addition, deletion, or update) to the data structure starves to take even the first atomic step over a shared node or pointer. A reader would perceive, indeed for good reasons, that once the modifications are made wait-free the entire data structure will become wait-free. However, to our knowledge, none of these works actually investigates how trivial or non-trivial it would be to arrive at the final implementation of concurrent wait-free CRUD and range-search. This is exactly where our work contributes.

\vspace{2mm}
\noindent\textbf{\large Proposed wait-free \lble proactive data structure}
\vspace{2mm}

In principle, \urv's design derives from that of a B$^+$Tree \cite{comer+:ACM1979}, a self-balancing data structure. However, we need to make the following considerations:

\noindent
\textbf{Wait-freedom:} Firstly, to ensure wait-freedom to an operation that needs to perform at least one \cas execution on a shared-memory word, it must \textit{announce} its invocation \cite{kogan2012methodology}. Even if delayed, the announcement has to happen on realizing that the first \cas was attempted a sufficient number of times, and yet it starved \cite{kogan2012methodology}. The announcement of invocation is then followed by a \textit{guaranteed help} by a concurrent operation at some finite point \cite{kogan2012methodology}. 

\noindent
\textbf{Linearizability:} Now, to ensure \lbty of a scan requires that its output reflects the relevant changes made by every update during its lifetime. The technique of repeated multi-scan followed by validation \cite{brown2012range}, and 
% if those updates were incorporated in its output causes degradation of overall performance of the data structure, as seen in the benchmarks of \cite{wei+:PPOPP:2021}. Even
% 
  collecting the updates at an augmented object, such as RangeCollector in \cite{chatterjee2017lock}, to let the range search incorporate them before it returns, have been found scaling poorly \cite{brown2012range,chatterjee2017lock}.
%   does not help much. 
   Differently, multi-versioning of objects, for example \cite{nelson2021bundled}, can have a (theoretical) possibility to stockpile an infinite number of versioned pointers between two nodes.   
Interestingly, \cite{arbel2018harnessing} exploits the memory reclamation mechanism to synchronize the range scans with delete operations via logically deleted nodes. However, for lock-freedom, they use a \textit{composite primitive} double-compare-single-swap (DCSS). In comparison, \cite{wei+:PPOPP:2021} uses only single-word \cas. However, managing the announcement by a starving updater that performs the first \cas to introduce a versioned node to the data structure requires care for a wait-free design.

\noindent
\textbf{Node Structure:} The ``fat" (array-based) data nodes, for example Kiwi \cite{basin2017kiwi}, improve traversal performance by memory contiguity \cite{kowalski2020high}. % used fat data nodes intending to make the range search the first-class citizen. 
However, the benchmarks in \cite{wei+:PPOPP:2021} indicate that it does not necessarily help as the number of concurrent updates picks up. 
%, but the basic lock-free binary search trees (BSTs) \cite{Natarajan+:LFBST:ppopp:2014} do significantly better. 
Similarly, the lock-free B+trees by Braginsky and Petrank \cite{braginsky+:SPAA:2012} used memory chunks, and our experiments show that their method substantially underperforms. Notwithstanding, it is wise to exploit memory contiguity wherever there could be a scope of ``slow" updates in a concurrent setting.

\noindent
\textbf{Proactive maintenance:} Finally, if the number of keys in a node exceeds (falls short of) its maximum (minimum) threshold after an insertion (deletion), it requires splitting (merging). The operation splitting the node divides it into two while adding a key to its parent node. It is possible that the split can percolate to the root of the data structure if the successive parent nodes reach their respective thresholds. Similarly, merging children nodes can cause cascading merges of successive parent nodes. With concurrency, it becomes extremely costly to tackle such cascaded split or merge of nodes from a leaf to the root. An alternative to this is a \textit{proactive approach} which checks threshold of nodes whiles traversing down a tree every time; if a node is found to have reached its threshold, without waiting for its children, a pre-emptive split or merge is performed. As a result, a restructure remains localized. To our knowledge, no existing concurrent tree structure employs this proactive strategy.

%\footnote{Uruvriksha is the Sanskrit word for a wide tree.}
With these considerations, we introduce a key-value store \textbf{Uruv}
(or, Uruvriksha) for wait-free updates and range search. More specifically, 
\begin{enumerate}[label=(\alph*)]
	\item \urv stores keys with associated values in leaf nodes structured as linked-list. The interconnected leaf nodes are indexed by a balanced tree of fat nodes, essentially, a classical B+ Tree \cite{comer+:ACM1979}, to facilitate fast key queries. (Section \ref{sec:Preliminaries})
	\item The key-nodes are augmented with list of versioned nodes to facilitate range scans synchronize with updates. (Section \ref{sec:MainAlgo})
	\item \urv uses single-word \cas primitives. Following the fast-path-slow-path technique of Kogan and Petrank \cite{kogan2012methodology}, we \textit{optimize} the helping procedure for wait-freedom. (Section  \ref{sec:WaitFree}). We prove \lbty and wait-freedom and present the upper bound of step complexity of operations. (Section \ref{sec:Correctness})
	\item Our C++ implementation of \urv significantly outperforms existing similar approaches -- lock-free B+tree of \cite{braginsky+:SPAA:2012}, and OpenBWTree \cite{wang2018building} for dictionary operations. It also outperforms a recently proposed method by Wei et al. \cite{wei+:PPOPP:2021} for concurrent workloads involving range search. (Section \ref{sec:Results})  
\end{enumerate}

\ignore{Traditional concurrency techniques tends to use mutexes. When multiple threads need to update a data structure, access is granted to only one thread at a time. This technique ensures updates do not get corrupted. However, there is a drawback with this technique. Since mutexes enable mutual exclusion, a thread, $t_1$, has to potentially wait for another thread, $t_2$, to finish its operation before accessing the data structure. If thread $t_2$ crashes or slows down considerably, thread $t_1$ will end up waiting indefinitely or for a long time. This will affect the data structure’s performance.

% Naive concurrency tends to use mutexes. 
 
Another type of technique to achieve synchronization among concurrent \op{s} by threads is \emph{non-blocking synchronization} which can potentially mitigate the drawbacks of locks. In this approach, concurrency is achieved without using mutexes and blocking of threads. Atomic operations like \texttt{compare-and-swap (CAS)} are used to achieve necessary modifications in the data structure. We deal with \emph{lock-free} \cite{Herlihy+:OnNatProg:opodis:2011} and \emph{wait-free} \cite{Herlihy+:OnNatProg:opodis:2011} non-blocking synchronization techniques in this paper. A significant attraction of lock-free synchronization is its progress guarantee: \textit{If multiple threads perform operations on a lock-free data structure, at least one of them will complete its operation in finite time.} A much stronger progress guarantee is provided by wait-free concurrency, wherein \emph{every} thread completes its operations in finite time.

Many lock-free data structures have been developed in recent years, including but not limited to linked lists \cite{chatterjee2016help, Heller+:LazyList:PPL:2007, Peri+:DAG:NETYS:2019, Timnat+:WFLis:opodis:2012, Valois:LFList:podc:1995, Zhang+:NBUnList:disc:2013} and binary search trees \cite{BrownER14, Chatterjee:+LFBST:PODC:2014, chatterjee2016help, EllenFRB10, Natarajan+:LFBST:ppopp:2014, Ramachandran+:LFIBST:ICDCN:2015}. However, not much work has been attempted for lock-free B$^+$Trees. \spnote{please add some sentences on why B+ trees are useful. Furthermore explain how B+ trees are relevant to our main theme of range queries.


Range search finds applications in a large number of analytics tasks. As the size of the main memory grows, it is imperative to perform such tasks without accessing the disc. In this context, the concurrent data structures that offer range search along with updates become very attractive for real-time analytics. 

Abhay: Hi sir, we have already talked about B+ Trees in the next paragraph. This paragraph just lists down relevant papers.} According to our literature review, Anastasia et al.\cite{braginsky+:SPAA:2012} proposed the first and only lock-free B$^+$Tree. Their tree uses a data structure, chunk, that can perform concurrent insert, delete and search operations on their nodes. Chunk is a continuous memory block where these operations can be performed with the help of a  lock-free linked list. We have developed an efficient lock-free range tree data structure, called Uruv, that is inspired by the B$^+$Tree’s design. Updates to \urv are lock-free, and lookups and range queries are wait-free.

The state of a data structure is defined by the keys and the data it contains. Sometimes, a subset of keys within some given range and their data is required. Range queries are operations that return a set of all the keys and their data in a data structure within a given range. The B$^+$Tree is a ranged index that inherently supports ranged queries. Making a range query concurrent with other operations is essential for data structure performance in a multi-threaded environment. A naive way to perform a range query is by locking a part of the data structure that belongs to the range. This faces the same performance problems we discussed regarding lock-based data structures. A lock-free range query achieves this result without using mutexes, enabling lock-free performance guarantees. We build on the work of Wei et al.\cite{wei+:PPOPP:2021}, who propose a constant-time lock-free snapshot algorithm. Their algorithm works by storing multiple versions of keys in a data structure. Each key’s version is identified by timestamp and stores data the key was mapped to at that time. (TO-DO Add related work for range query)

We begin by summarizing the B$^+$Tree's design in Section \ref{sec:Preliminaries}. We then explain Uruv's design in Section \ref{sec:Design}. We detail the lock-free algorithms in Section \ref{sec:MainAlgo} and build its wait-free construction in Section \ref{sec:WaitFree}. Further algorithmic details are in the Appendix. We prove its correctness in Section \ref{sec:Correctness} and then discuss the complexity of \urv in Section \ref{sec:Analysis}. We discuss our experimental results in Section \ref{sec:Results} and conclude the paper in Section \ref{sec:Conclusions}.
}
