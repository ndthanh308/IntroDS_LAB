\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{CAEMN22}

\bibitem[ACKS15]{awasthi2015hardness}
Pranjal Awasthi, Moses Charikar, Ravishankar Krishnaswamy, and Ali~Kemal Sinop.
\newblock The hardness of approximation of euclidean k-means.
\newblock {\em arXiv preprint arXiv:1502.03316}, 2015.

\bibitem[ADHP09]{aloise2009np}
Daniel Aloise, Amit Deshpande, Pierre Hansen, and Preyas Popat.
\newblock Np-hardness of euclidean sum-of-squares clustering.
\newblock {\em Machine learning}, 75(2):245--248, 2009.

\bibitem[ADK09]{aggarwal2009adaptive}
Ankit Aggarwal, Amit Deshpande, and Ravi Kannan.
\newblock Adaptive sampling for k-means clustering.
\newblock In {\em Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques}, pages 15--28. Springer, 2009.

\bibitem[AV07]{arthur2007k}
David Arthur and Sergei Vassilvitskii.
\newblock k-means++: The advantages of careful seeding.
\newblock In {\em Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1027--1035. Society for Industrial and Applied
  Mathematics, 2007.

\bibitem[BERS20]{bhattacharya2019noisy}
Anup Bhattacharya, Jan Eube, Heiko R{\"o}glin, and Melanie Schmidt.
\newblock Noisy, greedy and not so greedy k-means++.
\newblock In {\em 28th Annual European Symposium on Algorithms (ESA 2020)}.
  Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r Informatik, 2020.

\bibitem[BLHK16a]{bachem2016fast}
Olivier Bachem, Mario Lucic, Hamed Hassani, and Andreas Krause.
\newblock Fast and provably good seedings for k-means.
\newblock In {\em Advances in neural information processing systems}, pages
  55--63, 2016.

\bibitem[BLHK16b]{bachem2016approximate}
Olivier Bachem, Mario Lucic, S~Hamed Hassani, and Andreas Krause.
\newblock Approximate k-means++ in sublinear time.
\newblock In {\em Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[BLK17]{bachem2017distributed}
Olivier Bachem, Mario Lucic, and Andreas Krause.
\newblock Distributed and provably good seedings for k-means in constant
  rounds.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 292--300. JMLR. org, 2017.

\bibitem[BMV{\etalchar{+}}12]{bahmani2012scalable}
Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar, and Sergei
  Vassilvitskii.
\newblock Scalable k-means++.
\newblock {\em Proceedings of the VLDB Endowment}, 5(7):622--633, 2012.

\bibitem[BVX19]{bhaskara2019kmeans++_with_outliers_and_penalties}
Aditya Bhaskara, Sharvaree Vadgama, and Hong Xu.
\newblock Greedy sampling for approximate clustering in the presence of
  outliers.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[CAEMN22]{cohenaddad2022best_apx}
Vincent Cohen-Addad, Hossein Esfandiari, Vahab Mirrokni, and Shyam Narayanan.
\newblock Improved approximations for euclidean $k$-means and $k$-median, via
  nested quasi-independent sets, 2022.

\bibitem[CGPR20]{choo2020kmeans}
Davin Choo, Christoph Grunau, Julian Portmann, and V{\'a}clav Rozhon.
\newblock k-means++: few more steps yield constant approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  1909--1917. PMLR, 2020.

\bibitem[Das19]{Dasgupta}
Sanjoy Dasgupta.
\newblock Lecture 3 – algorithms for k-means clustering, 2013.
\newblock accessed May 8th, 2019.

\bibitem[G{\"O}RT22]{grunau_ozudogru_rozhon_tetek2022nearly}
Christoph Grunau, Ahmet~Alper {\"O}z{\"u}do{\u{g}}ru, V{\'a}clav Rozho{\v{n}},
  and Jakub T{\v{e}}tek.
\newblock A nearly tight analysis of greedy k-means++.
\newblock {\em arXiv preprint arXiv:2207.07949}, 2022.

\bibitem[GR20]{grunau_rozhon2020adapting_kmeans_to_outliers}
Christoph Grunau and Václav Rozhoň.
\newblock Adapting $k$-means algorithms for outliers, 2020.

\bibitem[LS19]{lattanzi2019better}
Silvio Lattanzi and Christian Sohler.
\newblock A better k-means++ algorithm via local search.
\newblock In {\em International Conference on Machine Learning}, pages
  3662--3671, 2019.

\bibitem[MRS20]{makarychev_reddy_shan2020improved}
Konstantin Makarychev, Aravind Reddy, and Liren Shan.
\newblock Improved guarantees for k-means++ and k-means++ parallel.
\newblock {\em Advances in Neural Information Processing Systems},
  33:16142--16152, 2020.

\bibitem[PVG{\etalchar{+}}11]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem[Roz20]{rozhovn2020simple}
V{\'a}clav Rozho{\v{n}}.
\newblock Simple and sharp analysis of k-means||.
\newblock In {\em International Conference on Machine Learning}, pages
  8266--8275. PMLR, 2020.

\bibitem[Wei16]{wei2016constant}
Dennis Wei.
\newblock A constant-factor bi-criteria approximation guarantee for k-means++.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  604--612, 2016.

\end{thebibliography}
