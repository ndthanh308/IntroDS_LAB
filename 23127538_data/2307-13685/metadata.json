{
  "title": "Noisy k-means++ Revisited",
  "authors": [
    "Christoph Grunau",
    "Ahmet Alper Özüdoğru",
    "Václav Rozhoň"
  ],
  "submission_date": "2023-07-25T17:45:41+00:00",
  "revised_dates": [],
  "abstract": "The $k$-means++ algorithm by Arthur and Vassilvitskii [SODA 2007] is a classical and time-tested algorithm for the $k$-means problem. While being very practical, the algorithm also has good theoretical guarantees: its solution is $O(\\log k)$-approximate, in expectation.\n  In a recent work, Bhattacharya, Eube, Roglin, and Schmidt [ESA 2020] considered the following question: does the algorithm retain its guarantees if we allow for a slight adversarial noise in the sampling probability distributions used by the algorithm? This is motivated e.g. by the fact that computations with real numbers in $k$-means++ implementations are inexact.\n  Surprisingly, the analysis under this scenario gets substantially more difficult and the authors were able to prove only a weaker approximation guarantee of $O(\\log^2 k)$. In this paper, we close the gap by providing a tight, $O(\\log k)$-approximate guarantee for the $k$-means++ algorithm with noise.",
  "categories": [
    "cs.DS"
  ],
  "primary_category": "cs.DS",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.13685",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 101850,
  "size_after_bytes": 102377
}