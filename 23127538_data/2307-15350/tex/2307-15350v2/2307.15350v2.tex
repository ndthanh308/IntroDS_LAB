\documentclass[10pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage{amsmath, amssymb, amsthm,graphicx}
\usepackage{natbib}
 \setlength{\textwidth}{18cm}
 \setlength{\textheight}{20cm}
% \setlength{\topmargin}{0cm}
 %\oddsidemargin=1pt
 \setlength{\oddsidemargin}{0mm}
\renewcommand{\baselinestretch}{1.3}
\renewcommand{\theequation}{\thesection .\arabic{equation}}
\newtheorem{rema}{Remark}
\newtheorem{asu}{Assumption}
\newtheorem{claim}{Claim}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defin}{Definition}
\newcommand{\rmd}{\,\mathrm{d}}
\def \n{\Vert}
\def \a {\theta}
\def \b {\beta}
\def \g {\gamma}
\def \d {\delta}
\def \l {{\lambda}}
\def \eps {\varepsilon}
\newcommand{\E}{\mathop{\mathbb{E}}}
\newcommand{\dist}{\mathop{\mathrm{dist}}}
\renewcommand{\P}{\mathop{\mathbb{P}}}
\newcommand{\ord}{\mathop{\mathrm{ord}}}
\newcommand{\supp}{\mathop{\mathrm{supp}}}
\def\R{{\mathbb{R}}}
\def\N{{\mathbb{N}}}
\def\Q{{\mathbb{Q}}}
\newcommand{\Z}{\mathbb{Q}}
\def\F{{\cal{F}}}
\def\XX{{\cal X}}
\def\XXX{{\cal X}^*}
\def\lf{\lfloor}
\def\rf{\rfloor}
\def\|{\,|\,}
\def\bn#1\en{\begin{align*}#1\end{align*}}
\def\bnn#1\enn{\begin{align}#1\end{align}}
\newcounter{tally}


\title{Optimal multi-environment causal regularization}
\date{}
\author{Philip Kennerberg and Ernst C. Wit}
\begin{document} 
\maketitle

\begin{abstract}
Causal inference is traditionally thought of as a method for learning the generative parameters of a stochastic system. Beyond this ontological concept, causal models also possess invariance properties, which make them attractive in changing environments. In this manuscript we derive the optimal out-of-sample causal predictor for a non-linear system that has been observed in $k+1$ within-sample environments. 
	
We study a model related to the one in \cite{kennerberg2023convergence}. In this model we consider $k$ shifted environments and one observational environment. Each environment corresponds to a non-linear structural equation model (SEM) with its own shift and noise vector, both in $L^2$. The strength of the shifts can be put in a certain order, and we may therefore speak of all shifts that are less or equally strong than a given shift. We consider the space of all shifts are $\gamma$ times less or equally strong than any weighted average of the observed shift vectors with weights on the unit sphere. For each $\beta\in\R^p$ we show that the supremum of the risk functions $R_{\tilde{A}}(\beta)$ over $\tilde{A}\in C^\gamma$ has a worst-risk decomposition into a (positive) non-linear combination of risk functions, depending on $\gamma$. We then define the causal regularizer, $\beta_\gamma$, as the  argument $\beta$ that minimizes this risk.  The main result of the paper is that this regularizer can be consistently estimated with a plug-in estimator outside a set of zero Lebesgue measure in the parameter space. A practical obstacle for such estimation is that it involves the solution of a general degree polynomial which cannot be done explicitly. Therefore we also prove that an approximate plug-in estimator using the bisection method is also consistent. An interesting by-product of the proof of the main result is that the plug-in estimation of the argmin of the maxima of a finite set of quadratic risk functions is consistent outside a set of zero Lebesgue measure in the parameter space. The paper concludes with a concentration in measure result (in the Appendix) for the plug-in estimator of the regularizer in the case when the co-variates are uncorrelated.

\end{abstract}
\section{Introduction}

\cite{icp} introduced invariant causal prediction as a way for predicting the outcome of a causal relationship between variables while maintaining its validity across different environments or contexts. It involves identifying causal relationships that are robust and invariant, meaning they hold true even when the conditions or settings change. 

Causal Dantzig is a method used for characterizing causal effects in observational studies by formulating it as a linear optimization problem \cite{causaldantzig}. Inspired by the Dantzig selector, the empirical version of this approach incorporates the idea of sparsity, assuming that only a few covariates have a direct causal impact on the outcome variable. By minimizing a penalized loss function subject to a set of linear constraints, Causal Dantzig selects a subset of relevant covariates and estimates their causal effects while controlling for confounding variables. 

Anchor regression is a statistical method used to estimate the causal effect of a treatment or intervention on an outcome variable by accounting for potential confounding variables \citep{Rot}. It involves selecting a subset of covariates, known as \emph{anchor variables}, that are strongly associated with both the treatment and outcome variables. By regressing the outcome variable on the treatment and anchor variables, anchor regression helps mitigate bias caused by confounding factors and provides a more accurate estimation of the treatment effect. It serves as a useful tool in causal inference and allows researchers to make informed decisions about the effectiveness of interventions while controlling for potential confounding variables. 

\cite{kania2022causal} introduced a related approach, called \emph{causal regularization}. This method does not require explicit information on the auxilary variables, and which comes with strong out-of-sample risk guarantees.  \cite{kennerberg2023convergence} studies an extension of this model considering $k$ shifted environments, and one observational environment. The paper proved  explict convergence properties of the causal regularizer in the multi-environment setting. 

In section~\ref{sec:outofsamplerisk} we introduce the multi-environment setting, where each environment corresponds to a non-linear structural equation model (SEM) with its own $L^2$ additive shift and $L^2$ additive noise vector. In section~\ref{sec:optimalCR}, we define the space of all out-of-sample environments, that correspond to shifts that are $\gamma$ times less or equally strong than any weighted average of the observed shift vectors. For each $\beta\in\R^p$ we show that if we consider the supremum of the risk functions $R_{\tilde{A}}(\beta)$ over $\tilde{A}\in C^\gamma$ then this supremum has a worst-risk decomposition into a (positive) linear combination of risk functions, depending on $\gamma$. Analogous to \cite{kania2022causal}, we then define the causal regularizer, $\beta_\gamma$, as the argument $\beta$ that minimizes this worst risk.  In section~\ref{sec:estimation} we show that this regularizer can be consistently estimated with a plug-in estimator outside a set of zero Lebesgue measure in the parameter space. A practical obstacle for such estimation is that it will involve solution of a general degree polynomial which cannot be done explicitly. Therefore we also prove that an approximate plug-in estimator using the bisection method is also consistent. An interesting by-product of the proof is that plug-in estimation of the argmin of the maxima of a finite set of quadratic risk functions is consistent outside a set of zero Lebesgue measure in the parameter space. The paper concludes with a concentration in measure result (in the Appendix) for the plug-in estimator of the regularizer in the case when the co-variates are uncorrelated.



\section{Out-of-sample risk}
\label{sec:outofsamplerisk}

In \cite{kennerberg2023convergence} the following model was studied. Consider structural equation models (SEMs) of the following type 
\begin{equation*}
\begin{bmatrix}
Y^{A_i}\\
X^{A_i}
\end{bmatrix} = 
B(\omega)\cdot\begin{bmatrix}
Y^{A_i}\\
X^{A_i}
\end{bmatrix}
+\epsilon_{A_i}
+\begin{bmatrix}
0\\
A_i
\end{bmatrix}
\end{equation*}
for $1\le i\le k$ , $k\in\N$, where $B(\omega)$ is a random real-valued $(p+1)\times(p+1)$ matrix and we say that we have $k$ different environments corresponding to each set of equations above. 
The random vector $A_i$ is called the shift corresponding to environment $i$. We assume that $I-B$ has full rank a.s., which implies that $X^{A_i}$ and $Y^{A_i}$ have unique solutions. We also consider the observational environment 
\begin{equation}\label{SEMO}
\begin{bmatrix}
Y^{O}\\
X^{O}
\end{bmatrix} = 
B(\omega)\cdot\begin{bmatrix}
Y^{O}\\
X^{O}
\end{bmatrix}
+\epsilon_{O}
\textit{ where }
\epsilon_O=
\begin{bmatrix}
\epsilon_{Y^{O}}\\
\epsilon_{X^{O}}
\end{bmatrix},
\end{equation}

We assume that we are given a set of $k$ shift vectors $A_1,...,A_k\in \R^p$ whose components $A_i(l)\in L^2$, $1\le l\le p$, $1\le i\le k$. From now on $A$ will denote the set $\{A_0,A_1,...,A_k\}$ and $\mathcal{A}$ will denote any convex subset of $p$-dimensional random vectors in $L^2(\P)$ that contains $A$. Where we denote $A_0=\textbf{0}$ (i.e. corresponding to the observational environment). We assume that the noise terms $\epsilon_O,\epsilon_{X^{A_1}},...,\epsilon_{X^{A_k}},\epsilon_{Y^{A_1}},...,\epsilon_{Y^{A_k}}\in\R^p$ and its components are in $L^2$. We also assume that the noise terms are uncorrelated with the shifts.
\\
Let $\mathcal{A}=(L^2(\P))^p$. Given a vector $w\in\mathcal{W}$ we define $A_w=\sum_{i=1}^kw_iA_i$ and
$$C^\gamma_w(B)=\left\{A\in \mathcal{A}\E\left[AA^T\|B\right]\preccurlyeq \gamma \E\left[A_wA_w^T\|B\right]\textsf{ a.s.}\right\}.$$
In particular if $B$ is any non-zero constant matrix then we get a linear SEM and
$$C^\gamma_w(B)=\left\{A\in \mathcal{A}:\mathbb{C}(A)\preccurlyeq \gamma \mathbb{C}(A_w)\right\}.$$ 
If $B$ is a simple map i.e. $B=\sum_{l=1}^mB_l1_{C_l}$ then we get a piecewise linear SEM and
$$C^\gamma_w(B)=\left\{A\in \mathcal{A}:\mathbb{C}(A1_{C_l})\preccurlyeq \gamma \mathbb{C}(A_w1_{C_l}), 1\le l\le m\right\}.$$
For any shift vector $A$ we denote $R_{A}(\beta)=\E\left[\left(Y^A-\beta X^{A}\right)^2\right]$.
Another case of special interest is the case when we only study shifts that are independent of $B$, i.e. 
$$C^\gamma_w(B)=\left\{A\in \mathcal{A}:\sigma(A)\perp\sigma(B),\E\left[AA^T\right]\preccurlyeq \gamma \E\left[A_wA_w^T\right]\right\}.$$
The condition $E\left[\epsilon A\|B\right]=0$ is equivalent to $\epsilon\E\left[A\right]=0$ a.s. in this case, which means that all shifts have zero mean unless there is no noise. Note that the linear case is contained in this class. For any shift vector $A$ denote $R_{A}(\beta)=\E\left[\left(Y^A-\beta X^{A}\right)^2\right]$. Letting
\begin{itemize}
\item[] $R^w_+(\beta)=\sum_{i=1}^kw_i^2R_{A_i}+R_O(\beta)$ and
\item[] $R^w_\Delta(\beta)=\sum_{i=1}^kw_i^2R_{A_i}-R_O(\beta)$
\end{itemize}
the following worst risk decomposition follows
\begin{prop}\label{suppropfixedw}
Let $\tau\ge -\frac 12$ and $w\in\mathcal{W}$ then 
$$\sup_{\tilde{A}\in C^{1+\tau}_w}R_{\tilde{A}}(\beta)=\frac{1}{2}R^{w}_+(\beta)+\frac{1+2\tau}{2}R^{w}_\Delta(\beta).$$
\end{prop}
We then defined our casual regularizer with parameter $\gamma\in [0,\infty]$, as the solution to
\begin{align}\label{betaw}
\beta_\gamma=\arg\min R_+^w(\beta)+\gamma R^w_\Delta(\beta)
\end{align}
Letting
\begin{itemize}
\item[] $G_+=\E\left[(X^O)^TX^O+\sum_{i=1}^kw_i^2(X^{A_i})^TX^{A_i}\right]$,
\item[]$G_\Delta=\E\left[\sum_{i=1}^kw_i^2(X^{A_i})^TX^{A_i}-(X^O)^TX^O\right]$,
\item[] $Z_+=\E\left[(X^O)^TY^O+\sum_{i=1}^kw_i^2(X^{A_i})^TY^{A_i}\right]$ and $Z_\Delta=\E\left[\sum_{i=1}^kw_i^2(X^{A_i})^TY^{A_i}-(X^O)^TY^O\right],$
\end{itemize}
we have the following explicit solution to the regularizer.
\begin{prop}
If $G_++\gamma G_\Delta$ is full rank then
$$ \beta_\gamma=\left(G_++\gamma G_\Delta\right)^{-1}\left(Z_++\gamma Z_\Delta\right). $$
\end{prop}
In \cite{kennerberg2023convergence} it is shown that the plug-in estimator for $\beta_\gamma$ is consistent. Furthermore, results regarding concentration in measure, rates of decay for conditional absolute central moments and the determinant of the conditional covariance matrix were proven.


\section{The optimal causal regularizer}
\label{sec:optimalCR}
Instead of having fixed weights for the different environments we consider here all the optimal weights across all of $\mathcal{W}$ in the following sense,
$$C^\gamma(B)=\left\{A'\in\mathcal{A}:\exists w\in\mathcal{W}, \E\left[A'(A')^T\|B\right]\preccurlyeq \gamma \E\left[A_w(A_w)^T\|B\right] a.s.\right\}.$$
It is worth noting that
$$C^\gamma(B)=\bigcup_{w\in\mathcal{W}}C_w^\gamma(B).$$
From now on we will remove the dependence on $B$ from $C^\gamma(B)$ and $C_w^\gamma(B)$, simply writing $C^\gamma$ and $C_w^\gamma$ respectively instead.
\begin{prop}\label{optimalsol}
Let $\tau\ge -\frac 12$ then 
\begin{align}\label{supw}
\sup_{\tilde{A}\in C^{1+\tau}}R_{\tilde{A}}(\beta)=\frac{1}{2}R^{w^*}_+(\beta)+\frac{1+2\tau}{2}R^{w^*}_\Delta(\beta),
\end{align}
for some $w^*\in\mathcal{W}$. Moreover $w^*$ is achieved by setting $w_i=0$ if $R_{A_i}(\beta)<\max_{1\le l\le k}R_{A_l}(\beta)$ and then distributing $w^*$ among the environments such that $R_{A_i}(\beta)=\max_{1\le l\le k}R_{A_l}(\beta)$ while still being on the unit circle.
\end{prop}
\begin{proof}
Note that if we let $u=\left((I-B)^{-1}_{1:p,\textbf{.}}\beta-((I-B)^{-1})_{p+1,\textbf{.}}\right)$ then
\begin{align}\label{supeq1}
&\sup_{\tilde{A}\in C^{1+\tau}}R_{\tilde{A}}(\beta)=
\sup_{\tilde{A}\in C^{1+\tau}}\E\left[u\left(\epsilon+\tilde{A}\right)\left(\epsilon+\tilde{A}\right)^Tu^T\right]=u\E\left[\epsilon\epsilon^T\right] u^T
+
\sup_{\tilde{A}\in C^{1+\tau}}u\E\left[\tilde{A}\tilde{A}^T\right]u^T
\end{align}
which implies that the supremum is attained (is a maximum) if and only if $\sup_{\tilde{A}\in C^{1+\tau}}u\E\left[\tilde{A}\tilde{A}^T\right]u^T$ attains a maximum. We now show that this is indeed the case. Note that since $C^\gamma=\bigcup_{w\in\mathcal{W}}C^\gamma_w$, any element in $C^\gamma$ can be identified with a $w\in\mathcal{W}$. Given some row vector $v$ of dimension $p$, consider $L(\tilde{A})=v\E\left[\tilde{A}\tilde{A}^T\right]v^T$ and $S=\sup_{\tilde{A}\in C^{\gamma}}L(A)$. There exists a sequence $\{B_n\}_n\subset C^\gamma$ such that $\lim_nL(B_n)=S$ and for each $n$, $B_n\in C^\gamma_{w_n}$ for some $w_n\in\mathcal{W}$. Since $\mathcal{W}$ is compact there exists a subsequence $\{w_{n_k}\}_k$ such that $w_{n_k}\to w^*$ for some $w^*\in\mathcal{W}$. Since $\lim_kL(B_{n_k})=S$ and $L(B_{n_k})\le \gamma L(A_{w_{n_k}})$ and $\lim_kL(A_{w_{n_k}})=L(A_{w^*})$ we must have that $S=L(A_{w^*})$, so the supremum is in fact a maximum attained at $A_{w^*}$ (that depends on $v$). We now show \eqref{supw}.
%Given an optimal $w^*$ as above,
If $\tilde{A}\in C_{w^*}^{1+\tau}$ then
\begin{align*}
\E\left[v\tilde{A}\tilde{A}^Tv^T\right]
&=
\E\left[\E\left[v\tilde{A}\tilde{A}^Tv^T\|B\right]\right]
=
\E\left[v\E\left[\tilde{A}\tilde{A}^T\|B\right]v^T\right]
\le
(1+\tau)\E\left[v\E\left[A_{w^*}A_{w^*}^T\|B\right]v^T\right]
\\
&=(1+\tau)\E\left[\E\left[vA_{w^*}A_{w^*}^Tv^T\|B\right]\right]
=\E\left[(1+\tau)vA_{w^*}A_{w^*}^Tv^T\right],
\end{align*}
i.e. $\sup_{\tilde{A}\in C^{1+\tau}_{w^*}} \E\left[v\tilde{A}\tilde{A}^Tv^T\right]\le \E\left[(1+\tau)vA_{w^*}A_{w^*}^Tv^T\right]$. Since $\sqrt{(1+\tau)}A_{w^*}\in C_{w^*}^{1+\tau}$ it follows that 
$$\sup_{\tilde{A}\in C^{1+\tau}_{w^*}} \E\left[v\tilde{A}\tilde{A}^Tv^T\right]=\E\left[(1+\tau)vA_{w^*}A_{w^*}^Tv^T\right].$$
Going back to \eqref{supeq1} we find
\begin{align}\label{supeq}
\sup_{\tilde{A}\in C^{1+\tau}_w}R_{\tilde{A}}(\beta)
&=
\E\left[v\epsilon\epsilon^Tv^T\right]+(1+\tau)\E\left[vA_{w^*} A_{w^*}^Tv^T\right]
\nonumber
\\
&=R_O(\beta)
+
(1+\tau)\E\left[v(A_{w^*}+\epsilon) (A_{w^*}+\epsilon)^Tv^T\right]
-
(1+\tau)\E\left[v\epsilon \epsilon^Tv^T\right]
\nonumber
\\
&=R_{O}(\beta)+(1+\tau)R_{A_{w^*}}(\beta)-(1+\tau)R_{O}(\beta)
\nonumber
\\
&=
(1+\tau)R_{A_{w^*}}(\beta)-\tau R_{O}(\beta).
\end{align}
Since
\begin{align*}
&R_{A_{w^*}}(\beta)=\E\left[\left(X^{A_{w^*}}\beta-Y^{A_{w^*}}\right)^2\right]=\E\left[v\left(\epsilon+\sum_{i=1}^kw^*_iA_i\right)\left(\epsilon+\sum_{i=1}^kw^*_iA_i\right)^Tv^T\right]
=\sum_{i=1}^k(w^*_i)^2R_{A_i,l}(\beta),
\end{align*}
we may plug this back into \eqref{supeq} and get
$$ \sup_{\tilde{A}\in C^{1+\tau}_{w^*}}R_{\tilde{A}}(\beta)=(1+\tau)\sum_{i=1}^k(w^*_i)^2R_{A_i}(\beta)-\tau R_{O}(\beta)=\frac{1}{2}R_+(\beta)+\frac{1+2\tau}{2}R_\Delta(\beta).$$


As for the second claim we proceed with a proof by contradiction. Suppose
$$\sup_{\tilde{A}\in C^{1+\tau}}R_{\tilde{A}}(\beta)\not=\sup_{w\in\mathcal{W}}\sup_{\tilde{A}\in C_w^{1+\tau}}R_{\tilde{A}}(\beta).$$
Analogously to \eqref{supeq1} we have that 
$$\sup_{\tilde{A}\in C_w^{1+\tau}}R_{\tilde{A}}(\beta)= u\E\left[\epsilon\epsilon^T\right] u^T
+
\sup_{\tilde{A}\in C_w^{1+\tau}}u\E\left[\tilde{A}\tilde{A}^T\right]u^T=u\E\left[\epsilon\epsilon^T\right] u^T
+
u\E\left[A_wA_w^T\right]u^T,$$
so by assumption $\sup_{w\in\mathcal{W}}u\E\left[A_wA_w^T\right]u^T\not=u\E\left[A_{w^*}A_{w^*}^T\right]u^T$
and this directly contradicts the definition if $w^*$. If we let 
$$g(w)=(1+\tau)\sum_{i=1}^k(w_i)^2R_{A_i}(\beta)-\tau R_O(\beta)=\frac{1}{2}R^w_+(\beta)+\frac{1+2\tau}{2}R^w_\Delta(\beta)$$
then $g$ is obviously continuous and since $\mathcal{W}$ is compact it attains a maximum on this set. If $R_{A_j}(\beta)>R_{A_i}(\beta)$ for all $i\not= j$ then it is readily seen that $g$ is maximized by setting $w_j=1$ and $w_i=0$ for $i\not=j$. In case there are several environments that are tied for the maximal environmental risk then we can distribute the weights among these environments freely as long as $w\in\mathcal{W}$.
\end{proof}
It is important to note that $w^*$ as defined above depends on $\beta$ and $\gamma$. Sometimes we make the dependence on $\beta$ explicit by writing $w^*(\beta)$. Now we may introduce the casual regularizer in this setting analogously to the fixed weight setting.  With the notation from Proposition \ref{optimalsol}, let 
\begin{align}\label{betaopt}
\mathcal{B}=\arg\min_{\beta\in\R^p}R^{w^*(\beta)}_+(\beta)+\gamma R^{w^*(\beta)}_\Delta(\beta)
\end{align}
denote the set of argmin solutions. Unlike \eqref{betaw} there may indeed be several solutions to \eqref{betaopt}. 


\section{Estimation of the regularizer}
\label{sec:estimation}
We now turn to the problem of estimating $\beta_\gamma$. To do this we must first set up a framework for how we handle samples from multiple different environments. We assume that for each environment $i$ we have an i.i.d. sequence $\{(Y_u^{A_i},X_u^{A_i})\}_{u=1}^\infty$ where $(Y_u^{A_i},X_u^{A_i})$ is distributed according to the given SEM for environment $i$ and we assume that the sequences $\{(Y_u^{A_i},X_u^{A_i})\}_{u=1}^\infty$ are mutually independent across the different $A_i$'s.
Suppose the sample size is $\textbf{n}=\left\{n_{A_0},...,n_{A_k}\right\}$, let $\mathbb{X}^{A_i}(\textbf{n})$ be the $n_{A_i}\times p$ matrix whose rows are $X^{A_i}_1,...,X^{A_i}_{n_{A_i}}$ (from top to bottom) and similarly let $\mathbb{Y}^{A_i}(\textbf{n})$ be the $n_{A_i}\times 1$ column vectors whose entries are $Y^{A_i}_1,...,Y^{A_i}_{n_{A_i}}$ (from top to bottom). Let $G^i=\E\left[(X^{A_i})^2\right]$, $G_Y^i=\E\left[(Y^{A_i})^2\right]$, $\mathbb{G}_Y^i=\frac{1}{n_{A_i}}\sum_{l=1}^{n_{A_i}}(Y^{A_i}_l)^2$, $Z^i=\E\left[X^{A_i}Y^{A_i}\right]$ and  $\mathbb{Z}^i=\frac{1}{n_{A_i}}\sum_{l=1}^{n_{A_i}}X^{A_i}_lY^{A_i}_l$.
Denote 
$$\mathbb{G}^i_+(\textbf{n})=\frac{1}{n_{A_i}}(\mathbb{X}^{A_i})^T\mathbb{X}^{A_i}+\frac{1}{n_O} (\mathbb{X}^{O})^T\mathbb{X}^{O},$$
$$\mathbb{G}^i_\Delta(\textbf{n})=\frac{1}{n_{A_i}}(\mathbb{X}^{A_i})^T\mathbb{X}^{A_i}-\frac{1}{n_O} (\mathbb{X}^{O})^T\mathbb{X}^{O},$$
$$\mathbb{G}^i(\textbf{n})=\frac{1}{n_{A_i}}(\mathbb{X}^{A_i})^T\mathbb{X}^{A_i},$$
$$\mathbb{G}_Y^i=\frac{1}{n_{A_i}}\sum_{l=1}^{n_{A_i}}(Y^{A_i}_l)^2,$$ 
$$\mathbb{Z}^i_+(\textbf{n})=\frac{1}{n_{A_i}}(\mathbb{X}^{A_i})^T\mathbb{Y}^{A_i}+\frac{1}{n_O} (\mathbb{X}^{O})^T\mathbb{Y}^{O},$$
$$\mathbb{Z}^i_\Delta(\textbf{n})=\frac{1}{n_{A_i}}(\mathbb{X}^{A_i})^T\mathbb{Y}^{A_i}-\frac{1}{n_O} (\mathbb{X}^{O})^T\mathbb{Y}^{O}$$ and
$$\mathbb{Z}^i=\frac{1}{n_{A_i}}\sum_{l=1}^{n_{A_i}}X^{A_i}_lY^{A_i}_l$$ 
Consider the set of corresponding plug-in estimators to \eqref{betaopt},
\begin{align}\label{plugin}
&\hat{B}(\textbf{n})=\arg\min_{\beta\in\R^p}\mathbb{R}^{\hat{w}^*(\beta),\beta}_+(\beta)+\gamma \mathbb{R}^{\hat{w}^*(\beta),\beta}_\Delta(\beta),
\end{align}
where
$$\R_+^{w,\beta}(\textbf{n})=\sum_{i=1}^kw_i^2\frac{2}{n_{A_i}}\n \mathbb{Y}^{A_i}(\textbf{n})- \beta^T\mathbb{X}^{A_i}(\textbf{n})\n_2^2+\frac{1}{n_O} \n \mathbb{Y}^{O}(\textbf{n})- \beta^T\mathbb{X}^{O}(\textbf{n})\n_2^2,$$
$$\R_\Delta^{w,\beta}(\textbf{n})=\sum_{i=1}^kw_i^2\frac{2}{n_{A_i}}\n \mathbb{Y}^{A_i}(\textbf{n})- \beta^T\mathbb{X}^{A_i}(\textbf{n})\n_2^2-\frac{1}{n_O} \n \mathbb{Y}^{O}(\textbf{n})- \beta^T\mathbb{X}^{O}(\textbf{n})\n_2^2$$
and $\hat{w}^*(\beta)$ are chosen as the weights $w$ that maximize $\mathbb{R}^{w,\beta}_+(\beta)+\gamma \mathbb{R}^{w,\beta}_\Delta(\beta)$. Note that there may be several plug-in estimators, stemming from the fact that there may be several solutions \eqref{plugin}.
\subsection{Main results}
The main result of this paper is the consistency of the estimator described in \eqref{betaopt} for "most" choices of parameters.
Denote 
%\begin{itemize}
%\item $a_i(l)=\E\left[\left(X^{A_i}(l)\right)^2\right]$,
%\item $b_i(l)=\E\left[X^{A_i}(l)Y^{A_i}\right]$
%\item $a_i(u,v)=\E\left[X^{A_i}(u)X^{A_i}(v)\right]$
%\item $c_i=\E\left[\left(Y^{A_i}\right)^2\right]$
%\end{itemize}
$a_i(l_1,l_2)=\E\left[X^{A_i}(l_1)X^{A_i}(l_2)\right]$, for $l_1\not=l_2$, $a_i(l)=a_i(l,l)=\E\left[\left(X^{A_i}(l)\right)^2\right]$, $b_i(l)=\E\left[X^{A_i}(l)Y^{A_i}\right]$ and $c_i=\E\left[\left(Y^{A_i}\right)^2\right]$. These are the parameters which we will use to identify the points in our parameter space which we define as follows.
Let $\Theta=(\R^+)^p\times(\R^+)^p\times\R^p\times\R^p\times\R^{p\times(p-1)}\times\R^{p\times(p-1)}\times\R^+\times \R^+\subset \R^{2(p^2+p+1)}$ denote the possible parameter space for each environment so that for environment $i$ we associate
$$\theta_i=\left(\{a_i(l)\}_1^p,\{b_i(l)\}_1^p,\{a_i(l_1,l_2)\}_{l_1\not=l_2, 1\le l_1\le p,1\le l_2\le p},c_i\right)$$
and similarly for a pairing of two environments, $(i,j)$, $i\not=j$ we can associate an element $\theta_{i,j}\in \Theta\times\Theta$ defined by
$$\theta_{i,j}=\left(\{a_i(l)\}_1^p,\{a_j(l)\}_1^p,\{b_i(l)\}_1^p,\{b_j(l)\}_1^p,\{a_i(l_1,l_2)\}_{l_1\not=l_2, 1\le l_1\le p,1\le l_2\le p},\{a_j(l_1,l_2)\}_{l_1\not=l_2, 1\le l_1\le p,1\le l_2\le p},c_i,c_j\right).$$
Let $\dist(x,E)=\inf\{\n y-x\n_2: y\in E\}$ for $x\in\R^m$, $E\subset\R^m$ for some $m\in\N$ and where $\n.\n_2$ denotes the Euclidean norm.
\begin{thm}\label{optimalregthm}
For every pair of environments $(i,j)$, $i\not=j$, outside a subset of Lebesgue measure zero $N\in\Theta\times\Theta$ of choices of $\theta_{i,j}$, we have that $\mathcal{B}\not=\emptyset$ (i.e. at least one solution to \eqref{betaopt} is guaranteed to exist) and $\dist\left(\hat{\beta}_\gamma(\textbf{n}),\mathcal{B}\right)\xrightarrow{a.s.}0$, for any $\hat{\beta}_\gamma(\textbf{n})\in\hat{B}(\textbf{n})$ as $n_{A_0}\wedge...\wedge n_{A_k}\to\infty$, outside $N$.
\end{thm}
Before the proof of Theorem \ref{optimalregthm} we will need to construct a sizeable toolbox in the form of several Lemmas that will now follow.
For each $\theta\in\Theta$ we may bijectively associate a pairing of an affine (and symmetric) matrix function and an affine row vector as follows.
\begin{defin}\label{parammatrix}
For every $\theta_{i,j}\in\Theta\times\Theta$ let the \textit{affine covariate matrix},  $M^{i,j}(\lambda)$ and the \textit{affine target vector} $C^{i,j}(\lambda)$ be defined element wise by
$$C^{i,j}(\lambda)_u=b_i(u)-\lambda\left(b_i(u)-b_j(u)\right),$$
$$M^{i,j}(\lambda)_{l,l} =a_i(u)+\lambda(a_i(u)-a_j(u))).$$
When $u\not=v$
$$M^{i,j}(\lambda)_{u,v} =a_i(u,v)+\lambda \left( a_i(u,v)-a_j(u,v)\right).$$
\end{defin}
We may regard $\det\left(M^{i,j}(\lambda)\right)$ as polynomial in $\R$ with coefficients in $\Theta\times\Theta$. Doing so we will denote the roots in terms of $\lambda$ as $\Lambda(\theta_{i,j})$ where we highlight the dependence on $\theta_{i,j}\in\Theta\times\Theta$. We will denote the real roots of $\det\left(M^{i,j}(\lambda)\right)$ as $\Lambda(\theta_{i,j})_\R$. Often times we will suppress the dependence on $\theta_{i,j}$ for brevity when we see fit.
The following result comes from complex analysis.
\begin{lemma}\label{realsimple}
Let $P(\lambda)=\sum_{u=0}^ma_u\lambda^u$ be a polynomial with real coefficients ($a_m\not=0$) with a real simple root $r$. For any $0<\epsilon<\min_{1\le u<z\le v}|r_u-r_z|$ there exists $\delta>0$ such that if we consider any polynomial of the form $\tilde{P}(\lambda)=\sum_{u=0}^mb_u\lambda^u$ with $|a_u-b_u|<\delta$ then $\tilde{P}$ must have a simple real root in $(r_u-\epsilon,r_u+\epsilon)$.
\end{lemma}
The above result is then applied to get the following lemma which is what we actually will need.
\begin{lemma}\label{simpleroots}
Let $P(\lambda)=\sum_{u=0}^ma_u\lambda^u$ be a polynomial with real coefficients ($a_m\not=0$) and whose real roots $r_1<...<r_v$ ($v\le m$) are simple. If $P_n(\lambda)=\sum_{u=0}^ma_{u,n}\lambda^u$ is a sequence of polynomials such that $a_{u,n}\to a_u$, $u=0,...,m$ then if we denote the real roots of $P_n(\lambda)$ as $r_1(n)<...<r_{w_n}(n)$ then $w_n=v$ for large enough $n$ and $r_u(n)\to r_u$ for $u=1,...,v$.
\end{lemma}
\begin{proof}
We know by Lemma \ref{realsimple} that for any $0<\epsilon<\min_{1\le u<z\le v}|r_u-r_z|$ there exists $\delta>0$ such that if $|a_u-a_{u,n}|<\delta$ then $P_n$ must have a simple real root in $(r_u-\epsilon,r_u+\epsilon)$, which shows that $w(n)\ge v$ for large enough $n$ and that $r_u(n)\to r_u$ for $u=1,...,v$. If $v=m$ we are done, since then all roots are real and simple. If $v=m$ we are done. Suppose instead that $v<m$. We know that all roots of $P_n$ must converge to those of $P$ so take any root $r$ of $P$ with non-zero imaginary part $c=\mathrm{Im}(r)$ then we know that for large enough $n$, $P_n$ must have a root $r'(n)$ such that $|r-r'(n)|<c/2$ which implies that the imaginary part of $r'(n)$ is non-zero for such $n$. But this is true for all roots of $P$ that are not real and since $P_n$ has the same degree as $P$ for large enough $n$ (when $a_{m,n}\not=0$), $P_n$ must have the same number of roots that are not real as $P$, i.e. $w_(n)=v$ for such $n$.
\end{proof}
A result from measure theory which will lie at the heart of the method by which we prove Theorem \ref{optimalregthm} is the following.
\begin{lemma}\label{PolyLemma}
A polynomial on $\R^n$, for any $n\in\N$ is either identically zero or is only zero on set of Lebesgue measure zero.
\end{lemma}
The following lemma is an immediate application of the one above and illustrates how we will apply the above lemma in this paper.
\begin{lemma}\label{discriminantLemma}
Any non-zero polynomial $P_{\theta_{i,j}}\left(\lambda\right)$ on $\R\times\Theta\times\Theta$ has simple roots (in terms of $\lambda$) outside a set of measure zero in $\Theta\times\Theta$.
\end{lemma}
\begin{proof}
$P$ only has simple roots if the discriminant is non-zero. The discriminant is a (non-zero) polynomial on $\Theta\times\Theta$ and therefore is only zero on a set of measure zero in $\Theta$.
\end{proof}
We will from now on omit the dependence on $\theta_{i,j}$ of polynomials of the kind above. The reason being that we regard it as polynomial on $\R$ with coefficients in $\Theta\times\Theta$ and we are interested in the roots in terms of $\lambda$.
\begin{lemma}\label{matrixLemma}
The following results holds for the affine parameter matrix $M^{i,j}(\lambda)$ as defined in \ref{parammatrix}.
\begin{itemize}
\item [1] Outside a set $N$ of measure zero in $\Theta$, there are at most $p$ elements in $\Lambda$ and for any $\lambda\in\Lambda$,
$rank\left(M^{i,j}(\lambda)\right)=p-1$.
\item[2] Moreover for any such $\lambda$ there is a unique set of $p-1$ real numbers $s_1(\theta,\lambda),...,s_{p-1}(\theta,\lambda)$, all of which must be non-zero and such that 
\begin{align}\label{coeffrepreq}
&M^{i,j}_{p,.}(\lambda)=\sum_{u=1}^{p-1}s_u(\theta,\lambda)M^{i,j}_{u,.}(\lambda),
\end{align}
outside $N$.
\end{itemize}
 
\end{lemma}
\begin{proof}
Denote $P(\lambda)=\det\left(M^{i,j}(\lambda)\right)$, by the fundamental theorem of algebra $P$ has at most $p$ distinct solutions in $\mathbb{C}$. The rank of $M^{i,j}(\lambda)$ is $p-1$ if and only if there exists some non-zero minor, that is to say there exists a minor that has no roots in common with $P$. Consider a submatrix $M_{u,v}(\lambda)$ (where the subscripts $u,v$ indicate that we remove row $u$ and column $v$) of $M^{i,j}(\lambda)$, and let $P_{u,v}(\lambda)=\det\left(M_{u,v}(\lambda)\right)$. We wish to show that outside a set of measure zero in $\Theta\times\Theta$, there exists a minor such that $P$ and $P_{u,v}$ has no roots in common. $P$ and $P_{u,v}$ has a root in common root in $\mathbb{C}$ if and only if their resultant is zero (since $\mathbb{C}$ is an algebraically closed field). But the resultant is a polynomial on $\Theta\times\Theta$ so by Lemma \ref{PolyLemma} this polynomial is either the zero polynomial or it is zero on a set of measure zero in $\Theta\times\Theta$. So to prove [1], it suffices to show that there exists any $\theta\in\Theta\times\Theta$ such that the resultant of $P$ and $P_{u,v}$ is not zero. Recall however that this is true if and only if for some $\theta$, $P(\lambda)=0$ implies $P_{u,v}(\lambda)\not=0$ for some $u,v\in\{1,...,p\}$ and all $\lambda\in\R$. For this purpose consider the matrix $M'(\lambda)$ where 
%
%the first row is given by $\left(\lambda\hspace{3mm}\lambda-1\hspace{3mm}0...\hspace{3mm}0\right)$, the first column is $\left(\lambda\hspace{3mm}\hspace{3mm}0...\hspace{3mm}0\right)^T$, 1 on all the remaining diagonal elements and then zero on the rest of the entries. 
\begin{equation*}
M'(\lambda) = 
\begin{pmatrix}
\lambda & \lambda-1 &0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0  \\
\vdots  & \vdots  & \ddots & \vdots & \vdots \\
0 & \cdots & 0 & 1 & 0  \\
0 & \cdots & 0 & 0 & 1
\end{pmatrix}
\end{equation*}
In this case the only root to $P$ is given by $\lambda=0$ while the only root for $P_{1,2}$ is given by $\lambda=1$. This proves the [1]. For the final result, it is enough to show that every first minor of the form $\det\left(M^{i,j}_{u,1}(\lambda)\right)$ for $u=1,...,p-1$ is non-zero since then $M^{i,j}_{p,.}(\lambda)$ must be linearly independent from all subsets of $n-2$ other rows. $\det\left(M^{i,j}_{1,u}(\lambda)\right)=0$ for a root of $P$ if and only if $res(P_{u,v},P)$ is zero. Again it suffices to show that for each $1\le u\le p-1$ there exists any $\theta\in\Theta\times\Theta$ such that $P$ and $P_{u,1}$ have no common root. Letting $\theta$ be such that $M^{i,j}(\lambda)$ is the diagonal matrix with $\lambda$ as diagonal element number $u$ and all other diagonal entries one implies that $P$ only has the root $0$ and $P_{u,1}$ has no roots at all. This completes the proof of the final claim.
\end{proof}
%We will now introduce a sequence of maps $\{T_v\}_{v=1}^{p-1}$ which (we will show) will bring $M^{i,j}(\lambda)$ do a row echelon form in $p-1$ steps. For any root of $\lambda\in\Lambda$, let $T_1(M^{i,j}(\lambda))$ denote the matrix resulting from subtracting $\frac{M^{i,j}(\lambda)(u,1)}{M^{i,j}(\lambda)(1,1)}M_{1,.}$ from row number $u$, $u=2,...,p$ in $M^{i,j}(\lambda)$. For $1<v<p$, let $T_v(M^{i,j}(\lambda))$ denote the matrix resulting from subtracting $\frac{T_{v-1}(M^{i,j}(\lambda))(v,u)}{T_{v-1}(M^{i,j}(\lambda))(v,v)}T_{v-1}(M^{i,j}(\lambda)_{v,.})$ (whenever $T_{v-1}(M^{i,j}(\lambda))(v,v)\not=0$) from row number $u$, $u=v+1,...,p$ in $T_{v-1}(M^{i,j}(\lambda))$. Then outside a zero set in $\Theta$, $T_vM^{i,j}(\lambda)$ is well-defined for $v=1,...,p-1$ and $(T_vM^{i,j}(\lambda))_{p,.}=(0,...,0)$.
%The following Lemma tells us that outside a zero set in $\Theta$ we may bring an affine matrix to a row echelon form in $p-1$ steps, without permuting any rows or columns while making the final row the only zero-row in this matrix and shows that the coefficients $\{s_u(\theta,\lambda)\}_{u=1}^{p-1}$ in Lemma $\ref{matrixLemma}$ are rational.
\begin{lemma}\label{matrixNoTiesLemma}
The coefficients $\{s_u(\theta,\lambda)\}_{u=1}^{p-1}$ in Lemma \ref{matrixLemma} are rational functions on $\R\times\Theta\times\Theta$, whose roots in terms of $\lambda$ do not lie in $\Lambda$ outside some zero set in $\Theta\times\Theta$.
%Then $d_1(\lambda)=(M^{i,j}(\lambda))(1,1)=0$, $\frac{(M^{i,j}(\lambda))(2,1)}{(M^{i,j}(\lambda))(1,1)}(M^{i,j}(\lambda))(1,2)\not=(M^{i,j}(\lambda))(2,2)$ and 
%$$\frac{T_{v}(M^{i,j}(\lambda))(v+1,v)}{(T_{v}(M^{i,j}(\lambda))(v,v)}M^{i,j}(\lambda)(v,v+1)\not=T_v(M^{i,j}(\lambda))(v+1,v+1),$$
%for $v=1,...,p-2$ outside a zero set in $\Theta$.
\end{lemma}
\begin{proof}
For any root of $\lambda\in\Lambda$, let $T_1(M^{i,j}(\lambda))$ denote the matrix resulting from subtracting $\frac{M^{i,j}(\lambda)(u,1)}{M^{i,j}(\lambda)(1,1)}M^{i,j}_{1,.}$ from row number $u$, $u=2,...,p$ in $M^{i,j}(\lambda)$, for those $\theta\in\Theta\times\Theta$ such that $M^{i,j}(\lambda)(1,1)\not=0$, $\forall \lambda\in\Lambda(\theta)$. For $1<v<p$, let $T_v(M^{i,j}(\lambda))$ denote the matrix resulting from subtracting $\frac{T_{v-1}(M^{i,j}(\lambda))(v,u)}{T_{v-1}(M^{i,j}(\lambda))(v,v)}T_{v-1}(M^{i,j}(\lambda)_{v,.})$ (whenever $T_{v-1}(M^{i,j}(\lambda))(v,v)\not=0$) from row number $u$, $u=v+1,...,p$ in $T_{v-1}(M^{i,j}(\lambda))$. Then outside a zero set in $\Theta\times\Theta$, $T_vM^{i,j}(\lambda)$ is well-defined for $v=1,...,p-1$ and $(T_vM^{i,j}(\lambda))_{p,.}=(0,...,0)$.
The following procedure shows us that outside a zero set in $\Theta\times\Theta$ we may bring $M^{i,j}(\lambda)$ to a row echelon form in $p-1$ steps, without permuting any rows or columns while making the final row the only zero-row in this matrix. First we see that $d_1(\lambda)=(M^{i,j}(\lambda))(1,1)$ and $P(\lambda)$ only have a common root if their resultant is zero. As before it suffices to find $\theta\in\Theta\times\Theta$ such that $d_1(\lambda)\not=0$ for any $\lambda\in\Lambda(\theta)$. Consider the matrix $\tilde{M}$ with $\tilde{M}(l,l)=1$ for $l=1,...,p-1$, $\tilde{M}(p,p)=p-\lambda$, $\tilde{M}(p,l)=1$ for $l=1,...,p-1$, $\tilde{M}(1,l)=1$ for $l=1,...,p-1$ and zero on all other entries. 
\begin{equation*}
\tilde{M}(\lambda) = 
\begin{pmatrix}
1 & 0 & \cdots & 0 & 1 \\
0 & 1 & \cdots & 0 & 1 \\
\vdots  & \vdots  & \ddots & \vdots & \vdots \\
0  & 0  & \cdots & 1 & 1  \\
1 & 1 & \cdots & 1 & \lambda+p 
\end{pmatrix}
\end{equation*}
Then $d_1\equiv1$ and $\Lambda(\theta)=\{0\}$ and this shows that $d_1$ and $P$ do not have common roots outside some zero set $N_1$, so that we may divide by $d_1$. On $N_1$, $\frac{1}{d_1}=\frac{1}{M^{i,j}(1,1)}$ is well defined and in order to clear the first column below row 1 we want to ensure that we do not clear the next element below on the diagonal ($(M^{i,j}(\lambda))(2,2)$) when doing so. We therefore have to check that $\frac{(M^{i,j}(\lambda))(2,1)}{(M^{i,j}(\lambda))(1,1)}(M^{i,j}(\lambda))(1,2)\not=(M^{i,j}(\lambda))(2,2)$, which is implied if $(M^{i,j}(\lambda))(2,1)(M^{i,j}(\lambda))(1,2)-(M^{i,j}(\lambda))(2,2)(M^{i,j}(\lambda))(1,1)\not=0$. $(M^{i,j}(\lambda))(2,1)(M^{i,j}(\lambda))(1,2)-(M^{i,j}(\lambda))(2,2)(M^{i,j}(\lambda))(1,1)$ is a polynomial on $\Theta\times\Theta$ and we wish to show it has no roots in common with $P$ outside a zero set $N_2\in\Theta\times\Theta$. It suffices to find a $\theta\in\Theta\times\Theta\setminus N_1$ such that the two polynomials do not have a root in common. As $\tilde{M}(2,1)\tilde{M}(1,2)-\tilde{M}(2,2)\tilde{M}(1,1)=-1$, the matrix $\tilde{M}$ still suffices for this purpose. This also shows that all entries in $T_1(M^{i,j}(\lambda))$ are rational functions on $\R\times\Theta\times\Theta$. Suppose now that $1<v\le p-1$, $(T_{v-1}(M^{i,j}(\lambda))(v,v)\not=0$, all entries in $(T_{v-1}(M^{i,j}(\lambda))$ are rational on $\R\times\Theta\times\Theta$  and
\begin{align}\label{dia}
&\frac{T_{v-1}(M^{i,j}(\lambda))(v,v-1)}{(T_{v-1}(M^{i,j}(\lambda))(v-1,v-1)}T_{v-1}\left(M^{i,j}(\lambda)\right)(v-1,v)\not=T_{v-1}\left(M^{i,j}(\lambda)\right)(v,v),
\end{align}
outside a zero set $N_{v-1}\in\Theta\times\Theta$. This means that we have successfully cleared everything below the main diagonal until the $v-1$:th element on the diagonal counting from the upper left corner. The elements of $(T_{v}(M^{i,j}(\lambda)))$ are obviously rational functions on $\R\times\Theta\times\Theta$ due to the induction hypothesis and the fact that the elements in $(T_{v}(M^{i,j}(\lambda)))$ are formed by products of rational functions in $(T_{v-1}(M^{i,j}(\lambda)))$. Moreover there are no poles in $\Lambda$, outside $N_{v-1}$ (this follows from \eqref{dia}), so the roots of $(T_{v}(M^{i,j}(\lambda))(v,v)$ coincides with those of its numerator polynomial, $Q_v$. To show $Q_v$ is not the zero polynomial take the above $\tilde{M}$ again and note that $(T_{v}(\tilde{M})(v,v)=\tilde{M}(v,v)$ for $v=1,...,p-1$. To show that
$$\frac{T_{v}(M^{i,j}(\lambda))(v+1,v)}{(T_{v}(M^{i,j}(\lambda))(v,v)}T_v(M^{i,j}(\lambda))(v,v+1)\not=T_v(M^{i,j}(\lambda))(v+1,v+1),$$
for $\lambda\in\Lambda$, it suffices to show that the numerator polynomial of 
$$T_{v}(M^{i,j}(\lambda))(v+1,v)T_{v}(M^{i,j}(\lambda))(v,v+1)-T_{v}(M^{i,j}(\lambda))(v+1,v+1)T_{v}(M^{i,j}(\lambda))(v,v),$$
does not have a root in common with $P$, i.e., their resultant is only zero on a zero set. For this purpose we can again recycle $\tilde{M}$
where we have that 
$$T_{v}(\tilde{M})(v+1,v)T_{v}(\tilde{M})(v,v+1)-T_{v}(\tilde{M})(v+1,v+1)T_{v}(\tilde{M})(v,v)=-1,$$
similarly to before, when $v<p$. We then let $N'_{v}$ denote the zero set where either $Q_v(\lambda)=0$ or $\frac{T_{v}(M^{i,j}(\lambda))(v+1,v)}{(T_{v}(M^{i,j}(\lambda))(v,v)}M^{i,j}_{v,v+1}=M^{i,j}(v+1,v+1)$ and then let $N_v=N_1\cup....\cup N_{v-1}\cup N_v'$ which is the desired zero set. Note that there will be two rational functions $e_{p-1},e_p$ on $\R\times\Theta\times\Theta$ such that
\begin{equation*}
\left(T_{p-2}(M^{i,j}(\lambda))\right)_{p,.} = 
\begin{pmatrix}
0 & \cdots & 0 & e_{p-1} & e_p \\
\end{pmatrix},
\end{equation*}
i.e. the only two possible non-zero elements of $\left(T_{p-2}(M^{i,j}(\lambda))\right)_{p,.}$ are the last two elements. Now we must have that
\begin{equation*}
\left(T_{p-1}(M^{i,j}(\lambda))\right)_{p,.} = 
\begin{pmatrix}
0 & \cdots & 0 \\
\end{pmatrix},
\end{equation*}
indeed, since if this was not the case we would have that $\mathrm{rank}\left(T_{p-1}(M^{i,j}(\lambda))\right)=p$ (since we have all elements on the main diagonal are non-zero and all elements below it have been cleared), but $\mathrm{rank}\left(T_{p-1}(M^{i,j}(\lambda))\right)=\mathrm{rank}\left(M^{i,j}(\lambda)\right)=p$ since $\lambda\in \Lambda$.
%\begin{equation*}
%\mathrm{span}
%\begin{pmatrix}
%T_{p-1}(M^{i,j}(\lambda))_{1,.} & \cdots & T_{p-1}(M^{i,j}(\lambda))_{p-1,.}
%\end{pmatrix}
%=
%\mathrm{span}
%\begin{pmatrix}
%M^{i,j}(\lambda)_{1,.} & \cdots & M^{i,j}(\lambda)_{p-1,.}
%\end{pmatrix}
%\end{equation*}
%but we know that $M^{i,j}(\lambda)_{p-1,.}$ can be expressed as a linear combination of $\{M^{i,j}(\lambda)_{u,.}\}_{u=1}^{p-1}$ and therefore
The fact that $T_{p-1}(M^{i,j}(\lambda))_{p,.}=0$ leads to the following equation.
\begin{align}\label{Mp}
&M^{i,j}(\lambda)_{p,.}=\frac{(M^{i,j}(\lambda))(p,1)}{(M^{i,j}(\lambda))(1,1)} M^{i,j}(\lambda)_{1,.}+\frac{T_1(M^{i,j}(\lambda))(p,2)}{T_1(M^{i,j}(\lambda))(2,2)} T_1(M^{i,j}(\lambda))_{2,.}+....
\nonumber
\\
&+\frac{T_{p-2}(M^{i,j}(\lambda))(p,p-1)}{T_{p-2}(M^{i,j}(\lambda))(p-1,p-1)} T_{p-2}(M^{i,j}(\lambda))_{p-1,.}
\end{align}
$T_1(M^{i,j}(\lambda))_{2,.}$ is a linear combination of $(M^{i,j}(\lambda))_{1,.}$ and $(M^{i,j}(\lambda))_{2,.}$ with coefficients that are rational on $\Theta\times\R$. Similarly $T_v(M^{i,j}(\lambda))_{v,.}$ is a linear combination of $\{(M^{i,j}(\lambda))_{u,.}\}_{u=1}^v$, for $v=2,...,p-1$, with coefficients that are rational on $\R\times\Theta\times\Theta$. This together with \eqref{Mp} and the uniqueness of the representation \eqref{coeffrepreq} shows that the coefficients $\{s_u\}_{u=1}^{p-1}$ are rational on $\R\times\Theta\times\Theta$.
\end{proof}

\begin{lemma}\label{coefficientslemma}
Suppose $A$ is a $p\times p$ matrix of rank $p-1$ such that $A_{p,.}$ is linearly independent of every subset of $p-2$ other rows of $A$. Let $A_{p,.}=\sum_{u=1}^{p-1}c_uA_{u,.}$ be its unique decomposition in $span\{A_{1,.},...,A_{p-1,.}\}$. If $\{A(n)\}_n$ is a sequence of $p\times p$ matrices of rank $p-1$ that also share the property that $A_{p,.}(n)$ is linearly independent of every subset of $p-2$ other rows of $A(n)$ and such that $\lim_{n\to\infty}\n A-A(n)\n=0$ then if we let $A_{p,.}(n)=\sum_{u=1}^{p-1}c_u(n)A_{u,.}(n)$ be its unique decomposition in $span\{A_{1,.}(n),...,A_{p-1,.}(n)\}$   it follows that $c_u(n)\to c_u$
\end{lemma}
\begin{proof}
%First note that since $A$ and $A(n)$ are of rank $p-1$, the uniqueness of the decompositions $A_{p,.}=\sum_{u=1}^{p-1}c_uA_{u,.}$ and $A_{p,.}(n)=\sum_{u=1}^{p-1}c_u(n)A_{u,.}(n)$ follow from Lemma \ref{matrixLemma}. 
Let $x=A_{p,.}$ and $V=\left(A_{1,.}^T A_{2,.}^T ... A_{p-1,.}^T\right)$ (which is then a $p\times (p-1)$ matrix) so that $x=VC$ if we let $C=\left(c_1 ... c_{p-1} \right)^T$. Note that $V$ has full column rank which implies $C=V^+x$ is a unique solution for $C$, where $V^+$ denotes the Penrose-inverse. This implies that $ \n C \n_\infty\le \n V^+\n_\infty \n A_{p,.}\n_\infty$. Similarly we get that $ \n C(n) \n_\infty\le \n V(n)^+\n_\infty \n A_{p,.}(n)\n_\infty$ where $V(n)^+$ is the Penrose inverse of the matrix $V(n)=\left(A_{1,.}(n)^T A_{2,.}(n)^T ... A_{p-1,.}(n)^T\right)$ and $C(n)=\left(c_1(n) ... c_{p-1}(n)\right)$. Since all the matrices $\{V(n)\}_n$ have rank $p-1$ and $\n V(n)-V \n_\infty$ by the law of large numbers it follows that $\n V(n)^+- V^+\n_\infty\xrightarrow{a.s.}0$ which implies $\n V(n)^+\n_\infty \xrightarrow{a.s.}\n V^+\n_\infty$. By the law of large numbers we also have that $\n A_{p,.}(n) \n_\infty\xrightarrow{a.s.}\n A_{p,.}\n_\infty$. All of this implies that the sequences $\{c_u(n)\}_n$ are bounded for $u=1,...,p-1$. Suppose now that for some $1\le m< p$, $c_m(n)\not\to c_m$. Then since the sequence $\{c_m(n)\}_n$ is bounded there exists a subsequence $\{c_m(n_k)\}_k$ such that $c_m(n_k)\to c'_m\not=c_u$ and $c_v(n_k)\to c'_v$, $1\le v\le p-1$. Since $A_{v,.}(n)\to A_{v,.}$, $1\le v\le p$ it follows that $A_{p,.}=\sum_{u=1}^{p-1}c'_uA_{u,.}$ but $c'_u\not=c_u$ this contradicts the uniqueness of the decomposition and this gives us the result.
\end{proof}
The following theorem is the single environment version of the consistency part of Theorem 1 from \cite{kennerberg2023convergence}
%\begin{thm}
%Suppose $G_++\gamma G_\Delta$ is non-singular then 
%\begin{itemize}
%\item $\tilde{\beta}_\gamma(\textbf{n})\xrightarrow{a.s.}\beta_\gamma,$
%as $n_{A_0}\wedge...\wedge n_{A_k}\to \infty$
%\item For any vector norm $\n. \n$, $0<\delta<1$ and $0<\theta< \frac 14$, there exists $N_{A_0},...,N_{A_k}$ such that if $n_{A_i}\ge N_{A_i}$ then
%\begin{align*}
%&\P\left( \n \tilde{\beta}_\gamma(\textbf{n}) -\beta_\gamma\n\ge c \right)\le
%(4p+2)\sum_{i=0}^ke^{-r(c)^2 n_{A_i}^{1-4\theta}}
%+5\left(1-\prod_{i=0}^k\left(F_{\left|X^{A_i}(1)\right|,...,\left|X^{A_i}(p)\right|,|Y^{A_i}|}(n_{A_i}^\alpha,...,n_{A_i}^\alpha)\right)^{n_{A_i}}\right),
%\end{align*}
%\end{itemize}
%where $r(c)\ge M\left(\delta\wedge c\wedge\sqrt{c}\right)$ where $M$ is a (finite) positive constant that depends on the chosen norm $\n.\n$, $\delta$, $p$ and $k$. We also denoted $\tilde{\beta}_\gamma(n)=\left(\mathbb{G}_++\gamma\mathbb{G}_\Delta \right)^{-1}_p\left(\mathbb{Z}_++\gamma\mathbb{Z}_\Delta \right)$ and $()^{-1}_p$ denotes any map that coincides with matrix inversion on the space of full rank $p\times p$ matrices.
%\end{thm}
%In particular if we set $k=1$ in the above theorem we get the following result which is the one that we will need.
\begin{thm}\label{converge}
Suppose $G^i_++\gamma G^i_\Delta$ is non-singular then $\tilde{\beta}_\gamma(\textbf{n})\xrightarrow{a.s.}\beta_\gamma,$
as $n_{A_0}\wedge...\wedge n_{A_k}\to \infty$, where we denoted $\tilde{\beta}_\gamma(n)=\left(\mathbb{G}^i_++\gamma\mathbb{G}^i_\Delta \right)^{-1}_p\left(\mathbb{Z}^i_++\gamma\mathbb{Z}^i_\Delta \right)$ and $()^{-1}_p$ denotes any map that coincides with matrix inversion on the space of full rank $p\times p$ matrices.
\end{thm}
We are now ready to prove the main result of the paper.
\begin{proof}[Proof of Theorem \ref{optimalregthm}]
Regardless of how the weights $w^*(\beta)$ are distributed among the the risk functions that attain the same maximum (for that $\beta$) we obviously have that
$$R_+^{w^*(\beta)}(\beta) +\gamma R_\Delta^{w^*(\beta)}(\beta)=(1+\tau)\left(R_{A_1}(\beta)\vee...\vee R_{A_k}(\beta)\right)-\tau R_O(\beta).$$
It will be helpful to re-write the minimizer in the following way
\begin{align*}
\beta_\gamma&=\arg\min_{\beta\in\R}(1+\tau)\left(R_{A_1}(\beta)\vee...\vee R_{A_k}(\beta)\right)-\tau R_O(\beta)
\\
&=\arg\min_{\beta\in\R}\left((1+\tau)R_{A_1}(\beta)-\tau R_O(\beta)\right)\vee...\vee \left((1+\tau)R_{A_k}(\beta)-\tau R_O(\beta)\right).
\end{align*}
Let $h_i(\beta)=(1+\tau)R_{A_i}(\beta)-\tau R_O(\beta)$ so that $\beta_\gamma=\arg\min_{\beta\in\R}h_1(\beta)\vee...\vee h_k(\beta)$. We also let $f(\beta)=h_1(\beta)\vee...\vee h_k(\beta)$. The minimum of $f$ can only be achieved at either an inflexion point for some $h_i$ or along the intersection (where $h$ might not be differentiable) of two $h_i$'s. The inflexion point of $h_i(\beta)$ is achieved at $\beta_{inf(i)}:=\left(G^i_++\gamma G^i_\Delta\right)^{-1}\left(Z^i_++\gamma Z^i_\Delta\right)$. Let $B_{inf}=\{\beta_{inf(1)},...,\beta_{inf(k)}\}$ denote the set of inflexion points and let $\hat{B}_{inf}(\textbf{n})$ denote the corresponding estimators. If $R_i$ and $R_j$ intersect let $B^{i,j}$ be its set of argmin points, this set will be finite (we will show this later). If $R_i$ and $R_j$ do not intersect then let $B^{i,j}=\emptyset$. Correspondingly, if $\hat{R}_i$ and $\hat{R}_j$ intersect we let $\hat{B}^{i,j}(\textbf{n})$ denote the set of argmin points along this intersection. If $\hat{R}_i$ and $\hat{R}_j$ do not intersect we set $\hat{B}^{i,j}(\textbf{n})=\emptyset$. Let $B_{int}=B^{1,1}\cup...\cup B^{1,k}\cup B^{2,3}\cup...\cup B^{2,k}\cup...\cup B^{k-1,k}\cup B^{k,k}$ denote all the intersections points between the different $h_i$'s and let $\hat{B}_{int}(\textbf{n})$ denote the corresponding set of estimators. We also let $B=B_{int}\cup B_{inf}$ and $\hat{B}(\textbf{n})=\hat{B}_{int}(\textbf{n})\cup \hat{B}_{inf}(\textbf{n})$. For $\beta\in B^{i,j}$ to be a candidate for the $\arg\min$ we must have that $h_i(\beta)=f(\beta)$. Let $ \hat{h}_i(\beta)=(1+\tau)\mathbb{R}_{A_i}-\tau \mathbb{R}_O$ and $\hat{f}(\beta)=\bigwedge_{i=1}^k\hat{h}_i(\beta)$. It is straight-forward to verify that $\arg\min \hat{h}_i(\beta)$ is achieved at its inflexion point $\beta^i=\left(\mathbb{G}^i_++\gamma\mathbb{G}^i_\Delta\right)^{-1}\left(\mathbb{Z}^i_++\gamma\mathbb{Z}^i_\Delta\right)$. With our new notation, the argmin of $f$ must be achieved at a point in either $\hat{B}_{inf}(\textbf{n})$ or $\hat{B}_{int}(\textbf{n})$, i.e.
\begin{align}
&\beta_\gamma=\arg\min_{\beta\in B_{inf} \cup  B_{int}\cap\R\cap\{\beta:\hspace{1mm} \exists 1\le i \le k,\hspace{1mm} f(\beta)=h_i(\beta)\}}f(\beta),
\end{align}
while
\begin{align}\label{betaeqemp}
&\hat{\beta}_\gamma(\textbf{n})=\arg\min_{\beta\in \hat{B}_{inf}(\textbf{n}) \cup \hat{B}_{int}(\textbf{n})\cap\R\cap\{\beta:\hspace{1mm} \exists 1\le i \le k, \hat{f}(\beta)= \hat{h}_i(\beta)\}} \hat{f}(\beta) .
\end{align}
We now construct the vector $V$ as follows, the first $k$ entries are $\beta_{inf(1)},...,\beta_{inf(k)}$ in chronological order. Then we place all the elements of $B^{i,j}$'s in rising order beginning with $B^{0,1}$ and ending with $B^{k-1,k}$ with the convention that we always place elements of $B^{i,j}$ for $i<j$ but not $i\ge j$. The individual elements of $B^{i,j}$ are to be ordered lexicographically. This way we can associate each argmin- candidate point with a unique index number in $V$. The number of elements in $V$ will be denoted $M$. We then construct $\hat{V}_n$ completely analogously from the corresponding estimators and let $\hat{M}^{i,j}(\textbf{n})$ be the number of elements in $\hat{V}_\textbf{n}$. We will later show that for large $\textbf{n}$, $\hat{M}^{i,j}(\textbf{n})=M$ a.s.. Let us define the optimal choice indices
$$L=\arg\min_{1\le l \le M}f(V(l)), $$
(note that $L$ is a set in general) so that $\beta_\gamma= g(V(L))$
and similarly let $\hat{L}_\textbf{n}$ be any arbitrary element of the set
$$\arg\min_{1\le l \le \hat{M}^{i,j}(\textbf{n})}\hat{f}(\hat{V}_\textbf{n}(l)), $$
so that $\hat{\beta}_\gamma(\textbf{n})= \hat{f}(\hat{V}_\textbf{n}(l)), \forall l\in \hat{L}_\textbf{n}.$
It may be the case that for some $\textbf{n}$, $\hat{L}_\textbf{n}\not\in L$. This can only happen in two different ways. The first one is that we do not have enough samples and the estimators deviate so much from their respective estimands that we make the wrong choice. The second one is due to $\mathbb{R}_i$ and $\mathbb{R}_j$ (for some $i$ and $j$) intersecting and inducing an argmin while $R_i$ and $R_j$ do not intersect. If $R_i$ and $R_j$ do not intersect they are separated by some fixed amount, for large enough $\textbf{n}$ so will $\mathbb{R}_i$ and $\mathbb{R}_j$ be, therefore this will not happen for large $\textbf{n}$. For any $l\not\in L$ there exists $a>0$ such that $V(l)-V(u)>a$, for any $u\in L$. We will show later that $\hat{V}_\textbf{n}(l)\xrightarrow{a.s.}V(l)$ for $1\le l\le M$ (recall that $\hat{M}^{i,j}_n=M$ for large $\textbf{n}$) and this will imply that $\hat{V}_\textbf{n}(l)-\hat{V}_\textbf{n}(u)>a$ for large enough $\textbf{n}$, $u\in L$. So for such large $\textbf{n}$ we must have $\hat{L}_\textbf{n}\in L$. So for large $\textbf{n}$ the right candidate is always picked and the theorem will follow if we can show that $\hat{V}_\textbf{n}(l)\xrightarrow{a.s.}V(l)$ for all $1\le l\le M$. This is clear by Theorem \ref{converge} for the inflexion points so it now remains to study possible minima along the intersections, i.e. we must show that with a lexicographical ordering of $B^{i,j}$, $\beta^{i,j}_1<...<\beta^{i,j}_q$ and of $\hat{B}^{i,j}(\textbf{n})$, $\hat{\beta}_1^{i,j}<...<\hat{\beta}_{\hat{q}}^{i,j}$ (with $\hat{q}=q$ for large $\textbf{n}$) we have that $\hat{\beta}^{i,j}_u\xrightarrow{a.s.}\beta^{i,j}_u$ for $u=1,...q$. 
\\
We now study any potential minima along the intersection of two risk functions $R_i(\beta)$ and $R_j(\beta)$. 
The determinant of $G^i$ is a polynomial on $\Theta$ so by Lemma \ref{PolyLemma} (since it is clearly not the zero polynomial over $\Theta$) it is only zero on set $N_1$ of Lebesgue measure zero in $\Theta$. But by definition $G^i$ is positive semi-definite so if it is of full rank then it is in fact positive definite. Expanding the risk function we see that
$$R_{A_i}(\beta)=\E\left[(Y^{A_i})^2\right]-2\beta\E\left[Y^{A_i}X^{A_i}\right]+\beta G^i\beta^T. $$
Since $G^i$ is positive definite outside a set of measure zero we see that by a diagonalization of $G^i$, $G^i=ODO^T$, where $O$ is an orthogonal matrix and $D$ a diagonal matrix with positive eigenvalues $\lambda_1,...,\lambda_p$ such that $\n \beta O \n=\n\beta\n$. Letting $\tilde{\beta}=\beta O$ (recall that we defined $\beta$ as a row vector and note that $\tilde{\beta}O^T=\beta $), we have that 
$$\beta G^i\beta^T=\beta ODO^T\beta^T=\tilde{\beta}D\tilde{\beta}^T=\sum_{u=1}^p\lambda_u\tilde{\beta}_u^2.$$
So if we let $\tilde{\lambda}=\min_{1\le u\le p}\lambda_u$ then
\begin{align*}
&R_{A_i}(\beta)= \E\left[(Y^{A_i})^2\right]-2\tilde{\beta}O^T\E\left[Y^{A_i}X^{A_i}\right]+\sum_{u=1}^p\lambda_u\tilde{\beta}_u^2\ge \E\left[(Y^{A_i})^2\right]-2\n \tilde{\beta}O^T \E\left[ Y^{A_i}X^{A_i}\right]\n_2+\tilde{\lambda}\n \tilde{\beta}\n_2^2
\ge
\\
&\E\left[(Y^{A_i})^2\right]-2\n \tilde{\beta}O^T\n \E\left[ \n Y^{A_i}X^{A_i}\n_2\right]+\tilde{\lambda}\n \tilde{\beta}\n_2^2
=\E\left[(Y^{A_i})^2\right] + \n\beta\n_2\left(\tilde{\lambda}\n\beta\n_2-2\E\left[ \n Y^{A_i}X^{A_i}\n_2\right]\right),
\end{align*}
where we used Cauchy-Schwarz and Jensen's inequality for the $\mathit{l}^2$-norm. This implies that $\liminf_{\n \beta \n\to\infty}R_{A_i}(\beta)=\infty$. Since $\liminf_{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta)} R_{A_i}(\beta)$ exists and is finite it cannot be a limit where $\n\beta\n\to\infty$ (the risk functions must obviously be finite at any intersection point). Therefore there exists a sequence $\{\beta_l\}_l$ with $\n \beta_l \n\le C$ for some $C<\infty$ such that $\liminf_{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta)} R_{A_i}(\beta)=\lim_{l\to\infty}R_{A_i}(\beta_l)$, so in other words
$$\liminf_{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta)} R_{A_i}(\beta)=\liminf_{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta), \n \beta\n\le C} R_{A_i}(\beta).$$
The set $\left\{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta)\right\}$ is closed since it is the kernel of the continuous function $R_{A_i}(\beta)-R_{A_j}(\beta)$. Therefore the set $\left\{\beta\in\R^p:R_{A_i}(\beta)=R_{A_j}(\beta), \n \beta\n\le C)\right\}$ is compact but the infinum of a continuous function over a compact set is in fact a minimum. 
Let $g(\beta)=R_i(\beta)-R_j(\beta)$. By the necessity part of the Kuhn-Tucker Theorem, a necessary condition for $\beta^*$ to be a minimum point for $R_{A_i}(\beta)$ subject to $g(\beta)=0$ is that $\nabla_\beta \mathcal{L}(\beta^*,\lambda^*)=0$ for some $\lambda^*$ and $g(\beta^*)=0$. This will only have a finite set of solutions in terms of $\lambda^*$ (as it will be polynomial in $\lambda^*$) and correspondingly a finite set of $\beta$'s, we then choose whichever one that minimizes $R_{A_i}$. Consider the Lagrangian
\begin{align*}
\mathcal{L}(\beta,\lambda)&=R_{A_i}(\beta)-\lambda g(\beta)
=
\sum_{l=0}^p\beta_l^2\left(\E\left[(X(l)^{A_i})^2-\lambda\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right] \right)
\\
&-2\sum_{l=1}^p\beta_l\E\left[Y^{A_i}(X(l)^{A_i})-\lambda\left(Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right)\right]
\\
&+2\sum_{l_1=0}^p\sum_{l_2\not=l_1}^p\beta_{l_1}\beta_{l_2}\E\left[X(l_1)^{A_i}X(l_2)^{A_i}-\lambda\left(X(l_1)^{A_i}X(l_2)^{A_i}-X(l_1)^{A_j}X(l_2)^{A_j}\right)\right]
+
\E\left[(Y^{A_i})^2-\lambda\left(Y^{A_i})^2-(Y^{A_j})^2\right)\right]
\\
&=\sum_{l=1}^p\beta_l^2\left(a_i(l)-\lambda\left( a_i(l)-a_j(l)\right) \right)
-2\sum_{l=1}^p\beta_l\left(b_i(l)-\lambda\left(b_i(l)-b_j(l)\right)\right)
\\
&+2\sum_{l_1=0}^p\sum_{l_2\not=l_1}^p\beta_{l_1}\beta_{l_2}\left(a_i(l_1,l_2)-\lambda\left(a_i(l_1,l_2)-a_j(l_1,l_2)\right)\right)
+c_i-\lambda\left(c_i-c_j\right).
\end{align*}
where $a_i(l_1,l_2)=\E\left[X(l_1)^{A_i}X(l_2)^{A_i}\right]$.
Setting $\nabla_\beta\mathcal{L}(\beta,\lambda)=0$ yields $M^{i,j}(\lambda)\beta=C^{i,j}_{\lambda}$, in other words
\begin{align*}
&\beta_l\left(\E\left[(X(l)^{A_i})^2-\lambda\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right] \right)
=
\E\left[Y^{A_i}(X(l)^{A_i})-\lambda\left(Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right)\right]
-
\\
&\sum_{l_2\not=l}^p\beta_{l_2}\E\left[X(l)^{A_i}X(l_2)^{A_i}-\lambda\left(X(l)^{A_i}X(l_2)^{A_i}-X(l)^{A_j}X(l_2)^{A_j}\right)\right],
\end{align*}
where we define the $p\times p$ matrix $M^{i,j}(\lambda)$ and the vector $C^{i,j}(\lambda)$ exactly as in \ref{parammatrix} using the given environments i.e.
$$M^{i,j}(\lambda)_{l,l} =a_i(l)-\lambda\left(a_i(l)-a_j(l)\right)$$
when $u\not=v$
$$M^{i,j}(\lambda)_{u,v} =-\left(a_i(u,v)-\lambda\left(a_i(u,v)-a_j(u,v)\right)\right)$$
and
$$C^{i,j}(\lambda)_u=b_i(u)-\lambda\left(b_i(u)-b_j(v)\right).$$
Define
$$d_l(\lambda)=\E\left[(X(l)^{A_i})^2-\lambda\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right],$$ 
$$e_l(\lambda)=\sum_{l_2\not=l}^p\beta_{l_2}\E\left[X(l)^{A_i}X(l_2)^{A_i}-\lambda\left(X(l)^{A_i}X(l_2)^{A_i}-X(l)^{A_j}X(l_2)^{A_j}\right)\right]\hspace{2mm} and$$
$$c_l(\lambda)=\E\left[Y^{A_i}(X(l)^{A_i})-\lambda\left(Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right)\right].$$
Let $P(\lambda)=\det\left(M^{i,j}(\lambda)\right)$. We will now establish the fact that outside a set of measure zero in $\Theta\times\Theta$, $M(\lambda_u)\beta=C^{i,j}(\lambda)$ has no solutions for any $\lambda\in\R$ such that $P(\lambda)=0$. By Lemma \ref{matrixLemma} outside a set of measure zero $N_2\in\Theta\times\Theta$, $\mathrm{rank}(M^{i,j}(\lambda))=p-1$ for $\lambda\in\Lambda$ and 
\begin{align}\label{linearcomb}
&M^{i,j}_{p,.}(\lambda)=\sum_{u=1}^{p-1}s_u(\theta,\lambda)M_{u,.}(\lambda)
\end{align}
with all $s_u(\theta,\lambda)\not=0$. 
%Letting $\lambda_u$ denote the u:th smallest root of $P(\lambda)$ we have that $\Theta\setminus N_4=\Theta_0\bigcup\left(\bigcup_{u=1}^p\Theta_{l,u}\right)$ where $\Theta_0=\{\theta: P(\lambda)\textsf{  has no real roots}\}$ and 
%$\Theta_{u}=\{\theta: P(\lambda)\textsf{  has u real roots}\}$.
From Lemma \ref{matrixNoTiesLemma} we see that outside a zero set $N_3\in\Theta\times\Theta$, $\{s_u(\theta,\lambda)\}_{u=1}^{p-1}$ are all rational functions on $\R\times\Theta\times\Theta$. Applying the same row operations to the vector $C^{i,j}(\lambda)$ as $M^{i,j}(\lambda)$ we now have that $M^{i,j}(\lambda)\beta=C^{i,j}(\lambda)$ has no solutions if
$$\left(C^{i,j}_{\lambda}(p)-\sum_{v\not=p}s_v(\theta,\lambda) C^{i,j}_{\lambda}(v)\right)\not=0.$$
Let $G$ denote the lowest common denominator of the terms in $\left(C^{i,j}_{\lambda}(p)-\sum_{v\not=p}s_v(\theta,\lambda) C^{i,j}_{\lambda}(v)\right)$ then 
$$Q(\theta,\lambda)= G(\theta,\lambda)\left(C^{i,j}_{\lambda}(p)-\sum_{v\not=p}s_v(\theta,\lambda) C^{i,j}_{\lambda}(v)\right)$$ 
is a polynomial on $\R\times\Theta\times\Theta$. To show that $Q$ has no roots in $\Lambda$ is equivalent to showing that $P$ and $Q$ have no common roots which is in turn equivalent to showing that their resultant is not zero. Again, we need to show that there exists $\theta\in\Theta$ such that $P$ and $Q$ have no common roots. Take the matrix $\tilde{M}$ from the proof of Lemma \ref{matrixNoTiesLemma}. As we already have seen for $\tilde{M}$, $s_1\equiv...\equiv s_{p-1}\equiv 1$. We readily see that in this case $G\equiv 1$ (since $G$ is the lowest common denominator of $\{s_u\}_{u=1}^{p-1}$) so any choice of parameters that leads to the first degree polynomial $C^{i,j}_{\lambda}(p)-\sum_{v\not=p}s_v(\theta,\lambda) C^{i,j}_{\lambda}(v)$ not having a root in $\lambda=0$ shows that outside a zero set $N_4$ there are no solutions to $M^{i,j}(\lambda_u)\beta=C^{i,j}(\lambda)$ for any $\lambda\in\Lambda$. We continue with the case when $\lambda\in\R$ is such that $M^{i,j}(\lambda)$ is full rank. If $M^{i,j}(\lambda)$ is full-rank then $\beta(\lambda)=(M^{i,j})^{-1}(\lambda)C^{i,j}_{\lambda}$, to find $\lambda$ we solve $g(\beta(\lambda))=0$. Note that 
$$\beta(\lambda)_u=\frac{1}{\det\left(M^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(M^{i,j}_{u,v}(\lambda)\right)C^{i,j}(\lambda)(v),$$ 
where we again use the convention that $M^{i,j}_{u,v}$ is the matrix $M^{i,j}$ with row $u$ and column $v$ removed. So for $\lambda\not\in\Lambda$
\small
\begin{align*}
&g(\beta(\lambda))=\left(a_i(l)-a_j(l)\right)\sum_{l=1}^p\left(\frac{1}{\det\left(M^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(M^{i,j}_{l,v}(\lambda)\right)(-1)^{l+v}C^{i,j}(\lambda)(v)\right)^2+c_i-c_j-
\\
&2\left(b_i(l)-b_j(l)\right)\sum_{l=1}^p\left(\frac{1}{\det\left(M^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(M^{i,j}_{l,v}(\lambda)\right)(-1)^{l+v}C^{i,j}(\lambda)(v)\right)+
\\
&2\left(a_i(l_1,l_2)-a_j(l_1,l_2)\right)\sum_{l_1=1}^p\sum_{l_2\not=l_2}\left(\frac{1}{\det\left(M^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(M^{i,j}_{l_1,v}(\lambda)\right)(-1)^{l_1+v}C^{i,j}(\lambda)(v)\right)\cdot
\\
&\left(\frac{1}{\det\left(M^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(M_{l_2,v}(\lambda)\right)(-1)^{l_2+v}C^{i,j}(\lambda)(v)\right).
\end{align*}
\normalsize
Furthermore from the above expression we see that for $\lambda\not\in\Lambda$, $g(\beta(\lambda))$ has the same roots as 
$$\tilde{P}(\lambda)=\det\left(M^{i,j}(\lambda)\right)^2g(\beta(\lambda)),$$ 
which is a polynomial. Outside a set of measure zero $N_5\in\Theta\times\Theta$, $\tilde{P}(\lambda)$ has only simple roots, according to Lemma \ref{discriminantLemma}. Let us now define $N=N_1\cup...\cup N_5$. Letting $\mathcal{R}$ denote the (finite) set of roots to $g(\beta(\lambda))=0$ then the set of intersection $\arg\min$'s will now be given by 
$$B^{i,j}=\left\{M^{-1}(\lambda^{i,j})C_{\lambda^{i,j}}: \lambda^{i,j}=\arg\min_{\lambda\in\mathcal{R}}R_{A_i}\left(M^{-1}(\lambda)C^{i,j}(\lambda)\right)\right\}.$$
From here on out we fix $\theta\in\Theta\times\Theta\setminus N$. We now consider the plug-in estimator of the minima along the intersection between $R_{A_i}$ and $R_{A_j}$,
$$\arg\min_{\beta\in\R^p} \mathbb{R}_{A_i}(\beta),\texttt{ subject to } \mathbb{R}_{A_i}(\beta)=\mathbb{R}_{A_j}(\beta).$$
We also consider the plug-in Lagrangian
\begin{align*}
&\left(\hat{\mathcal{L}}(\beta,\lambda)\right)(\textbf{n})=\left(\mathbb{R}_{A_i}(\beta)\right)(\textbf{n})-\lambda \hat{g}(\beta)
=
\sum_{l=1}^p\beta_l^2\left(-\lambda\left( \hat{a}_i(l)-\hat{a}_j(l)\right) \right)
-2\sum_{l=1}^p\beta_l\left(\hat{b}_i(l)-\lambda\left(\hat{b}_i(l)-\hat{b}_j(l)\right)\right)
\\
&2\sum_{l_1=0}^p\sum_{l_2\not=l_1}^p\beta_{l_1}\beta_{l_2}\left(\hat{w}_i(l_1,l_2)-\lambda\left(\hat{w}_i(l_1,l_2)-\hat{w}_j(l_1,l_2)\right)\right)
+\hat{c}_i-\lambda\left(\hat{c}_i-\hat{c}_j\right),
\end{align*}
where $\hat{g}(\beta)=\mathbb{R}_{A_i}(\beta)-\mathbb{R}_{A_j}(\beta)$, $\hat{a}_i(l)=\frac{1}{n_{A_i}}\sum_{u=1}^{n_{A_i}}\left(X^{A_i}_u(l)\right)^2$, $\hat{b}_i(l)=\frac{1}{n_{A_i}}\sum_{u=1}^{n_{A_i}}X^{A_i}_u(l)Y^{A_i}$, $\hat{c}_i=\frac{1}{n_{A_i}}\sum_{u=1}^{n_{A_i}}\left(Y^{A_i}\right)^2$, $\hat{a}_i(l_1,l_2)=\frac{1}{n_{A_i}}\sum_{u=1}^{n_{A_i}}X^{A_i}_u(l_1)X^{A_i}(l_2)$. Then similarly we see that solving $\nabla_\beta \left(\hat{\mathcal{L}}(\beta,\lambda)\right)(\textbf{n})=0$ leads to $\widehat{M}^{i,j}(\lambda)\beta=\widehat{C}^{i,j}_\lambda$, where 
$$\widehat{C}^{i,j}_u(\lambda)=\hat{b}_i(u)-\lambda\left(\hat{b}_i(u)-\hat{b}_j(v)\right), $$
$$\widehat{M}^{i,j}_{l,l}(\lambda)=\hat{a}_i(l)-\lambda\left(\hat{a}_i(l)-\hat{a}_j(l)\right),$$
and when $u\not=v$
$$\widehat{M}^{i,j}(\lambda)_{u,v} =-\left(\hat{a}_i(u,v)-\lambda\left(\hat{a}_i(u,v)-\hat{a}_j(u,v)\right)\right).$$
Let $\hat{\Lambda}_n$ denote the set of roots (in terms of $\lambda)$ to $\hat{P}(\lambda)= \det\left(\hat{M}^{i,j}(\lambda)\right)=0$ and let $\hat{\Lambda}_\R(\textbf{n})$ denote the real roots of this polynomial. We wish to establish that for large enough $\textbf{n}$, $\hat{M}^{i,j}(\lambda)=\hat{C}^{i,j}_\lambda$ has no solutions for $\lambda\in\hat{\Lambda}_n$. Recall that $P$ only has simple roots which implies, by Lemma \ref{simpleroots} and the law of large numbers that the elements in $\hat{\Lambda}_\R(\textbf{n})$ converge a.s. to those in $\Lambda_\R$, where we let $\Lambda_\R=\{\lambda_1,...,\lambda_m\}$ (where $\lambda_1<...<\lambda_m$ and $m$ depends on $\theta$ but $\theta$ is fixed at this point) denote the real roots of $P$. Note that by the proof of \ref{matrixLemma}, for each $\lambda\in\Lambda$ (and therefore also in $\Lambda_\R$) the first $p-1$ first minors along the first column in $M^{i,j}(\lambda)$ is bounded below by some $\delta>0$ ($\delta>0$ also depends on $\theta$, but $\theta$ is fixed), i.e. $\left|\det\left(M_{u,1}(\lambda)\right)\right|>\delta$ for each $\lambda\in\Lambda$ and $1\le u\le p-1$. For large enough $\textbf{n}$, the number of elements in $\hat{\Lambda}_n$ and $\Lambda$ coincides by Lemma \ref{simpleroots}. From Lemma \ref{simpleroots} we also have $\hat{\lambda}_z\xrightarrow{a.s.}\lambda_z$, for $1\le z\le m$. Since the determinant is continuous it follows that $\left|\det\left(\hat{M}^{i,j}_{1,v}(\hat{\lambda}_z)\right)\right|>\delta$, $v=1,...,p-1$ for large enough $\textbf{n}$. Therefore $\mathsf{rank}\left( \hat{M}^{i,j}(\hat{\lambda}_z)\right)=p-1$ and we have the unique representation (in terms of the rows $\hat{M}^{i,j}_{u,.}(\lambda)$)
\begin{align}\label{linearcombhat}
&\hat{M}^{i,j}_{p,.}(\hat{\lambda}_z)=\sum_{u=1}^{p-1}\hat{s}_u(\hat{\lambda}_z)\hat{M}^{i,j}_{u,.}(\hat{\lambda}_z)
\end{align}
for all $\hat{\lambda}_z\in\hat{\Lambda}(\textbf{n})_\R$ with all $\hat{s}_u(\hat{\lambda}_z)\not=0$. Note that 
\begin{align}\label{Mdiff}
&\n \hat{M}^{i,j}(\hat{\lambda}_z)-M(\lambda_z) \n\le \n \hat{M}^{i,j}(\hat{\lambda}_z-\lambda_z) \n+\n\hat{M}^{i,j}(\lambda_z)-M(\lambda_z) \n,
\end{align}
for the first term above $\n \hat{M}^{i,j}(\hat{\lambda}_z-\lambda_z) \n=\left|\hat{\lambda}_z-\lambda_z\right|\n\hat{M}'(\textbf{n})\n$, where $\hat{M}'_{u,v}(\textbf{n})=\hat{w}_i(u,v)-\hat{w}_j(u,v)$ when $u\not=v$ and $\hat{M}'_{l,l}(\textbf{n})=\hat{a}_i(l)-\hat{a}_j(l)$. By the law of large numbers $\hat{M}'(\textbf{n})$ converges to the matrix $M'$ with $M'_{u,v}=a_i(u,v)-a_j(u,v)$ when $u\not=v$ and $M'_{l,l}=a_i(l)-a_j(l)$. Due to the fact that $\hat{\lambda}_z\xrightarrow{a.s.}\lambda_z$ it is therefore clear that the first term in \eqref{Mdiff} vanishes. The second therm in \eqref{Mdiff} will vanish by the law of large numbers, hence $\n \hat{M}^{i,j}(\hat{\lambda}_z)-M^{i,j}(\lambda_z) \n\xrightarrow{a.s.}0$.
Due to the fact that $\n \hat{M}^{i,j}(\hat{\lambda}_z)-M^{i,j}(\lambda_z) \n\xrightarrow{a.s.}0$ and Lemma \ref{coefficientslemma} it follows that $\hat{s}_u(\hat{\lambda}_z)\xrightarrow{a.s.}s_u(\lambda_z)$ for $u=1,...,p-1$ and $1\le z\le m$. Let
$$H(\lambda)= G(\lambda)Q(\lambda)= \left(C^{i,j}_{\lambda}(p)-\sum_{v\not=p}s_v(\theta,\lambda) C^{i,j}_{\lambda}(v)\right),$$
we know that since we chose $\theta\not\in N$, $H(\lambda)>\delta'$ (since $Q(\lambda)\not=0$) for some $\delta'>0$ and every $\lambda\in \Lambda(\theta)$. We now let 
$$\hat{H}(\lambda)= \left(\hat{C}^{i,j}_{\lambda}(p)-\sum_{v\not=p}\hat{s}_v(\lambda) \hat{C}^{i,j}_{\lambda}(v)\right),$$ 
it follows immediately that $\hat{H}(\hat{\lambda}_z)\xrightarrow{a.s.}H(\lambda_z)$ by the fact that $\hat{s}_u(\hat{\lambda}_z)\xrightarrow{a.s.}s_u(\lambda_z)$ and $\hat{C}^{i,j}_{\hat{\lambda}_z}(v)\xrightarrow{a.s.}C^{i,j}_{\lambda_z}(v)$ for $v=1,...p$ (this can be proved by employing the same exact strategy a for \eqref{Mdiff}). So for large $\textbf{n}$, $\hat{H}(\hat{\lambda}_z)>\delta$ which implies that there are no solutions to $\hat{M}^{i,j}(\lambda)\beta=\hat{C}^{i,j}_\lambda$ for $\lambda\in \hat{\Lambda}_\R(\textbf{n})$ for large enough $\textbf{n}$. Let $\hat{\beta}(\lambda)=(\hat{M}^{i,j})^{-1}(\lambda)\hat{C}^{i,j}_\lambda$ for $\lambda\not\in \hat{\Lambda}_n$. To find any candidates for $\hat{\lambda}$ we then solve $\hat{g}(\beta(\lambda))=0$. Similar to before we see that
\small
\begin{align}\label{groots}
\hat{g}(\beta(\lambda))&=\sum_{l=1}^p\left(\frac{1}{\det\left(\hat{M}^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(\hat{M}^{i,j}_{l,v}(\lambda)\right)(-1)^{l+v}\hat{C}^{i,j}_\lambda(v)\right)^2\left(\hat{a}_i(l)-\hat{a}_j(l)\right)+\hat{c}_i-\hat{c}_j\nonumber
\\
&-2\sum_{l=1}^p\left(\frac{1}{\det\left(\hat{M}^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(\hat{M}^{i,j}_{l,v}(\lambda)\right)(-1)^{l+v}\hat{C}^{i,j}_\lambda(v)\right)\left(\hat{b}_i(l)-\hat{b}_j(l)\right)\nonumber
\\
&+2\left(\hat{w}_i(l_1,l_2)-\hat{w}_j(l_1,l_2)\right)\sum_{l_1=1}^p\sum_{l_2\not=l_2}\left(\frac{1}{\det\left(\hat{M}^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(\hat{M}^{i,j}_{l_1,v}(\lambda)\right)(-1)^{l_1+v}\hat{C}^{i,j}_\lambda(v)\right)
\nonumber
\\
&\times\left(\frac{1}{\det\left(\hat{M}^{i,j}(\lambda)\right)}\sum_{v=1}^p\det\left(\hat{M}^{i,j}_{l_2,v}(\lambda)\right)(-1)^{l_2+v}\hat{C}^{i,j}_\lambda(v)\right).
\end{align}
\normalsize
Analogously to before that if let $\hat{\tilde{P}}(\lambda)=\det\left(\hat{M}^{i,j}(\lambda)\right)^2\hat{g}(\beta(\lambda))$ then $\hat{\tilde{P}}(\lambda)$ has the same roots as $\hat{g}(\beta(\lambda))$. By the law of large numbers and continuity we see that $\det\left(\hat{M}^{i,j}(\lambda)\right)\xrightarrow{a.s.}\det\left( M^{i,j}(\lambda)\right)\not=0$ (since $\lambda\not\in\Lambda$), $\det\left(\hat{M}^{i,j}_{u,v}(\lambda)\right)\xrightarrow{a.s.} \det\left(M^{i,j}_{u,v}(\lambda)\right)$ for all $u,v$, $\hat{b}_.(.)\xrightarrow{a.s.}b_.(.)$, $\hat{a}_.(.)\xrightarrow{a.s.}a_.(.)$, $\hat{w}_.(.,.)\xrightarrow{a.s.}w_.(.,.)$ and finally that $\hat{c}_.\xrightarrow{a.s.}c_.$. We can thus conclude that all coefficients of $\hat{\tilde{P}}$ converge to the corresponding coefficients of $\tilde{P}$ and therefore, by Lemma \ref{simpleroots}, the real roots of $\hat{\tilde{P}}$ converge to those of $\tilde{P}$ and that for large enough $\textbf{n}$, the number of such roots of $\tilde{P}$ and $\hat{\tilde{P}}$ coincides. Letting $\mathcal{R}=\{\lambda_1,...,\lambda_m\}$ (with $\lambda_1<...<\lambda_{\hat{m}}$) and $\hat{\mathcal{R}}=\{\hat{\lambda}_1,...,\hat{\lambda}_{\hat{m}}\}$ (with $\hat{\lambda}_1<...<\hat{\lambda}_{\hat{m}}$) denote the simple real roots of $\tilde{P}$ and $\hat{\tilde{P}}$ respectively we have that $\hat{\lambda}_u\xrightarrow{a.s.}\lambda_u$ for $u=1,..,m$ and for large $n$, $\hat{m}=m$, i.e. the elements of $\hat{\mathcal{R}}$ converge to those of $\mathcal{R}$. If we now define
$$\hat{B}^{i,j}(\textbf{n})=\left\{\hat{M}^{-1}(\lambda^{i,j})\hat{C}^{i,j}_{\lambda^{i,j}}: \lambda^{i,j}=\arg\min_{\lambda\in\hat{\mathcal{R}} }\R_{A_i}\left((\hat{M}^{i,j})^{-1}(\lambda)\hat{C}^{i,j}_\lambda\right)\right\}. $$
By the law of large numbers and continuity (since $\det\left(M^{i,j}(\lambda)\right)\not=0$ for $\lambda\in\Lambda$) $(\hat{M}^{i,j})^{-1}(\lambda)\hat{C}^{i,j}_{\lambda}\xrightarrow{a.s.}(M^{i,j})^{-1}(\lambda)C^{i,j}(\lambda)$ for $\lambda\not\in\Lambda$. Therefore $\max_{\beta\in\hat{B}^{i,j}}\dist\left(\beta,B^{i,j}\right)\xrightarrow{a.s.}0$ as well as $\#\hat{B}^{i,j}(\textbf{n})=\#B^{i,j}$ (with $\#A$ denoting the number of elements in the set $A$) for large $\textbf{n}$.
With a lexicographical ordering of $B^{i,j}$, $\beta^{i,j}_1<...<\beta^{i,j}_q$ and of $\hat{B}^{i,j}(\textbf{n})$, $\hat{\beta}_1^{i,j}<...<\hat{\beta}_{\hat{q}}^{i,j}$ (with $\hat{q}=q$ for large $\textbf{n}$) we have that $\hat{\beta}^{i,j}_u\xrightarrow{a.s.}\beta^{i,j}_u$ for $u=1,...q$.
\end{proof}
Notice that in the proof above we did not use the SEM structure of the underlying system at any point. In fact an interesting result we can extract from the proof above, regarding consistent plug-in estimation of the maximum $\arg\min$ solution of a finite set of quadratic risk functions. 
\begin{thm}
Given $k$ random vectors of length $p$, $\{X_u\}_{u=1}^k$ such that $X_j(l)\in L^2$ for $1\le l\le p$ and $\{Y_u\}_{u=1}^k$ such that $Y_u\in L^2$ consider the risk functions $R_u(\beta)=\E\left[\left(\beta X_u-Y_1\right)^2\right]$. If we denote 
$$\mathcal{B}=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p}\bigvee_{u=1}^kR_u(\beta')\right\},$$
$$\hat{\mathcal{B}}(\textbf{n})=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p}\bigvee_{u=1}^k\R_u(\beta')\right\}$$
then outside a zero set in $\Theta\times\Theta$, 
$$\dist\left(\hat{\beta}(\textbf{n}),\mathcal{B}\right)\xrightarrow{a.s.}0,$$
as $n_{A_1}\wedge...\wedge n_{A_k}\to\infty$, for any $\hat{\beta}(\textbf{n})\in\hat{\mathcal{B}}(\textbf{n})$.
\end{thm}
\begin{proof}
Let $f(\beta)=R_i(\beta)$ for $\beta\in\R^p$, $1\le i\le k$. To find an explicit solution to the inflexion point $\beta'=\arg\min_{\beta\in\R^p}R_i(\beta)$ we solve the equation $\nabla_\beta  f(\beta)=0$. To that end we compute,
$\nabla_\beta f(\beta)=\nabla_\beta R_i(\beta)$. In order to compute $\nabla_\beta R_{A_i}(\beta)$ we should first justify differentiation under the integral sign. Fix $M\in\R^+$ and let $\beta_j \in (-M,M)$ while we fix the other components of $\beta$. Set $g(\beta)=\left(X^{A_i}\beta-Y^{A_i}\right)^2$ then 
\begin{align*}
&\E\left[\left|\frac{\partial g(\beta)}{\partial \beta_j}(\beta)\right|\right]
=
2\E\left[\left|X^{A_i}(j)\left(X^{A_i}\beta-Y^{A_i}\right) \right|\right]
\le 2 \sqrt{\E\left[ X^{A_i}(j)^2 \right]}\sqrt{\E\left[ 2 (X^{A_i}\beta)^2+2(Y^{A_i})^2\right]}\le
\\
&2 \sqrt{\E\left[ X^{A_i}(j)^2 \right]}\sqrt{\E\left[  2X^{A_i}(X^{A_i})^TM^2+2(Y^{A_i})^2\right]},
\end{align*}
which is finite due to the assumption on the finiteness on the second moments of $\epsilon$ and $A_i$. This implies $\frac{\partial R_{A_i}(\beta)}{\partial \beta_j}$ exists for $\beta_j\in(-M,M)$, since $M$ is arbitrary it implies that $\frac{\partial R_{A_i}(\beta)}{\partial \beta_j}$ exists globally and furthermore,
\begin{align*}
&\nabla_\beta R_{A_i}(\beta)=2\E\left[(X^{A_i})^T\left(X^{A_i}\beta-Y^{A_i} \right)\right].
\end{align*}
which leads to $\beta'=(G^i)^{-1}Z^i$. Since we assumed $G^i$ is full rank it follows from the law of large numbers that $(\mathbb{G}^i(\textbf{n}))^{-1}\mathbb{Z}^i(\textbf{n})\xrightarrow{a.s.}\beta'$ (note that $\mathbb{G}^i(\textbf{n})$ must be full rank for large $\textbf{n}$, by continuity of the determinant). Now that we can do consistent plug-in estimation of the inflexion points the rest of the proof is completely analogous to that of Theorem \ref{optimalregthm}.
\end{proof}
%\begin{thm}
%Given $k\in\N$ random vectors of length $p$, $\{X_u\}_{u=1}^k$ such that $X_j(l)\in L^2$ for $1\le l\le p$ and $\{Y_u\}_{u=1}^k$ such that $Y_u\in L^2$. Let $w=(w_1,...,w_k)$ be a vector of non-negative real numbers and denote $R^w_+(\beta)=\sum_{i=1}^kw_i^2R_{X_i}(\beta)$. consider the risk functions $R_u(\beta)=\E\left[\left(\beta X_u-Y_1\right)^2\right]$. If we denote 
%$$\mathcal{B}=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p}\bigvee_{u=1}^kR_u(\beta')\right\},$$
%$$\hat{\mathcal{B}}(\textbf{n})=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p}\bigvee_{u=1}^k\R_u(\beta')\right\}$$
%then outside a zero set in $\Theta\times\Theta$, 
%$$\dist\left(\hat{\beta}_\lambda(\textbf{n}),\mathcal{B}\right)\xrightarrow{a.s.}0,$$
%as $n_{A_1}\wedge...\wedge n_{A_k}\to\infty$, for any $\hat{\beta}_\lambda(\textbf{n})\in\hat{\mathcal{B}}(\textbf{n})$.
%\end{thm}
%\begin{proof}
%Let $f(\beta)=R_i(\beta)$ for $\beta\in\R^p$, $1\le i\le k$. To find an explicit solution to the inflexion point $\beta'=\arg\min_{\beta\in\R^p}R_i(\beta)$ we solve the equation $\nabla_\beta  f(\beta)=0$. To that end we compute,
%$\nabla_\beta f(\beta)=\nabla_\beta R_i(\beta)$. In order to compute $\nabla_\beta R_{A_i}(\beta)$ we should first justify differentiation under the integral sign. Fix $M\in\R^+$ and let $\beta_j \in (-M,M)$ while we fix the other components of $\beta$. Set $g(\beta)=\left(X^{A_i}\beta-Y^{A_i}\right)^2$ then 
%\begin{align*}
%&\E\left[\left|\frac{\partial g(\beta)}{\partial \beta_j}(\beta)\right|\right]
%=
%2\E\left[\left|X^{A_i}(j)\left(X^{A_i}\beta-Y^{A_i}\right) \right|\right]
%\le 2 \sqrt{\E\left[ X^{A_i}(j)^2 \right]}\sqrt{\E\left[ 2 (X^{A_i}\beta)^2+2(Y^{A_i})^2\right]}\le
%\\
%&2 \sqrt{\E\left[ X^{A_i}(j)^2 \right]}\sqrt{\E\left[  2X^{A_i}(X^{A_i})^TM^2+2(Y^{A_i})^2\right]},
%\end{align*}
%which is finite due to the assumption on the finiteness on the second moments of $\epsilon$ and $A_i$. This implies $\frac{\partial R_{A_i}(\beta)}{\partial \beta_j}$ exists for $\beta_j\in(-M,M)$, since $M$ is arbitrary it implies that $\frac{\partial R_{A_i}(\beta)}{\partial \beta_j}$ exists globally and furthermore,
%\begin{align*}
%&\nabla_\beta R_{A_i}(\beta)=2\E\left[(X^{A_i})^T\left(X^{A_i}\beta-Y^{A_i} \right)\right].
%\end{align*}
%which leads to $\beta'=(G^i)^{-1}Z^i$. Since we assumed $G^i$ is full rank it follows from the law of large numbers that $(\mathbb{G}^i(\textbf{n}))^{-1}\mathbb{Z}^i(\textbf{n})\xrightarrow{a.s.}\beta'$ (note that $\mathbb{G}^i(\textbf{n})$ must be full rank for large $\textbf{n}$, by continuity of the determinant). Now that we can do consistent plug-in estimation of the inflexion points the rest of the proof is completely analogous to that of Theorem \ref{optimalregthm}.
%\end{proof}


The following theorem tells us that plug-in estimation of the argmin along the intersection of two risk curves will be consistent outside a zero set.
\begin{thm}\label{intersectionthm}
Given two random vectors of length $p$, $X_1$, $X_2$ such that $X_j(u)\in L^2$ for $1\le u\le p$ and $Y_1,Y_2$ such that $Y_1,Y_2\in L^2$ consider the risk functions $R_1(\beta)=\E\left[\left(\beta X_1-Y_1\right)^2\right]$, $R_2(\beta)=\E\left[\left(\beta X_2-Y_2\right)^2\right]$. If we define 
$$\mathcal{B}=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p: R_1(\beta')=R_2(\beta')}R_1(\beta')\right\},$$
$$\hat{\mathcal{B}}(\textbf{n})=\left\{\beta\in\R^p: \beta=\arg\min_{\beta'\in\R^p:\R_1(\beta')=\R_2(\beta')}\R_1(\beta')\right\}$$
Then outside a zero-set in $\Theta\times\Theta$,
$$\dist\left(\hat{\beta}_\lambda(\textbf{n}),\mathcal{B}\right)\xrightarrow{a.s.}0.$$
as $n_{A_1}\wedge...\wedge n_{A_k}\to\infty$, for any $\hat{\beta}_\lambda(\textbf{n})\in\hat{\mathcal{B}}(\textbf{n})$.
\end{thm}
\subsection{The approximate plug-in estimator}
From a practitioners point of view there is an issue with the plug-in estimators in the above theorems. Namely it requires the computation of the roots of a polynomial of degree $p$. By the Abel-Ruffini theorem we can't solve general polynomials of this type in terms of radicals. There is however an approximate plug-in estimator (or in reality a family of estimators) that will also be consistent, which do not require us to compute the roots analytically. The solution lies in the proof of Theorem \ref{optimalregthm}, when computing the roots in \eqref{groots} we note that they are simple roots so we may approximate these roots by using the bisection method. We now describe how to compute this approximate plug-in estimator
\\
Let $\{c_m\}_m$ be any sequence in $\N$ such that $c_n\to\infty$, assume we have $\textbf{n}=(n_O,n_{A_1},...,n_{A_k})$ samples from every environment and define $n=n_O\wedge n_{A_1}\wedge...\wedge n_{A_k}$. Let $\tilde{P}$ and $\hat{\tilde{P}}$ be as in the proof of Theorem \ref{optimalregthm} and let $\bar{\beta}_\gamma(\lambda)$ be the approximation of $\hat{\beta}_\gamma(\lambda)$ where we replace the roots of $\hat{\tilde{P}}$ with the following approximation. Since $\tilde{P}(\lambda)$ is a polynomial of degree $p-1$ we may write $\tilde{P}(\lambda)=\sum_{u=0}^{p-1}e_u\lambda^u$ it then follows that $\hat{\tilde{P}}(\lambda)=\sum_{u=0}^{p-1}\hat{e}_u\lambda^u$, where $\hat{e}_u$ is the plug-in estimator of $e_u$. By the Lagrange bound all real roots of $\hat{\tilde{P}}$ can be contained in $\left[-\left(1\vee\sum_{u=1}^{p-2}\left|\frac{\hat{e}_{u}}{\hat{e}_{p-1}}\right|\right),1\vee\sum_{u=1}\left|\frac{\hat{e}_{u}}{\hat{e}_{p-1}}\right|\right]$. Let $R_n=1\vee\sum_{u=1}^{p-2}\left|\frac{\hat{e}_{u}}{\hat{e}_{p-1}}\right|$, then since $\hat{e}_{u}\to e_u$ by the law of large numbers for $u=0,...,p-1$ we have that $R_n\xrightarrow{a.s.}R$ where $R=1\vee\sum_{u=1}^{p-2}\left|\frac{e_{u}}{e_{p-1}}\right|$. So for large $\textbf{n}$, $[-R_n,R_n]\subset[-(R+1),R+1]$. By Theorem 1 in \cite{rump} we have that the minimal distance between any roots of $\tilde{P}$ is bounded below by 
$$\Delta:= \left(1\vee\left|e_{p-1}\right|\right)^{(p-1)(\ln (p-1)+1)}D(\tilde{P})\frac{(2(p-1))^{p-2}}{s^{(p-1)(\ln(p-1)+3)}},$$
where $D(\tilde{P})$ denotes the discriminant of $P$ and $s=\sum_{u=0}^{p-1}|e_u|$. Similarly we let 
$$\hat{\Delta}= \left(1\vee\left|\hat{e}_{p-1}\right|\right)^{(p-1)(\ln (p-1)+1)}D(\hat{\tilde{P}})\frac{(2(p-1))^{p-2}}{\hat{s}^{(p-1)(\ln(p-1)+3)}}$$
then clearly $\hat{\Delta}\xrightarrow{a.s.}\Delta$ by continuity and the law of large numbers.  Let $\epsilon>0$, we now divide $[-R_n,R_n]$ into $m_n:=2\left \lceil{\frac{R_n}{\hat{\Delta}+\epsilon}}\right \rceil $ intervals $\{I_u\}_{u=1}^{m_n}$ of equal length such that $|I_u|<\Delta$. Any such interval $I_u$ has a root to $\hat{\tilde{P}}$ if and only if the two endpoints of $I_u$ are of opposite sign. Consider the $v$:th interval with a sign change for $\hat{\tilde{P}}$, this interval must contain $\hat{\lambda_v}$ (the $v$:th smallest real root to $\hat{\tilde{P}}$), we compute $c_n$ number of bisections to get our approximation $\bar{\hat{\lambda}}_v$ of $\hat{\lambda}$, which then will have the property $\left| \bar{\hat{\lambda}}_u - \hat{\lambda}_u\right|<\frac{1}{2^{c_n}}$ and therefore $\bar{\hat{\lambda}}_v\xrightarrow{a.s.}\lambda_v$. We summarize our findings above in the following theorem.
\begin{thm}
The bisection estimator $\bar{\beta}_\gamma(\lambda)$ described above has the same consistency property as $\hat{\beta}_\gamma$ in Theorem \ref{intersectionthm} in the sense that
$$\dist\left(\hat{\beta}_\lambda(\textbf{n}),\mathcal{B}\right)\xrightarrow{a.s.}0.$$
Moreover we will compute at most $(p-1)(\textbf{n}(1)\wedge\textbf{n}(2))$ bisections in total when we have $\textbf{n}$ samples.
\end{thm}

\subsubsection{Algorithm for computing the approximate plug-in estimator}
We now summarize exactly how to compute the approximate plug-in estimator given $n_{A_i}$ samples from environment $1\le i\le k$.
\begin{itemize}
\item
Compute the inflexion point of to every $\mathbb{R}_{A_i}(\beta)$ which is given by $\left(G^i_++\gamma G^i_\Delta\right)^{-1}\left(Z^i_++\gamma Z^i_\Delta\right)$.
\item
For every $1\le i<j\le k$ such that $\mathbb{R}_{A_i}(\beta)$ and $\mathbb{R}_{A_j}(\beta)$ intersect, compute all the roots to 
$$\hat{\tilde{P}}(\lambda)=\det\left(\hat{M}^{i,j}(\lambda)\right)^2\hat{g}(\beta(\lambda)),$$ 
(where $\hat{g}(\beta(\lambda))$ is given by \eqref{groots}) using the bisection method outlined above. For every such root $\lambda$, compute $\beta(\lambda)=\left(M^{i,j}\right)^{-1}(\lambda)C^{i,j}(\lambda)$
\item
The $\arg\min$ solution is now given by the $\beta$ amongst the ones computed above that minimizes 
$$\mathbb{R}_{O}(\beta)\wedge\mathbb{R}_{A_1}(\beta)\wedge...\wedge\mathbb{R}_{A_k}(\beta).$$
\end{itemize}


\begin{thebibliography}{6}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Kania and Wit(2022)]{kania2022causal}
Lucas Kania and Ernst Wit.
\newblock Causal regularization: On the trade-off between in-sample risk and
  out-of-sample risk guarantees.
\newblock \emph{arXiv preprint arXiv:2205.01593}, 2022.

\bibitem[Kennerberg and Wit(2023)]{kennerberg2023convergence}
Philip Kennerberg and Ernst~C. Wit.
\newblock Convergence properties of multi-environment causal regularization,
  2023.

\bibitem[Peters et~al.(2016)Peters, Bhlmann, and Meinshausen]{icp}
Jonas Peters, Peter Bhlmann, and Nicolai Meinshausen.
\newblock Causal inference by using invariant prediction: identification and
  confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 78\penalty0 (5):\penalty0 947--1012, 2016.
\newblock \doi{10.1111/rssb.12167}.

\bibitem[Rothenh{\"a}usler et~al.(2021)Rothenh{\"a}usler, Meinshausen,
  B{\"u}hlmann, and Peters]{Rot}
Dominik Rothenh{\"a}usler, Nicolai Meinshausen, Peter B{\"u}hlmann, and Jonas
  Peters.
\newblock Anchor regression: Heterogeneous data meet causality.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 83\penalty0 (2):\penalty0 215--246, 2021.

\bibitem[Rothenhusler et~al.(2019)Rothenhusler, Bhlmann, and
  Meinshausen]{causaldantzig}
Dominik Rothenhusler, Peter Bhlmann, and Nicolai Meinshausen.
\newblock Causal dantzig: Fast inference in linear structural equation models
  with hidden variables under additive interventions.
\newblock \emph{Ann. Statist.}, 47\penalty0 (3):\penalty0 1688--1722, 06 2019.
\newblock \doi{10.1214/18-AOS1732}.

\bibitem[Rump(1979)]{rump}
Siegfried~M. Rump.
\newblock Polynomial minimum root separation.
\newblock \emph{Mathematics of Computation}, 33\penalty0 (145):\penalty0
  327--336, 1979.

\end{thebibliography}

	

%\begin{thebibliography}{99}
%\bibitem[Rothenhusler, Meinshausen, Bhlmann and Peters (2021)]{Rot}
%Rothenhusler D., Meinshausen N., Bhlmann P. and Peters J. (2021). "Anchor regression: Heterogeneous data meet causality," Journal of the Royal Statistical Society Series B, Royal Statistical Society, vol. 83(2), pages 215-246, April.
%\end {thebibliography}
\newpage
\appendix
\renewcommand{\thesection}{A.\arabic{section}}
\section{Concentration in measure, in the uncorrelated case}
We shall now consider concentration in measure when we assume that all covariates across the different environments are uncorrelated. This is by no means a main result of the paper (as its placement should indicate) but we feel it has enough merit to include in the Appendix. In the case when all covariates across the different environments are uncorrelated, we can explicitly solve the equations that arise from the Kuhn-Tucker theorem, although it is \textit{very} cumbersome. The result that we prove will be valid outside a specified zero-set in $N\in\Theta\times\Theta$. The attentive reader will observe however that the set in $\Theta\times\Theta$ where the covariates are uncorrelated itself is a zero set so it is important that the two zero-sets do not coincide.
Let 
\begin{align*}
N:=&\left\{\theta_i\times\theta_j\in \Theta\times\Theta:\left(\frac{(b_i(l)-b_j(l))^2a_i(l)}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}\right)^2
-
\frac{c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}\not=0
\right.\nonumber
\\
&\left., b_i(l)\not=\frac{a_i(l)b_j(l)}{a_j(l)}\right\},
\end{align*}
this is clearly a zero set of $\Theta\times\Theta$ and it does not coincide with the set where all the covariates are uncorrelated. We also define the function
$$H(i,c,\delta)=e^{-c\left(\left(\sqrt{\delta}\wedge\delta\wedge c\wedge\sqrt{c}\wedge c^{\frac 14}\wedge c^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) -\max_{l}\left(h_1^{i,l}(n_{A_i}^\alpha)\wedge h_2^{i}(n_{A_i}^\alpha)\wedge h_3^{i,l}(n_{A_i}^\alpha)\right)\right)_+^2n_{A_i}^{1-4\alpha}}
+
1-F_{\left|X^{A_i}\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,...,n_{A_i}^\alpha)^{n_{A_i}},$$
where 
$$\mu=\left(\frac{(b_i(l)-b_j(l))^2a_i(l)}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}\right)^2
-
\frac{c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2} $$
and
\small
$$h^{i,l}_1(R)=\E\left[|X^{A_i}(l)|1_{\left\{|X^{A_i}(l)|\le R\right\}^c}\right],\hspace{4mm}h^{i}_2(R)=\E\left[|Y^{A_i}|1_{\left\{|Y^{A_i}|\le R\right\}^c}\right]\textit{ and }\hspace{4mm}  h^{i,l}_3(R)=\E\left[|Y^{A_i}||X^{A_i}(l)|1_{\left\{|Y^{A_i}|\le R,|X^{A_i}(l)|\le R\right\}^c}\right].$$
\normalsize
\begin{thm}\label{concmeas}
For every pair of environments $(i,j)$, assume that $\theta_i\times\theta_j\not\in N$.
Then for any vector norm $\n.\n$,
\begin{align*}
\P\left(\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right)
&\le
(8p+4)\sum_{i=0}^ke^{-r(c)^2 n_{A_i}^{1-4\alpha}}
\\
&+10\left(1-\prod_{i=0}^k\left(F_{\left|X^{A_i}(1)\right|,...,\left|X^{A_i}(p)\right|,|Y^{A_i}|}(n_{A_i}^\alpha,...,n_{A_i}^\alpha)\right)^{n_{A_i}}\right)
+
M' \max_iH(i,c,\delta),
\end{align*}
where $M'$ depends on $k$ and the chosen vector norm. $r(c)\ge M\left(\delta\wedge c\wedge\sqrt{c}\right)$,
$M$ is a (finite) positive constant that depends on the chosen norm $\n.\n$, $\delta$, $p$ and $k$,
\end{thm}
Central to the proof of Theorem \ref{concmeas} will be the following lemma
\begin{lemma}\label{intersectionlemma}
Assume $\theta_i\times\theta_j\not\in N$ then for any $1\le l\le p$ and $0<\delta<1$
\begin{align*}
&\P\left(\left|\beta_\gamma(l)-(\hat{\beta}_\gamma(\textbf{n}))(l)\right|\ge c\right)
\le
\\
&M e^{-c\left(\left(\sqrt{\delta}\wedge\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) -h_1^{i,l}(n_{A_i}^\alpha)\wedge h_1^{j,l}(n_{A_j}^\alpha)\wedge h_2^{i}(n_{A_i}^\alpha)\wedge h_2^{j}(n_{A_j}^\alpha)\wedge h_3^{i,l}(n_{A_i}^\alpha)\wedge h_3^{j,l}(n_{A_j}^\alpha)\right)_+^2n_{A_i}^{1-4\alpha}}
+
\\
&1-F_{\left|X^{A_i}(l)\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,n_{A_i}^\alpha)^{n_{A_i}}+1-F_{\left|X^{A_j}(l)\right|,\left|Y^{A_j}\right|}(n_{A_i}^\alpha,n_{A_j}^\alpha)^{n_{A_j}},
\end{align*}
where the constant $c$ depends only on the the model parameters, $M\le 2210$,
$$h^{i,l}_1(R)=\E\left[|X^{A_i}(l)|1_{\left\{|X^{A_i}(l)|\le R\right\}^c}\right],\hspace{4mm}h^{i}_2(R)=\E\left[|Y^{A_i}|1_{\left\{|Y^{A_i}|\le R\right\}^c}\right],\hspace{4mm}  h^{i,l}_3(R)=\E\left[|Y^{A_i}||X^{A_i}(l)|1_{\left\{|Y^{A_i}|\le R,|X^{A_i}(l)|\le R\right\}^c}\right].$$
\end{lemma}
For the proof, see the Appendix.
\begin{proof}[Proof of Theorem \ref{concmeas}]
Note that 
\begin{align*}
\beta_\gamma&=\arg\min_{\beta\in\R}(1+\tau)\left(R_{A_1}(\beta)\vee...\vee R_{A_k}(\beta)\right)-\tau R_O(\beta)
\\
&=\arg\min_{\beta\in\R}\left((1+\tau)R_{A_1}(\beta)-\tau R_O(\beta)\right)\vee...\vee \left((1+\tau)R_{A_k}(\beta)-\tau R_O(\beta)\right)
\end{align*}
Let $h_i(\beta)=(1+\tau)R_{A_i}(\beta)-\tau R_O(\beta)$ so that $\beta_\gamma=\arg\min_{\beta\in\R}h_1(\beta)\vee...\vee h_k(\beta)$. We also let $g(\beta)=h_1(\beta)\vee...\vee h_k(\beta)$. The minimum of $g$ can only be achieved at either an inflexion point for some $h_i$ or along the intersection (where $h$ might not be differentiable) of two $h_i$'s (if there is indeed an argmin along this intersection). The inflexion point of $h_i(\beta)$ is achieved at $\beta_{inf(i)}:=\left(G^i_++\gamma G^i_\Delta\right)^{-1}\left(Z^i_++\gamma Z^i_\Delta\right)$. Let $B_{inf}=\{\beta_{inf(1)},...,\beta_{inf(k)}\}$ denote the set of inflexion points and let $\hat{B}_{inf}(\textbf{n})$ denote the corresponding estimators. If $R_i$ and $R_j$ intersect and have an argmin along this intersection let $\beta_{int}^{i,j}$ be this argmin point. If $R_i$ and $R_j$ do not intersect or attain an argmin we will set $\beta_{int}^{i,j}=+\infty$. Correspondingly, if $\hat{R}_i$ and $\hat{R}_j$ intersect and attain an argmin along this intersection we let $\hat{\beta}_{int}^{i,j}(\textbf{n})$ denote the plug in estimator of this argmin along this intersection (as in Lemma \ref{intersectionlemma}). If $\hat{R}_i$ and $\hat{R}_j$ do not intersect or an argmin is not attained along their intersection we set $\hat{\beta}_{int}^{i,j}(\textbf{n})=+\infty$. Let $B_{int}=\left\{\beta_{int}^{1,1},...,\beta_{int}^{1,k},\beta_{int}^{2,3},...,\beta_{int}^{2,k},...,\beta_{int}^{k-1,k},\beta_{int}^{k,k}\right\}$ denote all the intersections points between the different $h_i$'s and let $\hat{B}_{int}(\textbf{n})$ denote the corresponding set of estimators. We also let $B=B_{int}\cup B_{inf}$ and $\hat{B}(\textbf{n})=\hat{B}_{int}(\textbf{n})\cup \hat{B}_{inf}(\textbf{n})$. For $\beta_{int}^{i,j}$ to be a candidate for the $\arg\min$ there are two conditions it must fulfil. First it must be finite and second we must have that $h_i(\beta_{int}^{i,j})=g(\beta_{int}^{i,j})$. Let $ \hat{h}_i(\beta)=(1+\tau)\mathbb{R}_{A_i}-\tau \mathbb{R}_O$ and $\hat{g}(\beta)=\bigwedge_{i=1}^k\hat{h}_i(\beta)$. It is straight-forward to verify that $\arg\min \hat{h}_i(\beta)$ is achieved at its inflexion point $\beta^i=\left(\mathbb{G}^i_++\gamma\mathbb{G}^i_\Delta\right)^{-1}\left(\mathbb{Z}^i_++\gamma\mathbb{Z}^i_\Delta\right)$. With our new notation, the argmin of $g$ must be achieved at a point in either $\hat{B}_{inf}(\textbf{n})$ or $\hat{B}_{int}(\textbf{n})$, i.e.
$$\beta_\gamma=\arg\min_{\beta\in B_{inf} \cup  B_{int}\cap\R\cap\{\beta:\hspace{1mm} \exists 1\le i \le k,\hspace{1mm} g(\beta)=h_i(\beta)\}}g(\beta), $$
while
$$\hat{\beta}_\gamma(\textbf{n})=\arg\min_{\beta\in \hat{B}_{inf}(\textbf{n}) \cup \hat{B}_{int}(\textbf{n})\cap\R\cap\{\beta:\hspace{1mm} \exists 1\le i \le k, \hat{g}(\beta)= \hat{h}_i(\beta)\}} \hat{g}(\beta) .$$
%If $\left|\beta-\hat{\beta} \right|<\nu/2$ for all $\beta\in B$ then $\left| \hat{\beta}_\lambda(\textbf{n})-\beta_\gamma\right|<\nu/3$, since whatever $\beta\in B$ is the minimizer that will also correspond to the estimator chosen from $\hat{B}(\textbf{n})$ due to the definition of $\nu$. If $\left|\beta-\hat{\beta} \right|\ge\nu/2$ then we might by mistake be choosing the wrong estimand. 
We now construct the vector $V$ as follows, the first $k$ entries are $\beta_{inf(1)},...,\beta_{inf(k)}$ in chronological order. Then we place all the $\beta_{int}^{i,j}$'s, 
$$\beta_{int}^{1,2},...,\beta_{int}^{1,k},\beta_{int}^{2,3},...,\beta_{int}^{2,k},....,\beta_{int}^{k-2,k-1},\beta_{int}^{k-2,k},\beta_{int}^{k-1,k}$$
with the convention that we always place $\beta_s^{i,j}$ for $i<j$ but never if $i\ge k$. This way we can associate each argmin- candidate point with a unique index number in $V$. The number of elements in $V$ will be $M=k+2\frac{k!}{2(k-2)!}=k^2$. We then construct $\hat{V}_n$ completely analogously from the corresponding estimators. Let
$$L=\arg\min_{1\le l \le M}g(V(l)), $$
so that $\beta_\gamma= g(V(L))$
and similarly let
$$\hat{L}_\textbf{n}=\arg\min_{1\le l \le M}g(\hat{V}_\textbf{n}(l)), $$
so that $\hat{\beta}_\gamma(\textbf{n})= g(\hat{V}_\textbf{n}(\hat{L}_\textbf{n})).$
Let $I=\{1\le i\le M:V(i)<\infty\}$ and 
$$\mu=\min_{i,j\in I}\left|\left(\frac{n_p^{i,j}}{d^{i,j}}\right)^2-\frac{n_q^{i,j}}{d^{i,j}}\right|,$$
which is non-zero by assumption. It may be the case that $\hat{L}_\textbf{n}$ is achieved for $l\not\in I$ (due to $\mathbb{R}_i$ and $\mathbb{R}_j$ intersecting while $R_i$ and $R_j$ do not intersect), but this is impossible whenever
$$C_\textbf{n}:=\bigcap_{1\le i<j\le k}\left\{\left|\left(\frac{n_p^{i,j}(\textbf{n})}{\hat{d}^{i,j}(\textbf{n})}\right)^2-\frac{\hat{n}_q^{i,j}(\textbf{n})}{\hat{d}^{i,j}(\textbf{n})}
-
\left(\left(\frac{n_p^{i,j}}{d(i,j)}\right)^2-\frac{n_q(i,j)}{d(i,j)}\right)\right|<\mu/2\right\}$$
occurs. From the proof of Lemma \ref{intersectionlemma} we know that
\begin{align*}
&\P\left(\left\{\left|\left(\frac{n_p^{i,j}(\textbf{n})}{\hat{d}^{i,j}(\textbf{n})}\right)^2-\frac{\hat{n}_q^{i,j}(\textbf{n})}{\hat{d}^{i,j}(\textbf{n})}
-
\left(\left(\frac{n_p^{i,j}}{d(i,j)}\right)^2-\frac{n_q(i,j)}{d(i,j)}\right)\right|<\mu/2\right\}^c\right)
\le
\\
&140\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_9\left(\delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
104\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{10}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&32\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{11}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
32\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{12}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+4\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{13}\left(\sqrt{\mu}\wedge\mu\right)\right)
+
\\
&4\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{14}\left(\sqrt{\mu}\wedge\mu\right)\right).
\end{align*}
Since there are at most $\frac{k(k-1)}{2}$ number of possible intersections we therefore may conclude that
\begin{align*}
&\P\left(C_\textbf{n}^c\right)
\le
\frac{k(k-1)}{2}\max_{i,j\in I}\left( 140\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_9\left(\delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\right.
\\
&\left.104\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{10}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+32\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{11}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+\right.\nonumber
\\
&\left.
32\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{12}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+4\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{13}\left(\sqrt{\mu}\wedge\mu\right)\right)
+\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{14}\left(\sqrt{\mu}\wedge\mu\right)\right)\right)
\le
\\
&M_1(k)\max_{i\in I}\left(\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_1'\left(\delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
\right.
\\
&\left.\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_2'\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_3'\left(\sqrt{\mu}\wedge\mu\right)\right)\right).
\end{align*}
Let $a=\min_{\beta\in B\setminus\beta_\gamma}g(\beta)$ and $\nu=g(a)-g(\beta_\gamma)$, then $\nu>0$ and since $g$ is continuous there exists some $b>0$ such that if $|\beta_\gamma-\beta|<b$ then $|g(\beta)-g(\beta_\gamma)|<\nu/3$. Therefore, if $\left|V(l)-\hat{V}_\textbf{n}(l) \right|<b$, $\forall l\in I$ and $C_\textbf{n}$ occurs then $\hat{V}_\textbf{n}(L)\le V(L)+\left|\hat{V}_\textbf{n}(L)-V(L)\right|\le \beta_\gamma+\nu/3$ while for $l\in I\setminus L$
$$\hat{V}_\textbf{n}(l)\ge V(l)-|V(l)-\hat{V}_\textbf{n}(l)|\ge \beta_\gamma+\nu-\nu/3=\beta_\gamma+\frac{2}{3}\nu,$$
while also $\hat{V}_\textbf{n}(l)=+\infty$ for $l\not\in I$ (since $C_\textbf{n}$ occurs). We can thus conclude that occurence of $C_\textbf{n}$ and $\left|V(l)-\hat{V}_\textbf{n}(l) \right|<b$, $\forall l\in I$ guarantees that $\hat{L}_\textbf{n}=L$. With this in mind,
\begin{align}\label{mainineq}
&\P\left(\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right)
\le 
\P\left(\left\{\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right\}\cap\left\{L=\hat{L}_\textbf{n}\right\}\right)
+
\P\left(\bigcup_{l\in I}\left\{\left|V(l)-\hat{V}_\textbf{n}(l) \right|\ge b\right\}\right)+\P\left(C_\textbf{n}^c \right).
\end{align}
Returning to \eqref{mainineq},
\begin{align}\label{vbound}
&\P\left(\bigcup_{l\in I}\left\{\left|V(l)-\hat{V}_\textbf{n}(l) \right|\ge b\right\}\right)
\le
\sum_{l\in I}\P\left(\left\{\left|V(l)-\hat{V}_\textbf{n}(l) \right|\ge b\right\}\cap C_\textbf{n}\right)+\P\left(C_\textbf{n}^c\right)
\le \nonumber
\\
&\sum_{i=1}^k\P\left(\n\beta_{inf(i)}-\hat{\beta}_{inf(i)}(\textbf{n}) \n\ge b\right)
+
\sum_{1\le i<j\le k, \beta_{int}^{i,j}<\infty}\P\left(\left\{\n\beta_{int}^{i,j}-\hat{\beta}_{int}^{i,j}(\textbf{n}) \n\ge b\right\}\cap C_\textbf{n}\right)
+\P\left(C_\textbf{n}^c\right).
\end{align}
If $\beta_\gamma=\beta_{inf(i)}$ for some $1\le i\le k$ then $\P\left(\left\{\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right\}\cap\left\{L=\hat{L}_\textbf{n}\right\}\right)\le \P\left(\n\beta_{inf(i)}-\hat{\beta}_{inf(i)}(\textbf{n}) \n\ge c\right)$ and similarly if $\beta_\gamma=\beta_{int}^{i,j}$ then  $\P\left(\left\{\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right\}\cap\left\{L=\hat{L}_\textbf{n}\right\}\right)\le \P\left(\n\beta_{int}^{i,j}-\hat{\beta}_{int}^{i,j}(\textbf{n}) \n\ge c\right)$. Also we have that 
\begin{align*}
&\P\left(\n \beta_{int}^{i,j}-\hat{\beta}_{int}^{i,j}(\textbf{n}) \n \ge c\right)
\le
\\
&M' e^{-c\left(\left(\sqrt{\delta}\wedge\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) -h_1^{i,l}(n_{A_i}^\alpha)\wedge h_1^{j,l}(n_{A_j}^\alpha)\wedge h_2^{i}(n_{A_i}^\alpha)\wedge h_2^{j}(n_{A_j}^\alpha)\wedge h_3^{i,l}(n_{A_i}^\alpha)\wedge h_3^{j,l}(n_{A_j}^\alpha)\right)_+^2n_{A_i}^{1-4\alpha}}
+
\\
&1-F_{\left|X^{A_i}(l)\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,n_{A_i}^\alpha)^{n_{A_i}}+1-F_{\left|X^{A_j}(l)\right|,\left|Y^{A_j}\right|}(n_{A_i}^\alpha,n_{A_j}^\alpha)^{n_{A_j}},
\end{align*}
for some constant $M'$ that depends on $k$ and the chosen vector norm.
Thus
\begin{align*}
&\P\left(\n\beta_\gamma-\hat{\beta}_\gamma(\textbf{n}) \n\ge c \right)
\le 
2\sum_{i=1}^k\P\left(\n\beta_{inf(i)}-\hat{\beta}_{inf(i)}(\textbf{n}) \n\ge b\wedge c\right)
+
\\
&2\sum_{1\le i<j\le k, \beta_{int}^{i,j}<\infty}\P\left(\n\beta_{int}^{i,j}-\hat{\beta}_{int}^{i,j}(\textbf{n}) \n\ge b\wedge c\right)
+2\P\left(C_\textbf{n}^c\right)
\le
(8p+4)\sum_{i=0}^ke^{-r(c)^2 n_{A_i}^{1-4\alpha}}
+
\\
&10\left(1-\prod_{i=0}^k\left(F_{\left|X^{A_i}(1)\right|,...,\left|X^{A_i}(p)\right|,|Y^{A_i}|}(n_{A_i}^\alpha,...,n_{A_i}^\alpha)\right)^{n_{A_i}}\right)
+
\\
&M' \max_i\left(e^{-c\left(\left(\sqrt{\delta}\wedge\delta\wedge c\wedge\sqrt{c}\wedge c^{\frac 14}\wedge c^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) -\max_{l}\left(h_1^{i,l}(n_{A_i}^\alpha)\wedge h_2^{i}(n_{A_i}^\alpha)\wedge h_3^{i,l}(n_{A_i}^\alpha)\right)\right)_+^2n_{A_i}^{1-4\alpha}}
+
1-F_{\left|X^{A_i}\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,...,n_{A_i}^\alpha)^{n_{A_i}}\right)
\end{align*}
\end{proof}
\subsection{Proof of Lemma \ref{intersectionlemma}}
\begin{proof}
For the sake of brevity we shall denote
$$a_i(l)=\E\left[(X(l)^{A_i})^2-\gamma\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right],\hspace{3mm} b_i(l)=\E\left[Y^{A_i}X(l)^{A_i}\right],\hspace{3mm} c_i=\E\left[Y^{A_i}Y^{A_i}\right].$$
$R_{A_i}(\beta)=R_{A_j}(\beta)$ implies that
\begin{align*}
&g(\beta)=\sum_{l=1}^p\beta_l^2\E\left[\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right] 
-
2\sum_{l=1}^p\beta_l\E\left[Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right] 
+\E\left[(Y^{A_i})^2-(Y^{A_j})^2\right]=0.
\end{align*}
Consider the Lagrangian,
\begin{align*}
&\mathcal{L}(\beta,\lambda)=R_{A_i}(\beta)-\lambda g(\beta)
=
\sum_{l=1}^p\beta_l^2\left(\E\left[(X(l)^{A_i})^2-\lambda\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right] \right)
\\
&-2\sum_{l=1}^p\beta_l\E\left[Y^{A_i}(X(l)^{A_i})-\lambda\left(Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right)\right]
+
\E\left[(Y^{A_i})^2-\lambda\left(Y^{A_i})^2-(Y^{A_j})^2\right)\right]
=
\\
&\sum_{l=1}^p\beta_l^2\left(a_i(l)-\lambda\left( a_i(l)-a_j(l)\right) \right)
-2\sum_{l=1}^p\beta_l\left(b_i(l)-\lambda\left(b_i(l)-b_j(l)\right)\right)
+c_i-\lambda\left(c_i-c_j\right).
\end{align*}
By the necessity part of the Kuhn-Tucker Theorem, a necessary condition for $\beta^*$ to be a minimum point for $R_{A_i}(\beta)$ subject to $g(\beta)=0$ is that $\nabla_\beta \mathcal{L}(\beta^*,\lambda^*)=0$ for some $\lambda^*$ and $g(\beta^*)=0$. First note that if $\lambda^*=\frac{a_i(l)}{a_i(l)-a_j(l)}$ then $\nabla_\beta \mathcal{L}(\beta^*,\lambda^*)=0$ implies that $b_i(l)=\frac{a_i(l)b_j(l)}{a_j(l)}$, a contradiction to our assumption. We can henceforth assume that $a_i(l)-\lambda(a_i(l)-a_j(l)) \not=0$. Solving for $\beta^*$ in $\nabla_\beta \mathcal{L}(\beta^*,\lambda^*)=0$ gives
$$\beta_l^*=\frac{\E\left[Y^{A_i}(X(l)^{A_i})-\lambda\left(Y^{A_i}(X(l)^{A_i})-Y^{A_j}(X(l)^{A_j})\right)\right]}{\E\left[(X(l)^{A_i})^2-\lambda\left( (X(l)^{A_i})^2-(X(l)^{A_j})^2\right)\right] } $$
so
$$\beta_l^*=\frac{b_i(l)-\lambda(b_i(l)-b_j(l))}{a_i(l)-\lambda(a_i(l)-a_j(l)) } .$$
To find $\lambda^*$ we solve $g(\beta^*)=0$,
\begin{align*}
&g(\beta^*)=\sum_{l=1}^p\left(\frac{b_i(l)-\lambda(b_i(l)-b_j(l))}{a_i(l)-\lambda(a_i(l)-a_j(l)) }\right)^2(a_i(l)-a_j(l)) 
-
2\sum_{l=1}^p\left(\frac{b_i(l)-\lambda(b_i(l)-b_j(l))}{a_i(l)-\lambda(a_i(l)-a_j(l)) }\right)(b_i(l)-b_j(l))
+c_i-c_j=0.
\end{align*}
%\begin{align*}
%&\sum_{l=1}^p\left(b_i(l)-\lambda(b_i(l)-b_j(l))\right)^2(a_i(l)-a_j(l)) 
%-
%\\
%&2\sum_{l=1}^p\left(b_i(l)-\lambda(b_i(l)-b_j(l))\right)\left(a_i(l)-\lambda(a_i(l)-a_j(l))\right)(b_i(l)-b_j(l))
%+c_i-c_j=0.
%\end{align*}
This is equivalent to,
\begin{align*}
&\lambda^2\left((b_i(l)-b_j(l))^2(a_i(l)-a_j(l)) -2(a_i(l)-a_j(l))(b_i(l)-b_j(l))^2  \right)
-
\\
&2\lambda\left(b_i(l)(a_i(l)-a_j(l))(b_i(l)-b_j(l))-b_i(l)(b_i(l)-b_j(l))(a_i(l)-a_j(l))-(b_i(l)-b_j(l))^2a_i(l)\right)+
\\
&c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))=0
\end{align*}
%and after further simplification,
%\begin{align*}
%&\lambda^2\left((a_j(l)-a_i(l))(b_i(l)-b_j(l))^2  \right)
%+
%2\lambda\left((b_i(l)-b_j(l))^2a_i(l)\right)-
%c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))=0.
%\end{align*}
and after further simplification (by assumption $(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2\not=0$),
\begin{align*}
&\lambda^2
+
2\lambda\frac{(b_i(l)-b_j(l))^2a_i(l)}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}
+
\frac{c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}=0.
\end{align*}
Thus,
\begin{align*}
&\lambda
=
\frac{(b_i(l)-b_j(l))^2a_i(l)}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}
\pm
\\
&\sqrt{\left(\frac{(b_i(l)-b_j(l))^2a_i(l)}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}\right)^2
-
\frac{c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))}{(a_j(l)-a_i(l))(b_i(l)-b_j(l))^2}}
\end{align*}
Let 
$$d=(b_i(l)-b_j(l))^2(a_i(l)-a_j(l)),\hspace{4mm}n_p=(b_i(l)-b_j(l))^2a_i(l),$$
$$n_q=c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))$$
$$\lambda_+
=\frac{n_p}{d}+
\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12},\hspace{4mm} \lambda_-
=\frac{n_p}{d}-
\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12}.$$
%\begin{align*}
%&\lambda_+
%=\frac{n_p}{d}+
%\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12}
%\end{align*}
%\begin{align*}
%&\lambda_-
%=\frac{n_p}{d}-
%\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12}
%\end{align*}
$$\hat{d}(\textbf{n})=(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2(\mathbb{G}^i(l)-\mathbb{G}^j(l)),\hspace{4mm} \hat{n}_p(\textbf{n})=\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2$$
%$$\hat{n}_p(\textbf{n})=\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2$$
$$\hat{n}_q(\textbf{n})=\mathbb{G}^i_Y-\mathbb{G}^j_Y+\mathbb{Z}^i(l)^2(\mathbb{G}^i(l)-\mathbb{G}^j(l))-2\mathbb{Z}^i(l)\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))$$
$$\lambda_+
=
\frac{n_p}{d}
+
\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12},\hspace{4mm} \lambda_-
=
\frac{n_p}{d}
-
\left(\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}\right)^{\frac 12}$$
$$\hat{\lambda}_+(\textbf{n})
=
\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}
+
\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)^{\frac 12},\hspace{4mm} \hat{\lambda}_-(\textbf{n})
=
\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}
-
\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)^{\frac 12}$$

%\begin{align*}
%&\hat{\lambda}_+(\textbf{n})
%=
%\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}
%+
%\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)^{\frac 12}
%\end{align*}
%\begin{align*}
%&\hat{\lambda}_-(\textbf{n})
%=
%\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}
%-
%\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)^{\frac 12}
%\end{align*}
$$\hat{\beta}_\textbf{n}(l)^+=\frac{\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}^+(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}^+(\mathbb{G}^i(l)-\mathbb{G}^j(l)) },\hspace{4mm} \hat{\beta}_\textbf{n}(l)^-=\frac{\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}^-(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}^-(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }$$
We now proceed with some preliminary calculations that will be necessary.
\begin{align}
&\left|\hat{d}(\textbf{n})-d\right|
=
\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2(\mathbb{G}^i(l)-\mathbb{G}^j(l))-(b_i(l)-b_j(l))^2(a_i(l)-a_j(l))\right| 
\le\nonumber
\\
&\left|\mathbb{Z}^i(l)-\mathbb{Z}^j(l)\right|^2\left|(\mathbb{G}^i(l)-\mathbb{G}^j(l))-(a_i(l)-a_j(l))\right|
+
\left|a_i(l)-a_j(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2-(b_i(l)-b_j(l))^2\right|
\le\nonumber
\\
&\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|+\left|\mathbb{Z}^j(l)-b_j(l)\right|+\left|b_i(l)\right|+\left|b_j(l)\right|\right)^2\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\right)
+\nonumber
\\
&\left|a_i(l)-a_j(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|+2\left|b_i(l)-b_j(l)\right|\right)
\le\nonumber
\\
&9\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2+\left|\mathbb{Z}^j(l)-b_j(l)\right|^2+\left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^2\right)\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\right)
+\nonumber
\\
&\left|a_i(l)-a_j(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|+2\left|b_i(l)-b_j(l)\right|\right)
\end{align}
thus
\begin{align}\label{d2terms}
&\P\left( \left|\hat{d}(\textbf{n})- d\right|\ge  b'\right)
\le\nonumber
\\
&\P\left(9\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2+\left|\mathbb{Z}^j(l)-b_j(l)\right|^2+\left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^2\right)\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\right)\ge b'/2\right)
+\nonumber
\\
&\P\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|+2\left|b_i(l)-b_j(l)\right|\right)\ge \left|a_i(l)-a_j(l)\right|^{-1}b'/2\right).
\end{align}
We first the bound the first term above,
\begin{align}\label{dterm1}
&\P\left(9\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2+\left|\mathbb{Z}^j(l)-b_j(l)\right|^2+\left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^2\right)\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\right)\ge b'/2\right)
\le\nonumber
\\
&\P\left(\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2+\left|\mathbb{Z}^j(l)-b_j(l)\right|^2\right)\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\right)\ge b'/36\right)
+\nonumber
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|\ge \left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^{-2}b'/36\right)
\le
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2+\left|\mathbb{Z}^j(l)-b_j(l)\right|^2\ge \sqrt{b'}/6\right)
+\nonumber
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge \left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^{-2}b'/72\right)
+
\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge \left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^{-2}b'/72\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_1b'^{\frac 14}\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_2b'^{\frac 14}\right)
+
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_3b'\right)
+
\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_4b'\right),
\end{align}
for suitable constants $c_1,..,c_4$. Now we bound the second term on the right-hand side of \eqref{d2terms}
\begin{align*}\label{dterm2}
&\P\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|+2\left|b_i(l)-b_j(l)\right|\right)\ge \left|a_i(l)-a_j(l)\right|^{-1}b'/2\right)
\le
\\
&\P\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|^2\ge \left|a_i(l)-a_j(l)\right|^{-1}b'/4\right)
+\nonumber
\\
&\P\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|2\left|b_i(l)-b_j(l)\right|\ge \left|a_i(l)-a_j(l)\right|^{-1}b'/4\right)
\le
\\
&\P\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))-(b_i(l)-b_j(l))\right|\ge \sqrt{\left|a_i(l)-a_j(l)\right|^{-1}b'/4}\right)
+
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)
\le
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \frac{\sqrt{\left|a_i(l)-a_j(l)\right|^{-1}b'}}{4}\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \frac{\sqrt{\left|a_i(l)-a_j(l)\right|^{-1}b'}}{4}\right)
+
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)
\le
\\
&2\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_5\left(\sqrt{b'}\wedge b'\right)\right)
+
2\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_6\left(\sqrt{b'}\wedge b'\right)\right).
\end{align*}
If we now plug \eqref{dterm1} and \eqref{dterm2} back into \eqref{d2terms} we get
\begin{align}
&\P\left( \left|\hat{d}(\textbf{n})- d\right|\ge  b'\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \frac{b'^{\frac 14}}{2\sqrt{3}}\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \frac{b'^{\frac 14}}{2\sqrt{3}}\right)
+
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge \left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^{-2}b'/72\right)
+\nonumber
\\
&\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge \left(\left|b_i(l)\right|+\left|b_j(l)\right|\right)^{-2}b'/72\right)
+\nonumber
\\
&2\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left(\frac{\sqrt{\left|a_i(l)-a_j(l)\right|^{-1}b'}}{4}\right)\wedge \left(\left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)\right)
+\nonumber
\\
&2\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \left(\frac{\sqrt{\left|a_i(l)-a_j(l)\right|^{-1}b'}}{4}\right)\wedge\left(\left|b_i(l)-b_j(l)\right|^{-1}\left|a_i(l)-a_j(l)\right|^{-1}b'/16\right)\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_7b'\right)
+
\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_8b'\right)
+
3\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_9\left(\sqrt{b'}\wedge b'\wedge  b'^{\frac 14}\right)\right)
+\nonumber
\\
&3\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{10}\left(\sqrt{b'}\wedge b'\wedge  b'^{\frac 14}\right)\right).
\end{align}
Next,
\begin{align*}
&\left|\hat{n}_p(\textbf{n})-n_p\right|
=
\left|\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2 -a_i(l)(b_i(l)-b_j(l))^2\right|
\le
\left|\mathbb{G}^i(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))^2 -(b_i(l)-b_j(l))^2\right|
+
\\
&(b_i(l)-b_j(l))^2\left|\mathbb{G}^i(l)-a_i(l)\right|
\le
\left|\mathbb{G}^i(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) +(b_i(l)-b_j(l))\right|
+
\\
&(b_i(l)-b_j(l))^2\left|\mathbb{G}^i(l)-a_i(l)\right|
\le
\left|\mathbb{G}^i(l)\right|\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|\left(\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|+ \left|b_i(l)-b_j(l)\right|\right)
+
\\
&(b_i(l)-b_j(l))^2\left|\mathbb{G}^i(l)-a_i(l)\right|
\le
\left(\left|\mathbb{G}^i(l)-a_i(l)\right| +|a_i(l)|\right)\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|^2
+
\\
&\left(\left|\mathbb{G}^i(l)-a_i(l)\right| +|a_i(l)|\right)\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|\left|b_i(l)-b_j(l)\right|
+
(b_i(l)-b_j(l))^2\left|\mathbb{G}^i(l)-a_i(l)\right|
\end{align*}
hence
\begin{align*}
&\P\left(\left|\hat{n}_p(\textbf{n})-n_p\right|\ge b'\right)
\le
\P\left(\left(\left|\mathbb{G}^i(l)-a_i(l)\right| +|a_i(l)|\right)\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|^2\ge b'/3\right)
+
\\
&\P\left(  \left(\left|\mathbb{G}^i(l)-a_i(l)\right| +|a_i(l)|\right)\left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|\left|b_i(l)-b_j(l)\right| \ge b'/3\right)
+
\\
&\P\left((b_i(l)-b_j(l))^2\left|\mathbb{G}^i(l)-a_i(l)\right| \ge b'/3\right)
\le
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|^2\ge b'/6\right)
+
\\
&\P\left(  \left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right| \ge \left|b_i(l)-b_j(l)\right|^{-1}|a_i(l)|^{-1}b'/6\right)
+
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \ge (b_i(l)-b_j(l))^{-2}b'/3\right)
\le
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \ge \sqrt{b'/6}\right)
+
\P\left( \left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right|\ge \left(b'/6\right)^{\frac 14}\right)
+
\\
&\P\left(  \left|(\mathbb{Z}^i(l)-\mathbb{Z}^j(l)) -(b_i(l)-b_j(l))\right| \ge \left|b_i(l)-b_j(l)\right|^{-1}|a_i(l)|^{-1}b'/6\right)
\le
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \ge \sqrt{b'/6}\right)
+
\\
&2\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{11}\left(b'^{\frac 14}\wedge b'\right)\right)
+
2\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{12}\left(b'^{\frac 14}\wedge b'\right)\right)
\end{align*}

\begin{align}\label{nqexpr}
&\left|\hat{n}_q(\textbf{n})-n_q\right|
=\nonumber
\\
&\left|\mathbb{G}^i_Y-\mathbb{G}^j_Y+\mathbb{Z}^i(l)^2(\mathbb{G}^i(l)-\mathbb{G}^j(l))-2\mathbb{Z}^i(l)\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))
-
\left(c_i-c_j+b_i(l)^2(a_i(l)-a_j(l))-2b_i(l)a_i(l)(b_i(l)-b_j(l))\right)\right|\nonumber
\\
&\le
\left|\mathbb{G}^i_Y-c_i\right|+\left|\mathbb{G}^j_Y-c_j\right|
+
\left|\mathbb{Z}^i(l)^2(\mathbb{G}^i(l)-\mathbb{G}^j(l))-
b_i(l)^2(a_i(l)-a_j(l))\right|
+\nonumber
\\
&2\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))
-
b_i(l)a_i(l)(b_i(l)-b_j(l))\right|
\le
\left|\mathbb{G}^i-c_i\right|+\left|\mathbb{G}^j_Y-c_j\right|
+
\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)\right|\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|
+\nonumber
\\
&b_i(l)^2\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)-(a_i(l)-a_j(l))\right|
+
\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)\right|\left|\mathbb{Z}^i(l)-\mathbb{Z}^j(l)
-
(b_i(l)-b_j(l))\right|
+
\left|b_i(l)-b_j(l)\right|\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)
-
b_i(l)a_i(l)\right|
\le\nonumber
\\
&\left|\mathbb{G}^i-c_i\right|+\left|\mathbb{G}^j_Y-c_j\right|
+
\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)\right|\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|
+
b_i(l)^2\left|\mathbb{G}^i(l)-a_i(l)\right|
+
b_i(l)^2\left|\mathbb{G}^j(l)-a_j(l)\right|
+\nonumber
\\
&\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)\right|\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|+\left|\mathbb{Z}^j(l)-b_j(l)\right|\right).
\end{align}
Let us first calculate the exceedance probability for the third term on the right-most side of \eqref{nqexpr}, 
\begin{align}\label{nqt3}
&\P\left(\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)\right|\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|\ge b'\right)
\le
\P\left(\left(\left|\mathbb{G}^i(l) -a_i(l)\right|+\left|\mathbb{G}^j(l)-a_j(l)\right|+\left|a_i(l)+a_j(l)\right| \right)\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|\ge b'\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge b'/3\right)
+
\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge b'/3\right)
+\nonumber
\\
&\P\left( \left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge \left|a_i(l)+a_j(l)\right|^{-1}b'/3\right)
\le
\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge \sqrt{b'/3}\right)
+
2\P\left(\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge \sqrt{b'/3}\right)
+\nonumber
\\
&\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{b'/3}\right)
+
\P\left(\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge \left|a_i(l)+a_j(l)\right|^{-1}\sqrt{b'/3}\right)
\le
\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge \sqrt{b'/3}\right)
+\nonumber
\\
&\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{b'/3}\right)
+
3\P\left(\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge c_{13}\sqrt{b'}\right).
\end{align}
For the final term above,
\begin{align}\label{t3t3}
&\P\left(\left|\mathbb{Z}^i(l)^2-b_i(l)^2\right|\ge t\right)
\le
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|+2|b_i(l)|\right)\ge t\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|^2\ge t/2\right)
+
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge |b_i(l)|^{-1}t/4\right)
\le
2\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{14}\left(\sqrt{t/2}\wedge t\right)\right).
\end{align}
By plugging in \eqref{t3t3} into \eqref{nqt3},
\begin{align}\label{nqt3finalform}
&\P\left(\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)\right|\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|\ge b'\right)
\le
\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge \sqrt{b'/3}\right)
+
\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{b'/3}\right)
+\nonumber
\\
&6\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{15}\left(\sqrt{b'}\wedge b'^{\frac 14}\right) \right).
\end{align}
The exceedance probability for the final term of \eqref{nqexpr} can be bounded as follows
\begin{align}\label{nqlastterm}
&\P\left(\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge t\right)
\le
\P\left(\left(\left|\mathbb{Z}^i(l)\right|\left|\mathbb{G}^i(l)-a_i(l)\right|+\left|a_i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|+|b_i(l)|\right)\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge t\right)
\le\nonumber
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|^2\ge t/4\right)
+
\P\left(\left|b_i(l)\right|\left|\mathbb{G}^i(l)-a_i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge t/4\right)
+\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left|a_i(l)\right|^{-1}t/4\right)
+
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left|b_i(l)\right|^{-1}t/4\right)
\le
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge \sqrt{t}/2\right)
+\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left(t/4\right)^{\frac 14}\right)
+
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge \sqrt{\left|b_i(l)\right|^{-1}t}/2\right)
+
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \sqrt{\left|b_i(l)\right|^{-1}t}/2\right)
+\nonumber
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \sqrt{\left|a_i(l)\right|^{-1}t}/2\right)
+
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left|b_i(l)\right|^{-1}t/4\right)
\le\nonumber
\\
&2\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{16}\sqrt{t}\right)
+
3\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{17}\left(t\wedge t^{\frac 14}\wedge\sqrt{t}\right)\right)
\end{align}
We now use \eqref{nqlastterm}, \eqref{nqt3finalform} and \eqref{nqexpr} to compute, 
\begin{align*}
&\P\left(\left|\hat{n}_q(\textbf{n})-n_q\right|\ge b'\right)
\le
\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge b'/8\right)
+\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge b'\ge b'/8\right)
+\P\left(\left|\mathbb{G}^i(l)-\mathbb{G}^j(l)\right|\left|\mathbb{Z}^i(l)^2-
b_i(l)^2\right|\ge b'/8\right)
+
\\
&\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge b_i(l)^{-2} b'/8\right)
+
\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge b_i(l)^{-2} b'/8\right)
+
\P\left(\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)\right|\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge b'/8\right)
+
\\
&\P\left(\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)\right|\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge b'/8\right)
+
\P\left(\left|\mathbb{Z}^i(l)\mathbb{G}^i(l)
-
b_i(l)a_i(l)\right|\ge \left|b_i(l)-b_j(l)\right|^{-1}b'/8\right)
\le
\\
&\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge b'/8\right)
+\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge b'/8\right)
+
\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge \sqrt{b'/24}\right)
+
\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{b'/24}\right)
+
\\
&6\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge f_i\left(\sqrt{b'/24}\left(\left|a_i(l)+a_j(l)\right|^{-1}\wedge 1\right)\right)\right)
+
\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge b_i(l)^{-2} b'/8\right)
+
\\
&\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge b_i(l)^{-2} b'/8\right)
+
2\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge \left(\sqrt{\left|b_i(l)\right|^{-1}b'/8}/2\right)\wedge\left(\sqrt{b'/8}/2\right)\right)
+
\\
&3\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \left(\left|a_i(l)\right|^{-1}b'/32\right)\wedge \left(b'/32\right)^{\frac 14}\wedge\left( \sqrt{\left|b_i(l)\right|^{-1}b'}/16\right)\right)
\le
\\
&\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge b'/8\right)
+
\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge b'/8\right)
+
4\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge c_{18}\left(\sqrt{b'}\wedge b'\right)\right)
+
\\
&2\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{b'/24}\wedge\left( b_i(l)^{-2} b'/8\right)\right)
+
9\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{19}\left(\sqrt{b'} \wedge b'^{\frac 14}\wedge b'\right)\right).
\end{align*}

Let $E_\textbf{n}=\left\{\left|\hat{d}(\textbf{n})- d\right|<\delta\right\}$ then by using the elementary inequality $\left|\frac{1}{x}-\frac{1}{y}\right|\le \frac{\left|x-y\right|}{x^2(1-\delta)}$, valid when $\left|x-y\right|<\delta$ and $0<\delta<1$ we have
\begin{align}
&\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge  b'\right)
\le
\P\left( \left\{\left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge  b'\right\}\cap E_\textbf{n}\right)
+
\P\left(E_\textbf{n}^c\right)
\le\nonumber
\\
&\P\left( \frac{1}{d^2(1-\delta)}\left|\hat{d}(\textbf{n})- d\right|\ge  b'\right)
+
\P\left(E_\textbf{n}^c\right)
=
\P\left( \left|\hat{d}(\textbf{n})- d\right|\ge  d^2(1-\delta)b'\right)
+
\P\left( \left|\hat{d}(\textbf{n})- d\right|\ge  \delta\right)
\le\nonumber
\\
&2\P\left( \left|\hat{d}(\textbf{n})- d\right|\ge  \left(d^2(1-\delta)b'\right) \wedge\delta\right)
\le
2\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{20}\left(\delta\wedge b'\right)\right)
+
2\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{21}\left(\delta\wedge b'\right)\right)
+\nonumber
\\
&6\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{22}\left(\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right)\right)
+
6\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{23}\left(\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right)\right).
\end{align}
With the new notation introduced at the start of the proof, 
\begin{align*}
&\mu=
\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d},
\end{align*}
which is non-zero by assumption. Moreover since we assume that there is an $\arg\min$ solution then $\lambda\in\R$ so we must also have that $\mu>0$. Let $C_\textbf{n}=\left\{\left|\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}-\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right) \right|<\mu/2\right\}$ then $\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}>0$ on $C_\textbf{n}$.
\begin{align}
&\P\left(C_\textbf{n}^c\right)
\le
\P\left(\left|\left(\frac{n_p}{d}\right)^2-\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2\right|<\mu/4\right)+\P\left(\left|\frac{n_q}{d}-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right|<\mu/4\right).
\end{align}
To tackle the above terms we first compute,
\begin{align}\label{ndfracdiffp}
&\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b' \right)
\le
\P\left( \left|\hat{n}_p(\textbf{n})\right|\left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge b'/2 \right)
+
\P\left( \left|\frac{1}{d}\right|\left|\hat{n}_p(\textbf{n})-n_p\right|\ge b'/2 \right)
\le\nonumber
\\
&\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge b'/4 \right)
+
\P\left( \left|n_p\right|\left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge b'/4 \right)
+
\P\left( \left|\frac{1}{d}\right|\left|\hat{n}_p(\textbf{n})-n_p\right|\ge b'/2 \right)
\le\nonumber
\\
&\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge \sqrt{b'}/2 \right)
+
\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \sqrt{b'}/2 \right)
+
\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \left|n_p\right|^{-1} b'/4\right)
+\nonumber
\\
&\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge \left|d\right|b'/2 \right)
\le
2\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge c_{24}\left(\sqrt{b'}\wedge b' \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge c_{25}\left(\sqrt{b'}\wedge b' \right)\right).
\end{align}
Analogously
\begin{align}\label{ndfracdiffq}
&\P\left( \left|\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_q}{d}\right|\ge b' \right)
\le
2\P\left( \left|\hat{n}_q(\textbf{n})-n_q\right|\ge c_{26}\left(\sqrt{b'}\wedge b' \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge c_{27}\left(\sqrt{b'}\wedge b' \right)\right).
\end{align}
Using \eqref{ndfracdiffp},
\begin{align*}
&\P\left( \left|\left(\frac{n_p}{d}\right)^2-\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2\right|\ge b' \right)
\le
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|^2\ge b'/2 \right)
+
\P\left( 2\left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\left|\frac{n_p}{d}\right|\ge b'/2 \right)
\le
\\
&\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge \sqrt{b'/2} \right)
+
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge \left|\frac{n_p}{d}\right|^{-1}b'/4 \right)
\le
\\
&2\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge c_{28}\left(\sqrt{b'}\wedge b' \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge c_{29}\left(\sqrt{b'}\wedge b' \right)\right)
\end{align*}
Due to the inequality $|\sqrt{x}-\sqrt{y}|\le\sqrt{|x-y|}$ (valid when $x,y\ge 0$)
\begin{align}\label{mainbnd}
&\P\left(\left\{ \left| \lambda_+-\hat{\lambda}_+(\textbf{n})\right|\ge b\right\}\cap C_\textbf{n}\right)
\le 
\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b/2 \right)
+\nonumber
\\
&\P\left( \left\{\left| \sqrt{\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}}
-
\sqrt{\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})} }\right|\ge b/2 \right\}\cap C_\textbf{n}\right)
\le\nonumber
\\
&\P\left( \sqrt{\left|\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}-\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)\right|}\ge b/2 \right)
+
\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b/2 \right)\nonumber
\le
\\
&\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b/2 \right)
+
\P\left( \left|\left(\frac{n_p}{d}\right)^2-\frac{n_q}{d}-\left(\left(\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right)^2-\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}\right)\right|\ge b^2/4 \right)
\le\nonumber
\\
&\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b/2 \right)
+
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\left|\frac{n_p}{d}+\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge b^2/8 \right)
+
\P\left( \left|\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_q}{d}\right|\ge b/8 \right).
\end{align}
We can now utilize \eqref{ndfracdiffp} to tackle the second term on the right-most side above
\begin{align*}
&\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\left|\frac{n_p}{d}+\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge b^2/8 \right)
\le
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|^2\ge b^2/16 \right)
+
\P\left( 2\left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\left|\frac{n_p}{d}\right|\ge b^2/16 \right)
\le
\\
&\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge b/4 \right)
+
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge \left|\frac{n_p}{d}\right|^{-1}b^2/32 \right)
\le
\\
&2\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge c_{30}\left(\sqrt{b}\wedge b \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge c_{31}\left(\sqrt{b}\wedge b\right)\right)
\end{align*}
%for the last term in \eqref{mainbnd},
%\begin{align*}
%&\P\left( \left|\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_q}{d}\right|\ge b/8 \right)
%\le
%\P\left( \left|\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_q}{d}\right|\ge b/8 \right)
%\end{align*}

Plugging in the above bound as well as \eqref{ndfracdiffq} into \eqref{mainbnd} yields,
\begin{align}\label{mejn}
&\P\left(\left\{ \left| \lambda_+-\hat{\lambda}_+(\textbf{n})\right|\ge b\right\}\cap C_\textbf{n}\right)
\le\nonumber
\\
&\P\left( \left|\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_p}{d}\right|\ge b/2 \right)
+
\P\left( \left|\frac{n_p}{d}-\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\left|\frac{n_p}{d}+\frac{\hat{n}_p(\textbf{n})}{\hat{d}(\textbf{n})}\right|\ge b^2/8 \right)
+
\P\left( \left|\frac{\hat{n}_q(\textbf{n})}{\hat{d}(\textbf{n})}- \frac{n_q}{d}\right|\ge b/8 \right)
\le\nonumber
\\
&2\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge \left(\sqrt{b/2}/2\right)\wedge\left(\left|d\right|b/4 \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \left(\left|n_p\right|^{-1} b/8\right)\wedge\left(\sqrt{b'}/4\right)\right)
+\nonumber
\\
&2\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge \left(\sqrt{b}/4\right)\wedge\left(\left|d\right|b/8 \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \left(\left|n_p\right|^{-1} b/16\right)\wedge\left(\sqrt{b'}/8\right)\right)
+\nonumber
\\
&2\P\left( \left|\hat{n}_q(\textbf{n})-n_q\right|\ge \left(\sqrt{b/8}/2\right)\wedge\left(\left|d\right|b/16 \right)  \right)
+
2\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \left(\left|n_q\right|^{-1} b/32\right)\wedge\left(\sqrt{b'}/16\right)\right)
\le\nonumber
\\
&4\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge c_{31}\left(\sqrt{b}\wedge b  \right)\right)
+
2\P\left( \left|\hat{n}_q(\textbf{n})-n_q\right|\ge c_{32}\left(\sqrt{b}\wedge b  \right)\right)
+
6\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge c_{33}\left(\sqrt{b}\wedge b\right)\right)
\end{align}
Let $v(b)=(c_{31}\wedge c_{32}\wedge c_{33})\left(\sqrt{b}\wedge b  \right)$ so that
\begin{align}\label{mejn2}
&\P\left(\left\{ \left| \lambda_+-\hat{\lambda}_+(\textbf{n})\right|\ge b\right\}\cap C_\textbf{n}\right)
\le
4\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge v(b)  \right)
+
2\P\left( \left|\hat{n}_q(\textbf{n})-n_q\right|\ge v(b)  \right)
+
6\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge v(b)\right)
\le\nonumber
\\
&4\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \ge \sqrt{v(b)/6}\right)
+
8\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge \frac 12\left(\left(v(b)/6\right)^{\frac 14}\wedge \left(\left|b_i(l)-b_j(l)\right|^{-1}|a_i(l)|^{-1}v(b)/6\right)\right)\right)
+\nonumber
\\
&8\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge \frac 12\left(\left(v(b)/6\right)^{\frac 14}\wedge \left(\left|b_i(l)-b_j(l)\right|^{-1}|a_i(l)|^{-1}v(b)/6\right)\right)\right)
+
2\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge v(b)/8\right)
+\nonumber
\\
&2\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge v(b)/8\right)
+
4\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge \sqrt{v(b)/24}\wedge\left( b_i(l)^{-2} v(b)/8\right)\right)
+\nonumber
\\
&8\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge \left(\sqrt{v(b)/24}\right)\wedge\left(b_i(l)^{-2} v(b)/8\right)\wedge \left(\sqrt{\left|b_i(l)\right|^{-1}b'/8}/2\right)\wedge\left(\sqrt{v(b)/8}/2\right)\right)
+\nonumber
\\
&18\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{34}\left(\sqrt{b'}\wedge b\wedge b^{\frac 14}\right)\right)
+
6\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{34}v(b)\right)
+
6\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{34}v(b)\right)
+\nonumber
\\
&18\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge  c_{35}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right) \right)
+
18\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{36}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
\le\nonumber
\\
&18\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{37}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+
44\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{38}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+\nonumber
\\
&10\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{39}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\right)\right)
+
34\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{40}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+\nonumber
\\
&2\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge c_{41}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+
2\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge c_{41}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right).
\end{align}
Next,
\begin{align}\label{Cnb}
&\P\left(C_\textbf{n}^c \right)
\le
4\P\left( \left|\hat{n}_p(\textbf{n})-n_p\right|\ge c_{30}\left(\sqrt{\mu}\wedge \mu \right)/4  \right)
+
4\P\left( \left|\hat{n}_q(\textbf{n})-n_q\right|\ge c_{26}\left(\sqrt{\mu}\wedge \mu \right)  \right)
+\nonumber
\\
&8\P\left( \left|\frac{1}{\hat{d}(\textbf{n})}- \frac{1}{d}\right|\ge \left(c_{27}\wedge c_{31}\right)\left(\sqrt{\mu}\wedge \mu \right)/4\right)
\le\nonumber
\\
&4\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right| \ge r_1\left(\sqrt{\mu}\wedge \mu^{\frac 14}\right)\right)
+\nonumber
\\
&8\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_2\left(\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
8\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge r_3\left(\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&4\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_4\left(\sqrt{\mu}\wedge\mu\right)\right)
+
4\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_5\left(\sqrt{\mu}\wedge\mu\right)\right)
+
16\P\left(\left|\mathbb{G}^i(l) -a_i(l)\right|\ge r_6\left(\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu\right)\right)
+\nonumber
\\
&8\P\left(\left|\mathbb{G}^j(l) -a_j(l)\right|\ge r_7\left(\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu\right)\right)
+
36\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge r_8\left(\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&16\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{20}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
16\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{21}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+\nonumber
\\
&96\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{22}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
96\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{23}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
\le\nonumber
\\
&140\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_9\left(\delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
104\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{10}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&32\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{11}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
32\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{12}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+\nonumber
\\
&4\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{13}\left(\sqrt{\mu}\wedge\mu\right)\right)
+
4\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{14}\left(\sqrt{\mu}\wedge\mu\right)\right).
\end{align}
Combining \eqref{mejn2} and \eqref{Cnb} yields
\begin{align}\label{lambdabd}
&\P\left( \left| \lambda_+-\hat{\lambda}_+(\textbf{n})\right|\ge b\right)
\le\nonumber
\\
&18\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{37}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+
44\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{38}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+\nonumber
\\
&10\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{39}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\right)\right)
+
34\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{40}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+\nonumber
\\
&2\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge c_{41}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+
2\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge c_{41}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\right)\right)
+\nonumber
\\
&140\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_9\left(\delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+
104\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{10}\left(\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&32\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{11}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
32\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{12}\left(\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
+
4\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{12}\left(\sqrt{\mu}\wedge\mu\right)\right)
+\nonumber
\\
&4\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{13}\left(\sqrt{\mu}\wedge\mu\right)\right)
\le\nonumber
\\
&50\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{14}\left(\delta\wedge b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right)\right)
+
42\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge r_{15}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
\nonumber
\\
&184\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_{16}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\wedge \delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&138\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{17}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\wedge\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&6\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{18}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
+
6\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{19}\left(b\wedge\sqrt{b}\wedge b^{\frac 14}\wedge b^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
\end{align}
The same exact bound will also be valid for $\P\left( \left| \lambda_--\hat{\lambda}_-(\textbf{n})\right|\ge b\right)$. 
Next,
\begin{align*}
&\left|\beta_\gamma-\hat{\beta}_\lambda(\textbf{n})\right|
=
\left|\frac{b_i(l)-\lambda(b_i(l)-b_j(l))}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|
\le
\\
&\left|a_i(l)-\lambda(a_i(l)-a_j(l))\right|^{-1}\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|
+
\\
&\left|\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right|\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|
\le
\\
&\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|
\left(\left|a_i(l)-\lambda(a_i(l)-a_j(l))\right|^{-1}+ 
\right.
\\
&\left.\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\right)
+
\\
&\left|b_i(l)-\lambda(b_i(l)-b_j(l))\right|\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|
\end{align*}
We now begin to tackle the exceedance probability for $\beta_\gamma-\hat{\beta}_\gamma(\textbf{n})$,
\begin{align}\label{beta_first}
&\P\left(\left|\beta_\gamma-\hat{\beta}_\gamma(\textbf{n})\right|\ge b'\right)
\le
\P\left(\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|\ge \left|a_i(l)-\lambda(a_i(l)-a_j(l))\right| b'/3 \right)
+\nonumber
\\
&\P\left(\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge b'/3\right)
+\nonumber
\\
&\P\left(\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge\left|b_i(l)-\lambda(b_i(l)-b_j(l))\right|b'/3\right)
\le\nonumber
\\
&2\P\left(\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|\ge \left(\left|a_i(l)-\lambda(a_i(l)-a_j(l))\right| b'/3\right)\wedge\sqrt{b'/3} \right)
+\nonumber
\\
&2\P\left(\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge\left|b_i(l)-\lambda(b_i(l)-b_j(l))\right|b'/3\right)
\end{align}
We shall employ the same trick as before to tackle the second term above. For this purpose consider the set
$$E''_\textbf{n}=\left\{\left|a_i(l)-\lambda(a_i(l)-a_j(l))-\left(\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) \right)\right|<\delta\right\}. $$
We can now dissect the fractional term,
\begin{align*}
&\P\left(\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge t\right)
\le
\\
&\P\left(\left\{\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge t\right\}\cap E''_\textbf{n}\right)
+
\P\left(E''^c_\textbf{n}\right)
\le
\\
&\P\left(\left|a_i(l)-\lambda(a_i(l)-a_j(l)-\left(\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) \right)\right|\ge \left(a_i(l)-\lambda(a_i(l)-a_j(l))\right)^2(1-\delta)t\right)
+
\P\left(E''^c_\textbf{n}\right)
\end{align*}
Letting $\tilde{c}(t)=\left(a_i(l)-\lambda(a_i(l)-a_j(l))\right)^2(1-\delta)t$,
\begin{align*}
&\P\left(\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge t\right)
\le
\\
&3\P\left(\left|a_i(l)-\mathbb{G}^i(l)\right|\ge \left(\tilde{c}/4\right)\wedge\left(\left|\lambda\right|^{-1}\tilde{c}/4\right)\wedge\left(\sqrt{\tilde{c}(t)}/2\right) \right)
+
2\P\left(\left|a_j(l)-\mathbb{G}^j(l)\right|\ge \left(\left|\lambda\right|^{-1}\tilde{c}(t)/4\right)\wedge\left(\sqrt{\tilde{c}(t)}/2\right) \right)
+
\\
&2\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  \sqrt{\tilde{c}(t)/7}\right)
+
\P\left(E''^c_\textbf{n}\right).
\end{align*}
Furthermore,
\begin{align*}
&\P\left(E''^c_\textbf{n}\right)
\le
3\P\left(\left|a_i(l)-\mathbb{G}^i(l)\right|\ge \left(\delta/4\right)\wedge\left(\left|\lambda\right|^{-1}\delta/4\right)\wedge\left(\sqrt{\delta}/2\right) \right)
+
2\P\left(\left|a_j(l)-\mathbb{G}^j(l)\right|\ge \left(\left|\lambda\right|^{-1}\delta/4\right)\wedge\left(\sqrt{\delta}/2\right) \right)
+
\\
&2\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  \sqrt{\delta/7}\right)
\end{align*}
Therefore we have the following bound for the last term in \eqref{beta_first},
\begin{align*}
&\P\left(\left|\frac{1}{a_i(l)-\lambda(a_i(l)-a_j(l)) }-\frac{1}{\mathbb{G}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{G}^i(l)-\mathbb{G}^j(l)) }\right|\ge t\right)
\le
6\P\left(\left|a_i(l)-\mathbb{G}^i(l)\right|\ge c_{42}\left(\delta\wedge\sqrt{\delta}\wedge t\wedge\sqrt{t}\right) \right)
+
\\
&4\P\left(\left|a_j(l)-\mathbb{G}^j(l)\right|\ge c_{43}\left(\delta\wedge\sqrt{\delta}\wedge t\wedge\sqrt{t}\right) \right)
+
4\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  c_{44}\left(\sqrt{\delta}\wedge\sqrt{t}\right)\right)
\end{align*}

For the first term on the right-most side of \eqref{beta_first},
\begin{align*}
&\P\left(\left|b_i(l)-\lambda(b_i(l)-b_j(l))-\left(\mathbb{Z}^i(l)-\hat{\lambda}_\textbf{n}(\mathbb{Z}^i(l)-\mathbb{Z}^j(l))\right)\right|\ge t \right)
\le
\P\left(\left|b_i(l)-\mathbb{Z}^i(l)\right|\ge t/7 \right)
+
\P\left(\left| b_i(l)-\mathbb{Z}^i(l)\right|\ge \left|\lambda\right|^{-1}t/7 \right)
+
\\
&\P\left(\left| b_j(l)-\mathbb{Z}^j(l)\right|\ge \left|\lambda\right|^{-1}t/7 \right)
+
\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge t/7 \right)
+
\\
&\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge \left|b_i(l)\right|^{-1}t/7 \right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge t/7 \right)
+
\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge \left|b_j(l)\right|^{-1}t/7 \right)
\le
\\
&\P\left(\left|b_i(l)-\mathbb{Z}^i(l)\right|\ge t/7 \right)
+
\P\left(\left| b_i(l)-\mathbb{Z}^i(l)\right|\ge \left|\lambda\right|^{-1}t/7 \right)
+
\P\left(\left| b_j(l)-\mathbb{Z}^j(l)\right|\ge \left|\lambda\right|^{-1}t/7 \right)
+
\\
&\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge \sqrt{t/7}\right)
+
\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  \sqrt{t/7}\right)
+
\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge \sqrt{t/7}\right)
+
\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  \sqrt{t/7}\right)
\le
\\
&3\P\left(\left|b_i(l)-\mathbb{Z}^i(l)\right|\ge c_{45}\left(t\wedge\sqrt{t}\right) \right)
+
2\P\left(\left|b_j(l)-\mathbb{Z}^j(l)\right|\ge c_{46}\left(t\wedge\sqrt{t} \right)\right)
+
2\P\left(\left|\lambda-\hat{\lambda}_\textbf{n}\right|\ge  \sqrt{t/7}\right)
\end{align*}
Returning to \eqref{beta_first},
\begin{align*}
&\P\left(\left|\beta_\gamma^+-\hat{\beta}_\lambda^+(\textbf{n})\right|\ge b'\right)
\le
6\P\left(\left|b_i(l)-\mathbb{Z}^i(l)\right|\ge c_{47}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right) \right)
+
4\P\left(\left|b_j(l)-\mathbb{Z}^j(l)\right|\ge c_{48}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right) \right)
+
\\
&12\P\left(\left|a_i(l)-\mathbb{G}^i(l)\right|\ge c_{49}\left(b'\wedge\sqrt{b'}\wedge \delta\wedge\sqrt{\delta}\right) \right)
+
8\P\left(\left|a_j(l)-\mathbb{G}^j(l)\right|\ge c_{50}\left(b'\wedge\sqrt{b'}\wedge \delta\wedge\sqrt{\delta}\right) \right)
+
\\
&12\P\left(\left|\lambda^+-\hat{\lambda}_\textbf{n}^+\right|\ge  c_{51}\left(\sqrt{b'}\wedge b'^{\frac 14}\wedge\sqrt{\delta}\right)\right)
\end{align*}
Let's wrap things up. The final estimate for the exceedance probability of $\beta_\gamma-\hat{\beta}_\lambda(\textbf{n})$ is
Let $c_b=\tilde{c}(\left|b_i(l)-\lambda(b_i(l)-b_j(l))\right|b'/3)$, $c_a=\left(\left|a_i(l)-\lambda(a_i(l)-a_j(l))\right| b'/3\right)\wedge\sqrt{b'/3}$
\begin{align}\label{betafinal}
&\P\left(\left|\beta_\gamma^+(l)-\hat{\beta}_\lambda^+(\textbf{n},l)\right|\ge b'\right)
\le\nonumber
\\
&6\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{47}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right) \right)
+
4\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{48}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\right) \right)
+\nonumber
\\
&12\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{49}\left(b'\wedge\sqrt{b'}\wedge \delta\wedge\sqrt{\delta}\right) \right)
+
8\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{50}\left(b'\wedge\sqrt{b'}\wedge \delta\wedge\sqrt{\delta}\right) \right)
+\nonumber
\\
&12\cdot 50\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge r_{14}\left(\delta\wedge b'\wedge\sqrt{b}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right)\right)
+\nonumber
\\
&12\cdot 42\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge r_{15}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge\delta\wedge \mu\wedge\sqrt{\mu}\right)\right)
\nonumber
\\
&12\cdot 184\P\left( \left|(\mathbb{Z}^i(l)-b_i(l)\right|\ge r_{16}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge \delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&12\cdot 138\P\left( \left|(\mathbb{Z}^j(l)-b_j(l)\right|\ge r_{17}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right)\right)
+\nonumber
\\
&12\cdot 6\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{18}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
+
12\cdot 6\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{19}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
\le\nonumber
\\
&2214\P\left(\left|\mathbb{Z}^i(l)-b_i(l)\right|\ge c_{52}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge \delta\wedge\mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right) \right)
+\nonumber
\\
&1660\P\left(\left|\mathbb{Z}^j(l)-b_j(l)\right|\ge c_{53}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\delta\wedge \mu\wedge\sqrt{\mu}\wedge \mu^{\frac 14}\wedge\mu^{\frac 18}\right) \right)
+\nonumber
\\
&612\P\left(\left|\mathbb{G}^i(l)-a_i(l)\right|\ge c_{54}\left(\sqrt{\delta}\wedge\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) \right)
+\nonumber
\\
&512\P\left(\left|\mathbb{G}^j(l)-a_j(l)\right|\ge c_{55}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge\delta\wedge\sqrt{\delta}\wedge \mu\wedge\sqrt{\mu}\right)\right)
+\nonumber
\\
&72\P\left(\left|\mathbb{G}^i_Y-c_i\right|\ge r_{18}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
+
72\P\left(\left|\mathbb{G}^j_Y-c_j\right|\ge r_{19}\left(b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\sqrt{\mu}\wedge\mu\right)\right)
\end{align}
For each $i\in \{1,...,k\}$ and $0\le l\le p$, define for $u=1,...,n_{A_i}$
$$W^i_u(l)=\frac{1}{n_{A_i}}\left(X^{A_i}_{u}(l)\right)^2-\frac{1}{n_{A_i}}\E\left[(X^{A_i}(l))^2\right],$$
Then $\{W^i_u\}_{u=1}^{n_{A_i}}$ are independent, $\E\left[W^i_u(l)\right]=0$ and 
$\sum_{l=1}^{n_{A_i}}W_u^i(l)=\mathbb{G}^i(l)- a^i(l).$ 
If we now also define $W^{i,R}_{u}(l)=W^i_u(l)1_{(X_u^{A_i}(l))^2\le R}$ and 
$$E^i_R(l)=\bigcap_{u=1}^{n_{A_i}}\left\{W^{i,R}_{u}(l)=W^i_u(l),\forall u,v\in \{1,...,p\}\right\}$$
then $E^i_R(l)=\bigcap_{u=1}^{n_{A_i}}\left\{|X^{A_i}_u(l)|\le R,\right\}$ and $\P\left(E_R^i(l)\right)=F_{X^{A_i}(l)}(R)^{n_{A_i}}$. We also define the set $E_i'(l)=\left\{|X^{A_i}(l)|\le R\right\}$.
On the set $E_{R}^i(l)$,
\begin{align*}
&\left| \mathbb{G}^i-a_i(l) \right| =
\left| \sum_{u=1}^{n_{A_i}} W^{i,R}_u(l)\right|+
\E\left[(X^{A_i})^21_{(E_i')^c}\right]
\end{align*}
It follows from the Hffding inequality that
\begin{align*}
&\P\left( \left| \sum_{l=1}^{n_{A_i}} W^{i,R}_u(l)\right| \ge t\right)\le 2e^{-\frac{2t^2}{R^4}n_{A_i}} ,
\end{align*}
Let $h_1^{i,l}(R)=\E\left[(X^{A_i}(l))^21_{(E_i'(l))^c}\right]$. Using the above notation and letting $R=n_{A_i}^\alpha$ ($0<\alpha<1/4$),
\begin{align*}
\P\left( \left| \mathbb{G}^i -a_i(l) \right|\ge c' \right)
\le 
\P\left( \left\{\left| \mathbb{G}^i -a_i(l) \right|\ge c'\right\}\cap E^i_R(l) \right)+\P\left((E^i_R(l))^c\right)
&\le
\P\left( \left| \sum_{u=1}^{n_{A_i}} W^{i,R}_u(l)\right|
\ge c' -h_1^i(n_{A_i}^\alpha) \right)+\P\left((E^i_R(l))^c\right)
\\ 
&\le 2e^{-2\left(\left(c' -h_1^i(n_{A_i}^\alpha)\right)_+\right)^2n_{A_i}^{1-4\alpha}}+1-F_{X^{A_i}(l)}(n_{A_i}^\alpha)^{n_{A_i}}.
\end{align*}
Analogous computations yields
\begin{align*}
&\P\left( \left| \mathbb{G}_Y^i -c_i \right|\ge t \right)
\le 
2e^{-2\left(\left(t -h_2^i(n_{A_i}^\alpha)\right)_+\right)^2n_{A_i}^{1-4\alpha}}+1-F_{Y^{A_i}}(n_{A_i}^\alpha)^{n_{A_i}},
\end{align*}
with $h_2(R)=\E\left[(Y^{A_i})^21_{\left\{|Y^{A_i}|\le R\right\}^c}\right]$ and
\begin{align*}
&\P\left( \left| \mathbb{Z}^i(l) -b_i(l) \right|\ge t \right)
\le 
2e^{-2\left(\left(t -h_3^{i,l}(n_{A_i}^\alpha)\right)_+\right)^2n_{A_i}^{1-4\alpha}}+1-F_{\left|X^{A_i}(l)\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,n_{A_i}^\alpha)^{n_{A_i}},
\end{align*}
with $h^{i,l}_3(R)=\E\left[|Y^{A_i}||X^{A_i}(l)|1_{\left\{|Y^{A_i}|\le R,|X^{A_i}(l)|\le R\right\}^c}\right]$. We now apply this to this to \eqref{betafinal} and the result follows.
%$M e^{-c\left(\left(\sqrt{\delta}\wedge\delta\wedge b'\wedge\sqrt{b'}\wedge b'^{\frac 14}\wedge b'^{\frac 18}\wedge\mu\wedge\sqrt{\mu}\right) -h_1^{i,l}(n_{A_i}^\alpha)\wedge h_2^{i}(n_{A_i}^\alpha)\wedge h_3^{i,l}(n_{A_i}^\alpha)\right)_+^2n_{A_i}^{1-4\alpha}}
%+
%1-F_{\left|X^{A_i}(l)\right|,\left|Y^{A_i}\right|}(n_{A_i}^\alpha,n_{A_i}^\alpha)^{n_{A_i}}$
\end{proof}
\end{document}