\documentclass[letterpaper]{article}
\usepackage{fullpage}
\usepackage{ktmacros}
\usepackage[backend=bibtex,maxnames=30,maxbibnames=30,maxcitenames=30,maxalphanames=30,giveninits=true,doi=false,url=true]{biblatex}
\newcommand*{\citet}[1]{\AtNextCite{\AtEachCitekey{\defcounter{maxnames}{2}}} \textcite{#1}}
\newcommand*{\citetall}[1]{\AtNextCite{\AtEachCitekey{\defcounter{maxnames}{999}}} \textcite{#1}}
\newcommand*{\citep}[1]{\cite{#1}}
\newcommand{\citeyearpar}[1]{\cite{#1}}

\bibliography{refs}

\newcommand{\agg}{\ensuremath{\mathcal{A}}}
\newcommand{\minbatch}{\ensuremath{B}}
\newcommand{\Dec}{\ensuremath{Dec}}
\newcommand{\sa}{\ensuremath{\mathcal{SA}}}
\newcommand{\SAA}{\ensuremath{\textsc{SA}_2}}
\newcommand{\gf}{\mathbb{F}}
\newcommand{\rappork}{{\sc Rappor}$_K$}


\newif\ifbiblatex
\biblatextrue

%%% Relevant Description from acmart.cls
\newcommand{\Description}[1]{}


\title{Samplable Anonymous Aggregation for Private Federated Data Analysis}
\author{Kunal Talwar\thanks{Apple.} \footnote{Corresponding Author. \texttt{ktalwar@apple.com}}
\and Shan Wang\footnotemark[1]
\and Audra McMillan\footnotemark[1]
\and Vojta Jina\thanks{Research performed while at Apple.}
\and Vitaly Feldman\footnotemark[1]
\and	Pansy Bansal\footnotemark[1]
\and	Bailey Basile\footnotemark[1]
\and	Aine Cahill\footnotemark[1]
\and	Yi Sheng Chan\footnotemark[1]
\and	Mike Chatzidakis\footnotemark[1]
\and	Junye Chen\footnotemark[1]
\and	Oliver Chick\footnotemark[1]
\and	Mona Chitnis\footnotemark[1]
\and	Suman Ganta\footnotemark[1]
\and	Yusuf Goren\footnotemark[1]
\and	Filip Granqvist\footnotemark[1]
\and	Kristine Guo\footnotemark[1]
\and	Frederic Jacobs\footnotemark[1]
\and	Omid Javidbakht\footnotemark[1]
\and	Albert Liu\footnotemark[1]
\and	Richard Low\footnotemark[1]
\and	Dan Mascenik\footnotemark[1]
\and	Steve Myers\footnotemark[1]
\and	David Park\footnotemark[1]
\and	Wonhee Park\footnotemark[1]
\and	Gianni Parsa\footnotemark[1]
\and	Tommy Pauly\footnotemark[1]
\and	Christian Priebe\footnotemark[1]
\and	Rehan Rishi\footnotemark[1]
\and	Guy N. Rothblum\footnotemark[1]
\and	Michael Scaria\footnotemark[3]
\and	Linmao Song\footnotemark[1]
\and	Congzheng Song\footnotemark[1]
\and	Karl Tarbe\footnotemark[1]
\and	Sebastian Vogt\footnotemark[1]
\and	Luke Winstrom\footnotemark[1]
\and	Shundong Zhou\footnotemark[1]
}
%\date{September 2022}

\begin{document}
\maketitle

\begin{abstract}
  We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Locally differentially private algorithms require little trust but are (provably) limited in their utility. Centrally differentially private algorithms can allow significantly better utility but require a trusted curator. This gap has led to significant interest in the design and implementation of simple cryptographic primitives, that can allow central-like utility guarantees without having to trust a central server.

  Our first contribution is to propose a new primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails.  {\em Shuffling} and {\em aggregation} primitives that have been proposed in earlier works enable this for some algorithms, but have significant limitations as primitives.
  We propose a {\em Samplable Anonymous Aggregation} primitive, which computes an aggregate over a random subset of the inputs and show that it leads to better privacy-utility trade-offs for various fundamental tasks.
  Secondly, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system. Our design combines additive secret-sharing  with anonymization and authentication infrastructures.
 \end{abstract}

 \section{Introduction}
 Learning aggregate population trends can allow for better data-driven decisions, and machine learning can improve user experience. Compared to learning from public curated datasets, learning from the user population offers several benefits. As an example, a next-word prediction model trained on words typed by users (a) can better fit the actual distribution of language used on devices, (b) can adapt faster to shifts in distribution, and (c) can more faithfully represent smaller sub-populations that may not be well-represented in curated datasets. At the same time, training such models may involve sensitive user data. This tension has led to increasing interest in cross-device federated learning  and analytics\footnote{Federated data analytics (such as collecting statistics and telemetry), and federated learning (such as training ML models) are the two most common forms of federated data analyses. While everything we say in this work applies to both kinds of analyses, for brevity we will often use federated learning when we mean federated learning and federated analytics.}, and such systems have been deployed at scale~\cite{ErlingssonPK14, Apple2017, DingKY17, ENPA:2021, ZhangRXZZK23}.

 In federated learning, the data of a user stays on their device, and only model updates are sent to a server. This reduction in the data sent to the servers may however not be sufficient to ensure user privacy.
 A long line of work~\cite{ShokriSSS17,NasrSH19, CarliniLEKS19, Feldman20, FeldmanZ20, CTWJ+21, CIJL+23}, has shown that trained models can memorize training data. Differential privacy~\cite{Dwork:2006, DworkR14} can address the privacy concerns by provably preventing memorization of data that is unique to one or a few users. Private gradient-based learning algorithms add sufficient noise to gradients (aggregated over a batch) to mask individual contributions. In the setting of a trusted curator, one can use centrally differentially private algorithms for data analysis (e.g. ~\cite{DworkR14,Kasiviswanathan:2008}) and machine learning (e.g. ~\cite{ChaudhuriMS11,BassilyST14,DLDP}). It is natural to aim to combine differential privacy with federated learning, and indeed private federated learning (PFL) and analytics systems aim to compute differentially private (DP) models and statistics in a federated setup.

 % Figure environment removed

 Natural implementations of private learning algorithms in the federated setup, however, require placing a fair amount of trust in the server enabling this computation. Indeed a long and influential line or research~\cite{MelisSCS19,FowlGCGG22,GeipingBDM20, WangSZSWQ19, YinMVAKM21,ZhaoMB20,ZhuLH19,WenGFGG22,PasquiniFA22,BoenischDSSSP23a, BoenischDSSSP23b} shows that an adversarial server can compromise individual privacy in some commonly-used setups.
 The desire to reduce the trust assumptions in PFL has led to significant interest in the design and implementation of specific primitives, that are useful enough for a variety of DP algorithms, yet simple enough for secure and scalable implementations. Secure aggregation~\cite{ODOpaper, Bonawitz17,unlynx,crypteps,honeycrisp,orchard} and shuffling~\cite{Bittau17,ErlingssonFMRTT19,CheuSUZZ19} are two prominent examples of such primitives.

 Our first contribution is to propose a new, more powerful primitive that we call Samplable Anonymous Aggregation. We show that this primitive is powerful enough to allow for strong differential privacy guarantees for some of the commonly-used algorithms for private learning and statistics. For private histograms, it allows us to get privacy-utility trade-offs that are close to those achievable centrally with a trusted curator. For private learning, it allows us to get privacy bounds close to those achieved by the moments accountant~\cite{DLDP} in the central setting; indeed it is the first federated learning primitive that is able to achieve this.

 As our second contribution, we describe the architecture of a scalable system that can implement this primitive under suitable trust assumptions. Our proposed architecture builds on the Prio system~\cite{Corrigan-GibbsB17}, where clients use additive secret-sharing to share their data to two or more servers. As long as at least one of the servers is honest, Prio ensures relevant security properties. Our proposed architecture combines Prio with additional elements including client-side sampling, and anonymization and device authentication infrastructures.  We critically analyze the security aspects of the architecture and propose additional hardenings that can make it difficult to violate the trust assumptions of the system. We now give additional details of our contributions.

 \medskip\noindent{\bf New Primitive:} Our proposed primitive of {\em Samplable Anonymous Aggregation} specifies the ideal functionality that we desire. It being an Aggregation means that we compute an aggregate of (some) client contributions. Samplability means that this aggregate can be computed over a random subset of the clients. Anonymity here means that the subset we aggregate over stays hidden: the adversary only learns the sum over a random subset, but does not know whether any specific client is in this random subset. We defer the precise definition to \cref{sec:sa2}.

 Compared to the shuffling primitive~\cite{Bittau17}, aggregation can allow for better privacy guarantees, particularly for private federated learning where there is a large asymptotic gap~\cite{CheuSUZZ19, BalleBGN19a} in the privacy-utility tradeoffs of aggregation vs. shuffling for the problem of mean estimation. Compared to the previously studied primitive of Aggregation~\cite{ODOpaper,Bonawitz17, BellBGLR20}, samplability allows for better privacy analysis due to {\em Privacy Amplification by Sampling}~\cite{Kasiviswanathan:2008}.
 These privacy gains are significant. As we show in~\cref{exa:pfl-sampling}, for typical parameters used in cross-device settings, privacy amplification by sampling can help improve the differential privacy guarantees from $\eps \approx 100$ to $\eps \approx 1$.

 When communication constraints are present, the utility benefit of samplable anonymous aggregation is non-trivial even for the simpler tasks. In \cref{fig:histogram}, we consider the task of learning a distribution over an alphabet of size $K$ (for example, learning the usage distribution of language tokens, or learning the distribution of error codes for an application). While the population $N$ may often be large, we may want to learn a good approximation of this distribution by collecting reports from a smaller number $M$ of samples, while preserving differential privacy. In the left Figure, we plot the expected squared error against the number of samples without privacy ({\sc NonPriv}), when using an aggregator or a shuffler alone ({\sc Agg}), and when using a samplable aggregator ({\sc SampAgg}). Since sampling improves privacy, a samplable aggregator allows us to add lower variance noise, compared to an aggregator, for the same final privacy parameter. Thus we see that {\sc SampAgg} improves on {\sc Agg}. This gap becomes even larger when we have a very skewed distribution where a non-default value occurs only with a small probability $\gamma$ and we care about the distribution of the non-default values (e.g. an error code occurs for a user only if there is an error). In this case, absent privacy concerns, one would send a report only when there is a non-default value or more broadly, use some kind of importance sampling. With a normal aggregator, importance sampling is not private as it reveals that a user has a non-default value. With a samplable anonymous aggregator, we can additionally benefit from this importance sampling and thus see larger gains for some parameters (\cref{fig:histogram} middle and right).

 \medskip\noindent{\bf Implementation:} Our proposed implementation of this primitive (detailed in \cref{sec:architecture}) uses a split-trust model, where we rely on two or more non-colluding servers. Effectively, the clients secret share their data to two servers, which can then run a multi-party computation to evaluate the aggregate.
 Our protocol needs a single message from each client, thus avoiding the complications due to churn and dropout that are inherent in multi-round protocols.

 In addition to non-colluding servers, our approach relies on an anonymization infrastructure that ensures that the messages received at the server cannot be linked to their sender. Using such an infrastructure allows us to get {\em strong anonymity} that ensures that even the {\em set} of clients contributing to a collection remains hidden from the server. To ensure that contributions are coming from genuine clients, we propose using an anonymous rate-limited device authentication method such as Privacy Pass~\cite{ietf-privacypass-rate-limit-tokens-01}.

 A large-scale deployment of a PFL system may be subject to attacks from adversarial client devices, that may seek to poison the computation, or attempt to manipulate the learnt statistics or the learnt model to behave in a certain way~\cite{BagdasaryanVHES20,BhagojiCMC19, SunKSM19,BaruchBG19,WangSRVASLP20,CheuSU19}.
 A robust system should limit the potential impact of an adversarial client.
 Prio allows devices to send zero-knowledge proofs of {\em validity} that can be verified by the server to guarantee this robustness to poisoning. Recent advances in efficient proofs in this framework~\cite{BBCGI19,BonehBCGI21, BellGGKMRS22, CastroP22,Talwar22,AddankiGJOP22,RatheeSWP22,BonehBCGI23}, enable this robustness with small computational and communication overheads.

 \medskip\noindent{\bf Organization:} The rest of the paper is organized as follows. We start with some differential privacy background in \cref{sec:prelims}, and present the formal definition of our primitive in \cref{sec:sa2}. We describe the proposed architecture for implementing the primitive, along with its security analysis and additional hardenings in~\cref{sec:architecture}. \cref{sec:discussion} discusses properties of the primitive and additional enhancements.
 We discuss additional related work in \cref{sec:related} and conclude with some open directions in \cref{sec:conclusions}.

 \section{Preliminaries}
 \label{sec:prelims}

 Differential privacy~\cite{Dwork:2006} defines a way to measure the individual privacy loss of a randomized algorithm.
 Intuitively, an algorithm is differentially private if the distribution of outputs is reasonably stable with respect to a single individual changing their data. There are several ways to formalize the notion of closeness of distributions that are commonly used to define variants of differential privacy (DP). The most popular are the hockey-stick divergence, used to define $(\eps,\delta)$-differential privacy, and the R\'enyi divergence, used to R\'enyi differential privacy (RDP).

 \begin{definition}[Hockey-stick divergence]
 The {\em hockey-stick} divergence between two random variables $P$ and $Q$ is defined by:
 \begin{displaymath}
   \dalpha{e^{\eps}}(P\|Q) = \int \max\{0, P(x)-e^{\eps} Q(x)\} dx,
 \end{displaymath}
   where we use the notation $P$ and $Q$ to refer to both the random variables and their probability density functions. We say that $P$ and $Q$ are $(\eps, \delta)$-indistinguishable if $\max\{\dalpha{e^{\eps}}(P\|Q), \dalpha{e^{\eps}}(Q\|P)\}\le\delta$. \end{definition}

 \begin{definition}[R\'enyi divergence]
 For two random variables $P$ and $Q$, the \emph{R\'enyi divergence} of $P$ and $Q$ of order $\alpha>1$ is \[D^{\alpha}(P\|Q) = \frac{1}{\alpha-1}\ln \E_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha} \right] .\]
 \end{definition}

 We say that two databases are neighboring if one can be obtained from the other by the addition or deletion of the data of a single individual. Another notion is based on replacement, where two databases are neighboring if one can be obtained from the other by replacing the data of one individual. As these are related up to a factor of two, we will not dwell on this distinction and state results for one or the other when appropriate. We can now define differentially private algorithms with respect to both the hockey-stick divergence\footnote{The definition in~\cite{Dwork:2006} is stated slightly differently, but is easily seen to be equivalent~\cite{BartheO13}}, and R\'enyi divergence.

 \begin{definition}[Central DP]\cite{Dwork:2006}\label{centralDP}
 An algorithm $\cA:\cD^n\to\output$ is $(\eps, \delta)$-\emph{differentially private} if for all neighboring databases $X$ and $X'$, $\cA(X)$ and $\cA(X')$ are $(\eps, \delta)$-indistinguishable. If $\delta=0$, we refer to $\cA$ as satisfying $\eps$-DP, and say the algorithm satisfies \emph{pure differential privacy}. If $\delta>0$, then we say $\cA$ satisfies \emph{approximate differential privacy}.

 An algorithm $\cA:\cD^n\to\output$ is $(\alpha, \rho(\alpha))$-\emph{R\'enyi differentially private} if for all neighboring databases $X$ and $X'$, $D^{\alpha}(\cA(X)\| \cA(X'))\le\rho(\alpha)$.
 \end{definition}

 The two variants of differential privacy are related. Any algorithm that satisfies pure DP also satisfies RDP. If an algorithm satisfies RDP, then it also satisfies approximate DP. We give the formal conversion statements in the following theorem.

 \begin{theorem}\cite{Bun:2016, mironov2017renyi, CanonneKS20}
 If $\cA:\cD^n\to\output$ is $\eps$-DP, then for any $\alpha>1$, $\cA$ satisfies $(\alpha, \frac{1}{2}\eps^2\alpha)$-RDP. Conversely, for any $\delta\in(0,1]$, if $\cA$ is $(\alpha, \eps)$-RDP then it is $(\eps~+~\frac{\log(1/\delta)+(\alpha-1)\log(1-1/\alpha)-\log(\alpha)}{\alpha-1}, \delta)$-DP.
 \end{theorem}

 Differential privacy satisfies two key properties that allow for private building blocks to be combined to design more complex algorithms. First, it satisfies {\em post-processing}: applying an arbitrary data-independent map does not increase the privacy cost. Second, it degrades smoothly as we adaptively compose multiple private algorithms. {\em Composition} theorems allow us to analyze the privacy cost of a sequence of private algorithms. There are different versions of these composition results, and other DP results in this work, for different DP variants. While they are typically similar in their aymptotics, their numerical constants can differ. We will aim to present the simplest variant for each DP result in this section. For practical privacy accounting, one would often use other variants of these results.

 \begin{theorem}[RDP Composition \cite{mironov2017renyi}\label{compositionofRDP}]
 Let $\cA_i:\cD^n\to\output_1$ for $i=1,2,\ldots,T$ be a (possibly adaptive) sequence of randomized algorithms such that $\cA_i$ is $(\alpha, \rho_i(\alpha))$-RDP. Then the algorithm defined by $X\mapsto(Y_1,Y_2,\ldots,Y_T)$ where $Y_i=\cA_i(X)$ is $(\alpha, \sum_{i=1}^T \rho_i(\alpha))$-RDP.
 \end{theorem}

 One can easily generalize Theorem~\ref{compositionofRDP} to adaptively compose any number of RDP algorithms. A similar theorem exists for composing approximate DP algorithms.
 \begin{theorem}[Advanced Composition \cite{DRV10}\label{composition}]
 Let $\cA_i:\cD^n\to\output_i$, for $i=1, 2,\ldots, T$ be a (possibly adaptive) sequence of randomized algorithms such that $\cA_i$ is $(\eps, \delta)$-DP. Then for any $\delta'>0$, the algorithm defined by  $X\mapsto(Y_1,Y_2,\ldots,Y_T)$ where $Y_i=\cA_i(X)$  is $(\eps', T\delta+\delta')$-DP where $\eps' = \eps\sqrt{2T\ln \frac 1 \delta'} + T\eps(e^{\eps}-1)$.
 \end{theorem}
 Note that in the expression for $\eps'$ above, the second term is $O(\eps^2 T)$ whenever $\eps < 1$. For $\eps < \frac{1}{\sqrt{T}}$, the first term will dominate, and thus the composition of $T$ $(\eps,\delta)$-DP mechanisms will be $(O(\eps\sqrt{T}), T\delta)$-DP for small $\eps$. Asymptotically, and in practice, the tightest privacy bounds on many practical DP algorithms are derived by composing the RDP guarantees and converting the final RDP guarantee to approximate DP.

 \subsection{Privacy Amplification by Sampling}\label{subsampling}

 Sampling is a common primitive used in data collection systems for a variety of reasons, including balancing communication and computation costs associated with large scale data analysis.
 Choosing to include only a sample of the available users in the differentially private computation provides additional privacy, by giving each user plausible deniability about whether their data was included in the computation or not. This can be formalized for a variety of sampling methods including sampling-with-replacement (selecting a random subset of a fixed size) and Poisson sampling (selecting each data point independently with a certain probability) in both approximate and R\'enyi DP~\cite{Kasiviswanathan:2008, Ullman:2017, Balle:2018, WangLF16b, Wang:2021}. Below we state the privacy amplification by sampling bound for Poisson sampling for approximate differential privacy.

 \begin{theorem}\label{samplingWOR}\cite{Kasiviswanathan:2008}
 Let $\cA$ be an $(\eps,\delta)$-DP mechanism, and $\gamma\in[0,1]$ be a sampling rate. Given a data set $X$, let $\cA_s$ be the algorithm that first samples a subset $X'$ by including each data point in $X$ with probability $\gamma$, then outputs $\cA(X')$. Then $\cA_s$ is $(\eps',\delta')$-DP for $\eps'=\log(1+\gamma(e^\eps -1))$ and
 $\delta'=\gamma\delta$.
 \end{theorem}

 Note that when $\eps$ is small, $\eps'\approx \gamma\eps$, so the privacy guarantee is, roughly, scaled by the sampling rate. Similar bounds exist for sampling-with-replacement, and R\'enyi DP, and numerical bounds for these for specific mechanisms of interest are implemented in various libraries\cite{DLDP,Mironov2019RnyiDP,tfprivacy, yousefpour2021opacus}.
 \begin{example}[Gaussian and Subsampled Gaussian Mechanisms]
 Let us consider a particular differentially private algorithm, and the impact that Poisson sampling has on this mechanism.
 The \emph{Gaussian mechanism} is a DP algorithm that hides the impact of a single individual by adding Gaussian noise to the output of the computation. Given $f:\cD^n\to\mathbb{R}^d$ a deterministic function, define the sensitivity of $f$ to be $\Delta_f = \max_{X,X' \mbox{ neighbors}}\|f(X)-f(X')\|_2$ to be the maximum amount that the output of the function can vary on any pair of neighboring datasets $X$ and $X'$. The Gaussian mechanism on input $X$ outputs $f(X)+\mathcal{N}(0, \Delta_f^2 \sigma^2 \mathbb{I}_d)$.
 It satisfies $(\eps,\delta)$-DP for $\sigma \geq \sqrt{2 \ln 1.25/\delta} / \eps$, and $(\alpha, \alpha/2\sigma^2)$-RDP.

 Further, given $q \in [0,1]$, the algorithm that first samples a subset $X'$ by including each user with probability $q$, then outputs $f(X')+\mathcal{N}(0, \Delta_f^2 \sigma^2 \mathbb{I}_d)$ is $(2q\eps, 2\delta)$-DP and $(\alpha, q^2 \alpha / 2\sigma^2)$-RDP as long as $\alpha$ is suitable bounded~\cite{DLDP,Mironov2019RnyiDP}.

 These analytic bounds can be improved by numerical methods. We can use bounds from the Analytic Gaussian Mechanism~\cite{Balle2018ImprovingTG}, and the moments accountant~\cite{DLDP}, and will use those in subsequent examples in this work.
 \end{example}

 \subsection{Local Differential Privacy}

 Definition~\ref{centralDP} is commonly referred to as the \emph{central model} of differential privacy, where the curator (or server) has direct access to the data. Local DP algorithms constrain the whole communication from each client itself to be differentially private. In the fully adaptive case, they can communicate with the server in an arbitrary order with adaptive interaction. Formally, a protocol satisfies local $(\eps,\delta)$-DP if the transcripts of the interaction on any two pairs of neighbouring datasets are $(\eps,\delta)$-indistinguishable. When there is only a single round of interaction, the condition on transcripts reduces to each user interacting with the server using a mechanism that is differentially private with respect to that user's data. We call such mechanisms for the local reports of a user \emph{local randomizers.}

 \begin{definition}[Local randomizer]\label{localrandomizer}
 An algorithm $\lr\colon \cD\to \output$ is an $(\eps, \delta)$-DP \emph{local randomizer}  if there exists a reference distribution $\mathcal{D}_{\mbox{ref}}$ such that
 for all data points $x\in \cD$, $\lr(x)$ and $\mathcal{D}_{\mbox{ref}}$ are $(\eps, \delta)$-indistinguishable.
 \end{definition}

 Formally, an adaptive single pass $(\eps,\delta)$-DP local protocol can be described by a sequence of local randomizers $\Aldp[i]:\out[1]\times\cdots\times\out[i-1]\times\mathcal{D}\to\out[i]$ for $i\in[n]$, where $\cD$ is the data domain, $\out[i]$ is the range space of $\Aldp[i]$ and the $i$-th user returns $z_i=\Aldp[i](z_{1:i-1}, x_i)$. We require that the local randomizer $\Aldp[i](z_{1:i-1}, \cdot)$ be $(\eps,\delta)$-DP for all values of auxiliary inputs $z_{1:i-1}\in\out[1]\times\cdots\times\out[i-1]$. We can also define local randomizers with respect to R\'enyi DP, however throughout this paper, we will focus on $(\epsilon, \delta)$-DP local randomizers.

 \begin{example}[{\sc Rappor}$_{K}$.] Given an input $x \in [K]$, the mechanism \rappork\ first transforms $x$ to a $K$-dimensional $0$-$1$ vector $e_x$, which is $1$ in the $x$th coordinate and $0$ elsewhere. It then flips each bit of $e_x$, independently, with probability $\frac{1}{e^{\eps_0}+1}$ to get a $0$-$1$ vector. This mechanism satisfies $(\eps_0,0)$-DP in the local model~\cite{ErlingssonPK14}.
 \end{example}

 The aggregate and shuffle models of differential privacy are distributed models of computation where the clients hold their own data and interact with the server in a federated model through an implementation of the appropriate primitive. We will discuss the impact of these primitives in the following section.

 \paragraph{Notation.} $\sum$ always refers to a commutative associative addition operation, such as summation over reals or over a finite field. We abuse notation and use sets to mean {\em multisets}.

 \section{Aggregation Systems and their Implications}
 \label{sec:sa2}
 Towards our final definition, we start by defining a simpler primitive: an aggregation functionality. While such a functionality has been defined in previous work, we define a version that is slightly more general.

 Our aggregator $\agg$  will be parameterized by a {\em cohort size} \minbatch. This parameter specifies the minimum number of contributions that must be aggregated before release. This would typically correspond to the batch size in a private federated learning setting, and we will often use batch and cohort interchangeably. The aggregator is also parameterized by a decoding function \Dec, that prescribes how messages from users are to be transformed before aggregating. It receives messages from a set of users, decodes each of them using \Dec, and outputs the sum of the decodings, as long as at least \minbatch\ messages are received.
 Formally, it receives $m_1,\ldots,m_k$ for some $k$, and returns

 \begin{align*}
     \agg_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}) = \left\{\begin{array}{ll} \sum_{i=1}^k Dec(m_i) & \mbox{if } k \geq \minbatch\\ \bot &\mbox{otherwise} \end{array} \right.
 \end{align*}
 When \minbatch\ and \Dec\ are obvious from context, we will often just write \agg\ to mean $\agg_{\minbatch}^{\Dec}$.
 A single-round protocol in the Aggregator model is defined by a randomizer $R$, and the decoding function \Dec\ used in the aggregator. We say that the protocol $(R, \Dec, \minbatch, \agg)$ is $(\eps,\delta)$-DP (resp. $(\alpha, \rho(\alpha))$-RDP) in the aggregator model if for any $k$, $\agg_{\minbatch}^{\Dec}(R(x_1), \ldots, R(x_k))$ is $(\eps,\delta)$-DP (resp. $(\alpha, \rho(\alpha))$-RDP). Note that we treat $k$ as public in this definition.

 A multi-round protocol in the Aggregator model is defined by a sequence of single-round protocols. The privacy cost of a multi-round protocol can be upper bounded by the sequential composition of the privacy cost of each of the single-round protocols. More generally, each user may choose to participate in only a subset of the single-round protocols, in which case better privacy bounds on the composition may be proven by a more careful analysis.

 \begin{example}\label{exa:pfl-agg-only}
     Let $x_i \in \Re^d$ with $\|x_i\|_2 \leq 1$. Let $R(x_i) = x_i + \mathcal{N}(0, \frac{\sigma^2}{\minbatch} \mathbb{I}_d)$ and let $\Dec$ be the identity function. Then the aggregation $\agg_{\minbatch}^{\Dec}(R(x_1), \ldots, R(x_k))$ is distributed as $\sum_i x_i + \mathcal{N}(0, \frac{k\sigma^2}{\minbatch} \mathbb{I}_d)$ whenever $k \geq \minbatch$ and equals $\bot$ otherwise. Then for $\eps = \frac{\sqrt{2\log \frac {1.25} \delta}}{\sigma}$, the protocol is $(\eps,\delta)$-DP in the aggregator model.
     As a concrete example, one can verify that for  $\sigma = 7$, this protocol is $(1, 10^{-8})$-DP in the aggregator model. Using the better numerical bounds from~\cite{Balle2018ImprovingTG}, the same privacy bound holds for $\sigma=5.1$.
 \end{example}


 \begin{example} \label{exa:fedstats}
 Consider any $\eps_0$-local DP mechanism $R^{(\eps_0)}$, an arbitrary $\Dec$ and for $\delta>0$, let $\eps = \eps_{\texttt{shuffle}}(\eps_0, \minbatch, \delta)$ be the bound\footnote{The analytical bound in~\cite{FeldmanMT2020} is $\ln\left(1+(e^{\eps_0}-1)\left(\frac{4\sqrt{2\ln(4/\delta)}}{\sqrt{(e^{\eps_0}+1)\minbatch}}+\frac{4}{n}\right)\right)$, and they also give numerical bounds that are tighter.} on the privacy of shuffled version of the local responses proved in~\cite{FeldmanMT2020}. Since aggregation is commutative and associative, the output of the aggregator
 is a post-processing of the shuffled responses. For any $k \leq \minbatch$, the output is independent of the data and hence private. For $k \geq \minbatch$, the shuffling result applies and gives us $(\eps, \delta)$-DP in the aggregator model.
 As a concrete example, for $\eps_0=4.0$ and $\minbatch= 10,000$, the numerical bounds in~\cite{FeldmanMT2020} imply that this protocol satisfies $(0.61, 1e-10)$-DP. When $R^{\eps_0}$ is the {\sc Rappor}$_K$ algorithm~\cite{ErlingssonPK14}, the aggregation allows for asymptotically optimal reconstruction of histograms~\cite{FeldmanMT2020}.
 \end{example}

 Note that the aggregator model assumes that all the $k$ clients are following the protocol. Any bound in the aggregator model extends to a bound in a robust version of this model, where at least $k\geq \minbatch$ of the clients follow the protocol (see~\cref{subsec:security}).
 Moreover, note that from the point of view of one of the clients, the noise is coming from $(\minbatch-1)$ other clients rather than $\minbatch$. Since we  will deal with large values of $\minbatch$, we ignore this distinction for simplicity.


 We next define the stronger primitive of Samplable Anonymous Aggregation. This primitive takes in an additional {\em sampling} parameter $q$. For inputs $x_1, \ldots, x_k$, the sampled aggregate is the aggregate as before of $\Dec(R(x_i))$s, but now taken only over a randomly chosen subset of indices. Formally, for a multiset $S$, let $S_{\downarrow q}$ be the random multiset defined by selecting each index independently with probability $q$. Then
 \begin{align*}
     \sa_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}) &=  \agg_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}_{\downarrow q})
 \end{align*}

 Note that in this definition, we have assumed that the aggregator only outputs the aggregate, but not the set of indices that contributed to the sum. This {\em secrecy of the sample} is what enables the privacy amplification by sampling analyses.

 As before, a single round protocol $(R, \Dec, \minbatch, \agg, q)$ satisfies a certain differential privacy guarantee in the Samplable Anonymous Aggregator (\SAA) model if $\sa_{\minbatch}^{\Dec}(R(x_1),\ldots, R(x_k))$ satisfies the differential privacy guarantee for any $k$. A multi-round protocol will be analyzed by sequential (adaptive) composition of single round protocols, possibly taking into account that each user may not participate in each round.

 \begin{example}\label{exa:pfl-sampling}
 Consider a Private Federated Learning process where at each iteration, each user independently with probability $q$ decides whether or not to participate in this iteration. If it decides to participate, it sends its gradient, clipped to norm 1, with additive $\mathcal{N}(0, \frac{\sigma^2}{\minbatch})$ noise added. The aggregator outputs the sum of these noisy gradient updates. Then by privacy amplification by sampling results, it follows that this protocol satisfies $(\eps, \delta)$-DP in the \SAA\ model for $\eps \approx q\frac{\sqrt{2\log \frac {1.25} \delta}}{\sigma}$. These analytic bounds can be improved using numerical analysis techniques. As an example, if $\sigma=5.1$ and $q=0.02$, the protocol satisfies $(0.034,10^{-8})$-DP in the \SAA\ model (using~\cite{Balle2018ImprovingTG} and privacy amplification by sampling). Running this algorithm over $2,500$ iterations leads to an overall privacy cost of $(0.8, 10^{-8})$-DP in the \SAA\ model using the moments accountant. Note that this is a significant improvement over what one would get in the aggregation model.
 Indeed without amplification, the resulting $\eps$ is larger than $100$. A more careful analysis, that exploits the fact that when sampling, each user only participates in a $\approx q$ fraction of the rounds gives an $\eps \approx 8.3$. The parameters above are in the same ballpark as used in large-scale deployments such as~\cite{xu2023federated} that lead to accurate models.

 We also note that this analysis does not make any assumptions about the server generating the models on which the gradients are computed. Those may be chosen adversarially, and the privacy analysis continues to hold, as it only requires that each step is a sampled Gaussian.
 \end{example}

 \begin{example}\label{exa:fedstats-sampling} Consider the aggregation of \rappork\ outputs as in \cref{exa:fedstats}. Recall that with $\eps_0 = 4.0$ and $B=10,000$, the shuffled output satisfies $(0.61, 10^{-10})$-DP. Now suppose that we had $1,000,000$ clients, and each was sampled with probability $q=0.02$, and their outputs were aggregated. With respect to the dataset of $1,000,000$ clients, this output satisfies $(\eps, 10^{-10})$-DP for $\eps < 0.02$. Thus in the \SAA\ model, for these parameters, the algorithm satisfies $(0.02, 10^{-10})$-DP.
 \end{example}

 One can also define a version of this definition where the set is chosen by sampling a random subset of indices of a fixed size $\minbatch$. This distinction is akin to the different flavors of privacy amplification by sampling. These different models are close to each other:  when $\minbatch < qk - \sqrt{qk\log \frac 1 \delta}$, then the Poisson sample is of size at least $\minbatch$ with probability $(1-\delta)$. In this case, the distribution of the set of chosen indices in the Poisson case is a mixture of distributions over random subsets of a fixed size $B'$ for a random $B' \geq \minbatch$.

 \subsection{The Impact of Sampling and Anonymity}
 \label{sec:experiments}
 This is an opportune moment to better understand the impact of sampling.  Sampling is often done for reasons of efficiency: when trying to learn a population average (or a population gradient), a small sample may be sufficient to get a reasonable estimate, and sampling may help both minimize the impact on the client device, and reduce the load on the server.

 Consider an iterative algorithm run for $T$ steps, with each step being run on a $q$ fraction of the data. When the sample is not anonymous, each user participates on average in $qT$ steps. Thus if each step is $(\eps,\delta)$-DP, the advanced composition theorem tells us that the algorithm is $\approx (\eps\sqrt{qT}, q\delta T)$-DP.

 On the other hand, when the sample is in fact random, privacy amplification by sampling implies a bound of $\approx (q\eps, q\delta)$ for each of the $T$ steps. Now composing over all $T$ steps, the overall algorithm is $\approx (q\eps\sqrt{T}, q\delta T)$-DP. This gain of about $\sqrt{q}$ in the privacy cost is borne out in numerical privacy accounting as well, as we saw in~\cref{exa:pfl-sampling}.

 Typical private federated learning and analytics applications run many such algorithms for various reasons, such as hyperparameter tuning, keeping the model current in the face of distribution shift, or getting telemetry over time to evaluate the impact of changes in the system. Running algorithm such as above, the $1/\sqrt{q}$ reduction in the privacy cost per algorithm translates to being able to run $1/q$ times as many algorithms at the same privacy cost.

 This is a significant win. The sampling factor $q$ can often be quite small when dealing with large populations of users. This gain, of being able to run $1/q$ times as many experiments comes at {\em absolutely no cost to utility}. From a statistical point of view, the algorithm being run in the two cases (random known sample vs. random unknown sample) is exactly the same. The gain in privacy cost here is coming {\em entirely} from being able to hide the identity of the client devices! Counter-intuitive at first, this gain is similar to what one achieves from {\em shuffling}. However, while that shuffling gain is applicable to a much narrower class of local DP algorithms, the gain from sampling applies more broadly.

 How large is this $1/q$ factor in practice? In typical PFL applications that have been reported, the batch size is typically in the range of a few thousand to tens of thousands, depending partly on the model size~\cite{xu2023federated,federated-personalization}. The population of devices potentially contributing to these collections varies depending on the task at hand. As an example, for training a keyboard model, ~\cite{xu2023federated} have reported population sizes ranging from ~1M (Portugese keyboard model for Portugal) to ~16M (Portugese keyboard model for Brazil), and other works~\cite{improving-on-device-speaker} have used estimates in the 100M range. Thus for these setups $1/q$ varies from $20$ on the extreme low end to $10000$ on the high end, with values in the hundreds being typical. Similar ranges of $1/q \approx 60$ have been reported for Federated statistics applications in ~\cite{cormode2022sample, zhu2020federated}. Thus making the aggregation samplable can allow us to run many more learning tasks compared to aggregation alone, for the same overall privacy budget.
 % Figure environment removed

 % Figure environment removed


 Similarly, in histogram tasks, one would often sample for communication cost reasons alone, and the samplable anonymity can result is significant improvements in the privacy-utility trade-off.  We demonstrate this numerically next.

 We consider the standard task of building a histogram over an alphabet of size $[K]$, where each of $N$ users has a value $i \in [K]$. To simulate practical communication constraints, we ask that at most $M$ users communicate with the server for each such task. We will consider the case where we want to do $T$ such tasks with a total privacy budget of $(1, 10^{-6})$-DP (e.g. we may want to run an iterative algorithm such as~\cite{ChadhaCDFHJMT24}, or learn multiple different histograms over the same set of users). We evaluate three algorithms, computing the expected squared error (or total variance) $\sum_{i=1}^K \|p_i - \tilde{p}_i\|_2^2$ of the an estimated distribution $\tilde{p}$. The baseline is the non-private algorithm, which only incurs error due to sampling, which is approximately\footnote{To be precise, this error is $\sum_i p_i(1-p_i)/M \leq (1-p_{max})/M$. Thus whenever $p_max << 1$, which is a uniform or power law distribution on a large $K$, this is extremely close to $1/M$.} $\frac{1}{M}$. The non-anonymous aggregation model allows adding Gaussian noise to an estimate computed from an $M/N$ fraction of the population. The noise can be calibrated so that the total privacy cost over $\lceil \frac{TM}{N} \rceil$ compositions is controlled. We use the optimal Analytical Gaussian Mechanism~\cite{Wang:2021} accounting to tune the noise here. With samplable anonymous aggregation, we can use privacy amplification by sampling and thus use the Moments Accountant~\cite{DLDP}. Figure~\ref{fig:hist-vary-T} plots the expected squared error vs. $M$ for a few different parameter settings. The plots show that \SAA allows for lower squared error, often improving significantly on the non-anonymous aggregation. We remark that the error for the two private algorithms here includes the sampling error and thus the private error is always strictly larger. Our plots show that with Samplable Aggregation, the additional error due to privacy is negligible, whereas with aggregation alone, the variance can be an order of magnitude larger.

 This impact is further amplified in the common case when there is a dominant default value, say $0$. This constraint that only a small fraction of the users have a non-default value can be thought of as a sparsity constraint. This arises naturally in many settings. For example the error codes on a device would usually denote "No Error", and we are primarily interested in the distribution of errors when an error occurs.
 Let the the universe by $[K]$ and the fraction of non-zero values by $\gamma$. The goal in this {\em counting needles} task is to estimate the distribution over the values different from $0$. Thus we measure the error of an estimate $q$ by $Err_{\ell_2^2}  =   \sum_{i=1}^{K-1} (q_i - p_i)^2$. The naive sampling approach here will have expected squared error of $\gamma / M$. The reader would notice that only a $\gamma$ fraction of the contributions provide useful (i.e. different from $0$) information. It is therefore natural to downsample the zero values and upsample the non-zero values. We consider a simple instantiation of importance sampling,  where $0$ values are downsampled at rate $M / 2N$, and other values are sampled at rate $\min(M/2\gamma N, 1)$. When $M \leq 2\gamma N$, this yield $M$ samples, roughly half of which are $0$ and the remaining half take a non-default value\footnote{We use this split for simplicity. It can be improved by a factor of two by choosing different downsampling probabilities.}. With aggregation alone, there is no way to privatize the importance sampling approach, and thus any protocol must build on uniform sampling. With anonymous aggregation, one can privatize the importance sampling algorithm, leading to better privacy-utility tradeoffs. Figure~\ref{fig:sparse-hist-vary-T} plots the expected squared error vs. $M$ for a few different parameter settings. Our plots show that with Samplable Anonymous Aggregation, the additional error due to privacy is significantly smaller than that with aggregation alone.%negligible, whereas with aggregation alone, the variance is significantly larger.
 In \cref{sec:additional_exp}, we present additional plots showing that our improvements continue to hold for a large range of parameters.

 \section{Architecture of a Scalable Samplable Anonymous Aggregator}
 \label{sec:architecture}
 In this section, we describe an architecture for implementing the \SAA\ primitive. Our architecture is built atop Prio~\citep{Corrigan-GibbsB17} which allows aggregation using two or more servers, and ensures that as long as one of the servers is honest, nothing except the aggregate is learnt. Briefly, Prio uses additive secret sharing, where a contribution $m \in \gf_p$ gets shared to the different servers. In the case of two aggregation servers, the first server gets a uniformly random $r \in \gf_p$, and the other server gets $m-r \in \gf_p$. In line with the IETF Distributed Aggregation Protocol draft~\cite{ietf-ppm-dap-04}, we will refer to these two aggregation servers as {\em leader} and {\em helper}. The marginal distributions of the two shares are uniformly random, and thus individually contain no information about the contribution. Each of the two servers aggregates the shares from a group of clients, and the sums are then aggregated, giving the sum of the original messages (modulo $p$) as the result. Additionally, Prio allows the clients to send zero-knowledge proofs of {\em validity}, which allow the aggregation servers to learn that a contribution is valid without learning anything more about the contributions.

 The architecture will build on a few additional components. The first component is a system for shipping {\em recipes} to devices. The recipe specifies the parameters $R, Dec, \minbatch, q$, as well as relevant information about the aggregation process. The device then tosses its own coins to decide on whether or not to contribute, and if so sends its contribution to the system. The contributions are routed through an anonymization mechanism. We also use an anonymous, rate-limited device authentication mechanism so that each device can only send one contribution to a collection. The contributions take the form of shares for the two aggregation servers, each encrypted using their respective public keys. The leader and helper implement a version of the Prio protocol, aggregating the secret shares, and only sharing aggregates once \minbatch\ many contributions are aggregated. We next describe these components in more detail.

 \subsection{Recipe Publishing and On-device Sampling}
 %\subsection{On-device Sampling}
 The recipe contains information needed to run the protocol. The randomizers $R$ itself may change from query to query; e.g. while $R$ may correspond to {\sc Rappor}$_K$, the mapping of the data on device to $[K]$, and the value $K$ itself needs to be specified in the recipe. The recipe contains the sampling rate, and the batch size, which allow the client device to verify that the protocol satisfies DP in the \SAA\ model for appropriate privacy parameters. The client device can then use its own randomness to determine whether or not to participate in the collection. If it does contribute, it runs the randomizer and computes an output. This output is then converted to a pair of secret shares as expected in Prio. The recipe also contains the public keys for the servers, which allows these shares to be encrypted so only the appropriate server can decrypt their shares. The recipes themselves can be publicly posted and logged on device, making them auditable. Additionally, the public keys can be posted on a certificate transparency log that the device checks, making them more reliable and manageable.

 \subsection{Anonymization Infrastructure}
 The privacy amplification by sampling in an \SAA\ pipeline depends on the adversary not learning the set of clients that actually contributes to the aggregate. To ensure this property, an anonymization infrastructure can be used. In addition to allowing for the strong anonymity needed in the \SAA\ primitive, this infrastructure  is also valuable in hardening the system. Further, an anonymization infrastructure can also be used by the devices to download the recipes themselves.


 A strawman approach way to ensure strong anonymity is for each device to send a contribution. Devices that choose not to contribute can instead contribute secret shares of {\em zero}, which will have no impact on the aggregate. This approach however incurs a very large communication (and computational) overhead. Indeed the communication and server-side computational cost of noisy stochastic gradient descent in this setup would be as large as {\em full-batch} gradient descent, making it impractical when the number of devices is large.

 We propose distributing the trust using a multi-hop architecture as done in Onion routing~\cite{GoldschlagRS99,DingledineMS04} and other relay-based services~\cite{rfc9298,ietf-ohttp,privaterelay}.
 Indeed these approaches have been formalized as  standard protocols~\cite{ietf-ohttp, privaterelay} that have been deployed at scale for other applications such as private browsing. These approaches can provide anonymity without relying on all devices having to send some communication, and are therefore orders of magnitude more efficient when the sampling rate is small. Our design is oblivious to the precise approach used to achieve anonymization.

 \subsection{Device Authentication and Rate Limiting}
 A device authentication method can be used to ensure that each contribution comes from an actual client. Further, one can use rate-limiting techniques to ensure that a client cannot send arbitrarily many contributions to a collection. Such an approach must build on top of a device attestation framework (e.g~\cite{devicecheck, playintegrity}), or a user attestation framework. For this discussion, we assume that each device has an attestation token that can be validated, but may leak the identity of the user or the device.

 Authentication of contributions from users/devices directly is not compatible with our anonymity goal. We thus need to authenticate devices in an anonymous way.  A simple approach uses a Trusted Execution Environment (TEE) for device authentication. Each contribution comes with an attestation token, which is validated inside a TEE. The TEE also maintains a de-duplication data structure such as a bloom filter to prevent a device from contributing multiple times. The TEE then removes the identifiable attestation token, and signs the leader and helper shares. The leader and helper verify that every share they get has been signed by the TEE. The program running in the TEE can be published and attested to by the clients, enabling the client device to be sure that its contributions can only be decrypted by this program.

 This functionality of anonymous rate-limited attestation can also be implemented using a split-trust model.  The IETF Privacy Pass proposal on rate-limited tokens~\cite{ietf-privacypass-rate-limit-tokens-01} implements precisely this functionality using blind signatures. In brief, the trust is split between two parties, one of which (the {\em attester}) can validate the (non-anonymous) device attestation and rate-limit, whereas the other (the {\em issuer}) can issue tokens meant for a certain target use case. Importantly, the attester learns nothing about the use case, nor the token, and the issuer learns nothing about the identity of the device requesting the token. The token is anonymous and unlinkable, and can be encrypted along with the leader (helper) secret-share. It can be verified by the aggregation servers (leader/helper) to ensure that a device has been authenticated/attested and rate-limited without learning the device's identity. Each of the aggregation servers can verify that the token is valid, and has not been used before.

 In our set up, the time-of-issue and time-of-use can be made uncorrelated by having each device request tokens at a time that is independent of when they need to use them, and cache these tokens. If each device requests these tokens at a random time, the token issuance time carries no information. This can make the system resilient to one of attester/issuer colluding with one of leader/helper. Finally, we note that like the anonymization infrastructure, such an authentication infrastructure is being built and deployed for other applications that require authentication~\cite{pat-cloudflare, pat-fastly}.

 \subsection{Share Aggregation}
 The contributions coming from the device consist of two shares, where each share is encrypted using the public key of the relevant server. The shares are sent to the respective server and each server aggregates their shares. Alternately, the encrypted shares can be sent to the leader, which can batch and forward the helper shares.
 The leader can collect the aggregates once the total number of contributions aggregated is at least $B$.
 Additional steps are needed to ensure that the leader and the helper are aggregating over the same set of contributions, so that a dropped message does not lead to the aggregate being arbitrarily corrupted. Note that while the leader and helper may end up selecting a subset of contributions to aggregate (e.g. dropping some contributions due to network packet loss or corruptions), the protocol still requires that the number of contributions that are successfully aggregated is at least $B$. The IETF draft on Distributed Aggregation Protocol (DAP)~\cite{ietf-ppm-dap-04} describes a specification of such a system, and one may follow the protocol specification to do this aggregation. As long as the helper (resp. leader) does not share any aggregates smaller than the minimum batch size, the privacy property of the system holds against a potentially malicious leader (resp. helper).

 \subsection{Verification of Contributions}
 \label{subsec:verification}
 The use of a Prio-based systems gives us the additional ability to ensure that malicious contributions cannot arbitrarily derail the computation. This robustness, while not needed for privacy, ensures that the aggregates are not easily corrupted by a few adversarial client contributions. Importantly, the validation preserves the zero-knowledge property: neither the leader nor the helper learns any additional information about the client contribution, except for its validity. A client following the protocol will always send a valid contribution, so that the validity predicate contains no information for an honest client. Thus this verification comes at no privacy cost for honest clients.

 While arbitrary validity predicates can in principle be verified, efficient algorithms exist for several important cases of practical interest. For example, when $R$ is the {\em randomized response} mechanism, the output is a $0$-$1$ vector with exactly one $1$. \citet{BonehBCGI21} show how to validate this property with at most $O(\sqrt{K})$ additional communication. Similarly, the output of {\sc Rappor} is a $0$-$1$ vector where the number of $1$s is bounded by $\tilde{O}(1+\frac{K}{e^{\eps_0}+1})$ with high probability\footnote{The $\tilde{O}(\cdot)$ hides factors that are logarithmic in the failure probability.}. Verifying this property, which again can be done efficiently using ~\citep{BonehBCGI21}, helps limit the impact of an adversarial client on the final aggregate. Similarly, when using the Gaussian noise mechanism as the randomizer $R$, the output, when viewed as a $d$-dimensional real vector, has bounded Euclidean norm with high probability. Several recent works~\cite{Talwar22,libprio,pine} study this question and show that this property can be efficiently verified with little overhead. Some of these protocols have been formalized as standards in the IETF draft on Verifiable Distributed Aggregation Functions~\cite{irtf-cfrg-vdaf-06}.



 % Figure environment removed


 \subsection{Security Analysis: Privacy}
 \label{subsec:security}
 In this section, we argue that the proposed system securely implements an \SAA\ primitive, under suitable assumptions on the adversary. Our privacy assurance is that an adversary learns nothing more than the output of an idealized \SAA\ functionality and the number of contributions received. We start with defining our privacy adversary model.

 \medskip\noindent{\bf Privacy Adversary Model:} We allow an adversary to control at most one of the leader or helper, and assume that the other aggregation server honestly follows the protocol. Additionally, we allow the adversary to control at most $B/2$ of the client devices.

 We next argue that the our proposed system provides the privacy guarantees of the \SAA\ primitive, assuming that anonymization and authentication infrastructures work as intended.
 \begin{theorem}
     Suppose that an adversary controls at most $k$ client devices, and at most one of leader and helper. Further assume that the device authentication and anonymous channel ensure that each device is limited to sending one contribution that is not linkable to the client device. Then for some $\tilde{B}\geq B-k$, the view of the adversary can be simulated given the number of received messages and $\sa_{\tilde{B}}^{\Dec}(M)$ where $M$ is the set of messages from clients not controlled by the adversary.
 \end{theorem}
 \medskip\noindent{\bf Proof Sketch:}
     We assume that the adversary controls the leader; the proof for the helper case is similar. First suppose that $k=0$. Then each client sends a message $m_i = R(x_i)$ to the server with probability $q$. Let $I$ be the set of clients that select themselves, and let $\hat{M}$ be the set of their messages. If there are fewer than $B$ messages received, the honest helper aborts and no information is learnt by the adversary except $|I|$ (the $|I|$ secret shares can be trivially simulated). If the number of received messages is $|I| \geq B$, then the zero knowledge property of the Prio protocol ensures that the view of the leader can be simulated from the aggregate of the messages in $\hat{M}$. The set $\hat{M}$ itself is a uniformly random subset of size $|I|$ and the anonymization infrastructure ensures this is still true from the point of view of the adversary. Thus the aggregate available to the leader is indeed distributed as $\sa_{|I|}^{\Dec}(M)$. This implies the claim for the case of $k'=0$, with $\tilde{B} = \max(B, |I|)$.

     When $k > 0$, then the adversary controls at most $k$ of the client devices. The device authentication ensures that it can control at most $\tilde{k} \leq k$ contributions, and it can simulate its view of those contributions itself. The above argument is now applied to the $|I| - \tilde{k}$ contributions not controlled by the adversary. Since the adversary now knows $\tilde{k}$ of the messages $R(x_i)$, the aggregate over the remaining $|I|-\tilde{k}$ messages from honest clients suffice to allow the simulator to compute the aggregate over all $|I|$ messages. The rest of the simulation is now identical to the $k=0$ case. Setting  $\tilde{B} = \max(B, |I|) - \tilde{k}$ now completes the proof.
 \qed

  We note that the additional leakage $B'$ here is independent of the data $\{x_i\}_{i=1}^k$ held by the clients. Further, by having each device that does not select itself send a dummy report with probability $q' \approx \frac{1}{\eps k}$, we can ensure that $B'$ is a differentially private estimate of the number of devices that actually contribute. Thus the information contained in $B'$ can be hidden, and one can argue that there is a simulator whose output is $(\eps,\delta)$-indistinguishable from the adversary's view.


 \subsection{Utility Analysis}

 The above theorem only addresses the privacy of our protocol. If one of the two servers is adversarial, the computed aggregate can be arbitrarily corrupted and the system gives no utility. Any utility results depend on the assumption that {\em both} the aggregators follow the protocol correctly. When all the clients are honest, the protocol correctly implements the desired samplable aggregation functionality. When the contributions are verified as discussed in~\cref{subsec:verification} (and the aggregation servers are honest), the system is robust to adversarial clients: the impact of $k$ adversarial clients is limited to the amount that $k$ valid contributions can influence the aggregate. This follows from the soundness of the zero knowledge proof of validity. The precise definition of validity here, and thus this robustness, depends on the specific randomizer and the validity test that is used. As an example, when we use this protocol to perform vector aggregation for vectors with Euclidean norm bounded by $1$, the robustness property says that the output of the protocol differs from the ideal functionality by at most $k$ in Euclidean distance. When $k \ll B$, this implies that the mean of the vectors is computed with $\ell_2$ error $\frac k B \ll 1$, making the estimate sufficiently useful, for example, in federated learning. Notice that the ideal functionality of aggregation itself can be corrupted by this amount by $k$ adversarial clients.

 \subsection{Additional Hardening}
 Our security analysis proves that our system implements \SAA\ as long as an attacker controls at most one of the aggregation servers, and a bounded number of devices contributing to the batch.  Indeed consider a hypothetical leader (or helper) trying to learn a specific user's message $m_{i^\star} = R(x_{i^\star})$. To mount this singling out attack, the server would have to (a) ensure that $i^\star$ decides to participate, (b) ensure no other honest user messages are in the batch, and (c) send at least $B-1$ known messages to the batch.  Our use of rate-limited authentication makes (c) very expensive for an attacker. Indeed an adversary that controls a small number of real devices can only control a small number of contributions to a collection. We next discuss various hardenings to further mitigate the risk of an attacker being able to mount such an attack.

 \subsubsection*{Public Recipes and Server Information} The parameters of each collection, including information about leader and helper parameters, can be placed on a public transparency log. This information would be auditable, and this would ensure that every client sees the same recipe.
 This hardening makes it infeasible for the server to prevent sufficiently many genuine clients from contributing to the collection. Coupled with anonymity, it helps ensure that the server cannot identify $m_{i^{\star}}$ from the set of received contributions, and thus prevents an adversarial server from singling out a single device.


 \subsubsection*{Rate validation by leader and helper} Based on the recipe, and the number of devices, the leader and the helper have an expectation on the rate at which contributions arrive. This rate can also be estimated based on historical trends for similar recipes. A noticeably higher rate of contributions may suggest a sybil attack and the leader and helper may validate that the rate is within reasonable bounds. As an example, the leader (or helper) could reject batches when the rate of contributions exceeds twice the expected rate in a certain window. This gives a fair bit of room for natural fluctuations in the rate of contributions. At the same time, if the non-attack contribution rate is close to its expected value, this ensures that at least half of the contributions come from devices not controlled by an attacker. As our privacy guarantees are robust, this would mean that the privacy parameter against the attacker that controls half the batch is worse by a factor of about $\sqrt{2}$.



 \subsubsection*{Limited Logging and Periodic Sub-Aggregation}
 In the case that an attacker can compromise both the leader and the helper, the privacy level falls to that ensurable by shuffling and sampling. In typical federated statistics applications, this is a reasonably strong guarantee; indeed the privacy parameters computed in \cref{exa:fedstats} depend on shuffling alone. In the federated learning setting, the shuffling guarantee is typically weaker and susceptible to attacks of the kind discussed in~\cite{BoenischDSSSP23a, BoenischDSSSP23b}.  The leader and helper maintain no state beyond the collection and so a security breach at the two would have to be relatively simultaneous. Further the protocol can periodically create sub-aggregates, and discard the raw shares that have been aggregated already. This can reduce the privacy risk to the already sub-aggregated contributions, as the sub-aggregate provides some privacy protection depending on the number of contributions it aggregates.

 More broadly, we aim to minimize logging at all steps. The client device does not send any telemetry that may indicate participation in a collection. While this may make it harder to debug the system, reduced logging makes the system more robust and minimizes data leakage in case of attacks.


  \subsubsection*{Reliance on Client Code}
 The implementation of the client part of the protocol is a part of the client operating system. Standard techniques for ensuring trust in this implementation include infrequent updates, auditability of the client code and signed images that ensure that each client device runs the same version of the operating system. In the proposed system, most of the privacy protection depends on the client code alone. For example, the parameter $B$ itself is part of the encrypted shares from the client, which ensures that there is agreement on this value between the client and the two aggregation servers. These hardenings ensure that the important system parameters that impact the privacy guarantee can not be altered by an adversarial leader/helper.


 \section{Discussion}
 \label{sec:discussion}
 \subsection{Federated Statistics vs. Federated Learning} Our aggregation system is designed for two somewhat different kinds of workloads. In typical federated statistics applications, one is interested in the heavy hitters over the population, and the aggregation of interest is a histogram over a noticeable fraction of the population. Thus the setup entails large batch sizes and few latency constraints. On the other hand private federated learning uses the aggregation to compute the gradient over a batch of devices, and multiple adaptive steps over small (relative to the population) batches are preferred to fewer steps over a larger batch. Thus low latency is typically desired in this setting. Our proposed system benefits from a unified design for these two settings needing the same underlying primitive. One may consider different system optimizations for the two cases to help improve the performance of the system.

 \subsection{Trust Models}

 Similar systems for private aggregation may depend on the user trusting a central curator to use the data in a manner consistent with the promised privacy assurance. The proposed system strengthens such policy-based enforcement of privacy promises, and can ensure that the privacy assurance depends primarily on the client code and an inadvertent mistake or a bad actor on the server side does not lead to a privacy violation.

 The privacy assurances of the proposed system depend on non-collusion between the leader and the helper: as long as at least one of the two servers is honest, the claimed privacy bounds hold. Such a trust model has been used for existing Prio deployments~\citep{firefox,ENPA:2021}, as well as in Private Relay~\cite{privaterelay} and Privacy Pass~\cite{ietf-privacypass-rate-limit-tokens-01}, and similar trust models have been used extensively in secure multiparty computation deployments~\cite{Wolinsky2012,Bogetoft09,Liina13,Lapets16,Abidin16,Bogdanov16,ChowB18}. In a deployment where the servers are run by different parts of the same organization, this split trust model prevents a single point of vulnerability, ensuring that any breach of the trust assumptions would be easier to detect. This can be significantly strengthened by having different entities run different servers. With standardized protocols such as DAP~\citep{ietf-ppm-dap-04}, the helper servers may be offered as a service for multiple deployments by different organizations using such a system for private federated learning and optimization. Such a market may help further improve trust in the non-collusion. We leave to future work other approaches to ensuring non-collusion and improving the trust in the system.

 Recall that our utility bounds need both servers to be honest. We believe this is a reasonable assumptions as these servers would typically be run by organizations that are incentivized to correctly compute the aggregate. Using more than two servers can relax this assumption and allow for utility results while only needing an honest majority. However, such results necessarily need to go beyond additive secret-sharing and thus are not conducive to the efficient computations and efficient validity proofs in Prio. We leave further investigation of these approaches to future work.

 \subsection{Central Noise Addition} While our aggregation primitive has the advantage of simplicity, one can also consider a richer primitive which aggregates non-noised contributions and outputs the noised aggregate. This can be useful in settings where the batch size itself may be hard to estimate. Naively implementing this primitive in the two-server framework would require both the leader and the helper to add sufficient noise, making the noise variance larger. However, there may be efficient cryptographic protocols that can allow the two servers to collaboratively generate (shares of) noise with the right variance, so that the noise is hidden from each of the two servers as long as one of them is honest. Such an approach was proposed in the fully distributed MPC setting by~\citet{ODOpaper}, and we leave the design of practical and efficient noise generation protocols in the two-server setting to future work.


 \subsection{Samplable Aggregation vs. Aggregation}

 While an aggregation functionality has been proposed in prior works, we have shown that the stronger functionality of a samplable anonymous aggregation allows us to get better privacy-utility trade-offs for private federated learning algorithms. We remark that in principle, sampling is not needed: the batched stochastic gradient descent algorithm can be replaced by a {\em full-batch} version of the algorithm which can be tuned to give the same privacy-utility trade-offs. However, this version of the algorithm will incur significantly higher cost for client and server, and lead to higher latency and thus fewer iterations in a given amount of time. The samplable aspect of the aggregation is thus crucial to be able to train accurate models in an acceptable timeframe. While some recent approaches to private learning~\cite{KairouzMSTTX21} can get around the privacy amplification by sampling, they require the use of correlated noise across iterations~\cite{DworkNPR10, ChanSS11} and hence are not compatible with our goal of adding noise in a distributed fashion. We leave secure implementations of these noise addition mechanisms to future work. We also remark that these techniques in their current form are not compatible with the use of adaptive optimizers on the server, which are often necessary for training high-accuracy models~\cite{AzamPFTSL23}.

 The sampling itself comes from two different sources of randomness. One that our analysis primarily relies on is the sampling done by a client to decide whether or not to participate in a specific batch. This randomness is derived from an on-device source of randomness, and implemented properly, would indeed be hidden from any reasonable adversary. Typically, devices only consider participating in the collection under specific circumstances; e.g. a mobile device may need to be idle and charging for it to participate. There is some inherent uncertainty in when the device considers participating and it may be reasonable to model this as being hidden from the adversary. Such modeling may allow us to prove better privacy bounds against reasonable adversaries.


 \subsection{Targeting}
 In some settings, the server may want to {\em target} a recipe to a smaller subset of client devices, e.g. we may want to learn a next-word prediction model for clients using a specific language keyboard. Thus the recipe specifies a set of criteria that a device must meet to be able to participate. Arbitrary targeting criteria may allow a server to single out a device, e.g. by crafting a recipe which will have 0 or 1 responses depending on the data on a device. To mitigate against such risks, we can restrict the set of allowed targeting criteria to be those for which we have (e.g. using a private histogram query) ascertained that the population meeting the criteria is large enough. Large populations and a sampling rate bounded away from $1$ ensure that the noise due to sampling is enough to provide a strong differential privacy guarantee for the realized batch size (e.g. \cite{ODOpaper, zhu2020federated,cormode2022sample}). Additionally, we can restrict the targeting to be based only on non-sensitive criteria. Finally, the recipes being public and inspectable further reduces the risk of undetected attacks.

 \subsection{Local Differential Privacy and Compression}
 The contributions from clients that are aggregated in the proposed framework can be high-dimensional. E.g. we may want to build a histogram using {\sc Rappor} over a large alphabet, or aggregate gradients for a large ML model. Naively, this may require the client upload to be rather large, which may be undesirable in settings where the upload bandwidth is limited. While the definition of \SAA-DP does not require the individual randomizers to satisfy a formal privacy guarantee, commonly used algorithms can be run with these randomizers satisfying differential privacy. Using local DP randomizers ensures a backup level of formal privacy. Additionally, it allows the clients to compress the contributions before sending them. Local randomizers are provably all compressible in the single-server setting~\cite{FeldmanTalwar21}, and low-communication local randomizers can often be easily adapted to the two-server setting. E.g. {\sc Rappor} can be replaced by {\sc ProjectiveGeometryResponse}~\citep{FeldmanNNT22} which reduces the communication while preserving the local DP guarantee. Similarly, a random projection can reduce the communication cost in Gaussian noise addition while preserving the accuracy~\cite{AsiFNNT23}.

 \section{Additional Related Work}
 \label{sec:related}
 Differential privacy was introduced in~\cite{Dwork:2006}, and there is by now a large literature on DP primitives, algorithms, complexity, and different models of DP. Several deployed systems use differential privacy. In addition to the private federated learning deployments cited earlier, local differential privacy has been used in~\cite{ErlingssonPK14, Apple2017,DingKY17}. In the central model, there have been DP releases by the US Census Bureau~\cite{onthemap, abowd20222020}, Google~\cite{google-mobility, aktay2020google, bavadekar2021google}, LinkedIn~\cite{rogers2020linkedins, rogers2020members}, Microsoft~\cite{global_victim_perpetrator, pereira2021us} and Facebook~\cite{FacebookURLs} amongst others~\cite{pseo, edp}.

 There has been a long line of work on the Shuffle model~\cite{Bittau17, ErlingssonFMRTT19, CheuSUZZ19}. For some tasks such as histogram estimation, the shuffle model allows for near-optimal privacy utility tradeoffs~\cite{FeldmanMT2020}. However, as we discussed in \cref{sec:experiments}, sampling allows for additional improvements when communication constraints prevent using the full population to build a histogram. For other tasks such as real summation, a single message shuffle provably leads to worse privacy-utility tradeoffs compared to aggregation~\cite{BalleBGN19a}, that can be surmounted by sending a small constant number of messages~\cite{BalleBGN20,GhaziMPV20,GhaziKMPS21}. For the vector aggregation task, which is of great interest for private federated learning, recent work~\cite{AsiFNNTZ24} shows that the Shuffle model is provably worse that aggregation: it requires sending $\Omega(d)$ messages for $d$-dimensional vectors, and is necessarily non-robust for a large class of protocols.

 There are two alternative approaches to implementing aggregation primitives that have been proposed in literature. One is to use Multi-party computation amongst the clients, facilitated by a trusted server.  Bonawitz et al.~\cite{Bonawitz17} proposed an aggregation protocol in this setting and recent works~\cite{SoGA20,BellBGLR20} have shown that this approach can scale.  It is however unclear how to ensure strong anonymity in this approach as a central server does the key distribution. Moreover, this approach requires multiple rounds of interaction involving client devices, leading to more complex protocols.
 A different approach using Trusted Execution Environments (TEEs) has been proposed for implementing shuffling~\cite{Bittau17} and aggregations~\cite{papaya}. This approach is difficult to scale, adapt and maintain and requires specialized hardware. Recent concurrent work~\cite{JinCMRYO2024, EichnerRBH+24} has proposed using TEEs for reliable execution of private federated analyses. Various attacks on existing hardware~\cite{BulckMW+18} however challenge the trust assumptions of these approaches.

 As alluded to earlier, Prio-based aggregation systems have previously been successfully deployed at scale by Mozilla for private telemetry measurment~\cite{firefox} and by several parties to enable private measurements of pandemic data in Exposure Notification Private Analytics (ENPA)~\cite{ENPA:2021}. In the case of ENPA, the system aggregated over 22 billion device aggregates in a little over a year~\cite{enpa-rwc}, with the servers being run by different organizations. While these systems demonstrate the feasibility and scalability of our proposed approach, our work argues for the additional samplability and anonymity as being part of the primitive, which can yield significant privacy benefits for PFL as discussed in~\cref{exa:pfl-sampling}.

 Several Multiparty Computation (MPC) deployments in recent years have used the split-trust model similar to ours, where the security depends on a small set of servers not colluding~\cite{Bogetoft09,Liina13,Lapets16,Abidin16,Bogdanov16,ChowB18}. These examples implement MPC for relatively complex algorithms, and are closer to the cross-silo federated learning setting as the number of parties participating in the computation is typically small.

 Another approach to federated learning that has been used in the cross-silo setting~\cite{de2020SCRAM, BlattGPG20,Froelicher21,oncological23} is to build on recent advances in homomorphic encryption. Here, the parties share encryptions of their data, typically under a secret-shared key. The computation of interest is then performed on the encrypted data, to derive an encryption of the desired result. Finally, the result can be decrypted by a sufficient number of parties collaboratively decrypting using their respective secret shares. Such an approach has been proposed for the cross-device private federated analytics setting in~\cite{unlynx,honeycrisp,orchard} and a related approach with a second non-colluding server helping with the decryption has been proposed in~\cite{crypteps}. Our approach built on Prio avoids extra rounds of interaction with the client devices, thus avoid the challenges of client dropout and churn. Additionally, the samplability in our work allows for much tighter privacy analyses.


 Efficient Anonymous Authetication protocols are an active area of research. Blind RSA signatures date back to~\cite{Chaum81}, and anonymous tokens have been recently studied in~\cite{DavidsonGSTV18,KreuterLOR20,SildeS22}. In addition to the Private Access Token work mentioned earlier~\cite{ietf-privacypass-rate-limit-tokens-01}, rate-limited anonymous tokens have also recently been studied in \cite{BenhamoudaRS23}.

 In the current work, we aimed to limit the impact of an attacker to what it could have done if it could send arbitrary but bounded gradients in proportion to the number of devices it controls.  This may still allow a variety of model poisoning attacks as discussed in several works mentioned earlier~\cite{BagdasaryanVHES20,BhagojiCMC19, BaruchBG19,WangSRVASLP20,CheuSU19}, and differentially private training can help mitigate some of these attacks~\cite{SunKSM19}. Designing practical defenses that can further limit the attacker's impact on the model is an active area of research~\cite{BlanchardEGS17,BernsteinWAA18,flame,XieKG20,WuW21,ShejwalkarHKR22,wang2023invariant}. Extending such defenses to work in our framework may require additional cryptographic primitives and is left to future work.


 \section{Conclusions}
 \label{sec:conclusions}
 In this work, we have proposed a new primitive that can allow many private federated analyses to be performed with utility guarantees close to the central setting without the strong trust assumptions it entails. In particular, the primitive supports private histograms and private federated learning with central-like utility guarantees. We propose an architecture for a large scale implementation of such a system, building on the Prio architecture and additional standard components. Our work makes it feasible to build scalable and useful private federated analysis systems that do not need to trust a single central server. Client devices in our proposed system send a single message, thus avoiding the complexity needed to handle churn in multi-round protocols. Previous deployments of Prio~\cite{firefox,ENPA:2021} attest to the scalability of such an approach.

 We have shown that the \SAA\ primitive enables central-like privacy-utility trade-offs for many problems. A natural research direction is to understand better the full power of this primitive. The number of rounds of interactivity that a practical deployment permits is typically small, motivating the study of the computational, sample, communication and round complexity of private algorithms armed with this primitive.

 An orthogonal direction is to enrich the primitive to allow a larger class of algorithms. The trust model and the two-server architecture proposed here is rich enough to enable additional functionalities in principle. Efficient algorithms to implement functionalities that are useful for differentially private algorithms can enable better utility for additional data analyses.
 Finally, it is natural to implement the \SAA\ primitive (and other richer primitives) under other trust models that further distribute the trust.

 \section{Acknowledgements}
 This work was shaped by early discussions with
 Ulfar Erlingsson, Abhishek Bhowmick, Julien Freudiger, Rogier Van Dalen, Andrew Cherakshyn, Fei Dong, Jaeman Park, Yulia Shuvkashvili, Hwasung Lee and Matt Seigel. We are grateful to Dan Boneh for valuable feedback on an earlier draft of this work. We would also like to thank Joey Meyer, Brian Lindblom, Cory Benfield, Elliot Briggs, John Duchi, Michael Hesse, Anil Katti, Cheney Lyford, Alex Palmer and Paul Pelzl for useful discussions. We also thank the anonymous CCS 2024 reviewers for their valuable feedback.

 \ifbiblatex
 \printbibliography
 \else
 \bibliography{refs}
 \fi
 \appendix

\section{Privacy Amplification by Randomized Donation Time}

In Section~\ref{subsampling}, we discussed how sampling a subset of the available users at any time provides additional privacy by giving each user plausible deniability about whether their data was included in the computation. In this section, we will discuss how added uncertainty in when a user participates in a computation can be used to further amplify privacy guarantees in iterative algorithms like DP-SGD.
Devices typically need to satisfy specific conditions in order to consider participating, for example a device may need to charging. Since there is often uncertainty in when any particular device becomes available for participation, there is uncertainty in donation time inherent in the system. Relying on this randomness to provide formal privacy guarantees would require careful modeling of this randomness. However,
we can also enable devices to deliberately (and according to a specific distribution) randomize the times at which they consider participating.
In this section, we will discuss how a specific algorithm for randomizing donation time can be used to amplify privacy guarantees in iterative algorithms like DP-SGD.

Suppose that an analyst is running an iterative algorithm in the following manner. When a device becomes available, it chooses whether or not to participate by flipping a biased coin and participating with probability $q$. If the device chooses to participate it then chooses an amount of time to wait before participating. When the wait time is complete, if the device is available, it downloads the recipe for the currently occurring round of the iterative algorithm and sends a privatized report to the aggregator. The aggregator aggregates the local reports for some amount of time and sends the aggregate to the data analyst. The analyst then begins the next round by selecting the next round's query. This process continues until the iterative algorithm is complete. By randomizing the wait time, we can inject uncertainty into which iteration the device participates in, and use this uncertainty to amplify the privacy guarantee. However, selecting a wait time that is greater than 0 runs the risk of the device not being available at the subsequent donation time, increasing latency and/or reducing utility. Thus, we need to carefully balance any privacy amplification with latency/utility concerns.

The key intuition we will use for defining an algorithm for choosing the wait time is that we want to build uncertainty into which iteration a device contributes to. Ideally, we would have devices randomly select an iteration to participate in, but this is difficult to implement since iterations may last for variable lengths of time making it difficult for devices to predict when a specific iteration may occur. Our algorithm uses parameters $K>0$ and $m\in\mathbb{N}$. For all $i\in\{0,\cdots,m-1\}$, the algorithm samples from a Bernoulli random variable $\chi_i$ with bias $1/m$. If $\max\chi_i=0$, then the device does not participate. Otherwise, the wait time is set to set to $K\cdot\arg\min\{i\;|\; \chi_i=1\}$. If we can ensure each iteration lasts for at \emph{most} $K$ minutes, then this algorithm either selects not to participate, or selects randomly among $m$ future iterations. In order to formalize the privacy amplification of this mechanism, we need one additional functionality of our aggregator. In addition to implementing the \SAA\ primitive, we will require that the aggregator does not release the result if the length of collection time is longer than $K$ minutes.

\begin{align*}
    \agg_{\minbatch, K}^{\Dec}&(t_0, \{(m_1, t_1), \ldots, (m_k, t_k)\}) \\&= \left\{\begin{array}{ll} \sum_{i=1}^k Dec(m_i) & \mbox{if } k \geq \minbatch \text{ and } t_k\in[t_0,t_0+K]\\ \bot &\mbox{otherwise} \end{array} \right.
\end{align*}

Suppose each device participates at most once in the iterative algorithm and each round is $(\eps, \delta)$-DP in the \SAA\ model. If this algorithm is run with the above randomized donation time algorithm in the \SAA\ model equipped with an aggregator with this additional functionality then it is $(\epsilon', \delta')$-DP where
\[\epsilon'=O\left(\epsilon\sqrt{\frac{\log(1/\delta)}{m}}\right) \text{ and } \delta'=O(m\delta).\]

To see this, note that the privacy loss of this algorithm is upper bounded by the privacy loss of the related algorithm where for all $i\in\{0,\cdots, m-1\}$, if $\chi_i=1$, then the device participates in the round occurring after a wait time of $i\cdot K$. For each round, each device is either not eligible, or participates with probability $1/m$. Thus, using Lemma~\ref{samplingWOR}, we can see that each round is $(\log(1+\frac{1}{m}(e^{\epsilon}-1)), \delta/m)$-DP. Since each device only considers participating in at most $m$ rounds, we can obtain the final privacy guarantee by using the advanced composition theorem \cite{DRV10} over $m$ rounds, so \[\epsilon' \le \log\left(1+\frac{1}{m}(e^{\epsilon}-1)\right)\sqrt{2\log(1/\delta)m}+\frac{1}{2}m \log^2\left(1+\frac{1}{m}(e^{\epsilon}-1)\right) \]\[ \text{ and } \delta'\le (m+1)\delta.\]

We note that for small $\eps, \frac{1}{m}$,  this $\eps' \approx \eps \sqrt{\frac{2\log 1/\delta}{m}}$, and the bounds can be improved by using numerical accounting techniques based on Renyi DP.

To maximize utility and minimize latency, the constant $K$ should be chosen so that most iterations are expected to receive enough donations within $K$ minutes. The constant $m$ should be chosen so that the device is likely to be available at the assigned donation time. This may be achieved by choosing $m$ small enough that $m\cdot K$ is a lower bound on the length of time that devices typically satisfy the conditions required in order for them to be available.

\section{Additional Experiments}
\label{sec:additional_exp}
In this section, we present additional experiments that vary parameters. For the case of histogram, the relevant parameters are $K$ and $T$. The population size $N$ has no impact on the calculation, as long as it is at least $M$. We have considered a large range for $M$ in our plots. We show all results for $\varepsilon=1.0$ and $\delta=1e-6$. Our results in \cref{fig:hist-vary-K-T} show a range of vocabulary sizes $K$ (varying left to right from 1000 to 100,000), and number of tasks solved $T$ (varying top to bottom from $T=1$ to $T=1000$). As we increase $K$, the statistical squared error of $\approx 1/M$ is unaffected, whereas the error due to privacy noise increases as $K\sigma^2 / M^2$.
As we increase $T$, the privacy budget available per task decreases as $\approx 1/\sqrt{T}$, thus leading $\sigma \approx \sqrt{\log M\frac 1 \delta} / N\eps_{\mbox{task}} \approx \sqrt{MT\log \frac 1 \delta}{N}$. The precise value of $\sigma$ is computed using tight numerical accounting tools, as these formulae are only valid for a certain range of parameters. Our plots show that the improvement over an aggregator alone continues to hold over a large range of parameter and is specially pronounced for large $T$ and $K$.

For the case of sparse histogram, the fraction of values that are non-default is an additional parameter. We show the same plots as the histograms case above, for $\gamma=0.1$ (\cref{fig:sparse-hist-vary-K-T-0.1}), $\gamma=0.01$ (\cref{fig:sparse-hist-vary-K-T-0.01}) and $\gamma=0.001$ (\cref{fig:sparse-hist-vary-K-T-0.001}). Once again, these plots demonstrate that the gain from our improved primitive are significant across a large range of parameters. When $\gamma$ is very small, there is fewer users to learn from, leading to larger privacy costs.
% Figure environment removed


% Figure environment removed

% Figure environment removed

% Figure environment removed

\end{document}
