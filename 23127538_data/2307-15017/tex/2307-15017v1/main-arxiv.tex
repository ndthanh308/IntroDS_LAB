\documentclass[letterpaper]{article}
\usepackage{fullpage}
\usepackage{ktmacros}
\pdfoutput=1
\usepackage[backend=bibtex,maxnames=30,maxbibnames=30,maxcitenames=30,maxalphanames=30,giveninits=true,doi=false,url=true]{biblatex}
\newcommand*{\citet}[1]{\AtNextCite{\AtEachCitekey{\defcounter{maxnames}{2}}} \textcite{#1}}
\newcommand*{\citetall}[1]{\AtNextCite{\AtEachCitekey{\defcounter{maxnames}{999}}} \textcite{#1}}
\newcommand*{\citep}[1]{\cite{#1}}
\newcommand{\citeyearpar}[1]{\cite{#1}}

\bibliography{refs}

\newcommand{\agg}{\ensuremath{\mathcal{A}}}
\newcommand{\minbatch}{\ensuremath{B}}
\newcommand{\Dec}{\ensuremath{Dec}}
\newcommand{\sa}{\ensuremath{\mathcal{SA}}}
\newcommand{\SAA}{\ensuremath{\textsc{SA}_2}}
\newcommand{\gf}{\mathbb{F}}
\newcommand{\rappork}{{\sc Rappor}$_K$}

\title{Samplable Anonymous Aggregation for Private Federated Data Analysis}
\author{Kunal Talwar\thanks{Apple} \footnote{Corresponding Author. \texttt{ktalwar@apple.com}}
\and Shan Wang\footnotemark[1]
\and Audra McMillan\footnotemark[1]
\and Vojta Jina\footnotemark[1]
\and Vitaly Feldman\footnotemark[1]
\and	Aine Cahill\footnotemark[1]
\and	Yi Sheng Chan\footnotemark[1]
\and	Mike Chatzidakis\footnotemark[1]
\and	Junye Chen\footnotemark[1]
\and	Oliver Chick\footnotemark[1]
\and	Mona Chitnis\footnotemark[1]
\and	Suman Ganta\footnotemark[1]
\and	Yusuf Goren\footnotemark[1]
\and	Filip Granqvist\footnotemark[1]
\and	Kristine Guo\footnotemark[1]
\and	Frederic Jacobs\footnotemark[1]
\and	Omid Javidbakht\footnotemark[1]
\and	Albert Liu\footnotemark[1]
\and	Richard Low\footnotemark[1]
\and	Dan Mascenik\footnotemark[1]
\and	Steve Myers\footnotemark[1]
\and	David Park\footnotemark[1]
\and	Wonhee Park\footnotemark[1]
\and	Gianni Parsa\footnotemark[1]
\and	Tommy Pauly\footnotemark[1]
\and	Christian Priebe\footnotemark[1]
\and	Rehan Rishi\footnotemark[1]
\and	Guy N. Rothblum\footnotemark[1]
\and	Michael Scaria\footnotemark[1]
\and	Linmao Song\footnotemark[1]
\and	Congzheng Song\footnotemark[1]
\and	Karl Tarbe\footnotemark[1]
\and	Sebastian Vogt\footnotemark[1]
\and	Luke Winstrom\footnotemark[1]
\and	Shundong Zhou\footnotemark[1]
}
\date{}
\begin{document}
\maketitle
\begin{abstract}
We revisit the problem of designing scalable protocols for private statistics and private federated learning when each device holds its private data. Our first contribution is to propose a simple primitive that allows for efficient implementation of several commonly used algorithms, and allows for privacy accounting that is close to that in the central setting without requiring the strong trust assumptions it entails. Second, we propose a system architecture that implements this primitive and perform a security analysis of the proposed system.
\end{abstract}


\section{Introduction}
Learning aggregate population trends can allow for better data-driven decisions, and application of machine learning can improve user experience. Compared to learning from public curated datasets, learning from a larger population offers several benefits. As an example, a next-word prediction model trained on words typed by users (a) can better fit the actual distribution of language used on devices, (b) can adapt faster to shifts in distribution, and (c) can more faithfully represent smaller sub-populations that may not be well-represented in curated datasets. At the same time, training such models may involve sensitive user data. This tension has led to increasing interest in cross-device federated learning  and analytics\footnote{Federated data analytics (such as collecting statistics and telemetry), and federated learning (such as training ML models) are the two most common forms of federated data analyses. While everything we say in this work applies to both kinds of analyses, for brevity we will often use federated learning when we mean federated learning and federated analytics.}, and such systems have been deployed at scale~\cite{ErlingssonPK14, Apple2017, DingKY17, ENPA:2021, ZhangRXZZK23}.

While federated learning allows for data minimization, it by itself may not be sufficient to ensure user privacy. Intuitively, for federated learning to learn a useful model using gradients, the gradients must contain a lot of information about individual data points. As a simple example, the gradient for a logistic regression model is simply a scaled version of the feature vector. A long line of work~\cite{ShokriSSS17,NasrSH19, CarliniLEKS19, Feldman20, FeldmanZ20, CTWJ+21, CIJL+23}, has shown that models trained using gradients can memorize training data. Differential privacy~\cite{Dwork:2006, DworkR14} can address the individual privacy concerns by provably preventing memorization of data that is unique to one or a few users. Private gradient-based learning algorithms add sufficient noise to gradients (aggregated over a batch) to mask individual contributions. In the setting of a trusted curator, one can use centrally differentially private algorithms for data analysis (e.g. ~\cite{DworkR14,Kasiviswanathan:2008}) and machine learning (e.g. ~\cite{ChaudhuriMS11,BassilyST14,DLDP}). It is natural to aim to combine differential privacy with federated learning, and indeed private federated learning (PFL) and analytics systems aim to compute differentially private (DP) models and statistics in a federated setup.

Natural implementations of private learning algorithms in the federated setup however require placing a fair bit of trust in the server enabling this computation. Indeed~\cite{BoenischDSSSP23a, BoenischDSSSP23b} show that an adversarial server can compromise individual privacy in some commonly-used setups. While secure multi-party computation algorithms can allow for perfect simulation of the central DP algorithms in a distributed setup, the general techniques here are often difficult to scale and make sufficiently robust. Trusted execution environments (TEEs) offer different trade-offs. While they allow for smaller overheads for machine learning tasks~\cite{TramerB19}, the trust assumptions of TEEs are challenged by various attacks on them~\cite{BulckMW+18}. The desire to reduce the trust assumptions in PFL has led to significant interest in the design and implementation of specific primitives, that are useful enough for a variety of DP algorithms, yet simple enough for secure and scalable implementations. Secure aggregation~\cite{ODOpaper, Bonawitz17,unlynx,crypteps,honeycrisp,orchard} and shuffling~\cite{Bittau17,ErlingssonFMRTT19,CheuSUZZ19} are two prominent examples of such primitives.

Our first contribution is to propose a new, more powerful primitive that we call Samplable Anonymous Aggregation. We show that this primitive is powerful enough to allow for strong differential privacy guarantees for some of the commonly-used algorithms for private learning and statistics. For private histograms, it allows us to get privacy-utility trade-offs that are close to those achievable centrally with a trusted curator. For private learning, it allows us to get privacy bounds close to those achieved by the moments accountant~\cite{DLDP} in the central setting; indeed it is the first federated learning primitive that is able to achieve this.

As our second contribution, we describe the architecture of a scalable system that can implement this primitive under suitable trust assumptions. Our proposed architecture builds on the Prio system~\cite{Corrigan-GibbsB17}, where clients use additive secret-sharing to share their data to two or more servers. As long as at least one of the servers is honest, Prio ensures relevant security properties. Our proposed architecture combines Prio with additional elements including client-side sampling, and anonymization and device authentication infrastructures to implement the desired primitive.  We critically analyze the security aspects of the architecture and proposed additional hardenings that can make it difficult to violate the trust assumptions of the system.


We now give additional details of our contributions. Our proposed primitive of {\em Samplable Anonymous Aggregation} specifies the ideal functionality that we desire. It being an Aggregation means that we compute an aggregate of (some) client contributions. Samplability means that this aggregate can be computed over a random subset of the clients. Anonymity here means that this subset over which we aggregate stays hidden: the adversary only learns the sum over a random subset, but does not know whether any specific client is in this random subset. We defer the precise definition to \cref{sec:sa2}.

Compared to the shuffling primitive~\cite{Bittau17}, aggregation can allow for better privacy guarantees, particularly for private federated learning where there is a large asymptotic gap~\cite{CheuSUZZ19, BalleBGN19a} in the privacy-utility tradeoffs of aggregation vs. (single-message) shuffle for the problem of mean estimation. Compared to the previously studied primitive of Aggregation~\cite{ODOpaper,Bonawitz17, BellBGLR20}, samplability allows for better privacy analysis due to {\em Privacy Amplification by Sampling}~\cite{Kasiviswanathan:2008}.
These privacy gains are significant. As we show in~\cref{exa:pfl-sampling}, for typical parameters used in cross-device settings, privacy amplification by sampling can help improve the differential privacy guarantees from $\eps \approx 100$ to $\eps \approx 1$.

Our proposed implementation of this primitive (detailed in \cref{sec:architecture}) uses a split-trust model, where we rely on two or more non-colluding servers. Effectively, the clients secret share their data to two servers, which can then run a multi-party computation to evaluate the aggregate. An alternate approach, used in the SecAgg protocol~\cite{Bonawitz17}, is to run MPC amongst the clients themselves. While recent works~\cite{SoGA20,BellBGLR20} have shown that this approach can scale, it is unclear how to ensure strong anonymity in this approach. Moreover, this approach requires multiple rounds of interaction involving client devices. Our protocol needs a single message from each client, thus avoiding the complications due to churn and dropout that are inherent in multi-round protocols. A different approach using Trusted Execution Environments (TEEs) has been proposed for implementing shuffling~\cite{Bittau17} and aggregations~\cite{papaya}. This approach is difficult to scale, adapt and maintain, and requires secure hardware which may itself have vulnerabilities as discussed above.

In addition to non-colluding servers, our approach relies on an anonymization infrastructure that ensures that the messages received at the server cannot be linked to their sender. Using such an infrastructure allows us to get {\em strong anonymity} that ensures that even the {\em set} of clients contributing to a collection remains hidden from the server. To ensure that contributions are coming from genuine clients, we propose using an anonymous rate-limited device authentication method such as Privacy Pass~\cite{ietf-privacypass-rate-limit-tokens-01}.

A large-scale deployment of a PFL system may be subject to attacks from adversarial client devices, that may seek to poison the computation, or attempt to manipulate the learnt statistics or the learnt model to behave in a certain way~\cite{BagdasaryanVHES20,BhagojiCMC19, SunKSM19,BaruchBG19,WangSRVASLP20,CheuSU19}. A client that sends, say, gradients based on valid data that is different from their actual data may however be difficult to distinguish from an honest client. A robust system should limit the potential impact of an adversarial client. Thus for example, an attacker controlling tens of clients should not be able to distort a count by thousands. Prio allows devices to send zero-knowledge proofs of {\em validity} that can be verified by the server to guarantee this robustness to poisoning. Recent advances in efficient proofs in this framework~\cite{BBCGI19,BonehBCGI21, BellGGKMRS22, CastroP22,Talwar22,AddankiGJOP22,RatheeSWP22,BonehBCGI23}, enable this robustness with small computational and communication overheads.

The rest of the paper is organized as follows. We start with some differential privacy background in \cref{sec:prelims}, and present the formal definition of our primitive in \cref{sec:sa2}. We describe the proposed architecture for implementing the primitive, along with its security analysis and additional hardenings in~\cref{sec:architecture}. \cref{sec:discussion} discusses properties of the primitive and additional enhancements.
We discuss additional related work in \cref{sec:related} and conclude with some open directions in \cref{sec:conclusions}.

\section{Preliminaries}
\label{sec:prelims}

Differential privacy~\cite{Dwork:2006} defines a way to measure the individual privacy loss of a randomized algorithm.
Intuitively, an algorithm is differentially private if the distribution of outputs is reasonably stable with respect to a single individual changing their data. There are several ways to formalize the notion of closeness of distributions that are commonly used to define variants of differential privacy (DP). The most popular are the hockey-stick divergence, used to define $(\eps,\delta)$-differential privacy, and the R\'enyi divergence, used to R\'enyi differential privacy (RDP).

\begin{definition}[Hockey-stick divergence]
The {\em hockey-stick} divergence between two random variables $P$ and $Q$ is defined by: \[\dalpha{e^{\eps}}(P\|Q) = \int \max\{0, P(x)-e^{\eps} Q(x)\} dx,\] where we use the notation $P$ and $Q$ to refer to both the random variables and their probability density functions. We say that $P$ and $Q$ are $(\eps, \delta)$-indistinguishable if $\max\{\dalpha{e^{\eps}}(P\|Q), \dalpha{e^{\eps}}(Q\|P)\}\le\delta$. \end{definition}

\begin{definition}[R\'enyi divergence]
For two random variables $P$ and $Q$, the \emph{R\'enyi divergence} of $P$ and $Q$ of order $\alpha>1$ is \[D^{\alpha}(P\|Q) = \frac{1}{\alpha-1}\ln \E_{x\sim Q}\left[\left(\frac{P(x)}{Q(x)}\right)^{\alpha} \right] .\]
\end{definition}

We say that two databases are neighboring if one can be obtained from the other by the addition or deletion of the data of a single individual. Another notion is based on replacement, where two databases are neighboring if one can be obtained from the other by replacing the data of one individual. As these are related up to a factor of two, we will not dwell on this distinction and state results for one or the other when appropriate. We can now define differentially private algorithms with respect to both the hockey-stick divergence, and R\'enyi divergence.

\begin{definition}[Central DP]\cite{Dwork:2006}\label{centralDP}
An algorithm $\cA:\cD^n\to\output$ is $(\eps, \delta)$-\emph{differentially private} if for all neighboring databases $X$ and $X'$, $\cA(X)$ and $\cA(X')$ are $(\eps, \delta)$-indistinguishable. If $\delta=0$, we refer to $\cA$ as satisfying $\eps$-DP, and say the algorithm satisfies \emph{pure differential privacy}. If $\delta>0$, then we say $\cA$ satisfies \emph{approximate differential privacy}.

An algorithm $\cA:\cD^n\to\output$ is $(\alpha, \rho(\alpha))$-\emph{R\'enyi differentially private} if for all neighboring databases $X$ and $X'$, $D^{\alpha}(\cA(X)\| \cA(X'))\le\rho(\alpha)$.
\end{definition}

The two variants of differential privacy are related. Any algorithm that satisfies pure DP also satisfies RDP. If an algorithm satisfies RDP, then it also satisfies approximate DP. We give the formal conversion statements in the following theorem.

\begin{theorem}\cite{Bun:2016, mironov2017renyi, CanonneKS20}
If $\cA:\cD^n\to\output$ is $\eps$-DP, then for any $\alpha>1$, $\cA$ satisfies $(\alpha, \frac{1}{2}\eps^2\alpha)$-RDP. Conversely, for any $\delta\in(0,1]$, if $\cA$ is $(\alpha, \eps)$-RDP then it is $(\eps~+~\frac{\log(1/\delta)+(\alpha-1)\log(1-1/\alpha)-\log(\alpha)}{\alpha-1}, \delta)$-DP.
\end{theorem}

Differential privacy satisfies two key properties that allow for private building blocks to be combined to design more complex algorithms. First, it satisfies {\em post-processing}: applying an arbitrary data-independent map does not increase the privacy cost. Second, it degrades smoothly as we adaptively compose multiple private algorithms. {\em Composition} theorems allow us to analyze the privacy cost of a sequence of private algorithms. There are different versions of these composition results, and other DP results in this work, for different DP variants. While they are typically similar in their aymptotics, their numerical constants can differ. We will aim to present the simplest variant for each DP result in this section. For practical privacy accounting, one would often use other variants of these results.

\begin{theorem}[Composition \cite{mironov2017renyi}\label{compositionofRDP}]
Let $\cA_1:\cD^n\to\output_1$ be an $(\alpha, \rho_1(\alpha))$-RDP algorithm, and $\cA_2:\cD^n\times\output_1\to\output_2$ be an $(\alpha, \rho_2(\alpha))$-RDP algorithm, then the algorithm defined by $X\mapsto(A,B)$ where $A=\cA_1(X)$ and $B=\cA_2(X,A)$ is $(\alpha, \rho_1(\alpha)+\rho_2(\alpha))$-RDP.
\end{theorem}

One can easily generalize Theorem~\ref{compositionofRDP} to adaptively compose any number of RDP algorithms. A similar theorem exists for composing approximate DP algorithms.  Asymptotically, and in practice, the tightest privacy bounds on practical DP algorithms are derived by composing the RDP guarantees and converting them to approximate DP.

\subsection{Privacy Amplification by Sampling}\label{subsampling}

Sampling is a common primitive used in data collection systems for a variety of reasons, including balancing communication and computation costs associated with large scale data analysis.
Choosing to include only a sample of the available users in the differentially private computation provides additional privacy, by giving each user plausible deniability about whether their data was included in the computation or not. This can be formalized for a variety of sampling methods including sampling-with-replacement (selecting a random subset of a fixed size) and Poisson sampling (selecting each data point independently with a certain probability) in both approximate and R\'enyi DP~\cite{Kasiviswanathan:2008, Ullman:2017, Balle:2018, WangLF16b, Wang:2021}. Below we state the privacy amplification by sampling bound for Poisson sampling for approximate differential privacy.

\begin{theorem}\label{samplingWOR}\cite{Kasiviswanathan:2008}
Let $\cA$ be an $(\eps,\delta)$-DP mechanism, and $\gamma\in[0,1]$ be a sampling rate. Given a data set $X$, let $\cA_s$ be the algorithm that first samples a subset $X'$ by including each data point in $X$ with probability $\gamma$, then outputs $\cA(X')$. Then $\cA_s$ is $(\eps',\delta')$-DP for $\eps'=\log(1+\gamma(e^\eps -1))$ and
$\delta'=\gamma\delta$.
\end{theorem}

Note that when $\eps$ is small, $\eps'\approx \gamma\eps$, so the privacy guarantee is, roughly, scaled by the sampling rate. Similar bounds exist for sampling-with-replacement, and R\'enyi DP, and numerical bounds for these for specific mechanisms of interest are implemented in various libraries\cite{DLDP,Mironov2019RnyiDP,tfprivacy, yousefpour2021opacus}.


\begin{example}[Gaussian and Subsampled Gaussian Mechanisms]
Let us consider a particular differentially private algorithm, and the impact that Poisson sampling has on this mechanism.
The \emph{Gaussian mechanism} is a DP algorithm that hides the impact of a single individual by adding Gaussian noise to the output of the computation. Given $f:\cD^n\to\mathbb{R}^d$ a deterministic function, define the sensitivity of $f$ to be $\Delta_f = \max_{X,X' \mbox{ neighbors}}\|f(X)-f(X')\|_2$ to be the maximum amount that the output of the function can vary on any pair of neighboring datasets $X$ and $X'$. The Gaussian mechanism on input $X$ outputs $f(X)+\mathcal{N}(0, \Delta_f^2 \sigma^2 \mathbb{I}_d)$.
It satisfies $(\eps,\delta)$-DP for $\sigma \geq \sqrt{2 \ln 1.25/\delta} / \eps$, and $(\alpha, \alpha/2\sigma^2)$-RDP.

Further, given $q \in [0,1]$, the algorithm that first samples a subset $X'$ by including each user with probability $q$, then outputs $f(X')+\mathcal{N}(0, \Delta_f^2 \sigma^2 \mathbb{I}_d)$ is $(2q\eps, 2\delta)$-DP and $(\alpha, q^2 \alpha / 2\sigma^2)$-RDP as long as $\alpha$ is suitable bounded~\cite{DLDP,Mironov2019RnyiDP}.

These analytic bounds can be improved by numerical methods. We can use bounds from the Analytic Gaussian Mechanism~\cite{Balle2018ImprovingTG}, and the moments accountant~\cite{DLDP}, and will use those in subsequent examples in this work.
\end{example}

\subsection{Local Differential Privacy}

Definition~\ref{centralDP} is commonly referred to as the \emph{central model} of differential privacy, where the curator (or server) has direct access to the data. Local DP algorithms constrain the whole communication from each client itself to be differentially private. In the fully adaptive case, they can communicate with the server in an arbitrary order with adaptive interaction. Formally, a protocol satisfies local $(\eps,\delta)$-DP if the transcripts of the interaction on any two pairs of neighbouring datasets are $(\eps,\delta)$-indistinguishable. When there is only a single round of interaction, the condition on transcripts reduces to each user interacting with the server using a mechanism that is differentially private with respect to that user's data. We call such mechanisms for the local reports of a user \emph{local randomizers.}

\begin{definition}[Local randomizer]\label{localrandomizer}
An algorithm $\lr\colon \cD\to \output$ is an $(\eps, \delta)$-DP \emph{local randomizer}  if there exists a reference distribution $\mathcal{R}$ such that
for all data points $x\in \cD$, $\lr(x)$ and $\mathcal{R}$ are $(\eps, \delta)$-indistinguishable.
\end{definition}

Formally, an adaptive single pass $(\eps,\delta)$-DP local protocol can be described by a sequence of local randomizers $\Aldp[i]:\out[1]\times\cdots\times\out[i-1]\times\mathcal{D}\to\out[i]$ for $i\in[n]$, where $\cD$ is the data domain, $\out[i]$ is the range space of $\Aldp[i]$ and the $i$-th user returns $z_i=\Aldp[i](z_{1:i-1}, x_i)$. We require that the local randomizer $\Aldp[i](z_{1:i-1}, \cdot)$ be $(\eps,\delta)$-DP for all values of auxiliary inputs $z_{1:i-1}\in\out[1]\times\cdots\times\out[i-1]$. We can also define local randomizers with respect to R\'enyi DP, however throughout this paper, we will focus on $(\epsilon, \delta)$-DP local randomizers.

\begin{example}[{\sc Rappor}$_{K}$.] Given an input $x \in [K]$, the mechanism \rappork\ first transforms $x$ to a $K$-dimensional $0$-$1$ vector $e_x$, which is $1$ in the $x$th coordinate and $0$ elsewhere. It then flips each bit of $e_x$, independently, with probability $\frac{1}{e^{\eps_0}+1}$ to get a $0$-$1$ vector. This mechanism satisfies $(\eps_0,0)$-DP in the local model~\cite{ErlingssonPK14}.
\end{example}

The aggregate and shuffle models of differential privacy are distributed models of computation where the clients hold their own data and interact with the server in a federated model through an implementation of the appropriate primitive. We will discuss the impact of these primitives in the following section.

\paragraph{Notation.} $\sum$ always refers to a commutative associative addition operation, such as summation over reals or over a finite field. We abuse notation and use sets to mean {\em multisets}.

\section{Aggregation Systems and their Implications}
\label{sec:sa2}
Towards our final definition, we start by defining a simpler primitive: an aggregation functionality. While such a functionality has been defined in previous work, we define a version that is slightly more general.

Our aggregator $\agg$  will be parameterized by a {\em cohort size} \minbatch. This parameter specifies the minimum number of contributions that must be aggregated before release. This would typically correspond to the batch size in a private federated learning setting, and we will often use batch and cohort interchangeably. The aggregator is also parameterized by a decoding function \Dec, that prescribes how messages from users are to be transformed before aggregating. It receives messages from a set of users, decodes each of them using \Dec, and outputs the sum of the decodings, as long as at least \minbatch\ messages are received.
Formally, it receives $m_1,\ldots,m_k$ for some $k$, and returns

\begin{align*}
    \agg_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}) = \left\{\begin{array}{ll} \sum_{i=1}^k Dec(m_i) & \mbox{if } k \geq \minbatch\\ \bot &\mbox{otherwise} \end{array} \right.
\end{align*}
When \minbatch\ and \Dec\ are obvious from context, we will often just write \agg\ to mean $\agg_{\minbatch}^{\Dec}$.
A single-round protocol in the Aggregator model is defined by a randomizer $R$, and the decoding function \Dec\ used in the aggregator. We say that the protocol $(R, \Dec, \minbatch, \agg)$ is $(\eps,\delta)$-DP (resp. $(\alpha, \rho(\alpha))$-RDP) in the aggregator model if for any $k$, $\agg_{\minbatch}^{\Dec}(R(x_1), \ldots, R(x_k))$ is $(\eps,\delta)$-DP (resp. $(\alpha, \rho(\alpha))$-RDP). Note that we treat $k$ as public in this definition.

A multi-round protocol in the Aggregator model is defined by a sequence of single-round protocols. The privacy cost of a multi-round protocol can be upper bounded by the sequential composition of the privacy cost of each of the single-round protocols. More generally, each user may choose to participate in only a subset of the single-round protocols, in which case better privacy bounds on the composition may be proven by a more careful analysis.

\begin{example}\label{exa:pfl-agg-only}
    Let $x_i \in \Re^d$ with $\|x_i\|_2 \leq 1$. Let $R(x_i) = x_i + \mathcal{N}(0, \frac{\sigma^2}{\minbatch} \mathbb{I}_d)$ and let $\Dec$ be the identity function. Then $\agg_{\minbatch}^{\Dec}(R(x_1), \ldots, R(x_k))$ is distributed as $\sum_i x_i + \mathcal{N}(0, \frac{k\sigma^2}{\minbatch} \mathbb{I}_d)$ whenever $k \geq \minbatch$ and equals $\bot$ otherwise. Then for $\eps = \frac{\sqrt{2\log \frac {1.25} \delta}}{\sigma}$, the protocol is $(\eps,\delta)$-DP in the aggregator model.
    As a concrete example, one can verify that for  $\sigma = 7$, this protocol is $(1, 10^{-8})$-DP in the aggregator model. Using the better numerical bounds from~\cite{Balle2018ImprovingTG}, the same privacy bound holds for $\sigma=5.1$.
\end{example}


\begin{example} \label{exa:fedstats}
Consider any $\eps_0$-local DP mechanism $R^{(\eps_0)}$, an arbitrary $\Dec$ and for $\delta>0$, let $\eps = \eps_{\texttt{shuffle}}(\eps_0, \minbatch, \delta) \le \ln\left(1+(e^{\eps_0}-1)\left(\frac{4\sqrt{2\ln(4/\delta)}}{\sqrt{(e^{\eps_0}+1)\minbatch}}+\frac{4}{n}\right)\right)$ be the bound on the privacy of shuffled version of the local responses proved in~\cite{FeldmanMT:2020}. Since aggregation is commutative and associative, the output of the aggregator
is a post-processing of the shuffled responses. For any $k \leq \minbatch$, the output is independent of the data and hence private. For $k \geq \minbatch$, the shuffling result applies and gives us $(\eps, \delta)$-DP in the aggregator model.
As a concrete example, for $\eps_0=4.0$ and $\minbatch= 10,000$, the numerical bounds in~\cite{FeldmanMT:2020} imply that this protocol satisfies $(0.61, 1e-10)$-DP. When $R^{\eps_0}$ is the {\sc Rappor}$_K$ algorithm~\cite{ErlingssonPK14}, the aggregation allows for asymptotically optimal reconstruction of histograms~\cite{FeldmanMT:2020}.
\end{example}

Note that the aggregator model assumes that all the $k$ clients are following the protocol. Any bound in the aggregator model extends to a bound in a robust version of this model, where at least $k\geq \minbatch$ of the clients follow the protocol (see~\cref{subsec:security}).
Moreover, note that from the point of view of one of the clients, the noise is coming from $(\minbatch-1)$ other clients rather than $\minbatch$. Since we  will deal with large values of $\minbatch$, we ignore this distinction for simplicity.


We next define the stronger primitive of Samplable Anonymous Aggregation. This primitive takes in an additional {\em sampling} parameter $q$. For inputs $x_1, \ldots, x_k$, the sampled aggregate is the aggregate as before of $\Dec(R(x_i))$s, but now taken only over a randomly chosen subset of indices. Formally, for a multiset $S$, let $S_{\downarrow q}$ be the random multiset defined by selecting each index independently with probability $q$. Then
\begin{align*}
    \sa_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}) &=  \agg_{\minbatch}^{\Dec}(\{m_1, \ldots, m_k\}_{\downarrow q})
\end{align*}

Note that in this definition, we have assumed that the aggregator only outputs the aggregate, but not the set of indices that contributed to the sum. This {\em secrecy of the sample} is what enables the privacy amplification by sampling analyses.

As before, a single round protocol $(R, \Dec, \minbatch, \agg, q)$ satisfies a certain differential privacy guarantee in the Samplable Anonymous Aggregator (\SAA) model if $\sa_{\minbatch}^{\Dec}(R(x_1),\ldots, R(x_k))$ satisfies the differential privacy guarantee for any $k$. A multi-round protocol will be analyzed by sequential (adaptive) composition of single round protocols, possibly taking into account that each user may not participate in each round.

\begin{example}\label{exa:pfl-sampling}
Consider a Private Federated Learning process where at each iteration, each user independently with probability $q$ decides whether or not to participate in this iteration. If it decides to participate, it sends its gradient, clipped to norm 1, with additive $\mathcal{N}(0, \frac{\sigma^2}{\minbatch})$ noise added. The aggregator outputs the sum of these noisy gradient updates. Then by privacy amplification by sampling results, it follows that this protocol satisfies $(\eps, \delta)$-DP in the \SAA\ model for $\eps \approx q\frac{\sqrt{2\log \frac {1.25} \delta}}{\sigma}$. These analytic bounds can be improved using numerical analysis techniques. As an example, if $\sigma=5.1$ and $q=0.02$, the protocol satisfies $(0.034,10^{-8})$-DP in the \SAA\ model (using~\cite{Balle2018ImprovingTG} and privacy amplification by sampling). Running this algorithm over $2,500$ iterations leads to an overall privacy cost of $(0.8, 10^{-8})$-DP in the \SAA\ model using the moments accountant. Note that this is a significant improvement over what one would get in the aggregation model. Indeed without amplification, the resulting $\eps$ is larger than $100$. The parameters above are in the same ballpark as used in large-scale deployments such as~\cite{xu2023federated} that lead to accurate models.

We also note that this analysis does not make any assumptions about the server generating the models on which the gradients are computed. Those may be chosen adversarially, and the privacy analysis continues to hold, as it only requires that each step is a sampled Gaussian.
\end{example}

\begin{example}\label{exa:fedstats-sampling} Consider the aggregation of \rappork\ outputs as in \cref{exa:fedstats}. Recall that with $\eps_0 = 4.0$ and $B=10,000$, the shuffled output satisfies $(0.61, 10^{-10})$-DP. Now suppose that we had $1,000,000$ clients, and each was sampled with probability $q=0.02$, and their outputs were aggregated. With respect to the dataset of $1,000,000$ clients, this output satisfies $(\eps, 10^{-10})$-DP for $\eps < 0.02$. Thus in the \SAA\ model, for these parameters, the algorithm satisfies $(0.02, 10^{-10})$-DP.
\end{example}

One can also define a version of this definition where the set is chosen by sampling a random subset of indices of a fixed size $\minbatch$. This distinction is akin to the different flavors of privacy amplification by sampling. These different models are close to each other:  when $\minbatch < qk - \sqrt{qk\log \frac 1 \delta}$, then the Poisson sample is of size at least $\minbatch$ with probability $(1-\delta)$. In this case, the distribution of the set of chosen indices in the Poisson case is a mixture of distributions over random subsets of a fixed size $B'$ for a random $B' \geq \minbatch$.


\section{Architecture of a Scalable Samplable Anonymous Aggregator}
\label{sec:architecture}
In this section, we describe an architecture for implementing the \SAA\ primitive. Our architecture is built atop Prio~\citep{Corrigan-GibbsB17} which allows aggregation using two or more servers, and ensures that as long as one of the servers is honest, nothing except the aggregate is learnt. Briefly, Prio uses additive secret sharing, where a contribution $m \in \gf_p$ gets shared to the different servers. In the case of two aggregation servers, the first server gets a uniformly random $r \in \gf_p$, and the other server gets $m-r \in \gf_p$. In line with the IETF Distributed Aggregation Protocol draft~\cite{ietf-ppm-dap-04}, we will refer to these two aggregation servers as {\em leader} and {\em helper}. The marginal distributions of the two shares are uniformly random, and thus individually contain no information about the contribution. Each of the two servers aggregates the shares from a group of clients, and the sums are then aggregated, giving the sum of the original messages (modulo $p$) as the result. Additionally, Prio allows the clients to send zero-knowledge proofs of {\em validity}, which allow the aggregation servers to learn that a contribution is valid without learning anything more about the contributions.

The architecture will build on a few additional components. The first component is a system for shipping {\em recipes} to devices. The recipe specifies the parameters $R, Dec, \minbatch, q$, as well as relevant information about the aggregation process. The device then tosses its own coins to decide on whether or not to contribute, and if so sends its contribution to the system. The contributions are routed through an anonymization mechanism. We also use an anonymous, rate-limited device authentication mechanism so that each device can only send one contribution to a collection. The contributions take the form of shares for the two aggregation servers, each encrypted using their respective public keys. The leader and helper implement a version of the Prio protocol, aggregating the secret shares, and only sharing aggregates once \minbatch\ many contributions are aggregated. We next describe these components in more detail.

\subsection{Recipe Publishing and On-device Sampling}

The recipe contains information needed to run the protocol. The randomizers $R$ itself may change from query to query; e.g. while $R$ may correspond to {\sc Rappor}$_K$, the mapping of the data on device to $[K]$, and the value $K$ itself needs to be specified in the recipe. The recipe contains the sampling rate, and the batch size, which allow the client device to verify that the protocol satisfies DP in the \SAA\ model for appropriate privacy parameters. The client device can then use its own randomness to determine whether or not to participate in the collection. If it does contribute, it runs the randomizer and computes an output. This output is then converted to a pair of secret shares as expected in Prio. The recipe also contains the public keys for the servers, which allows these shares to be encrypted so only the appropriate server can decrypt their shares. The recipes themselves can be publicly posted and logged on device, making them auditable. Additionally, the public keys can be posted on a certificate transparency log that the device checks, making them more reliable and manageable.

\subsection{Anonymization Infrastructure}
The privacy amplification by sampling in an \SAA\ pipeline depends on the adversary not learning the set of clients that actually contributes to the aggregate. To ensure this property, an anonymization infrastructure can be used. In addition to allowing for the strong anonymity needed in the \SAA\ primitive, this infrastructure  is also valuable in hardening the system. Further, an anonymization infrastructure can also be used by the devices to download the recipes themselves.

A natural way to ensure strong anonymity is for each device to send a contribution. Devices that choose not to contribute can instead contribute secret shares of {\em zero}, which will have no impact on the aggregate. The zero-knowledge property of Prio then ensures that we get strong anonymity. When the sampling rates are small, this approach however incurs a very large communication (and computational) overhead. Indeed the communication and server-side computational cost of noisy stochastic gradient descent in this setup would be as large as {\em full-batch} gradient descent, making it impractical when the number of devices is large.

A different approach is to distribute the trust using a multi-hop architecture as done in Onion routing~\cite{GoldschlagRS99,DingledineMS04} and other relay-based services~\cite{rfc9298,ietf-ohttp,privaterelay}.
These approaches can provide anonymity without relying on all devices having to send some communication, and are therefore orders of magnitude more efficient when the sampling rate is small. While our design is oblivious to the precise approach used to achieve anonymization, this approach is likely to be the more scalable one in most applications.

\subsection{Device Authentication and Rate Limiting}
A device authentication method can be used to ensure that each contribution comes from an actual client. Further, one can use rate-limiting techniques to ensure that a client cannot send arbitrarily many contributions to a collection. Such an approach must build on top of a device attestation framework (e.g~\cite{devicecheck, playintegrity}), or a user attestation framework. For this discussion, we assume that each device has an attestation token that can be validated, but may leak the identity of the user or the device.

Authentication of contributions from users/devices directly is not compatible with our anonymity goal. We thus need to authenticate devices in an anonymous way. We discuss two approaches to this. The first approach uses a Trusted Execution Environment (TEE) for device authentication. Each contribution comes with an attestation token, which is validated inside a TEE. The TEE also maintains a de-duplication data structure such as a bloom filter to prevent a device from contributing multiple times. The TEE then removes the identifiable attestation token, and signs the leader and helper shares. The leader and helper verify that every share they get has been signed by the TEE. The program running in the TEE can be published and attested to by the clients, enabling the client device to be sure that its contributions can only be decrypted by this program.

In effect, the TEE in the first approach exchanges an identifiable attestation token for {\em one} non-identifiable token that can be validated by the leader or helper. This functionality of anonymous rate-limited attestation can itself be implemented using blind signatures in a split trust model, as done in the IETF Privacy Pass proposal on rate-limited tokens~\cite{ietf-privacypass-rate-limit-tokens-01}. In brief, the trust is split between two parties, one of which (the {\em attester}) can validate the (non-anonymous) device attestation and rate-limit, whereas the other (the {\em issuer}) can issue tokens meant for a certain target use case. Importantly, the attester learns nothing about the use case, nor the token, and the issuer learns nothing about the identity of the device requesting the token. The token is anonymous and unlinkable, but can be verified by the aggregation servers (leader/helper) to ensure that a device has been authenticated/attested and rate-limited without learning the device's identity. Indeed each of the aggregation servers can verify that the token is valid, and has not been used before.

In our set up, the time-of-issue and time-of-use can be made uncorrelated by having each device request tokens at a time that is independent of when they need to use them, and cache these tokens. If each device requests these tokens at a random time, the token issuance time carries no information. This can make the system resilient to one of attester/issuer colluding with one of leader/helper.

This device authentication ensures that an adversary that controls a small number of devices can only control a small number of contributions to a collection.

\subsection{Share Aggregation}
The contributions coming from the device consist of two shares, where each share is encrypted using the public key of the relevant server. The shares are sent to the respective server and each server aggregates their shares. Alternately, the encrypted shares can be sent to the leader, which can batch and forward the helper shares. The leader can collect the aggregates once the total number of contributions aggregated is at least $B$.
Additional steps are needed to ensure that the leader and the helper are aggregating over the same set of contributions, so that a dropped message does not lead to the aggregate being arbitrarily corrupted. Note that while the leader and helper may end up selecting a subset of contributions to aggregate (e.g. dropping some contributions due to network packet loss or corruptions), the protocol still requires that the number of contributions that are successfully aggregated is at least $B$. The IETF draft on Distributed Aggregation Protocol (DAP)~\cite{ietf-ppm-dap-04} describes a specification of such a system, and one may follow the protocol specification to do this aggregation. As long as the helper (resp. leader) does not share any aggregates smaller than the minimum batch size, the privacy property of the system holds against a potentially malicious leader (resp. helper).

\subsection{Verification of Contributions}
\label{subsec:verification}
The use of a Prio-based systems gives us the additional ability to ensure that malicious contributions cannot arbitrarily derail the computation. This robustness, while not needed for privacy, ensures that the aggregates are not easily corrupted by a few adversarial client contributions. Importantly, the validation preserves the zero-knowledge property: neither the leader nor the helper learns any additional information about the client contribution, except for its validity. A client following the protocol will always send a valid contribution, so that the validity predicate contains no information for an honest client. Thus this verification comes at no privacy cost for honest clients.

While arbitrary validity predicates can in principle be verified, efficient algorithms exist for several important cases of practical interest. For example, when $R$ is the {\em randomized response} mechanism, the output is a $0$-$1$ vector with exactly one $1$. \citet{BonehBCGI21} show how to validate this property with at most $O(\sqrt{K})$ additional communication. Similarly, the output of {\sc Rappor} is a $0$-$1$ vector where the number of $1$s is bounded by $\tilde{O}(1+\frac{K}{e^{\eps_0}+1})$ with high probability\footnote{The $\tilde{O}(\cdot)$ hides factors that are logarithmic in the failure probability.}. Verifying this property, which again can be done efficiently using ~\citep{BonehBCGI21}, helps limit the impact of an adversarial client on the final aggregate. Similarly, when using the Gaussian noise mechanism as the randomizer $R$, the output, when viewed as a $d$-dimensional real vector, has bounded Euclidean norm with high probability. Several recent works~\cite{Talwar22,libprio,pine} study this question and show that this property can be efficiently verified with little overhead. Some of these protocols have been formalized as standards in the IETF draft on Verifiable Distributed Aggregation Functions~\cite{irtf-cfrg-vdaf-06}.

\subsection{Security Analysis}
\label{subsec:security}
In this section, we argue that the proposed system implements an \SAA\ primitive, under suitable assumptions on the adversary. We start with defining our adversary model.

\medskip\noindent{\bf Adversary Model:} We allow an adversary to control at most one of the leader or helper, and assume that the other aggregation server honestly follows the protocol. Additionally, we allow the adversary to control at most $B/2$ of the client devices.

We next argue that the our proposed system provides the privacy guarantees of the \SAA\ primitive.
\begin{theorem}
    Suppose that an adversary controls at most $k'$ client devices, and at most one of leader and helper. Further assume that the device authentication and anonymous channel ensure that each device is limited to one contribution that is not linkable to the device. Then for some $B'$ and some $\tilde{B}\geq B-k'$, the view of the adversary can be simulated given $B'$ and $\sa_{\tilde{B}}^{\Dec}(M)$ where $M$ is the set of messages from clients not controlled by the adversary.
\end{theorem}
\medskip\noindent{\bf Proof Sketch:}
    We assume that the adversary controls the leader; the proof for the helper case is similar. First suppose that $k'=0$. Then each client sends a message $m_i = R(x_i)$ to the server with probability $q$. Let $I$ be the set of clients that select themselves, and let $\hat{M}$ be the set of their messages. If there are fewer than $B$ messages received, the honest helper aborts and no information is learnt by the adversary except $|I|$ (the $|I|$ secret shares can be trivially simulated). If the number of received messages is $|I| \geq B$, then the zero knowledge property of the Prio protocol ensures that the view of the leader can be simulated from the aggregate of the messages in $\hat{M}$. The set $\hat{M}$ itself is a uniformly random subset of size $|I|$ and the anonymization infrastructure ensures this is still true from the point of view of the adversary. Thus the aggregate available to the leader is indeed distributed as $\sa_{|I|}^{\Dec}(M)$. This implies the claim for the case of $k'=0$, with $B'=|I|$ and $\tilde{B} = \max(B, B')$.

    When $k' > 0$, then the adversary controls at most $k'$ of the client devices. The device authentication ensures that it can control at most $\tilde{k} leq k'$ contributions, and it can simulate its view of those contributions itself. The above argument is now applied to the $|I| - \tilde{k}$ contributions not controlled by the adversary. Since the adversary now knows $\tilde{k}$ of the messages $R(x_i)$, the aggregate over the remaining $|I|-\tilde{k}$ messages from honest clients suffice to allow the simulator to compute the aggregate over all $|I|$ messages. The rest of the simulation is now identical to the $k'=0$ case. Setting $B' = |I|$, and $\tilde{B} = \max(B, B') - \tilde{k}$ now completes the proof.
\qed

We note that the additional leakage $B'$ here is independent of the data $\{x_i\}_{i=1}^k$ held by the clients. Further, by having each device that does not select itself send a dummy report with probability $q' \approx \frac{1}{\eps k}$, we can ensure that $B'$ is a differentially private estimate of the number of devices that actually contribute. Thus the information contained in $B'$ can be hidden, and one can argue that there is a simulator whose output is $(\eps,\delta)$-indistinguishable from the adversary's view.

Further, note that for the sampling-without-replacement flavor of \SAA, the leakage of $B'$ is irrelevant. \knote{Recheck whether this has been defined.} Indeed the output of our protocol is equivalent to (a) sampling $B'$ for a Binomial distribution with appropriate parameters (Bin($k-k'$,$q$)), and (b) sampling an output from the sampling-with-replacement version of \SAA\ picking exactly $B'$ indices. Thus analyses of this sampling-with-replacement version of \SAA\ apply to the proposed system without the need for the dummy contributions.

The above theorem only addresses the privacy of our protocol. If one of the two servers is adversarial, the computed aggregate can be arbitrarily corrupted and the system gives no utility. When the contributions are verified as discussed in~\cref{subsec:verification}, and both servers are honest, the system is robust to adversarial clients: the impact of $k'$ adversarial clients is limited to the amount that $k'$ valid contributions can influence the aggregate. This follows from the soundness of the zero knowledge proof of validity. The precise definition of validity here, and thus this robustness depends on the specific randomizer and the validity test that is used.

Note that in the argument above, the strong anonymity property comes from the use of an anonymization subsystem. In the absence of this subsystem (or against an attacker that can compromise the anonymization infrastructure), the system implements an aggregator primitive. The privacy amplification by sampling bounds in~\cref{exa:pfl-sampling} and \cref{exa:fedstats-sampling} will not hold in this case, and the privacy assurance against this attacker will degrade to that implied by aggregation alone (\cref{exa:pfl-agg-only} and \cref{exa:fedstats}). The anonymity thus helps improve the privacy guarantee in cases where the sampling rate is small.

\subsection{Additional Hardening}
Our security analysis proves that our system implements \SAA\ as long as an attacker controls at most one of the aggregation servers, and a bounded number of devices contributing to the batch. We next discuss various hardenings to further mitigate the risk of an attacker controlling a large number of contributions.
\subsubsection*{Public Recipes and Server Information} The parameters of each collection, including information about leader and helper parameters, can be placed on a public transparency log. This information would be auditable, and this would ensure that every client sees the same recipe.
This hardening makes it infeasible for the server to prevent sufficiently many genuine clients from contributing to the collection. Coupled with anonymity, it helps prevent an adversarial server from singling out a single device.

\subsubsection*{Rate validation by leader and helper} Based on the recipe, and the number of devices, the leader and the helper have an expectation on the rate at which contributions arrive. This rate can also be estimated based on historical trends for similar recipes. A noticeably higher rate of contributions may suggest a sybil attack and the leader and helper may validate that the rate is within reasonable bounds. As an example, the leader (or helper) could reject batches when the rate of contributions exceeds twice the expected rate in a certain window. This gives a fair bit of room for natural fluctuations in the rate of contributions. At the same time, if the non-attack contribution rate is close to its expected value, this ensures that at least half of the contributions come from devices not controlled by an attacker. As our privacy guarantees are robust, this would mean that the privacy parameter against the attacker that controls half the batch is worse by a factor of about $\sqrt{2}$.




\subsubsection*{Limited Logging and Periodic Sub-Aggregation}
In the case that an attacker can compromise both the leader and the helper, the privacy level falls to that ensurable by shuffling and sampling. In typical federated statistics applications, this is a reasonably strong guarantee; indeed the privacy parameters computed in \cref{exa:fedstats} depend on shuffling alone. In the federated learning setting, the shuffling guarantee is typically weaker and susceptible to attacks of the kind discussed in~\cite{BoenischDSSSP23a, BoenischDSSSP23b}.  The leader and helper maintain no state beyond the collection and so a security breach at the two would have to be relatively simultaneous. Further the protocol can periodically create sub-aggregates, and discard the raw shares that have been aggregated already. This can reduce the privacy risk to the already sub-aggregated contributions, as the sub-aggregate provides some privacy protection depending on the number of contributions it aggregates.

More broadly, we aim to minimize logging at all steps. The client device does not send any telemetry that may indicate participation in a collection. While this may make it harder to debug the system, reduced logging makes the system more robust and minimizes data leakage in case of attacks.


\subsubsection*{Reliance on Client Code}
The implementation of the client part of the protocol is a part of the client operating system. Standard techniques for ensuring trust in this implementation include infrequent updates, auditability of the client code and signed images that ensure that each client device runs the same version of the operating system. In the proposed system, most of the privacy protection depends on the client code alone. For example, the parameter $B$ itself is part of the encrypted shares from the client, which ensures that there is agreement on this value between the client and the two aggregation servers. These hardenings ensure that the important system parameters that impact the privacy guarantee can not be altered by an adversarial leader/helper.

\section{Discussion}
\label{sec:discussion}
\subsubsection*{Federated Statistics vs. Federated Learning} Our aggregation system is designed for two somewhat different kinds of workloads. In typical federated statistics applications, one is interested in the heavy hitters over the population, and the aggregation of interest is a histogram over a noticeable fraction of the population. Thus the setup entails large batch sizes and few latency constraints. On the other hand private federated learning uses the aggregation to compute the gradient over a batch of devices, and multiple adaptive steps over small (relative to the population) batches are preferred to fewer steps over a larger batch. Thus low latency is typically desired in this setting. Our proposed system benefits from a unified design for these two settings needing the same underlying primitive. One may consider different system optimizations for the two cases to help improve the performance of the system.

\subsubsection*{Trust Models}

Similar systems for private aggregation may depend on the user trusting a central curator to use the data in a manner consistent with the promised privacy assurance. The proposed system strengthens such policy-based enforcement of privacy promises, and can ensure that the privacy assurance depends primarily on the client code and an inadvertent mistake or a bad actor on the server side does not lead to a privacy violation.

The privacy assurances of the proposed system depend on non-collusion between the leader and the helper: as long as at least one of the two servers is honest, the claimed privacy bounds hold. Such a trust model has been used for existing Prio deployments~\citep{firefox,ENPA:2021}, as well as in Private Relay~\cite{privaterelay} and Privacy Pass~\cite{ietf-privacypass-rate-limit-tokens-01}, and similar trust models have been used extensively in secure multiparty computation deployments~\cite{Wolinsky2012,Bogetoft09,Liina13,Lapets16,Abidin16,Bogdanov16,ChowB18}. In a deployment where the servers are run by different parts of the same organization, this split trust model prevents a single point of vulnerability, ensuring that any breach of the trust assumptions would be easier to detect. This can be significantly strengthened by having different entities run different servers. With standardized protocols such as DAP~\citep{ietf-ppm-dap-04}, the helper servers may be offered as a service for multiple deployments by different organizations using such a system for private federated learning and optimization. Such a market may help further improve trust in the non-collusion. We leave to future work other approaches to ensuring non-collusion and improving the trust in the system.


\subsubsection*{Central Noise Addition} While our aggregation primitive has the advantage of simplicity, one can also consider a richer primitive which aggregates non-noised contributions and outputs the noised aggregate. This can be useful in settings where the batch size itself may be hard to estimate. Naively implementing this primitive in the two-server framework would require both the leader and the helper to add sufficient noise, making the noise variance twice as large as needed. However, there may be efficient cryptographic protocols that can allow the two servers to collaboratively generate (shares of) noise with the right variance, so that the noise is hidden from each of the two servers as long as one of them is honest. Such an approach was proposed in the fully distributed MPC setting by~\citet{ODOpaper}, and we leave the design of practical and efficient noise generation protocols in the two-server setting to future work.

\subsubsection*{Samplable Aggregation vs. Aggregation}

While an aggregation functionality has been proposed in prior works, we have shown that the stronger functionality of a samplable anonymous aggregation allows us to get better privacy-utility trade-offs for private federated learning algorithms. We remark that in principle, sampling is not needed: the batched stochastic gradient descent algorithm can be replaced by a {\em full-batch} version of the algorithm which can be tuned to give the same privacy-utility trade-offs. However, this version of the algorithm will incur significantly higher cost for client and server, and lead to higher latency and thus fewer iterations in a given amount of time. The samplable aspect of the aggregation is thus crucial to be able to train accurate models in an acceptable timeframe. While some recent approaches to private learning~\cite{KairouzMSTTX21} can get around the privacy amplification by sampling, they require the use of correlated noise across iterations~\cite{DworkNPR10, ChanSS11} and hence are not compatible with our goal of adding noise in a distributed fashion. We leave secure implementations of these noise addition mechanisms to future work.

The sampling itself comes from two different sources of randomness. One that our analysis primarily relies on is the sampling done by a client to decide whether or not to participate in a specific batch. This randomness is derived from an on-device source of randomness, and implemented properly, would indeed be hidden from any reasonable adversary. Typically, devices only consider participating in the collection under specific circumstances; e.g. a mobile device may need to be idle and charging for it to participate. There is some inherent uncertainty in when the device considers participating and it may be reasonable to model this as being hidden from the adversary. Such modeling may allow us to prove better privacy bounds against reasonable adversaries.

\subsubsection*{Targeting}
In some settings, the server may want to {\em target} a recipe to a smaller subset of client devices, e.g. we may want to learn a next-word prediction model for clients using a specific language keyboard. Thus the recipe specifies a set of criteria that a device must meet to be able to participate. Arbitrary targeting criteria may allow a server to single out a device, e.g. by crafting a recipe which will have 0 or 1 responses depending on the data on a device. To mitigate against such risks, we can restrict the set of allowed targeting criteria to be those for which we have (e.g. using a private histogram query) ascertained that the population meeting the criteria is large enough. Large populations and a sampling rate bounded away from $1$ ensure that the noise due to sampling is enough to provide a strong differential privacy guarantee for the realized batch size (e.g. \cite{ODOpaper, zhu2020federated,cormode2022sample}). Additionally, we can restrict the targeting to be based only on non-sensitive criteria. Finally, the recipes being public and inspectable further reduces the risk of undetected attacks.

\subsubsection*{Local Differential Privacy and Compression}
The contributions from clients that are aggregated in the proposed framework can be high-dimensional. E.g. we may want to build a histogram using {\sc Rappor} over a large alphabet, or aggregate gradients for a large ML model. Naively, this may require the client upload to be rather large, which may be undesirable in settings where the upload bandwidth is limited. While the definition of \SAA-DP does not require the individual randomizers to satisfy a formal privacy guarantee, commonly used algorithms can be run with these randomizers satisfying differential privacy. Using local DP randomizers ensures a backup level of formal privacy. Additionally, it allows the clients to compress the contributions before sending them. Local randomizers are provably all compressible in the single-server setting~\cite{FeldmanTalwar21}, and low-communication local randomizers can often be easily adapted to the two-server setting. E.g. {\sc Rappor} can be replaced by {\sc ProjectiveGeometryResponse}~\citep{FeldmanNNT22} which reduces the communication while preserving the local DP guarantee. Similarly, a random projection can reduce the communication cost in Gaussian noise addition while preserving the accuracy~\cite{AsiFNNT23}.
\section{Additional Related Work}
\label{sec:related}
Differential privacy was introduced in~\cite{Dwork:2006}, and there is by now a large literature on DP primitives, algorithms, complexity, and different models of DP. Several deployed systems use differential privacy. In addition to the private federated learning deployments cited earlier, local differential privacy has been used in~\cite{ErlingssonPK14, Apple2017,DingKY17}. In the central model, there have been DP releases by the US Census Bureau~\cite{onthemap, abowd20222020}, Google~\cite{google-mobility, aktay2020google, bavadekar2021google}, LinkedIn~\cite{rogers2020linkedins, rogers2020members}, Microsoft~\cite{global_victim_perpetrator, pereira2021us} and Facebook~\cite{FacebookURLs} amongst others~\cite{pseo, edp}.

As alluded to earlier, Prio-based aggregation systems have previously been successfully deployed at scale by Mozilla for private telemetry measurment~\cite{firefox} and by several parties to enable private measurements of pandemic data in Exposure Notification Private Analytics (ENPA)~\cite{ENPA:2021}. In the case of ENPA, the system aggregated over 22 billion device aggregates in a little over a year~\cite{enpa-rwc}, with the servers being run by different organizations. While these systems demonstrate the feasibility and scalability of our proposed approach, our work argues for the additional samplability and anonymity as being part of the primitive, which can yield significant privacy benefits for PFL as discussed in~\cref{exa:pfl-sampling}.

Several Multiparty Computation (MPC) deployments in recent years have used the split-trust model similar to ours, where the security depends on a small set of servers not colluding~\cite{Bogetoft09,Liina13,Lapets16,Abidin16,Bogdanov16,ChowB18}. These examples implement MPC for relatively complex algorithms, and are closer to the cross-silo federated learning setting as the number of parties participating in the computation is typically small.

Another approach to federated learning that has been used in the cross-silo setting~\cite{de2020SCRAM, BlattGPG20,Froelicher21,oncological23} is to build on recent advances in homomorphic encryption. Here, the parties share encryptions of their data, typically under a secret-shared key. The computation of interest is then performed on the encrypted data, to derive an encryption of the desired result. Finally, the result can be decrypted by a sufficient number of parties collaboratively decrypting using their respective secret shares. Such an approach has been proposed for the cross-device private federated analytics setting in~\cite{unlynx,honeycrisp,orchard} and a related approach with a second non-colluding server helping with the decryption has been proposed in~\cite{crypteps}. Our approach built on Prio avoids extra rounds of interaction with the client devices, thus avoid the challenges of client dropout and churn. Additionally, the samplability in our work allows for much tighter privacy analyses.

In the current work, we aimed to limit the impact of an attacker to what it could have done if it could send arbitrary but bounded gradients in proportion to the number of devices it controls.  This may still allow a variety of model poisoning attacks as discussed in several works mentioned earlier~\cite{BagdasaryanVHES20,BhagojiCMC19, BaruchBG19,WangSRVASLP20,CheuSU19}, and differentially private training can help mitigate some of these attacks~\cite{SunKSM19}. Designing practical defenses that can further limit the attacker's impact on the model is an active area of research~\cite{BlanchardEGS17,BernsteinWAA18,flame,XieKG20,WuW21,ShejwalkarHKR22,wang2023invariant}. Extending such defenses to work in our framework may require additional cryptographic primitives and is left to future work.

\section{Conclusions}
\label{sec:conclusions}
In this work, we have proposed a new primitive that can allow many private federated analyses to be performed with utilities close to the central setting without the strong trust assumptions it entails. In particular, the primitive supports private histograms and private federated learning with central-like utilities. We propose an architecture for a large scale implementation of such a system, building on the Prio architecture and additional standard components. Our work makes it feasible to build scalable and useful private federated analysis systems that do not need to trust a single central server. Client devices in our proposed system send a single message, thus avoiding the complexity needed to handle churn in multi-round protocols. Previous deployments of Prio~\cite{firefox,ENPA:2021} attest to the scalability of such an approach.

We have shown that the \SAA\ primitive enables central-like privacy-utility trade-offs for many problems. A natural research direction is to understand better the full power of this primitive. The number of rounds of interactivity that a practical deployment permits is typically small, motivating the study of the computational, sample, communication and round complexity of private algorithms armed with this primitive.

An orthogonal direction is to enrich the primitive to allow a larger class of algorithms. The trust model and the two-server architecture proposed here is rich enough to enable additional functionalities in principle. Efficient algorithms to implement functionalities that are useful for differentially private algorithms can enable better utility for additional data analyses.
Finally, it is natural to implement the \SAA\ primitive (and other richer primitives) under other trust models that further distribute the trust.

\section{Acknowledgements}
This work was shaped by early discussions with
Ulfar Erlingsson, Abhishek Bhowmick, Julien Freudiger, Rogier Van Dalen, Andrew Cherakshyn, Fei Dong, Jaeman Park, Yulia Shuvkashvili, Hwasung Lee and Matt Seigel. We are grateful to Dan Boneh for valuable feedback on an earlier draft of this work. We would also like to thank Joey Meyer, Brian Lindblom, Cory Benfield, Elliot Briggs, John Duchi, Michael Hesse, Anil Katti, Cheney Lyford, Alex Palmer and Paul Pelzl for useful discussions.


\printbibliography
\appendix


\section{Privacy Amplification by Randomized Donation Time}

In Section~\ref{subsampling}, we discussed how sampling a subset of the available users at any time provides additional privacy by giving each user plausible deniability about whether their data was included in the computation. In this section, we will discuss how added uncertainty in when a user participates in a computation can be used to further amplify privacy guarantees in iterative algorithms like DP-SGD.
Devices typically need to satisfy specific conditions in order to consider participating, for example a device may need to charging. Since there is often uncertainty in when any particular device becomes available for participation, there is uncertainty in donation time inherent in the system. Relying on this randomness to provide formal privacy guarantees would require careful modeling of this randomness. However,
we can also ask devices to deliberately (and according to a specific distribution) randomize the times at which they consider participating.
In this section, we will discuss how a specific algorithm for randomizing donation time can be used to amplify privacy guarantees in iterative algorithms like DP-SGD.

Suppose that an analyst is running an iterative algorithm in the following manner. When a device becomes available, it chooses whether or not to participate by flipping a biased coin and participating with probability $q$. If the device chooses to participate it then chooses an amount of time to wait before participating. When the wait time is complete, if the device is available, it downloads the recipe for the currently occurring round of the iterative algorithm and sends a privatized report to the aggregator. The aggregator aggregates the local reports for some amount of time and sends the aggregate to the data analyst. The analyst then begins the next round by selecting the next round's query. This process continues until the iterative algorithm is complete. By randomizing the wait time, we can inject uncertainty into the donation time of any device and use this uncertainty to amplify the privacy guarantee. However, selecting a wait time that is greater than 0 runs the risk of the device not being available at the subsequent donation time, increasing latency and/or reducing utility. Thus, we need to be carefully balance any privacy amplification with utility concerns.

The key intuition we will use for defining an algorithm for choosing the wait time is that we want to build uncertainty into which iteration a device contributes to. Ideally, we would have devices randomly select an iteration to participate in, but this is difficult to implement since iterations may last for variable lengths of time making it difficult for devices to predict when a specific iteration may occur. Instead,
we first select constants $K>0$ and $m\in\mathbb{N}$. For all $i\in\{0,\cdots,m-1\}$, the algorithm samples from a Bernoulli random variable $\chi_i$ with bias $1/m$. If $\max\chi_i=0$, then the device does not participate. Otherwise, the wait time is set to set to $K\cdot\arg\min\{i\;|\; \chi_i=1\}$. If we can ensure each iteration lasts for at \emph{most} $K$ minutes, then this algorithm either selects not to participate, or selects randomly among $m$ future iterations. In order to formalize the privacy amplification of this mechanism, we need one additional functionality of our aggregator. In addition to implementing the \SAA\ primitive, we will require that the aggregator does not release the result if the length of collection time is longer than $K$ minutes.

\begin{align*}
    \agg_{\minbatch, K}^{\Dec}&(t_0, \{(m_1, t_1), \ldots, (m_k, t_k)\}) \\&= \left\{\begin{array}{ll} \sum_{i=1}^k Dec(m_i) & \mbox{if } k \geq \minbatch \text{ and } t_k\in[t_0,t_0+K]\\ \bot &\mbox{otherwise} \end{array} \right.
\end{align*}

Suppose each device participates at most once in the iterative algorithm and each round is $(\eps, \delta)$-DP in the \SAA\ model. If this algorithm is run with the above randomized donation time algorithm in the \SAA\ model equipped with an aggregator with this additional functionality then it is $(\epsilon', \delta')$-DP where
\[\epsilon'=O\left(\epsilon\sqrt{\frac{\log(1/\delta)}{m}}\right) \text{ and } \delta'=O(m\delta).\]

To see this, note that the privacy loss of this algorithm is upper bounded by the privacy loss of the related algorithm where for all $i\in\{0,\cdots, m-1\}$, if $\chi_i=1$, then the device participates in the round occurring after a wait time of $i\cdot K$. For each round, each device is either not eligible, or participates with probability $1/m$. Thus, using Lemma~\ref{samplingWOR}, we can see that each round is $(\log(1+\frac{1}{m}(e^{\epsilon}-1)), \delta/m)$-DP. Since each device only considers participating in at most $m$ rounds, we can obtain the final privacy guarantee by using the advanced composition theorem \cite{DRV10} over $m$ rounds, so \[\epsilon' \le \log\left(1+\frac{1}{m}(e^{\epsilon}-1)\right)\sqrt{2\log(1/\delta)m}+\frac{1}{2}m \log\left(1+\frac{1}{m}(e^{\epsilon}-1)\right)^2 \]\[ \text{ and } \delta'\le (m+1)\delta.\]

To maximize utility and minimize latency, the constant $K$ should be chosen so that most iterations are expected to receive enough donations within $K$ minutes. The constant $m$ should be chosen so that the device is likely to be available at the assigned donation time. This may be achieved by choosing $m$ small enough that $m\cdot K$ is a lower bound on the length of time that devices typically satisfy the conditions required in order for them to be available.

\end{document}
