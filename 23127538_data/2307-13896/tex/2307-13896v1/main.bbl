\begin{thebibliography}{10}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }
\providecommand{\doi}[1]{https://doi.org/#1}

\bibitem{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.: Language models
  are few-shot learners. Advances in neural information processing systems
  \textbf{33},  1877--1901 (2020)

\bibitem{cai2022aug}
Cai, D., Wu, Y., Yuan, H., Wang, S., Lin, F.X., Xu, M.: Aug-fedprompt:
  Practical few-shot federated nlp with data-augmented prompts. arXiv preprint
  arXiv:2212.00192  (2022)

\bibitem{chowdhery2022palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et~al.: Palm: Scaling
  language modeling with pathways. arXiv preprint arXiv:2204.02311  (2022)

\bibitem{devlin2018bert}
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep
  bidirectional transformers for language understanding. arXiv preprint
  arXiv:1810.04805  (2018)

\bibitem{fan2022private}
Fan, C., Hu, J., Huang, J.: Private semi-supervised federated learning. In:
  IJCAI. pp. 2009--2015 (2022)

\bibitem{fan2021federated}
Fan, C., Huang, J.: Federated few-shot learning with adversarial learning. In:
  2021 19th International Symposium on Modeling and Optimization in Mobile, Ad
  hoc, and Wireless Networks (WiOpt). pp.~1--8. IEEE (2021)

\bibitem{gao2020making}
Gao, T., Fisch, A., Chen, D.: Making pre-trained language models better
  few-shot learners. arXiv preprint arXiv:2012.15723  (2020)

\bibitem{gu2021ppt}
Gu, Y., Han, X., Liu, Z., Huang, M.: Ppt: Pre-trained prompt tuning for
  few-shot learning. arXiv preprint arXiv:2109.04332  (2021)

\bibitem{he2021towards}
He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., Neubig, G.: Towards a unified
  view of parameter-efficient transfer learning. arXiv preprint
  arXiv:2110.04366  (2021)

\bibitem{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning
  for nlp. In: International Conference on Machine Learning. pp. 2790--2799.
  PMLR (2019)

\bibitem{hu2021lora}
Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
  Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
  arXiv:2106.09685  (2021)

\bibitem{lester2021power}
Lester, B., Al-Rfou, R., Constant, N.: The power of scale for
  parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691  (2021)

\bibitem{li2019federated}
Li, T., et~al.: Federated learning: Challenges, methods, and future directions.
  arXiv preprint arXiv:1908.07873  (2019)

\bibitem{li2021prefix}
Li, X.L., Liang, P.: Prefix-tuning: Optimizing continuous prompts for
  generation. arXiv preprint arXiv:2101.00190  (2021)

\bibitem{lin2020exploring}
Lin, Z., Madotto, A., Fung, P.: Exploring versatile generative language model
  via parameter-efficient transfer learning. arXiv preprint arXiv:2004.03829
  (2020)

\bibitem{liu2021p}
Liu, X., Ji, K., Fu, Y., Tam, W.L., Du, Z., Yang, Z., Tang, J.: P-tuning v2:
  Prompt tuning can be comparable to fine-tuning universally across scales and
  tasks. arXiv preprint arXiv:2110.07602  (2021)

\bibitem{liu2021gpt}
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., Tang, J.: Gpt
  understands, too. arXiv preprint arXiv:2103.10385  (2021)

\bibitem{fedavg}
{McMahan}, H.B., {Moore}, E., {Ramage}, D., {Hampson}, S., y~{Arcas}, B.A.:
  Communication-efficient learning of deep networks from decentralized data.
  In: AISTATS (2017)

\bibitem{pfeiffer2020adapterfusion}
Pfeiffer, J., Kamath, A., R{\"u}ckl{\'e}, A., Cho, K., Gurevych, I.:
  Adapterfusion: Non-destructive task composition for transfer learning. arXiv
  preprint arXiv:2005.00247  (2020)

\bibitem{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.: Improving
  language understanding by generative pre-training (2018)

\bibitem{ruckle2020adapterdrop}
R{\"u}ckl{\'e}, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers,
  N., Gurevych, I.: Adapterdrop: On the efficiency of adapters in transformers.
  arXiv preprint arXiv:2010.11918  (2020)

\bibitem{sattler2019robust}
Sattler, F., Wiedemann, S., M{\"u}ller, K.R., Samek, W.: Robust and
  communication-efficient federated learning from non-iid data. IEEE TNNLS
  (2019)

\bibitem{schick2020exploiting}
Schick, T., Sch{\"u}tze, H.: Exploiting cloze questions for few shot text
  classification and natural language inference. arXiv preprint
  arXiv:2001.07676  (2020)

\bibitem{shin2020autoprompt}
Shin, T., Razeghi, Y., Logan~IV, R.L., Wallace, E., Singh, S.: Autoprompt:
  Eliciting knowledge from language models with automatically generated
  prompts. arXiv preprint arXiv:2010.15980  (2020)

\bibitem{shome2021fedaffect}
Shome, D., Kar, T.: Fedaffect: Few-shot federated learning for facial
  expression recognition. In: Proceedings of the IEEE/CVF International
  Conference on Computer Vision. pp. 4168--4175 (2021)

\bibitem{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
  H.T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.: Lamda: Language models for
  dialog applications. arXiv preprint arXiv:2201.08239  (2022)

\bibitem{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
  Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.: Llama: Open and
  efficient foundation language models. arXiv preprint arXiv:2302.13971  (2023)

\bibitem{wu2020personalized}
{Wu}, Q., {He}, K., {Chen}, X.: Personalized federated learning for intelligent
  iot applications: A cloud-edge based framework. IEEE Computer Graphics and
  Applications  (2020)

\bibitem{zhao2018federated}
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning
  with non-iid data. arXiv preprint arXiv:1806.00582  (2018)

\bibitem{zhao2022personalized}
Zhao, Y., Yu, G., Wang, J., Domeniconi, C., Guo, M., Zhang, X., Cui, L.:
  Personalized federated few-shot learning. IEEE Transactions on Neural
  Networks and Learning Systems  (2022)

\end{thebibliography}
