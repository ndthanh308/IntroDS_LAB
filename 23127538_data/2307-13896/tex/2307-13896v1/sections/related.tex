\section{Related Work}
\textbf{Federated learning(FL). }~\cite{fedavg, zhao2018federated, sattler2019robust, li2019federated, wu2020personalized} is a distributed learning method that aims to train a global model on decentralized data while preserving data privacy. In FL, clients download copies of the global model and compute local gradients using their private data in each round. A central server coordinates the distributed clients, aggregating the local parameters to update the global model without exchanging raw data. Our research focuses on federated few-shot learning, addressing the challenge of training an effective global model in a federated environment with limited client-side labeled data. 

Several approaches have been proposed to tackle this scenario. FedFSL~\cite{fan2021federated} performs classification of unseen data classes using only a small number of labeled samples. FedSSL~\cite{fan2022private} utilizes semi-supervised learning to fully leverage labeled and unlabeled data sources for training. pFedFSL~\cite{zhao2022personalized} identifies well-performing models on specific clients (without revealing local data to the server or other clients) and selects suitable clients for collaboration, enabling personalized and distinctive feature space learning for each client. FedAffect~\cite{shome2021fedaffect} updates the feature extractor network on disjoint unlabeled private facial data to learn robust and diverse facial representations. Similarly, our work focuses on utilizing the power of pre-trained models to address this issue. Another approach, AUG-FedPrompt~\cite{cai2022aug}, performs data augmentation by annotating a large amount of unlabeled data, achieving operations similar to full fine-tuning with minimal initial labeled data. However, this method entails high communication costs due to full-parameter fine-tuning of the model.

\textbf{Large Language Models (LLMs). }
Pre-trained language models (PLMs) based on the transformer architecture, such as BERT~\cite{devlin2018bert} and GPT~\cite{radford2018improving}, have significantly enhanced the performance of natural language processing tasks. Expanding on this foundation, researchers continue to explore the upper limits of language model parameter sizes to unlock the potential of PLMs. The emergence of Large Language Models (LLMs) like GPT-3~\cite{brown2020language}, PaLM~\cite{chowdhery2022palm}, LaMDA~\cite{thoppilan2022lamda} and LLaMa~\cite{touvron2023llama} exemplifies this trend. These models exhibit remarkable few-shot capabilities by leveraging natural language prompts and task demonstrations as contextual input. However, these capabilities are built upon LLMs with parameter counts often exceeding 10 billion, making their application in real-world scenarios challenging. Consequently, researchers have begun investigating prompt performance on smaller-scale language models. Relevant approaches include PET~\cite{schick2020exploiting}, which reformulates input examples into cloze-style phrases to facilitate the language model's comprehension of the given task. AutoPrompt~\cite{shin2020autoprompt} selects a subset of discrete characters as triggers through gradient-based search and constructs templates to predict the probability of corresponding label words using models. LM-BFF~\cite{gao2020making} introduces a generative method for pattern construction, followed by a search-based technique to derive the associated verbalizers. Additionally, PPT~\cite{gu2021ppt} employ continuous prompts and transform templates into continuous vectors for optimization.


\textbf{Parameter-Efficient Fine-Tuning(PEFT). } PEFT enables efficient adaptation of LLMs to various downstream tasks without the need to fine-tune all parameters of the LLMs. The PEFT method fine-tunes only a small subset or additional parameters, while keeping the majority of pre-training parameters fixed, resulting in significant reductions in computation and storage costs. Notably, advanced PEFT techniques achieve performance comparable to full fine-tuning. Prefix-Tuning~\cite{li2021prefix} introduces a continuous and task-specific sequence of vectors, known as a prefix, added before the model input. This approach fixes all parameters of the LLMs and focuses on updating and optimizing the prefix specific to the task, resulting in minimal additional computational and storage overhead in downstream tasks. P-Tuning~\cite{liu2021gpt} leverages a limited number of continuous embedding parameters as prompts to optimize the performance of GPT in natural language understanding (NLU) tasks. In contrast, Prefix-Tuning is tailored for natural language generation (NLG) tasks. Moreover, P-Tuning solely introduces parameters in the embedding layer, whereas Prefix-Tuning incorporates trainable parameters across all layers. In contrast, P-Tuning V2~\cite{liu2021p} optimizes and adapts to NLU tasks by employing continuous prompts and updating prompt parameters at each layer of the model.Prompt Tuning~\cite{lester2021power} fixes all parameters of the LLMs and allows for the addition of a small, task-specific set of $k$ tokens to be prepended to the input text for each downstream task. As the model size reaches a certain scale, Prompt Tuning alone proves sufficient to achieve fine-tuning performance. Adapter Tuning~\cite{houlsby2019parameter, he2021towards, lin2020exploring, pfeiffer2020adapterfusion, ruckle2020adapterdrop} involves introducing new network layers or modules within the internal network layers of the LLMs to adapt to downstream tasks. Low-Rank Adaptation (LoRA)~\cite{hu2021lora}, while freezing the original model parameters, incorporates additional network layers and exclusively trains the parameters of these newly added layers to achieve results similar to full-model fine-tuning.

