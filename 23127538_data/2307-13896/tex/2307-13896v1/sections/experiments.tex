\section{Experiment}
\label{sec:exp}
We verify our approach on the widely used IMDB and Yelp datasets for evaluating sentiment analysis task. We performed an analysis of our method in both federated and centralized training settings, as well as an analysis of Low-Parameter Federated Learning (LP-FL) and Full-Parameter Federated Learning (FP-FL) in the federated environment. 
\subsection{Datasets and Task Description}
\label{sec:dataset}

\textbf{IMDB} dataset is a benchmark dataset for sentiment classification task. It contains tens of thousands of textual reviews specifically related to movie evaluations. We employed the complete training set, which contained 25,000 labeled data points. Nonetheless, during the experiments, only a portion of the data, not surpassing 10\%, was utilized as the labeled dataset, while the remaining data was designated as the unlabeled dataset. IMDB provides a well-balanced collection of movie reviews for sentiment classification tasks, which is widely compared in recent studies. We utilize the following task descriptions to process the text $x$ from the IMDB dataset:
\begin{align*}
P_1(x) &= \text{It was [MASK]. x.} \\
P_2(x) &= \text{Just [MASK]! x.} \\
P_3(x) &= \text{x. All in all, it was [MASK].} \\
P_4(x) &= \text{x. In summary, the movie is [MASK].}
\end{align*}
We conducted label mapping for the dataset as well:
\begin{align*}
v(0) = \text{[} &\text{'boring', 'disappointing', 'terrible', 'predictable', 'obvious', } \\
&\text{'dull', 'commonplace', 'awful', 'simple', 'confusing'}\text{]} \\
v(1) = \text{[} &\text{'good', 'great', 'excellent', 'funny', 'interesting', } \\
&\text{'amazing', 'fantastic', 'awesome', 'nice', 'inspiring'}\text{]}
\end{align*}

\textbf{Yelp} dataset is widely used for sentiment classification task~\cite{schick2020exploiting, gao2020making} collected from the Yelp platform. The dataset contains business details such as names, locations, categories, and star ratings. It also consists of user profiles with IDs, names, and review histories. The reviews themselves include text content, ratings, and additional metadata. Our study focuses on the yelp-polarity dataset, which is a binary sentiment classification dataset. Likewise, we conducted a random sampling of 25,000 labeled data points from the dataset. During the experiments, only a small subset of this data was employed as the labeled dataset, leaving the remaining samples designated as the unlabeled dataset. Below are the task descriptions for text $x$ in yelp-polarity dataset:
\begin{align*}
P_1(x) &= \text{It was [MASK]. x.} \\
P_2(x) &= \text{Just [MASK]! x.} \\
P_3(x) &= \text{x. All in all, it was [MASK].} \\
P_4(x) &= \text{x. In summary, the restaurant is [MASK].} 
\end{align*}
And the label mapping:
\begin{align*}
v(0) = \text{[} &\text{'dirty', 'bad', 'terrible', 'rude', 'obvious', } \\
&\text{'dull', 'commonplace', 'awful', 'simple', 'bland'}\text{]} \\
v(1) = \text{[} &\text{'good', 'cozy', 'inviting', 'delicious', 'impressive', } \\
&\text{'clean', 'organized', 'awesome', 'nice', 'fabulous'}\text{]}
\end{align*}

Our method is validated on the IMDB and Yelp datasets. The complete sets of 25,000 training and testing samples are used for IMDB, while we randomly select 25,000 samples from the training and testing sets for Yelp. The Large Language Model (LLM) employed in our experiments is BERT-Large model with 336M parameters. 

Considering previous research and practical considerations, we choose the following hyperparameter values: a batch size of 8, local epochs as 5, global training rounds as 5, a learning rate of $5 \times 10^{-5}$, a maximum sequence length of 128, and a rank of 8 for Low-Rank Adaptation (LoRA). We conduct experiments in federated settings with 2, 5, and 10 participating parties. During the training process, we selectively employ a fraction of the training dataset, specifically 1\%, 5\%, and 10\%, as labeled datasets. Consequently, each federated participant receives an equal distribution of total 250/1250/2500 labeled samples. Meanwhile, to establish the validation set for the global model, we randomly sample 1000 instances from the remaining data. The left 23750/22750/21500 samples are further allocated evenly to each  participant as the unlabeled data. 

Following each round, once all participants have concluded their local training, the LoRA parameters are transmitted to the server. The server employs FedAvg to update the LoRA parameters in the global model and redistributes the updated parameters to all participants for local model updating. During this phase, participants employ the updated model to randomly annotate 25\% of the data from the unlabeled dataset and added to the labeled dataset, while being removed from the unlabeled dataset. This ensured the utilization of all labeled and unlabeled data for fine-tuning the global model before the final global update.


\subsection{Experimental Results of LP-FL on IMDB and Yelp}
\label{sec:LP_FL_result}

\begin{table}[!ht]

\caption{LP-FL results}
\label{tab:LP_FL_result}
\centering

\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ p{1.5cm}<{\centering} | p{2.5cm}<{\centering}  p{2cm}<{\centering}  p{2cm}<{\centering}  p{2cm}<{\centering}  p{2cm}<{\centering} }
% \caption{Result}

% \multicolumn{2}{|c|}{\multirow{2}*{multi col row}} &\multicolumn{3}{|c|}{multi col} & \multirow{2}{*}{multi row}\\
% \cline{3-5}
% \multicolumn{2}{|c|}{} & (2,3) & (2,4) & (2,5) & \\
\hline
% \multicolumn{2}{c}{\multirow{2}*{clients}} & \multirow{2}*{1} & \multirow{2}*{2} & \multirow{2}*{5} & \multirow{2}*{10} \\
% Dataset & labelled data & \\
\multirow{2}*{Dataset} & \multirow{2}*{Labeled Data} & \multirow{2}*{Centralized} & \multicolumn{3}{c}{Our LP-FL (client num)} \\
\cline{4-6}
& & & 2 & 5 & 10 \\
% \multicolumn{2}{c}{clients} & \text{\small{Cen-LLM-LoRA}} & 2 & 5 & 10 \\
\hline
\multirow{3}*{IMDB} & 1\% & 0.850 & \textbf{0.858} & 0.840 & 0.822 \\
\cline{3-6}
& 5\% & \textbf{0.869} & 0.867 & 0.861 & 0.853 \\
\cline{3-6}
& 10\% & \textbf{0.878} & 0.873 & 0.864 & 0.859 \\
\hline
\hline
\multirow{3}*{Yelp} & 1\% & 0.913 & \textbf{0.922} & 0.903 & 0.900 \\
\cline{3-6}
& 5\% & 0.914 & \textbf{0.927} & 0.919 & 0.911 \\
\cline{3-6}
& 10\% & 0.919 & \textbf{0.929} & 0.925 & 0.919 \\
\hline

\end{tabular}
% \caption*{the model's accuracy using subsets of the IMDB and Yelp datasets comprising 1\%, 5\%, and 10\% of the data, under both centralized training and federated learning with 2, 5, and 10 participants.}
\vspace{10pt}
\end{table}

% Figure environment removed

\textbf{Result on IMDB and Yelp. } Table~\ref{tab:LP_FL_result} illustrates the outcomes of our method in the context of sentiment analysis task. We have the following observations.

\begin{itemize}[noitemsep,leftmargin=*]
\item \emph{Within the federated environment, our approach consistently achieves a prediction accuracy of over 82\% across the test dataset.} 
\item \emph{A comparative analysis is conducted between our method under the federated environment and the centralized training approach. } Remarkably, our method exhibits minimal decrease in prediction accuracy when there are only two federated participants, demonstrating nearly identical performance to the centralized training approach. 
\item \emph{Moreover, when five and ten client participants engage in the federated training, the prediction accuracy only decrease by approximately 1\% and 2\% respectively.}

\item \emph{In scenarios involving ten federated participants, each contributing a mere 0.1\% of the data (equivalent to 24 data instances), the sample size adequately corresponds to the labeled samples commonly available on mobile devices in practical settings. Nevertheless, our approach consistently yields satisfactory results, with the final model's prediction accuracy only decrease mildly compared to centralized training.}
\end{itemize}
% Within the federated environment, our approach consistently achieves a prediction accuracy of over 82\% across the test dataset.A comparative analysis is conducted between our method under the federated environment and the centralized training approach. Remarkably, our method exhibits minimal decrease in prediction accuracy when there are only two federated participants, demonstrating nearly identical performance to the centralized training approach. Moreover, when five and ten client participants engage in the federated training, the prediction accuracy only decrease by approximately 1\% and 2\% respectively. 



% Figure environment removed


\textbf{Convergence and Stability of LP-FL. } 
Throughout the experiments, we closely monitor the evolution of the global model's prediction accuracy during each round of global training. 

Notably, we show in Figure ~\ref{fig:validation_accuracy} an extreme case of using a minimal of 1\% subset of the IMDB and Yelp datasets. Although the accuracy was low in the initial 1-2 rounds, it quickly rises to a reasonable level by just a few more rounds of training.

 % with the increase of the global training rounds, the prediction accuracy of the global model began to improve rapidly and finally reached the expectation. This is particularly evident when utilizing a mere 1\% of the data with five or ten federated participants.

This nice property of quick convergence can be partly attributed to our low-parameter methodology of training. That is, with limited training samples, we seek to fine-tune only about 30\% of the total model parameters, preventing over-fitting to the small number of samples. 

By employing our proposed iterative and semi-supervised methodology, we can enhance the utilization of unlabeled data in the presence of scarce labeled data. We establish a criterion that the prediction accuracy of the global model after FedAvg exceeds 70\% before using it to annotate the unlabeled data. This approach allows us to capitalize on a greater quantity and higher quality of data for fine-tuning purposes. Furthermore, we ensure the completion of annotating all unlabeled data prior to initiating the final global update, thus ensuring consistency in the training data volume and fully exploiting all available local data. 

As shown in Table~\ref{fig:LP_FL_results} , the final prediction accuracy of the global model of LP-FL, when compared to centralized training (third column), exhibits a performance loss of 2.8\% (0.822 vs. 0.850) on IMDB and 1.3\% (0.900 vs. 0.913) on Yelp using 1\% subset. In contrast, if we increase the labeled data to 10\%, the performance loss diminishes to 1.9\% (0.859 vs. 0.878) on IMDB and becomes negligible on Yelp. These trends align with our expectations and falls within an acceptable range, thereby demonstrating the stability of LP-FL.


% \begin{table}[ht]
% \caption{Comparison with FP-CT and FP-FL}
% \label{tab:comparison_with_full}
% \centering
% \renewcommand{\arraystretch}{1.5}
% \begin{tabular}{ p{1.5cm}<{\centering} | p{2.5cm}<{\centering}  p{1.5cm}<{\centering}  p{2.5cm}<{\centering}  p{2.5cm}<{\centering} }

% \hline
% Dataset & Labeled Data & Method & CT(centralized) & FL(2-clients) \\
% \hline
% \multirow{4}*{IMDB} & \multirow{2}*{1\%} & LP- & 0.850 & 0.858 \\

% & & FP- & 0.855 & 0.850 \\
% \cline{3-5}
% & \multirow{2}*{5\%} & LP- & 0.869 & \textbf{0.867} \\
% & & FP- & \textbf{0.872} & 0.864 \\
% \hline
% \multirow{4}*{Yelp} & \multirow{2}*{1\%} & LP- & 0.913 & 0.922 \\

% & & FP- & 0.914 & 0.916 \\
% \cline{3-5}
% & \multirow{2}*{5\%} & LP- & 0.914 & \textbf{0.926} \\
% & & FP- & \textbf{0.922} & 0.924 \\
% \hline
% \end{tabular}
% \end{table}

\begin{table}[ht]
\caption{Comparison with FP-CT and FP-FL}
\label{tab:comparison_with_full}
\centering
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ p{1.5cm}<{\centering} | p{1.5cm}<{\centering} p{1.5cm}<{\centering}  p{2.5cm}<{\centering} p{1.5cm}<{\centering}  p{2.5cm}<{\centering}}

\hline
\multirow{2}*{Dataset} & \multirow{2}*{Labeled} & \multicolumn{2}{c}{LP-} & \multicolumn{2}{c}{FP-} \\
\cline{3-6}
& & CT & FL(2-clients) &  CT & FL(2-clients) \\
\hline
\multirow{2}*{IMDB} & 1\% & 0.850 & \textbf{0.858} & 0.855 & 0.850 \\
\cline{3-6}
& 5\% & 0.869 & 0.867 & \textbf{0.872} & 0.864 \\
\hline
\hline
\multirow{2}*{Yelp} & 1\% & 0.913 & \textbf{0.922} & 0.914 & 0.916 \\
\cline{3-6}
& 5\% & 0.914 & \textbf{0.926} & 0.922 & 0.924 \\
\hline
\end{tabular}
\end{table}

% Figure environment removed

% \textbf{Comparison with Full-Parameter Centralized Training (FP-CT) and Full-Parameter Federated Learning (FP-FL). } We compare the performance of our method with FP-CT and FP-FL in Table~\ref{tab:comparison_with_full}. As expected, the FP-CT yields better prediction accuracy compared to Low-parameter Centralized Training (LP-CT). However, in the federated environment, FP-FL leads to slightly lower prediction accuracy compared to our proposed LP-FL (0.850 vs. 0.858 and 0.864 vs. 0.867 on IMDB, 0.916 vs. 0.922 and 0.924 vs. 0.926 on Yelp). 
% % More surprisingly, we find that LP-FL can equal or surpass LP-CT in certain settings, as shown in Table~\ref{fig:comparison_with_full} column of LP-CT and LP-FL.

% \textbf{The prediction accuracy of LP-CT is slightly lower than that of FP-CT, although the decrease is not substantial.} The smallest observed difference is only 0.1\% (0.913 vs. 0.914), while the larger discrepancy is merely 0.8\% (0.914 vs. 0.922), as shown in Table~\ref{tab:comparison_with_full} column of LP-CT and FP-CT. The reason behind this is simple: FP-CT utilizes the complete set of model parameters for fine-tuning, enabling it to acquire more knowledge compared to LP-CT, which utilizes only approximately 30\% of the parameters. Consequently, FP-CT exhibits superior performance.

% \textbf{Unexpectedly, the prediction accuracy of the LP-FL consistently outperforms the FP-FL in the federated environment.} This observation contradicts the majority of related work in centralized training. We attribute these findings to the semi-supervised nature of our approach. We annotate the unlabeled dataset only after achieving a high level of model prediction accuracy. However, there are still some erroneous samples present. Additionally, the large parameter size of the language model contributes to its complexity, making it more prone to memorizing noise. Under FP-FL, the erroneous annotations introduce more noise to the model, adversely affecting prediction accuracy. On the other hand, LP-FL only fine-tunes around 30\% of the parameters, reducing the model's complexity and minimizing the impact of erroneous annotations. As a result, over-fitting to noise caused by erroneous annotations is reduced under LP-FL, leading to better performance compared to FP-FL.

% \textbf{The presence of noise induced by erroneous annotations, derived from the semi-supervised nature, is also observable in LP-CT and LP-FL.} It has been noted in our investigations that the prediction accuracy of LP-CT is also lower than that LP-FL in certain settings (0.850 vs. 0.858 on IMDB, 0.913 vs. 0.922 and 0.914 vs. 0.926 on Yelp) as shown in Table~\ref{tab:comparison_with_full} column of LP-CT and LP-FL. Although LP-CT reduces model complexity and mitigates the impact of erroneous annotations, the quantity of erroneous annotations still affects prediction accuracy. Centralized training models have access to a larger unlabeled dataset, leading to more erroneous annotations after labeling. In contrast, federated participants have smaller local unlabeled datasets, resulting in fewer erroneous annotations. Given the same model complexity, a larger number of erroneous annotations increases the likelihood of over-fitting to incorrect labels, resulting in lower prediction accuracy.

% % Therefore, our semi-supervised approach is more suitable for training in the federated environment. The limited dataset size restricts the generation of erroneous annotations, thereby reducing their negative impact on prediction accuracy.
% Hence, our proposed LP-FL, incorporating a low-parameter setting and employing a federated framework, efficiently reduces the over-fitting to erroneously labeled data in the realm of semi-supervised learning.

\subsection{Ablation Studies}
We compare the performance of our method with Full-Parameter Centralized Training (FP-CT) and Full-Parameter Federated Learning (FP-FL) in Table~\ref{tab:comparison_with_full}. Based on the comparisons, we draw the following conclusions:

\textbf{FP-CT is better than LP-CT. }As expected, the prediction accuracy of LP-CT is slightly lower than that of FP-CT, although the decrease is not substantial. The smallest observed difference is only 0.1\% (0.913 vs. 0.914), while the larger discrepancy is merely 0.8\% (0.914 vs. 0.922), as shown in Table~\ref{tab:comparison_with_full} column of LP-CT and FP-CT. The reason behind this is simple: FP-CT utilizes the complete set of model parameters for fine-tuning, enabling it to acquire more knowledge compared to LP-CT, which utilizes only approximately 30\% of the parameters. Consequently, FP-CT exhibits superior performance.

\textbf{LP-FL is better than FP-FL. }Unexpectedly, the prediction accuracy of the LP-FL consistently outperforms the FP-FL in the federated environment (0.858 vs. 0.850 and 0.867 vs. 0.864 on IMDB, 0.922 vs. 0.916 and 0.926 vs. 0.924 on Yelp). This observation contradicts the majority of related work in centralized training. We attribute these findings to the semi-supervised nature of our approach. We annotate the unlabeled dataset only after achieving a high level of model prediction accuracy. However, there are still some erroneous samples present. Additionally, the large parameter size of the language model contributes to its complexity, making it more prone to memorizing noise. Under FP-FL, the erroneous annotations introduce more noise to the model, adversely affecting prediction accuracy. On the other hand, LP-FL only fine-tunes around 30\% of the parameters, reducing the model's complexity and minimizing the impact of erroneous annotations. As a result, over-fitting to noise caused by erroneous annotations is reduced under LP-FL, leading to better performance compared to FP-FL.

\textbf{LP-FL is equal or better than LP-CT. }The presence of noise induced by erroneous annotations, derived from the semi-supervised nature, is also observable in LP-CT and LP-FL. It has been noted in our investigations that the prediction accuracy of LP-CT is also lower than that LP-FL in certain settings (0.850 vs. 0.858 on IMDB, 0.913 vs. 0.922 and 0.914 vs. 0.926 on Yelp) as shown in Table~\ref{tab:comparison_with_full} column of LP-CT and LP-FL. Although LP-CT reduces model complexity and mitigates the impact of erroneous annotations, the quantity of erroneous annotations still affects prediction accuracy. Centralized training models have access to a larger unlabeled dataset, leading to more erroneous annotations after labeling. In contrast, federated participants have smaller local unlabeled datasets, resulting in fewer erroneous annotations. Given the same model complexity, a larger number of erroneous annotations increases the likelihood of over-fitting to incorrect labels, resulting in lower prediction accuracy.

\textbf{FP-FL is equal or better than FP-CT. }FP-FL demonstrates prediction accuracy that is comparable to, and sometimes even surpasses, that of FP-CT. This finding provides additional evidence that in a semi-supervised setting, a greater number of erroneous annotations result in increased noise, thereby impacting the model's prediction accuracy. Due to the relatively fewer erroneous annotations encountered by individual models in the federated environment compared to centralized training, FP-FL occasionally outperforms FP-CT.
%我们分别在集中式训练下和联邦环境下与全参数微调进行了对比，结果发现在集中式训练下，全参微调的预测准确率略高于LoRA微调，而在联邦环境下，全参微调的预测准确率却略低于LoRA微调。甚至在全参微调下和LoRA微调下都存在集中式训练的模型预测准确率低于联邦环境下的。

%正如表中显示的那样，在联邦环境下，LoRA微调的模型预测准确率总是略高于全参微调的结果，而在集中式训练下，却与之相反，与大部分工作在全参微调与LoRA微调的对比相同。

%我们认为这是由于我们的方法是半监督训练，尽管我们在模型预测准确率达到较高水平才去标注无标签数据集，但仍然存在较多的错误样本，而大语言模型的参数规模较大，使模型太过复杂，越是复杂的模型越容易记住噪声，全参微调下这些错误的标注给我们的模型带来了较多的噪声，而LoRA微调仅微调30%左右的参数，相当于降低了模型复杂度，错误的标注带来的噪声并没有那么大。这种对于噪声的过度拟合导致了我们的方法在LoRA微调下的结果好于全参微调。

%同时，在LoRA微调下，集中式训练方式下的模型预测准确率也存在低于联邦环境下的结果，这也证实了我们的结论。尽管LoRA微调降低了模型复杂度，使错误的标注带来的噪声没有那么大，但是错误标注的数据量大小同样会影响模型的预测准确率。集中式训练的模型拥有较大的无标签数据集，标注后产生的错误标注也会更多，相比联邦环境下的参与者的本地无标签数据集较小，相比之下产生的错误标注也会更少，在同样的模型复杂度下，错误标记的数据越多模型越容易受到影响，越容易过度拟合错误的标注数据，从而使模型预测准确率变低。

%因此，半监督的方法反而更适合在联邦环境下训练，有限的数据集限制了模型只能产生少量的错误标注，减少错误的标注数据对模型预测准确率的降低