% Figure environment removed

% Figure environment removed

\section{Approaches}
We describe the training workflow of Low-Parameter Federated Learning (LP-FL) in Figure~\ref{fig:introduction}. Assuming $M$ as a Large language model (LLM) with a vocabulary size of $V$ and the mask token [MASK] $ \in V$. Now we consider a federated environment with $K$ participating clients. As shown in Figure~\ref{fig:task_description}, each client $k$ possesses a labeled dataset $T_k$ comprising $n_k$ data instances, along with a much larger unlabeled dataset $U_k$ with $u_k$ instances, where $n_k \ll u_k$. For each client, given an input sequence example $x=(s_1,...,s_n)$ where each word $s_j \in V$, we can employ the $P(x)$ to add a task description phrase with a mask token to the input sequence, allowing the LLM to predict the word for the mask position. Using the mapping $L \rightarrow V$, we associate the label $l \in L$ of $x$ with a word $v \in V$. 

According to the above definition, our goal is to leverage the LLM $M$ to predict the probability of each $v \in V$ at the position of the mask token in $P(x)$. For an input sequence example $x$, we compute the score of $x$ with label $l \in L$ as follows:
\begin{equation}
s_P(l|x)=\sum_{v \in V}M(v|P(x)) \ .
\end{equation}
Upon acquiring the probability distribution over the labels through a softmax function, we can compare the predicted probability of the true word and measure the prediction with a standard cross-entropy loss, given below:
\begin{equation}
L_{CE} = \frac{1}{n} \sum_{i = 1}^n(-\log \frac{e^{s_P(l|x)}}{\sum_{l' \in L}e^{s_P(l'|x)}}) \ .
\label{loss}
\end{equation}
Give CE loss in Eq.(\ref{loss}), we can fine-tune the LLM $M$. 

For each client, we utilize multiple task descriptions $P \in \mathcal{P}$ to fine-tune the local model with the aforementioned approach. Furthermore, to address the communication burden arising from the substantial parameter size of the LLM, we utilize the Parameter-Efficient Fine-Tuning technique known as Low-Rank Adaptation (LoRA)~\cite{hu2021lora} during the fine-tuning phase. 

LoRA involves preserving the parameters of the LLM itself without modification. Specifically, LoRA introduces a fine-tuning-only bottleneck module which forms a residual connection to its original parameters $W_0$ to the original LLM. This bottleneck module is composed of two matrices A and B. Matrix $A$ reduces the input dimension from $d$ to $r$, then matrix $B$ restores the output dimension from $r$ to $k$, thereby simulating the concept of intrinsic rank, as shown in Eq.(\ref{lora}). 
\begin{equation}
\begin{split}
&W_0 + \Delta{W} = W_0 + BA, \\
s.t. \quad &  B \in \mathcal{R}^{d \times r}, A \in  \mathcal{R}^{r \times k}, r \ll min(d,k) \ . 
\end{split}
\label{lora}
\end{equation}

Throughout the training period, the pre-trained $W_0$ remains fixed and does not undergo gradient updates. The trainable parameters are the bottleneck parameter $A$ and $B$. In our work, we fixed the $r$ as $8$, which the trainable parameters will be about 30\% of the all parameters. Following the completion of a global training round, each client exclusively uploads $\Delta W$ to the server for parameter averaging. We adopt FedAvg as the method for parameter averaging as follows:
\begin{equation}
\min_{\Delta{W}\in \mathbb{R}^{d \times k}} \sum_{k=1}^{K}\frac{n_k}{n}F_k(\Delta{W}), \quad s.t. \quad F_k(\Delta{W})=\frac{1}{n_k}\sum_{i\in P_k}f_i(\Delta{W}) \ ,
\end{equation}
in which $f_i(w)$ is the loss function for client $i$, and $n$ is the total number of clients in the system. 

We employ an iterative approach wherein models are trained on continuously expanding datasets over multiple epochs. Each client assigns soft labels to a subset of their local unlabeled data using the global model after FedAvg. When labeling the unlabeled data $x \in U$, the prediction accuracy $a_P$ of each task description $P$ on the validation set is utilized as a weight. The soft label for $x$ is then obtained by performing a weighted average, which as follows:
\begin{equation}
s(l|x) = \frac{1}{Z} \sum_{P \in \mathcal{P}} a_P \cdot s_P(l|x), \quad s.t. \quad Z = \sum_{P \in \mathcal{P}} a_P \ .
\end{equation}

The model is subsequently fine-tuned on both the original labeled data and the newly annotated data, iterating through all rounds of training. 
We provide the algorithm details in~\ref{algo:Fed-LoRA} to illustrate the training procedures.

\input{sections/algo_2_dp}



