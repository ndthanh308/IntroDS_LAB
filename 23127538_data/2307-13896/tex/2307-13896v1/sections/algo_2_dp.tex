{%\SetAlgoNoLine
\begin{algorithm}[htp]
\small
\DontPrintSemicolon
%\LinesNumberedHidden
\caption{\textbf{LP-FL overview}. ${M}$ is a global LLM; $T$ is the local labeled data, $U$ is the local unlabeled data ($|T| \ll |U|$); $l$ is step size, $G$ is the number of global rounds; $E$ is the number of  epochs.} \label{algo:Fed-LoRA}
% \KwIn{A global PLM ${M}$, the learning-rate $l$, the global round $G$ and the local epochs $E$.
% }
% \KwOut{A global fine-tuned model ${M}$.}
\textbf{Server executes:} \;
\begin{Indp} Initialize global LLM ${M}$ with LoRA bottleneck modules, and only the parameters of LoRA bottleneck modules $w$ can be trained;

     $l \leftarrow 5 \times 10^{-5}$; \ $G \leftarrow 5$; \ $E \leftarrow 5$; \

\While{g $\leq$ $G$}{
    % $m \leftarrow \max(C\cdot K, 1)$ \;
    % $S_t \leftarrow $ (a random subset of $m$ clients) \;

    \For{\textup{each client} $k$ in $K$ clients \textup{\textbf{in parallel}}}{ 
    $w_{g+1}^k \leftarrow $ ClientUpdate $(k, w_g)$ \;
    }
    $w_{g+1} \leftarrow FedAvg(w_{g+1}^{1:K}) $  \;
    
    Server sends $w_{t+1}$ back to clients \;
    $g \leftarrow g+1$ \;
}
Return $M$ \;
\;

\end{Indp}
\textbf{ClientUpdate}$(k, w)$: // \textit{Run on client k} \;
\begin{Indp}
    $T$ is the local labeled data, $U$ is the local unlabeled data
    
    \eIf{$g < 5$} {
    $S \leftarrow $ label the $25\%$ of the $U$ \;
    $T \leftarrow T + S$ \;
    $U \leftarrow U - S$
    }{
    continue
    }
    
    $\mathcal{B} \leftarrow $ (split T and U into batches of size $B$) \;
    
    \For{\textup{each local epoch} $i$ \textup{from 1 to} $E$}{
        \For{\textup{batch b} $\in \mathcal{B}$ }{
            $w \leftarrow w$ - $\eta \nabla \ell(w;b)$
        }
    }

Return $w$ \;
\end{Indp}
\end{algorithm}
}