\documentclass[runningheads]{llncs}
\pdfoutput=1
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{textcomp}

\usepackage{booktabs} % For formal tables
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{soul,color}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{enumitem}
\usepackage{algorithmic}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{booktabs}
\usepackage{caption}
% \usepackage{subcaption}
\usepackage{url}
\usepackage{cite}
% \newtheorem{prop}{Proposition}
% \newtheorem{theorem}{Theorem}
% \newtheorem{lemma}{Lemma}
% \newtheorem{remark}{Remark}
% \newtheorem{definition}{Definition}
% \newtheorem{corollary}{Corollary}
\usepackage{wrapfig, blindtext}
\usepackage{etoolbox}
\usepackage{bbm}
\usepackage{float} 
\usepackage{makecell}
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{subfig}


\newcommand{\fdl}{federated learning\ }
\newcommand{\Fdl}{Federated learning\ }
\newcommand\revise{\colorbox{yellow}{\textcolor{red}{REVISE}}}
\newcommand\att{\colorbox{yellow}{\textcolor{red}{ATTENTION}}}
\newcommand{\etal}{et al.}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\pt}{pre-training\ }
\newcommand{\Pt}{Pre-training\ }
\newcommand{\llm}{large language model\ }
\newcommand{\LLM}{Large Language Model\ }
\newcommand{\LLMs}{Large Language Models\ }
\newcommand{\synm}{synonymous\ }
\newcommand{\hlc}[2][yellow]{{%
    \colorlet{foo}{#1}%
    \sethlcolor{foo}\hl{#2}}%
}

\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Low-Parameter Federated Learning with Large Language Models}
%
\titlerunning{Low-Parameter Federated Learning with Large Language Models}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%

\author{Jingang Jiang  \and Xiangyang Liu \and
Chenyou Fan}
%
\authorrunning{J. Jiang et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%

\institute{South China Normal University, Guangdong, China \\
\email{\{2022024923,2022024952\}@m.scnu.edu.cn, fanchenyou@scnu.edu.cn}}



\maketitle              % typeset the header of the contribution
%
\begin{abstract}
% We study few-shot Natural Language Understanding (NLU) tasks with Large Language Models (LLMs) in a federated learning scenarios. This is a practical yet challenging task due to the insufficient labeled data and limited communication capacities in common Federated Learning (FL) scenarios, such as learning with mobile devices. Recent studies show that LLMs for NLU can be prompted to perform few-show NLU tasks, such as sentiment analysis and arithmetic reasoning. 
% % However, the enormous parameter size leads to substantial communication costs in federated learning, rendering the implementation of the conventional FL approaches such as FedAvg unfeasible. 
% However, the huge sizes of LLMs result in formidable computation and communication costs so that classical FL schemes such as FedAvg can be impractical, especially with mobile devices.
% To address these challenges, we propose a novel method called Low-Parameter Federated Learning (LP-FL), which combines the few-shot prompt learning capabilities inherited from LLMs with our proposed efficient communication and federating techniques. 
% % Our approach  enables federated clients to assign soft labels to unlabeled data by utilizing the knowledge embedded in the global model. This is achieved through iterative and semi-supervised training, effectively leveraging both labeled and unlabeled data to tackle data insufficiency. 
% % To reduce communication costs, our LP-FL utilize the Low-Rank Adaptation (LoRA) technique to achieve efficient parameter fine-tuning and federation. To further tackle the data insufficiency issue, we design a training progress which continually expands the dataset by incorporating both labeled and soft-labeled data. 
% Our LP-FL approach addresses the data insufficiency issue by enabling federated clients to assign soft labels to unlabeled data using the knowledge gradually learned and emerged in the global model. This is achieved through iterative soft-label assigning which  expands the labeled set continually through the FL process. Additionally, to reduce computation and communication costs, LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnable parameter construction, efficient local model fine-tuning, and affordable global model federation. 
% %We also design a training process that continually expands the dataset by incorporating both labeled and soft-labeled data.
% We found that our proposed LP-FL can solve the sentiment analysis task in various FL settings consistently better than Full-Parameter Federated Learning (FP-FL). Our method can equal or even surpass centralized training in few-shot settings due to its resistance to over-fitting.
We study few-shot Natural Language Understanding (NLU) tasks with Large Language Models (LLMs) in federated learning (FL) scenarios. It is a challenging task due to limited labeled data and communication capacities in FL, especially with mobile devices. Recent studies show LLMs can be prompted to perform few-shot NLU tasks like sentiment analysis and arithmetic reasoning. However, the huge sizes of LLMs result in high computation and communication costs, making classical FL schemes impractical. To address these challenges, we propose Low-Parameter Federated Learning (LP-FL). LP-FL combines few-shot prompt learning from LLMs with efficient communication and federating techniques. Our approach enables federated clients to assign soft labels to unlabeled data using gradually learned knowledge from the global model. Through iterative soft-label assigning, we continually expand the labeled set during the FL process. Additionally, to reduce computation and communication costs, LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnable parameter construction, efficient local model fine-tuning, and affordable global model federation. LP-FL consistently outperforms Full-Parameter Federated Learning (FP-FL) in sentiment analysis tasks across various FL settings. Its resistance to over-fitting allows LP-FL to equal or surpass centralized training in few-shot scenarios.

% 联邦环境下各客户端无法收集到足够多的标签数据用来训练一直是研究者们关注的问题，大模型prompt learning在包括情感分析在内的很多文本任务表现出不错的few-shot能力，但是大模型的参数规模太大导致联邦通信成本高昂，因此我们设计了一个既能很好利用prompt learning的few-shot能力，又能降低通信成本的训练方法（Fed-LoRA）。我们设计了一个迭代的半监督的训练过程，每个客户端先利用本地的少量有标签数据用类似继续mlm预训练的方式微调模型，为了降低通信成本，我们使用lora参数高效微调，联邦lora的参数，客户端使用联邦后的全局模型给无标签数据标注软标签，使用原本的标签数据和标注后的软标签数据继续训练，利用规模越来越大的数据集训练几代模型。在情感分析任务上，最后我们的方法相比与集中式训练下（cen-lora）的模型只损失2%左右的性能，甚至在传统文本分类的全参数微调上也有一定提高。



\keywords{Federated Learning \and Large Language Model \and Prompt Learning \and Low-Rank Adaptation \and Sentiment Analysis.}
\end{abstract}
%
%
%

\input{sections/introduction}
\input{sections/related}
\input{sections/approach}
\input{sections/experiments}


\section{Conclusion} % Good for now
We have validated that pre-trained large language models (LLMs) can attain reasonable classification accuracy in sentiment classification tasks with minimal labeled samples across different clients with federated learning paradigm. 
% In the context of natural low-resource settings, where a substantial number of mobile devices lack access to an adequate amount of labeled data, our approach enables maximum device participation in federated training.
Our proposed Low-Parameter Federated Learning (LP-FL) permits federated clients to assign soft labels to unlabeled data, utilizing the evolving knowledge from the global model. 
Given the inherent constraints of mobile devices, such as limited computational resources and unreliable network environments, we achieve comparable, and sometimes superior performance to Full-Parameter Federated Learning (FP-FL) by fine-tuning and federating only a fraction of the model parameters at each local client.  Our FP-FL approach establishes an effective learning framework for leveraging LLMs in the FL environment and shows promising applications over mobile devices. 


{
\bibliographystyle{splncs04}
\bibliography{egbib,fed,plm}
}



\end{document}
