@inproceedings{intro,
 author = {K. Nimmi, B. Janet, A. Kalai Selvan, N. Sivakumaran},
 booktitle = {Applied Soft Computing},
 title = {pre-trained ensemble model for identification of emotion during COVID-19 based on emergency response support system dataset},
 url = {https://doi.org/10.1016/j.asoc.2022.108842.},
 volume = {122},
 year = {2022}
}
@inproceedings{zadeh2018multi,
  title={Multi-attention recurrent network for human communication comprehension},
  author={Zadeh, Amir and Liang, Paul Pu and Poria, Soujanya and Vij, Prateek and Cambria, Erik and Morency, Louis-Philippe},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{washington2023review,
  title={A Review of and Roadmap for Data Science and Machine Learning for the Neuropsychiatric Phenotype of Autism},
  author={Washington, Peter and Wall, Dennis P},
  journal={arXiv preprint arXiv:2303.03577},
  year={2023}
}

@inproceedings{ekmanemo,
  title={Facial signs of emotional experience},
  author={Ekman P., Freisen W. V. and Ancoli, S.},
  booktitle={Journal of Personality and Social Psychology},
  url = {https://psycnet.apa.org/record/1981-25797-001},
 volume = {39(6)},
  year={1980}
}

@inproceedings{bagher-zadeh-etal-2018-multimodal,
    title = "Multimodal Language Analysis in the Wild: {CMU}-{MOSEI} Dataset and Interpretable Dynamic Fusion Graph",
    author = "Bagher Zadeh, AmirAli  and
      Liang, Paul Pu  and
      Poria, Soujanya  and
      Cambria, Erik  and
      Morency, Louis-Philippe",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1208",
    doi = "10.18653/v1/P18-1208",
    pages = "2236--2246",
}

@article{bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1810.04805}
}

@article{contrasive,
  author    = {Ting Chen and
               Simon Kornblith and
               Mohammad Norouzi and
               Geoffrey E. Hinton},
  title     = {A Simple Framework for Contrastive Learning of Visual Representations},
  journal   = {CoRR},
  volume    = {abs/2002.05709},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.05709},
  eprinttype = {arXiv},
  eprint    = {2002.05709},
  timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-05709.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{gpt-3, 
  doi = {10.48550/ARXIV.2005.14165},
  
  url = {https://arxiv.org/abs/2005.14165},
  
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Language Models are Few-Shot Learners},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{cpc1,
  doi = {10.48550/ARXIV.1807.03748},
  
  url = {https://arxiv.org/abs/1807.03748},
  
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Representation Learning with Contrastive Predictive Coding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{cpc2,
  doi = {10.48550/ARXIV.1807.03748},
  
  url = {https://arxiv.org/abs/1807.03748},
  
  author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Representation Learning with Contrastive Predictive Coding},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{wav2vec,
  doi = {10.48550/ARXIV.1904.05862},
  
  url = {https://arxiv.org/abs/1904.05862},
  
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {wav2vec: Unsupervised Pre-training for Speech Recognition},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{context,
  doi = {10.48550/ARXIV.1505.05192},
  
  url = {https://arxiv.org/abs/1505.05192},
  
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Unsupervised Visual Representation Learning by Context Prediction},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{data2vec,
  doi = {10.48550/ARXIV.2202.03555},
  
  url = {https://arxiv.org/abs/2202.03555},
  
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{audiovisual,
  doi = {10.48550/ARXIV.2007.04134},
  
  url = {https://arxiv.org/abs/2007.04134},
  
  author = {Shukla, Abhinav and Petridis, Stavros and Pantic, Maja},
  
  keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Multimedia (cs.MM), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{roberta,
  doi = {10.48550/ARXIV.1907.11692},
  
  url = {https://arxiv.org/abs/1907.11692},
  
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{speech,
  doi = {10.48550/ARXIV.1811.07691},
  
  url = {https://arxiv.org/abs/1811.07691},
  
  author = {Lian, Zheng and Li, Ya and Tao, Jianhua and Huang, Jian},
  
  keywords = {Machine Learning (cs.LG), Human-Computer Interaction (cs.HC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Improving speech emotion recognition via Transformer-based Predictive Coding through transfer learning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{speaker,
  doi = {10.48550/ARXIV.1812.00271},
  
  url = {https://arxiv.org/abs/1812.00271},
  
  author = {Ravanelli, Mirco and Bengio, Yoshua},
  
  keywords = {Audio and Speech Processing (eess.AS), Computation and Language (cs.CL), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Sound (cs.SD), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Learning Speaker Representations with Mutual Information},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@article{audio_self,
	doi = {10.1109/taffc.2021.3062406},
  
	url = {https://doi.org/10.1109%2Ftaffc.2021.3062406},
  
	year = 2023,
	month = {jan},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {14},
  
	number = {1},
  
	pages = {406--420},
  
	author = {Abhinav Shukla and Stavros Petridis and Maja Pantic},
  
	title = {Does Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?},
  
	journal = {{IEEE} Transactions on Affective Computing}
}



@misc{cpc,
Author = {Aaron van den Oord and Yazhe Li and Oriol Vinyals},
Title = {Representation Learning with Contrastive Predictive Coding},
Year = {2018},
Eprint = {arXiv:1807.03748},
}
@misc{autoreg,
Author = {Yu-An Chung and Wei-Ning Hsu and Hao Tang and James Glass},
Title = {An Unsupervised Autoregressive Model for Speech Representation Learning},
Year = {2019},
Eprint = {arXiv:1904.03240}}

@article{COVAREPA,
  title={COVAREP — A collaborative voice analysis repository for speech technologies},
  author={Gilles Degottex and John Kane and Thomas Drugman and Tuomo Raitio and Stefan Scherer},
  journal={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year={2014},
  pages={960-964}
}

@article{torous2021growing,
  title={The growing field of digital psychiatry: current evidence and the future of apps, social media, chatbots, and virtual reality},
  author={Torous, John and Bucci, Sandra and Bell, Imogen H and Kessing, Lars V and Faurholt-Jepsen, Maria and Whelan, Pauline and Carvalho, Andre F and Keshavan, Matcheri and Linardon, Jake and Firth, Joseph},
  journal={World Psychiatry},
  volume={20},
  number={3},
  pages={318--335},
  year={2021},
  publisher={Wiley Online Library}
}

@article{washington2020data,
  title={Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry},
  author={Washington, Peter and Park, Natalie and Srivastava, Parishkrita and Voss, Catalin and Kline, Aaron and Varma, Maya and Tariq, Qandeel and Kalantarian, Haik and Schwartz, Jessey and Patnaik, Ritik and others},
  journal={Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
  volume={5},
  number={8},
  pages={759--769},
  year={2020},
  publisher={Elsevier}
}

@article{pepa2021automatic,
  title={Automatic emotion recognition in clinical scenario: a systematic review of methods},
  author={Pepa, Lucia and Spalazzi, Luca and Capecci, Marianna and Ceravolo, Maria Gabriella},
  journal={IEEE Transactions on Affective Computing},
  year={2021},
  publisher={IEEE}
}

@article{glass,
author = {Washington, Peter and Voss, Catalin and Kline, Aaron and Haber, Nick and Daniels, Jena and Fazel, Azar and De, Titas and Feinstein, Carl and Winograd, Terry and Wall, Dennis},
title = {SuperpowerGlass: A Wearable Aid for the At-Home Therapy of Children with Autism},
year = {2017},
issue_date = {September 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {3},
url = {https://doi.org/10.1145/3130977},
doi = {10.1145/3130977},
abstract = {We have developed a system for automatic facial expression recognition running on Google Glass, delivering real-time social cues to children with Autism Spectrum Disorder (ASD). The system includes multiple mechanisms to engage children and their parents, who administer this technology within the home. We completed an at-home design trial with 14 families that used the learning aid over a 3-month period. We found that children with ASD generally respond well to wearing the system at home and opt for the most expressive feedback choice. We further evaluated app usage, facial engagement, and model accuracy. We found that the device can act as a powerful training aid when used periodically in the home, that interactive video content from wearable therapy sessions should be augmented with sufficient context about the content to produce long-term engagement, and that the design of wearable systems for children with ASD should be heavily dependent on the functioning level of the child. We contribute general design implications for developing wearable aids used by children with ASD and other behavioral disorders as well as their parents during at-home parent-administered therapy sessions.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {112},
numpages = {22},
keywords = {Wearable Computing, Autism, Behavior Therapy}
}

@article{guesswhat, title={Labeling images with facial emotion and the potential for pediatric healthcare}, volume={98}, ISSN={0933-3657}, url={http://dx.doi.org/10.1016/j.artmed.2019.06.004}, DOI={10.1016/j.artmed.2019.06.004}, journal={Artificial intelligence in medicine}, author={Kalantarian, Haik and Jedoui, Khaled and Washington, Peter and Tariq, Qandeel and Dunlap, Kaiti and Schwartz, Jessey and Wall, Dennis P.}, year={2019}, pages={77–86}, language={en} }

@article{izquierdo2018emotion,
  title={Emotion recognition for semi-autonomous vehicles framework},
  author={Izquierdo-Reyes, Javier and Ramirez-Mendoza, Ricardo A and Bustamante-Bello, Martin R and Pons-Rovira, Jose L and Gonzalez-Vargas, Jose E},
  journal={International Journal on Interactive Design and Manufacturing (IJIDeM)},
  volume={12},
  pages={1447--1454},
  year={2018},
  publisher={Springer}
}

@article{sini2020automatic,
  title={Automatic emotion recognition for the calibration of autonomous driving functions},
  author={Sini, Jacopo and Marceddu, Antonio Costantino and Violante, Massimo},
  journal={Electronics},
  volume={9},
  number={3},
  pages={518},
  year={2020},
  publisher={MDPI}
}

@article{dai2015emotion,
  title={Emotion recognition and affective computing on vocal social media},
  author={Dai, Weihui and Han, Dongmei and Dai, Yonghui and Xu, Dongrong},
  journal={Information \& Management},
  volume={52},
  number={7},
  pages={777--788},
  year={2015},
  publisher={Elsevier}
}

@article{seng2017video,
  title={Video analytics for customer emotion and satisfaction at contact centers},
  author={Seng, Kah Phooi and Ang, Li-Minn},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={48},
  number={3},
  pages={266--278},
  year={2017},
  publisher={IEEE}
}

@article{kline2019superpower,
  title={Superpower glass},
  author={Kline, Aaron and Voss, Catalin and Washington, Peter and Haber, Nick and Schwartz, Hessey and Tariq, Qandeel and Winograd, Terry and Feinstein, Carl and Wall, Dennis P},
  journal={GetMobile: Mobile Computing and Communications},
  volume={23},
  number={2},
  pages={35--38},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{voss2016superpower,
  title={Superpower glass: delivering unobtrusive real-time social cues in wearable systems},
  author={Voss, Catalin and Washington, Peter and Haber, Nick and Kline, Aaron and Daniels, Jena and Fazel, Azar and De, Titas and McCarthy, Beth and Feinstein, Carl and Winograd, Terry and others},
  booktitle={Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
  pages={1218--1226},
  year={2016}
}

@article{voss2019effect,
  title={Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: a randomized clinical trial},
  author={Voss, Catalin and Schwartz, Jessey and Daniels, Jena and Kline, Aaron and Haber, Nick and Washington, Peter and Tariq, Qandeel and Robinson, Thomas N and Desai, Manisha and Phillips, Jennifer M and others},
  journal={JAMA pediatrics},
  volume={173},
  number={5},
  pages={446--454},
  year={2019},
  publisher={American Medical Association}
}

@article{daniels2018exploratory,
  title={Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism},
  author={Daniels, Jena and Schwartz, Jessey N and Voss, Catalin and Haber, Nick and Fazel, Azar and Kline, Aaron and Washington, Peter and Feinstein, Carl and Winograd, Terry and Wall, Dennis P},
  journal={NPJ digital medicine},
  volume={1},
  number={1},
  pages={32},
  year={2018},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{kline2020superpower,
  title={superpower glass: An augmented reality intervention for improving social deficits in children with autism spectrum disorder},
  author={Kline, Aaron and Ning, Michael and Husic, Arman and Washington, Peter and Voss, Catalin and Dunlap, Kaitlyn L and Penev, Yordan and Leblanc, Emilie and Haber, Nick and Wall, Dennis},
  booktitle={INSAR 2020 Virtual Meeting},
  year={2020},
  organization={INSAR}
}

@inproceedings{washington2016wearable,
  title={A wearable social interaction aid for children with autism},
  author={Washington, Peter and Voss, Catalin and Haber, Nick and Tanaka, Serena and Daniels, Jena and Feinstein, Carl and Winograd, Terry and Wall, Dennis},
  booktitle={Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems},
  pages={2348--2354},
  year={2016}
}

@article{daniels2018feasibility,
  title={Feasibility testing of a wearable behavioral aid for social learning in children with autism},
  author={Daniels, Jena and Haber, Nick and Voss, Catalin and Schwartz, Jessey and Tamura, Serena and Fazel, Azar and Kline, Aaron and Washington, Peter and Phillips, Jennifer and Winograd, Terry and others},
  journal={Applied clinical informatics},
  volume={9},
  number={01},
  pages={129--140},
  year={2018},
  publisher={Schattauer GmbH}
}

@article{kalantarian2019guess,
  title={Guess What? Towards Understanding Autism from Structured Video Using Facial Affect},
  author={Kalantarian, Haik and Washington, Peter and Schwartz, Jessey and Daniels, Jena and Haber, Nick and Wall, Dennis P},
  journal={Journal of healthcare informatics research},
  volume={3},
  pages={43--66},
  year={2019},
  publisher={Springer}
}

@inproceedings{kalantarian2018gamified,
  title={A gamified mobile system for crowdsourcing video for autism research},
  author={Kalantarian, Haik and Washington, Peter and Schwartz, Jessey and Daniels, Jena and Haber, Nick and Wall, Dennis},
  booktitle={2018 IEEE international conference on healthcare informatics (ICHI)},
  pages={350--352},
  year={2018},
  organization={IEEE}
}

@article{kalantarian2020performance,
  title={The performance of emotion classifiers for children with parent-reported autism: quantitative feasibility study},
  author={Kalantarian, Haik and Jedoui, Khaled and Dunlap, Kaitlyn and Schwartz, Jessey and Washington, Peter and Husic, Arman and Tariq, Qandeel and Ning, Michael and Kline, Aaron and Wall, Dennis Paul and others},
  journal={JMIR mental health},
  volume={7},
  number={4},
  pages={e13174},
  year={2020},
  publisher={JMIR Publications Inc., Toronto, Canada}
}

@article{kalantarian2018mobile,
  title={A mobile game for automatic emotion-labeling of images},
  author={Kalantarian, Haik and Jedoui, Khaled and Washington, Peter and Wall, Dennis P},
  journal={IEEE transactions on games},
  volume={12},
  number={2},
  pages={213--218},
  year={2018},
  publisher={IEEE}
}

@article{washington2022improved,
  title={Improved Digital Therapy for Developmental Pediatrics Using Domain-Specific Artificial Intelligence: Machine Learning Study},
  author={Washington, Peter and Kalantarian, Haik and Kent, John and Husic, Arman and Kline, Aaron and Leblanc, Emilie and Hou, Cathy and Mutlu, Onur Cezmi and Dunlap, Kaitlyn and Penev, Yordan and others},
  journal={JMIR Pediatrics and Parenting},
  volume={5},
  number={2},
  pages={e26760},
  year={2022},
  publisher={JMIR Publications Toronto, Canada}
}

@article{hou2021leveraging,
  title={Leveraging video data from a digital smartphone autism therapy to train an emotion detection classifier},
  author={Hou, Cathy and Kalantarian, Haik and Washington, Peter and Dunlap, Kaiti and Wall, Dennis P},
  journal={medRxiv},
  pages={2021--07},
  year={2021},
  publisher={Cold Spring Harbor Laboratory Press}
}
@ARTICLE{crema,
  author={Cao, Houwei and Cooper, David G. and Keutmann, Michael K. and Gur, Ruben C. and Nenkova, Ani and Verma, Ragini},
  journal={IEEE Transactions on Affective Computing}, 
  title={CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset}, 
  year={2014},
  volume={5},
  number={4},
  pages={377-390},
  doi={10.1109/TAFFC.2014.2336244}}

  @article{ravdess,
  doi = {10.1371/journal.pone.0196391},
  url = {https://doi.org/10.1371/journal.pone.0196391},
  year = {2018},
  month = may,
  publisher = {Public Library of Science ({PLoS})},
  volume = {13},
  number = {5},
  pages = {e0196391},
  author = {Steven R. Livingstone and Frank A. Russo},
  editor = {Joseph Najbauer},
  title = {The Ryerson Audio-Visual Database of Emotional Speech and Song ({RAVDESS}): A dynamic,  multimodal set of facial and vocal expressions in North American English},
  journal = {{PLOS} {ONE}}
}

@article{IEMOCAPIE,
  title={IEMOCAP: interactive emotional dyadic motion capture database},
  author={Carlos Busso and Murtaza Bulut and Chi-Chun Lee and Ebrahim (Abe) Kazemzadeh and Emily Mower Provost and Samuel Kim and Jeannette N. Chang and Sungbok Lee and Shrikanth S. Narayanan},
  journal={Language Resources and Evaluation},
  year={2008},
  volume={42},
  pages={335-359}
}


@article{kaldids,
author = {Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukáš and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlíček, Petr and Qian, Yanmin and Schwarz, Petr and Silovský, Jan and Stemmer, Georg and Vesel, Karel},
year = {2011},
month = {01},
pages = {},
title = {The Kaldi speech recognition toolkit},
journal = {IEEE 2011 Workshop on Automatic Speech Recognition and Understanding}
}

@misc{pase,
  doi = {10.48550/ARXIV.1904.03416},
  url = {https://arxiv.org/abs/1904.03416},
  author = {Pascual,  Santiago and Ravanelli,  Mirco and Serrà,  Joan and Bonafonte,  Antonio and Bengio,  Yoshua},
  keywords = {Machine Learning (cs.LG),  Sound (cs.SD),  Audio and Speech Processing (eess.AS),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Electrical engineering,  electronic engineering,  information engineering,  FOS: Electrical engineering,  electronic engineering,  information engineering},
  title = {Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks},
  publisher = {arXiv},
  year = {2019},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{secost,
  author       = {Anurag Kumar and
                  Vamsi Krishna Ithapu},
  title        = {SeCoST: Sequential Co-Supervision for Weakly Labeled Audio Event Detection},
  journal      = {CoRR},
  volume       = {abs/1910.11789},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.11789},
  eprinttype    = {arXiv},
  eprint       = {1910.11789},
  timestamp    = {Tue, 21 Dec 2021 10:09:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-11789.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

