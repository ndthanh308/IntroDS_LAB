%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Autonomous Decisions\cite{doxFrame}}\label{sec:autonom}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\normalsize
In this section, we formalize a notion of \emph{autonomous decision} and then characterize when a system exists that can take autonomous decisions to accomplish their goals.
%

Notions of autonomy are discussed in various scientific fields, as we outline in \autoref{sec:autoSys}.
Our formalization aims at capturing that \texttt{(bel\label{as:belief})} an autonomous system must take decisions based on its internal world view (i.e. its belief) and that \texttt{(rat\label{as:rational})} the system chooses the choice alternative that promises the best outcome, i.e. the system is somehow rational. 
We distinguish autonomous decisions from \emph{automatic decisions}, that play out rule-determined choices and are not the rational consequence \wrt the belief content.
The difference between autonomous and automatic decisions is illustrated by \autoref{ex:autonAutom} below.
%
\begin{mexample}{Autonomous decisions \vs automatic decisions}\label{ex:autonAutom}
	Suppose that \ego has a permanently broken sensor that flips the colors (cf. \autoref{fig:switched}) and that \ego believes in its sensors. 

	% Figure environment removed
		So, if \other is red, \ego thinks that \other is blue and vice versa.
	Suppose moreover, that \ego knows that a red car is hasty and a blue car is slow.
	If \ego decides autonomously, then it will follow a strategy highlighted by bold arrows, \ie it will go straight on, if a red car is approaching and it will take the turn when a blue car is approaching. This strategy promises the best outcome \wrt \ego's beliefs.
	 Since it beliefs in its sensors and chooses rationally, it takes the worst possible decisions. 

	Let us now consider a system that plays out automatic decisions. Suppose an engineer is aware, that the sensor switches colours\footnote{Maybe just under certain conditions and hence the engineer did not bother to change the belief formation and decided to patch this problem by implementing a rule}. He hence equips \ego with a rule, that switches the chosen actions accordingly: \enquote{Do not take the turn, when you think a red car is approaching. Takes the turn, when you think a blue car is approaching}. 
		This choice does not make sense to \ego, it is not rational \wrt to its belief content, but, in our scenario it is better when evaluated on the design time world.
\end{mexample}
%
	The above example \autoref{ex:autonAutom} highlights, that autonomous decisions are not necessarily better than automatic decisions. 
	Since the system \HAS is missing some relevant aspect of the design time world, it takes the wrong decisions. 
	We characterize in Def.~\ref{def:optimal} when a system \HAS can act successfully.
	
\begin{excursus}{Autonomous vs Automatic}
	Although, in the above example the automatic system outperforms the autonomous system,
	autonomous systems promise to be more capable of dealing with new situations. 
	An autonomous system chooses the best possible option based on extrapolation of the system's world view.	
	So once a robust world model for extrapolation has been build, an autonomous system will take \enquote{sensible} decisions.
	Hence a design task when building a system \HAS, is to determine the relevant aspects of the world models and to assess the impact of missing information and sensor perturbations.
	The quality of this extrapolation can be validated by means of runtime monitoring.

Since an automatic system plays out rules, an unforeseen event in the real world might result in situations where no rule applies anymore. 
	The challenge for developing an automatic system hence lies in defining a robust set of rules.  
	This set of rules also has to be evaluated regarding all possible scenarios. 
	It has to be evaluated whether all cases have been identified, when the rule should trigger and whether in these situations, the encoded behaviour is appropriate.
	The expected occurrence a situations satisfying the rule's antecedent are often quite rare, what makes the validation during runtime more difficult.
\end{excursus}

In order to formalize when an \emph{autonomous decision} can be taken successfully, we contrast 
\begin{enumerate}
	\item \emph{truth-observing strategies} (strategies that have access to ground-truth) with 
	\item \emph{doxastic strategies} (strategies that can only observe the formed beliefs) and 
	\item \emph{possible-world strategies} (strategies that run as simulation within the beliefs).
\end{enumerate}
The best truth-observing strategy represents what any system can possibly achieve.
The best doxastic strategy represents what a system with a given belief formation can possibly achieve. 
If the best doxastic strategy performs as good as the best truth-observing strategy, we say that an autonomous system is successful. 
Since our systems choose rationally, they choose what seems to be the best choice according to their belief content. 
The best possible-world strategy describes  what the system believes to be the best strategy in all possible worlds.

The different strategy notions are introduced step by step in the following and an overview of the notions is given in \autoref{tab:nutshell} on page \pageref{tab:nutshell}.
%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Truth-Observing Strategy} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
In our framework, we use truth-observing strategies as reference of what would be achievable, if \ego could directly access the ground truth \universeD via a set of propositions $\props\subseteq\Props_{d}$. 
%%%
To this end, we say \ego implements a \emph{$\props$-truth-observing strategy} $\strats:(2^{\props})^+\rightarrow\Act_{\ego}$, if \ego chooses its actions based on the history of values of $\props$ as observed in the ground-truth model \universeD. 
When \ego is at state \state of \universeD, a state that was reached via path \Path with $\Label(\Path_{\leq i})|_{\props}=\history$ and $\state=\Path(i)$, it chooses ${\strats}(\history)$. 
A truth-observing strategy \strats together with a sequence of environment actions $\e\in\Act_{\env}^{\omega}$ determines a set of traces, $\Cmp(\e,\strats)$.
Formally, $\Cmp(\e,\strats) = \{\cmp_0 \cmp_1\ldots \in (2^{\Props_D})^\omega |
\exists \textit{ path } \Path \textit{ from } \Init_D, \forall i\geq 0:
\cmp_i = \Label_D(\Path(i)) \land 
\act_i:=\strats(\Label_D(\Path_{\leq i})|_{\props}) \land
(\act_i,\e(i))\in\Label_D(\Path_i,\Path_{i+1})\}$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Doxastic Strategy} Since a system \HAS has no direct access to the ground truth,  it has to decide based on its history of beliefs. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
We formalize this by the notion of doxastic strategy.
At a state $\state=\Path(i)$ in \universeD \ego takes a decision based on the history of its beliefs $\bhist_0\ldots\bhist_i$ that \ego has built along $\Path_{\leq i}$.
So to implement the \emph{doxastic strategy} $\stratb:\Beliefs^{+}\rightarrow\Act_{\ego}$ on $\universeD$,  \ego chooses $\stratb(\LabelB(\Path_{\leq i}))$.
%
A strategy \stratb together with a  sequence of environment actions $\e\in\Act_{\env}^{\omega}$ determines a set of traces in \universeD, just like for truth-observing strategies. 
The set of traces is $\Cmp(\e,\stratb) = \{\cmp_0 \cmp_1\ldots \in (2^{\Props_D})^\omega |
\exists \textit{ path } \Path \textit{ from } \Init_\epi, \forall i\geq 0:
\cmp_i = \Label_\epi(\Path(i)) \land 
\act_i:=\stratb(\LabelB(\Path_{\leq i})) \land
(\act_i,\e(i))\in\Label_\epi(\Path_i,\Path_{i+1})\}$.

Note that  doxastic strategy indirectly depends on what is observable:  the belief formation \LabelB (cf. Def.~\ref{def:bform}) observes only a certain set of observations.\\

\myparagraph{Dominance, $\strat'\leq_{\universe,\psi}\strat$}Since truth-observing and doxastic strategies both determine traces for a given sequence of environment actions,  we can compare them straight forwardly:  A strategy \strat achieves a goal list \goalList up to $n$ on \universe, if no matter what the environment does, \strat achieves $\goalList$ up to $n$, \ie the set $\bigcup_{\e\in\Act_{\env}^{\omega}}\cmp\in\Cmp(\e,\strat)$ satisfies $\goalList$ up to $n$ (cf. page \pageref{def:goals}).
%
A strategy \emph{\strat \goalList-dominates a strategy $\strat'$ on $\universe$}, $\strat'\leq_{\universe,\psi}\strat$, iff $\strat'$ achieves \goalList up to $n'$ and \strat up to $n$ where $n'\leq n$.
We also say \emph{$\strat'$ $\varphi$-dominates $\strat$}, $\strat'\leq_{\universe,\varphi}\strat$, for an LTL property $\varphi$, iff $\strat'\leq_{\universe,\psi}\strat$ for the goal list $\psi$ with the singleton goal set $\Phi=\{\varphi\}$.
We omit $\universe$ if it is clear from the context.
%
\begin{mexample}{Truth-Observing and Doxastic Strategies}\label{ex:tods}
	As an example of a dominant \props-truth-observing strategy, let us consider 
	\begin{itemize}[leftmargin=4mm]
		\item the goal list of \autoref{ex:goals}  on page \pageref{ex:goals}, ($\varphi_c$, \ie no collisions, is more important than $\varphi_t$,\ie do a turn), 
		\item the world model in \autoref{fig:setup}(b) on page \pageref{fig:setup}, 
		\item the propositions $\props:=\{\xego,\textsf{s},\textsf{h}\}$ to be observable by \strats and 
		\item the strategy \strats that chooses to drive straight on, if \other is hasty, and that chooses to turn, if \other is slow\\
	(it maps \Small{$1s\mapsto f$, $1s,2s\mapsto f$, $1s,2s,3s \mapsto f$, $1s,2s,3s,4s\mapsto f$, and  $1h\mapsto f$, $1h,2h\mapsto t$, \ldots}).
	\end{itemize}
	Strategy \strats achieves \goalList only up to $\varphi_c$, \ie collision-freedom, and \strats is a dominant ($\props$-truth-observing) strategy, since in all cases collision-freedom is guaranteed and
	in case the car is slow, no other strategy can do better, \ie realize both, collision-freedom and the turn.

	Let us now consider a doxastic strategy \stratb.
	\begin{itemize}
		\item We consider the same goal list and world model as for \strats.
		\item We take the konwledge-consistent belief formation \LabelB as sketched in \autoref{fig:bellab} on page \pageref{fig:bellab}, \ie \ego always believes in its sensor readings and its sensor initially switches colours.
			
			Its set of observables is $\Obs:=\Props_{x_e}\cup\Props_{y_e}\cup\{\textsf{undef, \bp, \rp}\}$ (cf. \autoref{ex:hist}, p.~\pageref{ex:hist}). The knowledge base is defined on p.~\pageref{ex:kb},~\autoref{ex:kb}, as  $\{$\fs
			$\varphi_z$ (\other is at most at $x=4$),
					{{$\varphi_t$}} (a turn is only possible at $x=2$),
					$\varphi_i$ (\ego starts at $x=1$), 
					$\varphi_{ct}$ (the initial car type does not change)\}\normalsize.

				\item Let \stratb be a doxastic strategy with \Small{$B_{0,1}\mapsto f$, $B_{0,1}\,B_{1,1}\mapsto f$, $\ldots$} and \Small{$B_{0,2}\mapsto f$, $B_{0,2}\,B_{1,2}\mapsto t$, $\ldots$}. \stratb is illustrated in \autoref{fig:belstrat}.				
	\end{itemize}
	% Figure environment removed

	Just like \strats,  \stratb chooses to turn when \other is hasty (\Small{$B_{0,2}\,B_{1,2}\mapsto t$}), and it chooses to drive straight on, if \other is slow (\Small{$B_{0,1}\,B_{1,1}\mapsto f$}). 
	As there is \enquote{no better} strategy, \stratb is dominant. 
\end{mexample}
%

Next, we want to capture that \ego chooses its actions based on the \emph{content} of its beliefs. In order to motivate our formalization, let us consider the following example, where \ego does not choose its actions based on its belief content.
\begin{mexample}{Decisions Not Based on the Belief Content}\label{ex:noCon}
	We modify our running example slightly: Let us assume the colour perception is severely broken and permanently switches red to blue and vice versa.	
	In \autoref{fig:bellab2} the changed world model is given along with a belief formation that relies on the colour perception, i.e., if the sensors say the other car is red (blue), then \ego believes the other car is red (blue).
	% Figure environment removed

	Let $\stratb'$ be a doxastic strategy with \Small{$B_{0,1}\mapsto f$, $B_{0,1},B_{1,2}\mapsto f$, $\ldots$} and \Small{$B_{0,2}\mapsto f$, $B_{0,2},B_{1,1}\mapsto t$, $\ldots$}. 
	Just as \strats and \stratb,  the strategy $\stratb'$ realizes a turn on \universeD, if \other is hasty (\Small{$B_{0,2},B_{1,1}\mapsto t$}), and \ego drives straight on, if \other is slow (\Small{$B_{0,1},B_{1,2}\mapsto f$}). 
	So $\stratb'$ is a dominant strategy, but $\stratb'$ makes no sense from \ego's perspective. 
	In case \other is hasty, \ego believes that \other is slow, since it trusts its sensors and \ego extrapolates that doing the turn  would cause a collision. 
	But in this case, $\stratb'$ demands to take the turn.
	Vice versa,  $\stratb'$ chooses to drive straight on, when \ego believes \other is hasty and it extrapolates that taking the turn is alright. 
\end{mexample}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Possible-worlds Strategy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\autoref{ex:noCon} motivates, what it means that \ego decides based on the content of its belief. 
We will formalize this as \enquote{\Ego always chooses an action, that a dominant strategy in \ego's current belief \belief would also choose at the believed current state}.
%
To capture this formally, we introduce the notion of \emph{possible-worlds strategy}. 

A \emph{possible-worlds strategy}  is a function $\stratw:{(2^{\Props_\belief})^{+}}\rightarrow\Act_\ego$ and it is applied to the alternative realities of \ego's current belief \belief. 
This results in believed traces.
We define this set of traces in an alternative reality $\reality=(\world,\cStates)\in\belief$ for a (believed) sequence of environment actions $\e\in\Act_{\env}^{\omega}(\world)$ as 
$\Cmp(\e,\stratw,\reality) = \{\cmp_0 \cmp_1\ldots \in (2^{\Props})^\omega | \exists \textit{ path } \Path\textit{ in }\world\textit{ from }\Init:  \forall i\geq 0:
\cmp_i = \Labelw(\Path(i)) \land 
\act_i:=\stratw(\Labelw(\Path_{\leq i})) \land
(\act_i,e(i))\in\Labelw({\Path}_{i},\Path_{i+1}) \}$.
We generalize the notion of \goalList-dominance to possible-worlds strategies. 
A possible-worlds strategy \stratw \goalList-dominates a possible-worlds strategy $\stratw'$ in \belief, if \stratw \goalList-dominates $\stratw'$ in all realities $\reality\in\belief$.
%
\begin{mexample}{Possible-Worlds Strategy}\label{ex:pws}
	Consider the possible-worlds strategy \stratw  that chooses to turn, if \other is hasty, and to drive straight on, if \other is slow, i.e., we consider \stratw with  \Small{$114hr\mapsto f$, $114hr, 212hr\mapsto t$,  $114hr, 212hr,221hr\mapsto f$, \ldots, $114sb\mapsto f$, $114sb,213sbb_p\mapsto f$, \ldots}. \stratw is sketched for the case that \other is hasty via bold arcs in \autoref{fig:bels}. 
	Note that \autoref{fig:bels} shows the excerpts of $B_{0,1}$ and $B_{1,2}$ as in \eg \autoref{fig:bellab2}.  
	$B_{0,1}$  expresses that \ego thinks that it is at the initial state \Small{$114hr$}. 
	\Ego follows \stratw by choosing \Small{$\stratb(114hr)=f$} when having this belief. 
	% Figure environment removed
	The belief $B_{1,2}$ (cf. \autoref{fig:bels} b) captures that \ego thinks to have already made one move and is now at state \Small{$212hr$}. According to \stratw, \ego has to choose $t$, since \Small{$\stratw(114hr,212hr)=t$}. \Ego hence has to choose $t$ when currently having the belief $B_{1,2}$.
\end{mexample}


\begin{table}
	\FramedBox{6.8cm}{\textwidth}{
		\small
		Strategy tyes:\\[\nlred]
		\begin{itemize}
			\item\small \emph{truth-observing strategy} $\strats:(2^{\props})^+\rightarrow\Act_{\ego}$\\ 
				observes the  ground truth world $\universeD$ via $\props\subseteq\Props_{\universeD}$ and takes decisions based on their history; serves as comparative reference of what is achievable given $\props$ could be observed directly\\[\nlred]
			\item\small \emph{doxastic strategy} $\stratb:\Beliefs^{+}\rightarrow\Act_{\ego}$\\
				observes the beliefs to take decisions and takes decisions based on their history; represents the decision making of autonomous and automatic systems\\[\nlred]
			\item\small \emph{possible-worlds strategy} $\stratw:{(2^{\Props_\belief})^{+}}\rightarrow\Act_\ego$\\
				captures how a system \HAS \enquote{simulates} its strategies within the alternative realities; decisions are taken based on the believed history within the respective alternative reality\\[\nlred]
		\end{itemize}
		A strategy \strat  \emph{\goalList-dominates} $\strat'$, $\strat'\leq\strat$, iff $\strat'$ achieves the goal list \goalList up to priority $m'$ but \strat achieves \goalList up to priority $m'$ with  $m'\leq m$.\\
		\normalsize
		}
		\caption{Strategy types \& dominance in a nutshell}\label{tab:nutshell}
\end{table}

%

A dominant possible-worlds strategy determines what is the best to do, given a  belief. 
So in order to express that \HAS chooses the action, that it thinks is currently the best, we refer to what a dominant possible-worlds strategy would choose for a given belief.

A peculiarity of possible-worlds strategies is, that they can be \emph{indecisive} for a belief \belief. 
That is, a possible-worlds strategy might determine two or more different actions for the set of believed current states.
More precisely,  \stratw is called current-state indecisive, if there are two paths, $\Path_1, \Path_2$, in \belief leading to believed current states and if \stratw chooses the action $\act_1$ at $\Path_1$ while it chooses $\act_2$ at $\Path_2$:  
%
\begin{mdefinition}{current-state (in)decisive}
	We call a possible-worlds strategy \stratw \emph{current-state indecisive} in belief \belief iff $\exists\reality_1,\reality_2\in\belief\land \bigwedge_{i\in\{1,2\}}\exists_i\Path_i\in\Paths(\reality_i):\bigwedge_{i\in\{1,2\}}\last(\Path_i)\in\cStates(\reality_{i})\land\stratw(\Label_{\reality_1}(\Path_1))\not=\stratw(\Label_{\reality_2}(\Path_2))$. 


	\stratw is \emph{current-state decisive} in \belief iff it is not current-state indecisive in \belief.
\end{mdefinition}
%

The indecisiveness may result from uncertainties of \ego. 
\Ego might be missing information that would allow it to determine the current situation sufficiently.
Since this information is missing, \ego instead forms a belief with a multitude of realities. 
That way a belief can encode even contradictory information. 
%
\begin{mexample}{Lack of information and indecisiveness}\label{ex:unknownGoal}
Let us assume that \ego has to get to a filling station on the shortest possible route. 
It is currently not sure where the filling station is.
It hence forms a belief $\belief$ of two realities, $\reality_1$ and $\reality_2$.
In reality $\reality_1$ the filling station is to its left, while in $\reality_2$ the filling station is to its right. 
In $\reality_1$ \ego must to move left while it must move right in $\reality_2$. 
Since \ego deems both realities possible, it cannot decide whether to turn right or left. 
\end{mexample}
%
\begin{mexample}{Indecisive possible-worlds strategy}\label{ex:undec}
Another example where a possible-world strategy is not able to determine a unique best choice, is given by $B_2$, the belief depicted in \autoref{fig:undec}. 
	% Figure environment removed	
	$B_2$ may be formed because \ego's sensor does not give any information about \other's colour, so that \ego believes that both colors are possible. 
	$B_2$ moreover captures that \ego believes to have made one step, i.e., it believes to be in state $s_2$ of reality $r_1$ or in state $u_2$ of reality $r_2$. 
	The strategy \stratw determines $f$ as the best option due to $s_2$ and $t$ as the best option due to $u_2$ of $r_2$. 
	Hence it is not obvious, whether to choose at the current state $t$ or $f$, given $B_2$.
\end{mexample}
We call the set of actions that a possible-worlds strategy chooses at the set of current states, its current-state choices, $\cAct(\stratw,\belief)$:
%
\begin{mdefinition}{current-state choices of \stratw, \Small{$\cAct(\stratw,\belief)$} }\label{def:cdec}
	Let \Small{$\cAct(\stratw,\belief)$} be the set with

	
	$\act \in \cAct(\stratw,\belief)\Leftrightarrow \exists \reality=(\world,\cStates)\in\belief: \exists \textit{ path }\pi\textit{ in }\world: \pi(0)\in\Init\land\textit{last}(\pi)\in\cStates\land\stratw(\pi)=\act$.
	
	We call \Small{$\cAct(\stratw,\belief)$} the current-state choices of \stratw. 
\end{mdefinition}
%
Given a strategy is decisive at the set of current states, we call it current-state decisive:
\begin{mdefinition}{current-state decisive possible-worlds strategy}\label{def:cdec}
	A possible-worlds strategy \stratw is \emph{current-state decisive in a belief \belief}, if \Small{$\cAct(\stratw,\belief)$} is a singleton.
\end{mdefinition}
Note, that the examples \autoref{ex:unknownGoal} and \autoref{ex:undec} illustrate that the existence of a current-state decisive possible-worlds strategy is not guaranteed.
\begin{mproposition}{Existence of a Current-state Decisive Strategy}\label{prop:ex-csds}
	We can decide whether there is a current-state decisive possible-worlds strategy \stratw achieving an LTL property $\psi$ in a given belief \belief.
	If it exists, we can synthesize such a strategy.
\end{mproposition}
\begin{proofsketch}{Prop.~\ref{prop:ex-csds}}
	We sketch how a current-state decisive possible-worlds strategy for a belief \belief can be synthesized.
	Given a belief \Small{$\belief=\{\reality_1,\ldots,\reality_n\}\in\Beliefs$}, we first build a single reality $\reality_{\belief}$ by the disjoint union of all alternative realities \Small{$\reality_{\belief}:=(($
	$\dbigcup_{r_i}\{\States_i\}$, 
	$\dbigcup_{r_i}\Edges_i$, 
	$\dbigcup_{r_i}\Label_i$,  
	$\dbigcup_{r_i}\Init_i)$, 
	$\dbigcup_{r_i}\{{\cStates}_i\})$}.
	If necessary, we can make the realities disjoint by renaming their states but keeping their structure.
	

 	We iterate through the list of \ego's actions.
	In iteration $i$, we modify $\reality_{\belief}$ to create the $\reality_i$ where the current action $\act_i$ becomes the only possible choice at all current states $\state_c$. 
	More precisely, in $\reality_i$ all transition that orginate at a current state $\state_c$ and that are labelled with an action $\act\not=\act_i$ are (re)directed to lead to $\sundef$.
	Using \cite{LTLSynth}, we synthesize a winning strategy \strat for $\psi \land \Box \neg \sundef$\footnote{The complexity is 2EXPTIME-complete}. If it exists, we stop iterating. 
	The synthesized winning strategy \strat applyies the same action at all current states, by construction. 
	It obiviously is also a winning strategy of \belief and current-state decisive.
	Since we check for all actions, whether such a strategy \strat exists, the algorithm is guaranteed to find a current-state decisive possible-worlds strategy, if it exists. Since there are only finitely many actions, the algoirithm terminates.
\end{proofsketch}
We consider actions that are the current-state choices of a dominant possible-worlds strategy as  rationally justified choices of an autonomous system. We therefore define the set of current-state choices of a belief:
%
\begin{mdefinition}{best choices in \belief, \Small{$\bAct(\belief)$} }\label{def:choiceb}
	Let a goal list $\goalList$ be given. Let \Small{$\bAct(\belief)$} be the set with
	
	$\act\in\Small{\bAct(\belief)}\Leftrightarrow \exists\tit{$\goalList$-dominant }\stratw\tit{ in }\belief:\act\in\Small{\cAct(\stratw,\belief)}$. 
\\
	We call \Small{$\bAct(\belief)$} the current-state choices in \belief for $\psi$. 
\end{mdefinition}
%  
The following example illustrates that in a belief \belief there can be several dominant current-state decisive possible-worlds strategies.
%
\begin{mexample}{current-state decisive and multiple action choices}\label{ex:undecMulti}
	Let us assume as in \autoref{ex:unknownGoal} that \ego has to get to a filling station on the shortest possible route. 
Let us assume \ego forms a belief $\belief$ of only one reality, $\reality$.
In reality $\reality$ there is a filling station to its left and to its right. 
	Hence \ego can turn right --let this be strategy $\stratw_1$-- and it can turn left --strategy $\stratw_2$. 
So \ego can choose to turn right or left according to  $\stratw_1$ and $\stratw_2$, respectively, and both strategies are current-state decisive.
\end{mexample}
%
\begin{mproposition}{Determining the best choices \Small{$\bAct(\belief)$}}\label{prop:actBel}
	Let a goal list $\goalList$ and a belief $\belief$ be given. 
	
	We can determine the set of best choices \Small{$\bAct(\belief)$} for $\goalList$ in \belief.
\end{mproposition}
\begin{proofsketch}{Prop.~\ref{prop:actBel}}	
	According to Def.~\ref{def:choiceb} \Small{$\bAct(\belief)$}  are the actions that are current-state choices of some $\psi$-dominant startegy $\stratw$ in \belief.
	We first determine the maximal $\mn$ of the goal list $\goalList$ that is achievable by any  possible-worlds strategy in \belief. With other words, a possible worlds strategy is dominant, if it achieves \goalList up to \mn.  
	To this end we check whether we can synthesize, \cite{LTLSynth}, a strategy in $\reality_{\belief}$\footnote{$\reality_{\belief}$ is the disjoint union of all alternative realities as defined in the proof of Prop.~\ref{prop:ex-csds} on page \pageref{prop:ex-csds}} that achieves $\psi$ for priority \mn, starting with $\mn=|\goalList|$ down to $\mn=0$.
%
	We then proceed as in the proof of Prop.~\ref{prop:ex-csds} on page \pageref{prop:ex-csds}, \ie by examining whether there is a possible-worlds strategy that achieves \goalList up to $\mn$ in the modified reality $\reality_i$ for action $i$, but we do not stop as soon as one could be synthesized but instead we examine all actions \Act.
\end{proofsketch}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Autonomous Decision}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
In this section we develop our notion of an autonomous decision and we define when systems are autonomous-decisive.\\ %

\emph{For the following let a doxastic model $\epi=(\universeD,\goalList, \LabelK,\Obs, \Beliefs, \LabelB)$ of a system \HAS be given.
	Let \Path be a finite initial path in \universeD, \history the observed history along $\Path$ and $\belief=\LabelB(\history)$ the formed belief.}\\



We call a system that decides based on its beliefs a \emph{doxastic system}.
Its decisions are determined by a doxastic strategy $\stratb$. 
\begin{mdefinition}{doxastic system}\label{def:doxSys}
	A \emph{doxastic system} $\sys$ is a pair $\sys=(\epi,\stratb)$ of a doxastic model $\epi$ and a doxastic strategy $\stratb$, $\stratb:\Beliefs^{+}\rightarrow\Act_{\ego}$ on $\universeD$.  
	For all finite paths \Path in \universeD, the system chooses $\stratb(\LabelB(\Path))$.
\end{mdefinition}
Doxastic systems do not base their decisions on the ground truth or on observations, but on their beliefs.
It is not constrained how they come to a decision though. 
Hence doxastic systems can be \eg automatic or autonomous systems (\cf \autoref{ex:autonAutom}). 

We regard autonomous systems as special doxastic systems, whose decisions are rational \wrt the content of the current belief.
So, $\stratb$ should choose actions that are the current-state choices of a dominant possible-worlds strategy \stratw.
Moreover, we require that \stratw should be current-state decisive.
When no current-state decisive strategy exists, this means that there is  no way to rationally avoid an unwanted consequence.
Then the current-state choice set \cAct(\stratw,\belief) means a gamble: $\act_1\in\cAct(\stratw,\belief)$ might achieve the targeted goal or another action $\act_2\in\cAct(\stratw,\belief), \act_1\not=\act_2,$ would be the right choice.


We hence regard it a design goal to develop systems that form beliefs where a current-state decisive possible-worlds strategy exists -- with other words, we strive to build a system \HAS that always builds a belief where it can determine a choice achieving its goals.
If a belief \belief is formed where no current-state decisive possible-worlds strategy exists, an engineer can adjusting the system's goals (\eg by weakening the goals to \enquote{if you are uncertain, choose the safe option}) or by improving the formed beliefs -- adding additional knowledge or adding sensors/observables.\\
\assumption{csdec}{In the following we assume that the belief formation \LabelB forms only beliefs \belief in which a current-state decisive strategy exists.}\\





A system that is not autonomous-decisive cannot rationally determine which action is currently appropriate. A goal for the  design of \HAS is hence to ensure that a system is autonomous-decisive. 


To summarize, we call a decision autonomous, if it is the rational choice for the current belief, that is 
 $\stratb$ chooses actions that are the current-state choices of a dominant, current-state decisive possible-worlds strategy:
%
\begin{mdefinition}{autonomous decision}\label{def:autonomDec}
	The system \HAS decides autonomously at \Path,  if it chooses an action $\act\in\bAct(\belief)$.
\end{mdefinition}
%
A system that always decides autonomously, follows a special doxastic strategy $\stratbb$ that always chooses an action $\act\in\bAct(\belief)$ when on a path \Path in \universeD, where $\belief=\LabelB(\Label_{\States}({\Path})|_{ \Obs},\LabelK(\last(\Path)))$ is the belief formed after the observed history along \Path and while having the believed current knowledge $\LabelK(\last(\Path))$.
Note that such a system follows a memoryless doxastic strategy.
The system's memory is \enquote{shifted} into the beliefs. 
The framework thus can capture how a system \HAS deals with the  finite memory also \wrt encoding the relevant.
%
\begin{mdefinition}{autonomous strategy}\label{def:autonomDec}
	A doxastic strategy $\stratbb : \Beliefs^{+} \rightarrow \Act$ is called an \emph{autonomous strategy}  iff 
		for all belief histories $\bar\belief\in\Beliefs^+$ it holds that {$\stratbb(\bar\belief)\in\bAct(\last(\bar\belief))$}.
\end{mdefinition}
%


We say that a system \sys autonomous-decisively achieves the goal list $\goalList$ up to $n$, if it implements an autonomous strategy $\stratbb$, \ie $\sys=(\epi,\stratbb)$, and \stratbb achieves $\psi$ up to $n$.

So far we do not require that an autonomous-decisive system \sys behaves appropriately in a given setting.
It is only guaranteed, that \sys acts rationally \wrt its beliefs. 
Its belief formation does not have to reflect the real world though.
Def. \ref{def:optimal} closes the gap.


By Def.~\ref{def:optimal} we basically enforce the belief formation \LabelB to form beliefs, so that \sys is as successful as the best system with direct access to the ground-truth of the design-time world \universeD.
\begin{mdefinition}{Optimal autonomous-decisive system}\label{def:optimal}
	The autonomous system $\sys=(\universeD,\goalList, \LabelK,\Obs, \Beliefs, \LabelB,\stratbb)$ is an \emph{optimal} autonomous-decisive system, 
	if the autonomous strategy $\stratbb$ is not $\goalList$-dominated by any \Props-truth-observing strategy.
\end{mdefinition}
%
In the following we are focusing on \emph{optimal} autonomous-decisive systems. 
For brevity we usually just speak of \emph{autonomous systems}.
We will discuss the relation of our notion of optimal autonomous-decisive systems with the notion of autonomous systems in \autoref{sec:autoSys}.


Def.~\ref{def:optimal} requires that the belief formation captures the gist of observations \wrt \sys's goals. 
It is a rather flexible way of constraining the belief formation: \LabelB has to preserve the \emph{relevant aspects} of \universeD. 
A more direct way to anchor beliefs in the ground-truth is given by the knowledge base. 

We say that $\sys$ is an \emph{optimal} autonomous-decisive system, 
	if the autonomous strategy $\stratbb$ is not $\goalList$-dominated by any \Props-truth-observing strategy\footnote{an \Props-truth-observing strategy is based on perfect observations of \universeD, cf. Tab.~\ref{tab:nutshell}}.
$\sys$'s belief formation \LabelB then builds beliefs, such that \sys is as successful as the best system with direct access to the complete ground truth, \universeD.
%

We can decide for a given doxastic model without belief formation, $\epi^{-}=(\universeD,\goalList,\LabelK,\Obs,\Beliefs,.)$, whether there is a knowledge-consistent belief formation \LabelB and
an autonomous strategy \stratbb, and we can synthesize the two (cf.~\autoref{th:auto}).  
%
\begin{mtheorem}[Autonomous Decisiveness~\cite{doxFrame}]\label{th:auto}
	Let  $\epi^{-}=(\universeD,\goalList,\LabelK,\Obs,\Beliefs,.)$ be a doxastic model without belief formation.

	We can decide whether there is a knowledge-consistent belief formation \LabelB and a  doxastic strategy \stratb such that $\sys=(\epi^{-},\LabelB,\stratb)$ is an optimal autonomous-decisive system. 
	If such \LabelB and \stratb exist, we can synthesize them. 
\end{mtheorem}
\begin{proofsketch}{\autoref{th:auto}}
	The proof can be sketched as follows. 
	We build a Kripke structure $\universeD'$ such that any \Obs-truth-observing strategy \strats in $\universeD'$ encodes (i) a belief formation $\LabelB$ and (ii) an autonomous strategy \stratbb, such that (a) \LabelB is knowledge-consistent and (b) if \strats achieves \goalList up to $n$, also $\stratbb$ does. 
	%
	The idea for the construction of $\universeD'$ is as follows. In $\universeD'$ the strategy \strats does not choose actions but beliefs.	
	%When a belief \belief is chosen in state \state, then all actions of the respective current-state choice $\Act(\belief)$ represent a rational and possible choice of \stratbb. 
	Therefore,  the transitions in \universeD are copied to $\universeD'$ and get relabelled with the belief \belief that justifies the action of \ego as a rational choice.
	We sketch the major steps of the construction as (i)-(iv) below:
	(i) We determine the current-state choices $\Act(\belief)$ for all beliefs in $\Beliefs$ by Prop.~\ref{prop:actBel}.
%
	(ii) We build the modified Kripke structure $\universeD'$: 
	Therefore we copy the state set \States of \universeD to become the state set $\States'$ of $\universeD'$.
	We then iterate over all states $\state\in\States$ of $\universeD$.
	If there is a transition from $\state$ via action $\act=(\act_1,\act_2)$ to $\state_2$ but no knowledge-consistent belief justifies \ego's action $\act_1$, \ie $\emptyset=\Beliefs_{\act,\state}:=\{\belief\in\Beliefs\mid \act_1\in{\Act(\belief)}$ and $\belief\models\LabelK(\state)\}$, we add a transition from \state to state \sundef and label this transition with $\act=(\perp,\act_2)$ to express that \ego will not choose this action, since it is no rational choice. %
	If $\Beliefs_{\act,\state}\not=\emptyset$, we iterate over all beliefs $\belief\in\Beliefs_{\act,\state}$ and introduce a transition from $\state'$ via $(\belief,\act_2)$ to $\state'_2$ in $\universeD'$, \ie we replace $\act_1$ by $\belief$.
	
	(iii) In order to judge how well the doxastic strategy \stratb has to perform for an autonomous-decisive system, we determine the maximal $\mn$ up to which $\goalList$ can be achieved by any \Props-truth-observing strategy in \universeD by iteratively applying strategy synthesis for LTL properties \cite{LTLSynth} starting from the maximum priority goals. 
%
	(iv) We then synthesize an \Obs-truth-observing strategy $\strats$ on {$\universeD'$}~\cite{LTLSynth} for the goal list $\goalList$ and priority $\mn$.
	In case $\strats$ achieves $\mn$, we define \LabelB by $\LabelB(\history):=\strats(\history)|_{\Beliefs}$, \ie the ${\LabelB}(\history)$ chooses the belief that labels the chosen transition.
	\stratbb may choose any action that is justified by $\LabelB(\history)$.
	Then $\sys=(\epi,\LabelB,\stratbb)$ is an optimal autonomous-decisive system. 
	If $\strats$ cannot achieve $\mn$, the truth-observing strategies on \universeD perform better, so no knowledge-consistent belief-formation for an autonomous optimal strategy exist for $\epi^{-}$.
\end{proofsketch}


%

To summarize, according to Theorem \autoref{th:auto} when designing an autonomous system \HAS, we can specify
\begin{itemize}
	\item the application domain via $\universeD$, 
	\item the list of goals \Goals, 
	\item the believed knowledge that the system \HAS will have, 
	\item what observations \HAS  can make and 
	\item how its internal representation the world is, \ie the possible worlds,
\end{itemize}
	and then we can determine whether it is at all possible to form beliefs such that the system \HAS is able to autonomously-decide and succeed as if it knew the ground truth. 
	Moreover, we can synthesize an appropriate belief labelling, so that the corresponding autonomous strategy is optimal for its goals. 

	Since we consider a quite liberal notion of autonomous system, it means that if the above check fails, it if often not possible to build an autonomous system with the given input and resources.  

%
Given we provide our system under construction full observability and let its beliefs reflect \universeD precisely and do not provide false believe knowledge, then an autonomous system \sys is guaranteed to exist.





The following example illustrates that the beliefs of an autonomous-decisive system \sys can be rather loosely linked to reality, observations are not (directly) represented in the beliefs and the possible worlds differ substantially from the ground-truth. 
Nevertheless, \sys can be successful. 

\begin{mexample}{Freedom of beliefs}
	In \autoref{fig:good} we sketch a belief formation where \Ego believes all the time, that \other has the wrong colour.
	The possible worlds do not reflect the ground truth world, \universeD, well, \eg in the possible worlds of $B_{0,1}$ and $B_{0,2}$ \other is red and the dominant strategy is not to turn, while in \universeD the dominant strategy is to do the turn, when \other is red. 
	Furthermore, the belief formation does rather abruptly update its beliefs (especially from $B_{0,2}$ to $B_{1,2}$).
	% Figure environment removed
	Although the formed beliefs may seem degenerate and inappropriately capturing the reality, the belief formation allows \ego to behave as good as when it would know the ground truth. 
	This freedom of belief formation allows efficient and compact encodings of the perceptions comprised to the relevant aspects. 
	A system \sys that is optimal according to Def.~\ref{def:optimal} will have a belief formation that captures the application domain \universeD as closely as necessary to satisfy the system's goals.   

	If \universeD has to be captured even closer than necessary for \sys's goals, this can be enforced by means of the knowledge base.
\end{mexample}


\begin{mexample}{Autonomous, Non-Autonomous,  Automatic}\label{ex:autonAutom2}
	Let us consider an example of an autonomous \ego in the  setting of \autoref{fig:bellab}, where the sensor only initially switches colours, and consider the possible-worlds strategy \stratw of \autoref{ex:pws} (turn, if \other is hasty, and drive on, if \other is slow). 
	%
	When \ego initially evaluates its situation in $s_1$ of \universeD, it believes that the situation is as described by {$B_{0,1}$}, i.e. \other is a hasty, red car.
	\Ego can decide to follow \stratw in {$B_{0,1}$}, as it seems a good choice -- \stratw is dominant and current-state decisive in {$B_{0,1}$}. 
	According to its extrapolation, it would move one step forward, and then it would successfully take the turn.
	%
	After actually moving forward, \ego evaluates the situation in $s_2$.
	In $s_2$ \ego believes in {$B_{1,1}$} reflecting that \ego now truthfully perceives \other's colour as blue (cf. \autoref{ex:knowledgeBelief}).  
	Again, \stratw is a dominant current-state decisive possible-worlds strategy and determines \textsf{f} as the next move.
	Along this line, it is easy to see that \ego can implement a doxastic strategy \stratb that chooses the action that \stratw determines for the respective $\LabelB_{\tsf{auton}}(h)$.


	For an example for where no autonomous \ego exists but an automatic \ego can be built, we modify our running example slightly.
	Let us assume that \ego is unsure of its own initial position, thinking that it may initially be at \Small{$x=1$} or \Small{$x=2$}, as sketched in \autoref{fig:bellab3}. 
	Let a belief formation $\LabelB_{\tsf{autom}}$ be given that evolves the initial beliefs {$B_{0,3}$} and {$B_{0,4}$} analogously to \autoref{ex:knowledgeBelief}, that is, \ego perceives the correct colour after moving one step forward.
	The possible-worlds strategy \stratw is still a dominant strategy but not current-state decisive, since for example in {$B_{0,3}$} \ego would do the turn at $s1$ due to reality $r_2$, and it would also drive straight on due to $r_1$. Hence, there is no dominant possible-worlds strategy in {$B_{0,3}$} that is able to determine one action. \Ego cannot decide autonomously.
	Nevertheless, we can specify a dominant doxastic strategy \stratb for this case, but its actions are not chosen based on the belief content: \Ego chooses to turn after one step when its initial belief was {$B_{0,4}$} (\Small{$B_{0,4},B_{1,4}\mapsto t$}), otherwise it drives straight on.
	This strategy is dominant and could be used to build an \emph{automatic} system, where \ego just plays out \stratb. Such a strategy might be useful when an engineer knows that \ego will start from \Small{$x=1$} but did not equip \ego with this information.
\end{mexample}

A system that is not autonomous-decisive cannot rationally determine by itself which action is currently appropriate. A goal for the  design of a system \HAS is hence to ensure that a system is autonomous-decisive. 
%\subsection{The Notion of Autonomous System}
\input{content/autonomnotion}
