\section{Ingredients of our Doxastic Framework \cite{doxFrame}}\label{sec:ingredients}
\normalsize
In this section, we introduce the ingredients of our framework alongside a running example.
The section is taken from \cite{doxFrame} and slightly enriched  (e.g. by \autoref{ex:Bel}  and \autoref{ex:lin}).


We consider two cars, \ego and \other, that are on separate lanes heading towards each other.
The left car, \ego, is our autonomous system \HAS. 
Its goals are avoiding collisions and to take the left turn.
From \ego's perspective \other is uncontrolled.
\autoref{fig:setup} sketches the initial setup and the possible actions of the two cars.  

% Figure environment removed
To formally describe what the two cars can do and how the initial situation may evolve in time and space, we use a labelled Kripke structure as defined in Def.~\ref{def:world} and call it a world. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{A World}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In our context, a world models the sphere of interest
and, for this purpose, represents its entities and their actions.\footnote{Thus we use the term \emph{world} here to denote digital or academic worlds, according to \cite{merriamWorld}}
\Ego and other agents are hence part of the world. 
They simultaneously choose actions and thereby how the world transitions from one state to the next. 
We assume that the actions are partitioned into \ego's actions, i.e. the ones that \ego can control, and the ones outside of \ego's control.
\begin{mdefinition}{world}\label{def:world}
	Formally, a \emph{world} \universe is a labelled Kripke structure 
	$\universe=(\States,\Edges,\Label,\Init)$, 
	where 
	\begin{itemize}
		\item \States is the set of states, 	
		\item $\Init\subseteq\States$ is the set of initial states, and  
		\item $\Edges\subseteq\States\times\States$ is the transition relation defining edges between states, 
		\item $\Act=\Act_{\ego}\times\Act_{\env}$ is a finite set of tuples defining the simultaneous actions of our autonomous agent \ego and its environment \env, which may include other agents.	
		\item $\Props$ is  a finite set of atomic propositions 

			%
		\item $\Label=\Label_\States\cup\Label_{\Edges}$ where
			\begin{itemize}
				\item $\Label_\Edges=\Edges\rightarrow {2^{\Act}}$ labels each edge with  a subset of $\Act$ and 
				\item $\Label_\States:\States\rightarrow {2^{\Props}}$ labels state with a subset of \Props,
			\end{itemize}

	\end{itemize}
	We assume that the transition relation is defined for all states and actions, i.e., $\forall \state\in\States,\forall \act\in\Act,\exists \edge\in\Edges: \act\in\Label_\Edges(\edge)$.
\end{mdefinition}
%
The edge labels $\Label_\Edges(\state)$ of an edge $(\state,\state')$ denote the set of actions the lead from the state \state to state \state'. 
The state labels $\Label_\States(\state)$ denote the set of atomic propositions that are valid at \state. 
We assume that all propositions have a finite domain and hence can be encoded as a finite combination of Booleans. 
In order to express that an action is not enabled at a state \state, \universe can transition into a dedicated state $\sundef$ that is accordingly labelled.\footnote{In this paper any strategy has hence to avoid $\sundef$.}


A sequence of states $\Path=\state_0\state_1\ldots\state_n\in\States^{*}\cup\States^{\omega}$ is a \emph{path} in \universe iff $\forall i, 0\leq i<|\Path|: (\state_{i},\state_{i+1})\in\Edges$.
A path hence describes a possible evolution of world's state. 
$\Path(i)$ denotes  the $i$-th state, $\state_i$. 
$\Path_{< m}$ denotes the prefix of the first $m$ states, $\state_0\ldots\state_{m-1}$, and $\last(\Path)$ is the last state of a finite path $\Path$. $\Path$ is initial iff $\pi(0)\in\Init$. 
%

Given a tuple $t=(a,\ldots,z)$ we assume that indices carry over to the components, \ie $t_i=(a_i,\ldots,z_i)$. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{A world}\label{ex:uni}
	In our running example, the actions of \ego are \textsf{f}, \enquote{moving one step forward if possible}, \textsf{t}, \enquote{turn and move one step forward}. 
	\Other is either a slow car or a hasty car. If \other is slow it moves one tile forward. 
	If \other is hasty, it leaves its initial position by moving two tiles forward, from all other positions it moves one tile forward. 
	\Other's actions are \textsf{f} and \textsf{F}, \enquote{move two positions forward}. 
	The actions of \ego and \other are annotated by pale blue and dark blue arrows in \autoref{fig:setup}.


	The propositions $\Props_{\textit{pos}}=\Props_{\xego}\cup\Props_{\yego}\cup\Props_{\xother}$ with  $\Props_{\xego}=\{\xego=i|1\leq i\leq 4\}$, $\Props_{\yego}=\{\yego=i|1\leq i \leq 3\}$, $\Props_{\xother}= \{\xother=i|1\leq i \leq 4\}$ encode the positions of the two cars, where $\xego=i$ and $\yego=i$ represent the horizontal and vertical position of \ego, and $\xother=i$ represents the horizontal position of \other. Its vertical position is always two. 
	\Other's car type is encoded via the propositions \textsf{s} (slow) and \textsf{h} (hasty). 
	We assume that \ego cannot observe \other's car type directly, but it has sensors perceiving \other's colour, which is either red or blue. 
	The proposition \blue (\red, \textsf{h}, \textsf{s}) is true, iff \other is a blue (red, hasty, slow) car. 
	The propositions \bp and \rp encode what \ego perceives as \other's colour. 
	They are used to modeling \ego's imperfect colour recognition,  while the propositions \blue and \red encode the true colour of \other. 
	We assume that \ego's colour perception works correctly, when \ego and \other are less than two tiles apart, otherwise the sensor switches colours ($\bp=\neg\blue$, $\rp=\neg\red$). Let $\Props_{\textit{cartype}}$ be the set $\{\textsf{h, s, \blue, \red, \bp, \rp}\}$. 
	The propositions in our example are hence $\Props=\Props_{\textit{pos}}\cup\Props_{\textit{cartype}}\cup\{\textsf{undef}\}$, where \textsf{undef} labels the sink state.


	% Figure environment removed
	Figure \ref{fig:wm} shows the Kripke structure of this world.
	States are labelled with the propositions that hold in the respective state. 
	The label $\mathsf{abcdef}\in\mathbb{N}^3\times\{f,s\}\times\{b,r\}\times\{b_p,r_p\}$  encodes that $\xego=a,\yego=b,\xother=c$ are true and \other's car type is \textsf{d}, its  colour is \textsf{e} whereas the perceived colour is \textsf{f}. 
	The valuations of all other propositions are false. 
	Likewise, the label \textsf{undef} encodes that the only valid proposition is \textsf{undef}.   
	Edges are labelled with sets of actions. We omit the sets' brackets for brevity.
	For example, the label \textsf{ff} denotes the set $\{\textsf{ff}\}$, that contains the one action \textsf{ff}, where \ego and \other simultaneously move one step forward, if possible.   
	Actions that are not enabled at a state lead to the sink state \sundef. 
	We omit the sink state and sink transitions in the sections that follow.
\end{mexample}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In our running example we consider the above world as our \emph{design-time world} \universeD, i.e. we use it as the reference for what is true, as \emph{ground truth}. 
%
As  discussed in \autoref{sec:designproc}, the design time world \universeD is the result of an analysis activity of the system design. 
\universeD represents the intended application domain including test criteria that the system must master. 
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Goal List}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our system \ego has to achieve a prioritized list of goals.
%
%
A \emph{goal} \goal is a linear-time temporal logic (LTL) formula~\cite{PrinciplesOfMC}. 
We denote the temporal operator \enquote{globally} by $\Box$, \enquote{eventually} by $\Diamond$, \enquote{next} by \Next and \enquote{until} by \Until. 
We interpret the LTL formulae over (infinite) \emph{traces}, which are infinite
sequences $\cmp = \cmp_0 \cmp_1\ldots \in (2^\Props)^\omega$ of valuations of \Props. 
Satisfaction of an LTL formula $\varphi$ by
a trace $\cmp$ is denoted as $\cmp \models \varphi$.

\begin{mdefinition}{goal list}\label{def:goals}
	A \emph{goal list} $\goalList = (\Goals, \prio)$ consists of a set $\Goals$ of LTL formulae and a priority function $\prio : \Goals \rightarrow \{1, \ldots, |\Goals|\}$ where
	$\goal\in\Goals$ is more important than $\goal'\in\Goals$ iff $\prio(\goal)<\prio(\goal')$.
\end{mdefinition}
We say that a trace $\cmp$ satisfies \goalList with priority $n$ if \cmp satisfies all goals of priority $n$ and more importance, \ie $\cmp \models \goal$ for all $\goal \in \Goals$ with $\prio(\goal) \leq n$.  
%$\cmp$ satisfies \goalList with priority up to $n$, if $n$ is the greatest such priority.  %
A set of traces \Cmp satisfies \goalList with priority  $n$, if all $\cmp\in\Cmp$ satisfy \goalList with priority $n$.
A set of traces \Cmp satisfies \goalList up to priority $n$, if \Cmp satisfies \goalList with priority $n$ and $n$ is the greatest such priority. 


For technical reasons the most important goal is $\varphi_g=\true$ and the second most important goal is $\varphi_{u}=\Box \neg \textsf{undef}$.
$\varphi_g$ ensures that at least one goal of the list can be realised. 
$\varphi_{u}$ results from our encoding of disabled transitions: Since we assume that the transition relation is total, we let disabled transitions lead to the state \sundef that is labeled with \udef. 
A strategy is not supposed to take disabled transitions, hence the state \sundef has to be avoided. 
Since we can simply shifting all goals by down-grading their priority and then insert $\varphi_g$ and $\varphi_u$ as the to top most goals, we neglect this issue in the following.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{Prioritized Goals}\label{ex:goals}
	We formalize collision freedom as $\varphi_{c}=\Box (\xego=2\land \yego=2\Rightarrow \xother\not=2)$, and $\varphi_{t}=\Diamond (\yego=3)$ expresses that \ego eventually does the turn. 
%	In order to rule out that disabled transitions are taken, we use the goal $\varphi_{u}=\Box \neg \textsf{undef}$.
	The priorities are given by $\prio(\varphi_c)=1, \prio(\varphi_t)=2$.



	Let us now take a closer look at what \ego should do in order to accomplish its goals.
	By inspection of the design time world \universeD, as given  e.g. in \autoref{fig:worldSimple}, we can see that
	if \other is slow, then \ego should not take the turn, but instead it should drive straight on, in order to avoid the collision. 
	If \other is hasty, then  \ego can take the turn and accomplish all its goals.
	% Figure environment removed
\end{mexample}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Observations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\Ego, being highly autonomous, will take decisions based on the beliefs that it has constructed about the world in which it operates. 
\Ego derives its beliefs from the observations made so far and the knowledge/strong beliefs it has about the world. 
%

A world is usually  only partially perceivable by \ego via observations. \emph{Observations} \Obs are propositions of \universeD whose valuations \ego can assess and that represent  e.g.  sensor readings or received messages from other agents. Observations shed light on \universeD, but they do not have to be truthful, as illustrated in \autoref{ex:uni}, where initially the values of \bp and \rp were switched, so that initially \ego does not perceive the correct colour. 
Despite incorrect observations, a system must, nevertheless, draw conclusions about the current state of the world on the basis of its previous observations. 
We refer to a partial trace leading to a state \state as a history of \state.
%
\begin{mdefinition}{P-observable history}\label{def:history}
	Let $P$ be a set of propositions, $P\subseteq  \Props$. 
	We call $h$ a ($P$-)\emph{history} of a state \state, if there is an initial path $\Path$ in \universe leading to \state, and
	$h=h_0h_1\ldots h_n$ is the sequence of state labels along $\Path$, $h_i=\Label(\Path(i))\cap P, \forall i, 0\leq i\leq n$. 
	We denote the set of $P$-histories  of \state as $\histories_{P}(\state)$ and the set of all $P$-histories as $\histories_{P}:=\bigcup_{\state\in\States}\histories_{P}(\state)$.  
	We say $h$ is \emph{observable} iff $P\subseteq \Obs$.
\end{mdefinition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{Observable History}\label{ex:hist}
	In our example \ego cannot observe \other's position due to a broken distance sensor, but it can observe its own position and the colour of \other, so $\Obs:=\Props_{x_e}\cup\Props_{y_e}\cup\{\textsf{undef, \bp, \rp}\}$.
	Given the world of \autoref{fig:setup} and \autoref{fig:wm}, \Small{$h\,=\,$114sbr$_p,$213sbb$_p,$312sbb$_p$,411sbb$_p$}%
	\footnote{We apologize for denoting the valuation of \Props rather informal in the following. We do this for the sake of brevity. $114sbr_p$ denotes the valuation where \xego is $1$, \yego is $1$, \xother is $4$, other is a \emph{s}low car, its colour is \emph{b}lue, the \emph{p}erceived colour is \emph{r}ed. Likewise, $11r_p$ denotes that \xego is $1$ and the \emph{p}erceived colour of other is \emph{r}ed.}%
	 is the history along the path \Small{s1$,$s2$,$s3$,$s5} wrt \Props, whereas \ego's observable history wrt \Obs observable history wrt \Obs is \Small{11r$_p$,21b$_p$,31b$_p$,41b$_p$}.
\end{mexample} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%If \world or $\Props'$ are clear from the context, we omit the respective index. 
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Beliefs} A belief describes what \ego currently thinks the world is like. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
For instance, \ego may think that it saw an approaching vehicle and  that this vehicle is a slow car. 
Due to \ego's belief that the other car is slow, \ego imagines possible future evolutions for a slow car approaching.

We formally capture beliefs as sets of (alternative) realities. 
A reality describes history, current state and possible futures of a world.
\begin{mdefinition}{belief, reality}\label{def:belief}
	A \emph{belief} \belief is the set of realities that \ego currently deems possible, $\belief=\{\reality_0,\ldots,\reality_n\}$. 
	A \emph{reality} is a pair $\reality=(\world,\cStates)$ of a \emph{(possible) world} $\world=(\States,\Edges,\Label,\Init)$ and a set of believed current states $\cStates\subseteq\States$, where any current state is reachable from an initial state and every path has at most one current state.
\end{mdefinition}
A reality specifies a set of current states, that represent the system's assessment of the current state of the world. 
Thereby a reality defines the possible pasts and futures: pasts are captured by the set of paths between initial states \Init and current states \cStates, the possible futures are paths from the current states. 

We also use the term \emph{alternative reality} to stress that a reality is only one possibility that \ego thinks is possible. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{Alternative Realities and Beliefs}\label{ex:altBel}
	To illustrate the notion of belief (cf. Def.~\ref{def:belief}), let us consider the two alternative realities of \ego as illustrated in \autoref{fig:exBel}(a)+(b). The believed past is marked by framing state labels.\\[2mm] 
	% Figure environment removed
	Since a belief is a set of alternative realities, singletons of either (a) or (b) form a belief. Also, the set of (a) and (b) forms a belief, where \ego thinks both alternatives are possible.
\end{mexample}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

\begin{excursus}{Believing in a Different World}\label{ex:Bel}
We want to recall that a system \HAS may believe in worlds that are substantially different from the assumed ground-truth as modelled in \universeD.
An autonomous system \HAS usually captures its application domain by simplified concepts and rules, that reflect its application domain coarsely but sufficiently. 
	Although our examples do not illustrate this point, the framework can be used to form alternative realities that are very different from the design time universe \universeD. 
	\HAS can for instance believe that its actions have a different effect than they have in reality. 
	It can believe it is at situations that are impossible in \universeD, \ie in its beliefs there are valuations of \Props that do not occur in \universeD.


	We assume in this paper though, that\\
	\assumption{act}{the believed actions are a subset of the actions of \universeD}, and
	\assumption{prop}{the propositions in possible worlds are a subset of the propositions \Props.}\\ 
	These two assumptions simplify the framework, but can easily be dropped. 
	But even keeping this restriction is not a severe limitation, since the beliefs do not have to reflect the design time world truthfully. 
\end{excursus}
Since a system \HAS has only finite resources, we assume it can only represent finitely many beliefs, that is, its \emph{set of possible beliefs} $\Beliefs$ is finite.

We write \Small{$\Worlds(\belief)$} for the set of worlds of a belief, \small{$\Worlds(\belief):=\bigcup_{(\cStates,\world)\in \belief}\{\world\}$}. We denote the set of \emph{possible worlds}, \ie the set of worlds taht occur in any possible belief, \Small{$\bigcup_{\belief\in \Beliefs}\Worlds(\belief)$}, as \Small{$\Worlds$}. 

The choice of \Worlds and \Beliefs constitutes an important design decision within the development process of \HAS, as it delimits the expressive power of beliefs. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{Possible Beliefs \Beliefs}\label{ex:bel}
	Our \ego has been designed to represent a certain set of scenarios, for which it can evaluate what to do by extrapolating the future. 
	Figures \ref{fig:belSetup}(a)-(d) sketch a set of possible worlds. 
	The other car may be hasty or slow, the road may be up to 6 tiles long, the intersection may be at \Small{$x=2$} or \Small{$x=3$}, and the start position of \other varies from \Small{$x=4$} to \Small{$x=6$}.
	Note, that \universeD of \autoref{fig:wm} is described by \autoref{fig:belSetup}~(a). 
	% Figure environment removed	
	Let \ego's possible beliefs \Beliefs be the beliefs that canonically evolve from these initial scenarios. 
\end{mexample}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Knowledge Base} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\emph{Believed knowledge} are statements that the system \HAS believes are true.
These can be built-in or they can be provided at some point in time during \ego's mission. 
We imagine that an engineer equips a system during its design with general statements about the application domain (e.g. \emph{\enquote{Cars drive on the street not under.}}, \emph{\enquote{Velocity is the change in position}}).
Moreover we imagine, that during \HAS's mission certain trusted sources (\emph{like a traffic control system}) provide statements that become strong beliefs. Examples of such statements are \emph{\enquote{In settled areas the speed limit is \SI{50}{km/h}}},  \emph{\enquote{I will be on the highway for the next 20 mins.}} or rules like \emph{\enquote{If A promises to give way, I can rely on it.}}. 

We specify the statements that a system \HAS believes in via an LTL variant, which we call \emph{Belief-LTL~(BLTL)}. A BLTL formula can be satisfied by a belief \belief.
%It is defined as a set of BLTL formulae called knowledge base $\kbase_{\state_{i}}$.


\begin{mdefinition}{BLTL}\label{def:bltl}
	
	Syntax:\\
	A BLTL formula is defined via the following grammar: $\K\psi\,|\,\Kc\psi\,|\,\neg\varphi\,|\,\varphi\land\varphi'$, where $\psi$ is an LTL formula and  $\varphi,\varphi'$ are BLTL formulae.\\
	
	Semantics:\\
	A belief \belief satisfies $\K\psi$, \ie $\belief\models\K\psi$, iff $\psi$ is satisfied by all worlds of all alternative realities of \belief, \ie $\psi$ holds on all traces arising at any initial state of any world $\world\in\Worlds(\belief)$.

	$\Kc$ is analogously defined but on traces from the current states. 
\end{mdefinition}

	$\K\psi$ reads as \emph{\enquote{\ego believes to know that initially $\psi$ held}}. 
$\Kc\varphi$ reads as \emph{\enquote{\ego believes to know that currently $\psi$ holds}}. 
Via BLTL we can specify statements about a belief \belief. 
We can describe the believed past using \K, \eg \enquote{I believe to know that at the start of the maneuver the other car was red}. 
Via $\K^c$ we  can describe what the believed current state is, \eg \enquote{I believe to know that now the car is blue} and we can  refer to the believed future \enquote{I believe to know that in future the car will stay blue.} 
Note, that the past, present and future described via a BLTL formula refers to the content of \belief and not to the ground truth. 
Moreover, note, that a BLTL formula does not allow us to specify constraints on the evolution of beliefs.

\begin{excursus}{Linear temporal properties of belief formation}\label{ex:lin}
Note that BLTL does not allow to specify temporal logic properties regarding the belief evolution. Therefore we would need a formalism that is interpreted on sequences of beliefs. We could for instance define LTBLTL:
\begin{mdefinition}{LTBLTL}\label{def:ltbltl}
	Any BLTL formula is an LTBLTL formula. Given $\varphi_1$ and $\varphi_2$ are BLTL formulae $\neg\varphi_1,\varphi_1\land\varphi_2,\Next\varphi_1,\varphi_1\Until\varphi_2$ is also an LTBLTL. 

	The satisfaction relation is defined in the usual way. We hence present here only the \Until case:
	
	An infinite sequence of beliefs, $\bar{\belief}$, satisfies $\varphi_1\Until\varphi_2$, $\bar{\belief}\models\psi$ if there is an $i$ such that $\forall j<i: \bar\belief(j)\bar\belief(j+1)\ldots\models \varphi_1$ and $\bar\belief(i)\bar\belief(i+1)\ldots\models\varphi_2$.
\end{mdefinition}
	LTBLTL formulae would allow to express properties on the belief formed along a run. For example, such a formula could capture (i)+(ii): (i) Initially \ego believes in $\varphi_1$=\enquote{\other is initially blue and stays blue at all times in all alternate realities}. (ii) \Ego continues to believe in $\varphi_1$ until \ego believes in $\varphi_2$=\enquote{\Other is initially red and stays red at all times}.

	We plan to study properties of the belief formation in future work.
\end{excursus} 
We assume that a finite set $\kbase$ of BLTL formulae is given that represents the believed knowledge that a system \HAS can have at any time.
From this, the engineer can select subsets that \HAS at a certain state.  
\kbase represents the knowledge an engineer can equip  \HAS with, \ie the prior knowledge that she establishes 
and the knowledge that can be transmitted to \HAS during its mission.
\begin{mdefinition}{knowledge base}
	A finite set $\kbase$ of BLTL formulae constitutes a \emph{knowledge base} $\kbase\in\kbases$.
A belief \belief satisfies ${\kbase}$, $\belief\models\kbase$, if $\belief\models\varphi$ holds for all $\varphi\in\kbase$.\\
	\Ego's knowledge base varies over time, we hence extend the labelling function $\Label$ by $\LabelK:\States\rightarrow \kbases$ to specify $\kbase$ as the available knowledge base $\kbase=\Label(\state)$ at a state \state. 
%\Ego's knowledge base is always observable. 
Given a history \history, $\kbase_\history$ denotes the knowledge base of $\last(\history)$. 
	\end{mdefinition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{A Knowledge Base}\label{ex:kb}
	Let \ego have the following knowledge base $\kbase=\{\varphi_z,\varphi_t,\varphi_i,\varphi_{ct}\}$ at all states, where\\[-5mm]
	{\small
\begin{enumerate}\itemindent\indred
		\item { {$\varphi_z=\K \Box ((\neg \xother=5 \land \neg \xother=6) \lor \textsf{undef})$\normalsize}} \\\hfill\textit{(\other is at most at $x=4$)}, \\[\nlred]
		\item {{ $\varphi_t=\K \Box (\neg\xego=2 \Rightarrow (\yego=1 \lor \textsf{undef}))$\normalsize}} \\\textit{(a turn is only possible at $x=2$)},\\[\nlred]
		\item\label{it:start} {{ $\varphi_i=\K\, \xego=1$\\\hfill\textit{(\ego starts at $x=1$)}\normalsize}} and \\[\nlred]
		\item {{ $\varphi_{ct}=\K \Box \bigwedge_{t\in\{\textsf{s, h}\}} (t \Rightarrow \Next (t\lor undef)) \land (\neg t\Rightarrow \Next ( \neg t\lor \textsf{undef}))$\normalsize}}\\\hfill\textit{(the initial car type does not change)}.
\end{enumerate}
Note that BLTL formulae are interpreted on realities and these have designated (maneuver) start states and current states. So \autoref{it:start} expresses that \ego starts its the maneuver at position 1, since by Def.~\ref{def:bltl} traces from initial paths are considered. 
In contrast, the formula $\varphi_c=\K^cx_e=3$ expresses that \ego believes that it currently is at position 3, as for $\K^c$ traces from the current states are considered. 
\normalsize}
\end{mexample}			
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Belief Formation} %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\Ego updates its beliefs \eg when it gets new information from its sensors, a clock tick or a message from another agent.
The belief formation function \LabelB captures formally how \ego builds its belief. 
	\begin{mdefinition}{belief formation, knowledge-consistent}\label{def:bform}
		The belief formation \LabelB, ${\LabelB}:\histories_{\Obs}\times\kbases\rightarrow \Beliefs$, specifies the belief ${\belief}=\LabelB(\history,\kbase)$ that \sys derives after perceiving a history $\history$ of observations $\Obs\subseteq\Props$ and while believing in $\kbase$. 

		A belief formation $\LabelB$ is called \emph{knowledge-consistent}, if all formed beliefs satisfy the respective knowledge base \kbase, i.e., for all paths $\pi$ of \universeD holds, $\LabelB(\history,\kbase)\models\kbase$ where $\history=\Label_\States(\pi)|_\Obs$ and $\kbase=\LabelK(last(\pi))$.
	\end{mdefinition}


Note, that the knowledge base is a mean to anchor \ego's beliefs to the ground truth.
We could for instance (1) label the states with a knowledge base that reflects the ground truth of a formula $\varphi$. 
Then any knowledge-consistent belief of \ego coincides with the ground truth regarding the evaluation of $\varphi$.
We could also enforce e.g. that (2) the system \HAS forms delayed beliefs, i.e. \HAS believes now in what it had observed two steps before. 

\begin{mdefinition}{history of beliefs, $\LabelBh$ }
	A \emph{belief history} is  a finite sequence of beliefs ${\Small{\bar\belief=\belief_0\belief_1\ldots \belief_n}}$.

	For a history of observations {\Small{$h=h_0h_1\ldots h_n$}} and a history of knowledge bases {\Small{$\khistory=\kbase_0\kbase_1\ldots \kbase_n$}}, we denote by $\LabelBh(h,\khistory)$ the \emph{resulting belief history},
	
	\Small{$\LabelBh(h,\khistory):=\LabelB(h_0,\kbase_0)\LabelB(h_0h_1,\kbase_1)\ldots\LabelB(h_0h_1\ldots h_n,\kbase_n)$}.
	
	\end{mdefinition}
	We write $\LabelB(\history)$ and $\LabelBh(h)$ instead of $\LabelB(\history,\kbase_\history)$ and $\LabelBh(h,\kbase)$, when the knowledgebase is clear from the context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mexample}{Knowledge-Consistent Belief Formation}\label{ex:knowledgeBelief}
	Let us now consider an example of a knowledge-consistent belief formation.
	We have already defined \ego's possible beliefs in \autoref{ex:bel}. 
	Let \ego have the knowledge base $\kbase':=\kbase\cup\{\varphi_b\}$ at all states, where \kbase is defined in \autoref{ex:kb} and 
	$\varphi_b=\Box (\textsf{h} \Leftrightarrow \rp) \land \Box (\textsf{s} \Leftrightarrow \bp)$ expresses that \ego is also convinced that a red car is hasty, while a blue car is slow.
	%
	In order to satisfy $\kbase$, only realities arising from the scenario (a) of \autoref{fig:belSetup} remain possible.

	
	 The initial belief of a knowledge-consistent belief formation has to consist of the alternative realities depicted in \autoref{fig:initBel}(a)+(c). 
	\Ego will belief to be in reality $\reality_b$ of Figure~\ref{fig:initBel}(a), when it is in the real world in the scenario of Figure \ref{fig:initBel}(b).
	In this scenario \other is a red car, but \ego incorrectly perceives that \other is blue, hence 	
	$\reality_b$ expresses \ego believe \enquote{I know, \other is blue and slow}. 
	Reality $\reality_b$ moreover describes that \ego thinks to be at the start of the manuever and it captures \ego's expectation of how the future will develop. 
	Similarly, \autoref{fig:initBel}(c) shows the reality $\reality_r$, which \ego things to be in, when it is in the real world in the scenario of \autoref{fig:initBel}(d) where it incorrectly perceives that the blue \other is red.     
	% Figure environment removed	

	So, since \ego's perception is mistaken, \ego is initially convinced that there is a hasty car, when there is a slow car and vice versa.  
	At the initial state, \ego hence thinks that it should do the turn, when it should not. 
	At the next time step, \ego moves one tile forward, while \other simultaneously moves either one or two tiles forward. 
	In both cases, \ego then perceives \other's colour correctly and updates its belief.

	Let us say, \ego considers the more recent observations as more reliable and hence corrects its belief on \other's car type and colour. 
	It updates the belief to \autoref{fig:secondBel}(a) when it is in the scenario %\ 
	\autoref{fig:initBel}(b), and to \autoref{fig:secondBel}(b), when in 
	\autoref{fig:initBel}(d).	
	Figure \ref{fig:bellab} sketches the belief formation so far.
	The observed history, i.e. the tuple of current observations, \Small{11r$_p$} is mapped to belief \Small{B$_{0,1}$} and \Small{11b$_{p} \mapsto $B$_{0,2}$}, \Small{11r$_p$,21b$_p \mapsto $B$_{1,1}$} and \Small{11b$_{p}$,21r$_p \mapsto $B$_{1,2}$}. 

	The sketched belief formation is knowledge-consistent. 
	Note in particular, that in order to be knoweldge consistent, \ego is not required to not change its mind regarding the car type, $\varphi_{ct}$ rather requires that \ego believes that the car type cannot change. That is, $\varphi_{ct}$ has to hold for each formed belief but \ego can form first a belief expressing \other is red and at the next step he can form a belief expressing \other is blue -- \ego would do this in scenario (d) of \autoref{fig:initBel}. 

	% Figure environment removed	

	Since at the second step, \ego's belief matches the reality, \ego is then able to assess the best strategy matching the real world scenario.
	For its strategic decision \ego can argue along the lines \enquote{Initially I thought the car is red and hasty and that it is a good idea to do the turn. Now I think the car is blue and slow and then the turn is not good idea, since I would collide with other. Since I believe, that my current belief matches the reality, I choose to drive straight on.}  
\end{mexample}
% Figure environment removed	

In the above example, all beliefs are singletons, \ie at each point in time \ego believes that there is only one possible reality. The next example illustrates the use of several alternative realities.
\begin{mexample}{Alternative Realities}\label{ex:realities}
	Let us assume that \ego is unsure of its own initial position, thinking that it may initially be at \Small{$x=1$} or \Small{$x=2$} (cf.~\autoref{fig:belSetup} (a)+(c)). 
	So when at state $s1$, \ego deems two realities possible (cf.~\autoref{fig:bellab3});
	in one reality, $\reality_1$, \ego is at \Small{$x=1$}, in the other, $\reality_2$, at \Small{$x=2$}.
	Since we assume that \ego has the same sensors as in \autoref{ex:knowledgeBelief}, in both realities,  $\reality_1$ and $\reality_2$, \other is believed to be red. 
	Similarly, \ego deems two realities possible when at $s6$ (also cf.~\autoref{fig:bellab3}.
	% Figure environment removed
\end{mexample}
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
\subsection{Doxastic Model} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We have by now introduced all components to model how an autonomous system \HAS build its beliefs based an perception and knowledge. 
We summarize the components by defining the notion of \emph{doxastic model}.
\begin{mdefinition}{Doxastic Models} 
	A doxastic model \epi of an autonomous system \HAS is given  by a tuple (\universeD,\goalList,\LabelK,\Obs,\Beliefs,\LabelB) of
	\begin{itemize} 
		\item the design-time world \universeD, 
		\item the prioritized list of goals \goalList, 
		\item the knowledge labelling \LabelK, 
		\item a set of observations $\Obs$, 
		\item the set of possible beliefs \Beliefs of \HAS and 
		\item a belief formation \LabelB.
	\end{itemize}
\end{mdefinition}
The belief formation describes how \HAS links observations made within the world \universeD to its inner representation of the world, i.e. the beliefs \Beliefs that it can possibly build.
The world \universeD is considered as ground truth during the design. 
Later design steps have to take care of the gap between \universeD and the real world.

Note, that we have not yet characterise how the system takes decisions. 
To this end we will introduce the notion of \emph{autonomous decision} (cf. Def.~\ref{def:doxSys}), in the next section that captures that a system takes its decisions based on the its beliefs.

