\section{A game-theoretic, doxastic framework}\label{sec:relFrameIR}
Above in \autoref{sec:designproc}, we describe when in the design process of a safety-critical autonomous system \HAS our approach of determining relevance can be helpful. 
In this section we are concerned with the formal ingredients of the framework. 
We discuss  the decisions taken in the design of the framework and point to related work.

In the terms of the IR literature, our relevance notion can be described as 
\emph{situational} --the circumstances of \HAS are taken into account--, 
\emph{subjective} --relevance is determined from the view point of \HAS--, 
\emph{goal-implied} --the goals of \HAS determine whether \HAS misses something relevant--, 
\emph{temporal and spacial} --the performance of \HAS during a maneuver is examined within space and time as captured in \universeD.
%and  \emph{context-dependent} --\HAS is considered with the context of \universeD--. 
The framework integrates these different dimensions, so that we can apply game theory to determine what observations and knowledge is necessary.

\emph{How does the framework integrate so many dimensions of relevance? How can a decision-procedure answer whether something is relevant?}

In short, we model beliefs on the one hand and we use a model of the application domain, \universeD, as ground truth on the other hand. 
We link the two via a two-player dynamic game -- one player is the autonomous system \HAS and the other player is the environment.	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Scope of the Framework}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We aim to support the development of safety-critical autonomous systems that can partially observe their environment. 
Their perceptions may be perturbed or may be contradicting each other. 
We assume that a system \HAS additionally uses its knowledge base to construct its beliefs. 
The knowledge base holds insights about the application domain, that an engineer provided at design time, as well as statements that \HAS gets from trusted sources during its mission.

By asking \enquote{What knowledge and what observations are necessary to build beliefs upon which \HAS can achieve its goals?} we treat belief formation as a central ingredient of our framework. 
Accordingly we use a \emph{doxastic model}, that is a model that captures beliefs explicitly.

A system \HAS necessarily builds approximating beliefs since its environment is vastly complex while its resources are limited (cf. \autoref{fig:beliefDeviation}).
A system \HAS aims for beliefs that capture the \emph{relevant} aspects.
Allowing the most freedom in building such beliefs provides the greatest potential for saving resources. 
% Figure environment removed

A belief describes what a system \HAS thinks is currently possible. 
To this end we use the possible world semantics \cite{reasoningA}. Accordingly, a belief is a set of possible worlds. 
Since our worlds capture their believed history, current state and future  we call them alternative realities (cf. \autoref{fig:relevanceRelationHAS}).% 
\footnote{The possible worlds semantics is often captured via Kripke structure $K$ where $K$'s states represent the worlds and $K$'s state transitions represent the accessibility relation, \ie $s\rightarrow s'$ means in world $s$ $s'$ is a possible world.},
We model that \HAS judges the best action based on its current belief. It does this by simulating whether the action will lead to a mission success in the future  of the believed realities.

Since we aim to characterize whether the system \HAS achieves its goals when choosing its actions based on its believes, we link the belief formation to ground truth \universeD, as illustrated in  \autoref{fig:worlds}. 
The feedback loop of \enquote{A system \HAS builds its beliefs based on its perceptions of \universeD.},
\enquote{A system \HAS chooses its actions based on its beliefs.} and
\enquote{A system \HAS's actions influence the state of \universeD.} establishes this link.

% Figure environment removed
Since we want to determine whether a sufficient belief can be formed by approximation of the ground truth \universeD,  we explicitly support beliefs that are structurally distinct from \universeD (cf. \autoref{fig:beliefDeviation}).
Therefore, the ground truth \universeD and the beliefs of \HAS are two separate structures in our framework.

In the framework, we model that \HAS has a knowledge base that captures insights built-in by engineers or received by trusted sources. 
The entries of the knowledge base represent \emph{believed knowledge}, that is \HAS thinks that the entries are true. 
But it is possible that the statements are false.
Our motivation of modelling a base of believed knowledge is, that \HAS will be equipped with rules approximating the reality. 
In order to detect rules and insights that are too coarse, we have to be able to model them in our framework. 
In the sequel we will often refer to the entries of the knowledge base simply as knowledge.

Given a belief formation, we use game theory to determine whether an autonomous safety-critical system \HAS will be successful in \universeD. We also use game theory to determine whether \HAS can form beliefs such that it will be successful. 
We regard a maneuver of \HAS as a dynamic game of the player \HAS and the surrounding world, which might include other agents. 
The system \HAS can control its actions while concurrently the environment chooses from its actions.  The combined actions determine the state change of the \universeD. 
We hence can examine evolutions along \HAS's maneuver in time and space with evolving context.  


At its core relevance is a relationship, as mentioned in \autoref{sec:relIR}. 
We examine what knowledge and observations of the world are relevant for \HAS.
\enquote{$X$ is relevant} entails \enquote{having made observation $X$/knowing $X$ makes a difference to \HAS} and not knowing/observing $X$ would hinder \HAS in achieving its goals in \universeD. 
We capture this aspect by defining knowledge/observations $X$ to be relevant, if there is no \enquote{smaller} $X'$ which enables \HAS achieving its goal (cf.~\autoref{def:wr}). 
We do this analogously to \cite{DF11}, where the minimal perimeter of a world model is determined. 
In a nutshell, we explore how well a system \HAS performs when we omit knowledge and observations.
If omission leads to a worse performance the omitted was relevant. 


The framework is concerned with \HAS's assessment of the environment via its sensors. 
To this end it only models first-order beliefs but not higher-order beliefs, i.e., beliefs about beliefs. 
Thus system $A$ cannot argue: \enquote{System $B$ will slow down -- I think that $B$ thinks there is a speed limit} or \enquote{System $B$ will slow down -- I  think that $B$ thinks that I think that $B$ should slow down}.  
Including higher-order beliefs will increase the overall complexity of the model. 
There are certainly application where modelling high-order beliefs is essential. We imagine that higher-order beliefs are essential when designing entertaining or comfort functions. 
There the mental state of a user has be taken into account and the system aims to optimally support the user rather guaranteeing goals. 
In contrast, safety-critical systems usually take decisions based on conservative approximations in order to be on the safe side. 



In game theory  \emph{rationality} is an central notion. 
Basically, a rational agent $A$ does what promises to result in the outcome $R$ that is best for $A$.
Different notions of rationality exist in literature varying in how to precisely and appropriately capture this notion for a given application.
We assume that \HAS chooses the action that it thinks will lead to the best result.
The system \HAS simulates the effect of its actions in its mind, i.e. it examines the effect on the current set of possible worlds.
So \HAS takes rationally belief-based decisions. 
We do not assume though, that \HAS rationally forms beliefs. 
For instance, we allow  that \HAS believes an object to be red, although according to its observations it is blue, we also allow \HAS to believe that an object is a house at one time instance and at the next time instance \HAS believes it is a tree.
We decided not to constrain the belief formation because of the way beliefs are constructed in autonomous systems. 
The belief of \HAS may be determined by a composition of different components, and there may not necessarily be an entity that ensures that the resulting belief is rational\footnote{What rational in this context should capture, would have to be discussed first.}. 

Our framework, nevertheless, supports the study of different kinds of belief formation functions and we consider it future work. 
We imagine that during the design, requirements regarding the belief formation might be specified. 
So, whether a belief formation exists, that satisfies the requirements, might be valuable insight when developing safety-critical autonomous systems. 
In this line of research, we are also interested in the formalisation of classes of requirements on the belief formation.
In particular, we are interested in belief formations satisfying certain robustness or stability criteria. 
A notion of robustness of  belief formation might express that a given rate of object misclassification can be tolerated.
A stability criterion might express that the beliefs are formed such that replanning is rare and triggered sufficiently early.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection{Works related to the Formal Approach \cite{doxFrame}}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Epistemology is the theory of knowledge and concerned with information-processing and cognitive success \cite{CollinsEpi,Sheffield}.
Doxastic means \enquote{relating to belief}~\cite{Collins}. 
By using the term \enquote{doxastic}, we want to stress that our formalism focuses on beliefs.
%
%
In the epistemic logic literature, the semantics of doxastic languages are often given via \emph{doxastic models}, that are special Kripke structures \cite{EpistemicsLogics}.
A doxastic model $(S,v,\rightarrow_i)$ consists of a set of nodes $S$ representing possible worlds $w$, a valuation function $v: S\rightarrow 2^{\Props}$ for the set of atomic facts \Props and a belief relation $\rightarrow_i$  for each player $i$, that specifies \enquote{$i$ deems $w'$ possible in $w$} if \enquote{$w$ $\rightarrow_i$ $w'$}. With other words, the belief of $i$ at $w$ is defined as the worlds accessible via the agent $i$'s belief relation, $\rightarrow_i$ \cite{EpistemicsLogics,DoxasticMeyer2003}. 
%
%


In this paper, we use complex possible worlds instead of the plain nodes of a Kripke structure. 
In our framework, each possible world is a Kripke structure itself, called alternative reality.
It encodes the believed histories, the current states and possible futures. 
%
A system \HAS uses alternative realities to simulate its strategy in order to decide on its current action. 
In our framework, a reality constitutes an extensive form two-player zero-sum game, where the winning condition is defined by the list of linear temporal logic (LTL) goals of \HAS.
The belief formation is based on partial observations and the currently available knowledge/strong beliefs.

A couple of epistemic temporal logics have been suggested for specifying aspects of knowledge throughout time for multi-agent systems. These logics combine temporal logics with knowledge operators, like KCTL \cite{CTLKMC}, KCTL$^*$ or HyperCTL$^{*}_{lp}$~\cite{HyperCTL}. They are interpreted over Kripke structures. 
But since an agent $i$ has its local view, only certain propositions are assumed to be observable, so that an observational equivalence relation $\sim_i$ on the traces arises. 
\enquote{Agent $i$ knows $\varphi$} then means that $\varphi$ holds on all $i$-equivalent initial traces. 
%
The alternating time temporal logics (ATL)~\cite{ATL} has been developed for reasoning about what agents can achieve by themselves or in groups throughout time. 
In ATL, the path quantifiers of CTL are replaced by modalities that allow to quantify paths in the control of groups of agents.
ATL is interpreted over concurrent game structures~(CGS), which are labelled state transition systems. 
By adding a knowledge operator, ATL has been extended to an epistemic variant, ATEL~\cite{ATEL}. 
To this end the concurrent game is extended by an observational equivalence relation per agent modelling the agent's limited view.
%

Just like the logics above, we assume that \HAS can only partially observe the ground truth.  
Our beliefs, however, cannot straightforwardly be expressed in terms of an equivalence on the ground truth, since an alternative reality may be a distinct Kripke structure and a belief does not have to include the ground truth.
%
In contrast to be above logics, we use in our framework a variant of LTL to specify constraints on the beliefs. 
A  so-called  BLTL formula is therefore interpreted on a belief \belief, i.e. a set of alternative realities. 
Since the set of possible beliefs \Beliefs is finite, a formula $\K\varphi$ means the finite conjunction $\bigwedge_{\reality\in\belief}\reality\models\varphi$. 


The field of epistemic planning is concerned with computing plans (\enquote{a finite succession of events} \cite{DELStrat}) that achieve the desirable state of knowledge from a given current state of knowledge~\cite{GIntroDEL}.  
DEL, dynamic epistemic logic, is a formalism to describe planning tasks succinctly by a semantic and action model based approach. 
Epistemic models capture the knowledge state of the agents, and epistemic action models describe how these are transformed. 
An evolution results from  a stepwise application of the available actions. 
In \cite{DELStrat} distributed synthesis of observational-strategies for multiplayer games are considered.  
%
While ATEL and DEL allow for reasoning about a combination of knowledge and strategies, we are interested in the belief \emph{formation}. 
We ask whether there exists a belief formation that justifies a strategy that successfully achieves temporal goals within a given ground truth world. 

Properties of belief formation are studied in the field of belief revision and update. 
Belief revision is done when a new piece of information contradicts the current information, and it aims to determine a consistent belief set. 
Belief updates may be necessary when the world is dynamic~\cite{BelRevIntro}. 
The works in this field are concerned with rational belief formation, following e.g. some guiding principle like making minimal changes~\cite{BelRevIntro}. 
%
In our work, we consider very general belief formation functions, since we focus on safety-critical autonomous systems.

BDI agents are rational agents with the mental attitudes of belief~(B), desire~(D) and intention~(I)~\cite{BDI}.
Beliefs describe what information the agent has, 
desires represent the agent's motivational state and specify what the agent would like to achieve, 
while intentions represent the currently chosen course of action. 
These attitudes allow an agent balancing between deliberation about its course of action and its commitment to the chosen course of action.
%
In our framework, an agent deliberates about its course of action at each state. 
We do not enforce commitment to a certain course of action, as we are interested in whether some belief formation exists.
Nevertheless, the framework conceptually allows capturing notions of commitment, and we plan to examine these in future work. 
Basically, a chosen action represents a set of believed best possible world strategies. These can be considered as the current intent. 
So, a notion of commitment could require that (some) strategies of the previous belief are still best strategies in the current belief.
An engineer may then specify when a system should be committed.


