\section{Relevance}\label{sec:relevance}
We consider the combination (\LabelK,\Obs,\Beliefs) of labeled knowledge, observations and possible beliefs as important set screws for an engineer to develop an optimal autonomous-decisive system.
 \Small{{{\RE}\,{\LabelK}}}, he can equip the system \HAS with prior knowledge and implement mechanisms to update \HAS's knowledge base during the mission, 
 \Small{{{\re}\,{\Obs}}}, he can provide more sensing capabilities and, 
 \Small{{{\re}\,{\Beliefs}}}, he can increase the resources for the internal representation of the world model.
In the following we denote (\LabelK,\Obs,\Beliefs) also as \kob.

To support an engineer, we characterise whether a tuple \kob is sufficient for a given setting.
The basic idea is: If a system is an \emph{optimal} autonomous system, then its formed beliefs conserve the relevant aspects of \universeD. 
Hence the \kob is sufficient if a relevance conserving belief formation exists. 
To answer whether \kob are relevant, we test whether it is possible to build an optimal autonomous system with less knowledge, observations or beliefs. 

For the following we consider a doxastic model $\epi$ to be given with $(\universeD,\goalList,\LabelK,\Obs,$ $\Beliefs,\LabelB)$ with a knowledge-consistent belief formation \LabelB and a system $\sys=(\epi,\stratb)$ with a doxastic strategy.\\


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Conservation of the Relevant}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We first define when the relevant is conserved. 
Therefore we compare \ego's (doxastic and autonomous-decisive) performance with the performance that \ego could have when it would access the ground-truth, \universeD. 


We first develop a notion of relevance conservation for doxastic systems in order to highlight that the requirements for autonomous systems are more demanding. \\

We say that the belief formation \LabelB conserves the relevant of \universeD, 
if \epi can perform based on its beliefs as successful as it could when directly and truthfully observing the ground-truth \universeD. %
%
\begin{mdefinition}{Relevance Conservation for Doxastic Systems}\label{def:wrelcon}
Let $\Obs_D\subseteq\Props$ be a set of propositions.
	The belief formation $\LabelB$ of a doxastic model $\epi=(\universeD,\goalList,\LabelK,\Obs,\Beliefs,\LabelB)$ \emph{conserves the relevant of a $\Obs$-observable \universeD}, if there exists a doxastic strategy \stratb  for \epi that is dominant \wrt all $\Props$-observing strategies $\strats$.
\end{mdefinition}
%
When $\LabelB$ is conserving the relevant of  completely observable design-time model \universeD. 
then \ego could --by implementing \stratb of Def.~\ref{def:wrelcon}-- perform as well as possible when the ground-truth \universeD would be completely observable. 
Def.~\ref{def:wrelcon} captures this aspect by comparing the performance of \ego that is observing \Obs with the performance on the ground-truth \universeD that is observable via $\Props$.  

But what does it mean that a belief formation $\LabelB$ conserves the relevant? Intuitively, it means that $\LabelB$ preserves \universeD in sufficient detail to map the history of beliefs \enquote{somehow} to the best action.
The choice of action does not have to be plausible \wrt the content of a system's beliefs though. 
It is up to the engineer to choose which strategy $\stratb$ will be implemented by the system.\\

The choice of action must be plausible \wrt the belief content though, when it comes to autonomously-decisive systems.
An autonomous-decisive system chooses at all times actions \act that are justified in the respective current belief \belief, \ie $\act\in\Small{\Act(\belief)}$ (cf. Def.~\ref{def:choiceb}).
We hence say that the belief formation conserves the relevant for autonomous-decisiveness, if at all times the \enquote{best actions} \wrt the belief content are chosen. 
%
\begin{mdefinition}{Relevance Conservation for Autonomous-Decisiveness}\label{def:relcon}
	Let $\Stratsbb$ be the set of autonomous strategies that exist for $\epi=(\universeD,\goalList,\LabelK,\Obs,\Beliefs,\LabelB)$.


	The belief formation $\LabelB$ of \epi \emph{conserves the relevant of a $\Obs$-observable design-time world \universeD for autonomous-decisiveness}, 
	if all $\stratbb\in\Stratsbb$ are dominant \wrt $\Props$-observing strategies $\strats$ on \universeD.
\end{mdefinition}
%
Note, that we assume (\cf Ass. \ref{ass:csdec}, p.~\pageref{ass:csdec}) that the belief formation \LabelB forms only beliefs \belief in which a dominant current-state decisive strategy exists. Hence  $\Stratsbb$ contains at least one strategy. 
 
 
 If \LabelB conserves the relevant for autonomous-decisiveness as defined in Def.~\ref{def:relcon}, then any of the autonomous strategies \stratbb of \epi observing \universeD via \Obs will perform  as successful as possible when directly accessing \universeD via $\Obs'$. 
 It may seem surprising, that Def.~\ref{def:relcon} refers to \emph{all} autonomous strategies $\stratbb\in\Stratsbb$. The reason is, that the final decision on the chosen action lies with the autonomous system. 
%

\begin{mexample}{Conservation of the Relevant}\label{ex:conserve}
	As an example of a belief formation that conserves the relevant for autonomous-decisiveness, we refer the reader back to  \autoref{ex:autonAutom2} on page \pageref{ex:autonAutom2}. 
There we sketched a setting where the sensors are initially broken but when the decision has to taken the sensors provide the relevant information. 
The resulting belief formation $\LabelB_{\textsf{auton}}$ allows a system to perform as well as when knowing the ground-truth, \ie not having initially disturbed sensor readings. 
	In  \autoref{ex:autonAutom} on page \pageref{ex:autonAutom} we saw an example of a belief formation that conserves the relevant for doxastic systems but not relevance for autonomous-decisiveness. 
	In the example, \ego's sensors are permanently switching colours and \ego has a knowledge base that forces it to believe that a read car is fast and a slow car is blue. 
	Consequently, \ego cannot autonomously determine what is best to do in \universeD. 
	But the belief-formation is such that an engineer can choose a strategy for \ego that deals with the color readings appropriately, \ie \enquote{switch them back}.
\end{mexample}

Conserving the relevant for autonomous-decisiveness is stronger than conserving the relevant for doxastic systems: 
%
%
\begin{proposition}[Relevance Conservation]\label{th:refine}
	\enspace\\[-2mm]\enspace
	\begin{enumerate}
		\item\label{it:autDox} If \LabelB conserves the relevant for autonomous-decisiveness, then \LabelB conserves the relevant for doxastic systems.
	
		\item\label{it:doxAut} If \LabelB conserves the relevant for doxastic systems, then \LabelB does not necessarily conserve the relevant for autonomous-decisiveness.
	\end{enumerate}
	\end{proposition} 
%
\begin{proofsketch}{Prop. \ref{th:refine}}
	Prop.~\ref{th:refine}(\ref{it:autDox}) follows directly from Def.~\ref{def:wrelcon} and Def.~\ref{def:relcon}. Prop.~\ref{th:refine}(\ref{it:doxAut}) follows from the example \ref{ex:conserve}.
\end{proofsketch}


The next proposition is concerned with systems, where the belief formation is captured via a set of rules. 
Such autonomous systems still play an important role especially in safety critical applications, although artificial intelligence systems, that intransparently build their beliefs, gain more and more importance.

Since the resources of a system \HAS are limited, we consider belief formation functions that can be represented by a finite number of regular expressions. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{mdefinition}{Regular Belief Formation}
We say \LabelB is regular, if $\LabelB$ can be defined via a finite number $n$ of regular expressions $\regEx_i$, i.e., 
for all observable histories $\history\in\histories_{\Obs}$ of \universeD it holds, that there is an $i, 1\leq i\leq n$ such that $\LabelB(\history)=\belief_i$ iff $\history\models\regEx_i$.
\end{mdefinition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Given a doxastic model with a regular belief formation \LabelB, we can decide whether \LabelB conserves the relevant for autonomous-decisiveness:
%
\begin{proposition}[Conservation of Relevance]\label{th:check}
	Given a regular belief formation \LabelB, we can decide whether \LabelB conserves the relevant for autonomous-decisiveness.
\end{proposition}
%
\begin{mproof}{Prop. \ref{th:check}}
	We first determine the maximal priority $\mn$ up to which the goal list $\goalList$ can be achieved on \universeD by applying iteratively strategy synthesis for LTL properties~\cite{LTLSynth} starting with the maximum goal list and then iteratively decreasing $\mn$. 	
	\LabelB of $\epi$ conserves the relevant, if all autonomous strategies $\stratbb\in\Stratsbb$ achieve at least $\mn$ (\cf~Def.~\ref{def:relcon}).
	We then construct an automaton $\aut_{\Act()\times\universeD}$, in which the environment is unconstrained and \ego chooses its actions from $\Act(\LabelB(h))$ after observing history $h$.
	It holds that iff $\aut_{\Act()\times\universeD}$ satisfies $\goalList$ up to $\mn$, then  \LabelB conserves the relevant for autonomous systems.

	Construction of $\aut_{\Act()\times\universeD}$: For each belief $\belief\in\Beliefs$, we can determine the current-state choices $\bAct(\belief)$ (Prop.~\ref{prop:actBel}). 
	Thus, the belief formation \LabelB can be considered as an \Obs-observing strategy assigning $\Act(\belief)$ to an observed history \history with $\belief=\LabelB(\history)$: 
	Since \LabelB is regular, we can build a mealy automaton $\aut_{\Act()}$ that determines $\Act(\LabelB(\history))$ for an observed history \history.
	When $\aut_{\Act()}$ transitions to an accepting state because of \history, this transition gets labelled with the current-state choices $\Act(\LabelB(\history))$.
	We derive a composed automaton ${\aut_{\Act()\times\universeD}}$ by parallel composition of the design-time world \universeD and $\aut_{\Act()}$.  
	In $\aut_{\Act()\times\universeD}$, \ego can take an action \act only if  $\aut_{\Act()}$ allows this, i.e. it is a current-state choice for the observed history. 
	If \ego may not take $\act_1$ in state \state, the combined action $\act=(\act_1,\act_2)$ for all $\act_2\in \Act_{\env}$, leads to the state \sundef. 
\end{mproof}
%

In the next section we will characterise what knowledge, observations and beliefs are relevant.
Thereby we turn to questions like \enquote{Can we do with less observations?}, \enquote{Can we do with less detailed beliefs?} or \enquote{Can we compensate missing observations by adding knowledge?}.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Relevance of \kob}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Our notion of \emph{relevance conservation} characterises combinations of \kob that allow  a system to form beliefs that are \emph{sufficiently precise} for the system to be optimal. 
In this section we define that \kob is \emph{relevant}, if it conserves the relevant (\ie is sufficient), and in additional also \emph{\enquote{minimal}}. 
%we examine what combinations of \kob, \ie believes knowledge, observations, possible beliefs, are \emph{necessary} for \ego to be an optimal autonomous system.

The three dimensions of \kob are of course interrelated. Intuitively, knowledge (\LabelK) about the world can replace observations that a system \HAS needs otherwise. 
Having more resources for the representation of the inner world model (\Beliefs) allows a system \HAS to store more of the made observations and allows it  to make finer predictions.
More observations (\Obs) vice versa allow \HAS to forget more  and thus have a simpler model of the history and future or to have less knowledge.
We hence expect that often several incomparable minima can be determined. 

To define a \emph{minimal \kob}, we first define partial order relations on  the set of knowledge labeling functions, the set of observations and the set of possible beliefs. 
We then infer a partial order to order tuples \kob. 


We chose the partial orders to reflect the decisions an engineer has to make during the design:
\begin{enumerate}[label=\small(PO\arabic*)\normalsize,itemindent=4mm]
	\item\label{i:obs}  $\Obs\leq\Obs'$ $:\Leftrightarrow$ $\Obs\subseteq\Obs'$\\
For this paper we assume that a greater set of observations means that more sensors are necessary. We are hence interested in determining the minimal set of required observations. 
	\item\label{i:bel}  $\Beliefs\leq\Beliefs'$ $:\Leftrightarrow$ $\Beliefs\subseteq\Beliefs'$\\
		%A belief $\belief\in\Beliefs$ represents in this paper merely the current-state choices. 
		%So, the minimal set of required beliefs represents the simplest encoding of different current-state choices.
		%this is not true due to the possibility to link the belief to the ground-truth
		For the design of a \HAS the size of the set of possible beliefs \Beliefs corresponds to the resources that are necessary to encode the beliefs.
	\item\label{i:know} $\LabelK\leq\LabelK'$ $:\Leftrightarrow$ $\forall \state\in\States: \LabelK(\state) \leq \LabelK'(\state)$ $:\Leftrightarrow$ $\forall \state\in\States: [\LabelK'(\state)]\subseteq [\LabelK(\state)]$,\\
		where [{\priorK}] denotes the set of traces on all possible worlds, $\Worlds$, that satisfy the believed knowledge $\priorK$. 
		As we deal with knowledge-consistent belief formations here, $\priorK\leq\priorK'$ means that \priorK' constrains the beliefs that can be formed less. 
		In other words, the system \HAS knows less since it has more uncertainty.
		
		$\LabelK\leq\LabelK'$ means that $\LabelK'$ declares more knowledge at least at one state of \universeD and it declares not less knowledge than \LabelK in all other states. 
		An engineer can provide prior knowledge, \eg she can hard-code the believed knowledge into \HAS, and she can implement the knowledge labelling, i.e. ensure that mechanisms are in place that will update the knowledge base during \HAS's missions.
	\item\label{i:kob} $\kob \leq \kobp$ $:\Leftrightarrow$ \ref{i:obs}-\ref{i:know} hold.

\end{enumerate}
By  \ref{i:kob} we now  define the notion of \emph{weak relevance}. 
A tuple \kob is weak relevant, if we cannot find a strictly smaller tuple \kobp.
We call this \emph{weak}, since there can be other tuples \kob that are incomparable with \kob.
Hence the question \enquote{Is \kob relevant} does not have a definite answer.
Nevertheless the notion of weak relevance allows to answer, whether a system \HAS can do with more observations in exchange for less knowledge or fewer possible beliefs or whether more knowledge allows \HAS to have fewer possible beliefs or less observations and so on. 

%
%
\begin{definition}[Weak Relevance]\label{def:wr}
	Let a design-time world $\universeD$ and a prioritised list of goals $\goalList$ be given.

	\kob is weakly relevant for $(\universeD,\goalList)$, if 
	\begin{enumerate}
		\item\label{def:cons} there is a belief formation \LabelB of $\epi:=(\universeD,\goalList,\LabelK,\Obs,\Beliefs,\LabelB)$ that conserves the relevant for autonomous systems and 
		\item\label{def:min} for all $\kobp\not=\kob$ with 
			$\LabelK'\leq\LabelK$, $\Obs'\leq\Obs$ and $\Beliefs'\leq\Beliefs$
			$\kobp$  there is no knowledge-consistent belief formation \LabelB' of $\epi':=(\universeD,\goalList,\LabelK',\Obs',\Beliefs',\LabelB')$ that conserves the relevant for autonomous systems.
	\end{enumerate}
	\LabelK is weakly relevant if there are \Obs and \Beliefs, such that \kob is weakly relevant. 
	Analogously we define that \Obs (\Beliefs) is weakly relevant if there are \LabelB, \LabelK and \Beliefs (\Obs). 
\end{definition}
To illustrate the notion, we consider an example. 
\begin{mexample}{Weak Relevance}
Let us assume \ego observes its position $\pos$, a time stamp $t$ and its speed $v$. Its goal is to determine its past average acceleration $\acc$.%
%
\footnote{We assume finite domains and hence finite encodings of numerical values. The computations will be rounded appropriately. \Ego's actions are computation steps.} 
%
Moreover, let us assume that the perception of position is flawed when it is raining while the speed is still correctly measured.  
Then only $\{v,t\}$ is weakly relevant, that is, they suffice to determine the average acceleration. 
	Neither the set $\{\pos,v,t\}$ is weakly relevant nor the set $\{\pos,t\}$. The further is not minimal, the latter does not conserve the relevant, since $\acc$ cannot be determined while it is raining. 

	
	Given \ego has the knowledge \enquote{it will not rain} both $\{\pos,t\}$ and $\{v,t\}$ are weakly relevant. 
\end{mexample} 

Let us now turn to questions  like \enquote{Is \Obs relevant, given \LabelK and \Beliefs?}, i.e. we assume tow component of the triple are known.
The question of relevance ten might have a definite answer, but not necessarily. We hence consider it an interesting notion.
In Def.~\ref{def:relevance} we define \LabelK (\Obs, \Beliefs) to be relevant, if there is no alternative minimal choice, i.e., the system \HAS has to have \LabelK (\Obs, \Beliefs) in order to be able to perform autonomously optimal.
\begin{mdefinition}{Relevance}\label{def:relevance}
	\Obs is relevant for (\universeD,\goalList) with (\LabelK,\Beliefs) iff
	\begin{enumerate}
		\item \Obs is weakly relevant and 
		\item there is no other \Obs' that is weakly relevant.
	\end{enumerate}
	Likewise we define \LabelK and  \Beliefs are relevant for (\universeD,\goalList) with (\Obs,\Beliefs) and respectively (\LabelK,\Obs). 
\end{mdefinition}
%
%
\begin{mtheorem}[Relevance]\label{th:relevant}
	Given a doxastic model   $\epi=(\universeD,\goalList,\priorK,\Obs,\LabelB)$ of \HAS within its environment, we can decide whether \ko is (weakly) relevant for \LabelB in \epi. 
\end{mtheorem}	
\begin{mproof}{\autoref{th:relevant}}
	To show \autoref{def:cons} of Def.~\ref{def:wr} we check whether there is a \Obs-observing strategy in \universeD. 
	To check \autoref{def:min} of Def.~\ref{def:wr} we build the \enquote{lesser} pairs $(\priorK',\Obs')$, i.e. $\priorK'\leq\priorK$, $\Obs'\leq\Obs$ and $(\priorK,\Obs)\not=(\priorK',\Obs')$, and check whether there is a belief labelling \LabelB' that conserves the relevant again by checking whether there a $\Obs'$-observing strategy in \universeD.
\end{mproof}
