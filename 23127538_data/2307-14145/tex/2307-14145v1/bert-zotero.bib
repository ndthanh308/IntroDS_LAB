
@article{isomura_vitro_2018,
	title = {In vitro neural networks minimise variational free energy},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-35221-w},
	doi = {10.1038/s41598-018-35221-w},
	abstract = {In this work, we address the neuronal encoding problem from a Bayesian perspective. Specifically, we ask whether neuronal responses in an in vitro neuronal network are consistent with ideal Bayesian observer responses under the free energy principle. In brief, we stimulated an in vitro cortical cell culture with stimulus trains that had a known statistical structure. We then asked whether recorded neuronal responses were consistent with variational message passing based upon free energy minimisation (i.e., evidence maximisation). Effectively, this required us to solve two problems: first, we had to formulate the Bayes-optimal encoding of the causes or sources of sensory stimulation, and then show that these idealised responses could account for observed electrophysiological responses. We describe a simulation of an optimal neural network (i.e., the ideal Bayesian neural code) and then consider the mapping from idealised in silico responses to recorded in vitro responses. Our objective was to find evidence for functional specialisation and segregation in the in vitro neural network that reproduced in silico learning via free energy minimisation. Finally, we combined the in vitro and in silico results to characterise learning in terms of trajectories in a variational information plane of accuracy and complexity.},
	pages = {16926},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Isomura, Takuya and Friston, Karl},
	urldate = {2022-01-31},
	date = {2018-11-16},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Learning algorithms, Neural encoding},
	file = {Full Text PDF:/Users/bert/Zotero/storage/ABAE2MCY/Isomura and Friston - 2018 - In vitro neural networks minimise variational free.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RN4XPWTT/s41598-018-35221-w.html:text/html},
}

@article{cano_musical_2019,
	title = {Musical Source Separation: An Introduction},
	volume = {36},
	issn = {1053-5888, 1558-0792},
	url = {https://ieeexplore.ieee.org/document/8588410/},
	doi = {10.1109/MSP.2018.2874719},
	shorttitle = {Musical Source Separation},
	pages = {31--40},
	number = {1},
	journaltitle = {{IEEE} Signal Processing Magazine},
	shortjournal = {{IEEE} Signal Process. Mag.},
	author = {Cano, Estefania and {FitzGerald}, Derry and Liutkus, Antoine and Plumbley, Mark D. and Stoter, Fabian-Robert},
	urldate = {2022-01-18},
	date = {2019-01},
	langid = {english},
	file = {Cano et al. - 2019 - Musical Source Separation An Introduction.pdf:/Users/bert/Zotero/storage/Y9469LLN/Cano et al. - 2019 - Musical Source Separation An Introduction.pdf:application/pdf},
}

@report{allen_seductive_2022,
	title = {The Seductive Allure of Cargo Cult Computationalism},
	url = {https://psyarxiv.com/9k5yx/},
	abstract = {Bruineberg and colleagues report a striking confusion, in which the formal Bayesian notion of a “Markov Blanket” has been frequently misunderstood and misapplied to phenomena of mind and life. I argue that misappropriation of formal concepts is pervasive in the “predictive processing” literature, and echo Richard Feynman in suggesting how we might resist the allure of cargo cult computationalism.},
	institution = {{PsyArXiv}},
	author = {Allen, Micah},
	urldate = {2022-01-07},
	date = {2022-01-06},
	langid = {english},
	doi = {10.31234/osf.io/9k5yx},
	note = {type: article},
	keywords = {cargo cult science, cognitive science, commentary, computational neuroscience, Computational Neuroscience, free energy principle, markov blankets, Neuroscience, philosophy of science, predictive processing},
	file = {Allen - 2022 - The Seductive Allure of Cargo Cult Computationalis.pdf:/Users/bert/Zotero/storage/BMD57SSH/Allen - 2022 - The Seductive Allure of Cargo Cult Computationalis.pdf:application/pdf},
}

@article{pezzulo_evolution_2022,
	title = {The evolution of brain architectures for predictive coding and active inference},
	volume = {377},
	url = {https://royalsocietypublishing.org/doi/full/10.1098/rstb.2020.0531},
	doi = {10.1098/rstb.2020.0531},
	abstract = {This article considers the evolution of brain architectures for predictive processing. We argue that brain mechanisms for predictive perception and action are not late evolutionary additions of advanced creatures like us. Rather, they emerged gradually from simpler predictive loops (e.g. autonomic and motor reflexes) that were a legacy from our earlier evolutionary ancestors—and were key to solving their fundamental problems of adaptive regulation. We characterize simpler-to-more-complex brains formally, in terms of generative models that include predictive loops of increasing hierarchical breadth and depth. These may start from a simple homeostatic motif and be elaborated during evolution in four main ways: these include the multimodal expansion of predictive control into an allostatic loop; its duplication to form multiple sensorimotor loops that expand an animal's behavioural repertoire; and the gradual endowment of generative models with hierarchical depth (to deal with aspects of the world that unfold at different spatial scales) and temporal depth (to select plans in a future-oriented manner). In turn, these elaborations underwrite the solution to biological regulation problems faced by increasingly sophisticated animals. Our proposal aligns neuroscientific theorising—about predictive processing—with evolutionary and comparative data on brain architectures in different animal species.

This article is part of the theme issue ‘Systems neuroscience through the lens of evolutionary theory’.},
	pages = {20200531},
	number = {1844},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Pezzulo, Giovanni and Parr, Thomas and Friston, Karl},
	urldate = {2022-01-05},
	date = {2022-02-14},
	note = {Publisher: Royal Society},
	keywords = {predictive processing, active inference, brain architecture, brain evolution, model selection, natural selection},
	file = {Pezzulo et al. - 2022 - The evolution of brain architectures for predictiv.pdf:/Users/bert/Zotero/storage/8NUP74QF/Pezzulo et al. - 2022 - The evolution of brain architectures for predictiv.pdf:application/pdf},
}

@book{murphy_probabilistic_2022,
	title = {Probabilistic Machine Learning: An introduction},
	url = {probml.ai},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin},
	date = {2022},
	file = {Murphy - 2022 - Probabilistic Machine Learning An introduction.pdf:/Users/bert/Zotero/storage/CE2ZLQ2M/Murphy - 2022 - Probabilistic Machine Learning An introduction.pdf:application/pdf},
}

@article{annila_statistical_2021,
	title = {Statistical Physics of Evolving Systems},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/12/1590},
	doi = {10.3390/e23121590},
	abstract = {Evolution is customarily perceived as a biological process. However, when formulated in terms of physics, evolution is understood to entail everything. Based on the axiom of everything comprising quanta of actions (e.g., quanta of light), statistical physics describes any system evolving toward thermodynamic balance with its surroundings systems. Fluxes of quanta naturally select those processes leveling out differences in energy as soon as possible. This least-time maxim results in ubiquitous patterns (i.e., power laws, approximating sigmoidal cumulative curves of skewed distributions, oscillations, and even the regularity of chaos). While the equation of evolution can be written exactly, it cannot be solved exactly. Variables are inseparable since motions consume driving forces that affect motions (and so on). Thus, evolution is inherently a non-deterministic process. Yet, the future is not all arbitrary but teleological, the final cause being the least-time free energy consumption itself. Eventually, trajectories are computable when the system has evolved into a state of balance where free energy is used up altogether.},
	pages = {1590},
	number = {12},
	journaltitle = {Entropy},
	author = {Annila, Arto},
	urldate = {2021-12-02},
	date = {2021-12},
	langid = {english},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {natural selection, dissipative systems, evolution, free energy, power laws, quantum of action},
	file = {Annila - 2021 - Statistical Physics of Evolving Systems.pdf:/Users/bert/Zotero/storage/R9UU9P9M/Annila - 2021 - Statistical Physics of Evolving Systems.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XBS28X96/1590.html:text/html},
}

@article{sarkka_temporal_2020,
	title = {Temporal Parallelization of Bayesian Smoothers},
	url = {http://arxiv.org/abs/1905.13002},
	abstract = {This paper presents algorithms for temporal parallelization of Bayesian smoothers. We define the elements and the operators to pose these problems as the solutions to all-prefix-sums operations for which efficient parallel scan-algorithms are available. We present the temporal parallelization of the general Bayesian filtering and smoothing equations and specialize them to linear/Gaussian models. The advantage of the proposed algorithms is that they reduce the linear complexity of standard smoothing algorithms with respect to time to logarithmic.},
	journaltitle = {{arXiv}:1905.13002 [cs, math, stat]},
	author = {Särkkä, Simo and García-Fernández, Ángel F.},
	urldate = {2021-12-02},
	date = {2020-02-20},
	eprinttype = {arxiv},
	eprint = {1905.13002},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Mathematics - Dynamical Systems, Statistics - Computation},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XE5BVMVK/1905.html:text/html;Särkkä and García-Fernández - 2020 - Temporal Parallelization of Bayesian Smoothers.pdf:/Users/bert/Zotero/storage/D2WRVVSQ/Särkkä and García-Fernández - 2020 - Temporal Parallelization of Bayesian Smoothers.pdf:application/pdf},
}

@article{ofner_predictive_2021,
	title = {Predictive coding, precision and natural gradients},
	url = {http://arxiv.org/abs/2111.06942},
	abstract = {There is an increasing convergence between biologically plausible computational models of inference and learning with local update rules and the global gradient-based optimization of neural network models employed in machine learning. One particularly exciting connection is the correspondence between the locally informed optimization in predictive coding networks and the error backpropagation algorithm that is used to train state-of-the-art deep artificial neural networks. Here we focus on the related, but still largely under-explored connection between precision weighting in predictive coding networks and the Natural Gradient Descent algorithm for deep neural networks. Precision-weighted predictive coding is an interesting candidate for scaling up uncertainty-aware optimization -- particularly for models with large parameter spaces -- due to its distributed nature of the optimization process and the underlying local approximation of the Fisher information metric, the adaptive learning rate that is central to Natural Gradient Descent. Here, we show that hierarchical predictive coding networks with learnable precision indeed are able to solve various supervised and unsupervised learning tasks with performance comparable to global backpropagation with natural gradients and outperform their classical gradient descent counterpart on tasks where high amounts of noise are embedded in data or label inputs. When applied to unsupervised auto-encoding of image inputs, the deterministic network produces hierarchically organized and disentangled embeddings, hinting at the close connections between predictive coding and hierarchical variational inference.},
	journaltitle = {{arXiv}:2111.06942 [cs]},
	author = {Ofner, Andre and Ratul, Raihan Kabir and Ghosh, Suhita and Stober, Sebastian},
	urldate = {2021-12-01},
	date = {2021-11-12},
	eprinttype = {arxiv},
	eprint = {2111.06942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/N4UDTDGJ/2111.html:text/html;Ofner et al. - 2021 - Predictive coding, precision and natural gradients.pdf:/Users/bert/Zotero/storage/VFFD82XW/Ofner et al. - 2021 - Predictive coding, precision and natural gradients.pdf:application/pdf},
}

@article{baltieri_kalman_2021,
	title = {Kalman filters as the steady-state solution of gradient descent on variational free energy},
	url = {http://arxiv.org/abs/2111.10530},
	abstract = {The Kalman filter is an algorithm for the estimation of hidden variables in dynamical systems under linear Gauss-Markov assumptions with widespread applications across different fields. Recently, its Bayesian interpretation has received a growing amount of attention especially in neuroscience, robotics and machine learning. In neuroscience, in particular, models of perception and control under the banners of predictive coding, optimal feedback control, active inference and more generally the so-called Bayesian brain hypothesis, have all heavily relied on ideas behind the Kalman filter. Active inference, an algorithmic theory based on the free energy principle, specifically builds on approximate Bayesian inference methods proposing a variational account of neural computation and behaviour in terms of gradients of variational free energy. Using this ambitious framework, several works have discussed different possible relations between free energy minimisation and standard Kalman filters. With a few exceptions, however, such relations point at a mere qualitative resemblance or are built on a set of very diverse comparisons based on purported differences between free energy minimisation and Kalman filtering. In this work, we present a straightforward derivation of Kalman filters consistent with active inference via a variational treatment of free energy minimisation in terms of gradient descent. The approach considered here offers a more direct link between models of neural dynamics as gradient descent and standard accounts of perception and decision making based on probabilistic inference, further bridging the gap between hypotheses about neural implementation and computational principles in brain and behavioural sciences.},
	journaltitle = {{arXiv}:2111.10530 [cs, eess, math, q-bio, stat]},
	author = {Baltieri, Manuel and Isomura, Takuya},
	urldate = {2021-12-01},
	date = {2021-11-20},
	eprinttype = {arxiv},
	eprint = {2111.10530},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2U2Z56R5/2111.html:text/html;Baltieri and Isomura - 2021 - Kalman filters as the steady-state solution of gra.pdf:/Users/bert/Zotero/storage/NMFMZS7A/Baltieri and Isomura - 2021 - Kalman filters as the steady-state solution of gra.pdf:application/pdf},
}

@article{ofner_predictive_2021-1,
	title = {Predictive coding, precision and natural gradients},
	url = {http://arxiv.org/abs/2111.06942},
	abstract = {There is an increasing convergence between biologically plausible computational models of inference and learning with local update rules and the global gradient-based optimization of neural network models employed in machine learning. One particularly exciting connection is the correspondence between the locally informed optimization in predictive coding networks and the error backpropagation algorithm that is used to train state-of-the-art deep artificial neural networks. Here we focus on the related, but still largely under-explored connection between precision weighting in predictive coding networks and the Natural Gradient Descent algorithm for deep neural networks. Precision-weighted predictive coding is an interesting candidate for scaling up uncertainty-aware optimization -- particularly for models with large parameter spaces -- due to its distributed nature of the optimization process and the underlying local approximation of the Fisher information metric, the adaptive learning rate that is central to Natural Gradient Descent. Here, we show that hierarchical predictive coding networks with learnable precision indeed are able to solve various supervised and unsupervised learning tasks with performance comparable to global backpropagation with natural gradients and outperform their classical gradient descent counterpart on tasks where high amounts of noise are embedded in data or label inputs. When applied to unsupervised auto-encoding of image inputs, the deterministic network produces hierarchically organized and disentangled embeddings, hinting at the close connections between predictive coding and hierarchical variational inference.},
	journaltitle = {{arXiv}:2111.06942 [cs]},
	author = {Ofner, Andre and Ratul, Raihan Kabir and Ghosh, Suhita and Stober, Sebastian},
	urldate = {2021-11-27},
	date = {2021-11-12},
	eprinttype = {arxiv},
	eprint = {2111.06942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/V5K938VA/Ofner et al. - 2021 - Predictive coding, precision and natural gradients.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/DWN6CKVX/2111.html:text/html},
}

@article{hopfield_physics_1994,
	title = {Physics, Computation, and Why Biology Looks so Different},
	volume = {171},
	issn = {0022-5193},
	url = {https://www.sciencedirect.com/science/article/pii/S0022519384712112},
	doi = {10.1006/jtbi.1994.1211},
	abstract = {The biological world is a physical system whose properties and behaviors seem entirely foreign to physics. The origins of this discrepancy lie in the very high information content in biological systems (the large amount of dynamically broken symmetry) and the evolutionary value placed on predicting the future (computation) in an environment which is inhomogeneous in time and in space. Within this context, "free will" can be described as a useful predictive myth.},
	pages = {53--60},
	number = {1},
	journaltitle = {Journal of Theoretical Biology},
	shortjournal = {Journal of Theoretical Biology},
	author = {Hopfield, J. J.},
	urldate = {2021-11-11},
	date = {1994-07-01},
	langid = {english},
	file = {Hopfield - 1994 - Physics, Computation, and Why Biology Looks so Dif.pdf:/Users/bert/Zotero/storage/QLUTAZLF/Hopfield - 1994 - Physics, Computation, and Why Biology Looks so Dif.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/8N7LLPLD/S0022519384712112.html:text/html},
}

@article{friston_world_2021,
	title = {World model learning and inference},
	volume = {144},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021003610},
	doi = {10.1016/j.neunet.2021.09.011},
	abstract = {Understanding information processing in the brain—and creating general-purpose artificial intelligence—are long-standing aspirations of scientists and engineers worldwide. The distinctive features of human intelligence are high-level cognition and control in various interactions with the world including the self, which are not defined in advance and are vary over time. The challenge of building human-like intelligent machines, as well as progress in brain science and behavioural analyses, robotics, and their associated theoretical formalisations, speaks to the importance of the world-model learning and inference. In this article, after briefly surveying the history and challenges of internal model learning and probabilistic learning, we introduce the free energy principle, which provides a useful framework within which to consider neuronal computation and probabilistic world models. Next, we showcase examples of human behaviour and cognition explained under that principle. We then describe symbol emergence in the context of probabilistic modelling, as a topic at the frontiers of cognitive robotics. Lastly, we review recent progress in creating human-like intelligence by using novel probabilistic programming languages. The striking consensus that emerges from these studies is that probabilistic descriptions of learning and inference are powerful and effective ways to create human-like artificial intelligent machines and to understand intelligence in the context of how humans interact with their world.},
	pages = {573--590},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Friston, Karl and Moran, Rosalyn J. and Nagai, Yukie and Taniguchi, Tadahiro and Gomi, Hiroaki and Tenenbaum, Josh},
	urldate = {2021-11-01},
	date = {2021-12-01},
	langid = {english},
	keywords = {Bayesian inference, Cognitive development, Free energy principle, Generative model, Predictive coding, Probabilistic inference},
	file = {Friston et al. - 2021 - World model learning and inference.pdf:/Users/bert/Zotero/storage/VV9FFDEN/Friston et al. - 2021 - World model learning and inference.pdf:application/pdf},
}

@inproceedings{wadehn_state_2019,
	location = {A Coruna, Spain},
	title = {State Space Models with Dynamical and Sparse Variances, and Inference by {EM} Message Passing},
	isbn = {978-90-827970-3-9},
	url = {https://ieeexplore.ieee.org/document/8902815/},
	doi = {10.23919/EUSIPCO.2019.8902815},
	abstract = {Sparse Bayesian learning ({SBL}) is a probabilistic approach to estimation problems based on representing sparsitypromoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models ({SSMs}). However, in classical {SBL} the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewiseconstant (in particular block-sparse) variances, we propose {SSMs} with dynamical and sparse variances ({SSM}-{DSV}). These are twolayer {SSMs}, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization ({EM}) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a {SSM}-{DSV}. Second, we used {SSM}-{DSV} to model the oculomotor system and employed {EM}-message passing for estimating neural controller signals from eye position data.},
	eventtitle = {2019 27th European Signal Processing Conference ({EUSIPCO})},
	pages = {1--5},
	booktitle = {2019 27th European Signal Processing Conference ({EUSIPCO})},
	publisher = {{IEEE}},
	author = {Wadehn, Federico and Weber, Thilo and Loeliger, Hans-Andrea},
	urldate = {2021-10-08},
	date = {2019-09},
	langid = {english},
	file = {Wadehn et al. - 2019 - State Space Models with Dynamical and Sparse Varia.pdf:/Users/bert/Zotero/storage/DX96LNT6/Wadehn et al. - 2019 - State Space Models with Dynamical and Sparse Varia.pdf:application/pdf},
}

@article{sajid_investigating_2021,
	title = {Investigating functional recovery mechanisms after brain damage},
	pages = {43},
	journaltitle = {{PhD} update report},
	author = {Sajid, Noor},
	date = {2021},
	langid = {english},
	file = {Sajid - 2021 - Investigating functional recovery mechanisms after.pdf:/Users/bert/Zotero/storage/PIC86QIA/Sajid - 2021 - Investigating functional recovery mechanisms after.pdf:application/pdf},
}

@article{houde_speech_2011,
	title = {Speech Production as State Feedback Control},
	volume = {5},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2011.00082},
	doi = {10.3389/fnhum.2011.00082},
	abstract = {Spoken language exists because of a remarkable neural process. Inside a speaker's brain, an intended message gives rise to neural signals activating the muscles of the vocal tract. The process is remarkable because these muscles are activated in just the right way that the vocal tract produces sounds a listener understands as the intended message. What is the best approach to understanding the neural substrate of this crucial motor control process? One of the key recent modeling developments in neuroscience has been the use of state feedback control ({SFC}) theory to explain the role of the {CNS} in motor control. {SFC} postulates that the {CNS} controls motor output by (1) estimating the current dynamic state of the thing (e.g., arm) being controlled, and (2) generating controls based on this estimated state. {SFC} has successfully predicted a great range of non-speech motor phenomena, but as yet has not received attention in the speech motor control community. Here, we review some of the key characteristics of speech motor control and what they say about the role of the {CNS} in the process. We then discuss prior efforts to model the role of {CNS} in speech motor control, and argue that these models have inherent limitations – limitations that are overcome by an {SFC} model of speech motor control which we describe. We conclude by discussing a plausible neural substrate of our model.},
	pages = {82},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {Houde, John and Nagarajan, Srikantan},
	urldate = {2021-09-26},
	date = {2011},
	file = {Houde and Nagarajan - 2011 - Speech Production as State Feedback Control.pdf:/Users/bert/Zotero/storage/MR6EDBHZ/Houde and Nagarajan - 2011 - Speech Production as State Feedback Control.pdf:application/pdf},
}

@article{kotiang_boolean_2021,
	title = {Boolean factor graph model for biological systems: the yeast cell-cycle network},
	volume = {22},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-021-04361-8},
	doi = {10.1186/s12859-021-04361-8},
	shorttitle = {Boolean factor graph model for biological systems},
	abstract = {The desire to understand genomic functions and the behavior of complex gene regulatory networks has recently been a major research focus in systems biology. As a result, a plethora of computational and modeling tools have been proposed to identify and infer interactions among biological entities. Here, we consider the general question of the effect of perturbation on the global dynamical network behavior as well as error propagation in biological networks to incite research pertaining to intervention strategies.},
	pages = {442},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	shortjournal = {{BMC} Bioinformatics},
	author = {Kotiang, Stephen and Eslami, Ali},
	urldate = {2021-09-21},
	date = {2021-09-17},
	keywords = {Boolean networks, Factor graph, Network perturbation, Systems biology},
	file = {Kotiang and Eslami - 2021 - Boolean factor graph model for biological systems.pdf:/Users/bert/Zotero/storage/WLQVND4V/Kotiang and Eslami - 2021 - Boolean factor graph model for biological systems.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UMLAX5QJ/s12859-021-04361-8.html:text/html},
}

@article{keusch_binarizing_2021,
	title = {A Binarizing {NUV} Prior and its Use for M-Level Control and Digital-to-Analog Conversion},
	url = {http://arxiv.org/abs/2105.02599},
	abstract = {Priors with a {NUV} representation (normal with unknown variance) have mostly been used for sparsity. In this paper, a novel {NUV} prior is proposed that effectively binarizes. While such a prior may have many uses, in this paper, we explore its use for discrete-level control (with M \${\textbackslash}geq\$ 2 levels) including, in particular, a practical scheme for digital-to-analog conversion. The resulting computations, for each planning period, amount to iterating forward-backward Gaussian message passing recursions (similar to Kalman smoothing), with a complexity (per iteration) that is linear in the planning horizon. In consequence, the proposed method is not limited to a short planning horizon and can therefore outperform "optimal" methods. A preference for sparse level switches can easily be incorporated.},
	journaltitle = {{arXiv}:2105.02599 [eess]},
	author = {Keusch, Raphael and Loeliger, Hans-Andrea},
	urldate = {2021-09-09},
	date = {2021-05-06},
	eprinttype = {arxiv},
	eprint = {2105.02599},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4AUSWRYY/2105.html:text/html;Keusch and Loeliger - 2021 - A Binarizing NUV Prior and its Use for M-Level Con.pdf:/Users/bert/Zotero/storage/DPYW2MLY/Keusch and Loeliger - 2021 - A Binarizing NUV Prior and its Use for M-Level Con.pdf:application/pdf},
}

@article{keusch_half-space_2021,
	title = {Half-Space and Box Constraints as {NUV} Priors: First Results},
	url = {http://arxiv.org/abs/2109.00036},
	shorttitle = {Half-Space and Box Constraints as {NUV} Priors},
	abstract = {Normals with unknown variance ({NUV}) can represent many useful priors and blend well with Gaussian models and message passing algorithms. {NUV} representations of sparsifying priors have long been known, and {NUV} representations of binary (and M-level) priors have been proposed very recently. In this document, we propose {NUV} representations of half-space constraints and box constraints, which allows to add such constraints to any linear Gaussian model with any of the previously known {NUV} priors without affecting the computational tractability.},
	journaltitle = {{arXiv}:2109.00036 [cs, eess, stat]},
	author = {Keusch, Raphael and Loeliger, Hans-Andrea},
	urldate = {2021-09-09},
	date = {2021-08-31},
	eprinttype = {arxiv},
	eprint = {2109.00036},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/AYU27NQG/2109.html:text/html;Keusch and Loeliger - 2021 - Half-Space and Box Constraints as NUV Priors Firs.pdf:/Users/bert/Zotero/storage/4TLWN2N9/Keusch and Loeliger - 2021 - Half-Space and Box Constraints as NUV Priors Firs.pdf:application/pdf},
}

@article{millidge_mathematical_2021,
	title = {A Mathematical Walkthrough and Discussion of the Free Energy Principle},
	url = {http://arxiv.org/abs/2108.13343},
	abstract = {The Free-Energy-Principle ({FEP}) is an influential and controversial theory which postulates a deep and powerful connection between the stochastic thermodynamics of self-organization and learning through variational inference. Specifically, it claims that any self-organizing system which can be statistically separated from its environment, and which maintains itself at a non-equilibrium steady state, can be construed as minimizing an information-theoretic functional -- the variational free energy -- and thus performing variational Bayesian inference to infer the hidden state of its environment. This principle has also been applied extensively in neuroscience, and is beginning to make inroads in machine learning by spurring the construction of novel and powerful algorithms by which action, perception, and learning can all be unified under a single objective. While its expansive and often grandiose claims have spurred significant debates in both philosophy and theoretical neuroscience, the mathematical depth and lack of accessible introductions and tutorials for the core claims of the theory have often precluded a deep understanding within the literature. Here, we aim to provide a mathematically detailed, yet intuitive walk-through of the formulation and central claims of the {FEP} while also providing a discussion of the assumptions necessary and potential limitations of the theory. Additionally, since the {FEP} is a still a living theory, subject to internal controversy, change, and revision, we also present a detailed appendix highlighting and condensing current perspectives as well as controversies about the nature, applicability, and the mathematical assumptions and formalisms underlying the {FEP}.},
	journaltitle = {{arXiv}:2108.13343 [cs]},
	author = {Millidge, Beren and Seth, Anil and Buckley, Christopher L.},
	urldate = {2021-09-05},
	date = {2021-08-30},
	eprinttype = {arxiv},
	eprint = {2108.13343},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/I694G5F3/Millidge et al. - 2021 - A Mathematical Walkthrough and Discussion of the F.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/JZAXM6D8/2108.html:text/html},
}

@article{kobyzev_normalizing_2020,
	title = {Normalizing Flows: An Introduction and Review of Current Methods},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	shorttitle = {Normalizing Flows},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	shortjournal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	urldate = {2021-08-20},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {1908.09257},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/IZ97ZDI4/1908.html:text/html;Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:/Users/bert/Zotero/storage/5339F7E8/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf},
}

@article{zhao_probabilistic_2020,
	title = {Probabilistic inference of Bayesian neural networks with generalized expectation propagation},
	volume = {412},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220310481},
	doi = {10.1016/j.neucom.2020.06.060},
	abstract = {Deep learning plays an important role in the field of machine learning. However, deterministic methods such as neural networks cannot capture the model uncertainty. Bayesian neural network ({BNN}) are recently under consideration since Bayesian models provide a theoretical framework to infer model uncertainty. Since it is often difficult to find an analytical solution for {BNNs}, an effective and efficient approximate inference method is very important for model training and prediction. The generalized version of expectation propagation ({GEP}) was recently proposed and considered a powerful approximate inference method, which is based on the minimization of Kullback–Leibler ({KL}) divergence of the true posterior and the approximate distributions. In this paper, we further instantiate the {GEP} to provide an effective and efficient approximate inference method for {BNNs}. We assess this method on {BNNs} including fully connected neural networks and convolutional neural networks on multiple benchmark datasets and show a better performance than some state-of-the-art approximate inference methods.},
	pages = {392--398},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Zhao, Jing and Liu, Xiao and He, Shaojie and Sun, Shiliang},
	urldate = {2021-08-11},
	date = {2020-10-28},
	langid = {english},
	keywords = {Approximate inference, Bayesian neural networks, Generalized expectation propagation},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/MWP7ERIN/S0925231220310481.html:text/html},
}

@article{lee_natures_2007,
	title = {Nature's guide for mentors},
	volume = {447},
	rights = {2007 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/447791a},
	doi = {10.1038/447791a},
	abstract = {Having a good mentor early in your career can mean the difference between success and failure in any field. Adrian Lee, Carina Dennis and Philip Campbell look at what makes a good mentor.},
	pages = {791--797},
	number = {7146},
	journaltitle = {Nature},
	author = {Lee, Adrian and Dennis, Carina and Campbell, Philip},
	urldate = {2021-08-09},
	date = {2007-06},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: Nature Research Journals
Number: 7146
Primary\_atype: Special Features
Publisher: Nature Publishing Group},
	file = {Lee et al. - 2007 - Nature's guide for mentors.pdf:/Users/bert/Zotero/storage/Q8378DSR/Lee et al. - 2007 - Nature's guide for mentors.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CZALGKCX/447791a.html:text/html},
}

@article{capucci_towards_2021,
	title = {Towards foundations of categorical cybernetics},
	url = {http://arxiv.org/abs/2105.06332},
	abstract = {We propose a categorical framework for processes which interact bidirectionally with both an environment and a 'controller'. Examples include open learners, in which the controller is an optimiser such as gradient descent, and an approach to compositional game theory closely related to open games, in which the controller is a composite of game-theoretic agents. We believe that 'cybernetic' is an appropriate name for the processes that can be described in this framework.},
	journaltitle = {{arXiv}:2105.06332 [math]},
	author = {Capucci, Matteo and Gavranović, Bruno and Hedges, Jules and Rischel, Eigil Fjeldgren},
	urldate = {2021-08-06},
	date = {2021-05-13},
	eprinttype = {arxiv},
	eprint = {2105.06332},
	keywords = {Mathematics - Category Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ULSH9RE8/2105.html:text/html;Capucci et al. - 2021 - Towards foundations of categorical cybernetics.pdf:/Users/bert/Zotero/storage/PEPUJDQ3/Capucci et al. - 2021 - Towards foundations of categorical cybernetics.pdf:application/pdf},
}

@article{van_der_himst_deep_2020,
	title = {Deep Active Inference for Partially Observable {MDPs}},
	volume = {1326},
	url = {http://arxiv.org/abs/2009.03622},
	doi = {10.1007/978-3-030-64919-7_8},
	abstract = {Deep active inference has been proposed as a scalable approach to perception and action that deals with large policy and state spaces. However, current models are limited to fully observable domains. In this paper, we describe a deep active inference model that can learn successful policies directly from high-dimensional sensory inputs. The deep learning architecture optimizes a variant of the expected free energy and encodes the continuous state representation by means of a variational autoencoder. We show, in the {OpenAI} benchmark, that our approach has comparable or better performance than deep Q-learning, a state-of-the-art deep reinforcement learning algorithm.},
	pages = {61--71},
	journaltitle = {{arXiv}:2009.03622 [cs, stat]},
	author = {van der Himst, Otto and Lanillos, Pablo},
	urldate = {2021-08-03},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2009.03622},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/N7DJUNP9/van der Himst and Lanillos - 2020 - Deep Active Inference for Partially Observable MDP.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/2I75F8S2/2009.html:text/html},
}

@article{giese_hierarchical_2021,
	title = {Hierarchical Deep Gaussian Processes Latent Variable Model via Expectation Propagation},
	url = {https://www.academia.edu/49546827/Hierarchical_Deep_Gaussian_Processes_Latent_Variable_Model_via_Expectation_Propagation},
	abstract = {Gaussian Processes ({GPs}) and related unsupervised learning techniques such as Gaussian Process Latent Variable Models ({GP}-{LVMs}) have been very successful in the accurate modeling of high-dimensional data based on limited amounts of training data.},
	journaltitle = {Artificial Neural Networks and Machine Learning – {ICANN} 2021 30th International Conference on Artificial Neural Networks},
	author = {Giese, Martin and Taubert, Nick},
	urldate = {2021-08-02},
	date = {2021},
	file = {Giese and Taubert - 2021 - Hierarchical Deep Gaussian Processes Latent Variab.pdf:/Users/bert/Zotero/storage/V64R8VK5/Giese and Taubert - 2021 - Hierarchical Deep Gaussian Processes Latent Variab.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YBKSFYMB/Hierarchical_Deep_Gaussian_Processes_Latent_Variable_Model_via_Expectation_Propagation.html:text/html},
}

@inproceedings{loeliger_sparsity_2016,
	location = {La Jolla, {CA}, {USA}},
	title = {On sparsity by {NUV}-{EM}, Gaussian message passing, and Kalman smoothing},
	isbn = {978-1-5090-2529-9},
	url = {http://ieeexplore.ieee.org/document/7888168/},
	doi = {10.1109/ITA.2016.7888168},
	abstract = {Normal priors with unknown variance ({NUV}) have long been known to promote sparsity and to blend well with parameter learning by expectation maximization ({EM}). In this paper, we advocate this approach for linear state space models for applications such as the estimation of impulsive signals, the detection of localized events, smoothing with occasional jumps in the state space, and the detection and removal of outliers.},
	eventtitle = {2016 Information Theory and Applications ({ITA})},
	pages = {1--10},
	booktitle = {2016 Information Theory and Applications Workshop ({ITA})},
	publisher = {{IEEE}},
	author = {Loeliger, Hans-Andrea and Bruderer, Lukas and Malmberg, Hampus and Wadehn, Federico and Zalmai, Nour},
	urldate = {2021-07-21},
	date = {2016-01},
	langid = {english},
	file = {Loeliger et al. - 2016 - On sparsity by NUV-EM, Gaussian message passing, a.pdf:/Users/bert/Zotero/storage/CPLS9D4P/Loeliger et al. - 2016 - On sparsity by NUV-EM, Gaussian message passing, a.pdf:application/pdf},
}

@article{parr_generalised_2019,
	title = {Generalised free energy and active inference},
	volume = {113},
	issn = {1432-0770},
	doi = {10.1007/s00422-019-00805-w},
	abstract = {Active inference is an approach to understanding behaviour that rests upon the idea that the brain uses an internal generative model to predict incoming sensory data. The fit between this model and data may be improved in two ways. The brain could optimise probabilistic beliefs about the variables in the generative model (i.e. perceptual inference). Alternatively, by acting on the world, it could change the sensory data, such that they are more consistent with the model. This implies a common objective function (variational free energy) for action and perception that scores the fit between an internal model and the world. We compare two free energy functionals for active inference in the framework of Markov decision processes. One of these is a functional of beliefs (i.e. probability distributions) about states and policies, but a function of observations, while the second is a functional of beliefs about all three. In the former (expected free energy), prior beliefs about outcomes are not part of the generative model (because they are absorbed into the prior over policies). Conversely, in the second (generalised free energy), priors over outcomes become an explicit component of the generative model. When using the free energy function, which is blind to future observations, we equip the generative model with a prior over policies that ensure preferred (i.e. priors over) outcomes are realised. In other words, if we expect to encounter a particular kind of outcome, this lends plausibility to those policies for which this outcome is a consequence. In addition, this formulation ensures that selected policies minimise uncertainty about future outcomes by minimising the free energy expected in the future. When using the free energy functional-that effectively treats future observations as hidden states-we show that policies are inferred or selected that realise prior preferences by minimising the free energy of future expectations. Interestingly, the form of posterior beliefs about policies (and associated belief updating) turns out to be identical under both formulations, but the quantities used to compute them are not.},
	pages = {495--513},
	number = {5},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Parr, Thomas and Friston, Karl J.},
	date = {2019-12},
	pmid = {31562544},
	pmcid = {PMC6848054},
	keywords = {Active inference, Animals, Bayesian, Behavior, Brain, Data selection, Epistemic value, Free energy, Humans, Intrinsic motivation, Markov Chains, Models, Neurological},
	file = {Parr and Friston - 2019 - Generalised free energy and active inference.pdf:/Users/bert/Zotero/storage/K8CIU6QU/Parr and Friston - 2019 - Generalised free energy and active inference.pdf:application/pdf},
}

@article{sajid_bayesian_2021,
	title = {Bayesian brains and the R{\textbackslash}'enyi divergence},
	url = {http://arxiv.org/abs/2107.05438},
	abstract = {Under the Bayesian brain hypothesis, behavioural variations can be attributed to different priors over generative model parameters. This provides a formal explanation for why individuals exhibit inconsistent behavioural preferences when confronted with similar choices. For example, greedy preferences are a consequence of confident (or precise) beliefs over certain outcomes. Here, we offer an alternative account of behavioural variability using R{\textbackslash}'enyi divergences and their associated variational bounds. R{\textbackslash}'enyi bounds are analogous to the variational free energy (or evidence lower bound) and can be derived under the same assumptions. Importantly, these bounds provide a formal way to establish behavioural differences through an \${\textbackslash}alpha\$ parameter, given fixed priors. This rests on changes in \${\textbackslash}alpha\$ that alter the bound (on a continuous scale), inducing different posterior estimates and consequent variations in behaviour. Thus, it looks as if individuals have different priors, and have reached different conclusions. More specifically, \${\textbackslash}alpha {\textbackslash}to 0{\textasciicircum}\{+\}\$ optimisation leads to mass-covering variational estimates and increased variability in choice behaviour. Furthermore, \${\textbackslash}alpha {\textbackslash}to + {\textbackslash}infty\$ optimisation leads to mass-seeking variational posteriors and greedy preferences. We exemplify this formulation through simulations of the multi-armed bandit task. We note that these \${\textbackslash}alpha\$ parameterisations may be especially relevant, i.e., shape preferences, when the true posterior is not in the same family of distributions as the assumed (simpler) approximate density, which may be the case in many real-world scenarios. The ensuing departure from vanilla variational inference provides a potentially useful explanation for differences in behavioural preferences of biological (or artificial) agents under the assumption that the brain performs variational Bayesian inference.},
	journaltitle = {{arXiv}:2107.05438 [cs, q-bio]},
	author = {Sajid, Noor and Faccio, Francesco and Da Costa, Lancelot and Parr, Thomas and Schmidhuber, Jürgen and Friston, Karl},
	urldate = {2021-07-19},
	date = {2021-07-12},
	eprinttype = {arxiv},
	eprint = {2107.05438},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NCUP6R9M/2107.html:text/html;Sajid et al. - 2021 - Bayesian brains and the R'enyi divergence.pdf:/Users/bert/Zotero/storage/XFRSC83Q/Sajid et al. - 2021 - Bayesian brains and the R'enyi divergence.pdf:application/pdf},
}

@article{caticha_entropy_2021,
	title = {Entropy, Information, and the Updating of Probabilities},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/7/895},
	doi = {10.3390/e23070895},
	abstract = {This paper is a review of a particular approach to the method of maximum entropy as a general framework for inference. The discussion emphasizes pragmatic elements in the derivation. An epistemic notion of information is defined in terms of its relation to the Bayesian beliefs of ideally rational agents. The method of updating from a prior to posterior probability distribution is designed through an eliminative induction process. The logarithmic relative entropy is singled out as a unique tool for updating (a) that is of universal applicability, (b) that recognizes the value of prior information, and (c) that recognizes the privileged role played by the notion of independence in science. The resulting framework—the {ME} method—can handle arbitrary priors and arbitrary constraints. It includes the {MaxEnt} and Bayes’ rules as special cases and, therefore, unifies entropic and Bayesian methods into a single general inference scheme. The {ME} method goes beyond the mere selection of a single posterior, and also addresses the question of how much less probable other distributions might be, which provides a direct bridge to the theories of fluctuations and large deviations.},
	pages = {895},
	number = {7},
	journaltitle = {Entropy},
	author = {Caticha, Ariel},
	urldate = {2021-07-17},
	date = {2021-07},
	langid = {english},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian inference, maximum entropy, updating probabilities},
	file = {Caticha - 2021 - Entropy, Information, and the Updating of Probabil.pdf:/Users/bert/Zotero/storage/ACPF26CE/Caticha - 2021 - Entropy, Information, and the Updating of Probabil.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/SP3ENP6J/htm.html:text/html},
}

@article{millidge_applications_2021,
	title = {Applications of the Free Energy Principle to Machine Learning and Neuroscience},
	url = {http://arxiv.org/abs/2107.00140},
	abstract = {In this {PhD} thesis, we explore and apply methods inspired by the free energy principle to two important areas in machine learning and neuroscience. The free energy principle is a general mathematical theory of the necessary information-theoretic behaviours of systems that maintain a separation from their environment. A core postulate of the theory is that complex systems can be seen as performing variational Bayesian inference and minimizing an information-theoretic quantity called the variational free energy. The thesis is structured into three independent sections. Firstly, we focus on predictive coding, a neurobiologically plausible process theory derived from the free energy principle which argues that the primary function of the brain is to minimize prediction errors, showing how predictive coding can be scaled up and extended to be more biologically plausible, and elucidating its close links with other methods such as Kalman Filtering. Secondly, we study active inference, a neurobiologically grounded account of action through variational message passing, and investigate how these methods can be scaled up to match the performance of deep reinforcement learning methods. We additionally provide a detailed mathematical understanding of the nature and origin of the information-theoretic objectives that underlie exploratory behaviour. Finally, we investigate biologically plausible methods of credit assignment in the brain. We first demonstrate a close link between predictive coding and the backpropagation of error algorithm. We go on to propose novel and simpler algorithms which allow for backprop to be implemented in purely local, biologically plausible computations.},
	journaltitle = {{arXiv}:2107.00140 [cs]},
	author = {Millidge, Beren},
	urldate = {2021-07-09},
	date = {2021-06-30},
	eprinttype = {arxiv},
	eprint = {2107.00140},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7Q7XALNX/2107.html:text/html;Millidge - 2021 - Applications of the Free Energy Principle to Machi.pdf:/Users/bert/Zotero/storage/KH54D9ZM/Millidge - 2021 - Applications of the Free Energy Principle to Machi.pdf:application/pdf},
}

@article{ortiz_visual_2021,
	title = {A visual introduction to Gaussian Belief Propagation},
	url = {http://arxiv.org/abs/2107.02308},
	abstract = {In this article, we present a visual introduction to Gaussian Belief Propagation ({GBP}), an approximate probabilistic inference algorithm that operates by passing messages between the nodes of arbitrarily structured factor graphs. A special case of loopy belief propagation, {GBP} updates rely only on local information and will converge independently of the message schedule. Our key argument is that, given recent trends in computing hardware, {GBP} has the right computational properties to act as a scalable distributed probabilistic inference framework for future machine learning systems.},
	journaltitle = {{arXiv}:2107.02308 [cs]},
	author = {Ortiz, Joseph and Evans, Talfan and Davison, Andrew J.},
	urldate = {2021-07-07},
	date = {2021-07-05},
	eprinttype = {arxiv},
	eprint = {2107.02308},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GNL8CSZP/2107.html:text/html;Ortiz et al. - 2021 - A visual introduction to Gaussian Belief Propagati.pdf:/Users/bert/Zotero/storage/MI77YUXI/Ortiz et al. - 2021 - A visual introduction to Gaussian Belief Propagati.pdf:application/pdf},
}

@article{khan_learning-algorithms_2020,
	title = {Learning-Algorithms from Bayesian Principles},
	pages = {23},
	author = {Khan, Mohammad Emtiyaz},
	date = {2020},
	langid = {english},
	file = {Khan - Learning-Algorithms from Bayesian Principles.pdf:/Users/bert/Zotero/storage/CIR3W3QN/Khan - Learning-Algorithms from Bayesian Principles.pdf:application/pdf},
}

@article{lin_handling_nodate,
	title = {Handling the Positive-Deﬁnite Constraint in the Bayesian Learning Rule},
	abstract = {The Bayesian learning rule is a natural-gradient variational inference method, which not only contains many existing learning algorithms as special cases but also enables the design of new algorithms. Unfortunately, when variational parameters lie in an open constraint set, the rule may not satisfy the constraint and requires line-searches which could slow down the algorithm. In this work, we address this issue for positive-deﬁnite constraints by proposing an improved rule that naturally handles the constraints. Our modiﬁcation is obtained by using Riemannian gradient methods, and is valid when the approximation attains a block-coordinate natural parameterization (e.g., Gaussian distributions and their mixtures). Our method outperforms existing methods without any signiﬁcant increase in computation. Our work makes it easier to apply the rule in the presence of positive-deﬁnite constraints in parameter spaces.},
	pages = {11},
	author = {Lin, Wu and Schmidt, Mark and Khan, Mohammad Emtiyaz},
	langid = {english},
	file = {Lin et al. - Handling the Positive-Deﬁnite Constraint in the Ba.pdf:/Users/bert/Zotero/storage/UDGGM3PN/Lin et al. - Handling the Positive-Deﬁnite Constraint in the Ba.pdf:application/pdf},
}

@inproceedings{karseras_tracking_2013,
	location = {Vancouver, {BC}, Canada},
	title = {Tracking dynamic sparse signals using Hierarchical Bayesian Kalman filters},
	isbn = {978-1-4799-0356-6},
	url = {http://ieeexplore.ieee.org/document/6638927/},
	doi = {10.1109/ICASSP.2013.6638927},
	abstract = {In this work we are interested in the problem of reconstructing time-varying signals for which the support is assumed to be sparse. For a single time instance it is possible to reconstruct the original signal efﬁciently by employing a suitable algorithm for sparse signal recovery, given the sparsity level of the signal. In the case of time-varying sparse signals the sparsity level is not necessarily known a-priori. Furthermore conventional tracking by Kalman ﬁltering fails to promote sparsity. Instead, a hierarchical Bayesian model is used in the tracking process which succeeds in modelling sparsity. One theorem is provided that extends previous work by providing some more general results. A second theorem gives the conditions under which all sparse signals are recovered exactly. It is demonstrated that the proposed method succeeds in recovering timevarying sparse signals with greater accuracy than the classic Kalman ﬁlter approach.},
	eventtitle = {{ICASSP} 2013 - 2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {6546--6550},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	publisher = {{IEEE}},
	author = {Karseras, Evripidis and Leung, Kin and Dai, Wei},
	urldate = {2021-07-05},
	date = {2013-05},
	langid = {english},
	file = {Karseras et al. - 2013 - Tracking dynamic sparse signals using Hierarchical.pdf:/Users/bert/Zotero/storage/2I8WTGQ9/Karseras et al. - 2013 - Tracking dynamic sparse signals using Hierarchical.pdf:application/pdf},
}

@article{da_costa_bayesian_2021,
	title = {Bayesian Mechanics for Stationary Processes},
	url = {http://arxiv.org/abs/2106.13830},
	abstract = {This paper develops a Bayesian mechanics for adaptive systems. Firstly, we model the interface between a system and its environment with a Markov blanket. This affords conditions under which states internal to the blanket encode information about external states. Second, we introduce dynamics and represent adaptive systems as Markov blankets at steady-state. This allows us to identify a wide class of systems whose internal states appear to infer external states, consistent with variational inference in Bayesian statistics and theoretical neuroscience. Finally, we partition the blanket into sensory and active states. It follows that active states can be seen as performing active inference and well-known forms of stochastic control (such as {PID} control), which are prominent formulations of adaptive behaviour in theoretical biology and engineering.},
	journaltitle = {{arXiv}:2106.13830 [math-ph, physics:nlin, q-bio]},
	author = {Da Costa, Lancelot and Friston, Karl and Heins, Conor and Pavliotis, Grigorios A.},
	urldate = {2021-07-03},
	date = {2021-06-25},
	eprinttype = {arxiv},
	eprint = {2106.13830},
	keywords = {Mathematics - Optimization and Control, Quantitative Biology - Neurons and Cognition, Mathematical Physics, Nonlinear Sciences - Adaptation and Self-Organizing Systems},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GW5V55HC/2106.html:text/html;Da Costa et al. - 2021 - Bayesian Mechanics for Stationary Processes.pdf:/Users/bert/Zotero/storage/3T2VD46Y/Da Costa et al. - 2021 - Bayesian Mechanics for Stationary Processes.pdf:application/pdf},
}

@article{de_vries_hearing_2020,
	title = {Hearing device and method for tuning hearing device parameters},
	author = {De Vries, Aalbert and Kraak, Joris and Cox, Marcus Gerardus Hermanus},
	date = {2020},
	note = {Publisher: {US} Patent 10,735,877},
}

@article{yu_efficient_2021,
	title = {Efficient Variational Bayesian Structure Learning of Dynamic Graphical Models},
	url = {http://arxiv.org/abs/2009.07703},
	abstract = {Estimating time-varying graphical models are of paramount importance in various social, financial, biological, and engineering systems, since the evolution of such networks can be utilized for example to spot trends, detect anomalies, predict vulnerability, and evaluate the impact of interventions. Existing methods require extensive tuning of parameters that control the graph sparsity and temporal smoothness. Furthermore, these methods are computationally burdensome with time complexity O({NP}{\textasciicircum}3) for P variables and N time points. As a remedy, we propose a low-complexity tuning-free Bayesian approach, named {BADGE}. Specifically, we impose temporally-dependent spike-and-slab priors on the graphs such that they are sparse and varying smoothly across time. A variational inference algorithm is then derived to learn the graph structures from the data automatically. Owning to the pseudo-likelihood and the mean-field approximation, the time complexity of {BADGE} is only O({NP}{\textasciicircum}2). Additionally, by identifying the frequency-domain resemblance to the time-varying graphical models, we show that {BADGE} can be extended to learning frequency-varying inverse spectral density matrices, and yields graphical models for multivariate stationary time series. Numerical results on both synthetic and real data show that that {BADGE} can better recover the underlying true graphs, while being more efficient than the existing methods, especially for high-dimensional cases.},
	journaltitle = {{arXiv}:2009.07703 [cs, stat]},
	author = {Yu, Hang and Wu, Songwei and Dauwels, Justin},
	urldate = {2021-06-27},
	date = {2021-03-15},
	eprinttype = {arxiv},
	eprint = {2009.07703},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Y7PSBRFQ/2009.html:text/html;Yu et al. - 2021 - Efficient Variational Bayesian Structure Learning .pdf:/Users/bert/Zotero/storage/UHCPSAV8/Yu et al. - 2021 - Efficient Variational Bayesian Structure Learning .pdf:application/pdf},
}

@article{evans_maximum_2021,
	title = {A Maximum Entropy Model of Bounded Rational Decision-Making with Prior Beliefs and Market Feedback},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/6/669},
	doi = {10.3390/e23060669},
	abstract = {Bounded rationality is an important consideration stemming from the fact that agents often have limits on their processing abilities, making the assumption of perfect rationality inapplicable to many real tasks. We propose an information-theoretic approach to the inference of agent decisions under Smithian competition. The model explicitly captures the boundedness of agents (limited in their information-processing capacity) as the cost of information acquisition for expanding their prior beliefs. The expansion is measured as the Kullblack–Leibler divergence between posterior decisions and prior beliefs. When information acquisition is free, the homo economicus agent is recovered, while in cases when information acquisition becomes costly, agents instead revert to their prior beliefs. The maximum entropy principle is used to infer least biased decisions based upon the notion of Smithian competition formalised within the Quantal Response Statistical Equilibrium framework. The incorporation of prior beliefs into such a framework allowed us to systematically explore the effects of prior beliefs on decision-making in the presence of market feedback, as well as importantly adding a temporal interpretation to the framework. We verified the proposed model using Australian housing market data, showing how the incorporation of prior knowledge alters the resulting agent decisions. Specifically, it allowed for the separation of past beliefs and utility maximisation behaviour of the agent as well as the analysis into the evolution of agent beliefs.},
	pages = {669},
	number = {6},
	journaltitle = {Entropy},
	author = {Evans, Benjamin Patrick and Prokopenko, Mikhail},
	urldate = {2021-06-25},
	date = {2021-06},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {bounded rationality, complexity economics, decision-making, information-theory, maximum entropy principle, quantal response statistical equilibrium},
	file = {Evans and Prokopenko - 2021 - A Maximum Entropy Model of Bounded Rational Decisi.pdf:/Users/bert/Zotero/storage/7LI9GMH7/Evans and Prokopenko - 2021 - A Maximum Entropy Model of Bounded Rational Decisi.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/DCJLA7D6/htm.html:text/html},
}

@article{sajid_exploration_2021,
	title = {Exploration and preference satisfaction trade-off in reward-free learning},
	url = {http://arxiv.org/abs/2106.04316},
	abstract = {Biological agents have meaningful interactions with their environment despite the absence of a reward signal. In such instances, the agent can learn preferred modes of behaviour that lead to predictable states -- necessary for survival. In this paper, we pursue the notion that this learnt behaviour can be a consequence of reward-free preference learning that ensures an appropriate trade-off between exploration and preference satisfaction. For this, we introduce a model-based Bayesian agent equipped with a preference learning mechanism (pepper) using conjugate priors. These conjugate priors are used to augment the expected free energy planner for learning preferences over states (or outcomes) across time. Importantly, our approach enables the agent to learn preferences that encourage adaptive behaviour at test time. We illustrate this in the {OpenAI} Gym {FrozenLake} and the 3D mini-world environments -- with and without volatility. Given a constant environment, these agents learn confident (i.e., precise) preferences and act to satisfy them. Conversely, in a volatile setting, perpetual preference uncertainty maintains exploratory behaviour. Our experiments suggest that learnable (reward-free) preferences entail a trade-off between exploration and preference satisfaction. Pepper offers a straightforward framework suitable for designing adaptive agents when reward functions cannot be predefined as in real environments.},
	journaltitle = {{arXiv}:2106.04316 [cs, q-bio]},
	author = {Sajid, Noor and Tigas, Panagiotis and Zakharov, Alexey and Fountas, Zafeirios and Friston, Karl},
	urldate = {2021-06-12},
	date = {2021-06-08},
	eprinttype = {arxiv},
	eprint = {2106.04316},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/VXZNG7ZV/2106.html:text/html;Sajid et al. - 2021 - Exploration and preference satisfaction trade-off .pdf:/Users/bert/Zotero/storage/M2AMSJDS/Sajid et al. - 2021 - Exploration and preference satisfaction trade-off .pdf:application/pdf},
}

@inproceedings{zalmai_unsupervised_2017,
	title = {Unsupervised feature extraction, signal labeling, and blind signal separation in a state space world},
	doi = {10.23919/EUSIPCO.2017.8081325},
	abstract = {The paper addresses the problem of joint signal separation and estimation in a single-channel discrete-time signal composed of a wandering baseline and overlapping repetitions of unknown (or known) signal shapes. All signals are represented by a linear state space model ({LSSM}). The baseline model is driven by white Gaussian noise, but the other signal models are triggered by sparse inputs. Sparsity is achieved by normal priors with unknown variance ({NUV}) from sparse Bayesian learning. All signals and system parameters are jointly estimated with an efficient expectation maximization ({EM}) algorithm based on Gaussian message passing, which works both for known and unknown signal shapes. The proposed method outputs a sparse multi-channel representation of the given signal, which can be interpreted as a signal labeling.},
	eventtitle = {2017 25th European Signal Processing Conference ({EUSIPCO})},
	pages = {838--842},
	booktitle = {2017 25th European Signal Processing Conference ({EUSIPCO})},
	author = {Zalmai, Nour and Keusch, Raphael and Malmberg, Hampus and Loeliger, Hans-Andrea},
	date = {2017-08},
	note = {{ISSN}: 2076-1465},
	keywords = {Dictionaries, Electrocardiography, Estimation, Gaussian noise, Message passing, Shape, Signal processing algorithms},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/ZVNDZK9U/8081325.html:text/html;Zalmai et al. - 2017 - Unsupervised feature extraction, signal labeling, .pdf:/Users/bert/Zotero/storage/E2RIXZ6Y/Zalmai et al. - 2017 - Unsupervised feature extraction, signal labeling, .pdf:application/pdf},
}

@incollection{levchuk_chapter_2019,
	title = {Chapter 4 - Active Inference in Multiagent Systems: Context-Driven Collaboration and Decentralized Purpose-Driven Team Adaptation},
	isbn = {978-0-12-817636-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128176368000041},
	shorttitle = {Chapter 4 - Active Inference in Multiagent Systems},
	abstract = {The Internet of things ({IoT}), from heart monitoring implants to home-heating control systems, is becoming integral to our daily lives. We expect the technologies that comprise {IoT} to become smarter; to autonomously reason, act, and communicate with other entities in the environment; and to achieve shared goals. To realize the full potential of these systems, we must understand the mechanisms that allow multiple intelligent entities to effectively operate, collaborate, and learn in changing and uncertain environments. The future {IoT} devices must not only maintain enough intelligence to perceive and act locally, but also possess team-level collaboration and adaptation processes. We posit that such processes embody energy-minimizing mechanisms found in all biological and physical systems, and operate over the objectives and constraints that can be defined and analyzed locally by individual devices without the need for global centralized control. In this chapter, we represent multiple {IoT} devices as a team of intelligent agents, and postulate that multiagent systems achieve adaptive behaviors by minimizing a team’s free energy, which decomposes into distributed iterative perception (inference) and control (action) processes. First, we discuss instantiation of this mechanism for a joint distributed decision-making problem. Next, we present experimental evidence that energy-based teams outperform utility-based teams. Finally, we discuss different learning processes that support team-level adaptation.},
	pages = {67--85},
	booktitle = {Artificial Intelligence for the Internet of Everything},
	publisher = {Academic Press},
	author = {Levchuk, Georgiy and Pattipati, Krishna and Serfaty, Daniel and Fouse, Adam and {McCormack}, Robert},
	editor = {Lawless, William and Mittu, Ranjeev and Sofge, Donald and Moskowitz, Ira S. and Russell, Stephen},
	urldate = {2021-06-09},
	date = {2019-01-01},
	langid = {english},
	doi = {10.1016/B978-0-12-817636-8.00004-1},
	keywords = {Free energy, Constraints, Humanmachine teams, Internet of Things ({IoT}), Multiple intelligent entities, Organizational structure, Perceptual control, Team adaptation, Variational inference},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/6T67EC82/B9780128176368000041.html:text/html},
}

@article{millidge_understanding_2021,
	title = {Understanding the origin of information-seeking exploration in probabilistic objectives for control},
	url = {http://arxiv.org/abs/2103.06859},
	abstract = {The exploration-exploitation trade-off is central to the description of adaptive behaviour in fields ranging from machine learning, to biology, to economics. While many approaches have been taken, one approach to solving this trade-off has been to equip or propose that agents possess an intrinsic 'exploratory drive' which is often implemented in terms of maximizing the agents information gain about the world -- an approach which has been widely studied in machine learning and cognitive science. In this paper we mathematically investigate the nature and meaning of such approaches and demonstrate that this combination of utility maximizing and information-seeking behaviour arises from the minimization of an entirely difference class of objectives we call divergence objectives. We propose a dichotomy in the objective functions underlying adaptive behaviour between {\textbackslash}emph\{evidence\} objectives, which correspond to well-known reward or utility maximizing objectives in the literature, and {\textbackslash}emph\{divergence\} objectives which instead seek to minimize the divergence between the agent's expected and desired futures, and argue that this new class of divergence objectives could form the mathematical foundation for a much richer understanding of the exploratory components of adaptive and intelligent action, beyond simply greedy utility maximization.},
	journaltitle = {{arXiv}:2103.06859 [cs]},
	author = {Millidge, Beren and Tschantz, Alexander and Seth, Anil and Buckley, Christopher},
	urldate = {2021-06-04},
	date = {2021-03-16},
	eprinttype = {arxiv},
	eprint = {2103.06859},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NBHWMSS6/2103.html:text/html;Millidge et al. - 2021 - Understanding the origin of information-seeking ex.pdf:/Users/bert/Zotero/storage/TMGM7TB4/Millidge et al. - 2021 - Understanding the origin of information-seeking ex.pdf:application/pdf},
}

@report{holmes_active_2021,
	title = {Active inference, selective attention, and the cocktail party problem},
	url = {https://psyarxiv.com/2rzu5/},
	abstract = {In this paper, we introduce a new generative model for an active inference account of preparatory and selective attention, in the context of a classic ‘cocktail party’ paradigm. In this setup, two talkers speak simultaneously and an instructive spatial cue directs attention to the left or right talker. We use this generative model to test competing hypotheses about the way that human listeners direct preparatory and selective attention. We show that assigning low precision to words at attended—relative to unattended—locations can explain why a listener reports words from a competing sentence. Under this model, temporal changes in sensory precision were not needed to account for faster reaction times with longer cue-target intervals, but were necessary to explain ramping effects on event-related potentials—resembling the contingent negative variation ({CNV})—during the preparatory interval. These simulations demonstrate that behavioural and electrophysiological correlates of voluntary attention emerge from neuronally plausible belief updating or message passing and, crucially, distinguish between the effects of deploying precision in different parts of a generative model.},
	institution = {{PsyArXiv}},
	author = {Holmes, Emma and Parr, Thomas and Griffiths, Timothy D. and Friston, Karl},
	urldate = {2021-06-04},
	date = {2021-04-12},
	doi = {10.31234/osf.io/2rzu5},
	note = {type: article},
	keywords = {Computational Neuroscience, Neuroscience, Attention, Audition, Cognitive Neuroscience, Cognitive Psychology, Perception, Social and Behavioral Sciences},
	file = {Holmes et al. - 2021 - Active inference, selective attention, and the coc.pdf:/Users/bert/Zotero/storage/IZYLYRML/Holmes et al. - 2021 - Active inference, selective attention, and the coc.pdf:application/pdf},
}

@article{kirkley_belief_2021,
	title = {Belief propagation for networks with loops},
	volume = {7},
	rights = {Copyright © 2021 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. Distributed under a Creative Commons Attribution {NonCommercial} License 4.0 ({CC} {BY}-{NC}).. https://creativecommons.org/licenses/by-nc/4.0/This is an open-access article distributed under the terms of the Creative Commons Attribution-{NonCommercial} license, which permits use, distribution, and reproduction in any medium, so long as the resultant use is not for commercial advantage and provided the original work is properly cited.},
	issn = {2375-2548},
	url = {https://advances.sciencemag.org/content/7/17/eabf1211},
	doi = {10.1126/sciadv.abf1211},
	abstract = {Belief propagation is a widely used message passing method for the solution of probabilistic models on networks such as epidemic models, spin models, and Bayesian graphical models, but it suffers from the serious shortcoming that it works poorly in the common case of networks that contain short loops. Here, we provide a solution to this long-standing problem, deriving a belief propagation method that allows for fast calculation of probability distributions in systems with short loops, potentially with high density, as well as giving expressions for the entropy and partition function, which are notoriously difficult quantities to compute. Using the Ising model as an example, we show that our approach gives excellent results on both real and synthetic networks, improving substantially on standard message passing methods. We also discuss potential applications of our method to a variety of other problems.
A novel belief propagation algorithm is derived for the solution of probabilistic models on networks containing short loops.
A novel belief propagation algorithm is derived for the solution of probabilistic models on networks containing short loops.},
	pages = {eabf1211},
	number = {17},
	journaltitle = {Science Advances},
	author = {Kirkley, Alec and Cantwell, George T. and Newman, M. E. J.},
	urldate = {2021-05-28},
	date = {2021-04-01},
	langid = {english},
	pmid = {33893102},
	note = {Publisher: American Association for the Advancement of Science
Section: Research Article},
	file = {Kirkley et al. - 2021 - Belief propagation for networks with loops.pdf:/Users/bert/Zotero/storage/AVLCZ28G/Kirkley et al. - 2021 - Belief propagation for networks with loops.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/78UYLDI4/eabf1211.html:text/html},
}

@thesis{weber_perception_2020,
	title = {Perception as Hierarchical Bayesian Inference - Toward non-invasive readouts of exteroceptive and interoceptive processing},
	rights = {http://rightsstatements.org/page/{InC}-{NC}/1.0/},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/476505},
	abstract = {Psychiatry is stuck with symptom-based diagnostic schemes, which display poor predictive validity. Computational psychiatry aims at providing a more mechanistic description of mental disorders by using computational models to understand how (pathologies in) neural circuits produce (maladaptive) behavior. This thesis considers how hierarchical Bayesian models of perception can contribute to this endeavor by bridging the gap between computation and physiology in paradigms of exteroceptive and interoceptive processing. In the first two chapters, we examine the auditory mismatch negativity ({MMN}), an electrophysiological response to unexpected changes in the auditory domain, using electroencephalography ({EEG}), pharmacology, and predictions from a hierarchical Bayesian filtering model. We find that amplitudes of mismatch related {EEG} responses reflect precision-weighted prediction errors on two hierarchical levels of our model, pertaining to beliefs about statistical regularities, and their volatility, respectively. Our pharmacological results indicate that {NMDA} receptor function is crucial for volatility processing, and that mismatch responses in our paradigm are differentially sensitive to cholinergic (muscarinic) versus dopaminergic receptor status. These findings suggest that auditory mismatch paradigms might be capable of distinguishing among different disturbances in the neuromodulation of {NMDA} receptors, which have been suggested as pathophysiological pathways underlying symptoms of schizophrenia. Following this, we approach the mapping from computational quantities of our hierarchical Bayesian filtering model to (readouts of) neural activity more formally. Conceptualizing the model as a network of interconnected nodes, we derive predictions about the neural circuitry necessary to implement the message passing in this network, and about the time course of evoked responses that would ensue. In the last part, we acknowledge that mental and physical health are interdependent, emphasizing the need for formal accounts of body-brain interactions. We outline how the application of hierarchical Bayesian models to such interactions can provide a common taxonomy for psychiatry and psychosomatics. This taxonomy motivates the search for non-invasive readouts of the brain’s monitoring and regulation of bodily variables. As an example for such monitoring, we examine the heartbeat evoked potential ({HEP}), an {EEG} response to single heartbeats. We present evidence for increased {HEP} amplitudes during attentional focus to the cardiac domain, rendering the {HEP} a readout of the adaptive context-dependent up and down-regulation of interoceptive processing. Together, our results showcase how a hierarchical Bayesian approach to perception can guide the search for non-invasive readouts of interoceptive and exteroceptive processing. These readouts have the potential to stratify patients suffering from psychiatric and psychosomatic symptoms based on a mechanistic understanding of the underlying pathology that spans the computational and the physiological level.},
	institution = {{ETH} Zurich},
	type = {Doctoral Thesis},
	author = {Weber, Lilian A. E.},
	urldate = {2021-05-20},
	date = {2020},
	langid = {english},
	doi = {10.3929/ethz-b-000476505},
	note = {Accepted: 2021-03-26T10:08:47Z},
	file = {Snapshot:/Users/bert/Zotero/storage/4L7UR9K9/476505.html:text/html;Weber - 2020 - Perception as Hierarchical Bayesian Inference - To.pdf:/Users/bert/Zotero/storage/N833AHLM/Weber - 2020 - Perception as Hierarchical Bayesian Inference - To.pdf:application/pdf},
}

@article{vitetta_multiple_2020,
	title = {Multiple Bayesian Filtering as Message Passing},
	volume = {68},
	issn = {1941-0476},
	doi = {10.1109/TSP.2020.2965296},
	abstract = {In this manuscript, a general method for deriving filtering algorithms that involve a network of interconnected Bayesian filters is proposed. This method is based on the idea that the processing accomplished inside each of the Bayesian filters and the interactions between them can be represented as message passing algorithms over a proper graphical model. The usefulness of our method is exemplified by developing new filtering techniques, based on the interconnection of a particle filter and an extended Kalman filter, for conditionally linear Gaussian systems. Numerical results for two specific dynamic systems evidence that the devised algorithms can achieve a better complexity-accuracy tradeoff than marginalized particle filtering and multiple particle filtering.},
	pages = {1002--1020},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	author = {Vitetta, Giorgio M. and Viesti, Pasquale Di and Sirignano, Emilio and Montorsi, Francesco},
	date = {2020},
	note = {Conference Name: {IEEE} Transactions on Signal Processing},
	keywords = {Message passing, Signal processing algorithms, Bayes methods, Factor Graph, Graphical models, Hidden Markov Model, Iterative methods, Kalman Filter, Kalman filters, Marginalized Particle Filter, Multiple Particle Filtering, Particle Filter, Sum-Product Algorithm},
	file = {IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/DD46AU3R/Vitetta et al. - 2020 - Multiple Bayesian Filtering as Message Passing.pdf:application/pdf},
}

@article{parr_message_2021,
	title = {Message Passing and Metabolism},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/5/606},
	doi = {10.3390/e23050606},
	abstract = {Active inference is an increasingly prominent paradigm in theoretical biology. It frames the dynamics of living systems as if they were solving an inference problem. This rests upon their flow towards some (non-equilibrium) steady state—or equivalently, their maximisation of the Bayesian model evidence for an implicit probabilistic model. For many models, these self-evidencing dynamics manifest as messages passed among elements of a system. Such messages resemble synaptic communication at a neuronal network level but could also apply to other network structures. This paper attempts to apply the same formulation to biochemical networks. The chemical computation that occurs in regulation of metabolism relies upon sparse interactions between coupled reactions, where enzymes induce conditional dependencies between reactants. We will see that these reactions may be viewed as the movement of probability mass between alternative categorical states. When framed in this way, the master equations describing such systems can be reformulated in terms of their steady-state distribution. This distribution plays the role of a generative model, affording an inferential interpretation of the underlying biochemistry. Finally, we see that—in analogy with computational neurology and psychiatry—metabolic disorders may be characterized as false inference under aberrant prior beliefs.},
	pages = {606},
	number = {5},
	journaltitle = {Entropy},
	author = {Parr, Thomas},
	urldate = {2021-05-17},
	date = {2021-05},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian, master equations, message passing, metabolism, non-equilibrium, stochastic},
	file = {Parr - 2021 - Message Passing and Metabolism.pdf:/Users/bert/Zotero/storage/NN3U6RMU/Parr - 2021 - Message Passing and Metabolism.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2CFNZDGW/htm.html:text/html},
}

@inproceedings{li_variational_2021,
	title = {Variational Parameter Learning in Sequential State-Space Model Via Particle Filtering},
	doi = {10.1109/ICASSP39728.2021.9414033},
	abstract = {Parameter learning of the state-space model ({SSM}) plays a significant role in the modelling of time-series data and dynamical systems. However, the closed-form inference of the parameter posterior is often limited by sequential construction and non-linearity of the {SSMs}, which has led to the development of sampling-based algorithms such as particle Markov chain Monte Carlo ({PMCMC}). We present a novel algorithm, the particle filter variational inference ({PF}-{VI}) algorithm, which achieves closed-form learning of {SSM} parameters while tractably inferring the non-linear sequential states. We apply the algorithm to a popular non-linear {SSM} example and compare its performance against two competing {PMCMC} algorithms.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {5589--5593},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Li, Chenhao and Godsill, Simon J.},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {Bayesian inference, Signal processing algorithms, Filtering, Heuristic algorithms, Inference algorithms, Markov processes, Monte Carlo methods, particle filtering, Signal processing, state-space model, variational Bayes},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/T62CIJVJ/9414033.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/SGXVIBPJ/Li and Godsill - 2021 - Variational Parameter Learning in Sequential State.pdf:application/pdf},
}

@inproceedings{tu_dhasp_2021,
	title = {{DHASP}: Differentiable Hearing Aid Speech Processing},
	doi = {10.1109/ICASSP39728.2021.9414571},
	shorttitle = {{DHASP}},
	abstract = {Hearing aids are expected to improve speech intelligibility for listeners with hearing impairment. An appropriate amplification fitting tuned for the listener’s hearing disability is critical for good performance. The developments of most prescriptive fittings are based on data collected in subjective listening experiments, which are usually expensive and time-consuming. In this paper, we explore an alternative approach to finding the optimal fitting by introducing a hearing aid speech processing framework, in which the fitting is optimised in an automated way using an intelligibility objective function based on the {HASPI} physiological auditory model. The framework is fully differentiable, thus can employ the back-propagation algorithm for efficient, data-driven optimisation. Our initial objective experiments show promising results for noise-free speech amplification, where the automatically optimised processors outperform one of the well recognised hearing aid prescriptions.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {296--300},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Tu, Zehai and Ma, Ning and Barker, Jon},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {Auditory system, differentiable framework, Fitting, Hearing aid speech processing, Hearing aids, intelligibility objective, Measurement, Program processors, Speech recognition, Training},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/BZC3GL6Y/9414571.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/NJQLITUE/Tu et al. - 2021 - DHASP Differentiable Hearing Aid Speech Processin.pdf:application/pdf},
}

@inproceedings{wisdom_whats_2021,
	title = {What’s all the Fuss about Free Universal Sound Separation Data?},
	doi = {10.1109/ICASSP39728.2021.9414774},
	abstract = {We introduce the Free Universal Sound Separation ({FUSS}) dataset, a new corpus for experiments in separating mixtures of an unknown number of sounds from an open domain of sound types. The dataset consists of 23 hours of single-source audio data drawn from 357 classes, which are used to create mixtures of one to four sources. To simulate reverberation, an acoustic room simulator is used to generate impulse responses of box-shaped rooms with frequency-dependent reflective walls. Additional open-source data augmentation tools are also provided to produce new mixtures with different combinations of sources and room simulations. Finally, we introduce an open-source baseline separation model, based on an improved time-domain convolutional network ({TDCN}++), that can separate a variable number of sources in a mixture. This model achieves 9.8 {dB} of scale-invariant signal-to-noise ratio improvement ({SI}-{SNRi}) on mixtures with two to four sources, while reconstructing single-source inputs with 35.8 {dB} absolute {SI}-{SNR}. We hope this dataset will lower the barrier to new research and allow for fast iteration and application of novel techniques from other machine learning domains to the sound separation challenge.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {186--190},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Wisdom, Scott and Erdogan, Hakan and Ellis, Daniel P. W. and Serizel, Romain and Turpault, Nicolas and Fonseca, Eduardo and Salamon, Justin and Seetharaman, Prem and Hershey, John R.},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {Shape, Data models, deep learning, Machine learning, open-source datasets, Reverberation, Task analysis, Time-domain analysis, Tools, Universal sound separation, variable source separation},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/EPQUXZLY/9414774.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/CHBXMHGY/Wisdom et al. - 2021 - What’s all the Fuss about Free Universal Sound Sep.pdf:application/pdf},
}

@inproceedings{martinez_ramirez_differentiable_2021,
	title = {Differentiable Signal Processing With Black-Box Audio Effects},
	doi = {10.1109/ICASSP39728.2021.9415103},
	abstract = {We present a data-driven approach to automate audio signal processing by incorporating stateful third-party, audio effects as layers within a deep neural network. We then train a deep encoder to analyze input audio and control effect parameters to perform the desired signal manipulation, requiring only input-target paired audio data as supervision. To train our network with non-differentiable black-box effects layers, we use a fast, parallel stochastic gradient approximation scheme within a standard auto differentiation graph, yielding efficient end-to-end backpropagation. We demonstrate the power of our approach with three separate automatic audio production applications: tube amplifier emulation, automatic removal of breaths and pops from voice recordings, and automatic music mastering. We validate our results with a subjective listening test, showing our approach not only can enable new automatic audio effects tasks, but can yield results comparable to a specialized, state-of-the-art commercial solution for music mastering.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {66--70},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Martínez Ramírez, Marco A. and Wang, Oliver and Smaragdis, Paris and Bryan, Nicholas J.},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {Signal processing, deep learning, audio effects, black-box optimization, differentiable signal processing, gradient approximation, Neural networks, Production, Robustness, Software, Stochastic processes, Training data},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/VJ4KDPNN/9415103.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/65RN9M99/Martínez Ramírez et al. - 2021 - Differentiable Signal Processing With Black-Box Au.pdf:application/pdf},
}

@inproceedings{subakan_attention_2021,
	title = {Attention Is All You Need In Speech Separation},
	doi = {10.1109/ICASSP39728.2021.9413901},
	abstract = {Recurrent Neural Networks ({RNNs}) have long been the dominant architecture in sequence-to-sequence learning. {RNNs}, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard {RNNs}, replacing recurrent computations with a multi-head attention mechanism.In this paper, we propose the {SepFormer}, a novel {RNN}-free Transformer-based neural network for speech separation. The Sep-Former learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art ({SOTA}) performance on the standard {WSJ}0-2/3mix datasets. It reaches an {SI}-{SNRi} of 22.3 {dB} on {WSJ}0-2mix and an {SI}-{SNRi} of 19.5 {dB} on {WSJ}0-3mix. The {SepFormer} inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {21--25},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {deep learning, Acoustics, attention, Computational modeling, Computer architecture, Conferences, Recurrent neural networks, source separation, Source separation, Speech processing, speech separation, transformer},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/BVQ2SCXU/9413901.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/KBWDQY4A/Subakan et al. - 2021 - Attention Is All You Need In Speech Separation.pdf:application/pdf},
}

@inproceedings{bittar_bayesian_2021,
	title = {A Bayesian Interpretation of the Light Gated Recurrent Unit},
	doi = {10.1109/ICASSP39728.2021.9414259},
	abstract = {We summarise previous work showing that the basic sigmoid activation function arises as an instance of Bayes’s theorem, and that recurrence follows from the prior. We derive a layerwise recurrence without the assumptions of previous work, and show that it leads to a standard recurrence with modest modifications to reflect use of log-probabilities. The resulting architecture closely resembles the Li-{GRU} which is the current state of the art for {ASR}. Although the contribution is mainly theoretical, we show that it is able to outperform the state of the art on the {TIMIT} and {AMI} datasets.},
	eventtitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {2965--2969},
	booktitle = {{ICASSP} 2021 - 2021 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Bittar, Alexandre and Garner, Philip N.},
	date = {2021-06},
	note = {{ISSN}: 2379-190X},
	keywords = {Bayesian inference, Bayes methods, Signal processing, Speech recognition, deep learning, Acoustics, Conferences, Li-{GRU}, Logic gates, Probabilistic logic, recurrent neural networks, speech recognition},
	file = {Bittar and Garner - 2021 - A Bayesian Interpretation of the Light Gated Recur.pdf:/Users/bert/Zotero/storage/UR5NMTZZ/Bittar and Garner - 2021 - A Bayesian Interpretation of the Light Gated Recur.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/8LGGDEA4/9414259.html:text/html},
}

@article{acerbi_variational_2018,
	title = {Variational Bayesian Monte Carlo},
	abstract = {Many probabilistic models of interest in scientific computing and machine learning have expensive, black-box likelihoods that prevent the application of standard techniques for Bayesian inference, such as {MCMC}, which would require access to the gradient or a large number of likelihood evaluations. We introduce here a novel sample-efficient inference framework, Variational Bayesian Monte Carlo ({VBMC}). {VBMC} combines variational inference with Gaussian-process based, active-sampling Bayesian quadrature, using the latter to efficiently approximate the intractable integral in the variational objective. Our method produces both a nonparametric approximation of the posterior distribution and an approximate lower bound of the model evidence, useful for model selection. We demonstrate {VBMC} both on several synthetic likelihoods and on a neuronal model with data from real neurons. Across all tested problems and dimensions (up to \$D = 10\$), {VBMC} performs consistently well in reconstructing the posterior and the model evidence with a limited budget of likelihood evaluations, unlike other methods that work only in very low dimensions. Our framework shows great promise as a novel tool for posterior and model inference with expensive, black-box likelihoods.},
	journaltitle = {{arXiv}:1810.05558 [cs, q-bio, stat]},
	author = {Acerbi, Luigi},
	urldate = {2021-05-12},
	date = {2018-11-29},
	eprinttype = {arxiv},
	eprint = {1810.05558},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, Quantitative Biology - Quantitative Methods},
	file = {Acerbi - 2018 - Variational Bayesian Monte Carlo.pdf:/Users/bert/Zotero/storage/HWVQA23U/Acerbi - 2018 - Variational Bayesian Monte Carlo.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/KGKFXNEZ/1810.html:text/html},
}

@article{wozny_probability_2010,
	title = {Probability Matching as a Computational Strategy Used in Perception},
	volume = {6},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000871},
	doi = {10.1371/journal.pcbi.1000871},
	abstract = {The question of which strategy is employed in human decision making has been studied extensively in the context of cognitive tasks; however, this question has not been investigated systematically in the context of perceptual tasks. The goal of this study was to gain insight into the decision-making strategy used by human observers in a low-level perceptual task. Data from more than 100 individuals who participated in an auditory-visual spatial localization task was evaluated to examine which of three plausible strategies could account for each observer's behavior the best. This task is very suitable for exploring this question because it involves an implicit inference about whether the auditory and visual stimuli were caused by the same object or independent objects, and provides different strategies of how using the inference about causes can lead to distinctly different spatial estimates and response patterns. For example, employing the commonly used cost function of minimizing the mean squared error of spatial estimates would result in a weighted averaging of estimates corresponding to different causal structures. A strategy that would minimize the error in the inferred causal structure would result in the selection of the most likely causal structure and sticking with it in the subsequent inference of location—“model selection.” A third strategy is one that selects a causal structure in proportion to its probability, thus attempting to match the probability of the inferred causal structure. This type of probability matching strategy has been reported to be used by participants predominantly in cognitive tasks. Comparing these three strategies, the behavior of the vast majority of observers in this perceptual task was most consistent with probability matching. While this appears to be a suboptimal strategy and hence a surprising choice for the perceptual system to adopt, we discuss potential advantages of such a strategy for perception.},
	pages = {e1000871},
	number = {8},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Wozny, David R. and Beierholm, Ulrik R. and Shams, Ladan},
	urldate = {2021-05-13},
	date = {2010-08-05},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Attention, Perception, Decision making, Monte Carlo method, Nervous system, Probability distribution, Sensory perception, Vision},
	file = {Full Text PDF:/Users/bert/Zotero/storage/GCKBKNR3/Wozny et al. - 2010 - Probability Matching as a Computational Strategy U.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XN436CM8/article.html:text/html},
}

@article{ghahramani_bayesian_nodate,
	title = {Bayesian Monte Carlo},
	abstract = {We investigate Bayesian alternatives to classical Monte Carlo methods for evaluating integrals. Bayesian Monte Carlo ({BMC}) allows the incorporation of prior knowledge, such as smoothness of the integrand, into the estimation. In a simple problem we show that this outperforms any classical importance sampling method. We also attempt more challenging multidimensional integrals involved in computing marginal likelihoods of statistical models (a.k.a. partition functions and model evidences). We ﬁnd that Bayesian Monte Carlo outperformed Annealed Importance Sampling, although for very high dimensional problems or problems with massive multimodality {BMC} may be less adequate. One advantage of the Bayesian approach to Monte Carlo is that samples can be drawn from any distribution. This allows for the possibility of active design of sample points so as to maximise information gain.},
	pages = {8},
	author = {Ghahramani, Zoubin and Rasmussen, Carl E},
	langid = {english},
	file = {Ghahramani and Rasmussen - Bayesian Monte Carlo.pdf:/Users/bert/Zotero/storage/ILE8AZWT/Ghahramani and Rasmussen - Bayesian Monte Carlo.pdf:application/pdf},
}

@article{rajabalinejad_bayesian_2010,
	title = {Bayesian Monte Carlo method},
	volume = {95},
	issn = {0951-8320},
	url = {https://www.sciencedirect.com/science/article/pii/S0951832010000992},
	doi = {10.1016/j.ress.2010.04.014},
	abstract = {To reduce cost of Monte Carlo ({MC}) simulations for time-consuming processes, Bayesian Monte Carlo ({BMC}) is introduced in this paper. The {BMC} method reduces number of realizations in {MC} according to the desired accuracy level. {BMC} also provides a possibility of considering more priors. In other words, different priors can be integrated into one model by using {BMC} to further reduce cost of simulations. This study suggests speeding up the simulation process by considering the logical dependence of neighboring points as prior information. This information is used in the {BMC} method to produce a predictive tool through the simulation process. The general methodology and algorithm of {BMC} method are presented in this paper. The {BMC} method is applied to the simplified break water model as well as the finite element model of 17th Street Canal in New Orleans, and the results are compared with the {MC} and Dynamic Bounds methods.},
	pages = {1050--1060},
	number = {10},
	journaltitle = {Reliability Engineering \& System Safety},
	shortjournal = {Reliability Engineering \& System Safety},
	author = {Rajabalinejad, M.},
	urldate = {2021-05-12},
	date = {2010-10-01},
	langid = {english},
	keywords = {Bayesian Monte Carlo, Dynamic bounds, Reliability analysis, Simulation},
	file = {Rajabalinejad - 2010 - Bayesian Monte Carlo method.pdf:/Users/bert/Zotero/storage/4UMCZYAG/Rajabalinejad - 2010 - Bayesian Monte Carlo method.pdf:application/pdf},
}

@incollection{belousov_control_2021,
	location = {Cham},
	title = {Control as Inference?: Comparing Path Integral and Message Passing Methods for Optimal Control},
	volume = {883},
	isbn = {978-3-030-41187-9 978-3-030-41188-6},
	url = {http://link.springer.com/10.1007/978-3-030-41188-6_16},
	shorttitle = {Control as Inference?},
	abstract = {The use of probabilistic methods for solving stochastic optimal control and reinforcement learning problems is a burgeoning ﬁeld. However, as the methodologies have been motivated from different ﬁelds, there is no unifying view of the various approaches. In this review we examine the two key, and distinct, model-based methods for continuous control: path integrals and linear Gaussian message passing. We show that, while the Bellman equation is at the foundation of each method, the path integral method uses inference to approximate the solution, while the message passing analytically solves an upper bound. Unifying these methods requires a further study of continuous-time likelihood functions and their connection to forward backward stochastic differential equations.},
	pages = {189--206},
	booktitle = {Reinforcement Learning Algorithms: Analysis and Applications},
	publisher = {Springer International Publishing},
	author = {Watson, Joe},
	editor = {Belousov, Boris and Abdulsamad, Hany and Klink, Pascal and Parisi, Simone and Peters, Jan},
	urldate = {2021-05-11},
	date = {2021},
	langid = {english},
	doi = {10.1007/978-3-030-41188-6_16},
	note = {Series Title: Studies in Computational Intelligence},
	file = {Watson - 2021 - Control as Inference Comparing Path Integral and.pdf:/Users/bert/Zotero/storage/UQB9HJH9/Watson - 2021 - Control as Inference Comparing Path Integral and.pdf:application/pdf},
}

@misc{noauthor_promotie_nodate,
	title = {promotie reglement {TUe} 2020},
	file = {promotie reglement TUe 2020.pdf:/Users/bert/Zotero/storage/V5TIJJBT/promotie reglement TUe 2020.pdf:application/pdf},
}

@report{greenhouse-tucknott_towards_2020,
	title = {Towards the unity of pathological and exertional fatigue: A predictive processing model},
	url = {https://psyarxiv.com/nqxz2/},
	shorttitle = {Towards the unity of pathological and exertional fatigue},
	abstract = {Fatigue is a common experience in both health and disease. Pathological (i.e. prolonged or chronic) and transient (i.e. exertional) fatigue symptoms are traditionally considered distinct, compounding a separation in research fields interested in the study of fatigue. Within the clinical neurosciences, nascent frameworks position pathological fatigue as a product of inference derived through hierarchical predictive processing. The metacognitive theory of dyshomeostasis (Stephan et al., 2016) states that pathological fatigue emerges from the detection of a persistent mismatch between prior interoceptive predictions and ascending sensory evidence (i.e. prediction error). Evaluation of allostatic control during the experience of dyshomeostasis signals low evidence for internal generative models, which undermine an agent’s feeling of mastery over the body and is experienced phenomenologically as fatigue. The same metacognitive mechanisms have been excluded as a theoretical account for the acute, transient experience of exertional fatigue. Here, we contest this proposition and offer a more parsimonious account of fatigue in which a common mechanism (i.e. the loss of certainty or confidence in allostatic predictions) is at the core of both pathological and non-pathological, exertional symptoms of fatigue.},
	institution = {{PsyArXiv}},
	author = {Greenhouse-Tucknott, Aaron and Butterworth, Jake and Wrightson, James and Smeeton, Nicholas and Critchley, {HUgo} and Dekerle, Jeanne and Harrison, Neil},
	urldate = {2021-04-27},
	date = {2020-08-04},
	doi = {10.31234/osf.io/nqxz2},
	note = {type: article},
	keywords = {Neuroscience, Behavioral Neuroscience, Exercise, Fatigue, Interoception, Metacognition, Predictive Coding, Predictive Processing},
	file = {Greenhouse-Tucknott et al. - 2020 - Towards the unity of pathological and exertional f.pdf:/Users/bert/Zotero/storage/VKKG9DX8/Greenhouse-Tucknott et al. - 2020 - Towards the unity of pathological and exertional f.pdf:application/pdf},
}

@online{noauthor_towards_nodate,
	title = {Towards the unity of pathological and exertional fatigue: A predictive coding model, 2020, Greenhouse-Tucknott et al.},
	url = {https://www.s4me.info/threads/towards-the-unity-of-pathological-and-exertional-fatigue-a-predictive-coding-model-2020-greenhouse-tucknott-et-al.16228/},
	shorttitle = {Towards the unity of pathological and exertional fatigue},
	abstract = {Preprint:
https://psyarxiv.com/nqxz2

Neil Harrison is involved in {ME}/{CFS}; possibly others also.

Towards the Unity of Pathological and Exertional...},
	titleaddon = {Science for {ME}},
	urldate = {2021-04-27},
	langid = {american},
	file = {Snapshot:/Users/bert/Zotero/storage/67HAKJDS/towards-the-unity-of-pathological-and-exertional-fatigue-a-predictive-coding-model-2020-greenho.html:text/html},
}

@article{millidge_neural_2021,
	title = {Neural Kalman Filtering},
	url = {http://arxiv.org/abs/2102.10021},
	abstract = {The Kalman filter is a fundamental filtering algorithm that fuses noisy sensory data, a previous state estimate, and a dynamics model to produce a principled estimate of the current state. It assumes, and is optimal for, linear models and white Gaussian noise. Due to its relative simplicity and general effectiveness, the Kalman filter is widely used in engineering applications. Since many sensory problems the brain faces are, at their core, filtering problems, it is possible that the brain possesses neural circuitry that implements equivalent computations to the Kalman filter. The standard approach to Kalman filtering requires complex matrix computations that are unlikely to be directly implementable in neural circuits. In this paper, we show that a gradient-descent approximation to the Kalman filter requires only local computations with variance weighted prediction errors. Moreover, we show that it is possible under the same scheme to adaptively learn the dynamics model with a learning rule that corresponds directly to Hebbian plasticity. We demonstrate the performance of our method on a simple Kalman filtering task, and propose a neural implementation of the required equations.},
	journaltitle = {{arXiv}:2102.10021 [cs]},
	author = {Millidge, Beren and Tschantz, Alexander and Seth, Anil and Buckley, Christopher},
	urldate = {2021-04-24},
	date = {2021-02-19},
	eprinttype = {arxiv},
	eprint = {2102.10021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/J2AVUA78/2102.html:text/html;Millidge et al. - 2021 - Neural Kalman Filtering.pdf:/Users/bert/Zotero/storage/KDJ3WUTF/Millidge et al. - 2021 - Neural Kalman Filtering.pdf:application/pdf},
}

@article{hoffmann_linear_2017,
	title = {Linear Optimal Control on Factor Graphs — A Message Passing Perspective —},
	volume = {50},
	issn = {2405-8963},
	url = {https://www.sciencedirect.com/science/article/pii/S2405896317313800},
	doi = {10.1016/j.ifacol.2017.08.914},
	series = {20th {IFAC} World Congress},
	abstract = {Factor graphs form a class of probabilistic graphical models representing the factorization of probability density functions as bipartite graphs. They can be used to exploit the conditional independence structure of the underlying model to efficiently solve inference problems by message passing. The present paper advocates the use of factor graphs in control and highlights similarities to, e. g., signal processing and communications where this class of models is widely used. By applying the factor graph framework to a probabilistic interpretation of optimal control, several classical results are recovered. The dynamic programming approach to linear quadratic Gaussian control is described as a message passing algorithm on factor graph on which possible extensions are exemplified. A factor graph-based iterative learning control scheme is outlined and an expectation maximization-based estimation of normal unknown variance priors is adapted for the derivation of sparse control signals, highlighting the benefits of using a unified framework across disciplines by mixing and matching corresponding graphical algorithms.},
	pages = {6314--6319},
	number = {1},
	journaltitle = {{IFAC}-{PapersOnLine}},
	shortjournal = {{IFAC}-{PapersOnLine}},
	author = {Hoffmann, Christian and Rostalski, Philipp},
	urldate = {2021-04-22},
	date = {2017-07-01},
	langid = {english},
	keywords = {Message passing, Expectation maximization, Factor graphs, Iterative learning control, Kalman filtering, {LQG} control, Normal unknown variance prior, Probabilistic models},
	file = {Hoffmann and Rostalski - 2017 - Linear Optimal Control on Factor Graphs — A Messag.pdf:/Users/bert/Zotero/storage/6UCD26HW/Hoffmann and Rostalski - 2017 - Linear Optimal Control on Factor Graphs — A Messag.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/BHJZ6QPQ/S2405896317313800.html:text/html},
}

@article{da_costa_neural_2021,
	title = {Neural Dynamics under Active Inference: Plausibility and Efficiency of Information Processing},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/4/454},
	doi = {10.3390/e23040454},
	shorttitle = {Neural Dynamics under Active Inference},
	abstract = {Active inference is a normative framework for explaining behaviour under the free energy principle—a theory of self-organisation originating in neuroscience. It specifies neuronal dynamics for state-estimation in terms of a descent on (variational) free energy—a measure of the fit between an internal (generative) model and sensory observations. The free energy gradient is a prediction error—plausibly encoded in the average membrane potentials of neuronal populations. Conversely, the expected probability of a state can be expressed in terms of neuronal firing rates. We show that this is consistent with current models of neuronal dynamics and establish face validity by synthesising plausible electrophysiological responses. We then show that these neuronal dynamics approximate natural gradient descent, a well-known optimisation algorithm from information geometry that follows the steepest descent of the objective in information space. We compare the information length of belief updating in both schemes, a measure of the distance travelled in information space that has a direct interpretation in terms of metabolic cost. We show that neural dynamics under active inference are metabolically efficient and suggest that neural representations in biological agents may evolve by approximating steepest descent in information space towards the point of optimal inference.},
	pages = {454},
	number = {4},
	journaltitle = {Entropy},
	author = {Da Costa, Lancelot and Parr, Thomas and Sengupta, Biswa and Friston, Karl},
	urldate = {2021-04-17},
	date = {2021-04},
	langid = {english},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {free energy principle, active inference, Bayesian brain, Fisher information length, information geometry, metabolic efficiency, natural gradient descent, process theory, self-organisation, variational Bayesian inference},
	file = {Da Costa et al. - 2021 - Neural Dynamics under Active Inference Plausibili.pdf:/Users/bert/Zotero/storage/QDV288YC/Da Costa et al. - 2021 - Neural Dynamics under Active Inference Plausibili.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/I6DIDBYT/htm.html:text/html},
}

@article{li_revisit_2020,
	title = {A revisit to {MacKay} algorithm and its application to deep network compression},
	volume = {14},
	issn = {2095-2236},
	url = {https://doi.org/10.1007/s11704-019-8390-z},
	doi = {10.1007/s11704-019-8390-z},
	abstract = {An iterative procedure introduced in {MacKay}’s evidence framework is often used for estimating the hyperparameter in empirical Bayes. Together with the use of a particular form of prior, the estimation of the hyperparameter reduces to an automatic relevance determination model, which provides a soft way of pruning model parameters. Despite the effectiveness of this estimation procedure, it has stayed primarily as a heuristic to date and its application to deep neural network has not yet been explored. This paper formally investigates the mathematical nature of this procedure and justifies it as a well-principled algorithm framework, which we call the {MacKay} algorithm. As an application, we demonstrate its use in deep neural networks, which have typically complicated structure with millions of parameters and can be pruned to reduce the memory requirement and boost computational efficiency. In experiments, we adopt {MacKay} algorithm to prune the parameters of both simple networks such as {LeNet}, deep convolution {VGG}-like networks, and residual netowrks for large image classification task. Experimental results show that the algorithm can compress neural networks to a high level of sparsity with little loss of prediction accuracy, which is comparable with the state-of-the-art.},
	pages = {144304},
	number = {4},
	journaltitle = {Frontiers of Computer Science},
	shortjournal = {Front. Comput. Sci.},
	author = {Li, Chune and Mao, Yongyi and Zhang, Richong and Huai, Jinpeng},
	urldate = {2021-04-12},
	date = {2020-01-03},
	langid = {english},
	file = {Springer Full Text PDF:/Users/bert/Zotero/storage/MZ6NX3YU/Li et al. - 2020 - A revisit to MacKay algorithm and its application .pdf:application/pdf},
}

@article{fagerholm_neural_2021,
	title = {Neural systems under change of scale},
	volume = {15},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2021.643148/abstract},
	doi = {10.3389/fncom.2021.643148},
	abstract = {We derive a theoretical construct that allows for the characterisation of both scalable and scale free systems within the Dynamic Causal Modelling framework. We define a dynamical system to be ‘scalable’ if the same equation of motion continues to apply as the system changes in size. As an example of such a system, we simulate planetary orbits varying in size and show that our proposed methodology can be used to recover Kepler’s third law from the timeseries. In contrast, a ‘scale free’ system is one in which there is no characteristic length scale, meaning that images of such a system are statistically unchanged at different levels of magnification. As an example of such a system, we use calcium imaging collected in murine cortex and show that the dynamical critical exponent, as defined in renormalization group theory, can be estimated in an empirical biological setting. We find that a task-relevant region of the cortex is associated with higher dynamical critical exponents in task vs. spontaneous states and vice versa for a task-irrelevant region.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front. Comput. Neurosci.},
	author = {Fagerholm, Erik and Gallero-Salas, Yasir and Foulkes, W. M. C. and Helmchen, Fritjof and Friston, Karl J. and Leech, Robert and Moran, Rosalyn J.},
	urldate = {2021-04-05},
	date = {2021},
	note = {Publisher: Frontiers},
	keywords = {compuatational modeling, {DCM}, mathematical foundations, renormalisation, scalabality},
}

@article{weistuch_refractory_2020,
	title = {The refractory period matters: unifying mechanisms of macroscopic brain waves},
	url = {http://arxiv.org/abs/2008.04940},
	shorttitle = {The refractory period matters},
	abstract = {The relationship between complex, brain oscillations and the dynamics of individual neurons is poorly understood. Here we utilize Maximum Caliber, a dynamical inference principle, to build a minimal, yet general model of the collective (mean-field) dynamics of large populations of neurons. In agreement with previous experimental observations, we describe a simple, testable mechanism, involving only a single type of neuron, by which many of these complex oscillatory patterns may emerge. Our model predicts that the refractory period of neurons, which has been previously neglected, is essential for these behaviors.},
	journaltitle = {{arXiv}:2008.04940 [q-bio]},
	author = {Weistuch, Corey and Mujica-Parodi, Lilianne R. and Dill, Ken},
	urldate = {2021-04-04},
	date = {2020-09-03},
	eprinttype = {arxiv},
	eprint = {2008.04940},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XKR64BB9/2008.html:text/html;Weistuch et al. - 2020 - The refractory period matters unifying mechanisms.pdf:/Users/bert/Zotero/storage/C4CVXNPQ/Weistuch et al. - 2020 - The refractory period matters unifying mechanisms.pdf:application/pdf},
}

@article{maisto_active_2021,
	title = {Active Tree Search in Large {POMDPs}},
	url = {http://arxiv.org/abs/2103.13860},
	abstract = {Model-based planning and prospection are widely studied in both cognitive neuroscience and artificial intelligence ({AI}), but from different perspectives - and with different desiderata in mind (biological realism versus scalability) that are difficult to reconcile. Here, we introduce a novel method to plan in large {POMDPs} - Active Tree Search - that combines the normative character and biological realism of a leading planning theory in neuroscience (Active Inference) and the scalability of Monte-Carlo methods in {AI}. This unification is beneficial for both approaches. On the one hand, using Monte-Carlo planning permits scaling up the biologically grounded approach of Active Inference to large-scale problems. On the other hand, the theory of Active Inference provides a principled solution to the balance of exploration and exploitation, which is often addressed heuristically in Monte-Carlo methods. Our simulations show that Active Tree Search successfully navigates binary trees that are challenging for sampling-based methods, problems that require adaptive exploration, and the large {POMDP} problem Rocksample. Furthermore, we illustrate how Active Tree Search can be used to simulate neurophysiological responses (e.g., in the hippocampus and prefrontal cortex) of humans and other animals that contain large planning problems. These simulations show that Active Tree Search is a principled realisation of neuroscientific and {AI} theories of planning, which offers both biological realism and scalability.},
	journaltitle = {{arXiv}:2103.13860 [cs, math, q-bio]},
	author = {Maisto, Domenico and Gregoretti, Francesco and Friston, Karl and Pezzulo, Giovanni},
	urldate = {2021-03-30},
	date = {2021-03-25},
	eprinttype = {arxiv},
	eprint = {2103.13860},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition, G.3, I.2, I.6, J.4, Mathematics - Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LXBYKIGT/2103.html:text/html;Maisto et al. - 2021 - Active Tree Search in Large POMDPs.pdf:/Users/bert/Zotero/storage/G7HURKSL/Maisto et al. - 2021 - Active Tree Search in Large POMDPs.pdf:application/pdf},
}

@article{sakthivadivel_formalising_2021,
	title = {Formalising the Use of the Activation Function in Neural Inference},
	url = {https://paperswithcode.com/paper/formalising-the-use-of-the-activation},
	abstract = {No code available yet.},
	author = {Sakthivadivel, Dalton},
	urldate = {2021-03-17},
	date = {2021},
	langid = {english},
	file = {Sakthivadivel - 2021 - Formalising the Use of the Activation Function in .pdf:/Users/bert/Zotero/storage/RNLC2FGF/Sakthivadivel - 2021 - Formalising the Use of the Activation Function in .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/5WRA2WIP/formalising-the-use-of-the-activation.html:text/html},
}

@book{sakthivadivel_characterising_2021,
	title = {Characterising the Non-Equilibrium Dynamics of a Neural Cell},
	abstract = {We examine the dynamical evolution of the state of a neurone, with particular care to the non-equilibrium nature of the forces influencing its movement in state space. We combine non-equilibrium statistical mechanics and dynamical systems theory to characterise the nature of the neural resting state, and its relationship to firing. The stereotypical shape of the action potential arises from this model, as well as bursting dynamics, and the non-equilibrium phase transition from resting to spiking. Geometric properties of the system are discussed, such as the birth and shape of the neural limit cycle, which provide a complementary understanding of these dynamics. This provides a multiscale model of the neural cell, from molecules to spikes, and explains various phenomena in a unified manner. Some more general notions for damped oscillators, birth-death processes, and stationary non-equilibrium systems are included.},
	author = {Sakthivadivel, Dalton},
	date = {2021-02-17},
	file = {Sakthivadivel - 2021 - Characterising the Non-Equilibrium Dynamics of a N.pdf:/Users/bert/Zotero/storage/F6XGZGVQ/Sakthivadivel - 2021 - Characterising the Non-Equilibrium Dynamics of a N.pdf:application/pdf},
}

@article{nakamoto_bitcoin_2009,
	title = {Bitcoin: A Peer-to-Peer Electronic Cash System},
	abstract = {A purely peer-to-peer version of electronic cash would allow online payments to be sent directly from one party to another without going through a financial institution. Digital signatures provide part of the solution, but the main benefits are lost if a trusted third party is still required to prevent double-spending. We propose a solution to the double-spending problem using a peer-to-peer network. The network timestamps transactions by hashing them into an ongoing chain of hash-based proof-of-work, forming a record that cannot be changed without redoing the proof-of-work. The longest chain not only serves as proof of the sequence of events witnessed, but proof that it came from the largest pool of {CPU} power. As long as a majority of {CPU} power is controlled by nodes that are not cooperating to attack the network, they'll generate the longest chain and outpace attackers. The network itself requires minimal structure. Messages are broadcast on a best effort basis, and nodes can leave and rejoin the network at will, accepting the longest proof-of-work chain as proof of what happened while they were gone.},
	pages = {9},
	author = {Nakamoto, Satoshi},
	date = {2009},
	langid = {english},
	file = {Nakamoto - 2009 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:/Users/bert/Zotero/storage/FDRC3KF6/Nakamoto - 2009 - Bitcoin A Peer-to-Peer Electronic Cash System.pdf:application/pdf},
}

@article{sarkka_recursive_2009,
	title = {Recursive Noise Adaptive Kalman Filtering by Variational Bayesian Approximations},
	volume = {54},
	issn = {0018-9286, 1558-2523},
	url = {http://ieeexplore.ieee.org/document/4796261/},
	doi = {10.1109/TAC.2008.2008348},
	abstract = {This article considers the application of variational Bayesian methods to joint recursive estimation of the dynamic state and the time-varying measurement noise parameters in linear state space models. The proposed adaptive Kalman ﬁltering method is based on forming a separable variational approximation to the joint posterior distribution of states and noise parameters on each time step separately. The result is a recursive algorithm, where on each step the state is estimated with Kalman ﬁlter and the sufﬁcient statistics of the noise variances are estimated with a ﬁxed-point iteration. The performance of the algorithm is demonstrated with simulated data.},
	pages = {596--600},
	number = {3},
	journaltitle = {{IEEE} Transactions on Automatic Control},
	shortjournal = {{IEEE} Trans. Automat. Contr.},
	author = {Sarkka, S. and Nummenmaa, A.},
	urldate = {2021-03-05},
	date = {2009-03},
	langid = {english},
	file = {Sarkka and Nummenmaa - 2009 - Recursive Noise Adaptive Kalman Filtering by Varia.pdf:/Users/bert/Zotero/storage/KJZTGI65/Sarkka and Nummenmaa - 2009 - Recursive Noise Adaptive Kalman Filtering by Varia.pdf:application/pdf},
}

@online{noauthor_recursive_nodate,
	title = {recursive noise adaptive Kalman filtering - Google Search},
	url = {https://www.google.com/search?q=recursive+noise+adaptive+Kalman+filtering&oq=recursive+noise+adaptive+Kalman+filtering&aqs=chrome..69i57.11157j0j4&sourceid=chrome&ie=UTF-8},
	urldate = {2021-03-05},
	file = {recursive noise adaptive Kalman filtering - Google Search:/Users/bert/Zotero/storage/D72P7GEQ/search.html:text/html},
}

@inproceedings{wadehn_state_2019-1,
	title = {State Space Models with Dynamical and Sparse Variances, and Inference by {EM} Message Passing},
	doi = {10.23919/EUSIPCO.2019.8902815},
	abstract = {Sparse Bayesian learning ({SBL}) is a probabilistic approach to estimation problems based on representing sparsity-promoting priors by Normals with Unknown Variances. This representation blends well with linear Gaussian state space models ({SSMs}). However, in classical {SBL} the unknown variances are a priori independent, which is not suited for modeling group sparse signals, or signals whose variances have structure. To model signals with, e.g., exponentially decaying or piecewise-constant (in particular block-sparse) variances, we propose {SSMs} with dynamical and sparse variances ({SSM}-{DSV}). These are two-layer {SSMs}, where the bottom layer models physical signals, and the top layer models dynamical variances that are subject to abrupt changes. Inference and learning in these hierarchical models is performed with a message passing version of the expectation maximization ({EM}) algorithm, which is a special instance of the more general class of variational message passing algorithms. We validated the proposed model and estimation algorithm with two applications, using both simulated and real data. First, we implemented a block-outlier insensitive Kalman smoother by modeling the disturbance process with a {SSM}-{DSV}. Second, we used {SSM}-{DSV} to model the oculomotor system and employed {EM}-message passing for estimating neural controller signals from eye position data.},
	eventtitle = {2019 27th European Signal Processing Conference ({EUSIPCO})},
	pages = {1--5},
	booktitle = {2019 27th European Signal Processing Conference ({EUSIPCO})},
	author = {Wadehn, F. and Weber, T. and Loeliger, H.-A.},
	date = {2019-09},
	note = {{ISSN}: 2076-1465},
	keywords = {Estimation, Message passing, Signal processing algorithms, Bayes methods, Kalman filters, message passing, Inference algorithms, Expectation maximization, Biological system modeling, bottom layer models, classical {SBL}, {EM} message passing, {EM}-message passing, estimation algorithm, expectation-maximisation algorithm, factor graphs, Gaussian processes, group sparse signals, hierarchical models, hierarchical state space models, inference, learning (artificial intelligence), linear Gaussian state space models, message passing version, model signals, neural controller signals, particular block-sparse, probabilistic approach, representing sparsity-promoting priors, Sparse Bayesian learning, sparse Bayesian learning., sparse variances, {SSM}-{DSV}, state-space methods, top layer models dynamical, two-layer {SSMs}, unknown variances, variational message passing algorithms},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/HARIXLET/8902815.html:text/html;Wadehn et al. - 2019 - State Space Models with Dynamical and Sparse Varia.pdf:/Users/bert/Zotero/storage/E7B3QI8M/Wadehn et al. - 2019 - State Space Models with Dynamical and Sparse Varia.pdf:application/pdf},
}

@video{biaslab_5ssd0_2020,
	title = {5SSD0 live class 18-Nov-2020  lesson B3},
	url = {https://www.youtube.com/watch?v=SE6STt-Y8MY&feature=youtu.be&ab_channel=BIASlab},
	author = {{BIASlab}},
	urldate = {2021-02-26},
	date = {2020-11-18},
}

@article{chauchat_factor_2020,
	title = {Factor Graph-Based Smoothing Without Matrix Inversion for Highly Precise Localization},
	issn = {1558-0865},
	doi = {10.1109/TCST.2020.3001387},
	abstract = {We consider the problem of localizing a manned, semiautonomous, or autonomous vehicle in the environment using information coming from the vehicle's sensors, a problem known as navigation or simultaneous localization and mapping ({SLAM}) depending on the context. To infer knowledge from sensors' measurements, while drawing on a priori knowledge about the vehicle's dynamics, modern approaches solve an optimization problem to compute the most likely trajectory given all past observations, an approach known as smoothing. Improving smoothing solvers is an active field of research in the {SLAM} community. Most work is focused on reducing computation load by inverting the involved linear system while preserving its sparsity. This article raises an issue that, to the best of our knowledge, has not been addressed yet: standard smoothing solvers require explicitly using the inverse of sensor noise covariance matrices. This means the parameters that reflect the noise magnitude must be sufficiently large for the smoother to properly function. When matrices are close to singular, which is the case when using high-precision modern inertial measurement units ({IMUs}), numerical issues necessarily arise, especially with 32-bit implementation demanded by most industrial aerospace applications. We discuss these issues and propose a solution that builds upon the Kalman filter to improve smoothing algorithms. We then leverage the results to devise a localization algorithm based on the fusion of {IMU} and vision sensors. Successful real experiments using an actual car equipped with a tactical grade high-performance {IMU} and a {LiDAR} illustrate the relevance of the approach to the field of autonomous vehicles.},
	pages = {1--14},
	journaltitle = {{IEEE} Transactions on Control Systems Technology},
	author = {Chauchat, P. and Barrau, A. and Bonnabel, S.},
	date = {2020},
	note = {Conference Name: {IEEE} Transactions on Control Systems Technology},
	keywords = {Kalman filters, Factor graphs, Autonomous vehicles, Covariance matrices, ill-conditionning, inertial navigation, information filter, Kalman filter, localization, Optimization, Simultaneous localization and mapping, simultaneous localization and mapping ({SLAM}), Smoothing methods, smoothing methods.},
	file = {Chauchat et al. - 2020 - Factor Graph-Based Smoothing Without Matrix Invers.pdf:/Users/bert/Zotero/storage/48LCMN3W/Chauchat et al. - 2020 - Factor Graph-Based Smoothing Without Matrix Invers.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/R9CFYZRW/9123496.html:text/html},
}

@inproceedings{wadehn_new_2016,
	location = {Monticello, {IL}, {USA}},
	title = {New square-root and diagonalized Kalman smoothers},
	isbn = {978-1-5090-4550-1},
	url = {http://ieeexplore.ieee.org/document/7852382/},
	doi = {10.1109/ALLERTON.2016.7852382},
	abstract = {Standard implementations of Kalman ﬁlters and smoothers often suffer from numerical instability issues, due to round-off errors, even for moderate-sized state space models. Recently, two inversion-free and computationally efﬁcient Kalman smoothers, an adapted version of the Modiﬁed-Bryson Frasier smoother ({MBF}), mainly tailored to input estimation and the Backward Information Filter Forward Marginal ({BIFM}) smoother for state estimation and output interpolation were presented. In this paper, we will ﬁrst suggest improvements to both the {MBF} and {BIFM} smoother implementations aimed at improving computational efﬁciency and, using this improved version of the {BIFM} smoother, we will elaborate on its usage in sensor networks with spatially correlated noise. The main novelty in this paper is the square-root version of the {BIFM} smoother, which can be used in numerically critical smoothing problems, as exempliﬁed in a force estimation problem using a multi-mass resonator model of an industrial milling machine.},
	eventtitle = {2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	pages = {1282--1290},
	booktitle = {2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	publisher = {{IEEE}},
	author = {Wadehn, Federico and Bruderer, Lukas and Sahdeva, Vijay and Loeliger, Hans-Andrea},
	urldate = {2019-01-23},
	date = {2016-09},
	langid = {english},
	file = {Wadehn e.a. - 2016 - New square-root and diagonalized Kalman smoothers.pdf:/Users/bert/Zotero/storage/HYKXSP9C/Wadehn e.a. - 2016 - New square-root and diagonalized Kalman smoothers.pdf:application/pdf},
}

@article{xue_speech_2021,
	title = {Speech Enhancement Based on Modulation-Domain Parametric Multichannel Kalman Filtering},
	volume = {29},
	issn = {2329-9304},
	doi = {10.1109/TASLP.2020.3040850},
	abstract = {Recently we presented a modulation-domain multichannel Kalman filtering ({MKF}) algorithm for speech enhancement, which jointly exploits the inter-frame modulation-domain temporal evolution of speech and the inter-channel spatial correlation to estimate the clean speech signal. The goal of speech enhancement is to suppress noise while keeping the speech undistorted, and a key problem is to achieve the best trade-off between speech distortion and noise reduction. In this paper, we extend the {MKF} by presenting a modulation-domain parametric {MKF} ({PMKF}) which includes a parameter that enables flexible control of the speech enhancement behaviour in each time-frequency ({TF}) bin. Based on the decomposition of the {MKF} cost function, a new cost function for {PMKF} is proposed, which uses the controlling parameter to weight the noise reduction and speech distortion terms. An optimal {PMKF} gain is derived using a minimum mean squared error ({MMSE}) criterion. We analyse the performance of the proposed {MKF}, and show its relationship to the speech distortion weighted multichannel Wiener filter ({SDW}-{MWF}). To evaluate the impact of the controlling parameter on speech enhancement performance, we further propose {PMKF} speech enhancement systems in which the controlling parameter is adaptively chosen in each {TF} bin. Experiments on a publicly available head-related impulse response ({HRIR}) database in different noisy and reverberant conditions demonstrate the effectiveness of the proposed method.},
	pages = {393--405},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Xue, W. and Moore, A. H. and Brookes, M. and Naylor, P. A.},
	date = {2021},
	note = {Conference Name: {IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	keywords = {Kalman filters, Speech processing, Kalman filtering, clean speech signal, Correlation, correlation methods, Distortion, head-related impulse response database, inter-channel spatial correlation, least mean squares methods, microphone arrays, minimum mean squared error criterion, {MKF} cost function, modulation, modulation domain, modulation-domain parametric {MKF}, modulation-domain parametric multichannel Kalman filtering algorithm, Noise measurement, noise reduction, Noise reduction, {PMKF} speech enhancement systems, speech distortion, speech distortion terms, speech distortion weighted multichannel Wiener filter, speech enhancement, Speech enhancement, speech enhancement behaviour, Wiener filters},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/82MJK8QN/9272832.html:text/html;Xue et al. - 2021 - Speech Enhancement Based on Modulation-Domain Para.pdf:/Users/bert/Zotero/storage/F27RQ2D6/Xue et al. - 2021 - Speech Enhancement Based on Modulation-Domain Para.pdf:application/pdf},
}

@inproceedings{yablonovitch_classical_2019,
	title = {Classical Machines, for Solving the Toughest Problems in Computer Science},
	rights = {\&\#169; 2019 The Author(s)},
	url = {https://www.osapublishing.org/abstract.cfm?uri=FiO-2019-JTu4A.114},
	doi = {10.1364/FIO.2019.JTu4A.114},
	abstract = {Classical analog processors are now solving hard Computer Science problems that were thought to require a Quantum Computer. There are now classical machines that solve Ising type optimizations, opening the door toward other {NP}-hard problems as well.},
	eventtitle = {Frontiers in Optics},
	pages = {JTu4A.114},
	booktitle = {Frontiers in Optics + Laser Science {APS}/{DLS} (2019), paper {JTu}4A.114},
	publisher = {Optical Society of America},
	author = {Yablonovitch, Eli and Xiao, T. Patrick and Vadlamani, Sri Krishna},
	urldate = {2021-02-15},
	date = {2019-09-15},
	keywords = {Neural networks, Field programmable gate arrays, Injection locking, Numerical simulation, Parametric oscillators, Quantum computing},
	file = {Snapshot:/Users/bert/Zotero/storage/Z5HHFSYG/abstract.html:text/html},
}

@article{shlezinger_data-driven_2020,
	title = {Data-Driven Factor Graphs for Deep Symbol Detection},
	url = {http://arxiv.org/abs/2002.00758},
	abstract = {Many important schemes in signal processing and communications, ranging from the {BCJR} algorithm to the Kalman filter, are instances of factor graph methods. This family of algorithms is based on recursive message passing-based computations carried out over graphical models, representing a factorization of the underlying statistics. Consequently, in order to implement these algorithms, one must have accurate knowledge of the statistical model of the considered signals. In this work we propose to implement factor graph methods in a data-driven manner. In particular, we propose to use machine learning ({ML}) tools to learn the factor graph, instead of the overall system task, which in turn is used for inference by message passing over the learned graph. We apply the proposed approach to learn the factor graph representing a finite-memory channel, demonstrating the resulting ability to implement {BCJR} detection in a data-driven fashion. We demonstrate that the proposed system, referred to as {BCJRNet}, learns to implement the {BCJR} algorithm from a small training set, and that the resulting receiver exhibits improved robustness to inaccurate training compared to the conventional channel-model-based receiver operating under the same level of uncertainty. Our results indicate that by utilizing {ML} tools to learn factor graphs from labeled data, one can implement a broad range of model-based algorithms, which traditionally require full knowledge of the underlying statistics, in a data-driven fashion.},
	journaltitle = {{arXiv}:2002.00758 [cs, eess, math, stat]},
	author = {Shlezinger, Nir and Farsad, Nariman and Eldar, Yonina C. and Goldsmith, Andrea J.},
	urldate = {2021-02-11},
	date = {2020-01-31},
	eprinttype = {arxiv},
	eprint = {2002.00758},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/WX77AZQ8/Shlezinger et al. - 2020 - Data-Driven Factor Graphs for Deep Symbol Detectio.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/I2YCQKMA/2002.html:text/html},
}

@inproceedings{watson_stochastic_2020,
	title = {Stochastic Optimal Control as Approximate Input Inference},
	url = {http://proceedings.mlr.press/v100/watson20a.html},
	abstract = {Optimal control of stochastic nonlinear dynamical systems is a major challenge in the domain of robot learning. Given the intractability of the global control problem, state-of-the-art algorithms f...},
	eventtitle = {Conference on Robot Learning},
	pages = {697--716},
	booktitle = {Conference on Robot Learning},
	publisher = {{PMLR}},
	author = {Watson, Joe and Abdulsamad, Hany and Peters, Jan},
	urldate = {2021-02-09},
	date = {2020-05-12},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Snapshot:/Users/bert/Zotero/storage/F95NGSKM/watson20a.html:text/html;Watson et al. - 2020 - Stochastic Optimal Control as Approximate Input In.pdf:/Users/bert/Zotero/storage/UI368ICA/Watson et al. - 2020 - Stochastic Optimal Control as Approximate Input In.pdf:application/pdf},
}

@article{millidge_whence_2020,
	title = {Whence the Expected Free Energy?},
	url = {http://arxiv.org/abs/2004.08128},
	abstract = {The Expected Free Energy ({EFE}) is a central quantity in the theory of active inference. It is the quantity that all active inference agents are mandated to minimize through action, and its decomposition into extrinsic and intrinsic value terms is key to the balance of exploration and exploitation that active inference agents evince. Despite its importance, the mathematical origins of this quantity and its relation to the Variational Free Energy ({VFE}) remain unclear. In this paper, we investigate the origins of the {EFE} in detail and show that it is not simply "the free energy in the future". We present a functional that we argue is the natural extension of the {VFE}, but which actively discourages exploratory behaviour, thus demonstrating that exploration does not directly follow from free energy minimization into the future. We then develop a novel objective, the Free-Energy of the Expected Future ({FEEF}), which possesses both the epistemic component of the {EFE} as well as an intuitive mathematical grounding as the divergence between predicted and desired futures.},
	journaltitle = {{arXiv}:2004.08128 [cs]},
	author = {Millidge, Beren and Tschantz, Alexander and Buckley, Christopher L.},
	urldate = {2021-02-07},
	date = {2020-09-28},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2004.08128},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Millidge et al. - 2020 - Whence the Expected Free Energy.pdf:/Users/bert/Zotero/storage/3VAMMJ74/Millidge et al. - 2020 - Whence the Expected Free Energy.pdf:application/pdf},
}

@article{askar_recursive_1984,
	title = {Recursive algorithms for Bayes smoothing with uncertain observations},
	volume = {29},
	issn = {1558-2523},
	doi = {10.1109/TAC.1984.1103554},
	abstract = {Recursive algorithms for the Bayes solution of fixed-interval, fixed-point, and fixed-lag smoothing under uncertain observations are presented. The Bayes smoothing algorithms are obtained for a Markovian system model with Markov uncertainty, a model more general than the one used in linear smoothing algorithms. The Bayes fixed-interval smoothing algorithm is applied to a Gauss-Markov example. The simulation results for this example indicate that the {MSE} performance of the Bayes smoother is significantly better than that of the linear smoother.},
	pages = {459--461},
	number = {5},
	journaltitle = {{IEEE} Transactions on Automatic Control},
	author = {Askar, M. and Derin, H.},
	date = {1984-05},
	note = {Conference Name: {IEEE} Transactions on Automatic Control},
	keywords = {Gaussian noise, Markov processes, Gaussian processes, Smoothing methods, Additive noise, Application software, Bayes procedures, Filtering algorithms, Recursive estimation, Uncertain systems, Uncertainty},
	file = {Askar and Derin - 1984 - Recursive algorithms for Bayes smoothing with unce.pdf:/Users/bert/Zotero/storage/N9ZYVN4V/Askar and Derin - 1984 - Recursive algorithms for Bayes smoothing with unce.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/V9SENAGP/1103554.html:text/html},
}

@article{askar_recursive_1983,
	title = {Recursive algorithms for the Bayes solution of the fixed-point and fixed-lag smoothing problems},
	volume = {28},
	issn = {1558-2523},
	doi = {10.1109/TAC.1983.1103161},
	abstract = {Recursive algorithms for the Bayes solutions of the fixed-point and fixed-lag smoothing problems are obtained. Recursive algorithms for the respective smoothed a posteriori densities are derived under assumptions that the signal to be estimated is a Markov process and the observation is a signal embedded in independent noise (not necessarily additive) which is also independent of the signal. The recursive algorithm for the fixed-point smoothing is applied to a binary Markov signal corrupted by an independent noise in a nonlinear manner.},
	pages = {996--998},
	number = {10},
	journaltitle = {{IEEE} Transactions on Automatic Control},
	author = {Askar, M. and Derin, H.},
	date = {1983-10},
	note = {Conference Name: {IEEE} Transactions on Automatic Control},
	keywords = {Kalman filters, Markov processes, Signal processing, Gaussian processes, Smoothing methods, Additive noise, Bayes procedures, Filtering algorithms, Recursive estimation, Bayesian methods, Maximum likelihood detection},
	file = {Askar and Derin - 1983 - Recursive algorithms for the Bayes solution of the.pdf:/Users/bert/Zotero/storage/IA6FP3T2/Askar and Derin - 1983 - Recursive algorithms for the Bayes solution of the.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/GMBFZ2IL/1103161.html:text/html},
}

@article{busch_pushnet_2020,
	title = {{PushNet}: Efficient and Adaptive Neural Message Passing},
	url = {http://arxiv.org/abs/2003.02228},
	doi = {10.3233/FAIA200199},
	shorttitle = {{PushNet}},
	abstract = {Message passing neural networks have recently evolved into a state-of-the-art approach to representation learning on graphs. Existing methods perform synchronous message passing along all edges in multiple subsequent rounds and consequently suffer from various shortcomings: Propagation schemes are inflexible since they are restricted to \$k\$-hop neighborhoods and insensitive to actual demands of information propagation. Further, long-range dependencies cannot be modeled adequately and learned representations are based on correlations of fixed locality. These issues prevent existing methods from reaching their full potential in terms of prediction performance. Instead, we consider a novel asynchronous message passing approach where information is pushed only along the most relevant edges until convergence. Our proposed algorithm can equivalently be formulated as a single synchronous message passing iteration using a suitable neighborhood function, thus sharing the advantages of existing methods while addressing their central issues. The resulting neural network utilizes a node-adaptive receptive field derived from meaningful sparse node neighborhoods. In addition, by learning and combining node representations over differently sized neighborhoods, our model is able to capture correlations on multiple scales. We further propose variants of our base model with different inductive bias. Empirical results are provided for semi-supervised node classification on five real-world datasets following a rigorous evaluation protocol. We find that our models outperform competitors on all datasets in terms of accuracy with statistical significance. In some cases, our models additionally provide faster runtime.},
	journaltitle = {{arXiv}:2003.02228 [cs, stat]},
	author = {Busch, Julian and Pi, Jiaxing and Seidl, Thomas},
	urldate = {2021-02-02},
	date = {2020-12-17},
	eprinttype = {arxiv},
	eprint = {2003.02228},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/EV85NDR6/Busch et al. - 2020 - PushNet Efficient and Adaptive Neural Message Pas.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/YSYSANLN/2003.html:text/html},
}

@article{vitetta_multiple_2019,
	title = {Multiple Bayesian Filtering as Message Passing},
	url = {http://arxiv.org/abs/1907.01358},
	doi = {10.1109/TSP.2020.2965296},
	abstract = {In this manuscript, a general method for deriving filtering algorithms that involve a network of interconnected Bayesian filters is proposed. This method is based on the idea that the processing accomplished inside each of the Bayesian filters and the interactions between them can be represented as message passing algorithms over a proper graphical model. The usefulness of our method is exemplified by developing new filtering techniques, based on the interconnection of a particle filter and an extended Kalman filter, for conditionally linear Gaussian systems. Numerical results for two specific dynamic systems evidence that the devised algorithms can achieve a better complexity-accuracy tradeoff than marginalized particle filtering and multiple particle filtering.},
	journaltitle = {{arXiv}:1907.01358 [math, stat]},
	author = {Vitetta, Giorgio M. and Di Viesti, Pasquale and Sirignano, Emilio and Montorsi, Francesco},
	urldate = {2021-02-02},
	date = {2019-07-25},
	eprinttype = {arxiv},
	eprint = {1907.01358},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/RZH5P3HM/Vitetta et al. - 2019 - Multiple Bayesian Filtering as Message Passing.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/C66P9I7X/1907.html:text/html},
}

@article{briers_smoothing_2009,
	title = {Smoothing algorithms for state–space models},
	volume = {62},
	issn = {1572-9052},
	url = {https://doi.org/10.1007/s10463-009-0236-2},
	doi = {10.1007/s10463-009-0236-2},
	abstract = {Two-filter smoothing is a principled approach for performing optimal smoothing in non-linear non-Gaussian state–space models where the smoothing distributions are computed through the combination of ‘forward’ and ‘backward’ time filters. The ‘forward’ filter is the standard Bayesian filter but the ‘backward’ filter, generally referred to as the backward information filter, is not a probability measure on the space of the hidden Markov process. In cases where the backward information filter can be computed in closed form, this technical point is not important. However, for general state–space models where there is no closed form expression, this prohibits the use of flexible numerical techniques such as Sequential Monte Carlo ({SMC}) to approximate the two-filter smoothing formula. We propose here a generalised two-filter smoothing formula which only requires approximating probability distributions and applies to any state–space model, removing the need to make restrictive assumptions used in previous approaches to this problem. {SMC} algorithms are developed to implement this generalised recursion and we illustrate their performance on various problems.},
	pages = {61},
	number = {1},
	journaltitle = {Annals of the Institute of Statistical Mathematics},
	shortjournal = {Annals of the Institute of Statistical Mathematics},
	author = {Briers, Mark and Doucet, Arnaud and Maskell, Simon},
	date = {2009-06-09},
	file = {Briers et al. - 2009 - Smoothing algorithms for state–space models.pdf:/Users/bert/Zotero/storage/PLJQ6BZW/Briers et al. - 2009 - Smoothing algorithms for state–space models.pdf:application/pdf},
}

@inproceedings{bruderer_local_2014,
	title = {Local statistical models from deterministic state space models, likelihood filtering, and local typicality},
	doi = {10.1109/ISIT.2014.6875004},
	abstract = {Surprisingly many signal processing problems can be approached by locally fitting autonomous deterministic linear state space models to the data. In this paper, we introduce local statistical models for such cases and discuss the computation both of the corresponding estimates and of local likelihoods for different models.},
	eventtitle = {2014 {IEEE} International Symposium on Information Theory},
	pages = {1106--1110},
	booktitle = {2014 {IEEE} International Symposium on Information Theory},
	author = {Bruderer, L. and Loeliger, H. and Zalmai, N.},
	date = {2014-06},
	note = {{ISSN}: 2157-8117},
	keywords = {autonomous deterministic linear state space models, deterministic state space model, filtering theory, likelihood filtering, local likelihood estimation, local statistical model, local typicality, locally fitting, Magnetomechanical effects, signal processing problem, statistical analysis, Vectors},
	file = {Bruderer et al. - 2014 - Local statistical models from deterministic state .pdf:/Users/bert/Zotero/storage/AX5TYVWH/Bruderer et al. - 2014 - Local statistical models from deterministic state .pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/PHNK5J5Z/6875004.html:text/html},
}

@article{shin_prior_2021,
	title = {Prior Preference Learning from Experts:Designing a Reward with Active Inference},
	url = {http://arxiv.org/abs/2101.08937},
	shorttitle = {Prior Preference Learning from Experts},
	abstract = {Active inference may be defined as Bayesian modeling of a brain with a biologically plausible model of the agent. Its primary idea relies on the free energy principle and the prior preference of the agent. An agent will choose an action that leads to its prior preference for a future observation. In this paper, we claim that active inference can be interpreted using reinforcement learning ({RL}) algorithms and find a theoretical connection between them. We extend the concept of expected free energy ({EFE}), which is a core quantity in active inference, and claim that {EFE} can be treated as a negative value function. Motivated by the concept of prior preference and a theoretical connection, we propose a simple but novel method for learning a prior preference from experts. This illustrates that the problem with inverse {RL} can be approached with a new perspective of active inference. Experimental results of prior preference learning show the possibility of active inference with {EFE}-based rewards and its application to an inverse {RL} problem.},
	journaltitle = {{arXiv}:2101.08937 [cs]},
	author = {Shin, Jinyoung and Kim, Cheolhyeong and Hwang, Hyung Ju},
	urldate = {2021-01-30},
	date = {2021-01-21},
	eprinttype = {arxiv},
	eprint = {2101.08937},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XFRFVDLB/2101.html:text/html;Shin et al. - 2021 - Prior Preference Learning from ExpertsDesigning a.pdf:/Users/bert/Zotero/storage/5VB4GB5L/Shin et al. - 2021 - Prior Preference Learning from ExpertsDesigning a.pdf:application/pdf},
}

@article{markovic_empirical_2021,
	title = {An empirical evaluation of active inference in multi-armed bandits},
	url = {http://arxiv.org/abs/2101.08699},
	abstract = {A key feature of sequential decision making under uncertainty is a need to balance between exploiting--choosing the best action according to the current knowledge, and exploring--obtaining information about values of other actions. The multi-armed bandit problem, a classical task that captures this trade-off, served as a vehicle in machine learning for developing bandit algorithms that proved to be useful in numerous industrial applications. The active inference framework, an approach to sequential decision making recently developed in neuroscience for understanding human and animal behaviour, is distinguished by its sophisticated strategy for resolving the exploration-exploitation trade-off. This makes active inference an exciting alternative to already established bandit algorithms. Here we derive an efficient and scalable approximate active inference algorithm and compare it to two state-of-the-art bandit algorithms: Bayesian upper confidence bound and optimistic Thompson sampling, on two types of bandit problems: a stationary and a dynamic switching bandit. Our empirical evaluation shows that the active inference algorithm does not produce efficient long-term behaviour in stationary bandits. However, in more challenging switching bandit problem active inference performs substantially better than the two bandit algorithms. The results open exciting venues for further research in theoretical and applied machine learning, as well as lend additional credibility to active inference as a general framework for studying human and animal behaviour.},
	journaltitle = {{arXiv}:2101.08699 [cs]},
	author = {Markovic, Dimitrije and Stojic, Hrvoje and Schwoebel, Sarah and Kiebel, Stefan J.},
	urldate = {2021-01-23},
	date = {2021-01-21},
	eprinttype = {arxiv},
	eprint = {2101.08699},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/WLPKU34V/2101.html:text/html;Markovic et al. - 2021 - An empirical evaluation of active inference in mul.pdf:/Users/bert/Zotero/storage/9DZ6GKK7/Markovic et al. - 2021 - An empirical evaluation of active inference in mul.pdf:application/pdf},
}

@article{alemi_fixing_2018,
	title = {Fixing a Broken {ELBO}},
	url = {http://arxiv.org/abs/1711.00464},
	abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound ({ELBO}) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical {ELBO}, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
	journaltitle = {{arXiv}:1711.00464 [cs, stat]},
	author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
	urldate = {2021-01-22},
	date = {2018-02-13},
	eprinttype = {arxiv},
	eprint = {1711.00464},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/YG2LJC9D/Alemi et al. - 2018 - Fixing a Broken ELBO.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/FCDBJ5NL/1711.html:text/html},
}

@article{vadlamani_physics_2020,
	title = {Physics successfully implements Lagrange multiplier optimization},
	volume = {117},
	rights = {Copyright © 2020 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-{NonCommercial}-{NoDerivatives} License 4.0 ({CC} {BY}-{NC}-{ND}).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/43/26639},
	doi = {10.1073/pnas.2015192117},
	abstract = {Optimization is a major part of human effort. While being mathematical, optimization is also built into physics. For example, physics has the Principle of Least Action; the Principle of Minimum Power Dissipation, also called Minimum Entropy Generation; and the Variational Principle. Physics also has Physical Annealing, which, of course, preceded computational Simulated Annealing. Physics has the Adiabatic Principle, which, in its quantum form, is called Quantum Annealing. Thus, physical machines can solve the mathematical problem of optimization, including constraints. Binary constraints can be built into the physical optimization. In that case, the machines are digital in the same sense that a flip–flop is digital. A wide variety of machines have had recent success at optimizing the Ising magnetic energy. We demonstrate in this paper that almost all those machines perform optimization according to the Principle of Minimum Power Dissipation as put forth by Onsager. Further, we show that this optimization is in fact equivalent to Lagrange multiplier optimization for constrained problems. We find that the physical gain coefficients that drive those systems actually play the role of the corresponding Lagrange multipliers.},
	pages = {26639--26650},
	number = {43},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Vadlamani, Sri Krishna and Xiao, Tianyao Patrick and Yablonovitch, Eli},
	urldate = {2021-01-13},
	date = {2020-10-27},
	langid = {english},
	pmid = {33046659},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {hardware accelerators, Ising solvers, physical optimization},
	file = {pnas.2015192117.sapp.pdf:/Users/bert/Zotero/storage/UA8LX35K/pnas.2015192117.sapp.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/34BSA3GZ/26639.html:text/html;Vadlamani et al. - 2020 - Physics successfully implements Lagrange multiplie.pdf:/Users/bert/Zotero/storage/63TFGXZA/Vadlamani et al. - 2020 - Physics successfully implements Lagrange multiplie.pdf:application/pdf},
}

@article{agozzino_how_2020,
	title = {How Do Cells Adapt? Stories Told in Landscapes},
	volume = {11},
	issn = {1947-5438, 1947-5446},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-chembioeng-011720-103410},
	doi = {10.1146/annurev-chembioeng-011720-103410},
	shorttitle = {How Do Cells Adapt?},
	abstract = {Cells adapt to changing environments. Perturb a cell and it returns to a point of homeostasis. Perturb a population and it evolves toward a fitness peak. We review quantitative models of the forces of adaptation and their visualizations on landscapes. While some adaptations result from single mutations or few-gene effects, others are more cooperative, more delocalized in the genome, and more universal and physical. For example, homeostasis and evolution depend on protein folding and aggregation, energy and protein production, protein diffusion, molecular motor speeds and efficiencies, and protein expression levels. Models provide a way to learn about the fitness of cells and cell populations by making and testing hypotheses.},
	pages = {155--182},
	number = {1},
	journaltitle = {Annual Review of Chemical and Biomolecular Engineering},
	shortjournal = {Annu. Rev. Chem. Biomol. Eng.},
	author = {Agozzino, Luca and Balázsi, Gábor and Wang, Jin and Dill, Ken A.},
	urldate = {2021-01-10},
	date = {2020-06-07},
	langid = {english},
	file = {Agozzino et al. - 2020 - How Do Cells Adapt Stories Told in Landscapes.pdf:/Users/bert/Zotero/storage/LWW83JB2/Agozzino et al. - 2020 - How Do Cells Adapt Stories Told in Landscapes.pdf:application/pdf},
}

@article{agozzino_minimal_2019,
	title = {Minimal constraints for maximum caliber analysis of dissipative steady-state systems},
	volume = {100},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.100.010105},
	doi = {10.1103/PhysRevE.100.010105},
	abstract = {Maximum caliber (Max Cal) is purported to be a general variational principle for nonequilibrium statistical physics. But recently, Jack and Evans [J. Stat. Mech.: Theory Exp. (2016) 093305] and Maes [Non-Dissipative Effects in Nonequilibrium Systems (Springer, New York, 2018)] have raised concerns about how Max Cal handles dissipative processes. Here, we show that the problem does not lie in Max Cal; the problem is in the use of insufficient constraints. We also present an exactly solvable single-particle model of dissipation, valid far from equilibrium, and its solution by maximum caliber. The model illustrates how the influx and efflux of work and heat into a flowing system alters the distribution of trajectories. Maximum caliber is a viable principle for dissipative systems.},
	pages = {010105},
	number = {1},
	journaltitle = {Physical Review E},
	shortjournal = {Phys. Rev. E},
	author = {Agozzino, Luca and Dill, Ken},
	urldate = {2021-01-09},
	date = {2019-07-24},
	note = {Publisher: American Physical Society},
	file = {Agozzino and Dill - 2019 - Minimal constraints for maximum caliber analysis o.pdf:/Users/bert/Zotero/storage/VAFLYFRR/Agozzino and Dill - 2019 - Minimal constraints for maximum caliber analysis o.pdf:application/pdf;APS Snapshot:/Users/bert/Zotero/storage/KJUWL8TS/PhysRevE.100.html:text/html},
}

@article{mujica-parodi_diet_2020,
	title = {Diet modulates brain network stability, a biomarker for brain aging, in young adults},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1913042117},
	doi = {10.1073/pnas.1913042117},
	abstract = {Epidemiological studies suggest that insulin resistance accelerates progression of age-based cognitive impairment, which neuroimaging has linked to brain glucose hypometabolism. As cellular inputs, ketones increase Gibbs free energy change for {ATP} by 27\% compared to glucose. Here we test whether dietary changes are capable of modulating sustained functional communication between brain regions (network stability) by changing their predominant dietary fuel from glucose to ketones. We first established network stability as a biomarker for brain aging using two large-scale (
              n
              = 292, ages 20 to 85 y;
              n
              = 636, ages 18 to 88 y) 3 T functional {MRI} ({fMRI}) datasets. To determine whether diet can influence brain network stability, we additionally scanned 42 adults, age {\textless} 50 y, using ultrahigh-field (7 T) ultrafast (802 ms) {fMRI} optimized for single-participant-level detection sensitivity. One cohort was scanned under standard diet, overnight fasting, and ketogenic diet conditions. To isolate the impact of fuel type, an independent overnight fasted cohort was scanned before and after administration of a calorie-matched glucose and exogenous ketone ester (
              d
              -β-hydroxybutyrate) bolus. Across the life span, brain network destabilization correlated with decreased brain activity and cognitive acuity. Effects emerged at 47 y, with the most rapid degeneration occurring at 60 y. Networks were destabilized by glucose and stabilized by ketones, irrespective of whether ketosis was achieved with a ketogenic diet or exogenous ketone ester. Together, our results suggest that brain network destabilization may reflect early signs of hypometabolism, associated with dementia. Dietary interventions resulting in ketone utilization increase available energy and thus may show potential in protecting the aging brain.},
	pages = {6170--6177},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {Proc Natl Acad Sci {USA}},
	author = {Mujica-Parodi, Lilianne R. and Amgalan, Anar and Sultan, Syed Fahad and Antal, Botond and Sun, Xiaofei and Skiena, Steven and Lithen, Andrew and Adra, Noor and Ratai, Eva-Maria and Weistuch, Corey and Govindarajan, Sindhuja Tirumalai and Strey, Helmut H. and Dill, Ken A. and Stufflebeam, Steven M. and Veech, Richard L. and Clarke, Kieran},
	urldate = {2021-01-09},
	date = {2020-03-17},
	langid = {english},
	file = {Mujica-Parodi et al. - 2020 - Diet modulates brain network stability, a biomarke.pdf:/Users/bert/Zotero/storage/83YLE5LI/Mujica-Parodi et al. - 2020 - Diet modulates brain network stability, a biomarke.pdf:application/pdf},
}

@book{kjellander_thermodynamics_2015,
	edition = {1},
	title = {Thermodynamics Kept Simple: A Molecular Approach},
	isbn = {978-0-429-19437-5},
	url = {https://www.taylorfrancis.com/books/9781482244113},
	shorttitle = {Thermodynamics Kept Simple},
	publisher = {{CRC} Press},
	author = {Kjellander, Roland},
	urldate = {2021-01-09},
	date = {2015-08-28},
	langid = {english},
	doi = {10.1201/b18907},
	file = {Kjellander - 2015 - Thermodynamics Kept Simple A Molecular Approach.pdf:/Users/bert/Zotero/storage/B48BMYZ8/Kjellander - 2015 - Thermodynamics Kept Simple A Molecular Approach.pdf:application/pdf},
}

@book{dill_molecular_2011,
	edition = {2nd},
	title = {Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience},
	url = {https://www.routledge.com/Molecular-Driving-Forces-Statistical-Thermodynamics-in-Biology-Chemistry/Dill-Bromberg/p/book/9780815344308},
	shorttitle = {Molecular Driving Forces},
	abstract = {Molecular Driving Forces, Second Edition is an introductory statistical thermodynamics text that describes the principles and forces that drive chemical and biological processes. It demonstrates how the complex behaviors of molecules can result from a few simple physical processes, and how simple models provide surprisingly accurate insights into the workings of the molecular world. 

Widely adopted in its First Edition, Molecular Driving Forces is regarded by teachers and students as an accessi},
	publisher = {Garland Science},
	author = {Dill, Ken A. and Bromberg, Sarina},
	urldate = {2021-01-09},
	date = {2011},
	langid = {english},
	file = {Dill and Bromberg - 2011 - Molecular Driving Forces Statistical Thermodynami.pdf:/Users/bert/Zotero/storage/5R68SPIV/Dill and Bromberg - 2011 - Molecular Driving Forces Statistical Thermodynami.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ZIV49QUR/9780815344308.html:text/html;Solution Manual for Molecular Driving Forces Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience by Bromberg, Dill, Stigter (z-lib.org).pdf:/Users/bert/Zotero/storage/EUIQVESX/Solution Manual for Molecular Driving Forces Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience by Bromberg, Dill, Stigter (z-lib.org).pdf:application/pdf},
}

@inproceedings{loeliger_localizing_2009,
	title = {Localizing, forgetting, and likelihood filtering in state-space models},
	doi = {10.1109/ITA.2009.5044943},
	abstract = {The context of this paper are cycle-free factor graphs such as hidden Markov models or linear state space models. The paper offers some observations and suggestions on ldquolocalizatingrdquo such models and their likelihoods. First, it is suggested that a localized version of the model likelihood, which is easily computed by forward sum-product message passing, may be useful for feature extraction and detection. Second, the notion of a ldquolocalrdquo model (local factor graph) is introduced. A first class of local models arises from exponential message damping and scale factors as in recursive least squares. A second class of local models arises from the problem of estimating the moment of a model switch from some known model A to some known model B. This problem can be solved by forward sum-product message passing in model A and backward sum-product message passing in model B. It is pointed out that this method is applicable to pulse position estimation for any pulse with a (deterministic or stochastic) state space model.},
	pages = {184--186},
	booktitle = {Information Theory and Applications Workshop, 2009},
	author = {Loeliger, H.-A. and Bolliger, L. and Reller, Christoph and Korl, S.},
	date = {2009-02},
	keywords = {Message passing, filtering theory, likelihood filtering, backward sum-product message passing, Context modeling, cycle-free factor graphs, Damping, exponential message damping, feature extraction, filtering, forward sum-product message passing, graph theory, Hidden Markov models, Least squares methods, linear state space models, local factor graph, local model, model likelihood, model switch, moment estimation, Recursive least squares, State estimation, Switches},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/JWBBUFTX/abs_all.html:text/html;Loeliger et al. - 2009 - Localizing, forgetting, and likelihood filtering i.pdf:/Users/bert/Zotero/storage/4ITUEMY4/Loeliger et al. - 2009 - Localizing, forgetting, and likelihood filtering i.pdf:application/pdf},
}

@article{botvinick_planning_2012,
	title = {Planning as inference},
	volume = {16},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661312001957},
	doi = {10.1016/j.tics.2012.08.006},
	abstract = {Recent developments in decision-making research are bringing the topic of planning back to center stage in cognitive science. This renewed interest reopens an old, but still unanswered question: how exactly does planning happen? What are the underlying information processing operations and how are they implemented in the brain? Although a range of interesting possibilities exists, recent work has introduced a potentially transformative new idea, according to which planning is accomplished through probabilistic inference.},
	pages = {485--488},
	number = {10},
	journaltitle = {Trends in Cognitive Sciences},
	author = {Botvinick, Matthew and Toussaint, Marc},
	urldate = {2015-03-29},
	date = {2012-10},
	file = {Botvinick and Toussaint - 2012 - Planning as inference.pdf:/Users/bert/Zotero/storage/FSWCZIKG/Botvinick and Toussaint - 2012 - Planning as inference.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/NU7TXPXU/S1364661312001957.html:text/html},
}

@article{karny_approximate_2014,
	title = {Approximate Bayesian recursive estimation},
	volume = {285},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025514000966},
	doi = {10.1016/j.ins.2014.01.048},
	series = {Processing and Mining Complex Data Streams},
	abstract = {Bayesian learning provides a firm theoretical basis of the design and exploitation of algorithms in data-streams processing (preprocessing, change detection, hypothesis testing, clustering, etc.). Primarily, it relies on a recursive parameter estimation of a firmly bounded complexity. As a rule, it has to approximate the exact posterior probability density (pd), which comprises unreduced information about the estimated parameter. In the recursive treatment of the data stream, the latest approximate pd is usually updated using the treated parametric model and the newest data and then approximated. The fact that approximation errors may accumulate over time course is mostly neglected in the estimator design and, at most, checked ex post. The paper inspects the estimator design with respect to the error accumulation and concludes that a sort of forgetting (pd flattening) is an indispensable part of a reliable approximate recursive estimation. The conclusion results from a Bayesian problem formulation complemented by the minimum Kullback–Leibler divergence principle. Claims of the paper are supported by a straightforward analysis, by elaboration of the proposed estimator to widely applicable parametric models and illustrated numerically.},
	pages = {100--111},
	journaltitle = {Information Sciences},
	author = {Kárný, Miroslav},
	urldate = {2015-10-12},
	date = {2014-11},
	keywords = {Approximate parameter estimation, Bayesian recursive estimation, Forgetting, Kullback–Leibler divergence},
	file = {Kárný - 2014 - Approximate Bayesian recursive estimation.pdf:/Users/bert/Zotero/storage/ZJRWKWZF/Kárný - 2014 - Approximate Bayesian recursive estimation.pdf:application/pdf;Kárný - 2014 - Approximate Bayesian recursive estimation.pdf:/Users/bert/Zotero/storage/4LU7VYNK/Kárný - 2014 - Approximate Bayesian recursive estimation.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/JKGEBNPV/S0020025514000966.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/5R26JCDN/S0020025514000966.html:text/html},
}

@inproceedings{turner_state-space_2010,
	title = {State-space inference and learning with Gaussian processes},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_TurnerDR10.pdf},
	pages = {868--875},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Turner, Ryan D. and Deisenroth, Marc P. and Rasmussen, Carl E.},
	urldate = {2014-04-10},
	date = {2010},
	file = {Turner et al. - 2010 - State-space inference and learning with Gaussian p.pdf:/Users/bert/Zotero/storage/MBBNIWCS/Turner et al. - 2010 - State-space inference and learning with Gaussian p.pdf:application/pdf;Turner et al. - 2010 - State-space inference and learning with Gaussian p.pdf:/Users/bert/Zotero/storage/A6LG8RDQ/Turner et al. - 2010 - State-space inference and learning with Gaussian p.pdf:application/pdf},
}

@article{hennig_entropy_2011,
	title = {Entropy Search for Information-Efficient Global Optimization},
	url = {http://arxiv.org/abs/1112.1217},
	abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly adresses the decision problem of maximizing information gain from each evaluation.},
	journaltitle = {{arXiv}:1112.1217 [cs, stat]},
	author = {Hennig, Philipp and Schuler, Christian J.},
	urldate = {2014-04-11},
	date = {2011-12},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/DLJ79CR4/1112.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/6JMVWGY9/1112.html:text/html;Hennig and Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf:/Users/bert/Zotero/storage/ECUSG59W/Hennig and Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf:application/pdf;Hennig and Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf:/Users/bert/Zotero/storage/CPY387A7/Hennig and Schuler - 2011 - Entropy Search for Information-Efficient Global Op.pdf:application/pdf},
}

@article{huber_recursive_2014,
	title = {Recursive Gaussian process: On-line regression and learning},
	volume = {45},
	issn = {01678655},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0167865514000786},
	doi = {10.1016/j.patrec.2014.03.004},
	shorttitle = {Recursive Gaussian process},
	pages = {85--91},
	journaltitle = {Pattern Recognition Letters},
	author = {Huber, Marco F.},
	urldate = {2014-04-10},
	date = {2014-08},
	langid = {english},
	file = {Huber - 2014 - Recursive Gaussian process On-line regression and.pdf:/Users/bert/Zotero/storage/XPCJES3U/Huber - 2014 - Recursive Gaussian process On-line regression and.pdf:application/pdf;Huber - 2014 - Recursive Gaussian process On-line regression and.pdf:/Users/bert/Zotero/storage/SN8B449V/Huber - 2014 - Recursive Gaussian process On-line regression and.pdf:application/pdf},
}

@article{schwartenbeck_exploration_2013,
	title = {Exploration, novelty, surprise, and free energy minimization},
	volume = {4},
	issn = {1664-1078},
	url = {http://www.frontiersin.org/Journal/10.3389/fpsyg.2013.00710/full},
	doi = {10.3389/fpsyg.2013.00710},
	journaltitle = {Frontiers in Psychology},
	author = {Schwartenbeck, Philipp and {FitzGerald}, Thomas and Dolan, Raymond J. and Friston, Karl},
	urldate = {2014-04-10},
	date = {2013},
	file = {Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf:/Users/bert/Zotero/storage/ZG5J3S98/Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf:application/pdf;Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf:/Users/bert/Zotero/storage/8XGRMI47/Schwartenbeck et al. - 2013 - Exploration, novelty, surprise, and free energy mi.pdf:application/pdf},
}

@article{loeliger_introduction_2004,
	title = {An introduction to factor graphs},
	volume = {21},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1267047},
	pages = {28--41},
	number = {1},
	journaltitle = {Signal Processing Magazine, {IEEE}},
	author = {Loeliger, H.-A.},
	urldate = {2014-04-10},
	date = {2004},
	keywords = {factor graphs, sum-product algorithm},
	file = {Loeliger - 2004 - An introduction to factor graphs.pdf:/Users/bert/Zotero/storage/NLPD38WB/Loeliger - 2004 - An introduction to factor graphs.pdf:application/pdf},
}

@article{wingate_automated_2013,
	title = {Automated variational inference in probabilistic programming},
	url = {http://arxiv.org/abs/1301.1299},
	journaltitle = {{arXiv} preprint {arXiv}:1301.1299},
	author = {Wingate, David and Weber, Theophane},
	urldate = {2014-04-10},
	date = {2013},
	keywords = {variational Bayes},
	file = {Wingate and Weber - 2013 - Automated variational inference in probabilistic p.pdf:/Users/bert/Zotero/storage/CS8YH7ED/Wingate and Weber - 2013 - Automated variational inference in probabilistic p.pdf:application/pdf},
}

@article{yedidia_constructing_2005,
	title = {Constructing free-energy approximations and generalized belief propagation algorithms},
	volume = {51},
	issn = {0018-9448},
	doi = {10.1109/TIT.2005.850085},
	abstract = {Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation ({BP}) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that {BP} fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain region-based free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation ({GBP}) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a "valid" or "maxent-normal" approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the "Bethe method", the "junction graph method", the "cluster variation method", and the "region graph method". Finally, we explain how to tell whether a region-based approximation, and its corresponding {GBP} algorithm, is likely to be accurate, and describe empirical results showing that {GBP} can significantly outperform {BP}.},
	pages = {2282--2312},
	number = {7},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Yedidia, Jonathan S. and Freeman, W.T. and Weiss, Y.},
	date = {2005-07},
	keywords = {Message passing, message passing, Inference algorithms, factor graphs, graph theory, sum-product algorithm, Approximation algorithms, Artificial intelligence, backpropagation, belief networks, Belief propagation, Belief propagation ({BP}), Bethe approximation, Bethe free energy, cluster variation method, Clustering algorithms, Codes, Computer errors, Computer vision, free energy approximation, {GBP} algorithm, generalized belief propagation, generalized belief propagation ({GBP}), inference mechanisms, inference problem, junction graph method, Kikuchi free energy, Physics computing, Probability, region graph method, sum–product algorithm},
	file = {Yedidia et al. - 2005 - Constructing free-energy approximations and genera.pdf:/Users/bert/Zotero/storage/UVCYT3E4/Yedidia et al. - 2005 - Constructing free-energy approximations and genera.pdf:application/pdf},
}

@article{whiteley_attention_2012,
	title = {Attention in a Bayesian Framework},
	volume = {6},
	issn = {1662-5161},
	url = {http://www.frontiersin.org/Journal/10.3389/fnhum.2012.00100/full},
	doi = {10.3389/fnhum.2012.00100},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {Whiteley, Louise and Sahani, Maneesh},
	urldate = {2014-04-10},
	date = {2012},
	file = {Whiteley and Sahani - 2012 - Attention in a Bayesian Framework.pdf:/Users/bert/Zotero/storage/NLZI68LP/Whiteley and Sahani - 2012 - Attention in a Bayesian Framework.pdf:application/pdf},
}

@article{winn_variational_2005,
	title = {Variational Message Passing},
	volume = {6},
	url = {http://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=15324435&AN=18003399&h=hVptawIRxY9bpe0GokNI9%2BsC%2F7JSK74XN%2BztPcGsGNYZQY5RlIgU6Iy2WzbduRcHJXoqZMRPgfHZO18TF6s%2BKw%3D%3D&crl=c},
	number = {4},
	journaltitle = {Journal of Machine Learning Research},
	author = {Winn, John and Bishop, Christopher M. and Jaakkola, Tommi},
	urldate = {2014-04-10},
	date = {2005},
	keywords = {variational Bayes, factor graphs},
	file = {Winn et al. - 2005 - Variational Message Passing.pdf:/Users/bert/Zotero/storage/X63AMVU9/Winn et al. - 2005 - Variational Message Passing.pdf:application/pdf},
}

@article{yildiz_birdsong_2013,
	title = {From Birdsong to Human Speech Recognition: Bayesian Inference on a Hierarchy of Nonlinear Dynamical Systems},
	volume = {9},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1003219},
	doi = {10.1371/journal.pcbi.1003219},
	shorttitle = {From Birdsong to Human Speech Recognition},
	pages = {e1003219},
	number = {9},
	journaltitle = {{PLoS} Computational Biology},
	author = {Yildiz, Izzet B. and von Kriegstein, Katharina and Kiebel, Stefan J.},
	editor = {Jirsa, Viktor K.},
	urldate = {2014-04-10},
	date = {2013-09},
	langid = {english},
	file = {Yildiz et al. - 2013 - From Birdsong to Human Speech Recognition Bayesia.pdf:/Users/bert/Zotero/storage/IZJAJ87I/Yildiz et al. - 2013 - From Birdsong to Human Speech Recognition Bayesia.pdf:application/pdf},
}

@article{blei_build_2014,
	title = {Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models},
	volume = {1},
	url = {http://www.annualreviews.org/doi/abs/10.1146/annurev-statistics-022513-115657},
	doi = {10.1146/annurev-statistics-022513-115657},
	shorttitle = {Build, Compute, Critique, Repeat},
	abstract = {We survey latent variable models for solving data-analysis problems. A latent variable model is a probabilistic model that encodes hidden patterns in the data. We uncover these patterns from their conditional distribution and use them to summarize data and form predictions. Latent variable models are important in many fields, including computational biology, natural language processing, and social network analysis. Our perspective is that models are developed iteratively: We build a model, use it to analyze data, assess how it succeeds and fails, revise it, and repeat. We describe how new research has transformed these essential activities. First, we describe probabilistic graphical models, a language for formulating latent variable models. Second, we describe mean field variational inference, a generic algorithm for approximating conditional distributions. Third, we describe how to use our analyses to solve problems: exploring the data, forming predictions, and pointing us in the direction of improved models.},
	pages = {203--232},
	number = {1},
	journaltitle = {Annual Review of Statistics and Its Application},
	author = {Blei, David M.},
	urldate = {2014-04-12},
	date = {2014},
	file = {Blei - 2014 - Build, Compute, Critique, Repeat Data Analysis wi.pdf:/Users/bert/Zotero/storage/KG84JY48/Blei - 2014 - Build, Compute, Critique, Repeat Data Analysis wi.pdf:application/pdf},
}

@article{stevens_automated_2013,
	title = {An automated and reproducible workflow for running and analyzing neural simulations using Lancet and {IPython} Notebook},
	volume = {7},
	issn = {1662-5196},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3874632/},
	doi = {10.3389/fninf.2013.00044},
	abstract = {Lancet is a new, simulator-independent Python utility for succinctly specifying, launching, and collating results from large batches of interrelated computationally demanding program runs. This paper demonstrates how to combine Lancet with {IPython} Notebook to provide a flexible, lightweight, and agile workflow for fully reproducible scientific research. This informal and pragmatic approach uses {IPython} Notebook to capture the steps in a scientific computation as it is gradually automated and made ready for publication, without mandating the use of any separate application that can constrain scientific exploration and innovation. The resulting notebook concisely records each step involved in even very complex computational processes that led to a particular figure or numerical result, allowing the complete chain of events to be replicated automatically. Lancet was originally designed to help solve problems in computational neuroscience, such as analyzing the sensitivity of a complex simulation to various parameters, or collecting the results from multiple runs with different random starting points. However, because it is never possible to know in advance what tools might be required in future tasks, Lancet has been designed to be completely general, supporting any type of program as long as it can be launched as a process and can return output in the form of files. For instance, Lancet is also heavily used by one of the authors in a separate research group for launching batches of microprocessor simulations. This general design will allow Lancet to continue supporting a given research project even as the underlying approaches and tools change.},
	journaltitle = {Frontiers in Neuroinformatics},
	author = {Stevens, Jean-Luc R. and Elver, Marco and Bednar, James A.},
	urldate = {2014-04-17},
	date = {2013-12},
	pmid = {24416014},
	pmcid = {PMC3874632},
	file = {Stevens et al. - 2013 - An automated and reproducible workflow for running.pdf:/Users/bert/Zotero/storage/GBGICYEW/Stevens et al. - 2013 - An automated and reproducible workflow for running.pdf:application/pdf;Stevens et al. - 2013 - An automated and reproducible workflow for running.pdf:/Users/bert/Zotero/storage/FE2LUNWZ/Stevens et al. - 2013 - An automated and reproducible workflow for running.pdf:application/pdf},
}

@article{sun_framework_2012,
	title = {A framework for Bayesian optimality of psychophysical laws},
	volume = {56},
	issn = {00222496},
	url = {http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.elsevier-099c0473-a046-305a-8e71-67dab2abf2aa},
	doi = {10.1016/j.jmp.2012.08.002},
	pages = {495--501},
	number = {6},
	journaltitle = {Journal of Mathematical Psychology},
	author = {Sun, John Z. and Wang, Grace I. and Goyal, Vivek K and Varshney, Lav R.},
	urldate = {2014-04-25},
	date = {2012-12},
	langid = {english},
	file = {A framework for Bayesian optimality of psychophysical laws - Journal of Mathematical Psychology - Tom 56, Numer 6 (2012) - Biblioteka Nauki - Yadda:/Users/bert/Zotero/storage/JA3ESQFD/bwmeta1.element.html:text/html;A framework for Bayesian optimality of psychophysical laws - Journal of Mathematical Psychology - Tom 56, Numer 6 (2012) - Biblioteka Nauki - Yadda:/Users/bert/Zotero/storage/87PDVAXL/bwmeta1.element.html:text/html;Sun et al. - 2012 - A framework for Bayesian optimality of psychophysi.pdf:/Users/bert/Zotero/storage/G9HW4RNT/Sun et al. - 2012 - A framework for Bayesian optimality of psychophysi.pdf:application/pdf;Sun et al. - 2012 - A framework for Bayesian optimality of psychophysi.pdf:/Users/bert/Zotero/storage/2TR3SWAQ/Sun et al. - 2012 - A framework for Bayesian optimality of psychophysi.pdf:application/pdf},
}

@article{ostwald_tutorial_2014,
	title = {A tutorial on variational Bayes for latent linear stochastic time-series models},
	volume = {60},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249614000352},
	doi = {10.1016/j.jmp.2014.04.003},
	abstract = {Variational Bayesian methods for the identification of latent stochastic time-series models comprising both observed and unobserved random variables have recently gained momentum in machine learning, theoretical neuroscience, and neuroimaging methods development. Despite their established use as a computationally efficient alternative to sampling-based methods, their practical application in mathematical psychology has so far been limited. In this tutorial we attempt to provide an introductory overview of the theoretical underpinnings that the variational Bayesian approach to latent stochastic time-series models rests on by discussing its application in the linear case.},
	pages = {1--19},
	journaltitle = {Journal of Mathematical Psychology},
	author = {Ostwald, Dirk and Kirilina, Evgeniya and Starke, Ludger and Blankenburg, Felix},
	urldate = {2014-06-21},
	date = {2014-06},
	file = {Ostwald et al. - 2014 - A tutorial on variational Bayes for latent linear .pdf:/Users/bert/Zotero/storage/XVVF3K9D/Ostwald et al. - 2014 - A tutorial on variational Bayes for latent linear .pdf:application/pdf;Ostwald_et_al_R2_Supplement.pdf:/Users/bert/Zotero/storage/VSPE9SRG/Ostwald_et_al_R2_Supplement.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/2T66PQJ2/S0022249614000352.html:text/html},
}

@book{mackay_humble_2006,
	title = {The humble Gaussian distribution},
	author = {{MacKay}, David},
	date = {2006},
	file = {MacKay - 2006 - The humble Gaussian distribution.pdf:/Users/bert/Zotero/storage/ICANDHRK/MacKay - 2006 - The humble Gaussian distribution.pdf:application/pdf},
}

@book{sarkka_bayesian_2013,
	location = {London ; New York},
	title = {Bayesian Filtering and Smoothing},
	isbn = {978-0-415-55809-9},
	abstract = {Filtering and smoothing methods are used to produce an accurate estimate of the state of a time-varying system based on multiple observational inputs (data). Interest in these methods has exploded in recent years, with numerous applications emerging in fields such as navigation, aerospace engineering, telecommunications and medicine. This compact, informal introduction for graduate students and advanced undergraduates presents the current state-of-the-art filtering and smoothing methods in a unified Bayesian framework. Readers learn what non-linear Kalman filters and particle filters are, how they are related, and their relative advantages and disadvantages. They also discover how state-of-the-art Bayesian parameter estimation methods can be combined with state-of-the-art filtering and smoothing algorithms. The book's practical and algorithmic approach assumes only modest mathematical prerequisites. Examples include {MATLAB} computations, and the numerous end-of-chapter exercises include computational assignments. {MATLAB}/{GNU} Octave source code is available for download at www.cambridge.org/sarkka, promoting hands-on work with the methods.},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo},
	date = {2013-10},
	file = {Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:/Users/bert/Zotero/storage/KVQA6EDY/Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:application/pdf},
}

@inproceedings{huber_recursive_2013,
	title = {Recursive Gaussian process regression},
	doi = {10.1109/ICASSP.2013.6638281},
	abstract = {For large data sets, performing Gaussian process regression is computationally demanding or even intractable. If data can be processed sequentially, the recursive regression method proposed in this paper allows incorporating new data with constant computation time. For this purpose two operations are performed alternating on a fixed set of so-called basis vectors used for estimating the latent function: First, inference of the latent function at the new inputs. Second, utilization of the new data for updating the estimate. Numerical simulations show that the proposed approach significantly reduces the computation time and at the same time provides more accurate estimates compared to existing on-line and/or sparse Gaussian process regression approaches.},
	pages = {3362--3366},
	booktitle = {2013 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Huber, M.F.},
	date = {2013-05},
	keywords = {Bayes methods, Kalman filters, Training, Gaussian processes, inference, filtering theory, Vectors, basis vectors, Bayesian filtering, constant computation time, data utilization, Joints, Kernel, large data sets, latent function estimation, numerical simulations, on-line regression, recursive Gaussian process regression, recursive processing, Regression Analysis, Runtime, smoothing},
	file = {Huber - 2013 - Recursive Gaussian process regression.pdf:/Users/bert/Zotero/storage/G3GPR8IK/Huber - 2013 - Recursive Gaussian process regression.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/JQPF6B2A/abs_all.html:text/html},
}

@article{frigola_variational_2014,
	title = {Variational Gaussian Process State-Space Models},
	url = {http://arxiv.org/abs/1406.4905},
	abstract = {State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present extensions to stochastic variational inference and online learning.},
	journaltitle = {{arXiv}:1406.4905 [cs, stat]},
	author = {Frigola, Roger and Chen, Yutian and Rasmussen, Carl E.},
	urldate = {2014-07-11},
	date = {2014-06},
	keywords = {Statistics - Machine Learning, Computer Science - Robotics, Computer Science - Learning, Computer Science - Systems and Control},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/FGP3RF5U/1406.html:text/html;Frigola et al. - 2014 - Variational Gaussian Process State-Space Models.pdf:/Users/bert/Zotero/storage/VRMUDHNH/Frigola et al. - 2014 - Variational Gaussian Process State-Space Models.pdf:application/pdf},
}

@article{daunizeau_vba:_2014,
	title = {{VBA}: A Probabilistic Treatment of Nonlinear Models for Neurobiological and Behavioural Data},
	volume = {10},
	issn = {1553-734X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900378/},
	doi = {10.1371/journal.pcbi.1003441},
	shorttitle = {{VBA}},
	abstract = {This work is in line with an on-going effort tending toward a computational (quantitative and refutable) understanding of human neuro-cognitive processes. Many sophisticated models for behavioural and neurobiological data have flourished during the past decade. Most of these models are partly unspecified (i.e. they have unknown parameters) and nonlinear. This makes them difficult to peer with a formal statistical data analysis framework. In turn, this compromises the reproducibility of model-based empirical studies. This work exposes a software toolbox that provides generic, efficient and robust probabilistic solutions to the three problems of model-based analysis of empirical data: (i) data simulation, (ii) parameter estimation/model selection, and (iii) experimental design optimization.},
	number = {1},
	journaltitle = {{PLoS} Computational Biology},
	author = {Daunizeau, Jean and Adam, Vincent and Rigoux, Lionel},
	urldate = {2014-07-30},
	date = {2014-01},
	pmid = {24465198},
	pmcid = {PMC3900378},
	file = {Daunizeau et al. - 2014 - VBA A Probabilistic Treatment of Nonlinear Models.pdf:/Users/bert/Zotero/storage/KK3UEVPG/Daunizeau et al. - 2014 - VBA A Probabilistic Treatment of Nonlinear Models.pdf:application/pdf;Daunizeau et al. - 2014 - VBA A Probabilistic Treatment of Nonlinear Models.pdf:/Users/bert/Zotero/storage/DM3D9Y4I/Daunizeau et al. - 2014 - VBA A Probabilistic Treatment of Nonlinear Models.pdf:application/pdf},
}

@incollection{yedidia_idiosyncratic_2000,
	title = {An Idiosyncratic Journey Beyond Mean Field Theory},
	url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6281795},
	pages = {37--49},
	booktitle = {Advanced Mean Field Methods},
	author = {Yedidia, Jonathan S.},
	urldate = {2014-07-31},
	date = {2000},
	file = {Snapshot:/Users/bert/Zotero/storage/JMZTSAXK/login.html:text/html;Snapshot:/Users/bert/Zotero/storage/3RGUJTHF/login.html:text/html;Yedidia - 2000 - An Idiosyncratic Journey Beyond Mean Field Theory.pdf:/Users/bert/Zotero/storage/QWVFY3ZM/Yedidia - 2000 - An Idiosyncratic Journey Beyond Mean Field Theory.pdf:application/pdf;Yedidia - 2000 - An Idiosyncratic Journey Beyond Mean Field Theory.pdf:/Users/bert/Zotero/storage/EVJPGIQS/Yedidia - 2000 - An Idiosyncratic Journey Beyond Mean Field Theory.pdf:application/pdf},
}

@article{hernandez-lobato_predictive_2014,
	title = {Predictive Entropy Search for Efficient Global Optimization of Black-box Functions},
	url = {http://arxiv.org/abs/1406.2541},
	abstract = {We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search ({PES}). At each iteration, {PES} selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. {PES} codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows {PES} to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search ({ES}). Furthermore, {PES} can easily perform a fully Bayesian treatment of the model hyperparameters while {ES} cannot. We evaluate {PES} in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of {PES} leads to significant gains in optimization performance.},
	journaltitle = {{arXiv}:1406.2541 [cs, stat]},
	author = {Hernández-Lobato, José Miguel and Hoffman, Matthew W. and Ghahramani, Zoubin},
	urldate = {2014-10-16},
	date = {2014-06},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/K857L5GG/1406.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/BB2YATEI/1406.html:text/html;Hernández-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf:/Users/bert/Zotero/storage/P8GXVYT4/Hernández-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf:application/pdf;Hernández-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf:/Users/bert/Zotero/storage/D9YKDT7A/Hernández-Lobato et al. - 2014 - Predictive Entropy Search for Efficient Global Opt.pdf:application/pdf},
}

@incollection{moffitt_julia_2014,
	title = {julia},
	booktitle = {Seven More Languages in seven weeks},
	author = {Moffitt, Jack and Tate, Bruce},
	date = {2014},
	file = {Moffitt and Tate - 2014 - julia.pdf:/Users/bert/Zotero/storage/UKIG84F6/Moffitt and Tate - 2014 - julia.pdf:application/pdf;Moffitt and Tate - 2014 - julia.pdf:/Users/bert/Zotero/storage/66AWG3RZ/Moffitt and Tate - 2014 - julia.pdf:application/pdf},
}

@article{knuth_bayesian_2014,
	title = {Bayesian Evidence and Model Selection},
	url = {http://arxiv.org/abs/1411.3013},
	abstract = {In this paper we review the concept of the Bayesian evidence and its application to model selection. The theory is presented along with a discussion of analytic, approximate and numerical techniques. Application to several practical examples within the context of signal processing are discussed.},
	journaltitle = {{arXiv}:1411.3013 [astro-ph, stat]},
	author = {Knuth, Kevin H. and Habeck, Michael and Malakar, Nabin K. and Mubeen, Asim M. and Placek, Ben},
	urldate = {2014-12-01},
	date = {2014-11},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology, Astrophysics - Instrumentation and Methods for Astrophysics, Statistics - Applications},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/BUT9F6VG/1411.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/2GZU3PVV/1411.html:text/html;Knuth et al. - 2014 - Bayesian Evidence and Model Selection.pdf:/Users/bert/Zotero/storage/AQ2L95P8/Knuth et al. - 2014 - Bayesian Evidence and Model Selection.pdf:application/pdf;Knuth et al. - 2014 - Bayesian Evidence and Model Selection.pdf:/Users/bert/Zotero/storage/89LLWHYB/Knuth et al. - 2014 - Bayesian Evidence and Model Selection.pdf:application/pdf},
}

@article{vanlier_optimal_2014,
	title = {Optimal experiment design for model selection in biochemical networks},
	volume = {8},
	rights = {2014 Vanlier et al.; licensee {BioMed} Central Ltd.},
	issn = {1752-0509},
	url = {http://www.biomedcentral.com/1752-0509/8/20/abstract},
	doi = {10.1186/1752-0509-8-20},
	abstract = {Mathematical modeling is often used to formalize hypotheses on how a biochemical network operates by discriminating between competing models. Bayesian model selection offers a way to determine the amount of evidence that data provides to support one model over the other while favoring simple models. In practice, the amount of experimental data is often insufficient to make a clear distinction between competing models. Often one would like to perform a new experiment which would discriminate between competing hypotheses. {PMID}: 24555498},
	pages = {20},
	number = {1},
	journaltitle = {{BMC} Systems Biology},
	author = {Vanlier, Joep and Tiemann, Christian A. and Hilbers, Peter {AJ} and Riel, Natal {AW} van},
	urldate = {2014-11-30},
	date = {2014-02},
	langid = {english},
	pmid = {24555498},
	keywords = {inference, Bayes factor, Model selection, Model Selection, uncertainty},
	file = {Snapshot:/Users/bert/Zotero/storage/FMTDGH7U/20.html:text/html;Snapshot:/Users/bert/Zotero/storage/MY3UK4HJ/20.html:text/html;Vanlier et al. - 2014 - Optimal experiment design for model selection in b.pdf:/Users/bert/Zotero/storage/WHI7KXH5/Vanlier et al. - 2014 - Optimal experiment design for model selection in b.pdf:application/pdf;Vanlier et al. - 2014 - Optimal experiment design for model selection in b.pdf:/Users/bert/Zotero/storage/QAST7KKQ/Vanlier et al. - 2014 - Optimal experiment design for model selection in b.pdf:application/pdf;Vanlier et al. - 2014 - Supplement - Optimal Experimental Design:/Users/bert/Zotero/storage/7RDI84GS/Vanlier et al. - 2014 - Supplement - Optimal Experimental Design.pdf:application/pdf;Vanlier et al. - 2014 - Supplement - Optimal Experimental Design:/Users/bert/Zotero/storage/52ER96LK/Vanlier et al. - 2014 - Supplement - Optimal Experimental Design.pdf:application/pdf},
}

@article{daunizeau_optimizing_2011,
	title = {Optimizing Experimental Design for Comparing Models of Brain Function},
	volume = {7},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1002280},
	doi = {10.1371/journal.pcbi.1002280},
	abstract = {Author Summary During the past two decades, brain mapping research has undergone a paradigm switch. In addition to localizing brain regions that encode specific sensory, motor or cognitive processes, neuroimaging data is nowadays further exploited to ask questions about how information is transmitted through brain networks. The ambition here is to ask questions such as: “what is the nature of the information that region A passes on to region B”. This can be experimentally addressed by, e.g., showing that the influence that A exerts onto B depends upon specific sensory, motor or cognitive manipulations. This means one has to compare (in a statistical sense) candidate network models of the brain (with different modulations of effective connectivity, say), based on experimental data. The question we address here is how one should design the experiment in order to best discriminate such candidate models. We approach the problem from a statistical decision theoretical perspective, whereby the optimal design is the one that minimizes the model selection error rate. We demonstrate the approach using simulated and empirical data and show how it can be applied to any experimental question that can be framed as a model comparison problem.},
	pages = {e1002280},
	number = {11},
	journaltitle = {{PLoS} Comput Biol},
	author = {Daunizeau, Jean and Preuschoff, Kerstin and Friston, Karl and Stephan, Klaas},
	urldate = {2014-12-01},
	date = {2011-11},
	file = {Daunizeau et al. - 2011 - Optimizing Experimental Design for Comparing Model.pdf:/Users/bert/Zotero/storage/BFHHJCVE/Daunizeau et al. - 2011 - Optimizing Experimental Design for Comparing Model.pdf:application/pdf;PLoS Snapshot:/Users/bert/Zotero/storage/JE8WZS6V/infodoi10.1371journal.pcbi.html:text/html;PLoS Snapshot:/Users/bert/Zotero/storage/U2855T58/infodoi10.1371journal.pcbi.html:text/html},
}

@article{liepe_maximizing_2013,
	title = {Maximizing the Information Content of Experiments in Systems Biology},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1002888},
	doi = {10.1371/journal.pcbi.1002888},
	abstract = {Author {SummaryFor} most biological signalling and regulatory systems we still lack reliable mechanistic models. And where such models exist, e.g. in the form of differential equations, we typically have only rough estimates for the parameters that characterize the biochemical reactions. In order to improve our knowledge of such systems we require better estimates for these parameters and here we show how judicious choice of experiments, based on a combination of simulations and information theoretical analysis, can help us. Our approach builds on the available, frequently rudimentary information, and identifies which experimental set-up provides most additional information about all the parameters, or individual parameters. We will also consider the related but subtly different problem of which experiments need to be performed in order to decrease the uncertainty about the behaviour of the system under altered conditions. We develop the theoretical framework in the necessary detail before illustrating its use and applying it to the repressilator model, the regulation of Hes1 and signal transduction in the Akt pathway.},
	pages = {e1002888},
	number = {1},
	journaltitle = {{PLoS} Comput Biol},
	author = {Liepe, Juliane and Filippi, Sarah and Komorowski, Michał and Stumpf, Michael P. H.},
	urldate = {2014-11-30},
	date = {2013-01},
	file = {Liepe et al. - 2013 - Maximizing the Information Content of Experiments .pdf:/Users/bert/Zotero/storage/Y29HF95U/Liepe et al. - 2013 - Maximizing the Information Content of Experiments .pdf:application/pdf;Liepe et al. - 2013 - Maximizing the Information Content of Experiments .pdf:/Users/bert/Zotero/storage/VSFVQTUM/Liepe et al. - 2013 - Maximizing the Information Content of Experiments .pdf:application/pdf;PLoS Snapshot:/Users/bert/Zotero/storage/NPPBY9RU/infodoi10.1371journal.pcbi.html:text/html;PLoS Snapshot:/Users/bert/Zotero/storage/STLRWD8P/infodoi10.1371journal.pcbi.html:text/html},
}

@thesis{van_witteveen_adaptive_2014,
	title = {Adaptive Reinforcement Learning},
	abstract = {This thesis investigates the applicability of the Probabilistic Inference for Learning {COntrol} ({PILCO}) algorithm to large systems and systems with time varying measurement noise. {PILCO} is a state-of-the-art model-learning Reinforcement Learning ({RL}) algorithm that uses a Gaussian Process ({GP}) model to average over uncertainties during learning. Simulated case studies on a second-order system and a cart-pole system show that both the Radial Basis Function ({RBF}) controller and the {GP} controller ﬁnd good solutions when the number of basis functions is chosen correctly. However, when a high number of basis functions is selected, the {RBF} controller fails completely, while the {GP} controller is able ﬁnd a suboptimal solution. In order to reduce the computational time for large systems is the identiﬁcation of the {GP} model parallelized. For a four dimensional model the parallelization results in a 20 to 40 percent reduction of the identiﬁcation computational time. A simulated case study of a cart-pole system shows a strong decrease in performance when increasing the measurement noise variance or kurtosis. The controller is robust for changing skewness of the measurement noise. Furthermore is the variance of the measurement noise an important parameter, because it has to be selected as a ﬁxed parameter of the {GP} controller prior to learning. Therefore Adaptive-Probabilistic Inference for Learning {COntrol} (A-{PILCO}) is proposed. This is a framework that initiates a new learning process when the measurement noise variance exceeds its conﬁdence bounds. By reducing the computational time signiﬁcantly for large and/or complex systems and by implementing the A-{PILCO} framework the {PILCO} algorithm becomes applicable larger set of systems.},
	institution = {{TU} Delft},
	type = {{MSc} thesis},
	author = {Van Witteveen, Kees},
	date = {2014},
	file = {Van Witteveen - 2014 - Adaptive Reinforcement Learning.pdf:/Users/bert/Zotero/storage/E4EWAILD/Van Witteveen - 2014 - Adaptive Reinforcement Learning.pdf:application/pdf},
}

@article{englert_probabilistic_2013,
	title = {Probabilistic model-based imitation learning},
	volume = {21},
	issn = {1059-7123, 1741-2633},
	url = {http://adb.sagepub.com/content/21/5/388},
	doi = {10.1177/1059712313491614},
	abstract = {Efficient skill acquisition is crucial for creating versatile robots. One intuitive way to teach a robot new tricks is to demonstrate a task and enable the robot to imitate the demonstrated behavior. This approach is known as imitation learning. Classical methods of imitation learning, such as inverse reinforcement learning or behavioral cloning, suffer substantially from the correspondence problem when the actions (i.e. motor commands, torques or forces) of the teacher are not observed or the body of the teacher differs substantially, e.g., in the actuation. To address these drawbacks we propose to learn a robot-specific controller that directly matches robot trajectories with observed ones. We present a novel and robust probabilistic model-based approach for solving a probabilistic trajectory matching problem via policy search. For this purpose, we propose to learn a probabilistic model of the system, which we exploit for mental rehearsal of the current controller by making predictions about future trajectories. These internal simulations allow for learning a controller without permanently interacting with the real system, which results in a reduced overall interaction time. Using long-term predictions from this learned model, we train robot-specific controllers that reproduce the expert’s distribution of demonstrations without the need to observe motor commands during the demonstration. The strength of our approach is that it addresses the correspondence problem in a principled way. Our method achieves a higher learning speed than both model-based imitation learning based on dynamics motor primitives and trial-and-error-based learning systems with hand-crafted cost functions. We successfully applied our approach to imitating human behavior using a tendon-driven compliant robotic arm. Moreover, we demonstrate the generalization ability of our approach in a multi-task learning setup.},
	pages = {388--403},
	number = {5},
	journaltitle = {Adaptive Behavior},
	author = {Englert, Peter and Paraschos, Alexandros and Deisenroth, Marc Peter and Peters, Jan},
	urldate = {2014-08-28},
	date = {2013-10},
	langid = {english},
	file = {Englert et al. - 2013 - Probabilistic model-based imitation learning.pdf:/Users/bert/Zotero/storage/D792CVNH/Englert et al. - 2013 - Probabilistic model-based imitation learning.pdf:application/pdf;Englert et al. - 2013 - Probabilistic model-based imitation learning.pdf:/Users/bert/Zotero/storage/49IU9HXJ/Englert et al. - 2013 - Probabilistic model-based imitation learning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ZYER3SPW/388.html:text/html;Snapshot:/Users/bert/Zotero/storage/RHYR9MGG/388.html:text/html},
}

@article{mathys_uncertainty_2014,
	title = {Uncertainty in perception and the Hierarchical Gaussian Filter},
	volume = {8},
	url = {http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00825/full},
	doi = {10.3389/fnhum.2014.00825},
	abstract = {In its full sense, perception rests on an agent's model of how its sensory input comes about and the inferences it draws based on this model. These inferences are necessarily uncertain. Here, we illustrate how the Hierarchical Gaussian Filter ({HGF}) offers a principled and generic way to deal with the several forms that uncertainty in perception takes. The {HGF} is a recent derivation of one-step update equations from Bayesian principles that rests on a hierarchical generative model of the environment and its (in)stability. It is computationally highly efficient, allows for online estimates of hidden states, and has found numerous applications to experimental data from human subjects. In this paper, we generalize previous descriptions of the {HGF} and its account of perceptual uncertainty. First, we explicitly formulate the extension of the {HGF}'s hierarchy to any number of levels; second, we discuss how various forms of uncertainty are accommodated by the minimization of variational free energy as encoded in the update equations; third, we combine the {HGF} with decision models and demonstrate the inversion of this combination; finally, we report a simulation study that compared four optimization methods for inverting the {HGF}/decision model combination at different noise levels. These four methods (Nelder–Mead simplex algorithm, Gaussian process-based global optimization, variational Bayes and Markov chain Monte Carlo sampling) all performed well even under considerable noise, with variational Bayes offering the best combination of efficiency and informativeness of inference. Our results demonstrate that the {HGF} provides a principled, flexible, and efficient—but at the same time intuitive—framework for the resolution of perceptual uncertainty in behaving agents.},
	pages = {825},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {Mathys, Christoph D. and Lomakina, Ekaterina I. and Daunizeau, Jean and Iglesias, Sandra and Brodersen, Kay H. and Friston, Karl J. and Stephan, Klaas E.},
	urldate = {2014-11-30},
	date = {2014},
	keywords = {free energy, Bayesian inference, decision-making, filtering, uncertainty, hierarchical modeling, volatility},
	file = {Mathys et al. - 2014 - Uncertainty in perception and the Hierarchical Gau.pdf:/Users/bert/Zotero/storage/3T4XLGCY/Mathys et al. - 2014 - Uncertainty in perception and the Hierarchical Gau.pdf:application/pdf},
}

@book{motlik_efficiency_2014,
	title = {Efficiency in Development Workflows},
	url = {http://codeship.io},
	publisher = {ebook by codeship},
	author = {Motlik, Florian},
	date = {2014},
	file = {Motlik - 2014 - Efficiency in Development Workflows.pdf:/Users/bert/Zotero/storage/JFHREB4N/Motlik - 2014 - Efficiency in Development Workflows.pdf:application/pdf},
}

@book{moldovan_conjugate_2010,
	title = {The Conjugate Prior for the Normal Distribution},
	abstract = {Scribe from Stat260: Bayesian Modelling and Inference - Michael Jordan class},
	author = {Moldovan, Teodor},
	date = {2010},
	file = {Moldovan - 2010 - The Conjugate Prior for the Normal Distribution.pdf:/Users/bert/Zotero/storage/4WWBFMC6/Moldovan - 2010 - The Conjugate Prior for the Normal Distribution.pdf:application/pdf},
}

@article{barto_novelty_2013,
	title = {Novelty or Surprise?},
	volume = {4},
	issn = {1664-1078},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3858647/},
	doi = {10.3389/fpsyg.2013.00907},
	abstract = {Novelty and surprise play significant roles in animal behavior and in attempts to understand the neural mechanisms underlying it. They also play important roles in technology, where detecting observations that are novel or surprising is central to many applications, such as medical diagnosis, text processing, surveillance, and security. Theories of motivation, particularly of intrinsic motivation, place novelty and surprise among the primary factors that arouse interest, motivate exploratory or avoidance behavior, and drive learning. In many of these studies, novelty and surprise are not distinguished from one another: the words are used more-or-less interchangeably. However, while undeniably closely related, novelty and surprise are very different. The purpose of this article is first to highlight the differences between novelty and surprise and to discuss how they are related by presenting an extensive review of mathematical and computational proposals related to them, and then to explore the implications of this for understanding behavioral and neuroscience data. We argue that opportunities for improved understanding of behavior and its neural basis are likely being missed by failing to distinguish between novelty and surprise.},
	journaltitle = {Frontiers in Psychology},
	author = {Barto, Andrew and Mirolli, Marco and Baldassarre, Gianluca},
	urldate = {2014-06-24},
	date = {2013-12},
	pmid = {24376428},
	pmcid = {PMC3858647},
	file = {Barto et al. - 2013 - Novelty or Surprise.pdf:/Users/bert/Zotero/storage/MSB8LSR9/Barto et al. - 2013 - Novelty or Surprise.pdf:application/pdf},
}

@article{hines_determination_2014,
	title = {Determination of parameter identifiability in nonlinear biophysical models: A Bayesian approach},
	volume = {143},
	issn = {0022-1295, 1540-7748},
	url = {http://jgp.rupress.org/content/143/3/401},
	doi = {10.1085/jgp.201311116},
	shorttitle = {Determination of parameter identifiability in nonlinear biophysical models},
	abstract = {A major goal of biophysics is to understand the physical mechanisms of biological molecules and systems. Mechanistic models are evaluated based on their ability to explain carefully controlled experiments. By fitting models to data, biophysical parameters that cannot be measured directly can be estimated from experimentation. However, it might be the case that many different combinations of model parameters can explain the observations equally well. In these cases, the model parameters are not identifiable: the experimentation has not provided sufficient constraining power to enable unique estimation of their true values. We demonstrate that this pitfall is present even in simple biophysical models. We investigate the underlying causes of parameter non-identifiability and discuss straightforward methods for determining when parameters of simple models can be inferred accurately. However, for models of even modest complexity, more general tools are required to diagnose parameter non-identifiability. We present a method based in Bayesian inference that can be used to establish the reliability of parameter estimates, as well as yield accurate quantification of parameter confidence.},
	pages = {401--416},
	number = {3},
	journaltitle = {The Journal of General Physiology},
	author = {Hines, Keegan E. and Middendorf, Thomas R. and Aldrich, Richard W.},
	urldate = {2014-07-01},
	date = {2014-03},
	langid = {english},
	pmid = {24516188},
	file = {Hines et al. - 2014 - Determination of parameter identifiability in nonl.pdf:/Users/bert/Zotero/storage/UUDTKYY4/Hines et al. - 2014 - Determination of parameter identifiability in nonl.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/IS5EQAYX/401.html:text/html;Snapshot:/Users/bert/Zotero/storage/RZEL9X4R/401.html:text/html},
}

@article{wilson_software_2013,
	title = {Software Carpentry: Lessons Learned},
	url = {http://arxiv.org/abs/1307.5448},
	shorttitle = {Software Carpentry},
	abstract = {Over the last 15 years, Software Carpentry has evolved from a week-long training course at the {US} national laboratories into a worldwide volunteer effort to raise standards in scientific computing. This article explains what we have learned along the way the challenges we now face, and our plans for the future.},
	journaltitle = {{arXiv}:1307.5448 [physics]},
	author = {Wilson, Greg},
	urldate = {2014-07-03},
	date = {2013-07},
	keywords = {Computer Science - Computers and Society, Computer Science - General Literature, Physics - Physics Education},
	file = {Wilson - 2013 - Software Carpentry Lessons Learned.pdf:/Users/bert/Zotero/storage/YZ9ID7PB/Wilson - 2013 - Software Carpentry Lessons Learned.pdf:application/pdf},
}

@article{elfwing_scaled_2013,
	title = {Scaled free-energy based reinforcement learning for robust and efficient learning in high-dimensional state spaces},
	volume = {7},
	url = {http://journal.frontiersin.org/Journal/10.3389/fnbot.2013.00003/abstract},
	doi = {10.3389/fnbot.2013.00003},
	abstract = {Free-energy based reinforcement learning ({FERL}) was proposed for learning in high-dimensional state- and action spaces, which cannot be handled by standard function approximation methods. In this study, we propose a scaled version of free-energy based reinforcement learning to achieve more robust and more efficient learning performance. The action-value function is approximated by the negative free-energy of a restricted Boltzmann machine, divided by a constant scaling factor that is related to the size of the Boltzmann machine (the square root of the number of state nodes in this study). Our first task is a digit floor gridworld task, where the states are represented by images of handwritten digits from the {MNIST} data set. The purpose of the task is to investigate the proposed method's ability, through the extraction of task-relevant features in the hidden layer, to cluster images of the same digit and to cluster images of different digits that corresponds to states with the same optimal action. We also test the method's robustness with respect to different exploration schedules, i.e., different settings of the initial temperature and the temperature discount rate in softmax action selection. Our second task is a robot visual navigation task, where the robot can learn its position by the different colors of the lower part of four landmarks and it can infer the correct corner goal area by the color of the upper part of the landmarks. The state space consists of binarized camera images with, at most, nine different colors, which is equal to 6642 binary states. For both tasks, the learning performance is compared with standard {FERL} and with function approximation where the action-value function is approximated by a two-layered feedforward neural network.},
	pages = {3},
	journaltitle = {Frontiers in Neurorobotics},
	author = {Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
	urldate = {2014-09-09},
	date = {2013},
	keywords = {free-energy, function approximation, reinforcement learning, restricted Boltzmann machine, robot navigation},
	file = {Elfwing et al. - 2013 - Scaled free-energy based reinforcement learning fo.pdf:/Users/bert/Zotero/storage/7WV7UVKE/Elfwing et al. - 2013 - Scaled free-energy based reinforcement learning fo.pdf:application/pdf},
}

@article{fitzgerald_model_2014,
	title = {Model averaging, optimal inference, and habit formation},
	volume = {8},
	url = {http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00457/abstract},
	doi = {10.3389/fnhum.2014.00457},
	abstract = {Postulating that the brain performs approximate Bayesian inference generates principled and empirically testable models of neuronal function—the subject of much current interest in neuroscience and related disciplines. Current formulations address inference and learning under some assumed and particular model. In reality, organisms are often faced with an additional challenge—that of determining which model or models of their environment are the best for guiding behavior. Bayesian model averaging—which says that an agent should weight the predictions of different models according to their evidence—provides a principled way to solve this problem. Importantly, because model evidence is determined by both the accuracy and complexity of the model, optimal inference requires that these be traded off against one another. This means an agent's behavior should show an equivalent balance. We hypothesize that Bayesian model averaging plays an important role in cognition, given that it is both optimal and realizable within a plausible neuronal architecture. We outline model averaging and how it might be implemented, and then explore a number of implications for brain and behavior. In particular, we propose that model averaging can explain a number of apparently suboptimal phenomena within the framework of approximate (bounded) Bayesian inference, focusing particularly upon the relationship between goal-directed and habitual behavior.},
	pages = {457},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {{FitzGerald}, Thomas H. B. and Dolan, Raymond J. and Friston, Karl},
	urldate = {2014-08-25},
	date = {2014},
	keywords = {active inference, Bayesian inference, Active Inference, habit, interference effect, predictive coding},
	file = {Data Sheet 1.DOCX:/Users/bert/Zotero/storage/9HFCS2XP/Data Sheet 1.DOCX:application/vnd.openxmlformats-officedocument.wordprocessingml.document;Data Sheet 1.DOCX:/Users/bert/Zotero/storage/67UFZU2K/Data Sheet 1.DOCX:application/vnd.openxmlformats-officedocument.wordprocessingml.document;FitzGerald et al. - 2014 - Model averaging, optimal inference, and habit form.pdf:/Users/bert/Zotero/storage/PKWVUYDI/FitzGerald et al. - 2014 - Model averaging, optimal inference, and habit form.pdf:application/pdf;FitzGerald et al. - 2014 - Model averaging, optimal inference, and habit form.pdf:/Users/bert/Zotero/storage/B3HMCQWI/FitzGerald et al. - 2014 - Model averaging, optimal inference, and habit form.pdf:application/pdf},
}

@article{mumford_computational_1992,
	title = {On the computational architecture of the neocortex. {II}. The role of cortico-cortical loops},
	volume = {66},
	issn = {0340-1200},
	abstract = {This paper is a sequel to an earlier paper which proposed an active role for the thalamus, integrating multiple hypotheses formed in the cortex via the thalamo-cortical loop. In this paper, I put forward a hypothesis on the role of the reciprocal, topographic pathways between two cortical areas, one often a 'higher' area dealing with more abstract information about the world, the other 'lower', dealing with more concrete data. The higher area attempts to fit its abstractions to the data it receives from lower areas by sending back to them from its deep pyramidal cells a template reconstruction best fitting the lower level view. The lower area attempts to reconcile the reconstruction of its view that it receives from higher areas with what it knows, sending back from its superficial pyramidal cells the features in its data which are not predicted by the higher area. The whole calculation is done with all areas working simultaneously, but with order imposed by synchronous activity in the various top-down, bottom-up loops. Evidence for this theory is reviewed and experimental tests are proposed. A third part of this paper will deal with extensions of these ideas to the frontal lobe.},
	pages = {241--251},
	number = {3},
	journaltitle = {Biological Cybernetics},
	author = {Mumford, D.},
	date = {1992},
	pmid = {1540675},
	keywords = {Animals, Humans, Cerebral Cortex, Cybernetics, Models, Neurological, Neurons, Pyramidal Tracts, Synapses},
	file = {Mumford - 1992 - On the computational architecture of the neocortex.pdf:/Users/bert/Zotero/storage/Y6IXL6XP/Mumford - 1992 - On the computational architecture of the neocortex.pdf:application/pdf;Mumford - 1992 - On the computational architecture of the neocortex.pdf:/Users/bert/Zotero/storage/X3FSFVIY/Mumford - 1992 - On the computational architecture of the neocortex.pdf:application/pdf},
}

@article{deisenroth_gaussian_2014,
	title = {Gaussian Processes for Data-Efficient Learning in Robotics and Control},
	issn = {0162-8828, 2160-9292},
	url = {http://www.computer.org/portal/web/csdl2/home/-/csdl/trans/tp/preprint/06654139-abs.html},
	doi = {10.1109/TPAMI.2013.218},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
	urldate = {2014-08-28},
	date = {2014},
	file = {Deisenroth et al. - 2014 - Gaussian Processes for Data-Efficient Learning in .pdf:/Users/bert/Zotero/storage/KIDTE888/Deisenroth et al. - 2014 - Gaussian Processes for Data-Efficient Learning in .pdf:application/pdf;Deisenroth et al. - 2014 - Gaussian Processes for Data-Efficient Learning in .pdf:/Users/bert/Zotero/storage/J4HXMFYX/Deisenroth et al. - 2014 - Gaussian Processes for Data-Efficient Learning in .pdf:application/pdf;Gaussian Processes for Data-Efficient Learning in Robotics and Control:/Users/bert/Zotero/storage/EXGS99UL/06654139-abs.html:text/html;Gaussian Processes for Data-Efficient Learning in Robotics and Control:/Users/bert/Zotero/storage/T6WLLULS/06654139-abs.html:text/html},
}

@article{conant_every_1970,
	title = {Every good regulator of a system must be a model of that system},
	abstract = {The design of a complex regulator often includes the making of a model of the system to be regulated. The making of such a model has hitherto been regarded as optional, as merely one of many possible ways. m this paper a theorem is presented which shows, under very broad conditions, that any regulator that is maximally both successful and simple must be isomorphic with the system being regulated. (The exact assumptions are given.) Making a model is thus necessary. The theorem has the interesting corollary that the living brain, so far as it is to be successful and efficient as a regulator for survival, must proceed, in learning, by the formation of a model (or models) of its environment. 1.},
	pages = {89--97},
	journaltitle = {Intl. J. Systems Science},
	author = {Conant, Roger C. and Ashby, W. Ross},
	date = {1970},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/RMKS39SM/summary.html:text/html;Conant and Ashby - 1970 - Every good regulator of a system must be a model o.pdf:/Users/bert/Zotero/storage/34RM7B6T/Conant and Ashby - 1970 - Every good regulator of a system must be a model o.pdf:application/pdf},
}

@article{bitzer_perceptual_2014,
	title = {Perceptual decision making: drift-diffusion model is equivalent to a Bayesian model},
	volume = {8},
	url = {http://journal.frontiersin.org/Journal/10.3389/fnhum.2014.00102/abstract},
	doi = {10.3389/fnhum.2014.00102},
	shorttitle = {Perceptual decision making},
	abstract = {Behavioral data obtained with perceptual decision making experiments are typically analyzed with the drift-diffusion model. This parsimonious model accumulates noisy pieces of evidence toward a decision bound to explain the accuracy and reaction times of subjects. Recently, Bayesian models have been proposed to explain how the brain extracts information from noisy input as typically presented in perceptual decision making tasks. It has long been known that the drift-diffusion model is tightly linked with such functional Bayesian models but the precise relationship of the two mechanisms was never made explicit. Using a Bayesian model, we derived the equations which relate parameter values between these models. In practice we show that this equivalence is useful when fitting multi-subject data. We further show that the Bayesian model suggests different decision variables which all predict equal responses and discuss how these may be discriminated based on neural correlates of accumulated evidence. In addition, we discuss extensions to the Bayesian model which would be difficult to derive for the drift-diffusion model. We suggest that these and other extensions may be highly useful for deriving new experiments which test novel hypotheses.},
	pages = {102},
	journaltitle = {Frontiers in Human Neuroscience},
	author = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel, Stefan J.},
	urldate = {2014-10-05},
	date = {2014},
	keywords = {uncertainty, Bayesian models, decision variable, drift diffusion model, parameter fitting, perceptual decision making, reaction time},
	file = {Bitzer et al. - 2014 - Perceptual decision making drift-diffusion model .pdf:/Users/bert/Zotero/storage/7SH2LKWF/Bitzer et al. - 2014 - Perceptual decision making drift-diffusion model .pdf:application/pdf},
}

@inproceedings{mcintyre_thinking_2007,
	title = {On thinking probabilistically},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.145.6184&rep=rep1&type=pdf},
	pages = {153--161},
	booktitle = {Extreme Events (Proceedings of the 15th Aha Hulikoa Workshop), {SOEST}, Unuiversity of Hawaii},
	publisher = {Citeseer},
	author = {{McIntyre}, Michael E.},
	urldate = {2014-04-12},
	date = {2007},
	file = {Mcintyre - 2007 - On thinking probabilistically.pdf:/Users/bert/Zotero/storage/ZWPWN9A8/Mcintyre - 2007 - On thinking probabilistically.pdf:application/pdf;Mcintyre - 2007 - On thinking probabilistically.pdf:/Users/bert/Zotero/storage/ICAPAQGB/Mcintyre - 2007 - On thinking probabilistically.pdf:application/pdf},
}

@article{coelho_bayesian_2011,
	title = {A Bayesian Framework for Parameter Estimation in Dynamical Models},
	volume = {6},
	url = {http://dx.doi.org/10.1371/journal.pone.0019616},
	doi = {10.1371/journal.pone.0019616},
	abstract = {Mathematical models in biology are powerful tools for the study and exploration of complex dynamics. Nevertheless, bringing theoretical results to an agreement with experimental observations involves acknowledging a great deal of uncertainty intrinsic to our theoretical representation of a real system. Proper handling of such uncertainties is key to the successful usage of models to predict experimental or field observations. This problem has been addressed over the years by many tools for model calibration and parameter estimation. In this article we present a general framework for uncertainty analysis and parameter estimation that is designed to handle uncertainties associated with the modeling of dynamic biological systems while remaining agnostic as to the type of model used. We apply the framework to fit an {SIR}-like influenza transmission model to 7 years of incidence data in three European countries: Belgium, the Netherlands and Portugal.},
	pages = {e19616},
	number = {5},
	journaltitle = {{PLoS} {ONE}},
	author = {Coelho, Flávio Codeço and Codeço, Cláudia Torres and Gomes, M. Gabriela M.},
	urldate = {2014-07-01},
	date = {2011-05},
	file = {Coelho et al. - 2011 - A Bayesian Framework for Parameter Estimation in D.pdf:/Users/bert/Zotero/storage/B3INAEN8/Coelho et al. - 2011 - A Bayesian Framework for Parameter Estimation in D.pdf:application/pdf;PLoS Snapshot:/Users/bert/Zotero/storage/NNQF95BH/infodoi10.1371journal.pone.html:text/html;PLoS Snapshot:/Users/bert/Zotero/storage/EYUHZ76X/infodoi10.1371journal.pone.html:text/html},
}

@unpublished{stone_eyes_2008,
	title = {Eyes, Flies, and Information Theory},
	url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.210.725},
	abstract = {2 Information: What is it good for? 4},
	author = {Stone, {JV}},
	date = {2008},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/XXED53N3/summary.html:text/html;Citeseer - Snapshot:/Users/bert/Zotero/storage/46FLDBUV/summary.html:text/html;Stone - 2008 - Eyes, Flies, and Information Theory.pdf:/Users/bert/Zotero/storage/QT2ILLN6/Stone - 2008 - Eyes, Flies, and Information Theory.pdf:application/pdf;Stone - 2008 - Eyes, Flies, and Information Theory.pdf:/Users/bert/Zotero/storage/FK9N9TCC/Stone - 2008 - Eyes, Flies, and Information Theory.pdf:application/pdf},
}

@inproceedings{deisenroth_pilco:_2011,
	title = {{PILCO}: A Model-Based and Data-Efficient Approach to Policy Search},
	shorttitle = {{PILCO}},
	abstract = {In this paper, we introduce pilco, a practical, data-efficient model-based policy search method. Pilco reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, pilco can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-ofthe-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks. 1. Introduction and Related},
	booktitle = {In Proceedings of the International Conference on Machine Learning},
	author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
	date = {2011},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/T99N7XJB/summary.html:text/html;Citeseer - Snapshot:/Users/bert/Zotero/storage/CW4MU53Z/summary.html:text/html;Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf:/Users/bert/Zotero/storage/NVSRANST/Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf:application/pdf;Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf:/Users/bert/Zotero/storage/NH92HSY8/Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf:application/pdf},
}

@incollection{luttinen_linear_2014,
	title = {Linear State-Space Model with Time-Varying Dynamics},
	rights = {©2014 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-662-44850-2 978-3-662-44851-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-44851-9_22},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper introduces a linear state-space model with time-varying dynamics. The time dependency is obtained by forming the state dynamics matrix as a time-varying linear combination of a set of matrices. The time dependency of the weights in the linear combination is modelled by another linear Gaussian dynamical model allowing the model to learn how the dynamics of the process changes. Previous approaches have used switching models which have a small set of possible state dynamics matrices and the model selects one of those matrices at each time, thus jumping between them. Our model forms the dynamics as a linear combination and the changes can be smooth and more continuous. The model is motivated by physical processes which are described by linear partial differential equations whose parameters vary in time. An example of such a process could be a temperature field whose evolution is driven by a varying wind direction. The posterior inference is performed using variational Bayesian approximation. The experiments on stochastic advection-diffusion processes and real-world weather processes show that the model with time-varying dynamics can outperform previously introduced approaches.},
	pages = {338--353},
	number = {8725},
	booktitle = {Machine Learning and Knowledge Discovery in Databases},
	publisher = {Springer Berlin Heidelberg},
	author = {Luttinen, Jaakko and Raiko, Tapani and Ilin, Alexander},
	editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
	urldate = {2014-10-21},
	date = {2014-01},
	langid = {english},
	keywords = {Artificial Intelligence (incl. Robotics), Data Mining and Knowledge Discovery, Information Storage and Retrieval, Pattern Recognition},
	file = {Luttinen et al. - 2014 - Linear State-Space Model with Time-Varying Dynamic.pdf:/Users/bert/Zotero/storage/33ZP9BML/Luttinen et al. - 2014 - Linear State-Space Model with Time-Varying Dynamic.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/8GPJ4ZLZ/978-3-662-44851-9_22.html:text/html},
}

@article{velimir_m._ilic_entropy_2009,
	title = {The Entropy Message Passing: A New Algorithm Over Factor Graphs},
	shorttitle = {The Entropy Message Passing},
	author = {Velimir M. Ilic, Miomir S. Stankovic},
	date = {2009},
	file = {The Entropy Message Passing\: A New Algorithm Over Factor Graphs - ResearchGate:/Users/bert/Zotero/storage/KH4B793N/45857027_The_Entropy_Message_Passing_A_New_Algorithm_Over_Factor_Graphs.html:text/html;The Entropy Message Passing\: A New Algorithm Over Factor Graphs - ResearchGate:/Users/bert/Zotero/storage/4PC5GS7S/45857027_The_Entropy_Message_Passing_A_New_Algorithm_Over_Factor_Graphs.html:text/html;Velimir M. Ilic - 2009 - The Entropy Message Passing A New Algorithm Over .pdf:/Users/bert/Zotero/storage/LH44SFH3/Velimir M. Ilic - 2009 - The Entropy Message Passing A New Algorithm Over .pdf:application/pdf;Velimir M. Ilic - 2009 - The Entropy Message Passing A New Algorithm Over .pdf:/Users/bert/Zotero/storage/H28RNNEZ/Velimir M. Ilic - 2009 - The Entropy Message Passing A New Algorithm Over .pdf:application/pdf},
}

@article{liang_generative_2013,
	title = {A Generative Product-of-Filters Model of Audio},
	url = {http://arxiv.org/abs/1312.5857},
	abstract = {We propose the product-of-filters ({PoF}) model, a generative model that decomposes audio spectra as sparse linear combinations of "filters" in the log-spectral domain. {PoF} makes similar assumptions to those used in the classic homomorphic filtering approach to signal processing, but replaces hand-designed decompositions built of basic signal processing operations with a learned decomposition based on statistical inference. This paper formulates the {PoF} model and derives a mean-field method for posterior inference and a variational {EM} algorithm to estimate the model's free parameters. We demonstrate {PoF}'s potential for audio processing on a bandwidth expansion task, and show that {PoF} can serve as an effective unsupervised feature extractor for a speaker identification task.},
	journaltitle = {{arXiv}:1312.5857 [cs, stat]},
	author = {Liang, Dawen and Hoffman, Matthew D. and Mysore, Gautham J.},
	urldate = {2014-10-16},
	date = {2013-12},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XWLE7REH/1312.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/LIUN8C3C/1312.html:text/html;Liang et al. - 2013 - A Generative Product-of-Filters Model of Audio.pdf:/Users/bert/Zotero/storage/ZCKS98EL/Liang et al. - 2013 - A Generative Product-of-Filters Model of Audio.pdf:application/pdf;Liang et al. - 2013 - A Generative Product-of-Filters Model of Audio.pdf:/Users/bert/Zotero/storage/S5HTFGCV/Liang et al. - 2013 - A Generative Product-of-Filters Model of Audio.pdf:application/pdf},
}

@article{riegler_merging_2013,
	title = {Merging Belief Propagation and the Mean Field Approximation: A Free Energy Approach},
	volume = {59},
	issn = {0018-9448, 1557-9654},
	url = {http://vbn.aau.dk/en/publications/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336e-4db1-9e17-b5918181727d).html},
	doi = {10.1109/TIT.2012.2218573},
	shorttitle = {Merging Belief Propagation and the Mean Field Approximation},
	pages = {588--602},
	number = {1},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Riegler, Erwin and Kirkelund, Gunvor Elisabeth and Manchon, Carles Navarro and Badiu, Mihai-Alin and Fleury, Bernard Henri},
	urldate = {2014-04-19},
	date = {2013-01},
	file = {Merging Belief Propagation and the Mean Field Approximation\: A Free Energy Approach - Research - Aalborg University:/Users/bert/Zotero/storage/M2G34QLT/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Merging Belief Propagation and the Mean Field Approximation\: A Free Energy Approach - Research - Aalborg University:/Users/bert/Zotero/storage/BXWWTIW3/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Merging Belief Propagation and the Mean Field Approximation\: A Free Energy Approach - Research - Aalborg University:/Users/bert/Zotero/storage/M3YDHFLM/merging-belief-propagation-and-the-mean-field-approximation-a-free-energy-approach(933cdedc-336.html:text/html;Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:/Users/bert/Zotero/storage/RIAKXF86/Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:application/pdf;Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:/Users/bert/Zotero/storage/AKFZB5IE/Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:application/pdf;Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:/Users/bert/Zotero/storage/5JHPAFNX/Riegler et al. - 2013 - Merging Belief Propagation and the Mean Field Appr.pdf:application/pdf},
}

@article{penny_bayesian_2012,
	title = {Bayesian Models of Brain and Behaviour},
	volume = {2012},
	url = {http://www.hindawi.com/journals/isrn/2012/785791/abs/},
	doi = {10.5402/2012/785791},
	abstract = {This paper presents a review of Bayesian models of brain and behaviour. We first review the basic principles of Bayesian inference. This is followed by descriptions of sampling and variational methods for approximate inference, and forward and backward recursions in time for inference in dynamical models. The review of behavioural models covers work in visual processing, sensory integration, sensorimotor integration, and collective decision making. The review of brain models covers a range of spatial scales from synapses to neurons and population codes, but with an emphasis on models of cortical hierarchies. We describe a simple hierarchical model which provides a mathematical framework relating constructs in Bayesian inference to those in neural computation. We close by reviewing recent theoretical developments in Bayesian inference for planning and control.},
	pages = {e785791},
	journaltitle = {International Scholarly Research Notices},
	author = {Penny, William},
	urldate = {2014-09-02},
	date = {2012-10},
	langid = {english},
	file = {Penny - 2012 - Bayesian Models of Brain and Behaviour.pdf:/Users/bert/Zotero/storage/2PXGY3N6/Penny - 2012 - Bayesian Models of Brain and Behaviour.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/652URK5D/785791.html:text/html},
}

@inproceedings{pedersen_variational_2011,
	title = {A variational message passing algorithm for sensor self-localization in wireless networks},
	doi = {10.1109/ISIT.2011.6033940},
	abstract = {We propose a novel algorithm for sensor self-localization in cooperative wireless networks where observations of relative sensor distances are available. The variational message passing ({VMP}) algorithm is used to implement a mean field solution to the estimation of the posterior probabilities of the sensor positions in an R2 scenario. Extension to R3 is straight-forward. Compared to non-parametric methods based on belief propagation, the {VMP} algorithm features significantly lower communication overhead between sensors. This is supported by performance simulations which show that the estimated mean localization error of the algorithm stabilizes after approximately 30 iterations.},
	pages = {2158--2162},
	booktitle = {2011 {IEEE} International Symposium on Information Theory Proceedings ({ISIT})},
	author = {Pedersen, C. and Pedersen, T. and Fleury, B.H.},
	date = {2011-07},
	keywords = {Message passing, Signal processing algorithms, message passing, Approximation algorithms, Belief propagation, Probability, communication overhead, cooperative communication, cooperative wireless network, estimated mean localization, iterative methods, mean field solution implementation, Mobile communication, Noise, nonparametric method, performance simulation, posterior probabilities estimation, sensor placement, sensor self-localization, variational message passing algorithm, {VMP} algorithm, Wireless networks, wireless sensor networks},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/47Y4TZSZ/login.html:text/html;Pedersen et al. - 2011 - A variational message passing algorithm for sensor.pdf:/Users/bert/Zotero/storage/A6EARJFX/Pedersen et al. - 2011 - A variational message passing algorithm for sensor.pdf:application/pdf},
}

@article{ishii_control_2002,
	title = {Control of Exploitation-Exploration Meta-Parameter in Reinforcement Learning},
	volume = {15},
	abstract = {In reinforcement learning, the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based reinforcement learning, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [60] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.},
	pages = {665--687},
	journaltitle = {Neural Networks},
	author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
	date = {2002},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/ZWQPXLD2/summary.html:text/html;Citeseer - Snapshot:/Users/bert/Zotero/storage/GNKRRWEZ/summary.html:text/html;Ishii et al. - 2002 - Control of Exploitation-Exploration Meta-Parameter.pdf:/Users/bert/Zotero/storage/D4YCQQJP/Ishii et al. - 2002 - Control of Exploitation-Exploration Meta-Parameter.pdf:application/pdf;Ishii et al. - 2002 - Control of Exploitation-Exploration Meta-Parameter.pdf:/Users/bert/Zotero/storage/AZIF5PBT/Ishii et al. - 2002 - Control of Exploitation-Exploration Meta-Parameter.pdf:application/pdf},
}

@article{watanabe_alternative_2012,
	title = {An alternative view of variational Bayes and asymptotic approximations of free energy},
	volume = {86},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1007/s10994-011-5264-5},
	doi = {10.1007/s10994-011-5264-5},
	abstract = {Bayesian learning, widely used in many applied data-modeling problems, is often accomplished with approximation schemes because it requires intractable computation of the posterior distributions. In this study, we focus on two approximation methods, variational Bayes and local variational approximation. We show that the variational Bayes approach for statistical models with latent variables can be viewed as a special case of local variational approximation, where the log-sum-exp function is used to form the lower bound of the log-likelihood. The minimum variational free energy, the objective function of variational Bayes, is analyzed and related to the asymptotic theory of Bayesian learning. This analysis additionally implies a relationship between the generalization performance of the variational Bayes approach and the minimum variational free energy.},
	pages = {273--293},
	number = {2},
	journaltitle = {Machine Learning},
	author = {Watanabe, Kazuho},
	urldate = {2014-11-30},
	date = {2012-02},
	langid = {english},
	keywords = {variational Bayes, Artificial Intelligence (incl. Robotics), Asymptotic analysis, Computing Methodologies, Control, Generalization error, Language Translation and Linguistics, Local variational approximation, Mechatronics, Robotics, Simulation and Modeling, Variational Bayes, Variational free energy},
	file = {Snapshot:/Users/bert/Zotero/storage/TDT7QDC8/10.html:text/html;Watanabe - 2012 - An alternative view of variational Bayes and asymp.pdf:/Users/bert/Zotero/storage/ICEXEQLV/Watanabe - 2012 - An alternative view of variational Bayes and asymp.pdf:application/pdf},
}

@article{lee_dynamic_2014,
	title = {Dynamic belief state representations},
	volume = {25},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438814000348},
	doi = {10.1016/j.conb.2014.01.018},
	series = {Theoretical and computational neuroscience},
	abstract = {Perceptual and control systems are tasked with the challenge of accurately and efficiently estimating the dynamic states of objects in the environment. To properly account for uncertainty, it is necessary to maintain a dynamical belief state representation rather than a single state vector. In this review, canonical algorithms for computing and updating belief states in robotic applications are delineated, and connections to biological systems are highlighted. A navigation example is used to illustrate the importance of properly accounting for correlations between belief state components, and to motivate the need for further investigations in psychophysics and neurobiology.},
	pages = {221--227},
	journaltitle = {Current Opinion in Neurobiology},
	author = {Lee, Daniel D and Ortega, Pedro A and Stocker, Alan A},
	urldate = {2014-11-07},
	date = {2014-04},
	file = {Lee et al. - 2014 - Dynamic belief state representations.pdf:/Users/bert/Zotero/storage/X3EP8FWV/Lee et al. - 2014 - Dynamic belief state representations.pdf:application/pdf;Lee et al. - 2014 - Dynamic belief state representations.pdf:/Users/bert/Zotero/storage/X39SKZFM/Lee et al. - 2014 - Dynamic belief state representations.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/U79ADIDM/S0959438814000348.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/5YRMD5YI/S0959438814000348.html:text/html},
}

@article{friston_free_2006,
	title = {A free energy principle for the brain},
	volume = {100},
	issn = {0928-4257},
	doi = {10.1016/j.jphysparis.2006.10.001},
	abstract = {By formulating Helmholtz's ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses. In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. The free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. The system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. These changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment assumes that the system's state and structure encode an implicit and probabilistic model of the environment. We will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.},
	pages = {70--87},
	number = {1},
	journaltitle = {Journal of Physiology, Paris},
	author = {Friston, Karl and Kilner, James and Harrison, Lee},
	date = {2006-09},
	pmid = {17097864},
	keywords = {Animals, Brain, Humans, Attention, Probability, Models, Neurological, Afferent Pathways, Bayes Theorem, Brain Mapping, Computer Simulation, Entropy, Learning, Visual Perception},
	file = {Friston et al. - 2006 - A free energy principle for the brain.pdf:/Users/bert/Zotero/storage/BHMDN68B/Friston et al. - 2006 - A free energy principle for the brain.pdf:application/pdf},
}

@article{friston_anatomy_2014,
	title = {The anatomy of choice: dopamine and decision-making},
	volume = {369},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/content/369/1655/20130481},
	doi = {10.1098/rstb.2013.0481},
	shorttitle = {The anatomy of choice},
	abstract = {This paper considers goal-directed decision-making in terms of embodied or active inference. We associate bounded rationality with approximate Bayesian inference that optimizes a free energy bound on model evidence. Several constructs such as expected utility, exploration or novelty bonuses, softmax choice rules and optimism bias emerge as natural consequences of free energy minimization. Previous accounts of active inference have focused on predictive coding. In this paper, we consider variational Bayes as a scheme that the brain might use for approximate Bayesian inference. This scheme provides formal constraints on the computational anatomy of inference and action, which appear to be remarkably consistent with neuroanatomy. Active inference contextualizes optimal decision theory within embodied inference, where goals become prior beliefs. For example, expected utility theory emerges as a special case of free energy minimization, where the sensitivity or inverse temperature (associated with softmax functions and quantal response equilibria) has a unique and Bayes-optimal solution. Crucially, this sensitivity corresponds to the precision of beliefs about behaviour. The changes in precision during variational updates are remarkably reminiscent of empirical dopaminergic responses—and they may provide a new perspective on the role of dopamine in assimilating reward prediction errors to optimize decision-making.},
	pages = {20130481},
	number = {1655},
	journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Friston, Karl and Schwartenbeck, Philipp and {FitzGerald}, Thomas and Moutoussis, Michael and Behrens, Timothy and Dolan, Raymond J.},
	urldate = {2014-10-14},
	date = {2014-11},
	langid = {english},
	pmid = {25267823},
	keywords = {active inference, free energy, Bayesian inference, bounded rationality, Active Inference, agency, utility theory},
	file = {Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf:/Users/bert/Zotero/storage/3XA7JII2/Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2RKREUJV/20130481.html:text/html},
}

@article{friston_what_2011,
	title = {What Is Optimal about Motor Control?},
	volume = {72},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627311009305},
	doi = {10.1016/j.neuron.2011.10.018},
	abstract = {This article poses a controversial question: is optimal control theory useful for understanding motor behavior or is it a misdirection? This question is becoming acute as people start to conflate internal models in motor control and perception (Poeppel et al., 2008; Hickok et al., 2011). However, the forward models in motor control are not the generative models used in perceptual inference. This Perspective tries to highlight the differences between internal models in motor control and perception and asks whether optimal control is the right way to think about things. The issues considered here may have broader implications for optimal decision theory and Bayesian approaches to learning and behavior in general.},
	pages = {488--498},
	number = {3},
	journaltitle = {Neuron},
	author = {Friston, Karl},
	urldate = {2015-02-02},
	date = {2011-11},
	file = {Friston - 2011 - What Is Optimal about Motor Control.pdf:/Users/bert/Zotero/storage/NIVDEF89/Friston - 2011 - What Is Optimal about Motor Control.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/F632GCDH/S0896627311009305.html:text/html},
}

@article{ortega_nonparametric_2012,
	title = {A Nonparametric Conjugate Prior Distribution for the Maximizing Argument of a Noisy Function},
	url = {http://arxiv.org/abs/1206.1898},
	abstract = {We propose a novel Bayesian approach to solve stochastic optimization problems that involve finding extrema of noisy, nonlinear functions. Previous work has focused on representing possible functions explicitly, which leads to a two-step procedure of first, doing inference over the function space and second, finding the extrema of these functions. Here we skip the representation step and directly model the distribution over extrema. To this end, we devise a non-parametric conjugate prior based on a kernel regressor. The resulting posterior distribution directly captures the uncertainty over the maximum of the unknown function. We illustrate the effectiveness of our model by optimizing a noisy, high-dimensional, non-convex objective function.},
	journaltitle = {{arXiv}:1206.1898 [cs, math, stat]},
	author = {Ortega, Pedro A. and Grau-Moya, Jordi and Genewein, Tim and Balduzzi, David and Braun, Daniel A.},
	urldate = {2014-10-29},
	date = {2012-06},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Statistics - Machine Learning},
	file = {Arg-Max Prior [Pedro A. Ortega]:/Users/bert/Zotero/storage/PFRTHSC3/argmaxprior.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/FIWQKGNP/1206.html:text/html;Ortega et al. - 2012 - A Nonparametric Conjugate Prior Distribution for t.pdf:/Users/bert/Zotero/storage/RX4QK4FS/Ortega et al. - 2012 - A Nonparametric Conjugate Prior Distribution for t.pdf:application/pdf},
}

@article{nielsen_perception-based_2015,
	title = {Perception-Based Personalization of Hearing Aids Using Gaussian Processes and Active Learning},
	volume = {23},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2014.2377581},
	abstract = {Personalization of multi-parameter hearing aids involves an initial fitting followed by a manual knowledge-based trial-and-error fine-tuning from ambiguous verbal user feedback. The result is an often suboptimal {HA} setting whereby the full potential of modern hearing aids is not utilized. This article proposes an interactive hearing-aid personalization system that obtains an optimal individual setting of the hearing aids from direct perceptual user feedback. Results obtained with ten hearing-impaired subjects show that ten to twenty pairwise user assessments between different settings—equivalent to 5-10 min—is sufficient for personalization of up to four hearing-aid parameters. A setting obtained by the system was significantly preferred by the subject over the initial fitting, and the obtained setting could be reproduced with reasonable precision. The system may have potential for clinical usage to assist both the hearing-care professional and the user.},
	pages = {162--173},
	number = {1},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Nielsen, J.B.B. and Nielsen, J. and Larsen, J.},
	date = {2015-01},
	keywords = {Signal processing algorithms, Speech processing, Gaussian processes, Active learning, Approximation methods, Gain, Gaussian process ({GP}), hearing aids, individualization, pairwise comparisons, personalization, Speech},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/QSQ7A9RC/articleDetails.html:text/html;Nielsen et al. - 2015 - Perception-Based Personalization of Hearing Aids U.pdf:/Users/bert/Zotero/storage/3KS2U8XM/Nielsen et al. - 2015 - Perception-Based Personalization of Hearing Aids U.pdf:application/pdf},
}

@article{forney_codes_2001,
	title = {Codes on graphs: normal realizations},
	volume = {47},
	issn = {0018-9448},
	doi = {10.1109/18.910573},
	shorttitle = {Codes on graphs},
	abstract = {A generalized state realization of the Wiberg (1996) type is called normal if symbol variables have degree 1 and state variables have degree 2. A natural graphical model of such a realization has leaf edges representing symbols, ordinary edges representing states, and vertices representing local constraints. Such a graph can be decoded by any version of the sum-product algorithm. Any state realization of a code can be put into normal form without essential change in the corresponding graph or in its decoding complexity. Group or linear codes are generated by group or linear state realizations. On a cycle-free graph, there exists a well-defined minimal canonical realization, and the sum-product algorithm is exact. However, the cut-set bound shows that graphs with cycles may have a superior performance-complexity tradeoff, although the sum-product algorithm is then inexact and iterative, and minimal realizations are not well-defined. Efficient cyclic and cycle-free realizations of Reed-Muller ({RM}) codes are given as examples. The dual of a normal group realization, appropriately defined, generates the dual group code. The dual realization has the same graph topology as the primal realization, replaces symbol and state variables by their character groups, and replaces primal local constraints by their duals. This fundamental result has many applications, including to dual state spaces, dual minimal trellises, duals to Tanner (1981) graphs, dual input/output (I/O) systems, and dual kernel and image representations. Finally a group code may be decoded using the dual graph, with appropriate Fourier transforms of the inputs and outputs; this can simplify decoding of high-rate codes},
	pages = {520--548},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Forney, G.D., Jr.},
	date = {2001-02},
	keywords = {graph theory, sum-product algorithm, Kernel, character groups, codes on graphs, computational complexity, cut-set bound, cycle-free graph, cyclic codes, decoding, decoding complexity, dual codes, dual group code, dual input/output systems, dual kernel representation, dual minimal trellises, dual state spaces, Fourier transforms, generalized state realization, graph topology, graphical model, graphical models, group codes, high-rate codes, Image representation, iterative algorithm, leaf edges, Linear code, linear codes, local constraints, minimal canonical realization, normal group realization, normal realizations, ordinary edges, Parity check codes, performance-complexity tradeoff, primal realization, Reed-Muller codes, Reed-Solomon codes, state variables, sum product algorithm, symbol variables, Tanner graphs, Topology, vertices},
	file = {Forney - 2001 - Codes on graphs normal realizations.pdf:/Users/bert/Zotero/storage/WGXV28H6/Forney - 2001 - Codes on graphs normal realizations.pdf:application/pdf;Forney - 2001 - Codes on graphs normal realizations.pdf:/Users/bert/Zotero/storage/SJD9BZCJ/Forney - 2001 - Codes on graphs normal realizations.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/STMYAD2L/login.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/7AI8UEQB/login.html:text/html},
}

@book{bishop_pattern_2006,
	title = {Pattern Recognition and Machine Learning},
	isbn = {0-387-31073-8},
	url = {http://www.springer.com/computer/image+processing/book/978-0-387-31073-2},
	abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown ...},
	publisher = {Springer-Verlag New York, Inc.},
	author = {Bishop, Christopher M.},
	urldate = {2014-04-10},
	date = {2006},
	keywords = {Artificial Intelligence (incl. Robotics), Pattern Recognition, Pattern Recognition and Machine Learning},
	file = {Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:/Users/bert/Zotero/storage/RWIF5QCB/Bishop - 2006 - Pattern Recognition and Machine Learning 2.pdf:application/pdf;Bishop - 2008 - PRML solutions manual.pdf:/Users/bert/Zotero/storage/YV36EFTP/Bishop - 2008 - PRML solutions manual.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/EEYPJMML/978-0-387-31073-2.html:text/html},
}

@article{loeliger_factor_2007,
	title = {The Factor Graph Approach to Model-Based Signal Processing},
	volume = {95},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2007.896497},
	pages = {1295--1322},
	number = {6},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Loeliger, Hans-Andrea and Dauwels, Justin and Hu, Junli and Korl, Sascha and Ping, Li and Kschischang, Frank R.},
	urldate = {2014-04-10},
	date = {2007-06},
	keywords = {factor graphs},
	file = {Loeliger et al. - 2007 - The Factor Graph Approach to Model-Based Signal Pr.pdf:/Users/bert/Zotero/storage/MCLRG5VM/Loeliger et al. - 2007 - The Factor Graph Approach to Model-Based Signal Pr.pdf:application/pdf},
}

@article{doersch_tutorial_2016,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	journaltitle = {{arXiv}:1606.05908 [cs, stat]},
	author = {Doersch, Carl},
	urldate = {2016-07-22},
	date = {2016-06},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7P66Y7TW/1606.html:text/html;Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:/Users/bert/Zotero/storage/DU9GJ3SV/Doersch - 2016 - Tutorial on Variational Autoencoders.pdf:application/pdf},
}

@thesis{winn_variational_2003,
	title = {Variational message passing and its applications},
	url = {http://johnwinn.org/Publications/thesis/Winn03_thesis.pdf},
	type = {phdthesis},
	author = {Winn, John},
	urldate = {2014-04-10},
	date = {2003},
	keywords = {variational Bayes, factor graphs},
	file = {Winn - 2003 - Variational message passing and its applications.pdf:/Users/bert/Zotero/storage/EJVW83L3/Winn - 2003 - Variational message passing and its applications.pdf:application/pdf;Winn - 2004 - Variational message passing and its applications.pdf:/Users/bert/Zotero/storage/KHZJFU7X/Winn - 2004 - Variational message passing and its applications.pdf:application/pdf},
}

@article{chung_hierarchical_2016,
	title = {Hierarchical Multiscale Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1609.01704},
	abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
	journaltitle = {{arXiv}:1609.01704 [cs]},
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	urldate = {2016-09-12},
	date = {2016-09},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LNR8L9NC/1609.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/25HPI3CV/1609.html:text/html;Chung et al. - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:/Users/bert/Zotero/storage/JWVXZ43M/Chung et al. - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:application/pdf;Chung et al. - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:/Users/bert/Zotero/storage/8RJL8NH3/Chung et al. - 2016 - Hierarchical Multiscale Recurrent Neural Networks.pdf:application/pdf},
}

@inproceedings{jitkrittum_kernel-based_2015,
	location = {Amsterdam, Netherlands},
	title = {Kernel-Based Just-In-Time Learning for Passing Expectation Propagation Messages},
	url = {http://www.auai.org/uai2015/proceedings/papers/235.pdf},
	abstract = {We propose an efficient nonparametric strategy for learning a message operator in expectation propagation ({EP}), which takes as input the set of incoming messages to a factor node, and produces an outgoing message as output. This learned operator replaces the multivariate integral required in classical {EP}, which may not have an analytic expression. We use kernel-based regression, which is trained on a set of probability distributions representing the incoming messages, and the associated outgoing messages. The kernel approach has two main advantages: first, it is fast, as it is implemented using a novel two-layer random feature representation of the input message distributions; second, it has principled uncertainty estimates, and can be cheaply updated online, meaning it can request and incorporate new training data when it encounters inputs on which it is uncertain. In experiments, our approach is able to solve learning problems where a single message operator is required for multiple, substantially different data sets (logistic regression for a variety of classification problems), where it is essential to accurately assess uncertainty and to efficiently and robustly update the message operator.},
	booktitle = {Proceedings of the Thirty-First Conference (2015)},
	author = {Jitkrittum, Wittawat and Gretton, Arthur and Heess, Nicolas and Eslami, S. M. Ali and Lakshminarayanan, Balaji and Sejdinovic, Dino and Szabó, Zoltán},
	date = {2015},
	file = {Jitkrittum et al. - 2015 - Kernel-Based Just-In-Time Learning for Passing Exp.pdf:/Users/bert/Zotero/storage/K6BI43XA/Jitkrittum et al. - 2015 - Kernel-Based Just-In-Time Learning for Passing Exp.pdf:application/pdf},
}

@article{pio-lopez_active_2016,
	title = {Active inference and robot control: a case study},
	volume = {13},
	rights = {© 2016 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/13/122/20160616},
	doi = {10.1098/rsif.2016.0616},
	shorttitle = {Active inference and robot control},
	abstract = {Active inference is a general framework for perception and action that is gaining prominence in computational and systems neuroscience but is less known outside these fields. Here, we discuss a proof-of-principle implementation of the active inference scheme for the control or the 7-{DoF} arm of a (simulated) {PR}2 robot. By manipulating visual and proprioceptive noise levels, we show under which conditions robot control under the active inference scheme is accurate. Besides accurate control, our analysis of the internal system dynamics (e.g. the dynamics of the hidden states that are inferred during the inference) sheds light on key aspects of the framework such as the quintessentially multimodal nature of control and the differential roles of proprioception and vision. In the discussion, we consider the potential importance of being able to implement active inference in robots. In particular, we briefly review the opportunities for modelling psychophysiological phenomena such as sensory attenuation and related failures of gain control, of the sort seen in Parkinson's disease. We also consider the fundamental difference between active inference and optimal control formulations, showing that in the former the heavy lifting shifts from solving a dynamical inverse problem to creating deep forward or generative models with dynamics, whose attracting sets prescribe desired behaviours.},
	pages = {20160616},
	number = {122},
	journaltitle = {Journal of The Royal Society Interface},
	author = {Pio-Lopez, Léo and Nizard, Ange and Friston, Karl and Pezzulo, Giovanni},
	urldate = {2016-10-03},
	date = {2016-09},
	langid = {english},
	pmid = {27683002},
	file = {Pio-Lopez et al. - 2016 - Active inference and robot control a case study.pdf:/Users/bert/Zotero/storage/YJPJ6Y7G/Pio-Lopez et al. - 2016 - Active inference and robot control a case study.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/KAA3CLJF/20160616.html:text/html},
}

@article{linderman_recurrent_2016,
	title = {Recurrent switching linear dynamical systems},
	url = {http://arxiv.org/abs/1610.08466},
	abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems ({SLDS}), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These "recurrent" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional {SLDS} models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.},
	journaltitle = {{arXiv}:1610.08466 [stat]},
	author = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
	urldate = {2016-11-02},
	date = {2016-10},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XDR55Y3P/1610.html:text/html;Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf:/Users/bert/Zotero/storage/XFSYCCUF/Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf:application/pdf},
}

@article{johnson_composing_2016,
	title = {Composing graphical models with neural networks for structured representations and fast inference},
	url = {http://arxiv.org/abs/1603.06277},
	abstract = {We propose a general modeling and inference framework that composes probabilistic graphical models with deep learning methods and combines their respective strengths. Our model family augments graphical structure in latent variables with neural network observation models. For inference, we extend variational autoencoders to use graphical model approximating distributions with recognition networks that output conjugate potentials. All components of these models are learned simultaneously with a single objective, giving a scalable algorithm that leverages stochastic variational inference, natural gradients, graphical model message passing, and the reparameterization trick. We illustrate this framework with several example models and an application to mouse behavioral phenotyping.},
	journaltitle = {{arXiv}:1603.06277 [stat]},
	author = {Johnson, Matthew J. and Duvenaud, David and Wiltschko, Alexander B. and Datta, Sandeep R. and Adams, Ryan P.},
	urldate = {2016-11-03},
	date = {2016-03},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1603.06277 PDF:/Users/bert/Zotero/storage/48DX9YBZ/Johnson et al. - 2016 - Composing graphical models with neural networks fo.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/9H7DW287/1603.html:text/html},
}

@article{deng_deep_2015,
	title = {Deep Discriminative and Generative Models for Pattern Recognition},
	url = {https://www.microsoft.com/en-us/research/publication/deep-discriminative-and-generative-models-for-pattern-recognition/},
	abstract = {In this chapter we describe deep generative and discriminative models as they have been applied to speech recognition. The former models describe the distribution of data, whereas the latter models describe the distribution of targets conditioned on data. Both models are characterized as being ‘deep’ as they use layers of latent or hidden variables. Understanding …},
	journaltitle = {Microsoft Research},
	author = {Deng, Li and Jaitly, Navdeep},
	urldate = {2016-11-21},
	date = {2015-11},
	file = {Deng and Jaitly - 2015 - Deep Discriminative and Generative Models for Patt.pdf:/Users/bert/Zotero/storage/UV9YCJGN/Deng and Jaitly - 2015 - Deep Discriminative and Generative Models for Patt.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NCBL6P6I/deep-discriminative-and-generative-models-for-pattern-recognition.html:text/html},
}

@article{hafner_tensorflow_2017,
	title = {{TensorFlow} Agents: Efficient Batched Reinforcement Learning in {TensorFlow}},
	url = {https://scirate.com/arxiv/1709.02878},
	shorttitle = {{TensorFlow} Agents},
	abstract = {We introduce {TensorFlow} Agents, an efficient infrastructure paradigm for building parallel reinforcement learning algorithms in {TensorFlow}. We simulate multiple environments in parallel, and group them to perform the neural network computation on a batch rather than individual observations. This allows the {TensorFlow} execution engine to parallelize computation, without the need for manual synchronization. Environments are stepped in separate Python processes to progress them in parallel without interference of the global interpreter lock. As part of this project, we introduce {BatchPPO}, an efficient implementation of the proximal policy optimization algorithm. By open sourcing {TensorFlow} Agents, we hope to provide a flexible starting point for future projects that accelerates future research in the field.},
	journaltitle = {{SciRate}},
	author = {Hafner, Danijar and Davidson, James and Vanhoucke, Vincent},
	urldate = {2017-09-12},
	date = {2017-09-12},
	file = {Hafner e.a. - 2017 - TensorFlow Agents Efficient Batched Reinforcement.pdf:/Users/bert/Zotero/storage/DAG4FEVC/Hafner e.a. - 2017 - TensorFlow Agents Efficient Batched Reinforcement.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ENB6JR3R/1709.html:text/html},
}

@article{ueltzhoffer_deep_2017,
	title = {Deep Active Inference},
	url = {http://arxiv.org/abs/1709.02341},
	abstract = {This work combines the free energy principle from cognitive neuroscience and the ensuing active inference dynamics with recent advances in variational inference on deep generative models and evolution strategies as efficient large-scale black-box optimisation technique, to introduce the "deep active inference" agent. This agent tries to minimize a variational free energy bound on the average surprise of its sensations, which is motivated by a homeostatic argument. It does so by changing the parameters of its generative model, together with a variational density approximating the posterior distribution over latent variables, given its observations, and by acting on its environment to actively sample input that is likely under its generative model. The internal dynamics of the agent are implemented using deep neural networks, as used in machine learning, and recurrent dynamics, making the deep active inference agent a scalable and very flexible class of active inference agents. Using the mountaincar problem, we show how goal-directed behaviour can be implemented by defining sensible prior expectations on the latent states in the agent's model, that it will try to fulfil. Furthermore, we show that the deep active inference agent can learn a generative model of the environment, which can be sampled from to understand the agent's beliefs about the environment and its interaction with it.},
	journaltitle = {{arXiv}:1709.02341 [q-bio]},
	author = {Ueltzhöffer, Kai},
	urldate = {2017-09-09},
	date = {2017-09-07},
	eprinttype = {arxiv},
	eprint = {1709.02341},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QTS9NCZ4/1709.html:text/html;Ueltzhöffer - 2017 - Deep Active Inference.pdf:/Users/bert/Zotero/storage/ZTZ7YVN3/Ueltzhöffer - 2017 - Deep Active Inference.pdf:application/pdf},
}

@article{russo_tutorial_2017,
	title = {A Tutorial on Thompson Sampling},
	url = {http://arxiv.org/abs/1707.02038},
	abstract = {Thompson sampling is an algorithm for online decision problems where actions are taken sequentially in a manner that must balance between exploiting what is known to maximize immediate performance and investing to accumulate new information that may improve future performance. The algorithm addresses a broad range of problems in a computationally efficient manner and is therefore enjoying wide use. This tutorial covers the algorithm and its application, illustrating concepts through a range of examples, including Bernoulli bandit problems, shortest path problems, dynamic pricing, recommendation, active learning with neural networks, and reinforcement learning in Markov decision processes. Most of these problems involve complex information structures, where information revealed by taking an action informs beliefs about other actions. We will also discuss when and why Thompson sampling is or is not effective and relations to alternative algorithms.},
	journaltitle = {{arXiv}:1707.02038 [cs]},
	author = {Russo, Daniel and Van Roy, Benjamin and Kazerouni, Abbas and Osband, Ian},
	urldate = {2017-09-07},
	date = {2017-07-07},
	eprinttype = {arxiv},
	eprint = {1707.02038},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2AGE52BN/1707.html:text/html;Russo e.a. - 2017 - A Tutorial on Thompson Sampling.pdf:/Users/bert/Zotero/storage/2FX4TIGZ/Russo e.a. - 2017 - A Tutorial on Thompson Sampling.pdf:application/pdf},
}

@article{ortega_bayesian_2009,
	title = {A Bayesian Rule for Adaptive Control based on Causal Interventions},
	url = {http://arxiv.org/abs/0911.5104},
	abstract = {Explaining adaptive behavior is a central problem in artificial intelligence research. Here we formalize adaptive agents as mixture distributions over sequences of inputs and outputs (I/O). Each distribution of the mixture constitutes a `possible world', but the agent does not know which of the possible worlds it is actually facing. The problem is to adapt the I/O stream in a way that is compatible with the true world. A natural measure of adaptation can be obtained by the Kullback-Leibler ({KL}) divergence between the I/O distribution of the true world and the I/O distribution expected by the agent that is uncertain about possible worlds. In the case of pure input streams, the Bayesian mixture provides a well-known solution for this problem. We show, however, that in the case of I/O streams this solution breaks down, because outputs are issued by the agent itself and require a different probabilistic syntax as provided by intervention calculus. Based on this calculus, we obtain a Bayesian control rule that allows modeling adaptive behavior with mixture distributions over I/O streams. This rule might allow for a novel approach to adaptive control based on a minimum {KL}-principle.},
	journaltitle = {{arXiv}:0911.5104 [cs]},
	author = {Ortega, Pedro A. and Braun, Daniel A.},
	urldate = {2017-09-04},
	date = {2009-11-26},
	eprinttype = {arxiv},
	eprint = {0911.5104},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/39UDMGEW/0911.html:text/html;Ortega and Braun - 2009 - A Bayesian Rule for Adaptive Control based on Caus.pdf:/Users/bert/Zotero/storage/3M92H3Q7/Ortega and Braun - 2009 - A Bayesian Rule for Adaptive Control based on Caus.pdf:application/pdf},
}

@article{ortega_minimum_2010,
	title = {A Minimum Relative Entropy Principle for Learning and Acting},
	abstract = {This paper proposes a method to construct an adaptive agent that is universal with respect to a given class of experts, where each expert is designed specifically for a particular environment. This adaptive control problem is formalized as the problem of minimizing the relative entropy of the adaptive agent from the expert that is most suitable for the unknown environment. If the agent is a passive observer, then the optimal solution is the well-known Bayesian predictor. However, if the agent is active, then its past actions need to be treated as causal interventions on the I/O stream rather than normal probability conditions. Here it is shown that the solution to this new variational problem is given by a stochastic controller called the Bayesian control rule, which implements adaptive behavior as a mixture of experts. Furthermore, it is shown that under mild assumptions, the Bayesian control rule converges to the control law of the most suitable expert. 1.},
	pages = {475--511},
	journaltitle = {J. Artif. Intell. Res. 2010},
	author = {Ortega, Pedro A. and Braun, Daniel A.},
	date = {2010},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/SR8MKQQW/summary.html:text/html;Ortega and Braun - 2010 - A Minimum Relative Entropy Principle for Learning .pdf:/Users/bert/Zotero/storage/AEMXPBKP/Ortega and Braun - 2010 - A Minimum Relative Entropy Principle for Learning .pdf:application/pdf},
}

@article{friston_free-energy_2010,
	title = {The free-energy principle: a unified brain theory?},
	volume = {11},
	pages = {127--138},
	number = {2},
	journaltitle = {Nature Reviews Neuroscience},
	author = {Friston, Karl},
	date = {2010},
	file = {Friston - 2010 - The free energy principle a unified brain theory.pdf:/Users/bert/Zotero/storage/IW5YCUVQ/Friston - 2010 - The free energy principle a unified brain theory.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement 1.pdf:/Users/bert/Zotero/storage/XACR3PYF/Friston - 2010 - Unified brain - Supplement 1.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement 2.pdf:/Users/bert/Zotero/storage/KYTIFYJ5/Friston - 2010 - Unified brain - Supplement 2.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement 3.pdf:/Users/bert/Zotero/storage/LFN9DKA2/Friston - 2010 - Unified brain - Supplement 3.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement 4.pdf:/Users/bert/Zotero/storage/W2WQCKB5/Friston - 2010 - Unified brain - Supplement 4.pdf:application/pdf;Friston - 2010 - Unified brain - Supplement 5.pdf:/Users/bert/Zotero/storage/FS8ZFWBJ/Friston - 2010 - Unified brain - Supplement 5.pdf:application/pdf},
}

@article{mandt_variational_2014,
	title = {Variational Tempering},
	url = {http://arxiv.org/abs/1411.1810},
	abstract = {Variational inference ({VI}) combined with data subsampling enables approximate posterior inference over large data sets, but suffers from poor local optima. We first formulate a deterministic annealing approach for the generic class of conditionally conjugate exponential family models. This approach uses a decreasing temperature parameter which deterministically deforms the objective during the course of the optimization. A well-known drawback to this annealing approach is the choice of the cooling schedule. We therefore introduce variational tempering, a variational algorithm that introduces a temperature latent variable to the model. In contrast to related work in the Markov chain Monte Carlo literature, this algorithm results in adaptive annealing schedules. Lastly, we develop local variational tempering, which assigns a latent temperature to each data point; this allows for dynamic annealing that varies across data. Compared to the traditional {VI}, all proposed approaches find improved predictive likelihoods on held-out data.},
	journaltitle = {{arXiv}:1411.1810 [cs, stat]},
	author = {Mandt, Stephan and {McInerney}, James and Abrol, Farhan and Ranganath, Rajesh and Blei, David},
	urldate = {2016-09-08},
	date = {2014-11-06},
	eprinttype = {arxiv},
	eprint = {1411.1810},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7X2KSL8I/1411.html:text/html;Mandt et al. - 2014 - Variational Tempering.pdf:/Users/bert/Zotero/storage/PCETHZU6/Mandt et al. - 2014 - Variational Tempering.pdf:application/pdf},
}

@article{kosiorek_hierarchical_2017,
	title = {Hierarchical Attentive Recurrent Tracking},
	url = {http://arxiv.org/abs/1706.09262},
	abstract = {Class-agnostic object tracking is particularly difficult in cluttered environments as target specific discriminative models cannot be learned a priori. Inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual features, this work develops a hierarchical attentive recurrent model for single object tracking in videos. The first layer of attention discards the majority of background by selecting a region containing the object of interest, while the subsequent layers tune in on visual features particular to the tracked object. This framework is fully differentiable and can be trained in a purely data driven fashion by gradient methods. To improve training convergence, we augment the loss function with terms for a number of auxiliary tasks relevant for tracking. Evaluation of the proposed model is performed on two datasets of increasing difficulty: pedestrian tracking on the {KTH} activity recognition dataset and the {KITTI} object tracking dataset.},
	journaltitle = {{arXiv}:1706.09262 [cs]},
	author = {Kosiorek, Adam R. and Bewley, Alex and Posner, Ingmar},
	urldate = {2017-08-30},
	date = {2017-06-28},
	eprinttype = {arxiv},
	eprint = {1706.09262},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/JSHU5NCN/1706.html:text/html;Kosiorek et al. - 2017 - Hierarchical Attentive Recurrent Tracking.pdf:/Users/bert/Zotero/storage/9J9RIL7R/Kosiorek et al. - 2017 - Hierarchical Attentive Recurrent Tracking.pdf:application/pdf},
}

@inproceedings{grubb_boosted_2010,
	location = {{USA}},
	title = {Boosted Backpropagation Learning for Training Deep Modular Networks},
	isbn = {978-1-60558-907-7},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104375},
	series = {{ICML}'10},
	abstract = {Divide-and-conquer is key to building sophisticated learning machines: hard problems are solved by composing a network of modules that solve simpler problems ({LeCun} et al., 1998; Rohde, 2002; Bradley, 2009). Many such existing systems rely on learning algorithms which are based on simple parametric gradient descent where the parametrization must be predetermined, or more specialized per-application algorithms which are usually ad-hoc and complicated. We present a novel approach for training generic modular networks that uses two existing techniques: the error propagation strategy of backpropagation and more recent research on descent in spaces of functions (Mason et al., 1999; Scholkopf \& Smola, 2001). Combining these two methods of optimization gives a simple algorithm for training heterogeneous networks of functional modules using simple gradient propagation mechanics and established learning algorithms. The resulting separation of concerns between learning individual modules and error propagation mechanics eases implementation, enables a larger class of modular learning strategies, and allows per-module control of complexity/regularization. We derive and demonstrate this functional backpropagation and contrast it with traditional gradient descent in parameter space, observing that in our example domain the method is significantly more robust to local optima.},
	pages = {407--414},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	publisher = {Omnipress},
	author = {Grubb, Alexander and Bagnell, J. Andrew},
	urldate = {2017-08-30},
	date = {2010},
	file = {Grubb and Bagnell - 2010 - Boosted Backpropagation Learning for Training Deep.pdf:/Users/bert/Zotero/storage/5TVDWPTF/Grubb and Bagnell - 2010 - Boosted Backpropagation Learning for Training Deep.pdf:application/pdf},
}

@article{gershman_amortized_2014,
	title = {Amortized Inference in Probabilistic Reasoning},
	volume = {36},
	url = {http://escholarship.org/uc/item/34j1h7k5},
	abstract = {Amortized Inference in Probabilistic Reasoning Samuel J. Gershman 1 (sjgershm@mit.edu) and Noah D. Goodman 2 (ngoodman@stanford.edu) 1 Department of Brain and Cognitive Sciences, {MIT} of Psychology, Stanford University 2 Department Abstract similar or related queries. For example, as you view an im- age, your head and eyes are continuously moving, generating an infinitude of slightly different queries. For these queries, it may be inaccurate to reuse a stored inference without modifi- cation. This raises the problem of amortized inference: how to flexibly reuse inferences so as to answer a variety of re- lated queries. Recently, Stuhlm¨uller et al. (2013) addressed this problem by using stored samples to estimate local condi- tional distributions, and then approximating answers to more complex queries by composing the local distributions. The work described in this paper seeks experimental evidence for a similar kind of flexible reuse in human reasoning. We presented subjects with a simple Bayesian network and asked them to answer a series of queries about it. One of these queries (the “target”) could be answered by reusing the answer to another query (the “sub-query”). We hypothesized that the effects of reuse would be evident compared to an in- ference with the same structure but no re-usable sub-query. Further, we hypothesized that this effect would be present only if the target was presented after the sub-query. Accord- ingly, we manipulated (between subjects) whether the target came before or after the sub-query. This design allowed us to look for two key signatures of reuse: correlations between related inferences (Experiment 1) and faster responses for in- ferences that exploit reuse (Experiment 2). Recent studies of probabilistic reasoning have postulated general-purpose inference algorithms that can be used to an- swer arbitrary queries. These algorithms are memoryless, in the sense that each query is processed independently, without reuse of earlier computation. We argue that the brain oper- ates in the setting of amortized inference, where numerous related queries must be answered (e.g., recognizing a scene from multiple viewpoints); in this setting, memoryless algo- rithms can be computationally wasteful. We propose a simple form of flexible reuse, according to which shared inferences are cached and composed together to answer new queries. We present experimental evidence that humans exploit this form of reuse: the answer to a complex query can be systematically predicted from a person’s response to a simpler query if the simpler query was presented first and entails a sub-inference (i.e., a sub-component of the more complex query). People are also faster at answering a complex query when it is preceded by a sub-inference. Our results suggest that the astonishing ef- ficiency of human probabilistic reasoning may be supported by interactions between inference and memory. Keywords: induction, Bayesian inference, memory “Cognition is recognition.” – Hofstadter (1995) Introduction One view of probabilistic reasoning holds that our brains are equipped with general-purpose inference algorithms that can be used to answer arbitrary queries (Griffiths et al., 2012; Pouget et al., 2013). An under-appreciated property of such algorithms borrowed from computer science is that they are memoryless: each query is (at least in principle) processed independently of others. While this property guarantees that inferences will not interfere with one another, it can also lead to gross computational inefficiency, since inferences are never reused; memorylessness implies that answering the same query twice requires the same amount of computation as answer two unique queries. 1 Whatever inference algorithms the brain uses, they are un- likely to be memoryless. Consider, for example, the image in Figure 1 (Gregory, 1970). Upon viewing it for the first time, most observers find it extremely difficult to identify what the image depicts. 2 However, once the image has been deciphered, all subsequent views are instantly recognized. Clearly, the visual system is not running a computationally expensive inference algorithm upon each viewing; the infer- ence is simply reused. In reality, it is rare to be faced with the exact same query multiple times. Much more pervasive is the appearance of Figure 1: What does this image depict? Amortized inference in Bayesian networks 1 To be fair, inference algorithms for dynamical systems, like Kalman filtering, involve reuse in a certain sense. However, these algorithms are not designed to reuse inferences when applied to sev- eral independent time series (even if the time series are identical). 2 Answer: a dalmatian. In this paper, we will restrict our attention to amortized in- ference for Bayesian networks. Let p(x) denote a probability distribution on variables x = \{x 1 , . . . , x M \}. A Bayesian net- work G is a directed acyclic graph with nodes corresponding},
	number = {36},
	journaltitle = {Proceedings of the Cognitive Science Society},
	author = {Gershman, Samuel and Goodman, Noah},
	urldate = {2017-08-30},
	date = {2014-01-01},
	file = {Gershman and Goodman - 2014 - Amortized Inference in Probabilistic Reasoning.pdf:/Users/bert/Zotero/storage/KT6PA6S4/Gershman and Goodman - 2014 - Amortized Inference in Probabilistic Reasoning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UKH53D8H/34j1h7k5.html:text/html},
}

@article{rezende_stochastic_2014,
	title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	journaltitle = {{arXiv}:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	urldate = {2017-08-30},
	date = {2014-01-16},
	eprinttype = {arxiv},
	eprint = {1401.4082},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Methodology, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/M4TUBRJB/1401.html:text/html;Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:/Users/bert/Zotero/storage/CJEIDQ9C/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf:application/pdf},
}

@article{salimans_fixed-form_2013,
	title = {Fixed-Form Variational Posterior Approximation through Stochastic Linear Regression},
	volume = {8},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1386166315},
	doi = {10.1214/13-BA858},
	abstract = {We propose a general algorithm for approximating nonstandard Bayesian posterior distributions. The algorithm minimizes the Kullback-Leibler divergence of an approximating distribution to the intractable posterior distribution. Our method can be used to approximate any posterior distribution, provided that it is given in closed form up to the proportionality constant. The approximation can be any distribution in the exponential family or any mixture of such distributions, which means that it can be made arbitrarily precise. Several examples illustrate the speed and accuracy of our approximation method in practice.},
	pages = {837--882},
	number = {4},
	journaltitle = {Bayesian Analysis},
	shortjournal = {Bayesian Anal.},
	author = {Salimans, Tim and Knowles, David A.},
	urldate = {2017-08-30},
	date = {2013-12},
	mrnumber = {MR3150471},
	zmnumber = {1329.62142},
	keywords = {variational Bayes, approximate inference, stochastic approximation},
	file = {Salimans and Knowles - 2013 - Fixed-Form Variational Posterior Approximation thr.pdf:/Users/bert/Zotero/storage/4YWJETLZ/Salimans and Knowles - 2013 - Fixed-Form Variational Posterior Approximation thr.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/74NL2RWA/1386166315.html:text/html},
}

@article{salimans_evolution_2017,
	title = {Evolution Strategies as a Scalable Alternative to Reinforcement Learning},
	url = {http://arxiv.org/abs/1703.03864},
	abstract = {We explore the use of Evolution Strategies, a class of black box optimization algorithms, as an alternative to popular {RL} techniques such as Q-learning and Policy Gradients. Experiments on {MuJoCo} and Atari show that {ES} is a viable solution strategy that scales extremely well with the number of {CPUs} available: By using hundreds to thousands of parallel workers, {ES} can solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training time. In addition, we highlight several advantages of {ES} as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
	journaltitle = {{arXiv}:1703.03864 [cs, stat]},
	author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sutskever, Ilya},
	urldate = {2017-08-29},
	date = {2017-03-10},
	eprinttype = {arxiv},
	eprint = {1703.03864},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/RE3BFKFP/1703.html:text/html;Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:/Users/bert/Zotero/storage/46RKVEU5/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf:application/pdf},
}

@inproceedings{gal_rapid_2015,
	title = {Rapid Prototyping of Probabilistic Models: Emerging Challenges in Variational Inference},
	booktitle = {Advances in Approximate Bayesian Inference workshop, {NIPS}},
	author = {Gal, Yarin},
	date = {2015},
	file = {Gal - 2015 - Rapid Prototyping of Probabilistic Models Emergin.pdf:/Users/bert/Zotero/storage/9J6R66K6/Gal - 2015 - Rapid Prototyping of Probabilistic Models Emergin.pdf:application/pdf},
}

@inproceedings{murphy_linear_2001,
	location = {Cambridge, {MA}, {USA}},
	title = {Linear Time Inference in Hierarchical {HMMs}},
	url = {http://dl.acm.org/citation.cfm?id=2980539.2980647},
	series = {{NIPS}'01},
	abstract = {The hierarchical hidden Markov model ({HHMM}) is a generalization of the hidden Markov model ({HMM}) that models sequences with structure at many length/time scales [{FST}98]. Unfortunately, the original inference algorithm is rather complicated, and takes O(T3) time, where is the length of the sequence, making it impractical for many domains. In this paper, we show how {HHMMs} are a special kind of dynamic Bayesian network ({DBN}), and thereby derive a much simpler inference algorithm, which only takes O(T) time. Furthermore, by drawing the connection between {HHMMs} and {DBNs}, we enable the application of many standard approximation techniques to further speed up inference.},
	pages = {833--840},
	booktitle = {Proceedings of the 14th International Conference on Neural Information Processing Systems: Natural and Synthetic},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P. and Paskin, Mark A.},
	urldate = {2017-08-27},
	date = {2001},
	file = {Murphy and Paskin - 2001 - Linear Time Inference in Hierarchical HMMs.pdf:/Users/bert/Zotero/storage/TSFKL75I/Murphy and Paskin - 2001 - Linear Time Inference in Hierarchical HMMs.pdf:application/pdf},
}

@article{wang_general_2015,
	title = {A General Method for Robust Bayesian Modeling},
	url = {http://arxiv.org/abs/1510.05078},
	abstract = {Robust Bayesian models are appealing alternatives to standard models, providing protection from data that contains outliers or other departures from the model assumptions. Historically, robust models were mostly developed on a case-by-case basis; examples include robust linear regression, robust mixture models, and bursty topic models. In this paper we develop a general approach to robust Bayesian modeling. We show how to turn an existing Bayesian model into a robust model, and then develop a generic strategy for computing with it. We use our method to study robust variants of several models, including linear regression, Poisson regression, logistic regression, and probabilistic topic models. We discuss the connections between our methods and existing approaches, especially empirical Bayes and James-Stein estimation.},
	journaltitle = {{arXiv}:1510.05078 [stat]},
	author = {Wang, Chong and Blei, David M.},
	urldate = {2017-08-27},
	date = {2015-10-17},
	eprinttype = {arxiv},
	eprint = {1510.05078},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/RZNY5TRF/1510.html:text/html;Wang and Blei - 2015 - A General Method for Robust Bayesian Modeling.pdf:/Users/bert/Zotero/storage/392HQUXC/Wang and Blei - 2015 - A General Method for Robust Bayesian Modeling.pdf:application/pdf},
}

@article{cui_continuous_2016,
	title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
	volume = {28},
	issn = {0899-7667},
	url = {http://www.mitpressjournals.org/doi/full/10.1162/NECO_a_00893},
	doi = {10.1162/NECO_a_00893},
	abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory ({HTM}) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of {HTM} sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the {HTM} sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The {HTM} model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the {HTM} sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
	pages = {2474--2504},
	number = {11},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
	urldate = {2017-08-26},
	date = {2016-09-14},
	file = {Cui et al. - 2016 - Continuous Online Sequence Learning with an Unsupe.pdf:/Users/bert/Zotero/storage/G56U5YTN/Cui et al. - 2016 - Continuous Online Sequence Learning with an Unsupe.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/6CNKTA55/NECO_a_00893.html:text/html},
}

@article{gelman_prior_2017,
	title = {The prior can generally only be understood in the context of the likelihood},
	url = {http://arxiv.org/abs/1708.07487},
	abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys' priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
	journaltitle = {{arXiv}:1708.07487 [stat]},
	author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
	urldate = {2017-08-26},
	date = {2017-08-24},
	eprinttype = {arxiv},
	eprint = {1708.07487},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9TQ5S4XS/1708.html:text/html;Gelman et al. - 2017 - The prior can generally only be understood in the .pdf:/Users/bert/Zotero/storage/58A5G4B5/Gelman et al. - 2017 - The prior can generally only be understood in the .pdf:application/pdf},
}

@article{arulkumaran_brief_2017,
	title = {A Brief Survey of Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1708.05866},
	abstract = {Deep reinforcement learning is poised to revolutionise the field of {AI} and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
	journaltitle = {{arXiv}:1708.05866 [cs, stat]},
	author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
	urldate = {2017-08-25},
	date = {2017-08-19},
	eprinttype = {arxiv},
	eprint = {1708.05866},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Learning},
	file = {Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:/Users/bert/Zotero/storage/AQAS8XRP/Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/N6H4732I/1708.html:text/html},
}

@article{sandhu_bayesian_2017,
	title = {Bayesian model selection using automatic relevance determination for nonlinear dynamical systems},
	volume = {320},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516313020},
	doi = {10.1016/j.cma.2017.01.042},
	abstract = {Bayesian model selection is augmented with automatic relevance determination ({ARD}) to perform model reduction of complex dynamical systems modelled by nonlinear, stochastic ordinary differential equations ({ODE}). Given noisy measurement data, a parametrically flexible model is envisioned to represent the dynamical system. A Bayesian model selection problem is posed to find the best model nested under the envisioned model. This model selection problem is transferred from the model space to hyper-parameter space by regularizing the parameter posterior space through a parametrized prior distribution called the {ARD} prior. The resulting joint prior pdf is the combination of parametrized {ARD} priors assigned to parameters whose relevance to the system dynamics is questionable and the known prior pdf for parameters whose relevance is known a priori. The hyper-parameter of each {ARD} prior explicitly represents the relevance of the corresponding model parameter. The hyper-parameters are estimated using the measurement data by performing evidence maximization or type-{II} maximum likelihood. Superfluous model parameters are switched off during evidence maximization by the corresponding {ARD} prior, forcing the model parameter to be irrelevant for prediction purposes. An efficient numerical implementation for evidence computation using Markov Chain Monte Carlo sampling of the parameter posterior distribution is presented for the case when the analytical evaluation of evidence is not possible. The {ARD} approach is validated with synthetic measurements generated from a nonlinear, unsteady aeroelastic oscillator consisting of a {NACA}0012 airfoil undergoing limit cycle oscillation. A set of intentionally flexible stochastic {ODEs} having different state-space formulation is proposed to model the synthetic data. {ARD} is used to obtain an optimal nested model corresponding to each proposed model. The optimal nested model with the maximum posterior model probability is chosen as the overall optimal model. {ARD} provides a flexible Bayesian platform to find the optimal nested model by eliminating the need to propose candidate nested models and its prior pdfs.},
	pages = {237--260},
	journaltitle = {Computer Methods in Applied Mechanics and Engineering},
	shortjournal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sandhu, Rimple and Pettit, Chris and Khalil, Mohammad and Poirel, Dominique and Sarkar, Abhijit},
	urldate = {2017-08-21},
	date = {2017-06-15},
	keywords = {Kalman filter, Automatic relevance determination, Bayesian model selection, Markov Chain Monte Carlo simulation},
	file = {Sandhu et al. - 2017 - Bayesian model selection using automatic relevance.pdf:/Users/bert/Zotero/storage/75V65HSX/Sandhu et al. - 2017 - Bayesian model selection using automatic relevance.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/6QFS8AFE/S0045782516313020.html:text/html},
}

@report{minka_discriminative_2005,
	title = {Discriminative models, not discriminative training},
	url = {http://research.microsoft.com/pubs/70229/tr-2005-144.pdf?origin=publication_detail},
	institution = {Technical Report {MSR}-{TR}-2005-144, Microsoft Research},
	author = {Minka, Tom},
	urldate = {2014-04-10},
	date = {2005},
	file = {Minka - 2005 - Discriminative models , not discriminative training.pdf:/Users/bert/Zotero/storage/N2ZUE9UP/Minka - 2005 - Discriminative models , not discriminative training.pdf:application/pdf;Minka - 2005 - Discriminative models , not discriminative training.pdf:/Users/bert/Zotero/storage/S72HH9ER/Minka - 2005 - Discriminative models , not discriminative training.pdf:application/pdf},
}

@article{ilin_recurrent_2017,
	title = {Recurrent Ladder Networks},
	url = {http://arxiv.org/abs/1707.09219},
	abstract = {We propose a recurrent extension of the Ladder network, which is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, handling temporal information, modeling relations and interactions between objects.},
	journaltitle = {{arXiv}:1707.09219 [cs, stat]},
	author = {Ilin, Alexander and Prémont-Schwarz, Isabeau and Hao, Tele Hotloo and Rasmus, Antti and Boney, Rinu and Valpola, Harri},
	urldate = {2017-08-18},
	date = {2017-07-28},
	eprinttype = {arxiv},
	eprint = {1707.09219},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2M7JL8PW/1707.html:text/html;Ilin et al. - 2017 - Recurrent Ladder Networks.pdf:/Users/bert/Zotero/storage/AGV9HRYL/Ilin et al. - 2017 - Recurrent Ladder Networks.pdf:application/pdf},
}

@article{friston_bayesian_2016,
	title = {Bayesian model reduction and empirical Bayes for group ({DCM}) studies},
	volume = {128},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S105381191501037X},
	doi = {10.1016/j.neuroimage.2015.11.015},
	abstract = {This technical note describes some Bayesian procedures for the analysis of group studies that use nonlinear models at the first (within-subject) level – e.g., dynamic causal models – and linear models at subsequent (between-subject) levels. Its focus is on using Bayesian model reduction to finesse the inversion of multiple models of a single dataset or a single (hierarchical or empirical Bayes) model of multiple datasets. These applications of Bayesian model reduction allow one to consider parametric random effects and make inferences about group effects very efficiently (in a few seconds). We provide the relatively straightforward theoretical background to these procedures and illustrate their application using a worked example. This example uses a simulated mismatch negativity study of schizophrenia. We illustrate the robustness of Bayesian model reduction to violations of the (commonly used) Laplace assumption in dynamic causal modelling and show how its recursive application can facilitate both classical and Bayesian inference about group differences. Finally, we consider the application of these empirical Bayesian procedures to classification and prediction.},
	pages = {413--431},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Friston, Karl J. and Litvak, Vladimir and Oswal, Ashwini and Razi, Adeel and Stephan, Klaas E. and van Wijk, Bernadette C. M. and Ziegler, Gabriel and Zeidman, Peter},
	urldate = {2016-06-28},
	date = {2016-03},
	keywords = {Bayesian model reduction, classification, Dynamic causal modelling, Empirical Bayes, Fixed effects, Hierarchical modelling, Random effects},
	file = {Friston et al. - 2016 - Bayesian model reduction and empirical Bayes for g.pdf:/Users/bert/Zotero/storage/JFISLADB/Friston et al. - 2016 - Bayesian model reduction and empirical Bayes for g.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/P9ZPLWXX/S105381191501037X.html:text/html},
}

@thesis{reller_state-space_2012,
	title = {State-Space Methods in Statistical Signal Processing: New Ideas and Applications},
	institution = {{ETH} Zurich},
	type = {phdthesis},
	author = {Reller, Christoph},
	urldate = {2014-04-10},
	date = {2012},
	file = {Reller - 2012 - State-Space Methods in Statistical Signal Processi.pdf:/Users/bert/Zotero/storage/MQX9UUKP/Reller - 2012 - State-Space Methods in Statistical Signal Processi.pdf:application/pdf},
}

@article{turner_time-frequency_2014,
	title = {Time-frequency analysis as probabilistic infrence},
	author = {Turner, Ryan D. and Sahani, Maneesh},
	date = {2014},
	file = {GitHub - ret26/probFB\: probabilistic time-frequency analysis:/Users/bert/Zotero/storage/KK2Z5ZNU/probFB.html:text/html;gp-audio.pdf:/Users/bert/Zotero/storage/F5FXUN3X/gp-audio.pdf:application/pdf;GTFtNMF < Public/Turner < Foswiki:/Users/bert/Zotero/storage/WAXI7G59/GTFtNMF.html:text/html;GTFtNMF.html:/Users/bert/Zotero/storage/L8N7R7N7/GTFtNMF.html:text/html;probFB.html:/Users/bert/Zotero/storage/YH5XQNH9/probFB.html:text/html;supplementary-material.pdf:/Users/bert/Zotero/storage/JE4DSHRE/supplementary-material.pdf:application/pdf;Turner - 2012 - grant research note- Audio time-frequency analysis as probabilistic inference.pdf:/Users/bert/Zotero/storage/IWJ8MTJS/turner-first-grant-note.pdf:application/pdf;Turner and Sahani - 2014 - Time-frequency analysis as probabilistic infrence.pdf:/Users/bert/Zotero/storage/IATFVS5D/Turner and Sahani - 2014 - Time-frequency analysis as probabilistic infrence.pdf:application/pdf},
}

@article{friston_active_2015,
	title = {Active inference and epistemic value},
	volume = {0},
	issn = {1758-8928},
	url = {http://dx.doi.org/10.1080/17588928.2015.1020053},
	doi = {10.1080/17588928.2015.1020053},
	abstract = {We offer a formal treatment of choice behaviour based on the premise that agents minimise the expected free energy of future outcomes. Crucially, the negative free energy or quality of a policy can be decomposed into extrinsic and epistemic (or intrinsic) value. Minimising expected free energy is therefore equivalent to maximising extrinsic value or expected utility (defined in terms of prior preferences or goals), while maximising information gain or intrinsic value (reducing uncertainty about the causes of valuable outcomes). The resulting scheme resolves the exploration-exploitation dilemma: epistemic value is maximised until there is no further information gain, after which exploitation is assured through maximisation of extrinsic value. This is formally consistent with the Infomax principle, generalising formulations of active vision based upon salience (Bayesian surprise) and optimal decisions based on expected utility and risk-sensitive (Kullback-Leibler) control. Furthermore, as with previous active inference formulations of discrete (Markovian) problems, ad hoc softmax parameters become the expected (Bayes-optimal) precision of beliefs about, or confidence in, policies. This article focuses on the basic theory, illustrating the ideas with simulations. A key aspect of these simulations is the similarity between precision updates and dopaminergic discharges observed in conditioning paradigms.},
	pages = {null},
	issue = {ja},
	journaltitle = {Cognitive Neuroscience},
	author = {Friston, Karl and Rigoli, Francesco and Ognibene, Dimitri and Mathys, Christoph and {FitzGerald}, Thomas and Pezzulo, Giovanni},
	urldate = {2015-02-22},
	date = {2015-02-17},
	pmid = {25689102},
	file = {Friston et al. - 2015 - Active inference and epistemic value.pdf:/Users/bert/Zotero/storage/PP98MUVH/Friston et al. - 2015 - Active inference and epistemic value.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/DJXDIQ7X/17588928.2015.html:text/html;Snapshot:/Users/bert/Zotero/storage/JUAJVU3Q/17588928.2015.html:text/html},
}

@article{bogacz_tutorial_2015,
	title = {A tutorial on the free-energy framework for modelling perception and learning},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249615000759},
	doi = {10.1016/j.jmp.2015.11.003},
	abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Bogacz, Rafal},
	urldate = {2016-02-02},
	date = {2015},
	file = {Bogacz - 2015 - A tutorial on the free-energy framework for modell.pdf:/Users/bert/Zotero/storage/5UBN2R6R/Bogacz - 2015 - A tutorial on the free-energy framework for modell.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/GJXIQ45H/S0022249615000759.html:text/html},
}

@article{chen_deep_2016,
	title = {Deep attractor network for single-microphone speaker separation},
	url = {http://arxiv.org/abs/1611.08930},
	abstract = {Despite the overwhelming success of deep learning in various speech processing tasks, the problem of separating simultaneous speakers in a mixture remains challenging. Two major difficulties in such systems are the arbitrary source permutation and unknown number of sources in the mixture. We propose a novel deep learning framework for single channel speech separation by creating attractor points in high dimensional embedding space of the acoustic signals which pull together the time-frequency bins corresponding to each source. Attractor points in this study are created by finding the centroids of the sources in the embedding space, which are subsequently used to determine the similarity of each bin in the mixture to each source. The network is then trained to minimize the reconstruction error of each source by optimizing the embeddings. The proposed model is different from prior works in that it implements an end-to-end training, and it does not depend on the number of sources in the mixture. Two strategies are explored in the test time, K-means and fixed attractor points, where the latter requires no post-processing and can be implemented in real-time. We evaluated our system on Wall Street Journal dataset and show 5.49{\textbackslash}textbackslash\% improvement over the previous state-of-the-art methods.},
	journaltitle = {{arXiv}:1611.08930 [cs]},
	author = {Chen, Zhuo and Luo, Yi and Mesgarani, Nima},
	urldate = {2017-08-07},
	date = {2016-11},
	keywords = {Computer Science - Learning, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/KYLGC89J/1611.html:text/html;Chen e.a. - 2016 - Deep attractor network for single-microphone speak.pdf:/Users/bert/Zotero/storage/42F37877/Chen e.a. - 2016 - Deep attractor network for single-microphone speak.pdf:application/pdf},
}

@inproceedings{joshi_personalizing_2017,
	title = {Personalizing Gesture Recognition Using Hierarchical Bayesian Neural Networks},
	url = {https://pdfs.semanticscholar.org/36e0/ba5f46d0eb0f75ed17b8776fa90c5ba83d43.pdf},
	booktitle = {Proc. {IEEE} Conf. on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Joshi, Ajjen and Ghosh, Soumya and Betke, Margrit and Sclaroff, Stan and Pfister, Hanspeter},
	urldate = {2017-08-07},
	date = {2017},
	file = {Joshi e.a. - 2017 - Personalizing Gesture Recognition Using Hierarchic.pdf:/Users/bert/Zotero/storage/YXUTD5HY/Joshi e.a. - 2017 - Personalizing Gesture Recognition Using Hierarchic.pdf:application/pdf},
}

@book{noauthor_expectation_nodate,
	title = {Expectation propagation as a way of life ({PDF} Download Available)},
	url = {https://www.researchgate.net/publication/269722347_Expectation_propagation_as_a_way_of_life},
	abstract = {Official Full-Text Paper ({PDF}): Expectation propagation as a way of life},
	file = {ep_arxiv.pdf:/Users/bert/Zotero/storage/ZT7R27L6/ep_arxiv.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/6NG574DP/269722347_Expectation_propagation_as_a_way_of_life.html:text/html},
}

@article{pitkow_inference_2017,
	title = {Inference in the Brain: Statistics Flowing in Redundant Population Codes},
	volume = {94},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731730466X},
	doi = {10.1016/j.neuron.2017.05.028},
	shorttitle = {Inference in the Brain},
	abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.},
	pages = {943--953},
	number = {5},
	journaltitle = {Neuron},
	author = {Pitkow, Xaq and Angelaki, Dora E.},
	urldate = {2017-08-05},
	date = {2017-06},
	keywords = {inference, coding, message-passing, nonlinear, nuisance, population code, redundant, theory},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/NAR9TVNX/S089662731730466X.html:text/html},
}

@article{engstrom_great_2017,
	title = {Great expectations: a predictive processing account of automobile driving},
	volume = {0},
	issn = {1463-922X},
	url = {http://dx.doi.org/10.1080/1463922X.2017.1306148},
	doi = {10.1080/1463922X.2017.1306148},
	shorttitle = {Great expectations},
	abstract = {Predictive processing has been proposed as a unifying framework for understanding brain function, suggesting that cognition and behaviour can be fundamentally understood based on the single principle of prediction error minimisation. According to predictive processing, the brain is a statistical organ that continuously attempts get a grip on states in the world by predicting how these states cause sensory input and minimising the deviations between the predicted and actual input. While these ideas have had a strong influence in neuroscience and cognitive science, they have so far not been adopted in applied human factors research. The present paper represents a first attempt to do so, exploring how predictive processing concepts can be used to understand automobile driving. It is shown how a framework based on predictive processing may provide a novel perspective on a range of driving phenomena and offer a unifying framework for traditionally disparate human factors models.},
	pages = {1--39},
	number = {0},
	journaltitle = {Theoretical Issues in Ergonomics Science},
	author = {Engström, Johan and Bärgman, Jonas and Nilsson, Daniel and Seppelt, Bobbie and Markkula, Gustav and Piccinini, Giulio Bianchi and Victor, Trent},
	urldate = {2017-08-05},
	date = {2017-04},
	keywords = {action, driver behaviour, driving, expectancy, perception, Predictive processing},
	file = {Engström e.a. - 2017 - Great expectations a predictive processing accoun.pdf:/Users/bert/Zotero/storage/HK8CXJJY/Engström e.a. - 2017 - Great expectations a predictive processing accoun.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4EAEF3VY/1463922X.2017.html:text/html},
}

@article{bamler_structured_2017,
	title = {Structured Black Box Variational Inference for Latent Time Series Models},
	url = {http://arxiv.org/abs/1707.01069},
	abstract = {Continuous latent time series models are prevalent in Bayesian modeling; examples include the Kalman filter, dynamic collaborative filtering, or dynamic topic models. These models often benefit from structured, non mean field variational approximations that capture correlations between time steps. Black box variational inference with reparameterization gradients ({BBVI}) allows us to explore a rich new class of Bayesian non-conjugate latent time series models; however, a naive application of {BBVI} to such structured variational models would scale quadratically in the number of time steps. We describe a {BBVI} algorithm analogous to the forward-backward algorithm which instead scales linearly in time. It allows us to efficiently sample from the variational distribution and estimate the gradients of the {ELBO}. Finally, we show results on the recently proposed dynamic word embedding model, which was trained using our method.},
	journaltitle = {{arXiv}:1707.01069 [cs, stat]},
	author = {Bamler, Robert and Mandt, Stephan},
	urldate = {2017-08-02},
	date = {2017-07},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/JFCI8R5L/1707.html:text/html;Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:/Users/bert/Zotero/storage/AIZEUATJ/Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:application/pdf},
}

@article{zion_golumbic_temporal_2012,
	title = {Temporal Context in Speech Processing and Attentional Stream Selection: A Behavioral and Neural perspective},
	volume = {122},
	issn = {0093-934X},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3340429/},
	doi = {10.1016/j.bandl.2011.12.010},
	shorttitle = {Temporal Context in Speech Processing and Attentional Stream Selection},
	abstract = {The human capacity for processing speech is remarkable, especially given that information in speech unfolds over multiple time scales concurrently. Similarly notable is our ability to filter out of extraneous sounds and focus our attention on one conversation, epitomized by the ‘Cocktail Party’ effect. Yet, the neural mechanisms underlying on-line speech decoding and attentional stream selection are not well understood. We review findings from behavioral and neurophysiological investigations that underscore the importance of the temporal structure of speech for achieving these perceptual feats. We discuss the hypothesis that entrainment of ambient neuronal oscillations to speech’s temporal structure, across multiple time-scales, serves to facilitate its decoding and underlies the selection of an attended speech stream over other competing input. In this regard, speech decoding and attentional stream selection are examples of ‘active sensing’, emphasizing an interaction between proactive and predictive top-down modulation of neuronal dynamics and bottom-up sensory input.},
	pages = {151--161},
	number = {3},
	journaltitle = {Brain and language},
	author = {Zion Golumbic, Elana M. and Poeppel, David and Schroeder, Charles E.},
	urldate = {2017-08-02},
	date = {2012-09},
	pmid = {22285024},
	pmcid = {PMC3340429},
	file = {Zion Golumbic et al. - 2012 - Temporal Context in Speech Processing and Attentio.pdf:/Users/bert/Zotero/storage/FEGX5ZZK/Zion Golumbic et al. - 2012 - Temporal Context in Speech Processing and Attentio.pdf:application/pdf},
}

@inproceedings{maystre_just_2017,
	title = {Just Sort It! A Simple and Effective Approach to Active Preference Learning},
	url = {http://proceedings.mlr.press/v70/maystre17a.html},
	abstract = {We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all compariso...},
	pages = {2344--2353},
	booktitle = {{PMLR}},
	author = {Maystre, Lucas and Grossglauser, Matthias},
	date = {2017-07},
	langid = {english},
	file = {Maystre en Grossglauser - 2017 - Just Sort It! A Simple and Effective Approach to A.pdf:/Users/bert/Zotero/storage/QWX4E6LX/Maystre en Grossglauser - 2017 - Just Sort It! A Simple and Effective Approach to A.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XVB676JR/maystre17a.html:text/html},
}

@inproceedings{asadi_alternative_2017,
	title = {An Alternative Softmax Operator for Reinforcement Learning},
	url = {http://proceedings.mlr.press/v70/asadi17a.html},
	abstract = {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is...},
	pages = {243--252},
	booktitle = {{PMLR}},
	author = {Asadi, Kavosh and Littman, Michael L.},
	date = {2017-07},
	langid = {english},
	file = {Asadi and Littman - 2017 - An Alternative Softmax Operator for Reinforcement .pdf:/Users/bert/Zotero/storage/9LU8TIIK/Asadi and Littman - 2017 - An Alternative Softmax Operator for Reinforcement .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BALRCF3G/asadi17a.html:text/html},
}

@inproceedings{chen_learning_2017,
	title = {Learning to Learn without Gradient Descent by Gradient Descent},
	url = {http://proceedings.mlr.press/v70/chen17e.html},
	abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they ca...},
	pages = {748--756},
	booktitle = {{PMLR}},
	author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gómez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and Freitas, Nando},
	date = {2017-07},
	langid = {english},
	file = {Chen e.a. - 2017 - Learning to Learn without Gradient Descent by Grad.pdf:/Users/bert/Zotero/storage/GWAYVUBV/Chen e.a. - 2017 - Learning to Learn without Gradient Descent by Grad.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/VW7FYHN4/chen17e.html:text/html},
}

@inproceedings{engel_neural_2017,
	title = {Neural Audio Synthesis of Musical Notes with {WaveNet} Autoencoders},
	url = {http://proceedings.mlr.press/v70/engel17a.html},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas t...},
	pages = {1068--1077},
	booktitle = {{PMLR}},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
	date = {2017-07},
	langid = {english},
	file = {Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:/Users/bert/Zotero/storage/YQCF8USE/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/8BRB8AYL/engel17a.html:text/html},
}

@article{hawkins_why_2017,
	title = {Why Does the Neocortex Have Layers and Columns, A Theory of Learning the 3D Structure of the World},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://www.biorxiv.org/content/early/2017/07/12/162263},
	doi = {10.1101/162263},
	abstract = {Neocortical regions are organized into columns and layers. Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remains a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Columns integrate their changing inputs over time to learn complete models of observed objects. Lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location is calculated within the sub-granular layers of each column. The representation of location is relative to the object being sensed. Pairing sensory features with locations is a requirement for modeling objects and therefore must occur somewhere in the neocortex. We propose it occurs in every column in every region. Our network model contains two layers and one or more columns. Simulations show that small single-column networks can learn to recognize hundreds of complex multi-dimensional objects. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.},
	pages = {162263},
	journaltitle = {{bioRxiv}},
	author = {Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},
	date = {2017-07},
	langid = {english},
	file = {Hawkins e.a. - 2017 - Why Does the Neocortex Have Layers and Columns, A .pdf:/Users/bert/Zotero/storage/6K6JB98L/Hawkins e.a. - 2017 - Why Does the Neocortex Have Layers and Columns, A .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/66XHN7C9/162263.html:text/html},
}

@incollection{elhilali_modeling_2017,
	title = {Modeling the Cocktail Party Problem},
	isbn = {978-3-319-51660-8 978-3-319-51662-2},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-51662-2_5},
	series = {Springer Handbook of Auditory Research},
	abstract = {Modeling the cocktail party problem entails developing a computational framework able to describe what the auditory system does when faced with a complex auditory scene. While completely intuitive and omnipresent in humans and animals alike, translating this remarkable ability into a quantitative model remains a challenge. This chapter touches on difficulties facing the field in terms of defining the theoretical principles that govern auditory scene analysis, as well as reconciling current knowledge about perceptual and physiological data with their formulation into computational models. The chapter reviews some of the computational theories, algorithmic strategies, and neural infrastructure proposed in the literature for developing information systems capable of processing multisource sound inputs. Because of divergent interests from various disciplines in the cocktail party problem, the body of literature modeling this effect is equally diverse and multifaceted. The chapter touches on the various approaches used in modeling auditory scene analysis from biomimetic models to strictly engineering systems.},
	pages = {111--135},
	booktitle = {The Auditory System at the Cocktail Party},
	publisher = {Springer, Cham},
	author = {Elhilali, Mounya},
	date = {2017},
	langid = {english},
	file = {Snapshot:/Users/bert/Zotero/storage/VKZQX6AC/978-3-319-51662-2_5.html:text/html},
}

@inproceedings{harding_auditory_2007,
	title = {Auditory Gist Perception: An Alternative to Attentional Selection of Auditory Streams?},
	isbn = {978-3-540-77342-9 978-3-540-77343-6},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-77343-6_26},
	doi = {10.1007/978-3-540-77343-6_26},
	series = {Lecture Notes in Computer Science},
	shorttitle = {Auditory Gist Perception},
	abstract = {The idea that the gist of a visual scene is perceived before attention is focused on the details of a particular object is becoming increasingly popular. In the auditory system, on the other hand, it is typically assumed that the sensory signal is first broken down into streams and then attention is applied to select one of the streams. We consider evidence for an alternative: that, in close analogy with the visual system, the gist of an auditory scene is perceived and only afterwards attention is paid to relevant constituents. We find that much experimental evidence is consistent with such a proposal, and we suggest some possibilities for gist representations.},
	pages = {399--416},
	booktitle = {Attention in Cognitive Systems. Theories and Systems from an Interdisciplinary Viewpoint},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Harding, Sue and Cooke, Martin and König, Peter},
	date = {2007-01},
	langid = {english},
	file = {Harding e.a. - 2007 - Auditory Gist Perception An Alternative to Attent.pdf:/Users/bert/Zotero/storage/M3Z4LNEW/Harding e.a. - 2007 - Auditory Gist Perception An Alternative to Attent.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/JIR74JBF/978-3-540-77343-6_26.html:text/html},
}

@article{snyder_recent_2017,
	title = {Recent advances in exploring the neural underpinnings of auditory scene perception},
	volume = {1396},
	issn = {1749-6632},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/nyas.13317/abstract},
	doi = {10.1111/nyas.13317},
	abstract = {Studies of auditory scene analysis have traditionally relied on paradigms using artificial sounds—and conventional behavioral techniques—to elucidate how we perceptually segregate auditory objects or streams from each other. In the past few decades, however, there has been growing interest in uncovering the neural underpinnings of auditory segregation using human and animal neuroscience techniques, as well as computational modeling. This largely reflects the growth in the fields of cognitive neuroscience and computational neuroscience and has led to new theories of how the auditory system segregates sounds in complex arrays. The current review focuses on neural and computational studies of auditory scene perception published in the last few years. Following the progress that has been made in these studies, we describe (1) theoretical advances in our understanding of the most well-studied aspects of auditory scene perception, namely segregation of sequential patterns of sounds and concurrently presented sounds; (2) the diversification of topics and paradigms that have been investigated; and (3) how new neuroscience techniques (including invasive neurophysiology in awake humans, genotyping, and brain stimulation) have been used in this field.},
	pages = {39--55},
	number = {1},
	journaltitle = {Annals of the New York Academy of Sciences},
	author = {Snyder, Joel S. and Elhilali, Mounya},
	urldate = {2017-08-01},
	date = {2017-05},
	langid = {english},
	keywords = {auditory scene analysis, auditory stream segregation, change deafness, concurrent sound segregation, informational masking},
	file = {Snapshot:/Users/bert/Zotero/storage/P7AFKBVK/abstract.html:text/html;Snyder en Elhilali - 2017 - Recent advances in exploring the neural underpinni.pdf:/Users/bert/Zotero/storage/B44XMNCJ/Snyder en Elhilali - 2017 - Recent advances in exploring the neural underpinni.pdf:application/pdf},
}

@inproceedings{levchuk_application_2017,
	title = {Application of free energy minimization to the design of adaptive multi-agent teams},
	volume = {10206},
	url = {http://dx.doi.org/10.1117/12.2263542},
	doi = {10.1117/12.2263542},
	abstract = {Many novel {DoD} missions, from disaster relief to cyber reconnaissance, require teams of humans and machines with diverse capabilities. Current solutions do not account for heterogeneity of agent capabilities, uncertainty of team knowledge, and dynamics of and dependencies between tasks and agent roles, resulting in brittle teams. Most importantly, the state-of-the-art team design solutions are either centralized, imposing role and relation assignment onto agents, or completely distributed, suitable for only homogeneous organizations such as swarms. Centralized design models can’t provide insights for team’s self-organization, i.e. adapting team structure over time in distributed collaborative manner by team members with diverse expertise and responsibilities. In this paper we present an information-theoretic formalization of team composition and structure adaptation using a minimization of variational free energy. The structure adaptation is obtained in an iterative distributed and collaborative manner without the need for centralized control. We show that our model is lightweight, predictive, and produces team structures that theoretically approximate an optimal policy for team adaptation. Our model also provides a unique coupling between the structure and action policy, and captures three essential processes of learning, perception, and control.},
	pages = {102060E--102060E--17},
	author = {Levchuk, Georgiy and Pattipati, Krishna and Fouse, Adam and Serfaty, Daniel},
	urldate = {2017-08-01},
	date = {2017},
}

@book{friston_et_al._spm12_2014,
	title = {{SPM}12 toolbox, http://www.fil.ion.ucl.ac.uk/spm/software/},
	author = {Friston et al., Karl J.},
	date = {2014},
}

@article{friston_free_2012,
	title = {A Free Energy Principle for Biological Systems},
	volume = {14},
	issn = {1099-4300},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3510653/},
	doi = {10.3390/e14112100},
	abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
	pages = {2100--2121},
	number = {11},
	journaltitle = {Entropy (Basel, Switzerland)},
	author = {Friston, Karl J.},
	urldate = {2017-07-28},
	date = {2012-11},
	pmid = {23204829},
	pmcid = {PMC3510653},
	file = {Friston - 2012 - A Free Energy Principle for Biological Systems.pdf:/Users/bert/Zotero/storage/DQSETY73/Friston - 2012 - A Free Energy Principle for Biological Systems.pdf:application/pdf},
}

@article{loeliger_factor_2017,
	title = {Factor Graphs for Quantum Probabilities},
	volume = {{PP}},
	issn = {0018-9448},
	doi = {10.1109/TIT.2017.2716422},
	abstract = {A factor-graph representation of quantum-mechanical probabilities (involving any number of measurements) is proposed. Unlike standard statistical models, the proposed representation uses auxiliary variables (state variables) that are not random variables. All joint probability distributions are marginals of some complex-valued function q, and it is demonstrated how the basic concepts of quantum mechanics relate to factorizations and marginals of q.},
	pages = {1--1},
	number = {99},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Loeliger, H. A. and Vontobel, P. O.},
	date = {2017},
	keywords = {Probability distribution, factor graphs, Hidden Markov models, graphical models, closing-the-box operation, marginalization, quantum coding, Quantum mechanics, Random variables, Standards, Tensile stress, tensor networks},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/2QZPSSZC/7950994.html:text/html;Loeliger and Vontobel - 2017 - Factor Graphs for Quantum Probabilities.pdf:/Users/bert/Zotero/storage/YYNFQLGY/Loeliger and Vontobel - 2017 - Factor Graphs for Quantum Probabilities.pdf:application/pdf},
}

@article{guo_lmmse_2008,
	title = {{LMMSE} turbo equalization based on factor graphs},
	volume = {26},
	issn = {0733-8716},
	doi = {10.1109/JSAC.2008.080208},
	abstract = {In this paper, a vector-form factor graph representation is derived for intersymbol interference ({ISI}) channels. The resultant graphs have a tree-structure that avoids the short cycle problem in existing graph approaches. Based on a joint Gaussian approximation, we establish a connection between the {LLR} (log-likelihood ratio) estimator for a linear system driven by binary inputs and the {LMMSE} (linear minimum mean-square error) estimator for a linear system driven by Gaussian inputs. This connection facilitates the application of the recently proposed Gaussian message passing technique to the cycle-free graphs for {ISI} channels. We also show the equivalence between the proposed approach and the Wang-Poor approach based on the {LMMSE} principle. An attractive advantage of the proposed approach is its intrinsic parallel structure. Simulation results are provided to demonstrate this property.},
	pages = {311--319},
	number = {2},
	journaltitle = {{IEEE} Journal on Selected Areas in Communications},
	author = {Guo, Q. and Ping, L.},
	date = {2008-02},
	keywords = {Message passing, message passing, graph theory, Parity check codes, equalisers, Equalizers, Gaussian approximation, intersymbol interference channel, Iterative algorithms, Iterative decoding, joint Gaussian approximation, linear minimum mean-square error, Linear systems, log-likelihood ratio estimator, mean square error methods, Performance loss, Tree graphs, tree-structure, turbo codes, turbo equalization, vector-form factor graph representation, Wang-Poor approach},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/2RIG8D7G/4444762.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/VMH3SQH7/4444762.html:text/html},
}

@article{friston_statistical_1994,
	title = {Statistical parametric maps in functional imaging: a general linear approach},
	volume = {2},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/hbm.460020402/full, see http://www.fil.ion.ucl.ac.uk/spm/software for SPM software},
	shorttitle = {Statistical parametric maps in functional imaging},
	pages = {189--210},
	number = {4},
	journaltitle = {Human brain mapping},
	author = {Friston, Karl J. and Holmes, Andrew P. and Worsley, Keith J. and Poline, J.-P. and Frith, Chris D. and Frackowiak, Richard {SJ}},
	urldate = {2017-07-28},
	date = {1994},
	file = {Friston e.a. - 1994 - Statistical parametric maps in functional imaging.pdf:/Users/bert/Zotero/storage/DUI38H62/Friston e.a. - 1994 - Statistical parametric maps in functional imaging.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/I6TX7S87/full.html:text/html},
}

@article{lunn_winbugs_2000,
	title = {{WinBUGS} - A Bayesian modelling framework: Concepts, structure, and extensibility},
	volume = {10},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/article/10.1023/A:1008929526011},
	doi = {10.1023/A:1008929526011},
	shorttitle = {{WinBUGS} - A Bayesian modelling framework},
	abstract = {{WinBUGS} is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the {BUGS} language or pictorially using a graphical interface called {DoodleBUGS}. {WinBUGS} processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with {WinBUGS} for users with specialized requirements. It is also possible to interface with {WinBUGS} at a lower level by incorporating new object types that may be used by {WinBUGS} without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the {WinBUGS} source-code.},
	pages = {325--337},
	number = {4},
	journaltitle = {Statistics and Computing},
	author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
	date = {2000-10},
	file = {Snapshot:/Users/bert/Zotero/storage/F8H86DPZ/A1008929526011.html:text/html;Snapshot:/Users/bert/Zotero/storage/6R5QT7W5/A1008929526011.html:text/html},
}

@book{minka_infer.net_2014,
	title = {Infer.{NET} 2.6, http://research.microsoft.com/infernet},
	url = {http://research.microsoft.com/infernet},
	publisher = {Microsoft Research Cambridge},
	author = {Minka, T. and Winn, J.M. and Guiver, J.P. and Webster, S. and Zaykov, Y. and Yangel, B. and Spengler, A. and Bronskill, J.},
	date = {2014},
}

@book{taylor_pyflux:_2016,
	title = {{PyFlux}: An open source time series library for Python, http://www.pyflux.com},
	url = {http://www.pyflux.com},
	author = {Taylor, Ross},
	date = {2016},
}

@article{bastos_canonical_2012,
	title = {Canonical Microcircuits for Predictive Coding},
	volume = {76},
	issn = {0896-6273},
	url = {http://www.cell.com/neuron/abstract/S0896-6273(12)00959-2},
	doi = {10.1016/j.neuron.2012.10.038},
	abstract = {This Perspective considers the influential notion of a canonical (cortical) microcircuit in light of recent theories about neuronal processing. Specifically, we conciliate quantitative studies of microcircuitry and the functional logic of neuronal computations. We revisit the established idea that message passing among hierarchical cortical areas implements a form of Bayesian inference—paying careful attention to the implications for intrinsic connections among neuronal populations. By deriving canonical forms for these computations, one can associate specific neuronal populations with specific computational roles. This analysis discloses a remarkable correspondence between the microcircuitry of the cortical column and the connectivity implied by predictive coding. Furthermore, it provides some intuitive insights into the functional asymmetries between feedforward and feedback connections and the characteristic frequencies over which they operate.},
	pages = {695--711},
	number = {4},
	journaltitle = {Neuron},
	author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
	date = {2012-11},
	pmid = {23177956},
	file = {Bastos e.a. - 2012 - Canonical Microcircuits for Predictive Coding.pdf:/Users/bert/Zotero/storage/M3L7X78L/Bastos e.a. - 2012 - Canonical Microcircuits for Predictive Coding.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XIXFSCTH/S0896-6273(12)00959-2.html:text/html},
}

@article{mlynarski_learning_2017,
	title = {Learning Mid-Level Auditory Codes from Natural Sound Statistics},
	url = {http://arxiv.org/abs/1701.07138},
	abstract = {Interaction with the world requires an organism to transform sensory signals into representations in which behaviorally meaningful properties of the environment are made explicit. These representations are derived through cascades of neuronal processing stages in which neurons at each stage recode the output of preceding stages. Explanations of sensory coding may thus involve understanding how low-level patterns are combined into more complex structures. Although models exist in the visual domain to explain how mid-level features such as junctions and curves might be derived from oriented filters in early visual cortex, little is known about analogous grouping principles for mid-level auditory representations. We propose a hierarchical generative model of natural sounds that learns combinations of spectrotemporal features from natural stimulus statistics. In the first layer the model forms a sparse convolutional code of spectrograms using a dictionary of learned spectrotemporal kernels. To generalize from specific kernel activation patterns, the second layer encodes patterns of time-varying magnitude of multiple first layer coefficients. Because second-layer features are sensitive to combinations of spectrotemporal features, the representation they support encodes more complex acoustic patterns than the first layer. When trained on corpora of speech and environmental sounds, some second-layer units learned to group spectrotemporal features that occur together in natural sounds. Others instantiate opponency between dissimilar sets of spectrotemporal features. Such groupings might be instantiated by neurons in the auditory cortex, providing a hypothesis for mid-level neuronal computation.},
	journaltitle = {{arXiv}:1701.07138 [cs, q-bio]},
	author = {Młynarski, Wiktor and {McDermott}, Josh H.},
	urldate = {2017-07-26},
	date = {2017-01},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Sound},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3XWITNJW/1701.html:text/html;Młynarski and McDermott - 2017 - Learning Mid-Level Auditory Codes from Natural Sou.pdf:/Users/bert/Zotero/storage/TKYJ9G2N/Młynarski and McDermott - 2017 - Learning Mid-Level Auditory Codes from Natural Sou.pdf:application/pdf},
}

@inproceedings{kucukelbir_automatic_2015,
	title = {Automatic Variational Inference in Stan},
	url = {http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf},
	pages = {568--576},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2017-07-28},
	date = {2015},
	file = {Kucukelbir e.a. - 2015 - Automatic Variational Inference in Stan.pdf:/Users/bert/Zotero/storage/9CC3TEAQ/Kucukelbir e.a. - 2015 - Automatic Variational Inference in Stan.pdf:application/pdf;NIPS Snapshort:/Users/bert/Zotero/storage/QXM9HLJL/5758-automatic-variational-inference-in-stan.html:text/html},
}

@article{pitkow_how_2017,
	title = {How the brain might work: statistics flowing in redundant population codes},
	url = {http://arxiv.org/abs/1702.03492},
	shorttitle = {How the brain might work},
	abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.},
	journaltitle = {{arXiv}:1702.03492 [q-bio]},
	author = {Pitkow, Xaq and Angelaki, Dora},
	urldate = {2017-07-26},
	date = {2017-02},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QLAFYUP8/1702.html:text/html;Pitkow and Angelaki - 2017 - How the brain might work statistics flowing in re.pdf:/Users/bert/Zotero/storage/NQMVM7R8/Pitkow and Angelaki - 2017 - How the brain might work statistics flowing in re.pdf:application/pdf},
}

@article{aponte_mpdcm:_2016,
	title = {mpdcm: A toolbox for massively parallel dynamic causal modeling},
	volume = {257},
	issn = {0165-0270},
	url = {http://www.sciencedirect.com/science/article/pii/S0165027015003428},
	doi = {10.1016/j.jneumeth.2015.09.009},
	shorttitle = {mpdcm},
	abstract = {Background Dynamic causal modeling ({DCM}) for {fMRI} is an established method for Bayesian system identification and inference on effective brain connectivity. {DCM} relies on a biophysical model that links hidden neuronal activity to measurable {BOLD} signals. Currently, biophysical simulations from {DCM} constitute a serious computational hindrance. Here, we present Massively Parallel Dynamic Causal Modeling (mpdcm), a toolbox designed to address this bottleneck. New method mpdcm delegates the generation of simulations from {DCM}'s biophysical model to graphical processing units ({GPUs}). Simulations are generated in parallel by implementing a low storage explicit Runge–Kutta's scheme on a {GPU} architecture. mpdcm is publicly available under the {GPLv}3 license. Results We found that mpdcm efficiently generates large number of simulations without compromising their accuracy. As applications of mpdcm, we suggest two computationally expensive sampling algorithms: thermodynamic integration and parallel tempering. Comparison with existing method(s) mpdcm is up to two orders of magnitude more efficient than the standard implementation in the software package {SPM}. Parallel tempering increases the mixing properties of the traditional Metropolis–Hastings algorithm at low computational cost given efficient, parallel simulations of a model. Conclusions Future applications of {DCM} will likely require increasingly large computational resources, for example, when the likelihood landscape of a model is multimodal, or when implementing sampling methods for multi-subject analysis. Due to the wide availability of {GPUs}, algorithmic advances can be readily available in the absence of access to large computer grids, or when there is a lack of expertise to implement algorithms in such grids.},
	pages = {7--16},
	journaltitle = {Journal of Neuroscience Methods},
	author = {Aponte, Eduardo A. and Raman, Sudhir and Sengupta, Biswa and Penny, Will D. and Stephan, Klaas E. and Heinzle, Jakob},
	urldate = {2017-02-21},
	date = {2016-01},
	keywords = {Bayesian model comparison, Dynamic causal modeling, {GPU}, Markov chain Monte Carlo, Model evidence, Model inversion, Parallel tempering, Thermodynamic integration},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/RPS3BE44/S0165027015003428.html:text/html},
}

@article{friston_embodied_2015,
	title = {Embodied Inference: or “I think therefore I am, if I am what I think”},
	shorttitle = {Embodied Inference},
	abstract = {This chapter considers situated and embodied cognition in terms of the free-energy principle. The free-energy formulation starts with the premise that biological agents must actively resist a natural tendency to disorder. It appeals to the idea that agents are essentially inference machines that},
	author = {Friston, Karl},
	date = {2015},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/HCHPFMKS/summary.html:text/html;Friston - Embodied Inference or “I think therefore I am, if.pdf:/Users/bert/Zotero/storage/JJYP7UWP/Friston - Embodied Inference or “I think therefore I am, if.pdf:application/pdf},
}

@article{mcgraw_personalized_2016,
	title = {Personalized Speech recognition on mobile devices},
	url = {http://arxiv.org/abs/1603.03185},
	abstract = {We describe a large vocabulary speech recognition system that is accurate, has low latency, and yet has a small enough memory and computational footprint to run faster than real-time on a Nexus 5 Android smartphone. We employ a quantized Long Short-Term Memory ({LSTM}) acoustic model trained with connectionist temporal classification ({CTC}) to directly predict phoneme targets, and further reduce its memory footprint using an {SVD}-based compression scheme. Additionally, we minimize our memory footprint by using a single language model for both dictation and voice command domains, constructed using Bayesian interpolation. Finally, in order to properly handle device-specific information, such as proper names and other context-dependent information, we inject vocabulary items into the decoder graph and bias the language model on-the-fly. Our system achieves 13.5\% word error rate on an open-ended dictation task, running with a median speed that is seven times faster than real-time.},
	journaltitle = {{arXiv}:1603.03185 [cs]},
	author = {{McGraw}, Ian and Prabhavalkar, Rohit and Alvarez, Raziel and Arenas, Montse Gonzalez and Rao, Kanishka and Rybach, David and Alsharif, Ouais and Sak, Hasim and Gruenstein, Alexander and Beaufays, Francoise and Parada, Carolina},
	urldate = {2016-11-02},
	date = {2016-03},
	keywords = {Computer Science - Learning, Computer Science - Sound, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/MHF2DB77/1603.html:text/html;McGraw et al. - 2016 - Personalized Speech recognition on mobile devices.pdf:/Users/bert/Zotero/storage/DUZDL35M/McGraw et al. - 2016 - Personalized Speech recognition on mobile devices.pdf:application/pdf},
}

@article{sato_how_2014,
	title = {How much to trust the senses: Likelihood learning},
	volume = {14},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?articleid=2213024},
	doi = {10.1167/14.13.13},
	shorttitle = {How much to trust the senses},
	pages = {13--13},
	number = {13},
	journaltitle = {Journal of Vision},
	author = {Sato, Yoshiyuki and Kording, Konrad P.},
	urldate = {2016-11-04},
	date = {2014-11},
	keywords = {Bayesian models, context-dependent learning, likelihood learning, sensorimotor integration},
	file = {Sato and Kording - 2014 - How much to trust the senses Likelihood learning.pdf:/Users/bert/Zotero/storage/779BAA5F/Sato and Kording - 2014 - How much to trust the senses Likelihood learning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/SD3WZYUI/article.html:text/html;Snapshot:/Users/bert/Zotero/storage/I6JZNKGG/13.html:text/html},
}

@article{robert_expected_2016,
	title = {The expected demise of the Bayes factor},
	volume = {72},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249615000504},
	doi = {10.1016/j.jmp.2015.08.002},
	series = {Bayes Factors for Testing Hypotheses in Psychological Research: Practical Relevance and New Developments},
	abstract = {This note is a discussion commenting on the paper by Ly et al. on “Harold Jeffreys’s Default Bayes Factor Hypothesis Tests: Explanation, Extension, and Application in Psychology” and on the perceived shortcomings of the classical Bayesian approach to testing, while reporting on an alternative approach advanced by Kamary et al. (2014) as a solution to this quintessential inference problem.},
	pages = {33--37},
	journaltitle = {Journal of Mathematical Psychology},
	author = {Robert, Christian P.},
	urldate = {2016-11-09},
	date = {2016-06},
	keywords = {Bayesian inference, Bayes factor, Consistency, Decision theory, evidence, Loss function, Mixtures of distributions, Testing of hypotheses},
	file = {Robert - 2016 - The expected demise of the Bayes factor.pdf:/Users/bert/Zotero/storage/44SP66LZ/Robert - 2016 - The expected demise of the Bayes factor.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/VKMLANZN/S0022249615000504.html:text/html},
}

@article{chung_empirical_2014,
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks ({RNNs}). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory ({LSTM}) unit and a recently proposed gated recurrent unit ({GRU}). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found {GRU} to be comparable to {LSTM}.},
	journaltitle = {{arXiv}:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	urldate = {2016-11-21},
	date = {2014-12},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/IPB3AA4N/1412.html:text/html;Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:/Users/bert/Zotero/storage/BSMJQBEW/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf:application/pdf},
}

@article{abbeel_toward_2016,
	title = {Toward a Science of Autonomy for Physical Systems: Paths},
	url = {http://arxiv.org/abs/1609.05814},
	shorttitle = {Toward a Science of Autonomy for Physical Systems},
	abstract = {An Autonomous Physical System ({APS}) will be expected to reliably and independently evaluate, execute, and achieve goals while respecting surrounding rules, laws, or conventions. In doing so, an {APS} must rely on a broad spectrum of dynamic, complex, and often imprecise information about its surroundings, the task it is to perform, and its own sensors and actuators. For example, cleaning in a home or commercial setting requires the ability to perceive, grasp, and manipulate many physical objects, the ability to reliably perform a variety of subtasks such as washing, folding, and stacking, and knowledge about local conventions such as how objects are classified and where they should be stored. The information required for reliable autonomous operation may come from external sources and from the robot's own sensor observations or in the form of direct instruction by a trainer. Similar considerations apply across many domains - construction, manufacturing, in-home assistance, and healthcare. For example, surgeons spend many years learning about physiology and anatomy before they touch a patient. They then perform roughly 1000 surgeries under the tutelage of an expert surgeon, and they practice basic maneuvers such as suture tying thousands of times outside the operating room. All of these elements come together to achieve expertise at this task. Endowing a system with robust autonomy by traditional programming methods has thus far had limited success. Several promising new paths to acquiring and processing such data are emerging. This white paper outlines three promising research directions for enabling an {APS} to learn the physical and information skills necessary to perform tasks with independence and flexibility: Deep Reinforcement Learning, Human-Robot Interaction, and Cloud Robotics.},
	journaltitle = {{arXiv}:1609.05814 [cs]},
	author = {Abbeel, Pieter and Goldberg, Ken and Hager, Gregory and Shah, Julie},
	urldate = {2016-11-10},
	date = {2016-09},
	keywords = {Computer Science - Robotics, Computer Science - Computers and Society},
	file = {Abbeel et al. - 2016 - Toward a Science of Autonomy for Physical Systems.pdf:/Users/bert/Zotero/storage/7G6SH2XR/Abbeel et al. - 2016 - Toward a Science of Autonomy for Physical Systems.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/QHD2A9FE/1609.html:text/html},
}

@inproceedings{weber_reinforced_2015,
	title = {Reinforced Variational Inference},
	url = {http://approximateinference.org/accepted/WeberEtAl2015.pdf},
	booktitle = {Advances in Approximate Bayesian Inference},
	author = {Weber, Theophane and Heess, Nicolas and Eslami, S. M. Ali and Schulman, John and Wingate, David and Silver, David},
	urldate = {2016-11-21},
	date = {2015},
	file = {Weber et al. - 2015 - Reinforced Variational Inference.pdf:/Users/bert/Zotero/storage/5RDKJ5DD/Weber et al. - 2015 - Reinforced Variational Inference.pdf:application/pdf},
}

@article{kolbaek_speech_2017,
	title = {Speech Intelligibility Potential of General and Specialized Deep Neural Network Based Speech Enhancement Systems},
	volume = {25},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2016.2628641},
	abstract = {In this paper, we study aspects of single microphone speech enhancement ({SE}) based on deep neural networks ({DNNs}). Specifically, we explore the generalizability capabilities of state-of-the-art {DNN}-based {SE} systems with respect to the background noise type, the gender of the target speaker, and the signal-to-noise ratio ({SNR}). Furthermore, we investigate how specialized {DNN}-based {SE} systems, which have been trained to be either noise type specific, speaker specific or {SNR} specific, perform relative to {DNN} based {SE} systems that have been trained to be noise type general, speaker general, and {SNR} general. Finally, we compare how a {DNN}-based {SE} system trained to be noise type general, speaker general, and {SNR} general performs relative to a state-of-the-art short-time spectral amplitude minimum mean square error ({STSA}-{MMSE}) based {SE} algorithm. We show that {DNN}-based {SE} systems, when trained specifically to handle certain speakers, noise types and {SNRs}, are capable of achieving large improvements in estimated speech quality ({SQ}) and speech intelligibility ({SI}), when tested in matched conditions. Furthermore, we show that improvements in estimated {SQ} and {SI} can be achieved by a {DNN}-based {SE} system when exposed to unseen speakers, genders and noise types, given a large number of speakers and noise types have been used in the training of the system. In addition, we show that a {DNN}-based {SE} system that has been trained using a large number of speakers and a wide range of noise types outperforms a state-of-the-art {STSA}-{MMSE} based {SE} method, when tested using a range of unseen speakers and noise types. Finally, a listening test using several {DNN}-based {SE} systems tested in unseen speaker conditions show that these systems can improve {SI} for some {SNR} and noise type configurations but degrade {SI} for others.},
	pages = {153--167},
	number = {1},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Kolbæk, M. and Tan, Z. H. and Jensen, J.},
	date = {2017-01},
	keywords = {Signal processing algorithms, Training, least mean squares methods, Noise measurement, Speech enhancement, Speech, Deep neural networks, {DNN}-based {SE} systems, generalizability, ideal ratio mask, intelligibility, neural nets, short-time spectral amplitude minimum mean square error based {SE} algorithm, {SI}, Signal to noise ratio, Silicon, single microphone speech enhancement, specialized deep neural network based speech enhancement systems, speech intelligibility, speech quality, {SQ}, {STSA}-{MMSE} based {SE} method},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/9HL2LAP9/7744475.html:text/html;Kolbæk et al. - 2017 - Speech Intelligibility Potential of General and Sp.pdf:/Users/bert/Zotero/storage/XMZG4B3P/Kolbæk et al. - 2017 - Speech Intelligibility Potential of General and Sp.pdf:application/pdf},
}

@article{kim_structured_2017,
	title = {Structured Attention Networks},
	url = {http://arxiv.org/abs/1702.00887},
	abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
	journaltitle = {{arXiv}:1702.00887 [cs]},
	author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
	urldate = {2017-02-16},
	date = {2017-02},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GT2S54YZ/1702.html:text/html;Kim et al. - 2017 - Structured Attention Networks.pdf:/Users/bert/Zotero/storage/M5NHUPQI/Kim et al. - 2017 - Structured Attention Networks.pdf:application/pdf},
}

@article{obleser_tell_2016,
	title = {Tell me something I don’t know},
	volume = {5},
	rights = {© 2016, Obleser. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {https://elifesciences.org/content/5/e15853v1},
	doi = {10.7554/eLife.15853},
	abstract = {The roles that neural oscillations play in the auditory cortex of the human brain are becoming clearer.},
	pages = {e15853},
	journaltitle = {{eLife}},
	author = {Obleser, Jonas},
	urldate = {2017-02-26},
	date = {2016-04},
	langid = {english},
	pmid = {27090088},
	keywords = {predictive coding, perception, auditory cortex, Human, prediction error, predictions, surprise},
	file = {Obleser - 2016 - Tell me something I don’t know.pdf:/Users/bert/Zotero/storage/952JNPYA/Obleser - 2016 - Tell me something I don’t know.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3VKXA6GJ/e15853.html:text/html},
}

@article{sedley_neural_2016,
	title = {Neural signatures of perceptual inference},
	volume = {5},
	rights = {© 2016, Sedley et al. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {https://elifesciences.org/content/5/e11476v2},
	doi = {10.7554/eLife.11476},
	abstract = {Changes to sensory predictions are encoded by beta oscillations, surprise due to prediction violations by gamma oscillations, and alpha oscillations may have a role in controlling the precision of predictions.},
	pages = {e11476},
	journaltitle = {{eLife}},
	author = {Sedley, William and Gander, Phillip E. and Kumar, Sukhbinder and Kovach, Christopher K. and Oya, Hiroyuki and Kawasaki, Hiroto and Iii, Matthew A. Howard and Griffiths, Timothy D.},
	urldate = {2017-02-26},
	date = {2016-03},
	langid = {english},
	pmid = {26949254},
	keywords = {predictive coding, perception, auditory cortex, Human, prediction error, predictions, surprise},
	file = {Sedley et al. - 2016 - Neural signatures of perceptual inference.pdf:/Users/bert/Zotero/storage/HVFRJAXC/Sedley et al. - 2016 - Neural signatures of perceptual inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/QBNJRUIJ/e11476.html:text/html},
}

@article{wostmann_tracking_2016,
	title = {Tracking the signal, cracking the code: speech and speech comprehension in non-invasive human electrophysiology},
	volume = {0},
	issn = {2327-3798},
	url = {http://dx.doi.org/10.1080/23273798.2016.1262051},
	doi = {10.1080/23273798.2016.1262051},
	shorttitle = {Tracking the signal, cracking the code},
	abstract = {Magneto- and electroencephalographic (M/{EEG}) signals recorded from the human scalp have allowed for substantial advances for neural models of speech comprehension over the past decades. These methods are currently advancing rapidly and continue to offer unparalleled insight in the near-to-real-time neural dynamics of speech processing. We provide a historically informed overview over dependent measures in the time and frequency domain and highlight recent advances resulting from these measures. We discuss the notorious challenges (and solutions) speech and language researchers are faced with when studying auditory brain responses in M/{EEG}. We argue that a key to understanding the neural basis of speech comprehension will lie in studying interactions between the neural tracking of speech and the functional neural network dynamics. This article is intended for both, non-experts who want to learn how to use M/{EEG} to study speech comprehension and scholars aiming for an overview of state-of-the-art M/{EEG} analysis methods.},
	pages = {1--15},
	number = {0},
	journaltitle = {Language, Cognition and Neuroscience},
	author = {Wöstmann, Malte and Fiedler, Lorenz and Obleser, Jonas},
	urldate = {2017-02-26},
	date = {2016-12},
	keywords = {Speech, Electroencephalogram, language, magnetoencephalogram},
	file = {Snapshot:/Users/bert/Zotero/storage/JC8NQ8TX/23273798.2016.html:text/html;Wöstmann et al. - 2016 - Tracking the signal, cracking the code speech and.pdf:/Users/bert/Zotero/storage/TNE2RWU5/Wöstmann et al. - 2016 - Tracking the signal, cracking the code speech and.pdf:application/pdf},
}

@article{zhang_deep_2016,
	title = {A Deep Ensemble Learning Method for Monaural Speech Separation},
	volume = {24},
	issn = {2329-9290},
	url = {http://dl.acm.org/citation.cfm?id=2992480.2992491},
	abstract = {Monaural speech separation is a fundamental problem in robust speech processing. Recently, deep neural network ({DNN})-based speech separation methods, which predict either clean speech or an ideal time-frequency mask, have demonstrated remarkable performance improvement. However, a single {DNN} with a given window length does not leverage contextual information sufficiently, and the differences between the two optimization objectives are not well understood. In this paper, we propose a deep ensemble method, named multicontext networks, to address monaural speech separation. The first multicontext network averages the outputs of multiple {DNNs} whose inputs employ different window lengths. The second multicontext network is a stack of multiple {DNNs}. Each {DNN} in a module of the stack takes the concatenation of original acoustic features and expansion of the soft output of the lower module as its input, and predicts the ratio mask of the target speaker; the {DNNs} in the same module employ different contexts. We have conducted extensive experiments with three speech corpora. The results demonstrate the effectiveness of the proposed method. We have also compared the two optimization objectives systematically and found that predicting the ideal time-frequency mask is more efficient in utilizing clean training speech, while predicting clean speech is less sensitive to {SNR} variations.},
	pages = {967--977},
	number = {5},
	journaltitle = {{IEEE}/{ACM} Trans. Audio, Speech and Lang. Proc.},
	author = {Zhang, Xiao-Lei and Wang, {DeLiang}},
	urldate = {2017-03-07},
	date = {2016-05},
	keywords = {Deep neural networks, ensemble learning, mapping-based separation, masking-based separation, monaural speech separation, multicontext networks},
	file = {Zhang and Wang - 2016 - A Deep Ensemble Learning Method for Monaural Speec.pdf:/Users/bert/Zotero/storage/W373X2I8/Zhang and Wang - 2016 - A Deep Ensemble Learning Method for Monaural Speec.pdf:application/pdf},
}

@article{chen_large-scale_2016,
	title = {Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises},
	volume = {139},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4948445},
	doi = {10.1121/1.4948445},
	abstract = {Supervised speech segregation has been recently shown to improve human speech intelligibility in noise, when trained and tested on similar noises. However, a major challenge involves the ability to generalize to entirely novel noises. Such generalization would enable hearing aid and cochlear implant users to improve speech intelligibility in unknown noisy environments. This challenge is addressed in the current study through large-scale training. Specifically, a deep neural network ({DNN}) was trained on 10 000 noises to estimate the ideal ratio mask, and then employed to separate sentences from completely new noises (cafeteria and babble) at several signal-to-noise ratios ({SNRs}). Although the {DNN} was trained at the fixed {SNR} of −−{\textbackslash}textlessmath display="inline" overflow="scroll" altimg="eq-00001.gif"{\textbackslash}textgreater{\textbackslash}textlessmo{\textbackslash}textgreater−{\textbackslash}textless/mo{\textbackslash}textgreater{\textbackslash}textless/math{\textbackslash}textgreater 2 {dB}, testing using hearing-impaired listeners demonstrated that speech intelligibility increased substantially following speech segregation using the novel noises and unmatched {SNR} conditions of 0 {dB} and 5 {dB}. Sentence intelligibility benefit was also observed for normal-hearing listeners in most noisy conditions. The results indicate that {DNN}-based supervised speech segregation with large-scale training is a very promising approach for generalization to new acoustic environments.},
	pages = {2604--2612},
	number = {5},
	journaltitle = {The Journal of the Acoustical Society of America},
	author = {Chen, Jitong and Yuxuan, Wang and Yoho, Sarah and Wang, {DeLiang} and Healy, Eric},
	urldate = {2017-03-07},
	date = {2016-05},
	file = {Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:/Users/bert/Zotero/storage/YL5CY7T9/Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:application/pdf;Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:/Users/bert/Zotero/storage/P9D6F4JD/Chen et al. - 2016 - Large-scale training to increase speech intelligib.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/G9TPWFFC/1.html:text/html},
}

@article{shapiro_21st_2005,
	title = {A 21st century view of evolution: genome system architecture, repetitive {DNA}, and natural genetic engineering},
	volume = {345},
	issn = {0378-1119},
	doi = {10.1016/j.gene.2004.11.020},
	shorttitle = {A 21st century view of evolution},
	abstract = {The last 50 years of molecular genetics have produced an abundance of new discoveries and data that make it useful to revisit some basic concepts and assumptions in our thinking about genomes and evolution. Chief among these observations are the complex modularity of genome organization, the biological ubiquity of mobile and repetitive {DNA} sequences, and the fundamental importance of {DNA} rearrangements in the evolution of sequenced genomes. This review will take a broad overview of these developments and suggest some new ways of thinking about genomes as sophisticated informatic storage systems and about evolution as a systems engineering process.},
	pages = {91--100},
	number = {1},
	journaltitle = {Gene},
	author = {Shapiro, James A.},
	date = {2005-01},
	pmid = {15716117},
	keywords = {Animals, Humans, {DNA} Transposable Elements, Evolution, Gene Rearrangement, Genes, Genome, Immunoglobulin, Insertional, Molecular, Mutagenesis, Nucleic Acid, Repetitive Sequences},
	file = {Shapiro - 2005 - A 21st century view of evolution genome system ar.pdf:/Users/bert/Zotero/storage/MS8S9MWP/Shapiro - 2005 - A 21st century view of evolution genome system ar.pdf:application/pdf},
}

@article{spratling_review_2017,
	title = {A review of predictive coding algorithms},
	volume = {112},
	issn = {0278-2626},
	url = {http://www.sciencedirect.com/science/article/pii/S027826261530035X},
	doi = {10.1016/j.bandc.2015.11.003},
	series = {Perspectives on Human Probabilistic Inferences and the 'Bayesian Brain'},
	abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term “predictive coding”. This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
	pages = {92--97},
	journaltitle = {Brain and Cognition},
	author = {Spratling, M. W.},
	urldate = {2017-03-26},
	date = {2017-03},
	keywords = {free energy, Neural networks, predictive coding, Cortex, Retina},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/W6RD4XMS/S027826261530035X.html:text/html;Spratling - 2017 - A review of predictive coding algorithms.pdf:/Users/bert/Zotero/storage/N7IGVDNU/Spratling - 2017 - A review of predictive coding algorithms.pdf:application/pdf},
}

@article{chennu_expectation_2013,
	title = {Expectation and Attention in Hierarchical Auditory Prediction},
	volume = {33},
	rights = {Copyright © 2013 the authors 0270-6474/13/3311194-12\$15.00/0},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/content/33/27/11194},
	doi = {10.1523/JNEUROSCI.0114-13.2013},
	abstract = {Hierarchical predictive coding suggests that attention in humans emerges from increased precision in probabilistic inference, whereas expectation biases attention in favor of contextually anticipated stimuli. We test these notions within auditory perception by independently manipulating top-down expectation and attentional precision alongside bottom-up stimulus predictability. Our findings support an integrative interpretation of commonly observed electrophysiological signatures of neurodynamics, namely mismatch negativity ({MMN}), P300, and contingent negative variation ({CNV}), as manifestations along successive levels of predictive complexity. Early first-level processing indexed by the {MMN} was sensitive to stimulus predictability: here, attentional precision enhanced early responses, but explicit top-down expectation diminished it. This pattern was in contrast to later, second-level processing indexed by the P300: although sensitive to the degree of predictability, responses at this level were contingent on attentional engagement and in fact sharpened by top-down expectation. At the highest level, the drift of the {CNV} was a fine-grained marker of top-down expectation itself. Source reconstruction of high-density {EEG}, supported by intracranial recordings, implicated temporal and frontal regions differentially active at early and late levels. The cortical generators of the {CNV} suggested that it might be involved in facilitating the consolidation of context-salient stimuli into conscious perception. These results provide convergent empirical support to promising recent accounts of attention and expectation in predictive coding.},
	pages = {11194--11205},
	number = {27},
	journaltitle = {Journal of Neuroscience},
	author = {Chennu, Srivas and Noreika, Valdas and Gueorguiev, David and Blenkmann, Alejandro and Kochen, Silvia and Ibáñez, Agustín and Owen, Adrian M. and Bekinschtein, Tristan A.},
	urldate = {2017-04-15},
	date = {2013-07},
	langid = {english},
	pmid = {23825422},
	file = {Chennu et al. - 2013 - Expectation and Attention in Hierarchical Auditory.pdf:/Users/bert/Zotero/storage/AKEJKKBL/Chennu et al. - 2013 - Expectation and Attention in Hierarchical Auditory.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3XYVBTHQ/11194.html:text/html},
}

@incollection{friston_variational_2017,
	title = {The Variational Principles of Cognition},
	rights = {©2017 Springer International Publishing {AG}},
	isbn = {978-3-319-53672-9 978-3-319-53673-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-53673-6_12},
	series = {Nonlinear Systems and Complexity},
	abstract = {This chapter provides a theoretical perspective on a dynamics, from the perspective of the free-energy principle. This variational principle offers a natural explanation for neuronal activity that is formulated in terms of dynamical systems and attracting sets. We will see that the free-energy principle emerges when we consider the ensemble dynamics of any pattern forming, self-organizing system. When we look closely what this principle implies for the behavior of systems like the brain, one finds a fairly simple explanation for active inference and the Bayesian brain hypothesis. Within the Bayesian brain framework, the ensuing dynamics can be separated, in a principled way, into those serving perceptual inference, learning and behavior. Dynamics here are central; not only to an understanding the nature of self-organizing systems but also to explain the adaptive nature of neuronal dynamics and plasticity in terms of optimization. The special focus of this chapter is on the pre-eminent role of heteroclinic cycles in providing deep and dynamic (generative) models of the sensorium; particularly, the sensations that we generate ourselves through movement. In what follows, we will briefly rehearse the basic theory and illustrate its implications using simulations of action (handwriting)—and its observation.},
	pages = {189--211},
	number = {20},
	booktitle = {Advances in Dynamics, Patterns, Cognition},
	publisher = {Springer International Publishing},
	author = {Friston, Karl},
	editor = {Aranson, Igor S. and Pikovsky, Arkady and Rulkov, Nikolai F. and Tsimring, Lev S.},
	urldate = {2017-05-09},
	date = {2017},
	langid = {english},
	keywords = {free energy, Bayesian brain, Pattern Recognition, Entropy, Applications of Nonlinear Dynamics and Chaos Theory, Complex Systems, Complexity, Dynamics, Neural activity},
	file = {Friston - 2017 - The Variational Principles of Cognition.pdf:/Users/bert/Zotero/storage/JGCLSNDP/Friston - 2017 - The Variational Principles of Cognition.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/7EGL9J25/978-3-319-53673-6_12.html:text/html},
}

@incollection{friston_variational_2017-1,
	title = {The Variational Principles of Action},
	rights = {©2017 Springer International Publishing {AG}},
	isbn = {978-3-319-51546-5 978-3-319-51547-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-51547-2_10},
	series = {Springer Tracts in Advanced Robotics},
	abstract = {This chapter provides a theoretical perspective on action and the control of movement from the point of view of the free-energy principle. This variational principle offers an explanation for neuronal activity and ensuing behavior that is formulated in terms of dynamical systems and attracting sets. We will see that the free-energy principle emerges when considering the ensemble dynamics of biological systems like ourselves. When we look closely what this principle implies for the behavior of systems like the brain, one finds a fairly straightforward explanation for many aspects of action and perception; in particular, their (approximately Bayesian) optimality. Within the Bayesian brain framework, the ensuing dynamics can be separated into those serving perceptual inference, learning and behavior. Variational principles play a key role in what follows; both in understanding the nature of self-organizing systems but also in explaining the adaptive nature of neuronal dynamics and plasticity in terms of optimization—and the process theories that mediate optimal inference and motor control. A special focus of this chapter is the pre-eminent role of heteroclinic cycles in providing deep and dynamic (generative) models of the sensorium; particularly the sensations that we generate ourselves through action. In what follows, we will briefly rehearse the basic theory and illustrate its implications using simulations of action (handwriting)—and its observation.},
	pages = {207--235},
	number = {117},
	booktitle = {Geometric and Numerical Foundations of Movements},
	publisher = {Springer International Publishing},
	author = {Friston, Karl},
	editor = {Laumond, Jean-Paul and Mansard, Nicolas and Lasserre, Jean-Bernard},
	urldate = {2017-05-09},
	date = {2017},
	langid = {english},
	keywords = {Artificial Intelligence (incl. Robotics), Geometry, Neurosciences, Numeric Computing, Robotics and Automation},
	file = {Friston - 2017 - The Variational Principles of Action.pdf:/Users/bert/Zotero/storage/9ZNGEZT9/Friston - 2017 - The Variational Principles of Action.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/PBFIVEMR/978-3-319-51547-2_10.html:text/html},
}

@article{campbell_universal_2016,
	title = {Universal Darwinism As a Process of Bayesian Inference},
	volume = {10},
	issn = {1662-5137},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4894882/},
	doi = {10.3389/fnsys.2016.00049},
	abstract = {Many of the mathematical frameworks describing natural selection are equivalent to Bayes' Theorem, also known as Bayesian updating. By definition, a process of Bayesian Inference is one which involves a Bayesian update, so we may conclude that these frameworks describe natural selection as a process of Bayesian inference. Thus, natural selection serves as a counter example to a widely-held interpretation that restricts Bayesian Inference to human mental processes (including the endeavors of statisticians). As Bayesian inference can always be cast in terms of (variational) free energy minimization, natural selection can be viewed as comprising two components: a generative model of an “experiment” in the external world environment, and the results of that “experiment” or the “surprise” entailed by predicted and actual outcomes of the “experiment.” Minimization of free energy implies that the implicit measure of “surprise” experienced serves to update the generative model in a Bayesian manner. This description closely accords with the mechanisms of generalized Darwinian process proposed both by Dawkins, in terms of replicators and vehicles, and Campbell, in terms of inferential systems. Bayesian inference is an algorithm for the accumulation of evidence-based knowledge. This algorithm is now seen to operate over a wide range of evolutionary processes, including natural selection, the evolution of mental models and cultural evolutionary processes, notably including science itself. The variational principle of free energy minimization may thus serve as a unifying mathematical framework for universal Darwinism, the study of evolutionary processes operating throughout nature.},
	journaltitle = {Frontiers in Systems Neuroscience},
	author = {Campbell, John O.},
	urldate = {2016-12-25},
	date = {2016-06},
	pmid = {27375438},
	pmcid = {PMC4894882},
	keywords = {natural selection, free energy, Bayesian inference, information, Universal Darwinism},
	file = {Campbell - 2016 - Universal Darwinism As a Process of Bayesian Infer.pdf:/Users/bert/Zotero/storage/3BBBFDKN/Campbell - 2016 - Universal Darwinism As a Process of Bayesian Infer.pdf:application/pdf},
}

@article{roelfsema_perceptual_2010,
	title = {Perceptual learning rules based on reinforcers and attention},
	volume = {14},
	issn = {1364-6613},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2835467/},
	doi = {10.1016/j.tics.2009.11.005},
	abstract = {How does the brain learn those visual features that are relevant for behavior? In this article, we focus on two factors that guide plasticity of visual representations. First, reinforcers cause the global release of diffusive neuromodulatory signals that gate plasticity. Second, attentional feedback signals highlight the chain of neurons between sensory and motor cortex responsible for the selected action. We here propose that the attentional feedback signals guide learning by suppressing plasticity of irrelevant features while permitting the learning of relevant ones. By hypothesizing that sensory signals that are too weak to be perceived can escape from this inhibitory feedback, we bring attentional learning theories and theories that emphasized the importance of neuromodulatory signals into a single, unified framework.},
	pages = {64--71},
	number = {2},
	journaltitle = {Trends in cognitive sciences},
	author = {Roelfsema, Pieter R. and van Ooyen, Arjen and Watanabe, Takeo},
	urldate = {2015-09-17},
	date = {2010-02},
	pmid = {20060771},
	pmcid = {PMC2835467},
	file = {Roelfsema et al. - 2010 - Perceptual learning rules based on reinforcers and.pdf:/Users/bert/Zotero/storage/WEQRR8P3/Roelfsema et al. - 2010 - Perceptual learning rules based on reinforcers and.pdf:application/pdf},
}

@incollection{gershman_neural_2010,
	title = {The Neural Costs of Optimal Control},
	url = {http://papers.nips.cc/paper/4167-the-neural-costs-of-optimal-control.pdf},
	pages = {712--720},
	booktitle = {Advances in Neural Information Processing Systems 23},
	publisher = {Curran Associates, Inc.},
	author = {Gershman, Samuel and Wilson, Robert},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	urldate = {2015-11-26},
	date = {2010},
	file = {Gershman and Wilson - 2010 - The Neural Costs of Optimal Control.pdf:/Users/bert/Zotero/storage/JT8WMSLC/Gershman and Wilson - 2010 - The Neural Costs of Optimal Control.pdf:application/pdf;NIPS Snapshort:/Users/bert/Zotero/storage/9ZA9AGVC/4167-the-neural-costs-of-optimal-control.html:text/html},
}

@article{henaff_model-based_2017,
	title = {Model-Based Planning in Discrete Action Spaces},
	url = {http://arxiv.org/abs/1705.07177},
	abstract = {Planning actions using learned and differentiable forward models of the world is a general approach which has a number of desirable properties, including improved sample complexity over model-free {RL} methods, reuse of learned models across different tasks, and the ability to perform efficient gradient-based optimization in continuous action spaces. However, this approach does not apply straightforwardly when the action space is discrete, which may have limited its adoption. In this work, we introduce two discrete planning tasks inspired by existing question-answering datasets and show that it is in fact possible to effectively perform planning via backprop in discrete action spaces using two simple yet principled modifications. Our experiments show that this approach can significantly outperform model-free {RL} based methods and supervised imitation learners.},
	journaltitle = {{arXiv}:1705.07177 [cs]},
	author = {Henaff, Mikael and Whitney, William F. and {LeCun}, Yann},
	urldate = {2017-05-26},
	date = {2017-05},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GNP4VC22/1705.html:text/html;Henaff et al. - 2017 - Model-Based Planning in Discrete Action Spaces.pdf:/Users/bert/Zotero/storage/RN8WRRDS/Henaff et al. - 2017 - Model-Based Planning in Discrete Action Spaces.pdf:application/pdf},
}

@article{santurkar_generative_2017,
	title = {Generative Compression},
	url = {http://arxiv.org/abs/1703.01467},
	abstract = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length coding schemes.},
	journaltitle = {{arXiv}:1703.01467 [cs]},
	author = {Santurkar, Shibani and Budden, David and Shavit, Nir},
	date = {2017-03},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/R6MB3XLN/1703.html:text/html;Santurkar et al. - 2017 - Generative Compression.pdf:/Users/bert/Zotero/storage/GHFNMKDE/Santurkar et al. - 2017 - Generative Compression.pdf:application/pdf},
}

@article{hassabis_neuroscience-inspired_2017,
	title = {Neuroscience-Inspired Artificial Intelligence},
	volume = {95},
	issn = {0896-6273},
	url = {http://www.cell.com/neuron/abstract/S0896-6273(17)30509-3},
	doi = {10.1016/j.neuron.2017.06.011},
	pages = {245--258},
	number = {2},
	journaltitle = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	date = {2017-07},
	pmid = {28728020},
	keywords = {Brain, Artificial intelligence, Learning, cognition, neural network},
	file = {Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:/Users/bert/Zotero/storage/ZSQ7RECE/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NIY9BBY7/S0896-6273(17)30509-3.html:text/html;Snapshot:/Users/bert/Zotero/storage/4DBSG97Q/S0896-6273(17)30509-3.html:text/html},
}

@article{mootoovaloo_bayes_2016,
	title = {Bayes Factors via Savage-Dickey Supermodels},
	url = {http://arxiv.org/abs/1609.02186},
	abstract = {We outline a new method to compute the Bayes Factor for model selection which bypasses the Bayesian Evidence. Our method combines multiple models into a single, nested, Supermodel using one or more hyperparameters. Since the models are now nested the Bayes Factors between the models can be efficiently computed using the Savage-Dickey Density Ratio ({SDDR}). In this way model selection becomes a problem of parameter estimation. We consider two ways of constructing the supermodel in detail: one based on combined models, and a second based on combined likelihoods. We report on these two approaches for a Gaussian linear model for which the Bayesian evidence can be calculated analytically and a toy nonlinear problem. Unlike the combined model approach, where a standard Monte Carlo Markov Chain ({MCMC}) struggles, the combined-likelihood approach fares much better in providing a reliable estimate of the log-Bayes Factor. This scheme potentially opens the way to computationally efficient ways to compute Bayes Factors in high dimensions that exploit the good scaling properties of {MCMC}, as compared to methods such as nested sampling that fail for high dimensions.},
	journaltitle = {{arXiv}:1609.02186 [astro-ph, stat]},
	author = {Mootoovaloo, A. and Bassett, Bruce A. and Kunz, M.},
	urldate = {2016-11-09},
	date = {2016-09},
	keywords = {Statistics - Methodology, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QJLXNJZ5/1609.html:text/html;Mootoovaloo et al. - 2016 - Bayes Factors via Savage-Dickey Supermodels.pdf:/Users/bert/Zotero/storage/DP3NK6PC/Mootoovaloo et al. - 2016 - Bayes Factors via Savage-Dickey Supermodels.pdf:application/pdf},
}

@inproceedings{gonzalez_glasses:_2016,
	title = {{GLASSES}: Relieving The Myopia Of Bayesian Optimisation},
	url = {http://jmlr.org/proceedings/papers/v51/gonzalez16b.html},
	shorttitle = {{GLASSES}},
	pages = {790--799},
	author = {Gonzalez, Javier and Osborne, Michael and Lawrence, Neil},
	urldate = {2016-11-10},
	date = {2016},
	file = {Gonzalez et al. - 2016 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf:/Users/bert/Zotero/storage/SBFKGZW2/Gonzalez et al. - 2016 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/PI7CNNIW/gonzalez16b.html:text/html},
}

@article{kucukelbir_automatic_2016,
	title = {Automatic Differentiation Variational Inference},
	url = {http://arxiv.org/abs/1603.00788},
	abstract = {Probabilistic modeling is iterative. A scientist posits a simple model, fits it to her data, refines it according to her analysis, and repeats. However, fitting complex models to large data is a bottleneck in this process. Deriving algorithms for new models can be both mathematically and computationally challenging, which makes it difficult to efficiently cycle through the steps. To this end, we develop automatic differentiation variational inference ({ADVI}). Using our method, the scientist only provides a probabilistic model and a dataset, nothing else. {ADVI} automatically derives an efficient variational inference algorithm, freeing the scientist to refine and explore many models. {ADVI} supports a broad class of models-no conjugacy assumptions are required. We study {ADVI} across ten different models and apply it to a dataset with millions of observations. {ADVI} is integrated into Stan, a probabilistic programming system; it is available for immediate use.},
	journaltitle = {{arXiv}:1603.00788 [cs, stat]},
	author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	urldate = {2016-12-11},
	date = {2016-03},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6QULX5HM/1603.html:text/html;Kucukelbir et al. - 2017 - Automatic Differentiation Variational Inference.pdf:/Users/bert/Zotero/storage/IVCNSZP2/Kucukelbir et al. - 2017 - Automatic Differentiation Variational Inference.pdf:application/pdf},
}

@article{kording_ten_2016,
	title = {Ten simple rules for structuring papers},
	rights = {© 2016, Posted by Cold Spring Harbor Laboratory Press. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {http://biorxiv.org/content/early/2016/11/28/088278},
	doi = {10.1101/088278},
	abstract = {Good scientific writing is essential to career development and to the progress of science. A well-structured manuscript allows readers and reviewers to get excited about the subject matter, to understand and verify the paper's contributions, and to integrate them into a broader context. However, many scientists struggle with producing high-quality manuscripts and are typically given little training in paper writing. Focusing on how readers consume information, we present a set of 10 simple rules to help you get across the main idea of your paper. These rules are designed to make your paper more influential and the process of writing more efficient and pleasurable.},
	pages = {088278},
	journaltitle = {{bioRxiv}},
	author = {Kording, Konrad P. and Mensh, Brett},
	urldate = {2016-12-12},
	date = {2016-11},
	langid = {english},
	file = {Kording and Mensh - 2016 - Ten simple rules for structuring papers.pdf:/Users/bert/Zotero/storage/52FHJMUS/Kording and Mensh - 2016 - Ten simple rules for structuring papers.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/5LWKDBIY/088278.html:text/html},
}

@inproceedings{tran_deep_2017,
	title = {Deep Probabilistic Programming},
	url = {https://openreview.net/pdf?id=Hy6b4Pqee},
	author = {Tran, Dustin and Hoffman, Matt and Saurous, Rif and Brevdo, Eugene and Murphy, Kevin P and Blei, David},
	urldate = {2017-01-10},
	date = {2017},
	file = {Tran et al. - 2017 - Deep Probabilistic Programming.pdf:/Users/bert/Zotero/storage/8R87LZLR/Tran et al. - 2017 - Deep Probabilistic Programming.pdf:application/pdf},
}

@article{lake_building_2016,
	title = {Building Machines That Learn and Think Like People},
	url = {http://arxiv.org/abs/1604.00289},
	abstract = {Recent progress in artificial intelligence ({AI}) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	journaltitle = {{arXiv}:1604.00289 [cs, stat]},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	urldate = {2017-03-07},
	date = {2016-04},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9KPAVT7D/1604.html:text/html;Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:/Users/bert/Zotero/storage/AFVCNZUZ/Lake et al. - 2016 - Building Machines That Learn and Think Like People.pdf:application/pdf},
}

@inproceedings{wildhaber_signal_2017,
	title = {Signal Detection and Discrimination for Medical Devices Using Windowed State Space Filters},
	url = {http://www.actapress.com/PaperInfo.aspx?paperId=456429},
	doi = {10.2316/P.2017.852-020},
	abstract = {We introduce a model-based approach for computationally efficient signal detection and discrimination, which is relevant for biological signals.},
	publisher = {{ACTA} Press},
	author = {Wildhaber, Reto Andreas and Zalmai, Nour and Jacomet, Marcel and Loeliger, Hans-Andrea},
	urldate = {2017-03-08},
	date = {2017-03},
	file = {Snapshot:/Users/bert/Zotero/storage/398QC5VW/PaperInfo.html:text/html;Wildhaber et al. - 2017 - Signal Detection and Discrimination for Medical De.pdf:/Users/bert/Zotero/storage/CZNBGALB/Wildhaber et al. - 2017 - Signal Detection and Discrimination for Medical De.pdf:application/pdf},
}

@article{wiecki_hddm:_2013,
	title = {{HDDM}: Hierarchical Bayesian estimation of the Drift-Diffusion Model in Python},
	volume = {7},
	issn = {1662-5196},
	url = {http://journal.frontiersin.org/article/10.3389/fninf.2013.00014/abstract},
	doi = {10.3389/fninf.2013.00014},
	shorttitle = {{HDDM}},
	abstract = {The diffusion model is a commonly used tool to infer latent psychological processes underlying decision making, and to link them to neural mechanisms based on reaction times. Although efficient open source software has been made available to quantitatively fit the model to data, current estimation methods require an abundance of reaction time measurements to recover meaningful parameters, and only provide point estimates of each parameter. In contrast, hierarchical Bayesian parameter estimation methods are useful for enhancing statistical power, allowing for simultaneous estimation of individual subject parameters and the group distribution that they are drawn from, while also providing measures of uncertainty in these parameters in the posterior distribution. Here, we present a novel Python-based toolbox called {HDDM} (hierarchical drift diffusion model), which allows fast and flexible estimation of the the drift-diffusion model and the related linear ballistic accumulator model. {HDDM} requires fewer data per subject / condition than non-hierarchical method, allows for full Bayesian data analysis, and can handle outliers in the data. Finally, {HDDM} supports the estimation of how trial-by-trial measurements (e.g. {fMRI}) influence decision making parameters. This paper will first describe the theoretical background of drift-diffusion model and Bayesian inference. We then illustrate usage of the toolbox on a real-world data set from our lab. Finally, parameter recovery studies show that {HDDM} beats alternative fitting methods like the chi-quantile method as well as maximum likelihood estimation. The software and documentation can be downloaded at: http://ski.clps.brown.edu/hddm\_docs},
	journaltitle = {Frontiers in Neuroinformatics},
	author = {Wiecki, Thomas V. and Sofer, Imri and Frank, Michael J.},
	urldate = {2017-03-26},
	date = {2013},
	keywords = {Software, drift diffusion model, Bayesian modeling, decision making, python},
	file = {Wiecki et al. - 2013 - HDDM Hierarchical Bayesian estimation of the Drif.pdf:/Users/bert/Zotero/storage/IP74RX2H/Wiecki et al. - 2013 - HDDM Hierarchical Bayesian estimation of the Drif.pdf:application/pdf},
}

@inproceedings{reece_introduction_2010,
	title = {An introduction to Gaussian processes for the Kalman filter expert},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5711863},
	pages = {1--9},
	booktitle = {Information Fusion ({FUSION}), 2010 13th Conference on},
	publisher = {{IEEE}},
	author = {Reece, Steven and Roberts, Stephen},
	urldate = {2014-04-10},
	date = {2010},
	file = {Reece and Roberts - 2010 - An introduction to Gaussian processes for the Kalm.pdf:/Users/bert/Zotero/storage/VAKTRLJN/Reece and Roberts - 2010 - An introduction to Gaussian processes for the Kalm.pdf:application/pdf;Reece and Roberts - 2010 - An introduction to Gaussian processes for the Kalm.pdf:/Users/bert/Zotero/storage/FMX3J9Y8/Reece and Roberts - 2010 - An introduction to Gaussian processes for the Kalm.pdf:application/pdf},
}

@article{palmer_bayesian_2017,
	title = {Bayesian Approaches to Autism: Towards Volatility, Action, and Behavior.},
	issn = {1939-1455 (Electronic), 0033-2909 (Print)},
	url = {http://psycnet.apa.org/psycarticles/2017-12911-001},
	doi = {10.1037/bul0000097},
	shorttitle = {Bayesian Approaches to Autism},
	abstract = {Autism spectrum disorder currently lacks an explanation that bridges cognitive, computational, and neural domains. In the past 5 years, progress has been sought in this area by drawing on Bayesian probability theory to describe both social and nonsocial aspects of autism in terms of systematic differences in the processing of sensory information in the brain. The present article begins by synthesizing the existing literature in this regard, including an introduction to the topic for unfamiliar readers. The key proposal is that autism is characterized by a greater weighting of sensory information in updating probabilistic representations of the environment. Here, we unpack further how the hierarchical setting of Bayesian inference in the brain (i.e., predictive processing) adds significant depth to this approach. In particular, autism may relate to finer mechanisms involved in the context-sensitive adjustment of sensory weightings, such as in how neural representations of environmental volatility inform perception. Crucially, in light of recent sensorimotor treatments of predictive processing (i.e., active inference), hypotheses regarding atypical sensory weighting in autism have direct implications for the regulation of action and behavior. Given that core features of autism relate to how the individual interacts with and samples the world around them (e.g., reduced social responding, repetitive behaviors, motor impairments, and atypical visual sampling), the extension of Bayesian theories of autism to action will be critical for yielding insights into this condition. ({PsycINFO} Database Record (c) 2017 {APA}, all rights reserved)},
	journaltitle = {{APA} {PsycNET}},
	author = {Palmer, Colin J. and Lawson, Rebecca P. and Hohwy, Jakob},
	urldate = {2017-04-12},
	date = {2017-03},
	file = {Palmer et al. - 2017 - Bayesian Approaches to Autism Towards Volatility,.pdf:/Users/bert/Zotero/storage/YFGEVGCQ/Palmer et al. - 2017 - Bayesian Approaches to Autism Towards Volatility,.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/8HE3G24M/2017-12911-001.html:text/html},
}

@incollection{friston_policies_2012,
	title = {Policies and Priors},
	rights = {©2012 Springer Science+Business Media, {LLC}},
	isbn = {978-1-4614-0750-8 978-1-4614-0751-5},
	url = {http://link.springer.com/chapter/10.1007/978-1-4614-0751-5_9},
	series = {Springer Series in Computational Neuroscience},
	abstract = {This chapter considers addiction from a purely theoretical point of view. It tries to substantiate the idea that addictive behaviour is a natural consequence of abnormal perceptual learning. In short, addictive behaviours emerge when behaviour confounds its own acquisition. Specifically, we consider what would happen if behaviour interfered with the neurotransmitter systems responsible for optimising the conditional certainty or precision of inferences about causal structure in the world. We will pursue this within a rather abstract framework provided by free-energy formulations of action and perception. Although this treatment does not touch upon many of the neurobiological or psychosocial issues in addiction research, it provides a principled framework within which to understand exchanges with the environment and how they can be disturbed. Our focus will be on behaviour as active inference and the key role of prior expectations. These priors play the role of policies in reinforcement learning and place crucial constraints on perceptual inference and subsequent action. A dynamical treatment of these policies suggests a fundamental distinction between fixed-point policies that lead to a single attractive state and itinerant policies that support wandering behavioural orbits among sets of attractive states. Itinerant policies may provide a useful metaphor for many forms of behaviour and, in particular, addiction. Under these sorts of policies, neuromodulatory (e.g., dopaminergic) perturbations can lead to false inference and consequent learning, which produce addictive and preservative behaviour.},
	pages = {237--283},
	number = {10},
	booktitle = {Computational Neuroscience of Drug Addiction},
	publisher = {Springer New York},
	author = {Friston, Karl},
	editor = {Gutkin, Boris and Ahmed, Serge H.},
	urldate = {2016-11-24},
	date = {2012},
	langid = {english},
	keywords = {Neurosciences, Computer Appl. in Life Sciences},
	file = {Friston - 2012 - Policies and Priors.pdf:/Users/bert/Zotero/storage/VYLY6WKZ/Friston - 2012 - Policies and Priors.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/S4K8IULY/978-1-4614-0751-5_9.html:text/html},
}

@article{hsu_learning_2017,
	title = {Learning Latent Representations for Speech Generation and Transformation},
	url = {http://arxiv.org/abs/1704.04222},
	abstract = {An ability to model a generative process and learn a latent representation for speech in an unsupervised fashion will be crucial to process vast quantities of unlabelled speech data. Recently, deep probabilistic generative models such as Variational Autoencoders ({VAEs}) have achieved tremendous success in modeling natural images. In this paper, we apply a convolutional {VAE} to model the generative process of natural speech. We derive latent space arithmetic operations to disentangle learned latent representations. We demonstrate the capability of our model to modify the phonetic content or the speaker identity for speech segments using the derived operations, without the need for parallel supervisory data.},
	journaltitle = {{arXiv}:1704.04222 [cs, stat]},
	author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
	urldate = {2017-05-25},
	date = {2017-04},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2HGHKHHP/1704.html:text/html;Hsu et al. - 2017 - Learning Latent Representations for Speech Generat.pdf:/Users/bert/Zotero/storage/YNSE6F9I/Hsu et al. - 2017 - Learning Latent Representations for Speech Generat.pdf:application/pdf},
}

@article{whittington_approximation_2017,
	title = {An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/NECO_a_00949},
	doi = {10.1162/NECO_a_00949},
	abstract = {To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.},
	pages = {1--34},
	journaltitle = {Neural Computation},
	author = {Whittington, James C. R. and Bogacz, Rafal},
	urldate = {2017-03-25},
	date = {2017-03},
	file = {Neural Computation Snapshot:/Users/bert/Zotero/storage/U8I2DCVY/NECO_a_00949.html:text/html;Whittington and Bogacz - 2017 - An Approximation of the Error Backpropagation Algo.pdf:/Users/bert/Zotero/storage/MY3SS3PX/Whittington and Bogacz - 2017 - An Approximation of the Error Backpropagation Algo.pdf:application/pdf},
}

@article{lotter_deep_2016,
	title = {Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning},
	url = {https://openreview.net/forum?id=B1ewdt9xe&noteId=B1ewdt9xe},
	author = {Lotter, William and Kreiman, Gabriel and Cox, David},
	urldate = {2017-04-29},
	date = {2016-11},
	file = {Lotter et al. - 2016 - Deep Predictive Coding Networks for Video Predicti.pdf:/Users/bert/Zotero/storage/9JEFG4QB/Lotter et al. - 2016 - Deep Predictive Coding Networks for Video Predicti.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/K5JQW44R/forum.html:text/html},
}

@article{kulhavy_kullback-leibler_1996,
	title = {A Kullback-Leibler distance approach to system identification},
	volume = {20},
	issn = {1367-5788},
	url = {http://www.sciencedirect.com/science/article/pii/S1367578897000102},
	doi = {10.1016/S1367-5788(97)00010-2},
	abstract = {The use of probability in system identification is shown to be equivalent to measuring Kullback-Leibler distance between the actual (empirical) and model distributions of data. When data are not known completely (being compressed, quantized, aggregated, missing etc.), the minimum distance approach can be seen as an asymptotic approximation of probabilistic inference. A class of problems is pointed out where inference via Kullback-Leibler distance brings an attractive, computationally less demanding alternative to maximum likelihood or Bayesian estimation.},
	pages = {119--130},
	journaltitle = {Annual Reviews in Control},
	author = {Kulhavý, Rudolf},
	urldate = {2015-10-13},
	date = {1996},
	keywords = {adaptation, Algorithms, asymptotic approximation, large deviations, model approximation, statistical inference, system identification},
	file = {Kulhavý - 1996 - A Kullback-Leibler distance approach to system ide.pdf:/Users/bert/Zotero/storage/R239QZFD/Kulhavý - 1996 - A Kullback-Leibler distance approach to system ide.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/7YDJY5YS/S1367578897000102.html:text/html},
}

@article{tran_edward:_2016,
	title = {Edward: A library for probabilistic modeling, inference, and criticism},
	url = {https://arxiv.org/abs/1610.09787},
	shorttitle = {Edward},
	journaltitle = {{arXiv} preprint {arXiv}:1610.09787},
	author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
	urldate = {2017-06-07},
	date = {2016},
	file = {Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf:/Users/bert/Zotero/storage/Z82RYVCF/Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf:application/pdf},
}

@thesis{korl_factor_2005,
	location = {Zurich},
	title = {A factor graph approach to signal modelling, system identification and filtering},
	institution = {Swiss Federal Institute of Technology},
	type = {phdthesis},
	author = {Korl, Sascha},
	date = {2005},
	file = {Korl - 2005 - A Factor Graph Approach to Signal Modelling , System Identification and Filtering.pdf:/Users/bert/Zotero/storage/IC6XZKIC/Korl - 2005 - A Factor Graph Approach to Signal Modelling , System Identification and Filtering.pdf:application/pdf},
}

@article{chung_recurrent_2015,
	title = {A Recurrent Latent Variable Model for Sequential Data},
	url = {http://arxiv.org/abs/1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network ({RNN}) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational {RNN} ({VRNN})1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the {RNN} dynamic hidden state.},
	journaltitle = {{arXiv}:1506.02216 [cs]},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	urldate = {2015-12-10},
	date = {2015-06},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XHDGA5RS/1506.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/GU2NJV3G/1506.html:text/html;Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf:/Users/bert/Zotero/storage/QL2DXZRD/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf;Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf:/Users/bert/Zotero/storage/33E2WR2Z/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential D.pdf:application/pdf},
}

@inproceedings{farmani_probabilistic_2014,
	location = {Reims (Fr)},
	title = {A Probabilistic Approach to Hearing Loss Compensation},
	booktitle = {{IEEE} International Workshop on Machine Learning for Signal Processing ({MLSP})},
	author = {Farmani, Mojtaba and De Vries, Bert},
	date = {2014-09},
	keywords = {Bayesian inference, Bayes methods, probabilistic approach, Kalman filter, inference mechanisms, hearing aids, Abstracts, automated Bayesian inference, {DRC}, hearing loss, hearing loss compensation algorithms, hearing loss models, hearing loss problem, {HL} compensation algorithms, {HL} model, medical signal processing, probabilistic model},
	file = {Farmani and de Vries - 2014 - A Probabilistic Approach to Hearing Loss Compensat.pdf:/Users/bert/Zotero/storage/G83RIMC4/Farmani and de Vries - 2014 - A Probabilistic Approach to Hearing Loss Compensat.pdf:application/pdf;Farmani and de Vries - 2014 - A Probabilistic Approach to Hearing Loss Compensat.pdf:/Users/bert/Zotero/storage/T6X9EZPA/Farmani and de Vries - 2014 - A Probabilistic Approach to Hearing Loss Compensat.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/4V9XJAVD/6958845.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/X25G7VTN/articleDetails.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/MMVRS2Q8/articleDetails.html:text/html;van de Laar and de Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:/Users/bert/Zotero/storage/AGYCSJCF/van de Laar and de Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:application/pdf},
}

@article{ollivier_training_2015,
	title = {Training recurrent networks online without backtracking},
	url = {http://arxiv.org/abs/1507.07680},
	abstract = {We introduce the "{NoBackTrack}" algorithm to train the parameters of dynamical systems such as recurrent neural networks. This algorithm works in an online, memoryless setting, thus requiring no backpropagation through time, and is scalable, avoiding the large computational and memory cost of maintaining the full gradient of the current state with respect to the parameters. The algorithm essentially maintains, at each time, a single search direction in parameter space. The evolution of this search direction is partly stochastic and is constructed in such a way to provide, at every time, an unbiased random estimate of the gradient of the loss function with respect to the parameters. Because the gradient estimate is unbiased, on average over time the parameter is updated as it should. The resulting gradient estimate can then be fed to a lightweight Kalman-like filter to yield an improved algorithm. For recurrent neural networks, the resulting algorithms scale linearly with the number of parameters. Small-scale experiments confirm the suitability of the approach, showing that the stochastic approximation of the gradient introduced in the algorithm is not detrimental to learning. In particular, the Kalman-like version of {NoBackTrack} is superior to backpropagation through time ({BPTT}) when the time span of dependencies in the data is longer than the truncation span for {BPTT}.},
	journaltitle = {{arXiv}:1507.07680 [cs, stat]},
	author = {Ollivier, Yann and Tallec, Corentin and Charpiat, Guillaume},
	urldate = {2017-12-29},
	date = {2015-07-28},
	eprinttype = {arxiv},
	eprint = {1507.07680},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LKYXYRPC/1507.html:text/html;Ollivier et al. - 2015 - Training recurrent networks online without backtra.pdf:/Users/bert/Zotero/storage/QJWJPTKK/Ollivier et al. - 2015 - Training recurrent networks online without backtra.pdf:application/pdf},
}

@article{ollivier_information-geometric_2011,
	title = {Information-Geometric Optimization Algorithms: A Unifying Picture via Invariance Principles},
	url = {http://arxiv.org/abs/1106.3708},
	shorttitle = {Information-Geometric Optimization Algorithms},
	abstract = {We present a canonical way to turn any smooth parametric family of probability distributions on an arbitrary search space \$X\$ into a continuous-time black-box optimization method on \$X\$, the {\textbackslash}emph\{information-geometric optimization\} ({IGO}) method. Invariance as a design principle minimizes the number of arbitrary choices. The resulting {\textbackslash}emph\{{IGO} flow\} conducts the natural gradient ascent of an adaptive, time-dependent, quantile-based transformation of the objective function. It makes no assumptions on the objective function to be optimized. The {IGO} method produces explicit {IGO} algorithms through time discretization. It naturally recovers versions of known algorithms and offers a systematic way to derive new ones. The cross-entropy method is recovered in a particular case, and can be extended into a smoothed, parametrization-independent maximum likelihood update ({IGO}-{ML}). For Gaussian distributions on \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, {IGO} is related to natural evolution strategies ({NES}) and recovers a version of the {CMA}-{ES} algorithm. For Bernoulli distributions on \${\textbackslash}\{0,1{\textbackslash}\}{\textasciicircum}d\$, we recover the {PBIL} algorithm. From restricted Boltzmann machines, we obtain a novel algorithm for optimization on \${\textbackslash}\{0,1{\textbackslash}\}{\textasciicircum}d\$. All these algorithms are unified under a single information-geometric optimization framework. Thanks to its intrinsic formulation, the {IGO} method achieves invariance under reparametrization of the search space \$X\$, under a change of parameters of the probability distributions, and under increasing transformations of the objective function. Theory strongly suggests that {IGO} algorithms have minimal loss in diversity during optimization, provided the initial diversity is high. First experiments using restricted Boltzmann machines confirm this insight. Thus {IGO} seems to provide, from information theory, an elegant way to spontaneously explore several valleys of a fitness landscape in a single run.},
	journaltitle = {{arXiv}:1106.3708 [math]},
	author = {Ollivier, Yann and Arnold, Ludovic and Auger, Anne and Hansen, Nikolaus},
	urldate = {2017-12-29},
	date = {2011-06-19},
	eprinttype = {arxiv},
	eprint = {1106.3708},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Z4MGJGQE/1106.html:text/html;Ollivier et al. - 2011 - Information-Geometric Optimization Algorithms A U.pdf:/Users/bert/Zotero/storage/N9ZYN73V/Ollivier et al. - 2011 - Information-Geometric Optimization Algorithms A U.pdf:application/pdf},
}

@article{hansen_cma_2016,
	title = {The {CMA} Evolution Strategy: A Tutorial},
	url = {http://arxiv.org/abs/1604.00772},
	shorttitle = {The {CMA} Evolution Strategy},
	abstract = {This tutorial introduces the {CMA} Evolution Strategy ({ES}), where {CMA} stands for Covariance Matrix Adaptation. The {CMA}-{ES} is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
	journaltitle = {{arXiv}:1604.00772 [cs, stat]},
	author = {Hansen, Nikolaus},
	urldate = {2017-12-29},
	date = {2016-04-04},
	eprinttype = {arxiv},
	eprint = {1604.00772},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ZE8GYLWE/1604.html:text/html;Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf:/Users/bert/Zotero/storage/G3FT8P6L/Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf:application/pdf},
}

@article{suchow_evolution_2017,
	title = {Evolution in Mind: Evolutionary Dynamics, Cognitive Processes, and Bayesian Inference},
	volume = {21},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661317300773},
	doi = {10.1016/j.tics.2017.04.005},
	shorttitle = {Evolution in Mind},
	abstract = {Evolutionary theory describes the dynamics of population change in settings affected by reproduction, selection, mutation, and drift. In the context of human cognition, evolutionary theory is most often invoked to explain the origins of capacities such as language, metacognition, and spatial reasoning, framing them as functional adaptations to an ancestral environment. However, evolutionary theory is useful for understanding the mind in a second way: as a mathematical framework for describing evolving populations of thoughts, ideas, and memories within a single mind. In fact, deep correspondences exist between the mathematics of evolution and of learning, with perhaps the deepest being an equivalence between certain evolutionary dynamics and Bayesian inference. This equivalence permits reinterpretation of evolutionary processes as algorithms for Bayesian inference and has relevance for understanding diverse cognitive capacities, including memory and creativity.},
	pages = {522--530},
	number = {7},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Suchow, Jordan W. and Bourgin, David D. and Griffiths, Thomas L.},
	urldate = {2017-12-29},
	date = {2017-07-01},
	keywords = {evolution, Bayesian inference, cognitive processes, creativity, learning, memory},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/CMRCSNCT/S1364661317300773.html:text/html;Suchow et al. - 2017 - Evolution in Mind Evolutionary Dynamics, Cognitiv.pdf:/Users/bert/Zotero/storage/EU3Y8VNQ/Suchow et al. - 2017 - Evolution in Mind Evolutionary Dynamics, Cognitiv.pdf:application/pdf},
}

@online{yost_psychoacoustics:_2015,
	title = {Psychoacoustics: A Brief Historical Overview - by William A. Yost},
	url = {http://acousticstoday.org/psychoacoustics-a-brief-historical-overview-by-william-a-yost/},
	shorttitle = {Psychoacoustics},
	titleaddon = {Acoustics Today},
	author = {Yost, William A},
	urldate = {2017-12-28},
	date = {2015-08-28},
	file = {Snapshot:/Users/bert/Zotero/storage/IECY2EXC/psychoacoustics-a-brief-historical-overview-by-william-a-yost.html:text/html;Yost - 2015 - Psychoacoustics A Brief Historical Overview - by .pdf:/Users/bert/Zotero/storage/LYKTX4XC/Yost - 2015 - Psychoacoustics A Brief Historical Overview - by .pdf:application/pdf},
}

@article{ahn_gauging_2017,
	title = {Gauging Variational Inference},
	url = {http://arxiv.org/abs/1703.01056},
	abstract = {Computing partition function is the most important statistical inference task arising in applications of Graphical Models ({GM}). Since it is computationally intractable, approximate methods have been used to resolve the issue in practice, where mean-field ({MF}) and belief propagation ({BP}) are arguably the most popular and successful approaches of a variational type. In this paper, we propose two new variational schemes, coined Gauged-{MF} (G-{MF}) and Gauged-{BP} (G-{BP}), improving {MF} and {BP}, respectively. Both provide lower bounds for the partition function by utilizing the so-called gauge transformation which modifies factors of {GM} while keeping the partition function invariant. Moreover, we prove that both G-{MF} and G-{BP} are exact for {GMs} with a single loop of a special structure, even though the bare {MF} and {BP} perform badly in this case. Our extensive experiments, on complete {GMs} of relatively small size and on large {GM} (up-to 300 variables) confirm that the newly proposed algorithms outperform and generalize {MF} and {BP}.},
	journaltitle = {{arXiv}:1703.01056 [stat]},
	author = {Ahn, Sungsoo and Chertkov, Michael and Shin, Jinwoo},
	urldate = {2017-12-28},
	date = {2017-03-03},
	eprinttype = {arxiv},
	eprint = {1703.01056},
	keywords = {Statistics - Machine Learning},
	file = {Ahn et al. - 2017 - Gauging Variational Inference.pdf:/Users/bert/Zotero/storage/NK6F959H/Ahn et al. - 2017 - Gauging Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/ZVBDWFR4/1703.html:text/html},
}

@article{simeone_brief_2017,
	title = {A Brief Introduction to Machine Learning for Engineers},
	url = {http://arxiv.org/abs/1709.02840},
	abstract = {This monograph aims at providing an introduction to key concepts, algorithms, and theoretical frameworks in machine learning, including supervised and unsupervised learning, statistical learning theory, probabilistic graphical models and approximate inference. The intended readership consists of electrical engineers with a background in probability and linear algebra. The treatment builds on first principles, and organizes the main ideas according to clearly defined categories, such as discriminative and generative models, frequentist and Bayesian approaches, exact and approximate inference, directed and undirected models, and convex and non-convex optimization. The mathematical framework uses information-theoretic measures as a unifying tool. The text offers simple and reproducible numerical examples providing insights into key motivations and conclusions. Rather than providing exhaustive details on the existing myriad solutions in each specific category, for which the reader is referred to textbooks and papers, this monograph is meant as an entry point for an engineer into the literature on machine learning.},
	journaltitle = {{arXiv}:1709.02840 [cs, math, stat]},
	author = {Simeone, Osvaldo},
	urldate = {2017-12-27},
	date = {2017-09-08},
	eprinttype = {arxiv},
	eprint = {1709.02840},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/KB96EDXI/1709.html:text/html;Simeone - 2017 - A Brief Introduction to Machine Learning for Engin.pdf:/Users/bert/Zotero/storage/ZKJ5BDJZ/Simeone - 2017 - A Brief Introduction to Machine Learning for Engin.pdf:application/pdf},
}

@article{isomura_cultured_2015,
	title = {Cultured Cortical Neurons Can Perform Blind Source Separation According to the Free-Energy Principle},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004643},
	doi = {10.1371/journal.pcbi.1004643},
	abstract = {Author Summary The ‘cocktail party’ effect is a phenomenon by which one is able to pick out and listen to a single person’s speech in a noisy room. In information engineering, this is termed blind source separation. Numerous computational studies demonstrate that simulated neural networks can perform blind source separation. However, if or how a living neural network learns to perform blind source separation remains unknown. Using a microelectrode array ({MEA}) system that allowed us to apply composite inputs and record responses from neurons throughout a cultured neural network, we discovered that even neurons in cultures of dissociated rat cortical cells can separate individual sources from a complex mixture of inputs in the absence of teacher signals. Given these findings, we then determined that the neural networks adapted to reduce free energy, as predicted by the free energy principle and Jaynes’ principle of maximum entropy. These results provide evidence that cultured neural networks can perform blind source separation and that they are governed by the free-energy principle, providing a compelling framework for understanding how the brain identifies and processes signals hidden in complex multivariate information.},
	pages = {e1004643},
	number = {12},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Isomura, Takuya and Kotani, Kiyoshi and Jimbo, Yasuhiko},
	urldate = {2017-12-27},
	date = {2015-12-21},
	keywords = {Free energy, Neural networks, Neurons, Action potentials, Electrode recording, Functional electrical stimulation, Neuronal plasticity, Synaptic plasticity},
	file = {Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:/Users/bert/Zotero/storage/LZA9T885/Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/7BLQC3TF/article.html:text/html},
}

@article{isomura_measure_2017,
	title = {A Measure of Information Available for Prediction},
	url = {http://www.preprints.org/manuscript/201711.0020/v1},
	doi = {10.20944/preprints201711.0020.v1},
	abstract = {Mutual information between the brain state and the external world state represents the amount of information stored in the brain that is associated with the external world. On the other hand, surprise of sensory input indicates the unpredictability of the current input. In other words, this is a measure of prediction capability, and an upper bound of surprise is known as free energy. According to the free-energy principle ({FEP}), the brain continues to minimize free energy to perceive the external world. For animals to survive, prediction capability is considered more important than just memorizing information. In this study, the fact that free energy represents a gap between the amount of information stored in the brain and that available for prediction is established, where the latter will be referred to as predictive information as an analogy with Bialek's predictive information. This concept involves the {FEP}, the infomax principle, and the predictive information theory, and will be a useful measure to quantify the amount of information available for prediction.},
	author = {Isomura, Takuya},
	urldate = {2017-12-27},
	date = {2017-11-02},
	langid = {english},
	file = {Isomura - 2017 - A Measure of Information Available for Prediction.pdf:/Users/bert/Zotero/storage/KHYE8BJG/Isomura - 2017 - A Measure of Information Available for Prediction.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/7E6NIH6R/v1.html:text/html},
}

@article{sengupta_information_2013,
	title = {Information and Efficiency in the Nervous System—A Synthesis},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pcbi.1003157},
	doi = {10.1371/journal.pcbi.1003157},
	abstract = {In systems biology, questions concerning the molecular and cellular makeup of an organism are of utmost importance, especially when trying to understand how unreliable components—like genetic circuits, biochemical cascades, and ion channels, among others—enable reliable and adaptive behaviour. The repertoire and speed of biological computations are limited by thermodynamic or metabolic constraints: an example can be found in neurons, where fluctuations in biophysical states limit the information they can encode—with almost 20–60\% of the total energy allocated for the brain used for signalling purposes, either via action potentials or by synaptic transmission. Here, we consider the imperatives for neurons to optimise computational and metabolic efficiency, wherein benefits and costs trade-off against each other in the context of self-organised and adaptive behaviour. In particular, we try to link information theoretic (variational) and thermodynamic (Helmholtz) free-energy formulations of neuronal processing and show how they are related in a fundamental way through a complexity minimisation lemma.},
	pages = {e1003157},
	number = {7},
	journaltitle = {{PLoS} Comput Biol},
	shortjournal = {{PLoS} Comput Biol},
	author = {Sengupta, Biswa and Stemmler, Martin B. and Friston, Karl J.},
	urldate = {2014-08-25},
	date = {2013-07-25},
	file = {PLoS Snapshot:/Users/bert/Zotero/storage/QQQ8QF9Y/infodoi10.1371journal.pcbi.html:text/html;Sengupta et al. - 2013 - Information and Efficiency in the Nervous System—A.pdf:/Users/bert/Zotero/storage/CRRHVLAF/Sengupta et al. - 2013 - Information and Efficiency in the Nervous System—A.pdf:application/pdf},
}

@article{isomura_local_2016,
	title = {A Local Learning Rule for Independent Component Analysis},
	volume = {6},
	rights = {2016 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep28073},
	doi = {10.1038/srep28073},
	abstract = {A Local Learning Rule for Independent Component Analysis},
	pages = {28073},
	journaltitle = {Scientific Reports},
	author = {Isomura, Takuya and Toyoizumi, Taro},
	urldate = {2017-12-27},
	date = {2016-06-21},
	file = {Isomura and Toyoizumi - 2016 - A Local Learning Rule for Independent Component An.pdf:/Users/bert/Zotero/storage/X33EE3GT/Isomura and Toyoizumi - 2016 - A Local Learning Rule for Independent Component An.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/AFUBAWKZ/srep28073.html:text/html},
}

@article{sedley_integrative_2017,
	title = {An Integrative Tinnitus Model Based on Sensory Precision},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223616301412},
	doi = {10.1016/j.tins.2016.10.004},
	abstract = {Tinnitus is a common disorder that often complicates hearing loss. Its mechanisms are incompletely understood. Current theories proposing pathophysiology from the ear to the cortex cannot individually – or collectively – explain the range of experimental evidence available. We propose a new framework, based on predictive coding, in which spontaneous activity in the subcortical auditory pathway constitutes a ‘tinnitus precursor’ which is normally ignored as imprecise evidence against the prevailing percept of ‘silence’. Extant models feature as contributory mechanisms acting to increase either the intensity of the precursor or its precision. If precision (i.e., postsynaptic gain) rises sufficiently then tinnitus is perceived. Perpetuation arises through focused attention, which further increases the precision of the precursor, and resetting of the default prediction to expect tinnitus.},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends in Neurosciences},
	author = {Sedley, William and Friston, Karl J. and Gander, Phillip E. and Kumar, Sukhbinder and Griffiths, Timothy D.},
	urldate = {2016-11-23},
	date = {2017},
	keywords = {predictive coding, auditory cortex, precision, tinnitus},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/2C7T6GUM/S0166223616301412.html:text/html;Sedley et al. - 2017 - An Integrative Tinnitus Model Based on Sensory Pre.pdf:/Users/bert/Zotero/storage/X6NLEE4Q/Sedley et al. - 2017 - An Integrative Tinnitus Model Based on Sensory Pre.pdf:application/pdf},
}

@article{valpola_neural_2014,
	title = {From neural {PCA} to deep unsupervised learning},
	url = {http://arxiv.org/abs/1411.7783},
	abstract = {A network supporting deep unsupervised learning is presented. The network is an autoencoder with lateral shortcut connections from the encoder to decoder at each level of the hierarchy. The lateral shortcut connections allow the higher levels of the hierarchy to focus on abstract invariant features. While standard autoencoders are analogous to latent variable models with a single layer of stochastic variables, the proposed network is analogous to hierarchical latent variables models. Learning combines denoising autoencoder and denoising sources separation frameworks. Each layer of the network contributes to the cost function a term which measures the distance of the representations produced by the encoder and the decoder. Since training signals originate from all levels of the network, all layers can learn efficiently even in deep networks. The speedup offered by cost terms from higher levels of the hierarchy and the ability to learn invariant features are demonstrated in experiments.},
	journaltitle = {{arXiv}:1411.7783 [cs, stat]},
	author = {Valpola, Harri},
	urldate = {2017-12-27},
	date = {2014-11-28},
	eprinttype = {arxiv},
	eprint = {1411.7783},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CSR77K2Q/1411.html:text/html;Valpola - 2014 - From neural PCA to deep unsupervised learning.pdf:/Users/bert/Zotero/storage/UJANLR3D/Valpola - 2014 - From neural PCA to deep unsupervised learning.pdf:application/pdf},
}

@article{altaner_nonequilibrium_2017,
	title = {Nonequilibrium thermodynamics and information theory: Basic concepts and relaxing dynamics},
	volume = {50},
	issn = {1751-8113, 1751-8121},
	url = {http://arxiv.org/abs/1702.07906},
	doi = {10.1088/1751-8121/aa841d},
	shorttitle = {Nonequilibrium thermodynamics and information theory},
	abstract = {Thermodynamics is based on the notions of energy and entropy. While energy is the elementary quantity governing physical dynamics, entropy is the fundamental concept in information theory. In this work, starting from first principles, we give a detailed didactic account on the relations between energy and entropy and thus physics and information theory. We show that thermodynamic process inequalities, like the Second Law, are equivalent to the requirement that an effective description for physical dynamics is strongly relaxing. From the perspective of information theory, strongly relaxing dynamics govern the irreversible convergence of a statistical ensemble towards the maximally non-commital probability distribution that is compatible with thermodynamic equilibrium parameters. In particular, Markov processes that converge to a thermodynamic equilibrium state are strongly relaxing. Our framework generalizes previous results to arbitrary open and driven systems, yielding novel thermodynamic bounds for idealized and real processes.},
	pages = {454001},
	number = {45},
	journaltitle = {Journal of Physics A: Mathematical and Theoretical},
	author = {Altaner, Bernhard},
	urldate = {2017-12-27},
	date = {2017-11-10},
	eprinttype = {arxiv},
	eprint = {1702.07906},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {Altaner - 2017 - Nonequilibrium thermodynamics and information theo.pdf:/Users/bert/Zotero/storage/7I6699DD/Altaner - 2017 - Nonequilibrium thermodynamics and information theo.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/LS4ZNNRE/1702.html:text/html},
}

@article{skopec_intelligent_2015,
	title = {Intelligent Evolution, Complexity and Self-Organization},
	volume = {13},
	rights = {Copyright of articles is assigned to {NeuroQuantology} from the date on which the article is accepted for publication. {NeuroQuantology} permits single copying of single published articles for private study or research, no matter where the copying is done. Authors republish their work for book chapters or their personal homepage without any permission.},
	issn = {1303-5150},
	url = {https://www.neuroquantology.com/index.php/journal/article/view/832},
	number = {3},
	journaltitle = {{NeuroQuantology}},
	author = {Skopec, Robert},
	urldate = {2017-12-27},
	date = {2015-06-02},
	langid = {english},
	keywords = {Evolution, Artificial Intelligence, dissipation, energy, life, Prigogine-England physics},
	file = {Full Text PDF:/Users/bert/Zotero/storage/KNSXMTI9/Skopec - 2015 - Intelligent Evolution, Complexity and Self-Organiz.pdf:application/pdf;Skopec - 2015 - Intelligent Evolution, Complexity and Self-Organiz.pdf:/Users/bert/Zotero/storage/ZB8LBD6S/Skopec - 2015 - Intelligent Evolution, Complexity and Self-Organiz.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MZUAULLX/719.html:text/html},
}

@article{crofts_life_2007,
	title = {Life, Information, Entropy, and Time},
	volume = {13},
	issn = {1076-2787},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2577055/},
	doi = {10.1002/cplx.20180},
	abstract = {Attempts to understand how information content can be included in an accounting of the energy flux of the biosphere have led to the conclusion that, in information transmission, one component, the semantic content, or “the meaning of the message,” adds no thermodynamic burden over and above costs arising from coding, transmission and translation. In biology, semantic content has two major roles. For all life forms, the message of the genotype encoded in {DNA} specifies the phenotype, and hence the organism that is tested against the real world through the mechanisms of Darwinian evolution. For human beings, communication through language and similar abstractions provides an additional supra-phenotypic vehicle for semantic inheritance, which supports the cultural heritages around which civilizations revolve. The following three postulates provide the basis for discussion of a number of themes that demonstrate some important consequences. (i) Information transmission through either pathway has thermodynamic components associated with data storage and transmission. (ii) The semantic content adds no additional thermodynamic cost. (iii) For all semantic exchange, meaning is accessible only through translation and interpretation, and has a value only in context. (1) For both pathways of semantic inheritance, translational and copying machineries are imperfect. As a consequence both pathways are subject to mutation and to evolutionary pressure by selection. Recognition of semantic content as a common component allows an understanding of the relationship between genes and memes, and a reformulation of Universal Darwinism. (2) The emergent properties of life are dependent on a processing of semantic content. The translational steps allow amplification in complexity through combinatorial possibilities in space and time. Amplification depends on the increased potential for complexity opened by 3D interaction specificity of proteins, and on the selection of useful variants by evolution. The initial interpretational steps include protein synthesis, molecular recognition, and catalytic potential that facilitate structural and functional roles. Combinatorial possibilities are extended through interactions of increasing complexity in the temporal dimension. (3) All living things show a behavior that indicates awareness of time, or chronognosis. The ∼4 billion years of biological evolution have given rise to forms with increasing sophistication in sensory adaptation. This has been linked to the development of an increasing chronognostic range, and an associated increase in combinatorial complexity. (4) Development of a modern human phenotype and the ability to communicate through language, led to the development of archival storage, and invention of the basic skills, institutions and mechanisms that allowed the evolution of modern civilizations. Combinatorial amplification at the supra-phenotypical level arose from the invention of syntax, grammar, numbers, and the subsequent developments of abstraction in writing, algorithms, etc. The translational machineries of the human mind, the “mutation” of ideas therein, and the “conversations” of our social intercourse, have allowed a limited set of symbolic descriptors to evolve into an exponentially expanding semantic heritage. (5) The three postulates above open interesting epistemological questions. An understanding of topics such dualism, the élan vital, the status of hypothesis in science, memetics, the nature of consciousness, the role of semantic processing in the survival of societies, and Popper's three worlds, require recognition of an insubstantial component. By recognizing a necessary linkage between semantic content and a physical machinery, we can bring these perennial problems into the framework of a realistic philosophy. It is suggested, following Popper, that the ∼4 billion years of evolution of the biosphere represents an exploration of the nature of reality at the physicochemical level, which, together with the conscious extension of this exploration through science and culture, provides a firm epistemological underpinning for such a philosophy.},
	pages = {14--50},
	number = {1},
	journaltitle = {Complexity},
	shortjournal = {Complexity},
	author = {Crofts, Antony R.},
	urldate = {2017-12-27},
	date = {2007},
	pmid = {18978960},
	pmcid = {PMC2577055},
	file = {Crofts - 2007 - Life, Information, Entropy, and Time.pdf:/Users/bert/Zotero/storage/XIPVP3FL/Crofts - 2007 - Life, Information, Entropy, and Time.pdf:application/pdf},
}

@article{donoho_50_2017,
	title = {50 Years of Data Science},
	volume = {26},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2017.1384734},
	doi = {10.1080/10618600.2017.1384734},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including {UC} Berkeley, {NYU}, {MIT}, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, {NJ}, September 18, 2015.},
	pages = {745--766},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Donoho, David},
	urldate = {2017-12-21},
	date = {2017-10-02},
	keywords = {Cross-study analysis, Data analysis, Data science, Meta analysis, Predictive modeling, Quantitative programming environments, Statistics},
	file = {Donoho - 2017 - 50 Years of Data Science.pdf:/Users/bert/Zotero/storage/XJADAKBE/Donoho - 2017 - 50 Years of Data Science.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/WDTD4VPY/10618600.2017.html:text/html},
}

@inproceedings{li_hyper-parameter_2016,
	location = {Arlington, Virginia, United States},
	title = {On Hyper-parameter Estimation in Empirical Bayes: A Revisit of the {MacKay} Algorithm},
	isbn = {978-0-9966431-1-5},
	url = {http://dl.acm.org/citation.cfm?id=3020948.3020998},
	series = {{UAI}'16},
	shorttitle = {On Hyper-parameter Estimation in Empirical Bayes},
	abstract = {An iterative procedure introduced in {MacKay}'s evidence framework is often used for estimating the hyper-parameter in empirical Bayes. Despite its effectiveness, the procedure has stayed primarily as a heuristic to date. This paper formally investigates the mathematical nature of this procedure and justifies it as a well-principled algorithm framework. This framework, which we call the {MacKay} algorithm, is shown to be closely related to the {EM} algorithm under certain Gaussian assumption.},
	pages = {477--486},
	booktitle = {Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence},
	publisher = {{AUAI} Press},
	author = {Li, Chune and Mao, Yongyi and Zhang, Richong and Huai, Jinpeng},
	urldate = {2017-12-18},
	date = {2016},
	file = {Li et al. - 2016 - On Hyper-parameter Estimation in Empirical Bayes .pdf:/Users/bert/Zotero/storage/D7WH5KYC/Li et al. - 2016 - On Hyper-parameter Estimation in Empirical Bayes .pdf:application/pdf},
}

@article{visser_zipfs_2013,
	title = {Zipf's law, power laws and maximum entropy},
	volume = {15},
	issn = {1367-2630},
	url = {http://stacks.iop.org/1367-2630/15/i=4/a=043021},
	doi = {10.1088/1367-2630/15/4/043021},
	abstract = {Zipf's law, and power laws in general, have attracted and continue to attract considerable attention in a wide variety of disciplines—from astronomy to demographics to software structure to economics to linguistics to zoology, and even warfare. A recent model of random group formation ({RGF}) attempts a general explanation of such phenomena based on Jaynes' notion of maximum entropy applied to a particular choice of cost function . In the present paper I argue that the specific cost function used in the {RGF} model is in fact unnecessarily complicated, and that power laws can be obtained in a much simpler way by applying maximum entropy ideas directly to the Shannon entropy subject only to a single constraint: that the average of the logarithm of the observable quantity is specified.},
	pages = {043021},
	number = {4},
	journaltitle = {New Journal of Physics},
	shortjournal = {New J. Phys.},
	author = {Visser, Matt},
	urldate = {2017-12-18},
	date = {2013},
	langid = {english},
	file = {Visser - 2013 - Zipf's law, power laws and maximum entropy.pdf:/Users/bert/Zotero/storage/DJJSNIBH/Visser - 2013 - Zipf's law, power laws and maximum entropy.pdf:application/pdf},
}

@article{caticha_entropic_2011,
	title = {Entropic Inference},
	url = {http://arxiv.org/abs/1011.0723},
	doi = {10.1063/1.3573619},
	abstract = {In this tutorial we review the essential arguments behing entropic inference. We focus on the epistemological notion of information and its relation to the Bayesian beliefs of rational agents. The problem of updating from a prior to a posterior probability distribution is tackled through an eliminative induction process that singles out the logarithmic relative entropy as the unique tool for inference. The resulting method of Maximum relative Entropy ({ME}), includes as special cases both {MaxEnt} and Bayes' rule, and therefore unifies the two themes of these workshops -- the Maximum Entropy and the Bayesian methods -- into a single general inference scheme.},
	pages = {20--29},
	journaltitle = {{arXiv}:1011.0723 [cond-mat, physics:physics, stat]},
	author = {Caticha, Ariel},
	urldate = {2017-12-17},
	date = {2011},
	eprinttype = {arxiv},
	eprint = {1011.0723},
	keywords = {Statistics - Methodology, Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/DZQFPTE4/1011.html:text/html;Caticha - 2011 - Entropic Inference.pdf:/Users/bert/Zotero/storage/RJ34P427/Caticha - 2011 - Entropic Inference.pdf:application/pdf},
}

@article{mohammad-djafari_entropy_2015,
	title = {Entropy, Information Theory, Information Geometry and Bayesian Inference in Data, Signal and Image Processing and Inverse Problems},
	volume = {17},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/17/6/3989},
	doi = {10.3390/e17063989},
	abstract = {The main content of this review article is first to review the main inference tools using Bayes rule, the maximum entropy principle ({MEP}), information theory, relative entropy and the Kullback–Leibler ({KL}) divergence, Fisher information and its corresponding geometries. For each of these tools, the precise context of their use is described. The second part of the paper is focused on the ways these tools have been used in data, signal and image processing and in the inverse problems, which arise in different physical sciences and engineering applications. A few examples of the applications are described: entropy in independent components analysis ({ICA}) and in blind source separation, Fisher information in data model selection, different maximum entropy-based methods in time series spectral estimation and in linear inverse problems and, finally, the Bayesian inference for general inverse problems. Some original materials concerning the approximate Bayesian computation ({ABC}) and, in particular, the variational Bayesian approximation ({VBA}) methods are also presented. {VBA} is used for proposing an alternative Bayesian computational tool to the classical Markov chain Monte Carlo ({MCMC}) methods. We will also see that {VBA} englobes joint maximum a posteriori ({MAP}), as well as the different expectation-maximization ({EM}) algorithms as particular cases.},
	pages = {3989--4027},
	number = {6},
	journaltitle = {Entropy},
	author = {Mohammad-Djafari, Ali},
	urldate = {2017-12-17},
	date = {2015-06-12},
	langid = {english},
	keywords = {Bayesian inference, maximum entropy principle, Kullback–Leibler divergence, Bayes, entropy, Fisher information, geometrical science of information, information theory, inverse problems, Laplace},
	file = {Mohammad-Djafari - 2015 - Entropy, Information Theory, Information Geometry .pdf:/Users/bert/Zotero/storage/QA6VFZRK/Mohammad-Djafari - 2015 - Entropy, Information Theory, Information Geometry .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/K9N4UQ3F/htm.html:text/html},
}

@article{heskes_t.m._empirical_2000,
	title = {Empirical Bayes for learning to learn},
	url = {http://repository.ubn.ru.nl/handle/2066/100959},
	pages = {367--374},
	journaltitle = {Langley, P. (ed.), {ICML}'00 : Proceedings of the Seventeenth International Conference on Machine Learning},
	author = {{Heskes, T.M.}},
	urldate = {2017-12-15},
	date = {2000},
	file = {Heskes, T.M. - 2000 - Empirical Bayes for learning to learn.pdf:/Users/bert/Zotero/storage/29Q5JDGA/Heskes, T.M. - 2000 - Empirical Bayes for learning to learn.pdf:application/pdf},
}

@article{auksztulewicz_task_2017,
	title = {Task relevance modulates the behavioural and neural effects of sensory predictions},
	volume = {15},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003143},
	doi = {10.1371/journal.pbio.2003143},
	abstract = {Author summary As natural environments change, animals need to continuously learn and update predictions about their current context to optimize behaviour. According to predictive coding, a general principle of brain function is the propagation of both neural predictions from hierarchically higher to lower brain regions and of the ensuing prediction-errors back up the cortical hierarchy. We show that the neural activity that signals internal predictions and prediction-errors depends on the current task or goals. We applied magnetoencephalography and computational modelling of behavioural data to a study in which human participants could generate spatial and temporal predictions about upcoming stimuli, while performing spatial or temporal tasks. We found that current context (task relevance) modulated the influence of predictions on behavioural and neural responses. At the level of behavioural responses, only the task-relevant predictions led to improvement in task performance. At the level of neural responses, we found that predictions and prediction-errors correlated with activity in different brain regions and in dissociable frequency bands—reflecting synchronized neural activity. Crucially, these specific neural signatures of prediction and prediction-error signalling were strongly modulated by their contextual relevance. Thus, our results show that current goals influence prediction and prediction-error signalling in the brain.},
	pages = {e2003143},
	number = {12},
	journaltitle = {{PLOS} Biology},
	shortjournal = {{PLOS} Biology},
	author = {Auksztulewicz, Ryszard and Friston, Karl J. and Nobre, Anna C.},
	urldate = {2017-12-10},
	date = {2017-12-04},
	keywords = {Vision, Learning, Coding mechanisms, Cognition, Convolution, Magnetoencephalography, Pitch perception, Sensory cues},
	file = {Auksztulewicz et al. - 2017 - Task relevance modulates the behavioural and neura.pdf:/Users/bert/Zotero/storage/NSQ82QPY/Auksztulewicz et al. - 2017 - Task relevance modulates the behavioural and neura.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/JLBIWRJL/article.html:text/html},
}

@article{kaplan_planning_2017,
	title = {Planning and navigation as active inference},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/12/07/230599},
	doi = {10.1101/230599},
	abstract = {This paper introduces an active inference formulation of planning and navigation. It illustrates how the exploitation-exploration dilemma is dissolved by acting to minimise uncertainty (i.e., expected surprise or free energy). We use simulations of a maze problem to illustrate how agents can solve quite complicated problems using context sensitive prior preferences to form subgoals. Our focus is on how epistemic behaviour, driven by novelty and the imperative to reduce uncertainty about the world, contextualises pragmatic or goal-directed behaviour. Using simulations, we illustrate the underlying process theory with synthetic behavioural and electrophysiological responses during exploration of a maze and subsequent navigation to a target location. An interesting phenomenon that emerged from the simulations was a putative distinction between place cells that fire when a subgoal is reached, and path cells, that fire until a subgoal is reached.},
	pages = {230599},
	journaltitle = {{bioRxiv}},
	author = {Kaplan, Raphael and Friston, Karl},
	urldate = {2017-12-09},
	date = {2017-12-07},
	langid = {english},
	file = {Kaplan and Friston - 2017 - Planning and navigation as active inference.pdf:/Users/bert/Zotero/storage/FASFM7TJ/Kaplan and Friston - 2017 - Planning and navigation as active inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/LZQ84Q8D/230599.html:text/html},
}

@article{gronau_tutorial_2017,
	title = {A Tutorial on Bridge Sampling},
	url = {http://arxiv.org/abs/1703.05984},
	abstract = {The marginal likelihood plays an important role in many areas of Bayesian statistics such as parameter estimation, model comparison, and model averaging. In most applications, however, the marginal likelihood is not analytically tractable and must be approximated using numerical methods. Here we provide a tutorial on bridge sampling (Bennett, 1976; Meng \& Wong, 1996), a reliable and relatively straightforward sampling method that allows researchers to obtain the marginal likelihood for models of varying complexity. First, we introduce bridge sampling and three related sampling methods using the beta-binomial model as a running example. We then apply bridge sampling to estimate the marginal likelihood for the Expectancy Valence ({EV}) model---a popular model for reinforcement learning. Our results indicate that bridge sampling provides accurate estimates for both a single participant and a hierarchical version of the {EV} model. We conclude that bridge sampling is an attractive method for mathematical psychologists who typically aim to approximate the marginal likelihood for a limited set of possibly high-dimensional models.},
	journaltitle = {{arXiv}:1703.05984 [stat]},
	author = {Gronau, Quentin F. and Sarafoglou, Alexandra and Matzke, Dora and Ly, Alexander and Boehm, Udo and Marsman, Maarten and Leslie, David S. and Forster, Jonathan J. and Wagenmakers, Eric-Jan and Steingroever, Helen},
	urldate = {2017-11-30},
	date = {2017-03-17},
	eprinttype = {arxiv},
	eprint = {1703.05984},
	keywords = {Statistics - Computation},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/EA2ZHCPK/1703.html:text/html;Gronau e.a. - 2017 - A Tutorial on Bridge Sampling.pdf:/Users/bert/Zotero/storage/MXS79MJY/Gronau e.a. - 2017 - A Tutorial on Bridge Sampling.pdf:application/pdf},
}

@article{keller_bayesian_2017,
	title = {Bayesian Model Averaging By Mixture Modeling},
	url = {http://arxiv.org/abs/1711.10016},
	abstract = {A new and numerically efficient method for Bayes factor computation and Bayesian model averaging, seen as a special case of the mixture model approach for Bayesian model selection in the seminal work of Kamari, 2014. Inheriting from the good properties of this approach, it allows to extend classical Bayesian model selection/averaging to cases where improper priors are chosen for the common parameter of the candidate models.},
	journaltitle = {{arXiv}:1711.10016 [stat]},
	author = {Keller, Merlin and Kamary, Kaniav},
	urldate = {2017-11-29},
	date = {2017-11-27},
	eprinttype = {arxiv},
	eprint = {1711.10016},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/PIIKJIU9/1711.html:text/html;Keller en Kamary - 2017 - Bayesian Model Averaging By Mixture Modeling.pdf:/Users/bert/Zotero/storage/YMMXESRT/Keller en Kamary - 2017 - Bayesian Model Averaging By Mixture Modeling.pdf:application/pdf},
}

@article{parr_uncertainty_2017,
	title = {Uncertainty, epistemics and active inference},
	volume = {14},
	rights = {© 2017 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/14/136/20170376},
	doi = {10.1098/rsif.2017.0376},
	abstract = {Biological systems—like ourselves—are constantly faced with uncertainty. Despite noisy sensory data, and volatile environments, creatures appear to actively maintain their integrity. To account for this remarkable ability to make optimal decisions in the face of a capricious world, we propose a generative model that represents the beliefs an agent might possess about their own uncertainty. By simulating a noisy and volatile environment, we demonstrate how uncertainty influences optimal epistemic (visual) foraging. In our simulations, saccades were deployed less frequently to regions with a lower sensory precision, while a greater volatility led to a shorter inhibition of return. These simulations illustrate a principled explanation for some cardinal aspects of visual foraging—and allow us to propose a correspondence between the representation of uncertainty and ascending neuromodulatory systems, complementing that suggested by Yu \& Dayan (Yu \& Dayan 2005 Neuron 46, 681–692. (doi:10.1016/j.neuron.2005.04.026)).},
	pages = {20170376},
	number = {136},
	journaltitle = {Journal of The Royal Society Interface},
	author = {Parr, Thomas and Friston, Karl J.},
	urldate = {2017-11-29},
	date = {2017-11-01},
	langid = {english},
	pmid = {29167370},
	file = {Parr and Friston - 2017 - Uncertainty, epistemics and active inference.pdf:/Users/bert/Zotero/storage/W9HVB6WW/Parr and Friston - 2017 - Uncertainty, epistemics and active inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CSPI6QJD/20170376.html:text/html},
}

@article{cooke_bayesian_2017,
	title = {Bayesian adaptive stimulus selection for dissociating models of psychophysical data},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/19/220590},
	doi = {10.1101/220590},
	abstract = {Comparing models facilitates testing different hypotheses regarding the computational basis of perception and action. Effective model comparison requires stimuli for which models make different predictions. Typically, experiments use a predetermined set of stimuli or sample stimuli randomly. Both methods have limitations; a predetermined set may not contain stimuli that dissociate the models whereas random sampling may be inefficient. To overcome these limitations, we expanded the psi-algorithm (Kontsevich \& Tyler, 1999) from estimating the parameters of a psychometric curve to distinguishing models. To test our algorithm, we applied it to two distinct problems. First, we investigated dissociating sensory noise models. We simulated ideal observers with different noise models performing a 2-afc task. Stimuli were selected randomly or using our algorithm. We found using our algorithm improved the accuracy of model comparison. We also validated the algorithm in subjects by inferring which noise model underlies speed perception. Our algorithm converged quickly to the model previously proposed (Stocker \& Simoncelli, 2006), whereas if stimuli were selected randomly model probabilities separated slower and sometimes supported alternative models. Second, we applied it to a different problem; comparing models of target selection under body acceleration. Previous work found target choice preference is modulated by whole body acceleration (Rincon-Gonzalez et al., 2016). However, the effect is subtle making model comparison difficult. We show that selecting stimuli adaptively could have led to stronger conclusions in model comparison. We conclude that our technique is more efficient and more reliable than current methods of stimuli selection for dissociating models.},
	pages = {220590},
	journaltitle = {{bioRxiv}},
	author = {Cooke, James R. H. and Selen, Luc P. J. and Beers, Robert J. van and Medendorp, W. Pieter},
	urldate = {2017-11-28},
	date = {2017-11-19},
	langid = {english},
	file = {Cooke e.a. - 2017 - Bayesian adaptive stimulus selection for dissociat.pdf:/Users/bert/Zotero/storage/FHVIXNM7/Cooke e.a. - 2017 - Bayesian adaptive stimulus selection for dissociat.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RBA5DSQ6/220590.html:text/html},
}

@inproceedings{lee_nonparametric_2012,
	title = {A nonparametric Bayesian approach to acoustic model discovery},
	url = {http://dl.acm.org/citation.cfm?id=2390531},
	pages = {40--49},
	booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Chia-ying and Glass, James},
	urldate = {2017-04-03},
	date = {2012},
	file = {Lee and Glass - 2012 - A nonparametric Bayesian approach to acoustic mode.pdf:/Users/bert/Zotero/storage/T9T7XSTV/Lee and Glass - 2012 - A nonparametric Bayesian approach to acoustic mode.pdf:application/pdf},
}

@article{lake_human-level_2015,
	title = {Human-level concept learning through probabilistic program induction},
	volume = {350},
	rights = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/350/6266/1332},
	doi = {10.1126/science.aab3050},
	abstract = {Handwritten characters drawn by a model
Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look “right” as judged by Turing-like tests of the model's output in comparison to what real humans produce.
Science, this issue p. 1332
People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human behavior.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.
Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model.},
	pages = {1332--1338},
	number = {6266},
	journaltitle = {Science},
	author = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	urldate = {2016-01-25},
	date = {2015-12-11},
	langid = {english},
	pmid = {26659050},
	file = {Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:/Users/bert/Zotero/storage/T6KEP2Z8/Lake et al. - 2015 - Human-level concept learning through probabilistic.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/T9HV46FZ/1332.html:text/html},
}

@inproceedings{lake_one-shot_2014,
	title = {One-shot learning of generative speech concepts},
	url = {http://www.bibsonomy.org/bibtex/26aa05340224719776c4d3017aa6ed882/dblp},
	eventtitle = {The annual meeting of the cognitive science society ({COGSCI}-2014)},
	author = {Lake, Brenden M. and Lee, Chia-ying and Glass, James and Tenenbaum, Joshua B.},
	urldate = {2017-03-07},
	date = {2014},
	file = {Lake et al. - 2014 - One-shot learning of generative speech concepts.pdf:/Users/bert/Zotero/storage/P4P7RZ6Y/Lake et al. - 2014 - One-shot learning of generative speech concepts.pdf:application/pdf},
}

@article{johnson_bayesian_2013,
	title = {Bayesian nonparametric hidden semi-Markov models},
	volume = {14},
	url = {http://www.jmlr.org/papers/v14/johnson13a.html},
	pages = {673--701},
	issue = {Feb},
	journaltitle = {Journal of Machine Learning Research},
	author = {Johnson, Matthew J. and Willsky, Alan S.},
	urldate = {2017-04-12},
	date = {2013},
	file = {GitHub - mattjj/pyhsmm:/Users/bert/Zotero/storage/38FD4WD9/pyhsmm.html:text/html;Johnson and Willsky - 2013 - Bayesian nonparametric hidden semi-Markov models.pdf:/Users/bert/Zotero/storage/AF3WPBHF/Johnson and Willsky - 2013 - Bayesian nonparametric hidden semi-Markov models.pdf:application/pdf},
}

@article{sahu_hierarchical_2012,
	title = {Hierarchical Bayesian autoregressive models for large space–time data with applications to ozone concentration modelling},
	volume = {28},
	issn = {1526-4025},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/asmb.1951/abstract},
	doi = {10.1002/asmb.1951},
	abstract = {Increasingly large volumes of space–time data are collected everywhere by mobile computing applications, and in many of these cases, temporal data are obtained by registering events, for example, telecommunication or Web traffic data. Having both the spatial and temporal dimensions adds substantial complexity to data analysis and inference tasks. The computational complexity increases rapidly for fitting Bayesian hierarchical models, as such a task involves repeated inversion of large matrices. The primary focus of this paper is on developing space–time autoregressive models under the hierarchical Bayesian setup. To handle large data sets, a recently developed Gaussian predictive process approximation method is extended to include autoregressive terms of latent space–time processes. Specifically, a space–time autoregressive process, supported on a set of a smaller number of knot locations, is spatially interpolated to approximate the original space–time process. The resulting model is specified within a hierarchical Bayesian framework, and Markov chain Monte Carlo techniques are used to make inference. The proposed model is applied for analysing the daily maximum 8-h average ground level ozone concentration data from 1997 to 2006 from a large study region in the Eastern United States. The developed methods allow accurate spatial prediction of a temporally aggregated ozone summary, known as the primary ozone standard, along with its uncertainty, at any unmonitored location during the study period. Trends in spatial patterns of many features of the posterior predictive distribution of the primary standard, such as the probability of noncompliance with respect to the standard, are obtained and illustrated. Copyright © 2012 John Wiley \& Sons, Ltd.},
	pages = {395--415},
	number = {5},
	journaltitle = {Applied Stochastic Models in Business and Industry},
	shortjournal = {Appl. Stochastic Models Bus. Ind.},
	author = {Sahu, Sujit Kumar and Bakar, Khandoker Shuvo},
	urldate = {2017-11-22},
	date = {2012-09-01},
	langid = {english},
	keywords = {Bayesian inference, big-n problem, downscaler, Gaussian predictive process, ozone concentration, space–time modelling},
	file = {Sahu and Bakar - 2012 - Hierarchical Bayesian autoregressive models for la.pdf:/Users/bert/Zotero/storage/6T5PWSMX/Sahu and Bakar - 2012 - Hierarchical Bayesian autoregressive models for la.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/C5CT2CP3/abstract.html:text/html},
}

@article{feldman_attention_2010,
	title = {Attention, Uncertainty, and Free-Energy},
	volume = {4},
	issn = {1662-5161},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3001758/},
	doi = {10.3389/fnhum.2010.00215},
	abstract = {We suggested recently that attention can be understood as inferring the level of uncertainty or precision during hierarchical perception. In this paper, we try to substantiate this claim using neuronal simulations of directed spatial attention and biased competition. These simulations assume that neuronal activity encodes a probabilistic representation of the world that optimizes free-energy in a Bayesian fashion. Because free-energy bounds surprise or the (negative) log-evidence for internal models of the world, this optimization can be regarded as evidence accumulation or (generalized) predictive coding. Crucially, both predictions about the state of the world generating sensory data and the precision of those data have to be optimized. Here, we show that if the precision depends on the states, one can explain many aspects of attention. We illustrate this in the context of the Posner paradigm, using the simulations to generate both psychophysical and electrophysiological responses. These simulated responses are consistent with attentional bias or gating, competition for attentional resources, attentional capture and associated speed-accuracy trade-offs. Furthermore, if we present both attended and non-attended stimuli simultaneously, biased competition for neuronal representation emerges as a principled and straightforward property of Bayes-optimal perception.},
	journaltitle = {Frontiers in Human Neuroscience},
	shortjournal = {Front Hum Neurosci},
	author = {Feldman, Harriet and Friston, Karl J.},
	urldate = {2017-08-10},
	date = {2010-12-02},
	pmid = {21160551},
	pmcid = {PMC3001758},
	file = {Feldman en Friston - 2010 - Attention, Uncertainty, and Free-Energy.pdf:/Users/bert/Zotero/storage/48CZWRPT/Feldman en Friston - 2010 - Attention, Uncertainty, and Free-Energy.pdf:application/pdf},
}

@article{tucker_rebar:_2017,
	title = {{REBAR}: Low-variance, unbiased gradient estimates for discrete latent variable models},
	url = {http://arxiv.org/abs/1703.07370},
	shorttitle = {{REBAR}},
	abstract = {Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the {REINFORCE} estimator. Recent work (Jang et al. 2016; Maddison et al. 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, {\textbackslash}emph\{unbiased\} gradient estimates. Then, we introduce a novel continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log likelihood.},
	journaltitle = {{arXiv}:1703.07370 [cs, stat]},
	author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, Dieterich and Sohl-Dickstein, Jascha},
	urldate = {2017-11-06},
	date = {2017-03-21},
	eprinttype = {arxiv},
	eprint = {1703.07370},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ZHY8BSFN/1703.html:text/html;Tucker e.a. - 2017 - REBAR Low-variance, unbiased gradient estimates f.pdf:/Users/bert/Zotero/storage/LRSUS97V/Tucker e.a. - 2017 - REBAR Low-variance, unbiased gradient estimates f.pdf:application/pdf},
}

@article{buckley_free_2017,
	title = {The free energy principle for action and perception: A mathematical review},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249617300962},
	doi = {10.1016/j.jmp.2017.09.004},
	shorttitle = {The free energy principle for action and perception},
	abstract = {The ‘free energy principle’ ({FEP}) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the {FEP} combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the {FEP} that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the {FEP}; (ii) provide a simple but complete agent-based model utilising the {FEP} and (iii) to disclose the assumption structure of this implementation of the {FEP} to help elucidate its significance for the brain sciences.},
	journaltitle = {Journal of Mathematical Psychology},
	shortjournal = {Journal of Mathematical Psychology},
	author = {Buckley, Christopher L. and Kim, Chang Sub and {McGregor}, Simon and Seth, Anil K.},
	urldate = {2017-11-03},
	date = {2017-10-21},
	keywords = {Free energy principle, Perception, Bayesian brain, Action, Agent-based model, Inference},
	file = {Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf:/Users/bert/Zotero/storage/C2J8TDLH/Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/V9382V5G/S0022249617300962.html:text/html},
}

@article{still_thermodynamic_2017,
	title = {Thermodynamic cost and benefit of data representations},
	url = {http://arxiv.org/abs/1705.00612},
	abstract = {This paper takes a thermodynamic approach to addressing the problem of how to represent data efficiently and meaningfully, a problem at the heart of learning and adaptation in both biological and artificial systems. Thermodynamic analysis of an information engine's cyclic operation reveals information theoretic quantities that are setting limits on performance. If run at fixed temperature, dissipation is lower bounded by a term proportional to irrelevant information. Data representation strategies that are optimal in the sense of minimizing dissipation must therefore strive to retain only relevant information. When an information engine is allowed to make use of a temperature difference, it can produce net work output, for which an upper bound is derived. Maximizing the bound yields a direct derivation of the Information Bottleneck method, a known technique in signal processing and machine learning, used precisely to filter relevant information from irrelevant clutter.},
	journaltitle = {{arXiv}:1705.00612 [cond-mat]},
	author = {Still, Susanne},
	urldate = {2017-11-03},
	date = {2017-04-29},
	eprinttype = {arxiv},
	eprint = {1705.00612},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5Q49LFZ8/1705.html:text/html;Still - 2017 - Thermodynamic cost and benefit of data representat.pdf:/Users/bert/Zotero/storage/JZDS5VQX/Still - 2017 - Thermodynamic cost and benefit of data representat.pdf:application/pdf},
}

@online{noauthor_new_nodate,
	title = {A New Thermodynamics Theory of the Origin of Life},
	url = {https://www.quantamagazine.org/a-new-thermodynamics-theory-of-the-origin-of-life-20140122/},
	abstract = {An {MIT} physicist has proposed the provocative idea that life exists because the law of increasing entropy drives matter to acquire lifelike physical properties.},
	titleaddon = {Quanta Magazine},
	urldate = {2017-11-03},
	file = {Snapshot:/Users/bert/Zotero/storage/8FUWAWAX/a-new-thermodynamics-theory-of-the-origin-of-life-20140122.html:text/html},
}

@article{ball_proteins_nodate,
	title = {Proteins remember the past to predict the future},
	url = {http://www.nature.com/news/proteins-remember-the-past-to-predict-the-future-1.11544},
	doi = {10.1038/nature.2012.11544},
	abstract = {Insight into what makes biological machines efficient could improve scientific models.},
	journaltitle = {Nature News},
	author = {Ball, Philip},
	urldate = {2017-11-03},
	file = {Snapshot:/Users/bert/Zotero/storage/EXMTLYRI/proteins-remember-the-past-to-predict-the-future-1.html:text/html},
}

@online{dedeo_nostalgia_2015,
	title = {Nostalgia Just Became a Law of Nature},
	url = {http://nautil.us/issue/21/information/nostalgia-just-became-a-law-of-nature},
	abstract = {John Ruskin called it the pathetic fallacy: to see rainstorms as passionate, drizzles as sad, and melting streams as innocent. After\&\#8230;},
	titleaddon = {Nautilus},
	author = {{DeDeo}, Simon},
	urldate = {2017-11-03},
	date = {2015-02-19},
	file = {Snapshot:/Users/bert/Zotero/storage/GIGBNS7E/nostalgia-just-became-a-law-of-nature.html:text/html},
}

@article{still_thermodynamics_2012,
	title = {The thermodynamics of prediction},
	volume = {109},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1203.3271},
	doi = {10.1103/PhysRevLett.109.120604},
	abstract = {A system responding to a stochastic driving signal can be interpreted as computing, by means of its dynamics, an implicit model of the environmental variables. The system's state retains information about past environmental fluctuations, and a fraction of this information is predictive of future ones. The remaining nonpredictive information reflects model complexity that does not improve predictive power, and thus represents the ineffectiveness of the model. We expose the fundamental equivalence between this model inefficiency and thermodynamic inefficiency, measured by dissipation. Our results hold arbitrarily far from thermodynamic equilibrium and are applicable to a wide range of systems, including biomolecular machines. They highlight a profound connection between the effective use of information and efficient thermodynamic operation: any system constructed to keep memory about its environment and to operate with maximal energetic efficiency has to be predictive.},
	number = {12},
	journaltitle = {Physical Review Letters},
	author = {Still, Susanne and Sivak, David A. and Bell, Anthony J. and Crooks, Gavin E.},
	urldate = {2017-11-03},
	date = {2012-09-19},
	eprinttype = {arxiv},
	eprint = {1203.3271},
	keywords = {Quantitative Biology - Quantitative Methods, Computer Science - Information Theory, Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/IPWM66WA/1203.html:text/html;Still et al. - 2012 - The thermodynamics of prediction.pdf:/Users/bert/Zotero/storage/CEUY432C/Still et al. - 2012 - The thermodynamics of prediction.pdf:application/pdf},
}

@article{gruters_eardrums_2017,
	title = {The eardrums move when the eyes move: A multisensory effect on the mechanics of hearing},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2017/10/13/156570},
	doi = {10.1101/156570},
	shorttitle = {The eardrums move when the eyes move},
	abstract = {Interactions between sensory pathways such as the visual and auditory systems are known to occur in the brain, but where they first occur is uncertain. Here we show a novel multimodal interaction evident at the eardrum. Ear canal microphone measurements in humans (n=19 ears in 16 subjects) and monkeys (n=5 ears in 3 subjects) performing a saccadic eye movement task to visual targets indicated that the eardrum moves in conjunction with the eye movement. The eardrum motion was oscillatory and began as early as 10 ms before saccade onset in humans or with saccade onset in monkeys. These eardrum movements, which we dub Eye Movement Related Eardrum Oscillations ({EMREOs}), occurred in the absence of a sound stimulus. The {EMREOs}' amplitude and phase depended on the direction and horizontal amplitude of the saccade. They lasted throughout the saccade and well into subsequent periods of steady fixation. We discuss the possibility that the mechanisms underlying {EMREOs} create eye movement-related binaural cues that may aid the brain in evaluating the relationship between visual and auditory stimulus locations as the eyes move.},
	pages = {156570},
	journaltitle = {{bioRxiv}},
	author = {Gruters, Kurtis G. and Murphy, David L. K. and Jenson, Cole D. and Smith, David W. and Shera, Christopher A. and Groh, Jennifer M.},
	urldate = {2017-11-03},
	date = {2017-10-13},
	langid = {english},
	file = {Gruters et al. - 2017 - The eardrums move when the eyes move A multisenso.pdf:/Users/bert/Zotero/storage/SQ9VVA7Y/Gruters et al. - 2017 - The eardrums move when the eyes move A multisenso.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/48BDIJMM/156570.html:text/html},
}

@misc{nowe_gentle_2016,
	title = {A gentle introduction to Reinforcement learning},
	author = {Nowe, Ann},
	date = {2016},
	file = {Nowe - 2016 - A gentle introduction to Reinforcement learning.pdf:/Users/bert/Zotero/storage/X3FZRUSC/Nowe - 2016 - A gentle introduction to Reinforcement learning.pdf:application/pdf},
}

@article{chodrow_divergence_2017,
	title = {Divergence, Entropy, Information: An Opinionated Introduction to Information Theory},
	url = {http://arxiv.org/abs/1708.07459},
	shorttitle = {Divergence, Entropy, Information},
	abstract = {Information theory is a mathematical theory of learning with deep connections with topics as diverse as artificial intelligence, statistical physics, and biological evolution. Many primers on the topic paint a broad picture with relatively little mathematical sophistication, while many others develop specific application areas in detail. In contrast, these informal notes aim to outline some elements of the information-theoretic "way of thinking," by cutting a rapid and interesting path through some of the theory's foundational concepts and theorems. We take the Kullback-Leibler divergence as our foundational concept, and then proceed to develop the entropy and mutual information. We discuss some of the main foundational results, including the Chernoff bounds as a characterization of the divergence; Gibbs' Theorem; and the Data Processing Inequality. A recurring theme is that the definitions of information theory support natural theorems that sound "obvious" when translated into English. More pithily, "information theory makes common sense precise." Since the focus of the notes is not primarily on technical details, proofs are provided only where the relevant techniques are illustrative of broader themes. Otherwise, proofs and intriguing tangents are referenced in liberally-sprinkled footnotes. The notes close with a highly nonexhaustive list of references to resources and other perspectives on the field.},
	journaltitle = {{arXiv}:1708.07459 [physics, stat]},
	author = {Chodrow, Philip},
	urldate = {2017-10-30},
	date = {2017-08-24},
	eprinttype = {arxiv},
	eprint = {1708.07459},
	keywords = {Mathematics - Statistics Theory, Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CQMKYJLN/1708.html:text/html;Chodrow - 2017 - Divergence, Entropy, Information An Opinionated I.pdf:/Users/bert/Zotero/storage/WNU3HAD8/Chodrow - 2017 - Divergence, Entropy, Information An Opinionated I.pdf:application/pdf},
}

@article{chen_relationship_2006,
	title = {The relationship between the power prior and hierarchical models},
	volume = {1},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1340371052},
	doi = {10.1214/06-BA118},
	abstract = {The power prior has emerged as a useful informative prior for the incorporation of historical data in a Bayesian analysis. Viewing hierarchical modeling as the "gold standard" for combining information across studies, we provide a formal justification of the power prior by examining formal analytical relationships between the power prior and hierarchical modeling in linear models. Asymptotic relationships between the power prior and hierarchical modeling are obtained for non-normal models, including generalized linear models, for example. These analytical relationships unify the theory of the power prior, demonstrate the generality of the power prior, shed new light on benchmark analyses, and provide insights into the elicitation of the power parameter in the power prior. Several theorems are presented establishing these formal connections, as well as a formal methodology for eliciting a guide value for the power parameter a0a0a\_0 via hierarchical models.},
	pages = {551--574},
	number = {3},
	journaltitle = {Bayesian Analysis},
	shortjournal = {Bayesian Anal.},
	author = {Chen, Ming-Hui and Ibrahim, Joseph G.},
	urldate = {2017-10-27},
	date = {2006-09},
	mrnumber = {MR2221288},
	zmnumber = {1331.62130},
	keywords = {Generalized linear model, hierarchical model, historical data, power prior, prior elicitation, random effects model},
	file = {Chen and Ibrahim - 2006 - The relationship between the power prior and hiera.pdf:/Users/bert/Zotero/storage/IVW6ZYEC/Chen and Ibrahim - 2006 - The relationship between the power prior and hiera.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/97EMSCAT/1340371052.html:text/html},
}

@inproceedings{masegosa_bayesian_2017,
	location = {Sydney, Australia},
	title = {Bayesian Models of Data Streams with Hierarchical Power Priors},
	url = {http://proceedings.mlr.press/v70/masegosa17a/masegosa17a.pdf},
	eventtitle = {{ICML}},
	booktitle = {Proceedings of the 34 th International Conference on Machine Learning},
	author = {Masegosa, Andres and Nielsen, Thomas and Ramos-Lopez, Darıo and Salmeron, Antonio and Madsen, Anders},
	date = {2017},
	file = {Masegosa et al. - 2017 - Bayesian Models of Data Streams with Hierarchical .pdf:/Users/bert/Zotero/storage/3KIVMFAA/Masegosa et al. - 2017 - Bayesian Models of Data Streams with Hierarchical .pdf:application/pdf},
}

@article{bengio_representation_2013,
	title = {Representation Learning: A Review and New Perspectives},
	volume = {35},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2013.50},
	shorttitle = {Representation Learning},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
	pages = {1798--1828},
	number = {8},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Bengio, Y. and Courville, A. and Vincent, P.},
	date = {2013-08},
	keywords = {Humans, Speech recognition, Machine learning, Neural networks, neural nets, Algorithms, Abstracts, Artificial Intelligence, {AI}, artificial intelligence, autoencoder, autoencoders, Boltzmann machine, data representation, data structures, Deep learning, density estimation, Feature extraction, feature learning, geometrical connections, Learning systems, machine learning algorithms, manifold learning, Manifolds, Neural Networks (Computer), probabilistic models, probability, representation learning, unsupervised feature learning, unsupervised learning},
	file = {Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf:/Users/bert/Zotero/storage/HZ9YN6MM/Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/GB3MYCTR/6472238.html:text/html},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	rights = {© 2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {0028-0836},
	url = {https://www.nature.com/nature/journal/v521/n7553/abs/nature14539.html},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.
View full text},
	pages = {436--444},
	number = {7553},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {{LeCun}, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	urldate = {2017-10-25},
	date = {2015-05-28},
	langid = {english},
	keywords = {Computer science, Mathematics and computing},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/Users/bert/Zotero/storage/XIWDSW7B/LeCun et al. - 2015 - Deep learning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/K25G9AW6/nature14539.html:text/html},
}

@article{chatterjee_aging_2017,
	title = {Aging and efficiency in living systems: Complexity, adaptation and self-organization},
	volume = {163},
	issn = {0047-6374},
	url = {http://www.sciencedirect.com/science/article/pii/S0047637416302755},
	doi = {10.1016/j.mad.2017.02.009},
	series = {{SI}:A new era for ageing?},
	shorttitle = {Aging and efficiency in living systems},
	abstract = {Living systems are open, out-of-equilibrium thermodynamic entities, that maintain order by locally reducing their entropy. Aging is a process by which these systems gradually lose their ability to maintain their out-of-equilibrium state, as measured by their free-energy rate density, and hence, their order. Thus, the process of aging reduces the efficiency of those systems, making them fragile and less adaptive to the environmental fluctuations, gradually driving them towards the state of thermodynamic equilibrium. In this paper, we discuss the various metrics that can be used to understand the process of aging from a complexity science perspective. Among all the metrics that we propose, action efficiency, is observed to be of key interest as it can be used to quantify order and self-organization in any physical system. Based upon our arguments, we present the dependency of other metrics on the action efficiency of a system, and also argue as to how each of the metrics, influences all the other system variables. In order to support our claims, we draw parallels between technological progress and biological growth. Such parallels are used to support the universal applicability of the metrics and the methodology presented in this paper. Therefore, the results and the arguments presented in this paper throw light on the finer nuances of the science of aging.},
	pages = {2--7},
	issue = {Supplement C},
	journaltitle = {Mechanisms of Ageing and Development},
	shortjournal = {Mechanisms of Ageing and Development},
	author = {Chatterjee, Atanu and Georgiev, Georgi and Iannacchione, Germano},
	urldate = {2017-10-22},
	date = {2017-04-01},
	keywords = {Entropy, Complexity, Action, Adaptation, Aging, Efficiency, Self-organization},
	file = {Chatterjee et al. - 2017 - Aging and efficiency in living systems Complexity.pdf:/Users/bert/Zotero/storage/2TFJLEK2/Chatterjee et al. - 2017 - Aging and efficiency in living systems Complexity.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/6MD7R639/S0047637416302755.html:text/html},
}

@article{chatterjee_energy_2016,
	title = {Energy, Entropy and Complexity: Thermodynamic and information-theoretic perspectives on ageing},
	shorttitle = {Energy, Entropy and Complexity},
	journaltitle = {M. Kyriazis (2016) Challenging Ageing: The anti-senescence effects of Hormesis, Environmental Enrichment, and Information Exposure. Bentham},
	author = {Chatterjee, Atanu},
	date = {2016},
	file = {Chatterjee - 2016 - Energy, Entropy and Complexity Thermodynamic and .pdf:/Users/bert/Zotero/storage/ZTG29J7M/Chatterjee - 2016 - Energy, Entropy and Complexity Thermodynamic and .pdf:application/pdf},
}

@article{heylighen_cybernetic_2014,
	title = {Cybernetic principles of aging and rejuvenation: the buffering- challenging strategy for life extension},
	volume = {7},
	issn = {1874-6128},
	shorttitle = {Cybernetic principles of aging and rejuvenation},
	abstract = {Aging is analyzed as the spontaneous loss of adaptivity and increase in fragility that characterizes dynamic systems. Cybernetics defines the general regulatory mechanisms that a system can use to prevent or repair the damage produced by disturbances. According to the law of requisite variety, disturbances can be held in check by maximizing buffering capacity, range of compensatory actions, and knowledge about which action to apply to which disturbance. This suggests a general strategy for rejuvenating the organism by increasing its capabilities of adaptation. Buffering can be optimized by providing sufficient rest together with plenty of nutrients: amino acids, antioxidants, methyl donors, vitamins, minerals, etc. Knowledge and the range of action can be extended by subjecting the organism to an as large as possible variety of challenges. These challenges are ideally brief so as not to deplete resources and produce irreversible damage. However, they should be sufficiently intense and unpredictable to induce an overshoot in the mobilization of resources for damage repair, and to stimulate the organism to build stronger capabilities for tackling future challenges. This allows them to override the trade-offs and limitations that evolution has built into the organism's repair processes in order to conserve potentially scarce resources. Such acute, "hormetic" stressors strengthen the organism in part via the "order from noise" mechanism that destroys dysfunctional structures by subjecting them to strong, random variations. They include heat and cold, physical exertion, exposure, stretching, vibration, fasting, food toxins, micro-organisms, environmental enrichment and psychological challenges. The proposed buffering-challenging strategy may be able to extend life indefinitely, by forcing a periodic rebuilding and extension of capabilities, while using the Internet as an endless source of new knowledge about how to deal with disturbances.},
	pages = {60--75},
	number = {1},
	journaltitle = {Current Aging Science},
	shortjournal = {Curr Aging Sci},
	author = {Heylighen, Francis},
	date = {2014},
	pmid = {24852018},
	keywords = {Animals, Humans, Cybernetics, Aging, Adaptation, Physiological, Age Factors, Health Status, Homeostasis, Hormesis, Longevity, Models, Biological, Rejuvenation, Stress, Physiological},
	file = {Heylighen - 2014 - Cybernetic principles of aging and rejuvenation t.pdf:/Users/bert/Zotero/storage/7JGZEKSB/Heylighen - 2014 - Cybernetic principles of aging and rejuvenation t.pdf:application/pdf},
}

@article{heylighen_getting_2008,
	title = {Getting Things Done: The Science behind Stress-Free Productivity},
	volume = {41},
	issn = {0024-6301},
	url = {http://www.sciencedirect.com/science/article/pii/S0024630108000848},
	doi = {10.1016/j.lrp.2008.09.004},
	shorttitle = {Getting Things Done},
	abstract = {In 2001 David Allen proposed ‘Getting Things Done’ ({GTD}) as a method for enhancing personal productivity and reducing the stress caused by information overload. This paper argues that recent insights in psychology and cognitive science support and extend {GTD}'s recommendations. We first summarize {GTD} with the help of a flowchart, and then review the theories of situated, embodied and distributed cognition that purport to explain how the brain processes information and plans actions in the real world. The conclusion is that the brain heavily relies on the environment to function as an external memory, a trigger for actions, and a source of ‘affordances’, disturbances and feedback. We show how these principles are practically implemented in {GTD}, with its focus on organizing tasks into ‘actionable’ external memories, and on opportunistic, situation-dependent execution. Finally, inspired by the concept of stigmergy, we propose an extension of {GTD} to support collaborative work.},
	pages = {585--605},
	number = {6},
	journaltitle = {Long Range Planning},
	shortjournal = {Long Range Planning},
	author = {Heylighen, Francis and Vidal, Clément},
	urldate = {2017-10-22},
	date = {2008-12-01},
	file = {Heylighen and Vidal - 2008 - Getting Things Done The Science behind Stress-Fre.pdf:/Users/bert/Zotero/storage/TTY9YMYJ/Heylighen and Vidal - 2008 - Getting Things Done The Science behind Stress-Fre.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/JNKV2TEB/S0024630108000848.html:text/html},
}

@misc{de_vany_evolutionary_2000,
	title = {Evolutionary Fitness},
	author = {De Vany, Athur},
	date = {2000},
	file = {De Vany - 2000 - Evolutionary Fitness.pdf:/Users/bert/Zotero/storage/Z7E6D3HL/De Vany - 2000 - Evolutionary Fitness.pdf:application/pdf},
}

@article{principe_cognitive_2014,
	title = {Cognitive Architectures for Sensory Processing},
	volume = {102},
	issn = {0018-9219},
	doi = {10.1109/JPROC.2014.2307023},
	abstract = {This paper describes our efforts to design a cognitive architecture for object recognition in video. Unlike most efforts in computer vision, our work proposes a Bayesian approach to object recognition in video, using a hierarchical, distributed architecture of dynamic processing elements that learns in a self-organizing way to cluster objects in the video input. A biologically inspired innovation is to implement a top-down pathway across layers in the form of causes, creating effectively a bidirectional processing architecture with feedback. To simplify discrimination, overcomplete representations are utilized. Both inference and parameter learning are performed using empirical priors, while imposing appropriate sparseness constraints. Preliminary results show that the cognitive architecture has features that resemble the functional organization of the early visual cortex. One example showing the use of top-down connections is given to disambiguate a synthetic video from correlated noise.},
	pages = {514--525},
	number = {4},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Principe, J. C. and Chalasani, R.},
	date = {2014-04},
	keywords = {Bayes methods, Data models, Computational modeling, Computer architecture, belief networks, inference mechanisms, Empirical Bayes, cognition, Bayesian approach, bidirectional processing architecture, biologically inspired innovation, cluster object, cognitive architecture, Cognitive science, computer vision, correlated noise, correlation theory, distributed architecture, dynamic processing elements, early visual cortex, feedback, functional organization, hierarchical architecture, human computer interaction, inference learning, object recognition, Object recognition, parameter learning, Predictive models, sensory processing, sparseness constraint, synthetic video, top-down approach, top–down, video signal processing, visual cortex},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/7RMTMVIP/6775277.html:text/html;Principe and Chalasani - 2014 - Cognitive Architectures for Sensory Processing.pdf:/Users/bert/Zotero/storage/C7ZWJ5HS/Principe and Chalasani - 2014 - Cognitive Architectures for Sensory Processing.pdf:application/pdf},
}

@inproceedings{bitzer_brain_2015,
	title = {The Brain Uses Reliability of Stimulus Information when Making Perceptual Decisions},
	url = {http://papers.nips.cc/paper/5789-the-brain-uses-reliability-of-stimulus-information-when-making-perceptual-decisions.pdf},
	pages = {1045--1053},
	booktitle = {Advances in Neural Information Processing Systems 28},
	publisher = {Curran Associates, Inc.},
	author = {Bitzer, Sebastian and Kiebel, Stefan},
	editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
	urldate = {2017-10-16},
	date = {2015},
	file = {Bitzer en Kiebel - 2015 - The Brain Uses Reliability of Stimulus Information.pdf:/Users/bert/Zotero/storage/HY9BQLXR/Bitzer en Kiebel - 2015 - The Brain Uses Reliability of Stimulus Information.pdf:application/pdf;NIPS Snapshort:/Users/bert/Zotero/storage/C42PUUQH/5789-the-brain-uses-reliability-of-stimulus-information-when-making-perceptual-decisions.html:text/html},
}

@article{brown_complete_1981,
	title = {A Complete Class Theorem for Statistical Problems with Finite Sample Spaces},
	volume = {9},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/2240418},
	abstract = {This paper contains a complete class theorem (Theorem 3.2) which applies to most statistical estimation problems having a finite sample space. This theorem also applies to many other statistical problems with finite sample spaces. The description of this complete class involves a stepwise algorithm. At each step of the process it is necessary to construct the Bayes procedures in a suitably modified version of the original problem. The complete class is a minimal complete class if the loss function is strictly convex. Some examples are given to illustrate the application of this complete class theorem. Among these is a new result concerning the estimation of the parameters of a multinomial distribution under a normalized quadratic loss function. (See Example 4.5).},
	pages = {1289--1300},
	number = {6},
	journaltitle = {The Annals of Statistics},
	author = {Brown, Lawrence D.},
	urldate = {2017-10-13},
	date = {1981},
	file = {Brown - 1981 - A Complete Class Theorem for Statistical Problems .pdf:/Users/bert/Zotero/storage/9VGQXBPC/Brown - 1981 - A Complete Class Theorem for Statistical Problems .pdf:application/pdf},
}

@article{markovic_modeling_2015,
	title = {Modeling the Evolution of Beliefs Using an Attentional Focus Mechanism},
	volume = {11},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004558},
	doi = {10.1371/journal.pcbi.1004558},
	abstract = {Author Summary When making decisions in our everyday life (e.g. where to eat) we first have to identify a set of environmental features that are relevant for the decision (e.g. the distance to the place, current time or the price). Although we are able to make such inferences almost effortlessly, this type of problems is computationally challenging, as we live in a complex environment that constantly changes and contains an immense number of features. Here we investigated the question of how the human brain solves this computational challenge. In particular, we designed a new experimental paradigm and derived novel behavioral models to test the hypothesis that attention modulates the formation of beliefs about the relevance of several environmental features. As each behavioral model accounted for a different hypothesis about the underlying computational mechanism we compared them in their ability to explain the measured behavior of human subjects performing the experimental task. The model comparison indicates that an attentional-focus mechanism is a key feature of behavioral models that accurately replicate subjects’ behavior. These findings suggest that the evolution of beliefs is modulated by a competitive attractor dynamics that forms prior expectation about future outcomes. Hence, the findings provide interesting and novel insights into the computational mechanisms underlying human behavior when making decisions in complex environments.},
	pages = {e1004558},
	number = {10},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Marković, Dimitrije and Gläscher, Jan and Bossaerts, Peter and O’Doherty, John and Kiebel, Stefan J.},
	date = {2015-10-23},
	keywords = {Behavior, Attention, Probability distribution, Sensory perception, Vision, Covariance, Experimental design},
	file = {Marković et al. - 2015 - Modeling the Evolution of Beliefs Using an Attenti.pdf:/Users/bert/Zotero/storage/2DXQ3I6E/Marković et al. - 2015 - Modeling the Evolution of Beliefs Using an Attenti.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/5Z52QMWZ/article.html:text/html},
}

@inproceedings{wadehn_outlier-insensitive_2016,
	title = {Outlier-insensitive Kalman smoothing and marginal message passing},
	doi = {10.1109/EUSIPCO.2016.7760447},
	abstract = {We propose a new approach to outlier-insensitive Kalman smoothing based on normal priors with unknown variance ({NUV}). In contrast to prior work, the actual computations amount essentially to iterations of a standard Kalman smoother (with few extra computations). The proposed approach is easily extended to nonlinear estimation problems by combining the outlier detection with an extended Kalman smoother. For the Kalman smoothing, we consider both a Modified Bryson-Frasier smoother and the recently proposed Backward Information Filter Forward Marginal smoother, neither of which requires matrix inversions.},
	eventtitle = {2016 24th European Signal Processing Conference ({EUSIPCO})},
	pages = {1242--1246},
	booktitle = {2016 24th European Signal Processing Conference ({EUSIPCO})},
	author = {Wadehn, F. and Bruderer, L. and Dauwels, J. and Sahdeva, V. and Yu, H. and Loeliger, H. A.},
	date = {2016-08},
	keywords = {Estimation, Message passing, Kalman filters, message passing, Smoothing methods, Standards, backward information filter, Europe, extended Kalman smoother, forward marginal smoother, marginal message passing, modified Bryson-Frasier smoother, outlier-insensitive Kalman smoothing, smoothing methods, standard Kalman smoother, unknown variance},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/KZG2M3SI/7760447.html:text/html;Wadehn e.a. - 2016 - Outlier-insensitive Kalman smoothing and marginal .pdf:/Users/bert/Zotero/storage/D28T2B9S/Wadehn e.a. - 2016 - Outlier-insensitive Kalman smoothing and marginal .pdf:application/pdf},
}

@article{friston_active_2017,
	title = {Active Inference, artificial curiosity and insight},
	volume = {29},
	doi = {10.1162/NECO_a_00999},
	pages = {2633 -- 2683},
	journaltitle = {Neural Computation},
	author = {Friston, Karl and Lin, Marco and Frith, Christopher and Pezzulo, Giovanni and Hobson, J Allen and Ondobaka, Sasha},
	date = {2017},
	file = {Friston et al. - 2017 - Active Inference, artificial curiosity and insight.pdf:/Users/bert/Zotero/storage/PASJAJJ4/Friston et al. - 2017 - Active Inference, artificial curiosity and insight.pdf:application/pdf},
}

@book{koller_probabilistic_2009,
	location = {Cambridge, {MA}, {USA}},
	title = {Probabilistic graphical models: principles and techniques},
	isbn = {0-262-01319-3},
	publisher = {{MIT} press},
	author = {Koller, Daphne and Friedman, Nir},
	date = {2009},
}

@inproceedings{ranganath_black_2014,
	location = {Reykjavik, Iceland},
	title = {Black Box Variational Inference},
	url = {http://proceedings.mlr.press/v33/ranganath14.html},
	abstract = {Variational inference has become a widely used method to approximate posteriors in complex latent variables models. However, deriving a variational inference algorithm generally requires significa...},
	pages = {814--822},
	booktitle = {Proc. Int Conference on Artificial Intelligence and Statistics ({AISTATS})},
	author = {Ranganath, Rajesh and Gerrish, Sean and Blei, David},
	date = {2014-04},
	file = {Full Text PDF:/Users/bert/Zotero/storage/A6IDW6JF/Ranganath e.a. - 2014 - Black Box Variational Inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/95228JU2/ranganath14.html:text/html},
}

@article{friston_deep_2017,
	title = {Deep temporal models and active inference},
	volume = {77},
	doi = {https://doi.org/10.1016/j.neubiorev.2017.04.009},
	pages = {388 -- 402},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	author = {Friston, Karl and Rosch, Richard and Parr, Thomas and Price, Cathy and Bowman, Howard},
	date = {2017},
	file = {Friston et al. - 2017 - Deep temporal models and active inference.pdf:/Users/bert/Zotero/storage/VBIXHTLS/Friston et al. - 2017 - Deep temporal models and active inference.pdf:application/pdf},
}

@article{carpenter_stan:_2017,
	title = {Stan: A Probabilistic Programming Language},
	volume = {76},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v76/i01/},
	doi = {10.18637/jss.v076.i01},
	shorttitle = {\textit{Stan}},
	pages = {1--32},
	number = {1},
	journaltitle = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	urldate = {2017-06-07},
	date = {2017},
	file = {Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:/Users/bert/Zotero/storage/MVA8VJH2/Carpenter et al. - 2017 - Stan A Probabilistic Programming Language.pdf:application/pdf},
}

@inproceedings{vasudeva_raju_inference_2016,
	location = {Barcelona, Spain},
	title = {Inference by Reparameterization in Neural Population Codes},
	url = {http://papers.nips.cc/paper/6476-inference-by-reparameterization-in-neural-population-codes.pdf},
	pages = {2029--2037},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Vasudeva Raju, Rajkumar and Pitkow, Xaq},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2017-10-02},
	date = {2016},
	file = {NIPS Snapshort:/Users/bert/Zotero/storage/5YLN32QV/6476-inference-by-reparameterization-in-neural-population-codes.html:text/html;Vasudeva Raju and Pitkow - 2016 - Inference by Reparameterization in Neural Populati.pdf:/Users/bert/Zotero/storage/QSE9TYFW/Vasudeva Raju and Pitkow - 2016 - Inference by Reparameterization in Neural Populati.pdf:application/pdf},
}

@inproceedings{turner_modeling_2008,
	location = {Vancouver, {BC}, Canada},
	title = {Modeling natural sounds with modulation cascade processes},
	booktitle = {Advances in Neural Information Processing Systems ({NIPS})},
	author = {Turner, Richard and Sahani, Maneesh},
	date = {2008},
	file = {Turner and Sahani - 2008 - Modeling natural sounds with modulation cascade pr.pdf:/Users/bert/Zotero/storage/PIQSTC3Z/Turner and Sahani - 2008 - Modeling natural sounds with modulation cascade pr.pdf:application/pdf},
}

@inproceedings{ng_algorithms_2000,
	location = {Palo Alto, {CA}},
	title = {Algorithms for Inverse Reinforcement Learning},
	abstract = {This paper addresses the problem of inverse reinforcement learning ({IRL}) in Markov decision processes, that is, the problem of extracting a reward function given observed, optimal behaviour. {IRL} may be useful for apprenticeship learning to acquire skilled behaviour, and for ascertaining the reward function being optimized by a natural system. We rst characterize the set of all reward functions for which a given policy is optimal. We then derive three algorithms for {IRL}. The rst two deal with the case where the entire policy is known; we handle tabulated reward functions on a nite state space and linear functional approximation of the reward function over a potentially in- nite state space. The third algorithm deals with the more realistic case in which the policy is known only through a nite set of observed trajectories. In all cases, a key issue is degeneracy{\textbackslash}textbarthe existence of a large set of reward functions for which the observed policy is optimal. To remove...},
	pages = {663--670},
	booktitle = {in Proc. 17th International Conf. on Machine Learning},
	publisher = {Morgan Kaufmann},
	author = {Ng, Andrew Y. and Russell, Stuart},
	date = {2000},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/JSHJSISC/summary.html:text/html;Ng and Russell - 2000 - Algorithms for Inverse Reinforcement Learning.pdf:/Users/bert/Zotero/storage/BXGC6HZ5/Ng and Russell - 2000 - Algorithms for Inverse Reinforcement Learning.pdf:application/pdf},
}

@inproceedings{dauwels_expectation_2005,
	location = {Adelaide, Australia},
	title = {Expectation maximization as message passing},
	doi = {10.1109/ISIT.2005.1523402},
	abstract = {Based on prior work by Eckford, it is shown how expectation maximization ({EM}) may be viewed, and used, as a message passing algorithm in factor graphs},
	pages = {583--586},
	booktitle = {International Symposium on Information Theory},
	author = {Dauwels, J. and Korl, S. and Loeliger, H.-A.},
	date = {2005-09},
	keywords = {Message passing, Signal processing algorithms, Kalman filters, message passing, expectation-maximisation algorithm, factor graphs, Filtering algorithms, graph theory, graphical models, Convergence, expectation maximization, Information technology, message passing algorithm, modeling},
	file = {Dauwels et al. - 2005 - Expectation maximization as message passing.pdf:/Users/bert/Zotero/storage/IK9YXECJ/Dauwels et al. - 2005 - Expectation maximization as message passing.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/SGKQY3PX/login.html:text/html},
}

@inproceedings{dauwels_steepest_2005,
	location = {Awaji Island, Japan},
	title = {Steepest descent as message passing},
	doi = {10.1109/ITW.2005.1531853},
	abstract = {It is shown how steepest descent (or steepest ascent) may be viewed as a message passing algorithm with "local" message update rules. For example, the well-known backpropagation algorithm for the training of feedforward neural networks may be viewed as message passing on a factor graph. The factor graph approach with its emphasis on "local" computations makes it easy to combine steepest descent with other message passing algorithms such as the sum/max-product algorithms, expectation maximization, Kalman filtering/smoothing, and particle filters. As an example, parameter estimation in a state space model is considered. For this example, it is shown how steepest descent can be used for the maximization step in expectation maximization.},
	booktitle = {{IEEE} Information Theory Workshop, 2005.},
	author = {Dauwels, J. and Korl, S. and Loeliger, H. A.},
	date = {2005-08},
	keywords = {Message passing, Kalman filters, Neural networks, Kalman filtering, expectation-maximisation algorithm, Filtering algorithms, graph theory, sum product algorithm, expectation maximization, backpropagation algorithm, Backpropagation algorithms, factor graph, gradient methods, Kalman smoothing, max-product algorithms, state space model, steepest descent},
	file = {Dauwels et al. - 2005 - Steepest Descent as Message Passing.pdf:/Users/bert/Zotero/storage/E7WU324K/Dauwels et al. - 2005 - Steepest Descent as Message Passing.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/VTLR975K/1531853.html:text/html},
}

@inproceedings{dauwels_variational_2007,
	location = {Nice, France},
	title = {On Variational Message Passing on Factor Graphs},
	doi = {10.1109/ISIT.2007.4557602},
	abstract = {In this paper, it is shown how (naive and structured) variational algorithms may be derived from a factor graph by mechanically applying generic message computation rules; in this way, one can bypass error-prone variational calculus. In prior work by Bishop et al., Xing et al., and Geiger, directed and undirected graphical models have been used for this purpose. The factor graph notation amounts to simpler generic variational message computation rules; by means of factor graphs, variational methods can straightforwardly be compared to and combined with various other message-passing inference algorithms, e.g., Kalman filters and smoothers, iterated conditional modes, expectation maximization ({EM}), gradient methods, and particle filters. Some of those combinations have been explored in the literature, others seem to be new. Generic message computation rules for such combinations are formulated.},
	pages = {2546--2550},
	booktitle = {{IEEE} International Symposium on Information Theory},
	author = {Dauwels, J.},
	date = {2007-06},
	keywords = {Kalman filters, factor graphs, variational message passing},
	file = {Dauwels - 2007 - (long article) On Variational Message Passing on Factor Graphs.pdf:/Users/bert/Zotero/storage/A9QRYYX8/Dauwels - 2007 - (long article) On Variational Message Passing on Factor Graphs.pdf:application/pdf;Dauwels - 2007 - on variational message passing on factor graphs.pdf:/Users/bert/Zotero/storage/3JK2RWUG/Dauwels - 2007 - on variational message passing on factor graphs.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/7BNTMEFI/login.html:text/html},
}

@inproceedings{al-bashabsheh_normal_2011,
	location = {St. Petersburg, Russia},
	title = {Normal factor graphs: A diagrammatic approach to linear algebra},
	doi = {10.1109/ISIT.2011.6033944},
	shorttitle = {Normal factor graphs},
	abstract = {Inspired by some new advances on normal factor graphs ({NFGs}), we introduce {NFGs} as a simple and intuitive diagrammatic approach towards encoding some concepts from linear algebra. We illustrate with examples the workings of such an approach and settle a conjecture of Peterson on the Pfaffian.},
	pages = {2178--2182},
	booktitle = {2011 {IEEE} International Symposium on Information Theory Proceedings},
	author = {Al-Bashabsheh, A. and Mao, Y. and Vontobel, P. O.},
	date = {2011-07},
	keywords = {Graphical models, graph theory, Tensile stress, Communities, Compounds, diagrammatic approach, Information theory, linear algebra, Linear algebra, normal factor graph, Peterson conjecture, Pfaffian approach, Tin},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/E9TD4PH9/6033944.html:text/html},
}

@report{minka_divergence_2005,
	title = {Divergence Measures and Message Passing},
	url = {https://www.seas.harvard.edu/courses/cs281/papers/minka-divergence.pdf},
	abstract = {This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (‘exclusive ’ versus ‘inclusive’ Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals. 1},
	author = {Minka, Thomas},
	date = {2005},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/MKK8FQG5/summary.html:text/html;Minka - 2005 - Divergence Measures and Message Passing.pdf:/Users/bert/Zotero/storage/4PG5GMZT/Minka - 2005 - Divergence Measures and Message Passing.pdf:application/pdf},
}

@inproceedings{pearl_reverend_1982,
	location = {Pittsburgh, Pennsylvania},
	title = {Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach},
	url = {http://dl.acm.org/citation.cfm?id=2876686.2876719},
	shorttitle = {Reverend Bayes on Inference Engines},
	abstract = {This paper presents generalizations of Bayes likelihood-ratio updating rule which facilitate an asynchronous propagation of the impacts of new beliefs and/or new evidence in hierarchically organized inference structures with multi-hypotheses variables. The computational scheme proposed specifies a set of belief parameters, communication messages and updating rules which guarantee that the diffusion of updated beliefs is accomplished in a single pass and complies with the tenets of Bayes calculus.},
	pages = {133--136},
	booktitle = {Proceedings of the Second {AAAI} Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Pearl, Judea},
	urldate = {2017-07-28},
	date = {1982},
}

@article{kschischang_factor_2001,
	title = {Factor graphs and the sum-product algorithm},
	volume = {47},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=910572},
	pages = {498--519},
	number = {2},
	journaltitle = {{IEEE} Transactions on Information theory},
	author = {Kschischang, Frank R. and Frey, Brendan J. and Loeliger, H.-A.},
	urldate = {2014-04-10},
	date = {2001},
	file = {Kschischang et al. - 2001 - Factor graphs and the sum-product algorithm.pdf:/Users/bert/Zotero/storage/N6A529KA/Kschischang et al. - 2001 - Factor graphs and the sum-product algorithm.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/L5EZJLN8/login.html:text/html},
}

@article{harper_replicator_2009,
	title = {The Replicator Equation as an Inference Dynamic},
	abstract = {The replicator equation is interpreted as a continuous inference equation and a formal similarity between the discrete replicator equation and Bayesian inference is described. Further connections between inference and the replicator equation are given including a discussion of information divergences and exponential families as solutions for the replicator dynamic, using Fisher information and information geometry.},
	journaltitle = {{arXiv}:0911.1763 [cs, math]},
	author = {Harper, Marc},
	urldate = {2017-09-12},
	date = {2009-11-09},
	eprinttype = {arxiv},
	eprint = {0911.1763},
	keywords = {Mathematics - Dynamical Systems, Computer Science - Information Theory, 37N25, Secondary: 62F15},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/I4S53EAW/0911.html:text/html;Harper - 2009 - The Replicator Equation as an Inference Dynamic.pdf:/Users/bert/Zotero/storage/SPM5S3MK/Harper - 2009 - The Replicator Equation as an Inference Dynamic.pdf:application/pdf},
}

@thesis{zalmai_state_2017,
	title = {A State Space World for Detecting and Estimating Events and Learning Sparse Signal Decompositions},
	rights = {http://rightsstatements.org/page/{InC}-{NC}/1.0/},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/176652},
	abstract = {Many signals can be labeled with a small set of events such that each event is categorized according to its surrounding signal shapes. In this thesis, we provide a general approach based on linear state space models to learn sparse signal decompositions from single-channel and multi-channel discrete-time measurements. The proposed approach provides a sparse multi-channel representation of a given signal, which can be interpreted as a signal labeling. This thesis is organized in three parts. In the first part, several important properties of linear state space models ({LSSMs}) are revisited. Especially, signals generated with an autonomous {LSSM} are thoroughly investigated and fully characterized. In particular, we show that the set of such signals forms a ring and that the correlation function between any autonomous {LSSM} signal and any discrete-time signal can be recursively and efficiently computed. These two properties along with the vast modeling capabilities of {LSSM} signals are at the heart of this thesis.  In the second part, we develop a general approach to detect events in (single-channel or multi-channel) discrete-time signals and estimate the parameters of such events. Since the number of events is assumed to be substantially smaller than the number of samples, the set of detected events is interpreted as a sparse representation of the given signal. An event locally creates characteristic signals which are modeled with a two-sided autonomous {LSSM}; the right-sided model accounts for the signals observed after that event while the left-sided model accounts for the signals before that event. Thus, the problem of event detection and estimation is substituted by fitting at any given time a {LSSM} signal to observations. For this purpose, new cost functions are defined: a {LSSM}-weighted squared error cost and a {LSSM}-weighted polynomial cost. These cost functions have the attractive property of being recursively computed. In addition, closed-form solutions for several minimization problems are available. As far as event detection is concerned, several hypothesis tests with a suitable notion of local likelihood are promoted. Surprisingly, event detection in various conditions, such as in the presence of an unknown additive or multiplicative interference signal, can be naturally dealt with. Finally, various important practical applications are addressed in detail in order to exemplify the potential of the proposed approach for event detection and estimation.  In the third and last part, we propose a general approach to learn sparse signal decompositions. We assume that each signal component can be sparsely represented in the input domain of some unknown {LSSM}. We model sparse inputs with zero-mean Gaussian random variables with unknown variances, as in the sparse Bayesian learning framework. Then, all unknown parameters are estimated by maximum likelihood with an expectation maximization ({EM}) algorithm where all parameters are jointly updated with closed-form expressions and all expectation quantities are efficiently computed with a Gaussian message passing algorithm. This general approach can deal with a large variety of sparse signal decomposition problems. Among them, we address the problems of learning repetitive signal shapes, learning classes of signal shapes, and decomposing a signal with scaled, time-shifted, and time-dilated versions of a signal shape. All these concepts and methods are illustrated with practical examples. --{\textgreater} Many signals can be labeled with a small set of events such that each event is categorized according to its surrounding signal shapes. In this thesis, we provide a general approach based on linear state space models to learn sparse signal decompositions from single-channel and multi-channel discrete-time measurements. The proposed approach provides a sparse multi-channel representation of a given signal, which can be interpreted as a signal labeling. This thesis is organized in three parts.In the first part, several important properties of linear state space models ({LSSMs}) are revisited. Especially, signals generated with an autonomous {LSSM} are thoroughly investigated and fully characterized. In particular, we show that the set of such signals forms a ring and that the correlation function between any autonomous {LSSM} signal and any discrete-time signal can be recursively and efficiently computed. These two properties along with the vast modeling capabilities of {LSSM} signalsare at the heart of this thesis.In the second part, we develop a general approach to detect events in (single-channel or multi-channel) discrete-time signals and estimate the parameters of such events. Since the number of events is assumed to be substantially smaller than the number of samples, the set of detected events is interpreted as a sparse representation of the given signal. An event locally creates characteristic signals which are modeled with a two-sided autonomous {LSSM}; the right-sided model accounts for thesignals observed after that event while the left-sided model accounts for the signals before that event. Thus, the problem of event detection and estimation is substituted by fitting at any given time a {LSSM} signal to observations. For this purpose, new cost functions are defined: a {LSSM}-weighted squared error cost and a {LSSM}-weighted polynomial cost. These cost functions have the attractive property of being recursively computed. In addition, closed-form solutions for several minimization problems are available. As far as event detection is concerned, several hypothesis tests with a suitable notion of local likelihood are promoted. Surprisingly, event detection in various conditions, such as in the presence of an unknown additive or multiplicative interference signal, can be naturally dealt with. Finally, various important practical applications are addressed in detail in order to exemplify the potential of the proposed approach for event detection and estimation.In the third and last part, we propose a general approach to learn sparse signal decompositions. We assume that each signal component can be sparsely represented in the input domain of some unknown {LSSM}. We model sparse inputs with zero-mean Gaussian random variables with unknown variances, as in the sparse Bayesian learning framework. Then, all unknown parameters are estimated by maximum likelihood with an expectation maximization ({EM}) algorithm where all parameters are jointly updated with closed-form expressions and all expectation quantities are efficiently computed with a Gaussian message passing algorithm. This general approach can deal with a large variety of sparse signal decomposition problems. Among them, we address the problems of learning repetitive signal shapes, learning classes of signal shapes, and decomposing a signal with scaled, time-shifted, and time-dilated versions of a signal shape. All these concepts and methods are illustrated with practical examples.},
	institution = {{ETH} Zurich},
	type = {Doctoral Thesis},
	author = {Zalmai, Nour},
	urldate = {2017-09-27},
	date = {2017},
	langid = {english},
	doi = {10.3929/ethz-b-000176652},
	file = {Snapshot:/Users/bert/Zotero/storage/T9C2DUGC/176652.html:text/html;Zalmai - 2017 - A State Space World for Detecting and Estimating E.pdf:/Users/bert/Zotero/storage/HCLCWFT4/Zalmai - 2017 - A State Space World for Detecting and Estimating E.pdf:application/pdf},
}

@thesis{bruderer_lukas_input_2015,
	location = {Zurich, {CH}},
	title = {Input Estimation and Dynamical System Identification: New Algorithms and Results},
	url = {http://e-collection.library.ethz.ch/eserv/eth:48178/eth-48178-02.pdf},
	institution = {{ETH} Zurich},
	type = {phdthesis},
	author = {Bruderer, Lukas},
	date = {2015},
	file = {Bruderer, Lukas - 2015 - Input Estimation and Dynamical System Identificati.pdf:/Users/bert/Zotero/storage/P6B5FTCK/Bruderer, Lukas - 2015 - Input Estimation and Dynamical System Identificati.pdf:application/pdf},
}

@inproceedings{bruderer_deconvolution_2015,
	title = {Deconvolution of weakly-sparse signals and dynamical-system identification by Gaussian message passing},
	doi = {10.1109/ISIT.2015.7282470},
	abstract = {We use ideas from sparse Bayesian learning for estimating the (weakly) sparse input signal of a linear state space model. Variational representations of the sparsifying prior lead to algorithms that essentially amount to Gaussian message passing. The approach is extended to the case where the state space model is not known and must be estimated. Experimental results with a real-world application substantiate the applicability of the proposed method.},
	eventtitle = {2015 {IEEE} International Symposium on Information Theory ({ISIT})},
	pages = {326--330},
	booktitle = {2015 {IEEE} International Symposium on Information Theory ({ISIT})},
	author = {Bruderer, L. and Malmberg, H. and Loeliger, H. A.},
	date = {2015-06},
	keywords = {Dictionaries, Estimation, Message passing, Signal processing algorithms, Bayes methods, message passing, Computational modeling, learning (artificial intelligence), state-space methods, deconvolution, Deconvolution, dynamical system identification, Gaussian message passing, linear state space model, signal representation, sparse Bayesian learning, variational representation, weakly-sparse signal deconvolution},
	file = {Bruderer et al. - 2015 - Deconvolution of weakly-sparse signals and dynamic.pdf:/Users/bert/Zotero/storage/FN5KZ7UW/Bruderer et al. - 2015 - Deconvolution of weakly-sparse signals and dynamic.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/4NBK9DPX/7282470.html:text/html},
}

@article{hsu_unsupervised_2017,
	title = {Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data},
	url = {http://arxiv.org/abs/1709.07902},
	abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35\% in mismatched train/test scenarios for automatic speech recognition tasks.},
	journaltitle = {{arXiv}:1709.07902 [cs, eess, stat]},
	author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
	urldate = {2017-09-27},
	date = {2017-09-22},
	eprinttype = {arxiv},
	eprint = {1709.07902},
	keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Sound, Computer Science - Computation and Language, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/AI4WWIQC/1709.html:text/html;Hsu et al. - 2017 - Unsupervised Learning of Disentangled and Interpre.pdf:/Users/bert/Zotero/storage/S9IBXZFR/Hsu et al. - 2017 - Unsupervised Learning of Disentangled and Interpre.pdf:application/pdf},
}

@article{luo_bidirectional_2017,
	title = {Bidirectional Backpropagation: Towards Biologically Plausible Error Signal Transmission in Neural Networks},
	url = {http://arxiv.org/abs/1702.07097},
	shorttitle = {Bidirectional Backpropagation},
	abstract = {The back-propagation ({BP}) algorithm has been considered the de-facto method for training deep neural networks. It back-propagates errors from the output layer to the hidden layers in an exact manner using the transpose of the feedforward weights. However, it has been argued that this is not biologically plausible because back-propagating error signals with the exact incoming weights is not considered possible in biological neural systems. In this work, we propose a biologically plausible paradigm of neural architecture based on related literature in neuroscience and asymmetric {BP}-like methods. Specifically, we propose two bidirectional learning algorithms with trainable feedforward and feedback weights. The feedforward weights are used to relay activations from the inputs to target outputs. The feedback weights pass the error signals from the output layer to the hidden layers. Different from other asymmetric {BP}-like methods, the feedback weights are also plastic in our framework and are trained to approximate the forward activations. Preliminary results show that our models outperform other asymmetric {BP}-like methods on the {MNIST} and the {CIFAR}-10 datasets.},
	journaltitle = {{arXiv}:1702.07097 [cs]},
	author = {Luo, Hongyin and Fu, Jie and Glass, James},
	urldate = {2017-09-26},
	date = {2017-02-23},
	eprinttype = {arxiv},
	eprint = {1702.07097},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/623MLGIR/1702.html:text/html;Luo e.a. - 2017 - Bidirectional Backpropagation Towards Biologicall.pdf:/Users/bert/Zotero/storage/B74MQ5KQ/Luo e.a. - 2017 - Bidirectional Backpropagation Towards Biologicall.pdf:application/pdf},
}

@article{kalman_new_1960,
	title = {A New Approach to Linear Filtering and Prediction Problems},
	url = {http://www.cs.unc.edu/~welch/kalman/media/pdf/Kalman1960.pdf},
	pages = {35--45},
	number = {82},
	journaltitle = {Transactions of the {ASME} – Journal of Basic Engineering},
	author = {Kalman, {RE}},
	urldate = {2017-09-23},
	date = {1960},
	keywords = {1960, kalman, kalman-filter},
}

@article{nolan_accurate_2017,
	title = {Accurate logistic variational message passing: algebraic and numerical details},
	volume = {6},
	issn = {2049-1573},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sta4.139/abstract},
	doi = {10.1002/sta4.139},
	shorttitle = {Accurate logistic variational message passing},
	abstract = {We provide full algebraic and numerical details required for fitting accurate logistic likelihood regression-type models via variational message passing with factor graph fragments. Existing methodology of this type involves the Jaakkola–Jordan device, which is prone to poor accuracy. We examine two alternatives: the Saul–Jordan tilted bound device and conjugacy enforcement via multivariate normal prespecification of a key message. Both of these approaches appear in related literature. Our contributions facilitate immediate implementation within variational message passing schemes. Copyright © 2017 John Wiley \& Sons, Ltd.},
	pages = {102--112},
	number = {1},
	journaltitle = {Stat},
	author = {Nolan, Tui H. and Wand, Matt P.},
	urldate = {2017-08-05},
	date = {2017-01},
	keywords = {factor graph, approximate Bayesian inference, generalized additive models, generalized linear mixed models, mean field variational Bayes},
	file = {Nolan and Wand - 2017 - Accurate logistic variational message passing alg.pdf:/Users/bert/Zotero/storage/NPIILPUV/Nolan and Wand - 2017 - Accurate logistic variational message passing alg.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/5CXWGK9C/abstract.html:text/html},
}

@article{friston_functional_2011,
	title = {Functional and effective connectivity: a review},
	volume = {1},
	issn = {2158-0022},
	doi = {10.1089/brain.2011.0008},
	shorttitle = {Functional and effective connectivity},
	abstract = {Over the past 20 years, neuroimaging has become a predominant technique in systems neuroscience. One might envisage that over the next 20 years the neuroimaging of distributed processing and connectivity will play a major role in disclosing the brain's functional architecture and operational principles. The inception of this journal has been foreshadowed by an ever-increasing number of publications on functional connectivity, causal modeling, connectomics, and multivariate analyses of distributed patterns of brain responses. I accepted the invitation to write this review with great pleasure and hope to celebrate and critique the achievements to date, while addressing the challenges ahead.},
	pages = {13--36},
	number = {1},
	journaltitle = {Brain Connectivity},
	author = {Friston, Karl J.},
	date = {2011},
	pmid = {22432952},
	keywords = {Animals, Brain, Humans, Models, Neurological, Brain Mapping, Nerve Net, Neural Pathways},
	file = {Friston - 2011 - Functional and effective connectivity a review.pdf:/Users/bert/Zotero/storage/N7TRTISY/Friston - 2011 - Functional and effective connectivity a review.pdf:application/pdf},
}

@article{rao_predictive_1999,
	title = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
	volume = {2},
	rights = {© 1999 Nature Publishing Group},
	issn = {1097-6256},
	url = {http://www.nature.com/neuro/journal/v2/n1/abs/nn0199_79.html?foxtrotcallback=true},
	doi = {10.1038/4580},
	shorttitle = {Predictive coding in the visual cortex},
	abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
	pages = {79--87},
	number = {1},
	journaltitle = {Nature Neuroscience},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	date = {1999-01},
	file = {Snapshot:/Users/bert/Zotero/storage/J5GLW8IH/nn0199_79.html:text/html},
}

@article{kontoroupi_online_2017,
	title = {Online Bayesian model assessment using nonlinear filters},
	volume = {24},
	issn = {1545-2263},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/stc.1880/abstract},
	doi = {10.1002/stc.1880},
	abstract = {Model assessment is an integral part of many engineering applications, because any analytical or numerical mathematical model used for predictive purposes is only an approximation of the real system. The Bayesian approach to model assessment requires the calculation of the evidence of each candidate model considered given the available measured data, which is a nontrivial task, and it is usually attempted offline, e.g., by using a stochastic simulation scheme or some deterministic approximation. Very few authors, in general, and hardly any in the field of structural dynamics, have investigated online application of model assessment. The current work explores how Bayesian model assessment and an online identification scheme for joint state and parameter estimation, in particular the unscented Kalman filter, whose computational efficiency has been widely recognized, could be integrated into a single method. This hierarchical Bayesian modeling approach involves two inference levels, namely, model assessment and parameter estimation. There is the possibility of adding another level within the hierarchy for noise estimation. An illustrative example involving several hysteretic candidate models is presented to demonstrate the implementation of the proposed procedure in structural health monitoring applications. Copyright © 2016 John Wiley \& Sons, Ltd.},
	pages = {n/a--n/a},
	number = {3},
	journaltitle = {Structural Control and Health Monitoring},
	shortjournal = {Struct. Control Health Monit.},
	author = {Kontoroupi, Thaleia and Smyth, Andrew W.},
	urldate = {2017-08-16},
	date = {2017-03-01},
	keywords = {Bayesian model assessment, model uncertainty, nonlinear system identification, online model assessment, unscented Kalman filter},
	file = {Kontoroupi and Smyth - 2017 - Online Bayesian model assessment using nonlinear f.pdf:/Users/bert/Zotero/storage/TFHMS9JF/Kontoroupi and Smyth - 2017 - Online Bayesian model assessment using nonlinear f.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/93Q2VLDJ/abstract.html:text/html},
}

@article{powers_pavlovian_2017,
	title = {Pavlovian conditioning–induced hallucinations result from overweighting of perceptual priors},
	volume = {357},
	rights = {Copyright © 2017 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-{reuseThis} is an article distributed under the terms of the Science Journals Default License.},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/357/6351/596},
	doi = {10.1126/science.aan3458},
	abstract = {Neural mechanisms for hallucinations
Pairing a stimulus in one modality (vision) with a stimulus in another (sound) can lead to task-induced hallucinations in healthy individuals. After many trials, people eventually report perceiving a nonexistent stimulus contingent on the presence of the previously paired stimulus. Powers et al. investigated how different groups of volunteers and patients respond to this conditioning paradigm. They used behavior, neuroimaging, and computational modeling to dissect the effect of perceptual priors versus sensory evidence on such induced hallucinations. People who are more prone to hear voices were more susceptible to the induced auditory hallucinations. The network of brain regions that was active during the conditioned hallucinations resembled the network observed during clinical symptom capture in individuals who hallucinate while in a brain scanner.
Science, this issue p. 596
Some people hear voices that others do not, but only some of those people seek treatment. Using a Pavlovian learning task, we induced conditioned hallucinations in four groups of people who differed orthogonally in their voice-hearing and treatment-seeking statuses. People who hear voices were significantly more susceptible to the effect. Using functional neuroimaging and computational modeling of perception, we identified processes that differentiated voice-hearers from non–voice-hearers and treatment-seekers from non–treatment-seekers and characterized a brain circuit that mediated the conditioned hallucinations. These data demonstrate the profound and sometimes pathological impact of top-down cognitive processes on perception and may represent an objective means to discern people with a need for treatment from those without.
Perceptual beliefs, stimulus associations, and belief volatility drive task-induced hallucinations in voice-hearers.
Perceptual beliefs, stimulus associations, and belief volatility drive task-induced hallucinations in voice-hearers.},
	pages = {596--600},
	number = {6351},
	journaltitle = {Science},
	author = {Powers, A. R. and Mathys, C. and Corlett, P. R.},
	date = {2017-08-11},
	pmid = {28798131},
	file = {aan3458-Powers-SM.pdf:/Users/bert/Zotero/storage/I5E8CWFH/aan3458-Powers-SM.pdf:application/pdf;Powers e.a. - 2017 - Pavlovian conditioning–induced hallucinations resu.pdf:/Users/bert/Zotero/storage/55J45QC3/Powers e.a. - 2017 - Pavlovian conditioning–induced hallucinations resu.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/9TYD8H4Q/tab-pdf.html:text/html},
}

@inproceedings{wang_robust_2017,
	title = {Robust Probabilistic Modeling with Bayesian Data Reweighting},
	url = {http://proceedings.mlr.press/v70/wang17g.html},
	abstract = {Probabilistic models analyze data by relying on a set of assumptions. Data that exhibit deviations from these assumptions can undermine inference and prediction quality. Robust models offer protect...},
	eventtitle = {International Conference on Machine Learning},
	pages = {3646--3655},
	booktitle = {{PMLR}},
	author = {Wang, Yixin and Kucukelbir, Alp and Blei, David M.},
	date = {2017-07-17},
	file = {Snapshot:/Users/bert/Zotero/storage/7B7CQQ9R/wang17g.html:text/html;Wang et al. - 2017 - Robust Probabilistic Modeling with Bayesian Data R.pdf:/Users/bert/Zotero/storage/JYYF4IW6/Wang et al. - 2017 - Robust Probabilistic Modeling with Bayesian Data R.pdf:application/pdf;wang17g-supp.pdf:/Users/bert/Zotero/storage/ZB4DEAUF/wang17g-supp.pdf:application/pdf},
}

@article{park_bayesian_2017,
	title = {Bayesian Efficient Coding},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {http://www.biorxiv.org/content/early/2017/08/22/178418},
	doi = {10.1101/178418},
	abstract = {The efficient coding hypothesis, which proposes that neurons are optimized to maximize information about the environment, has provided a guiding theoretical framework for sensory and systems neuroscience. More recently, a theory known as the Bayesian Brain hypothesis has focused on the brain's ability to integrate sensory and prior sources of information in order to perform Bayesian inference. However, there is as yet no comprehensive theory connecting these two theoretical frameworks. Here we bridge this gap by formalizing a Bayesian theory of efficient coding. We define Bayesian efficient codes in terms of four basic ingredients: (1) a stimulus prior distribution; (2) an encoding model; (3) a capacity constraint, specifying a neural resource limit; and (4) a loss function, quantifying the desirability or undesirability of various posterior distributions. Classic efficient codes can be seen as a special case in which the loss function is the posterior entropy, leading to a code that maximizes mutual information, but alternate loss functions give solutions that differ dramatically from information-maximizing codes. In particular, we show that decorrelation of sensory inputs, which is optimal under classic efficient codes in low-noise settings, can be disadvantageous for loss functions that penalize large errors. Bayesian efficient coding therefore enlarges the family of normatively optimal codes and provides a more general framework for understanding the design principles of sensory systems. We examine Bayesian efficient codes for linear receptive fields and nonlinear input-output functions, and show that our theory invites reinterpretation of Laughlin's seminal analysis of efficient coding in the blowfly visual system.},
	pages = {178418},
	journaltitle = {{bioRxiv}},
	author = {Park, Il Memming and Pillow, Jonathan W.},
	urldate = {2017-08-30},
	date = {2017-08-22},
	file = {Park and Pillow - 2017 - Bayesian Efficient Coding.pdf:/Users/bert/Zotero/storage/QU4KWXBX/Park and Pillow - 2017 - Bayesian Efficient Coding.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4A2Z3KU2/178418.html:text/html},
}

@article{hinton_generative_1997,
	title = {Generative models for discovering sparse distributed representations},
	volume = {352},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/content/352/1358/1177},
	doi = {10.1098/rstb.1997.0101},
	abstract = {We describe a hierarchical, generative model that can be viewed as a nonlinear generalization of factor analysis and can be implemented in a neural network. The model uses bottom–up, top–down and lateral connections to perform Bayesian perceptual inference correctly. Once perceptual inference has been performed the connection strengths can be updated using a very simple learning rule that only requires locally available information. We demonstrate that the network learns to extract sparse, distributed, hierarchical representations.},
	pages = {1177--1190},
	number = {1358},
	journaltitle = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
	author = {Hinton, Geoffrey E. and Ghahramani, Zoubin},
	urldate = {2017-08-30},
	date = {1997-08-29},
	pmid = {9304685},
	file = {Hinton and Ghahramani - 1997 - Generative models for discovering sparse distribut.pdf:/Users/bert/Zotero/storage/NA95FND9/Hinton and Ghahramani - 1997 - Generative models for discovering sparse distribut.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/9QBH26RR/1177.html:text/html},
}

@article{adams_predictions_2013,
	title = {Predictions not commands: active inference in the motor system},
	volume = {218},
	issn = {1863-2661},
	doi = {10.1007/s00429-012-0475-5},
	shorttitle = {Predictions not commands},
	abstract = {The descending projections from motor cortex share many features with top-down or backward connections in visual cortex; for example, corticospinal projections originate in infragranular layers, are highly divergent and (along with descending cortico-cortical projections) target cells expressing {NMDA} receptors. This is somewhat paradoxical because backward modulatory characteristics would not be expected of driving motor command signals. We resolve this apparent paradox using a functional characterisation of the motor system based on Helmholtz's ideas about perception; namely, that perception is inference on the causes of visual sensations. We explain behaviour in terms of inference on the causes of proprioceptive sensations. This explanation appeals to active inference, in which higher cortical levels send descending proprioceptive predictions, rather than motor commands. This process mirrors perceptual inference in sensory cortex, where descending connections convey predictions, while ascending connections convey prediction errors. The anatomical substrate of this recurrent message passing is a hierarchical system consisting of functionally asymmetric driving (ascending) and modulatory (descending) connections: an arrangement that we show is almost exactly recapitulated in the motor system, in terms of its laminar, topographic and physiological characteristics. This perspective casts classical motor reflexes as minimising prediction errors and may provide a principled explanation for why motor cortex is agranular.},
	pages = {611--643},
	number = {3},
	journaltitle = {Brain Structure \& Function},
	shortjournal = {Brain Struct Funct},
	author = {Adams, Rick A. and Shipp, Stewart and Friston, Karl J.},
	date = {2013-05},
	pmid = {23129312},
	pmcid = {PMC3637647},
	keywords = {Animals, Humans, Models, Neurological, Cerebral Cortex, Neurons, Brain Mapping, Visual Perception, Nerve Net, Neural Pathways, Motor Activity, Predictive Value of Tests, Reflex},
	file = {Adams et al. - 2013 - Predictions not commands active inference in the .pdf:/Users/bert/Zotero/storage/B7PMTL3F/Adams et al. - 2013 - Predictions not commands active inference in the .pdf:application/pdf},
}

@article{wan_distributed_2017,
	title = {Distributed simultaneous localization and mapping for mobile robot networks via hybrid dynamic belief propagation},
	volume = {13},
	issn = {1550-1477},
	url = {http://dx.doi.org/10.1177/1550147717726715},
	doi = {10.1177/1550147717726715},
	abstract = {This article proposes a hybrid dynamic belief propagation for simultaneous localization and mapping in the mobile robot network. The positions of landmarks and the poses of moving robots at each time slot are estimated simultaneously in an online and distributed manner, by fusing the odometry data of each robot and the measurements of robot–robot or robot–landmark relative distance and angle. The joint belief state of all robots and landmarks is encoded by a factor graph and the marginal posterior probability distribution of each variable is inferred by belief propagation. We show how to calculate, broadcast, and update messages between neighboring nodes in the factor graph. Specifically, we combine parametric and nonparametric techniques to tackle the problem arisen from non-Gaussian distributions and nonlinear models. Simulation and experimental results on publicly available dataset show the validity of our algorithm.},
	pages = {1550147717726715},
	number = {8},
	journaltitle = {International Journal of Distributed Sensor Networks},
	shortjournal = {International Journal of Distributed Sensor Networks},
	author = {Wan, Jiuqing and Bu, Shaocong and Yu, Jinsong and Zhong, Liping},
	urldate = {2017-09-02},
	date = {2017-08-01},
	file = {Wan et al. - 2017 - Distributed simultaneous localization and mapping .pdf:/Users/bert/Zotero/storage/MSZA4VHW/Wan et al. - 2017 - Distributed simultaneous localization and mapping .pdf:application/pdf},
}

@article{kanai_cerebral_2015,
	title = {Cerebral hierarchies: predictive processing, precision and the pulvinar},
	volume = {370},
	rights = {. © 2015 The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {0962-8436, 1471-2970},
	url = {http://rstb.royalsocietypublishing.org/content/370/1668/20140169},
	doi = {10.1098/rstb.2014.0169},
	shorttitle = {Cerebral hierarchies},
	abstract = {This paper considers neuronal architectures from a computational perspective and asks what aspects of neuroanatomy and neurophysiology can be disclosed by the nature of neuronal computations? In particular, we extend current formulations of the brain as an organ of inference—based upon hierarchical predictive coding—and consider how these inferences are orchestrated. In other words, what would the brain require to dynamically coordinate and contextualize its message passing to optimize its computational goals? The answer that emerges rests on the delicate (modulatory) gain control of neuronal populations that select and coordinate (prediction error) signals that ascend cortical hierarchies. This is important because it speaks to a hierarchical anatomy of extrinsic (between region) connections that form two distinct classes, namely a class of driving (first-order) connections that are concerned with encoding the content of neuronal representations and a class of modulatory (second-order) connections that establish context—in the form of the salience or precision ascribed to content. We explore the implications of this distinction from a formal perspective (using simulations of feature–ground segregation) and consider the neurobiological substrates of the ensuing precision-engineered dynamics, with a special focus on the pulvinar and attention.},
	pages = {20140169},
	number = {1668},
	journaltitle = {Philosophical Transactions of the Royal Society of London B: Biological Sciences},
	author = {Kanai, Ryota and Komura, Yutaka and Shipp, Stewart and Friston, Karl},
	urldate = {2015-12-29},
	date = {2015-05},
	pmid = {25823866},
	file = {Kanai et al. - 2015 - Cerebral hierarchies predictive processing, preci.pdf:/Users/bert/Zotero/storage/ZURK2D2N/Kanai et al. - 2015 - Cerebral hierarchies predictive processing, preci.pdf:application/pdf;Kanai et al. - 2015 - Cerebral hierarchies predictive processing, preci.pdf:/Users/bert/Zotero/storage/5MC6M2AV/Kanai et al. - 2015 - Cerebral hierarchies predictive processing, preci.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/Q9LHTWZ3/20140169.html:text/html;Snapshot:/Users/bert/Zotero/storage/7AR75XJD/20140169.html:text/html},
}

@article{salvatier_probabilistic_2016,
	title = {Probabilistic programming in Python using {PyMC}3},
	volume = {2},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-55},
	doi = {10.7717/peerj-cs.55},
	pages = {e55},
	journaltitle = {{PeerJ} Computer Science},
	author = {Salvatier, John and Wiecki, Thomas V. and Fonnesbeck, Christopher},
	urldate = {2017-06-07},
	date = {2016-04},
	file = {Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf:/Users/bert/Zotero/storage/EC9I8V7V/Salvatier et al. - 2016 - Probabilistic programming in Python using PyMC3.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/IJRJTF5E/cs-55.html:text/html},
}

@article{kiraly_modelling_2017,
	title = {Modelling Competitive Sports: Bradley-Terry-{\textbackslash}'\{E\}l{\textbackslash}H\{o\} Models for Supervised and On-Line Learning of Paired Competition Outcomes},
	url = {http://arxiv.org/abs/1701.08055},
	shorttitle = {Modelling Competitive Sports},
	abstract = {Prediction and modelling of competitive sports outcomes has received much recent attention, especially from the Bayesian statistics and machine learning communities. In the real world setting of outcome prediction, the seminal {\textbackslash}'\{E\}l{\textbackslash}H\{o\} update still remains, after more than 50 years, a valuable baseline which is difficult to improve upon, though in its original form it is a heuristic and not a proper statistical "model". Mathematically, the {\textbackslash}'\{E\}l{\textbackslash}H\{o\} rating system is very closely related to the Bradley-Terry models, which are usually used in an explanatory fashion rather than in a predictive supervised or on-line learning setting. Exploiting this close link between these two model classes and some newly observed similarities, we propose a new supervised learning framework with close similarities to logistic regression, low-rank matrix completion and neural networks. Building on it, we formulate a class of structured log-odds models, unifying the desirable properties found in the above: supervised probabilistic prediction of scores and wins/draws/losses, batch/epoch and on-line learning, as well as the possibility to incorporate features in the prediction, without having to sacrifice simplicity, parsimony of the Bradley-Terry models, or computational efficiency of {\textbackslash}'\{E\}l{\textbackslash}H\{o\}'s original approach. We validate the structured log-odds modelling approach in synthetic experiments and English Premier League outcomes, where the added expressivity yields the best predictions reported in the state-of-art, close to the quality of contemporary betting odds.},
	journaltitle = {{arXiv}:1701.08055 [cs, stat]},
	author = {Király, Franz J. and Qian, Zhaozhi},
	urldate = {2017-09-21},
	date = {2017-01-27},
	eprinttype = {arxiv},
	eprint = {1701.08055},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Computer Science - Learning, Statistics - Applications},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3EHWX43I/1701.html:text/html;Király en Qian - 2017 - Modelling Competitive Sports Bradley-Terry-' E l.pdf:/Users/bert/Zotero/storage/KTT6AD8Z/Király en Qian - 2017 - Modelling Competitive Sports Bradley-Terry-' E l.pdf:application/pdf},
}

@article{zhou_racing_2017,
	title = {Racing Thompson: an Efficient Algorithm for Thompson Sampling with Non-conjugate Priors},
	url = {http://arxiv.org/abs/1708.04781},
	shorttitle = {Racing Thompson},
	abstract = {Thompson sampling has impressive empirical performance for many multi-armed bandit problems. But current algorithms for Thompson sampling only work for the case of conjugate priors since these algorithms require to infer the posterior, which is often computationally intractable when the prior is not conjugate. In this paper, we propose a novel algorithm for Thompson sampling which only requires to draw samples from a tractable distribution, so our algorithm is efficient even when the prior is non-conjugate. To do this, we reformulate Thompson sampling as an optimization problem via the Gumbel-Max trick. After that we construct a set of random variables and our goal is to identify the one with highest mean. Finally, we solve it with techniques in best arm identification.},
	journaltitle = {{arXiv}:1708.04781 [cs, stat]},
	author = {Zhou, Yichi and Zhu, Jun and Zhuo, Jingwei},
	urldate = {2017-09-21},
	date = {2017-08-16},
	eprinttype = {arxiv},
	eprint = {1708.04781},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/T3BKF7SI/1708.html:text/html;Zhou e.a. - 2017 - Racing Thompson an Efficient Algorithm for Thomps.pdf:/Users/bert/Zotero/storage/U9NCIYFE/Zhou e.a. - 2017 - Racing Thompson an Efficient Algorithm for Thomps.pdf:application/pdf},
}

@article{zhu_big_2014,
	title = {Big Learning with Bayesian Methods},
	url = {http://arxiv.org/abs/1411.6370},
	abstract = {Explosive growth in data and availability of cheap computing resources have sparked increasing interest in Big learning, an emerging subfield that studies scalable machine learning algorithms, systems, and applications with Big Data. Bayesian methods represent one important class of statistic methods for machine learning, with substantial recent developments on adaptive, flexible and scalable Bayesian learning. This article provides a survey of the recent advances in Big learning with Bayesian methods, termed Big Bayesian Learning, including nonparametric Bayesian methods for adaptively inferring model complexity, regularized Bayesian inference for improving the flexibility via posterior regularization, and scalable algorithms and systems based on stochastic subsampling and distributed computing for dealing with large-scale applications.},
	journaltitle = {{arXiv}:1411.6370 [cs, stat]},
	author = {Zhu, Jun and Chen, Jianfei and Hu, Wenbo and Zhang, Bo},
	urldate = {2017-09-21},
	date = {2014-11-24},
	eprinttype = {arxiv},
	eprint = {1411.6370},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology, G.3, Computer Science - Learning, Statistics - Applications, F.1.2},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/8ZKSNLKK/1411.html:text/html;Zhu e.a. - 2014 - Big Learning with Bayesian Methods.pdf:/Users/bert/Zotero/storage/8NHEYHGP/Zhu e.a. - 2014 - Big Learning with Bayesian Methods.pdf:application/pdf},
}

@article{shi_zhusuan:_2017,
	title = {{ZhuSuan}: A Library for Bayesian Deep Learning},
	url = {http://arxiv.org/abs/1709.05870},
	shorttitle = {{ZhuSuan}},
	abstract = {In this paper we introduce {ZhuSuan}, a python probabilistic programming library for Bayesian deep learning, which conjoins the complimentary advantages of Bayesian methods and deep learning. {ZhuSuan} is built upon Tensorflow. Unlike existing deep learning libraries, which are mainly designed for deterministic neural networks and supervised tasks, {ZhuSuan} is featured for its deep root into Bayesian inference, thus supporting various kinds of probabilistic models, including both the traditional hierarchical Bayesian models and recent deep generative models. We use running examples to illustrate the probabilistic programming on {ZhuSuan}, including Bayesian logistic regression, variational auto-encoders, deep sigmoid belief networks and Bayesian recurrent neural networks.},
	journaltitle = {{arXiv}:1709.05870 [cs, stat]},
	author = {Shi, Jiaxin and Chen, Jianfei and Zhu, Jun and Sun, Shengyang and Luo, Yucen and Gu, Yihong and Zhou, Yuhao},
	urldate = {2017-09-21},
	date = {2017-09-18},
	eprinttype = {arxiv},
	eprint = {1709.05870},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/KEW49A34/1709.html:text/html;Shi e.a. - 2017 - ZhuSuan A Library for Bayesian Deep Learning.pdf:/Users/bert/Zotero/storage/FMV8KBGY/Shi e.a. - 2017 - ZhuSuan A Library for Bayesian Deep Learning.pdf:application/pdf},
}

@inproceedings{kuleshov_deep_2017,
	location = {Sydney},
	title = {Deep Hybrid Models: Bridging Discriminative and Generative Approaches},
	url = {http://auai.org/uai2017/proceedings/papers/297.pdf},
	eventtitle = {Uncertainty in Artificial Intelligence ({UAI})},
	author = {Kuleshov, Volodymyr and Ermon, Stefano},
	urldate = {2017-09-20},
	date = {2017},
	file = {Kuleshov and Ermon - 2017 - Deep Hybrid Models Bridging Discriminative and Ge.pdf:/Users/bert/Zotero/storage/ED4INC52/Kuleshov and Ermon - 2017 - Deep Hybrid Models Bridging Discriminative and Ge.pdf:application/pdf},
}

@inproceedings{qian_speech_2017,
	title = {Speech Enhancement Using Bayesian Wavenet},
	eventtitle = {Interspeech},
	author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Florêncio, Dinei and Hasegawa-Johnson, Mark},
	date = {2017},
	file = {Qian e.a. - 2017 - Speech Enhancement Using Bayesian Wavenet.pdf:/Users/bert/Zotero/storage/F243HKMV/Qian e.a. - 2017 - Speech Enhancement Using Bayesian Wavenet.pdf:application/pdf},
}

@article{baltieri_active_2017,
	title = {An active inference implementation of phototaxis},
	url = {http://arxiv.org/abs/1707.01806},
	abstract = {Active inference is emerging as a possible unifying theory of perception and action in cognitive and computational neuroscience. On this theory, perception is a process of inferring the causes of sensory data by minimising the error between actual sensations and those predicted by an inner {\textbackslash}emph\{generative\} (probabilistic) model. Action on the other hand is drawn as a process that modifies the world such that the consequent sensory input meets expectations encoded in the same internal model. These two processes, inferring properties of the world and inferring actions needed to meet expectations, close the sensory/motor loop and suggest a deep symmetry between action and perception. In this work we present a simple agent-based model inspired by this new theory that offers insights on some of its central ideas. Previous implementations of active inference have typically examined a "perception-oriented" view of this theory, assuming that agents are endowed with a detailed generative model of their surrounding environment. In contrast, we present an "action-oriented" solution showing how adaptive behaviour can emerge even when agents operate with a simple model which bears little resemblance to their environment. We examine how various parameters of this formulation allow phototaxis and present an example of a different, "pathological" behaviour.},
	journaltitle = {{arXiv}:1707.01806 [q-bio]},
	author = {Baltieri, Manuel and Buckley, Christopher L.},
	urldate = {2017-08-06},
	date = {2017-07-06},
	eprinttype = {arxiv},
	eprint = {1707.01806},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/T5GCYG5U/1707.html:text/html;Baltieri and Buckley - 2017 - An active inference implementation of phototaxis.pdf:/Users/bert/Zotero/storage/AA8ZDU6U/Baltieri and Buckley - 2017 - An active inference implementation of phototaxis.pdf:application/pdf},
}

@article{duan_rl$^2$:_2016,
	title = {{RL}\${\textasciicircum}2\$: Fast Reinforcement Learning via Slow Reinforcement Learning},
	url = {http://arxiv.org/abs/1611.02779},
	shorttitle = {{RL}\${\textasciicircum}2\$},
	abstract = {Deep reinforcement learning (deep {RL}) has been successful in learning sophisticated behaviors automatically; however, the learning process requires a huge number of trials. In contrast, animals can learn new tasks in just a few trials, benefiting from their prior knowledge about the world. This paper seeks to bridge this gap. Rather than designing a "fast" reinforcement learning algorithm, we propose to represent it as a recurrent neural network ({RNN}) and learn it from data. In our proposed method, {RL}\${\textasciicircum}2\$, the algorithm is encoded in the weights of the {RNN}, which are learned slowly through a general-purpose ("slow") {RL} algorithm. The {RNN} receives all information a typical {RL} algorithm would receive, including observations, actions, rewards, and termination flags; and it retains its state across episodes in a given Markov Decision Process ({MDP}). The activations of the {RNN} store the state of the "fast" {RL} algorithm on the current (previously unseen) {MDP}. We evaluate {RL}\${\textasciicircum}2\$ experimentally on both small-scale and large-scale problems. On the small-scale side, we train it to solve randomly generated multi-arm bandit problems and finite {MDPs}. After {RL}\${\textasciicircum}2\$ is trained, its performance on new {MDPs} is close to human-designed algorithms with optimality guarantees. On the large-scale side, we test {RL}\${\textasciicircum}2\$ on a vision-based navigation task and show that it scales up to high-dimensional problems.},
	journaltitle = {{arXiv}:1611.02779 [cs, stat]},
	author = {Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L. and Sutskever, Ilya and Abbeel, Pieter},
	urldate = {2017-09-12},
	date = {2016-11-08},
	eprinttype = {arxiv},
	eprint = {1611.02779},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/K4M9EC5U/1611.html:text/html;Duan e.a. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow Reinf.pdf:/Users/bert/Zotero/storage/SZHAVBYE/Duan e.a. - 2016 - RL\$^2\$ Fast Reinforcement Learning via Slow Reinf.pdf:application/pdf},
}

@article{fraccaro_deep_nodate,
	title = {Deep Latent Variable Models for Sequential Data},
	pages = {146},
	author = {Fraccaro, Marco},
	langid = {english},
	file = {Fraccaro - Deep Latent Variable Models for Sequential Data.pdf:/Users/bert/Zotero/storage/BNV8HVRU/Fraccaro - Deep Latent Variable Models for Sequential Data.pdf:application/pdf},
}

@article{yuille_belief_nodate,
	title = {Belief Propagation, Mean-ﬁeld, and Bethe approximations},
	pages = {28},
	author = {Yuille, Alan},
	langid = {english},
	file = {Yuille - Belief Propagation, Mean-ﬁeld, and Bethe approxima.pdf:/Users/bert/Zotero/storage/H7EFZGFP/Yuille - Belief Propagation, Mean-ﬁeld, and Bethe approxima.pdf:application/pdf},
}

@article{norilo_kronos:_2015,
	title = {Kronos: A Declarative Metaprogramming Language for Digital Signal Processing},
	volume = {39},
	issn = {0148-9267},
	url = {https://doi.org/10.1162/COMJ_a_00330},
	doi = {10.1162/COMJ_a_00330},
	shorttitle = {Kronos},
	abstract = {Kronos is a signal-processing programming language based on the principles of semifunctional reactive systems. It is aimed at efficient signal processing at the elementary level, and built to scale towards higher-level tasks by utilizing the powerful programming paradigms of “metaprogramming” and reactive multirate systems. The Kronos language features expressive source code as well as a streamlined, efficient runtime. The programming model presented is adaptable for both sample-stream and event processing, offering a cleanly functional programming paradigm for a wide range of musical signal-processing problems, exemplified herein by a selection and discussion of code examples.},
	pages = {30--48},
	number = {4},
	journaltitle = {Computer Music Journal},
	shortjournal = {Computer Music Journal},
	author = {Norilo, Vesa},
	urldate = {2018-11-27},
	date = {2015-12-01},
	file = {Norilo - 2015 - Kronos A Declarative Metaprogramming Language for.pdf:/Users/bert/Zotero/storage/ZABWBKIX/Norilo - 2015 - Kronos A Declarative Metaprogramming Language for.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/DUEVXVKN/COMJ_a_00330.html:text/html},
}

@article{benrimoh_active_2018,
	title = {Active Inference and Auditory Hallucinations},
	url = {https://doi.org/10.1162/cpsy_a_00022},
	doi = {10.1162/cpsy_a_00022},
	abstract = {Auditory verbal hallucinations ({AVH}) are often distressing symptoms of several neuropsychiatric conditions, including schizophrenia. Using a Markov decision process formulation of active inference, we develop a novel model of {AVH} as false (positive) inference. Active inference treats perception as a process of hypothesis testing, in which sensory data are used to disambiguate between alternative hypotheses about the world. Crucially, this depends upon a delicate balance between prior beliefs about unobserved (hidden) variables and the sensations they cause. A false inference that a voice is present, even in the absence of auditory sensations, suggests that prior beliefs dominate perceptual inference. Here we consider the computational mechanisms that could cause this imbalance in perception. Through simulation, we show that the content of (and confidence in) prior beliefs depends on beliefs about policies (here sequences of listening and talking) and on beliefs about the reliability of sensory data. We demonstrate several ways in which hallucinatory percepts could occur when an agent expects to hear a voice in the presence of imprecise sensory data. This model expresses, in formal terms, alternative computational mechanisms that underwrite {AVH} and, speculatively, can be mapped onto neurobiological changes associated with schizophrenia. The interaction of action and perception is important in modeling {AVH}, given that speech is a fundamentally enactive and interactive process—and that hallucinators often actively engage with their voices.},
	pages = {1--22},
	journaltitle = {Computational Psychiatry},
	shortjournal = {Computational Psychiatry},
	author = {Benrimoh, David and Parr, Thomas and Vincent, Peter and Adams, Rick A. and Friston, Karl},
	urldate = {2018-11-24},
	date = {2018-11-20},
	file = {Benrimoh et al. - 2018 - Active Inference and Auditory Hallucinations.pdf:/Users/bert/Zotero/storage/7UW3NH72/Benrimoh et al. - 2018 - Active Inference and Auditory Hallucinations.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4WVR7GL3/cpsy_a_00022.html:text/html},
}

@article{leike_scalable_2018,
	title = {Scalable agent alignment via reward modeling: a research direction},
	url = {https://arxiv.org/abs/1811.07871},
	shorttitle = {Scalable agent alignment via reward modeling},
	author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
	urldate = {2018-11-21},
	date = {2018-11-19},
	langid = {english},
	file = {Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf:/Users/bert/Zotero/storage/LTT3BNHF/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/QP6RTAWG/1811.html:text/html},
}

@inproceedings{corduneanu_variational_2001,
	title = {Variational Bayesian Model Selection for Mixture Distributions},
	eventtitle = {{AI}-{STATS}},
	pages = {8},
	booktitle = {{AI}-{STATS}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Corduneanu, Adrian and Bishop, Christopher M},
	date = {2001},
	langid = {english},
	file = {Corduneanu and Bishop - 2001 - Variational Bayesian Model Selection for Mixture D.pdf:/Users/bert/Zotero/storage/DFYG5U3E/Corduneanu and Bishop - 2001 - Variational Bayesian Model Selection for Mixture D.pdf:application/pdf},
}

@inproceedings{el-hay_continuous-time_2010,
	location = {{USA}},
	title = {Continuous-time Belief Propagation},
	isbn = {978-1-60558-907-7},
	url = {http://dl.acm.org/citation.cfm?id=3104322.3104367},
	series = {{ICML}'10},
	abstract = {Many temporal processes can be naturally modeled as a stochastic system that evolves continuously over time. The representation language of continuous-time Bayesian networks allows to succinctly describe multi-component continuous-time stochastic processes. A crucial element in applications of such models is inference. Here we introduce a variational approximation scheme, which is a natural extension of Belief Propagation for continuous-time processes. In this scheme, we view messages as inhomogeneous Markov processes over individual components. This leads to a relatively simple procedure that allows to easily incorporate adaptive ordinary differential equation ({ODE}) solvers to perform individual steps. We provide the theoretical foundations for the approximation, and show how it performs on a range of networks. Our results demonstrate that our method is quite accurate on singly connected networks, and provides close approximations in more complex ones.},
	pages = {343--350},
	booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	publisher = {Omnipress},
	author = {El-Hay, Tal and Cohn, Ido and Friedman, Nir and Kupferman, Raz},
	urldate = {2018-11-13},
	date = {2010},
	file = {El-Hay et al. - 2010 - Continuous-time Belief Propagation.pdf:/Users/bert/Zotero/storage/RW95J463/El-Hay et al. - 2010 - Continuous-time Belief Propagation.pdf:application/pdf},
}

@article{parr_attention_2019,
	title = {Attention or salience?},
	volume = {29},
	issn = {2352-250X},
	url = {http://www.sciencedirect.com/science/article/pii/S2352250X18301593},
	doi = {10.1016/j.copsyc.2018.10.006},
	series = {Attention \& Perception},
	abstract = {While attention is widely recognised as central to perception, the term is often used to mean very different things. Prominent theories of attention — notably the premotor theory — relate it to planned or executed eye movements. This contrasts with the notion of attention as a gain control process that weights the information carried by different sensory channels. We draw upon recent advances in theoretical neurobiology to argue for a distinction between attentional gain mechanisms and salience attribution. The former depends upon estimating the precision of sensory data, while the latter is a consequence of the need to actively engage with the sensorium. Having established this distinction, we consider the intimate relationship between attention and salience.},
	pages = {1--5},
	journaltitle = {Current Opinion in Psychology},
	shortjournal = {Current Opinion in Psychology},
	author = {Parr, Thomas and Friston, Karl J},
	urldate = {2018-11-13},
	date = {2019-10-01},
	file = {Parr and Friston - 2019 - Attention or salience.pdf:/Users/bert/Zotero/storage/6BP47NWI/Parr and Friston - 2019 - Attention or salience.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/MJ9ED8KT/S2352250X18301593.html:text/html},
}

@article{rahmati_developmental_2017,
	title = {Developmental Emergence of Sparse Coding: A Dynamic Systems Approach},
	volume = {7},
	rights = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-13468-z},
	doi = {10.1038/s41598-017-13468-z},
	shorttitle = {Developmental Emergence of Sparse Coding},
	abstract = {During neocortical development, network activity undergoes a dramatic transition from largely synchronized, so-called cluster activity, to a relatively sparse pattern around the time of eye-opening in rodents. Biophysical mechanisms underlying this sparsification phenomenon remain poorly understood. Here, we present a dynamic systems modeling study of a developing neural network that provides the first mechanistic insights into sparsification. We find that the rest state of immature networks is strongly affected by the dynamics of a transient, unstable state hidden in their firing activities, allowing these networks to either be silent or generate large cluster activity. We address how, and which, specific developmental changes in neuronal and synaptic parameters drive sparsification. We also reveal how these changes refine the information processing capabilities of an in vivo developing network, mainly by showing a developmental reduction in the instability of network’s firing activity, an effective availability of inhibition-stabilized states, and an emergence of spontaneous attractors and state transition mechanisms. Furthermore, we demonstrate the key role of {GABAergic} transmission and depressing glutamatergic synapses in governing the spatiotemporal evolution of cluster activity. These results, by providing a strong link between experimental observations and model behavior, suggest how adult sparse coding networks may emerge developmentally.},
	pages = {13015},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Rahmati, Vahid and Kirmse, Knut and Holthoff, Knut and Schwabe, Lars and Kiebel, Stefan J.},
	urldate = {2018-11-07},
	date = {2017-10-12},
	file = {Rahmati et al. - 2017 - Developmental Emergence of Sparse Coding A Dynami.pdf:/Users/bert/Zotero/storage/MBMEIEXD/Rahmati et al. - 2017 - Developmental Emergence of Sparse Coding A Dynami.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/6DYNGW6R/s41598-017-13468-z.html:text/html},
}

@article{innes_dont_2018,
	title = {Don't Unroll Adjoint: Differentiating {SSA}-Form Programs},
	url = {http://arxiv.org/abs/1810.07951},
	shorttitle = {Don't Unroll Adjoint},
	abstract = {This paper presents reverse-mode algorithmic differentiation ({AD}) based on source code transformation, in particular of the Static Single Assignment ({SSA}) form used by modern compilers. The approach can support control flow, nesting, mutation, recursion, data structures, higher-order functions, and other language constructs, and the output is given to an existing compiler to produce highly efficient differentiated code. Our implementation is a new {AD} tool for the Julia language, called Zygote, which presents high-level dynamic semantics while transparently compiling adjoint code under the hood. We discuss the benefits of this approach to both the usability and performance of {AD} tools.},
	journaltitle = {{arXiv}:1810.07951 [cs]},
	author = {Innes, Michael},
	urldate = {2018-11-02},
	date = {2018-10-18},
	eprinttype = {arxiv},
	eprint = {1810.07951},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/AWPQTL9C/1810.html:text/html;Innes - 2018 - Don't Unroll Adjoint Differentiating SSA-Form Pro.pdf:/Users/bert/Zotero/storage/YY8Z35SI/Innes - 2018 - Don't Unroll Adjoint Differentiating SSA-Form Pro.pdf:application/pdf},
}

@article{lackey_belief_2018,
	title = {A belief propagation algorithm based on domain decomposition},
	url = {http://arxiv.org/abs/1810.10005},
	abstract = {This note provides a detailed description and derivation of the domain decomposition algorithm that appears in previous works by the author. Given a large re-estimation problem, domain decomposition provides an iterative method for assembling Boltzmann distributions associated to small subproblems into an approximation of the Bayesian posterior of the whole problem. The algorithm is amenable to using Boltzmann sampling to approximate these Boltzmann distributions. In previous work, we have shown the capability of heuristic versions of this algorithm to solve {LDPC} decoding and circuit fault diagnosis problems too large to fit on quantum annealing hardware used for sampling. Here, we rigorously prove soundness of the method.},
	journaltitle = {{arXiv}:1810.10005 [quant-ph]},
	author = {Lackey, Brad},
	urldate = {2018-10-31},
	date = {2018-10-23},
	eprinttype = {arxiv},
	eprint = {1810.10005},
	keywords = {Computer Science - Data Structures and Algorithms, Quantum Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/F82MUHZP/1810.html:text/html;Lackey - 2018 - A belief propagation algorithm based on domain dec.pdf:/Users/bert/Zotero/storage/R2Z5UN4V/Lackey - 2018 - A belief propagation algorithm based on domain dec.pdf:application/pdf},
}

@article{rezende_taming_2018,
	title = {Taming {VAEs}},
	url = {http://arxiv.org/abs/1810.00597},
	abstract = {In spite of remarkable progress in deep latent variable generative modeling, training still remains a challenge due to a combination of optimization and generalization issues. In practice, a combination of heuristic algorithms (such as hand-crafted annealing of {KL}-terms) is often used in order to achieve the desired results, but such solutions are not robust to changes in model architecture or dataset. The best settings can often vary dramatically from one problem to another, which requires doing expensive parameter sweeps for each new case. Here we develop on the idea of training {VAEs} with additional constraints as a way to control their behaviour. We first present a detailed theoretical analysis of constrained {VAEs}, expanding our understanding of how these models work. We then introduce and analyze a practical algorithm termed Generalized {ELBO} with Constrained Optimization, {GECO}. The main advantage of {GECO} for the machine learning practitioner is a more intuitive, yet principled, process of tuning the loss. This involves defining of a set of constraints, which typically have an explicit relation to the desired model performance, in contrast to tweaking abstract hyper-parameters which implicitly affect the model behavior. Encouraging experimental results in several standard datasets indicate that {GECO} is a very robust and effective tool to balance reconstruction and compression constraints.},
	journaltitle = {{arXiv}:1810.00597 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Viola, Fabio},
	urldate = {2018-10-31},
	date = {2018-10-01},
	eprinttype = {arxiv},
	eprint = {1810.00597},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/MQGKWSP4/1810.html:text/html;Rezende and Viola - 2018 - Taming VAEs.pdf:/Users/bert/Zotero/storage/TVUFL34X/Rezende and Viola - 2018 - Taming VAEs.pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational Inference: A Review for Statisticians},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	url = {http://arxiv.org/abs/1601.00670},
	doi = {10.1080/01621459.2017.1285773},
	shorttitle = {Variational Inference},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference ({VI}), a method from machine learning that approximates probability densities through optimization. {VI} has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind {VI} is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of {VI} applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in {VI} and highlight important open problems. {VI} is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	pages = {859--877},
	number = {518},
	journaltitle = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and {McAuliffe}, Jon D.},
	urldate = {2018-10-29},
	date = {2017-04-03},
	eprinttype = {arxiv},
	eprint = {1601.00670},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/I257PR5E/1601.html:text/html;Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:/Users/bert/Zotero/storage/ZBC9BWQK/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf},
}

@article{friston_post_2011,
	title = {Post hoc Bayesian model selection},
	volume = {56},
	issn = {1053-8119},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3112494/},
	doi = {10.1016/j.neuroimage.2011.03.062},
	abstract = {This note describes a Bayesian model selection or optimization procedure for post hoc inferences about reduced versions of a full model. The scheme provides the evidence (marginal likelihood) for any reduced model as a function of the posterior density over the parameters of the full model. It rests upon specifying models through priors on their parameters, under the assumption that the likelihood remains the same for all models considered. This provides a quick and efficient scheme for scoring arbitrarily large numbers of models, after inverting a single (full) model. In turn, this enables the selection among discrete models that are distinguished by the presence or absence of free parameters, where free parameters are effectively removed from the model using very precise shrinkage priors. An alternative application of this post hoc model selection considers continuous model spaces, defined in terms of hyperparameters (sufficient statistics) of the prior density over model parameters. In this instance, the prior (model) can be optimized with respect to its evidence. The expressions for model evidence become remarkably simple under the Laplace (Gaussian) approximation to the posterior density. Special cases of this scheme include Savage–Dickey density ratio tests for reduced models and automatic relevance determination in model optimization. We illustrate the approach using general linear models and a more complicated nonlinear state-space model.},
	pages = {2089--2099},
	number = {4},
	journaltitle = {Neuroimage},
	shortjournal = {Neuroimage},
	author = {Friston, Karl and Penny, Will},
	urldate = {2014-12-02},
	date = {2011-06-15},
	pmid = {21459150},
	pmcid = {PMC3112494},
	file = {Friston and Penny - 2011 - Post hoc Bayesian model selection.pdf:/Users/bert/Zotero/storage/J9MJVF4H/Friston and Penny - 2011 - Post hoc Bayesian model selection.pdf:application/pdf},
}

@report{minka_gates:_2008,
	title = {Gates: A Graphical Notation for Mixture Models},
	url = {https://www.microsoft.com/en-us/research/publication/gates-a-graphical-notation-for-mixture-models/},
	abstract = {Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms for probabilistic inference, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates.},
	author = {Minka, Tom and Winn, John},
	date = {2008-12},
	file = {Minka en Winn - 2008 - Gates A Graphical Notation for Mixture Models.pdf:/Users/bert/Zotero/storage/LHECS9GU/Minka en Winn - 2008 - Gates A Graphical Notation for Mixture Models.pdf:application/pdf},
}

@article{van_de_meent_introduction_2018,
	title = {An Introduction to Probabilistic Programming},
	url = {http://arxiv.org/abs/1809.10756},
	abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language ({PPL}) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted {PPL} we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
	journaltitle = {{arXiv}:1809.10756 [cs, stat]},
	author = {van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
	urldate = {2018-10-01},
	date = {2018-09-27},
	eprinttype = {arxiv},
	eprint = {1809.10756},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3TAXC3HR/1809.html:text/html;van de Meent e.a. - 2018 - An Introduction to Probabilistic Programming.pdf:/Users/bert/Zotero/storage/LX27IVGZ/van de Meent e.a. - 2018 - An Introduction to Probabilistic Programming.pdf:application/pdf},
}

@inproceedings{titsias_doubly_2014,
	location = {Beijing, China},
	title = {Doubly Stochastic Variational Bayes for Non-conjugate Inference},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3045112},
	series = {{ICML}'14},
	abstract = {We propose a simple and effective variational inference algorithm based on stochastic optimisation that can be widely applied for Bayesian non-conjugate inference in continuous parameter spaces. This algorithm is based on stochastic approximation and allows for efficient use of gradient information from the model joint density. We demonstrate these properties using illustrative examples as well as in challenging and diverse Bayesian inference problems such as variable selection in logistic regression and fully Bayesian inference over kernel hyperparameters in Gaussian process regression.},
	pages = {II--1971--II--1980},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	publisher = {{JMLR}.org},
	author = {Titsias, Michalis K. and Lázaro-Gredilla, Miguel},
	urldate = {2018-09-24},
	date = {2014},
	file = {Titsias and Lázaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for Non-conjug.pdf:/Users/bert/Zotero/storage/5LHSEMNX/Titsias and Lázaro-Gredilla - 2014 - Doubly Stochastic Variational Bayes for Non-conjug.pdf:application/pdf},
}

@article{reed_few-shot_2017,
	title = {Few-shot Autoregressive Density Estimation: Towards Learning to Learn Distributions},
	url = {http://arxiv.org/abs/1710.10304},
	shorttitle = {Few-shot Autoregressive Density Estimation},
	abstract = {Deep autoregressive models have shown state-of-the-art performance in density estimation for natural images on large-scale datasets such as {ImageNet}. However, such models require many thousands of gradient-based weight updates and unique image examples for training. Ideally, the models would rapidly learn visual concepts from only a handful of examples, similar to the manner in which humans learns across many vision tasks. In this paper, we show how 1) neural attention and 2) meta learning techniques can be used in combination with autoregressive models to enable effective few-shot density estimation. Our proposed modifications to {PixelCNN} result in state-of-the art few-shot density estimation on the Omniglot dataset. Furthermore, we visualize the learned attention policy and find that it learns intuitive algorithms for simple tasks such as image mirroring on {ImageNet} and handwriting on Omniglot without supervision. Finally, we extend the model to natural images and demonstrate few-shot image generation on the Stanford Online Products dataset.},
	journaltitle = {{arXiv}:1710.10304 [cs]},
	author = {Reed, Scott and Chen, Yutian and Paine, Thomas and Oord, Aäron van den and Eslami, S. M. Ali and Rezende, Danilo and Vinyals, Oriol and de Freitas, Nando},
	urldate = {2017-11-06},
	date = {2017-10-27},
	eprinttype = {arxiv},
	eprint = {1710.10304},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/JGFPGL4U/1710.html:text/html;Reed e.a. - 2017 - Few-shot Autoregressive Density Estimation Toward.pdf:/Users/bert/Zotero/storage/6CUJ6SQN/Reed e.a. - 2017 - Few-shot Autoregressive Density Estimation Toward.pdf:application/pdf;Reed e.a. - 2018 - Few-shot Autoregressive Density Estimation Toward.pdf:/Users/bert/Zotero/storage/NVWSLD77/Reed e.a. - 2018 - Few-shot Autoregressive Density Estimation Toward.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3DQ36ZXH/forum.html:text/html},
}

@article{petzschner_bayesian_2015,
	title = {A Bayesian perspective on magnitude estimation},
	volume = {19},
	issn = {1364-6613},
	url = {http://www.cell.com/article/S1364661315000509/abstract},
	doi = {10.1016/j.tics.2015.03.002},
	abstract = {Our representation of the physical world requires judgments of magnitudes, such as loudness, distance, or time. Interestingly, magnitude estimates are often not veridical but subject to characteristic biases. These biases are strikingly similar across different sensory modalities, suggesting common processing mechanisms that are shared by different sensory systems. However, the search for universal neurobiological principles of magnitude judgments requires guidance by formal theories. Here, we discuss a unifying Bayesian framework for understanding biases in magnitude estimation. This Bayesian perspective enables a re-interpretation of a range of established psychophysical findings, reconciles seemingly incompatible classical views on magnitude estimation, and can guide future investigations of magnitude estimation and its neurobiological mechanisms in health and in psychiatric diseases, such as schizophrenia.},
	pages = {285--293},
	number = {5},
	journaltitle = {Trends in Cognitive Sciences},
	author = {Petzschner, Frederike H. and Glasauer, Stefan and Stephan, Klaas E.},
	urldate = {2015-09-19},
	date = {2015-01},
	pmid = {25843543},
	keywords = {generative model, perceptual inference, Psychophysics, schizophrenia, Stevens’ power law, Weber-Fechner law},
	file = {Petzschner et al. - 2015 - A Bayesian perspective on magnitude estimation.pdf:/Users/bert/Zotero/storage/H84RWTZI/Petzschner et al. - 2015 - A Bayesian perspective on magnitude estimation.pdf:application/pdf;Petzschner et al. - 2015 - A Bayesian perspective on magnitude estimation.pdf:/Users/bert/Zotero/storage/6RHWAHZG/Petzschner et al. - 2015 - A Bayesian perspective on magnitude estimation.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XPN9JU2P/S1364-6613(15)00050-9.html:text/html;Snapshot:/Users/bert/Zotero/storage/WXJX75RL/S1364-6613(15)00050-9.html:text/html},
}

@article{markovic_comparative_2016,
	title = {Comparative Analysis of Behavioral Models for Adaptive Learning in Changing Environments},
	volume = {10},
	issn = {1662-5188},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4837154/},
	doi = {10.3389/fncom.2016.00033},
	abstract = {Probabilistic models of decision making under various forms of uncertainty have been applied in recent years to numerous behavioral and model-based {fMRI} studies. These studies were highly successful in enabling a better understanding of behavior and delineating the functional properties of brain areas involved in decision making under uncertainty. However, as different studies considered different models of decision making under uncertainty, it is unclear which of these computational models provides the best account of the observed behavioral and neuroimaging data. This is an important issue, as not performing model comparison may tempt researchers to over-interpret results based on a single model. Here we describe how in practice one can compare different behavioral models and test the accuracy of model comparison and parameter estimation of Bayesian and maximum-likelihood based methods. We focus our analysis on two well-established hierarchical probabilistic models that aim at capturing the evolution of beliefs in changing environments: Hierarchical Gaussian Filters and Change Point Models. To our knowledge, these two, well-established models have never been compared on the same data. We demonstrate, using simulated behavioral experiments, that one can accurately disambiguate between these two models, and accurately infer free model parameters and hidden belief trajectories (e.g., posterior expectations, posterior uncertainties, and prediction errors) even when using noisy and highly correlated behavioral measurements. Importantly, we found several advantages of Bayesian inference and Bayesian model comparison compared to often-used Maximum-Likelihood schemes combined with the Bayesian Information Criterion. These results stress the relevance of Bayesian data analysis for model-based neuroimaging studies that investigate human decision making under uncertainty.},
	journaltitle = {Frontiers in Computational Neuroscience},
	author = {Marković, Dimitrije and Kiebel, Stefan J.},
	urldate = {2017-08-07},
	date = {2016-04},
	pmid = {27148030},
	pmcid = {PMC4837154},
	file = {Marković and Kiebel - 2016 - Comparative Analysis of Behavioral Models for Adap.pdf:/Users/bert/Zotero/storage/XHFFUH6G/Marković and Kiebel - 2016 - Comparative Analysis of Behavioral Models for Adap.pdf:application/pdf},
}

@article{kim_recognition_2017,
	title = {Recognition Dynamics in the Brain under the Free Energy Principle},
	url = {http://arxiv.org/abs/1710.09118},
	abstract = {We formulate the computational processes of perception in the framework of the principle of least action by postulating the theoretical action as a time integral of the free energy in the brain sciences. The free energy principle is accordingly rephrased as that for autopoietic grounds all viable organisms attempt to minimize the sensory uncertainty about the unpredictable environment over a temporal horizon. By varying the informational action, we derive the brain's recognition dynamics ({RD}) which conducts Bayesian filtering of the external causes from noisy sensory inputs. Consequently, we effectively cast the gradient-descent scheme of minimizing the free energy into Hamiltonian mechanics by addressing only positions and momenta of the organisms' representations of the causal environment. To manifest the utility of our theory, we show how the {RD} may be implemented in a neuronally based biophysical model at a single-cell level and subsequently in a coarse-grained, hierarchical architecture of the brain. We also present formal solutions to the {RD} for a model brain in linear regime and analyze the perceptual trajectories around attractors in neural state space.},
	journaltitle = {{arXiv}:1710.09118 [q-bio]},
	author = {Kim, Chang Sub},
	urldate = {2018-09-07},
	date = {2017-10-25},
	eprinttype = {arxiv},
	eprint = {1710.09118},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ANGBERTX/1710.html:text/html;Kim - 2017 - Recognition Dynamics in the Brain under the Free E.pdf:/Users/bert/Zotero/storage/QCNV7S6D/Kim - 2017 - Recognition Dynamics in the Brain under the Free E.pdf:application/pdf},
}

@article{friston_does_2018,
	title = {Does predictive coding have a future?},
	volume = {21},
	rights = {2018 The Publisher},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-018-0200-7},
	doi = {10.1038/s41593-018-0200-7},
	abstract = {In the 20th century we thought the brain extracted knowledge from sensations. The 21st century witnessed a ‘strange inversion’, in which the brain became an organ of inference, actively constructing explanations for what’s going on ‘out there’, beyond its sensory epithelia. One paper played a key role in this paradigm shift.},
	pages = {1019},
	number = {8},
	journaltitle = {Nature Neuroscience},
	author = {Friston, Karl},
	urldate = {2018-08-30},
	date = {2018-08},
	langid = {english},
	file = {Friston - 2018 - Does predictive coding have a future.pdf:/Users/bert/Zotero/storage/DHZN9DTP/Friston - 2018 - Does predictive coding have a future.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/I2NINNIK/s41593-018-0200-7.html:text/html},
}

@inproceedings{ge_turing:_2018,
	title = {Turing: A Language for Flexible Probabilistic Inference},
	url = {http://proceedings.mlr.press/v84/ge18b.html},
	shorttitle = {Turing},
	abstract = {Probabilistic programming promises to simplify and democratize probabilistic machine learning, but successful probabilistic programming systems require flexible, generic and efficient inference eng...},
	eventtitle = {International Conference on Artificial Intelligence and Statistics},
	pages = {1682--1690},
	booktitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Ge, Hong and Xu, Kai and Ghahramani, Zoubin},
	urldate = {2018-08-09},
	date = {2018-03-31},
	langid = {english},
	file = {Ge e.a. - 2018 - Turing A Language for Flexible Probabilistic Infe.pdf:/Users/bert/Zotero/storage/NZS72SEP/Ge e.a. - 2018 - Turing A Language for Flexible Probabilistic Infe.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/GB9KXXPM/ge18b.html:text/html},
}

@article{revels_forward-mode_2016,
	title = {Forward-Mode Automatic Differentiation in Julia},
	url = {http://arxiv.org/abs/1607.07892},
	abstract = {We present {ForwardDiff}, a Julia package for forward-mode automatic differentiation ({AD}) featuring performance competitive with low-level languages like C++. Unlike recently developed {AD} tools in other popular high-level languages such as Python and {MATLAB}, {ForwardDiff} takes advantage of just-in-time ({JIT}) compilation to transparently recompile {AD}-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, {ForwardDiff} provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, {ForwardDiff}'s gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how {ForwardDiff} is used effectively within {JuMP}, a modeling language for optimization. According to our usage statistics, 41 unique repositories on {GitHub} depend on {ForwardDiff}, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics. This document is an extended abstract that has been accepted for presentation at the {AD}2016 7th International Conference on Algorithmic Differentiation.},
	journaltitle = {{arXiv}:1607.07892 [cs]},
	author = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
	urldate = {2018-07-22},
	date = {2016-07-26},
	eprinttype = {arxiv},
	eprint = {1607.07892},
	keywords = {Computer Science - Mathematical Software},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/G3IMMCU7/1607.html:text/html;Revels et al. - 2016 - Forward-Mode Automatic Differentiation in Julia.pdf:/Users/bert/Zotero/storage/H7FMHW32/Revels et al. - 2016 - Forward-Mode Automatic Differentiation in Julia.pdf:application/pdf},
}

@article{wand_fast_2017,
	title = {Fast Approximate Inference for Arbitrarily Large Semiparametric Regression Models via Message Passing},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1197833},
	doi = {10.1080/01621459.2016.1197833},
	abstract = {We show how the notion of message passing can be used to streamline the algebra and computer coding for fast approximate inference in large Bayesian semiparametric regression models. In particular, this approach is amenable to handling arbitrarily large models of particular types once a set of primitive operations is established. The approach is founded upon a message passing formulation of mean field variational Bayes that utilizes factor graph representations of statistical models. The underlying principles apply to general Bayesian hierarchical models although we focus on semiparametric regression. The notion of factor graph fragments is introduced and is shown to facilitate compartmentalization of the required algebra and coding. The resultant algorithms have ready-to-implement closed form expressions and allow a broad class of arbitrarily large semiparametric regression models to be handled. Ongoing software projects such as Infer.{NET} and Stan support variational-type inference for particular model classes. This article is not concerned with software packages per se and focuses on the underlying tenets of scalable variational inference algorithms. Supplementary materials for this article are available online.},
	pages = {137--168},
	number = {517},
	journaltitle = {Journal of the American Statistical Association},
	author = {Wand, M. P.},
	urldate = {2018-07-22},
	date = {2017-01-02},
	keywords = {Factor graphs, Generalized additive models, Generalized linear mixed models, Low-rank smoothing splines, Mean field variational Bayes, Scalable statistical methodology, Variational message passing},
	file = {Snapshot:/Users/bert/Zotero/storage/4HCBGT6H/01621459.2016.html:text/html;Wand - 2017 - Fast Approximate Inference for Arbitrarily Large S.pdf:/Users/bert/Zotero/storage/F7N83EZP/Wand - 2017 - Fast Approximate Inference for Arbitrarily Large S.pdf:application/pdf},
}

@online{koop_variational_2018,
	title = {Variational Bayes inference in high-dimensional time-varying parameter models},
	url = {https://mpra.ub.uni-muenchen.de/87972/},
	abstract = {This paper proposes a mean field variational Bayes algorithm for efficient posterior and predictive inference in time-varying parameter models. Our approach involves: i) computationally trivial Kalman filter updates of regression coefficients, ii) a dynamic variable selection prior that removes irrelevant variables in each time period, and iii) a fast approximate state-space estimator of the regression volatility parameter. In an exercise involving simulated data we evaluate the new algorithm numerically and establish its computational advantages. Using macroeconomic data for the {US} we find that regression models that combine time-varying parameters with the information in many predictors have the potential to improve forecasts over a number of alternatives.},
	type = {{MPRA} Paper},
	author = {Koop, Gary and Korobilis, Dimitris},
	urldate = {2018-07-22},
	date = {2018-07-15},
	langid = {english},
	file = {Koop and Korobilis - 2018 - Variational Bayes inference in high-dimensional ti.pdf:/Users/bert/Zotero/storage/7E8828BB/Koop and Korobilis - 2018 - Variational Bayes inference in high-dimensional ti.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/D6F5XWT2/87972.html:text/html},
}

@article{alexander_frontal_2018,
	title = {Frontal cortex function as derived from hierarchical predictive coding},
	volume = {8},
	rights = {2018 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-21407-9},
	doi = {10.1038/s41598-018-21407-9},
	abstract = {The frontal lobes are essential for human volition and goal-directed behavior, yet their function remains unclear. While various models have highlighted working memory, reinforcement learning, and cognitive control as key functions, a single framework for interpreting the range of effects observed in prefrontal cortex has yet to emerge. Here we show that a simple computational motif based on predictive coding can be stacked hierarchically to learn and perform arbitrarily complex goal-directed behavior. The resulting Hierarchical Error Representation ({HER}) model simulates a wide array of findings from {fMRI}, {ERP}, single-units, and neuropsychological studies of both lateral and medial prefrontal cortex. By reconceptualizing lateral prefrontal activity as anticipating prediction errors, the {HER} model provides a novel unifying account of prefrontal cortex function with broad implications for understanding the frontal cortex across multiple levels of description, from the level of single neurons to behavior.},
	pages = {3843},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Alexander, William H. and Brown, Joshua W.},
	urldate = {2018-07-21},
	date = {2018-03-01},
	langid = {english},
	file = {41598_2018_21407_MOESM1_ESM.docx:/Users/bert/Zotero/storage/I94WC6Y8/41598_2018_21407_MOESM1_ESM.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document;Alexander and Brown - 2018 - Frontal cortex function as derived from hierarchic.pdf:/Users/bert/Zotero/storage/AQ3CZPGU/Alexander and Brown - 2018 - Frontal cortex function as derived from hierarchic.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RXC2EAK5/s41598-018-21407-9.html:text/html},
}

@article{oord_representation_2018,
	title = {Representation Learning with Contrastive Predictive Coding},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	journaltitle = {{arXiv}:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	urldate = {2018-07-19},
	date = {2018-07-10},
	eprinttype = {arxiv},
	eprint = {1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/C9NDQA5Z/1807.html:text/html;Oord e.a. - 2018 - Representation Learning with Contrastive Predictiv.pdf:/Users/bert/Zotero/storage/QEAKN5JC/Oord e.a. - 2018 - Representation Learning with Contrastive Predictiv.pdf:application/pdf},
}

@online{cepelewicz_make_2018,
	title = {To Make Sense of the Present, Brains May Predict the Future},
	url = {https://www.quantamagazine.org/to-make-sense-of-the-present-brains-may-predict-the-future-20180710/},
	abstract = {A controversial theory suggests that perception, motor control, memory and other brain functions all depend on comparisons between ongoing actual experiences},
	titleaddon = {Quanta Magazine},
	author = {Cepelewicz, {ByJordana}},
	urldate = {2018-07-11},
	date = {2018},
	file = {Cepelewicz - 2018 - To Make Sense of the Present, Brains May Predict t.pdf:/Users/bert/Zotero/storage/ECNR63JK/Cepelewicz - 2018 - To Make Sense of the Present, Brains May Predict t.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/28XLIFMS/to-make-sense-of-the-present-brains-may-predict-the-future-20180710.html:text/html},
}

@article{isomura_measure_2018,
	title = {A Measure of Information Available for Inference},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/20/7/512},
	doi = {10.3390/e20070512},
	abstract = {The mutual information between the state of a neural network and the state of the external world represents the amount of information stored in the neural network that is associated with the external world. In contrast, the surprise of the sensory input indicates the unpredictability of the current input. In other words, this is a measure of inference ability, and an upper bound of the surprise is known as the variational free energy. According to the free-energy principle ({FEP}), a neural network continuously minimizes the free energy to perceive the external world. For the survival of animals, inference ability is considered to be more important than simply memorized information. In this study, the free energy is shown to represent the gap between the amount of information stored in the neural network and that available for inference. This concept involves both the {FEP} and the infomax principle, and will be a useful measure for quantifying the amount of information available for inference.},
	pages = {512},
	number = {7},
	journaltitle = {Entropy},
	author = {Isomura, Takuya},
	urldate = {2018-07-10},
	date = {2018-07-07},
	langid = {english},
	keywords = {free-energy principle, independent component analysis, infomax principle, internal model hypothesis, principal component analysis, unconscious inference},
	file = {Isomura - 2018 - A Measure of Information Available for Inference.pdf:/Users/bert/Zotero/storage/TAF2DQBR/Isomura - 2018 - A Measure of Information Available for Inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MBGY956W/512.html:text/html},
}

@article{frassle_generative_2018,
	title = {A generative model of whole-brain effective connectivity},
	volume = {179},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811918304762},
	doi = {10.1016/j.neuroimage.2018.05.058},
	abstract = {The development of whole-brain models that can infer effective (directed) connection strengths from {fMRI} data represents a central challenge for computational neuroimaging. A recently introduced generative model of {fMRI} data, regression dynamic causal modeling ({rDCM}), moves towards this goal as it scales gracefully to very large networks. However, large-scale networks with thousands of connections are difficult to interpret; additionally, one typically lacks information (data points per free parameter) for precise estimation of all model parameters. This paper introduces sparsity constraints to the variational Bayesian framework of {rDCM} as a solution to these problems in the domain of task-based {fMRI}. This sparse {rDCM} approach enables highly efficient effective connectivity analyses in whole-brain networks and does not require a priori assumptions about the network's connectivity structure but prunes fully (all-to-all) connected networks as part of model inversion. Following the derivation of the variational Bayesian update equations for sparse {rDCM}, we use both simulated and empirical data to assess the face validity of the model. In particular, we show that it is feasible to infer effective connection strengths from {fMRI} data using a network with more than 100 regions and 10,000 connections. This demonstrates the feasibility of whole-brain inference on effective connectivity from {fMRI} data – in single subjects and with a run-time below 1 min when using parallelized code. We anticipate that sparse {rDCM} may find useful application in connectomics and clinical neuromodeling – for example, for phenotyping individual patients in terms of whole-brain network structure.},
	pages = {505--529},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Frässle, Stefan and Lomakina, Ekaterina I. and Kasper, Lars and Manjaly, Zina M. and Leff, Alex and Pruessmann, Klaas P. and Buhmann, Joachim M. and Stephan, Klaas E.},
	urldate = {2018-07-06},
	date = {2018-10-01},
	keywords = {Generative model, Dynamic causal modeling, Bayesian regression, Connectomics, Effective connectivity, Sparsity},
	file = {Frässle et al. - 2018 - A generative model of whole-brain effective connec.pdf:/Users/bert/Zotero/storage/TZMRS82G/Frässle et al. - 2018 - A generative model of whole-brain effective connec.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/P26MZRZ9/S1053811918304762.html:text/html},
}

@article{baltieri_modularity_2018,
	title = {The modularity of action and perception revisited using control theory and active inference},
	url = {http://arxiv.org/abs/1806.02649},
	abstract = {The assumption that action and perception can be investigated independently is entrenched in theories, models and experimental approaches across the brain and mind sciences. In cognitive science, this has been a central point of contention between computationalist and 4Es (enactive, embodied, extended and embedded) theories of cognition, with the former embracing the "classical sandwich", modular, architecture of the mind and the latter actively denying this separation can be made. In this work we suggest that the modular independence of action and perception strongly resonates with the separation principle of control theory and furthermore that this principle provides formal criteria within which to evaluate the implications of the modularity of action and perception. We will also see that real-time feedback with the environment, often considered necessary for the definition of 4Es ideas, is not however a sufficient condition to avoid the "classical sandwich". Finally, we argue that an emerging framework in the cognitive and brain sciences, active inference, extends ideas derived from control theory to the study of biological systems while disposing of the separation principle, describing non-modular models of behaviour strongly aligned with 4Es theories of cognition.},
	journaltitle = {{arXiv}:1806.02649 [q-bio]},
	author = {Baltieri, Manuel and Buckley, Christopher L.},
	urldate = {2018-07-04},
	date = {2018-06-07},
	eprinttype = {arxiv},
	eprint = {1806.02649},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Y2GQVQCX/1806.html:text/html;Baltieri and Buckley - 2018 - The modularity of action and perception revisited .pdf:/Users/bert/Zotero/storage/QFQL3JJZ/Baltieri and Buckley - 2018 - The modularity of action and perception revisited .pdf:application/pdf},
}

@inproceedings{vontobel_factor-graph_2011,
	title = {A factor-graph approach to Lagrangian and Hamiltonian dynamics},
	doi = {10.1109/ISIT.2011.6033945},
	abstract = {Factor graphs are graphical models with origins in coding theory. The sum-product, the max-product, and the min-sum algorithms, which operate by message passing on a factor graph, subsume a great variety of algorithms in coding, signal processing, and artificial intelligence. This paper aims at extending the field of possible applications of factor graphs to Lagrangian and Hamiltonian dynamics. The starting point is the principle of least action (more precisely, the principle of stationary action). The resulting factor graphs require a new message-passing algorithm that we call the stationary-sum algorithm. As it turns out, some of the properties of this algorithm are equivalent to Liouville's theorem. Moreover, duality results for factor graphs allow to easily derive Noether's theorem. We also discuss connections and differences to Kalman filtering.},
	eventtitle = {2011 {IEEE} International Symposium on Information Theory Proceedings},
	pages = {2183--2187},
	booktitle = {2011 {IEEE} International Symposium on Information Theory Proceedings},
	author = {Vontobel, P. O.},
	date = {2011-07},
	keywords = {Kalman filters, message passing, Heuristic algorithms, Kalman filtering, graph theory, message passing algorithm, coding theory, Equations, factor graph approach, Hamiltonian dynamics, Lagrangian dynamics, Lagrangian functions, least action principle, Liouville theorem, Mathematical model, signal processing, stationary action principle, stationary sum algorithm, Trajectory, Transforms},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/4VIBVPPT/6033945.html:text/html;Vontobel - 2011 - A factor-graph approach to Lagrangian and Hamilton.pdf:/Users/bert/Zotero/storage/IX6V8XSQ/Vontobel - 2011 - A factor-graph approach to Lagrangian and Hamilton.pdf:application/pdf},
}

@article{biehl_expanding_2018,
	title = {Expanding the Active Inference Landscape: More Intrinsic Motivations in the Perception-Action Loop},
	url = {http://arxiv.org/abs/1806.08083},
	shorttitle = {Expanding the Active Inference Landscape},
	abstract = {Active inference is an ambitious theory that treats perception, inference and action selection of autonomous agents under the heading of a single principle. It suggests biologically plausible explanations for many cognitive phenomena, including consciousness. In active inference, action selection is driven by an objective function that evaluates possible future actions with respect to current, inferred beliefs about the world. Active inference at its core is independent from extrinsic rewards, resulting in a high level of robustness across e.g.{\textbackslash} different environments or agent morphologies. In the literature, paradigms that share this independence have been summarised under the notion of intrinsic motivations. In general and in contrast to active inference, these models of motivation come without a commitment to particular inference and action selection mechanisms. In this article, we study if the inference and action selection machinery of active inference can also be used by alternatives to the originally included intrinsic motivation. The perception-action loop explicitly relates inference and action selection to the environment and agent memory, and is consequently used as foundation for our analysis. We reconstruct the active inference approach, locate the original formulation within, and show how alternative intrinsic motivations can be used while keeping many of the original features intact. Furthermore, we illustrate the connection to universal reinforcement learning by means of our formalism. Active inference research may profit from comparisons of the dynamics induced by alternative intrinsic motivations. Research on intrinsic motivations may profit from an additional way to implement intrinsically motivated agents that also share the biological plausibility of active inference.},
	journaltitle = {{arXiv}:1806.08083 [cs]},
	author = {Biehl, Martin and Guckelsberger, Christian and Salge, Christoph and Smith, Simón C. and Polani, Daniel},
	urldate = {2018-06-22},
	date = {2018-06-21},
	eprinttype = {arxiv},
	eprint = {1806.08083},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Systems and Control, 62F15, 91B06, I.2.0, I.2.6, I.5.0, I.5.1},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/PV6G8LMC/1806.html:text/html;Biehl e.a. - 2018 - Expanding the Active Inference Landscape More Int.pdf:/Users/bert/Zotero/storage/A56V98IR/Biehl e.a. - 2018 - Expanding the Active Inference Landscape More Int.pdf:application/pdf},
}

@article{grochow_beyond_2018,
	title = {Beyond Number of Bit Erasures: New Complexity Questions Raisedby Recently Discovered Thermodynamic Costs of Computation},
	volume = {49},
	issn = {0163-5700},
	url = {http://doi.acm.org/10.1145/3232679.3232689},
	doi = {10.1145/3232679.3232689},
	shorttitle = {Beyond Number of Bit Erasures},
	abstract = {Recent advances in nonequilibrium statistical mechanics have led to a deeper understanding of the thermodynamic cost of computation than that put forth by Landauer and then studied extensively in the computational complexity community. In particular, Landauer's work led to a focus on the number of bit erasures in a computation, due to its relation to the change in entropy between input and output. However new advances in physics{\textbar}which have been experimentally con rmed{\textbar}mean we can now calculate additional thermodynamic costs beyond merely the change in entropy between input and output. As a consequence, we now understand that while logically reversible computing can have some thermodynamic bene ts, it is far from the end of the story. The purpose of this paper is to highlight new open questions in computational complexity raised by consideration of these new thermodynamic costs. Beyond leading to a revised viewpoint on the bene ts of logical reversibility, these questions touch on randomized algorithms, average-case complexity, the thermodynamic cost of error correcting codes, and noisy/inexact/approximate computation.},
	pages = {33--56},
	number = {2},
	journaltitle = {{SIGACT} News},
	author = {Grochow, Joshua A. and Wolpert, David H.},
	urldate = {2018-06-22},
	date = {2018-06},
}

@inproceedings{frandina_variational_2013,
	title = {Variational Foundations of Online Backpropagation},
	isbn = {978-3-642-40727-7 978-3-642-40728-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-40728-4_11},
	doi = {10.1007/978-3-642-40728-4_11},
	series = {Lecture Notes in Computer Science},
	abstract = {On-line Backpropagation has become very popular and it has been the subject of in-depth theoretical analyses and massive experimentation. Yet, after almost three decades from its publication, it is still surprisingly the source of tough theoretical questions and of experimental results that are somewhat shrouded in mystery. Although seriously plagued by local minima, the batch-mode version of the algorithm is clearly posed as an optimization problem while, in spite of its effectiveness, in many real-world problems the on-line mode version has not been given a clean formulation, yet. Using variational arguments, in this paper, the on-line formulation is proposed as the minimization of a classic functional that is inspired by the principle of minimal action in analytic mechanics. The proposed approach clashes sharply with common interpretations of on-line learning as an approximation of batch-mode, and it suggests that processing data all at once might be just an artificial formulation of learning that is hopeless in difficult real-world problems.},
	eventtitle = {International Conference on Artificial Neural Networks},
	pages = {82--89},
	booktitle = {Artificial Neural Networks and Machine Learning – {ICANN} 2013},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Frandina, Salvatore and Gori, Marco and Lippi, Marco and Maggini, Marco and Melacci, Stefano},
	urldate = {2018-06-18},
	date = {2013-09-10},
	langid = {english},
	file = {Frandina e.a. - 2013 - Variational Foundations of Online Backpropagation.pdf:/Users/bert/Zotero/storage/Q3PIBGZU/Frandina e.a. - 2013 - Variational Foundations of Online Backpropagation.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YPQYRD33/978-3-642-40728-4_11.html:text/html},
}

@article{parr_discrete_2018,
	title = {The Discrete and Continuous Brain: From Decisions to Movement—and Back Again},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01102},
	doi = {10.1162/neco_a_01102},
	shorttitle = {The Discrete and Continuous Brain},
	abstract = {To act upon the world, creatures must change continuous variables such as muscle length or chemical concentration. In contrast, decision making is an inherently discrete process, involving the selection among alternative courses of action. In this article, we consider the interface between the discrete and continuous processes that translate our decisions into movement in a Newtonian world—and how movement informs our decisions. We do so by appealing to active inference, with a special focus on the oculomotor system. Within this exemplar system, we argue that the superior colliculus is well placed to act as a discrete-continuous interface. Interestingly, when the neuronal computations within the superior colliculus are formulated in terms of active inference, we find that many aspects of its neuroanatomy emerge from the computations it must perform in this role.},
	pages = {1--29},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Parr, Thomas and Friston, Karl J.},
	urldate = {2018-06-16},
	date = {2018-06-12},
	file = {Parr en Friston - 2018 - The Discrete and Continuous Brain From Decisions .pdf:/Users/bert/Zotero/storage/KC2DEGHN/Parr en Friston - 2018 - The Discrete and Continuous Brain From Decisions .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/FRQWC6NA/neco_a_01102.html:text/html},
}

@article{schiller_motion_nodate,
	title = {Motion Mountain - The Adventure of Physics},
	pages = {601},
	author = {Schiller, Christoph},
	langid = {english},
	file = {Schiller - Motion Mountain - The Adventure of Physics.pdf:/Users/bert/Zotero/storage/AM5K2B9U/Schiller - Motion Mountain - The Adventure of Physics.pdf:application/pdf},
}

@incollection{hesp_multi-scale_2018,
	title = {A Multi-scale View of the Emergent Complexity of Life: A Free-energy Proposal},
	shorttitle = {A Multi-scale View of the Emergent Complexity of Life},
	abstract = {We review some of the main implications of the free-energy principle ({FEP}) for the study of the self-organization of living systems – and how the {FEP} can help us to understand (and model) biotic self-organization across the many temporal and spatial scales over which life exists. In order to maintain its integrity as a bounded system, any biological system - from single cells to complex organisms and societies - has to limit the disorder or dispersion (i.e., the long-run entropy) of its constituent states. We review how this can be achieved by living systems that minimize their variational free energy. Variational free energy is an information theoretic construct, originally introduced into theoretical neuroscience and biology to explain perception, action, and learning. It has since been extended to explain the evolution, development, form, and function of entire organisms, providing a principled model of biotic self-organization and autopoiesis. It has provided insights into biological systems across spatiotemporal scales, ranging from microscales (e.g., sub- and multicellular dynamics), to intermediate scales (e.g., groups of interacting animals and culture), through to macroscale phenomena (the evolution of entire species). A crucial corollary of the {FEP} is that an organism just is (i.e., embodies or entails) an implicit model of its environment. As such, organisms come to embody causal relationships of their ecological niche, which, in turn, is influenced by their resulting behaviors. Crucially, free-energy minimization can be shown to be equivalent to the maximization of Bayesian model evidence. This allows us to cast natural selection in terms of Bayesian model selection, providing a robust theoretical account of how organisms come to match or accommodate the spatiotemporal complexity of their surrounding niche. In line with the theme of this volume; namely, biological complexity and self-organization, this chapter will examine a variational approach to self-organization across multiple dynamical scales.},
	author = {Hesp, Casper and Ramstead, Maxwell and Constant, Axel and Badcock, Paul and Kirchhoff, Michael and Friston, Karl},
	date = {2018-05-01},
	file = {Hesp et al. - 2018 - A Multi-scale View of the Emergent Complexity of L.pdf:/Users/bert/Zotero/storage/A4DKK8JT/Hesp et al. - 2018 - A Multi-scale View of the Emergent Complexity of L.pdf:application/pdf},
}

@article{zhao_variational_2017,
	title = {Variational Joint Filtering},
	url = {http://arxiv.org/abs/1707.09049},
	abstract = {State space models provide an interpretable framework for complex time series by combining an intuitive dynamical system model with a probabilistic observation model. We developed a flexible online learning framework for latent nonlinear state dynamics and filtered latent states. Our method utilizes the stochastic gradient variational Bayes method to jointly optimize the parameters of the nonlinear dynamics, observation model, and the black-box recognition model. Unlike previous approaches, our framework can incorporate non-trivial observation noise models and has potential of inferring in real-time. We test our method on point process and Gaussian observations driven by continuous attractor dynamics and nonstationary systems, demonstrating its ability to recover the phase portrait, filtered trajectory, and produce long-term predictions for online machine learning.},
	journaltitle = {{arXiv}:1707.09049 [stat]},
	author = {Zhao, Yuan and Park, Il Memming},
	urldate = {2018-06-01},
	date = {2017-07-27},
	eprinttype = {arxiv},
	eprint = {1707.09049},
	keywords = {Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/L8T2EIYK/1707.html:text/html;Zhao and Park - 2017 - Variational Joint Filtering.pdf:/Users/bert/Zotero/storage/V8LHQFV4/Zhao and Park - 2017 - Variational Joint Filtering.pdf:application/pdf},
}

@article{friston_bayesian_2018,
	title = {Bayesian model reduction},
	url = {http://arxiv.org/abs/1805.07092},
	abstract = {This paper reviews recent developments in statistical structure learning; namely, Bayesian model reduction. Bayesian model reduction is a special but ubiquitous case of Bayesian model comparison that, in the setting of variational Bayes, furnishes an analytic solution for (a lower bound on) model evidence induced by a change in priors. This analytic solution finesses the problem of scoring large model spaces in model comparison or structure learning. This is because each new model can be cast in terms of an alternative set of priors over model parameters. Furthermore, the reduced free energy (i.e. evidence bound on the reduced model) finds an expedient application in hierarchical models, where it plays the role of a summary statistic. In other words, it contains all the necessary information contained in the posterior distributions over parameters of lower levels. In this technical note, we review Bayesian model reduction - in terms of common forms of reduced free energy - and illustrate recent applications in structure learning, hierarchical or empirical Bayes and as a metaphor for neurobiological processes like abductive reasoning and sleep.},
	journaltitle = {{arXiv}:1805.07092 [stat]},
	author = {Friston, Karl and Parr, Thomas and Zeidman, Peter},
	urldate = {2018-05-28},
	date = {2018-05-18},
	eprinttype = {arxiv},
	eprint = {1805.07092},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9XW27LI8/1805.html:text/html;Friston et al. - 2018 - Bayesian model reduction.pdf:/Users/bert/Zotero/storage/86X8JTVR/Friston et al. - 2018 - Bayesian model reduction.pdf:application/pdf},
}

@article{hermans_gradient_2018,
	title = {Gradient Energy Matching for Distributed Asynchronous Gradient Descent},
	url = {http://arxiv.org/abs/1805.08469},
	abstract = {Distributed asynchronous {SGD} has become widely used for deep learning in large-scale systems, but remains notorious for its instability when increasing the number of workers. In this work, we study the dynamics of distributed asynchronous {SGD} under the lens of Lagrangian mechanics. Using this description, we introduce the concept of energy to describe the optimization process and derive a sufficient condition ensuring its stability as long as the collective energy induced by the active workers remains below the energy of a target synchronous process. Making use of this criterion, we derive a stable distributed asynchronous optimization procedure, {GEM}, that estimates and maintains the energy of the asynchronous system below or equal to the energy of sequential {SGD} with momentum. Experimental results highlight the stability and speedup of {GEM} compared to existing schemes, even when scaling to one hundred asynchronous workers. Results also indicate better generalization compared to the targeted {SGD} with momentum.},
	journaltitle = {{arXiv}:1805.08469 [cs, stat]},
	author = {Hermans, Joeri and Louppe, Gilles},
	urldate = {2018-05-26},
	date = {2018-05-22},
	eprinttype = {arxiv},
	eprint = {1805.08469},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/33BSNDST/1805.html:text/html;Hermans and Louppe - 2018 - Gradient Energy Matching for Distributed Asynchron.pdf:/Users/bert/Zotero/storage/G7PJVG98/Hermans and Louppe - 2018 - Gradient Energy Matching for Distributed Asynchron.pdf:application/pdf},
}

@article{karnani_physical_2009,
	title = {The physical character of information},
	volume = {465},
	rights = {Copyright © 2009 The Royal Society. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/465/2107/2155},
	doi = {10.1098/rspa.2009.0063},
	abstract = {The mathematical theory of communication defines information in syntax without reference to its physical representation and semantic significance. However, in an everyday context, information is tied to its representation and its content is valued. The dichotomy between the formal definition and the practical perception of information is examined by the second law of thermodynamics that was recently formulated as an equation of motion. Thermodynamic entropy shows that the physical representation of information is not inconsequential in generation, transmission and processing of information. According to the principle of increasing entropy, communication by dissipative transformations is a natural process among many other evolutionary phenomena that level energy-density differences between components of a communication system and its surroundings. In addition, information-guided processes direct down along descents on free energy landscapes. The non-integrable equation for irreversible processes reveals that there is no universal analytical algorithm to match source to channel. Noise infiltration is also regarded by the second law as an inevitable consequence of energy transduction between a communication system and its surroundings. Communication is invariably associated with misunderstanding because mechanisms and means of information processing at the receiver differ from those at the sender. The significance of information is ascribed to the increase in thermodynamic entropy in the receiver system that results from execution of the received message.},
	pages = {2155--2175},
	number = {2107},
	journaltitle = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Karnani, Mahesh and Pääkkönen, Kimmo and Annila, Arto},
	urldate = {2018-05-20},
	date = {2009-07-08},
	langid = {english},
	file = {Karnani et al. - 2009 - The physical character of information.pdf:/Users/bert/Zotero/storage/D2G88WTI/Karnani et al. - 2009 - The physical character of information.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/U2NKWE6B/2155.html:text/html},
}

@article{constant_variational_2018,
	title = {A variational approach to niche construction},
	volume = {15},
	rights = {© 2018 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/15/141/20170685},
	doi = {10.1098/rsif.2017.0685},
	abstract = {In evolutionary biology, niche construction is sometimes described as a genuine evolutionary process whereby organisms, through their activities and regulatory mechanisms, modify their environment such as to steer their own evolutionary trajectory, and that of other species. There is ongoing debate, however, on the extent to which niche construction ought to be considered a bona fide evolutionary force, on a par with natural selection. Recent formulations of the variational free-energy principle as applied to the life sciences describe the properties of living systems, and their selection in evolution, in terms of variational inference. We argue that niche construction can be described using a variational approach. We propose new arguments to support the niche construction perspective, and to extend the variational approach to niche construction to current perspectives in various scientific fields.},
	pages = {20170685},
	number = {141},
	journaltitle = {Journal of The Royal Society Interface},
	author = {Constant, Axel and Ramstead, Maxwell J. D. and Veissière, Samuel P. L. and Campbell, John O. and Friston, Karl J.},
	urldate = {2018-05-19},
	date = {2018-04-01},
	langid = {english},
	pmid = {29643221},
	file = {Constant et al. - 2018 - A variational approach to niche construction.pdf:/Users/bert/Zotero/storage/VB5V3W8M/Constant et al. - 2018 - A variational approach to niche construction.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MYJ6FDG2/20170685.html:text/html},
}

@article{rojo_hamiltons_2005,
	title = {Hamilton's principle: why is the integrated difference of kinetic and potential energy minimized?},
	volume = {73},
	issn = {0002-9505, 1943-2909},
	url = {http://arxiv.org/abs/physics/0504016},
	doi = {10.1119/1.1930887},
	shorttitle = {Hamilton's principle},
	abstract = {I present an intuitive answer to an often asked question: why is the integrated difference K-U between the kinetic and potential energy the quantity to be minimized in Hamilton's principle? Using elementary arguments, I map the problem of finding the path of a moving particle connecting two points to that of finding the minimum potential energy of a static string. The mapping implies that the configuration of a non--stretchable string of variable tension corresponds to the spatial path dictated by the Principle of Least Action; that of a stretchable string in space-time is the one dictated by Hamilton's principle. This correspondence provides the answer to the question above: while a downward force curves the trajectory of a particle in the (x,t) plane downward, an upward force of the same magnitude stretches the string to the same configuration x(t).},
	pages = {831--836},
	number = {9},
	journaltitle = {American Journal of Physics},
	author = {Rojo, Alberto G.},
	urldate = {2018-05-18},
	date = {2005-09},
	eprinttype = {arxiv},
	eprint = {physics/0504016},
	keywords = {Physics - Physics Education, Physics - General Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/EHC42FGF/0504016.html:text/html;Rojo - 2005 - Hamilton's principle why is the integrated differ.pdf:/Users/bert/Zotero/storage/9T3VZGVL/Rojo - 2005 - Hamilton's principle why is the integrated differ.pdf:application/pdf},
}

@article{isomura_vitro_2018-1,
	title = {In vitro neural networks minimise variational free energy},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/early/2018/05/16/323550},
	doi = {10.1101/323550},
	abstract = {In this work, we address the neuronal encoding problem from a Bayesian perspective. Specifically, we ask whether neuronal responses in an in vitro neuronal network are consistent with ideal Bayesian observer responses under the free energy principle. In brief, we stimulated an in vitro cortical cell culture with stimulus trains that had a known statistical structure. We then asked whether recorded neuronal responses were consistent with variational message passing (i.e., belief propagation) based upon free energy minimisation (i.e., evidence maximisation). Effectively, this required us to solve two problems: first, we had to formulate the Bayes-optimal encoding of the causes or sources of sensory stimulation, and then show that these idealised responses could account for observed electrophysiological responses. We describe a simulation of an optimal neural network (i.e., the ideal Bayesian neural code) and then consider the mapping from idealised in silico responses to recorded in vitro responses. Our objective was to find evidence for functional specialisation and segregation in the in vitro neural network that reproduced in silico learning via free energy minimisation. Finally, we combined the in vitro and in silico results to characterise learning in terms of trajectories in a variational information plane of accuracy and complexity.},
	pages = {323550},
	journaltitle = {{bioRxiv}},
	author = {Isomura, Takuya and Friston, Karl},
	urldate = {2018-05-17},
	date = {2018-05-16},
	langid = {english},
	file = {Isomura and Friston - 2018 - In vitro neural networks minimise variational free.pdf:/Users/bert/Zotero/storage/FF42GSZJ/Isomura and Friston - 2018 - In vitro neural networks minimise variational free.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/S9VCXNYD/323550.html:text/html},
}

@article{ben-naim_informationaltheoretical_2009,
	title = {An Informational–Theoretical Formulation of the Second Law of Thermodynamics},
	volume = {86},
	issn = {0021-9584},
	url = {https://doi.org/10.1021/ed086p99},
	doi = {10.1021/ed086p99},
	abstract = {This paper presents a formulation of the second law of thermodynamics couched in terms of Shannon's measure of information. This formulation has an advantage over other formulations of the second law. First, it shows explicitly what is the thing that changes in a spontaneous process in an isolated system, which is traditionally referred to as the entropy. Second, and more importantly, this formulation offers a probabilistic answer to the question of why that thing changes in one direction only.},
	pages = {99},
	number = {1},
	journaltitle = {Journal of Chemical Education},
	shortjournal = {J. Chem. Educ.},
	author = {Ben-Naim, Arieh},
	urldate = {2018-05-05},
	date = {2009-01-01},
	file = {ACS Full Text Snapshot:/Users/bert/Zotero/storage/7GSA55FN/ed086p99.html:text/html;Ben-Naim - 2009 - An Informational–Theoretical Formulation of the Se.pdf:/Users/bert/Zotero/storage/79RVM47Z/Ben-Naim - 2009 - An Informational–Theoretical Formulation of the Se.pdf:application/pdf},
}

@article{haddad_thermodynamics:_2017,
	title = {Thermodynamics: The Unique Universal Science},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/19/11/621},
	doi = {10.3390/e19110621},
	shorttitle = {Thermodynamics},
	abstract = {Thermodynamics is a physical branch of science that governs the thermal behavior of dynamical systems from those as simple as refrigerators to those as complex as our expanding universe. The laws of thermodynamics involving conservation of energy and nonconservation of entropy are, without a doubt, two of the most useful and general laws in all sciences. The first law of thermodynamics, according to which energy cannot be created or destroyed, merely transformed from one form to another, and the second law of thermodynamics, according to which the usable energy in an adiabatically isolated dynamical system is always diminishing in spite of the fact that energy is conserved, have had an impact far beyond science and engineering. In this paper, we trace the history of thermodynamics from its classical to its postmodern forms, and present a tutorial and didactic exposition of thermodynamics as it pertains to some of the deepest secrets of the universe.},
	pages = {621},
	number = {11},
	journaltitle = {Entropy},
	author = {Haddad, Wassim M.},
	urldate = {2018-05-05},
	date = {2017-11-17},
	langid = {english},
	keywords = {life, entropy, and death, classical thermodynamics, consciousness, dynamical systems theory, entropic arrow of time, gravity, relativity theory, statistical thermodynamics, system thermodynamics},
	file = {Haddad - 2017 - Thermodynamics The Unique Universal Science.pdf:/Users/bert/Zotero/storage/729GG8SC/Haddad - 2017 - Thermodynamics The Unique Universal Science.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/LLQEPUJA/621.html:text/html},
}

@article{sengupta_how_2018,
	title = {How Robust are Deep Neural Networks?},
	url = {http://arxiv.org/abs/1804.11313},
	abstract = {Convolutional and Recurrent, deep neural networks have been successful in machine learning systems for computer vision, reinforcement learning, and other allied fields. However, the robustness of such neural networks is seldom apprised, especially after high classification accuracy has been attained. In this paper, we evaluate the robustness of three recurrent neural networks to tiny perturbations, on three widely used datasets, to argue that high accuracy does not always mean a stable and a robust (to bounded perturbations, adversarial attacks, etc.) system. Especially, normalizing the spectrum of the discrete recurrent network to bound the spectrum (using power method, Rayleigh quotient, etc.) on a unit disk produces stable, albeit highly non-robust neural networks. Furthermore, using the \${\textbackslash}epsilon\$-pseudo-spectrum, we show that training of recurrent networks, say using gradient-based methods, often result in non-normal matrices that may or may not be diagonalizable. Therefore, the open problem lies in constructing methods that optimize not only for accuracy but also for the stability and the robustness of the underlying neural network, a criterion that is distinct from the other.},
	journaltitle = {{arXiv}:1804.11313 [cs, math, stat]},
	author = {Sengupta, Biswa and Friston, Karl J.},
	urldate = {2018-05-05},
	date = {2018-04-30},
	eprinttype = {arxiv},
	eprint = {1804.11313},
	keywords = {Mathematics - Dynamical Systems, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2RBF9HGW/1804.html:text/html;Sengupta and Friston - 2018 - How Robust are Deep Neural Networks.pdf:/Users/bert/Zotero/storage/26U38MT9/Sengupta and Friston - 2018 - How Robust are Deep Neural Networks.pdf:application/pdf},
}

@article{aubreville_deep_2018,
	title = {Deep Denoising for Hearing Aid Applications},
	url = {http://arxiv.org/abs/1805.01198},
	abstract = {Reduction of unwanted environmental noises is an important feature of today's hearing aids ({HA}), which is why noise reduction is nowadays included in almost every commercially available device. The majority of these algorithms, however, is restricted to the reduction of stationary noises. In this work, we propose a denoising approach based on a three hidden layer fully connected deep learning network that aims to predict a Wiener filtering gain with an asymmetric input context, enabling real-time applications with high constraints on signal delay. The approach is employing a hearing instrument-grade filter bank and complies with typical hearing aid demands, such as low latency and on-line processing. It can further be well integrated with other algorithms in an existing {HA} signal processing chain. We can show on a database of real world noise signals that our algorithm is able to outperform a state of the art baseline approach, both using objective metrics and subject tests.},
	journaltitle = {{arXiv}:1805.01198 [cs, eess]},
	author = {Aubreville, Marc and Ehrensperger, Kai and Rosenkranz, Tobias and Graf, Benjamin and Puder, Henning and Maier, Andreas},
	urldate = {2018-05-05},
	date = {2018-05-03},
	eprinttype = {arxiv},
	eprint = {1805.01198},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/J26BKJ6W/1805.html:text/html;Aubreville et al. - 2018 - Deep Denoising for Hearing Aid Applications.pdf:/Users/bert/Zotero/storage/2G828JQ9/Aubreville et al. - 2018 - Deep Denoising for Hearing Aid Applications.pdf:application/pdf},
}

@article{eysenbach_diversity_2018,
	title = {Diversity is All You Need: Learning Skills without a Reward Function},
	url = {http://arxiv.org/abs/1802.06070},
	shorttitle = {Diversity is All You Need},
	abstract = {Intelligent creatures can explore their environments and learn useful skills without supervision. In this paper, we propose {DIAYN} ("Diversity is All You Need"), a method for learning useful skills without a reward function. Our proposed method learns skills by maximizing an information theoretic objective using a maximum entropy policy. On a variety of simulated robotic tasks, we show that this simple objective results in the unsupervised emergence of diverse skills, such as walking and jumping. In a number of reinforcement learning benchmark environments, our method is able to learn a skill that solves the benchmark task despite never receiving the true task reward. In these environments, some of the learned skills correspond to solving the task, and each skill that solves the task does so in a distinct manner. Our results suggest that unsupervised discovery of skills can serve as an effective pretraining mechanism for overcoming challenges of exploration and data efficiency in reinforcement learning},
	journaltitle = {{arXiv}:1802.06070 [cs]},
	author = {Eysenbach, Benjamin and Gupta, Abhishek and Ibarz, Julian and Levine, Sergey},
	urldate = {2018-05-03},
	date = {2018-02-16},
	eprinttype = {arxiv},
	eprint = {1802.06070},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv\:1802.06070 PDF:/Users/bert/Zotero/storage/SX6G87CZ/Eysenbach e.a. - 2018 - Diversity is All You Need Learning Skills without.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/MHLIBBAB/1802.html:text/html},
}

@article{grant_recasting_2018,
	title = {Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
	url = {http://arxiv.org/abs/1801.08930},
	abstract = {Meta-learning allows an intelligent agent to leverage prior learning episodes as a basis for quickly improving performance on a novel task. Bayesian hierarchical modeling provides a theoretical framework for formalizing meta-learning as inference for a set of parameters that are shared across tasks. Here, we reformulate the model-agnostic meta-learning algorithm ({MAML}) of Finn et al. (2017) as a method for probabilistic inference in a hierarchical Bayesian model. In contrast to prior methods for meta-learning via hierarchical Bayes, {MAML} is naturally applicable to complex function approximators through its use of a scalable gradient descent procedure for posterior inference. Furthermore, the identification of {MAML} as hierarchical Bayes provides a way to understand the algorithm's operation as a meta-learning procedure, as well as an opportunity to make use of computational strategies for efficient inference. We use this opportunity to propose an improvement to the {MAML} algorithm that makes use of techniques from approximate inference and curvature estimation.},
	journaltitle = {{arXiv}:1801.08930 [cs]},
	author = {Grant, Erin and Finn, Chelsea and Levine, Sergey and Darrell, Trevor and Griffiths, Thomas},
	urldate = {2018-05-03},
	date = {2018-01-26},
	eprinttype = {arxiv},
	eprint = {1801.08930},
	keywords = {Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UGNYKTS8/1801.html:text/html;Grant e.a. - 2018 - Recasting Gradient-Based Meta-Learning as Hierarch.pdf:/Users/bert/Zotero/storage/4LKVJ3UR/Grant e.a. - 2018 - Recasting Gradient-Based Meta-Learning as Hierarch.pdf:application/pdf},
}

@article{grimm_toolbox_2018,
	title = {A toolbox for rendering virtual acoustic environments in the context of audiology},
	url = {http://arxiv.org/abs/1804.11300},
	abstract = {A toolbox for creation and rendering of dynamic virtual acoustic environments ({TASCAR}) that allows direct user interaction was developed for application in hearing aid research and audiology. This technical paper describes the general software structure and the time-domain simulation methods, i.e., transmission model, image source model, and render formats, used to produce virtual acoustic environments with moving objects. Implementation-specific properties are described, and the computational performance of the system was measured as a function of simulation complexity. Results show that on commercially available commonly used hardware the simulation of several hundred virtual sound sources is possible in the time domain.},
	journaltitle = {{arXiv}:1804.11300 [cs, eess]},
	author = {Grimm, Giso and Luberadzka, Joanna and Hohmann, Volker},
	urldate = {2018-05-02},
	date = {2018-04-30},
	eprinttype = {arxiv},
	eprint = {1804.11300},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2QCJHF5Y/1804.html:text/html;Grimm et al. - 2018 - A toolbox for rendering virtual acoustic environme.pdf:/Users/bert/Zotero/storage/QLXUYTEP/Grimm et al. - 2018 - A toolbox for rendering virtual acoustic environme.pdf:application/pdf},
}

@article{friston_am_2018,
	title = {Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?)},
	volume = {9},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00579/full?utm_source=F-NTF&utm_medium=EMLX&utm_campaign=PRD_FEOPS_20170000_ARTICLE},
	doi = {10.3389/fpsyg.2018.00579},
	shorttitle = {Am I Self-Conscious?},
	abstract = {Is self-consciousness necessary for consciousness? The answer is yes. So there you have it – the answer is yes. This was my response to a question I was asked to address in a recent {AEON} piece (https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference). What follows is based upon the notes for that essay, with a special focus on self-organisation, self-evidencing and self-modelling. I will try to substantiate my (polemic) answer from the perspective of a physicist. In brief, the argument goes as follows: if we want to talk about creatures, like ourselves, then we have to identify the characteristic behaviours they must exhibit. This is fairly easy to do by noting that living systems return to a set of attracting states time and time again. Mathematically, this implies the existence of a Lyapunov function that turns out to be model evidence (i.e., self-evidence) in Bayesian statistics or surprise (i.e., self-information) in information theory. This means that all biological processes can be construed as performing some form of inference, from evolution through to conscious processing. If this is the case, at what point do we invoke consciousness? The proposal on offer here is that the mind comes into being when self-evidencing has a temporal thickness or counterfactual depth, which grounds inferences about the consequences of my action. On this view, consciousness is nothing more than inference about my future; namely, the self-evidencing consequences of what I could do.},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Friston, Karl},
	urldate = {2018-04-24},
	date = {2018},
	keywords = {active inference, free energy, Bayesian, Model selection, predictive coding, Consciousness, processing, Variational},
	file = {Friston - 2018 - Am I Self-Conscious (Or Does Self-Organization En.pdf:/Users/bert/Zotero/storage/72QH7HXL/Friston - 2018 - Am I Self-Conscious (Or Does Self-Organization En.pdf:application/pdf},
}

@article{such_deep_2017,
	title = {Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
	url = {http://arxiv.org/abs/1712.06567},
	shorttitle = {Deep Neuroevolution},
	abstract = {Deep artificial neural networks ({DNNs}) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies ({ES}) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning ({RL}) problems. However, {ES} can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at {DNN} scales. Here we demonstrate they can: we evolve the weights of a {DNN} with a simple, gradient-free, population-based genetic algorithm ({GA}) and it performs well on hard deep {RL} problems, including Atari and humanoid locomotion. The Deep {GA} successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which {GAs} can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on {RL} problems. To demonstrate the latter, we show that combining {DNNs} with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. {DQN}, A3C, {ES}, and the {GA}) fail. Additionally, the Deep {GA} parallelizes better than {ES}, A3C, and {DQN}, and enables a state-of-the-art compact encoding technique that can represent million-parameter {DNNs} in thousands of bytes.},
	journaltitle = {{arXiv}:1712.06567 [cs]},
	author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
	urldate = {2018-04-23},
	date = {2017-12-18},
	eprinttype = {arxiv},
	eprint = {1712.06567},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3CE75INW/1712.html:text/html;Such e.a. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf:/Users/bert/Zotero/storage/Z2I7ETQY/Such e.a. - 2017 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf:application/pdf},
}

@article{bartolotta_bayesian_2016,
	title = {The Bayesian Second Law of Thermodynamics},
	volume = {94},
	issn = {2470-0045, 2470-0053},
	url = {http://arxiv.org/abs/1508.02421},
	doi = {10.1103/PhysRevE.94.022102},
	abstract = {We derive a generalization of the Second Law of Thermodynamics that uses Bayesian updates to explicitly incorporate the effects of a measurement of a system at some point in its evolution. By allowing an experimenter's knowledge to be updated by the measurement process, this formulation resolves a tension between the fact that the entropy of a statistical system can sometimes fluctuate downward and the information-theoretic idea that knowledge of a stochastically-evolving system degrades over time. The Bayesian Second Law can be written as \${\textbackslash}Delta H({\textbackslash}rho\_m, {\textbackslash}rho) + {\textbackslash}langle {\textbackslash}mathcal\{Q\}{\textbackslash}rangle\_\{F{\textbar}m\}{\textbackslash}geq 0\$, where \${\textbackslash}Delta H({\textbackslash}rho\_m, {\textbackslash}rho)\$ is the change in the cross entropy between the original phase-space probability distribution \${\textbackslash}rho\$ and the measurement-updated distribution \${\textbackslash}rho\_m\$, and \${\textbackslash}langle {\textbackslash}mathcal\{Q\}{\textbackslash}rangle\_\{F{\textbar}m\}\$ is the expectation value of a generalized heat flow out of the system. We also derive refined versions of the Second Law that bound the entropy increase from below by a non-negative number, as well as Bayesian versions of the Jarzynski equality. We demonstrate the formalism using simple analytical and numerical examples.},
	number = {2},
	journaltitle = {Physical Review E},
	author = {Bartolotta, Anthony and Carroll, Sean M. and Leichenauer, Stefan and Pollack, Jason},
	urldate = {2018-04-14},
	date = {2016-08-01},
	eprinttype = {arxiv},
	eprint = {1508.02421},
	keywords = {Condensed Matter - Statistical Mechanics, High Energy Physics - Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UIY4EFHC/1508.html:text/html;Bartolotta et al. - 2016 - The Bayesian Second Law of Thermodynamics.pdf:/Users/bert/Zotero/storage/7IXURZXB/Bartolotta et al. - 2016 - The Bayesian Second Law of Thermodynamics.pdf:application/pdf},
}

@article{lamont_correspondence_2017,
	title = {A correspondence between thermodynamics and inference},
	url = {http://arxiv.org/abs/1706.01428},
	abstract = {We explore a natural analogy between Bayesian statistics and thermal physics in which sample size corresponds to inverse temperature. This analogy motivates the definition of two novel statistical quantities: a learning capacity and a Gibbs entropy. The analysis of the learning capacity, corresponding to the heat capacity in thermal physics, leads to a critical insight into why some models have anomalously good learning performance. The mechanism is a statistical analogue of the failure of the equipartition theorem formula for the heat capacity. We explore the properties of the learning capacity in a number of examples, including a sloppy model. We also propose that the Gibbs entropy provides a natural device for counting distinguishable distributions in the context of Bayesian inference. This insight results in a new solution to a long-standing problem in Bayesian inference: the definition of an objective or uninformative prior. We use the Gibbs entropy to define a generalized principle of indifference ({GPI}) in which every distinguishable model is assigned equal a priori probability. This approach both resolves a number of long standing inconsistencies in objective Bayesian inference and unifies several seemingly unrelated Bayesian methods with the information-based paradigm of inference.},
	journaltitle = {{arXiv}:1706.01428 [physics, stat]},
	author = {{LaMont}, Colin H. and Wiggins, Paul A.},
	urldate = {2018-04-14},
	date = {2017-06-05},
	eprinttype = {arxiv},
	eprint = {1706.01428},
	keywords = {Mathematics - Statistics Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/C6TPVDJJ/1706.html:text/html;LaMont and Wiggins - 2017 - A correspondence between thermodynamics and infere.pdf:/Users/bert/Zotero/storage/ZTL9TID5/LaMont and Wiggins - 2017 - A correspondence between thermodynamics and infere.pdf:application/pdf},
}

@article{wolpert_free_2016,
	title = {The Free Energy Requirements of Biological Organisms; Implications for Evolution},
	volume = {18},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/18/4/138},
	doi = {10.3390/e18040138},
	abstract = {Recent advances in nonequilibrium statistical physics have provided unprecedented insight into the thermodynamics of dynamic processes. The author recently used these advances to extend Landauer’s semi-formal reasoning concerning the thermodynamics of bit erasure, to derive the minimal free energy required to implement an arbitrary computation. Here, I extend this analysis, deriving the minimal free energy required by an organism to run a given (stochastic) map π from its sensor inputs to its actuator outputs. I use this result to calculate the input-output map π of an organism that optimally trades off the free energy needed to run π with the phenotypic fitness that results from implementing π. I end with a general discussion of the limits imposed on the rate of the terrestrial biosphere’s information processing by the flux of sunlight on the Earth.},
	pages = {138},
	number = {4},
	journaltitle = {Entropy},
	author = {Wolpert, David H.},
	urldate = {2018-04-13},
	date = {2016-04-13},
	langid = {english},
	keywords = {information processing rate of the biosphere, Landauer bound, thermodynamics of computation},
	file = {Snapshot:/Users/bert/Zotero/storage/8BZT3FFR/htm.html:text/html;Wolpert - 2016 - The Free Energy Requirements of Biological Organis.pdf:/Users/bert/Zotero/storage/MK3FLVPW/Wolpert - 2016 - The Free Energy Requirements of Biological Organis.pdf:application/pdf},
}

@article{grau-moya_non-equilibrium_2017,
	title = {Non-Equilibrium Relations for Bounded Rational Decision-Making in Changing Environments},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/20/1/1},
	doi = {10.3390/e20010001},
	abstract = {Living organisms from single cells to humans need to adapt continuously to respond to changes in their environment. The process of behavioural adaptation can be thought of as improving decision-making performance according to some utility function. Here, we consider an abstract model of organisms as decision-makers with limited information-processing resources that trade off between maximization of utility and computational costs measured by a relative entropy, in a similar fashion to thermodynamic systems undergoing isothermal transformations. Such systems minimize the free energy to reach equilibrium states that balance internal energy and entropic cost. When there is a fast change in the environment, these systems evolve in a non-equilibrium fashion because they are unable to follow the path of equilibrium distributions. Here, we apply concepts from non-equilibrium thermodynamics to characterize decision-makers that adapt to changing environments under the assumption that the temporal evolution of the utility function is externally driven and does not depend on the decision-maker’s action. This allows one to quantify performance loss due to imperfect adaptation in a general manner and, additionally, to find relations for decision-making similar to Crooks’ fluctuation theorem and Jarzynski’s equality. We provide simulations of several exemplary decision and inference problems in the discrete and continuous domains to illustrate the new relations.},
	pages = {1},
	number = {1},
	journaltitle = {Entropy},
	author = {Grau-Moya, Jordi and Krüger, Matthias and Braun, Daniel A.},
	urldate = {2018-04-13},
	date = {2017-12-21},
	langid = {english},
	keywords = {free energy, bounded rationality, adaptation, anticipation},
	file = {Grau-Moya et al. - 2017 - Non-Equilibrium Relations for Bounded Rational Dec.pdf:/Users/bert/Zotero/storage/XBRWRY6T/Grau-Moya et al. - 2017 - Non-Equilibrium Relations for Bounded Rational Dec.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/WLPZGLBR/1.html:text/html},
}

@inproceedings{hong_bayesian_2016,
	location = {New York, New York, {USA}},
	title = {Bayesian Reinforcement Learning with Behavioral Feedback},
	isbn = {978-1-57735-770-4},
	url = {http://dl.acm.org/citation.cfm?id=3060832.3060840},
	series = {{IJCAI}'16},
	abstract = {In the standard reinforcement learning setting, the agent learns optimal policy solely from state transitions and rewards from the environment. We consider an extended setting where a trainer additionally provides feedback on the actions executed by the agent. This requires appropriately incorporating the feedback, even when the feedback is not necessarily accurate. In this paper, we present a Bayesian approach to this extended reinforcement learning setting. Specifically, we extend Kalman Temporal Difference learning to compute the posterior distribution over Q-values given the state transitions and rewards from the environment as well as the feedback from the trainer. Through experiments on standard reinforcement learning tasks, we show that learning performance can be significantly improved even with inaccurate feedback.},
	pages = {1571--1577},
	booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Hong, Teakgyu and Lee, Jongmin and Kim, Kee-Eung and Ortega, Pedro A. and Lee, Daniel},
	urldate = {2018-04-13},
	date = {2016},
	file = {Hong et al. - 2016 - Bayesian Reinforcement Learning with Behavioral Fe.pdf:/Users/bert/Zotero/storage/EFXCN9EC/Hong et al. - 2016 - Bayesian Reinforcement Learning with Behavioral Fe.pdf:application/pdf},
}

@article{jaynes_gibbs_1965,
	title = {Gibbs vs Boltzmann Entropies},
	volume = {33},
	issn = {0002-9505},
	url = {http://adsabs.harvard.edu/abs/1965AmJPh..33..391J},
	doi = {10.1119/1.1971557},
	abstract = {The status of the Gibbs and Boltzmann expressions for entropy has been a matter of some confusion in the literature. We show that: (1) the Gibbs H function yields the correct entropy as defined in phenomenological thermodynamics; (2) the Boltzmann H yields an "entropy" that is in error by a nonnegligible amount whenever interparticle forces affect
thermodynamic properties; (3) Boltzmann's other interpretation of entropy, S = k log W, is consistent with the Gibbs H, and derivable from it; (4) the Boltzmann H theorem does not constitute a demonstration of the second law for dilute gases; (5) the dynamical invariance of the Gibbs H gives a simple proof of the second law for arbitrary
interparticle forces; (6) the second law is a special case of a general requirement for any macroscopic process to be experimentally
reproducible. Finally, the "anthropomorphic" nature of entropy, on both the statistical and phenomenological levels, is stressed.},
	pages = {391--398},
	journaltitle = {American Journal of Physics},
	shortjournal = {American Journal of Physics},
	author = {Jaynes, E. T.},
	urldate = {2018-04-13},
	date = {1965-05-01},
	file = {Jaynes - 1965 - Gibbs vs Boltzmann Entropies.pdf:/Users/bert/Zotero/storage/4S7LWSA3/Jaynes - 1965 - Gibbs vs Boltzmann Entropies.pdf:application/pdf},
}

@book{brewer_bayesian_nodate,
	title = {Bayesian inference and computation: a beginner’s guide},
	shorttitle = {1Bayesian inference and computation},
	abstract = {Most scientific observations are not sufficient to give us definite answers to all our questions. It is rare that we get a dataset which totally answers every question with certainty. Even if that did happen, we would quickly move on to other questions. What a dataset usually can do is make hypotheses more},
	author = {Brewer, Brendon J.},
	file = {Brewer - Bayesian inference and computation a beginner’s g.pdf:/Users/bert/Zotero/storage/6TSGWU2U/Brewer - Bayesian inference and computation a beginner’s g.pdf:application/pdf;Citeseer - Snapshot:/Users/bert/Zotero/storage/Q73Z9TJU/summary.html:text/html},
}

@article{higson_dynamic_2017,
	title = {Dynamic nested sampling: an improved algorithm for parameter estimation and evidence calculation},
	url = {http://arxiv.org/abs/1704.03459},
	shorttitle = {Dynamic nested sampling},
	abstract = {We introduce dynamic nested sampling: a generalisation of the nested sampling algorithm in which the number of "live points" varies to allocate samples more efficiently. In empirical tests the new method increases parameter estimation accuracy by up to a factor of {\textasciitilde}8 compared to standard nested sampling with the same number of samples - equivalent to speeding up the computation by a factor of {\textasciitilde}64. We also show that the accuracy of Bayesian evidence estimates can be improved, and that both can be improved simultaneously. Unlike in standard nested sampling more accurate results can be obtained by continuing the calculation for longer. We have implemented dynamic nested sampling in the publicly available {PerfectNS} package, which is used for the numerical tests in this paper. Our approach can be easily included in popular nested sampling software packages such as {MultiNest} and {PolyChord}.},
	journaltitle = {{arXiv}:1704.03459 [astro-ph, physics:physics, stat]},
	author = {Higson, Edward and Handley, Will and Hobson, Mike and Lasenby, Anthony},
	urldate = {2018-04-13},
	date = {2017-04-11},
	eprinttype = {arxiv},
	eprint = {1704.03459},
	keywords = {Statistics - Computation, Statistics - Methodology, Astrophysics - Instrumentation and Methods for Astrophysics, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/66CEYMH5/1704.html:text/html;Higson et al. - 2017 - Dynamic nested sampling an improved algorithm for.pdf:/Users/bert/Zotero/storage/LKEPG6DA/Higson et al. - 2017 - Dynamic nested sampling an improved algorithm for.pdf:application/pdf},
}

@article{fry_computation_2015,
	title = {Computation by Biological Systems},
	volume = {1},
	url = {http://www.mdtcanada.ca/bmst/v1/e3.pdf},
	journaltitle = {Biomedical Sciences Today},
	author = {Fry, Robert L.},
	date = {2015},
	file = {Fry - 2015 - Computation by Biological Systems.pdf:/Users/bert/Zotero/storage/LQS2VW76/Fry - 2015 - Computation by Biological Systems.pdf:application/pdf},
}

@inproceedings{anderson_energy_2016,
	title = {Energy efficiency limits in approximate computing: A fundamental physical perspective},
	doi = {10.1109/ACSSC.2016.7869661},
	shorttitle = {Energy efficiency limits in approximate computing},
	abstract = {Approximate computing offers pathways to increased energy efficiency in computational settings where perfect accuracy is not required. In this work, we explore fundamental links between approximation, information loss, and limiting energy efficiency of computation. We distinguish physical and semantic aspects of approximation in digital computation, and show how the physical aspects - whether of deterministic or non-deterministic origin - can be quantified at a fundamental physical level in physical-information-theoretic measures that directly link loss of input information to energy dissipation. We then present a fundamental energy efficiency bound that explicitly incorporates these measures, and illustrate its evaluation in simple examples involving approximate adders.},
	eventtitle = {2016 50th Asilomar Conference on Signals, Systems and Computers},
	pages = {1653--1657},
	booktitle = {2016 50th Asilomar Conference on Signals, Systems and Computers},
	author = {Anderson, N. G.},
	date = {2016-11},
	keywords = {Entropy, information theory, adders, approximate adders, approximate computing, Approximate computing, computational settings, Computers, digital computation, energy conservation, energy dissipation, Energy dissipation, energy efficiency limits, Frequency measurement, fundamental energy efficiency, information loss, information theoretic measures, Limiting, limiting energy efficiency, nondeterministic origin, Registers, semantic aspects},
	file = {Anderson - 2016 - Energy efficiency limits in approximate computing.pdf:/Users/bert/Zotero/storage/KVTIE2YK/Anderson - 2016 - Energy efficiency limits in approximate computing.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/5JI5RV5T/7869661.html:text/html},
}

@misc{ganesh_intention_nodate,
	title = {Intention is Physical},
	url = {https://fqxi.org/community/forum/topic/2778},
	abstract = {Essay {AbstractIn} this essay, I will present the fundamental relationship between energy dissipation and learning dynamics in physical systems. I will use this relationship to explain how intention is physical, and present recent results from non-equilibrium thermodynamics to unify individual learnin via @{FQXi}},
	author = {Ganesh, Natesh},
	urldate = {2018-03-30},
	langid = {american},
	file = {Intention is Physical by Natesh Ganesh.pdf:/Users/bert/Zotero/storage/Y87DWEP6/Intention is Physical by Natesh Ganesh.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UXIP9GDK/2778.html:text/html},
}

@article{ben-naim_entropy_2017,
	title = {Entropy, Shannon’s Measure of Information and Boltzmann’s H-Theorem},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/19/2/48},
	doi = {10.3390/e19020048},
	abstract = {We start with a clear distinction between Shannon’s Measure of Information ({SMI}) and the Thermodynamic Entropy. The first is defined on any probability distribution; and therefore it is a very general concept. On the other hand Entropy is defined on a very special set of distributions. Next we show that the Shannon Measure of Information ({SMI}) provides a solid and quantitative basis for the interpretation of the thermodynamic entropy. The entropy measures the uncertainty in the distribution of the locations and momenta of all the particles; as well as two corrections due to the uncertainty principle and the indistinguishability of the particles. Finally we show that the H-function as defined by Boltzmann is an {SMI} but not entropy. Therefore; much of what has been written on the H-theorem is irrelevant to entropy and the Second Law of Thermodynamics.},
	pages = {48},
	number = {2},
	journaltitle = {Entropy},
	author = {Ben-Naim, Arieh},
	urldate = {2018-03-30},
	date = {2017-01-24},
	langid = {english},
	keywords = {entropy, H-theorem, Second Law of Thermodynamics, Shannon’s measure of information},
	file = {Ben-Naim - 2017 - Entropy, Shannon’s Measure of Information and Bolt.pdf:/Users/bert/Zotero/storage/RMYPRTET/Ben-Naim - 2017 - Entropy, Shannon’s Measure of Information and Bolt.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/N5ZZMQK5/48.html:text/html},
}

@article{kafri_novel_2016,
	title = {A Novel Approach to Probability},
	volume = {06},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/journal/PaperInformation.aspx?PaperID=64966&#abstract},
	doi = {10.4236/apm.2016.64017},
	abstract = {When P indistinguishable balls are randomly distributed among L distinguishable boxes, and considering the dense system , our natural intuition tells us that the box with the average number of balls P/L has the highest probability and that none of boxes are empty; however in reality, the probability of the empty box is always the highest. This fact is with contradistinction to sparse system (i.e. energy distribution in gas) in which the average value has the highest probability. Here we show that when we postulate the requirement that all possible configurations of balls in the boxes have equal probabilities, a realistic “long tail” distribution is obtained. This formalism when applied for sparse systems converges to distributions in which the average is preferred. We calculate some of the distributions resulted from this postulate and obtain most of the known distributions in nature, namely: Zipf’s law, Benford’s law, particles energy distributions, and more. Further generalization of this novel approach yields not only much better predictions for elections, polls, market share distribution among competing companies and so forth, but also a compelling probabilistic explanation for Planck’s famous empirical finding that the energy of a photon is hv.},
	pages = {201},
	number = {4},
	journaltitle = {Advances in Pure Mathematics},
	author = {Kafri, Oded},
	urldate = {2018-03-30},
	date = {2016-03-15},
	langid = {english},
	file = {Kafri - 2016 - A Novel Approach to Probability.pdf:/Users/bert/Zotero/storage/BP82GKWR/Kafri - 2016 - A Novel Approach to Probability.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/VW5CB49D/PaperInformation.html:text/html},
}

@inproceedings{jaynes_evolution_1984,
	location = {Orsay, France},
	title = {The Evolution Of Carnot's Principle},
	doi = {10.1007/978-94-009-3049-0_15},
	abstract = {: We trace the development of the technical ideas showing that the Second Law of Thermodynamics became, over a Century ago, a general principle of reasoning, applicable to scientific inference in other fields than thermodynamics. Both the logic and the procedure of our present maximum entropy applications are easily recognized in the methods for predicting equilibrium conditions introduced by Gibbs in 1875. Chemical thermodynamics has been based on them ever since. What is new in this field is not the method, but the recognition of its generality. 1. {INTRODUCTION} 2 2. {CARNOT}'S {PRINCIPLE} 3 3. {FIRST} {METAMORPHOSIS}: {KELVIN} 4 4. {SECOND} {METAMORPHOSIS}: {CLAUSIUS} 6 5. {THIRD} {METAMORPHOSIS}: {GIBBS} 7 6. {FOURTH} {METAMORPHOSIS}: {BOLTZMANN} 8 7. {CONCLUSION} 11 {APPENDIX} A: {COMMENTS} {ON} {KELVIN}'S {RELATION} 14 {APPENDIX} B: {ANTI}--{CARNOT} {ENGINES} 15 {APPENDIX} C: {REVERSIBILITY} 16 {REFERENCES} 17 *The opening talk at the {EMBO} Workshop on Maximum--Entropy Methods in x--ray crystallographic and biological macromolecule st...},
	eventtitle = {{EMBO} Workshop on Maximum Entropy Methods in x-ray crystallographic and biological macromolecule structure determination},
	author = {Jaynes, E.T.},
	date = {1984},
	file = {Jaynes - 1984 - The Evolution Of Carnot's Principle.pdf:/Users/bert/Zotero/storage/Q6DJG6H7/Jaynes - 1984 - The Evolution Of Carnot's Principle.pdf:application/pdf},
}

@article{lee_hierarchical_2003,
	title = {Hierarchical Bayesian Inference in the Visual Cortex},
	volume = {20},
	abstract = {this paper, we propose a Bayesian theory of hierarchical cortical computation based both on (a) the mathematical and computational ideas of computer vision and pattern the- ory and on (b) recent neurophysiological experimental evidence. We ,2 have proposed that Grenander's pattern theory 3 could potentially model the brain as a generafive model in such a way that feedback serves to disambiguate and 'explain away' the earlier representa- tion. The Helmholtz machine 4, 5 was an excellent step towards approximating this proposal, with feedback implementing priors. Its development, however, was rather limited, dealing only with binary images. Moreover, its feedback mechanisms were engaged only during the learning of the feedforward connections but not during perceptual inference, though the Gibbs sampling process for inference can potentially be interpreted as top-down feedback disambiguating low level representations? Rao and Ballard's predictive coding/Kalman filter model 6 did integrate generafive feedback in the perceptual inference process, but it was primarily a linear model and thus severely limited in practical utility. The data-driven Markov Chain Monte Carlo approach of Zhu and colleagues 7, 8 might be the most successful recent application of this proposal in solving real and difficult computer vision problems using generafive models, though its connection to the visual cortex has not been explored. Here, we bring in a powerful and widely applicable paradigm from artificial intelligence and computer vision to propose some new ideas about the algorithms of visual cortical process- ing and the nature of representations in the visual cortex. We will review some of our and others' neurophysiological experimental data to lend support to these ideas},
	number = {7},
	journaltitle = {Journal Optical Society of America},
	author = {Lee, Tai Sing and Mumford, David},
	date = {2003},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/FAWH5Q2Z/summary.html:text/html;Lee and Mumford - 2003 - Hierarchical Bayesian Inference in the Visual Cort.pdf:/Users/bert/Zotero/storage/95UW9Z6U/Lee and Mumford - 2003 - Hierarchical Bayesian Inference in the Visual Cort.pdf:application/pdf},
}

@article{moens_variational_2017,
	title = {Variational Treatment of Trial-by-Trial Drift-Diffusion Models of Behaviour},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/16/220517},
	doi = {10.1101/220517},
	abstract = {Drift-Diffusion Models ({DDM}) are widely used to model behaviour. Accounting for trial-to-trial variations of the parameters in {DDM} leads to better fits and allows researchers to investigate variations of parameters over time. However, parameter estimation with conventional integration techniques is slow and cannot be applied to large problems. Here we propose to rely on Variational Bayesian inference to estimate the posterior probability of the parameters at the local (trial) and global (subject and population) levels. We present a data-driven model that uses Inference Networks to estimate an approximate distribution for each of the local parameters given an arbitrary set of regressors. We apply the method to simulated and real datasets and show that it converges within reasonable time frame, achieves good fitting performance and highlights complex auto-correlation structures between trials. Using simulations, we show that our approach outperforms the gold-standard Linear-Regression Model, estimated both using Variational Inference and Markov-Chain Monte-Carlo method. Therefore, the present Bayesian approach, relying on variational inference networks, is a promising method to solve trial-by-trial {DDM}.},
	pages = {220517},
	journaltitle = {{bioRxiv}},
	author = {Moens, Vincent and Zenon, Alexandre},
	urldate = {2018-03-27},
	date = {2017-11-16},
	langid = {english},
	file = {Moens en Zenon - 2017 - Variational Treatment of Trial-by-Trial Drift-Diff.pdf:/Users/bert/Zotero/storage/PFIIVTSA/Moens en Zenon - 2017 - Variational Treatment of Trial-by-Trial Drift-Diff.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/LRJ8TCUL/220517.html:text/html},
}

@article{fry_physical_2017,
	title = {Physical Intelligence and Thermodynamic Computing},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/19/3/107},
	doi = {10.3390/e19030107},
	abstract = {This paper proposes that intelligent processes can be completely explained by thermodynamic principles. They can equally be described by information-theoretic principles that, from the standpoint of the required optimizations, are functionally equivalent. The underlying theory arises from two axioms regarding distinguishability and causality. Their consequence is a theory of computation that applies to the only two kinds of physical processes possible—those that reconstruct the past and those that control the future. Dissipative physical processes fall into the first class, whereas intelligent ones comprise the second. The first kind of process is exothermic and the latter is endothermic. Similarly, the first process dumps entropy and energy to its environment, whereas the second reduces entropy while requiring energy to operate. It is shown that high intelligence efficiency and high energy efficiency are synonymous. The theory suggests the usefulness of developing a new computing paradigm called Thermodynamic Computing to engineer intelligent processes. The described engineering formalism for the design of thermodynamic computers is a hybrid combination of information theory and thermodynamics. Elements of the engineering formalism are introduced in the reverse-engineer of a cortical neuron. The cortical neuron provides perhaps the simplest and most insightful example of a thermodynamic computer possible. It can be seen as a basic building block for constructing more intelligent thermodynamic circuits.},
	pages = {107},
	number = {3},
	journaltitle = {Entropy},
	author = {Fry, Robert L.},
	urldate = {2018-03-27},
	date = {2017-03-09},
	langid = {english},
	keywords = {entropy, Carnot cycle, causality, distinguishability, intelligent processes, questions},
	file = {Fry - 2017 - Physical Intelligence and Thermodynamic Computing.pdf:/Users/bert/Zotero/storage/37JU3HDS/Fry - 2017 - Physical Intelligence and Thermodynamic Computing.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/36H7HCL5/107.html:text/html},
}

@article{baltieri_probabilistic_2018,
	title = {A probabilistic interpretation of {PID} controllers using active inference},
	rights = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/03/19/284562},
	doi = {10.1101/284562},
	abstract = {In the past few decades, probabilistic interpretations of brain functions have become widespread in cognitive science and neuroscience. The Bayesian brain hypothesis, predictive coding, the free energy principle and active inference are increasingly popular theories of cognitive functions that claim to unify understandings of life and cognition within general mathematical frameworks derived from information theory, statistical physics and machine learning. Furthermore, it has been argued that one such proposal, active inference, combines both information and control theory and has its roots in cybernetics studies of the brain. The connections between information and control theory have been discussed since the 1950's by scientists like Shannon and Kalman and have recently risen to prominence in modern stochastic optimal control theory. However, the implications of the confluence of these two theoretical frameworks for the biological sciences have been slow to emerge. Here we argue that if the active inference proposal is to be taken as a general process theory for biological systems, we need to consider how existing control theoretical approaches to biological systems relate to it. In this work we will focus on {PID} (Proportional-Integral-Derivative) controllers, one of the most common types of regulators employed in engineering and more recently used to explain behaviour in biological systems, e.g. chemotaxis in bacteria and amoebae or robust adaptation in biochemical networks. Using active inference, we derive a probabilistic interpretation of {PID} controllers, showing how they can fit a more general theory of life and cognition under the principle of (variational) free energy minimisation once we use only simple linear generative models.},
	pages = {284562},
	journaltitle = {{bioRxiv}},
	author = {Baltieri, Manuel and Buckley, Christopher L.},
	urldate = {2018-03-22},
	date = {2018-03-19},
	langid = {english},
	file = {Baltieri and Buckley - 2018 - A probabilistic interpretation of PID controllers .pdf:/Users/bert/Zotero/storage/E6RJ28JI/Baltieri and Buckley - 2018 - A probabilistic interpretation of PID controllers .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4U2LTUYZ/284562.html:text/html},
}

@article{khan_vprop:_2017,
	title = {Vprop: Variational Inference using {RMSprop}},
	url = {http://arxiv.org/abs/1712.01038},
	shorttitle = {Vprop},
	abstract = {Many computationally-efficient methods for Bayesian deep learning rely on continuous optimization algorithms, but the implementation of these methods requires significant changes to existing code-bases. In this paper, we propose Vprop, a method for Gaussian variational inference that can be implemented with two minor changes to the off-the-shelf {RMSprop} optimizer. Vprop also reduces the memory requirements of Black-Box Variational Inference by half. We derive Vprop using the conjugate-computation variational inference method, and establish its connections to Newton's method, natural-gradient methods, and extended Kalman filters. Overall, this paper presents Vprop as a principled, computationally-efficient, and easy-to-implement method for Bayesian deep learning.},
	journaltitle = {{arXiv}:1712.01038 [cs, stat]},
	author = {Khan, Mohammad Emtiyaz and Liu, Zuozhu and Tangkaratt, Voot and Gal, Yarin},
	urldate = {2018-03-22},
	date = {2017-12-04},
	eprinttype = {arxiv},
	eprint = {1712.01038},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/233DNHYA/1712.html:text/html;Khan e.a. - 2017 - Vprop Variational Inference using RMSprop.pdf:/Users/bert/Zotero/storage/QBKD5UQT/Khan e.a. - 2017 - Vprop Variational Inference using RMSprop.pdf:application/pdf},
}

@inproceedings{lomas_interface_2016,
	location = {New York, {NY}, {USA}},
	title = {Interface Design Optimization As a Multi-Armed Bandit Problem},
	isbn = {978-1-4503-3362-7},
	url = {http://doi.acm.org/10.1145/2858036.2858425},
	doi = {10.1145/2858036.2858425},
	series = {{CHI} '16},
	abstract = {"Multi-armed bandits" offer a new paradigm for the {AI}-assisted design of user interfaces. To help designers understand the potential, we present the results of two experimental comparisons between bandit algorithms and random assignment. Our studies are intended to show designers how bandits algorithms are able to rapidly explore an experimental design space and automatically select the optimal design configuration. Our present focus is on the optimization of a game design space. The results of our experiments show that bandits can make data-driven design more efficient and accessible to interface designers, but that human participation is essential to ensure that {AI} systems optimize for the right metric. Based on our results, we introduce several design lessons that help keep human design judgment in the loop. We also consider the future of human-technology teamwork in {AI}-assisted design and scientific inquiry. Finally, as bandits deploy fewer low-performing conditions than typical experiments, we discuss ethical implications for bandits in large-scale experiments in education.},
	pages = {4142--4153},
	booktitle = {Proceedings of the 2016 {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Lomas, J. Derek and Forlizzi, Jodi and Poonwala, Nikhil and Patel, Nirmal and Shodhan, Sharan and Patel, Kishan and Koedinger, Ken and Brunskill, Emma},
	urldate = {2018-03-20},
	date = {2016},
	keywords = {continuous improvement, data-driven design, design science, educational games, multi-armed bandits, optimization},
	file = {Lomas e.a. - 2016 - Interface Design Optimization As a Multi-Armed Ban.pdf:/Users/bert/Zotero/storage/5SC2YAKK/Lomas e.a. - 2016 - Interface Design Optimization As a Multi-Armed Ban.pdf:application/pdf},
}

@article{scott_multi-armed_2015,
	title = {Multi-armed Bandit Experiments in the Online Service Economy},
	volume = {31},
	issn = {1524-1904},
	url = {http://dx.doi.org/10.1002/asmb.2104},
	doi = {10.1002/asmb.2104},
	abstract = {The modern service economy is substantively different from the agricultural and manufacturing economies that preceded it. In particular, the cost of experimenting is dominated by opportunity cost rather than the cost of obtaining experimental units. The different economics require a new class of experiments, in which stochastic models play an important role. This article briefly summarizes multi-armed bandit experiments, where the experimental design is modified as the experiment progresses to reduce the cost of experimenting. Special attention is paid to Thompson sampling, which is a simple and effective way to run a multi-armed bandit experiment. Copyright © 2015 John Wiley \& Sons, Ltd.},
	pages = {37--45},
	number = {1},
	journaltitle = {Appl. Stoch. Model. Bus. Ind.},
	author = {Scott, Steven L.},
	urldate = {2018-03-20},
	date = {2015-01},
	keywords = {Bayesian, reinforcement learning, sequential experiment, Thompson sampling},
	file = {Scott - 2015 - Multi-armed Bandit Experiments in the Online Servi.pdf:/Users/bert/Zotero/storage/D9D3GHYK/Scott - 2015 - Multi-armed Bandit Experiments in the Online Servi.pdf:application/pdf},
}

@article{urteaga_variational_2017,
	title = {Variational inference for the multi-armed contextual bandit},
	url = {http://arxiv.org/abs/1709.03163},
	abstract = {In many biomedical, science, and engineering problems, one must sequentially decide which action to take next so as to maximize rewards. Reinforcement learning is an area of machine learning that studies how this maximization balances exploration and exploitation, optimizing interactions with the world while simultaneously learning how the world operates. One general class of algorithms for this type of learning is the multi-armed bandit setting and, in particular, the contextual bandit case, in which observed rewards are dependent on each action as well as on given information or 'context' available at each interaction with the world. The Thompson sampling algorithm has recently been shown to perform well in real-world settings and to enjoy provable optimality properties for this set of problems. It facilitates generative and interpretable modeling of the problem at hand, though complexity of the model limits its application, since one must both sample from the distributions modeled and calculate their expected rewards. We here show how these limitations can be overcome using variational approximations, applying to the reinforcement learning case advances developed for the inference case in the machine learning community over the past two decades. We consider bandit applications where the true reward distribution is unknown and approximate it with a mixture model, whose parameters are inferred via variational inference.},
	journaltitle = {{arXiv}:1709.03163 [cs, stat]},
	author = {Urteaga, Iñigo and Wiggins, Chris H.},
	urldate = {2018-03-20},
	date = {2017-09-10},
	eprinttype = {arxiv},
	eprint = {1709.03163},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Computer Science - Learning, I.2.6},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LCV5KEFB/1709.html:text/html;Urteaga en Wiggins - 2017 - Variational inference for the multi-armed contextu.pdf:/Users/bert/Zotero/storage/74HKBKVH/Urteaga en Wiggins - 2017 - Variational inference for the multi-armed contextu.pdf:application/pdf},
}

@article{buckley_theory_2018,
	title = {A theory of how active behavior stabilises neural activity: Neural gain modulation by closed-loop environmental feedback},
	volume = {14},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005926},
	doi = {10.1371/journal.pcbi.1005926},
	shorttitle = {A theory of how active behavior stabilises neural activity},
	abstract = {During active behaviours like running, swimming, whisking or sniffing, motor actions shape sensory input and sensory percepts guide future motor commands. Ongoing cycles of sensory and motor processing constitute a closed-loop feedback system which is central to motor control and, it has been argued, for perceptual processes. This closed-loop feedback is mediated by brainwide neural circuits but how the presence of feedback signals impacts on the dynamics and function of neurons is not well understood. Here we present a simple theory suggesting that closed-loop feedback between the brain/body/environment can modulate neural gain and, consequently, change endogenous neural fluctuations and responses to sensory input. We support this theory with modeling and data analysis in two vertebrate systems. First, in a model of rodent whisking we show that negative feedback mediated by whisking vibrissa can suppress coherent neural fluctuations and neural responses to sensory input in the barrel cortex. We argue this suppression provides an appealing account of a brain state transition (a marked change in global brain activity) coincident with the onset of whisking in rodents. Moreover, this mechanism suggests a novel signal detection mechanism that selectively accentuates active, rather than passive, whisker touch signals. This mechanism is consistent with a predictive coding strategy that is sensitive to the consequences of motor actions rather than the difference between the predicted and actual sensory input. We further support the theory by re-analysing previously published two-photon data recorded in zebrafish larvae performing closed-loop optomotor behaviour in a virtual swim simulator. We show, as predicted by this theory, that the degree to which each cell contributes in linking sensory and motor signals well explains how much its neural fluctuations are suppressed by closed-loop optomotor behaviour. More generally we argue that our results demonstrate the dependence of neural fluctuations, across the brain, on closed-loop brain/body/environment interactions strongly supporting the idea that brain function cannot be fully understood through open-loop approaches alone.},
	pages = {e1005926},
	number = {1},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Buckley, Christopher L. and Toyoizumi, Taro},
	urldate = {2018-03-17},
	date = {2018-01-17},
	langid = {english},
	keywords = {Neurons, Control theory, Fish, Membrane potential, Motor neurons, Swimming, Touch, Vibrissae},
	file = {Buckley and Toyoizumi - 2018 - A theory of how active behavior stabilises neural .pdf:/Users/bert/Zotero/storage/ZQGNBM9S/Buckley and Toyoizumi - 2018 - A theory of how active behavior stabilises neural .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MH89HRKL/article.html:text/html},
}

@article{cusumano-towner_probabilistic_2017,
	title = {Probabilistic programs for inferring the goals of autonomous agents},
	url = {http://arxiv.org/abs/1704.04977},
	abstract = {Intelligent systems sometimes need to infer the probable goals of people, cars, and robots, based on partial observations of their motion. This paper introduces a class of probabilistic programs for formulating and solving these problems. The formulation uses randomized path planning algorithms as the basis for probabilistic models of the process by which autonomous agents plan to achieve their goals. Because these path planning algorithms do not have tractable likelihood functions, new inference algorithms are needed. This paper proposes two Monte Carlo techniques for these "likelihood-free" models, one of which can use likelihood estimates from neural networks to accelerate inference. The paper demonstrates efficacy on three simple examples, each using under 50 lines of probabilistic code.},
	journaltitle = {{arXiv}:1704.04977 [cs]},
	author = {Cusumano-Towner, Marco F. and Radul, Alexey and Wingate, David and Mansinghka, Vikash K.},
	urldate = {2018-03-16},
	date = {2017-04-17},
	eprinttype = {arxiv},
	eprint = {1704.04977},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/23QMKS35/1704.html:text/html;Cusumano-Towner et al. - 2017 - Probabilistic programs for inferring the goals of .pdf:/Users/bert/Zotero/storage/VVVBZN36/Cusumano-Towner et al. - 2017 - Probabilistic programs for inferring the goals of .pdf:application/pdf},
}

@misc{noauthor_audiogram_nodate,
	title = {Audiogram estimation using Bayesian active learning ({IN} {REVIEW})},
}

@article{street_neurobiology_2016,
	title = {Neurobiology as Information Physics},
	volume = {10},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/articles/10.3389/fnsys.2016.00090/full},
	doi = {10.3389/fnsys.2016.00090},
	abstract = {Abstract This article reviews thermodynamic relationships in the brain in an attempt to consolidate current research in systems neuroscience. The present synthesis supports proposals that thermodynamic information in the brain can be quantified to an appreciable degree of objectivity, that many qualitative properties of information in systems of the brain can be inferred by observing changes in thermodynamic quantities, and that many features of the brain’s anatomy and architecture illustrate relatively simple information-energy relationships. The brain may provide a unique window into the relationship between energy and information.},
	journaltitle = {Frontiers in Systems Neuroscience},
	shortjournal = {Front. Syst. Neurosci.},
	author = {Street, Sterling},
	urldate = {2018-03-12},
	date = {2016},
	keywords = {optimization, Bekenstein bound, Free Energy Principle, information thermodynamics, Landauer limit},
	file = {Street - 2016 - Neurobiology as Information Physics.pdf:/Users/bert/Zotero/storage/K25LVMPM/Street - 2016 - Neurobiology as Information Physics.pdf:application/pdf},
}

@article{zenon_information-theoretic_2017,
	title = {An information-theoretic perspective on the costs of cognition},
	rights = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/10/25/208280},
	doi = {10.1101/208280},
	abstract = {Current theories of cognitive effort provide either resource-based or motivation-based accounts of its cost. Despite the usefulness of these proposals, it remains unclear how to exactly quantify effort costs - or why certain tasks are more effortful than others. Here, we provide a novel perspective, based on the assumption that the brain constructs a probabilistic internal model of the world under efficient coding principles. We propose that effort cost is a function of the amount of information required to update the internal model to effectively solve a task. This novel theory naturally explains why some tasks - for example, unfamiliar or dual tasks - are costly and permits to precisely quantify these costs using information-theoretic measures. Finally, we argue that information costs translate into local metabolic costs - which sheds light on the adaptive value of cost-avoidance mechanisms (cognitive effort) in preventing the accumulation of local metabolic alterations over time.},
	pages = {208280},
	journaltitle = {{bioRxiv}},
	author = {Zenon, Alexandre and Solopchuk, Oleg and Pezzulo, Giovanni},
	urldate = {2018-03-12},
	date = {2017-10-25},
	langid = {english},
	file = {Snapshot:/Users/bert/Zotero/storage/8LLKFQLU/208280.html:text/html;Zenon e.a. - 2017 - An information-theoretic perspective on the costs .pdf:/Users/bert/Zotero/storage/46GIC52E/Zenon e.a. - 2017 - An information-theoretic perspective on the costs .pdf:application/pdf},
}

@article{lin_variational_2018,
	title = {Variational Message Passing with Structured Inference Networks},
	url = {https://openreview.net/forum?id=HyH9lbZAW&noteId=B1ytDAtlG},
	abstract = {Recent efforts on combining deep models with probabilistic graphical models are promising in providing flexible models that are also easy to interpret. We propose a variational message-passing...},
	author = {Lin, Wu and Hubacher, Nicolas and Khan, Mohammad Emtiyaz},
	urldate = {2018-02-27},
	date = {2018-02-15},
	file = {Lin e.a. - 2018 - Variational Message Passing with Structured Infere.pdf:/Users/bert/Zotero/storage/3RTIQNJW/Lin e.a. - 2018 - Variational Message Passing with Structured Infere.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CXVKQB74/forum.html:text/html},
}

@article{kalchbrenner_efficient_2018,
	title = {Efficient Neural Audio Synthesis},
	url = {http://arxiv.org/abs/1802.08435},
	abstract = {Sequential models achieve state-of-the-art results in audio, visual and textual domains with respect to both estimating the data distribution and generating high-quality samples. Efficient sampling for this class of models has however remained an elusive problem. With a focus on text-to-speech synthesis, we describe a set of general techniques for reducing sampling time while maintaining high output quality. We first describe a single-layer recurrent neural network, the {WaveRNN}, with a dual softmax layer that matches the quality of the state-of-the-art {WaveNet} model. The compact form of the network makes it possible to generate 24kHz 16-bit audio 4x faster than real time on a {GPU}. Second, we apply a weight pruning technique to reduce the number of weights in the {WaveRNN}. We find that, for a constant number of parameters, large sparse networks perform better than small dense networks and this relationship holds for sparsity levels beyond 96\%. The small number of weights in a Sparse {WaveRNN} makes it possible to sample high-fidelity audio on a mobile {CPU} in real time. Finally, we propose a new generation scheme based on subscaling that folds a long sequence into a batch of shorter sequences and allows one to generate multiple samples at once. The Subscale {WaveRNN} produces 16 samples per step without loss of quality and offers an orthogonal method for increasing sampling efficiency.},
	journaltitle = {{arXiv}:1802.08435 [cs, eess]},
	author = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
	urldate = {2018-02-26},
	date = {2018-02-23},
	eprinttype = {arxiv},
	eprint = {1802.08435},
	keywords = {Computer Science - Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6I7DXIQH/1802.html:text/html;Kalchbrenner e.a. - 2018 - Efficient Neural Audio Synthesis.pdf:/Users/bert/Zotero/storage/59K5RLM4/Kalchbrenner e.a. - 2018 - Efficient Neural Audio Synthesis.pdf:application/pdf},
}

@inproceedings{stern_matchbox:_2009,
	location = {New York, {NY}, {USA}},
	title = {Matchbox: Large Scale Online Bayesian Recommendations},
	isbn = {978-1-60558-487-4},
	url = {http://doi.acm.org/10.1145/1526709.1526725},
	doi = {10.1145/1526709.1526725},
	series = {{WWW} '09},
	shorttitle = {Matchbox},
	abstract = {We present a probabilistic model for generating personalised recommendations of items to users of a web service. The Matchbox system makes use of content information in the form of user and item meta data in combination with collaborative filtering information from previous user behavior in order to predict the value of an item for a user. Users and items are represented by feature vectors which are mapped into a low-dimensional `trait space' in which similarity is measured in terms of inner products. The model can be trained from different types of feedback in order to learn user-item preferences. Here we present three alternatives: direct observation of an absolute rating each user gives to some items, observation of a binary preference (like/ don't like) and observation of a set of ordinal ratings on a user-specific scale. Efficient inference is achieved by approximate message passing involving a combination of Expectation Propagation ({EP}) and Variational Message Passing. We also include a dynamics model which allows an item's popularity, a user's taste or a user's personal rating scale to drift over time. By using Assumed-Density Filtering ({ADF}) for training, the model requires only a single pass through the training data. This is an on-line learning algorithm capable of incrementally taking account of new data so the system can immediately reflect the latest user preferences. We evaluate the performance of the algorithm on the {MovieLens} and Netflix data sets consisting of approximately 1,000,000 and 100,000,000 ratings respectively. This demonstrates that training the model using the on-line {ADF} approach yields state-of-the-art performance with the option of improving performance further if computational resources are available by performing multiple {EP} passes over the training data.},
	pages = {111--120},
	booktitle = {Proceedings of the 18th International Conference on World Wide Web},
	publisher = {{ACM}},
	author = {Stern, David H. and Herbrich, Ralf and Graepel, Thore},
	urldate = {2018-02-19},
	date = {2009},
	keywords = {advertising, bayesian inference, collaborative filtering, machine learning, online services, recommender system},
	file = {Stern e.a. - 2009 - Matchbox Large Scale Online Bayesian Recommendati.pdf:/Users/bert/Zotero/storage/EHCBPSYU/Stern e.a. - 2009 - Matchbox Large Scale Online Bayesian Recommendati.pdf:application/pdf;Train Matchbox Recommender - Azure Machine Learning Studio | Microsoft Docs:/Users/bert/Zotero/storage/6NRAXSL6/train-matchbox-recommender.html:text/html},
}

@article{kaila_natural_2008,
	title = {Natural selection for least action},
	volume = {464},
	rights = {Copyright © 2008 The Royal Society. This is an open-access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/464/2099/3055},
	doi = {10.1098/rspa.2008.0178},
	abstract = {The second law of thermodynamics is a powerful imperative that has acquired several expressions during the past centuries. Connections between two of its most prominent forms, i.e. the evolutionary principle by natural selection and the principle of least action, are examined. Although no fundamentally new findings are provided, it is illuminating to see how the two principles rationalizing natural motions reconcile to one law. The second law, when written as a differential equation of motion, describes evolution along the steepest descents in energy and, when it is given in its integral form, the motion is pictured to take place along the shortest paths in energy. In general, evolution is a non-Euclidian energy density landscape in flattening motion.},
	pages = {3055--3070},
	number = {2099},
	journaltitle = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Kaila, Ville R. I. and Annila, Arto},
	urldate = {2018-02-18},
	date = {2008-11-08},
	langid = {english},
	file = {Kaila and Annila - 2008 - Natural selection for least action.pdf:/Users/bert/Zotero/storage/W4AMIJBY/Kaila and Annila - 2008 - Natural selection for least action.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/HGYIPAWM/3055.html:text/html},
}

@article{friston_woodlice_2018,
	title = {Of woodlice and men: A Bayesian account of cognition, life and consciousness. An interview with Kark Friston},
	volume = {2},
	pages = {17--43},
	journaltitle = {{ALIUS} Bulletin},
	author = {Friston, Karl J and Fortier, Martin and Friedman, Daniel A.},
	date = {2018},
	file = {Friston et al. - 2018 - Of woodlice and men A Bayesian account of cogniti.pdf:/Users/bert/Zotero/storage/FXDB3WF5/Friston et al. - 2018 - Of woodlice and men A Bayesian account of cogniti.pdf:application/pdf},
}

@article{chen_factor_2018,
	title = {Factor graph fragmentization of expectation propagation},
	url = {http://arxiv.org/abs/1801.05108},
	abstract = {Expectation propagation is a general approach to fast approximate inference for graphical models. The existing literature treats models separately when it comes to deriving and coding expectation propagation inference algorithms. This comes at the cost of similar, long-winded algebraic steps being repeated and slowing down algorithmic development. We demonstrate how factor graph fragmentization can overcome this impediment. This involves adoption of the message passing on a factor graph approach to expectation propagation and identification of factor graph sub-graphs, which we call fragments, that are common to wide classes of models. Key fragments and their corresponding messages are catalogued which means that their algebra does not need to be repeated. This allows compartmentalization of coding and efficient software development.},
	journaltitle = {{arXiv}:1801.05108 [stat]},
	author = {Chen, Wilson Y. and Wand, Matt P.},
	urldate = {2018-02-08},
	date = {2018-01-15},
	eprinttype = {arxiv},
	eprint = {1801.05108},
	keywords = {Statistics - Methodology, 62F15},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/63KE886H/1801.html:text/html;Chen en Wand - 2018 - Factor graph fragmentization of expectation propag.pdf:/Users/bert/Zotero/storage/IAU2EM8E/Chen en Wand - 2018 - Factor graph fragmentization of expectation propag.pdf:application/pdf},
}

@article{maddison_concrete_2016,
	title = {The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
	url = {http://arxiv.org/abs/1611.00712},
	shorttitle = {The Concrete Distribution},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	journaltitle = {{arXiv}:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	urldate = {2018-02-06},
	date = {2016-11-02},
	eprinttype = {arxiv},
	eprint = {1611.00712},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7YIJQVL5/1611.html:text/html;Maddison e.a. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf:/Users/bert/Zotero/storage/KQJLJ59X/Maddison e.a. - 2016 - The Concrete Distribution A Continuous Relaxation.pdf:application/pdf},
}

@article{brasher_sometimes_2018,
	title = {Sometimes You Want to Go Where Everybody Knows your Name},
	url = {http://arxiv.org/abs/1801.10182},
	abstract = {We introduce a new metric for measuring how well a model personalizes to a user's specific preferences. We define personalization as a weighting between performance on user specific data and performance on a more general global dataset that represents many different users. This global term serves as a form of regularization that forces us to not overfit to individual users who have small amounts of data. In order to protect user privacy, we add the constraint that we may not centralize or share user data. We also contribute a simple experiment in which we simulate classifying sentiment for users with very distinct vocabularies. This experiment functions as an example of the tension between doing well globally on all users, and doing well on any specific individual user. It also provides a concrete example of how to employ our new metric to help reason about and resolve this tension. We hope this work can help frame and ground future work into personalization.},
	journaltitle = {{arXiv}:1801.10182 [cs]},
	author = {Brasher, Reuben and Roth, Nat and Wagle, Justin},
	urldate = {2018-02-06},
	date = {2018-01-30},
	eprinttype = {arxiv},
	eprint = {1801.10182},
	keywords = {Computer Science - Learning, Computer Science - Information Retrieval},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6JEU5PGT/1801.html:text/html;Brasher e.a. - 2018 - Sometimes You Want to Go Where Everybody Knows you.pdf:/Users/bert/Zotero/storage/KSXLYQB3/Brasher e.a. - 2018 - Sometimes You Want to Go Where Everybody Knows you.pdf:application/pdf},
}

@article{kirchhoff_markov_2018,
	title = {The Markov blankets of life: autonomy, active inference and the free energy principle},
	volume = {15},
	rights = {© 2018 The Authors.. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/15/138/20170792},
	doi = {10.1098/rsif.2017.0792},
	shorttitle = {The Markov blankets of life},
	abstract = {This work addresses the autonomous organization of biological systems. It does so by considering the boundaries of biological systems, from individual cells to Home sapiens, in terms of the presence of Markov blankets under the active inference scheme—a corollary of the free energy principle. A Markov blanket defines the boundaries of a system in a statistical sense. Here we consider how a collective of Markov blankets can self-assemble into a global system that itself has a Markov blanket; thereby providing an illustration of how autonomous systems can be understood as having layers of nested and self-sustaining boundaries. This allows us to show that: (i) any living system is a Markov blanketed system and (ii) the boundaries of such systems need not be co-extensive with the biophysical boundaries of a living organism. In other words, autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me, and all the way out to include elements of the local environment.},
	pages = {20170792},
	number = {138},
	journaltitle = {Journal of The Royal Society Interface},
	author = {Kirchhoff, Michael and Parr, Thomas and Palacios, Ensor and Friston, Karl and Kiverstein, Julian},
	urldate = {2018-01-20},
	date = {2018-01-01},
	langid = {english},
	pmid = {29343629},
	file = {Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf:/Users/bert/Zotero/storage/G9CAYECV/Kirchhoff et al. - 2018 - The Markov blankets of life autonomy, active infe.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/I8WADPLJ/20170792.html:text/html},
}

@article{daunizeau_semi-analytical_2017,
	title = {Semi-analytical approximations to statistical moments of sigmoid and softmax mappings of normal variables},
	url = {http://arxiv.org/abs/1703.00091},
	abstract = {This note is concerned with accurate and computationally efficient approximations of moments of Gaussian random variables passed through sigmoid or softmax mappings. These approximations are semi-analytical (i.e. they involve the numerical adjustment of parametric forms) and highly accurate (they yield 5\% error at most). We also highlight a few niche applications of these approximations, which arise in the context of, e.g., drift-diffusion models of decision making or non-parametric data clustering approaches. We provide these as examples of efficient alternatives to more tedious derivations that would be needed if one was to approach the underlying mathematical issues in a more formal way. We hope that this technical note will be helpful to modellers facing similar mathematical issues, although maybe stemming from different academic prospects.},
	journaltitle = {{arXiv}:1703.00091 [q-bio, stat]},
	author = {Daunizeau, Jean},
	urldate = {2018-01-18},
	date = {2017-02-28},
	eprinttype = {arxiv},
	eprint = {1703.00091},
	keywords = {Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {Daunizeau - 2017 - Semi-analytical approximations to statistical mome.pdf:/Users/bert/Zotero/storage/A364T2GT/Daunizeau - 2017 - Semi-analytical approximations to statistical mome.pdf:application/pdf},
}

@article{daunizeau_parameters_2017,
	title = {On parameters transformations for emulating sparse priors using variational-Laplace inference},
	url = {http://arxiv.org/abs/1703.07168},
	abstract = {So-called sparse estimators arise in the context of model fitting, when one a priori assumes that only a few (unknown) model parameters deviate from zero. Sparsity constraints can be useful when the estimation problem is under-determined, i.e. when number of model parameters is much higher than the number of data points. Typically, such constraints are enforced by minimizing the L1 norm, which yields the so-called {LASSO} estimator. In this work, we propose a simple parameter transform that emulates sparse priors without sacrificing the simplicity and robustness of L2-norm regularization schemes. We show how L1 regularization can be obtained with a "sparsify" remapping of parameters under normal Bayesian priors, and we demonstrate the ensuing variational Laplace approach using Monte-Carlo simulations.},
	journaltitle = {{arXiv}:1703.07168 [q-bio, stat]},
	author = {Daunizeau, Jean},
	urldate = {2018-01-18},
	date = {2017-03-06},
	eprinttype = {arxiv},
	eprint = {1703.07168},
	keywords = {Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/DBLKW6CJ/1703.html:text/html;Daunizeau - 2017 - On parameters transformations for emulating sparse.pdf:/Users/bert/Zotero/storage/XCMIGFYM/Daunizeau - 2017 - On parameters transformations for emulating sparse.pdf:application/pdf},
}

@article{frank_common_2016,
	title = {Common Probability Patterns Arise from Simple Invariances},
	volume = {18},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/18/5/192},
	doi = {10.3390/e18050192},
	abstract = {Shift and stretch invariance lead to the exponential-Boltzmann probability distribution. Rotational invariance generates the Gaussian distribution. Particular scaling relations transform the canonical exponential and Gaussian patterns into the variety of commonly observed patterns. The scaling relations themselves arise from the fundamental invariances of shift, stretch and rotation, plus a few additional invariances. Prior work described the three fundamental invariances as a consequence of the equilibrium canonical ensemble of statistical mechanics or the Jaynesian maximization of information entropy. By contrast, I emphasize the primacy and sufficiency of invariance alone to explain the commonly observed patterns. Primary invariance naturally creates the array of commonly observed scaling relations and associated probability patterns, whereas the classical approaches derived from statistical mechanics or information theory require special assumptions to derive commonly observed scales.},
	pages = {192},
	number = {5},
	journaltitle = {Entropy},
	author = {Frank, Steven A.},
	urldate = {2017-12-29},
	date = {2016-05-19},
	langid = {english},
	keywords = {maximum entropy, information theory, extreme value distributions, measurement, statistical mechanics},
	file = {Frank - 2016 - Common Probability Patterns Arise from Simple Inva.pdf:/Users/bert/Zotero/storage/RXP3THUJ/Frank - 2016 - Common Probability Patterns Arise from Simple Inva.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BDYT6SIH/html.html:text/html},
}

@article{frank_inductive_2014,
	title = {The inductive theory of natural selection: summary and synthesis},
	url = {http://arxiv.org/abs/1412.1285},
	shorttitle = {The inductive theory of natural selection},
	abstract = {The theory of natural selection has two forms. Deductive theory describes how populations change over time. One starts with an initial population and some rules for change. From those assumptions, one calculates the future state of the population. Deductive theory predicts how populations adapt to environmental challenge. Inductive theory describes the causes of change in populations. One starts with a given amount of change. One then assigns different parts of the total change to particular causes. Inductive theory analyzes alternative causal models for how populations have adapted to environmental challenge. This chapter emphasizes the inductive analysis of cause.},
	journaltitle = {{arXiv}:1412.1285 [physics, q-bio]},
	author = {Frank, Steven A.},
	urldate = {2017-12-29},
	date = {2014-12-03},
	eprinttype = {arxiv},
	eprint = {1412.1285},
	keywords = {Computer Science - Neural and Evolutionary Computing, Physics - Biological Physics, Quantitative Biology - Populations and Evolution},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QP7ESZA2/1412.html:text/html;Frank - 2014 - The inductive theory of natural selection summary.pdf:/Users/bert/Zotero/storage/W8AIK9E9/Frank - 2014 - The inductive theory of natural selection summary.pdf:application/pdf},
}

@article{frank_how_2014,
	title = {How to read probability distributions as statements about process},
	volume = {16},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1409.5196},
	doi = {10.3390/e16116059},
	abstract = {Probability distributions can be read as simple expressions of information. Each continuous probability distribution describes how information changes with magnitude. Once one learns to read a probability distribution as a measurement scale of information, opportunities arise to understand the processes that generate the commonly observed patterns. Probability expressions may be parsed into four components: the dissipation of all information, except the preservation of average values, taken over the measurement scale that relates changes in observed values to changes in information, and the transformation from the underlying scale on which information dissipates to alternative scales on which probability pattern may be expressed. Information invariances set the commonly observed measurement scales and the relations between them. In particular, a measurement scale for information is defined by its invariance to specific transformations of underlying values into measurable outputs. Essentially all common distributions can be understood within this simple framework of information invariance and measurement scale.},
	pages = {6059--6098},
	number = {11},
	journaltitle = {Entropy},
	author = {Frank, Steven A.},
	urldate = {2017-12-29},
	date = {2014-11-18},
	eprinttype = {arxiv},
	eprint = {1409.5196},
	keywords = {Quantitative Biology - Quantitative Methods, Mathematics - Probability, Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability, Statistics - Other Statistics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/42CYPSYP/1409.html:text/html;Frank - 2014 - How to read probability distributions as statement.pdf:/Users/bert/Zotero/storage/HNHG467N/Frank - 2014 - How to read probability distributions as statement.pdf:application/pdf},
}

@article{frank_universal_2017,
	title = {Universal expressions of population change by the Price equation: Natural selection, information, and maximum entropy production},
	volume = {7},
	issn = {2045-7758},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/ece3.2922/abstract},
	doi = {10.1002/ece3.2922},
	shorttitle = {Universal expressions of population change by the Price equation},
	abstract = {The Price equation shows the unity between the fundamental expressions of change in biology, in information and entropy descriptions of populations, and in aspects of thermodynamics. The Price equation partitions the change in the average value of a metric between two populations. A population may be composed of organisms or particles or any members of a set to which we can assign probabilities. A metric may be biological fitness or physical energy or the output of an arbitrarily complicated function that assigns quantitative values to members of the population. The first part of the Price equation describes how directly applied forces change the probabilities assigned to members of the population when holding constant the metrical values of the members—a fixed metrical frame of reference. The second part describes how the metrical values change, altering the metrical frame of reference. In canonical examples, the direct forces balance the changing metrical frame of reference, leaving the average or total metrical values unchanged. In biology, relative reproductive success (fitness) remains invariant as a simple consequence of the conservation of total probability. In physics, systems often conserve total energy. Nonconservative metrics can be described by starting with conserved metrics, and then studying how coordinate transformations between conserved and nonconserved metrics alter the geometry of the dynamics and the aggregate values of populations. From this abstract perspective, key results from different subjects appear more simply as universal geometric principles for the dynamics of populations subject to the constraints of particular conserved quantities.},
	pages = {3381--3396},
	number = {10},
	journaltitle = {Ecology and Evolution},
	shortjournal = {Ecol Evol},
	author = {Frank, Steven A.},
	urldate = {2017-12-29},
	date = {2017-05-01},
	langid = {english},
	keywords = {Fisher information, evolutionary theory, Jaynes maximum entropy, thermodynamics},
	file = {Frank - 2017 - Universal expressions of population change by the .pdf:/Users/bert/Zotero/storage/NY832NP4/Frank - 2017 - Universal expressions of population change by the .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/377UVWTP/abstract.html:text/html},
}

@article{ororbia_ii_learning_2017,
	title = {Learning to Adapt by Minimizing Discrepancy},
	url = {http://arxiv.org/abs/1711.11542},
	abstract = {We explore whether useful temporal neural generative models can be learned from sequential data without back-propagation through time. We investigate the viability of a more neurocognitively-grounded approach in the context of unsupervised generative modeling of sequences. Specifically, we build on the concept of predictive coding, which has gained influence in cognitive science, in a neural framework. To do so we develop a novel architecture, the Temporal Neural Coding Network, and its learning algorithm, Discrepancy Reduction. The underlying directed generative model is fully recurrent, meaning that it employs structural feedback connections and temporal feedback connections, yielding information propagation cycles that create local learning signals. This facilitates a unified bottom-up and top-down approach for information transfer inside the architecture. Our proposed algorithm shows promise on the bouncing balls generative modeling problem. Further experiments could be conducted to explore the strengths and weaknesses of our approach.},
	journaltitle = {{arXiv}:1711.11542 [cs, stat]},
	author = {Ororbia {II}, Alexander G. and Haffner, Patrick and Reitter, David and Giles, C. Lee},
	urldate = {2017-12-29},
	date = {2017-11-30},
	eprinttype = {arxiv},
	eprint = {1711.11542},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/AVF7HIP2/1711.html:text/html;Ororbia II et al. - 2017 - Learning to Adapt by Minimizing Discrepancy.pdf:/Users/bert/Zotero/storage/7Q3XJYMZ/Ororbia II et al. - 2017 - Learning to Adapt by Minimizing Discrepancy.pdf:application/pdf},
}

@article{ollivier_online_2017,
	title = {Online Natural Gradient as a Kalman Filter},
	url = {http://arxiv.org/abs/1703.00209},
	abstract = {We establish a full relationship between Kalman filtering and Amari's natural gradient in statistical learning. Namely, using an online natural gradient descent on data log-likelihood to evaluate the parameter of a probabilistic model from a series of observations, is exactly equivalent to using an extended Kalman filter to estimate the parameter (assumed to have constant dynamics). In the i.i.d. case, this relation is a consequence of the "information filter" phrasing of the extended Kalman filter. In the recurrent (state space, non-i.i.d.) case, we prove that the joint Kalman filter over states and parameters is a natural gradient on top of real-time recurrent learning ({RTRL}), a classical algorithm to train recurrent models. This exact algebraic correspondence provides relevant settings for natural gradient hyperparameters such as learning rates or initialization and regularization of the Fisher information matrix.},
	journaltitle = {{arXiv}:1703.00209 [math, stat]},
	author = {Ollivier, Yann},
	urldate = {2017-12-29},
	date = {2017-03-01},
	eprinttype = {arxiv},
	eprint = {1703.00209},
	keywords = {Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/SRMQ9G7R/1703.html:text/html;Ollivier - 2017 - Online Natural Gradient as a Kalman Filter.pdf:/Users/bert/Zotero/storage/BV5W8SXZ/Ollivier - 2017 - Online Natural Gradient as a Kalman Filter.pdf:application/pdf},
}

@article{tallec_unbiased_2017,
	title = {Unbiased Online Recurrent Optimization},
	url = {http://arxiv.org/abs/1702.05043},
	abstract = {The novel Unbiased Online Recurrent Optimization ({UORO}) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. {UORO} is computationally as costly as Truncated Backpropagation Through Time (truncated {BPTT}), a widespread algorithm for online learning of recurrent networks. {UORO} is a modification of {NoBackTrack} that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Like {NoBackTrack}, {UORO} provides unbiased gradient estimates; unbiasedness is the core hypothesis in stochastic gradient descent theory, without which convergence to a local optimum is not guaranteed. On the contrary, truncated {BPTT} does not provide this property, leading to possible divergence. On synthetic tasks where truncated {BPTT} is shown to diverge, {UORO} converges. For instance, when a parameter has a positive short-term but negative long-term influence, truncated {BPTT} diverges unless the truncation span is very significantly longer than the intrinsic temporal range of the interactions, while {UORO} performs well thanks to the unbiasedness of its gradients.},
	journaltitle = {{arXiv}:1702.05043 [cs]},
	author = {Tallec, Corentin and Ollivier, Yann},
	urldate = {2017-12-29},
	date = {2017-02-16},
	eprinttype = {arxiv},
	eprint = {1702.05043},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/D9QVIJCP/1702.html:text/html;Tallec and Ollivier - 2017 - Unbiased Online Recurrent Optimization.pdf:/Users/bert/Zotero/storage/EZAM4YI7/Tallec and Ollivier - 2017 - Unbiased Online Recurrent Optimization.pdf:application/pdf},
}

@article{innes_differentiable_2019,
	title = {A Differentiable Programming System to Bridge Machine Learning and Scientific Computing},
	url = {http://arxiv.org/abs/1907.07587},
	abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe Zygote, a Differentiable Programming system that is able to take gradients of general program structures. We implement this system in the Julia programming language. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning, but more importantly, it enables us to incorporate a large ecosystem of libraries in our models in a straightforward way. We discuss our approach to automatic differentiation, including its support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present several examples of differentiating programs.},
	journaltitle = {{arXiv}:1907.07587 [cs]},
	author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
	urldate = {2019-07-31},
	date = {2019-07-17},
	eprinttype = {arxiv},
	eprint = {1907.07587},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/N8ZSCA3R/1907.html:text/html;Innes et al. - 2019 - A Differentiable Programming System to Bridge Mach.pdf:/Users/bert/Zotero/storage/433Z3KNK/Innes et al. - 2019 - A Differentiable Programming System to Bridge Mach.pdf:application/pdf},
}

@article{parsa_hierarchical_2018,
	title = {A Hierarchical Bayesian Linear Regression Model with Local Features for Stochastic Dynamics Approximation},
	url = {http://arxiv.org/abs/1807.03931},
	abstract = {One of the challenges with model-based control of stochastic dynamical systems is that the state transition dynamics are involved, making it diﬃcult and ineﬃcient to make good-quality predictions of the states. Moreover, there are not many representational models for the majority of autonomous systems, as it is not easy to build a compact model that captures all the subtleties and uncertainties in the system dynamics. In this work, we present a hierarchical Bayesian linear regression model with local features to learn the dynamics of such systems. The model is hierarchical since we consider non-stationary priors for the model parameters which increases its ﬂexibility. To solve the maximum likelihood ({ML}) estimation problem for this hierarchical model, we use the variational expectation maximization ({EM}) algorithm, and enhance the procedure by introducing hidden target variables. The algorithm is guaranteed to converge to the optimal log-likelihood values under certain reasonable assumptions. It also yields parsimonious model structures, and consistently provides fast and accurate predictions for all our examples, including two illustrative systems and a challenging micro-robotic system, involving large training and test sets. These results demonstrate the eﬀectiveness of the method in approximating stochastic dynamics, which make it suitable for future use in a paradigm, such as model-based reinforcement learning, to compute optimal control policies in real time.},
	journaltitle = {{arXiv}:1807.03931 [cs, stat]},
	author = {Parsa, Behnoosh and Rajasekaran, Keshav and Meier, Franziska and Banerjee, Ashis G.},
	urldate = {2019-07-29},
	date = {2018-07-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1807.03931},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Parsa et al. - 2018 - A Hierarchical Bayesian Linear Regression Model wi.pdf:/Users/bert/Zotero/storage/TNHEAM6U/Parsa et al. - 2018 - A Hierarchical Bayesian Linear Regression Model wi.pdf:application/pdf},
}

@article{kamary_testing_2014,
	title = {Testing hypotheses via a mixture estimation model},
	url = {http://arxiv.org/abs/1412.2044},
	abstract = {We consider a novel paradigm for Bayesian testing of hypotheses and Bayesian model comparison. Our alternative to the traditional construction of posterior probabilities that a given hypothesis is true or that the data originates from a specific model is to consider the models under comparison as components of a mixture model. We therefore replace the original testing problem with an estimation one that focus on the probability weight of a given model within a mixture model. We analyze the sensitivity on the resulting posterior distribution on the weights of various prior modeling on the weights. We stress that a major appeal in using this novel perspective is that generic improper priors are acceptable, while not putting convergence in jeopardy. Among other features, this allows for a resolution of the Lindley-Jeffreys paradox. When using a reference Beta B(a,a) prior on the mixture weights, we note that the sensitivity of the posterior estimations of the weights to the choice of a vanishes with the sample size increasing and avocate the default choice a=0.5, derived from Rousseau and Mengersen (2011). Another feature of this easily implemented alternative to the classical Bayesian solution is that the speeds of convergence of the posterior mean of the weight and of the corresponding posterior probability are quite similar.},
	journaltitle = {{arXiv}:1412.2044 [stat]},
	author = {Kamary, Kaniav and Mengersen, Kerrie and Robert, Christian P. and Rousseau, Judith},
	urldate = {2019-07-21},
	date = {2014-12-05},
	eprinttype = {arxiv},
	eprint = {1412.2044},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GDRRUHZC/1412.html:text/html;Kamary et al. - 2014 - Testing hypotheses via a mixture estimation model.pdf:/Users/bert/Zotero/storage/YG7BTQTW/Kamary et al. - 2014 - Testing hypotheses via a mixture estimation model.pdf:application/pdf},
}

@article{ghahramani_probabilistic_2015,
	title = {Probabilistic machine learning and artificial intelligence},
	volume = {521},
	rights = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14541},
	doi = {10.1038/nature14541},
	abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
	pages = {452--459},
	number = {7553},
	journaltitle = {Nature},
	author = {Ghahramani, Zoubin},
	urldate = {2019-07-13},
	date = {2015-05},
	langid = {english},
	file = {Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf:/Users/bert/Zotero/storage/KUAS5VNL/Ghahramani - 2015 - Probabilistic machine learning and artificial inte.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CEVFZYGD/nature14541.html:text/html},
}

@article{millidge_deep_2019,
	title = {Deep Active Inference as Variational Policy Gradients},
	url = {http://arxiv.org/abs/1907.03876},
	abstract = {Active Inference is a theory of action arising from neuroscience which casts action and planning as a bayesian inference problem to be solved by minimizing a single quantity - the variational free energy. Active Inference promises a unifying account of action and perception coupled with a biologically plausible process theory. Despite these potential advantages, current implementations of Active Inference can only handle small, discrete policy and state-spaces and typically require the environmental dynamics to be known. In this paper we propose a novel deep Active Inference algorithm which approximates key densities using deep neural networks as flexible function approximators, which enables Active Inference to scale to significantly larger and more complex tasks. We demonstrate our approach on a suite of {OpenAIGym} benchmark tasks and obtain performance comparable with common reinforcement learning baselines. Moreover, our algorithm shows similarities with maximum entropy reinforcement learning and the policy gradients algorithm, which reveals interesting connections between the Active Inference framework and reinforcement learning.},
	journaltitle = {{arXiv}:1907.03876 [cs]},
	author = {Millidge, Beren},
	urldate = {2019-07-13},
	date = {2019-07-08},
	eprinttype = {arxiv},
	eprint = {1907.03876},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Y9SRCJCI/1907.html:text/html;Millidge - 2019 - Deep Active Inference as Variational Policy Gradie.pdf:/Users/bert/Zotero/storage/4SN2I9EA/Millidge - 2019 - Deep Active Inference as Variational Policy Gradie.pdf:application/pdf},
}

@article{masrani_thermodynamic_2019,
	title = {The Thermodynamic Variational Objective},
	url = {http://arxiv.org/abs/1907.00031},
	abstract = {We introduce the thermodynamic variational objective ({TVO}) for learning in both continuous and discrete deep generative models. The {TVO} arises from a key connection between variational inference and thermodynamic integration that results in a tighter lower bound to the log marginal likelihood than the standard variational evidence lower bound ({ELBO}), while remaining as broadly applicable. We provide a computationally efficient gradient estimator for the {TVO} that applies to continuous, discrete, and non-reparameterizable distributions and show that the objective functions used in variational inference, variational autoencoders, wake sleep, and inference compilation are all special cases of the {TVO}. We evaluate the {TVO} for learning of discrete and continuous variational auto encoders, and find it achieves state of the art for learning in discrete variable models, and outperform {VAEs} on continuous variable models without using the reparameterization trick.},
	journaltitle = {{arXiv}:1907.00031 [cs, stat]},
	author = {Masrani, Vaden and Le, Tuan Anh and Wood, Frank},
	urldate = {2019-07-02},
	date = {2019-06-28},
	eprinttype = {arxiv},
	eprint = {1907.00031},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/839NA45Z/1907.html:text/html;Masrani et al. - 2019 - The Thermodynamic Variational Objective.pdf:/Users/bert/Zotero/storage/9VVYAPF2/Masrani et al. - 2019 - The Thermodynamic Variational Objective.pdf:application/pdf},
}

@article{friston_free_2019,
	title = {A free energy principle for a particular physics},
	url = {http://arxiv.org/abs/1906.10184},
	abstract = {This monograph attempts a theory of every 'thing' that can be distinguished from other things in a statistical sense. The ensuing statistical independencies, mediated by Markov blankets, speak to a recursive composition of ensembles (of things) at increasingly higher spatiotemporal scales. This decomposition provides a description of small things; e.g., quantum mechanics - via the Schrodinger equation, ensembles of small things - via statistical mechanics and related fluctuation theorems, through to big things - via classical mechanics. These descriptions are complemented with a Bayesian mechanics for autonomous or active things. Although this work provides a formulation of every thing, its main contribution is to examine the implications of Markov blankets for self-organisation to nonequilibrium steady-state. In brief, we recover an information geometry and accompanying free energy principle that allows one to interpret the internal states of something as representing or making inferences about its external states. The ensuing Bayesian mechanics is compatible with quantum, statistical and classical mechanics and may offer a formal description of lifelike particles.},
	journaltitle = {{arXiv}:1906.10184 [q-bio]},
	author = {Friston, Karl},
	urldate = {2019-06-29},
	date = {2019-06-24},
	eprinttype = {arxiv},
	eprint = {1906.10184},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QEC46B34/1906.html:text/html;Friston - 2019 - A free energy principle for a particular physics.pdf:/Users/bert/Zotero/storage/QK2H87S6/Friston - 2019 - A free energy principle for a particular physics.pdf:application/pdf},
}

@article{annila_intractable_2012,
	title = {On intractable tracks},
	volume = {25},
	issn = {0836-1398},
	url = {http://www.ingentaconnect.com/content/pe/pe/2012/00000025/00000002/art00013},
	doi = {10.4006/0836-1398-25.2.233},
	pages = {233--238},
	number = {2},
	journaltitle = {Physics Essays},
	author = {Annila, Arto and Salthe, Stanley},
	urldate = {2019-06-25},
	date = {2012-06},
	langid = {english},
	file = {Annila and Salthe - 2012 - On intractable tracks.pdf:/Users/bert/Zotero/storage/T92224TT/Annila and Salthe - 2012 - On intractable tracks.pdf:application/pdf},
}

@article{maren_derivation_2019,
	title = {Derivation of the Variational Bayes Equations},
	url = {http://arxiv.org/abs/1906.08804},
	abstract = {The derivation of key equations for the variational Bayes approach is well-known in certain circles. However, translating the fundamental derivations (e.g., as found in Beal (2003)) to the notation of Friston (2013, 2015) is somewhat delicate. Further, the notion of using variational Bayes in the context of a system with Markov blankets requires special attention. This Technical Report presents the derivation in detail. It further illustrates how the variational Bayes method provides a framework for a new computational engine, incorporating the 2-D cluster variation method ({CVM}), which provides a necessary free energy equation that can be minimized across both the external and representational system's states, respectively.},
	journaltitle = {{arXiv}:1906.08804 [cs, q-bio]},
	author = {Maren, Alianna J.},
	urldate = {2019-06-24},
	date = {2019-06-20},
	eprinttype = {arxiv},
	eprint = {1906.08804},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/H5Z5MYZ6/1906.html:text/html;Maren - 2019 - Derivation of the Variational Bayes Equations.pdf:/Users/bert/Zotero/storage/RUZQBR8Z/Maren - 2019 - Derivation of the Variational Bayes Equations.pdf:application/pdf},
}

@article{tabor_bayesian_2019,
	title = {Bayesian Learning Models of Pain: A Call to Action},
	volume = {26},
	issn = {2352-1546},
	url = {http://www.sciencedirect.com/science/article/pii/S2352154618300810},
	doi = {10.1016/j.cobeha.2018.10.006},
	series = {Pain and Aversive Motivation},
	shorttitle = {Bayesian Learning Models of Pain},
	abstract = {Learning is fundamentally about action, enabling the successful navigation of a changing and uncertain environment. The experience of pain is central to this process, indicating the need for a change in action so as to mitigate potential threat to bodily integrity. This review considers the application of Bayesian models of learning in pain that inherently accommodate uncertainty and action, which, we shall propose are essential in understanding learning in both acute and persistent cases of pain.},
	pages = {54--61},
	journaltitle = {Current Opinion in Behavioral Sciences},
	shortjournal = {Current Opinion in Behavioral Sciences},
	author = {Tabor, Abby and Burr, Christopher},
	urldate = {2019-06-12},
	date = {2019-04-01},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/I8X26AZN/S2352154618300810.html:text/html;Tabor and Burr - 2019 - Bayesian Learning Models of Pain A Call to Action.pdf:/Users/bert/Zotero/storage/ANQQXG2R/Tabor and Burr - 2019 - Bayesian Learning Models of Pain A Call to Action.pdf:application/pdf},
}

@article{vasquez_melnet:_2019,
	title = {{MelNet}: A Generative Model for Audio in the Frequency Domain},
	url = {http://arxiv.org/abs/1906.01083},
	shorttitle = {{MelNet}},
	abstract = {Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.},
	journaltitle = {{arXiv}:1906.01083 [cs, eess, stat]},
	author = {Vasquez, Sean and Lewis, Mike},
	urldate = {2019-06-05},
	date = {2019-06-04},
	eprinttype = {arxiv},
	eprint = {1906.01083},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5YSVMEKW/1906.html:text/html;Vasquez and Lewis - 2019 - MelNet A Generative Model for Audio in the Freque.pdf:/Users/bert/Zotero/storage/DEP7JKYJ/Vasquez and Lewis - 2019 - MelNet A Generative Model for Audio in the Freque.pdf:application/pdf},
}

@article{park_structural_2013,
	title = {Structural and Functional Brain Networks: From Connections to Cognition},
	volume = {342},
	rights = {Copyright © 2013, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/342/6158/1238411},
	doi = {10.1126/science.1238411},
	shorttitle = {Structural and Functional Brain Networks},
	abstract = {Structured Abstract
{BackgroundThe} human brain presents a puzzling and challenging paradox: Despite a fixed anatomy, characterized by its connectivity, its functional repertoire is vast, enabling action, perception, and cognition. This contrasts with organs like the heart that have a dynamic anatomy but just one function. The resolution of this paradox may reside in the brain's network architecture, which organizes local interactions to cope with diverse environmental demands—ensuring adaptability, robustness, resilience to damage, efficient message passing, and diverse functionality from a fixed structure. This review asks how recent advances in understanding brain networks elucidate the brain’s many-to-one (degenerate) function-structure relationships. In other words, how does diverse function arise from an apparently static neuronal architecture? We conclude that the emergence of dynamic functional connectivity, from static structural connections, calls for formal (computational) approaches to neuronal information processing that may resolve the dialectic between structure and function.
{\textless}img class="highwire-embed" alt="Embedded Image" src="https://science.sciencemag.org/sites/default/files/highwire/sci/342/6158/1238411/embed/inline-graphic-1.gif"/{\textgreater}Schematic of the multiscale hierarchical organization of brain networks. Brain function or cognition can be described as the global integration of local (segregated) neuronal operations that underlies hierarchical message passing among cortical areas, and which is facilitated by hierarchical modular network architectures.
{AdvancesMuch} of our understanding of brain connectivity rests on the way that it is measured and modeled. We consider two complementary approaches: the first has its basis in graph theory that aims to describe the network topology of (undirected) connections of the sort measured by noninvasive brain imaging of anatomical connections and functional connectivity (correlations) between remote sites. This is compared with model-based definitions of context-sensitive (directed) effective connectivity that are grounded in the biophysics of neuronal interactions.Recent topological network analyses of brain circuits suggest that modular and hierarchical structural networks are particularly suited for the functional integration of local (functionally specialized) neuronal operations that underlie cognition. Measurements of spontaneous activity reveal functional connectivity patterns that are similar to structural connectivity, suggesting that structural networks constrain functional networks. However, task-related responses that require context-sensitive integration disclose a divergence between function and structure that appears to rest mainly on long-range connections. In contrast to methods that describe network topology phenomenologically, model-based theoretical and computational approaches focus on the mechanisms of neuronal interactions that accommodate the dynamic reconfiguration of effective connectivity.We highlight the consilience between hierarchical topologies (based on structural and functional connectivity) and the effective connectivity that would be required for hierarchical message passing of the sort suggested by computational neuroscience.
{OutlookIn} summary, neuronal interactions represent dynamics on a fixed structural connectivity that underlie cognition and behavior. Such divergence of function from structure is, perhaps, the most intriguing property of the brain and invites intensive future research. By studying the dynamics and self-organization of functional networks, we may gain insight into the true nature of the brain as the embodiment of the mind. The repertoire of functional networks rests upon the (hidden) structural architecture of connections that enables hierarchical functional integration. Understanding these networks will require theoretical models of neuronal processing that underlies cognition.
How rich functionality emerges from the invariant structural architecture of the brain remains a major mystery in neuroscience. Recent applications of network theory and theoretical neuroscience to large-scale brain networks have started to dissolve this mystery. Network analyses suggest that hierarchical modular brain networks are particularly suited to facilitate local (segregated) neuronal operations and the global integration of segregated functions. Although functional networks are constrained by structural connections, context-sensitive integration during cognition tasks necessarily entails a divergence between structural and functional networks. This degenerate (many-to-one) function-structure mapping is crucial for understanding the nature of brain networks. The emergence of dynamic functional networks from static structural connections calls for a formal (computational) approach to neuronal information processing that may resolve this dialectic between structure and function.},
	pages = {1238411},
	number = {6158},
	journaltitle = {Science},
	author = {Park, Hae-Jeong and Friston, Karl},
	urldate = {2019-05-30},
	date = {2013-11-01},
	langid = {english},
	pmid = {24179229},
	file = {Snapshot:/Users/bert/Zotero/storage/U7AVDXXX/1238411.html:text/html},
}

@article{tagliasacchi_self-supervised_2019,
	title = {Self-supervised audio representation learning for mobile devices},
	url = {http://arxiv.org/abs/1905.11796},
	abstract = {We explore self-supervised models that can be potentially deployed on mobile devices to learn general purpose audio representations. Specifically, we propose methods that exploit the temporal context in the spectrogram domain. One method estimates the temporal gap between two short audio segments extracted at random from the same audio clip. The other methods are inspired by Word2Vec, a popular technique used to learn word embeddings, and aim at reconstructing a temporal spectrogram slice from past and future slices or, alternatively, at reconstructing the context of surrounding slices from the current slice. We focus our evaluation on small encoder architectures, which can be potentially run on mobile devices during both inference (re-using a common learned representation across multiple downstream tasks) and training (capturing the true data distribution without compromising users' privacy when combined with federated learning). We evaluate the quality of the embeddings produced by the self-supervised learning models, and show that they can be re-used for a variety of downstream tasks, and for some tasks even approach the performance of fully supervised models of similar size.},
	journaltitle = {{arXiv}:1905.11796 [cs, eess, stat]},
	author = {Tagliasacchi, Marco and Gfeller, Beat and Quitry, Félix de Chaumont and Roblek, Dominik},
	urldate = {2019-05-29},
	date = {2019-05-24},
	eprinttype = {arxiv},
	eprint = {1905.11796},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LRWZAGK5/1905.html:text/html;Tagliasacchi et al. - 2019 - Self-supervised audio representation learning for .pdf:/Users/bert/Zotero/storage/6LMT97AX/Tagliasacchi et al. - 2019 - Self-supervised audio representation learning for .pdf:application/pdf},
}

@inproceedings{eslami_just--time_2014,
	title = {Just-In-Time Learning for Fast and Flexible Inference},
	url = {http://papers.nips.cc/paper/5595-just-in-time-learning-for-fast-and-flexible-inference.pdf},
	pages = {154--162},
	booktitle = {Advances in Neural Information Processing Systems 27},
	publisher = {Curran Associates, Inc.},
	author = {Eslami, S. M. Ali and Tarlow, Daniel and Kohli, Pushmeet and Winn, John},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	urldate = {2019-05-28},
	date = {2014},
	file = {Eslami et al. - 2014 - Just-In-Time Learning for Fast and Flexible Infere.pdf:/Users/bert/Zotero/storage/KRBBXLI5/Eslami et al. - 2014 - Just-In-Time Learning for Fast and Flexible Infere.pdf:application/pdf;NIPS Snapshot:/Users/bert/Zotero/storage/D85ESIR4/5595-just-in-time-learning-for-fast-and-flexible-inference.html:text/html},
}

@article{talts_validating_2018,
	title = {Validating Bayesian Inference Algorithms with Simulation-Based Calibration},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} ({SBC}), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that {SBC} is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	journaltitle = {{arXiv}:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	urldate = {2019-05-21},
	date = {2018-04-18},
	eprinttype = {arxiv},
	eprint = {1804.06788},
	keywords = {Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/JBIBKTA7/1804.html:text/html;Talts et al. - 2018 - Validating Bayesian Inference Algorithms with Simu.pdf:/Users/bert/Zotero/storage/G8U9ZQY9/Talts et al. - 2018 - Validating Bayesian Inference Algorithms with Simu.pdf:application/pdf},
}

@article{xu_learning_2018,
	title = {Learning Deep Structured Multi-Scale Features using Attention-Gated {CRFs} for Contour Prediction},
	url = {http://arxiv.org/abs/1801.00524},
	abstract = {Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks ({CNN}) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary {CNN} architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields ({AG}-{CRFs}) are proposed. The experiments ran on two publicly available datasets ({BSDS}500 and {NYUDv}2) demonstrate the effectiveness of the latent {AG}-{CRF} model and of the overall hierarchical framework.},
	journaltitle = {{arXiv}:1801.00524 [cs]},
	author = {Xu, Dan and Ouyang, Wanli and Alameda-Pineda, Xavier and Ricci, Elisa and Wang, Xiaogang and Sebe, Nicu},
	urldate = {2019-05-09},
	date = {2018-01-01},
	eprinttype = {arxiv},
	eprint = {1801.00524},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/8RRNNG72/1801.html:text/html;Xu et al. - 2018 - Learning Deep Structured Multi-Scale Features usin.pdf:/Users/bert/Zotero/storage/HQTXNZMR/Xu et al. - 2018 - Learning Deep Structured Multi-Scale Features usin.pdf:application/pdf},
}

@inproceedings{cox_forneylab.jl:_2018,
	title = {{ForneyLab}.jl: fast and flexible automated inference through message passing in Julia},
	shorttitle = {{ForneyLab}.jl},
	author = {Cox, Marco and Laar, Thijs van de and Vries, Arnout de},
	date = {2018},
	keywords = {Message passing, Inference, julia {\textless}Dryas julia{\textgreater}},
}

@article{van_de_laar_simulating_2019,
	title = {Simulating Active Inference Processes by Message Passing},
	volume = {6},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2019.00020/full},
	doi = {10.3389/frobt.2019.00020},
	abstract = {The free energy principle ({FEP}) offers a variational calculus-based description for how biological agents persevere through interactions with their environment. Active inference ({AI}) is a corollary of the {FEP}, which states that biological agents act to fulfill prior beliefs about preferred future observations (target priors). Purposeful behavior then results from variational free energy minimization with respect to a generative model of the environment with included target priors. However, manual derivations for free energy minimizing algorithms on custom dynamic models can become tedious and error-prone. While probabilistic programming ({PP}) techniques enable automatic derivation of inference algorithms on free-form models, full automation of {AI} requires specialized tools for inference on dynamic models, together with the description of an experimental protocol that governs the interaction between the agent and its simulated environment. The contributions of the present paper are two-fold. Firstly, we illustrate how {AI} can be automated with the use of {ForneyLab}, a recent {PP} toolbox that specializes in variational inference on flexibly definable dynamic models. More specifically, we describe {AI} agents in a dynamic environment as probabilistic state space models ({SSM}) and perform inference for perception and control in these agents by message passing on a factor graph representation of the {SSM}. Secondly, we propose a formal experimental protocol for simulated {AI}. We exemplify how this protocol leads to goal-directed behavior for flexibly definable {AI} agents in two classical {RL} examples, namely the Bayesian thermostat and the mountain car parking problems.},
	journaltitle = {Frontiers in Robotics and {AI}},
	shortjournal = {Front. Robot. {AI}},
	author = {van de Laar, Thijs W. and de Vries, Bert},
	urldate = {2019-05-09},
	date = {2019},
	keywords = {active inference, message passing, free-energy principle, Forney-style factor graphs, state-space models},
	file = {van de Laar and de Vries - 2019 - Simulating Active Inference Processes by Message P.pdf:/Users/bert/Zotero/storage/XQFHK6KN/van de Laar and de Vries - 2019 - Simulating Active Inference Processes by Message P.pdf:application/pdf},
}

@incollection{minka_gates_2009,
	title = {Gates},
	url = {http://papers.nips.cc/paper/3379-gates.pdf},
	pages = {1073--1080},
	booktitle = {Advances in Neural Information Processing Systems 21},
	publisher = {Curran Associates, Inc.},
	author = {Minka, Tom and Winn, John},
	editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
	urldate = {2019-05-09},
	date = {2009},
	file = {Minka and Winn - 2009 - Gates.pdf:/Users/bert/Zotero/storage/4Z7GDW5P/Minka and Winn - 2009 - Gates.pdf:application/pdf;NIPS Snapshot:/Users/bert/Zotero/storage/2YK2G7MY/3379-gates.html:text/html},
}

@inproceedings{van_de_laar_variational_2017,
	title = {Variational stabilized linear forgetting in state-space models},
	eventtitle = {Signal Processing Conference ({EUSIPCO}), 2017 25th European},
	pages = {818--822},
	booktitle = {Signal Processing Conference ({EUSIPCO}), 2017 25th European},
	publisher = {{IEEE}},
	author = {van de Laar, Thijs and Cox, Marco and van Diepen, Anouk and de Vries, Bert},
	date = {2017},
	file = {Snapshot:/Users/bert/Zotero/storage/XSUSTSCH/8081321.html:text/html;supplement.pdf:/Users/bert/Zotero/storage/3JXUGVJ5/supplement.pdf:application/pdf;van de Laar et al. - 2017 - Variational stabilized linear forgetting in state-.pdf:/Users/bert/Zotero/storage/LNTVJJZE/van de Laar et al. - 2017 - Variational stabilized linear forgetting in state-.pdf:application/pdf;Variational Stabilized Linear Forgetting in State-Space Models.pdf:/Users/bert/Zotero/storage/VX4PGGFV/Variational Stabilized Linear Forgetting in State-Space Models.pdf:application/pdf},
}

@article{botvinick_reinforcement_2019,
	title = {Reinforcement Learning, Fast and Slow},
	volume = {0},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(19)30061-0},
	doi = {10.1016/j.tics.2019.02.006},
	number = {0},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
	urldate = {2019-05-06},
	date = {2019-04-16},
	pmid = {31003893},
	file = {Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf:/Users/bert/Zotero/storage/S4RX7NVV/Botvinick et al. - 2019 - Reinforcement Learning, Fast and Slow.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/LDL6KG5R/S1364-6613(19)30061-0.html:text/html},
}

@article{colombo_first_2018,
	title = {First principles in the life sciences: the free-energy principle, organicism, and mechanism},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-018-01932-w},
	doi = {10.1007/s11229-018-01932-w},
	shorttitle = {First principles in the life sciences},
	abstract = {The free-energy principle states that all systems that minimize their free energy resist a tendency to physical disintegration. Originally proposed to account for perception, learning, and action, the free-energy principle has been applied to the evolution, development, morphology, anatomy and function of the brain, and has been called a postulate, an unfalsifiable principle, a natural law, and an imperative. While it might afford a theoretical foundation for understanding the relationship between environment, life, and mind, its epistemic status is unclear. Also unclear is how the free-energy principle relates to prominent theoretical approaches to life science phenomena, such as organicism and mechanism. This paper clarifies both issues, and identifies limits and prospects for the free-energy principle as a first principle in the life sciences.},
	journaltitle = {Synthese},
	shortjournal = {Synthese},
	author = {Colombo, Matteo and Wright, Cory},
	urldate = {2019-05-01},
	date = {2018-09-10},
	langid = {english},
	keywords = {Free energy, Adaptation, Life, Mechanism, Organicism},
	file = {Colombo and Wright - 2018 - First principles in the life sciences the free-en.pdf:/Users/bert/Zotero/storage/A8K25IS9/Colombo and Wright - 2018 - First principles in the life sciences the free-en.pdf:application/pdf},
}

@article{kleidon_life_2010,
	title = {Life, hierarchy, and the thermodynamic machinery of planet Earth},
	volume = {7},
	issn = {1571-0645},
	url = {http://www.sciencedirect.com/science/article/pii/S1571064510001107},
	doi = {10.1016/j.plrev.2010.10.002},
	abstract = {Throughout Earth's history, life has increased greatly in abundance, complexity, and diversity. At the same time, it has substantially altered the Earth's environment, evolving some of its variables to states further and further away from thermodynamic equilibrium. For instance, concentrations in atmospheric oxygen have increased throughout Earth's history, resulting in an increased chemical disequilibrium in the atmosphere as well as an increased redox gradient between the atmosphere and the Earth's reducing crust. These trends seem to contradict the second law of thermodynamics, which states for isolated systems that gradients and free energy are dissipated over time, resulting in a state of thermodynamic equilibrium. This seeming contradiction is resolved by considering planet Earth as a coupled, hierarchical and evolving non-equilibrium thermodynamic system that has been substantially altered by the input of free energy generated by photosynthetic life. Here, I present this hierarchical thermodynamic theory of the Earth system. I first present simple considerations to show that thermodynamic variables are driven away from a state of thermodynamic equilibrium by the transfer of power from some other process and that the resulting state of disequilibrium reflects the past net work done on the variable. This is applied to the processes of planet Earth to characterize the generation and transfer of free energy and its dissipation, from radiative gradients to temperature and chemical potential gradients that result in chemical, kinetic, and potential free energy and associated dynamics of the climate system and geochemical cycles. The maximization of power transfer among the processes within this hierarchy yields thermodynamic efficiencies much lower than the Carnot efficiency of equilibrium thermodynamics and is closely related to the proposed principle of Maximum Entropy Production ({MEP}). The role of life is then discussed as a photochemical process that generates substantial amounts of chemical free energy which essentially skips the limitations and inefficiencies associated with the transfer of power within the thermodynamic hierarchy of the planet. This perspective allows us to view life as being the means to transform many aspects of planet Earth to states even further away from thermodynamic equilibrium than is possible by purely abiotic means. In this perspective pockets of low-entropy life emerge from the overall trend of the Earth system to increase the entropy of the universe at the fastest possible rate. The implications of the theory are discussed regarding fundamental deficiencies in Earth system modeling, applications of the theory to reconstructions of Earth system history, and regarding the role of human activity for the future of the planet.},
	pages = {424--460},
	number = {4},
	journaltitle = {Physics of Life Reviews},
	shortjournal = {Physics of Life Reviews},
	author = {Kleidon, Axel},
	urldate = {2019-04-28},
	date = {2010-12-01},
	keywords = {Entropy, Evolution, Life, Atmosphere–biosphere interactions, Earth system, Gaia hypothesis, Global change, Habitability, Hierarchy, Holistic theory, Human impacts, Maximum Entropy Production, Maximum power principle, Power transfer, Thermodynamics},
	file = {Kleidon - 2010 - Life, hierarchy, and the thermodynamic machinery o.pdf:/Users/bert/Zotero/storage/C3N5D5ZU/Kleidon - 2010 - Life, hierarchy, and the thermodynamic machinery o.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/JL9WLQEX/S1571064510001107.html:text/html},
}

@article{annila_natural_2016,
	title = {Natural thermodynamics},
	volume = {444},
	issn = {0378-4371},
	url = {http://www.sciencedirect.com/science/article/pii/S0378437115009644},
	doi = {10.1016/j.physa.2015.10.105},
	abstract = {The principle of increasing entropy is derived from statistical physics of open systems assuming that quanta of actions, as undividable basic build blocks, embody everything. According to this tenet, all systems evolve from one state to another either by acquiring quanta from their surroundings or by discarding quanta to the surroundings in order to attain energetic balance in least time. These natural processes result in ubiquitous scale-free patterns: skewed distributions that accumulate in a sigmoid manner and hence span log–log scales mostly as straight lines. Moreover, the equation for least-time motions reveals that evolution is by nature a non-deterministic process. Although the obtained insight in thermodynamics from the notion of quanta in motion yields nothing new, it accentuates that contemporary comprehension is impaired when modeling evolution as a computable process by imposing conservation of energy and thereby ignoring that quantum of actions are the carriers of energy from the system to its surroundings.},
	pages = {843--852},
	journaltitle = {Physica A: Statistical Mechanics and its Applications},
	shortjournal = {Physica A: Statistical Mechanics and its Applications},
	author = {Annila, Arto},
	urldate = {2019-04-28},
	date = {2016-02-15},
	keywords = {Free energy, Dissipation, Geodesic, Photon, Quantum, The principle of least action},
	file = {Annila - 2016 - Natural thermodynamics.pdf:/Users/bert/Zotero/storage/5V4E393Q/Annila - 2016 - Natural thermodynamics.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/JJALLCQW/S0378437115009644.html:text/html},
}

@article{catal_bayesian_2019,
	title = {Bayesian policy selection using active inference},
	url = {http://arxiv.org/abs/1904.08149},
	abstract = {Learning to take actions based on observations is a core requirement for artificial agents to be able to be successful and robust at their task. Reinforcement Learning ({RL}) is a well-known technique for learning such policies. However, current {RL} algorithms often have to deal with reward shaping, have difficulties generalizing to other environments and are most often sample inefficient. In this paper, we explore active inference and the free energy principle, a normative theory from neuroscience that explains how self-organizing biological systems operate by maintaining a model of the world and casting action selection as an inference problem. We apply this concept to a typical problem known to the {RL} community, the mountain car problem, and show how active inference encompasses both {RL} and learning from demonstrations.},
	journaltitle = {{arXiv}:1904.08149 [cs]},
	author = {Çatal, Ozan and Nauta, Johannes and Verbelen, Tim and Simoens, Pieter and Dhoedt, Bart},
	urldate = {2019-04-28},
	date = {2019-04-17},
	eprinttype = {arxiv},
	eprint = {1904.08149},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/GE2DNWBD/1904.html:text/html;Çatal et al. - 2019 - Bayesian policy selection using active inference.pdf:/Users/bert/Zotero/storage/6GUQESXW/Çatal et al. - 2019 - Bayesian policy selection using active inference.pdf:application/pdf},
}

@article{krishnan_deep_2015,
	title = {Deep Kalman Filters},
	url = {http://arxiv.org/abs/1511.05121},
	abstract = {Kalman Filters are one of the most influential models of time-varying phenomena. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption in a variety of disciplines. Motivated by recent variational methods for learning deep generative models, we introduce a unified algorithm to efficiently learn a broad spectrum of Kalman filters. Of particular interest is the use of temporal generative models for counterfactual inference. We investigate the efficacy of such models for counterfactual inference, and to that end we introduce the "Healing {MNIST}" dataset where long-term structure, noise and actions are applied to sequences of digits. We show the efficacy of our method for modeling this dataset. We further show how our model can be used for counterfactual inference for patients, based on electronic health record data of 8,000 patients over 4.5 years.},
	journaltitle = {{arXiv}:1511.05121 [cs, stat]},
	author = {Krishnan, Rahul G. and Shalit, Uri and Sontag, David},
	urldate = {2019-04-26},
	date = {2015-11-16},
	eprinttype = {arxiv},
	eprint = {1511.05121},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/PKVWTNPX/1511.html:text/html;Krishnan et al. - 2015 - Deep Kalman Filters.pdf:/Users/bert/Zotero/storage/6DHTSFYQ/Krishnan et al. - 2015 - Deep Kalman Filters.pdf:application/pdf},
}

@book{frederick_natural_2012,
	location = {Sheffield, {UK}},
	edition = {1 edition},
	title = {Natural Corporate Management: From the Big Bang to Wall Street},
	isbn = {978-1-906093-80-8},
	shorttitle = {Natural Corporate Management},
	abstract = {This groundbreaking new book by business scholar William C. Frederick presents an innovative, exciting even revolutionary view of corporate management and the challenges it confronts in today s world. The author proposes a management paradigm shift transforming the way corporations do business. Management scholarship and research may well be rechanneled from current orientations to new models, concepts, and theories of what it takes to manage corporations in a planetary world confronting climate change, energy crises, and securing the well-being of all global citizens. Natural Corporate Management ({NCM}) is an awareness and an acceptance by the managers of today s business corporations of the close functional linkage between natural forces and human economic choices. {NCM} is not a set of techniques or methods but is a growing consciousness by managers of the presence and influence of nature in all managerial decisions. The book s central theme is that business and nature are locked into an evolutionary partnership that defines all aspects of corporate management, including decisions, policy, goal-seeking, organizational design, workplace behavior, and productive operations. This partnership of Nature and Nurture yields economic, social, and ecological dividends for corporations, their stakeholders, and the global community. An Evolutionary Cascade depicts the various phases of evolutionary change physical, organic, genetic, human, neurological, symbolic beginning with the Big Bang origin of the Universe and continuing to modern times. These evolutionary events collectively influence the operational activities of all business firms. A Natural Theory of the Firm summarizes the {NCM} approach, as well as the mind-set of corporate managers, and the bio-socio-economic consequences of their decisions. This theoretically-innovative book proposes an agenda of corporate actions to promote long-term sustainability and economic},
	pagetotal = {280},
	publisher = {Greenleaf Publishing},
	author = {Frederick, William C.},
	date = {2012-11-10},
}

@article{annila_why_2008,
	title = {Why did life emerge?},
	volume = {7},
	issn = {1473-5504, 1475-3006},
	url = {http://arxiv.org/abs/0910.2621},
	doi = {10.1017/S1473550408004308},
	abstract = {Many mechanisms, functions and structures of life have been unraveled. However, the fundamental driving force that propelled chemical evolution and led to life has remained obscure. The 2nd law of thermodynamics, written as an equation of motion, reveals that elemental abiotic matter evolves from the equilibrium via chemical reactions that couple to external energy toward complex biotic non-equilibrium systems. Each time a new mechanism of energy transduction emerges, e.g., by random variation in syntheses, evolution prompts by punctuation and settles to a stasis when the accessed free energy has been consumed. The evolutionary course toward an increasingly larger energy transduction system accumulates a diversity of energy transduction mechanisms, i.e., species. The rate of entropy increase is identified as the fitness criterion among the diverse mechanisms which places the theory of evolution by natural selection on the fundamental thermodynamic principle with no demarcation line between inanimate and animate.},
	pages = {293},
	number = {3},
	journaltitle = {International Journal of Astrobiology},
	author = {Annila, Arto and Annila, Erkki},
	urldate = {2019-04-22},
	date = {2008-10},
	eprinttype = {arxiv},
	eprint = {0910.2621},
	keywords = {Quantitative Biology - Populations and Evolution},
	file = {Annila and Annila - 2008 - Why did life emerge.pdf:/Users/bert/Zotero/storage/Q2BHP486/Annila and Annila - 2008 - Why did life emerge.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/SP4CUQ5I/0910.html:text/html},
}

@article{varpula_thoughts_2013,
	title = {Thoughts about Thinking: Cognition According to the Second Law of Thermodynamics},
	volume = {5},
	shorttitle = {Thoughts about Thinking},
	abstract = {A holistic account of the human brain is provided by the Second Law of Thermodynamics. According to this universal precept, the central nervous system is governed by the quest to consume free energy in the least possible time. The brain is like any other system of nature that has evolved over eons and continues to develop over an individual's life time. This physical portrayal is singularly appropriate because power-law characteristics as well as oscillatory and at times unpredictable functioning are not exclusive attributes of the brain but are found in other systems throughout nature. The neural network comprises pathways for signal propagation just as other natural systems have pathways for the transmission of energy in various forms. These universalities support the view of the evolution and development of the human brain as a natural thermodynamic process. In a like manner perception, sensation and learning as well as the processes of memory, emotions and consciousness can be regarded as natural expressions of the neural network under the suzerainty of the Second Law. The outcomes of cognitive processes, like other natural processes, are non-deterministic because the interactive effects of flows of energy as signals with differences in energy as their driving forces cannot be separated from each other. This naturalistic framework also provides insight into mental disorders and cognitive defects.},
	journaltitle = {Adv. Stud. Biol},
	shortjournal = {Adv. Stud. Biol},
	author = {Varpula, Saara and Annila, Arto and Beck, Charles},
	date = {2013-01-01},
	file = {Varpula et al. - 2013 - Thoughts about Thinking Cognition According to th.pdf:/Users/bert/Zotero/storage/2LZ2SZL3/Varpula et al. - 2013 - Thoughts about Thinking Cognition According to th.pdf:application/pdf},
}

@article{moens_learning_2019,
	title = {Learning and forgetting using reinforced Bayesian change detection},
	volume = {15},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1006713},
	doi = {10.1371/journal.pcbi.1006713},
	abstract = {Agents living in volatile environments must be able to detect changes in contingencies while refraining to adapt to unexpected events that are caused by noise. In Reinforcement Learning ({RL}) frameworks, this requires learning rates that adapt to past reliability of the model. The observation that behavioural flexibility in animals tends to decrease following prolonged training in stable environment provides experimental evidence for such adaptive learning rates. However, in classical {RL} models, learning rate is either fixed or scheduled and can thus not adapt dynamically to environmental changes. Here, we propose a new Bayesian learning model, using variational inference, that achieves adaptive change detection by the use of Stabilized Forgetting, updating its current belief based on a mixture of fixed, initial priors and previous posterior beliefs. The weight given to these two sources is optimized alongside the other parameters, allowing the model to adapt dynamically to changes in environmental volatility and to unexpected observations. This approach is used to implement the “critic” of an actor-critic {RL} model, while the actor samples the resulting value distributions to choose which action to undertake. We show that our model can emulate different adaptation strategies to contingency changes, depending on its prior assumptions of environmental stability, and that model parameters can be fit to real data with high accuracy. The model also exhibits trade-offs between flexibility and computational costs that mirror those observed in real data. Overall, the proposed method provides a general framework to study learning flexibility and decision making in {RL} contexts.},
	pages = {e1006713},
	number = {4},
	journaltitle = {{PLOS} Computational Biology},
	author = {Moens, Vincent and Zénon, Alexandre},
	editor = {Friston, Karl J.},
	urldate = {2019-04-18},
	date = {2019-04-17},
	langid = {english},
	file = {Moens and Zénon - 2019 - Learning and forgetting using reinforced Bayesian .pdf:/Users/bert/Zotero/storage/NRBCG2I9/Moens and Zénon - 2019 - Learning and forgetting using reinforced Bayesian .pdf:application/pdf},
}

@article{jafarian_structure_2019,
	title = {Structure Learning in Coupled Dynamical Systems and Dynamic Causal Modelling},
	url = {https://arxiv.org/abs/1904.03093v1},
	abstract = {Identifying a coupled dynamical system out of many plausible candidates, each
of which could serve as the underlying generator of some observed measurements,
is a profoundly ill posed problem that commonly arises when modelling real
world phenomena. In this review, we detail a set of statistical procedures for
inferring the structure of nonlinear coupled dynamical systems (structure
learning), which has proved useful in neuroscience research. A key focus here
is the comparison of competing models of (ie, hypotheses about) network
architectures and implicit coupling functions in terms of their Bayesian model
evidence. These methods are collectively referred to as dynamical casual
modelling ({DCM}). We focus on a relatively new approach that is proving
remarkably useful; namely, Bayesian model reduction ({BMR}), which enables rapid
evaluation and comparison of models that differ in their network architecture.
We illustrate the usefulness of these techniques through modelling
neurovascular coupling (cellular pathways linking neuronal and vascular
systems), whose function is an active focus of research in neurobiology and the
imaging of coupled neuronal systems.},
	author = {Jafarian, Amirhossein and Zeidman, Peter and Litvak, Vladimir and Friston, Karl},
	urldate = {2019-04-12},
	date = {2019-03-28},
	langid = {english},
	file = {Jafarian et al. - 2019 - Structure Learning in Coupled Dynamical Systems an.pdf:/Users/bert/Zotero/storage/GU53I8C7/Jafarian et al. - 2019 - Structure Learning in Coupled Dynamical Systems an.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/G6XP7KSK/1904.html:text/html},
}

@inproceedings{reller_glue_2012,
	title = {Glue factors, likelihood computation, and filtering in state space models},
	doi = {10.1109/Allerton.2012.6483284},
	abstract = {Factor graphs of statistical models can be augmented by a glue factor that expresses some additional (initial, final, or otherwise “local”) condition. That applies, in particular, to (otherwise time-invariant) linear Gaussian state space models, which are thus generalized to pulse-like models that are localized anywhere in time. The model likelihood can then be computed by (forward-backward or forward-only) sum-product message passing, which leads to the concept of a likelihood filter. We propose to build (forward-only) likelihood filters from a bank of second-order linear systems. We also observe that such likelihood filters can be cascaded into a new sort of neural network that works naturally with multichannel time signals at multiple time scales.},
	eventtitle = {2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	pages = {686--689},
	booktitle = {2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
	author = {Reller, C. and Devarakonda, M. V. R. S. and Loeliger, H.},
	date = {2012-10},
	keywords = {Message passing, Filtering, Signal processing, Computational modeling, factor graphs, Gaussian processes, linear Gaussian state space models, filtering theory, statistical analysis, filtering, graph theory, Hidden Markov models, Switches, Probability, neural nets, neural network, glue factors, likelihood computation, likelihood filter, multichannel time signals, pulse-like models, statistical models},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/ED7X2688/6483284.html:text/html;Reller et al. - 2012 - Glue factors, likelihood computation, and filterin.pdf:/Users/bert/Zotero/storage/3PMNNDU3/Reller et al. - 2012 - Glue factors, likelihood computation, and filterin.pdf:application/pdf},
}

@book{rojo_principle_2018,
	title = {The Principle of Least Action by Alberto Rojo},
	url = {/core/books/principle-of-least-action/CEDF9D5B2E7ED03F63575F6E11F87BDF},
	abstract = {Cambridge Core - History, Philosophy and Foundations of  Physics - The Principle of Least Action -  by Alberto Rojo},
	author = {Rojo, Alberto and Bloch, Anthony},
	urldate = {2019-03-11},
	date = {2018-03},
	langid = {english},
	doi = {10.1017/9781139021029},
	file = {Rojo and Bloch - 2018 - The Principle of Least Action by Alberto Rojo.pdf:/Users/bert/Zotero/storage/YKUXFA8C/Rojo and Bloch - 2018 - The Principle of Least Action by Alberto Rojo.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YSIRAVW8/CEDF9D5B2E7ED03F63575F6E11F87BDF.html:text/html},
}

@article{rana_second_2016,
	title = {Second law, Landauer's Principle and Autonomous information machine},
	url = {http://arxiv.org/abs/1611.01993},
	abstract = {Second law of thermodynamics can be apparently violated for systems whose dynamics depends on acquired information by measurement. However, when one consider measurement and erasure process together along with the system it saves the second law. We consider a simple example of information machine where information is used as a resource to increase its performance. The system is connected to two baths, a work source and a moving tape which is used as an information reservoir. The performance of the device is autonomous. The system acts as an engine, erasure or refrigerator. Even combination of any two is possible. All these possibilities are allowed by generalized second law.},
	journaltitle = {{arXiv}:1611.01993 [cond-mat]},
	author = {Rana, Shubhashis and Jayannavar, A. M.},
	urldate = {2019-03-07},
	date = {2016-11-07},
	eprinttype = {arxiv},
	eprint = {1611.01993},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/STBEJ6AC/1611.html:text/html;Rana and Jayannavar - 2016 - Second law, Landauer's Principle and Autonomous in.pdf:/Users/bert/Zotero/storage/25ND6THB/Rana and Jayannavar - 2016 - Second law, Landauer's Principle and Autonomous in.pdf:application/pdf},
}

@article{henderson_deep_2017,
	title = {Deep Reinforcement Learning that Matters},
	url = {http://arxiv.org/abs/1709.06560},
	abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning ({RL}). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep {RL} methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep {RL} more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
	journaltitle = {{arXiv}:1709.06560 [cs, stat]},
	author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
	urldate = {2019-03-07},
	date = {2017-09-19},
	eprinttype = {arxiv},
	eprint = {1709.06560},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3XVDBSMU/1709.html:text/html;Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:/Users/bert/Zotero/storage/UX5GRFYY/Henderson et al. - 2017 - Deep Reinforcement Learning that Matters.pdf:application/pdf},
}

@article{rasmussen_neural_2017,
	title = {A neural model of hierarchical reinforcement learning},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180234},
	doi = {10.1371/journal.pone.0180234},
	abstract = {We develop a novel, biologically detailed neural model of reinforcement learning ({RL}) processes in the brain. This model incorporates a broad range of biological features that pose challenges to neural {RL}, such as temporally extended action sequences, continuous environments involving unknown time delays, and noisy/imprecise computations. Most significantly, we expand the model into the realm of hierarchical reinforcement learning ({HRL}), which divides the {RL} process into a hierarchy of actions at different levels of abstraction. Here we implement all the major components of {HRL} in a neural model that captures a variety of known anatomical and physiological properties of the brain. We demonstrate the performance of the model in a range of different environments, in order to emphasize the aim of understanding the brain’s general reinforcement learning ability. These results show that the model compares well to previous modelling work and demonstrates improved performance as a result of its hierarchical ability. We also show that the model’s behaviour is consistent with available data on human hierarchical {RL}, and generate several novel predictions.},
	pages = {e0180234},
	number = {7},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Rasmussen, Daniel and Voelker, Aaron and Eliasmith, Chris},
	urldate = {2019-03-05},
	date = {2017-07-06},
	langid = {english},
	keywords = {Behavior, Neurons, Learning, Algorithms, Human performance, Memory, Neostriatum, Simulation and modeling},
	file = {Rasmussen et al. - 2017 - A neural model of hierarchical reinforcement learn.pdf:/Users/bert/Zotero/storage/TMLZMBKL/Rasmussen et al. - 2017 - A neural model of hierarchical reinforcement learn.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CJZ38YT3/article.html:text/html},
}

@book{baumann_concepts_nodate,
	title = {Concepts in Theoretical Physics, Part 1A Mathematical Tripos},
	author = {Baumann, Daniel},
	file = {Baumann - Concepts in Theoretical Physics, Part 1A Mathemati.pdf:/Users/bert/Zotero/storage/QEKY5SJN/Baumann - Concepts in Theoretical Physics, Part 1A Mathemati.pdf:application/pdf},
}

@inproceedings{petersen_approximate_2018,
	title = {On Approximate Nonlinear Gaussian Message Passing on Factor Graphs},
	doi = {10.1109/SSP.2018.8450699},
	abstract = {Factor graphs have recently gained increasing attention as a unified framework for representing and constructing algorithms for signal processing, estimation, and control. One capability that does not seem to be well explored within the factor graph tool kit is the ability to handle deterministic nonlinear transformations, such as those occuring in nonlinear filtering and smoothing problems, using tabulated message passing rules. In this contribution, we provide general forward (filtering) and backward (smoothing) approximate Gaussian message passing rules for deterministic nonlinear transformation nodes in arbitrary factor graphs fulfilling a Markov property, based on numerical quadrature procedures for the forward pass and a Rauch-Tung-Striebel-type approximation of the backward pass. These message passing rules can be employed for deriving many algorithms for solving nonlinear problems using factor graphs, as is illustrated by the proposition of a nonlinear modified Bryson-Frazier ({MBF}) smoother based on the presented message passing rules.},
	eventtitle = {2018 {IEEE} Statistical Signal Processing Workshop ({SSP})},
	pages = {513--517},
	booktitle = {2018 {IEEE} Statistical Signal Processing Workshop ({SSP})},
	author = {Petersen, E. and Hoffmann, C. and Rostalski, P.},
	date = {2018-06},
	keywords = {Message passing, Signal processing algorithms, Markov processes, Signal processing, Conferences, Smoothing methods, Approximation algorithms, Factor Graphs, message Passing, nonlinear Filtering, nonlinear Smoothing, sigma Point Filtering},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/4DYXQNMT/8450699.html:text/html;Petersen et al. - 2018 - On Approximate Nonlinear Gaussian Message Passing .pdf:/Users/bert/Zotero/storage/Z9MJXP7S/Z9MJXP7S.pdf:application/pdf},
}

@article{omelianenko_artificial_2019,
	title = {Artificial Swarm Intelligence and Cooperative Robotic Systems},
	url = {https://www.preprints.org/manuscript/201901.0282/v1},
	doi = {10.20944/preprints201901.0282.v1},
	abstract = {In this paper, we look at how Artificial Swarm Intelligence can evolve using evolutionary algorithms that try to minimize the sensory surprise of the system. We will show how to apply the free-energy principle, borrowed from statistical physics, to quantitatively describe the optimization method (sensory surprise minimization), which can be used to support lifelong learning. We provide our ideas about how to combine this optimization method with evolutionary algorithms in order to boost the development of specialized Artificial Neural Networks, which define the proprioceptive configuration of particular robotic units that are part of a swarm. We consider how optimization of the free-energy can promote the homeostasis of the swarm system, i.e. ensures that the system remains within its sensory boundaries throughout its active lifetime. We will show how complex distributed cognitive systems can be build in the form of hierarchical modular system, which consists of specialized micro-intelligent agents connected through information channels. We will also consider the co-evolution of various robotic swarm units, which can result in development of proprioception and a comprehensive awareness of the properties of the environment. And finally, we will give a brief outline of how this system can be implemented in practice and of our progress in this area.},
	author = {Omelianenko, Iaroslav},
	urldate = {2019-02-24},
	date = {2019-01-28},
	langid = {english},
	file = {Omelianenko - 2019 - Artificial Swarm Intelligence and Cooperative Robo.pdf:/Users/bert/Zotero/storage/225DUW4P/Omelianenko - 2019 - Artificial Swarm Intelligence and Cooperative Robo.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ABSFL9RE/v1.html:text/html},
}

@article{annila_divide_2015,
	title = {On the divide between animate and inanimate},
	volume = {6},
	issn = {1759-2208},
	url = {https://doi.org/10.1186/s13322-015-0008-8},
	doi = {10.1186/s13322-015-0008-8},
	abstract = {Vitalism was abandoned already for a long time ago, yet the impression that animate beings differ in some fundamental way from inanimate objects continues to thrive. Here, we argue that scale free patterns, found throughout nature, present convincing evidence that this demarcation is only imaginary. Therefore, all systems ought to be regarded alike, i.e., all are consuming free energy in least time. This way evolutionary processes can be understood as a series of changes from one state to another, so that flows of energy themselves naturally select those ways and means, such as species and societies or gadgets and galaxies to consume free energy in the least time in quest of attaining thermodynamic balance in respective surroundings. This holistic worldview, albeit an accurate account of nature, was shelved soon after its advent at the turn of the 18th century, because the general tenet did not meet that time expectations of a deterministic law, but now it is time to reconsider the old universal imperative against observations rather than expectations.},
	pages = {2},
	number = {1},
	journaltitle = {Journal of Systems Chemistry},
	shortjournal = {Journal of Systems Chemistry},
	author = {Annila, Arto and Kolehmainen, Erkki},
	urldate = {2019-02-22},
	date = {2015-02-19},
	file = {Annila and Kolehmainen - 2015 - On the divide between animate and inanimate.pdf:/Users/bert/Zotero/storage/M7VXBGKP/Annila and Kolehmainen - 2015 - On the divide between animate and inanimate.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RK4JSSBH/s13322-015-0008-8.html:text/html},
}

@article{caticha_relative_2004,
	title = {Relative Entropy and Inductive Inference},
	volume = {707},
	issn = {0094243X},
	url = {http://arxiv.org/abs/physics/0311093},
	doi = {10.1063/1.1751358},
	abstract = {We discuss how the method of maximum entropy, {MaxEnt}, can be extended beyond its original scope, as a rule to assign a probability distribution, to a full-fledged method for inductive inference. The main concept is the (relative) entropy S[p{\textbar}q] which is designed as a tool to update from a prior probability distribution q to a posterior probability distribution p when new information in the form of a constraint becomes available. The extended method goes beyond the mere selection of a single posterior p, but also addresses the question of how much less probable other distributions might be. Our approach clarifies how the entropy S[p{\textbar}q] is used while avoiding the question of its meaning. Ultimately, entropy is a tool for induction which needs no interpretation. Finally, being a tool for generalization from special examples, we ask whether the functional form of the entropy depends on the choice of the examples and we find that it does. The conclusion is that there is no single general theory of inductive inference and that alternative expressions for the entropy are possible.},
	pages = {75--96},
	journaltitle = {{AIP} Conference Proceedings},
	author = {Caticha, Ariel},
	urldate = {2019-02-05},
	date = {2004},
	eprinttype = {arxiv},
	eprint = {physics/0311093},
	keywords = {Physics - Data Analysis, Statistics and Probability, Physics - General Physics},
	file = {arXiv\:physics/0311093 PDF:/Users/bert/Zotero/storage/2LJAJALS/Caticha - 2004 - Relative Entropy and Inductive Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/4SBCABK3/0311093.html:text/html},
}

@article{parr_neuronal_2019,
	title = {Neuronal message passing using Mean-field, Bethe, and Marginal approximations},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-018-38246-3},
	doi = {10.1038/s41598-018-38246-3},
	abstract = {Neuronal computations rely upon local interactions across synapses. For a neuronal network to perform inference, it must integrate information from locally computed messages that are propagated among elements of that network. We review the form of two popular (Bayesian) message passing schemes and consider their plausibility as descriptions of inference in biological networks. These are variational message passing and belief propagation – each of which is derived from a free energy functional that relies upon different approximations (mean-field and Bethe respectively). We begin with an overview of these schemes and illustrate the form of the messages required to perform inference using Hidden Markov Models as generative models. Throughout, we use factor graphs to show the form of the generative models and of the messages they entail. We consider how these messages might manifest neuronally and simulate the inferences they perform. While variational message passing offers a simple and neuronally plausible architecture, it falls short of the inferential performance of belief propagation. In contrast, belief propagation allows exact computation of marginal posteriors at the expense of the architectural simplicity of variational message passing. As a compromise between these two extremes, we offer a third approach – marginal message passing – that features a simple architecture, while approximating the performance of belief propagation. Finally, we link formal considerations to accounts of neurological and psychiatric syndromes in terms of aberrant message passing.},
	pages = {1889},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Parr, Thomas and Markovic, Dimitrije and Kiebel, Stefan J. and Friston, Karl J.},
	urldate = {2019-02-19},
	date = {2019-02-13},
	file = {Parr et al. - 2019 - Neuronal message passing using Mean-field, Bethe, .pdf:/Users/bert/Zotero/storage/2IQWQLUQ/Parr et al. - 2019 - Neuronal message passing using Mean-field, Bethe, .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/HZEN74RG/s41598-018-38246-3.html:text/html},
}

@article{wolpert_overview_2018,
	title = {Overview of Information Theory, Computer Science Theory, and Stochastic Thermodynamics for Thermodynamics of Computation},
	url = {http://arxiv.org/abs/1901.00386},
	abstract = {I give a quick overview of some of the theoretical background necessary for using modern nonequilibrium statistical physics to investigate the thermodynamics of computation. I first present some of the necessary concepts from information theory, and then introduce some of the most important types of computational machine considered in computer science theory. After this I present a central result from modern nonequilibrium statistical physics: an exact expression for the entropy flow out of a system undergoing a given dynamics with a given initial distribution over states. This central expression is crucial for analyzing how the total entropy flow out of a computer depends on its global structure, since that global structure determines the initial distributions into all of the computer's subsystems, and therefore (via the central expression) the entropy flows generated by all of those subsystems. I illustrate these results by analyzing some of the subtleties concerning the benefits that are sometimes claimed for implementing an irreversible computation with a reversible circuit constructed out of Fredkin gates.},
	journaltitle = {{arXiv}:1901.00386 [cond-mat]},
	author = {Wolpert, David H.},
	urldate = {2019-02-15},
	date = {2018-12-30},
	eprinttype = {arxiv},
	eprint = {1901.00386},
	keywords = {Condensed Matter - Statistical Mechanics, 03D15, F.1, F.2.3},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/MS2G92LE/1901.html:text/html;Wolpert - 2018 - Overview of Information Theory, Computer Science T.pdf:/Users/bert/Zotero/storage/ZXQ7DTLN/Wolpert - 2018 - Overview of Information Theory, Computer Science T.pdf:application/pdf},
}

@article{obermeyer_tensor_2019,
	title = {Tensor Variable Elimination for Plated Factor Graphs},
	url = {http://arxiv.org/abs/1902.03210},
	abstract = {A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.},
	journaltitle = {{arXiv}:1902.03210 [cs, stat]},
	author = {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Chiu, Justin and Pradhan, Neeraj and Rush, Alexander and Goodman, Noah},
	urldate = {2019-02-14},
	date = {2019-02-08},
	eprinttype = {arxiv},
	eprint = {1902.03210},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/EZ3KGA8Y/1902.html:text/html;Obermeyer et al. - 2019 - Tensor Variable Elimination for Plated Factor Grap.pdf:/Users/bert/Zotero/storage/9BKDW2KE/Obermeyer et al. - 2019 - Tensor Variable Elimination for Plated Factor Grap.pdf:application/pdf},
}

@article{jaynes_information_1957,
	title = {Information Theory and Statistical Mechanics},
	volume = {106},
	url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	doi = {10.1103/PhysRev.106.620},
	abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
	pages = {620--630},
	number = {4},
	journaltitle = {Physical Review},
	shortjournal = {Phys. Rev.},
	author = {Jaynes, E. T.},
	urldate = {2019-02-12},
	date = {1957-05-15},
	file = {APS Snapshot:/Users/bert/Zotero/storage/ZJZYZR5Y/PhysRev.106.html:text/html;Jaynes - 1957 - Information Theory and Statistical Mechanics.pdf:/Users/bert/Zotero/storage/65IIJQRF/Jaynes - 1957 - Information Theory and Statistical Mechanics.pdf:application/pdf},
}

@article{wu_fixing_2018,
	title = {Fixing Variational Bayes: Deterministic Variational Inference for Bayesian Neural Networks},
	url = {http://arxiv.org/abs/1810.03958},
	shorttitle = {Fixing Variational Bayes},
	abstract = {Bayesian neural networks ({BNNs}) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes ({VB}) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for {BNNs} in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We fix {VB} and turn it into a robust inference tool for Bayesian neural networks. We achieve this with two innovations: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate strong predictive performance over alternative approaches.},
	journaltitle = {{arXiv}:1810.03958 [cs, stat]},
	author = {Wu, Anqi and Nowozin, Sebastian and Meeds, Edward and Turner, Richard E. and Hernández-Lobato, José Miguel and Gaunt, Alexander L.},
	urldate = {2019-02-07},
	date = {2018-10-09},
	eprinttype = {arxiv},
	eprint = {1810.03958},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/E7XVYMLE/1810.html:text/html;Wu et al. - 2018 - Fixing Variational Bayes Deterministic Variationa.pdf:/Users/bert/Zotero/storage/EBKVAJC5/Wu et al. - 2018 - Fixing Variational Bayes Deterministic Variationa.pdf:application/pdf},
}

@article{aitchison_unified_2018,
	title = {A unified theory of adaptive stochastic gradient descent as Bayesian filtering},
	url = {http://arxiv.org/abs/1807.07540},
	abstract = {We formulate stochastic gradient descent ({SGD}) as a Bayesian filtering problem. Inference in the Bayesian setting naturally gives rise to {BRMSprop} and {BAdam}: Bayesian variants of {RMSprop} and Adam. Remarkably, the Bayesian approach recovers many features of state-of-the-art adaptive {SGD} methods, including amoungst others root-mean-square normalization, Nesterov acceleration and {AdamW}. As such, the Bayesian approach provides one explanation for the empirical effectiveness of state-of-the-art adaptive {SGD} algorithms. Empirically comparing {BRMSprop} and {BAdam} with naive {RMSprop} and Adam on {MNIST}, we find that Bayesian methods have the potential to considerably reduce test loss and classification error.},
	journaltitle = {{arXiv}:1807.07540 [cs, stat]},
	author = {Aitchison, Laurence},
	urldate = {2019-02-07},
	date = {2018-07-19},
	eprinttype = {arxiv},
	eprint = {1807.07540},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Aitchison - 2018 - A unified theory of adaptive stochastic gradient d.pdf:/Users/bert/Zotero/storage/TMQVN73T/Aitchison - 2018 - A unified theory of adaptive stochastic gradient d.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/HZ7T4XYW/1807.html:text/html},
}

@inproceedings{ghosh_assumed_2016,
	title = {Assumed Density Filtering Methods for Learning Bayesian Neural Networks},
	rights = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12391},
	abstract = {Buoyed by the success of deep multilayer neural networks, there is renewed interest in scalable learning of Bayesian neural networks. Here, we study algorithms that utilize recent advances in Bayesian inference to efficiently learn distributions over network weights. In particular, we focus on recently proposed assumed density filtering based methods for learning Bayesian neural networks -- Expectation and Probabilistic backpropagation. Apart from scaling to large datasets, these techniques seamlessly deal with non-differentiable activation functions and provide parameter (learning rate, momentum) free learning. In this paper, we first rigorously compare the two algorithms and in the process develop several extensions, including a version of {EBP} for continuous regression problems and a {PBP} variant for binary classification. Next, we extend both algorithms to deal with multiclass classification and count regression problems. On a variety of diverse real world benchmarks, we find our extensions to be effective, achieving results competitive with the state-of-the-art.},
	eventtitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	booktitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	author = {Ghosh, Soumya and Fave, Francesco Maria Delle and Yedidia, Jonathan},
	urldate = {2019-02-07},
	date = {2016-02-21},
	langid = {english},
	file = {Ghosh et al. - 2016 - Assumed Density Filtering Methods for Learning Bay.pdf:/Users/bert/Zotero/storage/MSAYFAHY/Ghosh et al. - 2016 - Assumed Density Filtering Methods for Learning Bay.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/QUGHCC6D/12391.html:text/html},
}

@article{wilkinson_end--end_2019,
	title = {End-to-End Probabilistic Inference for Nonstationary Audio Analysis},
	url = {http://arxiv.org/abs/1901.11436},
	abstract = {A typical audio signal processing pipeline includes multiple disjoint analysis stages, including calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a spectral mixture Gaussian process model with nonstationary priors over the amplitude variance parameters. Further, we formulate this nonlinear model's state space representation, making it amenable to infinite-horizon Gaussian process regression with approximate inference via expectation propagation, which scales linearly in the number of time steps and quadratically in the state dimensionality. By doing so, we are able to process audio signals with hundreds of thousands of data points. We demonstrate, on various tasks with empirical data, how this inference scheme outperforms more standard techniques that rely on extended Kalman filtering.},
	journaltitle = {{arXiv}:1901.11436 [cs, eess, stat]},
	author = {Wilkinson, William J. and Andersen, Michael Riis and Reiss, Joshua D. and Stowell, Dan and Solin, Arno},
	urldate = {2019-02-05},
	date = {2019-01-31},
	eprinttype = {arxiv},
	eprint = {1901.11436},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CNPULFNX/1901.html:text/html;Wilkinson et al. - 2019 - End-to-End Probabilistic Inference for Nonstationa.pdf:/Users/bert/Zotero/storage/QWTEG366/Wilkinson et al. - 2019 - End-to-End Probabilistic Inference for Nonstationa.pdf:application/pdf},
}

@article{fellows_virel:_2018,
	title = {{VIREL}: A Variational Inference Framework for Reinforcement Learning},
	url = {http://arxiv.org/abs/1811.01132},
	shorttitle = {{VIREL}},
	abstract = {Applying probabilistic models to reinforcement learning ({RL}) enables the application of powerful optimisation tools such as variational inference to {RL}. However, existing inference frameworks and their algorithms pose significant challenges for learning optimal policies, e.g., the absence of mode capturing behaviour in pseudo-likelihood methods and difficulties learning deterministic policies in maximum entropy {RL} based approaches. We propose {VIREL}, a novel, theoretically grounded probabilistic inference framework for {RL} that utilises a parametrised action-value function to summarise future dynamics of the underlying {MDP}. This gives {VIREL} a mode-seeking form of {KL} divergence, the ability to learn deterministic optimal polices naturally from inference and the ability to optimise value functions and policies in separate, iterative steps. In applying variational expectation-maximisation to {VIREL} we thus show that the actor-critic algorithm can be reduced to expectation-maximisation, with policy improvement equivalent to an E-step and policy evaluation to an M-step. We then derive a family of actor-critic methods from {VIREL}, including a scheme for adaptive exploration. Finally, we demonstrate that actor-critic algorithms from this family outperform state-of-the-art methods based on soft value functions in several domains.},
	journaltitle = {{arXiv}:1811.01132 [cs, stat]},
	author = {Fellows, Matthew and Mahajan, Anuj and Rudner, Tim G. J. and Whiteson, Shimon},
	urldate = {2019-02-05},
	date = {2018-11-02},
	eprinttype = {arxiv},
	eprint = {1811.01132},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/S7V8J7NL/1811.html:text/html;Fellows et al. - 2018 - VIREL A Variational Inference Framework for Reinf.pdf:/Users/bert/Zotero/storage/9JVLLKHZ/Fellows et al. - 2018 - VIREL A Variational Inference Framework for Reinf.pdf:application/pdf},
}

@inproceedings{loeliger_factor_2018,
	title = {Factor Graphs with {NUV} Priors and Iteratively Reweighted Descent for Sparse Least Squares and More},
	doi = {10.1109/ISTC.2018.8625332},
	abstract = {Normal priors with unknown variance ({NUV}) are well known to include a large class of sparsity promoting priors and to blend well with Gaussian message passing. Essentially equivalently, sparsifying norms (including the L1 norm) as well as the Huber cost function from robust statistics have variational representations that lead to algorithms based on iteratively reweighted L2-regularization. In this paper, we rephrase these well-known facts in terms of factor graphs. In particular, we propose a smoothed-{NUV} representation of the Huber function and of a related nonconvex cost function, and we illustrate their use for sparse least-squares with outliers and in a natural (piecewise smooth) prior for imaging. We also point out pertinent iterative algorithms including variations of gradient descent and coordinate descent.},
	eventtitle = {2018 {IEEE} 10th International Symposium on Turbo Codes Iterative Information Processing ({ISTC})},
	pages = {1--5},
	booktitle = {2018 {IEEE} 10th International Symposium on Turbo Codes Iterative Information Processing ({ISTC})},
	author = {Loeliger, H. and Ma, B. and Malmberg, H. and Wadehn, F.},
	date = {2018-12},
	keywords = {Message passing, Approximation algorithms, Standards, Cost function, Imaging, Information processing, Turbo codes},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/UXPJ75N3/8625332.html:text/html;Loeliger et al. - 2018 - Factor Graphs with NUV Priors and Iteratively Rewe.pdf:/Users/bert/Zotero/storage/HU6NWGMT/Loeliger et al. - 2018 - Factor Graphs with NUV Priors and Iteratively Rewe.pdf:application/pdf},
}

@article{tribus_energy_1971,
	title = {Energy and Information},
	volume = {225},
	issn = {0036-8733},
	url = {http://www.nature.com/doifinder/10.1038/scientificamerican0971-179},
	doi = {10.1038/scientificamerican0971-179},
	pages = {179--188},
	number = {3},
	journaltitle = {Scientific American},
	author = {Tribus, Myron and {McIrvine}, Edward C.},
	urldate = {2019-01-21},
	date = {1971-09},
	file = {Tribus and McIrvine - 1971 - Energy and Information.pdf:/Users/bert/Zotero/storage/XJZTWW8U/Tribus and McIrvine - 1971 - Energy and Information.pdf:application/pdf},
}

@article{ortega_information-theoretic_2015,
	title = {Information-Theoretic Bounded Rationality},
	url = {http://arxiv.org/abs/1512.06789},
	abstract = {Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. {EXPECTIMAX} and {MINIMAX}) as limit cases, as well as trust- and risk-sensitive planning.},
	journaltitle = {{arXiv}:1512.06789 [cs, math, stat]},
	author = {Ortega, Pedro A. and Braun, Daniel A. and Dyer, Justin and Kim, Kee-Eung and Tishby, Naftali},
	urldate = {2019-01-21},
	date = {2015-12-21},
	eprinttype = {arxiv},
	eprint = {1512.06789},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Statistics - Machine Learning, Computer Science - Systems and Control},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CBFWNHC5/1512.html:text/html;Ortega et al. - 2015 - Information-Theoretic Bounded Rationality.pdf:/Users/bert/Zotero/storage/N39LT4G7/Ortega et al. - 2015 - Information-Theoretic Bounded Rationality.pdf:application/pdf},
}

@article{koelsch_predictive_2019,
	title = {Predictive Processes and the Peculiar Case of Music},
	volume = {23},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(18)30254-7},
	doi = {10.1016/j.tics.2018.10.006},
	pages = {63--77},
	number = {1},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Koelsch, Stefan and Vuust, Peter and Friston, Karl},
	urldate = {2019-01-10},
	date = {2019-01-01},
	pmid = {30471869},
	keywords = {active inference, predictive coding, auditory processing, embodiment, {ERAN}, {MMN}, music perception},
	file = {Koelsch et al. - 2019 - Predictive Processes and the Peculiar Case of Musi.pdf:/Users/bert/Zotero/storage/E9JVKL75/Koelsch et al. - 2019 - Predictive Processes and the Peculiar Case of Musi.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/M9UERSR9/S1364-6613(18)30254-7.html:text/html},
}

@article{schulman_gradient_2015,
	title = {Gradient Estimation Using Stochastic Computation Graphs},
	url = {http://arxiv.org/abs/1506.05254},
	abstract = {In a variety of problems originating in supervised, unsupervised, and reinforcement learning, the loss function is defined by an expectation over a collection of random variables, which might be part of a probabilistic model or the external world. Estimating the gradient of this loss function, using samples, lies at the core of gradient-based learning algorithms for these problems. We introduce the formalism of stochastic computation graphs---directed acyclic graphs that include both deterministic functions and conditional probability distributions---and describe how to easily and automatically derive an unbiased estimator of the loss function's gradient. The resulting algorithm for computing the gradient estimator is a simple modification of the standard backpropagation algorithm. The generic scheme we propose unifies estimators derived in variety of prior work, along with variance-reduction techniques therein. It could assist researchers in developing intricate models involving a combination of stochastic and deterministic operations, enabling, for example, attention, memory, and control actions.},
	journaltitle = {{arXiv}:1506.05254 [cs]},
	author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
	urldate = {2019-01-10},
	date = {2015-06-17},
	eprinttype = {arxiv},
	eprint = {1506.05254},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/AT3WD32T/1506.html:text/html;Schulman et al. - 2015 - Gradient Estimation Using Stochastic Computation G.pdf:/Users/bert/Zotero/storage/4FRCZKQM/Schulman et al. - 2015 - Gradient Estimation Using Stochastic Computation G.pdf:application/pdf},
}

@article{ortega_thermodynamics_2013,
	title = {Thermodynamics as a theory of decision-making with information-processing costs},
	volume = {469},
	rights = {© 2013 The Author(s) Published by the Royal Society. All rights reserved.},
	issn = {1364-5021, 1471-2946},
	url = {http://rspa.royalsocietypublishing.org/content/469/2153/20120683},
	doi = {10.1098/rspa.2012.0683},
	abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
	pages = {20120683},
	number = {2153},
	journaltitle = {Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences},
	author = {Ortega, Pedro A. and Braun, Daniel A.},
	urldate = {2015-03-06},
	date = {2013-05-08},
	langid = {english},
	file = {Ortega and Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf:/Users/bert/Zotero/storage/DDL9J54W/Ortega and Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/J73DUXDP/20120683.html:text/html},
}

@incollection{s._lent_information_2019,
	title = {Information and Entropy in Physical Systems: A Review of Landauer’s Principle, Theory and Experiments},
	isbn = {978-3-319-93457-0},
	shorttitle = {Information and Entropy in Physical Systems},
	abstract = {The Landauer Principle connects the information theoretic notion of entropy to the physics of statistical mechanics. When a physical system performs a logical operation that erases or loses information, without a copy being preserved, it must transfer a minimum amount of heat, {\textbackslash}(k\_B T {\textbackslash}log (2){\textbackslash}), to the environment. How can there be such a connection between the abstract idea of information and the concrete physical reality of heat? To address this question, we adopt the Jaynes approach of grounding statistical mechanics in the Shannon notion of entropy. Probability is a quantification of incomplete information. Entropy should not be conceived in terms of disorder, but rather as a measure on a probability distribution that characterizes the amount of missing information the distribution represents. The thermodynamic entropy is a special case of the Shannon entropy applied to a physical system in equilibrium with a heat bath so that its average energy is fixed. The thermal probability distribution is obtained by maximizing the Shannon entropy, subject to the physical constraints of the problem. It is then possible to naturally extend this description to include a physical memory device, which must be in a nonequilibrium long-lived metastable state. We can then explicitly demonstrate how the requirement for a fundamental minimum energy dissipation is tied to erasure of an unknown bit. Both classical and quantum cases are considered. We show that the classical thermodynamic entropy is in some situations best matched in quantum mechanics, not by the von Neumann entropy, but by a perhaps less familiar quantity—the quantum entropy of outcomes. The case of free expansion of an ideal quantum gas is examined in this context.},
	pages = {1--63},
	author = {S. Lent, Craig},
	date = {2019-01-01},
	doi = {10.1007/978-3-319-93458-7_1},
	file = {S. Lent - 2019 - Information and Entropy in Physical Systems A Rev.pdf:/Users/bert/Zotero/storage/NAY7QNXW/S. Lent - 2019 - Information and Entropy in Physical Systems A Rev.pdf:application/pdf},
}

@article{ben-naim_can_2017,
	title = {Can entropy be defined for, and the Second Law applied to living systems?},
	url = {http://arxiv.org/abs/1705.02461},
	abstract = {This article provides answers to the questions posed in the title. Contrary to the most common views, we show that neither the entropy, nor the Second Law may be used for either living systems, or to life phenomenon.},
	journaltitle = {{arXiv}:1705.02461 [cond-mat, physics:physics, q-bio]},
	author = {Ben-Naim, Arieh},
	urldate = {2019-01-02},
	date = {2017-05-06},
	eprinttype = {arxiv},
	eprint = {1705.02461},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Popular Physics, Quantitative Biology - Other Quantitative Biology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/J7JKS53N/1705.html:text/html;Ben-Naim - 2017 - Can entropy be defined for, and the Second Law app.pdf:/Users/bert/Zotero/storage/9YHMB787/Ben-Naim - 2017 - Can entropy be defined for, and the Second Law app.pdf:application/pdf},
}

@article{deli_thermodynamic_2018,
	title = {The Thermodynamic Analysis of Neural Computation},
	volume = {2018},
	url = {https://www.scitechnol.com/abstract/the-thermodynamic-analysis-of-neural-computation-7339.html},
	abstract = {The brain displays a low-frequency ground energy confirmation, called the resting state, which is characterized by an energy/ information balance via self-regulatory mechanisms. Despite the high-frequency evoked activity, e.g., the detail-oriented sensory processing of environmental data and the accumulation of information, nevertheless the brain’s automatic regulation is always able to recover the resting state. Indeed, we show that the two energetic processes, activation that decreases temporal dimensionality via transient bifurcations and the ensuing brain’s response, lead to complementary and symmetric procedures that satisfy the Landauer’s principle. Landauer’s principle, which states that information era- sure requires energy predicts heat accumulation in the system, this means that information accumulation is correlated with increases in temperature and lead to actions that recover the resting state. We explain how brain synaptic networks frame a closed system, similar to the Carnot cycle where the information/ energy cycle accumulates energy in synaptic connections. In deep learning, representation of information might occur via the same mechanism},
	journaltitle = {Journal of Neuroscience \& Clinical Research},
	author = {Deli, E and Peters, Jf and Tozzi, A},
	urldate = {2018-12-30},
	date = {2018-03-20},
	langid = {english},
	file = {Deli et al. - 2018 - The Thermodynamic Analysis of Neural Computation.pdf:/Users/bert/Zotero/storage/PS3223P3/Deli et al. - 2018 - The Thermodynamic Analysis of Neural Computation.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ZGMX7CBR/the-thermodynamic-analysis-of-neural-computation-FV5x.html:text/html},
}

@article{goldt_thermodynamic_2017,
	title = {Thermodynamic efficiency of learning a rule in neural networks},
	url = {https://arxiv.org/abs/1706.09713v1},
	doi = {10.1088/1367-2630/aa89ff},
	abstract = {Biological systems have to build models from their sensory data that allow
them to efficiently process previously unseen inputs. Here, we study a neural
network learning a linearly separable rule using examples provided by a
teacher. We analyse the ability of the network to apply the rule to new inputs,
that is to generalise from past experience. Using stochastic thermodynamics, we
show that the thermodynamic costs of the learning process provide an upper
bound on the amount of information that the network is able to learn from its
teacher for both batch and online learning. This allows us to introduce a
thermodynamic efficiency of learning. We analytically compute the dynamics and
the efficiency of a noisy neural network performing online learning in the
thermodynamic limit. In particular, we analyse three popular learning
algorithms, namely Hebbian, Perceptron and {AdaTron} learning. Our work extends
the methods of stochastic thermodynamics to a new type of learning problem and
might form a suitable basis for investigating the thermodynamics of
decision-making.},
	author = {Goldt, Sebastian and Seifert, Udo},
	urldate = {2018-12-31},
	date = {2017-06-29},
	langid = {english},
	file = {Goldt and Seifert - 2017 - Thermodynamic efficiency of learning a rule in neu.pdf:/Users/bert/Zotero/storage/Y8MIXBNB/Goldt and Seifert - 2017 - Thermodynamic efficiency of learning a rule in neu.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3KLI68U3/1706.html:text/html},
}

@online{noauthor_sensory_nodate,
	title = {Sensory cortex is optimized for prediction of future input {\textbar} {eLife}},
	url = {https://elifesciences.org/articles/31557},
	urldate = {2018-12-31},
	file = {Sensory cortex is optimized for prediction of future input | eLife:/Users/bert/Zotero/storage/ATTQN7XQ/31557.html:text/html},
}

@article{kurthen_bayesian_2018,
	title = {Bayesian Causal Inference},
	url = {https://arxiv.org/abs/1812.09895v1},
	abstract = {We address the problem of two-variable causal inference. This task is to
infer an existing causal relation between two random variables, i.e. \$X
{\textbackslash}rightarrow Y\$ or \$Y {\textbackslash}rightarrow X\$, from purely observational data. We briefly
review a number of state-of-the-art methods for this, including very recent
ones. A novel inference method is introduced, Bayesian Causal Inference ({BCI}),
which assumes a generative Bayesian hierarchical model to pursue the strategy
of Bayesian model selection. In the model the distribution of the cause
variable is given by a Poisson lognormal distribution, which allows to
explicitly regard discretization effects. We assume Fourier diagonal Field
covariance operators. The generative model assumed provides synthetic causal
data for benchmarking our model in comparison to existing State-of-the-art
models, namely {LiNGAM}, {ANM}-{HSIC}, {ANM}-{MML}, {IGCI} and {CGNN}. We explore how well
the above methods perform in case of high noise settings, strongly discretized
data and very sparse data. {BCI} performs generally reliable with synthetic data
as well as with the real world {TCEP} benchmark set, with an accuracy comparable
to state-of-the-art algorithms.},
	author = {Kurthen, Maximilian and Enßlin, Torsten A.},
	urldate = {2018-12-30},
	date = {2018-12-24},
	langid = {english},
	file = {Kurthen and Enßlin - 2018 - Bayesian Causal Inference.pdf:/Users/bert/Zotero/storage/MLQ6YBCQ/Kurthen and Enßlin - 2018 - Bayesian Causal Inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4YC7QXXB/1812.html:text/html},
}

@article{pedersen_drift_2017,
	title = {The drift diffusion model as the choice rule in reinforcement learning},
	volume = {24},
	issn = {1531-5320},
	doi = {10.3758/s13423-016-1199-y},
	abstract = {Current reinforcement-learning models often assume simplified decision processes that do not fully reflect the dynamic complexities of choice processes. Conversely, sequential-sampling models of decision making account for both choice accuracy and response time, but assume that decisions are based on static decision values. To combine these two computational models of decision making and learning, we implemented reinforcement-learning models in which the drift diffusion model describes the choice process, thereby capturing both within- and across-trial dynamics. To exemplify the utility of this approach, we quantitatively fit data from a common reinforcement-learning paradigm using hierarchical Bayesian parameter estimation, and compared model variants to determine whether they could capture the effects of stimulant medication in adult patients with attention-deficit hyperactivity disorder ({ADHD}). The model with the best relative fit provided a good description of the learning process, choices, and response times. A parameter recovery experiment showed that the hierarchical Bayesian modeling approach enabled accurate estimation of the model parameters. The model approach described here, using simultaneous estimation of reinforcement-learning and drift diffusion model parameters, shows promise for revealing new insights into the cognitive and neural mechanisms of learning and decision making, as well as the alteration of such processes in clinical groups.},
	pages = {1234--1251},
	number = {4},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Pedersen, Mads Lund and Frank, Michael J. and Biele, Guido},
	date = {2017},
	pmid = {27966103},
	pmcid = {PMC5487295},
	keywords = {Humans, Decision making, Bayes Theorem, Bayesian modeling, Adult, Attention Deficit Disorder with Hyperactivity, Choice Behavior, Mathematical models, Models, Theoretical, Reinforcement (Psychology), Reinforcement learning},
	file = {Pedersen et al. - 2017 - The drift diffusion model as the choice rule in re.pdf:/Users/bert/Zotero/storage/PMJYV6TG/Pedersen et al. - 2017 - The drift diffusion model as the choice rule in re.pdf:application/pdf},
}

@book{kondepudi_modern_2015,
	location = {Chichester ; New York},
	edition = {2nd edition},
	title = {Modern Thermodynamics: From Heat Engines to Dissipative Structures},
	isbn = {978-0-471-97394-2},
	shorttitle = {Modern Thermodynamics},
	abstract = {Thermodynamics is a core part of most science and engineering curricula. However, most texts that are currently available to students still treat thermodynamics very much as it was presented in the 19th century, generally for historical rather than pedagogical reasons. Modern Thermodynamics takes a different approach, and deals with the relationship between irreversible processes and entropy.The relationship between irreversible processes and entropy is introduced early on, enabling the reader to benefit from seeing the relationship in such processes as heat conduction and chemical reactions. This text presents thermodynamics in a contemporary and exciting manner, with a wide range of applications, and many exercises and examples. Students are also encouraged to use computers through the provision of Mathematica code and Internet / {WWW} addresses where real data and additional information can be found.  {FEATURES}  · A truly modern approach to thermodynamics, presenting it as a science of irreversible processes whilst avoiding dividing the subject into equilibrium and non-equilibrium thermodynamics.  · An extensive range of applications drawn from science and engineering, along with many real world examples, and exercises.  · Written by two well-known authors, of whom Professor llya Prigogine was awarded the Nobel Prize for his  research into thermodynamics.  {CONTENTS}: Part I: Historical Roots: From Heat Engines to Cosmology: Basic Concepts; First Law of Thermodynamics; Second Law of Thermodynamics and the Arrow of Time; Entropy in the Realm of Chemical Reactions; Part ll: Equilibrium Thermodynamics: Extremum Principles and General Thermodynamic Relations; Basic Thermodynamics of Gases, Liquids and Solids; Thermodynamics of Phase Change; Thermodynamics of Solutions; Thermodynamics of Chemical Transformations; Fields and Internal Degrees of Freedom; Thermodynamics of Radiation; Part {III}: Fluctuations and Stability: The Gibbs' Theory of Stability; Critical Phenomena and Configurational Heat Capacity; Theory of Stability and Fluctuations Based on Entropy Production; Part {IV}: Linear Nonequilibrium Thermodynamics: Nonequilibrium Thermodynamics: The Foundations; Nonequilibrium Thermodynamics: The Linear Regime; Nonequilibrium Stationary States and their Stability: Linear Regime; Part V: Order Through Fluctuations: Nonlinear Thermodynamics; Dissipative Structures; Postface: Where do we go from here?},
	pagetotal = {508},
	publisher = {John Wiley \& Sons},
	author = {Kondepudi, Dilip and Prigogine, Ilya},
	date = {2015},
	file = {Kondepudi and Prigogine - 2015 - Modern Thermodynamics From Heat Engines to Dissip.pdf:/Users/bert/Zotero/storage/PJCZ2NZN/Kondepudi and Prigogine - 2015 - Modern Thermodynamics From Heat Engines to Dissip.pdf:application/pdf},
}

@article{ben-naim_can_2017-1,
	title = {Can entropy be defined for and the Second Law applied to the entire universe?},
	url = {http://arxiv.org/abs/1705.01100},
	abstract = {This article provides answers to the two questions posed in the title. It is argued that, contrary to many statements made in the literature, neither entropy, nor the Second Law may be used for the entire universe. The origin of this misuse of entropy and the second law may be traced back to Clausius himself. More resent (erroneous) justification is also discussed.},
	journaltitle = {{arXiv}:1705.01100 [cond-mat, physics:physics]},
	author = {Ben-Naim, Arieh},
	urldate = {2018-12-27},
	date = {2017-05-02},
	eprinttype = {arxiv},
	eprint = {1705.01100},
	keywords = {Condensed Matter - Statistical Mechanics, Physics - Chemical Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/SYT93SP2/1705.html:text/html;Ben-Naim - 2017 - Can entropy be defined for and the Second Law appl.pdf:/Users/bert/Zotero/storage/XTZE7TFV/Ben-Naim - 2017 - Can entropy be defined for and the Second Law appl.pdf:application/pdf},
}

@patent{de_vries_bert_adaptive_1994,
	title = {Adaptive filter based on a recursive delay line},
	holder = {University of Florida},
	type = {patent},
	number = {5301135},
	author = {De Vries, Bert and Principe, Jose},
	date = {1994-04-05},
	file = {De Vries, Bert and Principe - Adaptive filter based on a recursive delay line.pdf:/Users/bert/Zotero/storage/7HHVZV8W/De Vries, Bert and Principe - Adaptive filter based on a recursive delay line.pdf:application/pdf},
}

@patent{vries_sound_2018,
	title = {Sound signal modelling based on recorded object sound},
	url = {https://patents.google.com/patent/EP3343951A1/en?q=de+Vries&q=Sound&q=signal&q=modelling&q=based&q=recorded&q=object&q=sound&oq=de+Vries+Sound+signal+modelling+based+on+recorded+object+sound},
	holder = {{GN} Hearing {AS}},
	type = {patenteu},
	number = {3343951A1},
	author = {Vries, Bert De and Berg, Almer {VAN} {DEN}},
	urldate = {2018-12-25},
	date = {2018-07-04},
	langid = {english},
	keywords = {hearing device, model, object, signal, sound},
	file = {Vries and BERG - 2018 - Sound signal modelling based on recorded object so.pdf:/Users/bert/Zotero/storage/Q4NKBRJT/Vries and BERG - 2018 - Sound signal modelling based on recorded object so.pdf:application/pdf},
}

@patent{vries_multi-band_2015,
	title = {Multi-band signal processor for digital audio signals},
	url = {https://patents.google.com/patent/US20150317995A1/en?q=de+Vries&q=A&q=Multi-band&q=Signal+Processor&q=Digital+Audio+Signals&oq=de+Vries+A+Multi-band+Signal+Processor+for+Digital+Audio+Signals},
	holder = {{GN} Hearing {AS}},
	type = {patentus},
	number = {20150317995A1},
	author = {Vries, Aalbert de and Werf, Erik Cornelis Diederik {VAN} {DER}},
	urldate = {2018-12-25},
	date = {2015-11-05},
	langid = {english},
	keywords = {signal, band, frequency, frequency bands, frequency domain},
	file = {Vries and WERF - 2015 - Multi-band signal processor for digital audio sign.pdf:/Users/bert/Zotero/storage/DRE3MEBG/Vries and WERF - 2015 - Multi-band signal processor for digital audio sign.pdf:application/pdf},
}

@patent{gran_performance_2017,
	title = {Performance based in situ optimization of hearing aids},
	url = {https://patents.google.com/patent/US9838805B2/en?q=Performance-based&q=In&q=Situ&q=Optimization&q=Hearing+Aids&oq=Performance-based+In+Situ+Optimization+of+Hearing+Aids},
	holder = {{GN} Hearing {AS}},
	type = {patentus},
	number = {9838805B2},
	author = {Gran, Karl-Fredrik Johan and Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {2017-12-05},
	langid = {english},
	keywords = {signal processing, configured, hearing, hearing aid, user},
	file = {GRAN and Vries - 2017 - Performance based in situ optimization of hearing .pdf:/Users/bert/Zotero/storage/B6LU7CWR/GRAN and Vries - 2017 - Performance based in situ optimization of hearing .pdf:application/pdf},
}

@patent{dittberner_learning_2017,
	title = {Learning hearing aid},
	url = {https://patents.google.com/patent/US9648430/en?oq=Aalbert+de+Vries},
	holder = {{GN} Hearing {AS}},
	type = {patentus},
	number = {9648430B2},
	author = {Dittberner, Andrew Burke and Vries, Aalbert de and {JAKOBSEN}, Gert Hoey},
	urldate = {2018-12-25},
	date = {2017-05-09},
	langid = {english},
	keywords = {hearing aid, user, aid system, detector, sound environment},
	file = {Dittberner et al. - 2017 - Learning hearing aid.pdf:/Users/bert/Zotero/storage/C28E7X7C/Dittberner et al. - 2017 - Learning hearing aid.pdf:application/pdf},
}

@article{ypma_online_2008,
	title = {Online Personalization of Hearing Instruments},
	volume = {2008},
	doi = {10.1155/2008/183456},
	abstract = {Online personalization of hearing instruments refers to learning preferred tuning parameter values from user feedback through a control wheel (or remote control), during normal operation of the hearing aid. We perform hearing aid parameter steering by applying a linear map from acoustic features to tuning parameters. We formulate personalization of the steering parameters as the maximization of an expected utility function. A sparse Bayesian approach is then investigated for its suitability to find efficient feature representations. The feasibility of our approach is demonstrated in an application to online personalization of a noise reduction algorithm. A patient trial indicates that the acoustic features chosen for learning noise control are meaningful, that environmental steering of noise reduction makes sense, and that our personalization algorithm learns proper values for tuning parameters.},
	journaltitle = {{EURASIP} Journal on Audio, Speech, and Music Processing},
	shortjournal = {{EURASIP} Journal on Audio, Speech, and Music Processing},
	author = {Ypma, Alexander and Geurts, Job and Özer, Serkan and van der Werf, Erik and de Vries, Bert},
	date = {2008-09-01},
	file = {Ypma et al. - 2008 - Online Personalization of Hearing Instruments.pdf:/Users/bert/Zotero/storage/N3A66EZN/Ypma et al. - 2008 - Online Personalization of Hearing Instruments.pdf:application/pdf},
}

@patent{vries_automated_2018,
	title = {Automated scanning for hearing aid parameters},
	url = {https://patents.google.com/patent/EP3267695B1/en?q=Automated&q=Scanning&q=Hearing+Aid&q=Parameters&oq=Automated+Scanning+for+Hearing+Aid+Parameters},
	holder = {{GN} Hearing {AS}},
	type = {patenteu},
	number = {3267695B1},
	author = {Vries, Aalbert De and Kraak, Joris},
	urldate = {2018-12-25},
	date = {2018-10-31},
	langid = {english},
	keywords = {signal processing, signal, hearing aid, user, θ},
	file = {Vries and Kraak - 2018 - Automated scanning for hearing aid parameters.pdf:/Users/bert/Zotero/storage/EFMWHNN8/Vries and Kraak - 2018 - Automated scanning for hearing aid parameters.pdf:application/pdf},
}

@patent{dittberner_location_2015,
	title = {A location learning hearing aid},
	url = {https://patents.google.com/patent/EP2884766A1/en?q=A&q=Location&q=Learning&q=Hearing+Aid&oq=A+Location+Learning+Hearing+Aid},
	holder = {{GN} Hearing {AS}},
	type = {patenteu},
	number = {2884766A1},
	author = {Dittberner, Andrew Burke and Vries, Aalbert De and Jakobsen, Gert Høy},
	urldate = {2018-12-25},
	date = {2015-06-17},
	langid = {english},
	keywords = {hearing aid, user, aid system, detector, sound environment},
	file = {Dittberner et al. - 2015 - A location learning hearing aid.pdf:/Users/bert/Zotero/storage/3K7B63V8/Dittberner et al. - 2015 - A location learning hearing aid.pdf:application/pdf},
}

@patent{vries_hearing_2015,
	title = {A hearing aid with probabilistic hearing loss compensation},
	url = {https://patents.google.com/patent/EP2871858A1/en?q=A&q=Hearing+Aid&q=Probabilistic&q=Hearing+Loss&q=Compensation&oq=A+Hearing+Aid+with+Probabilistic+Hearing+Loss+Compensation},
	holder = {{GN} Hearing {AS}},
	type = {patenteu},
	number = {2871858A1},
	author = {Vries, Aalbert De and Farmani, Mojtaba},
	urldate = {2018-12-25},
	date = {2015-05-13},
	langid = {english},
	keywords = {hearing loss, model, hearing, audio signal, probabilistic},
	file = {Vries and Farmani - 2015 - A hearing aid with probabilistic hearing loss comp.pdf:/Users/bert/Zotero/storage/VHII9H32/Vries and Farmani - 2015 - A hearing aid with probabilistic hearing loss comp.pdf:application/pdf},
}

@patent{vries_efficient_2010,
	title = {Efficient Evaluation of Hearing Ability},
	url = {https://patents.google.com/patent/US20100257128A1/en?q=Efficient&q=evaluation&q=hearing&q=ability&oq=Efficient+evaluation+of+hearing+ability},
	holder = {{GN} Hearing {AS}},
	type = {patentus},
	number = {20100257128A1},
	author = {Vries, Aalbert de and Stadler, Svante Sten Johan and Leijon, Arne and Dijkstra, Tjeerd Maarten Hein and Ypma, Alexander},
	urldate = {2018-12-25},
	date = {2010-10-07},
	langid = {english},
	keywords = {model, hearing, hearing ability, person, population},
	file = {Vries et al. - 2010 - Efficient Evaluation of Hearing Ability.pdf:/Users/bert/Zotero/storage/PZ4UVPNY/Vries et al. - 2010 - Efficient Evaluation of Hearing Ability.pdf:application/pdf},
}

@patent{ypma_asymmetric_2010,
	title = {Asymmetric adjustment},
	url = {https://patents.google.com/patent/US20100111338A1/en?q=Asymmetric&q=adjustment&oq=Asymmetric+adjustment},
	holder = {{GN} Hearing {AS}},
	type = {patentus},
	number = {20100111338A1},
	author = {Ypma, Alexander and Vries, Aalbert de and Leenen, Joseph Renier Gerardus Maria and {GERURTS}, Job},
	urldate = {2018-12-25},
	date = {2010-05-06},
	langid = {english},
	keywords = {model, hearing, hearing aid, user, ear},
	file = {Ypma et al. - 2010 - Asymmetric adjustment.pdf:/Users/bert/Zotero/storage/6YIW59QX/Ypma et al. - 2010 - Asymmetric adjustment.pdf:application/pdf},
}

@patent{ypma_learning_2014,
	title = {Learning control of hearing aid parameter settings},
	url = {https://patents.google.com/patent/US20140146986/en},
	holder = {{GN} Hearing {AS}},
	abstract = {In a hearing aid with a signal processor for signal processing in accordance with selected values of a set of parameters Θ, a method of automatic adjustment of a set z of the signal processing parameters Θ, using a set of learning parameters θ of the signal processing parameters Θ is provided, wherein the method includes extracting signal features u of a signal in the hearing aid, recording a measure r of an adjustment e made by the user of the hearing aid, modifying z by the equation z=u θ+r, and absorbing the user adjustment e in θ by the equation θN=Φ(u,r)+θP, wherein θN is the new values of the learning parameter set θ, θP is the previous values of the learning parameter set θ, and Φ is a function of the signal features u and the recorded adjustment measure r.},
	type = {patentus},
	number = {20140146986A1},
	author = {Ypma, Alexander and Berg, Almer Jacob Van Den and Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {2014-05-29},
	langid = {english},
	keywords = {signal, hearing aid, user, aid according, parameter},
	file = {Ypma et al. - 2014 - Learning control of hearing aid parameter settings.pdf:/Users/bert/Zotero/storage/EGZTGQ5J/Ypma et al. - 2014 - Learning control of hearing aid parameter settings.pdf:application/pdf},
}

@patent{vries_optimization_2010,
	title = {Optimization of hearing aid parameters},
	url = {https://patents.google.com/patent/US20100008526/en},
	holder = {{GN} Hearing {AS}},
	abstract = {The present invention relates to a new method for effective estimation of signal processing parameters in a hearing aid. It is based on an interactive estimation process that incorporates—possibly inconsistent—user feedback. In particular, the present invention relates to optimization of hearing aid signal processing parameters based on Bayesian incremental preference elicitation.},
	type = {patentus},
	number = {20100008526A1},
	author = {Vries, Aalbert de and Ypma, Alexander},
	urldate = {2018-12-25},
	date = {2010-01-14},
	langid = {english},
	keywords = {hearing aid, user, θ, method according, ω},
	file = {Vries and Ypma - 2010 - Optimization of hearing aid parameters.pdf:/Users/bert/Zotero/storage/5KYV9DV6/Vries and Ypma - 2010 - Optimization of hearing aid parameters.pdf:application/pdf},
}

@patent{zhao_method_2007,
	title = {Method and apparatus for improved estimation of non-stationary noise for speech enhancement},
	url = {https://patents.google.com/patent/US20070055508/en},
	holder = {{GN} Hearing {AS}},
	abstract = {A central aspect of the invention relates to a method of enhancing speech, the method comprising the steps of, receiving noisy speech comprising a clean speech component and a non-stationary noise component, providing a speech model, providing a noise model having at least one shape and a gain, dynamically modifying the noise model based on the speech model and the received noisy speech, enhancing the noisy speech at least based on the modified noise model. Hereby is achieved a method of speech enhancement that is able to suppress highly non-stationary noise. Another aspect of the invention relates to a speech enhancement system that may be adapted to be used in a hearing system, such as a hearing aid or a headset.},
	type = {patentus},
	number = {20070055508A1},
	author = {Zhao, David and Kleijn, Willem and Ypma, Alexander and Devries, Bert},
	urldate = {2018-12-25},
	date = {2007-03-08},
	langid = {english},
	keywords = {model, method, noise, noisy, speech},
	file = {Zhao et al. - 2007 - Method and apparatus for improved estimation of no.pdf:/Users/bert/Zotero/storage/I7PHHZUV/Zhao et al. - 2007 - Method and apparatus for improved estimation of no.pdf:application/pdf},
}

@patent{vries_method_1998,
	title = {Method and system for training a neural network with adaptive weight updating and adaptive pruning in principal component space},
	url = {https://patents.google.com/patent/US5812992A/en},
	holder = {Sarnoff David Research Center Inc},
	abstract = {A signal processing system and method for accomplishing signal processing using a neural network that incorporates adaptive weight updating and adaptive pruning for tracking non-stationary signal is presented. The method updates the structural parameters of the neural network in principal component space (eigenspace) for every new available input sample. The non-stationary signal is recursively transformed into a matrix of eigenvectors with a corresponding matrix of eigenvalues. The method applies principal component pruning consisting of deleting the eigenmodes corresponding to the smallest saliencies, where a sum of the smallest saliencies is less than a predefined threshold level. Removing eigenmodes with low saliencies reduces the effective number of parameters and generally improves generalization. The output is then computed by using the remaining eigenmodes and the weights of the neural network are updated using adaptive filtering techniques.},
	type = {patentus},
	number = {5812992A},
	author = {Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {1998-09-22},
	keywords = {signal, principal components, saliencies, set, sub},
	file = {Vries - 1998 - Method and system for training a neural network wi.pdf:/Users/bert/Zotero/storage/B4E37G5A/Vries - 1998 - Method and system for training a neural network wi.pdf:application/pdf},
}

@patent{vries_fitting_2010,
	title = {Fitting methodology and hearing prosthesis based on signal-to-noise ratio loss data},
	url = {https://patents.google.com/patent/US7804973B2/en},
	holder = {{GN} Hearing {AS}},
	abstract = {An individual with a hearing loss often experiences at least two distinct problems: 1) the hearing loss itself i.e. an increase in hearing threshold level, and 2) a signal-to-noise ratio loss ({SNR} loss) i.e. a loss of ability to understand high level speech in noise as compared to normal hearing individuals. According to one aspect of the present invention, this problem is solved by selecting parameter values of a noise reduction algorithm or algorithms based on the individual user's {SNR} loss. Thereby, a degree of restoration/improvement of the {SNR} of noise-contaminated input signals of the hearing prosthesis has been made dependent on user specific loss data. According to another aspect of the present invention, a hearing prosthesis capable of controlling parameters of a noise reduction algorithms in dependence on the user's current listening environment as recognized and indicated by the environmental classifier has been provided.},
	type = {patentus},
	number = {7804973B2},
	author = {Vries, Aalbert de and Vries, Rob Anton Jurjen De},
	urldate = {2018-12-25},
	date = {2010-09-28},
	langid = {english},
	keywords = {noise reduction, signal, hearing, noise, hearing prosthesis},
	file = {Vries and Vries - 2010 - Fitting methodology and hearing prosthesis based o.pdf:/Users/bert/Zotero/storage/WFSHUX5Z/Vries and Vries - 2010 - Fitting methodology and hearing prosthesis based o.pdf:application/pdf},
}

@patent{parra_method_2004,
	title = {Method and apparatus for adaptive speech detection by applying a probabilistic description to the classification and tracking of signal components},
	url = {https://patents.google.com/patent/US6691087B2/en},
	holder = {{LG} Electronics Inc, Sarnoff Corp},
	abstract = {A signal processing system for detecting the presence of a desired signal component by applying a probabilistic description to the classification and tracking of various signal components (e.g., desired versus non-desired signal components) in an input signal is disclosed.},
	type = {patentus},
	number = {6691087B2},
	author = {Parra, Lucas and Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {2004-02-10},
	langid = {english},
	keywords = {signal, method, frames, β, μ},
	file = {Parra and Vries - 2004 - Method and apparatus for adaptive speech detection.pdf:/Users/bert/Zotero/storage/ZNNAU6ZM/Parra and Vries - 2004 - Method and apparatus for adaptive speech detection.pdf:application/pdf},
}

@patent{devries_improved_2000,
	title = {Improved noise spectrum tracking for speech enhancement},
	url = {https://patents.google.com/patent/WO2000036592A1/en?q=Noise&q=Spectrum&q=Tracking&q=Speech+Enhancement&oq=Noise+Spectrum+Tracking+for+Speech+Enhancement},
	holder = {Sarnoff Corporation},
	type = {patent},
	number = {{WO}2000036592A1},
	author = {Devries, Bert},
	urldate = {2018-12-25},
	date = {2000-06-22},
	langid = {english},
	keywords = {signal, noise, speech, components, frame},
	file = {Devries - 2000 - Improved noise spectrum tracking for speech enhanc.pdf:/Users/bert/Zotero/storage/5UU6YH3H/Devries - 2000 - Improved noise spectrum tracking for speech enhanc.pdf:application/pdf},
}

@patent{lubin_method_2000,
	title = {Method and apparatus for training a neural network to learn and use fidelity metric as a control mechanism},
	url = {https://patents.google.com/patent/US6075884A/en?q=Method&q=apparatus&q=training&q=neural+network&q=learn&q=use&q=fidelity&q=metric&q=control&q=mechanism&oq=Method+and+apparatus+for+training+a+neural+network+to+learn+and+use+fidelity+metric+as+a+control+mechanism},
	holder = {Sarnoff Corp},
	type = {patentus},
	number = {6075884A},
	author = {Lubin, Jeffrey and Peterson, Heidi A. and Spence, Clay D. and Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {2000-06-13},
	langid = {english},
	keywords = {neural network, apparatus, encoder, fidelity, plurality},
	file = {Lubin et al. - 2000 - Method and apparatus for training a neural network.pdf:/Users/bert/Zotero/storage/RLUF9H4W/Lubin et al. - 2000 - Method and apparatus for training a neural network.pdf:application/pdf},
}

@patent{vries_method_2000,
	title = {Method and apparatus for filtering signals using a gamma delay line based estimation of power spectrum},
	url = {https://patents.google.com/patent/US6073152A/en},
	holder = {Sarnoff Corp},
	abstract = {A signal processing system for generating a signal component reduced output signal by estimating the power spectrum of a non-stationary signal component within a non-stationary input signal is disclosed. The estimated power spectrum of the signal component is determined by using a gamma delay line ({GDL}) filter. The weights of the {GDL} filter are adaptively updated in accordance with a detection method that is designed to detect the presence of the non-stationary signal component. By subtracting the estimated power spectrum of the signal component from the power spectrum of the entire input signal, a signal component reduced output signal can be generated.},
	type = {patentus},
	number = {6073152A},
	author = {Vries, Aalbert de},
	urldate = {2018-12-25},
	date = {2000-06-06},
	keywords = {signal, filter, gdl, non, power spectrum},
	file = {Vries - 2000 - Method and apparatus for filtering signals using a.pdf:/Users/bert/Zotero/storage/F4KCLEZF/Vries - 2000 - Method and apparatus for filtering signals using a.pdf:application/pdf},
}

@patent{brill_method_1999,
	title = {Method and apparatus for assessing the visibility of differences between two image sequences},
	url = {https://patents.google.com/patent/EP0898764A1/en?q=de+Vries&inventor=M.+Brill&oq=M.+Brill+de+Vries},
	holder = {Sarnoff Corp},
	type = {patenteu},
	number = {0898764A1},
	author = {Brill, Michael Henry and Vries, Aalbert De and Finard, Olga and Lubin, Jeffrey},
	urldate = {2018-12-25},
	date = {1999-03-03},
	langid = {english},
	keywords = {contrast, image, input, input image, temporal},
}

@article{gravenstein_sampling_1989,
	title = {Sampling intervals for clinical monitoring of variables during anesthesia},
	volume = {5},
	issn = {0748-1977},
	url = {http://link.springer.com/10.1007/BF01618365},
	doi = {10.1007/BF01618365},
	abstract = {Although five minutes is the sampling interval mentioned by the American Society of Anesthesiologists for monitoring blood pr\&sure and heart rate during anesthesia, most patients are monitored more closely by continuous auscultation and with the help of automated instruments. Yet this difference between the interval recommended and that actually used indicates that sampling intervals are not defined clearly enough. Therefore, we present three methods with which to determine sampling intervals during monitoring. To explore the feasibility of these methods we examined data gathered every 7.5 seconds during three typical, noncatastrophic physiologic perturbations induced in an anesthetized dog. We chose hypercapnia secondary to rebreathing, hypotension secondary to deep anesthesia, and hypoxemia secondary to a low concentration of inspired oxygen as realistic examples of what can occur during operation and anesthesia. We studied three variables: respired carbon dioxide, femoral arterial blood pressure, and oxygen saturation of hemoglobin (pulse oximeter). The data obtained during monitoring were subjected to three methods of analysis: (1) recording of sets of data, with various starting times, "at five-minute intervals only (moving grid); (2) Fourier analysis; and (3) analysis of slopes. For the data of the experiment, the Fourier analysis yielded, on average, longer sampling intervals than did the analysis of slopes.},
	pages = {17--21},
	number = {1},
	journaltitle = {Journal of Clinical Monitoring},
	author = {Gravenstein, Joachim S. and Vries, Aalbert and Beneken, Jan E. W.},
	urldate = {2018-12-25},
	date = {1989-01},
	langid = {english},
	file = {Gravenstein et al. - 1989 - Sampling intervals for clinical monitoring of vari.pdf:/Users/bert/Zotero/storage/DREXJY6K/Gravenstein et al. - 1989 - Sampling intervals for clinical monitoring of vari.pdf:application/pdf},
}

@thesis{de_vries_temporal_1991,
	location = {Gainesville, {FL}},
	title = {Temporal processing with neural networks},
	url = {http://ufdc.ufl.edu/UF00082173/00001},
	institution = {University of Florida},
	type = {phdthesis},
	author = {De Vries, Bert},
	urldate = {2018-12-25},
	date = {1991-12},
	langid = {english},
	file = {De Vries - 1991 - Temporal processing with neural networks.pdf:/Users/bert/Zotero/storage/D8XFJY5T/De Vries - 1991 - Temporal processing with neural networks.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YMEY2VFT/00001.html:text/html},
}

@article{principe_gamma-filter-new_1993,
	title = {The gamma-filter-a new class of adaptive {IIR} filters with restricted feedback},
	volume = {41},
	issn = {1053-587X},
	doi = {10.1109/78.193206},
	abstract = {Generalized feedforward filters, a class of adaptive filters that combines attractive properties of finite impulse response ({FIR}) filters with some of the power of infinite impulse response ({IIR}) filters, are described. A particular case, the gamma filter, generalizes Widrow's adaptive transversal filter (adaline) to an infinite impulse response filter. Yet, the stability condition for the gamma filter is trivial, and {LMS} adaptation is of the same computational complexity as the conventional transversal filter structure. Preliminary results indicate that the gamma filter is more efficient than the adaptive transversal filter. The authors extend the Wiener-Kopf equation to the gamma filter and develop some analysis tools.{\textless}{\textless}{ETX}{\textgreater}{\textgreater}},
	pages = {649--656},
	number = {2},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	author = {Principe, J. C. and Vries, B. de and Oliveira, P. G. de},
	date = {1993-02},
	keywords = {computational complexity, Equations, adaptive filters, Adaptive filters, adaptive {IIR} filters, Computational complexity, digital filters, feedforward filters, Finite impulse response filter, frequency stability, gamma-filter, {IIR} filters, infinite impulse response, Least squares approximation, least squares approximations, {LMS} adaptation, restricted feedback, Stability, stability condition, Transversal filters, Widrow's adaptive transversal filter, Wiener-Kopf equation},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/6BCZDRPN/193206.html:text/html;Principe et al. - 1993 - The gamma-filter-a new class of adaptive IIR filte.pdf:/Users/bert/Zotero/storage/N9I5C3ID/Principe et al. - 1993 - The gamma-filter-a new class of adaptive IIR filte.pdf:application/pdf},
}

@article{vries_gamma_1992,
	title = {The gamma model—A new neural model for temporal processing},
	volume = {5},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005800358},
	doi = {10.1016/S0893-6080(05)80035-8},
	abstract = {In this paper we develop the gamma neural model, a new neural net architecture for processing of temporal patterns. Time varying patterns are normally segmented into a sequence of static patterns that are successively presented to a neural net. In the approach presented here segmentation is avoided. Only current signal values are presented to the neural net, which adapts its own internal memory to store the past. Thus, in the gamma neural net, an adaptive short term mechanism obviates a priori signal segmentation. We evaluate the relation between the gamma net and competing dynamic neural models. Interestingly, the gamma model brings many popular dynamic net architectures, such as the time-delay-neural-net and the concentration-in-time-neural-net, into a unifying framework. In fact, the gamma memory structure appears as general as a temporal convolution memory structure with arbitrary time varying weight kernel w(t). Yet, the gamma model remains mathematically equivalent to the additive (Grossberg) model with constant weights. We present a back propagation procedure to adapt the weights in a particular feedforward structure, the focused gamma net.},
	pages = {565--576},
	number = {4},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Vries, Bert de and Principe, Jose C.},
	urldate = {2018-12-25},
	date = {1992-07-01},
	keywords = {{ARMA} model, Dynamic neural nets, Focused back propagation, Gamma model, Short term memory, Temporal processing},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/RITB5HEM/S0893608005800358.html:text/html;Vries and Principe - 1992 - The gamma model—A new neural model for temporal pr.pdf:/Users/bert/Zotero/storage/DNZK86NI/Vries and Principe - 1992 - The gamma model—A new neural model for temporal pr.pdf:application/pdf},
}

@incollection{de_vries_short_1994,
	title = {Short term memory structures for dynamic neural   networks},
	booktitle = {ch.5 in Artificial Neural Networks for Speech and Vision, Richard Mammone (ed.)},
	publisher = {Chapman \& Hall},
	author = {De Vries, Bert},
	date = {1994},
}

@incollection{principe_locally_1998,
	title = {Locally Recurrent Networks: The Gamma Operator, Properties, and Extensions},
	shorttitle = {Chapter 10. Locally Recurrent Networks},
	abstract = {Locally recurrent networks have shown great potential for processing time- varying signals. This paper reviews various memory structures for time- varying signal processing with neural networks. In particular, we focus on the gamma structure and variations such as the Laguerre and gamma {II} memory networks. The paper presents the basic theory of memory structures and several interpretations of their function.},
	booktitle = {Ch.10 in Neural   Networks and Pattern Recognition, Omidvar and Dayhoff (eds.)},
	publisher = {Academic Press},
	author = {Principe, Jose and Celebi, Samel and de Vries, Bert and Harris, John},
	date = {1998-12-31},
	doi = {10.1016/B978-012526420-4/50011-2},
}

@article{zhao_online_2008,
	title = {Online Noise Estimation Using Stochastic-Gain {HMM} for Speech Enhancement},
	volume = {16},
	issn = {1558-7916},
	url = {https://doi.org/10.1109/TASL.2008.916055},
	doi = {10.1109/TASL.2008.916055},
	abstract = {We propose a noise estimation algorithm for single-channel noise suppression in dynamic noisy environments. A stochastic-gain hidden Markov model ({SG}-{HMM}) is used to model the statistics of nonstationary noise with time-varying energy. The noise model is adaptive and the model parameters are estimated online from noisy observations using a recursive estimation algorithm. The parameter estimation is derived for the maximum-likelihood criterion and the algorithm is based on the recursive expectation maximization ({EM}) framework. The proposed method facilitates continuous adaptation to changes of both noise spectral shapes and noise energy levels, e.g., due to movement of the noise source. Using the estimated noise model, we also develop an estimator of the noise power spectral density ({PSD}) based on recursive averaging of estimated noise sample spectra. We demonstrate that the proposed scheme achieves more accurate estimates of the noise model and noise {PSD}, and as part of a speech enhancement system facilitates a lower level of residual noise.},
	pages = {835--846},
	number = {4},
	journaltitle = {Trans. Audio, Speech and Lang. Proc.},
	author = {Zhao, D. Y. and Kleijn, W. B. and Ypma, A. and de Vries, B.},
	urldate = {2018-12-25},
	date = {2008-05},
	keywords = {Gain modeling, noise estimation, noise model adaptation, noise suppression, stochastic-gain hidden Markov model ({SG}-{HMM})},
}

@online{dijkstra_learning_2007,
	title = {The Learning Hearing Aid: Common-Sense Reasoning in Hearing Aid Circuits},
	url = {http://www.hearingreview.com/2007/10/the-learning-hearing-aid-common-sense-reasoning-in-hearing-aid-circuits/},
	shorttitle = {The Learning Hearing Aid},
	abstract = {Why probability theory will play a large role in the future of hearing health care.},
	titleaddon = {Hearing Review},
	author = {Dijkstra, Tjeerd and Ypma, Alexander and De Vries, Bert and Leenen, Jos},
	urldate = {2018-12-25},
	date = {2007-10},
	langid = {american},
	file = {Snapshot:/Users/bert/Zotero/storage/2DMHCK7Y/the-learning-hearing-aid-common-sense-reasoning-in-hearing-aid-circuits.html:text/html},
}

@article{vullings_adaptive_2011,
	title = {An adaptive Kalman filter for {ECG} signal enhancement},
	volume = {58},
	issn = {1558-2531},
	doi = {10.1109/TBME.2010.2099229},
	abstract = {The ongoing trend of {ECG} monitoring techniques to become more ambulatory and less obtrusive generally comes at the expense of decreased signal quality. To enhance this quality, consecutive {ECG} complexes can be averaged triggered on the heartbeat, exploiting the quasi-periodicity of the {ECG}. However, this averaging constitutes a tradeoff between improvement of the {SNR} and loss of clinically relevant physiological signal dynamics. Using a bayesian framework, in this paper, a sequential averaging filter is developed that, in essence, adaptively varies the number of complexes included in the averaging based on the characteristics of the {ECG} signal. The filter has the form of an adaptive Kalman filter. The adaptive estimation of the process and measurement noise covariances is performed by maximizing the bayesian evidence function of the sequential {ECG} estimation and by exploiting the spatial correlation between several simultaneously recorded {ECG} signals, respectively. The noise covariance estimates thus obtained render the filter capable of ascribing more weight to newly arriving data when these data contain morphological variability, and of reducing this weight in cases of no morphological variability. The filter is evaluated by applying it to a variety of {ECG} signals. To gauge the relevance of the adaptive noise-covariance estimation, the performance of the filter is compared to that of a Kalman filter with fixed, (a posteriori) optimized noise covariance. This comparison demonstrates that, without using a priori knowledge on signal characteristics, the filter with adaptive noise estimation performs similar to the filter with optimized fixed noise covariance, favoring the adaptive filter in cases where no a priori information is available or where signal characteristics are expected to fluctuate.},
	pages = {1094--1103},
	number = {4},
	journaltitle = {{IEEE} transactions on bio-medical engineering},
	shortjournal = {{IEEE} Trans Biomed Eng},
	author = {Vullings, Rik and de Vries, Bert and Bergmans, Jan W. M.},
	date = {2011-04},
	pmid = {21156383},
	keywords = {Humans, Electrocardiography, Computer Simulation, Algorithms, Arrhythmias, Cardiac, Diagnosis, Computer-Assisted, Models, Cardiovascular, Models, Statistical, Reproducibility of Results, Sensitivity and Specificity, Signal Processing, Computer-Assisted},
	file = {Vullings et al. - 2011 - An adaptive Kalman filter for ECG signal enhanceme.pdf:/Users/bert/Zotero/storage/UCHKZZ6G/Vullings et al. - 2011 - An adaptive Kalman filter for ECG signal enhanceme.pdf:application/pdf},
}

@article{papo_brain_2013,
	title = {Brain temperature: what it means and what it can do for (cognitive) neuroscientists},
	url = {http://arxiv.org/abs/1310.2906},
	shorttitle = {Brain temperature},
	abstract = {The effects of temperature on various aspects of neural activity from single cell to neural circuit level have long been known. However, how temperature affects the system-level of activity typical of experiments using non-invasive imaging techniques, such as magnetic brain imaging of electroencephalography, where neither its direct measurement nor its manipulation are possible, is essentially unknown. Starting from its basic physical definition, we discuss possible ways in which temperature may be used both as a parameter controlling the evolution of other variables through which brain activity is observed, and as a collective variable describing brain activity. These two aspects are further illustrated in the case of learning-related brain activity. Finally, methods to extract a temperature from experimental data are reviewed.},
	journaltitle = {{arXiv}:1310.2906 [q-bio]},
	author = {Papo, David},
	urldate = {2018-12-24},
	date = {2013-10-10},
	eprinttype = {arxiv},
	eprint = {1310.2906},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CEVC36BS/1310.html:text/html;Papo - 2013 - Brain temperature what it means and what it can d.pdf:/Users/bert/Zotero/storage/GQ72KK48/Papo - 2013 - Brain temperature what it means and what it can d.pdf:application/pdf},
}

@article{cullen_active_2018,
	title = {Active Inference in {OpenAI} Gym: A Paradigm for Computational Investigations Into Psychiatric Illness},
	volume = {3},
	issn = {2451-9022},
	url = {http://www.sciencedirect.com/science/article/pii/S2451902218301617},
	doi = {10.1016/j.bpsc.2018.06.010},
	series = {Computational Methods and Modeling in Psychiatry},
	shorttitle = {Active Inference in {OpenAI} Gym},
	abstract = {Background
Artificial intelligence has recently attained humanlike performance in a number of gamelike domains. These advances have been spurred by brain-inspired architectures and algorithms such as hierarchical filtering and reinforcement learning. {OpenAI} Gym is an open-source platform in which to train, test, and benchmark algorithms—it provides a range of tasks, including those of classic arcade games such as Doom. Here we describe how the platform might be used as a simulation, test, and diagnostic paradigm for psychiatric conditions.
Methods
To illustrate how active inference models of game play could be used to test mechanistic and algorithmic properties of psychiatric disorders, we provide two exemplar analyses. The first speaks to the impact of aging on cognition, examining game-play behaviors in a model of aging in which we compared age-dependent changes of younger (n = 9, 22 ± 1 years of age) and older (n = 7, 56 ± 5 years of age) adult players. The second is an illustration of a putative feature of anhedonia in which we simulated diminished sensitivity to reward.
Results
These simulations demonstrate how active inference can be used to test predicted changes in both neurobiology and beliefs in psychiatric cohorts. We show that, as well as behavioral measures, putative neural correlates of active inference can be simulated, and hypothesized (model-based) differences in local field potentials and blood oxygen level–dependent responses can be produced.
Conclusions
We show that active inference, through epistemic and value-based goals, enables simulated subjects to actively develop detailed representations of gaming environments, and we demonstrate the use of a principled algorithmic and neurobiological framework for testing hypotheses in psychiatric illness.},
	pages = {809--818},
	number = {9},
	journaltitle = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	shortjournal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	author = {Cullen, Maell and Davey, Ben and Friston, Karl J. and Moran, Rosalyn J.},
	urldate = {2018-12-14},
	date = {2018-09-01},
	keywords = {Free energy principle, Active inference, Computational phenotyping, Computational psychiatry, Game-based imaging biomarkers, Markov decision process},
	file = {1-s2.0-S2451902218301617-mmc1.pdf:/Users/bert/Zotero/storage/XE4ZBIYX/1-s2.0-S2451902218301617-mmc1.pdf:application/pdf;Cullen et al. - 2018 - Active Inference in OpenAI Gym A Paradigm for Com.pdf:/Users/bert/Zotero/storage/S77WPN9G/Cullen et al. - 2018 - Active Inference in OpenAI Gym A Paradigm for Com.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/L2W2A2P9/S2451902218301617.html:text/html},
}

@article{innes_fashionable_2018,
	title = {Fashionable Modelling with Flux},
	url = {http://arxiv.org/abs/1811.01457},
	abstract = {Machine learning as a discipline has seen an incredible surge of interest in recent years due in large part to a perfect storm of new theory, superior tooling, renewed interest in its capabilities. We present in this paper a framework named Flux that shows how further refinement of the core ideas of machine learning, built upon the foundation of the Julia programming language, can yield an environment that is simple, easily modifiable, and performant. We detail the fundamental principles of Flux as a framework for differentiable programming, give examples of models that are implemented within Flux to display many of the language and framework-level features that contribute to its ease of use and high productivity, display internal compiler techniques used to enable the acceleration and performance that lies at the heart of Flux, and finally give an overview of the larger ecosystem that Flux fits inside of.},
	journaltitle = {{arXiv}:1811.01457 [cs]},
	author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
	urldate = {2018-12-09},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1811.01457},
	keywords = {Computer Science - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/V26KC2K5/1811.html:text/html;Innes et al. - 2018 - Fashionable Modelling with Flux.pdf:/Users/bert/Zotero/storage/YBU7WR3P/Innes et al. - 2018 - Fashionable Modelling with Flux.pdf:application/pdf},
}

@article{sokolov_structural_2018,
	title = {Structural and effective brain connectivity underlying biological motion detection},
	rights = {© 2018 . Published under the {PNAS} license.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/early/2018/12/03/1812859115},
	doi = {10.1073/pnas.1812859115},
	abstract = {The perception of actions underwrites a wide range of socio-cognitive functions. Previous neuroimaging and lesion studies identified several components of the brain network for visual biological motion ({BM}) processing, but interactions among these components and their relationship to behavior remain little understood. Here, using a recently developed integrative analysis of structural and effective connectivity derived from high angular resolution diffusion imaging ({HARDI}) and functional magnetic resonance imaging ({fMRI}), we assess the cerebro-cerebellar network for processing of camouflaged point-light {BM}. Dynamic causal modeling ({DCM}) informed by probabilistic tractography indicates that the right superior temporal sulcus ({STS}) serves as an integrator within the temporal module. However, the {STS} does not appear to be a “gatekeeper” in the functional integration of the occipito-temporal and frontal regions: The fusiform gyrus ({FFG}) and middle temporal cortex ({MTC}) are also connected to the right inferior frontal gyrus ({IFG}) and insula, indicating multiple parallel pathways. {BM}-specific loops of effective connectivity are seen between the left lateral cerebellar lobule Crus I and right {STS}, as well as between the left Crus I and right insula. The prevalence of a structural pathway between the {FFG} and {STS} is associated with better {BM} detection. Moreover, a canonical variate analysis shows that the visual sensitivity to {BM} is best predicted by {BM}-specific effective connectivity from the {FFG} to {STS} and from the {IFG}, insula, and {STS} to the early visual cortex. Overall, the study characterizes the architecture of the cerebro-cerebellar network for {BM} processing and offers prospects for assessing the social brain.},
	pages = {201812859},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Sokolov, Arseny A. and Zeidman, Peter and Erb, Michael and Ryvlin, Philippe and Friston, Karl J. and Pavlova, Marina A.},
	urldate = {2018-12-08},
	date = {2018-12-04},
	langid = {english},
	pmid = {30514816},
	keywords = {biological motion, diffusion tensor imaging, dynamic causal modelling, functional {MRI}, network analysis},
	file = {Snapshot:/Users/bert/Zotero/storage/YEFX58DU/1812859115.html:text/html;Sokolov et al. - 2018 - Structural and effective brain connectivity underl.pdf:/Users/bert/Zotero/storage/VSS6NVNA/Sokolov et al. - 2018 - Structural and effective brain connectivity underl.pdf:application/pdf},
}

@article{baez_relative_2016,
	title = {Relative Entropy in Biological Systems},
	volume = {18},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1512.02742},
	doi = {10.3390/e18020046},
	abstract = {In this paper we review various information-theoretic characterizations of the approach to equilibrium in biological systems. The replicator equation, evolutionary game theory, Markov processes and chemical reaction networks all describe the dynamics of a population or probability distribution. Under suitable assumptions, the distribution will approach an equilibrium with the passage of time. Relative entropy - that is, the Kullback--Leibler divergence, or various generalizations of this - provides a quantitative measure of how far from equilibrium the system is. We explain various theorems that give conditions under which relative entropy is nonincreasing. In biochemical applications these results can be seen as versions of the Second Law of Thermodynamics, stating that free energy can never increase with the passage of time. In ecological applications, they make precise the notion that a population gains information from its environment as it approaches equilibrium.},
	pages = {46},
	number = {2},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Baez, John C. and Pollard, Blake S.},
	urldate = {2020-03-14},
	date = {2016-02-02},
	eprinttype = {arxiv},
	eprint = {1512.02742},
	keywords = {Quantitative Biology - Quantitative Methods, Mathematics - Probability, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/V56ER3V4/1512.html:text/html;Baez and Pollard - 2016 - Relative Entropy in Biological Systems.pdf:/Users/bert/Zotero/storage/PQCMW2GP/Baez and Pollard - 2016 - Relative Entropy in Biological Systems.pdf:application/pdf},
}

@inproceedings{jaynes_straight_1999,
	title = {Straight line fitting - A Bayesian solution},
	abstract = {Fitting the {\textbackslash}best" straight line to a scatter plot of data D f(x1; y1) : : :(xn; yn)g in which both variables xi; yi are subject to unknown error is undoubtedly the most common problem of inference faced by scientists, engineers, medical researchers, and economists. The problem is to estimate the parameters ; in the straight line equation y = + x , and assess the accuracy of the estimates. Whenever we try to discover or estimate a relationship between two factors we are almost sure to be in this situation. But from the viewpoint of orthodox statistics the problem turned out to be a horrendous can of worms; generations of e orts led only to a long line of false starts, and no satisfactory solution. We give the Bayesian solution to the problem, which turns out to be eminently satisfactory and straightforward, although a little tricky in the derivation. However, not much of the nal result is really new. Arnold Zellner (1971) gave a very similar solution long ago, but it went unnoticed by those who had the most need to know about it. We give a pedagogical introduction to the problem and add a few nal touches, dealing with choice of priors and parameterizations. In any event, whether or not the following solution has anything new in it, the currently great and universal importance of the problem would warrant bringing the result to the attention of the scienti c community. Many workers, from astronomers to biologists, are still struggling with the problem, unaware that the solution is known.},
	author = {Jaynes, Edwin T.},
	date = {1999},
	file = {Jaynes - 1999 - STRAIGHT LINE FITTING A BAYESIAN SOLUTION.pdf:/Users/bert/Zotero/storage/Q3WDEVMD/Jaynes - 1999 - STRAIGHT LINE FITTING A BAYESIAN SOLUTION.pdf:application/pdf},
}

@article{so_modulation-domain_2011,
	title = {Modulation-domain Kalman filtering for single-channel speech enhancement},
	volume = {53},
	issn = {0167-6393},
	url = {http://www.sciencedirect.com/science/article/pii/S0167639311000197},
	doi = {10.1016/j.specom.2011.02.001},
	abstract = {In this paper, we investigate the modulation-domain Kalman filter ({MDKF}) and compare its performance with other time-domain and acoustic-domain speech enhancement methods. In contrast to previously reported modulation domain-enhancement methods based on fixed bandpass filtering, the {MDKF} is an adaptive and linear {MMSE} estimator that uses models of the temporal changes of the magnitude spectrum for both speech and noise. Also, because the Kalman filter is a joint magnitude and phase spectrum estimator, under non-stationarity assumptions, it is highly suited for modulation-domain processing, as phase information has been shown to play an important role in the modulation domain. We have found that the Kalman filter is better suited for processing in the modulation-domain, rather than in the time-domain, since the low order linear predictor is sufficient at modelling the dynamics of slow changes in the modulation domain, while being insufficient at modelling the long-term correlation speech information in the time domain. As a result, the {MDKF} method produces enhanced speech that has very minimal distortion and residual noise, in the ideal case. The results from objective experiments and blind subjective listening tests using the {NOIZEUS} corpus show that the {MDKF} (with clean speech parameters) outperforms all the acoustic and time-domain enhancement methods that were evaluated, including the time-domain Kalman filter with clean speech parameters. A practical {MDKF} that uses the {MMSE}-{STSA} method to enhance noisy speech in the acoustic domain prior to {LPC} analysis was also evaluated and showed promising results.},
	pages = {818--829},
	number = {6},
	journaltitle = {Speech Communication},
	shortjournal = {Speech Communication},
	author = {So, Stephen and Paliwal, Kuldip K.},
	urldate = {2020-02-28},
	date = {2011-07-01},
	langid = {english},
	keywords = {Kalman filtering, Speech enhancement, Modulation domain},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/ESEAQPDC/S0167639311000197.html:text/html;So and Paliwal - 2011 - Modulation-domain Kalman filtering for single-chan.pdf:/Users/bert/Zotero/storage/KVCNXDG3/So and Paliwal - 2011 - Modulation-domain Kalman filtering for single-chan.pdf:application/pdf},
}

@article{tschantz_reinforcement_2020,
	title = {Reinforcement Learning through Active Inference},
	url = {http://arxiv.org/abs/2002.12636},
	abstract = {The central tenet of reinforcement learning ({RL}) is that agents seek to maximize the sum of cumulative rewards. In contrast, active inference, an emerging framework within cognitive and computational neuroscience, proposes that agents act to maximize the evidence for a biased generative model. Here, we illustrate how ideas from active inference can augment traditional {RL} approaches by (i) furnishing an inherent balance of exploration and exploitation, and (ii) providing a more flexible conceptualization of reward. Inspired by active inference, we develop and implement a novel objective for decision making, which we term the free energy of the expected future. We demonstrate that the resulting algorithm successfully balances exploration and exploitation, simultaneously achieving robust performance on several challenging {RL} benchmarks with sparse, well-shaped, and no rewards.},
	journaltitle = {{arXiv}:2002.12636 [cs, eess, math, stat]},
	author = {Tschantz, Alexander and Millidge, Beren and Seth, Anil K. and Buckley, Christopher L.},
	urldate = {2020-03-07},
	date = {2020-02-28},
	eprinttype = {arxiv},
	eprint = {2002.12636},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/YKZT4ND4/2002.html:text/html;Tschantz et al. - 2020 - Reinforcement Learning through Active Inference.pdf:/Users/bert/Zotero/storage/EV236I73/Tschantz et al. - 2020 - Reinforcement Learning through Active Inference.pdf:application/pdf},
}

@misc{malham_introduction_2016,
	title = {An introduction to Lagrangian and Hamiltonian mechanics},
	author = {Malham, Simon},
	date = {2016},
	file = {Malham - 2016 - Malham-2016-An-introduction-to-Lagrangian-and-Hami.pdf:/Users/bert/Zotero/storage/F5U8634T/Malham - 2016 - Malham-2016-An-introduction-to-Lagrangian-and-Hami.pdf:application/pdf},
}

@article{wilkinson_end--end_2019-1,
	title = {End-to-End Probabilistic Inference for Nonstationary Audio Analysis},
	url = {http://arxiv.org/abs/1901.11436},
	abstract = {A typical audio signal processing pipeline includes multiple disjoint analysis stages, including calculation of a time-frequency representation followed by spectrogram-based feature analysis. We show how time-frequency analysis and nonnegative matrix factorisation can be jointly formulated as a spectral mixture Gaussian process model with nonstationary priors over the amplitude variance parameters. Further, we formulate this nonlinear model's state space representation, making it amenable to infinite-horizon Gaussian process regression with approximate inference via expectation propagation, which scales linearly in the number of time steps and quadratically in the state dimensionality. By doing so, we are able to process audio signals with hundreds of thousands of data points. We demonstrate, on various tasks with empirical data, how this inference scheme outperforms more standard techniques that rely on extended Kalman filtering.},
	journaltitle = {{arXiv}:1901.11436 [cs, eess, stat]},
	author = {Wilkinson, William J. and Andersen, Michael Riis and Reiss, Joshua D. and Stowell, Dan and Solin, Arno},
	urldate = {2020-03-05},
	date = {2019-04-27},
	eprinttype = {arxiv},
	eprint = {1901.11436},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3AZQNPYL/1901.html:text/html;Wilkinson et al. - 2019 - End-to-End Probabilistic Inference for Nonstationa.pdf:/Users/bert/Zotero/storage/5KGE4YZZ/Wilkinson et al. - 2019 - End-to-End Probabilistic Inference for Nonstationa.pdf:application/pdf},
}

@article{gratton_glass_2017,
	title = {{GLASS}: A General Likelihood Approximate Solution Scheme},
	url = {http://arxiv.org/abs/1708.08479},
	shorttitle = {{GLASS}},
	abstract = {We present a technique for constructing suitable posterior probability distributions in situations for which the sampling distribution of the data is not known. This is very useful for modern scientific data analysis in the era of "big data", for which exact likelihoods are commonly either unknown, computationally prohibitively expensive or inapplicable because of systematic effects in the data. The scheme involves implicitly computing the changes in an approximate sampling distribution as model parameters are changed via explicitly-computed moments of statistics constructed from the data.},
	journaltitle = {{arXiv}:1708.08479 [astro-ph, stat]},
	author = {Gratton, Steven},
	urldate = {2020-03-02},
	date = {2017-08-28},
	eprinttype = {arxiv},
	eprint = {1708.08479},
	keywords = {Mathematics - Statistics Theory, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/Y9T48GTG/Gratton - 2017 - GLASS A General Likelihood Approximate Solution S.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/KP2PPPXE/1708.html:text/html},
}

@online{noauthor_191104018_nodate,
	title = {[1911.04018] Feedback Recurrent {AutoEncoder}},
	url = {https://arxiv.org/abs/1911.04018},
	urldate = {2020-03-02},
	file = {[1911.04018] Feedback Recurrent AutoEncoder:/Users/bert/Zotero/storage/JHT4XYU3/1911.html:text/html},
}

@article{leike_optimal_2017,
	title = {Optimal Belief Approximation},
	url = {http://arxiv.org/abs/1610.09018},
	doi = {10.3390/e19080402},
	abstract = {In Bayesian statistics probability distributions express beliefs. However, for many problems the beliefs cannot be computed analytically and approximations of beliefs are needed. We seek a loss function that quantifies how "embarrassing" it is to communicate a given approximation. We reproduce and discuss an old proof showing that there is only one ranking under the requirements that (1) the best ranked approximation is the non-approximated belief and (2) that the ranking judges approximations only by their predictions for actual outcomes. The loss function that is obtained in the derivation is equal to the Kullback-Leibler divergence when normalized. This loss function is frequently used in the literature. However, there seems to be confusion about the correct order in which its functional arguments, the approximated and non-approximated beliefs, should be used. The correct order ensures that the recipient of a communication is only deprived of the minimal amount of information. We hope that the elementary derivation settles the apparent confusion. For example when approximating beliefs with Gaussian distributions the optimal approximation is given by moment matching. This is in contrast to many suggested computational schemes.},
	journaltitle = {{arXiv}:1610.09018 [physics, stat]},
	author = {Leike, Reimar H. and Enßlin, Torsten A.},
	urldate = {2020-02-28},
	date = {2017-08-03},
	eprinttype = {arxiv},
	eprint = {1610.09018},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NQPKB632/1610.html:text/html;Leike and Enßlin - 2017 - Optimal Belief Approximation.pdf:/Users/bert/Zotero/storage/NQPDH3T9/Leike and Enßlin - 2017 - Optimal Belief Approximation.pdf:application/pdf},
}

@article{brewer_1_nodate,
	title = {1 Bayesian Inference and Computation: A Beginner’s Guide},
	pages = {10},
	journaltitle = {Cambridge University Press},
	author = {Brewer, Brendon J},
	langid = {english},
	file = {Brewer - 1 Bayesian Inference and Computation A Beginner’s.pdf:/Users/bert/Zotero/storage/G7RGFPI3/Brewer - 1 Bayesian Inference and Computation A Beginner’s.pdf:application/pdf},
}

@misc{mackay_nested_2006,
	title = {Nested Sampling},
	author = {{MacKay}, David},
	date = {2006},
	file = {MacKay - 2006 - Nested Sampling.pdf:/Users/bert/Zotero/storage/CASGQHCS/MacKay - 2006 - Nested Sampling.pdf:application/pdf},
}

@article{skilling_advice_2010,
	title = {Advice on “Computational methods for Bayesian model choice”},
	abstract = {At last year’s meeting, Robert and Wraith (2009) gave an account of algorithms in current statistical practice. After giving an overview of the principles of computational inference, this paper follows and refers to theirs, though from a more critical and principled viewpoint. Their mistaken account of nested sampling is corrected.},
	pages = {12},
	author = {Skilling, John},
	date = {2010},
	langid = {english},
	file = {Skilling - 2010 - Advice on “Computational methods for Bayesian mode.pdf:/Users/bert/Zotero/storage/NGK4QAMV/Skilling - 2010 - Advice on “Computational methods for Bayesian mode.pdf:application/pdf},
}

@article{sengupta_approximate_2016,
	title = {Approximate Bayesian inference as a gauge theory},
	volume = {14},
	issn = {1545-7885},
	url = {http://arxiv.org/abs/1705.06614},
	doi = {10.1371/journal.pbio.1002400},
	abstract = {In a published paper {\textbackslash}cite\{Sengupta2016\}, we have proposed that the brain (and other self-organized biological and artificial systems) can be characterized via the mathematical apparatus of a gauge theory. The picture that emerges from this approach suggests that any biological system (from a neuron to an organism) can be cast as resolving uncertainty about its external milieu, either by changing its internal states or its relationship to the environment. Using formal arguments, we have shown that a gauge theory for neuronal dynamics -- based on approximate Bayesian inference -- has the potential to shed new light on phenomena that have thus far eluded a formal description, such as attention and the link between action and perception. Here, we describe the technical apparatus that enables such a variational inference on manifolds.},
	pages = {e1002400},
	number = {3},
	journaltitle = {{PLOS} Biology},
	author = {Sengupta, Biswa and Friston, Karl},
	urldate = {2017-08-26},
	date = {2016-03-08},
	eprinttype = {arxiv},
	eprint = {1705.06614},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition, Free energy, Attention, Nervous system, Probability distribution, Manifolds, Differential geometry, Non-Euclidean geometry, Symmetry},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UX2WFHHK/1705.html:text/html;Sengupta and Friston - 2016 - Approximate Bayesian inference as a gauge theory.pdf:/Users/bert/Zotero/storage/P8BWA4X7/Sengupta and Friston - 2016 - Approximate Bayesian inference as a gauge theory.pdf:application/pdf;Sengupta et al. - 2016 - Towards a Neuronal Gauge Theory.pdf:/Users/bert/Zotero/storage/7IJNHPG9/Sengupta et al. - 2016 - Towards a Neuronal Gauge Theory.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/833TQ2TT/article.html:text/html},
}

@article{wagenmakers_bayesian_2010,
	title = {Bayesian hypothesis testing for psychologists: A tutorial on the Savage–Dickey method},
	volume = {60},
	issn = {0010-0285},
	url = {http://www.sciencedirect.com/science/article/pii/S0010028509000826},
	doi = {10.1016/j.cogpsych.2009.12.001},
	shorttitle = {Bayesian hypothesis testing for psychologists},
	abstract = {In the field of cognitive psychology, the p-value hypothesis test has established a stranglehold on statistical reporting. This is unfortunate, as the p-value provides at best a rough estimate of the evidence that the data provide for the presence of an experimental effect. An alternative and arguably more appropriate measure of evidence is conveyed by a Bayesian hypothesis test, which prefers the model with the highest average likelihood. One of the main problems with this Bayesian hypothesis test, however, is that it often requires relatively sophisticated numerical methods for its computation. Here we draw attention to the Savage–Dickey density ratio method, a method that can be used to compute the result of a Bayesian hypothesis test for nested models and under certain plausible restrictions on the parameter priors. Practical examples demonstrate the method’s validity, generality, and flexibility.},
	pages = {158--189},
	number = {3},
	journaltitle = {Cognitive Psychology},
	author = {Wagenmakers, Eric-Jan and Lodewyckx, Tom and Kuriyal, Himanshu and Grasman, Raoul},
	urldate = {2015-01-20},
	date = {2010-05},
	keywords = {Humans, Bayes factor, Model selection, Model Selection, hierarchical modeling, Models, Bayes Theorem, Random effects, Cognitive science, Behavioral Research, Data Interpretation, Hierarchical modeling, Likelihood Functions, Order-restrictions, Statistical, Statistical evidence},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/5QZEHUID/S0010028509000826.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/HQLIDGAZ/S0010028509000826.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/QDIK5S99/S0010028509000826.html:text/html;Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:/Users/bert/Zotero/storage/8DYAYYVK/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:application/pdf;Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:/Users/bert/Zotero/storage/MKM7I3F9/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:application/pdf;Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:/Users/bert/Zotero/storage/3SAKWTD2/Wagenmakers et al. - 2010 - Bayesian hypothesis testing for psychologists A t.pdf:application/pdf},
}

@article{sharma_natural_2007,
	title = {Natural process – Natural selection},
	volume = {127},
	issn = {0301-4622},
	url = {http://www.sciencedirect.com/science/article/pii/S0301462207000117},
	doi = {10.1016/j.bpc.2007.01.005},
	abstract = {Life is supported by a myriad of chemical reactions. To describe the overall process we have formulated entropy for an open system undergoing chemical reactions. The entropy formula allows us to recognize various ways for the system to move towards more probable states. These correspond to the basic processes of life i.e. proliferation, differentiation, expansion, energy intake, adaptation and maturation. We propose that the rate of entropy production by various mechanisms is the fitness criterion of natural selection. The quest for more probable states results in organization of matter in functional hierarchies.},
	pages = {123--128},
	number = {1},
	journaltitle = {Biophysical Chemistry},
	shortjournal = {Biophysical Chemistry},
	author = {Sharma, Vivek and Annila, Arto},
	urldate = {2019-04-28},
	date = {2007-04-01},
	keywords = {Entropy, Evolution, Catalysis},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/I6RGI275/S0301462207000117.html:text/html;Sharma and Annila - 2007 - Natural process – Natural selection.pdf:/Users/bert/Zotero/storage/K7E9ICEK/Sharma and Annila - 2007 - Natural process – Natural selection.pdf:application/pdf},
}

@article{herzog_iterative_2019,
	title = {Iterative Approximate Nonlinear Inference via Gaussian Message Passing on Factor Graphs},
	issn = {2475-1456},
	doi = {10.1109/LCSYS.2019.2919260},
	abstract = {Factor graphs are graphical models able to represent the factorization of probability density functions. By visualizing conditional independence statements, they provide an intuitive and versatile interface to sparsity exploiting message passing algorithms as a unified framework for constructing algorithms in signal processing, estimation, and control in a mix-and-match style. Especially when assuming Gaussian distributed variables, tabulated message passing rules allow for easy automated derivations of algorithms. The present paper’s contribution consists in the combination of statistical or Jacobian-based linearization approaches to handling nonlinear factors with efficient message parametrizations in a Gaussian message passing setting. Tabulated message passing rules for a multivariate nonlinear factor node are presented that implement a re-linearization about the most current belief (marginal) of each adjacent variable. When utilized in a nonlinear Kalman smoothing setting, the iterated nonlinear Modified Bryson-Frazier smoother is recovered, while retaining the flexibility of the factor graph framework. This application is illustrated by deriving an input estimation algorithm for a nonlinear system.},
	pages = {1--1},
	journaltitle = {{IEEE} Control Systems Letters},
	author = {Herzog, C. and Hoffmann, n and Petersen, E. and Rostalski, P.},
	date = {2019},
	keywords = {Estimation, Message passing, Signal processing algorithms, Kalman filters, message passing, Inference algorithms, Machine learning, Kalman filtering, Gaussian processes, Smoothing methods, graph theory, inference mechanisms, iterative methods, graphical models, Random variables, smoothing methods, signal processing, machine learning, approximation theory, conditional independence statements, efficient message parametrizations, estimation, factor graph framework, factorization, Gaussian distributed variables, Gaussian distribution, Gaussian message passing setting, input estimation algorithm, iterated nonlinear modified Bryson-Frazier smoother, iterative approximate nonlinear inference, Jacobian-based linearization, mix-and-match style, multivariate nonlinear factor node, nonlinear factors, nonlinear filters, nonlinear Kalman smoothing setting, nonlinear system, probability density functions, sparsity exploiting message passing algorithms, statistical learning, Statistical learning, statistical linearization, stochastic systems, Stochastic systems., tabulated message passing rules},
	file = {Herzog et al. - 2019 - Iterative Approximate Nonlinear Inference via Gaus.pdf:/Users/bert/Zotero/storage/M43GUZ3T/Herzog et al. - 2019 - Iterative Approximate Nonlinear Inference via Gaus.pdf:application/pdf;Hoffmann et al. - 2019 - Iterative Approximate Nonlinear Inference via Gaus.pdf:/Users/bert/Zotero/storage/L92TBMVY/Hoffmann et al. - 2019 - Iterative Approximate Nonlinear Inference via Gaus.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/EU69ETHQ/8723648.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/QQBHIDKM/8723648.html:text/html},
}

@article{hoffman_stochastic_2012,
	title = {Stochastic Variational Inference},
	url = {http://arxiv.org/abs/1206.7051},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	journaltitle = {{arXiv}:1206.7051 [cs, stat]},
	author = {Hoffman, Matt and Blei, David M. and Wang, Chong and Paisley, John},
	urldate = {2017-08-27},
	date = {2012-06-29},
	eprinttype = {arxiv},
	eprint = {1206.7051},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UBPCVFVW/1206.html:text/html;Hoffman et al. - 2012 - Stochastic Variational Inference.pdf:/Users/bert/Zotero/storage/ZCZEH6LV/Hoffman et al. - 2012 - Stochastic Variational Inference.pdf:application/pdf;Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:/Users/bert/Zotero/storage/UMI36C5R/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf:application/pdf},
}

@article{daunizeau_variational_2017,
	title = {The variational Laplace approach to approximate Bayesian inference},
	url = {http://arxiv.org/abs/1703.02089},
	abstract = {Variational approaches to approximate Bayesian inference provide very efficient means of performing parameter estimation and model selection. Among these, so-called variational-Laplace or {VL} schemes rely on Gaussian approximations to posterior densities on model parameters. In this note, we review the main variants of {VL} approaches, that follow from considering nonlinear models of continuous and/or categorical data. En passant, we also derive a few novel theoretical results that complete the portfolio of existing analyses of variational Bayesian approaches, including investigations of their asymptotic convergence. We also suggest practical ways of extending existing {VL} approaches to hierarchical generative models that include (e.g., precision) hyperparameters.},
	journaltitle = {{arXiv}:1703.02089 [q-bio, stat]},
	author = {Daunizeau, Jean},
	urldate = {2018-01-18},
	date = {2017-03-02},
	eprinttype = {arxiv},
	eprint = {1703.02089},
	keywords = {Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2IH22B88/1703.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/RCRKALSD/1703.html:text/html;Daunizeau - 2017 - The variational Laplace approach to approximate Ba.pdf:/Users/bert/Zotero/storage/EYHS6XC7/Daunizeau - 2017 - The variational Laplace approach to approximate Ba.pdf:application/pdf},
}

@article{brewer_computing_2017,
	title = {Computing Entropies with Nested Sampling},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {http://www.mdpi.com/1099-4300/19/8/422},
	doi = {10.3390/e19080422},
	abstract = {The Shannon entropy, and related quantities such as mutual information, can be used to quantify uncertainty and relevance. However, in practice, it can be difficult to compute these quantities for arbitrary probability distributions, particularly if the probability mass functions or densities cannot be evaluated. This paper introduces a computational approach, based on Nested Sampling, to evaluate entropies of probability distributions that can only be sampled. I demonstrate the method on three examples: a simple Gaussian example where the key quantities are available analytically; (ii) an experimental design example about scheduling observations in order to measure the period of an oscillating signal; and (iii) predicting the future from the past in a heavy-tailed scenario.},
	pages = {422},
	number = {8},
	journaltitle = {Entropy},
	author = {Brewer, Brendon J.},
	urldate = {2018-04-13},
	date = {2017-08-18},
	langid = {english},
	keywords = {Bayesian inference, entropy, information theory, Monte Carlo, mutual information, nested sampling},
	file = {Brewer - 2017 - Computing Entropies with Nested Sampling.pdf:/Users/bert/Zotero/storage/YREW3D3T/Brewer - 2017 - Computing Entropies with Nested Sampling.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/5A5QNMKL/422.html:text/html;Snapshot:/Users/bert/Zotero/storage/XIKQ5TC9/htm.html:text/html},
}

@article{skilling_galilean_2019,
	title = {Galilean and Hamiltonian Monte Carlo},
	volume = {33},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2504-3900/33/1/19},
	doi = {10.3390/proceedings2019033019},
	abstract = {Galilean Monte Carlo ({GMC}) allows exploration in a big space along systematic trajectories, thus evading the square-root inefficiency of independent steps. Galilean Monte Carlo has greater generality and power than its historical precursor Hamiltonian Monte Carlo because it discards second-order propagation under forces in favour of elementary force-free motion. Nested sampling (for which {GMC} was originally designed) has similar dominance over simulated annealing, which loses power by imposing an unnecessary thermal blurring over energy.},
	pages = {19},
	number = {1},
	journaltitle = {Proceedings},
	author = {Skilling, John},
	urldate = {2020-02-23},
	date = {2019},
	langid = {english},
	keywords = {Galilean Monte Carlo, Hamiltonian Monte Carlo, Nested sampling, simulated annealing},
	file = {Skilling - 2019 - Galilean and Hamiltonian Monte Carlo.pdf:/Users/bert/Zotero/storage/JZX4H42K/Skilling - 2019 - Galilean and Hamiltonian Monte Carlo.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/32AQ8AAI/19.html:text/html},
}

@article{knollmuller_encoding_2018,
	title = {Encoding prior knowledge in the structure of the likelihood},
	url = {http://arxiv.org/abs/1812.04403},
	abstract = {The inference of deep hierarchical models is problematic due to strong dependencies between the hierarchies. We investigate a specific transformation of the model parameters based on the multivariate distributional transform. This transformation is a special form of the reparametrization trick, flattens the hierarchy and leads to a standard Gaussian prior on all resulting parameters. The transformation also transfers all the prior information into the structure of the likelihood, hereby decoupling the transformed parameters a priori from each other. A variational Gaussian approximation in this standardized space will be excellent in situations of relatively uninformative data. Additionally, the curvature of the log-posterior is well-conditioned in directions that are weakly constrained by the data, allowing for fast inference in such a scenario. In an example we perform the transformation explicitly for Gaussian process regression with a priori unknown correlation structure. Deep models are inferred rapidly in highly and slowly in poorly informed situations. The flat model show exactly the opposite performance pattern. A synthesis of both, the deep and the flat perspective, provides their combined advantages and overcomes the individual limitations, leading to a faster inference.},
	journaltitle = {{arXiv}:1812.04403 [cs, stat]},
	author = {Knollmüller, Jakob and Enßlin, Torsten A.},
	urldate = {2020-02-21},
	date = {2018-12-11},
	eprinttype = {arxiv},
	eprint = {1812.04403},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Q6APGQSB/1812.html:text/html;Knollmüller and Enßlin - 2018 - Encoding prior knowledge in the structure of the l.pdf:/Users/bert/Zotero/storage/U52JIM6V/Knollmüller and Enßlin - 2018 - Encoding prior knowledge in the structure of the l.pdf:application/pdf},
}

@misc{skilling_this_2008,
	title = {This Physicist's view of Gelman's Bayes},
	url = {http://www.mrao.cam.ac.uk/~steve/maxent2009/images/rant.pdf},
	author = {Skilling, John},
	date = {2008},
	file = {Skilling - 2008 - This Physicist's view of Gelman's Bayes.pdf:/Users/bert/Zotero/storage/ZP7XHC96/Skilling - 2008 - This Physicist's view of Gelman's Bayes.pdf:application/pdf},
}

@article{patra_approximating_2015,
	title = {Approximating the entire spectrum of nonequilibrium steady-state distributions using relative entropy: An application to thermal conduction},
	volume = {92},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.92.023304},
	doi = {10.1103/PhysRevE.92.023304},
	shorttitle = {Approximating the entire spectrum of nonequilibrium steady-state distributions using relative entropy},
	abstract = {Distribution functions for systems in nonequilibrium steady states are usually determined through detailed experiments, both in numerical and real-life settings in the laboratory. However, for a protocol-driven distribution function, it is usually prohibitive to perform such detailed experiments for the entire range of the protocol. In this article we show that distribution functions of nonequilibrium steady states ({NESS}) evolving under a slowly varying protocol can be accurately obtained from limited data and the closest known detailed state of the system. In this manner, one needs to perform only a few detailed experiments to obtain the nonequilibrium distribution function for the entire gamut of nonlinearity. We achieve this by maximizing the relative entropy functional ({MaxRent}) subject to constraints supplied by the problem definition and new measurements. {MaxRent} is found to be superior to the principle of maximum entropy ({MaxEnt}), which maximizes Shannon's informational entropy for estimating distributions but lacks the ability to incorporate additional prior information. The {MaxRent} principle is illustrated using a toy model of ϕ4 thermal conduction consisting of a single lattice point. An external protocol controlled position-dependent temperature field drives the system out of equilibrium. Two different thermostatting schemes are employed: the Hoover-Holian deterministic thermostat (which produces multifractal dynamics under strong nonlinearity) and the Langevin stochastic thermostat (which produces phase-space-filling dynamics). Out of the 80 possible states produced by the protocol, we assume that four states are known to us in detail, one of which is used as input into {MaxRent} at a time. We find that {MaxRent} approximates the phase-space density functions for every value of the protocol, even when they are far from the known distribution. {MaxEnt}, however, is unable to capture the fine details of the phase-space distribution functions. We expect this method to be useful in other external protocol-driven nonequilibrium cases as well, making it unnecessary to perform detailed experiments for all values of the protocol when one wishes to obtain approximate distributions.},
	pages = {023304},
	number = {2},
	journaltitle = {Physical Review E},
	shortjournal = {Phys. Rev. E},
	author = {Patra, Puneet Kumar and Meléndez, Marc and Bhattacharya, Baidurya},
	urldate = {2020-02-21},
	date = {2015-08-17},
	file = {APS Snapshot:/Users/bert/Zotero/storage/77LJ9JJD/PhysRevE.92.html:text/html;Patra et al. - 2015 - Approximating the entire spectrum of nonequilibriu.pdf:/Users/bert/Zotero/storage/ZNAGICG3/Patra et al. - 2015 - Approximating the entire spectrum of nonequilibriu.pdf:application/pdf},
}

@inproceedings{skilling_bayesian_2012,
	location = {Waterloo, Ontario, Canada},
	title = {Bayesian computation in big spaces-nested sampling and Galilean Monte Carlo},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.3703630},
	doi = {10.1063/1.3703630},
	abstract = {We hold this truth to be self-evident, that good principle and good practice go hand in hand. The principles of Bayesian analysis derive from elementary symmetries, and nothing more. In sympathy with those same symmetries, and noting that every invariance broken is generality forgone, we develop the practice of Bayesian computation. This approach leads to nested sampling and Galilean Monte Carlo. Nested sampling is the canonical prior-to-posterior compression algorithm, and Galilean Monte Carlo ({GMC}) is the canonical multidimensional exploration strategy. Though inspired by high dimension, these general methods apply to problems of all size.},
	eventtitle = {{BAYESIAN} {INFERENCE} {AND} {MAXIMUM} {ENTROPY} {METHODS} {IN} {SCIENCE} {AND} {ENGINEERING}: 31st International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering},
	pages = {145--156},
	author = {Skilling, John},
	urldate = {2020-02-21},
	date = {2012},
	langid = {english},
	file = {Skilling - 2012 - Bayesian computation in big spaces-nested sampling.pdf:/Users/bert/Zotero/storage/N68IBE3A/Skilling - 2012 - Bayesian computation in big spaces-nested sampling.pdf:application/pdf},
}

@article{skilling_information_2019,
	title = {Information Geometry Conflicts With Independence},
	volume = {33},
	doi = {10.3390/proceedings2019033020},
	abstract = {Information Geometry conflicts with the independence that is required for science and for rational inference generally.},
	pages = {20},
	journaltitle = {Proceedings},
	shortjournal = {Proceedings},
	author = {Skilling, John},
	date = {2019-12-05},
	file = {Skilling - 2019 - Information Geometry Conflicts With Independence.pdf:/Users/bert/Zotero/storage/YY7UAVWX/Skilling - 2019 - Information Geometry Conflicts With Independence.pdf:application/pdf},
}

@inproceedings{caticha_basics_2015,
	location = {Clos Lucé, Amboise, France},
	title = {The basics of information geometry},
	url = {http://aip.scitation.org/doi/abs/10.1063/1.4905960},
	doi = {10.1063/1.4905960},
	abstract = {To what extent can we distinguish one probability distribution from another? Are there quantitative measures of distinguishability? The goal of this tutorial is to approach such questions by introducing the notion of the “distance” between two probability distributions and exploring some basic ideas of such an “information geometry”.},
	eventtitle = {{BAYESIAN} {INFERENCE} {AND} {MAXIMUM} {ENTROPY} {METHODS} {IN} {SCIENCE} {AND} {ENGINEERING} ({MAXENT} 2014)},
	pages = {15--26},
	author = {Caticha, Ariel},
	urldate = {2020-02-21},
	date = {2015},
	langid = {english},
	file = {Caticha - 2015 - The basics of information geometry.pdf:/Users/bert/Zotero/storage/YPRD34JH/Caticha - 2015 - The basics of information geometry.pdf:application/pdf},
}

@article{skilling_critique_2014,
	title = {Critique of information geometry},
	volume = {1636},
	issn = {0094-243X},
	url = {https://aip.scitation.org/doi/abs/10.1063/1.4903705},
	doi = {10.1063/1.4903705},
	pages = {24--29},
	number = {1},
	journaltitle = {{AIP} Conference Proceedings},
	shortjournal = {{AIP} Conference Proceedings},
	author = {Skilling, John},
	urldate = {2020-02-21},
	date = {2014-12-05},
	file = {Skilling - 2014 - Critique of information geometry.pdf:/Users/bert/Zotero/storage/MZNVZRBL/Skilling - 2014 - Critique of information geometry.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MHPGG2IK/1.html:text/html},
}

@article{silverman_extraction_2019,
	title = {Extraction of Information from Crowdsourcing: Experimental Test Employing Bayesian, Maximum Likelihood, and Maximum Entropy Methods},
	volume = {9},
	rights = {http://creativecommons.org/licenses/by/4.0/},
	url = {http://www.scirp.org/Journal/Paperabs.aspx?paperid=95939},
	doi = {10.4236/ojs.2019.95038},
	shorttitle = {Extraction of Information from Crowdsourcing},
	abstract = {A crowdsourcing experiment in which viewers (the “crowd”) of a British Broadcasting Corporation ({BBC}) television show submitted estimates of the number of coins in a tumbler was shown in an antecedent paper (Part 1) to follow a log-normal distribution ∧(m,s2). The coin-estimation experiment is an archetype of a broad class of image analysis and object counting problems suitable for solution by crowdsourcing. The objective of the current paper (Part 2) is to determine the location and scale parameters (m,s) of ∧(m,s2) by both Bayesian and maximum likelihood ({ML}) methods and to compare the results. One outcome of the analysis is the resolution, by means of Jeffreys’ rule, of questions regarding the appropriate Bayesian prior. It is shown that Bayesian and {ML} analyses lead to the same expression for the location parameter, but different expressions for the scale parameter, which become identical in the limit of an infinite sample size. A second outcome of the analysis concerns use of the sample mean as the measure of information of the crowd in applications where the distribution of responses is not sought or known. In the coin-estimation experiment, the sample mean was found to differ widely from the mean number of coins calculated from ∧(m,s2). This discordance raises critical questions concerning whether, and under what conditions, the sample mean provides a reliable measure of the information of the crowd. This paper resolves that problem by use of the principle of maximum entropy ({PME}). The {PME} yields a set of equations for finding the most probable distribution consistent with given prior information and only that information. If there is no solution to the {PME} equations for a specified sample mean and sample variance, then the sample mean is an unreliable statistic, since no measure can be assigned to its uncertainty. Parts 1 and 2 together demonstrate that the information content of crowdsourcing resides in the distribution of responses (very often log-normal in form), which can be obtained empirically or by appropriate modeling.},
	pages = {571--600},
	number = {5},
	journaltitle = {Open Journal of Statistics},
	author = {Silverman, M. P.},
	urldate = {2020-02-12},
	date = {2019-09-05},
	langid = {english},
	file = {Silverman - 2019 - Extraction of Information from Crowdsourcing Expe.pdf:/Users/bert/Zotero/storage/YJ24TUR6/Silverman - 2019 - Extraction of Information from Crowdsourcing Expe.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/53AVQW7Z/paperinformation.html:text/html},
}

@article{oates_modern_2019,
	title = {A Modern Retrospective on Probabilistic Numerics},
	volume = {29},
	issn = {0960-3174, 1573-1375},
	url = {http://arxiv.org/abs/1901.04457},
	doi = {10.1007/s11222-019-09902-z},
	abstract = {This article attempts to place the emergence of probabilistic numerics as a mathematical-statistical research field within its historical context and to explore how its gradual development can be related both to applications and to a modern formal treatment. We highlight in particular the parallel contributions of Sul'din and Larkin in the 1960s and how their pioneering early ideas have reached a degree of maturity in the intervening period, mediated by paradigms such as average-case analysis and information-based complexity. We provide a subjective assessment of the state of research in probabilistic numerics and highlight some difficulties to be addressed by future works.},
	pages = {1335--1351},
	number = {6},
	journaltitle = {Statistics and Computing},
	shortjournal = {Stat Comput},
	author = {Oates, C. J. and Sullivan, T. J.},
	urldate = {2020-02-08},
	date = {2019-11},
	eprinttype = {arxiv},
	eprint = {1901.04457},
	keywords = {Statistics - Machine Learning, Mathematics - Probability, 62-03, 65-03, 01A60, 01A65, 01A67, Mathematics - History and Overview, Mathematics - Numerical Analysis},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LFLL66HA/1901.html:text/html;Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf:/Users/bert/Zotero/storage/3HZVQ7ZV/Oates and Sullivan - 2019 - A Modern Retrospective on Probabilistic Numerics.pdf:application/pdf},
}

@article{zhang_unifying_2019,
	title = {Unifying Message Passing Algorithms Under the Framework of Constrained Bethe Free Energy Minimization},
	url = {http://arxiv.org/abs/1703.10932},
	abstract = {Variational message passing ({VMP}), belief propagation ({BP}) and expectation propagation ({EP}) have found their wide applications in complex statistical signal processing problems. In addition to viewing them as a class of algorithms operating on graphical models, this paper unifies them under an optimization framework, namely, Bethe free energy minimization with differently and appropriately imposed constraints. This new perspective in terms of constraint manipulation can offer additional insights on the connection between different message passing algorithms and is valid for a generic statistical model. It also founds a theoretical framework to systematically derive message passing variants. Taking the sparse signal recovery ({SSR}) problem as an example, a low-complexity {EP} variant can be obtained by simple constraint reformulation, delivering better estimation performance with lower complexity than the standard {EP} algorithm. Furthermore, we can resort to the framework for the systematic derivation of hybrid message passing for complex inference tasks. Notably, a hybrid message passing algorithm is exemplarily derived for joint {SSR} and statistical model learning with near-optimal inference performance and scalable complexity.},
	journaltitle = {{arXiv}:1703.10932 [cs, math]},
	author = {Zhang, Dan and Song, Xiaohang and Wang, Wenjin and Fettweis, Gerhard and Gao, Xiqi},
	urldate = {2020-01-30},
	date = {2019-12-05},
	eprinttype = {arxiv},
	eprint = {1703.10932},
	keywords = {Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/F2CFYKJA/1703.html:text/html;Zhang et al. - 2019 - Unifying Message Passing Algorithms Under the Fram.pdf:/Users/bert/Zotero/storage/7DELAAWN/Zhang et al. - 2019 - Unifying Message Passing Algorithms Under the Fram.pdf:application/pdf},
}

@article{da_costa_natural_2020,
	title = {Natural selection finds natural gradient},
	url = {http://arxiv.org/abs/2001.08028},
	abstract = {Active inference provides a biologically plausible process theory of brain function. It specifies neuronal dynamics for state-estimation in terms of a gradient descent on (variational) free energy -- a measure of the fit between an internal (generative) model and sensory observations. When formulated for discrete state-space generative models, the free energy gradient turns out to be a prediction error -- plausibly encoded in the average membrane potentials of neuronal populations. Conversely, the expected probability of a state can then be expressed in terms of firing rates. We establish a construct validity to this scheme -- by showing that it is consistent with current models of neuronal dynamics -- and face validity, as it is able to synthesize a wide range of biologically plausible electrophysiological responses. We then show that these neuronal dynamics approximate natural gradient descent, a well-known optimisation algorithm from information geometry that prescribes locally optimal belief updates. Lastly, numerical simulations suggest that both schemes perform equally well on average. The performance of belief updating is scored in terms of information length, a measure of the distance traveled in information space, which has a direct interpretation in terms of metabolic efficiency. These results show that active inference is consistent with state-of-the-art models of neuronal dynamics and coincides with the natural gradient. This suggests that natural selection, by selecting the phenotypes that optimise metabolic and computational efficiency, has implicitly approximated the steepest direction in information space; namely, natural gradient descent.},
	journaltitle = {{arXiv}:2001.08028 [q-bio]},
	author = {Da Costa, Lancelot and Parr, Thomas and Sengupta, Biswa and Friston, Karl},
	urldate = {2020-01-24},
	date = {2020-01-22},
	eprinttype = {arxiv},
	eprint = {2001.08028},
	keywords = {Quantitative Biology - Neurons and Cognition, Quantitative Biology - Populations and Evolution},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/TH94IJIP/2001.html:text/html;Da Costa et al. - 2020 - Natural selection finds natural gradient.pdf:/Users/bert/Zotero/storage/8RTZ4RJG/Da Costa et al. - 2020 - Natural selection finds natural gradient.pdf:application/pdf},
}

@article{da_costa_active_2020,
	title = {Active inference on discrete state-spaces: a synthesis},
	url = {http://arxiv.org/abs/2001.07203},
	shorttitle = {Active inference on discrete state-spaces},
	abstract = {Active inference is a normative principle underwriting perception, action, planning, decision-making and learning in biological or artificial agents. From its inception, its associated process theory has grown to incorporate complex generative models, enabling simulation of a wide range of complex behaviours. Due to successive developments in active inference, it is often difficult to see how its underlying principle relates to process theories and practical implementation. In this paper, we try to bridge this gap by providing a complete mathematical synthesis of active inference on discrete state-space models. This technical summary provides an overview of the theory, derives neuronal dynamics from first principles and relates this dynamics to biological processes. Furthermore, this paper provides a fundamental building block needed to understand active inference for mixed generative models; allowing continuous sensations to inform discrete representations. This paper may be used as follows: to guide research towards outstanding challenges, a practical guide on how to implement active inference to simulate experimental behaviour, or a pointer towards various in-silico neurophysiological responses that may be used to make empirical predictions.},
	journaltitle = {{arXiv}:2001.07203 [q-bio]},
	author = {Da Costa, Lancelot and Parr, Thomas and Sajid, Noor and Veselic, Sebastijan and Neacsu, Victorita and Friston, Karl},
	urldate = {2020-01-22},
	date = {2020-01-20},
	eprinttype = {arxiv},
	eprint = {2001.07203},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7L7XV6TP/2001.html:text/html;Da Costa et al. - 2020 - Active inference on discrete state-spaces a synth.pdf:/Users/bert/Zotero/storage/CYL8FTC9/Da Costa et al. - 2020 - Active inference on discrete state-spaces a synth.pdf:application/pdf},
}

@article{annila_natural_2009,
	title = {Natural hierarchy emerges from energy dispersal},
	volume = {95},
	issn = {1872-8324},
	doi = {10.1016/j.biosystems.2008.10.008},
	abstract = {Hierarchical organization of 'systems within systems' is an apparent characteristic of nature. For many biotic and abiotic systems it is known how the nested structural and functional order builds up, yet the general principle why matter evolves to hierarchies has remained unfamiliar to many. We clarify that increasingly larger integrated systems result from the quest to decrease free energy according to the 2nd law of thermodynamics. The argumentation is based on the recently derived equation of motion for natural processes. Hierarchically organized energy transduction machinery emerges naturally when it provides increased rates of energy dispersal. Likewise, a hierarchical system will dismantle into its constituents when they as independent systems will provide higher rates of entropy increase. Since energy flows via interactions, decreasing strengths of interactions over increasingly larges lengths scales mark natural boundaries for nested integrated systems.},
	pages = {227--233},
	number = {3},
	journaltitle = {Bio Systems},
	shortjournal = {{BioSystems}},
	author = {Annila, Arto and Kuismanen, Esa},
	date = {2009-03},
	pmid = {19038306},
	keywords = {Entropy, Thermodynamics},
	file = {Annila and Kuismanen - 2009 - Natural hierarchy emerges from energy dispersal.pdf:/Users/bert/Zotero/storage/J7SAVZKK/Annila and Kuismanen - 2009 - Natural hierarchy emerges from energy dispersal.pdf:application/pdf},
}

@article{mehta_high-bias_2019,
	title = {A high-bias, low-variance introduction to Machine Learning for physicists},
	volume = {810},
	issn = {0370-1573},
	url = {http://www.sciencedirect.com/science/article/pii/S0370157319300766},
	doi = {10.1016/j.physrep.2019.03.001},
	series = {A high-bias, low-variance introduction to Machine Learning for physicists},
	abstract = {Machine Learning ({ML}) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in {ML} and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including {MaxEnt} models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between {ML} and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern {ML}/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in {ML} where physicists may be able to contribute.},
	pages = {1--124},
	journaltitle = {Physics Reports},
	shortjournal = {Physics Reports},
	author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
	urldate = {2020-01-02},
	date = {2019-05-30},
	langid = {english},
	file = {Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine .pdf:/Users/bert/Zotero/storage/X5EGXXFC/Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine .pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/L69ZC9HM/S0370157319300766.html:text/html},
}

@article{vergassola_infotaxis_2007,
	title = {‘Infotaxis’ as a strategy for searching without gradients},
	volume = {445},
	doi = {10.1038/nature05464},
	abstract = {Chemotactic bacteria rely on local concentration gradients to guide them towards the source of a nutrient. Such local cues pointing towards the location of the source are not always available at macroscopic scales because mixing in a flowing medium breaks up regions of high concentration into random and disconnected patches. Thus, animals sensing odours in air or water detect them only intermittently as patches sweep by on the wind or currents. A macroscopic searcher must devise a strategy of movement based on sporadic cues and partial information. Here we propose a search algorithm, which we call 'infotaxis', designed to work under such conditions. Any search process can be thought of as acquisition of information on source location; for infotaxis, information plays a role similar to concentration in chemotaxis. The infotaxis strategy locally maximizes the expected rate of information gain. We demonstrate its efficiency using a computational model of odour plume propagation and experimental data on mixing flows. Infotactic trajectories feature 'zigzagging' and 'casting' paths similar to those observed in the flight of moths. The proposed search algorithm is relevant to the design of olfactory robots, but the general idea of infotaxis can be applied more broadly in the context of searching with sparse information.},
	pages = {406--9},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Vergassola, Massimo and Villermaux, Emmanuel and Shraiman, Boris},
	date = {2007-02-01},
	file = {Vergassola et al. - 2007 - ‘Infotaxis’ as a strategy for searching without gr.pdf:/Users/bert/Zotero/storage/4EIP2WXG/Vergassola et al. - 2007 - ‘Infotaxis’ as a strategy for searching without gr.pdf:application/pdf},
}

@article{granziol_meme_2019,
	title = {{MEMe}: An Accurate Maximum Entropy Method for Efficient Approximations in Large-Scale Machine Learning},
	volume = {21},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1906.01101},
	doi = {10.3390/e21060551},
	shorttitle = {{MEMe}},
	abstract = {Efficient approximation lies at the heart of large-scale machine learning problems. In this paper, we propose a novel, robust maximum entropy algorithm, which is capable of dealing with hundreds of moments and allows for computationally efficient approximations. We showcase the usefulness of the proposed method, its equivalence to constrained Bayesian variational inference and demonstrate its superiority over existing approaches in two applications, namely, fast log determinant estimation and information-theoretic Bayesian optimisation.},
	pages = {551},
	number = {6},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Granziol, Diego and Ru, Binxin and Zohren, Stefan and Doing, Xiaowen and Osborne, Michael and Roberts, Stephen},
	urldate = {2020-01-01},
	date = {2019-05-31},
	eprinttype = {arxiv},
	eprint = {1906.01101},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/C3ZDN7CH/1906.html:text/html;Granziol et al. - 2019 - MEMe An Accurate Maximum Entropy Method for Effic.pdf:/Users/bert/Zotero/storage/Q5RZT7GQ/Granziol et al. - 2019 - MEMe An Accurate Maximum Entropy Method for Effic.pdf:application/pdf},
}

@article{parr_markov_2020,
	title = {Markov blankets, information geometry and stochastic thermodynamics},
	volume = {378},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2019.0159},
	doi = {10.1098/rsta.2019.0159},
	abstract = {This paper considers the relationship between thermodynamics, information and inference. In particular, it explores the thermodynamic concomitants of belief updating, under a variational (free energy) principle for self-organization. In brief, any (weakly mixing) random dynamical system that possesses a Markov blanket—i.e. a separation of internal and external states—is equipped with an information geometry. This means that internal states parametrize a probability density over external states. Furthermore, at non-equilibrium steady-state, the flow of internal states can be construed as a gradient flow on a quantity known in statistics as Bayesian model evidence. In short, there is a natural Bayesian mechanics for any system that possesses a Markov blanket. Crucially, this means that there is an explicit link between the inference performed by internal states and their energetics—as characterized by their stochastic thermodynamics.
            This article is part of the theme issue ‘Harmonizing energy-autonomous computing and intelligence’.},
	pages = {20190159},
	number = {2164},
	journaltitle = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	shortjournal = {Phil. Trans. R. Soc. A},
	author = {Parr, Thomas and Da Costa, Lancelot and Friston, Karl},
	urldate = {2019-12-29},
	date = {2020-02-07},
	langid = {english},
	file = {Parr et al. - 2020 - Markov blankets, information geometry and stochast.pdf:/Users/bert/Zotero/storage/SWNB2443/Parr et al. - 2020 - Markov blankets, information geometry and stochast.pdf:application/pdf},
}

@article{giffin_maximum_2009,
	title = {Maximum Entropy: The Universal Method for Inference},
	url = {http://arxiv.org/abs/0901.2987},
	shorttitle = {Maximum Entropy},
	abstract = {In this thesis we start by providing some detail regarding how we arrived at our present understanding of probabilities and how we manipulate them - the product and addition rules by Cox. We also discuss the modern view of entropy and how it relates to known entropies such as the thermodynamic entropy and the information entropy. Next, we show that Skilling's method of induction leads us to a unique general theory of inductive inference, the {ME} method and precisely how it is that other entropies such as those of Renyi or Tsallis are ruled out for problems of inference. We then explore the compatibility of Bayes and {ME} updating. We show that {ME} is capable of producing every aspect of orthodox Bayesian inference and proves the complete compatibility of Bayesian and entropy methods. The realization that the {ME} method incorporates Bayes' rule as a special case allows us to go beyond Bayes' rule and to process both data and expected value constraints simultaneously. We discuss the general problem of non-commuting constraints, when they should be processed sequentially and when simultaneously. The generic "canonical" form of the posterior distribution for the problem of simultaneous updating with data and moments is obtained. This is a major achievement since it shows that {ME} is not only capable of processing information in the form of constraints, like {MaxEnt} and information in the form of data, as in Bayes' Theorem, but also can process both forms simultaneously, which Bayes and {MaxEnt} cannot do alone. Finally, we illustrate some potential applications for this new method by applying {ME} to potential problems of interest.},
	journaltitle = {{arXiv}:0901.2987 [physics]},
	author = {Giffin, Adom},
	urldate = {2019-12-23},
	date = {2009-01-20},
	eprinttype = {arxiv},
	eprint = {0901.2987},
	keywords = {Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/JZ9WQCCS/0901.html:text/html;Giffin - 2009 - Maximum Entropy The Universal Method for Inferenc.pdf:/Users/bert/Zotero/storage/SFR6Z4H3/Giffin - 2009 - Maximum Entropy The Universal Method for Inferenc.pdf:application/pdf},
}

@inproceedings{engel_ddsp:_2019,
	title = {{DDSP}: Differentiable Digital Signal Processing},
	url = {https://openreview.net/forum?id=B1x1ma4tDr},
	shorttitle = {{DDSP}},
	abstract = {Most generative models of audio directly generate samples in one of two domains: time or frequency. While sufficient to express any signal, these representations are inefficient, as they do not...},
	eventtitle = {International Conference on Learning Representations},
	author = {Engel, Jesse and Hantrakul, Lamtharn (Hanoi) and Gu, Chenjie and Roberts, Adam},
	urldate = {2019-12-21},
	date = {2019-09-25},
	file = {Engel et al. - 2019 - DDSP Differentiable Digital Signal Processing.pdf:/Users/bert/Zotero/storage/J22FYHW5/Engel et al. - 2019 - DDSP Differentiable Digital Signal Processing.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2YUZQPHF/forum.html:text/html},
}

@article{ghojogh_hidden_2019,
	title = {Hidden Markov Model: Tutorial},
	url = {https://engrxiv.org/w9v2b/},
	doi = {10.31224/osf.io/w9v2b},
	shorttitle = {Hidden Markov Model},
	abstract = {This is a tutorial paper for Hidden Markov Model ({HMM}). First, we briefly review the background on Expectation Maximization ({EM}), Lagrange multiplier, factor graph, the sum-product algorithm, the max-product algorithm, and belief propagation by the forward-backward procedure. Then, we introduce probabilistic graphical models including Markov random field and Bayesian network. Markov property and Discrete Time Markov Chain ({DTMC}) are also introduced. We, then, explain likelihood estimation and {EM} in {HMM} in technical details. We explain evaluation in {HMM} where direct calculation and the forward-backward belief propagation are both explained. Afterwards, estimation in {HMM} is covered where both the greedy approach and the Viterbi algorithm are detailed. Then, we explain how to train {HMM} using {EM} and the Baum-Welch algorithm. We also explain how to use {HMM} in some applications such as speech and action recognition.},
	author = {Ghojogh, Benyamin and Karray, Fakhri and Crowley, Mark},
	urldate = {2019-12-17},
	date = {2019-12-10},
	file = {Ghojogh et al. - 2019 - Hidden Markov Model Tutorial.pdf:/Users/bert/Zotero/storage/9I3SRYZM/Ghojogh et al. - 2019 - Hidden Markov Model Tutorial.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/LPSZGTBR/w9v2b.html:text/html},
}

@article{kingma_introduction_2019,
	title = {An Introduction to Variational Autoencoders},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	pages = {307--392},
	number = {4},
	journaltitle = {Foundations and Trends® in Machine Learning},
	shortjournal = {{FNT} in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2019-12-17},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1906.02691},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/U96S43D9/1906.html:text/html;Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:/Users/bert/Zotero/storage/EZGT79FB/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf},
}

@article{berseth_smirl:_2019,
	title = {{SMiRL}: Surprise Minimizing {RL} in Dynamic Environments},
	url = {http://arxiv.org/abs/1912.05510},
	shorttitle = {{SMiRL}},
	abstract = {All living organisms struggle against the forces of nature to carve out niches where they can maintain homeostasis. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing {RL} ({SMiRL}). {SMiRL} trains an agent with the objective of maximizing the probability of observed states under a model trained on previously seen states. The resulting agents can acquire proactive behaviors that seek out and maintain stable conditions, such as balancing and damage avoidance, that are closely tied to an environment's prevailing sources of entropy, such as wind, earthquakes, and other agents. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls and navigate to escape enemy agents, without any task-specific reward supervision. We further show that {SMiRL} can be used together with a standard task reward to accelerate reward-driven learning.},
	journaltitle = {{arXiv}:1912.05510 [cs, stat]},
	author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
	urldate = {2019-12-12},
	date = {2019-12-11},
	eprinttype = {arxiv},
	eprint = {1912.05510},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/VNC2ZC99/1912.html:text/html;Berseth et al. - 2019 - SMiRL Surprise Minimizing RL in Dynamic Environme.pdf:/Users/bert/Zotero/storage/QVUDS57P/Berseth et al. - 2019 - SMiRL Surprise Minimizing RL in Dynamic Environme.pdf:application/pdf},
}

@article{tschantz_scaling_2019,
	title = {Scaling active inference},
	url = {http://arxiv.org/abs/1911.10601},
	abstract = {In reinforcement learning ({RL}), agents often operate in partially observed and uncertain environments. Model-based {RL} suggests that this is best achieved by learning and exploiting a probabilistic model of the world. 'Active inference' is an emerging normative framework in cognitive and computational neuroscience that offers a unifying account of how biological agents achieve this. On this framework, inference, learning and action emerge from a single imperative to maximize the Bayesian evidence for a niched model of the world. However, implementations of this process have thus far been restricted to low-dimensional and idealized situations. Here, we present a working implementation of active inference that applies to high-dimensional tasks, with proof-of-principle results demonstrating efficient exploration and an order of magnitude increase in sample efficiency over strong model-free baselines. Our results demonstrate the feasibility of applying active inference at scale and highlight the operational homologies between active inference and current model-based approaches to {RL}.},
	journaltitle = {{arXiv}:1911.10601 [cs, eess, math, stat]},
	author = {Tschantz, Alexander and Baltieri, Manuel and Seth, Anil K. and Buckley, Christopher L.},
	urldate = {2019-12-11},
	date = {2019-11-24},
	eprinttype = {arxiv},
	eprint = {1911.10601},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6I7ZZWM5/1911.html:text/html;Tschantz et al. - 2019 - Scaling active inference.pdf:/Users/bert/Zotero/storage/IIVRCF6C/Tschantz et al. - 2019 - Scaling active inference.pdf:application/pdf},
}

@article{turner_free_nodate,
	title = {Free Energy in Statistical Physics and Inference},
	author = {Turner, Richard},
	file = {FreeEnergyNotes.pdf:/Users/bert/Zotero/storage/GP9XWXBC/FreeEnergyNotes.pdf:application/pdf},
}

@article{czegel_evolutionary_2019,
	title = {Evolutionary implementation of Bayesian computations},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/685842v1},
	doi = {10.1101/685842},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}A wide variety of human and non-human behavior is computationally well accounted for by probabilistic generative models, formalized consistently in a Bayesian framework. Recently, it has been suggested that another family of adaptive systems, namely, those governed by Darwinian evolutionary dynamics, are capable of implementing building blocks of Bayesian computations. These algorithmic similarities rely on the analogous competition dynamics of generative models and of Darwinian replicators to fit possibly high-dimensional and stochastic environments. Identified computational building blocks include Bayesian update over a single variable and replicator dynamics, transition between hidden states and mutation, and Bayesian inference in hierarchical models and multilevel selection. Here we provide a coherent mathematical discussion of these observations in terms of Bayesian graphical models and a step-by-step introduction to their evolutionary interpretation. We also extend existing results by adding two missing components: a correspondence between likelihood optimization and phenotypic adaptation, and between expectation-maximization-like dynamics in mixture models and ecological competition. These correspondences suggest a deeper algorithmic analogy between evolutionary dynamics and statistical learning, pointing towards a unified computational understanding of mechanisms Nature invented to adapt to high-dimensional and uncertain environments.{\textless}/p{\textgreater}},
	pages = {685842},
	journaltitle = {{bioRxiv}},
	author = {Czégel, Dániel and Giaffar, Hamza and Zachar, István and Szathmáry, Eörs},
	urldate = {2019-11-29},
	date = {2019-06-28},
	langid = {english},
	file = {Czégel et al. - 2019 - Evolutionary implementation of Bayesian computatio.pdf:/Users/bert/Zotero/storage/NJJRDU9I/Czégel et al. - 2019 - Evolutionary implementation of Bayesian computatio.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YUWRUWYF/685842v1.html:text/html},
}

@article{barbour_nature_2009,
	title = {The Nature of Time},
	url = {http://arxiv.org/abs/0903.3489},
	abstract = {A review of some basic facts of classical dynamics shows that time, or precisely duration, is redundant as a fundamental concept. Duration and the behaviour of clocks emerge from a timeless law that governs change.},
	journaltitle = {{arXiv}:0903.3489 [gr-qc]},
	author = {Barbour, Julian},
	urldate = {2019-11-25},
	date = {2009-03-20},
	eprinttype = {arxiv},
	eprint = {0903.3489},
	keywords = {General Relativity and Quantum Cosmology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/C5D3YCU5/0903.html:text/html;Barbour - 2009 - The Nature of Time.pdf:/Users/bert/Zotero/storage/I2ZQDFZB/Barbour - 2009 - The Nature of Time.pdf:application/pdf},
}

@book{gregory_classical_2006,
	location = {Cambridge, {UK} ; New York},
	edition = {1 edition},
	title = {Classical Mechanics},
	isbn = {978-0-521-53409-3},
	abstract = {Gregory's Classical Mechanics is a major new textbook for undergraduates in mathematics and physics. It is a thorough, self-contained and highly readable account of a subject many students find difficult. The author's clear and systematic style promotes a good understanding of the subject; each concept is motivated and illustrated by worked examples, while problem sets provide plenty of practice for understanding and technique. Computer assisted problems, some suitable for projects, are also included. The book is structured to make learning the subject easy; there is a natural progression from core topics to more advanced ones and hard topics are treated with particular care. A theme of the book is the importance of conservation principles. These appear first in vectorial mechanics where they are proved and applied to problem solving. They reappear in analytical mechanics, where they are shown to be related to symmetries of the Lagrangian, culminating in Noether's theorem.},
	pagetotal = {596},
	publisher = {Cambridge University Press},
	author = {Gregory, R. Douglas},
	date = {2006-04-17},
	file = {Gregory - 2006 - Classical Mechanics.pdf:/Users/bert/Zotero/storage/RDJX8JYP/Gregory - 2006 - Classical Mechanics.pdf:application/pdf;Gregory R.D. - Classical mechanics_ Solution manual-CUP (2006).pdf:/Users/bert/Zotero/storage/SCDYK9X8/Gregory R.D. - Classical mechanics_ Solution manual-CUP (2006).pdf:application/pdf},
}

@book{cline_variational_2019,
	title = {Variational Principles in Classical Mechanics: Revised Second Edition},
	isbn = {978-0-9988372-9-1},
	shorttitle = {Variational Principles in Classical Mechanics},
	abstract = {Two dramatically different philosophical approaches to classical mechanics were proposed during the 17th – 18th centuries. Newton developed his vectorial formulation that uses time-dependent differential equations of motion to relate vector observables like force and rate of change of momentum. Euler, Lagrange, Hamilton, and Jacobi, developed powerful alternative variational formulations based on the assumption that nature follows the principle of least action. These variational formulations now play a pivotal role in science and engineering. This book introduces variational principles and their application to classical mechanics. The relative merits of the intuitive Newtonian vectorial formulation, and the more powerful variational formulations are compared. Applications to a wide variety of topics illustrate the intellectual beauty, remarkable power, and broad scope provided by use of variational principles in physics. This second edition adds discussion of the use of variational principles applied to the following topics: 1) Systems subject to initial boundary conditions 2) The hierarchy of the related formulations based on action, Lagrangian, Hamiltonian, and equations of motion, to systems that involve symmetries 3) Non-conservative systems. 4) Variable-mass systems. 5) The General Theory of Relativity.},
	pagetotal = {587},
	publisher = {River Campus Libraries},
	author = {Cline, Douglas},
	date = {2019-01-08},
	file = {Cline - 2019 - Variational Principles in Classical Mechanics Rev.pdf:/Users/bert/Zotero/storage/D5ZALXXR/Cline - 2019 - Variational Principles in Classical Mechanics Rev.pdf:application/pdf},
}

@article{carrara_inferential_2019,
	title = {The Inferential Foundations of Mutual Information},
	url = {http://arxiv.org/abs/1907.06992},
	abstract = {The goal of this paper is to design a functional that is capable of quantifying the amount of global correlations encoded in a given probability distribution \${\textbackslash}rho\$. This is achieved by imposing what we call the {\textbackslash}textit\{Principle of Constant Correlations\} ({PCC}) and using eliminative induction. Together with our design goal, the ({PCC}) guides us in choosing the appropriate design criteria for constructing the desired functional. The residual functional after eliminative induction is the mutual information ({MI}) and therefore the ({MI}) is designed to quantify the amount of global correlations encoded in \${\textbackslash}rho\$. The ({MI}) is the unique functional capable of determining whether a certain class of inferential transformations, \${\textbackslash}rho{\textbackslash}xrightarrow\{*\}{\textbackslash}rho'\$, preserve, destroy or create correlations, which provides conceptual clarity by ruling out other possible global correlation quantifiers . Further, our design derivation allows us to improve the notion and efficacy of statistical sufficiency by expressing it in terms of a normalized ({MI}) that represents the percentage in which a statistic or transformation is a sufficient.},
	journaltitle = {{arXiv}:1907.06992 [physics, stat]},
	author = {Carrara, Nicholas and Vanslette, Kevin},
	urldate = {2019-11-23},
	date = {2019-11-11},
	eprinttype = {arxiv},
	eprint = {1907.06992},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory, Physics - Data Analysis, Statistics and Probability},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XUKM58E2/1907.html:text/html;Carrara and Vanslette - 2019 - The Inferential Foundations of Mutual Information.pdf:/Users/bert/Zotero/storage/U2C8GL3W/Carrara and Vanslette - 2019 - The Inferential Foundations of Mutual Information.pdf:application/pdf},
}

@article{difranzo_entropic_2019,
	title = {The Entropic Dynamics Approach to the Paradigmatic Quantum Mechanical Phenomena},
	url = {http://arxiv.org/abs/1907.01730},
	abstract = {Standard Quantum Mechanics, although successful in terms of calculating and predicting results, is inherently difficult to understand and can suffer from misinterpretation. Entropic Dynamics is an epistemic approach to quantum mechanics based on logical inference. It incorporates the probabilities that naturally arise in situations where there is missing information. The development of Entropic Dynamics involves describing a particle in terms of a probability density, and then following the time evolution of the probability density based on diffusion-like motion and maximization of entropy. Here we will apply Entropic Dynamics to several of the paradigmatic examples used in the instruction of quantum mechanics. These examples include wave packet expansion, interference, the double slit experiment, the harmonic oscillator, and entanglement. In the section on the double slit experiment, some interesting insight is gained concerning the occurrence of minima in the absence of a mechanism for destructive interference, since probabilities only add. Also, the section on the harmonic oscillator provides interesting insight into rotation and angular momentum as it pertains to the flow of probability. The last topic consists of some remarks concerning the state of education research as it pertains to quantum mechanics and the ways in which Entropic Dynamics might address them.},
	journaltitle = {{arXiv}:1907.01730 [quant-ph]},
	author = {{DiFranzo}, Susan},
	urldate = {2019-11-22},
	date = {2019-06-28},
	eprinttype = {arxiv},
	eprint = {1907.01730},
	keywords = {Quantum Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XQML28CX/1907.html:text/html;DiFranzo - 2019 - The Entropic Dynamics Approach to the Paradigmatic.pdf:/Users/bert/Zotero/storage/BT4PZXME/DiFranzo - 2019 - The Entropic Dynamics Approach to the Paradigmatic.pdf:application/pdf},
}

@inproceedings{jaynes_bayesian_1986,
	title = {Bayesian Methods: General Background},
	shorttitle = {Bayesian Methods},
	abstract = {: We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recent work and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory "feel" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding. {HERODOTUS} 2 {BERNOULLI} 2 {BAYES} 4 {LAPLACE} 5 {JEFFREYS} 6 {COX} 8 {SHANNON} 9 {COMMUNICATION} {DIFFICULTIES} 10 {IS} {OUR} {LOGIC} {OPEN} {OR} {CLOSED}? 13 {DOWNWARD} {ANALYSIS} {IN} {STATISTICAL} {MECHANICS} 14 {CURRENT} {PROBLEMS} 15 {REFERENCES} 17 ?  Presented at the Fourth Annual Workshop on Bayesian/Maximum Entropy Methods, University of Calgary, August 1984. In the Proceedings Volume, Maximum Entropy and Bayesian Methods in Applied Statistics, J. H....},
	author = {Jaynes, E. T.},
	date = {1986},
	file = {Citeseer - Snapshot:/Users/bert/Zotero/storage/Z24XSCDA/summary.html:text/html;Jaynes - 1986 - Bayesian Methods General Background.pdf:/Users/bert/Zotero/storage/D3NDPGU3/Jaynes - 1986 - Bayesian Methods General Background.pdf:application/pdf},
}

@article{vanslette_inferential_2018,
	title = {The Inferential Design of Entropy and its Application to Quantum Measurements},
	url = {http://arxiv.org/abs/1804.09142},
	abstract = {This thesis synthesizes probability and entropic inference with Quantum Mechanics ({QM}) and quantum measurement [1-6]. It is shown that the standard and quantum relative entropies are tools designed for the purpose of updating probability distributions and density matrices, respectively [1]. The derivation of the standard and quantum relative entropies are completed in tandem and follow from the same inferential principle - the principle of minimal updating [21,66]. As the quantum maximum entropy method is derived using the standard quantum mechanical formalism, the quantum maximum entropy method may be appended to the standard quantum mechanical formalism and remove collapse as a required postulate, in agreement with [11]. The quantum maximum entropy method is found to be a "universal method of density matrix inference" as it can process information about data and moments simultaneously (giving joint generalized quantum inference solutions), which when processed separately gives the Quantum Bayes Rule [2,39] or a canonical quantum (von Neumann) maximum entropy solution [10], respectively, as special cases. The second part of this thesis revolves around a foundational theory of {QM} called Entropic Dynamics ({ED}) [13]. Rather than appending an interpretation to {QM}, {ED} states its interpretation, "that particles have definite, yet unknown, positions and that entropic probability updating works" - only then does {ED} derive {QM} as an application of inference consistent with these assumptions. This shift in interpretation allows one to solve the quantum measurement problem [3,14] and avoid being ruled out by quantum no-go theorems [4]. Observables are divvied-up into two classes in {ED}: they are the ontic "beables" [15] (particle position), and the epistemic "inferables" [3], which are not predisposed to be part of the ontology as they are inferred in general from position detections.},
	journaltitle = {{arXiv}:1804.09142 [quant-ph]},
	author = {Vanslette, Kevin},
	urldate = {2019-11-22},
	date = {2018-04-24},
	eprinttype = {arxiv},
	eprint = {1804.09142},
	keywords = {Quantum Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/E6LJSK9R/1804.html:text/html;Vanslette - 2018 - The Inferential Design of Entropy and its Applicat.pdf:/Users/bert/Zotero/storage/EASLTEQN/Vanslette - 2018 - The Inferential Design of Entropy and its Applicat.pdf:application/pdf},
}

@article{caticha_entropic_2018,
	title = {Entropic Dynamics: Mechanics without Mechanism},
	url = {http://arxiv.org/abs/1704.02663},
	shorttitle = {Entropic Dynamics},
	abstract = {Entropic Dynamics is a framework in which dynamical laws such as those that arise in physics are derived as an application of entropic methods of inference. No underlying action principle is postulated. Instead, the dynamics is driven by entropy subject to constraints reflecting the information that is relevant to the problem at hand. In this work I review the derivation of quantum theory but the fact that Entropic Dynamics is based on inference methods that are of universal applicability suggests that it may be possible to adapt these methods to fields other than physics.},
	journaltitle = {{arXiv}:1704.02663 [cond-mat, physics:quant-ph]},
	author = {Caticha, Ariel},
	urldate = {2019-11-22},
	date = {2018-02-21},
	eprinttype = {arxiv},
	eprint = {1704.02663},
	keywords = {Condensed Matter - Statistical Mechanics, Quantum Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/47AA4A84/1704.html:text/html;Caticha - 2018 - Entropic Dynamics Mechanics without Mechanism.pdf:/Users/bert/Zotero/storage/Q6VKEGAF/Caticha - 2018 - Entropic Dynamics Mechanics without Mechanism.pdf:application/pdf},
}

@article{abedi_entropic_2019,
	title = {Entropic Dynamics of Stocks and European Options},
	volume = {21},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1908.06355},
	doi = {10.3390/e21080765},
	abstract = {We develop an entropic framework to model the dynamics of stocks and European Options. Entropic inference is an inductive inference framework equipped with proper tools to handle situations where incomplete information is available. The objective of the paper is to lay down an alternative framework for modeling dynamics. An important information about the dynamics of a stock's price is scale invariance. By imposing the scale invariant symmetry, we arrive at choosing the logarithm of the stock's price as the proper variable to model. The dynamics of stock log price is derived using two pieces of information, the continuity of motion and the directionality constraint. The resulting model is the same as the Geometric Brownian Motion, {GBM}, of the stock price which is manifestly scale invariant. Furthermore, we come up with the dynamics of probability density function, which is a Fokker--Planck equation. Next, we extend the model to value the European Options on a stock. Derivative securities ought to be prices such that there is no arbitrage. To ensure the no-arbitrage pricing, we derive the risk-neutral measure by incorporating the risk-neutral information. Consequently, the Black--Scholes model and the Black--Scholes-Merton differential equation are derived.},
	pages = {765},
	number = {8},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Abedi, Mohammad and Bartolomeo, Daniel},
	urldate = {2019-11-22},
	date = {2019-08-06},
	eprinttype = {arxiv},
	eprint = {1908.06355},
	keywords = {Quantitative Finance - Pricing of Securities},
	file = {Abedi and Bartolomeo - 2019 - Entropic Dynamics of Stocks and European Options.pdf:/Users/bert/Zotero/storage/8LFHS6ZZ/Abedi and Bartolomeo - 2019 - Entropic Dynamics of Stocks and European Options.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/8KKW7XAH/1908.html:text/html},
}

@article{golan_info-metrics_2018,
	title = {Info-Metrics for Modeling and Inference},
	volume = {28},
	issn = {1572-8641},
	url = {https://doi.org/10.1007/s11023-018-9477-2},
	doi = {10.1007/s11023-018-9477-2},
	abstract = {Info-metrics is a framework for rational inference based on insufficient information. The complete info-metric framework, accompanied with many interdisciplinary examples and case studies, as well as graphical representations of the theory appear in the new book “Foundations of Info-Metrics: Modeling, Inference and Imperfect Information,” Oxford University Press, 2018. In this commentary, I describe that framework in general terms, demonstrate some of the ideas via simple examples, and provide arguments for using it to transform information into useful knowledge.},
	pages = {787--793},
	number = {4},
	journaltitle = {Minds and Machines},
	shortjournal = {Minds \& Machines},
	author = {Golan, Amos},
	urldate = {2019-11-22},
	date = {2018-12-01},
	langid = {english},
	keywords = {Decision making, Optimization, Inference, Info-metrics, Information, Modeling},
	file = {Golan - 2018 - Info-Metrics for Modeling and Inference.pdf:/Users/bert/Zotero/storage/NY47XZ56/Golan - 2018 - Info-Metrics for Modeling and Inference.pdf:application/pdf},
}

@article{alon_how_2009,
	title = {How to choose a good scientific problem},
	volume = {35},
	issn = {1097-4164},
	doi = {10.1016/j.molcel.2009.09.013},
	abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.},
	pages = {726--728},
	number = {6},
	journaltitle = {Molecular Cell},
	shortjournal = {Mol. Cell},
	author = {Alon, Uri},
	date = {2009-09-24},
	pmid = {19782018},
	keywords = {Humans, Choice Behavior, Biomedical Research, Career Choice, Education, Graduate, Emotions, Mentors, Peer Review, Research, Periodicals as Topic, Research Design, Time Factors},
	file = {Alon - 2009 - How to choose a good scientific problem.pdf:/Users/bert/Zotero/storage/JLI5Z9HT/Alon - 2009 - How to choose a good scientific problem.pdf:application/pdf},
}

@article{rackauckas_diffeqflux.jl_2019,
	title = {{DiffEqFlux}.jl - A Julia Library for Neural Differential Equations},
	url = {http://arxiv.org/abs/1902.02376},
	abstract = {{DiﬀEqFlux}.jl is a library for fusing neural networks and diﬀerential equations. In this work we describe diﬀerential equations from the viewpoint of data science and discuss the complementary nature between machine learning models and diﬀerential equations. We demonstrate the ability to incorporate {DiﬀerentialEquations}.jl-deﬁned diﬀerential equation problems into a Flux-deﬁned neural network, and vice versa. The advantages of being able to use the entire {DiﬀerentialEquations}.jl suite for this purpose is demonstrated by counter examples where simple integration strategies fail, but the sophisticated integration strategies provided by the {DiﬀerentialEquations}.jl library succeed. This is followed by a demonstration of delay diﬀerential equations and stochastic diﬀerential equations inside of neural networks. We show high-level functionality for deﬁning neural ordinary diﬀerential equations (neural networks embedded into the diﬀerential equation) and describe the extra models in the Flux model zoo which includes neural stochastic diﬀerential equations. We conclude by discussing the various adjoint methods used for backpropogation of the diﬀerential equation solvers. {DiﬀEqFlux}.jl is an important contribution to the area, as it allows the full weight of the diﬀerential equation solvers developed from decades of research in the scientiﬁc computing ﬁeld to be readily applied to the challenges posed by machine learning and data science.},
	journaltitle = {{arXiv}:1902.02376 [cs, stat]},
	author = {Rackauckas, Chris and Innes, Mike and Ma, Yingbo and Bettencourt, Jesse and White, Lyndon and Dixit, Vaibhav},
	urldate = {2019-11-21},
	date = {2019-02-06},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1902.02376},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Rackauckas et al. - 2019 - DiffEqFlux.jl - A Julia Library for Neural Differe.pdf:/Users/bert/Zotero/storage/LS7DDGG6/Rackauckas et al. - 2019 - DiffEqFlux.jl - A Julia Library for Neural Differe.pdf:application/pdf},
}

@article{abedi_entropic_2019-1,
	title = {Entropic Dynamics of Exchange Rates and Options},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/21/6/586},
	doi = {10.3390/e21060586},
	abstract = {An Entropic Dynamics of exchange rates is laid down to model the dynamics of foreign exchange rates, {FX}, and European Options on {FX}. The main objective is to represent an alternative framework to model dynamics. Entropic inference is an inductive inference framework equipped with proper tools to handle situations where incomplete information is available. Entropic Dynamics is an application of entropic inference, which is equipped with the entropic notion of time to model dynamics. The scale invariance is a symmetry of the dynamics of exchange rates, which is manifested in our formalism. To make the formalism manifestly invariant under this symmetry, we arrive at choosing the logarithm of the exchange rate as the proper variable to model. By taking into account the relevant information about the exchange rates, we derive the Geometric Brownian Motion, {GBM}, of the exchange rate, which is manifestly invariant under the scale transformation. Securities should be valued such that there is no arbitrage opportunity. To this end, we derive a risk-neutral measure to value European Options on {FX}. The resulting model is the celebrated Garman\&ndash;Kohlhagen model.},
	pages = {586},
	number = {6},
	journaltitle = {Entropy},
	author = {Abedi, Mohammad and Bartolomeo, Daniel},
	urldate = {2019-11-18},
	date = {2019-06},
	langid = {english},
	keywords = {maximum entropy, Black–Scholes–Merton partial differential equation, entropic dynamics, entropic inference, Fokker–Planck equation, Garman–Kohlhagen model},
	file = {Abedi and Bartolomeo - 2019 - Entropic Dynamics of Exchange Rates and Options.pdf:/Users/bert/Zotero/storage/B3QLS9G3/Abedi and Bartolomeo - 2019 - Entropic Dynamics of Exchange Rates and Options.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/U9WVSCJN/586.html:text/html},
}

@article{knuth_deeper_2015,
	title = {The Deeper Roles of Mathematics in Physical Laws},
	url = {http://arxiv.org/abs/1504.06686},
	abstract = {Many have wondered how mathematics, which appears to be the result of both human creativity and human discovery, can possibly exhibit the degree of success and seemingly-universal applicability to quantifying the physical world as exemplified by the laws of physics. In this essay, I claim that much of the utility of mathematics arises from our choice of description of the physical world coupled with our desire to quantify it. This will be demonstrated in a practical sense by considering one of the most fundamental concepts of mathematics: additivity. This example will be used to show how many physical laws can be derived as constraint equations enforcing relevant symmetries in a sense that is far more fundamental than commonly appreciated.},
	journaltitle = {{arXiv}:1504.06686 [physics]},
	author = {Knuth, Kevin H.},
	urldate = {2019-11-17},
	date = {2015-08-30},
	eprinttype = {arxiv},
	eprint = {1504.06686},
	keywords = {Physics - Popular Physics, Mathematics - History and Overview},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5Q3F5BQZ/1504.html:text/html;Knuth - 2015 - The Deeper Roles of Mathematics in Physical Laws.pdf:/Users/bert/Zotero/storage/BB3VV7DI/Knuth - 2015 - The Deeper Roles of Mathematics in Physical Laws.pdf:application/pdf},
}

@article{caticha_entropic_2015,
	title = {Entropic Dynamics},
	volume = {17},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1509.03222},
	doi = {10.3390/e17096110},
	abstract = {Entropic Dynamics is a framework in which dynamical laws are derived as an application of entropic methods of inference. No underlying action principle is postulated. Instead, the dynamics is driven by entropy subject to the constraints appropriate to the problem at hand. In this paper we review three examples of entropic dynamics. First we tackle the simpler case of a standard diffusion process which allows us to address the central issue of the nature of time. Then we show that imposing the additional constraint that the dynamics be non-dissipative leads to Hamiltonian dynamics. Finally, considerations from information geometry naturally lead to the type of Hamiltonian that describes quantum theory.},
	pages = {6110--6128},
	number = {12},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Caticha, Ariel},
	urldate = {2019-11-12},
	date = {2015-09-01},
	eprinttype = {arxiv},
	eprint = {1509.03222},
	keywords = {Quantum Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/VYDYNX54/1509.html:text/html;Caticha - 2015 - Entropic Dynamics.pdf:/Users/bert/Zotero/storage/N5U7259D/Caticha - 2015 - Entropic Dynamics.pdf:application/pdf},
}

@book{caticha_entropic_2012,
	title = {Entropic Inference and the Foundations of Physics},
	url = {https://www.twirpx.com/file/1706750/},
	abstract = {N.-Y.: International Society for Bayesian Analysis, 2012. - 293p. Science consists in using information about the world for the purpose of predicting, explaining, understanding, and or controlling phenomena of interest. The basic dfficulty is that the available information is usually insufficient...},
	author = {Caticha, Ariel},
	urldate = {2019-11-07},
	date = {2012},
	langid = {russian},
	file = {Caticha - 2012 - Entropic Inference and the Foundations of Physics.pdf:/Users/bert/Zotero/storage/E8PALNV8/Caticha - 2012 - Entropic Inference and the Foundations of Physics.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/8PSVVRLD/1706750.html:text/html},
}

@article{majdak_formation_2019,
	title = {Formation of three-dimensional auditory space},
	url = {http://arxiv.org/abs/1901.03990},
	abstract = {Human listeners need to permanently interact with their three-dimensional (3-D) environment. To this end, they require efficient perceptual mechanisms to form a sufficiently accurate 3-D auditory space. In this chapter, we discuss the formation of the 3-D auditory space from various perspectives. The aim is to show the link between cognition, acoustics, neurophysiology, and psychophysics, when it comes to spatial hearing. First, we present recent cognitive concepts for creating internal models of the complex auditory environment. Second, we describe the acoustic signals available at our ears and discuss the spatial information they convey. Third, we look into neurophysiology, seeking for the neural substrates of the 3-D auditory space. Finally, we elaborate on psychophysical spatial tasks and percepts that are possible just because of the formation of the auditory space.},
	journaltitle = {{arXiv}:1901.03990 [q-bio]},
	author = {Majdak, Piotr and Baumgartner, Robert and Jenny, Claudia},
	urldate = {2019-11-05},
	date = {2019-01-13},
	eprinttype = {arxiv},
	eprint = {1901.03990},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/BCU9T9JK/1901.html:text/html;Majdak et al. - 2019 - Formation of three-dimensional auditory space.pdf:/Users/bert/Zotero/storage/J93TX5YT/Majdak et al. - 2019 - Formation of three-dimensional auditory space.pdf:application/pdf},
}

@article{cosma_entropical_2011,
	title = {{ENTROPICAL} {ASPECTS} {IN} {AUDITORY} {PROCESSES} {AND} {PSYCHOACOUSTICAL} {LAW} {OF} {WEBER}–{FECHNER}},
	volume = {24},
	doi = {10.1142/S0217984910023384},
	abstract = {For hearing sense, the mechanoreceptors fire action potentials when their membranes are physically stretched. Based on the statistical physics, we analyzed the entropical aspects in auditory processes of hearing. We develop a model that connects the logarithm of relative intensity of sound (loudness) to the level of energy disorder within the system of cellular sensory system. The increasing of entropy and disorder in the system is connected to the free energy available to signal the production of action potentials in inner hair cells of the vestibulocochlear auditory organ.},
	journaltitle = {Modern Physics Letters B},
	shortjournal = {Modern Physics Letters B},
	author = {Cosma, Ioan and Popescu, Diana},
	date = {2011-11-21},
	file = {Cosma and Popescu - 2011 - ENTROPICAL ASPECTS IN AUDITORY PROCESSES AND PSYCH.pdf:/Users/bert/Zotero/storage/4M4Q98WU/Cosma and Popescu - 2011 - ENTROPICAL ASPECTS IN AUDITORY PROCESSES AND PSYCH.pdf:application/pdf},
}

@incollection{ting_locally_2010,
	location = {Boston, {MA}},
	title = {Locally Weighted Regression for Control},
	isbn = {978-0-387-30164-8},
	url = {https://doi.org/10.1007/978-0-387-30164-8_488},
	pages = {613--624},
	booktitle = {Encyclopedia of Machine Learning},
	publisher = {Springer {US}},
	author = {Ting, Jo-Anne and Vijayakumar, Sethu and Schaal, Stefan},
	editor = {Sammut, Claude and Webb, Geoffrey I.},
	urldate = {2019-10-25},
	date = {2010},
	langid = {english},
	doi = {10.1007/978-0-387-30164-8_488},
	file = {Ting et al. - 2010 - Locally Weighted Regression for Control.pdf:/Users/bert/Zotero/storage/JZWB54XM/Ting et al. - 2010 - Locally Weighted Regression for Control.pdf:application/pdf},
}

@article{obermeyer_functional_2019,
	title = {Functional Tensors for Probabilistic Programming},
	url = {http://arxiv.org/abs/1910.10775},
	abstract = {It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction --functional tensors-- that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. Moreover, functional tensors are a natural candidate for generalized variable elimination and parallel-scan filtering algorithms that enable parallel exact inference for a large family of tractable modeling motifs. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro programming language. In experiments we show that the resulting framework enables a large variety of inference strategies, including those that mix exact and approximate inference.},
	journaltitle = {{arXiv}:1910.10775 [cs, stat]},
	author = {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Phan, Du and Chen, Jonathan P.},
	urldate = {2019-10-25},
	date = {2019-10-23},
	eprinttype = {arxiv},
	eprint = {1910.10775},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/QG47I58Y/1910.html:text/html;Obermeyer et al. - 2019 - Functional Tensors for Probabilistic Programming.pdf:/Users/bert/Zotero/storage/ITSHJGQZ/Obermeyer et al. - 2019 - Functional Tensors for Probabilistic Programming.pdf:application/pdf},
}

@article{van_de_laar_application_2019,
	title = {Application of the Free Energy Principle to Estimation and Control},
	url = {http://arxiv.org/abs/1910.09823},
	abstract = {The free energy principle ({FEP}) constitutes the basis for one of the prominent theories for perception and learning in biological agents. Based on a generative model ({GM}) and beliefs over hidden states, the free energy principle enables an agent to sense and act by minimizing a free energy bound on Bayesian surprise. Inclusion of prior beliefs in the {GM} about desired states leads to active inference ({ActInf}). In this work, we apply {ActInf} to general state estimation and control. Suitable codings of prior beliefs recover solutions for standard cost-based optimization frameworks. In contrast to standard cost and constraint-based solutions, {ActInf} gives rise to a minimization problem that includes both an information-theoretic surprise term and a model-predictive control type of cost term. {ActInf} thus subsumes classical stochastic control as a special case. We illustrate the performance of {ActInf} under varying system parameters and compare to classical solutions for estimation and control.},
	journaltitle = {{arXiv}:1910.09823 [cs, eess]},
	author = {van de Laar, Thijs and Özçelikkale, Ayça and Wymeersch, Henk},
	urldate = {2019-10-23},
	date = {2019-10-22},
	eprinttype = {arxiv},
	eprint = {1910.09823},
	keywords = {Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/54UABFBS/1910.html:text/html;van de Laar et al. - 2019 - Application of the Free Energy Principle to Estima.pdf:/Users/bert/Zotero/storage/Z9ZDK5CK/van de Laar et al. - 2019 - Application of the Free Energy Principle to Estima.pdf:application/pdf},
}

@article{jeffery_statistical_2019,
	title = {On the statistical mechanics of life: Schr{\textbackslash}"odinger revisited},
	url = {http://arxiv.org/abs/1908.08374},
	shorttitle = {On the statistical mechanics of life},
	abstract = {We study the statistical underpinnings of life. We question some common assumptions about the thermodynamics of life and illustrate how, contrary to widespread belief, even in a closed system entropy growth can accompany an increase in macroscopic order. We consider viewing metabolism in living things as microscopic variables directly driven by the second law of thermodynamics, while viewing the macroscopic variables of structure, complexity and homeostasis as mechanisms that are entropically favored because they open channels for entropy to grow via metabolism. This perspective reverses the conventional relation between structure and metabolism, by emphasizing the role of structure for metabolism rather than the other way around. Structure extends in time, preserving information along generations, particularly in the genetic code, but also in human culture. We also consider why the increase in order/complexity over time is often stepwise and sometimes collapses catastrophically. We point out the relevance of the notions of metastable states and channels between these, which are discovered by random motion of the system and lead it into ever-larger regions of the phase space, driven by thermodynamics. We note that such changes in state can lead to either increase or decrease in order; and sometimes to complete collapse, as in biological extinction. Finally, we comment on the implications of these dynamics for the future of humanity.},
	journaltitle = {{arXiv}:1908.08374 [physics]},
	author = {Jeffery, Kate and Pollack, Robert and Rovelli, Carlo},
	urldate = {2019-10-10},
	date = {2019-08-14},
	eprinttype = {arxiv},
	eprint = {1908.08374},
	keywords = {Physics - Biological Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/F6KS9VS2/1908.html:text/html;Jeffery et al. - 2019 - On the statistical mechanics of life Schrodinge.pdf:/Users/bert/Zotero/storage/VS7GNEIF/Jeffery et al. - 2019 - On the statistical mechanics of life Schrodinge.pdf:application/pdf},
}

@article{tseran_natural_nodate,
	title = {Natural Variational Continual Learning},
	abstract = {The goal of continual learning is to sequentially learn new skills without forgetting old ones. Recent continual-learning approaches have employed gradient-based approximate Bayesian inference methods to derive such algorithms. In this paper, we propose a natural-gradient method that uniﬁes two recent approaches based on Laplace approximation and variational inference, respectively. Our method enables a plug-and-play implementation where the accuracy of the approximation can be traded off for the ease of implementation. Our method also enables a principled application of approximate Bayesian inference for continual learning, and gives competitive performance to the existing approaches.},
	pages = {5},
	author = {Tseran, Hanna and Khan, Mohammad Emtiyaz and Harada, Tatsuya and Bui, Thang D},
	langid = {english},
	file = {Tseran et al. - Natural Variational Continual Learning.pdf:/Users/bert/Zotero/storage/SRJDYLTH/Tseran et al. - Natural Variational Continual Learning.pdf:application/pdf},
}

@article{yellapragada_variational_2019,
	title = {Variational Bayes: A report on approaches and applications},
	url = {http://arxiv.org/abs/1905.10744},
	shorttitle = {Variational Bayes},
	abstract = {Deep neural networks have achieved impressive results on a wide variety of tasks. However, quantifying uncertainty in the network's output is a challenging task. Bayesian models offer a mathematical framework to reason about model uncertainty. Variational methods have been used for approximating intractable integrals that arise in Bayesian inference for neural networks. In this report, we review the major variational inference concepts pertinent to Bayesian neural networks and compare various approximation methods used in literature. We also talk about the applications of variational bayes in Reinforcement learning and continual learning.},
	journaltitle = {{arXiv}:1905.10744 [cs, stat]},
	author = {Yellapragada, Manikanta Srikar and Konkimalla, Chandra Prakash},
	urldate = {2019-10-01},
	date = {2019-05-26},
	eprinttype = {arxiv},
	eprint = {1905.10744},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/HIEVRTHK/1905.html:text/html;Yellapragada and Konkimalla - 2019 - Variational Bayes A report on approaches and appl.pdf:/Users/bert/Zotero/storage/4JA4KX4X/Yellapragada and Konkimalla - 2019 - Variational Bayes A report on approaches and appl.pdf:application/pdf},
}

@inproceedings{mckinnon_learning_2019,
	title = {Learning Probabilistic Models for Safe Predictive Control in Unknown Environments},
	doi = {10.23919/ECC.2019.8796295},
	abstract = {Researchers rely increasingly on tools from machine learning to improve the performance of control algorithms on real world tasks and enable robots to operate for long periods of time without intervention. Many of these algorithms require a model for the dynamics of the robot. In particular, researchers designing methods for safe learning control often rely on an upper bound on model error to make guarantees about the worst-case closed-loop performance of their algorithm. There are different options for how to learn such a model of the robot dynamics. We study probabilistic models for use in the context of stochastic model predictive control. Two popular choices for learning the robot dynamics are Gaussian Process ({GP}) regression and various forms of local linear regression. In this paper, we present a study comparing {GPs} with a particular form of local linear regression for learning robot dynamics with the aim of guaranteeing safety when a robot operates in novel conditions. We show results based on experimental data from a 900 kg ground robot using vision for localisation.},
	eventtitle = {2019 18th European Control Conference ({ECC})},
	pages = {2472--2479},
	booktitle = {2019 18th European Control Conference ({ECC})},
	author = {{McKinnon}, C. D. and Schoellig, A. P.},
	date = {2019-06},
	keywords = {Gaussian processes, learning (artificial intelligence), probabilistic models, machine learning, closed loop systems, control algorithms, control engineering computing, Gaussian process regression, ground robot, local linear regression, mobile robots, predictive control, regression analysis, robot dynamics, robot programming, safe learning control, safe predictive control, stochastic model predictive control, worst-case closed-loop performance},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/WCFYXQIL/8796295.html:text/html;McKinnon and Schoellig - 2019 - Learning Probabilistic Models for Safe Predictive .pdf:/Users/bert/Zotero/storage/5D9QSPJU/McKinnon and Schoellig - 2019 - Learning Probabilistic Models for Safe Predictive .pdf:application/pdf},
}

@article{mirza_introducing_2019,
	title = {Introducing a Bayesian model of selective attention based on active inference},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-50138-8},
	doi = {10.1038/s41598-019-50138-8},
	abstract = {Information gathering comprises actions whose (sensory) consequences resolve uncertainty (i.e., are salient). In other words, actions that solicit salient information cause the greatest shift in beliefs (i.e., information gain) about the causes of our sensations. However, not all information is relevant to the task at hand: this is especially the case in complex, naturalistic scenes. This paper introduces a formal model of selective attention based on active inference and contextual epistemic foraging. We consider a visual search task with a special emphasis on goal-directed and task-relevant exploration. In this scheme, attention modulates the expected fidelity (precision) of the mapping between observations and hidden states in a state-dependent or context-sensitive manner. This ensures task-irrelevant observations have little expected information gain, and so the agent – driven to reduce expected surprise (i.e., uncertainty) – does not actively seek them out. Instead, it selectively samples task-relevant observations, which inform (task-relevant) hidden states. We further show, through simulations, that the atypical exploratory behaviours in conditions such as autism and anxiety may be due to a failure to appropriately modulate sensory precision in a context-specific way.},
	pages = {1--22},
	number = {1},
	journaltitle = {Scientific Reports},
	author = {Mirza, M. Berk and Adams, Rick A. and Friston, Karl and Parr, Thomas},
	urldate = {2019-09-30},
	date = {2019-09-26},
	langid = {english},
	file = {Mirza et al. - 2019 - Introducing a Bayesian model of selective attentio.pdf:/Users/bert/Zotero/storage/TY8XE8RF/Mirza et al. - 2019 - Introducing a Bayesian model of selective attentio.pdf:application/pdf},
}

@article{smith_active_2019,
	title = {An active inference approach to modeling concept learning},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial}-{NoDerivs} 4.0 International), {CC} {BY}-{NC}-{ND} 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/633677v3},
	doi = {10.1101/633677},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Within computational neuroscience, the algorithmic and neural basis of concept learning remains poorly understood. Concept learning requires both a type of internal model expansion process (adding novel hidden states that explain new observations), and a model reduction process (merging different states into one underlying cause and thus reducing model complexity via meta-learning). Although various algorithmic models of concept learning have been proposed within machine learning and cognitive science, many are limited to various degrees by an inability to generalize, the need for very large amounts of training data, and/or insufficiently established biological plausibility. In this paper, we articulate a model of concept learning based on active inference and its accompanying neural process theory, with the idea that a generative model can be equipped with extra (hidden state or cause) ‘slots’ that can be engaged when an agent learns about novel concepts. This can be combined with a Bayesian model reduction process, in which any concept learning – associated with these slots – can be reset in favor of a simpler model with higher model evidence. We use simulations to illustrate this model’s ability to add new concepts to its state space (with relatively few observations) and increase the granularity of the concepts it currently possesses. We further show that it accomplishes a simple form of ‘one-shot’ generalization to new stimuli. Although deliberately simple, these results suggest that active inference may offer useful resources in developing neurocomputational models of concept learning.{\textless}/p{\textgreater}},
	pages = {633677},
	journaltitle = {{bioRxiv}},
	author = {Smith, Ryan and Schwartenbeck, Philipp and Parr, Thomas and Friston, Karl J.},
	urldate = {2019-09-28},
	date = {2019-05-23},
	langid = {english},
	file = {Smith et al. - 2019 - An active inference approach to modeling concept l.pdf:/Users/bert/Zotero/storage/369VX5QW/Smith et al. - 2019 - An active inference approach to modeling concept l.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/JAXBZXTP/633677v3.html:text/html},
}

@book{annila_back_2019,
	title = {Back to Reality},
	url = {https://www.mv.helsinki.fi/home/aannila/arto/pmp50.pdf},
	author = {Annila, Arto},
	date = {2019},
	file = {Annila - 2019 - Back to Reality.pdf:/Users/bert/Zotero/storage/MD8TWNV7/Annila - 2019 - Back to Reality.pdf:application/pdf},
}

@article{tschantz_learning_2019,
	title = {Learning action-oriented models through active inference},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/764969v1},
	doi = {10.1101/764969},
	abstract = {{\textless}p{\textgreater}Converging theories suggest that organisms learn and exploit probabilistic models of their environment. However, it remains unclear how such models can be learned in practice. The open-ended complexity of natural environments means that it is generally infeasible for organisms to model their environment comprehensively. Alternatively, action-oriented models attempt to encode a parsimonious representation of adaptive agent-environment interactions. One approach to learning action-oriented models is to learn online in the presence of goal-directed behaviours. This constrains an agent to behaviourally relevant trajectories, reducing the diversity of the data a model need account for. Unfortunately, this approach can cause models to prematurely converge to sub-optimal solutions, through a process we refer to as a bad-bootstrap. Here, we exploit the normative framework of active inference to show that efficient action-oriented models can be learned by balancing goal-oriented and epistemic (information-seeking) behaviours in a principled manner. We illustrate our approach using a simple agent-based model of bacterial chemotaxis. We first demonstrate that learning via goal-directed behaviour indeed constrains models to behaviorally relevant aspects of the environment, but that this approach is prone to sub-optimal convergence. We then demonstrate that epistemic behaviours facilitate the construction of accurate and comprehensive models, but that these models are not tailored to any specific behavioural niche and are therefore less efficient in their use of data. Finally, we show that active inference agents learn models that are parsimonious, tailored to action, and which avoid bad bootstraps and sub-optimal convergence. Critically, our results indicate that models learned through active inference can support adaptive behaviour in spite of, and indeed because of, their departure from veridical representations of the environment. Our approach provides a principled method for learning adaptive models from limited interactions with an environment, highlighting a route to sample efficient learning algorithms.{\textless}/p{\textgreater}},
	pages = {764969},
	journaltitle = {{bioRxiv}},
	author = {Tschantz, Alexander and Seth, Anil K. and Buckley, Christopher L.},
	urldate = {2019-09-12},
	date = {2019-09-11},
	langid = {english},
	file = {Tschantz et al. - 2019 - Learning action-oriented models through active inf.pdf:/Users/bert/Zotero/storage/GRWRQAVR/Tschantz et al. - 2019 - Learning action-oriented models through active inf.pdf:application/pdf},
}

@article{benhamou_new_2018,
	title = {A new approach to learning in Dynamic Bayesian Networks ({DBNs})},
	url = {http://arxiv.org/abs/1812.09027},
	abstract = {In this paper, we revisit the parameter learning problem, namely the estimation of model parameters for Dynamic Bayesian Networks ({DBNs}). {DBNs} are directed graphical models of stochastic processes that encompasses and generalize Hidden Markov models ({HMMs}) and Linear Dynamical Systems ({LDSs}). Whenever we apply these models to economics and finance, we are forced to make some modeling assumptions about the state dynamics and the graph topology (the {DBN} structure). These assumptions may be incorrectly specified and contain some additional noise compared to reality. Trying to use a best fit approach through maximum likelihood estimation may miss this point and try to fit at any price these models on data. We present here a new methodology that takes a radical point of view and instead focus on the final efficiency of our model. Parameters are hence estimated in terms of their efficiency rather than their distributional fit to the data. The resulting optimization problem that consists in finding the optimal parameters is a hard problem. We rely on Covariance Matrix Adaptation Evolution Strategy ({CMA}-{ES}) method to tackle this issue. We apply this method to the seminal problem of trend detection in financial markets. We see on numerical results that the resulting parameters seem less error prone to over fitting than traditional moving average cross over trend detection and perform better. The method developed here for algorithmic trading is general. It can be applied to other real case applications whenever there is no physical law underlying our {DBNs}.},
	journaltitle = {{arXiv}:1812.09027 [cs, stat]},
	author = {Benhamou, E. and Atif, J. and Laraki, R.},
	urldate = {2019-09-06},
	date = {2018-12-21},
	eprinttype = {arxiv},
	eprint = {1812.09027},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/YX2DFI9P/1812.html:text/html;Benhamou et al. - 2018 - A new approach to learning in Dynamic Bayesian Net.pdf:/Users/bert/Zotero/storage/SM479NJE/Benhamou et al. - 2018 - A new approach to learning in Dynamic Bayesian Net.pdf:application/pdf},
}

@report{benhamou_kalman_2018,
	location = {Rochester, {NY}},
	title = {Kalman Filter Demystified: From Intuition to Probabilistic Graphical Model to Real Case in Financial Markets},
	url = {https://papers.ssrn.com/abstract=3292762},
	shorttitle = {Kalman Filter Demystified},
	abstract = {In this paper, we revisit the Kalman filter theory. After giving the intuition on a simplified financial markets example, we revisit the maths underlying it. We then show that Kalman filter can be presented in a very different fashion using graphical models. This enables us to establish the connection between Kalman filter and Hidden Markov Models. We then look at their application in financial markets and provide various intuitions in terms of their applicability for complex systems such as financial markets. Although this paper has been written more like a self contained work connecting Kalman filter to Hidden Markov Models and hence revisiting well known and establish results, it contains new results and brings additional contributions to the field. First, leveraging on the link between Kalman filter and {HMM}, it gives new algorithms for inference for extended Kalman filters. Second, it presents an alternative to the traditional estimation of parameters using {EM} algorithm thanks to the usage of {CMA}-{ES} optimization. Third, it examines the application of Kalman filter and its Hidden Markov models version to financial markets, providing various dynamics assumptions and tests. We conclude by connecting Kalman filter approach to trend following technical analysis system and showing their superior performances for trend following detection.},
	number = {{ID} 3292762},
	institution = {Social Science Research Network},
	type = {{SSRN} Scholarly Paper},
	author = {Benhamou, Eric},
	urldate = {2019-09-06},
	date = {2018-11-29},
	langid = {english},
	keywords = {graphical model, {CMA} {ES}, hidden markov models, kalman  filter, systematic trading, trend detection},
	file = {Benhamou - 2018 - Kalman filter demystified from intuition to proba.pdf:/Users/bert/Zotero/storage/VE5YP6RQ/Benhamou - 2018 - Kalman filter demystified from intuition to proba.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/X5GGXRMY/papers.html:text/html},
}

@collection{colombo_andy_2019,
	location = {New York, {NY}},
	title = {Andy Clark and His Critics},
	isbn = {978-0-19-066281-3},
	abstract = {Andy Clark is a leading philosopher of cognitive science, whose work has had an extraordinary impact throughout philosophy, psychology, neuroscience, and robotics. His monographs have led the way for new research programs in the philosophy of mind and cognition: Microcognition (1989) and Associative Engines (1993) introduced the philosophical community to connectionist research and the novel issues it raised; Being There (1997) showed the relevance of embodiment, dynamical systems theory, and minimal computation frameworks for the study of the mind; Natural Born Cyborgs ({OUP} 2003) presented an accessible development of embodied and embedded approaches to understanding human nature and cognition; Supersizing the Mind ({OUP} 2008) developed this yet further along with the famous "Extended Mind" hypothesis; and Surfing Uncertainty ({OUP} 2017) presents a framework for uniting perception, action, and the embodied mind.In Andy Clark and His Critics, a range of high-profile researchers in philosophy of mind, philosophy of cognitive science, and empirical cognitive science, critically engage with Clark's work across the themes of: Extended, Embodied, Embedded, Enactive, and Affective Minds; Natural Born Cyborgs; and Perception, Action, and Prediction. Daniel Dennett provides a foreword on the significance of Clark's work, and Clark replies to each section of the book, thus advancing current literature with original contributions that will form the basis for new discussions, debates and directions in the discipline.},
	pagetotal = {328},
	publisher = {Oxford University Press},
	editor = {Colombo, Matteo and Irvine, Elizabeth and Stapleton, Mog},
	date = {2019-05-31},
	file = {Colombo et al. - 2019 - Andy Clark and His Critics.pdf:/Users/bert/Zotero/storage/PDKIW6JL/Colombo et al. - 2019 - Andy Clark and His Critics.pdf:application/pdf},
}

@article{nguyen_variational_2018,
	title = {Variational Continual Learning},
	url = {https://openreview.net/forum?id=BkQqq0gRb},
	abstract = {This paper develops variational continual learning ({VCL}), a simple but general framework for continual learning that fuses online variational inference ({VI}) and recent advances in Monte Carlo {VI}...},
	author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
	urldate = {2019-09-04},
	date = {2018-02-15},
	file = {Nguyen et al. - 2018 - Variational Continual Learning.pdf:/Users/bert/Zotero/storage/GTDTHJ25/Nguyen et al. - 2018 - Variational Continual Learning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XWTDUSMS/forum.html:text/html},
}

@article{swaroop_improving_2019,
	title = {Improving and Understanding Variational Continual Learning},
	url = {http://arxiv.org/abs/1905.02099},
	abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer learning. In this paper, we explore how the Variational Continual Learning ({VCL}) framework achieves these desiderata on two benchmarks in continual learning: split {MNIST} and permuted {MNIST}. We first report significantly improved results on what was already a competitive approach. The improvements are achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why {VCL} performs as it does, and we compare the solution to what an `ideal' continual learning solution might be.},
	journaltitle = {{arXiv}:1905.02099 [cs, stat]},
	author = {Swaroop, Siddharth and Nguyen, Cuong V. and Bui, Thang D. and Turner, Richard E.},
	urldate = {2019-09-04},
	date = {2019-05-06},
	eprinttype = {arxiv},
	eprint = {1905.02099},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/8AWIDKVB/1905.html:text/html;Swaroop et al. - 2019 - Improving and Understanding Variational Continual .pdf:/Users/bert/Zotero/storage/Z4ZAXH2N/Swaroop et al. - 2019 - Improving and Understanding Variational Continual .pdf:application/pdf},
}

@article{gong_icebreaker:_2019,
	title = {Icebreaker: Element-wise Active Information Acquisition with Bayesian Deep Latent Gaussian Model},
	url = {http://arxiv.org/abs/1908.04537},
	shorttitle = {Icebreaker},
	abstract = {In this paper we introduce the ice-start problem, i.e., the challenge of deploying machine learning models when only little or no training data is initially available, and acquiring each feature element of data is associated with costs. This setting is representative for the real-world machine learning applications. For instance, in the health-care domain, when training an {AI} system for predicting patient metrics from lab tests, obtaining every single measurement comes with a high cost. Active learning, where only the label is associated with a cost does not apply to such problem, because performing all possible lab tests to acquire a new training datum would be costly, as well as unnecessary due to redundancy. We propose Icebreaker, a principled framework to approach the ice-start problem. Icebreaker uses a full Bayesian Deep Latent Gaussian Model ({BELGAM}) with a novel inference method. Our proposed method combines recent advances in amortized inference and stochastic gradient {MCMC} to enable fast and accurate posterior inference. By utilizing {BELGAM}'s ability to fully quantify model uncertainty, we also propose two information acquisition functions for imputation and active prediction problems. We demonstrate that {BELGAM} performs significantly better than the previous {VAE} (Variational autoencoder) based models, when the data set size is small, using both machine learning benchmarks and real-world recommender systems and health-care applications. Moreover, based on {BELGAM}, Icebreaker further improves the performance and demonstrate the ability to use minimum amount of the training data to obtain the highest test time performance.},
	journaltitle = {{arXiv}:1908.04537 [cs, stat]},
	author = {Gong, Wenbo and Tschiatschek, Sebastian and Turner, Richard and Nowozin, Sebastian and Hernández-Lobato, José Miguel and Zhang, Cheng},
	urldate = {2019-09-04},
	date = {2019-08-13},
	eprinttype = {arxiv},
	eprint = {1908.04537},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NDSWU5Z9/1908.html:text/html;Gong et al. - 2019 - Icebreaker Element-wise Active Information Acquis.pdf:/Users/bert/Zotero/storage/MYWK3DEN/Gong et al. - 2019 - Icebreaker Element-wise Active Information Acquis.pdf:application/pdf},
}

@article{katzfuss_ensemble_2019,
	title = {Ensemble Kalman Methods for High-Dimensional Hierarchical Dynamic Space-Time Models},
	volume = {0},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2019.1592753},
	doi = {10.1080/01621459.2019.1592753},
	abstract = {We propose a new class of filtering and smoothing methods for inference in high-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models. The main idea is to combine the ensemble Kalman filter and smoother, developed in the geophysics literature, with state-space algorithms from the statistics literature. Our algorithms address a variety of estimation scenarios, including online and off-line state and parameter estimation. We take a Bayesian perspective, for which the goal is to generate samples from the joint posterior distribution of states and parameters. The key benefit of our approach is the use of ensemble Kalman methods for dimension reduction, which allows inference for high-dimensional state vectors. We compare our methods to existing ones, including ensemble Kalman filters, particle filters, and particle {MCMC}. Using a real data example of cloud motion and data simulated under a number of nonlinear and non-Gaussian scenarios, we show that our approaches outperform these existing methods. Supplementary materials for this article are available online.},
	pages = {1--43},
	number = {0},
	journaltitle = {Journal of the American Statistical Association},
	author = {Katzfuss, Matthias and Stroud, Jonathan R. and Wikle, Christopher K.},
	urldate = {2019-09-04},
	date = {2019-03-20},
	keywords = {Data assimilation, Geoscience applications, Gibbs sampler, Particle filter, Spatio-temporal statistics, State-space models},
	file = {Katzfuss et al. - 2019 - Ensemble Kalman Methods for High-Dimensional Hiera.pdf:/Users/bert/Zotero/storage/8FV4IGT2/Katzfuss et al. - 2019 - Ensemble Kalman Methods for High-Dimensional Hiera.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/K4MQLVUT/01621459.2019.html:text/html},
}

@article{peters_uncertainty_2017,
	title = {Uncertainty and stress: Why it causes diseases and how it is mastered by the brain},
	volume = {156},
	issn = {0301-0082},
	url = {http://www.sciencedirect.com/science/article/pii/S0301008217300369},
	doi = {10.1016/j.pneurobio.2017.05.004},
	shorttitle = {Uncertainty and stress},
	abstract = {The term ‘stress’ – coined in 1936 – has many definitions, but until now has lacked a theoretical foundation. Here we present an information-theoretic approach – based on the ‘free energy principle’ – defining the essence of stress; namely, uncertainty. We address three questions: What is uncertainty? What does it do to us? What are our resources to master it? Mathematically speaking, uncertainty is entropy or ‘expected surprise’. The ‘free energy principle’ rests upon the fact that self-organizing biological agents resist a tendency to disorder and must therefore minimize the entropy of their sensory states. Applied to our everyday life, this means that we feel uncertain, when we anticipate that outcomes will turn out to be something other than expected – and that we are unable to avoid surprise. As all cognitive systems strive to reduce their uncertainty about future outcomes, they face a critical constraint: Reducing uncertainty requires cerebral energy. The characteristic of the vertebrate brain to prioritize its own high energy is captured by the notion of the ‘selfish brain’. Accordingly, in times of uncertainty, the selfish brain demands extra energy from the body. If, despite all this, the brain cannot reduce uncertainty, a persistent cerebral energy crisis may develop, burdening the individual by ‘allostatic load’ that contributes to systemic and brain malfunction (impaired memory, atherogenesis, diabetes and subsequent cardio- and cerebrovascular events). Based on the basic tenet that stress originates from uncertainty, we discuss the strategies our brain uses to avoid surprise and thereby resolve uncertainty.},
	pages = {164--188},
	journaltitle = {Progress in Neurobiology},
	shortjournal = {Progress in Neurobiology},
	author = {Peters, Achim and {McEwen}, Bruce S. and Friston, Karl},
	urldate = {2017-08-17},
	date = {2017-09-01},
	keywords = {Animals, Brain, Humans, Attention, Uncertainty, Variational free energy, Learning, Allostatic load, Atherosclerosis, Bayesian Brain, Brain energy metabolism, Diabetes Mellitus, Memory Disorders, mortality, Selfish Brain, Stress definition, Stress habituation, Stress, Psychological},
	file = {Peters e.a. - 2017 - Uncertainty and stress Why it causes diseases and.pdf:/Users/bert/Zotero/storage/VUG94XFF/Peters e.a. - 2017 - Uncertainty and stress Why it causes diseases and.pdf:application/pdf;Peters et al. - 2017 - Uncertainty and stress Why it causes diseases and.pdf:/Users/bert/Zotero/storage/7RQKHJB2/Peters et al. - 2017 - Uncertainty and stress Why it causes diseases and.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/C8CUEUXJ/S0301008217300369.html:text/html},
}

@inproceedings{pereyra_comparing_2016,
	title = {Comparing Bayesian models in the absence of ground truth},
	author = {Pereyra, Marcelo and {McLaughlin}, Steve},
	date = {2016},
	keywords = {model selection, Bayesian inference, Estimation, Bayes methods, Markov processes, Monte Carlo methods, Signal processing, Computational modeling, statistical analysis, Markov chain Monte Carlo, Europe, Mathematical model, signal processing, additive mixture meta-model representation, Bayesian statistical models, computational imaging, model selection approach, proximal Markov chain Monte Carlo algorithm, signal processing method, Statistical signal processing},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/5WQAVPBJ/7760304.html:text/html;Pereyra and McLaughlin - 2016 - Comparing Bayesian models in the absence of ground.pdf:/Users/bert/Zotero/storage/VSXDRWGB/Pereyra and McLaughlin - 2016 - Comparing Bayesian models in the absence of ground.pdf:application/pdf;Pereyra and McLaughlin - 2016 - Comparing Bayesian models in the absence of ground.pdf:/Users/bert/Zotero/storage/NBYXV43K/Pereyra and McLaughlin - 2016 - Comparing Bayesian models in the absence of ground.pdf:application/pdf},
}

@article{parr_anatomy_2018,
	title = {The Anatomy of Inference: Generative Models and Brain Structure},
	volume = {12},
	issn = {1662-5188},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6243103/},
	doi = {10.3389/fncom.2018.00090},
	shorttitle = {The Anatomy of Inference},
	abstract = {To infer the causes of its sensations, the brain must call on a generative (predictive) model. This necessitates passing local messages between populations of neurons to update beliefs about hidden variables in the world beyond its sensory samples. It also entails inferences about how we will act. Active inference is a principled framework that frames perception and action as approximate Bayesian inference. This has been successful in accounting for a wide range of physiological and behavioral phenomena. Recently, a process theory has emerged that attempts to relate inferences to their neurobiological substrates. In this paper, we review and develop the anatomical aspects of this process theory. We argue that the form of the generative models required for inference constrains the way in which brain regions connect to one another. Specifically, neuronal populations representing beliefs about a variable must receive input from populations representing the Markov blanket of that variable. We illustrate this idea in four different domains: perception, planning, attention, and movement. In doing so, we attempt to show how appealing to generative models enables us to account for anatomical brain architectures. Ultimately, committing to an anatomical theory of inference ensures we can form empirical hypotheses that can be tested using neuroimaging, neuropsychological, and electrophysiological experiments.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front Comput Neurosci},
	author = {Parr, Thomas and Friston, Karl J.},
	urldate = {2018-12-01},
	date = {2018-11-13},
	pmid = {null},
	pmcid = {PMC6243103},
	keywords = {predictive processing, Bayesian, message passing, Active inference., Generative Model, Neuroanatomy},
	file = {Parr and Friston - 2018 - The Anatomy of Inference Generative Models and Br.pdf:/Users/bert/Zotero/storage/ZDXPTLUE/Parr and Friston - 2018 - The Anatomy of Inference Generative Models and Br.pdf:application/pdf},
}

@article{levine_reinforcement_2018,
	title = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
	url = {http://arxiv.org/abs/1805.00909},
	shorttitle = {Reinforcement Learning and Control as Probabilistic Inference},
	abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
	journaltitle = {{arXiv}:1805.00909 [cs, stat]},
	author = {Levine, Sergey},
	urldate = {2018-05-03},
	date = {2018-05-02},
	eprinttype = {arxiv},
	eprint = {1805.00909},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Robotics, Computer Science - Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NSIBHI99/1805.html:text/html;arXiv.org Snapshot:/Users/bert/Zotero/storage/V5C4N24Q/1805.html:text/html;Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf:/Users/bert/Zotero/storage/GE9RCP9W/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf:application/pdf},
}

@article{van_de_laar_probabilistic_2016,
	title = {A Probabilistic Modeling Approach to Hearing Loss Compensation},
	volume = {24},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2016.2599275},
	abstract = {Hearing Aid ({HA}) algorithms need to be tuned (“fitted”) to match the impairment of each specific patient. The lack of a fundamental {HA} fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20\% of {HA} patients. This paper proposes a probabilistic modeling approach to the design of {HA} algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding (1) signal processing algorithm, (2) the fitting solution as well as (3) a principled performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on {HA} or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model.},
	pages = {2200--2213},
	number = {11},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Van de Laar, Thijs and De Vries, Bert},
	date = {2016-11},
	keywords = {Signal processing algorithms, message passing, Signal processing, Auditory system, Hearing aids, Probabilistic logic, factor graphs, statistical analysis, inference mechanisms, Gain, hearing aids, medical signal processing, machine learning, hearing, ear, automated inference, factor graph representation, fundamental {HA} fitting theory, hearing aid algorithms, hearing loss compensation, message passing algorithms, physiological models, principled performance evaluation metric, probabilistic modeling, probabilistic modeling approach, signal processing algorithm, Tuning},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/YEBQEZ9P/7539610.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/9BWTARZL/7539610.html:text/html;Laar and Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:/Users/bert/Zotero/storage/VDKFXFET/Laar and Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:application/pdf;Laar and Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:/Users/bert/Zotero/storage/CCLH2RHJ/Laar and Vries - 2016 - A Probabilistic Modeling Approach to Hearing Loss .pdf:application/pdf},
}

@article{karl_deep_2016,
	title = {Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data},
	url = {http://arxiv.org/abs/1605.06432},
	shorttitle = {Deep Variational Bayes Filters},
	abstract = {We introduce Deep Variational Bayes Filters ({DVBF}), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, {DVBF} can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.},
	journaltitle = {{arXiv}:1605.06432 [cs, stat]},
	author = {Karl, Maximilian and Soelch, Maximilian and Bayer, Justin and van der Smagt, Patrick},
	urldate = {2017-08-30},
	date = {2016-05-20},
	eprinttype = {arxiv},
	eprint = {1605.06432},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Learning, Computer Science - Systems and Control},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/HGKWXMYX/1605.html:text/html;Karl et al. - 2016 - Deep Variational Bayes Filters Unsupervised Learn.pdf:/Users/bert/Zotero/storage/673HT7IZ/Karl et al. - 2016 - Deep Variational Bayes Filters Unsupervised Learn.pdf:application/pdf},
}

@inproceedings{ganesh_thermodynamic_2017,
	title = {A Thermodynamic Treatment of Intelligent Systems},
	doi = {10.1109/ICRC.2017.8123676},
	abstract = {The focus of the computing industry continues to shift towards designing and building intelligent systems that can handle and learn from large amounts of data. The availability of powerful processing hardware like {GPUs} and {TPUs}, has powered the tremendous success of many sophisticated resource intensive machine learning algorithms. However as device scaling and energy dissipation fast approach the physical limits, we have started to look away from traditional {CMOS} devices and the von Neumann architecture, and towards neuromorphic and other bio-inspired computing systems built using novel devices such as memristors and photonics. Biological systems like the human brain are a product of millions of years of bottom-up self- organizing evolutionary processes and often operate near the limits of energy efficiency. Improved understanding of these systems, their capabilities and the self- organization processes that produce them from a computational perspective will go a long way in helping us build better learning machines. The use of non- equilibrium thermodynamics and information theory to understand such biological processes have gained significant momentum in recent years. In this paper, based on a prediction focused definition of intelligence, results characterizing the thermodynamic constraints on finite state automata models of intelligent physical systems will be presented. As we seek to reboot computing, these constraints can prove be to extremely beneficial in guiding the design and fabrication of energy efficient intelligent systems.},
	eventtitle = {2017 {IEEE} International Conference on Rebooting Computing ({ICRC})},
	pages = {1--4},
	booktitle = {2017 {IEEE} International Conference on Rebooting Computing ({ICRC})},
	author = {Ganesh, N.},
	date = {2017-11},
	keywords = {learning (artificial intelligence), Entropy, Trajectory, Thermodynamics, biocomputing, biological processes, biological systems, Biological systems, computational perspective, computing industry, device scaling, energy dissipation fast approach the physical limits, energy efficiency, energy efficient intelligent systems, evolutionary computation, evolutionary processes, finite state automata, finite state machines, improved understanding, intelligent physical systems, Intelligent systems, learning machines, neuromorphic bio-inspired computing systems, non equilibrium thermodynamics, organization processes, Self-assembly, sophisticated resource intensive machine learning algorithms, thermodynamic constraints, thermodynamic treatment},
	file = {Ganesh - 2017 - A Thermodynamic Treatment of Intelligent Systems.pdf:/Users/bert/Zotero/storage/2QPHYA3Y/Ganesh - 2017 - A Thermodynamic Treatment of Intelligent Systems.pdf:application/pdf;Ganesh - 2017 - A Thermodynamic Treatment of Intelligent Systems.pdf:/Users/bert/Zotero/storage/V9CZ3D2I/Ganesh - 2017 - A Thermodynamic Treatment of Intelligent Systems.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/IHWNNTJR/8123676.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/QVUJRT4B/8123676.html:text/html},
}

@article{friston_life_2013,
	title = {Life as we know it},
	volume = {10},
	rights = {. © 2013 The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/3.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {1742-5689, 1742-5662},
	url = {http://rsif.royalsocietypublishing.org/content/10/86/20130475},
	doi = {10.1098/rsif.2013.0475},
	abstract = {This paper presents a heuristic proof (and simulations of a primordial soup) suggesting that life—or biological self-organization—is an inevitable and emergent property of any (ergodic) random dynamical system that possesses a Markov blanket. This conclusion is based on the following arguments: if the coupling among an ensemble of dynamical systems is mediated by short-range forces, then the states of remote systems must be conditionally independent. These independencies induce a Markov blanket that separates internal and external states in a statistical sense. The existence of a Markov blanket means that internal states will appear to minimize a free energy functional of the states of their Markov blanket. Crucially, this is the same quantity that is optimized in Bayesian inference. Therefore, the internal states (and their blanket) will appear to engage in active Bayesian inference. In other words, they will appear to model—and act on—their world to preserve their functional and structural integrity, leading to homoeostasis and a simple form of autopoiesis.},
	pages = {20130475},
	number = {86},
	journaltitle = {Journal of The Royal Society Interface},
	author = {Friston, Karl},
	urldate = {2018-04-14},
	date = {2013-09-06},
	langid = {english},
	pmid = {23825119},
	file = {Friston - 2013 - Life as we know it.pdf:/Users/bert/Zotero/storage/CE6C5TYG/Friston - 2013 - Life as we know it.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3AHQ8VXU/20130475.html:text/html;Snapshot:/Users/bert/Zotero/storage/5SF6ZYEP/rsif.2013.html:text/html},
}

@article{desormeau_ramstead_answering_nodate,
	title = {Answering Schrödinger's question: A free-energy formulation},
	issn = {1571-0645},
	url = {http://www.sciencedirect.com/science/article/pii/S1571064517301409},
	doi = {10.1016/j.plrev.2017.09.001},
	shorttitle = {Answering Schrödinger's question},
	abstract = {The free-energy principle ({FEP}) is a formal model of neuronal processes that is widely recognised in neuroscience as a unifying theory of the brain and biobehaviour. More recently, however, it has been extended beyond the brain to explain the dynamics of living systems, and their unique capacity to avoid decay. The aim of this review is to synthesise these advances with a meta-theoretical ontology of biological systems called variational neuroethology, which integrates the {FEP} with Tinbergen's four research questions to explain biological systems across spatial and temporal scales. We exemplify this framework by applying it to Homo sapiens, before translating variational neuroethology into a systematic research heuristic that supplies the biological, cognitive, and social sciences with a computationally tractable guide to discovery.},
	journaltitle = {Physics of Life Reviews},
	shortjournal = {Physics of Life Reviews},
	author = {Désormeau Ramstead, Maxwell James and Badcock, Paul Benjamin and Friston, Karl John},
	urldate = {2017-09-29},
	keywords = {Free energy principle, Complex adaptive systems, Evolutionary systems theory, Hierarchically mechanistic mind, Physics of the mind, Variational neuroethology},
	file = {Désormeau Ramstead et al. - Answering Schrödinger's question A free-energy fo.pdf:/Users/bert/Zotero/storage/D74ZFKBV/Désormeau Ramstead et al. - Answering Schrödinger's question A free-energy fo.pdf:application/pdf;Ramstead et al. - 2018 - Answering Schrödinger's question A free-energy fo.pdf:/Users/bert/Zotero/storage/FN8HGNCP/Ramstead et al. - 2018 - Answering Schrödinger's question A free-energy fo.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/8TBZRYMI/S1571064517301409.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/IHC6GWEG/S1571064517301409.html:text/html},
}

@article{de_vries_factor_2017,
	title = {A Factor Graph Description of Deep Temporal Active Inference},
	volume = {11},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00095/full},
	doi = {10.3389/fncom.2017.00095},
	abstract = {Active inference is a corollary of the Free Energy Principle that prescribes how self-organizing biological agents interact with their environment. The study of active inference processes relies on the definition of a generative probabilistic model and a description of how a free energy functional is minimized by neuronal message passing under that model. This paper presents a tutorial introduction to specifying active inference processes by Forney-style factor graphs ({FFG}). The {FFG} framework provides both an insightful representation of the probabilistic model and a biologically plausible inference scheme that, in principle, can be automatically executed in a computer simulation. As an illustrative example, we present an {FFG} for a deep temporal active inference process. The graph clearly shows how policy selection by expected free energy minimization results from free energy minimization per se, in an appropriate generative policy model.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front. Comput. Neurosci.},
	author = {de Vries, Bert and Friston, Karl J.},
	urldate = {2018-01-12},
	date = {2017},
	keywords = {active inference, Active inference, Message passing, message passing, Belief propagation, free-energy principle, Factor Graphs, multi-scale dynamical systems},
	file = {de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:/Users/bert/Zotero/storage/YJWVRQKI/de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:application/pdf;de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:/Users/bert/Zotero/storage/956FATPM/de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:application/pdf},
}

@article{cox_factor_2018,
	title = {A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms},
	journaltitle = {Under review},
	author = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
	date = {2018},
	keywords = {Bayesian inference, Message passing, Factor graphs, Julia, Probabilistic programming},
	file = {Cox et al. - 2019 - A factor graph approach to automated design of Bay.pdf:/Users/bert/Zotero/storage/TFWY8XA2/Cox et al. - 2019 - A factor graph approach to automated design of Bay.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/5RIUGVUP/S0888613X18304298.html:text/html;ScienceDirect Snapshot:/Users/bert/Zotero/storage/NGNAYKC6/S0888613X18304298.html:text/html},
}

@article{chen_neural_2018,
	title = {Neural Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any {ODE} solver, without access to its internal operations. This allows end-to-end training of {ODEs} within larger models.},
	journaltitle = {{arXiv}:1806.07366 [cs, stat]},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	urldate = {2018-12-14},
	date = {2018-06-19},
	eprinttype = {arxiv},
	eprint = {1806.07366},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LTGCHMJW/1806.html:text/html;Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:/Users/bert/Zotero/storage/Y242DDN4/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf:application/pdf},
}

@article{tomov_discovery_2019,
	title = {Discovery of Hierarchical Representations for Efficient Planning},
	rights = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/499418v2},
	doi = {10.1101/499418},
	abstract = {{\textless}h3{\textgreater}Summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}We propose that humans spontaneously organize environments into clusters of states that support hierarchical planning, enabling them to tackle challenging problems by breaking them down into sub-problems at various levels of abstraction. People constantly rely on such hierarchical presentations to accomplish tasks big and small – from planning one’s day, to organizing a wedding, to getting a {PhD} – often succeeding on the very first attempt. We formalize a Bayesian model of hierarchy discovery that explains how humans discover such useful abstractions. Building on principles developed in structure learning and robotics, the model predicts that hierarchy discovery should be sensitive to the topological structure, reward distribution, and distribution of tasks in the environment. In five simulations, we show that the model accounts for previously reported effects of environment structure on planning behavior, such as detection of bottleneck states and transitions. We then test the novel predictions of the model in eight behavioral experiments, demonstrating how the distribution of tasks and rewards can influence planning behavior via the discovered hierarchy, sometimes facilitating and sometimes hindering performance. We find evidence that the hierarchy discovery process unfolds incrementally across trials. We also find that people use uncertainty to guide their learning in a way that is informative for hierarchy discovery. Finally, we propose how hierarchy discovery and hierarchical planning might be implemented in the brain. Together, these findings present an important advance in our understanding of how the brain might use Bayesian inference to discover and exploit the hidden hierarchical structure of the environment.{\textless}/p{\textgreater}},
	pages = {499418},
	journaltitle = {{bioRxiv}},
	author = {Tomov, Momchil S. and Yagati, Samyukta and Kumar, Agni and Yang, Wanqian and Gershman, Samuel J.},
	urldate = {2019-08-13},
	date = {2019-03-28},
	langid = {english},
	file = {Full Text PDF:/Users/bert/Zotero/storage/BD6RBSAS/Tomov et al. - 2019 - Discovery of Hierarchical Representations for Effi.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/U6UPYQQN/499418v2.html:text/html},
}

@article{gershman_what_2019,
	title = {What does the free energy principle tell us about the brain?},
	url = {http://arxiv.org/abs/1901.07945},
	abstract = {The free energy principle has been proposed as a unifying theory of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
	journaltitle = {{arXiv}:1901.07945 [q-bio]},
	author = {Gershman, Samuel J.},
	urldate = {2019-08-13},
	date = {2019-01-23},
	eprinttype = {arxiv},
	eprint = {1901.07945},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/HKCZ8ZXH/1901.html:text/html;Gershman - 2019 - What does the free energy principle tell us about .pdf:/Users/bert/Zotero/storage/GDQ5V9B3/Gershman - 2019 - What does the free energy principle tell us about .pdf:application/pdf},
}

@article{kuchling_morphogenesis_2019,
	title = {Morphogenesis as Bayesian inference: A variational approach to pattern formation and control in complex biological systems},
	issn = {1873-1457},
	doi = {10.1016/j.plrev.2019.06.001},
	shorttitle = {Morphogenesis as Bayesian inference},
	abstract = {Recent advances in molecular biology such as gene editing [1], bioelectric recording and manipulation [2] and live cell microscopy using fluorescent reporters [3], [4] - especially with the advent of light-controlled protein activation through optogenetics [5] - have provided the tools to measure and manipulate molecular signaling pathways with unprecedented spatiotemporal precision. This has produced ever increasing detail about the molecular mechanisms underlying development and regeneration in biological organisms. However, an overarching concept - that can predict the emergence of form and the robust maintenance of complex anatomy - is largely missing in the field. Classic (i.e., dynamic systems and analytical mechanics) approaches such as least action principles are difficult to use when characterizing open, far-from equilibrium systems that predominate in Biology. Similar issues arise in neuroscience when trying to understand neuronal dynamics from first principles. In this (neurobiology) setting, a variational free energy principle has emerged based upon a formulation of self-organization in terms of (active) Bayesian inference. The free energy principle has recently been applied to biological self-organization beyond the neurosciences [6], [7]. For biological processes that underwrite development or regeneration, the Bayesian inference framework treats cells as information processing agents, where the driving force behind morphogenesis is the maximization of a cell's model evidence. This is realized by the appropriate expression of receptors and other signals that correspond to the cell's internal (i.e., generative) model of what type of receptors and other signals it should express. The emerging field of the free energy principle in pattern formation provides an essential quantitative formalism for understanding cellular decision-making in the context of embryogenesis, regeneration, and cancer suppression. In this paper, we derive the mathematics behind Bayesian inference - as understood in this framework - and use simulations to show that the formalism can reproduce experimental, top-down manipulations of complex morphogenesis. First, we illustrate this 'first principle' approach to morphogenesis through simulated alterations of anterior-posterior axial polarity (i.e., the induction of two heads or two tails) as in planarian regeneration. Then, we consider aberrant signaling and functional behavior of a single cell within a cellular ensemble - as a first step in carcinogenesis as false 'beliefs' about what a cell should 'sense' and 'do'. We further show that simple modifications of the inference process can cause - and rescue - mis-patterning of developmental and regenerative events without changing the implicit generative model of a cell as specified, for example, by its {DNA}. This formalism offers a new road map for understanding developmental change in evolution and for designing new interventions in regenerative medicine settings.},
	journaltitle = {Physics of Life Reviews},
	shortjournal = {Phys Life Rev},
	author = {Kuchling, Franz and Friston, Karl and Georgiev, Georgi and Levin, Michael},
	date = {2019-06-12},
	pmid = {31320316},
	keywords = {Bayesian inference, Free energy principle, Developmental biology, Morphogenesis, Regeneration, Top-down modeling},
	file = {Kuchling et al. - 2019 - Morphogenesis as Bayesian inference A variational.pdf:/Users/bert/Zotero/storage/3LRCW552/Kuchling et al. - 2019 - Morphogenesis as Bayesian inference A variational.pdf:application/pdf},
}

@thesis{baltieri_active_nodate,
	title = {Active inference: building a new bridge between control theory and embodied cognitive science},
	abstract = {The application of Bayesian techniques to the study and computational modelling of biological systems is one of the most remarkable advances in the natural and cognitive sciences over the last 50 years. More recently, it has been proposed that Bayesian frameworks are not only useful for building descriptive models of biological functions, but that living systems themselves can be seen as Bayesian (inference) machines. On this view, the statistical tools more traditionally used to account for data in biology, neuroscience and psychology, are now used to model the mechanisms underlying functions and properties of living systems as if the systems themselves were the ones “calculating” those probabilities following Bayesian inference schemes. The free energy principle ({FEP}) is a framework proposed in light of this paradigm shift, advocating the minimisation of variational free energy, a proxy for sensory surprisal, as a general computational principle for biological systems. More intuitively and under some simplifying assumptions, the minimisation of variational free energy reduces, for an agent, to the minimisation of prediction errors on sensory input. Initially proposed as a candidate unifying theory of brain functioning, the {FEP} was later extended to encompass hypotheses on the origins of life, and is nowadays discussed in the cognitive science community for its possible implications for theories of the mind. In particular, one of the most popular process theories derived from the {FEP}, active inference, describes a biologically plausible algorithmic implementation of this principle with several repercussions on our understanding of cognition.},
	type = {phdthesis},
	author = {Baltieri, Manuel},
	langid = {english},
	file = {Baltieri - Active inference building a new bridge between co.pdf:/Users/bert/Zotero/storage/EJ56HNRK/Baltieri - Active inference building a new bridge between co.pdf:application/pdf},
}

@article{bui_partitioned_2018,
	title = {Partitioned Variational Inference: A unified framework encompassing federated and continual learning},
	url = {http://arxiv.org/abs/1811.11206},
	shorttitle = {Partitioned Variational Inference},
	abstract = {Variational inference ({VI}) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference ({PVI}), that explicitly acknowledges these algorithmic dimensions of {VI}, unifies disparate literature, and provides guidance on usage. Crucially, the proposed {PVI} framework allows us to identify new ways of performing {VI} that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard {VI}.},
	journaltitle = {{arXiv}:1811.11206 [cs, stat]},
	author = {Bui, Thang D. and Nguyen, Cuong V. and Swaroop, Siddharth and Turner, Richard E.},
	urldate = {2019-07-31},
	date = {2018-11-27},
	eprinttype = {arxiv},
	eprint = {1811.11206},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/M395CFDX/1811.html:text/html;Bui et al. - 2018 - Partitioned Variational Inference A unified frame.pdf:/Users/bert/Zotero/storage/JWSF45WW/Bui et al. - 2018 - Partitioned Variational Inference A unified frame.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2020-11-03},
	date = {2017-12-05},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/37WBV397/1706.html:text/html;Vaswani et al. - 2017 - Attention Is All You Need.pdf:/Users/bert/Zotero/storage/TAL6ULUW/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@article{frank_fundamental_2020,
	title = {The fundamental equations of change in statistical ensembles and biological populations},
	url = {http://arxiv.org/abs/2010.14544},
	abstract = {A recent article in Nature Physics unified key results from thermodynamics, statistics, and information theory. The unification arose from a general equation for the rate of change in the information content of a system. The general equation describes the change in the moments of an observable quantity over a probability distribution. One term in the equation describes the change in the probability distribution. The other term describes the change in the observable values for a given state. We show the equivalence of this general equation for moment dynamics with the widely known Price equation from evolutionary theory, named after George Price. We introduce the Price equation from its biological roots, review a mathematically abstract form of the equation, and discuss the potential for this equation to unify diverse mathematical theories from different disciplines. The new work in Nature Physics and many applications in biology show that this equation also provides the basis for deriving many novel theoretical results within each discipline.},
	journaltitle = {{arXiv}:2010.14544 [cs, math, q-bio]},
	author = {Frank, Steven A. and Bruggeman, Frank J.},
	urldate = {2020-11-03},
	date = {2020-10-27},
	eprinttype = {arxiv},
	eprint = {2010.14544},
	keywords = {Computer Science - Information Theory, Quantitative Biology - Populations and Evolution},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/BPRKBWZV/2010.html:text/html;Frank and Bruggeman - 2020 - The fundamental equations of change in statistical.pdf:/Users/bert/Zotero/storage/S2ERSHKF/Frank and Bruggeman - 2020 - The fundamental equations of change in statistical.pdf:application/pdf},
}

@article{nicholson_time-information_2020,
	title = {Time-information uncertainty relations in thermodynamics},
	url = {http://arxiv.org/abs/2001.05418},
	doi = {10.1038/s41567-020-0981-y},
	abstract = {Physical systems that power motion and create structure in a fixed amount of time dissipate energy and produce entropy. Whether living or synthetic, systems performing these dynamic functions must balance dissipation and speed. Here, we show that rates of energy and entropy exchange are subject to a speed limit -- a time-information uncertainty relation -- imposed by the rates of change in the information content of the system. This uncertainty relation bounds the time that elapses before the change in a thermodynamic quantity has the same magnitude as its initial standard deviation. From this general bound, we establish a family of speed limits for heat, work, entropy production, and entropy flow depending on the experimental constraints on the system. In all of these inequalities, the time scale of transient dynamical fluctuations is universally bounded by the Fisher information. Moreover, they all have a mathematical form that mirrors the Mandelstam-Tamm version of the time-energy uncertainty relation in quantum mechanics. These bounds on the speed of arbitrary observables apply to transient systems away from thermodynamic equilibrium, independent of the physical assumptions about the stochastic dynamics or their function.},
	journaltitle = {{arXiv}:2001.05418 [cond-mat]},
	author = {Nicholson, Schuyler B. and Garcia-Pintos, Luis Pedro and del Campo, Adolfo and Green, Jason R.},
	urldate = {2020-11-03},
	date = {2020-01-15},
	eprinttype = {arxiv},
	eprint = {2001.05418},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4E27HN78/2001.html:text/html;Nicholson et al. - 2020 - Time-information uncertainty relations in thermody.pdf:/Users/bert/Zotero/storage/QZEKKLRI/Nicholson et al. - 2020 - Time-information uncertainty relations in thermody.pdf:application/pdf},
}

@article{defossez_real_2020,
	title = {Real Time Speech Enhancement in the Waveform Domain},
	url = {http://arxiv.org/abs/2006.12847},
	abstract = {We present a causal speech enhancement model working on the raw waveform that runs in real-time on a laptop {CPU}. The proposed model is based on an encoder-decoder architecture with skip-connections. It is optimized on both time and frequency domains, using multiple loss functions. Empirical evidence shows that it is capable of removing various kinds of background noise including stationary and non-stationary noises, as well as room reverb. Additionally, we suggest a set of data augmentation techniques applied directly on the raw waveform which further improve model performance and its generalization abilities. We perform evaluations on several standard benchmarks, both using objective metrics and human judgements. The proposed model matches state-of-the-art performance of both causal and non causal methods while working directly on the raw waveform.},
	journaltitle = {{arXiv}:2006.12847 [cs, eess, stat]},
	author = {Defossez, Alexandre and Synnaeve, Gabriel and Adi, Yossi},
	urldate = {2020-10-30},
	date = {2020-09-06},
	eprinttype = {arxiv},
	eprint = {2006.12847},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/93CT7GPW/2006.html:text/html;Defossez et al. - 2020 - Real Time Speech Enhancement in the Waveform Domai.pdf:/Users/bert/Zotero/storage/FYVQ2SBK/Defossez et al. - 2020 - Real Time Speech Enhancement in the Waveform Domai.pdf:application/pdf},
}

@article{wan_f-divergence_2020,
	title = {\$f\$-Divergence Variational Inference},
	url = {http://arxiv.org/abs/2009.13093},
	abstract = {This paper introduces the \$f\$-divergence variational inference (\$f\$-{VI}) that generalizes variational inference to all \$f\$-divergences. Initiated from minimizing a crafty surrogate \$f\$-divergence that shares the statistical consistency with the \$f\$-divergence, the \$f\$-{VI} framework not only unifies a number of existing {VI} methods, e.g. Kullback-Leibler {VI}, R{\textbackslash}'\{e\}nyi's \${\textbackslash}alpha\$-{VI}, and \${\textbackslash}chi\$-{VI}, but offers a standardized toolkit for {VI} subject to arbitrary divergences from \$f\$-divergence family. A general \$f\$-variational bound is derived and provides a sandwich estimate of marginal likelihood (or evidence). The development of the \$f\$-{VI} unfolds with a stochastic optimization scheme that utilizes the reparameterization trick, importance weighting and Monte Carlo approximation; a mean-field approximation scheme that generalizes the well-known coordinate ascent variational inference ({CAVI}) is also proposed for \$f\$-{VI}. Empirical examples, including variational autoencoders and Bayesian neural networks, are provided to demonstrate the effectiveness and the wide applicability of \$f\$-{VI}.},
	journaltitle = {{arXiv}:2009.13093 [cs, math, stat]},
	author = {Wan, Neng and Li, Dapeng and Hovakimyan, Naira},
	urldate = {2020-10-27},
	date = {2020-10-20},
	eprinttype = {arxiv},
	eprint = {2009.13093},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/BKIHP49T/2009.html:text/html;Wan et al. - 2020 - \$f\$-Divergence Variational Inference.pdf:/Users/bert/Zotero/storage/KY9FMM48/Wan et al. - 2020 - \$f\$-Divergence Variational Inference.pdf:application/pdf},
}

@article{minka_divergence_2005-1,
	title = {Divergence Measures and Message Passing},
	url = {http://130.203.136.95/viewdoc/citations;jsessionid=B5EB52599DAAD14C34917164D20FC245?doi=10.1.1.314.3445},
	abstract = {{CiteSeerX} - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): This paper presents a unifying view of messagepassing algorithms, as methods to approximate a complex Bayesian network by a simpler network with minimum information divergence. In this view, the difference between mean-field methods and belief propagation is not the amount of structure they model, but only the measure of loss they minimize (‘exclusive ’ versus ‘inclusive’ Kullback-Leibler divergence). In each case, message-passing arises by minimizing a localized version of the divergence, local to each factor. By examining these divergence measures, we can intuit the types of solution they prefer (symmetry-breaking, for example) and their suitability for different tasks. Furthermore, by considering a wider variety of divergence measures (such as alpha-divergences), we can achieve different complexity and performance goals. 1},
	author = {Minka, Thomas},
	urldate = {2020-10-27},
	date = {2005},
	langid = {english},
	file = {Minka - 2005 - Divergence Measures and Message Passing.pdf:/Users/bert/Zotero/storage/MYLAEQF9/Minka - 2005 - Divergence Measures and Message Passing.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/JLAKN9PG/citations\;jsessionid=B5EB52599DAAD14C34917164D20FC245.html:text/html},
}

@article{welling_structured_2012,
	title = {Structured Region Graphs: Morphing {EP} into {GBP}},
	url = {http://arxiv.org/abs/1207.1426},
	shorttitle = {Structured Region Graphs},
	abstract = {{GBP} and {EP} are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce 'structured region graphs', a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region. Reduction operators on these structures allow conversion between {EP} and {GBP} free energies. Thus it is revealed that all {EP} approximations on discrete variables are special cases of {GBP}, and conversely that some wellknown {GBP} approximations, such as overlapping squares, are special cases of {EP}. Furthermore, region graphs derived from {EP} have a number of good structural properties, including maxent-normality and overall counting number of one. The result is a convenient framework for producing high-quality approximations with a user-adjustable level of complexity},
	journaltitle = {{arXiv}:1207.1426 [cs]},
	author = {Welling, Max and Minka, Thomas P. and Teh, Yee Whye},
	urldate = {2020-10-27},
	date = {2012-07-04},
	eprinttype = {arxiv},
	eprint = {1207.1426},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2NZRR3Q9/1207.html:text/html;Welling et al. - 2012 - Structured Region Graphs Morphing EP into GBP.pdf:/Users/bert/Zotero/storage/7VVCQIMX/Welling et al. - 2012 - Structured Region Graphs Morphing EP into GBP.pdf:application/pdf},
}

@article{welling_choice_2012,
	title = {On the Choice of Regions for Generalized Belief Propagation},
	url = {http://arxiv.org/abs/1207.4158},
	abstract = {Generalized belief propagation ({GBP}) has proven to be a promising technique for approximate inference tasks in {AI} and machine learning. However, the choice of a good set of clusters to be used in {GBP} has remained more of an art then a science until this day. This paper proposes a sequential approach to adding new clusters of nodes and their interactions (i.e. "regions") to the approximation. We first review and analyze the recently introduced region graphs and find that three kinds of operations ("split", "merge" and "death") leave the free energy and (under some conditions) the fixed points of {GBP} invariant. This leads to the notion of "weakly irreducible" regions as the natural candidates to be added to the approximation. Computational complexity of the {GBP} algorithm is controlled by restricting attention to regions with small "region-width". Combining the above with an efficient (i.e. local in the graph) measure to predict the improved accuracy of {GBP} leads to the sequential "region pursuit" algorithm for adding new regions bottom-up to the region graph. Experiments show that this algorithm can indeed perform close to optimally.},
	journaltitle = {{arXiv}:1207.4158 [cs]},
	author = {Welling, Max},
	urldate = {2020-10-27},
	date = {2012-07-11},
	eprinttype = {arxiv},
	eprint = {1207.4158},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/P6TNVJWF/1207.html:text/html;Welling - 2012 - On the Choice of Regions for Generalized Belief Pr.pdf:/Users/bert/Zotero/storage/FDMHDF4Y/Welling - 2012 - On the Choice of Regions for Generalized Belief Pr.pdf:application/pdf},
}

@article{shih_probabilistic_2020,
	title = {Probabilistic Circuits for Variational Inference in Discrete Graphical Models},
	url = {http://arxiv.org/abs/2010.11446},
	abstract = {Inference in discrete graphical models with variational methods is difficult because of the inability to re-parameterize gradients of the Evidence Lower Bound ({ELBO}). Many sampling-based methods have been proposed for estimating these gradients, but they suffer from high bias or variance. In this paper, we propose a new approach that leverages the tractability of probabilistic circuit models, such as Sum Product Networks ({SPN}), to compute {ELBO} gradients exactly (without sampling) for a certain class of densities. In particular, we show that selective-{SPNs} are suitable as an expressive variational distribution, and prove that when the log-density of the target model is a polynomial the corresponding {ELBO} can be computed analytically. To scale to graphical models with thousands of variables, we develop an efficient and effective construction of selective-{SPNs} with size \$O(kn)\$, where \$n\$ is the number of variables and \$k\$ is an adjustable hyperparameter. We demonstrate our approach on three types of graphical models -- Ising models, Latent Dirichlet Allocation, and factor graphs from the {UAI} Inference Competition. Selective-{SPNs} give a better lower bound than mean-field and structured mean-field, and is competitive with approximations that do not provide a lower bound, such as Loopy Belief Propagation and Tree-Reweighted Belief Propagation. Our results show that probabilistic circuits are promising tools for variational inference in discrete graphical models as they combine tractability and expressivity.},
	journaltitle = {{arXiv}:2010.11446 [cs]},
	author = {Shih, Andy and Ermon, Stefano},
	urldate = {2020-10-27},
	date = {2020-10-22},
	eprinttype = {arxiv},
	eprint = {2010.11446},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6R8WW78U/2010.html:text/html;Shih and Ermon - 2020 - Probabilistic Circuits for Variational Inference i.pdf:/Users/bert/Zotero/storage/AQ598A2H/Shih and Ermon - 2020 - Probabilistic Circuits for Variational Inference i.pdf:application/pdf},
}

@article{burgt_organic_2020,
	title = {Organic materials and devices for brain-inspired computing: From artificial implementation to biophysical realism},
	volume = {45},
	issn = {0883-7694, 1938-1425},
	url = {https://www.cambridge.org/core/journals/mrs-bulletin/article/organic-materials-and-devices-for-braininspired-computing-from-artificial-implementation-to-biophysical-realism/F907554D6706D272EA5ECEF05416B274},
	doi = {10.1557/mrs.2020.194},
	shorttitle = {Organic materials and devices for brain-inspired computing},
	abstract = {, Many of the current artificial intelligence ({AI}) applications that are rapidly becoming indispensable in our society rely on software-based artificial neural networks or deep learning algorithms that are powerful, but energy-inefficient. The brain in comparison is highly efficient at similar classification and pattern finding tasks. Neuromorphic engineering attempts to take advantage of the efficiency of the brain by mimicking several crucial concepts to efficiently emulate {AI} tasks. Organic electronic materials have been particularly successful in mimicking both the basic functionality of the brain, including important spiking phenomena, but also in low-power operation of hardware-implemented artificial neural networks as well as interfacing with physiological environments due to their biocompatible nature. This article provides an overview of the basic functional operation of the brain and its artificial counterparts, with a particular focus on organic materials and devices. We highlight efforts to mimic brain functions such as spatiotemporal processing, homeostasis, and functional connectivity and emphasize current challenges for efficient neuromorphic computing applications. Finally, we present our view of future directions in this exciting and rapidly growing field of organic neuromorphic devices.},
	pages = {631--640},
	number = {8},
	journaltitle = {{MRS} Bulletin},
	author = {Burgt, Yoeri van de and Gkoupidenis, Paschalis},
	urldate = {2020-10-18},
	date = {2020-08},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Burgt and Gkoupidenis - 2020 - Organic materials and devices for brain-inspired c.pdf:/Users/bert/Zotero/storage/3DBHBT7J/Burgt and Gkoupidenis - 2020 - Organic materials and devices for brain-inspired c.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/CXIPPX8T/F907554D6706D272EA5ECEF05416B274.html:text/html},
}

@article{fagerholm_principle_2020,
	title = {The principle of stationary action in neural systems},
	url = {http://arxiv.org/abs/2010.02993},
	abstract = {The principle of stationary action is a cornerstone of modern physics, providing a powerful framework for investigating dynamical systems found in classical mechanics through to quantum field theory. However, computational neuroscience, despite its heavy reliance on concepts in physics, is anomalous in this regard as its main equations of motion are not compatible with a Lagrangian formulation and hence with the principle of stationary action. Taking the Dynamic Causal Modelling neuronal state equation, Hodgkin-Huxley model, and the Leaky Integrate-and-Fire model as examples, we show that it is possible to write complex oscillatory forms of these equations in terms of a single Lagrangian. We therefore bring mathematical descriptions in computational neuroscience under the remit of the principle of stationary action and use this reformulation to express symmetries and associated conservation laws arising in neural systems.},
	journaltitle = {{arXiv}:2010.02993 [physics, q-bio]},
	author = {Fagerholm, Erik D. and Friston, Karl J. and Moran, Rosalyn J. and Leech, Robert},
	urldate = {2020-10-14},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02993},
	keywords = {Quantitative Biology - Neurons and Cognition, Physics - Classical Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LW5QZUZG/2010.html:text/html;Fagerholm et al. - 2020 - The principle of stationary action in neural syste.pdf:/Users/bert/Zotero/storage/G6MJ65C3/Fagerholm et al. - 2020 - The principle of stationary action in neural syste.pdf:application/pdf},
}

@article{annila_economies_2009,
	title = {Economies Evolve by Energy Dispersal},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/11/4/606},
	doi = {10.3390/e11040606},
	abstract = {Economic activity can be regarded as an evolutionary process governed by the 2nd law of thermodynamics. The universal law, when formulated locally as an equation of motion, reveals that a growing economy develops functional machinery and organizes hierarchically in such a way as to tend to equalize energy density differences within the economy and in respect to the surroundings it is open to. Diverse economic activities result in flows of energy that will preferentially channel along the most steeply descending paths, leveling a non-Euclidean free energy landscape. This principle of 'maximal energy dispersal‘, equivalent to the maximal rate of entropy production, gives rise to economic laws and regularities. The law of diminishing returns follows from the diminishing free energy while the relation between supply and demand displays a quest for a balance among interdependent energy densities. Economic evolution is dissipative motion where the driving forces and energy flows are inseparable from each other. When there are multiple degrees of freedom, economic growth and decline are inherently impossible to forecast in detail. Namely, trajectories of an evolving economy are non-integrable, i.e. unpredictable in detail because a decision by a player will affect also future decisions of other players. We propose that decision making is ultimately about choosing from various actions those that would reduce most effectively subjectively perceived energy gradients.},
	pages = {606--633},
	number = {4},
	journaltitle = {Entropy},
	author = {Annila, Arto and Salthe, Stanley},
	urldate = {2020-10-07},
	date = {2009-12},
	langid = {english},
	note = {Number: 4
Publisher: Molecular Diversity Preservation International},
	keywords = {natural selection, evolution, entropy, thermodynamics, energy transduction, hierarchy, natural process, statistical physics},
	file = {Annila and Salthe - 2009 - Economies Evolve by Energy Dispersal.pdf:/Users/bert/Zotero/storage/NTU4MV2S/Annila and Salthe - 2009 - Economies Evolve by Energy Dispersal.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/MHZPYATY/606.html:text/html},
}

@article{watson_active_2020,
	title = {Active Inference or Control as Inference? A Unifying View},
	url = {http://arxiv.org/abs/2010.00262},
	shorttitle = {Active Inference or Control as Inference?},
	abstract = {Active inference ({AI}) is a persuasive theoretical framework from computational neuroscience that seeks to describe action and perception as inference-based computation. However, this framework has yet to provide practical sensorimotor control algorithms that are competitive with alternative approaches. In this work, we frame active inference through the lens of control as inference ({CaI}), a body of work that presents trajectory optimization as inference. From the wider view of `probabilistic numerics', {CaI} offers principled, numerically robust optimal control solvers that provide uncertainty quantification, and can scale to nonlinear problems with approximate inference. We show that {AI} may be framed as partially-observed {CaI} when the cost function is defined specifically in the observation states.},
	journaltitle = {{arXiv}:2010.00262 [cs, stat]},
	author = {Watson, Joe and Imohiosen, Abraham and Peters, Jan},
	urldate = {2020-10-05},
	date = {2020-10-01},
	eprinttype = {arxiv},
	eprint = {2010.00262},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5NPCLWSP/2010.html:text/html;Watson et al. - 2020 - Active Inference or Control as Inference A Unifyi.pdf:/Users/bert/Zotero/storage/5VGCJFAH/Watson et al. - 2020 - Active Inference or Control as Inference A Unifyi.pdf:application/pdf},
}

@article{da_costa_relationship_2020,
	title = {The relationship between dynamic programming and active inference: the discrete, finite-horizon case},
	url = {http://arxiv.org/abs/2009.08111},
	shorttitle = {The relationship between dynamic programming and active inference},
	abstract = {Active inference is a normative framework for generating behaviour based upon the free energy principle, a theory of global brain function. This framework has been successfully used to solve reinforcement learning and stochastic control tasks, yet, the formal relation between active inference and reward maximisation has not been fully explicated. In this paper, we consider the relation between active inference and dynamic programming under the Bellman equation, which underlies many approaches to reinforcement learning and control. We show that, on finite-horizon partially observed Markov decision processes, dynamic programming is a limiting case of active inference. In a fully observed environment, active inference agents seek to sample a target distribution encoding preferences. When these target states correspond to rewarding states, this maximises expected reward as in reinforcement learning. When states are partially observed or ambiguous, active inference agents will choose the action that minimises both risk and ambiguity. This allows active inference agents to supplement goal-seeking with exploratory behaviour. This speaks to the unifying potential of active inference, as the objective optimised during action selection subsumes many important quantities used in decision-making in the physical, engineering, and life sciences.},
	journaltitle = {{arXiv}:2009.08111 [cs, math, q-bio]},
	author = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
	urldate = {2020-09-22},
	date = {2020-09-19},
	eprinttype = {arxiv},
	eprint = {2009.08111},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6SKBJWXC/2009.html:text/html;Da Costa et al. - 2020 - The relationship between dynamic programming and a.pdf:/Users/bert/Zotero/storage/VFT4Y7SH/Da Costa et al. - 2020 - The relationship between dynamic programming and a.pdf:application/pdf},
}

@inproceedings{mathys_hierarchical_2020,
	location = {Gent, Belgium},
	title = {Hierarchical Gaussian Filtering of Sufficient Statistic Time Series for Active Inference},
	eventtitle = {1st Int'l Workshop on Active Inference},
	booktitle = {1st Int'l Workshop on Active Inference},
	author = {Mathys, Chris and Weber, Lilian},
	date = {2020},
	file = {Mathys and Weber - 2020 - Hierarchical Gaussian Filtering of Sufficient Stat.pdf:/Users/bert/Zotero/storage/MKB6XNBJ/Mathys and Weber - 2020 - Hierarchical Gaussian Filtering of Sufficient Stat.pdf:application/pdf},
}

@article{kirsch_unpacking_2020,
	title = {Unpacking Information Bottlenecks: Unifying Information-Theoretic Objectives in Deep Learning},
	url = {http://arxiv.org/abs/2003.12537},
	shorttitle = {Unpacking Information Bottlenecks},
	abstract = {The information bottleneck ({IB}) principle offers both a mechanism to explain how deep neural networks train and generalize, as well as a regularized objective with which to train models. However, multiple competing objectives have been proposed based on this principle, and the information-theoretic quantities in these objectives are difficult to compute for large deep neural networks. This, in turn, limits their use as a training objective. In this work, we review these quantities, compare and unify previously proposed objectives and relate them to surrogate objectives more friendly to optimization. We find that these surrogate objectives allow us to apply the information bottleneck to modern neural network architectures. We demonstrate our insights on Permutation-{MNIST}, {MNIST} and {CIFAR}10.},
	journaltitle = {{arXiv}:2003.12537 [cs, stat]},
	author = {Kirsch, Andreas and Lyle, Clare and Gal, Yarin},
	urldate = {2020-09-09},
	date = {2020-04-08},
	eprinttype = {arxiv},
	eprint = {2003.12537},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/X7FFLBWM/2003.html:text/html;Kirsch et al. - 2020 - Unpacking Information Bottlenecks Unifying Inform.pdf:/Users/bert/Zotero/storage/XXBCED4B/Kirsch et al. - 2020 - Unpacking Information Bottlenecks Unifying Inform.pdf:application/pdf},
}

@article{hafner_action_2020,
	title = {Action and Perception as Divergence Minimization},
	url = {http://arxiv.org/abs/2009.01791},
	abstract = {We introduce a unified objective for action and perception of intelligent agents. Extending representation learning and control, we minimize the joint divergence between the world and a target distribution. Intuitively, such agents use perception to align their beliefs with the world, and use actions to align the world with their beliefs. Minimizing the joint divergence to an expressive target maximizes the mutual information between the agent's representations and inputs, thus inferring representations that are informative of past inputs and exploring future inputs that are informative of the representations. This lets us derive intrinsic objectives, such as representation learning, information gain, empowerment, and skill discovery from minimal assumptions. Moreover, interpreting the target distribution as a latent variable model suggests expressive world models as a path toward highly adaptive agents that seek large niches in their environments, while rendering task rewards optional. The presented framework provides a common language for comparing a wide range of objectives, facilitates understanding of latent variables for decision making, and offers a recipe for designing novel objectives. We recommend deriving future agent objectives from the joint divergence to facilitate comparison, to point out the agent's target distribution, and to identify the intrinsic objective terms needed to reach that distribution.},
	journaltitle = {{arXiv}:2009.01791 [cs, math, stat]},
	author = {Hafner, Danijar and Ortega, Pedro A. and Ba, Jimmy and Parr, Thomas and Friston, Karl and Heess, Nicolas},
	urldate = {2020-09-08},
	date = {2020-09-03},
	eprinttype = {arxiv},
	eprint = {2009.01791},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/Q9KAVTCB/2009.html:text/html;Hafner et al. - 2020 - Action and Perception as Divergence Minimization.pdf:/Users/bert/Zotero/storage/SBHAAW98/Hafner et al. - 2020 - Action and Perception as Divergence Minimization.pdf:application/pdf;Hafner et al. - 2020 - Action and Perception as Divergence Minimization.pdf:/Users/bert/Zotero/storage/NTPEBTGT/Hafner et al. - 2020 - Action and Perception as Divergence Minimization.pdf:application/pdf},
}

@article{chen_wavegrad_2020,
	title = {{WaveGrad}: Estimating Gradients for Waveform Generation},
	url = {http://arxiv.org/abs/2009.00713},
	shorttitle = {{WaveGrad}},
	abstract = {This paper introduces {WaveGrad}, a conditional model for waveform generation through estimating gradients of the data density. This model is built on the prior work on score matching and diffusion probabilistic models. It starts from Gaussian white noise and iteratively refines the signal via a gradient-based sampler conditioned on the mel-spectrogram. {WaveGrad} is non-autoregressive, and requires only a constant number of generation steps during inference. It can use as few as 6 iterations to generate high fidelity audio samples. {WaveGrad} is simple to train, and implicitly optimizes for the weighted variational lower-bound of the log-likelihood. Empirical experiments reveal {WaveGrad} to generate high fidelity audio samples matching a strong likelihood-based autoregressive baseline with less sequential operations.},
	journaltitle = {{arXiv}:2009.00713 [cs, eess, stat]},
	author = {Chen, Nanxin and Zhang, Yu and Zen, Heiga and Weiss, Ron J. and Norouzi, Mohammad and Chan, William},
	urldate = {2020-09-03},
	date = {2020-09-02},
	eprinttype = {arxiv},
	eprint = {2009.00713},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/IB2T8GFH/2009.html:text/html;Chen et al. - 2020 - WaveGrad Estimating Gradients for Waveform Genera.pdf:/Users/bert/Zotero/storage/KX5G28P3/Chen et al. - 2020 - WaveGrad Estimating Gradients for Waveform Genera.pdf:application/pdf},
}

@article{maestrini_inverse_2020,
	title = {The Inverse G-Wishart Distribution and Variational Message Passing},
	url = {http://arxiv.org/abs/2005.09876},
	abstract = {Message passing on a factor graph is a powerful paradigm for the coding of approximate inference algorithms for arbitrarily graphical large models. The notion of a factor graph fragment allows for compartmentalization of algebra and computer code. We show that the Inverse G-Wishart family of distributions enables fundamental variational message passing factor graph fragments to be expressed elegantly and succinctly. Such fragments arise in models for which approximate inference concerning covariance matrix or variance parameters is made, and are ubiquitous in contemporary statistics and machine learning.},
	journaltitle = {{arXiv}:2005.09876 [cs, stat]},
	author = {Maestrini, L. and Wand, M. P.},
	urldate = {2020-09-03},
	date = {2020-06-17},
	eprinttype = {arxiv},
	eprint = {2005.09876},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/QNQ8PC47/Maestrini and Wand - 2020 - The Inverse G-Wishart Distribution and Variational.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/49ZAVJM8/2005.html:text/html},
}

@article{zhao_information_2018,
	title = {The Information Autoencoding Family: A Lagrangian Perspective on Latent Variable Generative Models},
	url = {http://arxiv.org/abs/1806.06514},
	shorttitle = {The Information Autoencoding Family},
	abstract = {A large number of objectives have been proposed to train latent variable generative models. We show that many of them are Lagrangian dual functions of the same primal optimization problem. The primal problem optimizes the mutual information between latent and visible variables, subject to the constraints of accurately modeling the data distribution and performing correct amortized inference. By choosing to maximize or minimize mutual information, and choosing different Lagrange multipliers, we obtain different objectives including {InfoGAN}, {ALI}/{BiGAN}, {ALICE}, {CycleGAN}, beta-{VAE}, adversarial autoencoders, {AVB}, {AS}-{VAE} and {InfoVAE}. Based on this observation, we provide an exhaustive characterization of the statistical and computational trade-offs made by all the training objectives in this class of Lagrangian duals. Next, we propose a dual optimization method where we optimize model parameters as well as the Lagrange multipliers. This method achieves Pareto optimal solutions in terms of optimizing information and satisfying the constraints.},
	journaltitle = {{arXiv}:1806.06514 [cs, stat]},
	author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
	urldate = {2020-09-03},
	date = {2018-07-07},
	eprinttype = {arxiv},
	eprint = {1806.06514},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/62FD8HMG/1806.html:text/html;Zhao et al. - 2018 - The Information Autoencoding Family A Lagrangian .pdf:/Users/bert/Zotero/storage/UGD22ND3/Zhao et al. - 2018 - The Information Autoencoding Family A Lagrangian .pdf:application/pdf},
}

@inproceedings{garrett_whence_1998,
	location = {Dordrecht},
	title = {Whence the Laws of Probability?},
	isbn = {978-94-011-5028-6},
	doi = {10.1007/978-94-011-5028-6_6},
	series = {Fundamental Theories of Physics},
	abstract = {A new derivation is given of the sum and product rules of probability. Probability is treated as a number associated with one binary proposition conditioned on another, so that the Boolean calculus of the propositions induces a calculus for the probabilities. This is the strategy of R. T. Cox (1946), with a refinement: a formula is derived for the probability of the {NAND} of two propositions in terms of the probabilities of those propositions. Because {NAND} is a primitive logic operation from which any other can be synthesised, there are no further probabilities that the {NAND} can depend on. A functional equation is then set up for the relation between the probabilities and is solved. By synthesising the non-primitive operations {NOT} and {AND} from {NAND} the sum and product rules are derived from this one formula, the fundamental ‘law of probability’.},
	pages = {71--86},
	booktitle = {Maximum Entropy and Bayesian Methods},
	publisher = {Springer Netherlands},
	author = {Garrett, Anthony J. M.},
	editor = {Erickson, Gary J. and Rychert, Joshua T. and Smith, C. Ray},
	date = {1998},
	langid = {english},
	keywords = {probability, Boolean algebra, laws of probability, product rule, sum rule},
	file = {Garrett - 1998 - Whence the Laws of Probability.pdf:/Users/bert/Zotero/storage/SQ3BSJQ9/Garrett - 1998 - Whence the Laws of Probability.pdf:application/pdf},
}

@online{noauthor_prior_nodate,
	title = {Prior Distributions for Variance Parameters in Hierarchical Models},
	url = {https://www.researchgate.net/publication/241681214_Prior_Distributions_for_Variance_Parameters_in_Hierarchical_Models},
	abstract = {Download Citation {\textbar} Prior Distributions for Variance Parameters in Hierarchical Models {\textbar} Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new... {\textbar} Find, read and cite all the research you need on {ResearchGate}},
	titleaddon = {{ResearchGate}},
	urldate = {2020-08-25},
	langid = {english},
	doi = {10.1214/06-BA117A},
	file = {Prior Distributions for Variance Parameters in Hie.pdf:/Users/bert/Zotero/storage/DTTPLX4A/Prior Distributions for Variance Parameters in Hie.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XC7GF9DQ/241681214_Prior_Distributions_for_Variance_Parameters_in_Hierarchical_Models.html:text/html},
}

@inproceedings{elidan_residual_2006,
	location = {Arlington, Virginia, {USA}},
	title = {Residual belief Propagation: informed scheduling for asynchronous message passing},
	isbn = {978-0-9749039-2-7},
	series = {{UAI}'06},
	shorttitle = {Residual belief Propagation},
	abstract = {Inference for probabilistic graphical models is still very much a practical challenge in large domains. The commonly used and effective belief propagation ({BP}) algorithm and its generalizations often do not converge when applied to hard, real-life inference tasks. While it is widely recognized that the scheduling of messages in these algorithms may have significant consequences, this issue remains largely unexplored. In this work, we address the question of how to schedule messages for asynchronous propagation so that a fixed point is reached faster and more often. We first show that any reasonable asynchronous {BP} converges to a unique fixed point under conditions similar to those that guarantee convergence of synchronous {BP}. In addition, we show that the convergence rate of a simple round-robin schedule is at least as good as that of synchronous propagation. We then propose residual belief propagation ({RBP}), a novel, easy-to-implement, asynchronous propagation algorithm that schedules messages in an informed way, that pushes down a bound on the distance from the fixed point. Finally, we demonstrate the superiority of {RBP} over state-of-the-art methods for a variety of challenging synthetic and real-life problems: {RBP} converges significantly more often than other methods; and it significantly reduces running time until convergence, even when other methods converge.},
	pages = {165--173},
	booktitle = {Proceedings of the Twenty-Second Conference on Uncertainty in Artificial Intelligence},
	publisher = {{AUAI} Press},
	author = {Elidan, Gal and {McGraw}, Ian and Koller, Daphne},
	urldate = {2020-08-23},
	date = {2006-07-13},
	file = {Elidan et al. - 2006 - Residual belief Propagation informed scheduling f.pdf:/Users/bert/Zotero/storage/6EHIJ9FQ/Elidan et al. - 2006 - Residual belief Propagation informed scheduling f.pdf:application/pdf},
}

@report{petersen_matrix_2012,
	title = {The Matrix Cookbook},
	url = {http://www2.compute.dtu.dk/pubdb/pubs/3274-full.html},
	abstract = {Matrix identities, relations and approximations. A desktop reference for quick overview of mathematics of matrices.},
	institution = {Technical University of Denmark},
	author = {Petersen, K.B. and Pedersen, M.S.},
	urldate = {2020-08-23},
	date = {2012-11-15},
	file = {Petersen and Pedersen - 2012 - The Matrix Cookbook.pdf:/Users/bert/Zotero/storage/NC9LUCEZ/Petersen and Pedersen - 2012 - The Matrix Cookbook.pdf:application/pdf;The Matrix Cookbook:/Users/bert/Zotero/storage/BMRBVEIQ/3274-full.html:text/html},
}

@article{schwoebel_active_2018,
	title = {Active Inference, Belief Propagation, and the Bethe Approximation},
	volume = {30},
	issn = {1530-888X},
	doi = {10.1162/neco_a_01108},
	abstract = {When modeling goal-directed behavior in the presence of various sources of uncertainty, planning can be described as an inference process. A solution to the problem of planning as inference was previously proposed in the active inference framework in the form of an approximate inference scheme based on variational free energy. However, this approximate scheme was based on the mean-field approximation, which assumes statistical independence of hidden variables and is known to show overconfidence and may converge to local minima of the free energy. To better capture the spatiotemporal properties of an environment, we reformulated the approximate inference process using the so-called Bethe approximation. Importantly, the Bethe approximation allows for representation of pairwise statistical dependencies. Under these assumptions, the minimizer of the variational free energy corresponds to the belief propagation algorithm, commonly used in machine learning. To illustrate the differences between the mean-field approximation and the Bethe approximation, we have simulated agent behavior in a simple goal-reaching task with different types of uncertainties. Overall, the Bethe agent achieves higher success rates in reaching goal states. We relate the better performance of the Bethe agent to more accurate predictions about the consequences of its own actions. Consequently, active inference based on the Bethe approximation extends the application range of active inference to more complex behavioral tasks.},
	pages = {2530--2567},
	number = {9},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Comput},
	author = {Schwoebel, Sarah and Kiebel, Stefan and Markovic, Dimitrije},
	date = {2018-09},
	pmid = {29949461},
	file = {Schwoebel et al. - 2018 - Active Inference, Belief Propagation, and the Beth.pdf:/Users/bert/Zotero/storage/GSIAAFSU/Schwoebel et al. - 2018 - Active Inference, Belief Propagation, and the Beth.pdf:application/pdf},
}

@article{van_doorn_-class_2020,
	title = {An In-Class Demonstration of Bayesian Inference},
	volume = {19},
	issn = {1475-7257},
	url = {https://doi.org/10.1177/1475725719848574},
	doi = {10.1177/1475725719848574},
	abstract = {Sir Ronald Fisher’s venerable experiment “The Lady Tasting Tea” is revisited from a Bayesian perspective. We demonstrate how a similar tasting experiment, conducted in a classroom setting, can familiarize students with several key concepts of Bayesian inference, such as the prior distribution, the posterior distribution, the Bayes factor, and sequential analysis.},
	pages = {36--45},
	number = {1},
	journaltitle = {Psychology Learning \& Teaching},
	shortjournal = {Psychology Learning \& Teaching},
	author = {van Doorn, Johnny and Matzke, Dora and Wagenmakers, Eric-Jan},
	urldate = {2020-08-10},
	date = {2020-03-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	file = {van Doorn et al. - 2020 - An In-Class Demonstration of Bayesian Inference.pdf:/Users/bert/Zotero/storage/5U8MTFFG/van Doorn et al. - 2020 - An In-Class Demonstration of Bayesian Inference.pdf:application/pdf},
}

@article{frank_measurement_2018,
	title = {Measurement invariance explains the universal law of generalization for psychological perception},
	volume = {115},
	issn = {1091-6490},
	doi = {10.1073/pnas.1809787115},
	abstract = {The universal law of generalization describes how animals discriminate between alternative sensory stimuli. On an appropriate perceptual scale, the probability that an organism perceives two stimuli as similar typically declines exponentially with the difference on the perceptual scale. Exceptions often follow a Gaussian probability pattern rather than an exponential pattern. Previous explanations have been based on underlying theoretical frameworks such as information theory, Kolmogorov complexity, or empirical multidimensional scaling. This article shows that the few inevitable invariances that must apply to any reasonable perceptual scale provide a sufficient explanation for the universal exponential law of generalization. In particular, reasonable measurement scales of perception must be invariant to shift by a constant value, which by itself leads to the exponential form. Similarly, reasonable measurement scales of perception must be invariant to multiplication, or stretch, by a constant value, which leads to the conservation of the slope of discrimination with perceptual difference. In some cases, an additional assumption about exchangeability or rotation of underlying perceptual dimensions leads to a Gaussian pattern of discrimination, which can be understood as a special case of the more general exponential form. The three measurement invariances of shift, stretch, and rotation provide a sufficient explanation for the universally observed patterns of perceptual generalization. All of the additional assumptions and language associated with information, complexity, and empirical scaling are superfluous with regard to the broad patterns of perception.},
	pages = {9803--9806},
	number = {39},
	journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
	shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
	author = {Frank, Steven A.},
	date = {2018},
	pmid = {30201714},
	pmcid = {PMC6166795},
	keywords = {Animals, Perception, Probability, animal behavior, categorization, Discrimination, Psychological, Generalization, Psychological, Models, Psychological, Normal Distribution, probability theory, scaling patterns, sensory information},
	file = {Frank - 2018 - Measurement invariance explains the universal law .pdf:/Users/bert/Zotero/storage/38ZPRZKE/Frank - 2018 - Measurement invariance explains the universal law .pdf:application/pdf},
}

@article{frank_universal_2017-1,
	title = {Universal expressions of population change by the Price equation: Natural selection, information, and maximum entropy production},
	volume = {7},
	issn = {2045-7758},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5433999/},
	doi = {10.1002/ece3.2922},
	shorttitle = {Universal expressions of population change by the Price equation},
	abstract = {The Price equation shows the unity between the fundamental expressions of change in biology, in information and entropy descriptions of populations, and in aspects of thermodynamics. The Price equation partitions the change in the average value of a metric between two populations. A population may be composed of organisms or particles or any members of a set to which we can assign probabilities. A metric may be biological fitness or physical energy or the output of an arbitrarily complicated function that assigns quantitative values to members of the population. The first part of the Price equation describes how directly applied forces change the probabilities assigned to members of the population when holding constant the metrical values of the members—a fixed metrical frame of reference. The second part describes how the metrical values change, altering the metrical frame of reference. In canonical examples, the direct forces balance the changing metrical frame of reference, leaving the average or total metrical values unchanged. In biology, relative reproductive success (fitness) remains invariant as a simple consequence of the conservation of total probability. In physics, systems often conserve total energy. Nonconservative metrics can be described by starting with conserved metrics, and then studying how coordinate transformations between conserved and nonconserved metrics alter the geometry of the dynamics and the aggregate values of populations. From this abstract perspective, key results from different subjects appear more simply as universal geometric principles for the dynamics of populations subject to the constraints of particular conserved quantities.},
	pages = {3381--3396},
	number = {10},
	journaltitle = {Ecology and Evolution},
	shortjournal = {Ecol Evol},
	author = {Frank, Steven A.},
	urldate = {2020-08-03},
	date = {2017-04-04},
	pmid = {28515874},
	pmcid = {PMC5433999},
	file = {Frank - 2017 - Universal expressions of population change by the .pdf:/Users/bert/Zotero/storage/TSJXPPQV/Frank - 2017 - Universal expressions of population change by the .pdf:application/pdf},
}

@article{frank_simple_2019,
	title = {Simple unity among the fundamental equations of science},
	url = {http://arxiv.org/abs/1904.00825},
	abstract = {The Price equation describes the change in populations. Change concerns some value, such as biological fitness, information or physical work. The Price equation reveals universal aspects for the nature of change, independently of the meaning ascribed to values. By understanding those universal aspects, we can see more clearly why fundamental mathematical results in different disciplines often share a common form. We can also interpret more clearly the meaning of key results within each discipline. For example, the mathematics of natural selection in biology has a form closely related to information theory and physical entropy. Does that mean that natural selection is about information or entropy? Or do natural selection, information and entropy arise as interpretations of a common underlying abstraction? The Price equation suggests the latter. The Price equation achieves its abstract generality by partitioning change into two terms. The first term naturally associates with the direct forces that cause change. The second term naturally associates with the changing frame of reference. In the Price equation's canonical form, total change remains zero because the conservation of total probability requires that all probabilities invariantly sum to one. Much of the shared common form for the mathematics of different disciplines may arise from that seemingly trivial invariance of total probability, which leads to the partitioning of total change into equal and opposite components of the direct forces and the changing frame of reference.},
	journaltitle = {{arXiv}:1904.00825 [cond-mat, q-bio]},
	author = {Frank, Steven A.},
	urldate = {2020-08-03},
	date = {2019-08-04},
	eprinttype = {arxiv},
	eprint = {1904.00825},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Quantitative Biology - Populations and Evolution},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/BHSDB6ZW/1904.html:text/html;Frank - 2019 - Simple unity among the fundamental equations of sc.pdf:/Users/bert/Zotero/storage/BQFQX4GK/Frank - 2019 - Simple unity among the fundamental equations of sc.pdf:application/pdf},
}

@article{friston_parcels_2020,
	title = {Parcels and particles: Markov blankets in the brain},
	url = {http://arxiv.org/abs/2007.09704},
	shorttitle = {Parcels and particles},
	abstract = {At the inception of human brain mapping, two principles of functional anatomy underwrote most conceptions - and analyses - of distributed brain responses: namely functional segregation and integration. There are currently two main approaches to characterising functional integration. The first is a mechanistic modelling of connectomics in terms of directed effective connectivity that mediates neuronal message passing and dynamics on neuronal circuits. The second phenomenological approach usually characterises undirected functional connectivity (i.e., measurable correlations), in terms of intrinsic brain networks, self-organised criticality, dynamical instability, etc. This paper describes a treatment of effective connectivity that speaks to the emergence of intrinsic brain networks and critical dynamics. It is predicated on the notion of Markov blankets that play a fundamental role in the self-organisation of far from equilibrium systems. Using the apparatus of the renormalisation group, we show that much of the phenomenology found in network neuroscience is an emergent property of a particular partition of neuronal states, over progressively larger scales. As such, it offers a way of linking dynamics on directed graphs to the phenomenology of intrinsic brain networks.},
	journaltitle = {{arXiv}:2007.09704 [q-bio]},
	author = {Friston, Karl J. and Fagerholm, Erik D. and Zarghami, Tahereh S. and Parr, Thomas and Hipólito, Inês and Magrou, Loïc and Razi, Adeel},
	urldate = {2020-07-22},
	date = {2020-07-19},
	eprinttype = {arxiv},
	eprint = {2007.09704},
	keywords = {Quantitative Biology - Neurons and Cognition, Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/RGPUH8D4/2007.html:text/html;Friston et al. - 2020 - Parcels and particles Markov blankets in the brai.pdf:/Users/bert/Zotero/storage/WZDUDXTA/Friston et al. - 2020 - Parcels and particles Markov blankets in the brai.pdf:application/pdf},
}

@article{friston_generative_2020,
	title = {Generative models, linguistic communication and active inference},
	issn = {0149-7634},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763420304668},
	doi = {10.1016/j.neubiorev.2020.07.005},
	abstract = {This paper presents a biologically plausible generative model and inference scheme that is capable of simulating communication between synthetic subjects who talk to each other. Building on active inference formulations of dyadic interactions, we simulate linguistic exchange to explore generative models that support dialogues. These models employ high-order interactions among abstract (discrete) states in deep (hierarchical) models. The sequential nature of language processing mandates generative models with a particular factorial structure—necessary to accommodate the rich combinatorics of language. We illustrate linguistic communication by simulating a synthetic subject who can play the ‘Twenty Questions’ game. In this game, synthetic subjects take the role of the questioner or answerer, using the same generative model. This simulation setup is used to illustrate some key architectural points and demonstrate that many behavioural and neurophysiological correlates of linguistic communication emerge under variational (marginal) message passing, given the right kind of generative model. For example, we show that theta-gamma coupling is an emergent property of belief updating, when listening to another.},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Friston, Karl J. and Parr, Thomas and Yufik, Yan and Sajid, Noor and Price, Catherine J. and Holmes, Emma},
	urldate = {2020-07-22},
	date = {2020-07-17},
	langid = {english},
	keywords = {free energy, Bayesian, message passing, inference, connectivity, hierarchical, Language, neuronal},
	file = {Friston et al. - 2020 - Generative models, linguistic communication and ac.pdf:/Users/bert/Zotero/storage/7XHV492T/Friston et al. - 2020 - Generative models, linguistic communication and ac.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/8S2U6HEZ/S0149763420304668.html:text/html},
}

@article{wilkinson_unifying_2019,
	title = {Unifying Probabilistic Models for Time-Frequency Analysis},
	url = {http://arxiv.org/abs/1811.02489},
	abstract = {In audio signal processing, probabilistic time-frequency models have many benefits over their non-probabilistic counterparts. They adapt to the incoming signal, quantify uncertainty, and measure correlation between the signal's amplitude and phase information, making time domain resynthesis straightforward. However, these models are still not widely used since they come at a high computational cost, and because they are formulated in such a way that it can be difficult to interpret all the modelling assumptions. By showing their equivalence to Spectral Mixture Gaussian processes, we illuminate the underlying model assumptions and provide a general framework for constructing more complex models that better approximate real-world signals. Our interpretation makes it intuitive to inspect, compare, and alter the models since all prior knowledge is encoded in the Gaussian process kernel functions. We utilise a state space representation to perform efficient inference via Kalman smoothing, and we demonstrate how our interpretation allows for efficient parameter learning in the frequency domain.},
	journaltitle = {{arXiv}:1811.02489 [cs, eess, stat]},
	author = {Wilkinson, William J. and Andersen, Michael Riis and Reiss, Joshua D. and Stowell, Dan and Solin, Arno},
	urldate = {2020-07-13},
	date = {2019-02-12},
	eprinttype = {arxiv},
	eprint = {1811.02489},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/DC4DVL3W/1811.html:text/html;Wilkinson et al. - 2019 - Unifying Probabilistic Models for Time-Frequency A.pdf:/Users/bert/Zotero/storage/MWCPPSNP/Wilkinson et al. - 2019 - Unifying Probabilistic Models for Time-Frequency A.pdf:application/pdf},
}

@article{dixit_tmi_2020,
	title = {{TMI}: Thermodynamic inference of data manifolds},
	volume = {2},
	issn = {2643-1564},
	url = {http://arxiv.org/abs/1911.09776},
	doi = {10.1103/PhysRevResearch.2.023201},
	shorttitle = {{TMI}},
	abstract = {The Gibbs-Boltzmann distribution offers a physically interpretable way to massively reduce the dimensionality of high dimensional probability distributions where the extensive variables are `features' and the intensive variables are `descriptors'. However, not all probability distributions can be modeled using the Gibbs-Boltzmann form. Here, we present {TMI}: {TMI}, \{{\textbackslash}bf T\}hermodynamic \{{\textbackslash}bf M\}anifold \{{\textbackslash}bf I\}nference; a thermodynamic approach to approximate a collection of arbitrary distributions. {TMI} simultaneously learns from data intensive and extensive variables and achieves dimensionality reduction through a multiplicative, positive valued, and interpretable decomposition of the data. Importantly, the reduced dimensional space of intensive parameters is not homogeneous. The Gibbs-Boltzmann distribution defines an analytically tractable Riemannian metric on the space of intensive variables allowing us to calculate geodesics and volume elements. We discuss the applications of {TMI} with multiple real and artificial data sets. Possible extensions are discussed as well.},
	pages = {023201},
	number = {2},
	journaltitle = {Physical Review Research},
	shortjournal = {Phys. Rev. Research},
	author = {Dixit, Purushottam D.},
	urldate = {2020-07-13},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {1911.09776},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/3JBY5X6D/1911.html:text/html;Dixit - 2020 - TMI Thermodynamic inference of data manifolds.pdf:/Users/bert/Zotero/storage/BEMUAGC5/Dixit - 2020 - TMI Thermodynamic inference of data manifolds.pdf:application/pdf},
}

@article{lee_derivation_2012,
	title = {A derivation of the master equation from path entropy maximization},
	volume = {137},
	issn = {0021-9606, 1089-7690},
	url = {http://arxiv.org/abs/1206.1416},
	doi = {10.1063/1.4743955},
	abstract = {The master equation and, more generally, Markov processes are routinely used as models for stochastic processes. They are often justified on the basis of randomization and coarse-graining assumptions. Here instead, we derive n-th order Markov processes and the master equation as unique solutions to an inverse problem. In particular, we find that when the constraints are not enough to uniquely determine the stochastic model, the n-th order Markov process emerges as the unique maximum entropy solution to this otherwise under-determined problem. This gives a rigorous alternative for justifying such models while providing a systematic recipe for generalizing widely accepted stochastic models usually assumed to follow from first principles.},
	pages = {074103},
	number = {7},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {The Journal of Chemical Physics},
	author = {Lee, Julian and Pressé, Steve},
	urldate = {2020-07-10},
	date = {2012-08-21},
	eprinttype = {arxiv},
	eprint = {1206.1416},
	keywords = {Mathematical Physics, Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ZFN3QHUG/1206.html:text/html;Lee and Pressé - 2012 - A derivation of the master equation from path entr.pdf:/Users/bert/Zotero/storage/KS3QJH94/Lee and Pressé - 2012 - A derivation of the master equation from path entr.pdf:application/pdf},
}

@article{dixit_building_2019,
	title = {Building Markov state models using optimal transport theory},
	volume = {{MMMK}},
	issn = {0021-9606},
	url = {https://aip.scitation.org/doi/full/10.1063/1.5086681%40jcp.2019.MMMK.issue-1},
	doi = {10.1063/1.5086681@jcp.2019.MMMK.issue-1},
	abstract = {Markov State Models ({MSMs}) describe the rates and routes in conformational dynamics of biomolecules. Computational estimation of {MSMs} can be expensive because molecular simulations are slow to find and sample the rare transient events. We describe here an efficient approximate way to determine {MSM} rate matrices by combining maximum caliber (maximizing path entropies) with optimal transport theory (minimizing some path cost function, as when routing trucks on transportation networks) to patch together transient dynamical information from multiple non-equilibrium simulations. We give toy examples.},
	pages = {054105},
	number = {1},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {J. Chem. Phys.},
	author = {Dixit, Purushottam D. and Dill, Ken A.},
	urldate = {2020-07-10},
	date = {2019-02-06},
	note = {Publisher: American Institute of Physics},
	file = {Dixit and Dill - 2019 - Building Markov state models using optimal transpo.pdf:/Users/bert/Zotero/storage/K79T348D/Dixit and Dill - 2019 - Building Markov state models using optimal transpo.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/E49SDPHV/1.5086681@jcp.2019.MMMK.html:text/html},
}

@article{dixit_maximum_2018,
	title = {Maximum Caliber: a general variational principle for dynamical systems},
	volume = {148},
	issn = {0021-9606, 1089-7690},
	url = {http://arxiv.org/abs/1711.03450},
	doi = {10.1063/1.5012990},
	shorttitle = {Maximum Caliber},
	abstract = {We review here \{{\textbackslash}it Maximum Caliber\} (Max Cal), a general variational principle for inferring distributions of paths in dynamical processes and networks. Max Cal is to dynamical trajectories what the principle of \{{\textbackslash}it Maximum Entropy\} (Max Ent) is to equilibrium states or stationary populations. In Max Cal, you maximize a path entropy over all possible pathways, subject to dynamical constraints, in order to predict relative path weights. Many well-known relationships of Non-Equilibrium Statistical Physics -- such as the Green-Kubo fluctuation-dissipation relations, Onsager's reciprocal relations, and Prigogine's Minimum Entropy Production -- are limited to near-equilibrium processes. Max Cal is more general. While it can readily derive these results under those limits, Max Cal is also applicable far from equilibrium. We give recent examples of {MaxCal} as a method of inference about trajectory distributions from limited data, finding reaction coordinates in bio-molecular simulations, and modeling the complex dynamics of non-thermal systems such as gene regulatory networks or the collective firing of neurons. We also survey its basis in principle, and some limitations.},
	pages = {010901},
	number = {1},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {The Journal of Chemical Physics},
	author = {Dixit, Purushottam D. and Wagoner, Jason and Weistuch, Corey and Pressé, Steve and Ghosh, Kingshuk and Dill, Ken A.},
	urldate = {2020-07-10},
	date = {2018-01-07},
	eprinttype = {arxiv},
	eprint = {1711.03450},
	keywords = {Condensed Matter - Statistical Mechanics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/84ZUIRVD/1711.html:text/html;Dixit et al. - 2018 - Maximum Caliber a general variational principle f.pdf:/Users/bert/Zotero/storage/E9A7XNYK/Dixit et al. - 2018 - Maximum Caliber a general variational principle f.pdf:application/pdf},
}

@article{tuszynski_mathematical_2014,
	title = {Mathematical and computational modeling in biology at multiple scales},
	volume = {11},
	issn = {1742-4682},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4396153/},
	doi = {10.1186/1742-4682-11-52},
	abstract = {A variety of topics are reviewed in the area of mathematical and computational modeling in biology, covering the range of scales from populations of organisms to electrons in atoms. The use of maximum entropy as an inference tool in the fields of biology and drug discovery is discussed. Mathematical and computational methods and models in the areas of epidemiology, cell physiology and cancer are surveyed. The technique of molecular dynamics is covered, with special attention to force fields for protein simulations and methods for the calculation of solvation free energies. The utility of quantum mechanical methods in biophysical and biochemical modeling is explored. The field of computational enzymology is examined.},
	journaltitle = {Theoretical Biology \& Medical Modelling},
	shortjournal = {Theor Biol Med Model},
	author = {Tuszynski, Jack A and Winter, Philip and White, Diana and Tseng, Chih-Yuan and Sahu, Kamlesh K and Gentile, Francesco and Spasevska, Ivana and Omar, Sara Ibrahim and Nayebi, Niloofar and Churchill, Cassandra {DM} and Klobukowski, Mariusz and El-Magd, Rabab M Abou},
	urldate = {2020-07-08},
	date = {2014-12-27},
	pmid = {25542608},
	pmcid = {PMC4396153},
	file = {Tuszynski et al. - 2014 - Mathematical and computational modeling in biology.pdf:/Users/bert/Zotero/storage/NLWX5LM9/Tuszynski et al. - 2014 - Mathematical and computational modeling in biology.pdf:application/pdf},
}

@article{presse_principles_2013,
	title = {Principles of maximum entropy and maximum caliber in statistical physics},
	volume = {85},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.85.1115},
	doi = {10.1103/RevModPhys.85.1115},
	abstract = {The variational principles called maximum entropy ({MaxEnt}) and maximum caliber ({MaxCal}) are reviewed. {MaxEnt} originated in the statistical physics of Boltzmann and Gibbs, as a theoretical tool for predicting the equilibrium states of thermal systems. Later, entropy maximization was also applied to matters of information, signal transmission, and image reconstruction. Recently, since the work of Shore and Johnson, {MaxEnt} has been regarded as a principle that is broader than either physics or information alone. {MaxEnt} is a procedure that ensures that inferences drawn from stochastic data satisfy basic self-consistency requirements. The different historical justifications for the entropy S=−∑ipilog pi and its corresponding variational principles are reviewed. As an illustration of the broadening purview of maximum entropy principles, maximum caliber, which is path entropy maximization applied to the trajectories of dynamical systems, is also reviewed. Examples are given in which maximum caliber is used to interpret dynamical fluctuations in biology and on the nanoscale, in single-molecule and few-particle systems such as molecular motors, chemical reactions, biological feedback circuits, and diffusion in microfluidics devices.},
	pages = {1115--1141},
	number = {3},
	journaltitle = {Reviews of Modern Physics},
	shortjournal = {Rev. Mod. Phys.},
	author = {Pressé, Steve and Ghosh, Kingshuk and Lee, Julian and Dill, Ken A.},
	urldate = {2020-07-07},
	date = {2013-07-16},
	note = {Publisher: American Physical Society},
	file = {APS Snapshot:/Users/bert/Zotero/storage/GI64LIL8/RevModPhys.85.html:text/html;Pressé et al. - 2013 - Principles of maximum entropy and maximum caliber .pdf:/Users/bert/Zotero/storage/L7VRYASJ/Pressé et al. - 2013 - Principles of maximum entropy and maximum caliber .pdf:application/pdf},
}

@article{hazoglou_communication_2015,
	title = {Communication: Maximum caliber is a general variational principle for nonequilibrium statistical mechanics},
	volume = {143},
	issn = {0021-9606},
	url = {https://aip.scitation.org/doi/full/10.1063/1.4928193},
	doi = {10.1063/1.4928193},
	shorttitle = {Communication},
	abstract = {There has been interest in finding a general variational principle for non-equilibrium statistical mechanics. We give evidence that Maximum Caliber (Max Cal) is such a principle. Max Cal, a variant of maximum entropy, predicts dynamical distribution functions by maximizing a path entropy subject to dynamical constraints, such as average fluxes. We first show that Max Cal leads to standard near-equilibrium results—including the Green-Kubo relations, Onsager’s reciprocal relations of coupled flows, and Prigogine’s principle of minimum entropy production—in a way that is particularly simple. We develop some generalizations of the Onsager and Prigogine results that apply arbitrarily far from equilibrium. Because Max Cal does not require any notion of “local equilibrium,” or any notion of entropy dissipation, or temperature, or even any restriction to material physics, it is more general than many traditional approaches. It also applicable to flows and traffic on networks, for example.},
	pages = {051104},
	number = {5},
	journaltitle = {The Journal of Chemical Physics},
	shortjournal = {J. Chem. Phys.},
	author = {Hazoglou, Michael J. and Walther, Valentin and Dixit, Purushottam D. and Dill, Ken A.},
	urldate = {2020-07-07},
	date = {2015-08-06},
	note = {Publisher: American Institute of Physics},
	file = {Hazoglou et al. - 2015 - Communication Maximum caliber is a general variat.pdf:/Users/bert/Zotero/storage/H37558TD/Hazoglou et al. - 2015 - Communication Maximum caliber is a general variat.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/9535ZH8X/1.html:text/html},
}

@article{ghosh_maximum_2020,
	title = {The Maximum Caliber Variational Principle for Nonequilibria},
	volume = {71},
	url = {https://doi.org/10.1146/annurev-physchem-071119-040206},
	doi = {10.1146/annurev-physchem-071119-040206},
	abstract = {{AbstractEver} since Clausius in 1865 and Boltzmann in 1877, the concepts of entropy and of its maximization have been the foundations for predicting how material equilibria derive from microscopic properties. But, despite much work, there has been no equally satisfactory general variational principle for nonequilibrium situations. However, in 1980, a new avenue was opened by E.T. Jaynes and by Shore and Johnson. We review here maximum caliber, which is a maximum-entropy-like principle that can infer distributions of flows over pathways, given dynamical constraints. This approach is providing new insights, particularly into few-particle complex systems, such as gene circuits, protein conformational reaction coordinates, network traffic, bird flocking, cell motility, and neuronal firing.},
	pages = {213--238},
	number = {1},
	journaltitle = {Annual Review of Physical Chemistry},
	author = {Ghosh, Kingshuk and Dixit, Purushottam D. and Agozzino, Luca and Dill, Ken A.},
	urldate = {2020-07-07},
	date = {2020},
	pmid = {32075515},
	note = {\_eprint: https://doi.org/10.1146/annurev-physchem-071119-040206},
	file = {Ghosh et al. - 2020 - The Maximum Caliber Variational Principle for None.pdf:/Users/bert/Zotero/storage/62DEX4FK/Ghosh et al. - 2020 - The Maximum Caliber Variational Principle for None.pdf:application/pdf},
}

@incollection{jaynes_probability_1990,
	location = {Dordrecht},
	title = {Probability Theory as Logic},
	isbn = {978-94-009-0683-9},
	url = {https://doi.org/10.1007/978-94-009-0683-9_1},
	series = {Fundamental Theories of Physics},
	abstract = {At the 1988 workshop we called attention to the “Mind Projection Fallacy” which is present in all fields that use probability. Here we give a more complete discussion showing why probabilities need not correspond to physical causal influences, or “propensities” affecting mass phenomena. Probability theory is far more useful if we recognize that probabilities express fundamentally logical inferences pertaining to individual cases. We note several examples of the difference this makes in real applications.},
	pages = {1--16},
	booktitle = {Maximum Entropy and Bayesian Methods},
	publisher = {Springer Netherlands},
	author = {Jaynes, E. T.},
	editor = {Fougère, Paul F.},
	urldate = {2020-07-07},
	date = {1990},
	langid = {english},
	doi = {10.1007/978-94-009-0683-9_1},
	keywords = {Bayesian Method, Confidence Factor, Physical Causation, Probability Theory, Product Rule},
	file = {Jaynes - 1990 - Probability Theory as Logic.pdf:/Users/bert/Zotero/storage/VRLWIQAL/Jaynes - 1990 - Probability Theory as Logic.pdf:application/pdf},
}

@article{matsoukas_thermodynamics_2019,
	title = {Thermodynamics Beyond Molecules: Statistical Thermodynamics of Probability Distributions},
	volume = {21},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/21/9/890},
	doi = {10.3390/e21090890},
	shorttitle = {Thermodynamics Beyond Molecules},
	abstract = {Statistical thermodynamics has a universal appeal that extends beyond molecular systems, and yet, as its tools are being transplanted to fields outside physics, the fundamental question, what is thermodynamics, has remained unanswered. We answer this question here. Generalized statistical thermodynamics is a variational calculus of probability distributions. It is independent of physical hypotheses but provides the means to incorporate our knowledge, assumptions and physical models about a stochastic processes that gives rise to the probability in question. We derive the familiar calculus of thermodynamics via a probabilistic argument that makes no reference to physics. At the heart of the theory is a space of distributions and a special functional that assigns probabilities to this space. The maximization of this functional generates the mathematical network of thermodynamic relationship. We obtain statistical mechanics as a special case and make contact with Information Theory and Bayesian inference.},
	pages = {890},
	number = {9},
	journaltitle = {Entropy},
	author = {Matsoukas, Themis},
	urldate = {2020-07-04},
	date = {2019-09},
	langid = {english},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {statistical thermodynamics, statistical mechanics, biased sampling, most probable distribution},
	file = {Matsoukas - 2019 - Thermodynamics Beyond Molecules Statistical Therm.pdf:/Users/bert/Zotero/storage/K65B2LK2/Matsoukas - 2019 - Thermodynamics Beyond Molecules Statistical Therm.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RYMIAAAD/htm.html:text/html},
}

@thesis{parr_computational_2019,
	title = {The computational neurology of active vision},
	rights = {open},
	url = {https://discovery.ucl.ac.uk/id/eprint/10084391/},
	abstract = {In this thesis, we appeal to recent developments in theoretical neurobiology – namely, active inference – to understand the active visual system and its disorders. Chapter 1 reviews the neurobiology of active vision. This introduces some of the key conceptual themes around attention and inference that recur through subsequent chapters. Chapter 2 provides a technical overview of active inference, and its interpretation in terms of message passing between populations of neurons. Chapter 3 applies the material in Chapter 2 to provide a computational characterisation of the oculomotor system. This deals with two key challenges in active vision: deciding where to look, and working out how to look there. The homology between this message passing and the brain networks solving these inference problems provide a basis for in silico lesion experiments, and an account of the aberrant neural computations that give rise to clinical oculomotor signs (including internuclear ophthalmoplegia). Chapter 4 picks up on the role of uncertainty resolution in deciding where to look, and examines the role of beliefs about the quality (or precision) of data in perceptual inference. We illustrate how abnormal prior beliefs influence inferences about uncertainty and give rise to neuromodulatory changes and visual hallucinatory phenomena (of the sort associated with synucleinopathies). We then demonstrate how synthetic pharmacological perturbations that alter these neuromodulatory systems give rise to the oculomotor changes associated with drugs acting upon these systems. Chapter 5 develops a model of visual neglect, using an oculomotor version of a line cancellation task. We then test a prediction of this model using magnetoencephalography and dynamic causal modelling. Chapter 6 concludes by situating the work in this thesis in the context of computational neurology. This illustrates how the variational principles used here to characterise the active visual system may be generalised to other sensorimotor systems and their disorders.},
	pagetotal = {223},
	institution = {{UCL} (University College London)},
	type = {Doctoral},
	author = {Parr, Thomas},
	urldate = {2020-06-29},
	date = {2019-10-28},
	note = {Conference Name: {UCL} (University College London)
Meeting Name: {UCL} (University College London)
Pages: 1-223
Publication Title: Doctoral thesis, {UCL} (University College London).},
	file = {Parr - 2019 - The computational neurology of active vision.pdf:/Users/bert/Zotero/storage/YGSTY7ZN/Parr - 2019 - The computational neurology of active vision.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BRRKWRDH/10084391.html:text/html},
}

@article{millidge_relationship_2020,
	title = {On the Relationship Between Active Inference and Control as Inference},
	url = {http://arxiv.org/abs/2006.12964},
	abstract = {Active Inference ({AIF}) is an emerging framework in the brain sciences which suggests that biological agents act to minimise a variational bound on model evidence. Control-as-Inference ({CAI}) is a framework within reinforcement learning which casts decision making as a variational inference problem. While these frameworks both consider action selection through the lens of variational inference, their relationship remains unclear. Here, we provide a formal comparison between them and demonstrate that the primary difference arises from how value is incorporated into their respective generative models. In the context of this comparison, we highlight several ways in which these frameworks can inform one another.},
	journaltitle = {{arXiv}:2006.12964 [cs, stat]},
	author = {Millidge, Beren and Tschantz, Alexander and Seth, Anil K. and Buckley, Christopher L.},
	urldate = {2020-06-27},
	date = {2020-06-24},
	eprinttype = {arxiv},
	eprint = {2006.12964},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9EIWPPDT/2006.html:text/html;Millidge et al. - 2020 - On the Relationship Between Active Inference and C.pdf:/Users/bert/Zotero/storage/AFKA5XYR/Millidge et al. - 2020 - On the Relationship Between Active Inference and C.pdf:application/pdf},
}

@article{von_melchner_visual_2000,
	title = {Visual behaviour mediated by retinal projections directed to the auditory pathway},
	volume = {404},
	rights = {2000 Macmillan Magazines Ltd.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/35009102/},
	doi = {10.1038/35009102},
	abstract = {An unresolved issue in cortical development concerns the relative contributions of intrinsic and extrinsic factors to the functional specification of different cortical areas1,2,3,4. Ferrets in which retinal projections are redirected neonatally to the auditory thalamus5 have visually responsive cells in auditory thalamus and cortex, form a retinotopic map in auditory cortex and have visual receptive field properties in auditory cortex that are typical of cells in visual cortex5,6,7,8. Here we report that this cross-modal projection and its representation in auditory cortex can mediate visual behaviour. When light stimuli are presented in the portion of the visual field that is ‘seen’ only by this projection, ‘rewired’ ferrets respond as though they perceive the stimuli to be visual rather than auditory. Thus the perceptual modality of a neocortical region is instructed to a significant extent by its extrinsic inputs. In addition, gratings of different spatial frequencies can be discriminated by the rewired pathway, although the grating acuity is lower than that of the normal visual pathway.},
	pages = {871--876},
	number = {6780},
	journaltitle = {Nature},
	author = {von Melchner, Laurie and Pallas, Sarah L. and Sur, Mriganka},
	urldate = {2020-06-24},
	date = {2000-04},
	langid = {english},
	note = {Number: 6780
Publisher: Nature Publishing Group},
	file = {Snapshot:/Users/bert/Zotero/storage/HPGNJXYR/35009102.html:text/html},
}

@article{friston_representation_2020,
	title = {Representation and agency},
	volume = {43},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/representation-and-agency/B51F4672D473120AC38D9EFBB2704F64/core-reader},
	doi = {10.1017/S0140525X19002929},
	abstract = {Gilead et al. raise some fascinating issues about representational substrates and structures in the predictive brain. This commentary drills down on a core theme in their arguments; namely, the structure of models that generate predictions. In particular, it highlights their factorial nature – both in terms of deep hierarchies over levels of abstraction and, crucially, time – and how this underwrites agency.},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Friston, Karl},
	urldate = {2020-06-23},
	date = {2020},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	file = {Snapshot:/Users/bert/Zotero/storage/TYQPHGW2/B51F4672D473120AC38D9EFBB2704F64.html:text/html},
}

@article{seifert_stochastic_2008,
	title = {Stochastic thermodynamics: Principles and perspectives},
	volume = {64},
	issn = {1434-6028, 1434-6036},
	url = {http://arxiv.org/abs/0710.1187},
	doi = {10.1140/epjb/e2008-00001-9},
	shorttitle = {Stochastic thermodynamics},
	abstract = {Stochastic thermodynamics provides a framework for describing small systems like colloids or biomolecules driven out of equilibrium but still in contact with a heat bath. Both, a first-law like energy balance involving exchanged heat and entropy production entering refinements of the second law can consistently be defined along single stochastic trajectories. Various exact relations involving the distribution of such quantities like integral and detailed fluctuation theorems for total entropy production and the Jarzynski relation follow from such an approach based on Langevin dynamics. Analogues of these relations can be proven for any system obeying a stochastic master equation like, in particular, (bio)chemically driven enzyms or whole reaction networks. The perspective of investigating such relations for stochastic field equations like the Kardar-Parisi-Zhang equation is sketched as well.},
	pages = {423--431},
	number = {3},
	journaltitle = {The European Physical Journal B},
	shortjournal = {Eur. Phys. J. B},
	author = {Seifert, Udo},
	urldate = {2020-06-20},
	date = {2008-08},
	eprinttype = {arxiv},
	eprint = {0710.1187},
	keywords = {Condensed Matter - Statistical Mechanics, Condensed Matter - Soft Condensed Matter},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/R47KHVFG/0710.html:text/html;Seifert - 2008 - Stochastic thermodynamics Principles and perspect.pdf:/Users/bert/Zotero/storage/P9HEA3KT/Seifert - 2008 - Stochastic thermodynamics Principles and perspect.pdf:application/pdf},
}

@article{_lyapunov_nodate,
	title = {Lyapunov function as potential function：A dynamical equivalence},
	volume = {23},
	issn = {1674-1056},
	url = {http://cpb.iphy.ac.cn/CN/abstract/abstract57497.shtml},
	doi = {10.1088/1674-1056/23/1/010505},
	pages = {10505--010505},
	number = {1},
	journaltitle = {中国物理B},
	shortjournal = {中国物理B},
	author = {袁若石, 马易安 and Yuan Ruo-Shi, Ma Yi-An},
	urldate = {2020-06-20},
	file = {Snapshot:/Users/bert/Zotero/storage/9CZXR9RS/abstract57497.html:text/html;袁若石 and Yuan Ruo-Shi - Lyapunov function as potential function：A dynamica.pdf:/Users/bert/Zotero/storage/7YDY4UG8/袁若石 and Yuan Ruo-Shi - Lyapunov function as potential function：A dynamica.pdf:application/pdf},
}

@article{kopitkov_no_2017,
	title = {No belief propagation required: Belief space planning in high-dimensional state spaces via factor graphs, the matrix determinant lemma, and re-use of calculation:},
	url = {https://journals-sagepub-com.dianus.libr.tue.nl/doi/10.1177/0278364917721629},
	doi = {10.1177/0278364917721629},
	shorttitle = {No belief propagation required},
	abstract = {We develop a computationally efficient approach for evaluating the information-theoretic term within belief space planning ({BSP}), where during belief propagatio...},
	journaltitle = {The International Journal of Robotics Research},
	author = {Kopitkov, Dmitry and Indelman, Vadim},
	urldate = {2020-06-13},
	date = {2017-08-10},
	langid = {english},
	note = {Publisher: {SAGE} {PublicationsSage} {UK}: London, England},
	file = {Kopitkov and Indelman - 2017 - No belief propagation required Belief space plann.pdf:/Users/bert/Zotero/storage/CZ6IIN38/Kopitkov and Indelman - 2017 - No belief propagation required Belief space plann.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/EMWQTNSC/0278364917721629.html:text/html},
}

@article{medved_understanding_2020,
	title = {Understanding Fluid Dynamics from Langevin and Fokker–Planck Equations},
	volume = {5},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2311-5521/5/1/40},
	doi = {10.3390/fluids5010040},
	abstract = {The Langevin equations ({LE}) and the Fokker\&ndash;Planck ({FP}) equations are widely used to describe fluid behavior based on coarse-grained approximations of microstructure evolution. In this manuscript, we describe the relation between {LE} and {FP} as related to particle motion within a fluid. The manuscript introduces undergraduate students to two {LEs}, their corresponding {FP} equations, and their solutions and physical interpretation.},
	pages = {40},
	number = {1},
	journaltitle = {Fluids},
	author = {Medved, Andrei and Davis, Riley and Vasquez, Paula A.},
	urldate = {2020-06-11},
	date = {2020-03},
	langid = {english},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {fluctuation dissipation, Fokker–Planck, Langevin, Matlab {GUI}, microrheology, mobility, Stokes–Einstein relation},
	file = {Medved et al. - 2020 - Understanding Fluid Dynamics from Langevin and Fok.pdf:/Users/bert/Zotero/storage/2NSD56YB/Medved et al. - 2020 - Understanding Fluid Dynamics from Langevin and Fok.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/4ZDWP8WK/htm.html:text/html},
}

@article{verrelst_bayesian_1998,
	title = {Bayesian Learning and the Fokker-Planck Machine},
	abstract = {In this paper we discuss the links between simulated annealing, Langevin optimisation, Markov Chain Monte Carlo methods and ensemble learning on the one hand and the Fokker-Planck machine on the other hand. The {FP} Machine was introduced as an alternative method for global optimization. Where most optimisation methods follow point-trajectories through search space (either deterministic or stochastic) , the {FP} Machine explicitly describes the density evolution of Markov processes. We will show that if the cost function that is inserted in to {FP} Machine, is derived from the Bayesian posterior distribution, the {FP} Machine can be seen as an alternative and extended method for ensemble learning, with its use in Bayesian marginalisation and global optimisation. I. Introduction A lot of attention has gone to Bayesian approaches to the training of neural networks over the last years. Thinking in terms of a (posterior) distribution p({wjD}) (based on data D and prior knowledge) over the possible...},
	author = {Verrelst, Herman and Suykens, Johan and J, Ewalle and De Moor, Bart},
	date = {1998-10-08},
	file = {Verrelst et al. - 1998 - Bayesian Learning and the Fokker-Planck Machine.pdf:/Users/bert/Zotero/storage/NNQ5I5V3/Verrelst et al. - 1998 - Bayesian Learning and the Fokker-Planck Machine.pdf:application/pdf},
}

@online{friston_free_2011,
	title = {Free Energy, Value, and Attractors},
	url = {https://www.hindawi.com/journals/cmmm/2012/937860/},
	abstract = {It has been suggested recently that action and perception can be understood as minimising the free energy of sensory samples. This ensures that agents sample the environment to maximise the evidence for their model of the world, such that exchanges with the environment are predictable and adaptive. However, the free energy account does not invoke reward or cost-functions from reinforcement-learning and optimal control theory. We therefore ask whether reward is necessary to explain adaptive behaviour. The free energy formulation uses ideas from statistical physics to explain action in terms of minimising sensory surprise. Conversely, reinforcement-learning has its roots in behaviourism and engineering and assumes that agents optimise a policy to maximise future reward. This paper tries to connect the two formulations and concludes that optimal policies correspond to empirical priors on the trajectories of hidden environmental states, which compel agents to seek out the (valuable) states they expect to encounter.},
	titleaddon = {Computational and Mathematical Methods in Medicine},
	type = {Research Article},
	author = {Friston, Karl and Ao, Ping},
	urldate = {2020-06-11},
	date = {2011-12-21},
	langid = {english},
	doi = {https://doi.org/10.1155/2012/937860},
	doi = {https://doi.org/10.1155/2012/937860},
	note = {{ISSN}: 1748-670X
Library Catalog: www.hindawi.com
Pages: e937860
Publisher: Hindawi
Volume: 2012},
	file = {Friston and Ao - 2011 - Free Energy, Value, and Attractors.pdf:/Users/bert/Zotero/storage/TTKLV8DE/Friston and Ao - 2011 - Free Energy, Value, and Attractors.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NWR8C6MW/937860.html:text/html},
}

@article{veissiere_thinking_2020,
	title = {Thinking through other minds: A variational approach to cognition and culture},
	volume = {43},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/thinking-through-other-minds-a-variational-approach-to-cognition-and-culture/9A10399BA85F428D5943DD847092C14A},
	doi = {10.1017/S0140525X19001213},
	shorttitle = {Thinking through other minds},
	abstract = {The processes underwriting the acquisition of culture remain unclear. How are shared habits, norms, and expectations learned and maintained with precision and reliability across large-scale sociocultural ensembles? Is there a unifying account of the mechanisms involved in the acquisition of culture? Notions such as “shared expectations,” the “selective patterning of attention and behaviour,” “cultural evolution,” “cultural inheritance,” and “implicit learning” are the main candidates to underpin a unifying account of cognition and the acquisition of culture; however, their interactions require greater specification and clarification. In this article, we integrate these candidates using the variational (free-energy) approach to human cognition and culture in theoretical neuroscience. We describe the construction by humans of social niches that afford epistemic resources called cultural affordances. We argue that human agents learn the shared habits, norms, and expectations of their culture through immersive participation in patterned cultural practices that selectively pattern attention and behaviour. We call this process “thinking through other minds” ({TTOM}) – in effect, the process of inferring other agents’ expectations about the world and how to behave in social context. We argue that for humans, information from and about other people's expectations constitutes the primary domain of statistical regularities that humans leverage to predict and organize behaviour. The integrative model we offer has implications that can advance theories of cognition, enculturation, adaptation, and psychopathology. Crucially, this formal (variational) treatment seeks to resolve key debates in current cognitive science, such as the distinction between internalist and externalist accounts of theory of mind abilities and the more fundamental distinction between dynamical and representational accounts of enactivism.},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Veissière, Samuel P. L. and Constant, Axel and Ramstead, Maxwell J. D. and Friston, Karl J. and Kirmayer, Laurence J.},
	urldate = {2020-06-03},
	date = {2020},
	langid = {english},
	note = {Publisher: Cambridge University Press},
	keywords = {embodiment, Cognition and culture, cultural affordances, cultural co-evolution, cultural learning, enactment, enculturation, epistemic affordances, niche construction, social learning, variational free-energy principle},
	file = {Snapshot:/Users/bert/Zotero/storage/P5AFERCM/9A10399BA85F428D5943DD847092C14A.html:text/html;Veissière et al. - 2020 - Thinking through other minds A variational approa.pdf:/Users/bert/Zotero/storage/MRGJ3EVH/Veissière et al. - 2020 - Thinking through other minds A variational approa.pdf:application/pdf},
}

@article{lynn_abstract_2020,
	title = {Abstract representations of events arise from mental errors in learning and memory},
	url = {http://arxiv.org/abs/1805.12491},
	abstract = {Humans are adept at uncovering abstract associations in the world around them, yet the underlying mechanisms remain poorly understood. Intuitively, learning the higher-order structure of statistical relationships should involve complex mental processes. Here we propose an alternative perspective: that higher-order associations instead arise from natural errors in learning and memory. Combining ideas from information theory and reinforcement learning, we derive a maximum entropy (or minimum complexity) model of people's internal representations of the transitions between stimuli. Importantly, our model (i) affords a concise analytic form, (ii) qualitatively explains the effects of transition network structure on human expectations, and (iii) quantitatively predicts human reaction times in probabilistic sequential motor tasks. Together, these results suggest that mental errors influence our abstract representations of the world in significant and predictable ways, with direct implications for the study and design of optimally learnable information sources.},
	journaltitle = {{arXiv}:1805.12491 [physics, q-bio]},
	author = {Lynn, Christopher W. and Kahn, Ari E. and Nyema, Nathaniel and Bassett, Danielle S.},
	urldate = {2020-05-29},
	date = {2020-03-25},
	eprinttype = {arxiv},
	eprint = {1805.12491},
	keywords = {Quantitative Biology - Neurons and Cognition, Physics - Biological Physics, Physics - Physics and Society},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5NDJZQB4/1805.html:text/html;Lynn et al. - 2020 - Abstract representations of events arise from ment.pdf:/Users/bert/Zotero/storage/3DDZZWQH/Lynn et al. - 2020 - Abstract representations of events arise from ment.pdf:application/pdf},
}

@article{ma_complete_2015,
	title = {A Complete Recipe for Stochastic Gradient {MCMC}},
	url = {http://arxiv.org/abs/1506.04696},
	abstract = {Many recent Markov chain Monte Carlo ({MCMC}) samplers leverage continuous dynamics to define a transition kernel that efficiently explores a target distribution. In tandem, a focus has been on devising scalable variants that subsample the data and use stochastic gradients in place of full-data gradients in the dynamic simulations. However, such stochastic gradient {MCMC} samplers have lagged behind their full-data counterparts in terms of the complexity of dynamics considered since proving convergence in the presence of the stochastic gradient noise is non-trivial. Even with simple dynamics, significant physical intuition is often required to modify the dynamical system to account for the stochastic gradient noise. In this paper, we provide a general recipe for constructing {MCMC} samplers--including stochastic gradient versions--based on continuous Markov processes specified via two matrices. We constructively prove that the framework is complete. That is, any continuous Markov process that provides samples from the target distribution can be written in our framework. We show how previous continuous-dynamic samplers can be trivially "reinvented" in our framework, avoiding the complicated sampler-specific proofs. We likewise use our recipe to straightforwardly propose a new state-adaptive sampler: stochastic gradient Riemann Hamiltonian Monte Carlo ({SGRHMC}). Our experiments on simulated data and a streaming Wikipedia analysis demonstrate that the proposed {SGRHMC} sampler inherits the benefits of Riemann {HMC}, with the scalability of stochastic gradient methods.},
	journaltitle = {{arXiv}:1506.04696 [math, stat]},
	author = {Ma, Yi-An and Chen, Tianqi and Fox, Emily B.},
	urldate = {2020-05-28},
	date = {2015-10-31},
	eprinttype = {arxiv},
	eprint = {1506.04696},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/KKPXATQJ/1506.html:text/html;Ma et al. - 2015 - A Complete Recipe for Stochastic Gradient MCMC.pdf:/Users/bert/Zotero/storage/25PXWRI9/Ma et al. - 2015 - A Complete Recipe for Stochastic Gradient MCMC.pdf:application/pdf},
}

@incollection{levchuk_chapter_2019-1,
	title = {Chapter 4 - Active Inference in Multiagent Systems: Context-Driven Collaboration and Decentralized Purpose-Driven Team Adaptation},
	isbn = {978-0-12-817636-8},
	url = {http://www.sciencedirect.com/science/article/pii/B9780128176368000041},
	shorttitle = {Chapter 4 - Active Inference in Multiagent Systems},
	abstract = {The Internet of things ({IoT}), from heart monitoring implants to home-heating control systems, is becoming integral to our daily lives. We expect the technologies that comprise {IoT} to become smarter; to autonomously reason, act, and communicate with other entities in the environment; and to achieve shared goals. To realize the full potential of these systems, we must understand the mechanisms that allow multiple intelligent entities to effectively operate, collaborate, and learn in changing and uncertain environments. The future {IoT} devices must not only maintain enough intelligence to perceive and act locally, but also possess team-level collaboration and adaptation processes. We posit that such processes embody energy-minimizing mechanisms found in all biological and physical systems, and operate over the objectives and constraints that can be defined and analyzed locally by individual devices without the need for global centralized control. In this chapter, we represent multiple {IoT} devices as a team of intelligent agents, and postulate that multiagent systems achieve adaptive behaviors by minimizing a team’s free energy, which decomposes into distributed iterative perception (inference) and control (action) processes. First, we discuss instantiation of this mechanism for a joint distributed decision-making problem. Next, we present experimental evidence that energy-based teams outperform utility-based teams. Finally, we discuss different learning processes that support team-level adaptation.},
	pages = {67--85},
	booktitle = {Artificial Intelligence for the Internet of Everything},
	publisher = {Academic Press},
	author = {Levchuk, Georgiy and Pattipati, Krishna and Serfaty, Daniel and Fouse, Adam and {McCormack}, Robert},
	editor = {Lawless, William and Mittu, Ranjeev and Sofge, Donald and Moskowitz, Ira S. and Russell, Stephen},
	urldate = {2020-05-26},
	date = {2019-01-01},
	langid = {english},
	doi = {10.1016/B978-0-12-817636-8.00004-1},
	keywords = {Free energy, Constraints, Humanmachine teams, Internet of Things ({IoT}), Multiple intelligent entities, Organizational structure, Perceptual control, Team adaptation, Variational inference},
	file = {Levchuk et al. - 2019 - Chapter 4 - Active Inference in Multiagent Systems.pdf:/Users/bert/Zotero/storage/5MWTHRF6/Levchuk et al. - 2019 - Chapter 4 - Active Inference in Multiagent Systems.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/BTXI426G/B9780128176368000041.html:text/html},
}

@article{khan_fast_2018,
	title = {Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam},
	url = {http://arxiv.org/abs/1806.04854},
	abstract = {Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference ({VI}) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field {VI}. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing {VI} methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.},
	journaltitle = {{arXiv}:1806.04854 [cs, stat]},
	author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
	urldate = {2020-05-23},
	date = {2018-08-02},
	eprinttype = {arxiv},
	eprint = {1806.04854},
	keywords = {Statistics - Computation, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UKPZ8SAN/1806.html:text/html;Khan et al. - 2018 - Fast and Scalable Bayesian Deep Learning by Weight.pdf:/Users/bert/Zotero/storage/3VFP9IAP/Khan et al. - 2018 - Fast and Scalable Bayesian Deep Learning by Weight.pdf:application/pdf},
}

@article{matsumoto_goal-directed_2020,
	title = {Goal-Directed Planning for Habituated Agents by Active Inference Using a Variational Recurrent Neural Network},
	volume = {22},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/5/564},
	doi = {10.3390/e22050564},
	abstract = {It is crucial to ask how agents can achieve goals by generating action plans using only partial models of the world acquired through habituated sensory-motor experiences. Although many existing robotics studies use a forward model framework, there are generalization issues with high degrees of freedom. The current study shows that the predictive coding ({PC}) and active inference ({AIF}) frameworks, which employ a generative model, can develop better generalization by learning a prior distribution in a low dimensional latent state space representing probabilistic structures extracted from well habituated sensory-motor trajectories. In our proposed model, learning is carried out by inferring optimal latent variables as well as synaptic weights for maximizing the evidence lower bound, while goal-directed planning is accomplished by inferring latent variables for maximizing the estimated lower bound. Our proposed model was evaluated with both simple and complex robotic tasks in simulation, which demonstrated sufficient generalization in learning with limited training data by setting an intermediate value for a regularization coefficient. Furthermore, comparative simulation results show that the proposed model outperforms a conventional forward model in goal-directed planning, due to the learned prior confining the search of motor plans within the range of habituated trajectories.},
	pages = {564},
	number = {5},
	journaltitle = {Entropy},
	author = {Matsumoto, Takazumi and Tani, Jun},
	urldate = {2020-05-23},
	date = {2020-05},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {active inference, predictive coding, goal directed planning, recurrent neural network, variational bayes},
	file = {Matsumoto and Tani - 2020 - Goal-Directed Planning for Habituated Agents by Ac.pdf:/Users/bert/Zotero/storage/KFNNBZIA/Matsumoto and Tani - 2020 - Goal-Directed Planning for Habituated Agents by Ac.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/8QT56ENF/564.html:text/html},
}

@article{khan_fast_2018-1,
	title = {Fast yet Simple Natural-Gradient Descent for Variational Inference in Complex Models},
	url = {http://arxiv.org/abs/1807.04489},
	abstract = {Bayesian inference plays an important role in advancing machine learning, but faces computational challenges when applied to complex models such as deep neural networks. Variational inference circumvents these challenges by formulating Bayesian inference as an optimization problem and solving it using gradient-based optimization. In this paper, we argue in favor of natural-gradient approaches which, unlike their gradient-based counterparts, can improve convergence by exploiting the information geometry of the solutions. We show how to derive fast yet simple natural-gradient updates by using a duality associated with exponential-family distributions. An attractive feature of these methods is that, by using natural-gradients, they are able to extract accurate local approximations for individual model components. We summarize recent results for Bayesian deep learning showing the superiority of natural-gradient approaches over their gradient counterparts.},
	journaltitle = {{arXiv}:1807.04489 [cs, math, stat]},
	author = {Khan, Mohammad Emtiyaz and Nielsen, Didrik},
	urldate = {2020-05-23},
	date = {2018-08-02},
	eprinttype = {arxiv},
	eprint = {1807.04489},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/VYIGZA3W/1807.html:text/html;Khan and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for Varia.pdf:/Users/bert/Zotero/storage/5JJLEBEU/Khan and Nielsen - 2018 - Fast yet Simple Natural-Gradient Descent for Varia.pdf:application/pdf},
}

@inproceedings{sojoudi_mathematical_2019,
	title = {Mathematical Models of Physiological Responses to Exercise},
	doi = {10.23919/ACC.2019.8814946},
	abstract = {This paper develops empirical mathematical models for physiological responses to exercise. We first find single-input single-output models describing heart rate variability, ventilation, oxygen consumption and carbon dioxide production in response to workload changes and then identify a single-input multi-output model from workload to these physiological variabilities. We also investigate the possibility of the existence of a universal model for physiological variability in different individuals during treadmill running. Simulations based on real data substantiate that the obtained models accurately capture the physiological responses to workload variations. In particular, it is observed that (i) different physiological responses to exercise can be captured by low-order linear or mildly nonlinear models; and (ii) there may exist a universal model for oxygen consumption that works for different individuals.},
	eventtitle = {2019 American Control Conference ({ACC})},
	pages = {1525--1532},
	booktitle = {2019 American Control Conference ({ACC})},
	author = {Sojoudi, Somayeh and Recht, Benjamin and Doyle, John C.},
	date = {2019-07},
	note = {{ISSN}: 2378-5861},
	keywords = {medical signal processing, physiological models, bioelectric potentials, carbon dioxide production, cardiovascular system, electrocardiography, empirical mathematical models, heart rate variability, low-order linear models, mathematical analysis, mildly nonlinear models, oxygen consumption, physiological variability, single-input multi-output model, single-input single-output models},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/66MFQPN2/8814946.html:text/html;Sojoudi et al. - 2019 - Mathematical Models of Physiological Responses to .pdf:/Users/bert/Zotero/storage/G8PFEUEM/Sojoudi et al. - 2019 - Mathematical Models of Physiological Responses to .pdf:application/pdf},
}

@thesis{ergul_real-world_2020,
	title = {A Real-World Implementation of Active Inference},
	institution = {Eindhoven University of Technology},
	type = {{MSc} thesis},
	author = {Ergul, Burak},
	date = {2020-04},
}

@article{cox_factor_2019,
	title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
	volume = {104},
	issn = {0888-613X},
	url = {http://www.sciencedirect.com/science/article/pii/S0888613X18304298},
	doi = {10.1016/j.ijar.2018.11.002},
	abstract = {The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs ({FFGs}), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed “{ForneyLab}”2 as a Julia toolbox for message passing-based inference in {FFGs}. We show by example how {ForneyLab} enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the {FFG} framework, both the model specification and inference methods are readily extensible in {ForneyLab}. In order to test this framework, we compared variational message passing as implemented by {ForneyLab} with automatic differentiation variational inference ({ADVI}) and Monte Carlo methods as implemented by state-of-the-art tools “Edward” and “Stan”. In terms of performance, extensibility and stability issues, {ForneyLab} appears to enjoy an edge relative to its competitors for automated inference in state-space models.},
	pages = {185--204},
	journaltitle = {International Journal of Approximate Reasoning},
	shortjournal = {International Journal of Approximate Reasoning},
	author = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
	urldate = {2018-11-16},
	date = {2019-01-01},
	keywords = {Bayesian inference, Message passing, Factor graphs, Julia, Probabilistic programming},
	file = {Cox & van de Laar e.a. - 2019 - A factor graph approach to automated design of Bay.pdf:/Users/bert/Zotero/storage/THHZ5RTY/Cox & van de Laar e.a. - 2019 - A factor graph approach to automated design of Bay.pdf:application/pdf},
}

@article{kobyzev_normalizing_2020-1,
	title = {Normalizing Flows: An Introduction and Review of Current Methods},
	url = {http://arxiv.org/abs/1908.09257},
	shorttitle = {Normalizing Flows},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	journaltitle = {{arXiv}:1908.09257 [cs, stat]},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	urldate = {2020-05-21},
	date = {2020-04-27},
	eprinttype = {arxiv},
	eprint = {1908.09257},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4C48M4RX/1908.html:text/html;Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:/Users/bert/Zotero/storage/4I3KH77L/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf},
}

@article{lin_fast_2019,
	title = {Fast and Simple Natural-Gradient Variational Inference with Mixture of Exponential-family Approximations},
	url = {http://arxiv.org/abs/1906.02914},
	abstract = {Natural-gradient methods enable fast and simple algorithms for variational inference, but due to computational difficulties, their use is mostly limited to {\textbackslash}emph\{minimal\} exponential-family ({EF}) approximations. In this paper, we extend their application to estimate {\textbackslash}emph\{structured\} approximations such as mixtures of {EF} distributions. Such approximations can fit complex, multimodal posterior distributions and are generally more accurate than unimodal {EF} approximations. By using a {\textbackslash}emph\{minimal conditional-{EF}\} representation of such approximations, we derive simple natural-gradient updates. Our empirical results demonstrate a faster convergence of our natural-gradient method compared to black-box gradient-based methods with reparameterization gradients. Our work expands the scope of natural gradients for Bayesian inference and makes them more widely applicable than before.},
	journaltitle = {{arXiv}:1906.02914 [cs, stat]},
	author = {Lin, Wu and Khan, Mohammad Emtiyaz and Schmidt, Mark},
	urldate = {2020-05-21},
	date = {2019-10-30},
	eprinttype = {arxiv},
	eprint = {1906.02914},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/486WT65U/1906.html:text/html;Lin et al. - 2019 - Fast and Simple Natural-Gradient Variational Infer.pdf:/Users/bert/Zotero/storage/3IKPWV3M/Lin et al. - 2019 - Fast and Simple Natural-Gradient Variational Infer.pdf:application/pdf},
}

@article{parr_modules_2020,
	title = {Modules or Mean-Fields?},
	volume = {22},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/5/552},
	doi = {10.3390/e22050552},
	abstract = {The segregation of neural processing into distinct streams has been interpreted by someas evidence in favour of a modular view of brain function. This implies a set of specialised \&lsquo;modules\&rsquo;,each of which performs a specific kind of computation in isolation of other brain systems, beforesharing the result of this operation with other modules. In light of a modern understanding ofstochastic non-equilibrium systems, like the brain, a simpler and more parsimonious explanationpresents itself. Formulating the evolution of a non-equilibrium steady state system in terms of itsdensity dynamics reveals that such systems appear on average to perform a gradient ascent on theirsteady state density. If this steady state implies a sufficiently sparse conditional independencystructure, this endorses a mean-field dynamical formulation. This decomposes the density over allstates in a system into the product of marginal probabilities for those states. This factorisation lendsthe system a modular appearance, in the sense that we can interpret the dynamics of each factorindependently. However, the argument here is that it is factorisation, as opposed to modularisation,that gives rise to the functional anatomy of the brain or, indeed, any sentient system. In thefollowing, we briefly overview mean-field theory and its applications to stochastic dynamicalsystems. We then unpack the consequences of this factorisation through simple numericalsimulations and highlight the implications for neuronal message passing and the computationalarchitecture of sentience},
	pages = {552},
	number = {5},
	journaltitle = {Entropy},
	author = {Parr, Thomas and Sajid, Noor and Friston, Karl J.},
	urldate = {2020-05-19},
	date = {2020-05},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {message passing, Bayesian mechanics, density dynamics, modularity, stochastic dynamics},
	file = {Parr et al. - 2020 - Modules or Mean-Fields.pdf:/Users/bert/Zotero/storage/82Q63IZT/Parr et al. - 2020 - Modules or Mean-Fields.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/QKG9RHSA/552.html:text/html},
}

@article{ambrogioni_automatic_2020,
	title = {Automatic structured variational inference},
	url = {http://arxiv.org/abs/2002.00643},
	abstract = {The aim of probabilistic programming is to automatize every aspect of probabilistic inference in arbitrary probabilistic models (programs) so that the user can focus her attention on modeling, without dealing with ad-hoc inference methods. Gradient based automatic differentiation stochastic variational inference offers an attractive option as the default method for (differentiable) probabilistic programming as it combines high performance with high computational efficiency. However, the performance of any (parametric) variational approach depends on the choice of an appropriate variational family. Here, we introduced a fully automatic method for constructing structured variational families inspired to the closed-form update in conjugate models. These pseudo-conjugate families incorporate the forward pass of the input probabilistic program and can capture complex statistical dependencies. Pseudo-conjugate families have the same space and time complexity of the input probabilistic program and are therefore tractable in a very large class of models. We validate our automatic variational method on a wide range of high dimensional inference problems including deep learning components.},
	journaltitle = {{arXiv}:2002.00643 [cs, stat]},
	author = {Ambrogioni, Luca and Hinne, Max and van Gerven, Marcel},
	urldate = {2020-05-18},
	date = {2020-02-03},
	eprinttype = {arxiv},
	eprint = {2002.00643},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ambrogioni et al. - 2020 - Automatic structured variational inference.pdf:/Users/bert/Zotero/storage/SIMIWDS8/Ambrogioni et al. - 2020 - Automatic structured variational inference.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/FGSMGGQS/2002.html:text/html},
}

@article{parr_inferring_2020,
	title = {Inferring What to Do (And What Not to)},
	volume = {22},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/5/536},
	doi = {10.3390/e22050536},
	abstract = {In recent years, the \&ldquo;planning as inference\&rdquo; paradigm has become central to the study of behaviour. The advance offered by this is the formalisation of motivation as a prior belief about \&ldquo;how I am going to act\&rdquo;. This paper provides an overview of the factors that contribute to this prior. These are rooted in optimal experimental design, information theory, and statistical decision making. We unpack how these factors imply a functional architecture for motivated behaviour. This raises an important question: how can we put this architecture to work in the service of understanding observed neurobiological structure? To answer this question, we draw from established techniques in experimental studies of behaviour. Typically, these examine the influence of perturbations of the nervous system\&mdash;which include pathological insults or optogenetic manipulations\&mdash;to see their influence on behaviour. Here, we argue that the message passing that emerges from inferring what to do can be similarly perturbed. If a given perturbation elicits the same behaviours as a focal brain lesion, this provides a functional interpretation of empirical findings and an anatomical grounding for theoretical results. We highlight examples of this approach that influence different sorts of goal-directed behaviour, active learning, and decision making. Finally, we summarise their implications for the neuroanatomy of inferring what to do (and what not to).},
	pages = {536},
	number = {5},
	journaltitle = {Entropy},
	author = {Parr, Thomas},
	urldate = {2020-05-17},
	date = {2020-05},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {active inference, Bayesian, message passing, inference, anatomy, experimental design, information gain},
	file = {Parr - 2020 - Inferring What to Do (And What Not to).pdf:/Users/bert/Zotero/storage/KY7G9HRX/Parr - 2020 - Inferring What to Do (And What Not to).pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/GW9TA4QV/536.html:text/html},
}

@article{dauce_toward_2018,
	title = {Toward predictive machine learning for active vision},
	url = {http://arxiv.org/abs/1710.10460},
	abstract = {We develop a comprehensive description of the active inference framework, as proposed by Friston (2010), under a machine-learning compliant perspective. Stemming from a biological inspiration and the auto-encoding principles, the sketch of a cognitive architecture is proposed that should provide ways to implement estimation-oriented control policies. Computer simulations illustrate the effectiveness of the approach through a foveated inspection of the input data. The pros and cons of the control policy are analyzed in detail, showing interesting promises in terms of processing compression. Though optimizing future posterior entropy over the actions set is shown enough to attain locally optimal action selection, offline calculation using class-specific saliency maps is shown better for it saves processing costs through saccades pathways pre-processing, with a negligible effect on the recognition/compression rates.},
	journaltitle = {{arXiv}:1710.10460 [cs]},
	author = {Daucé, Emmanuel},
	urldate = {2020-05-17},
	date = {2018-01-08},
	eprinttype = {arxiv},
	eprint = {1710.10460},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/75F2ILM6/1710.html:text/html;Daucé - 2018 - Toward predictive machine learning for active visi.pdf:/Users/bert/Zotero/storage/TA8R6RLZ/Daucé - 2018 - Toward predictive machine learning for active visi.pdf:application/pdf},
}

@article{frank_price_2018,
	title = {The Price equation program: simple invariances unify population dynamics, thermodynamics, probability, information and inference},
	volume = {20},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1810.09262},
	doi = {10.3390/e20120978},
	shorttitle = {The Price equation program},
	abstract = {The fundamental equations of various disciplines often seem to share the same basic structure. Natural selection increases information in the same way that Bayesian updating increases information. Thermodynamics and the forms of common probability distributions express maximum increase in entropy, which appears mathematically as loss of information. Physical mechanics follows paths of change that maximize Fisher information. The information expressions typically have analogous interpretations as the Newtonian balance between force and acceleration, representing a partition between direct causes of change and opposing changes in the frame of reference. This web of vague analogies hints at a deeper common mathematical structure. I suggest that the Price equation expresses that underlying universal structure. The abstract Price equation describes dynamics as the change between two sets. One component of dynamics expresses the change in the frequency of things, holding constant the values associated with things. The other component of dynamics expresses the change in the values of things, holding constant the frequency of things. The separation of frequency from value generalizes Shannon's separation of the frequency of symbols from the meaning of symbols in information theory. The Price equation's generalized separation of frequency and value reveals a few simple invariances that define universal geometric aspects of change. For example, the conservation of total frequency, although a trivial invariance by itself, creates a powerful constraint on the geometry of change. That constraint plus a few others seem to explain the common structural forms of the equations in different disciplines. From that abstract perspective, interpretations such as selection, information, entropy, force, acceleration, and physical work arise from the same underlying geometry expressed by the Price equation.},
	pages = {978},
	number = {12},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Frank, Steven A.},
	urldate = {2020-05-16},
	date = {2018-12-16},
	eprinttype = {arxiv},
	eprint = {1810.09262},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Quantitative Biology - Populations and Evolution},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/HZLFSDSN/1810.html:text/html;Frank - 2018 - The Price equation program simple invariances uni.pdf:/Users/bert/Zotero/storage/MB5EG78Y/Frank - 2018 - The Price equation program simple invariances uni.pdf:application/pdf},
}

@online{noauthor_lewis_nodate,
	title = {Lewis Smith - A gentle introduction to information geometry},
	url = {http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html},
	urldate = {2020-05-13},
	file = {Lewis Smith - A gentle introduction to information geometry:/Users/bert/Zotero/storage/DN93J298/2019-09-27-info-geom.html:text/html},
}

@article{perdigao_debates_2020,
	title = {Debates: Does Information Theory Provide a New Paradigm for Earth Science? Emerging Concepts and Pathways of Information Physics},
	volume = {56},
	rights = {©2020. American Geophysical Union. All Rights Reserved.},
	issn = {1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019WR025270},
	doi = {10.1029/2019WR025270},
	shorttitle = {Debates},
	abstract = {Entropy and Information are key concepts not only in Information Theory but also in Physics: historically in the fields of Thermodynamics, Statistical and Analytical Mechanics, and, more recently, in the field of Information Physics. In this paper we argue that Information Physics reconciles and generalizes statistical, geometric, and mechanistic views on information. We start by demonstrating how the use and interpretation of Entropy and Information coincide in Information Theory, Statistical Thermodynamics, and Analytical Mechanics, and how this can be taken advantage of when addressing Earth Science problems in general and hydrological problems in particular. In the second part we discuss how Information Physics provides ways to quantify Information and Entropy from fundamental physical principles. This extends their use to cases where the preconditions to calculate Entropy in the classical manner as an aggregate statistical measure are not met. Indeed, these preconditions are rarely met in the Earth Sciences due either to limited observations or the far-from-equilibrium nature of evolving systems. Information Physics therefore offers new opportunities for improving the treatment of Earth Science problems.},
	pages = {e2019WR025270},
	number = {2},
	journaltitle = {Water Resources Research},
	author = {Perdigão, Rui A. P. and Ehret, Uwe and Knuth, Kevin H. and Wang, Jingfeng},
	urldate = {2020-05-13},
	date = {2020},
	langid = {english},
	note = {\_eprint: https://agupubs.onlinelibrary.wiley.com/doi/pdf/10.1029/2019WR025270},
	keywords = {entropy, information theory, thermodynamics, complex systems, information physics, kinematic geometry},
	file = {Perdigão et al. - 2020 - Debates Does Information Theory Provide a New Par.pdf:/Users/bert/Zotero/storage/7EEI5QKP/Perdigão et al. - 2020 - Debates Does Information Theory Provide a New Par.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2ESPZMXZ/2019WR025270.html:text/html},
}

@article{dewar_maximum_2009,
	title = {Maximum Entropy Production as an Inference Algorithm that Translates Physical Assumptions into Macroscopic Predictions: Don’t Shoot the Messenger},
	volume = {11},
	doi = {10.3390/e11040931},
	shorttitle = {Maximum Entropy Production as an Inference Algorithm that Translates Physical Assumptions into Macroscopic Predictions},
	abstract = {Is Maximum Entropy Production ({MEP}) a physical principle? In this paper I tentatively suggest it is not, on the basis that {MEP} is equivalent to Jaynes’ Maximum Entropy ({MaxEnt}) inference algorithm that passively translates physical assumptions into macroscopic predictions, as applied to non-equilibrium systems. {MaxEnt} itself has no physical content; disagreement between {MaxEnt} predictions and experiment falsifies the physical assumptions, not {MaxEnt}. While it remains to be shown rigorously that {MEP} is indeed equivalent to {MaxEnt} for systems arbitrarily far from equilibrium, work in progress tentatively supports this conclusion. In terms of its role within non-equilibrium statistical mechanics, {MEP} might then be better understood as Messenger of Essential Physics.},
	journaltitle = {Entropy},
	shortjournal = {Entropy},
	author = {Dewar, Roderick},
	date = {2009-12-01},
	file = {Dewar - 2009 - Maximum Entropy Production as an Inference Algorit.pdf:/Users/bert/Zotero/storage/7J2EEKNS/Dewar - 2009 - Maximum Entropy Production as an Inference Algorit.pdf:application/pdf},
}

@article{friston_sentience_2020,
	title = {Sentience and the Origins of Consciousness: From Cartesian Duality to Markovian Monism},
	volume = {22},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/5/516},
	doi = {10.3390/e22050516},
	shorttitle = {Sentience and the Origins of Consciousness},
	abstract = {This essay addresses Cartesian duality and how its implicit dialectic might be repaired using physics and information theory. Our agenda is to describe a key distinction in the physical sciences that may provide a foundation for the distinction between mind and matter, and between sentient and intentional systems. From this perspective, it becomes tenable to talk about the physics of sentience and \&lsquo;forces\&rsquo; that underwrite our beliefs (in the sense of probability distributions represented by our internal states), which may ground our mental states and consciousness. We will refer to this view as Markovian monism, which entails two claims: (1) fundamentally, there is only one type of thing and only one type of irreducible property (hence monism). (2) All systems possessing a Markov blanket have properties that are relevant for understanding the mind and consciousness: if such systems have mental properties, then they have them partly by virtue of possessing a Markov blanket (hence Markovian). Markovian monism rests upon the information geometry of random dynamic systems. In brief, the information geometry induced in any system\&mdash;whose internal states can be distinguished from external states\&mdash;must acquire a dual aspect. This dual aspect concerns the (intrinsic) information geometry of the probabilistic evolution of internal states and a separate (extrinsic) information geometry of probabilistic beliefs about external states that are parameterised by internal states. We call these intrinsic (i.e., mechanical, or state-based) and extrinsic (i.e., Markovian, or belief-based) information geometries, respectively. Although these mathematical notions may sound complicated, they are fairly straightforward to handle, and may offer a means through which to frame the origins of consciousness.},
	pages = {516},
	number = {5},
	journaltitle = {Entropy},
	author = {Friston, Karl J. and Wiese, Wanja and Hobson, J. Allan},
	urldate = {2020-05-10},
	date = {2020-05},
	langid = {english},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {information geometry, consciousness, Markovian monism},
	file = {Friston et al. - 2020 - Sentience and the Origins of Consciousness From C.pdf:/Users/bert/Zotero/storage/9UHZKZ2U/Friston et al. - 2020 - Sentience and the Origins of Consciousness From C.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/Y7WN7UKV/516.html:text/html},
}

@article{fagerholm_conservation_2020,
	title = {Conservation laws by virtue of scale symmetries in neural systems},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007865},
	doi = {10.1371/journal.pcbi.1007865},
	abstract = {In contrast to the symmetries of translation in space, rotation in space, and translation in time, the known laws of physics are not universally invariant under transformation of scale. However, a special case exists in which the action is scale invariant if it satisfies the following two constraints: 1) it must depend upon a scale-free Lagrangian, and 2) the Lagrangian must change under scale in the same way as the inverse time, 1t. Our contribution lies in the derivation of a generalised Lagrangian, in the form of a power series expansion, that satisfies these constraints. This generalised Lagrangian furnishes a normal form for dynamic causal models–state space models based upon differential equations–that can be used to distinguish scale symmetry from scale freeness in empirical data. We establish face validity with an analysis of simulated data, in which we show how scale symmetry can be identified and how the associated conserved quantities can be estimated in neuronal time series.},
	pages = {e1007865},
	number = {5},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Fagerholm, Erik D. and Foulkes, W. M. C. and Gallero-Salas, Yasir and Helmchen, Fritjof and Friston, Karl J. and Moran, Rosalyn J. and Leech, Robert},
	urldate = {2020-05-09},
	date = {2020-05-04},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Free energy, Calcium imaging, Conservation of energy, Dynamical systems, Equations of motion, Functional magnetic resonance imaging, Macaque, Monkeys},
	file = {Fagerholm et al. - 2020 - Conservation laws by virtue of scale symmetries in.pdf:/Users/bert/Zotero/storage/YLBYEWU5/Fagerholm et al. - 2020 - Conservation laws by virtue of scale symmetries in.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/WN89HXPX/article.html:text/html},
}

@article{al-bashabsheh_normal_2012,
	title = {Normal Factor Graphs as Probabilistic Models},
	url = {http://arxiv.org/abs/1209.3300},
	abstract = {We present a new probabilistic modelling framework based on the recent notion of normal factor graph ({NFG}). We show that the proposed {NFG} models and their transformations unify some existing models such as factor graphs, convolutional factor graphs, and cumulative distribution networks. The two subclasses of the {NFG} models, namely the constrained and generative models, exhibit a duality in their dependence structure. Transformation of {NFG} models further extends the power of this modelling framework. We point out the well-known {NFG} representations of parity and generator realizations of a linear code as generative and constrained models, and comment on a more prevailing duality in this context. Finally, we address the algorithmic aspect of computing the exterior function of {NFGs} and the inference problem on {NFGs}.},
	journaltitle = {{arXiv}:1209.3300 [cs, math]},
	author = {Al-Bashabsheh, Ali and Mao, Yongyi},
	urldate = {2020-05-02},
	date = {2012-09-14},
	eprinttype = {arxiv},
	eprint = {1209.3300},
	keywords = {Computer Science - Information Theory},
	file = {Al-Bashabsheh and Mao - 2012 - Normal Factor Graphs as Probabilistic Models.pdf:/Users/bert/Zotero/storage/FXQR4Z85/Al-Bashabsheh and Mao - 2012 - Normal Factor Graphs as Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/KESFUZMR/1209.html:text/html},
}

@article{gottwald_two_2020,
	title = {The Two Kinds of Free Energy and the Bayesian Revolution},
	url = {http://arxiv.org/abs/2004.11763},
	abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information-processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information-processing of perception.},
	journaltitle = {{arXiv}:2004.11763 [cs, q-bio]},
	author = {Gottwald, Sebastian and Braun, Daniel A.},
	urldate = {2020-04-30},
	date = {2020-04-24},
	eprinttype = {arxiv},
	eprint = {2004.11763},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/ZFXRRDCF/2004.html:text/html;Gottwald and Braun - 2020 - The Two Kinds of Free Energy and the Bayesian Revo.pdf:/Users/bert/Zotero/storage/NPUZABGF/Gottwald and Braun - 2020 - The Two Kinds of Free Energy and the Bayesian Revo.pdf:application/pdf},
}

@article{goekoop_how_2020,
	title = {How higher goals are constructed and collapse under stress: a hierarchical Bayesian control systems perspective},
	url = {http://arxiv.org/abs/2004.09426},
	shorttitle = {How higher goals are constructed and collapse under stress},
	abstract = {In this paper, we aim to integrate findings from the fields of machine learning and network science to argue that organisms can be modeled as hierarchical Bayesian control systems with small world and bottleneck (bow tie) network structure. The nested hierarchical organization of such networks allows organisms to form increasingly integrated percepts and concepts of their inner and outer context, which can be compared to increasingly encompassing predictive models of the world (goal states), to allow for an optimal control of actions. We argue that hierarchical Bayesian inference produces a hierarchy of goal states, from which it follows that organisms must have some form of `highest goals'. For all organisms, these predictive models involve interior (self) models, exterior (social) models and overarching (normative) models. We show how such goals are constructed from progressively lesser goal states and that goal hierarchies tend to decompose in a top-down manner under severe and prolonged levels of stress. This loss of high-level control leads to a disinhibition of subordinate hierarchical levels, producing `critical' behavior and tipping points (a sudden loss of homeostasis). Such phase transitions amount either to disease or the death of the organism. This model can be used to model organisms of any type, including humans. In humans, learning higher-level world models corresponds to personality development. A top-down collapse of high-level integrative goal states under stress is identified as a common factor in all forms of mental disease (psychopathology). The paper concludes by discussing ways of testing these hypotheses empirically.},
	journaltitle = {{arXiv}:2004.09426 [q-bio]},
	author = {Goekoop, Rutger and de Kleijn, Roy},
	urldate = {2020-04-25},
	date = {2020-04-20},
	eprinttype = {arxiv},
	eprint = {2004.09426},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/EQFNUHV5/2004.html:text/html;Goekoop and de Kleijn - 2020 - How higher goals are constructed and collapse unde.pdf:/Users/bert/Zotero/storage/7RYBMRE5/Goekoop and de Kleijn - 2020 - How higher goals are constructed and collapse unde.pdf:application/pdf},
}

@article{esling_universal_2019,
	title = {Universal audio synthesizer control with normalizing flows},
	url = {http://arxiv.org/abs/1907.00971},
	abstract = {The ubiquity of sound synthesizers has reshaped music production and even entirely defined new music genres. However, the increasing complexity and number of parameters in modern synthesizers make them harder to master. Hence, the development of methods allowing to easily create and explore with synthesizers is a crucial need. Here, we introduce a novel formulation of audio synthesizer control. We formalize it as finding an organized latent audio space that represents the capabilities of a synthesizer, while constructing an invertible mapping to the space of its parameters. By using this formulation, we show that we can address simultaneously automatic parameter inference, macro-control learning and audio-based preset exploration within a single model. To solve this new formulation, we rely on Variational Auto-Encoders ({VAE}) and Normalizing Flows ({NF}) to organize and map the respective auditory and parameter spaces. We introduce the disentangling flows, which allow to perform the invertible mapping between separate latent spaces, while steering the organization of some latent dimensions to match target variation factors by splitting the objective as partial density evaluation. We evaluate our proposal against a large set of baseline models and show its superiority in both parameter inference and audio reconstruction. We also show that the model disentangles the major factors of audio variations as latent dimensions, that can be directly used as macro-parameters. We also show that our model is able to learn semantic controls of a synthesizer by smoothly mapping to its parameters. Finally, we discuss the use of our model in creative applications and its real-time implementation in Ableton Live},
	journaltitle = {{arXiv}:1907.00971 [cs, eess, stat]},
	author = {Esling, Philippe and Masuda, Naotake and Bardet, Adrien and Despres, Romeo and Chemla--Romeu-Santos, Axel},
	urldate = {2020-04-24},
	date = {2019-07-01},
	eprinttype = {arxiv},
	eprint = {1907.00971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Human-Computer Interaction, Computer Science - Multimedia},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/R2J2SNWR/1907.html:text/html;Esling et al. - 2019 - Universal audio synthesizer control with normalizi.pdf:/Users/bert/Zotero/storage/6GRPXJ2S/Esling et al. - 2019 - Universal audio synthesizer control with normalizi.pdf:application/pdf},
}

@online{noauthor_-class_nodate,
	title = {An In-Class Demonstration of Bayesian Inference - Johnny van Doorn, Dora Matzke, Eric-Jan Wagenmakers, 2020},
	url = {https://journals.sagepub.com/doi/full/10.1177/1475725719848574},
	urldate = {2020-04-21},
}

@article{heins_deep_2020,
	title = {Deep Active Inference and Scene Construction},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-{NonCommercial} 4.0 International), {CC} {BY}-{NC} 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.04.14.041129v1},
	doi = {10.1101/2020.04.14.041129},
	abstract = {{\textless}p{\textgreater}Adaptive agents must act in intrinsically uncertain environments with complex latent structure. Here, we elaborate a model of visual foraging - in a hierarchical context - wherein agents infer a higher-order visual pattern (a "scene") by sequentially sampling ambiguous cues. Inspired by previous models of scene construction - that cast perception and action as consequences of approximate Bayesian inference - we use active inference to simulate decisions of agents categorising a scene in a hierarchically-structured setting. Under active inference, agents develop probabilistic beliefs about their environment, while actively sampling it to maximise the evidence for their internal generative model. This approximate evidence maximisation (i.e. self-evidencing) comprises drives to both maximise rewards and resolve uncertainty about hidden states. This is realised via minimisation of a free energy functional of posterior beliefs about both the world as well as the actions used to sample or perturb it, corresponding to perception and action, respectively. We show that active inference, in the context of hierarchical scene construction, gives rise to many empirical evidence accumulation phenomena, such as noise-sensitive reaction times and epistemic saccades. We explain these behaviours in terms of the principled drives that constitute the \textit{expected free energy}, the key quantity for evaluating policies under active inference. In addition, we report novel behaviours exhibited by these active inference agents that furnish new predictions for research on evidence accumulation and perceptual decision-making. We discuss the implications of this hierarchical active inference scheme for tasks that require planned sequences of information-gathering actions to infer compositional latent structure (such as visual scene construction and sentence comprehension). Finally, we propose experiments to contextualise active inference in relation to other formulations of evidence accumulation (e.g. drift-diffusion models) in tasks that require planning in uncertain environments with higher-order structure.{\textless}/p{\textgreater}},
	pages = {2020.04.14.041129},
	journaltitle = {{bioRxiv}},
	author = {Heins, R. Conor and Mirza, M. Berk and Parr, Thomas and Friston, Karl and Kagan, Igor and Pooresmaeili, Arezoo},
	urldate = {2020-04-15},
	date = {2020-04-15},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	file = {Heins et al. - 2020 - Deep Active Inference and Scene Construction.pdf:/Users/bert/Zotero/storage/9N6CII4Y/Heins et al. - 2020 - Deep Active Inference and Scene Construction.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UILGF4TV/2020.04.14.html:text/html},
}

@article{kusmierz_learning_2017,
	title = {Learning with three factors: modulating Hebbian plasticity with errors},
	volume = {46},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438817300612},
	doi = {10.1016/j.conb.2017.08.020},
	series = {Computational Neuroscience},
	shorttitle = {Learning with three factors},
	abstract = {Synaptic plasticity is a central theme in neuroscience. A framework of three-factor learning rules provides a powerful abstraction, helping to navigate through the abundance of models of synaptic plasticity. It is well-known that the dopamine modulation of learning is related to reward, but theoretical models predict other functional roles of the modulatory third factor; it may encode errors for supervised learning, summary statistics of the population activity for unsupervised learning or attentional feedback. Specialized structures may be needed in order to generate and propagate third factors in the neural network.},
	pages = {170--177},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Kuśmierz, Łukasz and Isomura, Takuya and Toyoizumi, Taro},
	urldate = {2020-04-11},
	date = {2017-10-01},
	langid = {english},
	file = {Kuśmierz et al. - 2017 - Learning with three factors modulating Hebbian pl.pdf:/Users/bert/Zotero/storage/2J7DG7SN/Kuśmierz et al. - 2017 - Learning with three factors modulating Hebbian pl.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/ZRQYU7WT/S0959438817300612.html:text/html},
}

@article{shimazaki_principles_2019,
	title = {The principles of adaptation in organisms and machines I: machine learning, information theory, and thermodynamics},
	url = {http://arxiv.org/abs/1902.11233},
	shorttitle = {The principles of adaptation in organisms and machines I},
	abstract = {How do organisms recognize their environment by acquiring knowledge about the world, and what actions do they take based on this knowledge? This article examines hypotheses about organisms' adaptation to the environment from machine learning, information-theoretic, and thermodynamic perspectives. We start with constructing a hierarchical model of the world as an internal model in the brain, and review standard machine learning methods to infer causes by approximately learning the model under the maximum likelihood principle. This in turn provides an overview of the free energy principle for an organism, a hypothesis to explain perception and action from the principle of least surprise. Treating this statistical learning as communication between the world and brain, learning is interpreted as a process to maximize information about the world. We investigate how the classical theories of perception such as the infomax principle relates to learning the hierarchical model. We then present an approach to the recognition and learning based on thermodynamics, showing that adaptation by causal learning results in the second law of thermodynamics whereas inference dynamics that fuses observation with prior knowledge forms a thermodynamic process. These provide a unified view on the adaptation of organisms to the environment.},
	journaltitle = {{arXiv}:1902.11233 [q-bio, stat]},
	author = {Shimazaki, Hideaki},
	urldate = {2020-04-11},
	date = {2019-02-28},
	eprinttype = {arxiv},
	eprint = {1902.11233},
	keywords = {Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/NRT4YEPS/1902.html:text/html;Shimazaki - 2019 - The principles of adaptation in organisms and mach.pdf:/Users/bert/Zotero/storage/N8F9279M/Shimazaki - 2019 - The principles of adaptation in organisms and mach.pdf:application/pdf},
}

@article{isomura_cultured_2015-1,
	title = {Cultured Cortical Neurons Can Perform Blind Source Separation According to the Free-Energy Principle},
	volume = {11},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004643},
	doi = {10.1371/journal.pcbi.1004643},
	abstract = {Blind source separation is the computation underlying the cocktail party effect––a partygoer can distinguish a particular talker’s voice from the ambient noise. Early studies indicated that the brain might use blind source separation as a signal processing strategy for sensory perception and numerous mathematical models have been proposed; however, it remains unclear how the neural networks extract particular sources from a complex mixture of inputs. We discovered that neurons in cultures of dissociated rat cortical cells could learn to represent particular sources while filtering out other signals. Specifically, the distinct classes of neurons in the culture learned to respond to the distinct sources after repeating training stimulation. Moreover, the neural network structures changed to reduce free energy, as predicted by the free-energy principle, a candidate unified theory of learning and memory, and by Jaynes’ principle of maximum entropy. This implicit learning can only be explained by some form of Hebbian plasticity. These results are the first in vitro (as opposed to in silico) demonstration of neural networks performing blind source separation, and the first formal demonstration of neuronal self-organization under the free energy principle.},
	pages = {e1004643},
	number = {12},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Isomura, Takuya and Kotani, Kiyoshi and Jimbo, Yasuhiko},
	urldate = {2020-04-11},
	date = {2015-12-21},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Free energy, Neural networks, Neurons, Action potentials, Electrode recording, Functional electrical stimulation, Neuronal plasticity, Synaptic plasticity},
	file = {Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:/Users/bert/Zotero/storage/FGTFR3KQ/Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YZQ32JGJ/article.html:text/html},
}

@article{baker_tramp_2020,
	title = {{TRAMP}: Compositional Inference with {TRee} Approximate Message Passing},
	url = {http://arxiv.org/abs/2004.01571},
	shorttitle = {{TRAMP}},
	abstract = {We introduce tramp, standing for {TRee} Approximate Message Passing, a python package for compositional inference in high-dimensional tree-structured models. The package provides an unifying framework to study several approximate message passing algorithms previously derived for a variety of machine learning tasks such as generalized linear models, inference in multi-layer networks, matrix factorization, and reconstruction using non-separable penalties. For some models, the asymptotic performance of the algorithm can be theoretically predicted by the state evolution, and the measurements entropy estimated by the free entropy formalism. The implementation is modular by design: each module, which implements a factor, can be composed at will with other modules to solve complex inference tasks. The user only needs to declare the factor graph of the model: the inference algorithm, state evolution and entropy estimation are fully automated.},
	journaltitle = {{arXiv}:2004.01571 [cond-mat, stat]},
	author = {Baker, Antoine and Aubin, Benjamin and Krzakala, Florent and Zdeborová, Lenka},
	urldate = {2020-04-08},
	date = {2020-04-03},
	eprinttype = {arxiv},
	eprint = {2004.01571},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Mathematics - Statistics Theory, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/MGAYTJII/2004.html:text/html;Baker et al. - 2020 - TRAMP Compositional Inference with TRee Approxima.pdf:/Users/bert/Zotero/storage/C32G2RRZ/Baker et al. - 2020 - TRAMP Compositional Inference with TRee Approxima.pdf:application/pdf},
}

@article{bejan_constructal_2016,
	title = {Constructal thermodynamics},
	volume = {34},
	issn = {03928764},
	url = {http://www.iieta.org/journals/ijht/paper/10.18280/ijht.34S101},
	doi = {10.18280/ijht.34S101},
	abstract = {Thermodynamics is brief, simple, unambiguous and improving. Yet, confusion reigns in the field. The word “entropy” is pasted on almost any new thing, without any respect for its proper definition in thermodynamics. Every author bows to his own maximum or minimum principle, even when it contradicts English, not just thermodynamics. Minimizing resistance cannot be the same as maximizing resistance. Minimizing entropy generation cannot be the same as maximizing entropy generation. Because of the word “entropy”, many believe that entropy generation minimization and maximization are covered by the second law, which is incorrect, twice. Because for an isolated system (or an adiabatic closed system) the second law states that the system entropy inventory increases during changes inside the system, many believe that the second law accounts for organization, evolution, and the arrow of time. This too is incorrect. It is time for a reality check, and this means to take a look at nature, at the physics, at the science of all the natural things that “happen”. Here then is a review of the few, the noble, the laws with which in science we cover the few distinct phenomena that nature is made of.},
	pages = {S1--S8},
	issue = {S1},
	journaltitle = {International Journal of Heat and Technology},
	shortjournal = {{IJHT}},
	author = {Bejan, Adrian},
	urldate = {2020-04-05},
	date = {2016-01-31},
	langid = {english},
	file = {Bejan - 2016 - Constructal thermodynamics.pdf:/Users/bert/Zotero/storage/QSCFXRWN/Bejan - 2016 - Constructal thermodynamics.pdf:application/pdf},
}

@article{bejan_constructal_2005,
	title = {The constructal law of organization in nature: tree-shaped flows and body size},
	volume = {208},
	rights = {© The Company of Biologists Limited 2005},
	issn = {0022-0949, 1477-9145},
	url = {https://jeb.biologists.org/content/208/9/1677},
	doi = {10.1242/jeb.01487},
	shorttitle = {The constructal law of organization in nature},
	abstract = {Skip to Next Section
The constructal law is the statement that for a flow system to persist in time it must evolve in such a way that it provides easier access to its currents. This is the law of configuration generation, or the law of design. The theoretical developments reviewed in this article show that this law accounts for (i) architectures that maximize flow access (e.g. trees), (ii) features that impede flow (e.g. impermeable walls, insulation) and (iii) static organs that support flow structures. The proportionality between body heat loss and body size raised to the power 3/4 is deduced from the discovery that the counterflow of two trees is the optimal configuration for achieving (i) and (ii) simultaneously: maximum fluid-flow access and minimum heat leak. Other allometric examples deduced from the constructal law are the flying speeds of insects, birds and aeroplanes, the porosity and hair strand diameter of the fur coats of animals, and the existence of optimal organ sizes. Body size and configuration are intrinsic parts of the deduced configuration. They are results, not assumptions. The constructal law extends physics (thermodynamics) to cover the configuration, performance, global size and global internal flow volume of flow systems. The time evolution of such configurations can be described as survival by increasing performance, compactness and territory.},
	pages = {1677--1686},
	number = {9},
	journaltitle = {Journal of Experimental Biology},
	author = {Bejan, Adrian},
	urldate = {2020-04-05},
	date = {2005-05-01},
	langid = {english},
	pmid = {15855399},
	note = {Publisher: The Company of Biologists Ltd
Section: Review article: Circulation and body size},
	keywords = {allometric laws, body heat, body size, constructal law, dendritic, flight, fur, hair, organ size, tree},
	file = {Bejan - 2005 - The constructal law of organization in nature tre.pdf:/Users/bert/Zotero/storage/BXQ5KF75/Bejan - 2005 - The constructal law of organization in nature tre.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/X2MAPREB/1677.html:text/html},
}

@article{bejan_life_2016,
	title = {Life and evolution as physics},
	volume = {9},
	issn = {null},
	url = {https://doi.org/10.1080/19420889.2016.1172159},
	doi = {10.1080/19420889.2016.1172159},
	abstract = {What is evolution and why does it exist in the biological, geophysical and technological realms — in short, everywhere? Why is there a time direction — a time arrow — in the changes we know are happening every moment and everywhere? Why is the present different than the past? These are questions of physics, about everything, not just biology. The answer is that nothing lives, flows and moves unless it is driven by power. Physics sheds light on the natural engines that produce the power destroyed by the flows, and on the free morphing that leads to flow architectures naturally and universally. There is a unifying tendency across all domains to evolve into flow configurations that provide greater access for movement. This tendency is expressed as the constructal law of evolutionary flow organization everywhere. Here I illustrate how this law of physics accounts for and unites the life and evolution phenomena throughout nature, animate and inanimate.},
	pages = {e1172159},
	number = {3},
	journaltitle = {Communicative \& Integrative Biology},
	author = {Bejan, Adrian},
	urldate = {2020-04-05},
	date = {2016-05-03},
	pmid = {27489579},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/19420889.2016.1172159},
	keywords = {evolution, life, thermodynamics, constructal law, growth, physics},
	file = {Bejan - 2016 - Life and evolution as physics.pdf:/Users/bert/Zotero/storage/3UVG57TS/Bejan - 2016 - Life and evolution as physics.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/3CYJM5KK/19420889.2016.html:text/html},
}

@article{bejan_thermodynamics_2018,
	title = {Thermodynamics today},
	volume = {160},
	issn = {0360-5442},
	url = {http://www.sciencedirect.com/science/article/pii/S0360544218313896},
	doi = {10.1016/j.energy.2018.07.092},
	abstract = {In this paper I use the example set by Prof. Jan Szargut as point of reference for a brief look at the current state of thermodynamics—the doctrine, its reach and importance. I start with my first encounter with Prof. Jan Szargut in 1979, and I show how his work influenced mine. Next, I review the structure that underpins thermodynamics as a discipline: the laws and the self-standing phenomena that they underpin, and graphic methods that convey these principles. Along the way, I draw attention to a recent trend that is caused by the inflation in scientific publishing due to the internet: the most common mistakes and misconceptions in thermodynamics, and how they are being spread. In sum, this paper is a call to action, to value, improve and defend the science of thermodynamics.},
	pages = {1208--1219},
	journaltitle = {Energy},
	shortjournal = {Energy},
	author = {Bejan, Adrian},
	urldate = {2020-04-05},
	date = {2018-10-01},
	langid = {english},
	keywords = {Evolution, Thermodynamics, Caloric, Constructal, Design, Discipline, Laws, Mechanics},
	file = {Bejan - 2018 - Thermodynamics today.pdf:/Users/bert/Zotero/storage/AJ6ZPJIT/Bejan - 2018 - Thermodynamics today.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/RBSJGZ55/S0360544218313896.html:text/html},
}

@article{friston_active_2020,
	title = {Active Listening},
	rights = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2020.03.18.997122v1},
	doi = {10.1101/2020.03.18.997122},
	abstract = {{\textless}p{\textgreater}This paper introduces active listening, as a unified framework for synthesising and recognising speech. The notion of active listening inherits from active inference, which considers perception and action under one universal imperative: to maximise the evidence for our (generative) models of the world. First, we describe a generative model of spoken words that simulates (i) how discrete lexical, prosodic, and speaker attributes give rise to continuous acoustic signals; and conversely (ii) how continuous acoustic signals are recognised as words. The 9active9 aspect involves (covertly) segmenting spoken sentences and borrows ideas from active vision. It casts speech segmentation as the selection of internal actions, corresponding to the placement of word boundaries. Practically, word boundaries are selected that maximise the evidence for an internal model of how individual words are generated. We establish face validity by simulating speech recognition and showing how the inferred content of a sentence depends on prior beliefs and background noise. Finally, we consider predictive validity by associating neuronal or physiological responses, such as the mismatch negativity and P300, with belief updating under active listening, which is greatest in the absence of accurate prior beliefs about what will be heard next.{\textless}/p{\textgreater}},
	pages = {2020.03.18.997122},
	journaltitle = {{bioRxiv}},
	author = {Friston, Karl J. and Sajid, Noor and Quiroga-Martinez, David Ricardo and Parr, Thomas and Price, Cathy J. and Holmes, Emma},
	urldate = {2020-04-04},
	date = {2020-03-20},
	langid = {english},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	file = {Friston et al. - 2020 - Active Listening.pdf:/Users/bert/Zotero/storage/W6FP79ID/Friston et al. - 2020 - Active Listening.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/IN95LIZP/2020.03.18.html:text/html},
}

@article{oliver_active_2020,
	title = {Active inference body perception and action for humanoid robots},
	url = {http://arxiv.org/abs/1906.03022},
	abstract = {Providing artificial agents with the same computational models of biological systems is a way to understand how intelligent behaviours may emerge. We present an active inference body perception and action model working for the first time in a humanoid robot. The model relies on the free energy principle proposed for the brain, where both perception and action goal is to minimise the prediction error through gradient descent on the variational free energy bound. The body state (latent variable) is inferred by minimising the difference between the observed (visual and proprioceptive) sensor values and the predicted ones. Simultaneously, the action makes sensory data sampling to better correspond to the prediction made by the inner model. We formalised and implemented the algorithm on the {iCub} robot and tested in 2D and 3D visual spaces for online adaptation to visual changes, sensory noise and discrepancies between the model and the real robot. We also compared our approach with classical inverse kinematics in a reaching task, analysing the suitability of such a neuroscience-inspired approach for real-world interaction. The algorithm gave the robot adaptive body perception and upper body reaching with head object tracking (toddler-like), and was able to incorporate visual features online (in a closed-loop manner) without increasing the computational complexity. Moreover, our model predicted involuntary actions in the presence of sensorimotor conflicts showing the path for a potential proof of active inference in humans.},
	journaltitle = {{arXiv}:1906.03022 [cs]},
	author = {Oliver, Guillermo and Lanillos, Pablo and Cheng, Gordon},
	urldate = {2020-03-31},
	date = {2020-01-29},
	eprinttype = {arxiv},
	eprint = {1906.03022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/RH3Q3JUH/1906.html:text/html;Oliver et al. - 2020 - Active inference body perception and action for hu.pdf:/Users/bert/Zotero/storage/WCKNBTAT/Oliver et al. - 2020 - Active inference body perception and action for hu.pdf:application/pdf},
}

@article{liu_overview_nodate,
	title = {An Overview of Information Geometry},
	abstract = {This paper summarizes the basics concepts of information geometry, and gives as example applications, Jeﬀreys prior in Bayesian probability, and natural gradient descent in machine learning. Necessary concepts in probability and statistics are explained in detail.},
	pages = {20},
	author = {Liu, Yuxi},
	langid = {english},
	file = {Liu - An Overview of Information Geometry.pdf:/Users/bert/Zotero/storage/DMAZ73ML/Liu - An Overview of Information Geometry.pdf:application/pdf},
}

@article{baudart_reactive_2019,
	title = {Reactive Probabilistic Programming},
	url = {http://arxiv.org/abs/1908.07563},
	abstract = {Synchronous reactive languages were introduced for designing and implementing real-time control software. These domain-specific languages allow for writing a modular and mathematically precise specification of the system, enabling a user to simulate, test, verify, and, finally, compile the system into executable code. However, to date these languages have had limited modern support for modeling uncertainty -- probabilistic aspects of the software's environment or behavior -- even though modeling uncertainty is a primary activity when designing a control system. In this paper we extend Z{\textbackslash}'elus, a synchronous programming language, to deliver {ProbZ}{\textbackslash}'elus, the first synchronous probabilistic programming language. {ProbZ}{\textbackslash}'elus is a probabilistic programming language in that it provides facilities for probabilistic models and inference: inferring latent model parameters from data. We present {ProbZ}{\textbackslash}'elus's measure-theoretic semantics in the setting of probabilistic, stateful stream functions. We then demonstrate a semantics-preserving compilation strategy to a first-order functional core calculus that lends itself to a simple semantic presentation of {ProbZ}{\textbackslash}'elus's inference algorithms. We also redesign the delayed sampling inference algorithm to provide bounded and streaming delayed sampling inference for {ProbZ}{\textbackslash}'elus models. Together with our evaluation on several reactive programs, our results demonstrate that {ProbZ}{\textbackslash}'elus provides efficient, bounded memory probabilistic inference.},
	journaltitle = {{arXiv}:1908.07563 [cs]},
	author = {Baudart, Guillaume and Mandel, Louis and Atkinson, Eric and Sherman, Benjamin and Pouzet, Marc and Carbin, Michael},
	urldate = {2020-03-22},
	date = {2019-08-20},
	eprinttype = {arxiv},
	eprint = {1908.07563},
	keywords = {Computer Science - Programming Languages},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/7XZVUA24/1908.html:text/html;Baudart et al. - 2019 - Reactive Probabilistic Programming.pdf:/Users/bert/Zotero/storage/SKEREPEQ/Baudart et al. - 2019 - Reactive Probabilistic Programming.pdf:application/pdf},
}

@article{noauthor_probabilistic_2003,
	title = {Probabilistic Reactive Programming},
	volume = {91},
	issn = {0018-9219},
	url = {http://ieeexplore.ieee.org/document/1173191/},
	doi = {10.1109/JPROC.2002.805826},
	pages = {64--83},
	number = {1},
	journaltitle = {Proceedings of the {IEEE}},
	shortjournal = {Proc. {IEEE}},
	urldate = {2020-03-21},
	date = {2003-01},
	langid = {english},
	file = {Probabilistic Reactive Programming:/Users/bert/Zotero/storage/9G3ETDH3/Probabilistic Reactive Programming.pdf:application/pdf},
}

@article{general_principle_2018,
	title = {Principle of maximum caliber and quantum physics},
	volume = {98},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.98.012110},
	doi = {10.1103/PhysRevE.98.012110},
	abstract = {{MaxCal} is a variational principle that can be used to infer distributions of paths in the phase space of dynamical systems. It has been successfully applied to different areas of classical physics, in particular statistical mechanics in and out of equilibrium. In this work, guided by the analogy of the formalism of {MaxCal} with that of the path integral formulation of quantum mechanics, we explore the extension of its applications to the realm of quantum physics, and show how the Lagrangians of both relativistic and nonrelativistic quantum fields can be built from {MaxCal}, with a suitable set of constraints. Related, the details of the constraints allow us to reinterpret the concept of inertia.},
	pages = {012110},
	number = {1},
	journaltitle = {Physical Review E},
	shortjournal = {Phys. Rev. E},
	author = {General, Ignacio J.},
	urldate = {2021-01-09},
	date = {2018-07-11},
	note = {Publisher: American Physical Society},
	file = {APS Snapshot:/Users/bert/Zotero/storage/WDZ4LVMV/PhysRevE.98.html:text/html;General - 2018 - Principle of maximum caliber and quantum physics.pdf:/Users/bert/Zotero/storage/S3DPHADD/General - 2018 - Principle of maximum caliber and quantum physics.pdf:application/pdf},
}

@online{noauthor_deriving_2020,
	title = {Deriving {PLA} from {MaxCal}},
	url = {https://shanhelab.com/2020/03/04/deriving-pla-from-maxcal/},
	abstract = {The principle of least action ({PLA}), or more accurately, the principle of stationary action, is one first principle in physics. {PLA} offers the deepest explanatory power of our external reality. In …},
	titleaddon = {Shan He Lab},
	urldate = {2021-01-09},
	date = {2020-03-04},
	langid = {english},
	file = {Snapshot:/Users/bert/Zotero/storage/P33T4JYM/deriving-pla-from-maxcal.html:text/html},
}

@article{mujica-parodi_diet_2020-1,
	title = {Diet modulates brain network stability, a biomarker for brain aging, in young adults},
	volume = {117},
	rights = {Copyright © 2020 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 ({CC} {BY}).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/117/11/6170},
	doi = {10.1073/pnas.1913042117},
	abstract = {Epidemiological studies suggest that insulin resistance accelerates progression of age-based cognitive impairment, which neuroimaging has linked to brain glucose hypometabolism. As cellular inputs, ketones increase Gibbs free energy change for {ATP} by 27\% compared to glucose. Here we test whether dietary changes are capable of modulating sustained functional communication between brain regions (network stability) by changing their predominant dietary fuel from glucose to ketones. We first established network stability as a biomarker for brain aging using two large-scale (n = 292, ages 20 to 85 y; n = 636, ages 18 to 88 y) 3 T functional {MRI} ({fMRI}) datasets. To determine whether diet can influence brain network stability, we additionally scanned 42 adults, age {\textless} 50 y, using ultrahigh-field (7 T) ultrafast (802 ms) {fMRI} optimized for single-participant-level detection sensitivity. One cohort was scanned under standard diet, overnight fasting, and ketogenic diet conditions. To isolate the impact of fuel type, an independent overnight fasted cohort was scanned before and after administration of a calorie-matched glucose and exogenous ketone ester (d-β-hydroxybutyrate) bolus. Across the life span, brain network destabilization correlated with decreased brain activity and cognitive acuity. Effects emerged at 47 y, with the most rapid degeneration occurring at 60 y. Networks were destabilized by glucose and stabilized by ketones, irrespective of whether ketosis was achieved with a ketogenic diet or exogenous ketone ester. Together, our results suggest that brain network destabilization may reflect early signs of hypometabolism, associated with dementia. Dietary interventions resulting in ketone utilization increase available energy and thus may show potential in protecting the aging brain.},
	pages = {6170--6177},
	number = {11},
	journaltitle = {Proceedings of the National Academy of Sciences},
	shortjournal = {{PNAS}},
	author = {Mujica-Parodi, Lilianne R. and Amgalan, Anar and Sultan, Syed Fahad and Antal, Botond and Sun, Xiaofei and Skiena, Steven and Lithen, Andrew and Adra, Noor and Ratai, Eva-Maria and Weistuch, Corey and Govindarajan, Sindhuja Tirumalai and Strey, Helmut H. and Dill, Ken A. and Stufflebeam, Steven M. and Veech, Richard L. and Clarke, Kieran},
	urldate = {2021-01-09},
	date = {2020-03-17},
	langid = {english},
	pmid = {32127481},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {beta-hydroxybutyrate, brain, glucose, ketone, neural},
	file = {Full Text PDF:/Users/bert/Zotero/storage/GT2GQ6PH/Mujica-Parodi et al. - 2020 - Diet modulates brain network stability, a biomarke.pdf:application/pdf},
}

@thesis{walther_non-equilibrium_2014,
	title = {Non-Equilibrium Physics from an Inference Perspective},
	institution = {Stony Brook University},
	type = {{MSc} thesis},
	author = {Walther, Valentin},
	date = {2014},
	file = {Walther - 2014 - Non-Equilibrium Physics from an Inference Perspect.pdf:/Users/bert/Zotero/storage/ZNZDLSHZ/Walther - 2014 - Non-Equilibrium Physics from an Inference Perspect.pdf:application/pdf},
}

@inproceedings{jaynes_macroscopic_1985,
	location = {Berlin, Heidelberg},
	title = {Macroscopic Prediction},
	isbn = {978-3-642-70795-7},
	doi = {10.1007/978-3-642-70795-7_18},
	series = {Springer Series in Synergetics},
	abstract = {Our topic is the principles for prediction of macroscopic phenomena in general, and the relation to microphenomena. Although physicists believe that we have understood the laws of microphysics quite well for fifty years, macrophenomena are observed to have a rich variety that is very hard to understand. We see not only lifeless thermal equilibrium and irreversible approaches to it, but lively behavior such as that of cyclic chemical reactions, lasers, self-organizing systems, biological systems.},
	pages = {254--269},
	booktitle = {Complex Systems — Operational Approaches in Neurobiology, Physics, and Computers},
	publisher = {Springer},
	author = {Jaynes, E. T.},
	editor = {Haken, Hermann},
	date = {1985},
	langid = {english},
	keywords = {Bubble Dynamic, Entropy Function, Irreversible Process, Stone Column, Variational Principle},
	file = {Jaynes - 1985 - Macroscopic Prediction.pdf:/Users/bert/Zotero/storage/WZWJB2JX/Jaynes - 1985 - Macroscopic Prediction.pdf:application/pdf},
}

@article{davis_probabilistic_2018,
	title = {Probabilistic Inference for Dynamical Systems},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/20/9/696},
	doi = {10.3390/e20090696},
	abstract = {A general framework for inference in dynamical systems is described, based on the language of Bayesian probability theory and making use of the maximum entropy principle. Taking the concept of a path as fundamental, the continuity equation and Cauchy\&rsquo;s equation for fluid dynamics arise naturally, while the specific information about the system can be included using the maximum caliber (or maximum path entropy) principle.},
	pages = {696},
	number = {9},
	journaltitle = {Entropy},
	author = {Davis, Sergio and González, Diego and Gutiérrez, Gonzalo},
	urldate = {2021-01-08},
	date = {2018-09},
	langid = {english},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {bayesian inference, dynamical systems, fluid equations},
	file = {Full Text PDF:/Users/bert/Zotero/storage/FHX53MA6/Davis et al. - 2018 - Probabilistic Inference for Dynamical Systems.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/RHWPJF62/htm.html:text/html},
}

@article{gottwald_two_2020-1,
	title = {The two kinds of free energy and the Bayesian revolution},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008420},
	doi = {10.1371/journal.pcbi.1008420},
	abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information processing of perception.},
	pages = {e1008420},
	number = {12},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Gottwald, Sebastian and Braun, Daniel A.},
	urldate = {2021-01-06},
	date = {2020-12-03},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Free energy, Decision making, Probability distribution, Optimization, Entropy, Information processing, Helmholtz free energy, Kullback Leibler divergence},
	file = {Gottwald and Braun - 2020 - The two kinds of free energy and the Bayesian revo.pdf:/Users/bert/Zotero/storage/V8RZP8F9/Gottwald and Braun - 2020 - The two kinds of free energy and the Bayesian revo.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/28F6S2WL/article.html:text/html},
}

@article{tschantz_reinforcement_2020-1,
	title = {{REINFORCEMENT} {LEARNING} {THROUGH} {ACTIVE} {INFERENCE}},
	abstract = {The central tenet of reinforcement learning ({RL}) is that agents seek to maximize the sum of cumulative rewards. In contrast, active inference, an emerging framework within cognitive and computational neuroscience, proposes that agents act to maximize the evidence for a biased generative model. Here, we illustrate how ideas from active inference can augment traditional {RL} approaches by (i) furnishing an inherent balance of exploration and exploitation, and (ii) providing a more ﬂexible conceptualization of reward. Inspired by active inference, we develop and implement a novel objective for decision making, which we term the free energy of the expected future. We demonstrate that the resulting algorithm successfully balances exploration and exploitation, simultaneously achieving robust performance on several challenging {RL} benchmarks with sparse, well-shaped, and no rewards.},
	pages = {14},
	author = {Tschantz, Alexander and Millidge, Beren and Seth, Anil K and Buckley, Christopher L},
	date = {2020},
	langid = {english},
	file = {Tschantz et al. - 2020 - REINFORCEMENT LEARNING THROUGH ACTIVE INFERENCE.pdf:/Users/bert/Zotero/storage/MRRY9JEP/Tschantz et al. - 2020 - REINFORCEMENT LEARNING THROUGH ACTIVE INFERENCE.pdf:application/pdf},
}

@article{gershman_amortized_2014-1,
	title = {Amortized Inference in Probabilistic Reasoning},
	volume = {36},
	issn = {1069-7977},
	url = {https://escholarship.org/uc/item/34j1h7k5},
	abstract = {Author(s): Gershman, Samuel; Goodman, Noah},
	number = {36},
	journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
	author = {Gershman, Samuel and Goodman, Noah},
	urldate = {2021-01-02},
	date = {2014},
	langid = {english},
	file = {Gershman and Goodman - 2014 - Amortized Inference in Probabilistic Reasoning.pdf:/Users/bert/Zotero/storage/3Z8ZBTWM/Gershman and Goodman - 2014 - Amortized Inference in Probabilistic Reasoning.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/Y7EGS38F/34j1h7k5.html:text/html},
}

@book{murphy_probabilistic_2021,
	title = {Probabilistic Machine Learning: An Introduction},
	url = {https://probml.github.io/pml-book/book1.html?s=03},
	publisher = {{MIT} Press},
	author = {Murphy, Kevin P.},
	date = {2021},
	file = {Murphy - 2021 - Probabilistic Machine Learning An Introduction.pdf:/Users/bert/Zotero/storage/AZ45Q8H8/Murphy - 2021 - Probabilistic Machine Learning An Introduction.pdf:application/pdf},
}

@report{smith_step-by-step_2021,
	title = {A Step-by-Step Tutorial on Active Inference and its Application to Empirical Data},
	url = {https://psyarxiv.com/b4jm6/},
	abstract = {The active inference framework, and in particular its recent formulation as a partially observable Markov decision process ({POMDP}), has gained increasing popularity in recent years as a useful approach for modelling neurocognitive processes. This framework is highly general and flexible in its ability to be customized to model any cognitive process, as well as simulate predicted neuronal responses based on its accompanying neural process theory. It also affords both simulation experiments for proof of principle and behavioral modelling for empirical studies. However, there are limited resources that explain how to build and run these models in practice, which limits their widespread use. Most introductions assume a technical background in programming, mathematics, and machine learning. In this paper we offer a step-by-step tutorial on how to build {POMDPs}, run simulations using standard {MATLAB} routines, and fit these models to empirical data. We assume a minimal background in programming and mathematics, thoroughly explain all equations, and provide exemplar scripts that can be customized for both theoretical and empirical studies. Our goal is to provide the reader with the requisite background knowledge and practical tools to apply active inference to their own research. We also provide optional technical sections and several appendices, which offer the interested reader additional technical details. This tutorial should provide the reader with all the tools necessary to use these models and to follow emerging advances in active inference research.},
	institution = {{PsyArXiv}},
	author = {Smith, Ryan and Friston, Karl and Whyte, Christopher},
	urldate = {2021-01-02},
	date = {2021-01-02},
	doi = {10.31234/osf.io/b4jm6},
	note = {type: article},
	keywords = {Computational Neuroscience, Neuroscience, Cognitive Neuroscience, Cognitive Psychology, Social and Behavioral Sciences, Computational modeling, Active Inference, Discrete state space models, Partially observable Markov decision processes ({POMDPs}), Tutorial},
	file = {Smith et al. - 2021 - A Step-by-Step Tutorial on Active Inference and it.pdf:/Users/bert/Zotero/storage/RALBBS2P/Smith et al. - 2021 - A Step-by-Step Tutorial on Active Inference and it.pdf:application/pdf},
}

@article{jaynes_minimum_1980,
	title = {The Minimum Entropy Production Principle},
	volume = {31},
	issn = {0066-426X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.pc.31.100180.003051},
	doi = {10.1146/annurev.pc.31.100180.003051},
	pages = {579--601},
	number = {1},
	journaltitle = {Annual Review of Physical Chemistry},
	shortjournal = {Annu. Rev. Phys. Chem.},
	author = {Jaynes, E T},
	urldate = {2020-12-30},
	date = {1980-10-01},
	note = {Publisher: Annual Reviews},
	file = {Jaynes - 1980 - The Minimum Entropy Production Principle.pdf:/Users/bert/Zotero/storage/I3YLIGKW/Jaynes - 1980 - The Minimum Entropy Production Principle.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BF8NGQFH/annurev.pc.31.100180.html:text/html},
}

@article{davis_probabilistic_2018-1,
	title = {Probabilistic Inference for Dynamical Systems},
	volume = {20},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/20/9/696},
	doi = {10.3390/e20090696},
	abstract = {A general framework for inference in dynamical systems is described, based on the language of Bayesian probability theory and making use of the maximum entropy principle. Taking the concept of a path as fundamental, the continuity equation and Cauchy\&rsquo;s equation for fluid dynamics arise naturally, while the specific information about the system can be included using the maximum caliber (or maximum path entropy) principle.},
	pages = {696},
	number = {9},
	journaltitle = {Entropy},
	author = {Davis, Sergio and González, Diego and Gutiérrez, Gonzalo},
	urldate = {2020-12-30},
	date = {2018-09},
	langid = {english},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {bayesian inference, dynamical systems, fluid equations},
	file = {Davis et al. - 2018 - Probabilistic Inference for Dynamical Systems.pdf:/Users/bert/Zotero/storage/ET37NV39/Davis et al. - 2018 - Probabilistic Inference for Dynamical Systems.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/HASWNUVZ/696.html:text/html},
}

@inproceedings{singh_audio_2014,
	title = {Audio Classification with Thermodynamic Criteria},
	doi = {10.1109/IC2E.2014.23},
	abstract = {Detecting sound events in audio recordings is a challenging problem. A detector must be trained for each sound to be classified. However, the recordings of the examples used to train the detector rarely match the conditions found in the test audio to be classified. If the event detection problem is posed as one of Bayes classification, the problem may be viewed as one of mismatch between the true distribution of the data and that represented by the classifier. The Bayes classification rule results in suboptimal performance under such mismatch, and a modified classification rule is required. Alternately stated, the classification rule must optimize a different objective criterion than the Bayes error rate computed from the training distributions. The use of entropy as an optimization criterion for various classification tasks has been well established in the literature. In this paper we show that free-energy, a thermodynamic concept directly related to entropy, can also be used as an objective criterion for classification in such scenarios. We demonstrate with examples on classification with {HMMs} that minimization of free-energy is an effective criterion for classification under conditions of mismatch.},
	eventtitle = {2014 {IEEE} International Conference on Cloud Engineering},
	pages = {526--533},
	booktitle = {2014 {IEEE} International Conference on Cloud Engineering},
	author = {Singh, R.},
	date = {2014-03},
	keywords = {Free energy, Bayes methods, Speech recognition, learning (artificial intelligence), Hidden Markov models, Entropy, entropy, Equations, Mathematical model, audio classification, Audio classification, audio recording, audio signal processing, Bayes classification rule, Bayes error rate, Bayesian classification, hidden Markov models, {HMM}, optimization criteria, signal classification, signal detection, sound event detection, Temperature, Temperature distribution, thermodynamic criteria, training distribution},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/8HBR4S5J/6903523.html:text/html;Singh - 2014 - Audio Classification with Thermodynamic Criteria.pdf:/Users/bert/Zotero/storage/YRGJL2IV/Singh - 2014 - Audio Classification with Thermodynamic Criteria.pdf:application/pdf},
}

@article{beck_bayesian_2010,
	title = {Bayesian system identification based on probability logic},
	doi = {10.1002/STC.424},
	abstract = {Probability logic with Bayesian updating provides a rigorous framework to quantify modeling uncertainty and perform system identification. It uses probability as a multi-valued propositional logic for plausible reasoning where the probability of a model is a measure of its relative plausibility within a set of models. System identification is thus viewed as inference about plausible system models and not as a quixotic quest for the true model. Instead of using system data to estimate the model parameters, Bayes' Theorem is used to update the relative plausibility of each model in a model class, which is a set of input–output probability models for the system and a probability distribution over this set that expresses the initial plausibility of each model. Robust predictive analyses informed by the system data use the entire model class with the probabilistic predictions of each model being weighed by its posterior probability. Additional robustness to modeling uncertainty comes from combining the robust predictions of each model class in a set of candidates for the system, where each contribution is weighed by the posterior probability of the model class. This application of Bayes' Theorem automatically applies a quantitative Ockham's razor that penalizes the data-fit of more complex model classes that extract more information from the data. Robust analyses involve integrals over parameter spaces that usually must be evaluated numerically by Laplace's method of asymptotic approximation or by Markov Chain Monte Carlo methods. An illustrative application is given using synthetic data corresponding to a structural health monitoring benchmark structure.},
	author = {Beck, J.},
	date = {2010},
	file = {Beck - 2010 - Bayesian system identification based on probabilit.pdf:/Users/bert/Zotero/storage/MMWRQDFP/Beck - 2010 - Bayesian system identification based on probabilit.pdf:application/pdf},
}

@thesis{bakker_cox-jaynes_2016,
	title = {On the Cox-Jaynes justification for objective Bayesian probability theory and the mind projection fallacy in physics},
	institution = {University of Amsterdam},
	type = {{MSc} thesis},
	author = {Bakker, Tim},
	date = {2016},
	file = {Bakker - 2016 - On the Cox-Jaynes justification for objective Baye.pdf:/Users/bert/Zotero/storage/PLUWYBYG/Bakker - 2016 - On the Cox-Jaynes justification for objective Baye.pdf:application/pdf},
}

@book{caticha_entropic_2020,
	title = {Entropic Physics: Probability, Entropy, and the Foundations of Physics},
	url = {https://www.albany.edu/physics/faculty/ariel-caticha},
	publisher = {lecture notes},
	author = {Caticha, Ariel},
	date = {2020},
	file = {Caticha - 2020 - Entropic Physics Probability, Entropy, and the Fo.pdf:/Users/bert/Zotero/storage/3YYW53KX/Caticha - 2020 - Entropic Physics Probability, Entropy, and the Fo.pdf:application/pdf},
}

@article{ao_equivalent_2014,
	title = {Equivalent formulations of “the equation of life”},
	volume = {23},
	issn = {1674-1056},
	url = {https://doi.org/10.1088/1674-1056/23/7/070513},
	doi = {10.1088/1674-1056/23/7/070513},
	abstract = {Motivated by progress in theoretical biology a recent proposal on a general and quantitative dynamical framework for nonequilibrium processes and dynamics of complex systems is briefly reviewed. It is nothing but the evolutionary process discovered by Charles Darwin and Alfred Wallace. Such general and structured dynamics may be tentatively named “the equation of life”. Three equivalent formulations are discussed, and it is also pointed out that such a quantitative dynamical framework leads naturally to the powerful Boltzmann-Gibbs distribution and the second law in physics. In this way, the equation of life provides a logically consistent foundation for thermodynamics. This view clarifies a particular outstanding problem and further suggests a unifying principle for physics and biology.},
	pages = {070513},
	number = {7},
	journaltitle = {Chinese Physics B},
	shortjournal = {Chinese Phys. B},
	author = {Ao, Ping},
	urldate = {2020-12-23},
	date = {2014-07},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
	file = {Ao - 2014 - Equivalent formulations of “the equation of life”.pdf:/Users/bert/Zotero/storage/ZTUZT9HJ/Ao - 2014 - Equivalent formulations of “the equation of life”.pdf:application/pdf},
}

@article{alemi_therml_2018,
	title = {{TherML}: Thermodynamics of Machine Learning},
	url = {http://arxiv.org/abs/1807.04162},
	shorttitle = {{TherML}},
	abstract = {In this work we offer a framework for reasoning about a wide class of existing objectives in machine learning. We develop a formal correspondence between this work and thermodynamics and discuss its implications.},
	journaltitle = {{arXiv}:1807.04162 [cond-mat, stat]},
	author = {Alemi, Alexander A. and Fischer, Ian},
	urldate = {2020-12-16},
	date = {2018-10-04},
	eprinttype = {arxiv},
	eprint = {1807.04162},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics},
	file = {Alemi and Fischer - 2018 - TherML Thermodynamics of Machine Learning.pdf:/Users/bert/Zotero/storage/HSWWXT8B/Alemi and Fischer - 2018 - TherML Thermodynamics of Machine Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/AVBB32HS/1807.html:text/html},
}

@article{catal_learning_2020,
	title = {Learning Generative State Space Models for Active Inference},
	volume = {14},
	issn = {1662-5188},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7701292/},
	doi = {10.3389/fncom.2020.574372},
	abstract = {In this paper we investigate the active inference framework as a means to enable autonomous behavior in artificial agents. Active inference is a theoretical framework underpinning the way organisms act and observe in the real world. In active inference, agents act in order to minimize their so called free energy, or prediction error. Besides being biologically plausible, active inference has been shown to solve hard exploration problems in various simulated environments. However, these simulations typically require handcrafting a generative model for the agent. Therefore we propose to use recent advances in deep artificial neural networks to learn generative state space models from scratch, using only observation-action sequences. This way we are able to scale active inference to new and challenging problem domains, whilst still building on the theoretical backing of the free energy principle. We validate our approach on the mountain car problem to illustrate that our learnt models can indeed trade-off instrumental value and ambiguity. Furthermore, we show that generative models can also be learnt using high-dimensional pixel observations, both in the {OpenAI} Gym car racing environment and a real-world robotic navigation task. Finally we show that active inference based policies are an order of magnitude more sample efficient than Deep Q Networks on {RL} tasks.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front Comput Neurosci},
	author = {Çatal, Ozan and Wauthier, Samuel and De Boom, Cedric and Verbelen, Tim and Dhoedt, Bart},
	urldate = {2020-12-16},
	date = {2020-11-16},
	pmid = {33304260},
	pmcid = {PMC7701292},
	file = {Çatal et al. - 2020 - Learning Generative State Space Models for Active .pdf:/Users/bert/Zotero/storage/YYR44IMD/Çatal et al. - 2020 - Learning Generative State Space Models for Active .pdf:application/pdf},
}

@article{berseth_smirl_2020,
	title = {{SMiRL}: Surprise Minimizing Reinforcement Learning in Dynamic Environments},
	url = {http://arxiv.org/abs/1912.05510},
	shorttitle = {{SMiRL}},
	abstract = {All living organisms struggle against the forces of nature to carve out a maintainable niche. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing reinforcement learning ({SMiRL}). {SMiRL} alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. This process maximizes a lower-bound on the negative entropy of the states, which can be seen as maximizing the agent's ability to maintain order in the environment. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that {SMiRL} can be used together with standard task rewards to accelerate reward-driven learning.},
	journaltitle = {{arXiv}:1912.05510 [cs, stat]},
	author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
	urldate = {2020-12-16},
	date = {2020-07-06},
	eprinttype = {arxiv},
	eprint = {1912.05510},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, G.3},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/54XFYKIZ/1912.html:text/html;Berseth et al. - 2020 - SMiRL Surprise Minimizing Reinforcement Learning .pdf:/Users/bert/Zotero/storage/EQ5IFS3R/Berseth et al. - 2020 - SMiRL Surprise Minimizing Reinforcement Learning .pdf:application/pdf},
}

@inproceedings{poole_variational_2019,
	title = {On Variational Bounds of Mutual Information},
	url = {http://proceedings.mlr.press/v97/poole19a.html},
	abstract = {Estimating and optimizing Mutual Information ({MI}) is core to many problems in machine learning, but bounding {MI} in high dimensions is challenging. To establish tractable and scalable objectives, re...},
	eventtitle = {International Conference on Machine Learning},
	pages = {5171--5180},
	booktitle = {International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Poole, Ben and Ozair, Sherjil and Oord, Aaron Van Den and Alemi, Alex and Tucker, George},
	urldate = {2020-12-16},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Poole et al. - 2019 - On Variational Bounds of Mutual Information.pdf:/Users/bert/Zotero/storage/38Q7I7JZ/Poole et al. - 2019 - On Variational Bounds of Mutual Information.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/ABXIBAQK/poole19a.html:text/html},
}

@inproceedings{barber_im_2003,
	location = {Cambridge, {MA}, {USA}},
	title = {The {IM} algorithm: a variational approach to Information Maximization},
	series = {{NIPS}'03},
	shorttitle = {The {IM} algorithm},
	abstract = {The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The resulting {IM} algorithm is analagous to the {EM} algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and {CDMA}.},
	pages = {201--208},
	booktitle = {Proceedings of the 16th International Conference on Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Barber, David and Agakov, Felix},
	urldate = {2020-12-16},
	date = {2003-12-09},
	file = {Barber and Agakov - 2003 - The IM algorithm a variational approach to Inform.pdf:/Users/bert/Zotero/storage/5JQJISD9/Barber and Agakov - 2003 - The IM algorithm a variational approach to Inform.pdf:application/pdf},
}

@inproceedings{senoz_online_2020,
	title = {Online Message Passing-based Inference in the Hierarchical Gaussian Filter},
	isbn = {1-72816-432-X},
	eventtitle = {2020 {IEEE} International Symposium on Information Theory ({ISIT})},
	pages = {2676--2681},
	publisher = {{IEEE}},
	author = {Şenöz, İsmail and de Vries, Bert},
	date = {2020},
}

@inproceedings{podusenko_online_2020,
	title = {Online variational message passing in hierarchical autoregressive models},
	isbn = {1-72816-432-X},
	eventtitle = {2020 {IEEE} International Symposium on Information Theory ({ISIT})},
	pages = {1337--1342},
	publisher = {{IEEE}},
	author = {Podusenko, Albert and Kouw, Wouter M and de Vries, Bert},
	date = {2020},
}

@inproceedings{ergul_learning_2020,
	title = {Learning Where to Park},
	eventtitle = {{IWAI}-2020, International Workshop on Active Inference},
	author = {Ergul, Burak and Van de Laar, Thijs and Koudahl, Magnus and Roa Villescas, Martin and de Vries, Bert},
	date = {2020},
}

@inproceedings{koudahl_worked_2020,
	title = {A Worked Example of Fokker-Planck based Active Inference},
	eventtitle = {{IWAI}-2020, International Workshop on Active Inference},
	author = {Koudahl, Magnus and de Vries, Bert},
	date = {2020},
}

@inproceedings{villescas_real-time_2020,
	title = {Real-time audio processing for hearing aids using a model-based bayesian inference framework},
	eventtitle = {Proceedings of the 23th International Workshop on Software and Compilers for Embedded Systems},
	pages = {82--85},
	author = {Villescas, Martin Roa and de Vries, Bert and Stuijk, Sander and Corporaal, Henk},
	date = {2020},
}

@article{zhao_online_2008-1,
	title = {Online noise estimation using stochastic-gain {HMM} for speech enhancement},
	volume = {16},
	doi = {10.1109/TASL.2008.916055},
	pages = {835--846},
	number = {4},
	journaltitle = {{IEEE} transactions on audio, speech, and language processing},
	author = {Zhao, David Y. and Kleijn, W. Bastiaan and Ypma, Alexander and de Vries, Bert},
	date = {2008},
	file = {Full Text:/Users/bert/Zotero/storage/FWV6YPQE/record.html:text/html;Snapshot:/Users/bert/Zotero/storage/74TCT4HG/4453864.html:text/html},
}

@inproceedings{zhang_bayesian_2009,
	title = {Bayesian periodogram smoothing for speech enhancement},
	url = {https://research.tue.nl/en/publications/bayesian-periodogram-smoothing-for-speech-enhancement},
	pages = {135--140},
	booktitle = {Advances in computational intelligence and learning: 17th European Symposium on Artificial Neural Networks ; {ESANN} 2009 ; Bruges, Belgium, April 22 - 23 - 24, 2009 ; proceedings},
	author = {Zhang, Xueru and Ypma, Alexander and de Vries, Bert},
	urldate = {2019-07-19},
	date = {2009-12-01},
	file = {Snapshot:/Users/bert/Zotero/storage/6ESRDNEW/bayesian-periodogram-smoothing-for-speech-enhancement.html:text/html},
}

@inproceedings{ypma_bayesian_2007,
	location = {Thessaloniki, Greece},
	title = {Bayesian Feature Selection for Hearing Aid Personalization},
	url = {https://ieeexplore.ieee.org/document/4414344},
	doi = {10.1109/MLSP.2007.4414344},
	abstract = {We formulate hearing aid personalization as a linear regression. Since sample sizes may be low and the number of features may be high we resort to a Bayesian approach for sparse linear regression that can deal with many features, in order to find efficient representations for on-line usage. We compare to a heuristic feature selection approach that we optimized for speed. Results on synthetic data with irrelevant and redundant features indicate that Bayesian backfitting has labelling accuracy comparable to the heuristic approach (for moderate sample sizes), but takes much larger training times. We then determine features for hearing aid personalization by applying the method to hearing aid preference data.},
	eventtitle = {Machine Learning for Signal Processing ({MLSP})},
	pages = {425--430},
	booktitle = {2007 {IEEE} Workshop on Machine Learning for Signal Processing},
	publisher = {{IEEE}},
	author = {Ypma, Alexander and Ozer, Serkan and van der Werf, Erik and de Vries, Bert},
	date = {2007},
}

@inproceedings{ypma_robust_2006,
	title = {Robust volume control personalisation from on-line preference feedback},
	doi = {10.1109/MLSP.2006.275591},
	pages = {441--446},
	booktitle = {2006 16th {IEEE} Signal Processing Society Workshop on Machine Learning for Signal Processing},
	publisher = {{IEEE}},
	author = {Ypma, Alexander and de Vries, Bert and Geurts, Job},
	date = {2006},
	file = {Snapshot:/Users/bert/Zotero/storage/QU4CH5PQ/4053690.html:text/html},
}

@inproceedings{van_de_laar_forneylab_2018,
	title = {{ForneyLab}: a toolbox for biologically plausible free energy minimization in dynamic neural models},
	shorttitle = {{ForneyLab}},
	booktitle = {Conference on Complex Systems ({CCS})(Thessaloniki)},
	author = {van de Laar, Thijs and Cox, Marco and Senoz, Ismail and Bocharov, Ivan and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/5D9PCFGC/van de Laar et al. - 2018 - ForneyLab a toolbox for biologically plausible fr.pdf:application/pdf},
}

@inproceedings{ypma_learning_2006,
	location = {Antwerp, Belgium},
	title = {A learning volume control that is robust to user inconsistency},
	eventtitle = {Second Annual {IEEE} Benelux/{DSP} Valley Signal Processing Symposium},
	booktitle = {Proceedings of the Second Annual {IEEE} Benelux/{DSP} Valley Signal Processing Symposium, Antwerp, March 2006},
	publisher = {{IEEE}},
	author = {Ypma, Alexander and de Vries, Bert and Geurts, Job},
	date = {2006},
}

@inproceedings{wijnings_approximate_2020,
	location = {Barcelona, Spain},
	title = {Approximate Inference by Kullback-Leibler Tensor Belief Propagation},
	url = {https://ieeexplore.ieee.org/document/9054354},
	doi = {10.1109/ICASSP40776.2020.9054354},
	abstract = {Probabilistic programming provides a structured approach to signal processing algorithm design. The design task is formulated as a generative model, and the algorithm is derived through automatic inference. Efficient inference is a major challenge; e.g., the Shafer-Shenoy algorithm ({SS}) performs badly on models with large treewidth, which arise from various real-world problems. We focus on reducing the size of discrete models with large treewidth by storing intermediate factors in compressed form, thereby decoupling the variables through conditioning on introduced weights.This work proposes pruning of these weights using Kullback-Leibler divergence. We adapt a strategy from the Gaussian mixture reduction literature, leading to Kullback-Leibler Tensor Belief Propagation ({KL}-{TBP}), in which we use agglomerative hierarchical clustering to subsequently merge pairs of weights. Experiments using benchmark problems show {KL}-{TBP} consistently achieves lower approximation error than existing methods with competitive runtime.},
	eventtitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Wijnings, Patrick and Struijk, Sander and de Vries, Bert and Corporaal, Henk},
	date = {2020},
	file = {Wijnings et al. - 2020 - Approximate Inference by Kullback-Leibler Tensor B.pdf:/Users/bert/Zotero/storage/78N9XF58/Wijnings et al. - 2020 - Approximate Inference by Kullback-Leibler Tensor B.pdf:application/pdf},
}

@inproceedings{wijnings_robust_2019,
	location = {Brighton, {UK}},
	title = {Robust Bayesian Beamforming for Sources at Different Distances with Applications in Urban Monitoring},
	volume = {2019},
	isbn = {978-1-4799-8131-1},
	url = {https://research.tue.nl/en/publications/robust-bayesian-beamforming-for-sources-at-different-distances-wi},
	doi = {10.1109/ICASSP.2019.8682835},
	abstract = {Acoustic smart sensor networks can provide valuable actionable intelligence to authorities for managing safety in the urban environment. A spatial filter (beamformer) for localization and separation of acoustic sources is a key component of such a network. However, classical methods such as delay-and-sum beamforming fail, because sources are located at varying distances from the sensor array. This causes a regularization problem where either far-away sources are wrongly attenuated, or noise is wrongly amplified.
We solve this by considering source strength and location as random variables. The posterior distributions are approximated using Gibbs sampling. Each marginal is computed by combining importance sampling and inverse transform sampling using Chebyshev polynomial approximation. This leads to an iterative algorithm with similarities to deconvolution beamforming.
Our method is robust against deviations in manifold model, can deal with sources at different distances and power levels, and does not require an a priori known number of sources.},
	eventtitle = {Title of host publication2019 {IEEE} International Conference on Acoustics, Speech, and Signal Processing, {ICASSP}},
	pages = {4325--4329},
	booktitle = {2019 {IEEE} International Conference on Acoustics, Speech, and Signal Processing, {ICASSP} 2019 - Proceedings},
	publisher = {{IEEE}},
	author = {Wijnings, Patrick and Struijk, Sander and de Vries, Bert and Corporaal, Henk},
	date = {2019},
}

@inproceedings{van_diepen_-situ_2017,
	title = {An in-situ trainable gesture classifier},
	url = {https://research.tue.nl/nl/publications/an-insitu-trainable-gesture-classifier(a06698db-d2b5-4691-b7a1-7a9bd5b6376c).html},
	abstract = {Gesture recognition, i.e., the recognition of pre-defined gestures by arm or hand movements, enables a natural extension of the way we currently interact with devices (Horsley, 2016). Commercially available gesture recognition systems are usually pre-trained: the developers specify a set of gestures, and the user is provided with an algorithm that can recognize just these gestures. To improve the user experience, it is often desirable to allow users to define their own gestures. In that case, the user needs to train the recognition system herself by a set of example gestures. Crucially, this scenario requires learning gestures from just a few training examples in order to avoid overburdening the user. We present a new in-situ trainable gesture classifier based on a hierarchical probabilistic modeling approach. Casting both learning and recognition as probabilistic inference tasks yields a principled way to design and evaluate algorithm candidates. Moreover, the Bayesian approach facilitates learning of prior knowledge about gestures, which leads to},
	booktitle = {Annual machine learning conference of the Benelux (Benelearn 2017)},
	author = {van Diepen, Anouk and Cox, Marco and de Vries, Bert and Duivesteijn, Wouter and Pechenizkiy, Mykola and Fletcher, G.H.L.},
	urldate = {2019-07-08},
	date = {2017-06-10},
	langid = {english},
}

@inproceedings{van_de_laar_variational_2017-1,
	title = {Variational stabilized linear forgetting in state-space models},
	doi = {10.23919/EUSIPCO.2017.8081321},
	pages = {818--822},
	booktitle = {2017 25th European Signal Processing Conference ({EUSIPCO})},
	publisher = {{IEEE}},
	author = {Van De Laar, Thijs and Cox, Marco and van Diepen, Anouk and de Vries, Bert},
	date = {2017},
	file = {Full Text:/Users/bert/Zotero/storage/XL5CGD2K/Van De Laar et al. - 2017 - Variational stabilized linear forgetting in state-.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/57ZZVFF6/8081321.html:text/html},
}

@inproceedings{ungureanu_bayesian_2013,
	location = {Eindhoven, The Netherlands},
	title = {A Bayesian Network for Detection of Seizures},
	eventtitle = {1st Jan Beneken  Conference on Modeling and Simulation of Human Physiology},
	author = {Ungureanu, C. and de Vries, Bert},
	date = {2013},
}

@inproceedings{senoz_bayesian_2020,
	location = {Berkeley, {CA}},
	title = {Bayesian joint state and parameter tracking in autoregressive models},
	url = {https://wmkouw.github.io/publication/vmpar-hgf/},
	abstract = {We address the problem of online Bayesian state and parameter tracking in autoregressive ({AR}) models with time-varying process noise variance. The involved marginalization and expectation integrals cannot be analytically solved. Moreover, the online tracking constraint makes sampling and batch learning methods unsuitable for this problem. We propose a hybrid variational message passing algorithm that robustly tracks the time-varying dynamics of the latent states, {AR} coefficients and process noise variance. Since message passing in a factor graph is a highly modular inference approach, the proposed methods easily extend to other non-stationary dynamic modeling problems.},
	eventtitle = {Conference on Learning for Dynamics and Control, 2020},
	author = {Senoz, Ismail and Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
	date = {2020},
}

@inproceedings{van_de_laar_forneylab_2018-1,
	title = {{ForneyLab}. jl: a Julia Toolbox for Factor Graph-based Probabilistic Programming},
	url = {https://biaslab.github.io/publication/forneylab-julia-toolbox/},
	shorttitle = {{ForneyLab}. jl},
	eventtitle = {{JuliaCon} 2018},
	booktitle = {{ForneyLab}. jl: a Julia Toolbox for Factor Graph-based Probabilistic Programming},
	author = {van de Laar, Thijs and Cox, Marco and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/9KLKF46Q/van de Laar et al. - 2018 - ForneyLab. jl a Julia Toolbox for Factor Graph-ba.pdf:application/pdf},
}

@inproceedings{senoz_online_2018,
	title = {Online variational message passing in the hierarchical Gaussian filter},
	pages = {1--6},
	booktitle = {2018 {IEEE} 28th International Workshop on Machine Learning for Signal Processing ({MLSP})},
	publisher = {{IEEE}},
	author = {Şenöz, İsmail and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/SKQZZ94K/Şenöz and de Vries - 2018 - Online variational message passing in the hierarch.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BCQK6MPU/8517019.html:text/html},
}

@inproceedings{podusenko_online_2019,
	location = {Ghent, Belgium},
	title = {Online Variational Message Passing in Autoregressive Models},
	url = {https://www.researchgate.net/publication/333966160_Online_variational_message_passing_in_autoregressive_models},
	abstract = {Autoregressive ({AR}) models are one of the most popular ways to describe different time-varying processes in nature, economics, etc. However, their parameters are often estimated in a batch manner which makes them inefficient for handling large-scale real-time data. In our work, we investigate the feasibility of online parameter estimation for these types of models. We translate the {AR} model to a probabilistic factor graph which takes advantage of the factorization of the model by implementing inference as a message passing algorithm. Due to the intractability of exact parameter inference for these types of models, sum-product message passing becomes impractical. This suggests to use alternative message passing algorithms based on approximate inference, e.g., variational message passing ({VMP}), which tries to find variational distributions that serve as good proxies for the exact solution. With {VMP}, the computations for online state and parameter estimation can be automated. In Figure 1, we show a simulated second-order autoregressive process (blue crosses) at discrete time steps. Our model online updates a distribution over the values of the future states. We have implemented online {VMP}-based inference by deriving local (variational) message passing update rules for the estimation of the parameters of an {AR} model. The proposed approach has both been verified on synthetic data and validated on real data.},
	eventtitle = {{WIC} Symposium on Information Theory in the Benelux},
	author = {Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
	date = {2019-05},
}

@inproceedings{piechowiak_bilateral_2017,
	title = {A bilateral hearing-aid algorithm that provides directional benefit while preserving situational awareness},
	volume = {6},
	pages = {159--166},
	booktitle = {Proceedings of the International Symposium on Auditory and Audiological Research},
	author = {Piechowiak, Tobias and de Vries, Rob and Ma, Changxue and Dittberner, Andrew},
	date = {2017},
	file = {Full Text:/Users/bert/Zotero/storage/JXBXPJZM/Piechowiak et al. - 2017 - A bilateral hearing-aid algorithm that provides di.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/N4JHQUXF/2017-19.html:text/html},
}

@inproceedings{petkov_discrete_2011,
	title = {Discrete Choice Models for Non-Intrusive Quality Assessment},
	booktitle = {Twelfth Annual Conference of the International Speech Communication Association},
	author = {Petkov, Petko N. and Kleijn, W. Bastiaan and de Vries, Bert},
	date = {2011},
	file = {Full Text:/Users/bert/Zotero/storage/2UG5DLS2/Petkov et al. - 2011 - Discrete Choice Models for Non-Intrusive Quality A.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/S5SGD6XG/i11_0193.html:text/html},
}

@inproceedings{nguyen_probabilistic_2017,
	title = {Probabilistic inference-based reinforcement learning},
	pages = {92},
	booktitle = {Benelearn 2017: Proceedings of the Twenty-Sixth Benelux Conference on Machine Learning, Technische Universiteit Eindhoven, 9-10 June 2017},
	author = {Nguyen, M. Q. and de Vries, Bert and Tjalkens, Tjalling},
	date = {2017},
	file = {Full Text:/Users/bert/Zotero/storage/MIM4VS48/Nguyen et al. - 2017 - Probabilistic inference-based reinforcement learni.pdf:application/pdf},
}

@inproceedings{mossavat_bayesian_2010,
	title = {A Bayesian hierarchical mixture of experts approach to estimate speech quality},
	doi = {10.1109/QOMEX.2010.5516203},
	pages = {200--205},
	booktitle = {2010 Second International Workshop on Quality of Multimedia Experience ({QoMEX})},
	publisher = {{IEEE}},
	author = {Mossavat, S. Iman and Amft, Oliver and de Vries, Bert and Petkov, Petko N. and Kleijn, W. Bastiaan},
	date = {2010},
	file = {Full Text:/Users/bert/Zotero/storage/XXQFLICI/Mossavat et al. - 2010 - A Bayesian hierarchical mixture of experts approac.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2XAM6USM/5516203.html:text/html},
}

@inproceedings{koudahl_agent_2019,
	location = {Ghent, Belgium},
	title = {Agent alignment through active inference},
	url = {https://www.researchgate.net/publication/333966165_Agent_alignment_through_active_inference},
	booktitle = {{WIC} Symposium on Information Theory in the Benelux},
	author = {Koudahl, Magnus and Kouw, Wouter M. and de Vries, Bert},
	urldate = {2019-07-08},
	date = {2019-04},
	file = {(6) (PDF) Agent alignment through active inference:/Users/bert/Zotero/storage/J88CK2PW/333966165_Agent_alignment_through_active_inference.html:text/html},
}

@inproceedings{koudahl_batman_2020,
	location = {Barcelona, Spain},
	title = {{BATMAN}: Bayesian Target modelling for Active Inference},
	url = {https://ieeexplore.ieee.org/document/9053624},
	doi = {10.1109/ICASSP40776.2020.9053624},
	abstract = {Active Inference is an emerging framework for designing intelligent agents. In an Active Inference setting, any task is formulated as a variational free energy minimisation problem on a generative probabilistic model. Goal-directed behaviour relies on a clear specification of desired future observations. Learning desired observations would open up the Active Inference approach to problems where these are difficult to specify a priori. This paper introduces the {BAyesian} Target Modelling for Active {iNference} ({BATMAN}) approach, which augments an Active Inference agent with an additional, separate model that learns desired future observations from a separate data source. The main contribution of this paper is the design of a coupled generative model structure that facilitates learning desired future observations for Active Inference agents and supports integration of Active Inference and classical methods in a joint framework. We provide proof-of-concept validation for {BATMAN} through simulations.},
	eventtitle = {{ICASSP} 2020 - 2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Koudahl, Magnus and de Vries, Bert},
	date = {2020},
}

@inproceedings{houben_construction_2008,
	location = {Lake Tahoe, {CA}},
	title = {Construction of a virtual subject response database to reduce subject testing},
	eventtitle = {International Hearing Aid Conference ({IHCON})},
	booktitle = {Proceedings of the International Hearing Aid Research Conference, Lake Tahoe, {CA}, August 13-17, 2008},
	author = {Houben, Rolph and de Vries, Bert},
	date = {2008},
}

@inproceedings{leenen_learning_2007,
	title = {Learning volume control for hearing aids},
	volume = {1},
	pages = {507--514},
	booktitle = {Proceedings of the International Symposium on Auditory and Audiological Research},
	author = {Leenen, Jos and van den Berg, Almer and Ypma, Alexander and Geurts, Job and de Vries, Bert},
	date = {2007},
	file = {Full Text:/Users/bert/Zotero/storage/QFDJR9MJ/Leenen et al. - 2007 - Learning volume control for hearing aids.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/M6B2E6VS/2007-50.html:text/html},
}

@inproceedings{leenen_current_2010,
	location = {Delft, the Netherlands},
	title = {Current {DSP} and Machine Learning Trends in the Hearing Aids industry},
	eventtitle = {{IEEE} Benelux Signal Processing Symposium: Signal Processing for Digital Hearing Aids},
	author = {Leenen, Jos and de Vries, Bert},
	date = {2010},
}

@inproceedings{leenen_low_2000,
	location = {Lake Tahoe, {CA}},
	title = {A low power digital {AGC} circiut for Dynamic range control of an A/D converter},
	eventtitle = {International Hearing Aid Research Conference ({IHCON})},
	author = {Leenen, Jos and de Vries, Bert},
	date = {2000},
}

@inproceedings{heskes_incremental_2005,
	location = {Brussels, Belgium},
	title = {Incremental Utility Elicitation for Adaptive Personalization},
	url = {https://www.researchgate.net/publication/220799913_Incremental_Utility_Elicitation_for_Adaptive_Personalization},
	abstract = {Medical devices often contain many tunable parameters. The optimal setting of these parameters depends on the patient's utility function, which is often unknown. This raises two questions. First, how should we optimize the parameters given partial information about the patient's utility? And secondly, what questions do we ask to efficiently elicit this utility information? In this paper, we present a coherent probabilistic decision-theoretic framework to answer these questions. We illustrate the potential of this framework on a toy problem and discuss directions for future research.},
	eventtitle = {17th Belgium-Netherlands Conference on Artificial Intelligence},
	pages = {127--134},
	booktitle = {Proceedings of the 17th Belgium-Netherlands Conference on Artificial Intelligence, 17-18 October 2005, Brussels, Belgium},
	author = {Heskes, Tom and de Vries, Bert},
	date = {2005},
}

@inproceedings{de_vries_hearing_2014,
	location = {Lake Tahoe, {CA}},
	title = {Hearing Aid Spectral Signal Processing with an Asynchronous Warped Filterbank},
	eventtitle = {{IHCON}},
	booktitle = {Poster presentation},
	author = {de Vries, Bert and van der Werf, Erik and Dittberner, Andrew},
	date = {2014},
}

@inproceedings{de_vries_efficient_2014,
	location = {Lake Tahoe, {CA}},
	title = {Efficient Hearing Aid Spectral Signal Processing with an Asynchronous Warped Filterbank},
	eventtitle = {International Hearing Aid Reearch Conference ({IHCON})},
	author = {de Vries, Bert and van der Werf, Erik},
	date = {2014},
}

@inproceedings{farmani_probabilistic_2014-1,
	location = {Reims, France},
	title = {A probabilistic approach to hearing loss compensation},
	isbn = {978-1-4799-3694-6},
	url = {https://www.researchgate.net/publication/269277056_A_Probabilistic_Approach_to_Hearing_Loss_Compensation},
	doi = {10.1109/MLSP.2014.6958845},
	abstract = {Modern hearing aids use Dynamic Range Compression ({DRC}) as the primary solution to combat Hearing Loss ({HL}). Unfortunately, common {DRC} based solutions to hearing loss are not directly based on a proper mathematical or algorithmic description of the hearing loss problem. In this paper, we propose a probabilistic model for describing hearing loss, and we use Bayesian inference for deriving optimal {HL} compensation algorithms. We will show that, for a simple specific generative {HL} model, the inferred {HL} compensation algorithm corresponds to the classic {DRC} solution. An advantage to our approach is that it is readily extensible to more complex hearing loss models, which by automated Bayesian inference would yield complex yet optimal hearing loss compensation algorithms.},
	eventtitle = {{IEEE} International Workshop on Machine Learning for Signal Processing ({MLSP})},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	author = {Farmani, Mojtaba and de Vries, Bert},
	date = {2014},
}

@article{dijkstra_learning_2007-1,
	title = {The learning hearing aid: common-sense reasoning in hearing aid circuits},
	volume = {14},
	shorttitle = {The learning hearing aid},
	pages = {34},
	number = {11},
	journaltitle = {Hearing Review},
	author = {Dijkstra, Tjeerd {MH} and Ypma, Alexander and de Vries, Bert and Leenen, {JRGM}},
	date = {2007},
	file = {Full Text:/Users/bert/Zotero/storage/2KLTUWTK/Dijkstra et al. - 2007 - The learning hearing aid common-sense reasoning i.pdf:application/pdf},
}

@inproceedings{dijkstra_hearclip_2008,
	location = {The Netherlands},
	title = {{HearClip}: an Application of Bayesian Machine Learning to Personalization of Hearing Aids},
	volume = {2008},
	eventtitle = {Dutch Society for Audiology Meeting},
	pages = {1--14},
	booktitle = {Proceedings of the Dutch Society for Audiology Meeting, 26 September 2008},
	author = {Dijkstra, Tjeerd and Heskes, Tom and de Vries, Bert and Birlutiu, Adriana and Groot, Perry and Mossavat, S. Iman and Houben, Rolph and Dreschler, Wouter},
	date = {2008},
}

@inproceedings{dijkstra_bayesian_2006,
	location = {Benidorm, Spain},
	title = {A Bayesian decision-theoretic framework for psychophysics},
	url = {https://www.researchgate.net/publication/255568702_BYF_Bayesian_decision-theoretic_framework_for_psychophysics},
	abstract = {We extend ideas of Körding and Wolpert (4) on how to relate psychophysical response distribu- tions with Bayesian decision theory. We derive the response distribution for a von Mises likelihood with conjugate von Mises prior under a maximum aposteriori decision rule. The resulting response distribution, the Renske distribution, shows a surprising property: when likelihood and prior dis- tribution are equally wide but in complete opposition (modes are 180 deg apart) the Renske dis- tribution is bimodal with modes at 90 and 270 deg. This contrasts with the analogous normally distributed case where the response distribution is also normal and thus unimodal.},
	eventtitle = {Valencia/{ISBA} 8th World Meeting on Bayesian Statistics},
	booktitle = {Proceedings of the Valencia/{ISBA} 8th World Meeting on Bayesian Statistics, Benidorm, Spain, June 1-6, 2006},
	author = {Dijkstra, Tjeerd and de Vries, Bert and Heskes, Tom and Zoeter, Onno},
	date = {2006},
}

@inproceedings{de_vries_software_2010,
	location = {Lake Tahoe, {CA}},
	title = {A software suite for automatic beamforming calibration},
	eventtitle = {International Hearing Aid Research Conference ({IHCON})},
	author = {de Vries, Rob and de Vries, Bert and Dittberner, Andrew},
	date = {2010},
}

@inproceedings{de_vries_towards_2002,
	title = {Towards {SNR}-loss restoration in digital hearing {AIDS}},
	volume = {4},
	doi = {10.1109/ICASSP.2002.5745535},
	abstract = {Loss of understanding speech in noise, known as signal to noise ratio ({SNR}) loss, is the largest problem of most hearing aid users today. This paper discusses the most important noise reduction ({NR}) agents available in current hearing aids and their ability to improve {SNR}-loss. Then, it is investigated how the many different {NR} agents can best be integrated to maximize results. A global minimization shows that a (Generalized Sidelobe Canceller) beamformer followed by single microphone {NR} agents is the theoretically optimal configuration. Since the performance of the different {NR} agents depends on the environment, it is suggested to use an environmental classifier to select the optimal combination of {NR} agents. Either by a fixed set of rules or by using a self training network catered to the specific {SNR}-loss of the hearing impaired.},
	pages = {IV--4004--IV--4007},
	booktitle = {2002 {IEEE} International Conference on Acoustics, Speech, and Signal Processing},
	publisher = {{IEEE}},
	author = {de Vries, Rob and de Vries, Bert},
	date = {2002-05},
	keywords = {Speech, Signal to noise ratio, Acquired immune deficiency syndrome, Delay, Detectors, Multimedia communication},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/LD3U5GC2/5745535.html:text/html},
}

@inproceedings{de_vries_fast_2010,
	title = {Fast model-based fitting through active data selection},
	pages = {1},
	booktitle = {Proceedings of the International Hearing Aid Research Conference ({IHCON}), August 13-17, 2008, Lake Tahoe, California},
	author = {de Vries, Bert and Mossavat, S. I. and Dijkstra, T. M. H. and Houben, Rolph},
	date = {2010},
	file = {Snapshot:/Users/bert/Zotero/storage/3KW4RCDG/fast-model-based-fitting-through-active-data-selection-2.html:text/html},
}

@inproceedings{de_vries_evidence-based_2007,
	title = {Evidence-based hearing aid algorithms},
	volume = {1},
	pages = {533--540},
	booktitle = {Proceedings of the International Symposium on Auditory and Audiological Research},
	author = {de Vries, Bert and Dijkstra, Tjeerd and Ypma, Alexander and Leenen, Jos},
	date = {2007},
	file = {Full Text:/Users/bert/Zotero/storage/YAHIH5RC/de Vries et al. - 2007 - Evidence-based hearing aid algorithms.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/F6TCXYMB/2007-53.html:text/html},
}

@inproceedings{de_vries_integrated_2004,
	title = {An integrated approach to hearing aid algorithm design for enhancement of audibility, intelligibility and comfort},
	eventtitle = {International Hearing Aid Research Conference ({IHCON})},
	pages = {65--68},
	booktitle = {Proc. {IEEE} Benelux Signal Process. Symp.({SPS}2004)},
	author = {de Vries, Bert and de Vries, Rob},
	date = {2004},
	file = {Full Text:/Users/bert/Zotero/storage/XGQ9YQWZ/de Vries and de Vries - 2004 - An integrated approach to hearing aid algorithm de.pdf:application/pdf},
}

@misc{de_vries_slimme_2019,
	title = {Slimme gehoorassistent},
	url = {https://ict-research.nl/wordpress/wp-content/uploads/2020/02/IO-magazine-NR4-DECEMBER-2019_web.pdf},
	abstract = {Researchers from Eindhoven
University of Technology have deve -
loped a smart agent, or advanced
tuning algorithm, for hearing aids.
The agent automatically suggests the
best sound experience for the user,
based on previous experiences from
multiple users. The researchers,
who have teamed up with Philips,
{GN} Hearing, Intel and {AMS}, expect
that the software will be ready for
application in hearing aid systems
in about five years.},
	editora = {de Vries, Bert},
	editoratype = {collaborator},
	date = {2019},
}

@inproceedings{de_vries_natural_2019,
	location = {Eindhoven, The Netherlands},
	title = {Natural Artificial Intelligence},
	url = {https://www.tue.nl/en/our-university/calendar-and-events/29-10-2019-ai-symposium/},
	eventtitle = {{AI} in Engineering Symposium},
	author = {de Vries, Bert},
	date = {2019},
}

@misc{de_vries_introducing_2014,
	title = {Introducing Data Science: Hearing aids on the brink of a paradigm shift},
	url = {https://www.audiology-worldnews.com/focus-on/1215-introducing-data-science-hearing-aids-on-the-brink-of-a-paradigm-shift},
	editora = {de Vries, Bert},
	editoratype = {collaborator},
	date = {2014},
}

@inproceedings{de_vries_integrated_2004-1,
	location = {Lake Tahoe, {CA}},
	title = {An Integrated Approach to Hearing Aid Algorithm Design},
	abstract = {Over the past decade, signal processing for industrial hearing aids has evolved from analog electronics for amplitude compression to sophisticated digital systems that integrate multiple adaptive algorithms for beamforming, feedback cancellation, amplitude compression, noise reduction and signal quality enhancement. Commonly, these modules are designed independently and consequently the opportunities for undesired interactive effects are abound. In this paper we present an approach to integrate adaptive modules for amplification, amplitude compression, noise reduction and signal quality (distortion) control.},
	eventtitle = {International Hearing Aid Research Conference ({IHCON})},
	booktitle = {International Hearing Aid Research Conference, proceedings, Lake Tahoe, {CA}, August 2004},
	author = {de Vries, Bert},
	date = {2004},
	file = {Snapshot:/Users/bert/Zotero/storage/V67F3YFM/an-integrated-approach-to-hearing-aid-algorithm-design.html:text/html},
}

@inproceedings{de_vries_bayesian_2006,
	location = {Benidorm, Spain},
	title = {Bayesian Incremental Utility Elicitation with Application to Hearing Aids Personalization},
	url = {https://www.academia.edu/22633558/Bayesian_Incremental_Utility_Elicitation_With_Application_to_Hearing_Aids_Personalization},
	abstract = {Medical devices such as hearing aids often contain many tunable parameters. The optimal setting of these parameters depends on the patient’s preference (utility) function, which is often unknown. This raises two questions: (1) how should we optimize the parameters given partial information about the patient’s utility? And (2), what questions do we ask to efficiently elicit this utility information? In this paper, we present a coherent probabilistic decision-theoretic framework to answer these questions. In particular, following [2] we will derive incremental utility elicitation as a special case of Bayesian experimental design},
	eventtitle = {Valencia {ISBA} 8th World Meeting on Bayesian Statistics},
	booktitle = {Proceedings of the Valencia/{ISBA} 8th World Meeting on Bayesian Statistics, Benidorm, Spain, June 1-6, 2006},
	author = {de Vries, Bert and Heskes, Tom and Dijkstra, Tjeerd and Ypma, Alexander},
	date = {2006},
}

@inproceedings{de_vries_is_2013,
	title = {Is hearing-aid signal processing ready for machine learning?},
	volume = {4},
	rights = {Copyright (c) 2013 The Danavox Jubilee Foundation},
	url = {https://proceedings.isaar.eu/index.php/isaarproc/article/view/2013-13},
	pages = {121--132},
	booktitle = {Proceedings of the International Symposium on Auditory and Audiological Research},
	author = {de Vries, Bert and Dittberner, Andrew},
	urldate = {2019-07-04},
	date = {2013-12-15},
	langid = {english},
	file = {de Vries and Dittberner - 2013 - Is hearing-aid signal processing ready for machine.pdf:/Users/bert/Zotero/storage/PTTVHH3X/de Vries and Dittberner - 2013 - Is hearing-aid signal processing ready for machine.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/QFZIRUH5/75.html:text/html},
}

@inproceedings{de_vries_complexity_2008,
	title = {The Complexity of Hearing Aid Fitting},
	url = {https://research.tue.nl/en/publications/the-complexity-of-hearing-aid-fitting},
	eventtitle = {International Symposium on Auditory and Audiological Reseach},
	booktitle = {Proceedings of the International Symposium on Auditory and Audiological Reseach, Helsingor, Denmark, August 2007},
	author = {de Vries, Bert},
	urldate = {2019-07-08},
	date = {2008},
	file = {Snapshot:/Users/bert/Zotero/storage/XMGHTCHA/the-complexity-of-hearing-aid-fitting.html:text/html},
}

@inproceedings{cox_forneylab_2018,
	location = {Boston, {MA}},
	title = {{ForneyLab}. jl: Fast and flexible automated inference through message passing in Julia},
	shorttitle = {{ForneyLab}. jl},
	eventtitle = {First International Conference on Probabilistic Programming},
	booktitle = {{ProbProg}},
	author = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/BSU7LT53/Cox et al. - 2018 - ForneyLab. jl Fast and flexible automated inferen.pdf:application/pdf},
}

@inproceedings{cox_robust_2018,
	title = {Robust Expectation propagation in factor graphs involving both continuous and binary variables},
	doi = {10.23919/EUSIPCO.2018.8553490},
	pages = {2583--2587},
	booktitle = {2018 26th European Signal Processing Conference ({EUSIPCO})},
	publisher = {{IEEE}},
	author = {Cox, Marco and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/7YWS6R8Y/Cox and De Vries - 2018 - Robust Expectation propagation in factor graphs in.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/BAYGR3K5/8553490.html:text/html},
}

@inproceedings{cox_gaussian_2017,
	title = {A Gaussian process mixture prior for hearing loss modeling},
	url = {https://research.tue.nl/en/publications/a-gaussian-process-mixture-prior-for-hearing-loss-modeling},
	eventtitle = {Annual machine learning conference of the Benelux (Benelearn 2017)},
	pages = {74--76},
	booktitle = {Benelearn 2017: Proceedings of the Twenty-Sixth Benelux Conference on Machine Learning, Technische Universiteit Eindhoven, 9-10 June 2017},
	author = {Cox, Marco and de Vries, Bert},
	urldate = {2019-07-08},
	date = {2017-06-09},
	file = {Full Text PDF:/Users/bert/Zotero/storage/V3AJWWZM/Cox and Vries - 2017 - A Gaussian process mixture prior for hearing loss .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/WDJYSFGE/a-gaussian-process-mixture-prior-for-hearing-loss-modeling.html:text/html},
}

@inproceedings{cox_parametric_2017,
	title = {A parametric approach to Bayesian optimization with pairwise comparisons},
	booktitle = {{NIPS} Workshop on Bayesian Optimization ({BayesOpt} 2017), December 9, 2017, Long Beach, {USA}},
	author = {Cox, Marco and de Vries, Bert},
	date = {2017},
	file = {Full Text:/Users/bert/Zotero/storage/8ZV6J4PE/Cox et al. - 2017 - A parametric approach to Bayesian optimization wit.pdf:application/pdf},
}

@inproceedings{bocharov_k-shot_2017,
	title = {K-shot learning of acoustic context},
	pages = {1--6},
	booktitle = {{NIPS} Workshop on Machine Learning for Audio Signal Processing},
	author = {Bocharov, Ivan and de Vries, Bert and Tjalkens, Tjalling},
	date = {2017},
	file = {Full Text:/Users/bert/Zotero/storage/QSHL95DG/Bocharov et al. - 2017 - K-shot learning of acoustic context.pdf:application/pdf},
}

@inproceedings{bocharov_acoustic_2018,
	title = {Acoustic Scene Classification from Few Examples},
	doi = {10.23919/EUSIPCO.2018.8553184},
	pages = {862--866},
	booktitle = {2018 26th European Signal Processing Conference ({EUSIPCO})},
	publisher = {{IEEE}},
	author = {Bocharov, Ivan and Tjalkens, Tjalling and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/S8CU8M6D/Bocharov et al. - 2018 - Acoustic Scene Classification from Few Examples.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/VHLWCSKN/8553184.html:text/html},
}

@inproceedings{birlutiu_towards_2010,
	title = {Towards hearing-aid personalization: preference elicitation from audiological data},
	doi = {10.1109/ICASSP.2002.5745535},
	shorttitle = {Towards hearing-aid personalization},
	pages = {1},
	booktitle = {Proceedings of the Scientific {ICT}-Research Event Netherlands ({SIREN}-08), 29 September 2008, Amsterdam, The Netherlands September 2008},
	publisher = {{STW} Technology Foundation},
	author = {Birlutiu, Adriana and Groot, Perry and Heskes, Tom and Mosavat, Iman and de Vries, Bert and Dijkstra, Tjeerd},
	date = {2010},
	file = {Full Text:/Users/bert/Zotero/storage/C3BFYJEF/Birlutiu et al. - 2010 - Towards hearing-aid personalization preference el.pdf:application/pdf},
}

@inproceedings{birlutiu_personalization_2008,
	title = {Personalization of Hearing Aids through Bayesian Preference Elicitation},
	url = {https://research.tue.nl/en/publications/personalization-of-hearing-aids-through-bayesian-preference-elici},
	booktitle = {Proceedings of the {NIPS} Workshop on User Adaptive Systems, Whistler, Canada, December 8, 2006},
	author = {Birlutiu, Adriana and de Vries, Bert},
	urldate = {2019-07-08},
	date = {2008},
	file = {Snapshot:/Users/bert/Zotero/storage/G6QTQEHP/personalization-of-hearing-aids-through-bayesian-preference-elici.html:text/html},
}

@article{de_vries_bayesian_2008,
	title = {Bayesian Machine Learning for Personalization of Hearing Aid Algorithms},
	url = {https://research.tue.nl/en/publications/bayesian-machine-learning-for-personalization-of-hearing-aid-algo},
	journaltitle = {International Hearing Aid Research Conference, Lake Tahoe, {CA}, August 2006},
	author = {de Vries, Bert},
	urldate = {2019-07-09},
	date = {2008},
	file = {Snapshot:/Users/bert/Zotero/storage/ESKRRPBX/bayesian-machine-learning-for-personalization-of-hearing-aid-algo.html:text/html},
}

@article{van_diepen_probabilistic_2018,
	title = {A Probabilistic Modelling Approach to One-Shot Gesture Recogntion},
	url = {https://arxiv.org/abs/1806.11408},
	journaltitle = {{arXiv} preprint {arXiv}:1806.11408},
	author = {van Diepen, Anouk and Cox, Marco and de Vries, Bert},
	date = {2018},
	file = {Full Text:/Users/bert/Zotero/storage/98SHT9HB/van Diepen et al. - 2018 - A Probabilistic Modelling Approach to One-Shot Ges.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/F4AFZDWR/1806.html:text/html},
}

@article{van_de_laar_probabilistic_2016-1,
	title = {A Probabilistic Modelling Approach to Hearing Loss Compensation},
	volume = {24},
	url = {https://biaslab.github.io/pdf/benelearn2017/a_probabilistic_modeling_approach_to_hlc.pdf},
	doi = {10.1109/TASLP.2016.2599275},
	abstract = {Hearing Aid ({HA}) algorithms need to be tuned (“fitted”) to match the impairment of each specific patient. The lack of a fundamental {HA} fitting theory is a strong contributing factor to an unsatisfying sound experience for about 20\% of {HA} patients. This paper proposes a probabilistic modeling approach to the design of {HA} algorithms. The proposed method relies on a generative probabilistic model for the hearing loss problem and provides for automated inference of the corresponding (1) signal processing algorithm, (2) the fitting solution as well as (3) a principled performance evaluation metric. All three tasks are realized as message passing algorithms in a factor graph representation of the generative model, which in principle allows for fast implementation on {HA} or mobile device hardware. The methods are theoretically worked out and simulated with a custom-built factor graph toolbox for a specific hearing loss model.},
	pages = {2200--2213},
	number = {11},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech and Language Processing ({TASLP})},
	author = {van de Laar, Thijs and de Vries, Bert},
	date = {2016},
	file = {Full Text:/Users/bert/Zotero/storage/7SGDERG6/van de Laar et al. - 2016 - A probabilistic modeling approach to hearing loss .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UXNAYTCJ/citation.html:text/html},
}

@article{cox_factor_2019-1,
	title = {A factor graph approach to automated design of Bayesian signal processing algorithms},
	volume = {104},
	doi = {10.1016/j.ijar.2018.11.002},
	pages = {185--204},
	journaltitle = {International Journal of Approximate Reasoning},
	author = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
	date = {2019},
	file = {Full Text:/Users/bert/Zotero/storage/Q5TIVYK3/Cox et al. - 2019 - A factor graph approach to automated design of Bay.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/GDIAW3TN/S0888613X18304298.html:text/html},
}

@article{kuper_numerical_2020,
	title = {Numerical Gaussian process Kalman filtering},
	url = {http://arxiv.org/abs/1912.01234},
	abstract = {In this manuscript we introduce numerical Gaussian process Kalman filtering ({GPKF}). Numerical Gaussian processes have recently been developed to simulate spatiotemporal models. The contribution of this paper is to embed numerical Gaussian processes into the recursive Kalman filter equations. This embedding enables us to do Kalman filtering on infinite-dimensional systems using Gaussian processes. This is possible because i) we are obtaining a linear model from numerical Gaussian processes, and ii) the states of this model are by definition Gaussian distributed random variables. Convenient properties of the numerical {GPKF} are that no spatial discretization of the model is necessary, and manual setting up of the Kalman filter, that is fine-tuning the process and measurement noise levels by hand is not required, as they are learned online from the data stream. We showcase the capability of the numerical {GPKF} in a simulation study of the advection equation.},
	journaltitle = {{arXiv}:1912.01234 [cs, eess, stat]},
	author = {Küper, Armin and Waldherr, Steffen},
	urldate = {2020-12-13},
	date = {2020-05-11},
	eprinttype = {arxiv},
	eprint = {1912.01234},
	keywords = {Statistics - Computation, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/CWP7NKB3/1912.html:text/html;Küper and Waldherr - 2020 - Numerical Gaussian process Kalman filtering.pdf:/Users/bert/Zotero/storage/TAGCA6HQ/Küper and Waldherr - 2020 - Numerical Gaussian process Kalman filtering.pdf:application/pdf},
}

@article{xiang_model-based_2020,
	title = {Model-based Bayesian analysis in acoustics—A tutorial},
	volume = {148},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/10.0001731},
	doi = {10.1121/10.0001731},
	abstract = {Bayesian analysis has been increasingly applied in many acoustical applications. In these applications, prediction models are often involved to better understand the process under investigation by purposely learning from the experimental observations. When involving the model-based data analysis within a Bayesian framework, issues related to incorporating the experimental data and assigning probabilities into the inferential learning procedure need fundamental consideration. This paper introduces Bayesian probability theory on a tutorial level, including fundamental rules for manipulating the probabilities, and the principle of maximum entropy for assignment of necessary probabilities prior to the data analysis. This paper also employs a number of examples recently published in this journal to explain detailed steps on how to apply the model-based Bayesian inference to solving acoustical problems.},
	pages = {1101--1120},
	number = {2},
	journaltitle = {The Journal of the Acoustical Society of America},
	shortjournal = {The Journal of the Acoustical Society of America},
	author = {Xiang, Ning},
	urldate = {2020-12-11},
	date = {2020-08-01},
	note = {Publisher: Acoustical Society of America},
	file = {Snapshot:/Users/bert/Zotero/storage/FMET38FB/10.html:text/html;Xiang - 2020 - Model-based Bayesian analysis in acoustics—A tutor.pdf:/Users/bert/Zotero/storage/SY9S5VEH/Xiang - 2020 - Model-based Bayesian analysis in acoustics—A tutor.pdf:application/pdf},
}

@article{blundell_weight_2015,
	title = {Weight Uncertainty in Neural Networks},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on {MNIST} classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	journaltitle = {{arXiv}:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	urldate = {2020-12-09},
	date = {2015-05-21},
	eprinttype = {arxiv},
	eprint = {1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/UXND8IA5/1505.html:text/html;Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:/Users/bert/Zotero/storage/IUQD47LZ/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@article{yu_influence_2020,
	title = {Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems},
	url = {http://arxiv.org/abs/2007.04915},
	shorttitle = {Influence Diagram Bandits},
	abstract = {We propose a novel framework for structured bandits, which we call an influence diagram bandit. Our framework captures complex statistical dependencies between actions, latent variables, and observations; and thus unifies and extends many existing models, such as combinatorial semi-bandits, cascading bandits, and low-rank bandits. We develop novel online learning algorithms that learn to act efficiently in our models. The key idea is to track a structured posterior distribution of model parameters, either exactly or approximately. To act, we sample model parameters from their posterior and then use the structure of the influence diagram to find the most optimistic action under the sampled parameters. We empirically evaluate our algorithms in three structured bandit problems, and show that they perform as well as or better than problem-specific state-of-the-art baselines.},
	journaltitle = {{arXiv}:2007.04915 [cs, stat]},
	author = {Yu, Tong and Kveton, Branislav and Wen, Zheng and Zhang, Ruiyi and Mengshoel, Ole J.},
	urldate = {2020-12-09},
	date = {2020-07-09},
	eprinttype = {arxiv},
	eprint = {2007.04915},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6SHQCXFY/2007.html:text/html;Yu et al. - 2020 - Influence Diagram Bandits Variational Thompson Sa.pdf:/Users/bert/Zotero/storage/HLWNIR2W/Yu et al. - 2020 - Influence Diagram Bandits Variational Thompson Sa.pdf:application/pdf},
}

@article{rubin_future_2020,
	title = {Future climates: Markov blankets and active inference in the biosphere},
	volume = {17},
	issn = {1742-5662, 1742-5662},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsif.2020.0503},
	doi = {10.1098/rsif.2020.0503},
	shorttitle = {Future climates},
	abstract = {We formalize the Gaia hypothesis about the Earth climate system using advances in theoretical biology based on the minimization of variational free energy. This amounts to the claim that non-equilibrium steady-state dynamics—that underwrite our climate—depend on the Earth system possessing a Markov blanket. Our formalization rests on how the metabolic rates of the biosphere (understood as Markov blanket's internal states) change with respect to solar radiation at the Earth's surface (i.e. external states), through the changes in greenhouse and albedo effects (i.e. active states) and ocean-driven global temperature changes (i.e. sensory states). Describing the interaction between the metabolic rates and solar radiation as climatic states—in a Markov blanket—amounts to describing the dynamics of the internal states as actively inferring external states. This underwrites climatic non-equilibrium steady-state through free energy minimization and thus a form of planetary autopoiesis.},
	pages = {20200503},
	number = {172},
	journaltitle = {Journal of The Royal Society Interface},
	shortjournal = {J. R. Soc. Interface.},
	author = {Rubin, Sergio and Parr, Thomas and Da Costa, Lancelot and Friston, Karl},
	urldate = {2020-11-29},
	date = {2020-11},
	langid = {english},
	file = {Rubin et al. - 2020 - Future climates Markov blankets and active infere.pdf:/Users/bert/Zotero/storage/ECUBNIB5/Rubin et al. - 2020 - Future climates Markov blankets and active infere.pdf:application/pdf},
}

@article{jafarian_parametric_2020,
	title = {Parametric dynamic causal modelling},
	url = {http://arxiv.org/abs/2008.11650},
	abstract = {This technical note introduces parametric dynamic causal modelling, a method for inferring slow changes in biophysical parameters that control fluctuations of fast neuronal states. The application domain we have in mind is inferring slow changes in variables (e.g., extracellular ion concentrations or synaptic efficacy) that underlie phase transitions in brain activity (e.g., paroxysmal seizure activity). The scheme is efficient and yet retains a biophysical interpretation, in virtue of being based on established neural mass models that are equipped with a slow dynamic on the parameters (such as synaptic rate constants or effective connectivity). In brief, we use an adiabatic approximation to summarise fast fluctuations in hidden neuronal states (and their expression in sensors) in terms of their second order statistics; namely, their complex cross spectra. This allows one to specify and compare models of slowly changing parameters (using Bayesian model reduction) that generate a sequence of empirical cross spectra of electrophysiological recordings. Crucially, we use the slow fluctuations in the spectral power of neuronal activity as empirical priors on changes in synaptic parameters. This introduces a circular causality, in which synaptic parameters underwrite fast neuronal activity that, in turn, induces activity-dependent plasticity in synaptic parameters. In this foundational paper, we describe the underlying model, establish its face validity using simulations and provide an illustrative application to a chemoconvulsant animal model of seizure activity.},
	journaltitle = {{arXiv}:2008.11650 [q-bio]},
	author = {Jafarian, Amirhossein and Zeidman, Peter and Wykes, Rob C. and Walker, Matthew and Friston, Karl J.},
	urldate = {2020-11-28},
	date = {2020-08-26},
	eprinttype = {arxiv},
	eprint = {2008.11650},
	keywords = {Quantitative Biology - Quantitative Methods},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2BKNFYXI/2008.html:text/html;Jafarian et al. - 2020 - Parametric dynamic causal modelling.pdf:/Users/bert/Zotero/storage/H5ZRD4CE/Jafarian et al. - 2020 - Parametric dynamic causal modelling.pdf:application/pdf},
}

@online{andrews_math_2020,
	title = {The Math is not the Territory: Navigating the Free Energy Principle},
	url = {http://philsci-archive.pitt.edu/18315/},
	shorttitle = {The Math is not the Territory},
	abstract = {The free energy principle ({FEP}) has seen extensive philosophical engagement— both from a general philosophy of science perspective and from the perspective of philosophies of specific sciences: cognitive science, neuroscience, and biology. The literature on the {FEP} has attempted to draw out specific philosophical commitments and entailments of the framework. But the most fundamental questions, from the perspective of philosophy of science, remain open: To what discipline(s) does the {FEP} belong? Does it make falsifiable claims? What sort of scientific object is it? Is it to be taken as a representation of contingent states of affairs in nature? Does it constitute knowledge? What role is it in- tended to play in relation to empirical research? Does the {FEP} even properly belong to the domain of science? To the extent that it has engaged with them at all, the extant literature has begged, dodged, dismissed, and skirted around these questions, without ever addressing them head-on. These questions must, I urge, be answered satisfactorily before we can make any headway on the philosophical consequences of the {FEP}. I take preliminary steps towards answering these questions in this paper, first by examining closely key formal elements of the framework and the implications they hold for its utility, and second, by highlighting potential modes of interpreting the {FEP} in light of an abundant philosophical literature on scientific modelling.},
	type = {Preprint},
	author = {Andrews, Mel},
	urldate = {2020-11-26},
	date = {2020-10-25},
	langid = {english},
	file = {Andrews - 2020 - The Math is not the Territory Navigating the Free.pdf:/Users/bert/Zotero/storage/73QSSKN9/Andrews - 2020 - The Math is not the Territory Navigating the Free.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/XKAUAHFH/18315.html:text/html},
}

@article{sedley_integrative_2016,
	title = {An Integrative Tinnitus Model Based on Sensory Precision},
	volume = {39},
	issn = {0166-2236},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5152595/},
	doi = {10.1016/j.tins.2016.10.004},
	abstract = {Tinnitus is a common disorder that often complicates hearing loss. Its mechanisms are incompletely understood. Current theories proposing pathophysiology from the ear to the cortex cannot individually – or collectively – explain the range of experimental evidence available. We propose a new framework, based on predictive coding, in which spontaneous activity in the subcortical auditory pathway constitutes a ‘tinnitus precursor’ which is normally ignored as imprecise evidence against the prevailing percept of ‘silence’. Extant models feature as contributory mechanisms acting to increase either the intensity of the precursor or its precision. If precision (i.e., postsynaptic gain) rises sufficiently then tinnitus is perceived. Perpetuation arises through focused attention, which further increases the precision of the precursor, and resetting of the default prediction to expect tinnitus., Existing tinnitus models, including mutually exclusive mechanisms, invoke causes from the ear to high-level cortical brain networks., The generic framework of predictive coding explains perception as the integration of sensory information and prior predictions, each weighted by its precision., In our model, previously proposed neural correlates of ‘tinnitus’ largely relate to hearing damage, rather than to tinnitus per se, and reflect an increase in the precision of spontaneous activity in the auditory pathway, which acts as a tinnitus precursor., Perception of tinnitus emerges if the precision of the precursor rises sufficiently to override the default (null hypothesis) percept of ‘silence’., Tinnitus becomes chronic when perceptual inference mechanisms learn to expect tinnitus, engaging connections between auditory and parahippocampal cortex.},
	pages = {799--812},
	number = {12},
	journaltitle = {Trends in Neurosciences},
	shortjournal = {Trends Neurosci},
	author = {Sedley, William and Friston, Karl J. and Gander, Phillip E. and Kumar, Sukhbinder and Griffiths, Timothy D.},
	urldate = {2020-11-21},
	date = {2016-12},
	pmid = {27871729},
	pmcid = {PMC5152595},
	file = {Sedley et al. - 2016 - An Integrative Tinnitus Model Based on Sensory Pre.pdf:/Users/bert/Zotero/storage/MPBER5MN/Sedley et al. - 2016 - An Integrative Tinnitus Model Based on Sensory Pre.pdf:application/pdf},
}

@article{hullfish_prediction_2019,
	title = {Prediction and perception: Insights for (and from) tinnitus},
	volume = {102},
	issn = {0149-7634},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763419300867},
	doi = {10.1016/j.neubiorev.2019.04.008},
	shorttitle = {Prediction and perception},
	abstract = {More than 150 years have passed since Helmholtz first described perception as a process of unconscious inference about the causes of sensations. His ideas have since inspired a wealth of literature investigating the mechanisms underlying these inferences. In recent years, much of this work has converged on the notion that the brain is a hierarchical generative model of its environment that predicts sensations and updates itself based on prediction errors. Here, we build a case for modeling tinnitus from this perspective, i.e. predictive coding. We emphasize two key claims: (1) acute tinnitus reflects an increase in sensory precision in related frequency channels and (2) chronic tinnitus reflects a change in the brain’s default prediction. We further discuss specific neural biomarkers that would constitute evidence for or against these claims. Finally, we explore the implications of our model for clinical intervention strategies. We conclude that predictive coding offers the basis for a unifying theory of cognitive neuroscience, which we demonstrate with several examples linking tinnitus to other lines of brain research.},
	pages = {1--12},
	journaltitle = {Neuroscience \& Biobehavioral Reviews},
	shortjournal = {Neuroscience \& Biobehavioral Reviews},
	author = {Hullfish, Jeffrey and Sedley, William and Vanneste, Sven},
	urldate = {2020-11-21},
	date = {2019-07-01},
	langid = {english},
	keywords = {Predictive coding, Active inference, Perception, Learning, Bayes, Inference, Belief, Phantom perception, Prediction error, Tinnitus, Unsupervised learning},
	file = {Hullfish et al. - 2019 - Prediction and perception Insights for (and from).pdf:/Users/bert/Zotero/storage/RXYXY777/Hullfish et al. - 2019 - Prediction and perception Insights for (and from).pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/5ERK3PSN/S0149763419300867.html:text/html},
}

@article{xie_factor-graph_2020,
	title = {A Factor-Graph Approach for Optimization Problems with Dynamics Constraints},
	url = {http://arxiv.org/abs/2011.06194},
	abstract = {In this paper, we introduce dynamics factor graphs as a graphical framework to solve dynamics problems and kinodynamic motion planning problems with full consideration of whole-body dynamics and contacts. A factor graph representation of dynamics problems provides an insightful visualization of their mathematical structure and can be used in conjunction with sparse nonlinear optimizers to solve challenging, high-dimensional optimization problems in robotics. We can easily formulate kinodynamic motion planning as a trajectory optimization problem with factor graphs. We demonstrate the flexibility and descriptive power of dynamics factor graphs by applying them to control various dynamical systems, ranging from a simple cart pole to a 12-{DoF} quadrupedal robot.},
	journaltitle = {{arXiv}:2011.06194 [cs]},
	author = {Xie, Mandy and Escontrela, Alejandro and Dellaert, Frank},
	urldate = {2020-11-17},
	date = {2020-11-10},
	eprinttype = {arxiv},
	eprint = {2011.06194},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/TMKKLSS9/2011.html:text/html;Xie et al. - 2020 - A Factor-Graph Approach for Optimization Problems .pdf:/Users/bert/Zotero/storage/5TPCE5FJ/Xie et al. - 2020 - A Factor-Graph Approach for Optimization Problems .pdf:application/pdf},
}

@article{hobson_minds_2020,
	title = {Minds and Brains, Sleep and Psychiatry},
	url = {https://prcp.psychiatryonline.org/doi/10.1176/appi.prcp.20200023},
	doi = {10.1176/appi.prcp.20200023},
	abstract = {{ObjectiveThis} article offers a philosophical thesis for psychiatric disorders that rests upon some simple truths about the mind and brain. Specifically, it asks whether the dual aspect monism—that emerges from sleep research and theoretical neurobiology—can be applied to pathophysiology and psychopathology in psychiatry.{MethodsOur} starting point is that the mind and brain are emergent aspects of the same (neuronal) dynamics; namely, the brain–mind. Our endpoint is that synaptic dysconnection syndromes inherit the same dual aspect; namely, aberrant inference or belief updating on the one hand, and a failure of neuromodulatory synaptic gain control on the other. We start with some basic considerations from sleep research that integrate the phenomenology of dreaming with the neurophysiology of sleep.{ResultsWe} then leverage this treatment by treating the brain as an organ of inference. Our particular focus is on the role of precision (i.e., the representation of uncertainty) in belief updating and the accompanying synaptic mechanisms.{ConclusionsFinally}, we suggest a dual aspect approach—based upon belief updating (i.e., mind processes) and its neurophysiological implementation (i.e., brain processes)—has a wide explanatory compass for psychiatry and various movement disorders. This approach identifies the kind of pathophysiology that underwrites psychopathology—and points to certain psychotherapeutic and psychopharmacological targets, which may stand in mechanistic relation to each other.},
	pages = {n/a--n/a},
	journaltitle = {Psychiatric Research and Clinical Practice},
	shortjournal = {{PRCP}},
	author = {Hobson, Allan J. and Gott, Jarrod A. and Friston, Karl J.},
	urldate = {2020-11-16},
	date = {2020-11-10},
	note = {Publisher: American Psychiatric Publishing},
	file = {Hobson et al. - 2020 - Minds and Brains, Sleep and Psychiatry.pdf:/Users/bert/Zotero/storage/V68USQRH/Hobson et al. - 2020 - Minds and Brains, Sleep and Psychiatry.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/F577D7GH/appi.prcp.html:text/html},
}

@article{wang_too_2020,
	title = {Too many cooks: Bayesian inference for coordinating multi-agent collaboration},
	url = {http://arxiv.org/abs/2003.11778},
	shorttitle = {Too many cooks},
	abstract = {Collaboration requires agents to coordinate their behavior on the fly, sometimes cooperating to solve a single task together and other times dividing it up into sub-tasks to work on in parallel. Underlying the human ability to collaborate is theory-of-mind, the ability to infer the hidden mental states that drive others to act. Here, we develop Bayesian Delegation, a decentralized multi-agent learning mechanism with these abilities. Bayesian Delegation enables agents to rapidly infer the hidden intentions of others by inverse planning. We test Bayesian Delegation in a suite of multi-agent Markov decision processes inspired by cooking problems. On these tasks, agents with Bayesian Delegation coordinate both their high-level plans (e.g. what sub-task they should work on) and their low-level actions (e.g. avoiding getting in each other's way). In a self-play evaluation, Bayesian Delegation outperforms alternative algorithms. Bayesian Delegation is also a capable ad-hoc collaborator and successfully coordinates with other agent types even in the absence of prior experience. Finally, in a behavioral experiment, we show that Bayesian Delegation makes inferences similar to human observers about the intent of others. Together, these results demonstrate the power of Bayesian Delegation for decentralized multi-agent collaboration.},
	journaltitle = {{arXiv}:2003.11778 [cs]},
	author = {Wang, Rose E. and Wu, Sarah A. and Evans, James A. and Tenenbaum, Joshua B. and Parkes, David C. and Kleiman-Weiner, Max},
	urldate = {2020-11-16},
	date = {2020-07-05},
	eprinttype = {arxiv},
	eprint = {2003.11778},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/DZRCJSM5/2003.html:text/html;Wang et al. - 2020 - Too many cooks Bayesian inference for coordinatin.pdf:/Users/bert/Zotero/storage/2K7YE2KY/Wang et al. - 2020 - Too many cooks Bayesian inference for coordinatin.pdf:application/pdf},
}

@article{shore_axiomatic_1980,
	title = {Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy},
	volume = {26},
	issn = {1557-9654},
	doi = {10.1109/TIT.1980.1056144},
	abstract = {Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.},
	pages = {26--37},
	number = {1},
	journaltitle = {{IEEE} Transactions on Information Theory},
	author = {Shore, J. and Johnson, R.},
	date = {1980-01},
	note = {Conference Name: {IEEE} Transactions on Information Theory},
	keywords = {Entropy functions},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/TNBWBEYT/1056144.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/AKIDJVCM/1056144.html:text/html;Submitted Version:/Users/bert/Zotero/storage/46E7E8EP/Shore and Johnson - 1980 - Axiomatic derivation of the principle of maximum e.pdf:application/pdf},
}

@article{abdulsamad_hierarchical_2020,
	title = {Hierarchical Decomposition of Nonlinear Dynamics and Control for System Identification and Policy Distillation},
	url = {http://arxiv.org/abs/2005.01432},
	abstract = {The control of nonlinear dynamical systems remains a major challenge for autonomous agents. Current trends in reinforcement learning ({RL}) focus on complex representations of dynamics and policies, which have yielded impressive results in solving a variety of hard control tasks. However, this new sophistication and extremely over-parameterized models have come with the cost of an overall reduction in our ability to interpret the resulting policies. In this paper, we take inspiration from the control community and apply the principles of hybrid switching systems in order to break down complex dynamics into simpler components. We exploit the rich representational power of probabilistic graphical models and derive an expectation-maximization ({EM}) algorithm for learning a sequence model to capture the temporal structure of the data and automatically decompose nonlinear dynamics into stochastic switching linear dynamical systems. Moreover, we show how this framework of switching models enables extracting hierarchies of Markovian and auto-regressive locally linear controllers from nonlinear experts in an imitation learning scenario.},
	journaltitle = {{arXiv}:2005.01432 [cs, stat]},
	author = {Abdulsamad, Hany and Peters, Jan},
	urldate = {2020-11-10},
	date = {2020-05-12},
	eprinttype = {arxiv},
	eprint = {2005.01432},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Abdulsamad and Peters - 2020 - Hierarchical Decomposition of Nonlinear Dynamics a.pdf:/Users/bert/Zotero/storage/H2I7ARYN/Abdulsamad and Peters - 2020 - Hierarchical Decomposition of Nonlinear Dynamics a.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/BWXQ2M62/2005.html:text/html},
}

@article{bezanson_julia_2017,
	title = {Julia: A Fresh Approach to Numerical Computing},
	volume = {59},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/141000671},
	doi = {10.1137/141000671},
	shorttitle = {Julia},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature" by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item High-level dynamic programs have to be slow. {\textbackslash}item One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
	pages = {65--98},
	number = {1},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
	urldate = {2022-02-03},
	date = {2017-01-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Julia, parallel, 65Y05, 68N15, 97P40, numerical, scientific computing},
	file = {Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:/Users/bert/Zotero/storage/NR5VVY86/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf},
}

@software{laar_forneylabjl_2022,
	title = {{ForneyLab}.jl},
	rights = {{MIT}},
	url = {https://github.com/biaslab/ForneyLab.jl},
	abstract = {Julia package for automatically generating Bayesian inference algorithms through message passing on Forney-style factor graphs.},
	publisher = {{BIASlab}},
	author = {Laar, Thijs van de and Cox, Marco},
	urldate = {2022-02-03},
	date = {2022-01-31},
	note = {original-date: 2018-06-27T09:12:26Z},
	keywords = {bayesian-methods, factor-graph, machine-learning, probabilistic-graphical-models, probabilistic-programming, state-space-models},
}

@software{bagaev_rocketjl_2020,
	title = {Rocket.jl},
	rights = {{MIT} license},
	url = {https://github.com/biaslab/Rocket.jl},
	author = {Bagaev, Dmitry},
	date = {2020},
}

@article{heins_pymdp_2022,
	title = {pymdp: A Python library for active inference in discrete state spaces},
	url = {http://arxiv.org/abs/2201.03904},
	shorttitle = {pymdp},
	abstract = {Active inference is an account of cognition and behavior in complex systems which brings together action, perception, and learning under the theoretical mantle of Bayesian inference. Active inference has seen growing applications in academic research, especially in fields that seek to model human or animal behavior. While in recent years, some of the code arising from the active inference literature has been written in open source languages like Python and Julia, to-date, the most popular software for simulating active inference agents is the {DEM} toolbox of {SPM}, a {MATLAB} library originally developed for the statistical analysis and modelling of neuroimaging data. Increasing interest in active inference, manifested both in terms of sheer number as well as diversifying applications across scientific disciplines, has thus created a need for generic, widely-available, and user-friendly code for simulating active inference in open-source scientific computing languages like Python. The Python package we present here, pymdp (see https://github.com/infer-actively/pymdp), represents a significant step in this direction: namely, we provide the first open-source package for simulating active inference with partially-observable Markov Decision Processes or {POMDPs}. We review the package's structure and explain its advantages like modular design and customizability, while providing in-text code blocks along the way to demonstrate how it can be used to build and run active inference processes with ease. We developed pymdp to increase the accessibility and exposure of the active inference framework to researchers, engineers, and developers with diverse disciplinary backgrounds. In the spirit of open-source software, we also hope that it spurs new innovation, development, and collaboration in the growing active inference community.},
	journaltitle = {{arXiv}:2201.03904 [cs, q-bio]},
	author = {Heins, Conor and Millidge, Beren and Demekas, Daphne and Klein, Brennan and Friston, Karl and Couzin, Iain and Tschantz, Alexander},
	urldate = {2022-02-03},
	date = {2022-01-11},
	eprinttype = {arxiv},
	eprint = {2201.03904},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition, Computer Science - Mathematical Software},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/RFEXR7P9/Heins et al. - 2022 - pymdp A Python library for active inference in di.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/T8RU4YHJ/2201.html:text/html},
}

@article{senoz_variational_2021,
	title = {Variational Message Passing and Local Constraint Manipulation in Factor Graphs},
	volume = {23},
	issn = {1099-4300},
	doi = {10.3390/e23070807},
	abstract = {Accurate evaluation of Bayesian model evidence for a given data set is a fundamental problem in model development. Since evidence evaluations are usually intractable, in practice variational free energy ({VFE}) minimization provides an attractive alternative, as the {VFE} is an upper bound on negative model log-evidence ({NLE}). In order to improve tractability of the {VFE}, it is common to manipulate the constraints in the search space for the posterior distribution of the latent variables. Unfortunately, constraint manipulation may also lead to a less accurate estimate of the {NLE}. Thus, constraint manipulation implies an engineering trade-off between tractability and accuracy of model evidence estimation. In this paper, we develop a unifying account of constraint manipulation for variational inference in models that can be represented by a (Forney-style) factor graph, for which we identify the Bethe Free Energy as an approximation to the {VFE}. We derive well-known message passing algorithms from first principles, as the result of minimizing the constrained Bethe Free Energy ({BFE}). The proposed method supports evaluation of the {BFE} in factor graphs for model scoring and development of new message passing-based inference algorithms that potentially improve evidence estimation accuracy.},
	pages = {807},
	number = {7},
	journaltitle = {Entropy (Basel, Switzerland)},
	shortjournal = {Entropy (Basel)},
	author = {Şenöz, İsmail and van de Laar, Thijs and Bagaev, Dmitry and de Vries, Bert},
	date = {2021-06-24},
	pmid = {34202913},
	pmcid = {PMC8303273},
	keywords = {Bayesian inference, message passing, factor graphs, Bethe free energy, variational message passing, variational inference, variational free energy},
	file = {Şenöz et al. - 2021 - Variational Message Passing and Local Constraint M.pdf:/Users/bert/Zotero/storage/2ECWWQ3V/Şenöz et al. - 2021 - Variational Message Passing and Local Constraint M.pdf:application/pdf},
}

@article{bagaevReactiveMessagePassing2023,
	title = {Reactive Message Passing for Scalable Bayesian Inference},
	volume = {2023},
	issn = {1058-9244},
	url = {https://www.hindawi.com/journals/sp/2023/6601690/},
	doi = {10.1155/2023/6601690},
	abstract = {We introduce reactive message passing ({RMP}) as a framework for executing schedule-free, scalable, and, potentially, more robust message passing-based inference in a factor graph representation of a probabilistic model. {RMP} is based on the reactive programming style, which only describes how nodes in a factor graph react to changes in connected nodes. We recognize reactive programming as the suitable programming abstraction for message passing-based methods that improve robustness, scalability, and execution time of the inference procedure and are useful for all future implementations of message passing methods. We also present our own implementation {ReactiveMP}.jl, which is a Julia package for realizing {RMP} through minimization of a constrained Bethe free energy. By user-defined specification of local form and factorization constraints on the variational posterior distribution, {ReactiveMP}.jl executes hybrid message passing algorithms including belief propagation, variational message passing, expectation propagation, and expectation maximization update rules. Experimental results demonstrate the great performance of our {RMP} implementation compared to other Julia packages for Bayesian inference across a range of probabilistic models. In particular, we show that the {RMP} framework is capable of performing Bayesian inference for large-scale probabilistic state-space models with hundreds of thousands of random variables on a standard laptop computer.},
	pages = {e6601690},
	journaltitle = {Scientific Programming},
	author = {Bagaev, Dmitry and de Vries, Bert},
	urldate = {2023-05-28},
	date = {2023-05-27},
	langid = {english},
	note = {Publisher: Hindawi},
	file = {Bagaev and de Vries - 2023 - Reactive Message Passing for Scalable Bayesian Inf.pdf:/Users/bert/Zotero/storage/HJ8G8ZQ5/Bagaev and de Vries - 2023 - Reactive Message Passing for Scalable Bayesian Inf.pdf:application/pdf},
}

@article{podusenko_aida_2021,
	title = {{AIDA}: An Active Inference-based Design Agent for Audio Processing Algorithms},
	url = {http://arxiv.org/abs/2112.13366},
	shorttitle = {{AIDA}},
	abstract = {In this paper we present {AIDA}, which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of {AIDA} is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid ({HA}) algorithm, whenever a {HA} client is not satisfied with their {HA} performance. {AIDA} interprets searching for the "most interesting alternative" as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, {AIDA} is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that {AIDA} comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full {AIDA} agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our {GitHub} repository.},
	journaltitle = {{arXiv}:2112.13366 [cs, eess, stat]},
	author = {Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries, Bert},
	urldate = {2022-01-04},
	date = {2021-12-26},
	eprinttype = {arxiv},
	eprint = {2112.13366},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LP87EYSU/2112.html:text/html;Podusenko et al. - 2021 - AIDA An Active Inference-based Design Agent for A.pdf:/Users/bert/Zotero/storage/FU7P2QL6/Podusenko et al. - 2021 - AIDA An Active Inference-based Design Agent for A.pdf:application/pdf},
}

@inproceedings{van_erp_hybrid_2022,
	title = {Hybrid Inference with Invertible Neural Networks in Factor Graphs},
	abstract = {This paper bridges the gap in the literature between neural networks and probabilistic graphical models. invertible neural networks are incorporated in factor graphs and inference in this model is described by linearization of the network. Consequently, hybrid probabilistic inference in the model is realized through message passing with local constraints on the Bethe free energy. We provide the local Bethe free energy for the invertible neural network node, which allows for evaluation of the performance of the entire probabilistic model. Experimental results show effective hybrid inference in a neural network-based probabilistic model for a binary classification task, paving the way towards a novel class of machine learning models.},
	eventtitle = {In preparation},
	author = {van Erp, Bart and de Vries, Bert},
	date = {2022},
	file = {van Erp and de Vries - 2022 - Hybrid Inference with Invertible Neural Networks i.pdf:/Users/bert/Zotero/storage/AKQBFXT5/van Erp and de Vries - 2022 - Hybrid Inference with Invertible Neural Networks i.pdf:application/pdf},
}

@inproceedings{kouw_variational_2021,
	location = {Atlanta, Georgia},
	title = {Variational message passing for online polynomial {NARMAX} identification},
	abstract = {We propose a variational Bayesian inference procedure
for online nonlinear system identification. For each
output observation, a set of parameter posterior distributions
is updated, which is then used to form a posterior predictive
distribution for future outputs. We focus on the class of
polynomial {NARMAX} models, which we cast into probabilistic
form and represent in terms of a Forney-style factor graph.
Inference in this graph is efficiently performed by a variational
message passing algorithm. We show empirically that our
variational Bayesian estimator outperforms an online recursive
least-squares estimator, most notably in small sample size
settings and low noise regimes, and performs on par with an
iterative least-squares estimator trained offline.},
	eventtitle = {American Control Conference},
	author = {Kouw, Wouter and Podusenko, Albert and Akbayrak, Semih and Schoukens, Maarten},
	date = {2021-12-10},
	file = {Kouw et al. - 2021 - Variational message passing for online polynomial .pdf:/Users/bert/Zotero/storage/C5C5QQMX/Kouw et al. - 2021 - Variational message passing for online polynomial .pdf:application/pdf},
}

@article{senoz_online_2018-1,
	title = {Online State and Parameter Estimation in the Hieararchical Gaussian Filter},
	journaltitle = {2018 {IEEE} International Workshop on Machine Learning for Signal Processing, {MLSP} 2018 - Proceedings},
	author = {Senoz, Ismail and De Vries, Bert and Kouw, Wouter M and de Vries, Bert},
	date = {2018-09},
	file = {Senoz - 2018 - Online State and Parameter Estimation in the Hiear.pdf:/Users/bert/Zotero/storage/RGRJTGXA/Senoz - 2018 - Online State and Parameter Estimation in the Hiear.pdf:application/pdf},
}

@article{podusenko_message_2021,
	title = {Message Passing-Based Inference for Time-Varying Autoregressive Models},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/6/683},
	doi = {10.3390/e23060683},
	abstract = {Time-varying autoregressive ({TVAR}) models are widely used for modeling of non-stationary signals. Unfortunately, online joint adaptation of both states and parameters in these models remains a challenge. In this paper, we represent the {TVAR} model by a factor graph and solve the inference problem by automated message passing-based inference for states and parameters. We derive structured variational update rules for a composite “{AR} node” with probabilistic observations that can be used as a plug-in module in hierarchical models, for example, to model the time-varying behavior of the hyper-parameters of a time-varying {AR} model. Our method includes tracking of variational free energy ({FE}) as a Bayesian measure of {TVAR} model performance. The proposed methods are verified on a synthetic data set and validated on real-world data from temperature modeling and speech enhancement tasks.},
	pages = {683},
	number = {6},
	journaltitle = {Entropy},
	author = {Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
	urldate = {2021-07-07},
	date = {2021-06},
	langid = {english},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {model selection, free energy, Bayesian inference, factor graph, hybrid message passing, non-stationary systems, probabilistic graphical models},
	file = {Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.html:/Users/bert/Zotero/storage/ULV5U54B/Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.html:text/html;Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.pdf:/Users/bert/Zotero/storage/7N38EY64/Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.pdf:application/pdf},
}

@inproceedings{cox_robust_2018-1,
	location = {Rome, Italy},
	title = {Robust Expectation Propagation in Factor Graphs Involving Both Continuous and Binary Variables},
	abstract = {Factor graphs provide a convenient framework for automatically generating (approximate) Bayesian inference algorithms based on message passing. Examples include the sumproduct algorithm (belief propagation), expectation maximization ({EM}), expectation propagation ({EP}) and variational message passing ({VMP}). While these message passing algorithms can be generated automatically, they depend on a library of precomputed message update rules. As a result, the applicability of the factor graph approach depends on the availability of such rules for all involved nodes. This paper describes the probit factor node for linking continuous and binary random variables in a factor graph. We derive (approximate) sum-product message update rules for this node through constrained moment matching, which leads to a robust version of the {EP} algorithm in which all messages are guaranteed to be proper. This enables automatic Bayesian inference in probabilistic models that involve both continuous and discrete latent variables, without the need for model-speciﬁc derivations. The usefulness of the node as a factor graph building block is demonstrated by applying it to perform Bayesian inference in a linear classiﬁcation model with corrupted class labels.},
	eventtitle = {26th European Signal Processing Conference ({EUSIPCO})},
	pages = {5},
	booktitle = {26th European Signal Processing Conference ({EUSIPCO})},
	author = {Cox, Marco},
	date = {2018},
	langid = {english},
	file = {Cox - 2018 - Robust Expectation Propagation in Factor Graphs In.pdf:/Users/bert/Zotero/storage/JGYYXEIZ/Cox - 2018 - Robust Expectation Propagation in Factor Graphs In.pdf:application/pdf},
}

@article{de_vries_factor_2017-1,
	title = {A Factor Graph Description of Deep Temporal Active Inference},
	volume = {11},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2017.00095/full},
	doi = {10.3389/fncom.2017.00095},
	abstract = {Active inference is a corollary of the Free Energy Principle that prescribes how self-organizing biological agents interact with their environment. The study of active inference processes relies on the definition of a generative probabilistic model and a description of how a free energy functional is minimized by neuronal message passing under that model. This paper presents a tutorial introduction to specifying active inference processes by Forney-style factor graphs ({FFG}). The {FFG} framework provides both an insightful representation of the probabilistic model and a biologically plausible inference scheme that, in principle, can be automatically executed in a computer simulation. As an illustrative example, we present an {FFG} for a deep temporal active inference process. The graph clearly shows how policy selection by expected free energy minimization results from free energy minimization per se, in an appropriate generative policy model.},
	journaltitle = {Frontiers in Computational Neuroscience},
	shortjournal = {Front. Comput. Neurosci.},
	author = {de Vries, Bert and Friston, Karl J.},
	urldate = {2018-12-05},
	date = {2017},
	keywords = {active inference, Active inference, Message passing, message passing, Belief propagation, free-energy principle, Factor Graphs, multi-scale dynamical systems},
	file = {de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:/Users/bert/Zotero/storage/M6ESEGYB/de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:application/pdf;de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:/Users/bert/Zotero/storage/237JJVQK/de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:application/pdf;Full Text PDF:/Users/bert/Zotero/storage/PNAAIKAL/de Vries and Friston - 2017 - A Factor Graph Description of Deep Temporal Active.pdf:application/pdf},
}

@inproceedings{podusenko_message_2021-1,
	location = {Gold Coast, Australia},
	title = {Message Passing-Based Inference in the Gamma Mixture Model},
	isbn = {978-1-72816-338-3},
	url = {https://ieeexplore.ieee.org/document/9596329/},
	doi = {10.1109/MLSP52302.2021.9596329},
	eventtitle = {2021 {IEEE} 31st International Workshop on Machine Learning for Signal Processing ({MLSP})},
	pages = {1--6},
	booktitle = {2021 {IEEE} 31st International Workshop on Machine Learning for Signal Processing ({MLSP})},
	publisher = {{IEEE}},
	author = {Podusenko, Albert and van Erp, Bart and Bagaev, Dmitry and Senoz, Ismail and de Vries, Bert},
	urldate = {2021-11-29},
	date = {2021-10-25},
	keywords = {Signal processing, Machine learning, Conferences, Probabilistic logic, Probability distribution, Factor Graphs, Mathematical models, Expectation-Maximization, Gamma Mixture Model, Message Passing, Mixture models, Moment Matching, Probabilistic Inference},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/WHFNS6QW/9596329.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/KDZXXX5C/Podusenko et al. - 2021 - Message Passing-Based Inference in the Gamma Mixtu.pdf:application/pdf},
}

@article{van_erp_bayesian_2021,
	title = {A Bayesian Modeling Approach to Situated Design of Personalized Soundscaping Algorithms},
	volume = {11},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2076-3417/11/20/9535},
	doi = {10.3390/app11209535},
	abstract = {Effective noise reduction and speech enhancement algorithms have great potential to enhance lives of hearing aid users by restoring speech intelligibility. An open problem in today’s commercial hearing aids is how to take into account users’ preferences, indicating which acoustic sources should be suppressed or enhanced, since they are not only user-specific but also depend on many situational factors. In this paper, we develop a fully probabilistic approach to “situated soundscaping”, which aims at enabling users to make on-the-spot (“situated”) decisions about the enhancement or suppression of individual acoustic sources. The approach rests on a compact generative probabilistic model for acoustic signals. In this framework, all signal processing tasks (source modeling, source separation and soundscaping) are framed as automatable probabilistic inference tasks. These tasks can be efficiently executed using message passing-based inference on factor graphs. Since all signal processing tasks are automatable, the approach supports fast future model design cycles in an effort to reach commercializable performance levels. The presented results show promising performance in terms of {SNR}, {PESQ} and {STOI} improvements in a situated setting.},
	pages = {9535},
	number = {20},
	journaltitle = {Applied Sciences},
	author = {van Erp, Bart and Podusenko, Albert and Ignatenko, Tanya and de Vries, Bert},
	urldate = {2021-10-14},
	date = {2021-10},
	langid = {english},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {factor graphs, noise reduction, speech enhancement, variational message passing, bayesian machine learning, situated soundscaping},
	file = {Full Text PDF:/Users/bert/Zotero/storage/IM5NRB8Z/van Erp et al. - 2021 - A Bayesian Modeling Approach to Situated Design of.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/GAURGH47/9535.html:text/html},
}

@inproceedings{van_erp_variational_2021,
	title = {Variational Log-Power Spectral Tracking for Acoustic Signals},
	doi = {10.1109/SSP49050.2021.9513757},
	abstract = {This paper proposes a generative hierarchical probabilistic model for acoustic signals where both the frequency decomposition and log-power spectrum appear as latent variables. In order to facilitate efficient inference, we represent the model in a factor graph that includes a probabilistic Fourier transform and a Gaussian scale model as modules. We derive novel ways of performing variational message passing-based inference in the Gaussian scale model. As a result, in this model a probabilistic representation of the log-power spectrum of an acoustic signal can be effectively inferred online. The proposed model may find applications as a front end wherever probabilistic log-power spectral features of a signal are needed. We validate the model and message passing-based inference methods by tracking the log-power spectrum of a speech signal.},
	eventtitle = {2021 {IEEE} Statistical Signal Processing Workshop ({SSP})},
	pages = {311--315},
	booktitle = {2021 {IEEE} Statistical Signal Processing Workshop ({SSP})},
	author = {van Erp, Bart and Şenöz, İsmail and de Vries, Bert},
	date = {2021-07},
	note = {{ISSN}: 2693-3551},
	keywords = {Signal processing algorithms, Inference algorithms, Signal processing, Acoustics, Conferences, Probabilistic logic, Fourier transforms},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/BAUK29ZK/9513757.html:text/html},
}

@inproceedings{wijnings_robust_2019-1,
	title = {Robust Bayesian Beamforming for Sources at Different Distances with Applications in Urban Monitoring},
	doi = {10.1109/ICASSP.2019.8682835},
	abstract = {Acoustic smart sensor networks can provide valuable actionable intelligence to authorities for managing safety in the urban environment. A spatial filter (beamformer) for localization and separation of acoustic sources is a key component of such a network. However, classical methods such as delay-and-sum beamforming fail, because sources are located at varying distances from the sensor array. This causes a regularization problem where either far-away sources are wrongly attenuated, or noise is wrongly amplified. We solve this by considering source strength and location as random variables. The posterior distributions are approximated using Gibbs sampling. Each marginal is computed by combining importance sampling and inverse transform sampling using Chebyshev polynomial approximation. This leads to an iterative algorithm with similarities to deconvolution beamforming. Our method is robust against deviations in manifold model, can deal with sources at different distances and power levels, and does not require an a priori known number of sources.},
	eventtitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {4325--4329},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	author = {Wijnings, Patrick W.A. and Stuijk, Sander and de Vries, Bert and Corporaal, Henk},
	date = {2019-05},
	note = {{ISSN}: 2379-190X, 1520-6149},
	keywords = {Bayes methods, Markov processes, Robustness, Acoustics, source separation, iterative methods, iterative algorithm, Signal to noise ratio, Manifolds, deconvolution, importance sampling, acoustic signal processing, Gibbs sampling, Array signal processing, sensor arrays, array signal processing, spatial filters, polynomial approximation, Sensor arrays, Acoustic applications, acoustic smart sensor networks, acoustic source localization, acoustic source separation, Attenuation, beamformer, Chebyshev approximation, Chebyshev polynomial approximation, deconvolution beamforming, intelligent sensors, inverse transform sampling, inverse transforms, Laplace equations, posterior distributions, random variables, regularization problem, robust Bayesian beamforming, sensor array, source strength, spatial filter, statistical distributions, urban environment, urban monitoring, valuable actionable intelligence, varying distances},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/9LUDJUBF/8682835.html:text/html;Wijnings et al. - 2019 - Robust Bayesian Beamforming for Sources at Differe.pdf:/Users/bert/Zotero/storage/ABP9U3PQ/Wijnings et al. - 2019 - Robust Bayesian Beamforming for Sources at Differe.pdf:application/pdf},
}

@article{friston_graphical_2017,
	title = {The graphical brain: belief propagation and active inference},
	url = {http://dx.doi.org/10.1162/NETN_a_00018},
	doi = {10.1162/NETN_a_00018},
	shorttitle = {The graphical brain},
	pages = {1--78},
	journaltitle = {Network Neuroscience},
	shortjournal = {Network Neuroscience},
	author = {Friston, Karl J and Parr, Thomas and de Vries, Bert},
	urldate = {2017-07-28},
	date = {2017-06-07},
	file = {Friston et al. - 2017 - The graphical brain Belief propagation and active.pdf:/Users/bert/Zotero/storage/YT4LZ29R/Friston et al. - 2017 - The graphical brain Belief propagation and active.pdf:application/pdf;Network Neuroscience Snapshot:/Users/bert/Zotero/storage/S2S8SHVY/NETN_a_00018.html:text/html;Snapshot:/Users/bert/Zotero/storage/H6EXAEDL/NETN_a_00018.html:text/html},
}

@inproceedings{bocharov_acoustic_2018-1,
	location = {Rome, Italy},
	title = {Acoustic Scene Classification from Few Examples},
	abstract = {In order to personalize the behavior of hearing aid devices in different acoustic environments, we need to develop personalized acoustic scene classiﬁers. Since we cannot afford to burden an individual hearing aid user with the task to collect a large acoustic database, we aim instead to train a scene classiﬁer on just one (or maximally a few) in-situ recorded acoustic waveform of a few seconds duration per scene. In this paper we develop such a ”one-shot” personalized scene classiﬁer, based on a Hidden Semi-Markov model. The presented classiﬁer consistently outperforms a more classical Dynamic-Time-Warping-{NearestNeighbor} classiﬁer, and correctly classiﬁes acoustic scenes about twice as well as a (random) chance classiﬁer after training on just one recording of 10 seconds duration per scene.},
	eventtitle = {26th European Signal Processing Conference ({EUSIPCO})},
	pages = {5},
	booktitle = {26th European Signal Processing Conference ({EUSIPCO})},
	author = {Bocharov, Ivan and Tjalkens, Tjalling and De Vries, Bert},
	date = {2018},
	langid = {english},
	file = {Bocharov et al. - 2018 - Acoustic Scene Classification from Few Examples.pdf:/Users/bert/Zotero/storage/B8CDW3KA/Bocharov et al. - 2018 - Acoustic Scene Classification from Few Examples.pdf:application/pdf},
}

@inproceedings{van_de_laar_forneylab_2018-2,
	location = {Thessaloniki, Greece},
	title = {{ForneyLab}: A Toolbox for Biologically Plausible Free Energy Minimization in Dynamic Neural Models},
	eventtitle = {Conference on Complex Systems ({CCS})},
	booktitle = {Conference on Complex Systems ({CCS})},
	author = {van de Laar, Thijs and Cox, Marco and Senoz, Ismail and Bocharov, Ivan and de Vries, Bert},
	date = {2018-09-27},
	file = {poster_ccs.pdf:/Users/bert/Zotero/storage/CDI6ENBD/poster_ccs.pdf:application/pdf;van de Laar e.a. - 2018 - ForneyLab A Toolbox for Biologically Plausible Fr.pdf:/Users/bert/Zotero/storage/L9K7YCHE/van de Laar e.a. - 2018 - ForneyLab A Toolbox for Biologically Plausible Fr.pdf:application/pdf},
}

@inproceedings{cox_forneylabjl_2018,
	location = {Boston, {MA}},
	title = {{ForneyLab}.jl: Fast and flexible automated inference through message passing in Julia},
	eventtitle = {International Conference on Probabilistic Programming},
	booktitle = {International Conference on Probabilistic Programming},
	author = {Cox, Marco and van de Laar, Thijs and de Vries, Bert},
	date = {2018-04-10},
	file = {Cox et al. - 2018 - ForneyLab.jl Fast and flexible automated inferenc.pdf:/Users/bert/Zotero/storage/YQ8ANTKK/Cox et al. - 2018 - ForneyLab.jl Fast and flexible automated inferenc.pdf:application/pdf;poster_probprog.pdf:/Users/bert/Zotero/storage/AGLB6VA3/poster_probprog.pdf:application/pdf},
}

@unpublished{bocharov_discovering_nodate,
	title = {Discovering Hearthstone archetypes using probabilistic modeling},
	author = {Bocharov, Ivan},
	file = {Bocharov - Discovering Hearthstone archetypes using probabili.pdf:/Users/bert/Zotero/storage/H3JK42VP/Bocharov - Discovering Hearthstone archetypes using probabili.pdf:application/pdf},
}

@article{van_diepen_probabilistic_2018-1,
	title = {A Probabilistic Modelling Approach to One-Shot Gesture Recogntion},
	url = {http://arxiv.org/abs/1806.11408},
	abstract = {Gesture recognition enables a natural extension of the way we currently interact with devices. Commercially available gesture recognition systems are usually pre-trained and offer no option for customization by the user. In order to improve the user experience, it is desirable to allow end users to define their own gestures. This scenario requires learning from just a few training examples if we want to impose only a light training load on the user. To this end, we propose a gesture classifier based on a hierarchical probabilistic modeling approach. In this framework, high-level features that are shared among different gestures can be extracted from a large labeled data set, yielding a prior distribution for gestures. When learning new types of gestures, the learned shared prior reduces the number of required training examples for individual gestures. We implemented the proposed gesture classifier for a Myo sensor bracelet and show favorable results for the tested system on a database of 17 different gesture types. Furthermore, we propose and implement two methods to incorporate the gesture classifier in a real-time gesture recognition system.},
	journaltitle = {{arXiv}:1806.11408 [eess]},
	author = {van Diepen, Anouk and Cox, Marco and de Vries, Bert},
	urldate = {2018-12-07},
	date = {2018-06-29},
	eprinttype = {arxiv},
	eprint = {1806.11408},
	keywords = {Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/SX8GIHCF/1806.html:text/html;van Diepen et al. - 2018 - A Probabilistic Modelling Approach to One-Shot Ges.pdf:/Users/bert/Zotero/storage/K38BCYRJ/van Diepen et al. - 2018 - A Probabilistic Modelling Approach to One-Shot Ges.pdf:application/pdf},
}

@inproceedings{akbayrak_reparameterization_2019,
	title = {Reparameterization Gradient Message Passing},
	abstract = {In this paper we consider efﬁcient message passing based inference in a factor graph representation of a probabilistic model. Current message passing methods, such as belief propagation, variational message passing or expectation propagation, rely on analytically pre-computed message update rules. In practical models, it is often not feasible to analytically derive all update rules for all factors in the graph and as a result, efﬁcient message passing-based inference cannot proceed. In related research on (non-message passing-based) inference, a “reparameterization trick” has lead to a considerable extension of the class of models for which automated inference is possible. In this paper, we introduce Reparameterization Gradient Message Passing ({RGMP}), which is a new message passing method based on the reparameterization gradient. In most models, the large majority of messages can be analytically derived and we resort to {RGMP} only when necessary. We will argue that this kind of hybrid message passing leads naturally to low-variance gradients.},
	pages = {5},
	booktitle = {submitted to {EUSIPCO}},
	author = {Akbayrak, Semih and de Vries, Bert},
	date = {2019},
	langid = {english},
	file = {Akbayrak and de Vries - 2019 - Reparameterization Gradient Message Passing.pdf:/Users/bert/Zotero/storage/MBL2M8W2/Akbayrak and de Vries - 2019 - Reparameterization Gradient Message Passing.pdf:application/pdf},
}

@inproceedings{van_diepen_-situ_2017-1,
	location = {Technische Universiteit Eindhoven},
	title = {An In-situ Trainable Gesture Classifier},
	eventtitle = {Benelearn 2017},
	pages = {66--69},
	booktitle = {Proceedings of the Twenty-Sixth Benelux Conference on Machine Learning},
	author = {van Diepen, Anouk and Cox, Marco and de Vries, Bert},
	date = {2017-06-09},
	file = {van Diepen et al. - 2017 - An In-situ Trainable Gesture Classifier.pdf:/Users/bert/Zotero/storage/I7VK8WHS/van Diepen et al. - 2017 - An In-situ Trainable Gesture Classifier.pdf:application/pdf},
}

@inproceedings{van_de_laar_forneylabjl_2018,
	location = {London, {UK}},
	title = {{ForneyLab}.jl: a Julia Toolbox for Factor Graph-based Probabilistic Programming},
	eventtitle = {{JuliaCon}},
	booktitle = {{JuliaCon}},
	author = {van de Laar, Thijs and Cox, Marco and de Vries, Bert},
	date = {2018-08-08},
	file = {van de Laar e.a. - 2018 - ForneyLab.jl a Julia Toolbox for Factor Graph-bas.pdf:/Users/bert/Zotero/storage/I38QA6S8/van de Laar e.a. - 2018 - ForneyLab.jl a Julia Toolbox for Factor Graph-bas.pdf:application/pdf},
}

@inproceedings{senoz_online_2018-2,
	title = {Online Variational Message Passing in the Hierarchical Gaussian Filter},
	doi = {10.1109/MLSP.2018.8517019},
	abstract = {We address the problem of online state and parameter estimation in hierarchical Bayesian nonlinear dynamic systems. We focus on the Hierarchical Gaussian Filter ({HGF}), which is a popular model in the computational neuroscience literature. For this filter, explicit equations for online state estimation (and offline parameter estimation) have been derived before. We extend this work by casting the {HGF} as a probabilistic factor graph and present variational message passing update rules that facilitate both online state and parameter estimation as well as online tracking of the free energy (or {ELBO}), which can be used as a proxy for Bayesian evidence. Due to the locality and modularity of the factor graph framework, our approach supports application of {HGF}’s and variations as plug-in modules to a wide variety of dynamic modelling applications.},
	eventtitle = {2018 {IEEE} 28th International Workshop on Machine Learning for Signal Processing ({MLSP})},
	pages = {1--6},
	booktitle = {2018 {IEEE} 28th International Workshop on Machine Learning for Signal Processing ({MLSP})},
	author = {Şenöz, I and de Vries, B.},
	date = {2018-09},
	keywords = {free energy, Message passing, Bayes methods, Computational modeling, State estimation, Mathematical model, Dynamical systems, Parameter estimation, Hierarchical Gaussian Filter, Iron, Online state and parameter estimation, Variational Message Passing},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/AF4S9E6Z/8517019.html:text/html;IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/3R9B7ZVN/8517019.html:text/html;Şenöz and Vries - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE HIERARCH.pdf:/Users/bert/Zotero/storage/QT9CNT43/Şenöz and Vries - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE HIERARCH.pdf:application/pdf;Şenöz and Vries - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE HIERARCH.pdf:/Users/bert/Zotero/storage/MAI9CXLN/Şenöz and Vries - 2018 - ONLINE VARIATIONAL MESSAGE PASSING IN THE HIERARCH.pdf:application/pdf},
}

@unpublished{kouw_schedule-free_2020,
	title = {Schedule-free variational message passing for Bayesian filtering},
	url = {https://neuromatch.io},
	abstract = {In Bayesian filtering, states and parameters of probabilistic state-space models are inferred in an online manner. Using the Free Energy Principle, the state-space model is cast to a generative model p and the posterior distributions of interest are approximated using recognition distributions or beliefs q. The factorisation of state-space models into state transitions and observation likelihoods over time supports forming a factor graph and performing inference via message passing.

Tools for message passing on factor graphs typically employ a scheduling procedure, in which a separate algorithm or compiler takes the model description and returns which nodes should pass messages where at what time. This can be sufficiently expensive to form a bottleneck. Moreover, it's not a biologically plausible mechanism for governing message passing. I explore the possibility of passing messages without a scheduler. A designated terminal node should pass an initial message, which will arrive at an initial variable. The corresponding belief is updated, a local Free Energy is computed and the belief is emitted to neighbouring factor nodes. From there on out, whenever an updated belief arrives at a factor node, the node fires messages to all other variables if the local Free Energy surpasses a threshold. If not, the node becomes silent.},
	type = {Short talk},
	howpublished = {Short talk},
	note = {Neuromatch 2020},
	author = {Kouw, Wouter M},
	date = {2020-03-31},
	file = {Kouw - 2020 - Schedule-free variational message passing for Baye.pdf:/Users/bert/Zotero/storage/IXUHTJDY/Kouw - 2020 - Schedule-free variational message passing for Baye.pdf:application/pdf},
}

@unpublished{kouw_schedule-free_2019,
	title = {Schedule-free message-passing},
	type = {{BIASlab} Seminar},
	howpublished = {{BIASlab} Seminar},
	author = {Kouw, Wouter},
	date = {2019-06-11},
	file = {Kouw - 2019 - Schedule-free message-passing.pdf:/Users/bert/Zotero/storage/PY77UF4T/Kouw - 2019 - Schedule-free message-passing.pdf:application/pdf},
}

@unpublished{van_de_laar_simulating_2018,
	title = {Simulating Active Inference Processes With Message Passing},
	author = {van de Laar, Thijs},
	date = {2018-05-11},
	langid = {english},
	file = {van de Laar - 2018 - Simulating Active Inference Processes With Message.pdf:/Users/bert/Zotero/storage/LHIZGJ3A/van de Laar - 2018 - Simulating Active Inference Processes With Message.pdf:application/pdf},
}

@unpublished{senoz_constrained_nodate,
	title = {Constrained Free Energy Minimization},
	author = {Senoz, Ismail},
	langid = {english},
	file = {Senoz - Constrained Free Energy Minimization.pdf:/Users/bert/Zotero/storage/SXQSN5YK/Senoz - Constrained Free Energy Minimization.pdf:application/pdf},
}

@unpublished{bocharov_autodiff_2019,
	location = {Eindhoven University of Technology},
	title = {{AutoDiff} (in Julia)},
	author = {Bocharov, Ivan},
	date = {2019-09-18},
	file = {Bocharov - 2019 - AutoDiff (in Julia).pdf:/Users/bert/Zotero/storage/SGNPFBYX/Bocharov - 2019 - AutoDiff (in Julia).pdf:application/pdf},
}

@unpublished{van_de_laar_you_2018,
	title = {Do You Wanna Build a Node, Man?},
	author = {van de Laar, Thijs},
	date = {2018-05-12},
	file = {van de Laar - 2018 - Do You Wanna Build a Node, Man.pdf:/Users/bert/Zotero/storage/BUAI7KPU/van de Laar - 2018 - Do You Wanna Build a Node, Man.pdf:application/pdf},
}

@unpublished{de_vries_biaslab_2018,
	title = {{BIASlab} reseach theme},
	author = {De Vries, Bert},
	date = {2018-09-19},
	file = {DeVries - sep2018- BIASlab research theme.pptx:/Users/bert/Zotero/storage/RYLJ5U4Y/DeVries - sep2018- BIASlab research theme.pptx:application/vnd.openxmlformats-officedocument.presentationml.presentation},
}

@unpublished{akbayrak_reparameterization_nodate,
	title = {Reparameterization Gradient Message Passing - Automatic Variational Inference for Factor Graphs},
	author = {Akbayrak, Semih},
	langid = {english},
	file = {Akbayrak - Reparameterization Gradient Message Passing - Auto.pdf:/Users/bert/Zotero/storage/7UQ5VAEM/Akbayrak - Reparameterization Gradient Message Passing - Auto.pdf:application/pdf},
}

@article{moran_brain_2014,
	title = {The Brain Ages Optimally to Model Its Environment: Evidence from Sensory Learning over the Adult Lifespan},
	volume = {10},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003422},
	doi = {10.1371/journal.pcbi.1003422},
	shorttitle = {The Brain Ages Optimally to Model Its Environment},
	abstract = {The aging brain shows a progressive loss of neuropil, which is accompanied by subtle changes in neuronal plasticity, sensory learning and memory. Neurophysiologically, aging attenuates evoked responses—including the mismatch negativity ({MMN}). This is accompanied by a shift in cortical responsivity from sensory (posterior) regions to executive (anterior) regions, which has been interpreted as a compensatory response for cognitive decline. Theoretical neurobiology offers a simpler explanation for all of these effects—from a Bayesian perspective, as the brain is progressively optimized to model its world, its complexity will decrease. A corollary of this complexity reduction is an attenuation of Bayesian updating or sensory learning. Here we confirmed this hypothesis using magnetoencephalographic recordings of the mismatch negativity elicited in a large cohort of human subjects, in their third to ninth decade. Employing dynamic causal modeling to assay the synaptic mechanisms underlying these non-invasive recordings, we found a selective age-related attenuation of synaptic connectivity changes that underpin rapid sensory learning. In contrast, baseline synaptic connectivity strengths were consistently strong over the decades. Our findings suggest that the lifetime accrual of sensory experience optimizes functional brain architectures to enable efficient and generalizable predictions of the world.},
	pages = {e1003422},
	number = {1},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Moran, Rosalyn J. and Symmonds, Mkael and Dolan, Raymond J. and Friston, Karl J.},
	urldate = {2022-02-03},
	date = {2014-01-23},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Free energy, Sensory perception, Learning, Neuronal plasticity, Coding mechanisms, Aging, Mismatch negativity, Pyramidal cells},
	file = {Moran et al. - 2014 - The Brain Ages Optimally to Model Its Environment.pdf:/Users/bert/Zotero/storage/Z5D7SRJG/Moran et al. - 2014 - The Brain Ages Optimally to Model Its Environment.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/FAFTMU35/article.html:text/html},
}

@article{dobzhansky_biology_1964,
	title = {Biology, Molecular and Organismic},
	volume = {4},
	issn = {00031569},
	url = {http://www.jstor.org/stable/3881145},
	pages = {443--452},
	number = {4},
	journaltitle = {American Zoologist},
	author = {Dobzhansky, Theodosius},
	urldate = {2022-02-03},
	date = {1964},
	note = {Publisher: Oxford University Press},
	file = {Dobzhansky - 1964 - Biology, Molecular and Organismic.pdf:/Users/bert/Zotero/storage/6YHJCH2W/Dobzhansky - 1964 - Biology, Molecular and Organismic.pdf:application/pdf},
}

@book{lanczos_variational_1986,
	location = {New York},
	edition = {4th Revised ed. edition},
	title = {The Variational Principles of Mechanics},
	isbn = {978-0-486-65067-8},
	pagetotal = {464},
	publisher = {Dover Publications},
	author = {Lanczos, Cornelius},
	date = {1986-03-01},
}

@article{friston_sophisticated_2021,
	title = {Sophisticated Inference},
	volume = {33},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01351},
	doi = {10.1162/neco_a_01351},
	abstract = {Active inference offers a first principle account of sentient behavior, from which special and important cases—for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design—can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about “what would happen if I did that” to “what I would believe about what would happen if I did that.” The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.},
	pages = {713--763},
	number = {3},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Friston, Karl and Da Costa, Lancelot and Hafner, Danijar and Hesp, Casper and Parr, Thomas},
	urldate = {2022-02-14},
	date = {2021-03-01},
	file = {Friston et al. - 2021 - Sophisticated Inference.pdf:/Users/bert/Zotero/storage/G442FQGW/Friston et al. - 2021 - Sophisticated Inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NKAXVIYP/Sophisticated-Inference.html:text/html},
}

@article{champion_branching_2021,
	title = {Branching Time Active Inference: the theory and its generality},
	url = {http://arxiv.org/abs/2111.11107},
	shorttitle = {Branching Time Active Inference},
	abstract = {Over the last 10 to 15 years, active inference has helped to explain various brain mechanisms from habit formation to dopaminergic discharge and even modelling curiosity. However, the current implementations suffer from an exponential (space and time) complexity class when computing the prior over all the possible policies up to the time-horizon. Fountas et al (2020) used Monte Carlo tree search to address this problem, leading to impressive results in two different tasks. In this paper, we present an alternative framework that aims to unify tree search and active inference by casting planning as a structure learning problem. Two tree search algorithms are then presented. The first propagates the expected free energy forward in time (i.e., towards the leaves), while the second propagates it backward (i.e., towards the root). Then, we demonstrate that forward and backward propagations are related to active inference and sophisticated inference, respectively, thereby clarifying the differences between those two planning strategies.},
	journaltitle = {{arXiv}:2111.11107 [cs]},
	author = {Champion, Théophile and Da Costa, Lancelot and Bowman, Howard and Grześ, Marek},
	urldate = {2022-02-14},
	date = {2021-11-22},
	eprinttype = {arxiv},
	eprint = {2111.11107},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/LF9C82VW/2111.html:text/html;Champion et al. - 2021 - Branching Time Active Inference the theory and it.pdf:/Users/bert/Zotero/storage/Y3FEWTXN/Champion et al. - 2021 - Branching Time Active Inference the theory and it.pdf:application/pdf},
}

@article{catal_robot_2021,
	title = {Robot navigation as hierarchical active inference},
	volume = {142},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021002021},
	doi = {10.1016/j.neunet.2021.05.010},
	abstract = {Localization and mapping has been a long standing area of research, both in neuroscience, to understand how mammals navigate their environment, as well as in robotics, to enable autonomous mobile robots. In this paper, we treat navigation as inferring actions that minimize (expected) variational free energy under a hierarchical generative model. We find that familiar concepts like perception, path integration, localization and mapping naturally emerge from this active inference formulation. Moreover, we show that this model is consistent with models of hippocampal functions, and can be implemented in silico on a real-world robot. Our experiments illustrate that a robot equipped with our hierarchical model is able to generate topologically consistent maps, and correct navigation behaviour is inferred when a goal location is provided to the system.},
	pages = {192--204},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Çatal, Ozan and Verbelen, Tim and Van de Maele, Toon and Dhoedt, Bart and Safron, Adam},
	urldate = {2022-02-14},
	date = {2021-10},
	langid = {english},
	file = {Çatal et al. - 2021 - Robot navigation as hierarchical active inference.pdf:/Users/bert/Zotero/storage/GPDSID94/Çatal et al. - 2021 - Robot navigation as hierarchical active inference.pdf:application/pdf},
}

@online{noauthor_robot_nodate,
	title = {Robot navigation as hierarchical active inference - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608021002021},
	urldate = {2022-02-14},
	file = {Robot navigation as hierarchical active inference - ScienceDirect:/Users/bert/Zotero/storage/7B7ECSGR/S0893608021002021.html:text/html},
}

@article{lanillos_active_2021,
	title = {Active Inference in Robotics and Artificial Agents: Survey and Challenges},
	url = {http://arxiv.org/abs/2112.01871},
	shorttitle = {Active Inference in Robotics and Artificial Agents},
	abstract = {Active inference is a mathematical framework which originated in computational neuroscience as a theory of how the brain implements action, perception and learning. Recently, it has been shown to be a promising approach to the problems of state-estimation and control under uncertainty, as well as a foundation for the construction of goal-driven behaviours in robotics and artificial agents in general. Here, we review the state-of-the-art theory and implementations of active inference for state-estimation, control, planning and learning; describing current achievements with a particular focus on robotics. We showcase relevant experiments that illustrate its potential in terms of adaptation, generalization and robustness. Furthermore, we connect this approach with other frameworks and discuss its expected benefits and challenges: a unified framework with functional biological plausibility using variational Bayesian inference.},
	journaltitle = {{arXiv}:2112.01871 [cs]},
	author = {Lanillos, Pablo and Meo, Cristian and Pezzato, Corrado and Meera, Ajith Anil and Baioumy, Mohamed and Ohata, Wataru and Tschantz, Alexander and Millidge, Beren and Wisse, Martijn and Buckley, Christopher L. and Tani, Jun},
	urldate = {2022-02-14},
	date = {2021-12-03},
	eprinttype = {arxiv},
	eprint = {2112.01871},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/8X8F52QR/2112.html:text/html;Lanillos et al. - 2021 - Active Inference in Robotics and Artificial Agents.pdf:/Users/bert/Zotero/storage/4CEU3X9J/Lanillos et al. - 2021 - Active Inference in Robotics and Artificial Agents.pdf:application/pdf},
}

@article{friston_free_2022,
	title = {The free energy principle made simpler but not too simple},
	url = {http://arxiv.org/abs/2201.06387},
	abstract = {This paper provides a concise description of the free energy principle, starting from a formulation of random dynamical systems in terms of a Langevin equation and ending with a Bayesian mechanics that can be read as a physics of sentience. It rehearses the key steps using standard results from statistical physics. These steps entail (i) establishing a particular partition of states based upon conditional independencies that inherit from sparsely coupled dynamics, (ii) unpacking the implications of this partition in terms of Bayesian inference and (iii) describing the paths of particular states with a variational principle of least action. Teleologically, the free energy principle offers a normative account of self-organisation in terms of optimal Bayesian design and decision-making, in the sense of maximising marginal likelihood or Bayesian model evidence. In summary, starting from a description of the world in terms of random dynamical systems, we end up with a description of self-organisation as sentient behaviour that can be interpreted as self-evidencing; namely, self-assembly, autopoiesis or active inference.},
	journaltitle = {{arXiv}:2201.06387 [cond-mat, physics:nlin, physics:physics, q-bio]},
	author = {Friston, Karl and Da Costa, Lancelot and Sajid, Noor and Heins, Conor and Ueltzhöffer, Kai and Pavliotis, Grigorios A. and Parr, Thomas},
	urldate = {2022-02-21},
	date = {2022-01-28},
	eprinttype = {arxiv},
	eprint = {2201.06387},
	keywords = {Quantitative Biology - Neurons and Cognition, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Statistical Mechanics, Physics - Biological Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/EWZCGE9G/2201.html:text/html;Friston et al. - 2022 - The free energy principle made simpler but not too.pdf:/Users/bert/Zotero/storage/8T2NHWAI/Friston et al. - 2022 - The free energy principle made simpler but not too.pdf:application/pdf},
}

@online{noauthor_biology_nodate,
	title = {{BIOLOGY}, {MOLECULAR} {AND} {ORGANISMIC} - {PubMed}},
	url = {https://pubmed.ncbi.nlm.nih.gov/14223586/},
	urldate = {2022-02-22},
	file = {BIOLOGY, MOLECULAR AND ORGANISMIC - PubMed:/Users/bert/Zotero/storage/6Q6YG3ME/14223586.html:text/html},
}

@article{defossez_hybrid_2021,
	title = {Hybrid Spectrogram and Waveform Source Separation},
	url = {http://arxiv.org/abs/2111.03600},
	abstract = {Source separation models either work on the spectrogram or waveform domain. In this work, we show how to perform end-to-end hybrid source separation, letting the model decide which domain is best suited for each source, and even combining both. The proposed hybrid version of the Demucs architecture won the Music Demixing Challenge 2021 organized by Sony. This architecture also comes with additional improvements, such as compressed residual branches, local attention or singular value regularization. Overall, a 1.4 {dB} improvement of the Signal-To-Distortion ({SDR}) was observed across all sources as measured on the {MusDB} {HQ} dataset, an improvement confirmed by human subjective evaluation, with an overall quality rated at 2.83 out of 5 (2.36 for the non hybrid Demucs), and absence of contamination at 3.04 (against 2.37 for the non hybrid Demucs and 2.44 for the second ranking model submitted at the competition).},
	journaltitle = {{arXiv}:2111.03600 [cs, eess, stat]},
	author = {Défossez, Alexandre},
	urldate = {2022-03-03},
	date = {2021-11-05},
	eprinttype = {arxiv},
	eprint = {2111.03600},
	keywords = {Statistics - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/YGHKM4FY/2111.html:text/html;Défossez - 2021 - Hybrid Spectrogram and Waveform Source Separation.pdf:/Users/bert/Zotero/storage/SAPFCSBE/Défossez - 2021 - Hybrid Spectrogram and Waveform Source Separation.pdf:application/pdf},
}

@article{palmieri_unifying_2022,
	title = {A Unifying View of Estimation and Control Using Belief Propagation With Application to Path Planning},
	volume = {10},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3148127},
	abstract = {The use of estimation techniques on stochastic models to solve control problems is an emerging paradigm that falls under the rubric of Active Inference ({AI}) and Control as Inference ({CAI}). In this work, we use probability propagation on factor graphs to show that various algorithms proposed in the literature can be seen as specific composition rules in a factor graph. We show how this unified approach, presented both in probability space and in log of the probability space, provides a very general framework that includes the Sum-product, the Max-product, Dynamic programming and mixed Reward/Entropy criteria-based algorithms. The framework also expands algorithmic design options that lead to new smoother or sharper policy distributions. We propose original recursions such as: a generalized Sum/Max-product algorithm, a Smooth Dynamic programming algorithm and a modified versions of the Reward/Entropy algorithm. The discussion is carried over with reference to a path planning problem where the recursions that arise from various cost functions, although they may appear similar in scope, bear noticeable differences. We provide a comprehensive table of composition rules and a comparison through simulations, first on a synthetic small grid with a single goal with obstacles, and then on a grid extrapolated from a real-world scene with multiple goals and a semantic map.},
	pages = {15193--15216},
	journaltitle = {{IEEE} Access},
	author = {Palmieri, Francesco A. N. and Pattipati, Krishna R. and Gennaro, Giovanni Di and Fioretti, Giovanni and Verolla, Francesco and Buonanno, Amedeo},
	date = {2022},
	note = {Conference Name: {IEEE} Access},
	keywords = {Estimation, Bayes methods, Heuristic algorithms, Probabilistic logic, Belief propagation, reinforcement learning, Markov decision process, dynamic programming, Dynamic programming, path planning, Path planning},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/BAU2YABL/9698186.html:text/html;Palmieri et al. - 2022 - A Unifying View of Estimation and Control Using Be.pdf:/Users/bert/Zotero/storage/NJUQAF4W/Palmieri et al. - 2022 - A Unifying View of Estimation and Control Using Be.pdf:application/pdf},
}

@inproceedings{waade_inferring_2021,
	location = {Cham},
	title = {Inferring in Circles: Active Inference in Continuous State Space Using Hierarchical Gaussian Filtering of Sufficient Statistics},
	isbn = {978-3-030-93736-2},
	doi = {10.1007/978-3-030-93736-2_57},
	series = {Communications in Computer and Information Science},
	shorttitle = {Inferring in Circles},
	abstract = {We create a continuous state space active inference agent based on the hierarchical Gaussian filter. It uses the {HGF} to track the sufficient statistics of noisy observations of a moving target that is performing a Gaussian random walk with drift and varying volatility. On the basis of this filtering, the agent predicts the target’s position, and minimizes surprisal by staying close to it. Our simulated agent represents the first full implementation of this approach. It demonstrates the feasibility of supplementing active inference with {HGF}-filtering of the sufficient statistics of observations, which is particularly useful in noisy and volatile continuous state space environments.},
	pages = {810--818},
	booktitle = {Machine Learning and Principles and Practice of Knowledge Discovery in Databases},
	publisher = {Springer International Publishing},
	author = {Waade, Peter Thestrup and Mikus, Nace and Mathys, Christoph},
	editor = {Kamp, Michael and Koprinska, Irena and Bibal, Adrien and Bouadi, Tassadit and Frénay, Benoît and Galárraga, Luis and Oramas, José and Adilova, Linara and Krishnamurthy, Yamuna and Kang, Bo and Largeron, Christine and Lijffijt, Jefrey and Viard, Tiphaine and Welke, Pascal and Ruocco, Massimiliano and Aune, Erlend and Gallicchio, Claudio and Schiele, Gregor and Pernkopf, Franz and Blott, Michaela and Fröning, Holger and Schindler, Günther and Guidotti, Riccardo and Monreale, Anna and Rinzivillo, Salvatore and Biecek, Przemyslaw and Ntoutsi, Eirini and Pechenizkiy, Mykola and Rosenhahn, Bodo and Buckley, Christopher and Cialfi, Daniela and Lanillos, Pablo and Ramstead, Maxwell and Verbelen, Tim and Ferreira, Pedro M. and Andresini, Giuseppina and Malerba, Donato and Medeiros, Ibéria and Fournier-Viger, Philippe and Nawaz, M. Saqib and Ventura, Sebastian and Sun, Meng and Zhou, Min and Bitetta, Valerio and Bordino, Ilaria and Ferretti, Andrea and Gullo, Francesco and Ponti, Giovanni and Severini, Lorenzo and Ribeiro, Rita and Gama, João and Gavaldà, Ricard and Cooper, Lee and Ghazaleh, Naghmeh and Richiardi, Jonas and Roqueiro, Damian and Saldana Miranda, Diego and Sechidis, Konstantinos and Graça, Guilherme},
	date = {2021},
	langid = {english},
	keywords = {Active inference, Continuous state space, Hierarchical gaussian filter, Precision-weighted prediction errors, Sufficient statistics filtering},
	file = {Waade et al. - 2021 - Inferring in Circles Active Inference in Continuo.pdf:/Users/bert/Zotero/storage/7MRB6BMI/Waade et al. - 2021 - Inferring in Circles Active Inference in Continuo.pdf:application/pdf},
}

@article{nielsen_thermodynamics_2020,
	title = {Thermodynamics in Ecology—An Introductory Review},
	volume = {22},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/22/8/820},
	doi = {10.3390/e22080820},
	abstract = {How to predict the evolution of ecosystems is one of the numerous questions asked of ecologists by managers and politicians. To answer this we will need to give a scientific definition to concepts like sustainability, integrity, resilience and ecosystem health. This is not an easy task, as modern ecosystem theory exemplifies. Ecosystems show a high degree of complexity, based upon a high number of compartments, interactions and regulations. The last two decades have offered proposals for interpretation of ecosystems within a framework of thermodynamics. The entrance point of such an understanding of ecosystems was delivered more than 50 years ago through Schrödinger’s and Prigogine’s interpretations of living systems as “negentropy feeders” and “dissipative structures”, respectively. Combining these views from the far from equilibrium thermodynamics to traditional classical thermodynamics, and ecology is obviously not going to happen without problems. There seems little reason to doubt that far from equilibrium systems, such as organisms or ecosystems, also have to obey fundamental physical principles such as mass conservation, first and second law of thermodynamics. Both have been applied in ecology since the 1950s and lately the concepts of exergy and entropy have been introduced. Exergy has recently been proposed, from several directions, as a useful indicator of the state, structure and function of the ecosystem. The proposals take two main directions, one concerned with the exergy stored in the ecosystem, the other with the exergy degraded and entropy formation. The implementation of exergy in ecology has often been explained as a translation of the Darwinian principle of “survival of the fittest” into thermodynamics. The fittest ecosystem, being the one able to use and store fluxes of energy and materials in the most efficient manner. The major problem in the transfer to ecology is that thermodynamic properties can only be calculated and not measured. Most of the supportive evidence comes from aquatic ecosystems. Results show that natural and culturally induced changes in the ecosystems, are accompanied by a variations in exergy. In brief, ecological succession is followed by an increase of exergy. This paper aims to describe the state-of-the-art in implementation of thermodynamics into ecology. This includes a brief outline of the history and the derivation of the thermodynamic functions used today. Examples of applications and results achieved up to now are given, and the importance to management laid out. Some suggestions for essential future research agendas of issues that needs resolution are given.},
	pages = {820},
	number = {8},
	journaltitle = {Entropy},
	author = {Nielsen, Søren Nors and Müller, Felix and Marques, Joao Carlos and Bastianoni, Simone and Jørgensen, Sven Erik},
	urldate = {2022-03-18},
	date = {2020-08},
	langid = {english},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {energy, entropy, exergy, far-from-equilibrium systems, maximum entropy production, maximum exergy storage, minimum dissipation, negentropy, thermodynamics of life},
	file = {Nielsen et al. - 2020 - Thermodynamics in Ecology—An Introductory Review.pdf:/Users/bert/Zotero/storage/WN3TTK8B/Nielsen et al. - 2020 - Thermodynamics in Ecology—An Introductory Review.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/YBWJ5EJQ/820.html:text/html},
}

@article{meera_reclaiming_2022,
	title = {Reclaiming saliency: rhythmic precision-modulated action and perception},
	url = {http://arxiv.org/abs/2203.12652},
	shorttitle = {Reclaiming saliency},
	abstract = {Computational models of visual attention in artificial intelligence and robotics have been inspired by the concept of a saliency map. These models account for the mutual information between the (current) visual information and its estimated causes. However, they fail to consider the circular causality between perception and action. In other words, they do not consider where to sample next, given current beliefs. Here, we reclaim salience as an active inference process that relies on two basic principles: uncertainty minimisation and rhythmic scheduling. For this, we make a distinction between attention and salience. Briefly, we associate attention with precision control, i.e., the confidence with which beliefs can be updated given sampled sensory data, and salience with uncertainty minimisation that underwrites the selection of future sensory data. Using this, we propose a new account of attention based on rhythmic precision-modulation and discuss its potential in robotics, providing numerical experiments that showcase advantages of precision-modulation for state and noise estimation, system identification and action selection for informative path planning.},
	journaltitle = {{arXiv}:2203.12652 [cs, q-bio]},
	author = {Meera, Ajith Anil and Novicky, Filip and Parr, Thomas and Friston, Karl and Lanillos, Pablo and Sajid, Noor},
	urldate = {2022-03-25},
	date = {2022-03-23},
	eprinttype = {arxiv},
	eprint = {2203.12652},
	keywords = {Quantitative Biology - Neurons and Cognition, Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/8H7B8KLF/2203.html:text/html;Meera et al. - 2022 - Reclaiming saliency rhythmic precision-modulated .pdf:/Users/bert/Zotero/storage/6G47FCPY/Meera et al. - 2022 - Reclaiming saliency rhythmic precision-modulated .pdf:application/pdf},
}

@article{papageorgiou_hierarchical_2022,
	title = {Hierarchical Bayesian Mixture Models for Time Series Using Context Trees as State Space Partitions},
	url = {http://arxiv.org/abs/2106.03023},
	abstract = {A general Bayesian framework is introduced for mixture modelling and inference with real-valued time series. At the top level, the state space is partitioned via the choice of a discrete context tree, so that the resulting partition depends on the values of some of the most recent samples. At the bottom level, a different model is associated with each region of the partition. This defines a very rich and flexible class of mixture models, for which we provide algorithms that allow for efficient, exact Bayesian inference. In particular, we show that the maximum a posteriori probability ({MAP}) model (including the relevant {MAP} context tree partition) can be precisely identified, along with its exact posterior probability. The utility of this general framework is illustrated in detail when a different autoregressive ({AR}) model is used in each state-space region, resulting in a mixture-of-{AR} model class. The performance of the associated algorithmic tools is demonstrated in the problems of model selection and forecasting on both simulated and real-world data, where they are found to provide results as good or better than state-of-the-art methods.},
	journaltitle = {{arXiv}:2106.03023 [stat]},
	author = {Papageorgiou, Ioannis and Kontoyiannis, Ioannis},
	urldate = {2022-03-28},
	date = {2022-02-02},
	eprinttype = {arxiv},
	eprint = {2106.03023},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Statistics - Applications},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9N93AE2J/2106.html:text/html;Papageorgiou and Kontoyiannis - 2022 - Hierarchical Bayesian Mixture Models for Time Seri.pdf:/Users/bert/Zotero/storage/U73IQ65P/Papageorgiou and Kontoyiannis - 2022 - Hierarchical Bayesian Mixture Models for Time Seri.pdf:application/pdf},
}

@article{broedersz_twenty-five_2022,
	title = {Twenty-five years of nanoscale thermodynamics},
	volume = {604},
	rights = {2022 Nature},
	url = {https://www.nature.com/articles/d41586-022-00869-y},
	doi = {10.1038/d41586-022-00869-y},
	abstract = {How the Jarzynski equality updated the second law of thermodynamics.},
	pages = {46--47},
	number = {7904},
	journaltitle = {Nature},
	author = {Broedersz, Chase P. and Ronceray, Pierre},
	urldate = {2022-04-07},
	date = {2022-04},
	langid = {english},
	note = {Bandiera\_abtest: a
Cg\_type: News And Views
Number: 7904
Publisher: Nature Publishing Group
Subject\_term: Biophysics, Physics},
	keywords = {Physics, Biophysics},
	file = {Broedersz and Ronceray - 2022 - Twenty-five years of nanoscale thermodynamics.pdf:/Users/bert/Zotero/storage/NPJGCJ9L/Broedersz and Ronceray - 2022 - Twenty-five years of nanoscale thermodynamics.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/2R8W7DDB/d41586-022-00869-y.html:text/html},
}

@report{barp_geometric_2022,
	title = {Geometric Methods for Sampling, Optimisation, Inference and Adaptive Agents},
	url = {http://arxiv.org/abs/2203.10592},
	abstract = {In this chapter, we identify fundamental geometric structures that underlie the problems of sampling, optimisation, inference and adaptive decision-making. Based on this identification, we derive algorithms that exploit these geometric structures to solve these problems efficiently. We show that a wide range of geometric theories emerge naturally in these fields, ranging from measure-preserving processes, information divergences, Poisson geometry, and geometric integration. Specifically, we explain how (i) leveraging the symplectic geometry of Hamiltonian systems enable us to construct (accelerated) sampling and optimisation methods, (ii) the theory of Hilbertian subspaces and Stein operators provides a general methodology to obtain robust estimators, (iii) preserving the information geometry of decision-making yields adaptive agents that perform active inference. Throughout, we emphasise the rich connections between these fields; e.g., inference draws on sampling and optimisation, and adaptive decision-making assesses decisions by inferring their counterfactual consequences. Our exposition provides a conceptual overview of underlying ideas, rather than a technical discussion, which can be found in the references herein.},
	number = {{arXiv}:2203.10592},
	institution = {{arXiv}},
	author = {Barp, Alessandro and Da Costa, Lancelot and França, Guilherme and Friston, Karl and Girolami, Mark and Jordan, Michael I. and Pavliotis, Grigorios A.},
	urldate = {2022-05-15},
	date = {2022-04-23},
	doi = {10.48550/arXiv.2203.10592},
	eprinttype = {arxiv},
	eprint = {2203.10592 [cs, math, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Statistics - Machine Learning, Mathematics - Differential Geometry},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/FL6ISHEB/2203.html:text/html;Barp et al. - 2022 - Geometric Methods for Sampling, Optimisation, Infe.pdf:/Users/bert/Zotero/storage/BQ9MYH2B/Barp et al. - 2022 - Geometric Methods for Sampling, Optimisation, Infe.pdf:application/pdf},
}

@report{loo_generalized_2020,
	title = {Generalized Variational Continual Learning},
	url = {http://arxiv.org/abs/2011.12328},
	abstract = {Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online {EWC}) and Variational Continual Learning ({VCL}). {VCL} employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to {VCL} recovers Online {EWC} as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized {VCL} ({GVCL}). In order to mitigate the observed overpruning effect of {VI}, we take inspiration from a common multi-task architecture, neural networks with task-specific {FiLM} layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, {GVCL} strongly outperforms existing baselines. In larger datasets, {GVCL} with {FiLM} layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.},
	number = {{arXiv}:2011.12328},
	institution = {{arXiv}},
	author = {Loo, Noel and Swaroop, Siddharth and Turner, Richard E.},
	urldate = {2022-05-19},
	date = {2020-11-24},
	doi = {10.48550/arXiv.2011.12328},
	eprinttype = {arxiv},
	eprint = {2011.12328 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/K3K2WJX4/2011.html:text/html;Loo et al. - 2020 - Generalized Variational Continual Learning.pdf:/Users/bert/Zotero/storage/VSID8CUB/Loo et al. - 2020 - Generalized Variational Continual Learning.pdf:application/pdf},
}

@report{benzing_unifying_2021,
	title = {Unifying Regularisation Methods for Continual Learning},
	url = {http://arxiv.org/abs/2006.06357},
	abstract = {Continual Learning addresses the challenge of learning a number of different tasks sequentially. The goal of maintaining knowledge of earlier tasks without re-accessing them starkly conflicts with standard {SGD} training for artificial neural networks. An influential method to tackle this problem without storing old data are so-called regularisation approaches. They measure the importance of each parameter for solving a given task and subsequently protect important parameters from large changes. In the literature, three ways to measure parameter importance have been put forward and they have inspired a large body of follow-up work. Here, we present strong theoretical and empirical evidence that these three methods, Elastic Weight Consolidation ({EWC}), Synaptic Intelligence ({SI}) and Memory Aware Synapses ({MAS}), are surprisingly similar and are all linked to the same theoretical quantity. Concretely, we show that, despite stemming from very different motivations, both {SI} and {MAS} approximate the square root of the Fisher Information, with the Fisher being the theoretically justified basis of {EWC}. Moreover, we show that for {SI} the relation to the Fisher -- and in fact its performance -- is due to a previously unknown bias. On top of uncovering unknown similarities and unifying regularisation approaches, we also demonstrate that our insights enable practical performance improvements for large batch training.},
	number = {{arXiv}:2006.06357},
	institution = {{arXiv}},
	author = {Benzing, Frederik},
	urldate = {2022-05-19},
	date = {2021-02-03},
	doi = {10.48550/arXiv.2006.06357},
	eprinttype = {arxiv},
	eprint = {2006.06357 [cs, stat]},
	note = {type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/JGG4HZQT/Benzing - 2021 - Unifying Regularisation Methods for Continual Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/J3R6Y6BY/2006.html:text/html},
}

@article{shimazaki_principles_2020,
	title = {The principles of adaptation in organisms and machines {II}: thermodynamics of the Bayesian brain},
	shorttitle = {The principles of adaptation in organisms and machines {II}},
	journaltitle = {{arXiv} preprint {arXiv}:2006.13158},
	author = {Shimazaki, Hideaki},
	date = {2020},
	file = {Shimazaki - 2020 - The principles of adaptation in organisms and mach.pdf:/Users/bert/Zotero/storage/5QTXIR4F/Shimazaki - 2020 - The principles of adaptation in organisms and mach.pdf:application/pdf},
}

@report{ramstead_bayesian_2022,
	title = {On Bayesian Mechanics: A Physics of and by Beliefs},
	url = {http://arxiv.org/abs/2205.11543},
	shorttitle = {On Bayesian Mechanics},
	abstract = {The aim of this paper is to introduce a field of study that has emerged over the last decade, called Bayesian mechanics. Bayesian mechanics is a probabilistic mechanics, comprising tools that enable us to model systems endowed with a particular partition (i.e., into particles), where the internal states (or the trajectories of internal states) of a particular system encode the parameters of beliefs about quantities that characterise the system. These tools allow us to write down mechanical theories for systems that look as if they are estimating posterior probability distributions over the causes of their sensory states, providing a formal language to model the constraints, forces, fields, and potentials that determine how the internal states of such systems move in a space of beliefs (i.e., on a statistical manifold). Here we will review the state of the art in the literature on the free energy principle, distinguishing between three ways in which Bayesian mechanics has been applied to particular systems (i.e., path-tracking, mode-tracking, and mode-matching). We will go on to examine the duality of the free energy principle and the constrained maximum entropy principle, both of which lie at the heart of Bayesian mechanics. We also discuss the implications of this duality for Bayesian mechanics and limitations of current treatments.},
	number = {{arXiv}:2205.11543},
	institution = {{arXiv}},
	author = {Ramstead, Maxwell J. D. and Sakthivadivel, Dalton A. R. and Heins, Conor and Koudahl, Magnus and Millidge, Beren and Da Costa, Lancelot and Klein, Brennan and Friston, Karl J.},
	urldate = {2022-05-25},
	date = {2022-05-23},
	doi = {10.48550/arXiv.2205.11543},
	eprinttype = {arxiv},
	eprint = {2205.11543 [cond-mat, physics:nlin, physics:physics]},
	note = {type: article},
	keywords = {Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Statistical Mechanics, Physics - Biological Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/2KVVVRS5/2205.html:text/html;Ramstead et al. - 2022 - On Bayesian Mechanics A Physics of and by Beliefs.pdf:/Users/bert/Zotero/storage/5X5EWD34/Ramstead et al. - 2022 - On Bayesian Mechanics A Physics of and by Beliefs.pdf:application/pdf},
}

@report{palmieri_comparison_2013,
	title = {A Comparison of Algorithms for Learning Hidden Variables in Normal Graphs},
	url = {http://arxiv.org/abs/1308.5576},
	abstract = {A Bayesian factor graph reduced to normal form consists in the interconnection of diverter units (or equal constraint units) and Single-Input/Single-Output ({SISO}) blocks. In this framework localized adaptation rules are explicitly derived from a constrained maximum likelihood ({ML}) formulation and from a minimum {KL}-divergence criterion using {KKT} conditions. The learning algorithms are compared with two other updating equations based on a Viterbi-like and on a variational approximation respectively. The performance of the various algorithm is verified on synthetic data sets for various architectures. The objective of this paper is to provide the programmer with explicit algorithms for rapid deployment of Bayesian graphs in the applications.},
	number = {{arXiv}:1308.5576},
	institution = {{arXiv}},
	author = {Palmieri, Francesco A. N.},
	urldate = {2022-06-06},
	date = {2013-08-26},
	eprinttype = {arxiv},
	eprint = {1308.5576 [cs, math, stat]},
	note = {type: article},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/5UQIYJ49/1308.html:text/html;Palmieri - 2013 - A Comparison of Algorithms for Learning Hidden Var.pdf:/Users/bert/Zotero/storage/MTEF564J/Palmieri - 2013 - A Comparison of Algorithms for Learning Hidden Var.pdf:application/pdf},
}

@report{da_costa_relationship_2020-1,
	title = {The relationship between dynamic programming and active inference: the discrete, finite-horizon case},
	url = {http://arxiv.org/abs/2009.08111},
	shorttitle = {The relationship between dynamic programming and active inference},
	abstract = {Active inference is a normative framework for generating behaviour based upon the free energy principle, a theory of self-organisation. This framework has been successfully used to solve reinforcement learning and stochastic control problems, yet, the formal relation between active inference and reward maximisation has not been fully explicated. In this paper, we consider the relation between active inference and dynamic programming under the Bellman equation, which underlies many approaches to reinforcement learning and control. We show that, on partially observable Markov decision processes, dynamic programming is a limiting case of active inference. In active inference, agents select actions to minimise expected free energy. In the absence of ambiguity about states, this reduces to matching expected states with a target distribution encoding the agent's preferences. When target states correspond to rewarding states, this maximises expected reward, as in reinforcement learning. When states are ambiguous, active inference agents will choose actions that simultaneously minimise ambiguity. This allows active inference agents to supplement their reward maximising (or exploitative) behaviour with novelty-seeking (or exploratory) behaviour. This clarifies the connection between active inference and reinforcement learning, and how both frameworks may benefit from each other.},
	number = {{arXiv}:2009.08111},
	institution = {{arXiv}},
	author = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
	urldate = {2022-06-06},
	date = {2020-09-22},
	doi = {10.48550/arXiv.2009.08111},
	eprinttype = {arxiv},
	eprint = {2009.08111 [cs, math, q-bio]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Optimization and Control, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/J5F3CM4S/2009.html:text/html;Da Costa et al. - 2020 - The relationship between dynamic programming and a.pdf:/Users/bert/Zotero/storage/S66IEM6T/Da Costa et al. - 2020 - The relationship between dynamic programming and a.pdf:application/pdf},
}

@report{millet_toward_2022,
	title = {Toward a realistic model of speech processing in the brain with self-supervised learning},
	url = {http://arxiv.org/abs/2206.01685},
	abstract = {Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on the issue of speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised architecture, Wav2Vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging ({fMRI}), while they listened to {\textasciitilde}1h of audio books. Our results are four-fold. First, we show that this algorithm learns brain-like representations with as little as 600 hours of unlabelled speech -- a quantity comparable to what infants can be exposed to during language acquisition. Second, its functional hierarchy aligns with the cortical hierarchy of speech processing. Third, different training regimes reveal a functional specialization akin to the cortex: Wav2Vec 2.0 learns sound-generic, speech-specific and language-specific representations similar to those of the prefrontal and temporal cortices. Fourth, we confirm the similarity of this specialization with the behavior of 386 additional participants. These elements, resulting from the largest neuroimaging benchmark to date, show how self-supervised learning can account for a rich organization of speech processing in the brain, and thus delineate a path to identify the laws of language acquisition which shape the human brain.},
	number = {{arXiv}:2206.01685},
	institution = {{arXiv}},
	author = {Millet, Juliette and Caucheteux, Charlotte and Orhan, Pierre and Boubenec, Yves and Gramfort, Alexandre and Dunbar, Ewan and Pallier, Christophe and King, Jean-Remi},
	urldate = {2022-06-12},
	date = {2022-06-03},
	doi = {10.48550/arXiv.2206.01685},
	eprinttype = {arxiv},
	eprint = {2206.01685 [cs, q-bio]},
	note = {type: article},
	keywords = {Computer Science - Artificial Intelligence, Quantitative Biology - Neurons and Cognition, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/9G7XR4YL/2206.html:text/html;Millet et al. - 2022 - Toward a realistic model of speech processing in t.pdf:/Users/bert/Zotero/storage/BALVRYPJ/Millet et al. - 2022 - Toward a realistic model of speech processing in t.pdf:application/pdf},
}

@misc{sennesh_deriving_2022,
	title = {Deriving time-averaged active inference from control principles},
	url = {http://arxiv.org/abs/2208.10601},
	doi = {10.48550/arXiv.2208.10601},
	abstract = {Active inference offers a principled account of behavior as minimizing average sensory surprise over time. Applications of active inference to control problems have heretofore tended to focus on finite-horizon or discounted-surprise problems, despite deriving from the infinite-horizon, average-surprise imperative of the free-energy principle. Here we derive an infinite-horizon, average-surprise formulation of active inference from optimal control principles. Our formulation returns to the roots of active inference in neuroanatomy and neurophysiology, formally reconnecting active inference to optimal feedback control. Our formulation provides a unified objective functional for sensorimotor control and allows for reference states to vary over time.},
	number = {{arXiv}:2208.10601},
	publisher = {{arXiv}},
	author = {Sennesh, Eli and Theriault, Jordan and van de Meent, Jan-Willem and Barrett, Lisa Feldman and Quigley, Karen},
	urldate = {2022-08-24},
	date = {2022-08-22},
	eprinttype = {arxiv},
	eprint = {2208.10601 [cs, eess, q-bio, stat]},
	keywords = {Electrical Engineering and Systems Science - Systems and Control, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/YU4YDV5A/2208.html:text/html;Sennesh et al. - 2022 - Deriving time-averaged active inference from contr.pdf:/Users/bert/Zotero/storage/KEWY4STT/Sennesh et al. - 2022 - Deriving time-averaged active inference from contr.pdf:application/pdf},
}

@article{sennesh_interoception_2022,
	title = {Interoception as modeling, allostasis as control},
	volume = {167},
	issn = {0301-0511},
	url = {https://www.sciencedirect.com/science/article/pii/S0301051121002350},
	doi = {10.1016/j.biopsycho.2021.108242},
	abstract = {The brain regulates the body by anticipating its needs and attempting to meet them before they arise – a process called allostasis. Allostasis requires a model of the changing sensory conditions within the body, a process called interoception. In this paper, we examine how interoception may provide performance feedback for allostasis. We suggest studying allostasis in terms of control theory, reviewing control theory’s applications to related issues in physiology, motor control, and decision making. We synthesize these by relating them to the important properties of allostatic regulation as a control problem. We then sketch a novel formalism for how the brain might perform allostatic control of the viscera by analogy to skeletomotor control, including a mathematical view on how interoception acts as performance feedback for allostasis. Finally, we suggest ways to test implications of our hypotheses.},
	pages = {108242},
	journaltitle = {Biological Psychology},
	shortjournal = {Biological Psychology},
	author = {Sennesh, Eli and Theriault, Jordan and Brooks, Dana and van de Meent, Jan-Willem and Barrett, Lisa Feldman and Quigley, Karen S.},
	urldate = {2022-09-01},
	date = {2022-01-01},
	langid = {english},
	keywords = {Interoception, Predictive processing, Allostasis},
	file = {ScienceDirect Snapshot:/Users/bert/Zotero/storage/F252236D/S0301051121002350.html:text/html;Sennesh et al. - 2022 - Interoception as modeling, allostasis as control.pdf:/Users/bert/Zotero/storage/3JQ5I2YW/Sennesh et al. - 2022 - Interoception as modeling, allostasis as control.pdf:application/pdf},
}

@article{gannot_iterative_1998,
	title = {Iterative and sequential Kalman filter-based speech enhancement algorithms},
	volume = {6},
	issn = {1558-2353},
	doi = {10.1109/89.701367},
	abstract = {Speech quality and intelligibility might significantly deteriorate in the presence of background noise, especially when the speech signal is subject to subsequent processing. In particular, speech coders and automatic speech recognition ({ASR}) systems that were designed or trained to act on clean speech signals might be rendered useless in the presence of background noise. Speech enhancement algorithms have therefore attracted a great deal of interest. In this paper, we present a class of Kalman filter-based algorithms with some extensions, modifications, and improvements of previous work. The first algorithm employs the estimate-maximize ({EM}) method to iteratively estimate the spectral parameters of the speech and noise parameters. The enhanced speech signal is obtained as a byproduct of the parameter estimation algorithm. The second algorithm is a sequential, computationally efficient, gradient descent algorithm. We discuss various topics concerning the practical implementation of these algorithms. Extensive experimental study using real speech and noise signals is provided to compare these algorithms with alternative speech enhancement algorithms, and to compare the performance of the iterative and sequential algorithms.},
	pages = {373--385},
	number = {4},
	journaltitle = {{IEEE} Transactions on Speech and Audio Processing},
	author = {Gannot, S. and Burshtein, D. and Weinstein, E.},
	date = {1998-07},
	keywords = {Kalman filters, Speech processing, speech enhancement, Speech enhancement, iterative methods, Iterative algorithms, intelligibility, speech quality, Equations, acoustic noise, automatic speech recognition, Automatic speech recognition, background noise, Background noise, estimate-maximize method, gradient descent algorithm, iterative Kalman filter-based speech enhancement algorithm, Maximum likelihood estimation, noise parameters, parameter estimation, Parameter estimation, sequential Kalman filter-based speech enhancement algorithm, spectral analysis, spectral parameters, speech coders, speech signal, Wiener filter},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/F4U6ZJE8/701367.html:text/html;IEEE Xplore Full Text PDF:/Users/bert/Zotero/storage/E3X3XWZ3/Gannot et al. - 1998 - Iterative and sequential Kalman filter-based speec.pdf:application/pdf},
}

@article{latash_laws_2021,
	title = {Laws of nature that define biological action and perception},
	volume = {36},
	issn = {1571-0645},
	url = {https://www.sciencedirect.com/science/article/pii/S1571064520300646},
	doi = {10.1016/j.plrev.2020.07.007},
	abstract = {We describe a physical approach to biological functions, with the emphasis on the motor and sensory functions. The approach assumes the existence of biology-specific laws of nature uniting salient physical variables and parameters. In contrast to movements in inanimate nature, actions are produced by changes in parameters of the corresponding laws of nature. For movements, parameters are associated with spatial referent coordinates ({RCs}) for the effectors. Stability of motor actions is ensured by the abundant mapping of {RCs} across hierarchical control levels. The sensory function is viewed as based on an interaction of efferent and afferent signals leading to an iso-perceptual manifold where percepts of salient sensory variables are stable. This approach offers novel interpretations for a variety of known neurophysiological and behavioral phenomena and makes a number of novel testable predictions. In particular, we discuss novel interpretations for the well-known phenomena of agonist-antagonist co-activation and vibration-induced illusions of both position and force. We also interpret results of several new experiments with unintentional force changes and with analysis of accuracy of perception of variables produced by elements of multi-element systems. Recently, this approach has been expanded to interpret motor disorders including spasticity and consequences of subcortical disorders (such as Parkinson's disease). We suggest that the approach can be developed for cognitive functions.},
	pages = {47--67},
	journaltitle = {Physics of Life Reviews},
	shortjournal = {Physics of Life Reviews},
	author = {Latash, Mark L.},
	urldate = {2022-09-09},
	date = {2021-03-01},
	langid = {english},
	keywords = {Abundance, Co-activation, Iso-perceptual manifold, Movement disorder, Referent coordinate, Uncontrolled manifold},
	file = {Latash - 2021 - Laws of nature that define biological action and p.pdf:/Users/bert/Zotero/storage/ASL9SL75/Latash - 2021 - Laws of nature that define biological action and p.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/HCGQCQDV/S1571064520300646.html:text/html},
}

@inproceedings{campbell_online_2021,
	title = {Online Variational Filtering and Parameter Learning},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/9a6a1aaafe73c572b7374828b03a1881-Abstract.html},
	abstract = {We present a variational method for online state estimation and parameter learning in state-space models ({SSMs}), a ubiquitous class of latent variable models for sequential data. As per standard batch variational techniques, we use stochastic gradients to simultaneously optimize a lower bound on the log evidence with respect to both model parameters and a variational approximation of the states' posterior distribution. However, unlike existing approaches, our method is able to operate in an entirely online manner, such that historic observations do not require revisitation after being incorporated and the cost of updates at each time step remains constant, despite the growing dimensionality of the joint posterior distribution of the states. This is achieved by utilizing backward decompositions of this joint posterior distribution and of its variational approximation, combined with Bellman-type recursions for the evidence lower bound and its gradients. We demonstrate the performance of this methodology across several examples, including high-dimensional {SSMs} and sequential Variational Auto-Encoders.},
	pages = {18633--18645},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Campbell, Andrew and Shi, Yuyang and Rainforth, Thomas and Doucet, Arnaud},
	urldate = {2022-09-09},
	date = {2021},
	file = {Campbell et al. - 2021 - Online Variational Filtering and Parameter Learnin.pdf:/Users/bert/Zotero/storage/W3369NZ9/Campbell et al. - 2021 - Online Variational Filtering and Parameter Learnin.pdf:application/pdf},
}

@misc{isomura_experimental_2022,
	title = {Experimental validation of the free-energy principle with in vitro neural networks},
	rights = {© 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2022.10.03.510742v1},
	doi = {10.1101/2022.10.03.510742},
	abstract = {Empirical applications of the free-energy principle are not straightforward because they entail a commitment to a particular process theory, especially at the cellular and synaptic levels. Using a recently established reverse engineering technique, we confirm the quantitative predictions of the free-energy principle using in vitro networks of rat cortical neurons that perform causal inference. Upon receiving electrical stimuli—generated by mixing two hidden sources—neurons self-organised to selectively encode the two sources. Pharmacological up- and downregulation of network excitability disrupted the ensuing inference, consistent with changes in prior beliefs about hidden sources. As predicted, changes in effective synaptic connectivity reduced variational free energy, where the connection strengths encoded parameters of the generative model. In short, we show that variational free energy minimisation can quantitatively predict the self-organisation of neuronal networks, in terms of their responses and plasticity. These results demonstrate the applicability of the free-energy principle to in vitro neural networks and establish its predictive validity in this setting.},
	publisher = {{bioRxiv}},
	author = {Isomura, Takuya and Kotani, Kiyoshi and Jimbo, Yasuhiko and Friston, Karl},
	urldate = {2022-10-08},
	date = {2022-10-07},
	langid = {english},
	note = {Pages: 2022.10.03.510742
Section: New Results},
	file = {Isomura et al. - 2022 - Experimental validation of the free-energy princip.pdf:/Users/bert/Zotero/storage/Q659UMGB/Isomura et al. - 2022 - Experimental validation of the free-energy princip.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NP6Z6S25/2022.10.03.html:text/html},
}

@misc{luo_understanding_2022,
	title = {Understanding Diffusion Models: A Unified Perspective},
	url = {http://arxiv.org/abs/2208.11970},
	doi = {10.48550/arXiv.2208.11970},
	shorttitle = {Understanding Diffusion Models},
	abstract = {Diffusion models have shown incredible capabilities as generative models; indeed, they power the current state-of-the-art models on text-conditioned image generation such as Imagen and {DALL}-E 2. In this work we review, demystify, and unify the understanding of diffusion models across both variational and score-based perspectives. We first derive Variational Diffusion Models ({VDM}) as a special case of a Markovian Hierarchical Variational Autoencoder, where three key assumptions enable tractable computation and scalable optimization of the {ELBO}. We then prove that optimizing a {VDM} boils down to learning a neural network to predict one of three potential objectives: the original source input from any arbitrary noisification of it, the original source noise from any arbitrarily noisified input, or the score function of a noisified input at any arbitrary noise level. We then dive deeper into what it means to learn the score function, and connect the variational perspective of a diffusion model explicitly with the Score-based Generative Modeling perspective through Tweedie's Formula. Lastly, we cover how to learn a conditional distribution using diffusion models via guidance.},
	number = {{arXiv}:2208.11970},
	publisher = {{arXiv}},
	author = {Luo, Calvin},
	urldate = {2022-10-08},
	date = {2022-08-25},
	eprinttype = {arxiv},
	eprint = {2208.11970 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/42NHEHMU/2208.html:text/html;Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf:/Users/bert/Zotero/storage/AHB9NGTQ/Luo - 2022 - Understanding Diffusion Models A Unified Perspect.pdf:application/pdf},
}

@misc{zhao_deep_2021,
	title = {Deep Bayesian Unsupervised Lifelong Learning},
	url = {http://arxiv.org/abs/2106.07035},
	doi = {10.48550/arXiv.2106.07035},
	abstract = {Lifelong Learning ({LL}) refers to the ability to continually learn and solve new problems with incremental available information over time while retaining previous knowledge. Much attention has been given lately to Supervised Lifelong Learning ({SLL}) with a stream of labelled data. In contrast, we focus on resolving challenges in Unsupervised Lifelong Learning ({ULL}) with streaming unlabelled data when the data distribution and the unknown class labels evolve over time. Bayesian framework is natural to incorporate past knowledge and sequentially update the belief with new data. We develop a fully Bayesian inference framework for {ULL} with a novel end-to-end Deep Bayesian Unsupervised Lifelong Learning ({DBULL}) algorithm, which can progressively discover new clusters without forgetting the past with unlabelled data while learning latent representations. To efficiently maintain past knowledge, we develop a novel knowledge preservation mechanism via sufficient statistics of the latent representation for raw data. To detect the potential new clusters on the fly, we develop an automatic cluster discovery and redundancy removal strategy in our inference inspired by Nonparametric Bayesian statistics techniques. We demonstrate the effectiveness of our approach using image and text corpora benchmark datasets in both {LL} and batch settings.},
	number = {{arXiv}:2106.07035},
	publisher = {{arXiv}},
	author = {Zhao, Tingting and Wang, Zifeng and Masoomi, Aria and Dy, Jennifer},
	urldate = {2022-10-10},
	date = {2021-06-13},
	eprinttype = {arxiv},
	eprint = {2106.07035 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4F2BMD83/2106.html:text/html;Zhao et al. - 2021 - Deep Bayesian Unsupervised Lifelong Learning.pdf:/Users/bert/Zotero/storage/NTZALQQ7/Zhao et al. - 2021 - Deep Bayesian Unsupervised Lifelong Learning.pdf:application/pdf},
}

@thesis{yin_efficient_2022,
	location = {Eindhoven},
	title = {Efficient and Accurate Spiking Neural Networks},
	pagetotal = {123},
	institution = {{TU} Eindhoven},
	type = {phdthesis},
	author = {Yin, Bojian},
	date = {2022},
	langid = {english},
	file = {Yin - 2022 - Efficient and Accurate Spiking Neural Networks.pdf:/Users/bert/Zotero/storage/MW6QD9BY/Yin - 2022 - Efficient and Accurate Spiking Neural Networks.pdf:application/pdf},
}

@inproceedings{wu_approximate_2022,
	title = {Approximate Inference for Stochastic Planning in Factored Spaces},
	url = {https://proceedings.mlr.press/v186/wu22a.html},
	abstract = {Stochastic planning can be reduced to probabilistic inference in large discrete graphical models, but hardness of inference requires approximation schemes to be used. In this paper we argue that such applications can be disentangled along two dimensions. The first is the direction of information flow in the idealized exact optimization objective, i.e., forward vs backward inference. The second is the type of approximation used to compute this objective, e.g., Belief Propagation ({BP}) vs mean field variational inference ({MFVI}). This new categorization allows us to unify a large amount of isolated efforts in prior work explaining their connections and differences as well as potential improvements. An extensive experimental evaluation over large stochastic planning problems shows the advantage of forward {BP} over several algorithms based on {MFVI}. An analysis of practical limitations of {MFVI} motivates a novel algorithm, collapsed state variational inference ({CSVI}), which provides a tighter approximation and achieves comparable planning performance with forward {BP}.},
	eventtitle = {International Conference on Probabilistic Graphical Models},
	pages = {433--444},
	booktitle = {Proceedings of The 11th International Conference on Probabilistic Graphical Models},
	publisher = {{PMLR}},
	author = {Wu, Zhennan and Khardon, Roni},
	urldate = {2022-10-25},
	date = {2022-09-19},
	langid = {english},
	note = {{ISSN}: 2640-3498},
	file = {Wu and Khardon - 2022 - Approximate Inference for Stochastic Planning in F.pdf:/Users/bert/Zotero/storage/SIZF9END/Wu and Khardon - 2022 - Approximate Inference for Stochastic Planning in F.pdf:application/pdf},
}

@misc{scheibler_diffusion-based_2022,
	title = {Diffusion-based Generative Speech Source Separation},
	url = {http://arxiv.org/abs/2210.17327},
	abstract = {We propose {DiffSep}, a new single channel source separation method based on score-matching of a stochastic differential equation ({SDE}). We craft a tailored continuous time diffusion-mixing process starting from the separated sources and converging to a Gaussian distribution centered on their mixture. This formulation lets us apply the machinery of score-based generative modelling. First, we train a neural network to approximate the score function of the marginal probabilities or the diffusion-mixing process. Then, we use it to solve the reverse time {SDE} that progressively separates the sources starting from their mixture. We propose a modified training strategy to handle model mismatch and source permutation ambiguity. Experiments on the {WSJ}0 2mix dataset demonstrate the potential of the method. Furthermore, the method is also suitable for speech enhancement and shows performance competitive with prior work on the {VoiceBank}-{DEMAND} dataset.},
	number = {{arXiv}:2210.17327},
	publisher = {{arXiv}},
	author = {Scheibler, Robin and Ji, Youna and Chung, Soo-Whan and Byun, Jaeuk and Choe, Soyeon and Choi, Min-Seok},
	urldate = {2022-11-03},
	date = {2022-11-02},
	eprinttype = {arxiv},
	eprint = {2210.17327 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/XLCMCAMA/2210.html:text/html;Scheibler et al. - 2022 - Diffusion-based Generative Speech Source Separatio.pdf:/Users/bert/Zotero/storage/IVYY8EVI/Scheibler et al. - 2022 - Diffusion-based Generative Speech Source Separatio.pdf:application/pdf},
}

@article{turchet_elk_2021,
	title = {Elk Audio {OS}: An Open Source Operating System for the Internet of Musical Things},
	volume = {2},
	issn = {2691-1914, 2577-6207},
	url = {https://dl.acm.org/doi/10.1145/3446393},
	doi = {10.1145/3446393},
	shorttitle = {Elk Audio {OS}},
	abstract = {As the Internet of Musical Things ({IoMusT}) emerges, audio-specific operating systems ({OSs}) are required on embedded hardware to ease development and portability of {IoMusT} applications. Despite the increasing importance of {IoMusT} applications, in this article, we show that there is no {OS} able to fulfill the diverse requirements of {IoMusT} systems. To address such a gap, we propose the Elk Audio {OS} as a novel and open source {OS} in this space. It is a Linux-based {OS} optimized for ultra-low-latency and high-performance audio and sensor processing on embedded hardware, as well as for handling wireless connectivity to local and remote networks. Elk Audio {OS} uses the Xenomai real-time kernel extension, which makes it suitable for the most demanding of low-latency audio tasks. We provide the first comprehensive overview of Elk Audio {OS}, describing its architecture and the key components of interest to potential developers and users. We explain operational aspects like the configuration of the architecture and the control mechanisms of the internal sound engine, as well as the tools that enable an easier and faster development of connected musical devices. Finally, we discuss the implications of Elk Audio {OS}, including the development of an open source community around it.},
	pages = {1--18},
	number = {2},
	journaltitle = {{ACM} Transactions on Internet of Things},
	shortjournal = {{ACM} Trans. Internet Things},
	author = {Turchet, Luca and Fischione, Carlo},
	urldate = {2022-11-09},
	date = {2021-05-31},
	langid = {english},
	file = {Turchet and Fischione - 2021 - Elk Audio OS An Open Source Operating System for .pdf:/Users/bert/Zotero/storage/AE3YNAFJ/Turchet and Fischione - 2021 - Elk Audio OS An Open Source Operating System for .pdf:application/pdf},
}

@online{noauthor_elk_nodate,
	title = {Elk Audio {OS}: An Open Source Operating System for the Internet of Musical Things: {ACM} Transactions on Internet of Things: Vol 2, No 2},
	url = {https://dl.acm.org/doi/10.1145/3446393},
	urldate = {2022-11-09},
	file = {Elk Audio OS\: An Open Source Operating System for the Internet of Musical Things\: ACM Transactions on Internet of Things\: Vol 2, No 2:/Users/bert/Zotero/storage/LUC7WQKS/3446393.html:text/html},
}

@article{neacsu_structure_2022,
	title = {Structure learning enhances concept formation in synthetic Active Inference agents},
	volume = {17},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0277199},
	doi = {10.1371/journal.pone.0277199},
	abstract = {Humans display astonishing skill in learning about the environment in which they operate. They assimilate a rich set of affordances and interrelations among different elements in particular contexts, and form flexible abstractions (i.e., concepts) that can be generalised and leveraged with ease. To capture these abilities, we present a deep hierarchical Active Inference model of goal-directed behaviour, and the accompanying belief update schemes implied by maximising model evidence. Using simulations, we elucidate the potential mechanisms that underlie and influence concept learning in a spatial foraging task. We show that the representations formed–as a result of foraging–reflect environmental structure in a way that is enhanced and nuanced by Bayesian model reduction, a special case of structure learning that typifies learning in the absence of new evidence. Synthetic agents learn associations and form concepts about environmental context and configuration as a result of inferential, parametric learning, and structure learning processes–three processes that can produce a diversity of beliefs and belief structures. Furthermore, the ensuing representations reflect symmetries for environments with identical configurations.},
	pages = {e0277199},
	number = {11},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Neacsu, Victorita and Mirza, M. Berk and Adams, Rick A. and Friston, Karl J.},
	urldate = {2022-11-17},
	date = {2022-11-14},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Free energy, Machine learning, Probability distribution, Sensory perception, Learning, Agent-based modeling, Foraging},
	file = {Neacsu et al. - 2022 - Structure learning enhances concept formation in s.pdf:/Users/bert/Zotero/storage/RZRL9R44/Neacsu et al. - 2022 - Structure learning enhances concept formation in s.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/TDGAIANN/article.html:text/html},
}

@article{merel_hierarchical_2019,
	title = {Hierarchical motor control in mammals and machines},
	volume = {10},
	rights = {2019 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-019-13239-6},
	doi = {10.1038/s41467-019-13239-6},
	abstract = {Advances in artificial intelligence are stimulating interest in neuroscience. However, most attention is given to discrete tasks with simple action spaces, such as board games and classic video games. Less discussed in neuroscience are parallel advances in “synthetic motor control”. While motor neuroscience has recently focused on optimization of single, simple movements, {AI} has progressed to the generation of rich, diverse motor behaviors across multiple tasks, at humanoid scale. It is becoming clear that specific, well-motivated hierarchical design elements repeatedly arise when engineering these flexible control systems. We review these core principles of hierarchical control, relate them to hierarchy in the nervous system, and highlight research themes that we anticipate will be critical in solving challenges at this disciplinary intersection.},
	pages = {5489},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Merel, Josh and Botvinick, Matthew and Wayne, Greg},
	urldate = {2022-11-23},
	date = {2019-12-02},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computer science, Motor control},
	file = {Merel et al. - 2019 - Hierarchical motor control in mammals and machines.pdf:/Users/bert/Zotero/storage/I3HQ9Q5E/Merel et al. - 2019 - Hierarchical motor control in mammals and machines.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/M3453985/s41467-019-13239-6.html:text/html},
}

@article{hipolito_embodied_2021,
	title = {Embodied skillful performance: where the action is},
	volume = {199},
	issn = {1573-0964},
	url = {https://doi.org/10.1007/s11229-020-02986-5},
	doi = {10.1007/s11229-020-02986-5},
	shorttitle = {Embodied skillful performance},
	abstract = {When someone masters a skill, their performance looks to us like second nature: it looks as if their actions are smoothly performed without explicit, knowledge-driven, online monitoring of their performance. Contemporary computational models in motor control theory, however, are instructionist: that is, they cast skillful performance as a knowledge-driven process. Optimal motor control theory ({OMCT}), as representative par excellence of such approaches, casts skillful performance as an instruction, instantiated in the brain, that needs to be executed—a motor command. This paper aims to show the limitations of such instructionist approaches to skillful performance. We specifically address the question of whether the assumption of control-theoretic models is warranted. The first section of this paper examines the instructionist assumption, according to which skillful performance consists of the execution of theoretical instructions harnessed in motor representations. The second and third sections characterize the implementation of motor representations as motor commands, with a special focus on formulations from {OMCT}. The final sections of this paper examine predictive coding and active inference—behavioral modeling frameworks that descend, but are distinct, from {OMCT}—and argue that the instructionist, control-theoretic assumptions are ill-motivated in light of new developments in active inference.},
	pages = {4457--4481},
	number = {1},
	journaltitle = {Synthese},
	shortjournal = {Synthese},
	author = {Hipólito, Inês and Baltieri, Manuel and Friston, Karl and Ramstead, Maxwell J. D.},
	urldate = {2022-11-24},
	date = {2021-12-01},
	langid = {english},
	keywords = {Active inference, Action-oriented representation, Instructionism, Motor representation, Optimal control theory, Skillful performance},
	file = {Hipólito et al. - 2021 - Embodied skillful performance where the action is.pdf:/Users/bert/Zotero/storage/J74W7XBB/Hipólito et al. - 2021 - Embodied skillful performance where the action is.pdf:application/pdf},
}

@misc{friston_path_2022,
	title = {Path integrals, particular kinds, and strange things},
	url = {http://arxiv.org/abs/2210.12761},
	abstract = {This paper describes a path integral formulation of the free energy principle. The ensuing account expresses the paths or trajectories that a particle takes as it evolves over time. The main results are a method or principle of least action that can be used to emulate the behaviour of particles in open exchange with their external milieu. Particles are defined by a particular partition, in which internal states are individuated from external states by active and sensory blanket states. The variational principle at hand allows one to interpret internal dynamics - of certain kinds of particles - as inferring external states that are hidden behind blanket states. We consider different kinds of particles, and to what extent they can be imbued with an elementary form of inference or sentience. Specifically, we consider the distinction between dissipative and conservative particles, inert and active particles and, finally, ordinary and strange particles. Strange particles (look as if they) infer their own actions, endowing them with apparent autonomy or agency. In short - of the kinds of particles afforded by a particular partition - strange kinds may be apt for describing sentient behaviour.},
	number = {{arXiv}:2210.12761},
	publisher = {{arXiv}},
	author = {Friston, Karl and Da Costa, Lancelot and Sakthivadivel, Dalton A. R. and Heins, Conor and Pavliotis, Grigorios A. and Ramstead, Maxwell and Parr, Thomas},
	urldate = {2022-11-26},
	date = {2022-11-04},
	eprinttype = {arxiv},
	eprint = {2210.12761 [math-ph, physics:nlin, physics:physics, q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition, Mathematical Physics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Physics - Biological Physics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4W3FP52I/2210.html:text/html;Friston et al. - 2022 - Path integrals, particular kinds, and strange thin.pdf:/Users/bert/Zotero/storage/NL6R86N4/Friston et al. - 2022 - Path integrals, particular kinds, and strange thin.pdf:application/pdf},
}

@article{ali_predictive_2022,
	title = {Predictive coding is a consequence of energy efficiency in recurrent neural networks},
	issn = {2666-3899},
	url = {https://www.sciencedirect.com/science/article/pii/S2666389922002719},
	doi = {10.1016/j.patter.2022.100639},
	abstract = {Predictive coding is a promising framework for understanding brain function. It postulates that the brain continuously inhibits predictable sensory input, ensuring preferential processing of surprising elements. A central aspect of this view is its hierarchical connectivity, involving recurrent message passing between excitatory bottom-up signals and inhibitory top-down feedback. Here we use computational modeling to demonstrate that such architectural hardwiring is not necessary. Rather, predictive coding is shown to emerge as a consequence of energy efficiency. When training recurrent neural networks to minimize their energy consumption while operating in predictive environments, the networks self-organize into prediction and error units with appropriate inhibitory and excitatory interconnections and learn to inhibit predictable sensory input. Moving beyond the view of purely top-down-driven predictions, we demonstrate, via virtual lesioning experiments, that networks perform predictions on two timescales: fast lateral predictions among sensory units and slower prediction cycles that integrate evidence over time.},
	pages = {100639},
	journaltitle = {Patterns},
	shortjournal = {Patterns},
	author = {Ali, Abdullahi and Ahmad, Nasir and de Groot, Elgar and Johannes van Gerven, Marcel Antonius and Kietzmann, Tim Christian},
	urldate = {2022-11-30},
	date = {2022-11-23},
	langid = {english},
	keywords = {recurrent neural networks, predictive coding, energy efficiency, brain-inspired machine learning},
	file = {Ali et al. - 2022 - Predictive coding is a consequence of energy effic.pdf:/Users/bert/Zotero/storage/UUTJE3AH/Ali et al. - 2022 - Predictive coding is a consequence of energy effic.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/G9H552GG/S2666389922002719.html:text/html},
}

@article{friston_action_2011,
	title = {Action understanding and active inference},
	volume = {104},
	issn = {1432-0770},
	doi = {10.1007/s00422-011-0424-z},
	abstract = {We have suggested that the mirror-neuron system might be usefully understood as implementing Bayes-optimal perception of actions emitted by oneself or others. To substantiate this claim, we present neuronal simulations that show the same representations can prescribe motor behavior and encode motor intentions during action-observation. These simulations are based on the free-energy formulation of active inference, which is formally related to predictive coding. In this scheme, (generalised) states of the world are represented as trajectories. When these states include motor trajectories they implicitly entail intentions (future motor states). Optimizing the representation of these intentions enables predictive coding in a prospective sense. Crucially, the same generative models used to make predictions can be deployed to predict the actions of self or others by simply changing the bias or precision (i.e. attention) afforded to proprioceptive signals. We illustrate these points using simulations of handwriting to illustrate neuronally plausible generation and recognition of itinerant (wandering) motor trajectories. We then use the same simulations to produce synthetic electrophysiological responses to violations of intentional expectations. Our results affirm that a Bayes-optimal approach provides a principled framework, which accommodates current thinking about the mirror-neuron system. Furthermore, it endorses the general formulation of action as active inference.},
	pages = {137--160},
	number = {1},
	journaltitle = {Biological Cybernetics},
	shortjournal = {Biol Cybern},
	author = {Friston, Karl and Mattout, Jérémie and Kilner, James},
	date = {2011-02},
	pmid = {21327826},
	pmcid = {PMC3491875},
	keywords = {Humans, Models, Neurological, Perception, Cybernetics, Bayes Theorem, Psychomotor Performance, Sensation, Nonlinear Dynamics, Handwriting, Motor Neurons},
	file = {Friston et al. - 2011 - Action understanding and active inference.pdf:/Users/bert/Zotero/storage/T45RVLP6/Friston et al. - 2011 - Action understanding and active inference.pdf:application/pdf},
}

@article{yin_efficient_nodate,
	title = {Efficient and Accurate Spiking Neural Networks},
	pages = {158},
	author = {Yin, Bojian},
	langid = {english},
	file = {Yin - Efficient and Accurate Spiking Neural Networks.pdf:/Users/bert/Zotero/storage/C35WETZJ/Yin - Efficient and Accurate Spiking Neural Networks.pdf:application/pdf},
}

@article{parr_computational_2021,
	title = {The computational neurology of movement under active inference},
	volume = {144},
	issn = {0006-8950},
	url = {https://doi.org/10.1093/brain/awab085},
	doi = {10.1093/brain/awab085},
	abstract = {We propose a computational neurology of movement based on the convergence of theoretical neurobiology and clinical neurology. A significant development in the former is the idea that we can frame brain function as a process of (active) inference, in which the nervous system makes predictions about its sensory data. These predictions depend upon an implicit predictive (generative) model used by the brain. This means neural dynamics can be framed as generating actions to ensure sensations are consistent with these predictions—and adjusting predictions when they are not. We illustrate the significance of this formulation for clinical neurology by simulating a clinical examination of the motor system using an upper limb coordination task. Specifically, we show how tendon reflexes emerge naturally under the right kind of generative model. Through simulated perturbations, pertaining to prior probabilities of this model’s variables, we illustrate the emergence of hyperreflexia and pendular reflexes, reminiscent of neurological lesions in the corticospinal tract and cerebellum. We then turn to the computational lesions causing hypokinesia and deficits of coordination. This in silico lesion-deficit analysis provides an opportunity to revisit classic neurological dichotomies (e.g. pyramidal versus extrapyramidal systems) from the perspective of modern approaches to theoretical neurobiology—and our understanding of the neurocomputational architecture of movement control based on first principles.},
	pages = {1799--1818},
	number = {6},
	journaltitle = {Brain},
	shortjournal = {Brain},
	author = {Parr, Thomas and Limanowski, Jakub and Rawji, Vishal and Friston, Karl},
	urldate = {2022-12-03},
	date = {2021-06-01},
	file = {Parr et al. - 2021 - The computational neurology of movement under acti.pdf:/Users/bert/Zotero/storage/AQ4SJDIQ/Parr et al. - 2021 - The computational neurology of movement under acti.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/UK7G9L4C/6168144.html:text/html},
}

@article{verduzco-flores_self-configuring_2022,
	title = {Self-configuring feedback loops for sensorimotor control},
	volume = {11},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.77216},
	doi = {10.7554/eLife.77216},
	abstract = {How dynamic interactions between nervous system regions in mammals performs online motor control remains an unsolved problem. In this paper, we show that feedback control is a simple, yet powerful way to understand the neural dynamics of sensorimotor control. We make our case using a minimal model comprising spinal cord, sensory and motor cortex, coupled by long connections that are plastic. It succeeds in learning how to perform reaching movements of a planar arm with 6 muscles in several directions from scratch. The model satisfies biological plausibility constraints, like neural implementation, transmission delays, local synaptic learning and continuous online learning. Using differential Hebbian plasticity the model can go from motor babbling to reaching arbitrary targets in less than 10 min of in silico time. Moreover, independently of the learning mechanism, properly configured feedback control has many emergent properties: neural populations in motor cortex show directional tuning and oscillatory dynamics, the spinal cord creates convergent force fields that add linearly, and movements are ataxic (as in a motor system without a cerebellum).},
	pages = {e77216},
	journaltitle = {{eLife}},
	author = {Verduzco-Flores, Sergio Oscar and De Schutter, Erik},
	editor = {Gallego, Juan Álvaro and Makin, Tamar R and Capogrosso, Marco and Berg, Rune W},
	urldate = {2022-12-03},
	date = {2022-11-14},
	note = {Publisher: {eLife} Sciences Publications, Ltd},
	keywords = {directional tuning, motor control, motor cortex, spinal cord, synaptic plasticity, synergy},
	file = {Verduzco-Flores and De Schutter - 2022 - Self-configuring feedback loops for sensorimotor c.pdf:/Users/bert/Zotero/storage/B8K9U4G9/Verduzco-Flores and De Schutter - 2022 - Self-configuring feedback loops for sensorimotor c.pdf:application/pdf},
}

@misc{friston_designing_2022,
	title = {Designing Ecosystems of Intelligence from First Principles},
	url = {http://arxiv.org/abs/2212.01354},
	doi = {10.48550/arXiv.2212.01354},
	abstract = {This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants\${\textbackslash}unicode\{x2014\}\$what we call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. In this context, we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world\${\textbackslash}unicode\{x2014\}\$also known as self-evidencing. Formally, this corresponds to maximizing (Bayesian) model evidence, via belief updating over several scales: i.e., inference, learning, and model selection. Operationally, this self-evidencing can be realized via (variational) message passing or belief propagation on a factor graph. Crucially, active inference foregrounds an existential imperative of intelligent systems; namely, curiosity or the resolution of uncertainty. This same imperative underwrites belief sharing in ensembles of agents, in which certain aspects (i.e., factors) of each agent's generative world model provide a common ground or frame of reference. Active inference plays a foundational role in this ecology of belief sharing\${\textbackslash}unicode\{x2014\}\$leading to a formal account of collective intelligence that rests on shared narratives and goals. We also consider the kinds of communication protocols that must be developed to enable such an ecosystem of intelligences and motivate the development of a shared hyper-spatial modeling language and transaction protocol, as a first\${\textbackslash}unicode\{x2014\}\$and key\${\textbackslash}unicode\{x2014\}\$step towards such an ecology.},
	number = {{arXiv}:2212.01354},
	publisher = {{arXiv}},
	author = {Friston, Karl J. and Ramstead, Maxwell J. D. and Kiefer, Alex B. and Tschantz, Alexander and Buckley, Christopher L. and Albarracin, Mahault and Pitliya, Riddhi J. and Heins, Conor and Klein, Brennan and Millidge, Beren and Sakthivadivel, Dalton A. R. and Smithe, Toby St Clere and Koudahl, Magnus and Tremblay, Safae Essafi and Petersen, Capm and Fung, Kaiser and Fox, Jason G. and Swanson, Steven and Mapes, Dan and René, Gabriel},
	urldate = {2022-12-08},
	date = {2022-12-02},
	eprinttype = {arxiv},
	eprint = {2212.01354 [nlin]},
	keywords = {Computer Science - Artificial Intelligence, Nonlinear Sciences - Adaptation and Self-Organizing Systems},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/4D9QP5EX/2212.html:text/html;Friston et al. - 2022 - Designing Ecosystems of Intelligence from First Pr.pdf:/Users/bert/Zotero/storage/M97Q353E/Friston et al. - 2022 - Designing Ecosystems of Intelligence from First Pr.pdf:application/pdf},
}

@misc{ciria_predictive_2021,
	title = {Predictive Processing in Cognitive Robotics: a Review},
	url = {http://arxiv.org/abs/2101.06611},
	shorttitle = {Predictive Processing in Cognitive Robotics},
	abstract = {Predictive processing has become an influential framework in cognitive sciences. This framework turns the traditional view of perception upside down, claiming that the main flow of information processing is realized in a top-down hierarchical manner. Furthermore, it aims at unifying perception, cognition, and action as a single inferential process. However, in the related literature, the predictive processing framework and its associated schemes such as predictive coding, active inference, perceptual inference, free-energy principle, tend to be used interchangeably. In the field of cognitive robotics there is no clear-cut distinction on which schemes have been implemented and under which assumptions. In this paper, working definitions are set with the main aim of analyzing the state of the art in cognitive robotics research working under the predictive processing framework as well as some related non-robotic models. The analysis suggests that, first, both research in cognitive robotics implementations and non-robotic models needs to be extended to the study of how multiple exteroceptive modalities can be integrated into prediction error minimization schemes. Second, a relevant distinction found here is that cognitive robotics implementations tend to emphasize the learning of a generative model, while in non-robotics models it is almost absent. Third, despite the relevance for active inference, few cognitive robotics implementations examine the issues around control and whether it should result from the substitution of inverse models with proprioceptive predictions. Finally, limited attention has been placed on precision weighting and the tracking of prediction error dynamics. These mechanisms should help to explore more complex behaviors and tasks in cognitive robotics research under the predictive processing framework.},
	number = {{arXiv}:2101.06611},
	publisher = {{arXiv}},
	author = {Ciria, Alejandra and Schillaci, Guido and Pezzulo, Giovanni and Hafner, Verena V. and Lara, Bruno},
	urldate = {2022-12-09},
	date = {2021-01-22},
	eprinttype = {arxiv},
	eprint = {2101.06611 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/B7CEWMXT/2101.html:text/html;Ciria et al. - 2021 - Predictive Processing in Cognitive Robotics a Rev.pdf:/Users/bert/Zotero/storage/NJFRA7TQ/Ciria et al. - 2021 - Predictive Processing in Cognitive Robotics a Rev.pdf:application/pdf},
}

@article{pezzulo_active_2015,
	title = {Active Inference, homeostatic regulation and adaptive behavioural control},
	volume = {134},
	issn = {0301-0082},
	url = {https://www.sciencedirect.com/science/article/pii/S0301008215000908},
	doi = {10.1016/j.pneurobio.2015.09.001},
	abstract = {We review a theory of homeostatic regulation and adaptive behavioural control within the Active Inference framework. Our aim is to connect two research streams that are usually considered independently; namely, Active Inference and associative learning theories of animal behaviour. The former uses a probabilistic (Bayesian) formulation of perception and action, while the latter calls on multiple (Pavlovian, habitual, goal-directed) processes for homeostatic and behavioural control. We offer a synthesis these classical processes and cast them as successive hierarchical contextualisations of sensorimotor constructs, using the generative models that underpin Active Inference. This dissolves any apparent mechanistic distinction between the optimization processes that mediate classical control or learning. Furthermore, we generalize the scope of Active Inference by emphasizing interoceptive inference and homeostatic regulation. The ensuing homeostatic (or allostatic) perspective provides an intuitive explanation for how priors act as drives or goals to enslave action, and emphasises the embodied nature of inference.},
	pages = {17--35},
	journaltitle = {Progress in Neurobiology},
	shortjournal = {Progress in Neurobiology},
	author = {Pezzulo, Giovanni and Rigoli, Francesco and Friston, Karl},
	urldate = {2022-12-17},
	date = {2015-11-01},
	langid = {english},
	keywords = {Active Inference, Adaptive control, Homeostatic regulation, Model-based control, Model-free control, Pavlovian control},
	file = {Pezzulo et al. - 2015 - Active Inference, homeostatic regulation and adapt.pdf:/Users/bert/Zotero/storage/3V4GUYTJ/Pezzulo et al. - 2015 - Active Inference, homeostatic regulation and adapt.pdf:application/pdf;ScienceDirect Snapshot:/Users/bert/Zotero/storage/WZA37CNU/S0301008215000908.html:text/html},
}

@article{pezzulo_hierarchical_2018,
	title = {Hierarchical Active Inference: A Theory of Motivated Control},
	volume = {22},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(18)30022-6},
	doi = {10.1016/j.tics.2018.01.009},
	shorttitle = {Hierarchical Active Inference},
	pages = {294--306},
	number = {4},
	journaltitle = {Trends in Cognitive Sciences},
	shortjournal = {Trends in Cognitive Sciences},
	author = {Pezzulo, Giovanni and Rigoli, Francesco and Friston, Karl J.},
	urldate = {2022-12-17},
	date = {2018-04-01},
	pmid = {29475638},
	note = {Publisher: Elsevier},
	keywords = {active inference, cognitive control, executive function, goal-directed decision making, hierarchical inference, motivated control},
	file = {Pezzulo et al. - 2018 - Hierarchical Active Inference A Theory of Motivat.pdf:/Users/bert/Zotero/storage/5ZDSV76B/Pezzulo et al. - 2018 - Hierarchical Active Inference A Theory of Motivat.pdf:application/pdf},
}

@misc{ahilan_succinct_2023,
	title = {A Succinct Summary of Reinforcement Learning},
	url = {http://arxiv.org/abs/2301.01379},
	doi = {10.48550/arXiv.2301.01379},
	abstract = {This document is a concise summary of many key results in single-agent reinforcement learning ({RL}). The intended audience are those who already have some familiarity with {RL} and are looking to review, reference and/or remind themselves of important ideas in the field.},
	number = {{arXiv}:2301.01379},
	publisher = {{arXiv}},
	author = {Ahilan, Sanjeevan},
	urldate = {2023-01-05},
	date = {2023-01-03},
	eprinttype = {arxiv},
	eprint = {2301.01379 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Ahilan - 2023 - A Succinct Summary of Reinforcement Learning.pdf:/Users/bert/Zotero/storage/QBGZUBF8/Ahilan - 2023 - A Succinct Summary of Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/BEQXB6DS/2301.html:text/html},
}

@misc{novicky_bistable_2022,
	title = {Bistable perception, precision and neuromodulation},
	url = {http://arxiv.org/abs/2212.09729},
	abstract = {Bistable perception follows from observing a static, ambiguous, (visual) stimulus with two possible interpretations. Here, we present an active (Bayesian) inference account of bistable perception and posit that perceptual transitions between different interpretations (i.e., inferences) of the same stimulus ensue from specific eye movements that shift the focus to a different visual feature. Formally, these inferences are a consequence of precision control that determines how confident beliefs are and change the frequency with which one can perceive - and alternate between - two distinct percepts. We hypothesised that there are multiple, but distinct, ways in which precision modulation can interact to give rise to a similar frequency of bistable perception. We validated this using numerical simulations of the Necker's cube paradigm and demonstrate the multiple routes that underwrite the frequency of perceptual alternation. Our results provide an (enactive) computational account of the intricate precision balance underwriting bistable perception. Importantly, these precision parameters can be considered the computational homologues of particular neurotransmitters - i.e., acetylcholine, noradrenaline, dopamine - that have been previously implicated in controlling bistable perception, providing a computational link between the neurochemistry and perception.},
	number = {{arXiv}:2212.09729},
	publisher = {{arXiv}},
	author = {Novicky, Filip and Parr, Thomas and Friston, Karl and Mirza, M. Berk and Sajid, Noor},
	urldate = {2022-12-23},
	date = {2022-12-19},
	eprinttype = {arxiv},
	eprint = {2212.09729 [q-bio]},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/6ZTQRLT5/2212.html:text/html;Novicky et al. - 2022 - Bistable perception, precision and neuromodulation.pdf:/Users/bert/Zotero/storage/729FQLC5/Novicky et al. - 2022 - Bistable perception, precision and neuromodulation.pdf:application/pdf},
}

@online{noauthor_reinforcement_nodate,
	title = {Reinforcement Learning or Active Inference? {\textbar} {PLOS} {ONE}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006421},
	urldate = {2023-01-12},
	file = {Reinforcement Learning or Active Inference? | PLOS ONE:/Users/bert/Zotero/storage/WESKXB2P/article.html:text/html},
}

@article{rutar_structure_2022,
	title = {Structure Learning in Predictive Processing Needs Revision},
	volume = {5},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-022-00131-8},
	doi = {10.1007/s42113-022-00131-8},
	abstract = {The predictive processing account aspires to explain all of cognition using a single, unifying principle. Among the major challenges is to explain how brains are able to infer the structure of their generative models. Recent attempts to further this goal build on existing ideas and techniques from engineering fields, like Bayesian statistics and machine learning. While apparently promising, these approaches make specious assumptions that effectively confuse structure learning with Bayesian parameter estimation in a fixed state space. We illustrate how this leads to a set of theoretical problems for the predictive processing account. These problems highlight a need for developing new formalisms specifically tailored to the theoretical aims of scientific explanation. We lay the groundwork for a possible way forward.},
	pages = {234--243},
	number = {2},
	journaltitle = {Computational Brain \& Behavior},
	shortjournal = {Comput Brain Behav},
	author = {Rutar, Danaja and de Wolff, Erwin and van Rooij, Iris and Kwisthout, Johan},
	urldate = {2023-01-30},
	date = {2022-06-01},
	langid = {english},
	file = {Rutar et al. - 2022 - Structure Learning in Predictive Processing Needs .pdf:/Users/bert/Zotero/storage/LRCDMP36/Rutar et al. - 2022 - Structure Learning in Predictive Processing Needs .pdf:application/pdf},
}

@misc{sakthivadivel_towards_2022,
	title = {Towards a Geometry and Analysis for Bayesian Mechanics},
	url = {http://arxiv.org/abs/2204.11900},
	doi = {10.48550/arXiv.2204.11900},
	abstract = {In this paper, a simple case of Bayesian mechanics under the free energy principle is formulated in axiomatic terms. We argue that any dynamical system with constraints on its dynamics necessarily looks as though it is performing inference against these constraints, and that in a non-isolated system, such constraints imply external environmental variables embedding the system. Using aspects of classical dynamical systems theory in statistical mechanics, we show that this inference is equivalent to a gradient ascent on the Shannon entropy functional, recovering an approximate Bayesian inference under a locally ergodic probability measure on the state space. We also use some geometric notions from dynamical systems theory\${\textbackslash}unicode\{x2014\}\$namely, that the constraints constitute a gauge degree of freedom\${\textbackslash}unicode\{x2014\}\$to elaborate on how the desire to stay self-organised can be read as a gauge force acting on the system. In doing so, a number of results of independent interest are given. Overall, we provide a related, but alternative, formalism to those driven purely by descriptions of random dynamical systems, and take a further step towards a comprehensive statement of the physics of self-organisation in formal mathematical language.},
	number = {{arXiv}:2204.11900},
	publisher = {{arXiv}},
	author = {Sakthivadivel, Dalton A. R.},
	urldate = {2023-02-03},
	date = {2022-04-25},
	eprinttype = {arxiv},
	eprint = {2204.11900 [cond-mat, physics:math-ph, physics:nlin, physics:physics]},
	keywords = {Mathematics - Dynamical Systems, Mathematical Physics, Nonlinear Sciences - Adaptation and Self-Organizing Systems, Condensed Matter - Statistical Mechanics, Physics - Biological Physics, Primary 37K58, 37L40, 51P05, Secondary 46N55, 60K35, 70S15},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/VG5EA25X/2204.html:text/html;Sakthivadivel - 2022 - Towards a Geometry and Analysis for Bayesian Mecha.pdf:/Users/bert/Zotero/storage/GWFTL8EK/Sakthivadivel - 2022 - Towards a Geometry and Analysis for Bayesian Mecha.pdf:application/pdf},
}

@article{pizlo_unifying_2019,
	title = {Unifying Physics and Psychophysics on the Basis of Symmetry, Least-Action ≈ Simplicity Principle, and Conservation Laws ≈ Veridicality},
	volume = {132},
	issn = {0002-9556},
	url = {https://scholarlypublishingcollective.org/uip/ajp/article/132/1/1/257542/Unifying-Physics-and-Psychophysics-on-the-Basis-of},
	doi = {10.5406/amerjpsyc.132.1.0001},
	abstract = {Abstract. Psychophysics is the branch of experimental psychology that deals with the study of sensation and perception. A consensus has grown up among experts in psychophysics in the last hundred years that the human being’s percepts are inferences, which are based on a minimum, or simplicity, principle that is applied to the currently available sensory data. These educated guesses play the critical role in establishing veridical perceptual representations of the three-dimensional environment, where by “veridical” we mean that the percept agrees with what is “out there.” These veridical representations cannot be achieved without making use of symmetries, much like those known in physics, where they are essential for characterizing our physical world and deriving the conservation laws. But, unlike in physics, the important role that symmetry plays in psychophysics has been demonstrated and explained only within the last 10 years. Symmetries represent regularities in our physical world. These symmetries also serve as the source of the redundancies that are inherent in 3D objects and make vision possible. The main goal of this article is to show that the similarity between the mathematical formalisms used in physics and in psychophysics is not coincidental and that exploring this similarity can benefit the sciences of perception and cognition. This article includes a brief tutorial about symmetry groups and their relationship to transformation groups as well as to their invariants. It was included to make this material available to readers who are not familiar with these topics.},
	pages = {1--25},
	number = {1},
	journaltitle = {The American Journal of Psychology},
	author = {Pizlo, Zygmunt},
	urldate = {2023-02-08},
	date = {2019-01-01},
	langid = {english},
	note = {Publisher: Duke University Press},
	file = {Pizlo - 2019 - Unifying Physics and Psychophysics on the Basis of.pdf:/Users/bert/Zotero/storage/TXZ7WU8X/Pizlo - 2019 - Unifying Physics and Psychophysics on the Basis of.pdf:application/pdf},
}

@inproceedings{deneve_bayesian_2004,
	title = {Bayesian inference in spiking neurons},
	volume = {17},
	url = {https://proceedings.neurips.cc/paper/2004/hash/cdd96eedd7f695f4d61802f8105ba2b0-Abstract.html},
	abstract = {We propose a new interpretation of spiking neurons as Bayesian integra-          tors accumulating evidence over time about events in the external world          or the body, and communicating to other neurons their certainties about          these events. In this model, spikes signal the occurrence of new infor-          mation, i.e. what cannot be predicted from the past activity. As a result,          firing statistics are close to Poisson, albeit providing a deterministic rep-          resentation of probabilities. We proceed to develop a theory of Bayesian          inference in spiking neural networks, recurrent interactions implement-          ing a variant of belief propagation.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {{MIT} Press},
	author = {Deneve, Sophie},
	urldate = {2023-02-04},
	date = {2004},
	file = {Full Text PDF:/Users/bert/Zotero/storage/ZMDR7SYV/Deneve - 2004 - Bayesian inference in spiking neurons.pdf:application/pdf},
}

@article{hanc_conservation_2004,
	title = {From conservation of energy to the principle of least action: A story line},
	volume = {72},
	issn = {0002-9505},
	url = {https://aapt.scitation.org/doi/abs/10.1119/1.1645282},
	doi = {10.1119/1.1645282},
	shorttitle = {From conservation of energy to the principle of least action},
	pages = {514--521},
	number = {4},
	journaltitle = {American Journal of Physics},
	author = {Hanc, Jozef and Taylor, Edwin F.},
	urldate = {2023-02-08},
	date = {2004-04},
	note = {Publisher: American Association of Physics Teachers},
	file = {Hanc and Taylor - 2004 - From conservation of energy to the principle of le.pdf:/Users/bert/Zotero/storage/JKHCZ38V/Hanc and Taylor - 2004 - From conservation of energy to the principle of le.pdf:application/pdf},
}

@article{de_sapio_least_2008,
	title = {Least action principles and their application to constrained and task-level problems in robotics and biomechanics},
	volume = {19},
	issn = {1573-272X},
	url = {https://doi.org/10.1007/s11044-007-9097-8},
	doi = {10.1007/s11044-007-9097-8},
	abstract = {Least action principles provide an insightful starting point from which problems involving constraints and task-level objectives can be addressed. In this paper, the principle of least action is first treated with regard to holonomic constraints in multibody systems. A variant of this, the principle of least curvature or straightest path, is then investigated in the context of geodesic paths on constrained motion manifolds. Subsequently, task space descriptions are addressed and the operational space approach is interpreted in terms of least action. Task-level control is then applied to the problem of cost minimization. Finally, task-level optimization is formulated with respect to extremizing an objective criterion, where the criterion is interpreted as the action of the system. Examples are presented which illustrate these approaches.},
	pages = {303--322},
	number = {3},
	journaltitle = {Multibody System Dynamics},
	shortjournal = {Multibody Syst Dyn},
	author = {De Sapio, Vincent and Khatib, Oussama and Delp, Scott},
	urldate = {2023-02-08},
	date = {2008-04-01},
	langid = {english},
	file = {De Sapio et al. - 2008 - Least action principles and their application to c.pdf:/Users/bert/Zotero/storage/TT57KBHZ/De Sapio et al. - 2008 - Least action principles and their application to c.pdf:application/pdf},
}

@article{pizlo_concept_2021,
	title = {The Concept of Symmetry and the Theory of Perception},
	volume = {15},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2021.681162},
	abstract = {Perceptual constancy refers to the fact that the perceived geometrical and physical characteristics of objects remain constant despite transformations of the objects such as rigid motion. Perceptual constancy is essential in everything we do, like recognition of familiar objects and scenes, planning and executing visual navigation, visuomotor coordination, and many more. Perceptual constancy would not exist without the geometrical and physical permanence of objects: their shape, size, and weight. Formally, perceptual constancy and permanence of objects are invariants, also known in mathematics and physics as symmetries. Symmetries of the Laws of Physics received a central status due to mathematical theorems of Emmy Noether formulated and proved over 100 years ago. These theorems connected symmetries of the physical laws to conservation laws through the least-action principle. We show how Noether's theorem is applied to mirror-symmetrical objects and establishes mental shape representation (perceptual conservation) through the application of a simplicity (least-action) principle. This way, the formalism of Noether's theorem provides a computational explanation of the relation between the physical world and its mental representation.},
	journaltitle = {Frontiers in Computational Neuroscience},
	author = {Pizlo, Zygmunt and de Barros, J. Acacio},
	urldate = {2023-02-09},
	date = {2021},
	file = {Pizlo and de Barros - 2021 - The Concept of Symmetry and the Theory of Percepti.pdf:/Users/bert/Zotero/storage/IBUN5PVN/Pizlo and de Barros - 2021 - The Concept of Symmetry and the Theory of Percepti.pdf:application/pdf},
}

@inproceedings{poljak_review_2022,
	title = {Review of Least Action Principle in Electromagnetics: Part {II}: Derivation of Maxwell's Equations},
	doi = {10.23919/SoftCOM55329.2022.9911284},
	shorttitle = {Review of Least Action Principle in Electromagnetics},
	abstract = {The 2nd paper in three-part study deals with a derivation of Maxwell's equations by using Hamilton's principle in electromagnetics and Noether's theorem for fields. Kinematical Maxwell's equations are derived from gauge symmetry, while two dynamical Maxwell's equations are derived by minimizing the functional of electromagnetic energy. The corresponding Lagrangian is given as difference between energy stored in the magnetic and electric field respectively.},
	eventtitle = {2022 International Conference on Software, Telecommunications and Computer Networks ({SoftCOM})},
	pages = {1--5},
	booktitle = {2022 International Conference on Software, Telecommunications and Computer Networks ({SoftCOM})},
	author = {Poljak, Dragan},
	date = {2022-09},
	note = {{ISSN}: 1847-358X},
	keywords = {Software, Telecommunications, Electromagnetics, Computer networks, Electric fields, Electromagnetic action Maxwell's equations, Hamilton's principle, Maxwell equations, Noether's theorem},
	file = {IEEE Xplore Abstract Record:/Users/bert/Zotero/storage/2WVQ6D8G/9911284.html:text/html;Poljak - 2022 - Review of Least Action Principle in Electromagneti.pdf:/Users/bert/Zotero/storage/JYRKBK4S/Poljak - 2022 - Review of Least Action Principle in Electromagneti.pdf:application/pdf},
}

@misc{costa_calculus_2023,
	title = {Calculus of Variations: Nature's Parsimonious Self-Computation},
	url = {https://hal.science/hal-03929775},
	shorttitle = {Calculus of Variations},
	abstract = {In addition to being intrinsically interesting, calculus of variations constitutes an area that has been extensively considered in both theoretic and applied areas, underlying a large number of structures and phenomena, including the stationary action principle in physics. Basically, given a functional expressing some measurement to be optimized (minimized or maximized), as well as respective boundary conditions, calculus of variations allows the determination of the respective optimal solution, be it a function, a curve, surface, as well as a large range of mathematical structures. The present work aims at developing an introduction to variational calculus which could be hopefully accessible to readers with some background in multivariate calculus. After defining the problem, and presenting some brief historic remarks, we proceed to present a motivational example of calculus of variations respective to a specific type of variation function, namely second-order polynomials adhering to the boundary conditions. In addition to illustrating several concepts related to calculus of variations, this approach also allowed the visualization of the aimed functional behavior as a function of the variation parameter. Next, the Euler-Lagrange equation for functions of one variable was informally derived, and then illustrates its application respectively to the minimum arc-length and soap surface minimization. The interesting possibility of numerical approaches to calculus of variations is also briefly described and illustrated respectively to Euler's direct method. Appendices providing a brief presentation of the chain rule, integration by parts, multivariate derivatives, as well as the multivariate, multiconstraint Lagrange multipliers method are also provided for quick reference.},
	author = {Costa, Luciano da Fontoura},
	urldate = {2023-02-09},
	date = {2023-01-09},
	langid = {english},
	file = {Costa - 2023 - Calculus of Variations Nature's Parsimonious Self.pdf:/Users/bert/Zotero/storage/V3SFJEFI/Costa - 2023 - Calculus of Variations Nature's Parsimonious Self.pdf:application/pdf},
}

@misc{summers_entropic_2023,
	title = {Entropic Dynamics in a Theoretical Framework for Biosystems},
	url = {https://www.preprints.org/manuscript/202302.0389/v1},
	doi = {10.20944/preprints202302.0389.v1},
	abstract = {Central to an understanding of the physical nature of biosystems is an apprehension of their ability to control entropy dynamics in their environment.  To achieve ongoing stability and survival, living systems must adaptively respond to incoming information signals concerning matter and energy perturbations in their biological continuum (biocontinuum).  Entropy dynamics for the living system are then determined by the natural drive for reconciliation of these information divergences in the context of the constraints formed by the geometry of the biocontinuum information space.  The configuration of this information geometry is determined by the inherent biological structure, processes and adaptive controls that are necessary for the stable functioning of the organism.  The trajectory of this adaptive reconciliation process can be described by an information-theoretic formulation of the living system’s procedure for actionable knowledge acquisition that incorporates the axiomatic inference of the Kullback Principle of Minimum Information Discrimination (a derivative of Jaynes’ principle of maximal entropy). Utilizing relative information for entropic inference provides for the incorporation of a background of the adaptive constraints in biosystems within the operations of Fisher biologic replicator dynamics.  This mathematical expression for entropic dynamics within the biocontinuum may then serve as a theoretical framework for the general analysis of biological phenomena.},
	number = {2023020389},
	publisher = {Preprints},
	author = {Summers, Richard L.},
	urldate = {2023-02-25},
	date = {2023-02-22},
	langid = {english},
	keywords = {information geometry, entropic dynamics, biocontinuum, biosystems, Kullback Principle of Minimum Information Discrimination},
	file = {Summers - 2023 - Entropic Dynamics in a Theoretical Framework for B.pdf:/Users/bert/Zotero/storage/DWNI6C7L/Summers - 2023 - Entropic Dynamics in a Theoretical Framework for B.pdf:application/pdf},
}

@online{mann_free_2021,
	title = {Free Energy: A User's Guide},
	rights = {cc\_by\_4},
	url = {http://philsci-archive.pitt.edu/19961/},
	shorttitle = {Free Energy},
	abstract = {Over the last fifteen years, an ambitious explanatory framework has been proposed to unify explanations across biology and cognitive science. Active inference, whose most famous tenet is the free energy principle, has inspired excitement and confusion in equal measure. Here, we lay the ground for proper critical analysis of active inference, in three ways. First, we give simplified versions of its core mathematical models. Second, we outline the historical development of active inference and its relationship to other theoretical approaches. Third, we describe three different kinds of claim -- labelled mathematical, empirical and general -- routinely made by proponents of the framework, and suggest dialectical links between them. Overall, we aim to increase philosophical understanding of active inference so that it may be more readily evaluated.

This is a manuscript draft of the Introduction to the Topical Collection "The Free Energy Principle: From Biology to Cognition", forthcoming in Biology \& Philosophy.},
	type = {Preprint},
	author = {Mann, Stephen Francis and Pain, Ross and Kirchhoff, Michael},
	urldate = {2023-03-15},
	date = {2021-12-06},
	langid = {english},
	file = {Mann et al. - 2021 - Free Energy A User's Guide.pdf:/Users/bert/Zotero/storage/IM6AXVDN/Mann et al. - 2021 - Free Energy A User's Guide.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NBPVATAU/19961.html:text/html},
}

@article{da_costa_reward_2023,
	title = {Reward Maximization Through Discrete Active Inference},
	issn = {1530-888X},
	doi = {10.1162/neco_a_01574},
	abstract = {Active inference is a probabilistic framework for modeling the behavior of biological and artificial agents, which derives from the principle of minimizing free energy. In recent years, this framework has been applied successfully to a variety of situations where the goal was to maximize reward, often offering comparable and sometimes superior performance to alternative approaches. In this article, we clarify the connection between reward maximization and active inference by demonstrating how and when active inference agents execute actions that are optimal for maximizing reward. Precisely, we show the conditions under which active inference produces the optimal solution to the Bellman equation, a formulation that underlies several approaches to model-based reinforcement learning and control. On partially observed Markov decision processes, the standard active inference scheme can produce Bellman optimal actions for planning horizons of 1 but not beyond. In contrast, a recently developed recursive active inference scheme (sophisticated inference) can produce Bellman optimal actions on any finite temporal horizon. We append the analysis with a discussion of the broader relationship between active inference and reinforcement learning.},
	pages = {1--46},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Comput},
	author = {Da Costa, Lancelot and Sajid, Noor and Parr, Thomas and Friston, Karl and Smith, Ryan},
	date = {2023-03-16},
	pmid = {36944240},
}

@article{isomura_cultured_2015-2,
	title = {Cultured Cortical Neurons Can Perform Blind Source Separation According to the Free-Energy Principle},
	volume = {11},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004643},
	doi = {10.1371/journal.pcbi.1004643},
	abstract = {Blind source separation is the computation underlying the cocktail party effect––a partygoer can distinguish a particular talker’s voice from the ambient noise. Early studies indicated that the brain might use blind source separation as a signal processing strategy for sensory perception and numerous mathematical models have been proposed; however, it remains unclear how the neural networks extract particular sources from a complex mixture of inputs. We discovered that neurons in cultures of dissociated rat cortical cells could learn to represent particular sources while filtering out other signals. Specifically, the distinct classes of neurons in the culture learned to respond to the distinct sources after repeating training stimulation. Moreover, the neural network structures changed to reduce free energy, as predicted by the free-energy principle, a candidate unified theory of learning and memory, and by Jaynes’ principle of maximum entropy. This implicit learning can only be explained by some form of Hebbian plasticity. These results are the first in vitro (as opposed to in silico) demonstration of neural networks performing blind source separation, and the first formal demonstration of neuronal self-organization under the free energy principle.},
	pages = {e1004643},
	number = {12},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Isomura, Takuya and Kotani, Kiyoshi and Jimbo, Yasuhiko},
	urldate = {2023-04-11},
	date = {2015-12-21},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Free energy, Neural networks, Neurons, Action potentials, Electrode recording, Functional electrical stimulation, Neuronal plasticity, Synaptic plasticity},
	file = {Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:/Users/bert/Zotero/storage/CA55I7QQ/Isomura et al. - 2015 - Cultured Cortical Neurons Can Perform Blind Source.pdf:application/pdf},
}

@online{noauthor_achievability_nodate,
	title = {On the Achievability of Blind Source Separation for High-Dimensional Nonlinear Source Mixtures {\textbar} Neural Computation {\textbar} {MIT} Press},
	url = {https://direct.mit.edu/neco/article/33/6/1433/98297/On-the-Achievability-of-Blind-Source-Separation},
	urldate = {2023-04-11},
	file = {On the Achievability of Blind Source Separation fo.pdf:/Users/bert/Zotero/storage/YXJPBYUP/On the Achievability of Blind Source Separation fo.pdf:application/pdf;On the Achievability of Blind Source Separation for High-Dimensional Nonlinear Source Mixtures | Neural Computation | MIT Press:/Users/bert/Zotero/storage/5B6JSKHY/On-the-Achievability-of-Blind-Source-Separation.html:text/html},
}

@article{neuenschwander_action_2006,
	title = {Action: Forcing Energy to Predict Motion},
	volume = {44},
	issn = {0031-921X},
	url = {https://aapt.scitation.org/doi/10.1119/1.2173320},
	doi = {10.1119/1.2173320},
	shorttitle = {Action},
	pages = {146--152},
	number = {3},
	journaltitle = {The Physics Teacher},
	author = {Neuenschwander, Dwight E. and Taylor, Edwin F. and Tuleja, Slavomir},
	urldate = {2023-04-18},
	date = {2006-03},
	note = {Publisher: American Association of Physics Teachers},
	file = {Neuenschwander et al. - 2006 - Action Forcing Energy to Predict Motion.pdf:/Users/bert/Zotero/storage/3IG28BA9/Neuenschwander et al. - 2006 - Action Forcing Energy to Predict Motion.pdf:application/pdf},
}

@article{hanc_variational_2005,
	title = {Variational mechanics in one and two dimensions},
	volume = {73},
	issn = {0002-9505},
	url = {https://aapt.scitation.org/doi/10.1119/1.1848516},
	doi = {10.1119/1.1848516},
	pages = {603--610},
	number = {7},
	journaltitle = {American Journal of Physics},
	author = {Hanc, Jozef and Taylor, Edwin F. and Tuleja, Slavomir},
	urldate = {2023-04-18},
	date = {2005-07},
	note = {Publisher: American Association of Physics Teachers},
	file = {Hanc et al. - 2005 - Variational mechanics in one and two dimensions.pdf:/Users/bert/Zotero/storage/WJ2ET5GF/Hanc et al. - 2005 - Variational mechanics in one and two dimensions.pdf:application/pdf},
}

@article{parr_generative_2023,
	title = {Generative models for sequential dynamics in active inference},
	issn = {1871-4099},
	url = {https://doi.org/10.1007/s11571-023-09963-x},
	doi = {10.1007/s11571-023-09963-x},
	abstract = {A central theme of theoretical neurobiology is that most of our cognitive operations require processing of discrete sequences of items. This processing in turn emerges from continuous neuronal dynamics. Notable examples are sequences of words during linguistic communication or sequences of locations during navigation. In this perspective, we address the problem of sequential brain processing from the perspective of active inference, which inherits from a Helmholtzian view of the predictive (Bayesian) brain. Underneath the active inference lies a generative model; namely, a probabilistic description of how (observable) consequences are generated by (unobservable) causes. We show that one can account for many aspects of sequential brain processing by assuming the brain entails a generative model of the sensed world that comprises central pattern generators, narratives, or well-defined sequences. We provide examples in the domains of motor control (e.g., handwriting), perception (e.g., birdsong recognition) through to planning and understanding (e.g., language). The solutions to these problems include the use of sequences of attracting points to direct complex movements—and the move from continuous representations of auditory speech signals to the discrete words that generate those signals.},
	journaltitle = {Cognitive Neurodynamics},
	shortjournal = {Cogn Neurodyn},
	author = {Parr, Thomas and Friston, Karl and Pezzulo, Giovanni},
	urldate = {2023-05-02},
	date = {2023-04-26},
	langid = {english},
	keywords = {Generative model, Active inference, Bayesian, Variational, Sequential dynamics},
	file = {Parr et al. - 2023 - Generative models for sequential dynamics in activ.pdf:/Users/bert/Zotero/storage/HXA6QIB9/Parr et al. - 2023 - Generative models for sequential dynamics in activ.pdf:application/pdf},
}

@misc{ramstead_inner_2023,
	title = {The inner screen model of consciousness: applying the free energy principle directly to the study of conscious experience},
	url = {https://psyarxiv.com/6afs3/},
	doi = {10.31234/osf.io/6afs3},
	shorttitle = {The inner screen model of consciousness},
	abstract = {This paper presents a model of consciousness that follows directly from the free- energy principle ({FEP}). We first rehearse the classical and quantum formulations of the {FEP}. In particular, we consider the “inner screen hypothesis” that follows from the quantum information theoretic version of the {FEP}. We then review applications of the {FEP} to the known sparse (nested and hierarchical) neuro-anatomy of the brain. We focus on the holographic structure of the brain, and how this structure supports (overt and covert) action.},
	publisher = {{PsyArXiv}},
	author = {Ramstead, Maxwell James and Albarracin, Mahault and Kiefer, Alex and Klein, Brennan and Fields, Chris and Friston, Karl and Safron, Adam},
	urldate = {2023-04-27},
	date = {2023-04-20},
	langid = {english},
	keywords = {Computational Neuroscience, Neuroscience, active inference, Free energy principle, consciousness, model, physics, attention schema theory, global workspace, unifying},
	file = {Ramstead et al. - 2023 - The inner screen model of consciousness applying .pdf:/Users/bert/Zotero/storage/DUDUCXTV/Ramstead et al. - 2023 - The inner screen model of consciousness applying .pdf:application/pdf},
}

@misc{weber_generalized_2023,
	title = {The generalized Hierarchical Gaussian Filter},
	url = {http://arxiv.org/abs/2305.10937},
	abstract = {Hierarchical Bayesian models of perception and learning feature prominently in contemporary cognitive neuroscience where, for example, they inform computational concepts of mental disorders. This includes predictive coding and hierarchical Gaussian filtering ({HGF}), which differ in the nature of hierarchical representations. Predictive coding assumes that higher levels in a given hierarchy influence the state (value) of lower levels. In {HGF}, however, higher levels determine the rate of change at lower levels. Here, we extend the space of generative models underlying {HGF} to include a form of nonlinear hierarchical coupling between state values akin to predictive coding and artificial neural networks in general. We derive the update equations corresponding to this generalization of {HGF} and conceptualize them as connecting a network of (belief) nodes where parent nodes either predict the state of child nodes or their rate of change. This enables us to (1) create modular architectures with generic computational steps in each node of the network, and (2) disclose the hierarchical message passing implied by generalized {HGF} models and to compare this to comparable schemes under predictive coding. We find that the algorithmic architecture instantiated by the generalized {HGF} is largely compatible with that of predictive coding but extends it with some unique predictions which arise from precision and volatility related computations. Our developments enable highly flexible implementations of hierarchical Bayesian models for empirical data analysis and are available as open source software.},
	number = {{arXiv}:2305.10937},
	publisher = {{arXiv}},
	author = {Weber, Lilian Aline and Waade, Peter Thestrup and Legrand, Nicolas and Møller, Anna Hedvig and Stephan, Klaas Enno and Mathys, Christoph},
	urldate = {2023-05-23},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.10937 [cs, q-bio]},
	keywords = {Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/bert/Zotero/storage/53NK778B/2305.html:text/html;Weber et al. - 2023 - The generalized Hierarchical Gaussian Filter.pdf:/Users/bert/Zotero/storage/BT9HJAU3/Weber et al. - 2023 - The generalized Hierarchical Gaussian Filter.pdf:application/pdf},
}

@article{akbayrak_extended_2021,
	title = {Extended Variational Message Passing for Automated Approximate Bayesian Inference},
	volume = {23},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/23/7/815},
	doi = {10.3390/e23070815},
	abstract = {Variational Message Passing ({VMP}) provides an automatable and efficient algorithmic framework for approximating Bayesian inference in factorized probabilistic models that consist of conjugate exponential family distributions. The automation of Bayesian inference tasks is very important since many data processing problems can be formulated as inference tasks on a generative probabilistic model. However, accurate generative models may also contain deterministic and possibly nonlinear variable mappings and non-conjugate factor pairs that complicate the automatic execution of the {VMP} algorithm. In this paper, we show that executing {VMP} in complex models relies on the ability to compute the expectations of the statistics of hidden variables. We extend the applicability of {VMP} by approximating the required expectation quantities in appropriate cases by importance sampling and Laplace approximation. As a result, the proposed Extended {VMP} ({EVMP}) approach supports automated efficient inference for a very wide range of probabilistic model specifications. We implemented {EVMP} in the Julia language in the probabilistic programming package {ForneyLab}.jl and show by a number of examples that {EVMP} renders an almost universal inference engine for factorized probabilistic models.},
	pages = {815},
	number = {7},
	journaltitle = {Entropy},
	author = {Akbayrak, Semih and Bocharov, Ivan and de Vries, Bert},
	urldate = {2023-05-26},
	date = {2021-07},
	langid = {english},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian inference, factor graphs, probabilistic programming, variational inference, variational message passing},
	file = {Akbayrak et al. - 2021 - Extended Variational Message Passing for Automated.pdf:/Users/bert/Zotero/storage/DPP3CLQC/Akbayrak et al. - 2021 - Extended Variational Message Passing for Automated.pdf:application/pdf},
}

@article{champion_realizing_2021,
	title = {Realizing Active Inference in Variational Message Passing: The Outcome-Blind Certainty Seeker},
	volume = {33},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco_a_01422},
	doi = {10.1162/neco_a_01422},
	shorttitle = {Realizing Active Inference in Variational Message Passing},
	abstract = {Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in {AI}. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and {AI} research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.},
	pages = {2762--2826},
	number = {10},
	journaltitle = {Neural Computation},
	shortjournal = {Neural Computation},
	author = {Champion, Théophile and Grześ, Marek and Bowman, Howard},
	urldate = {2023-05-26},
	date = {2021-09-16},
	file = {Champion et al. - 2021 - Realizing Active Inference in Variational Message .pdf:/Users/bert/Zotero/storage/NHUTRTHU/Champion et al. - 2021 - Realizing Active Inference in Variational Message .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/G342SF55/Realizing-Active-Inference-in-Variational-Message.html:text/html},
}

@misc{martin_abadi_tensorflow_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015},
}

@article{smirnova_organoid_2023,
	title = {Organoid intelligence ({OI}): the new frontier in biocomputing and intelligence-in-a-dish},
	journaltitle = {Frontiers in Science},
	author = {Smirnova, Lena and Caffo, Brian S and Gracias, David H and Huang, Qi and Morales Pantoja, Itzy E and Tang, Bohao and Zack, Donald J and Berlinicke, Cynthia A and Boyd, J Lomax and Harris, Timothy D and {others}},
	date = {2023},
	note = {Publisher: Frontiers},
}

@inreference{noauthor_distributive_2022,
	title = {Distributive property},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {https://en.wikipedia.org/w/index.php?title=Distributive_property&oldid=1124679546},
	abstract = {In mathematics, the distributive property of binary operations generalizes the distributive law, which asserts that the equality

is always true in elementary algebra.
For example, in elementary arithmetic, one has

One says that multiplication distributes over addition.
This basic property of numbers is part of the definition of most algebraic structures that have two operations called addition and multiplication, such as complex numbers, polynomials, matrices, rings, and fields. It is also encountered in Boolean algebra and mathematical logic, where each of the logical and (denoted 
  
    
      
        
        ∧
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash},{\textbackslash}land {\textbackslash},\}
  ) and the logical or (denoted 
  
    
      
        
        ∨
        
      
    
    \{{\textbackslash}displaystyle {\textbackslash},{\textbackslash}lor {\textbackslash},\}
  ) distributes over the other.},
	booktitle = {Wikipedia},
	urldate = {2023-05-26},
	date = {2022-11-29},
	langid = {english},
	note = {Page Version {ID}: 1124679546},
	file = {Snapshot:/Users/bert/Zotero/storage/LGFIXLJC/Distributive_property.html:text/html},
}

@misc{beckers_principled_2022,
	title = {Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization},
	url = {http://arxiv.org/abs/2210.09134},
	doi = {10.48550/arXiv.2210.09134},
	abstract = {Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. This novel parameter pruning scheme solves the shortcomings of many current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and minimizes the same objective that is used during training. Next to these theoretical benefits, our experiments indicate better model performance in comparison to state-of-the-art pruning schemes.},
	number = {{arXiv}:2210.09134},
	publisher = {{arXiv}},
	author = {Beckers, Jim and van Erp, Bart and Zhao, Ziyue and Kondrashov, Kirill and de Vries, Bert},
	urldate = {2023-05-26},
	date = {2022-10-17},
	eprinttype = {arxiv},
	eprint = {2210.09134 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
	file = {arXiv Fulltext PDF:/Users/bert/Zotero/storage/L74NKN63/Beckers et al. - 2022 - Principled Pruning of Bayesian Neural Networks thr.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/DVBAR68G/2210.html:text/html},
}

@misc{martin_abadi_tensorflow_2015-1,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	date = {2015},
}

@article{morales_pantoja_first_2023,
	title = {First Organoid Intelligence ({OI}) workshop to form an {OI} community},
	volume = {6},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/articles/10.3389/frai.2023.1116870},
	abstract = {The brain is arguably the most powerful computation system known. It is extremely efficient in processing large amounts of information and can discern signals from noise, adapt, and filter faulty information all while running on only 20 watts of power. The human brain's processing efficiency, progressive learning, and plasticity are unmatched by any computer system. Recent advances in stem cell technology have elevated the field of cell culture to higher levels of complexity, such as the development of three-dimensional (3D) brain organoids that recapitulate human brain functionality better than traditional monolayer cell systems. Organoid Intelligence ({OI}) aims to harness the innate biological capabilities of brain organoids for biocomputing and synthetic intelligence by interfacing them with computer technology. With the latest strides in stem cell technology, bioengineering, and machine learning, we can explore the ability of brain organoids to compute, and store given information (input), execute a task (output), and study how this affects the structural and functional connections in the organoids themselves. Furthermore, understanding how learning generates and changes patterns of connectivity in organoids can shed light on the early stages of cognition in the human brain. Investigating and understanding these concepts is an enormous, multidisciplinary endeavor that necessitates the engagement of both the scientific community and the public. Thus, on Feb 22–24 of 2022, the Johns Hopkins University held the first Organoid Intelligence Workshop to form an {OI} Community and to lay out the groundwork for the establishment of {OI} as a new scientific discipline. The potential of {OI} to revolutionize computing, neurological research, and drug development was discussed, along with a vision and roadmap for its development over the coming decade.},
	journaltitle = {Frontiers in Artificial Intelligence},
	author = {Morales Pantoja, Itzy E. and Smirnova, Lena and Muotri, Alysson R. and Wahlin, Karl J. and Kahn, Jeffrey and Boyd, J. Lomax and Gracias, David H. and Harris, Timothy D. and Cohen-Karni, Tzahi and Caffo, Brian S. and Szalay, Alexander S. and Han, Fang and Zack, Donald J. and Etienne-Cummings, Ralph and Akwaboah, Akwasi and Romero, July Carolina and Alam El Din, Dowlette-Mary and Plotkin, Jesse D. and Paulhamus, Barton L. and Johnson, Erik C. and Gilbert, Frederic and Curley, J. Lowry and Cappiello, Ben and Schwamborn, Jens C. and Hill, Eric J. and Roach, Paul and Tornero, Daniel and Krall, Caroline and Parri, Rheinallt and Sillé, Fenna and Levchenko, Andre and Jabbour, Rabih E. and Kagan, Brett J. and Berlinicke, Cynthia A. and Huang, Qi and Maertens, Alexandra and Herrmann, Kathrin and Tsaioun, Katya and Dastgheyb, Raha and Habela, Christa Whelan and Vogelstein, Joshua T. and Hartung, Thomas},
	urldate = {2023-05-26},
	date = {2023},
	file = {Morales Pantoja et al. - 2023 - First Organoid Intelligence (OI) workshop to form .pdf:/Users/bert/Zotero/storage/HWDPIYJZ/Morales Pantoja et al. - 2023 - First Organoid Intelligence (OI) workshop to form .pdf:application/pdf},
}
