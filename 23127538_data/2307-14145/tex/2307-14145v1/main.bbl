% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{akbayrak_extended_2021}{article}{}
      \name{author}{3}{}{%
        {{hash=2bc05fbad55c61f701f9eb0e5edc81ea}{%
           family={Akbayrak},
           familyi={A\bibinitperiod},
           given={Semih},
           giveni={S\bibinitperiod}}}%
        {{hash=fb016e1f0297982d392dd25ec008571d}{%
           family={Bocharov},
           familyi={B\bibinitperiod},
           given={Ivan},
           giveni={I\bibinitperiod}}}%
        {{hash=2caeea1b18c3a6d6099b66a5232eb983}{%
           family={Vries},
           familyi={V\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{b61099349ad201d345d553a1b3e5cc7d}
      \strng{fullhash}{b61099349ad201d345d553a1b3e5cc7d}
      \strng{bibnamehash}{b61099349ad201d345d553a1b3e5cc7d}
      \strng{authorbibnamehash}{b61099349ad201d345d553a1b3e5cc7d}
      \strng{authornamehash}{b61099349ad201d345d553a1b3e5cc7d}
      \strng{authorfullhash}{b61099349ad201d345d553a1b3e5cc7d}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Variational Message Passing ({VMP}) provides an automatable and efficient algorithmic framework for approximating Bayesian inference in factorized probabilistic models that consist of conjugate exponential family distributions. The automation of Bayesian inference tasks is very important since many data processing problems can be formulated as inference tasks on a generative probabilistic model. However, accurate generative models may also contain deterministic and possibly nonlinear variable mappings and non-conjugate factor pairs that complicate the automatic execution of the {VMP} algorithm. In this paper, we show that executing {VMP} in complex models relies on the ability to compute the expectations of the statistics of hidden variables. We extend the applicability of {VMP} by approximating the required expectation quantities in appropriate cases by importance sampling and Laplace approximation. As a result, the proposed Extended {VMP} ({EVMP}) approach supports automated efficient inference for a very wide range of probabilistic model specifications. We implemented {EVMP} in the Julia language in the probabilistic programming package {ForneyLab}.jl and show by a number of examples that {EVMP} renders an almost universal inference engine for factorized probabilistic models.}
      \field{issn}{1099-4300}
      \field{journaltitle}{Entropy}
      \field{langid}{english}
      \field{month}{7}
      \field{note}{Number: 7 Publisher: Multidisciplinary Digital Publishing Institute}
      \field{number}{7}
      \field{title}{Extended Variational Message Passing for Automated Approximate Bayesian Inference}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{23}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{815}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/e23070815
      \endverb
      \verb{file}
      \verb Akbayrak et al. - 2021 - Extended Variational Message Passing for Automated.pdf:/Users/bert/Zotero/storage/DPP3CLQC/Akbayrak et al. - 2021 - Extended Variational Message Passing for Automated.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.mdpi.com/1099-4300/23/7/815
      \endverb
      \verb{url}
      \verb https://www.mdpi.com/1099-4300/23/7/815
      \endverb
      \keyw{Bayesian inference,factor graphs,probabilistic programming,variational inference,variational message passing}
    \endentry
    \entry{bagaevReactiveMessagePassing2023}{article}{}
      \name{author}{2}{}{%
        {{hash=63366e44ffa7043ff4bd1398c7a44165}{%
           family={Bagaev},
           familyi={B\bibinitperiod},
           given={Dmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=2caeea1b18c3a6d6099b66a5232eb983}{%
           family={Vries},
           familyi={V\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \strng{fullhash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \strng{bibnamehash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \strng{authorbibnamehash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \strng{authornamehash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \strng{authorfullhash}{bec4896e4bdcafc0e730fadcbf0f4602}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce reactive message passing ({RMP}) as a framework for executing schedule-free, scalable, and, potentially, more robust message passing-based inference in a factor graph representation of a probabilistic model. {RMP} is based on the reactive programming style, which only describes how nodes in a factor graph react to changes in connected nodes. We recognize reactive programming as the suitable programming abstraction for message passing-based methods that improve robustness, scalability, and execution time of the inference procedure and are useful for all future implementations of message passing methods. We also present our own implementation {ReactiveMP}.jl, which is a Julia package for realizing {RMP} through minimization of a constrained Bethe free energy. By user-defined specification of local form and factorization constraints on the variational posterior distribution, {ReactiveMP}.jl executes hybrid message passing algorithms including belief propagation, variational message passing, expectation propagation, and expectation maximization update rules. Experimental results demonstrate the great performance of our {RMP} implementation compared to other Julia packages for Bayesian inference across a range of probabilistic models. In particular, we show that the {RMP} framework is capable of performing Bayesian inference for large-scale probabilistic state-space models with hundreds of thousands of random variables on a standard laptop computer.}
      \field{day}{27}
      \field{issn}{1058-9244}
      \field{journaltitle}{Scientific Programming}
      \field{langid}{english}
      \field{month}{5}
      \field{note}{Publisher: Hindawi}
      \field{title}{Reactive Message Passing for Scalable Bayesian Inference}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{2023}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{e6601690}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1155/2023/6601690
      \endverb
      \verb{file}
      \verb Bagaev and de Vries - 2023 - Reactive Message Passing for Scalable Bayesian Inf.pdf:/Users/bert/Zotero/storage/HJ8G8ZQ5/Bagaev and de Vries - 2023 - Reactive Message Passing for Scalable Bayesian Inf.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://www.hindawi.com/journals/sp/2023/6601690/
      \endverb
      \verb{url}
      \verb https://www.hindawi.com/journals/sp/2023/6601690/
      \endverb
    \endentry
    \entry{beckers_principled_2022}{misc}{}
      \name{author}{5}{}{%
        {{hash=c122659255f578ec53a361a26dbc15d8}{%
           family={Beckers},
           familyi={B\bibinitperiod},
           given={Jim},
           giveni={J\bibinitperiod}}}%
        {{hash=cfa44d20885516297af56f7111efe385}{%
           family={Erp},
           familyi={E\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=0364baa48634932f61c388220acef376}{%
           family={Zhao},
           familyi={Z\bibinitperiod},
           given={Ziyue},
           giveni={Z\bibinitperiod}}}%
        {{hash=6d2dd71f5ecb5d3e572be3b03ed80227}{%
           family={Kondrashov},
           familyi={K\bibinitperiod},
           given={Kirill},
           giveni={K\bibinitperiod}}}%
        {{hash=2caeea1b18c3a6d6099b66a5232eb983}{%
           family={Vries},
           familyi={V\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{7e3454b36b1de3023dc2afb15cd5f9a5}
      \strng{fullhash}{54639fa0e5f1bfc1be32336b870bfdc1}
      \strng{bibnamehash}{7e3454b36b1de3023dc2afb15cd5f9a5}
      \strng{authorbibnamehash}{7e3454b36b1de3023dc2afb15cd5f9a5}
      \strng{authornamehash}{7e3454b36b1de3023dc2afb15cd5f9a5}
      \strng{authorfullhash}{54639fa0e5f1bfc1be32336b870bfdc1}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian model reduction provides an efficient approach for comparing the performance of all nested sub-models of a model, without re-evaluating any of these sub-models. Until now, Bayesian model reduction has been applied mainly in the computational neuroscience community. In this paper, we formulate and apply Bayesian model reduction to perform principled pruning of Bayesian neural networks, based on variational free energy minimization. This novel parameter pruning scheme solves the shortcomings of many current state-of-the-art pruning methods that are used by the signal processing community. The proposed approach has a clear stopping criterion and minimizes the same objective that is used during training. Next to these theoretical benefits, our experiments indicate better model performance in comparison to state-of-the-art pruning schemes.}
      \field{day}{17}
      \field{eprinttype}{arxiv}
      \field{month}{10}
      \field{number}{{arXiv}:2210.09134}
      \field{title}{Principled Pruning of Bayesian Neural Networks through Variational Free Energy Minimization}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2210.09134
      \endverb
      \verb{eprint}
      \verb 2210.09134 [cs, eess]
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/bert/Zotero/storage/L74NKN63/Beckers et al. - 2022 - Principled Pruning of Bayesian Neural Networks thr.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/DVBAR68G/2210.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2210.09134
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2210.09134
      \endverb
      \keyw{Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing}
    \endentry
    \entry{bezanson_julia_2017}{article}{}
      \name{author}{4}{}{%
        {{hash=17e68031e06f57a7e83caeb4e0aed827}{%
           family={Bezanson},
           familyi={B\bibinitperiod},
           given={Jeff},
           giveni={J\bibinitperiod}}}%
        {{hash=01a6ca61fcd12a3a071ec49304df57f8}{%
           family={Edelman},
           familyi={E\bibinitperiod},
           given={Alan},
           giveni={A\bibinitperiod}}}%
        {{hash=bca7c93f55e669f71c4ff95e68ac9538}{%
           family={Karpinski},
           familyi={K\bibinitperiod},
           given={Stefan},
           giveni={S\bibinitperiod}}}%
        {{hash=de802ff42d9c902868c57a332dbac5e0}{%
           family={Shah},
           familyi={S\bibinitperiod},
           given={Viral\bibnamedelima B.},
           giveni={V\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \strng{namehash}{07e3452af3652a626dc1d02355fda942}
      \strng{fullhash}{651af5e2dc744eabe31cd448eb05d640}
      \strng{bibnamehash}{07e3452af3652a626dc1d02355fda942}
      \strng{authorbibnamehash}{07e3452af3652a626dc1d02355fda942}
      \strng{authornamehash}{07e3452af3652a626dc1d02355fda942}
      \strng{authorfullhash}{651af5e2dc744eabe31cd448eb05d640}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be “laws of nature" by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item High-level dynamic programs have to be slow. {\textbackslash}item One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the Julia programming language and its design---a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.}
      \field{day}{1}
      \field{issn}{0036-1445}
      \field{journaltitle}{{SIAM} Review}
      \field{month}{1}
      \field{note}{Publisher: Society for Industrial and Applied Mathematics}
      \field{number}{1}
      \field{shortjournal}{{SIAM} Rev.}
      \field{shorttitle}{Julia}
      \field{title}{Julia: A Fresh Approach to Numerical Computing}
      \field{urlday}{3}
      \field{urlmonth}{2}
      \field{urlyear}{2022}
      \field{volume}{59}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{65\bibrangedash 98}
      \range{pages}{34}
      \verb{doi}
      \verb 10.1137/141000671
      \endverb
      \verb{file}
      \verb Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:/Users/bert/Zotero/storage/NR5VVY86/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb https://epubs.siam.org/doi/10.1137/141000671
      \endverb
      \verb{url}
      \verb https://epubs.siam.org/doi/10.1137/141000671
      \endverb
      \keyw{Julia,parallel,65Y05,68N15,97P40,numerical,scientific computing}
    \endentry
    \entry{champion_realizing_2021}{article}{}
      \name{author}{3}{}{%
        {{hash=084b1526e0d6aabedb1d10e2baca4940}{%
           family={Champion},
           familyi={C\bibinitperiod},
           given={Théophile},
           giveni={T\bibinitperiod}}}%
        {{hash=1d202848b4807e6784ee0143f7940f50}{%
           family={Grześ},
           familyi={G\bibinitperiod},
           given={Marek},
           giveni={M\bibinitperiod}}}%
        {{hash=906dbd4c811afa1403515435beaefda9}{%
           family={Bowman},
           familyi={B\bibinitperiod},
           given={Howard},
           giveni={H\bibinitperiod}}}%
      }
      \strng{namehash}{37a32931fb9f074a3c4af76e94c9f79e}
      \strng{fullhash}{37a32931fb9f074a3c4af76e94c9f79e}
      \strng{bibnamehash}{37a32931fb9f074a3c4af76e94c9f79e}
      \strng{authorbibnamehash}{37a32931fb9f074a3c4af76e94c9f79e}
      \strng{authornamehash}{37a32931fb9f074a3c4af76e94c9f79e}
      \strng{authorfullhash}{37a32931fb9f074a3c4af76e94c9f79e}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Active inference is a state-of-the-art framework in neuroscience that offers a unified theory of brain function. It is also proposed as a framework for planning in {AI}. Unfortunately, the complex mathematics required to create new models can impede application of active inference in neuroscience and {AI} research. This letter addresses this problem by providing a complete mathematical treatment of the active inference framework in discrete time and state spaces and the derivation of the update equations for any new model. We leverage the theoretical connection between active inference and variational message passing as described by John Winn and Christopher M. Bishop in 2005. Since variational message passing is a well-defined methodology for deriving Bayesian belief update equations, this letter opens the door to advanced generative models for active inference. We show that using a fully factorized variational distribution simplifies the expected free energy, which furnishes priors over policies so that agents seek unambiguous states. Finally, we consider future extensions that support deep tree searches for sequential policy optimization based on structure learning and belief propagation.}
      \field{day}{16}
      \field{issn}{0899-7667}
      \field{journaltitle}{Neural Computation}
      \field{month}{9}
      \field{number}{10}
      \field{shortjournal}{Neural Computation}
      \field{shorttitle}{Realizing Active Inference in Variational Message Passing}
      \field{title}{Realizing Active Inference in Variational Message Passing: The Outcome-Blind Certainty Seeker}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{33}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2762\bibrangedash 2826}
      \range{pages}{65}
      \verb{doi}
      \verb 10.1162/neco_a_01422
      \endverb
      \verb{file}
      \verb Champion et al. - 2021 - Realizing Active Inference in Variational Message .pdf:/Users/bert/Zotero/storage/NHUTRTHU/Champion et al. - 2021 - Realizing Active Inference in Variational Message .pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/G342SF55/Realizing-Active-Inference-in-Variational-Message.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1162/neco_a_01422
      \endverb
      \verb{url}
      \verb https://doi.org/10.1162/neco_a_01422
      \endverb
    \endentry
    \entry{cox_factor_2019}{article}{}
      \name{author}{3}{}{%
        {{hash=4b41e1d66ebdde7171f697bf9892597c}{%
           family={Cox},
           familyi={C\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod}}}%
        {{hash=013ae0ec21570cef47f38a73391b6b1d}{%
           family={Laar},
           familyi={L\bibinitperiod},
           given={Thijs},
           giveni={T\bibinitperiod},
           prefix={van\bibnamedelima de},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=2caeea1b18c3a6d6099b66a5232eb983}{%
           family={Vries},
           familyi={V\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{5162e215c531d8e582746526d63ec292}
      \strng{fullhash}{5162e215c531d8e582746526d63ec292}
      \strng{bibnamehash}{5162e215c531d8e582746526d63ec292}
      \strng{authorbibnamehash}{5162e215c531d8e582746526d63ec292}
      \strng{authornamehash}{5162e215c531d8e582746526d63ec292}
      \strng{authorfullhash}{5162e215c531d8e582746526d63ec292}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The benefits of automating design cycles for Bayesian inference-based algorithms are becoming increasingly recognized by the machine learning community. As a result, interest in probabilistic programming frameworks has much increased over the past few years. This paper explores a specific probabilistic programming paradigm, namely message passing in Forney-style factor graphs ({FFGs}), in the context of automated design of efficient Bayesian signal processing algorithms. To this end, we developed “{ForneyLab}”2 as a Julia toolbox for message passing-based inference in {FFGs}. We show by example how {ForneyLab} enables automatic derivation of Bayesian signal processing algorithms, including algorithms for parameter estimation and model comparison. Crucially, due to the modular makeup of the {FFG} framework, both the model specification and inference methods are readily extensible in {ForneyLab}. In order to test this framework, we compared variational message passing as implemented by {ForneyLab} with automatic differentiation variational inference ({ADVI}) and Monte Carlo methods as implemented by state-of-the-art tools “Edward” and “Stan”. In terms of performance, extensibility and stability issues, {ForneyLab} appears to enjoy an edge relative to its competitors for automated inference in state-space models.}
      \field{day}{1}
      \field{issn}{0888-613X}
      \field{journaltitle}{International Journal of Approximate Reasoning}
      \field{month}{1}
      \field{shortjournal}{International Journal of Approximate Reasoning}
      \field{title}{A factor graph approach to automated design of Bayesian signal processing algorithms}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2018}
      \field{volume}{104}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{185\bibrangedash 204}
      \range{pages}{20}
      \verb{doi}
      \verb 10.1016/j.ijar.2018.11.002
      \endverb
      \verb{file}
      \verb Cox & van de Laar e.a. - 2019 - A factor graph approach to automated design of Bay.pdf:/Users/bert/Zotero/storage/THHZ5RTY/Cox & van de Laar e.a. - 2019 - A factor graph approach to automated design of Bay.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://www.sciencedirect.com/science/article/pii/S0888613X18304298
      \endverb
      \verb{url}
      \verb http://www.sciencedirect.com/science/article/pii/S0888613X18304298
      \endverb
      \keyw{Bayesian inference,Message passing,Factor graphs,Julia,Probabilistic programming}
    \endentry
    \entry{noauthor_distributive_2022}{inreference}{}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labeltitlesource}{title}
      \field{abstract}{In mathematics, the distributive property of binary operations generalizes the distributive law, which asserts that the equality is always true in elementary algebra. For example, in elementary arithmetic, one has One says that multiplication distributes over addition. This basic property of numbers is part of the definition of most algebraic structures that have two operations called addition and multiplication, such as complex numbers, polynomials, matrices, rings, and fields. It is also encountered in Boolean algebra and mathematical logic, where each of the logical and (denoted ∧ \{{\textbackslash}displaystyle {\textbackslash},{\textbackslash}land {\textbackslash},\} ) and the logical or (denoted ∨ \{{\textbackslash}displaystyle {\textbackslash},{\textbackslash}lor {\textbackslash},\} ) distributes over the other.}
      \field{booktitle}{Wikipedia}
      \field{day}{29}
      \field{langid}{english}
      \field{month}{11}
      \field{note}{Page Version {ID}: 1124679546}
      \field{title}{Distributive property}
      \field{urlday}{26}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb Snapshot:/Users/bert/Zotero/storage/LGFIXLJC/Distributive_property.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://en.wikipedia.org/w/index.php?title=Distributive_property&oldid=1124679546
      \endverb
      \verb{url}
      \verb https://en.wikipedia.org/w/index.php?title=Distributive_property&oldid=1124679546
      \endverb
    \endentry
    \entry{friston_bayesian_2018}{article}{}
      \name{author}{3}{}{%
        {{hash=9f452a84796a900973333b32a5f8bc61}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=11ecca46a14363f21a56a51e1cb6c0ad}{%
           family={Parr},
           familyi={P\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=8b8982cbf55b148746afedc4ea8162f7}{%
           family={Zeidman},
           familyi={Z\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{fda25de939d07aeaf2de17ef4b87845c}
      \strng{fullhash}{fda25de939d07aeaf2de17ef4b87845c}
      \strng{bibnamehash}{fda25de939d07aeaf2de17ef4b87845c}
      \strng{authorbibnamehash}{fda25de939d07aeaf2de17ef4b87845c}
      \strng{authornamehash}{fda25de939d07aeaf2de17ef4b87845c}
      \strng{authorfullhash}{fda25de939d07aeaf2de17ef4b87845c}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper reviews recent developments in statistical structure learning; namely, Bayesian model reduction. Bayesian model reduction is a special but ubiquitous case of Bayesian model comparison that, in the setting of variational Bayes, furnishes an analytic solution for (a lower bound on) model evidence induced by a change in priors. This analytic solution finesses the problem of scoring large model spaces in model comparison or structure learning. This is because each new model can be cast in terms of an alternative set of priors over model parameters. Furthermore, the reduced free energy (i.e. evidence bound on the reduced model) finds an expedient application in hierarchical models, where it plays the role of a summary statistic. In other words, it contains all the necessary information contained in the posterior distributions over parameters of lower levels. In this technical note, we review Bayesian model reduction - in terms of common forms of reduced free energy - and illustrate recent applications in structure learning, hierarchical or empirical Bayes and as a metaphor for neurobiological processes like abductive reasoning and sleep.}
      \field{day}{18}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{{arXiv}:1805.07092 [stat]}
      \field{month}{5}
      \field{title}{Bayesian model reduction}
      \field{urlday}{28}
      \field{urlmonth}{5}
      \field{urlyear}{2018}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 1805.07092
      \endverb
      \verb{file}
      \verb arXiv.org Snapshot:/Users/bert/Zotero/storage/9XW27LI8/1805.html:text/html;Friston et al. - 2018 - Bayesian model reduction.pdf:/Users/bert/Zotero/storage/86X8JTVR/Friston et al. - 2018 - Bayesian model reduction.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1805.07092
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1805.07092
      \endverb
      \keyw{Statistics - Methodology}
    \endentry
    \entry{friston_active_2015}{article}{}
      \name{author}{6}{}{%
        {{hash=9f452a84796a900973333b32a5f8bc61}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=103c6f1e922e29f43590be466a0cb206}{%
           family={Rigoli},
           familyi={R\bibinitperiod},
           given={Francesco},
           giveni={F\bibinitperiod}}}%
        {{hash=fc4575c484ed62fef0505e03b18df463}{%
           family={Ognibene},
           familyi={O\bibinitperiod},
           given={Dimitri},
           giveni={D\bibinitperiod}}}%
        {{hash=d531a432bbd0476710542d1c3b647224}{%
           family={Mathys},
           familyi={M\bibinitperiod},
           given={Christoph},
           giveni={C\bibinitperiod}}}%
        {{hash=d7fda358d14b999c8dfd9ffcb6bc21de}{%
           family={{FitzGerald}},
           familyi={F\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=a407d9f6bc90219c5764e676c5b8caa7}{%
           family={Pezzulo},
           familyi={P\bibinitperiod},
           given={Giovanni},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{fullhash}{911876d8dcdb85faf05c78a1cf297991}
      \strng{bibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorbibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authornamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorfullhash}{911876d8dcdb85faf05c78a1cf297991}
      \field{extraname}{1}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We offer a formal treatment of choice behaviour based on the premise that agents minimise the expected free energy of future outcomes. Crucially, the negative free energy or quality of a policy can be decomposed into extrinsic and epistemic (or intrinsic) value. Minimising expected free energy is therefore equivalent to maximising extrinsic value or expected utility (defined in terms of prior preferences or goals), while maximising information gain or intrinsic value (reducing uncertainty about the causes of valuable outcomes). The resulting scheme resolves the exploration-exploitation dilemma: epistemic value is maximised until there is no further information gain, after which exploitation is assured through maximisation of extrinsic value. This is formally consistent with the Infomax principle, generalising formulations of active vision based upon salience (Bayesian surprise) and optimal decisions based on expected utility and risk-sensitive (Kullback-Leibler) control. Furthermore, as with previous active inference formulations of discrete (Markovian) problems, ad hoc softmax parameters become the expected (Bayes-optimal) precision of beliefs about, or confidence in, policies. This article focuses on the basic theory, illustrating the ideas with simulations. A key aspect of these simulations is the similarity between precision updates and dopaminergic discharges observed in conditioning paradigms.}
      \field{day}{17}
      \field{issn}{1758-8928}
      \field{issue}{ja}
      \field{journaltitle}{Cognitive Neuroscience}
      \field{month}{2}
      \field{title}{Active inference and epistemic value}
      \field{urlday}{22}
      \field{urlmonth}{2}
      \field{urlyear}{2015}
      \field{volume}{0}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{null}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1080/17588928.2015.1020053
      \endverb
      \verb{file}
      \verb Friston et al. - 2015 - Active inference and epistemic value.pdf:/Users/bert/Zotero/storage/PP98MUVH/Friston et al. - 2015 - Active inference and epistemic value.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/DJXDIQ7X/17588928.2015.html:text/html;Snapshot:/Users/bert/Zotero/storage/JUAJVU3Q/17588928.2015.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://dx.doi.org/10.1080/17588928.2015.1020053
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1080/17588928.2015.1020053
      \endverb
    \endentry
    \entry{friston_sophisticated_2021}{article}{}
      \name{author}{5}{}{%
        {{hash=9f452a84796a900973333b32a5f8bc61}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=332cb0e034bc330deba64ea078b1d415}{%
           family={Da\bibnamedelima Costa},
           familyi={D\bibinitperiod\bibinitdelim C\bibinitperiod},
           given={Lancelot},
           giveni={L\bibinitperiod}}}%
        {{hash=6a1d86e250a782086160211ed8814f06}{%
           family={Hafner},
           familyi={H\bibinitperiod},
           given={Danijar},
           giveni={D\bibinitperiod}}}%
        {{hash=dae81c24a9309976c5b87d2d54bcd926}{%
           family={Hesp},
           familyi={H\bibinitperiod},
           given={Casper},
           giveni={C\bibinitperiod}}}%
        {{hash=11ecca46a14363f21a56a51e1cb6c0ad}{%
           family={Parr},
           familyi={P\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{fullhash}{92c182412af6878e001d3d0549d541a2}
      \strng{bibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorbibnamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authornamehash}{a5aa6cc9fa51d6be7eef2a1be62bf301}
      \strng{authorfullhash}{92c182412af6878e001d3d0549d541a2}
      \field{extraname}{2}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Active inference offers a first principle account of sentient behavior, from which special and important cases—for example, reinforcement learning, active learning, Bayes optimal inference, Bayes optimal design—can be derived. Active inference finesses the exploitation-exploration dilemma in relation to prior preferences by placing information gain on the same footing as reward or value. In brief, active inference replaces value functions with functionals of (Bayesian) beliefs, in the form of an expected (variational) free energy. In this letter, we consider a sophisticated kind of active inference using a recursive form of expected free energy. Sophistication describes the degree to which an agent has beliefs about beliefs. We consider agents with beliefs about the counterfactual consequences of action for states of affairs and beliefs about those latent states. In other words, we move from simply considering beliefs about “what would happen if I did that” to “what I would believe about what would happen if I did that.” The recursive form of the free energy functional effectively implements a deep tree search over actions and outcomes in the future. Crucially, this search is over sequences of belief states as opposed to states per se. We illustrate the competence of this scheme using numerical simulations of deep decision problems.}
      \field{day}{1}
      \field{issn}{0899-7667}
      \field{journaltitle}{Neural Computation}
      \field{month}{3}
      \field{number}{3}
      \field{shortjournal}{Neural Computation}
      \field{title}{Sophisticated Inference}
      \field{urlday}{14}
      \field{urlmonth}{2}
      \field{urlyear}{2022}
      \field{volume}{33}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{713\bibrangedash 763}
      \range{pages}{51}
      \verb{doi}
      \verb 10.1162/neco_a_01351
      \endverb
      \verb{file}
      \verb Friston et al. - 2021 - Sophisticated Inference.pdf:/Users/bert/Zotero/storage/G442FQGW/Friston et al. - 2021 - Sophisticated Inference.pdf:application/pdf;Snapshot:/Users/bert/Zotero/storage/NKAXVIYP/Sophisticated-Inference.html:text/html
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1162/neco_a_01351
      \endverb
      \verb{url}
      \verb https://doi.org/10.1162/neco_a_01351
      \endverb
    \endentry
    \entry{friston_designing_2022}{misc}{}
      \name{author}{20}{}{%
        {{hash=558c08870d21f7acad878d94e125a377}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl\bibnamedelima J.},
           giveni={K\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=6a73b5c0ecdaa5d8b11cdabd8722a4d8}{%
           family={Ramstead},
           familyi={R\bibinitperiod},
           given={Maxwell\bibnamedelimb J.\bibnamedelimi D.},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=2fc9c8b19fa98a7a0714a87c642e841b}{%
           family={Kiefer},
           familyi={K\bibinitperiod},
           given={Alex\bibnamedelima B.},
           giveni={A\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=3f98dc382188ff01295eb411612cb9ae}{%
           family={Tschantz},
           familyi={T\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
        {{hash=6ba2af6672c84ba8cffc8b86bc35d608}{%
           family={Buckley},
           familyi={B\bibinitperiod},
           given={Christopher\bibnamedelima L.},
           giveni={C\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=7c7919bd7bc85f504a37bd9f88dd1b65}{%
           family={Albarracin},
           familyi={A\bibinitperiod},
           given={Mahault},
           giveni={M\bibinitperiod}}}%
        {{hash=1f942826d05bb0cceedcb4d7fb303470}{%
           family={Pitliya},
           familyi={P\bibinitperiod},
           given={Riddhi\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=fb4bbb3afb98b49b445df774c32de2a4}{%
           family={Heins},
           familyi={H\bibinitperiod},
           given={Conor},
           giveni={C\bibinitperiod}}}%
        {{hash=96c0ca963915880eb9e9d845290a647d}{%
           family={Klein},
           familyi={K\bibinitperiod},
           given={Brennan},
           giveni={B\bibinitperiod}}}%
        {{hash=dcf103531a4bb5a439a2233cf52e9703}{%
           family={Millidge},
           familyi={M\bibinitperiod},
           given={Beren},
           giveni={B\bibinitperiod}}}%
        {{hash=30263511bfb0db011091d6d55c2b8be7}{%
           family={Sakthivadivel},
           familyi={S\bibinitperiod},
           given={Dalton\bibnamedelimb A.\bibnamedelimi R.},
           giveni={D\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
        {{hash=32cdf1101c03fd9136bf8cd272b01583}{%
           family={Smithe},
           familyi={S\bibinitperiod},
           given={Toby\bibnamedelimb St\bibnamedelima Clere},
           giveni={T\bibinitperiod\bibinitdelim S\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
        {{hash=3f9715382a039e62fed7ea317bf9c852}{%
           family={Koudahl},
           familyi={K\bibinitperiod},
           given={Magnus},
           giveni={M\bibinitperiod}}}%
        {{hash=bde367596d2d1969e77b2567b286f3c3}{%
           family={Tremblay},
           familyi={T\bibinitperiod},
           given={Safae\bibnamedelima Essafi},
           giveni={S\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=b539e21b7e071c4cf771ea373614e1a2}{%
           family={Petersen},
           familyi={P\bibinitperiod},
           given={Capm},
           giveni={C\bibinitperiod}}}%
        {{hash=5ac007c332bda0caad21bf8f276bc862}{%
           family={Fung},
           familyi={F\bibinitperiod},
           given={Kaiser},
           giveni={K\bibinitperiod}}}%
        {{hash=f35c0ad2f0e64ad37388bf0272897099}{%
           family={Fox},
           familyi={F\bibinitperiod},
           given={Jason\bibnamedelima G.},
           giveni={J\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=556dc9c4ab42e4f73c58ffd2ca79fab0}{%
           family={Swanson},
           familyi={S\bibinitperiod},
           given={Steven},
           giveni={S\bibinitperiod}}}%
        {{hash=165b94027479be942055bedad58b0ef2}{%
           family={Mapes},
           familyi={M\bibinitperiod},
           given={Dan},
           giveni={D\bibinitperiod}}}%
        {{hash=e3561bc5c4e27cca0e81fafd03f55489}{%
           family={René},
           familyi={R\bibinitperiod},
           given={Gabriel},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{4d2eb0daa411155bb357b1254b5ffb8a}
      \strng{fullhash}{ac5dd604a1e56be4c8f50fbbf81ddb18}
      \strng{bibnamehash}{4d2eb0daa411155bb357b1254b5ffb8a}
      \strng{authorbibnamehash}{4d2eb0daa411155bb357b1254b5ffb8a}
      \strng{authornamehash}{4d2eb0daa411155bb357b1254b5ffb8a}
      \strng{authorfullhash}{ac5dd604a1e56be4c8f50fbbf81ddb18}
      \field{extraname}{3}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This white paper lays out a vision of research and development in the field of artificial intelligence for the next decade (and beyond). Its denouement is a cyber-physical ecosystem of natural and synthetic sense-making, in which humans are integral participants\${\textbackslash}unicode\{x2014\}\$what we call ''shared intelligence''. This vision is premised on active inference, a formulation of adaptive behavior that can be read as a physics of intelligence, and which inherits from the physics of self-organization. In this context, we understand intelligence as the capacity to accumulate evidence for a generative model of one's sensed world\${\textbackslash}unicode\{x2014\}\$also known as self-evidencing. Formally, this corresponds to maximizing (Bayesian) model evidence, via belief updating over several scales: i.e., inference, learning, and model selection. Operationally, this self-evidencing can be realized via (variational) message passing or belief propagation on a factor graph. Crucially, active inference foregrounds an existential imperative of intelligent systems; namely, curiosity or the resolution of uncertainty. This same imperative underwrites belief sharing in ensembles of agents, in which certain aspects (i.e., factors) of each agent's generative world model provide a common ground or frame of reference. Active inference plays a foundational role in this ecology of belief sharing\${\textbackslash}unicode\{x2014\}\$leading to a formal account of collective intelligence that rests on shared narratives and goals. We also consider the kinds of communication protocols that must be developed to enable such an ecosystem of intelligences and motivate the development of a shared hyper-spatial modeling language and transaction protocol, as a first\${\textbackslash}unicode\{x2014\}\$and key\${\textbackslash}unicode\{x2014\}\$step towards such an ecology.}
      \field{day}{2}
      \field{eprinttype}{arxiv}
      \field{month}{12}
      \field{number}{{arXiv}:2212.01354}
      \field{title}{Designing Ecosystems of Intelligence from First Principles}
      \field{urlday}{8}
      \field{urlmonth}{12}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2212.01354
      \endverb
      \verb{eprint}
      \verb 2212.01354 [nlin]
      \endverb
      \verb{file}
      \verb arXiv.org Snapshot:/Users/bert/Zotero/storage/4D9QP5EX/2212.html:text/html;Friston et al. - 2022 - Designing Ecosystems of Intelligence from First Pr.pdf:/Users/bert/Zotero/storage/M97Q353E/Friston et al. - 2022 - Designing Ecosystems of Intelligence from First Pr.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2212.01354
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2212.01354
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Nonlinear Sciences - Adaptation and Self-Organizing Systems}
    \endentry
    \entry{friston_et_al._spm12_2014}{book}{}
      \name{author}{1}{}{%
        {{hash=fe604574f8fbbff943468381bd385d98}{%
           family={Friston\bibnamedelimb et\bibnamedelima al.},
           familyi={F\bibinitperiod\bibinitdelim e\bibinitperiod\bibinitdelim a\bibinitperiod},
           given={Karl\bibnamedelima J.},
           giveni={K\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{fe604574f8fbbff943468381bd385d98}
      \strng{fullhash}{fe604574f8fbbff943468381bd385d98}
      \strng{bibnamehash}{fe604574f8fbbff943468381bd385d98}
      \strng{authorbibnamehash}{fe604574f8fbbff943468381bd385d98}
      \strng{authornamehash}{fe604574f8fbbff943468381bd385d98}
      \strng{authorfullhash}{fe604574f8fbbff943468381bd385d98}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{SPM}12 toolbox, http://www.fil.ion.ucl.ac.uk/spm/software/}
      \field{year}{2014}
      \field{dateera}{ce}
    \endentry
    \entry{heins_pymdp_2022}{article}{}
      \name{author}{7}{}{%
        {{hash=fb4bbb3afb98b49b445df774c32de2a4}{%
           family={Heins},
           familyi={H\bibinitperiod},
           given={Conor},
           giveni={C\bibinitperiod}}}%
        {{hash=dcf103531a4bb5a439a2233cf52e9703}{%
           family={Millidge},
           familyi={M\bibinitperiod},
           given={Beren},
           giveni={B\bibinitperiod}}}%
        {{hash=d94ee9f2b21b65e1984a9aae24d9b1f2}{%
           family={Demekas},
           familyi={D\bibinitperiod},
           given={Daphne},
           giveni={D\bibinitperiod}}}%
        {{hash=96c0ca963915880eb9e9d845290a647d}{%
           family={Klein},
           familyi={K\bibinitperiod},
           given={Brennan},
           giveni={B\bibinitperiod}}}%
        {{hash=9f452a84796a900973333b32a5f8bc61}{%
           family={Friston},
           familyi={F\bibinitperiod},
           given={Karl},
           giveni={K\bibinitperiod}}}%
        {{hash=9c45a6508408a168352c55ff00c904c1}{%
           family={Couzin},
           familyi={C\bibinitperiod},
           given={Iain},
           giveni={I\bibinitperiod}}}%
        {{hash=3f98dc382188ff01295eb411612cb9ae}{%
           family={Tschantz},
           familyi={T\bibinitperiod},
           given={Alexander},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{0ae0d33a6b17be533d43c093acf0b2e6}
      \strng{fullhash}{250a98601836b43843c5c09875a1f347}
      \strng{bibnamehash}{0ae0d33a6b17be533d43c093acf0b2e6}
      \strng{authorbibnamehash}{0ae0d33a6b17be533d43c093acf0b2e6}
      \strng{authornamehash}{0ae0d33a6b17be533d43c093acf0b2e6}
      \strng{authorfullhash}{250a98601836b43843c5c09875a1f347}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Active inference is an account of cognition and behavior in complex systems which brings together action, perception, and learning under the theoretical mantle of Bayesian inference. Active inference has seen growing applications in academic research, especially in fields that seek to model human or animal behavior. While in recent years, some of the code arising from the active inference literature has been written in open source languages like Python and Julia, to-date, the most popular software for simulating active inference agents is the {DEM} toolbox of {SPM}, a {MATLAB} library originally developed for the statistical analysis and modelling of neuroimaging data. Increasing interest in active inference, manifested both in terms of sheer number as well as diversifying applications across scientific disciplines, has thus created a need for generic, widely-available, and user-friendly code for simulating active inference in open-source scientific computing languages like Python. The Python package we present here, pymdp (see https://github.com/infer-actively/pymdp), represents a significant step in this direction: namely, we provide the first open-source package for simulating active inference with partially-observable Markov Decision Processes or {POMDPs}. We review the package's structure and explain its advantages like modular design and customizability, while providing in-text code blocks along the way to demonstrate how it can be used to build and run active inference processes with ease. We developed pymdp to increase the accessibility and exposure of the active inference framework to researchers, engineers, and developers with diverse disciplinary backgrounds. In the spirit of open-source software, we also hope that it spurs new innovation, development, and collaboration in the growing active inference community.}
      \field{day}{11}
      \field{eprinttype}{arxiv}
      \field{journaltitle}{{arXiv}:2201.03904 [cs, q-bio]}
      \field{month}{1}
      \field{shorttitle}{pymdp}
      \field{title}{pymdp: A Python library for active inference in discrete state spaces}
      \field{urlday}{3}
      \field{urlmonth}{2}
      \field{urlyear}{2022}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 2201.03904
      \endverb
      \verb{file}
      \verb arXiv Fulltext PDF:/Users/bert/Zotero/storage/RFEXR7P9/Heins et al. - 2022 - pymdp A Python library for active inference in di.pdf:application/pdf;arXiv.org Snapshot:/Users/bert/Zotero/storage/T8RU4YHJ/2201.html:text/html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2201.03904
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2201.03904
      \endverb
      \keyw{Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition,Computer Science - Mathematical Software}
    \endentry
    \entry{lanczos_variational_1986}{book}{}
      \name{author}{1}{}{%
        {{hash=566cbec89d411ac615d67809e3083f19}{%
           family={Lanczos},
           familyi={L\bibinitperiod},
           given={Cornelius},
           giveni={C\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York}%
      }
      \list{publisher}{1}{%
        {Dover Publications}%
      }
      \strng{namehash}{566cbec89d411ac615d67809e3083f19}
      \strng{fullhash}{566cbec89d411ac615d67809e3083f19}
      \strng{bibnamehash}{566cbec89d411ac615d67809e3083f19}
      \strng{authorbibnamehash}{566cbec89d411ac615d67809e3083f19}
      \strng{authornamehash}{566cbec89d411ac615d67809e3083f19}
      \strng{authorfullhash}{566cbec89d411ac615d67809e3083f19}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{day}{1}
      \field{edition}{4th Revised ed. edition}
      \field{isbn}{978-0-486-65067-8}
      \field{month}{3}
      \field{pagetotal}{464}
      \field{title}{The Variational Principles of Mechanics}
      \field{year}{1986}
      \field{dateera}{ce}
    \endentry
    \entry{loeliger_sparsity_2016}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=bb0471027d3704ad88faec92a728ab52}{%
           family={Loeliger},
           familyi={L\bibinitperiod},
           given={Hans-Andrea},
           giveni={H\bibinithyphendelim A\bibinitperiod}}}%
        {{hash=260ca80c39d70bb1ecb78e24cf82fb34}{%
           family={Bruderer},
           familyi={B\bibinitperiod},
           given={Lukas},
           giveni={L\bibinitperiod}}}%
        {{hash=eaf4cd6df556c52c348c6f6a71148278}{%
           family={Malmberg},
           familyi={M\bibinitperiod},
           given={Hampus},
           giveni={H\bibinitperiod}}}%
        {{hash=f22af1ad20b4d95ac838f17ecfb7cb59}{%
           family={Wadehn},
           familyi={W\bibinitperiod},
           given={Federico},
           giveni={F\bibinitperiod}}}%
        {{hash=b09c11250dd8968d6afa028abec3f0af}{%
           family={Zalmai},
           familyi={Z\bibinitperiod},
           given={Nour},
           giveni={N\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {La Jolla, {CA}, {USA}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{b522557c40c360d33d1cf4bcd04fe4c3}
      \strng{fullhash}{3507075320aea6fb87a2169a32ac5e7a}
      \strng{bibnamehash}{b522557c40c360d33d1cf4bcd04fe4c3}
      \strng{authorbibnamehash}{b522557c40c360d33d1cf4bcd04fe4c3}
      \strng{authornamehash}{b522557c40c360d33d1cf4bcd04fe4c3}
      \strng{authorfullhash}{3507075320aea6fb87a2169a32ac5e7a}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Normal priors with unknown variance ({NUV}) have long been known to promote sparsity and to blend well with parameter learning by expectation maximization ({EM}). In this paper, we advocate this approach for linear state space models for applications such as the estimation of impulsive signals, the detection of localized events, smoothing with occasional jumps in the state space, and the detection and removal of outliers.}
      \field{booktitle}{2016 Information Theory and Applications Workshop ({ITA})}
      \field{eventtitle}{2016 Information Theory and Applications ({ITA})}
      \field{isbn}{978-1-5090-2529-9}
      \field{langid}{english}
      \field{month}{1}
      \field{title}{On sparsity by {NUV}-{EM}, Gaussian message passing, and Kalman smoothing}
      \field{urlday}{21}
      \field{urlmonth}{7}
      \field{urlyear}{2021}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/ITA.2016.7888168
      \endverb
      \verb{file}
      \verb Loeliger et al. - 2016 - On sparsity by NUV-EM, Gaussian message passing, a.pdf:/Users/bert/Zotero/storage/CPLS9D4P/Loeliger et al. - 2016 - On sparsity by NUV-EM, Gaussian message passing, a.pdf:application/pdf
      \endverb
      \verb{urlraw}
      \verb http://ieeexplore.ieee.org/document/7888168/
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/document/7888168/
      \endverb
    \endentry
    \entry{senoz_variational_2021}{article}{}
      \name{author}{4}{}{%
        {{hash=e5283bae0a45a8c8ab15a81ea917f578}{%
           family={Şenöz},
           familyi={Ş\bibinitperiod},
           given={İsmail},
           giveni={İ\bibinitperiod}}}%
        {{hash=013ae0ec21570cef47f38a73391b6b1d}{%
           family={Laar},
           familyi={L\bibinitperiod},
           given={Thijs},
           giveni={T\bibinitperiod},
           prefix={van\bibnamedelima de},
           prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod}}}%
        {{hash=63366e44ffa7043ff4bd1398c7a44165}{%
           family={Bagaev},
           familyi={B\bibinitperiod},
           given={Dmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=2caeea1b18c3a6d6099b66a5232eb983}{%
           family={Vries},
           familyi={V\bibinitperiod},
           given={Bert},
           giveni={B\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{779ced473070d7ff7e6b3fbc8448d961}
      \strng{fullhash}{debb53b59e42191c283773a0d4a45346}
      \strng{bibnamehash}{779ced473070d7ff7e6b3fbc8448d961}
      \strng{authorbibnamehash}{779ced473070d7ff7e6b3fbc8448d961}
      \strng{authornamehash}{779ced473070d7ff7e6b3fbc8448d961}
      \strng{authorfullhash}{debb53b59e42191c283773a0d4a45346}
      \field{sortinit}{Ş}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Accurate evaluation of Bayesian model evidence for a given data set is a fundamental problem in model development. Since evidence evaluations are usually intractable, in practice variational free energy ({VFE}) minimization provides an attractive alternative, as the {VFE} is an upper bound on negative model log-evidence ({NLE}). In order to improve tractability of the {VFE}, it is common to manipulate the constraints in the search space for the posterior distribution of the latent variables. Unfortunately, constraint manipulation may also lead to a less accurate estimate of the {NLE}. Thus, constraint manipulation implies an engineering trade-off between tractability and accuracy of model evidence estimation. In this paper, we develop a unifying account of constraint manipulation for variational inference in models that can be represented by a (Forney-style) factor graph, for which we identify the Bethe Free Energy as an approximation to the {VFE}. We derive well-known message passing algorithms from first principles, as the result of minimizing the constrained Bethe Free Energy ({BFE}). The proposed method supports evaluation of the {BFE} in factor graphs for model scoring and development of new message passing-based inference algorithms that potentially improve evidence estimation accuracy.}
      \field{day}{24}
      \field{issn}{1099-4300}
      \field{journaltitle}{Entropy (Basel, Switzerland)}
      \field{month}{6}
      \field{number}{7}
      \field{shortjournal}{Entropy (Basel)}
      \field{title}{Variational Message Passing and Local Constraint Manipulation in Factor Graphs}
      \field{volume}{23}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{807}
      \range{pages}{1}
      \verb{doi}
      \verb 10.3390/e23070807
      \endverb
      \verb{file}
      \verb Şenöz et al. - 2021 - Variational Message Passing and Local Constraint M.pdf:/Users/bert/Zotero/storage/2ECWWQ3V/Şenöz et al. - 2021 - Variational Message Passing and Local Constraint M.pdf:application/pdf
      \endverb
      \keyw{Bayesian inference,message passing,factor graphs,Bethe free energy,variational message passing,variational inference,variational free energy}
    \endentry
    \entry{smirnova_organoid_2023}{article}{}
      \true{moreauthor}
      \true{morelabelname}
      \name{author}{10}{}{%
        {{hash=219b27d12cb83fdde7ca4ce68c0ed1d7}{%
           family={Smirnova},
           familyi={S\bibinitperiod},
           given={Lena},
           giveni={L\bibinitperiod}}}%
        {{hash=4f589afb697a7475bab19b5204e287da}{%
           family={Caffo},
           familyi={C\bibinitperiod},
           given={Brian\bibnamedelima S},
           giveni={B\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
        {{hash=33bbe78f0b9a14c6c7245fec12cccb02}{%
           family={Gracias},
           familyi={G\bibinitperiod},
           given={David\bibnamedelima H},
           giveni={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=3f2f88c6d17b9caf7c8daf637b818023}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Qi},
           giveni={Q\bibinitperiod}}}%
        {{hash=3fc2f4db51aadc37a3eb9a18da7eba79}{%
           family={Morales\bibnamedelima Pantoja},
           familyi={M\bibinitperiod\bibinitdelim P\bibinitperiod},
           given={Itzy\bibnamedelima E},
           giveni={I\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=0ed05e1bd835ab2e128b8e3dc4f970fc}{%
           family={Tang},
           familyi={T\bibinitperiod},
           given={Bohao},
           giveni={B\bibinitperiod}}}%
        {{hash=735715e1ab3b8aaca4d4be12fb0e8c71}{%
           family={Zack},
           familyi={Z\bibinitperiod},
           given={Donald\bibnamedelima J},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=7830bf288c0d5877b43d32dd6cf68eee}{%
           family={Berlinicke},
           familyi={B\bibinitperiod},
           given={Cynthia\bibnamedelima A},
           giveni={C\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=1c426549bd342c6e259596f174f49807}{%
           family={Boyd},
           familyi={B\bibinitperiod},
           given={J\bibnamedelima Lomax},
           giveni={J\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
        {{hash=4dffef849d517b52d6b497f634446afd}{%
           family={Harris},
           familyi={H\bibinitperiod},
           given={Timothy\bibnamedelima D},
           giveni={T\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{a5a6c5557eb061b6f119a222e7af1cc7}
      \strng{fullhash}{7d2f272de6a6d35ed837f411910a3828}
      \strng{bibnamehash}{a5a6c5557eb061b6f119a222e7af1cc7}
      \strng{authorbibnamehash}{a5a6c5557eb061b6f119a222e7af1cc7}
      \strng{authornamehash}{a5a6c5557eb061b6f119a222e7af1cc7}
      \strng{authorfullhash}{7d2f272de6a6d35ed837f411910a3828}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Frontiers in Science}
      \field{note}{Publisher: Frontiers}
      \field{title}{Organoid intelligence ({OI}): the new frontier in biocomputing and intelligence-in-a-dish}
      \field{year}{2023}
      \field{dateera}{ce}
    \endentry
  \enddatalist
\endrefsection
\endinput

