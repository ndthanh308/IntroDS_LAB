
\section{FE Minimization by Reactive Message Passing}\label{sec:RMP}


\subsection{Why message passing-based inference?}\label{sec:why-MP}

Up to this point, our arguments strongly supported AIF as an information processing engine for the robot. Unfortunately, the computational demands for simulating a non-trivial synthetic AIF agent are extreme. For comparison, consider the human brain that minimizes in real-time, for less than 20 watts, a highly time-varying FE functional (visual data rate about of about a million bits per second) over about $100$ trillion latent variables (synapses). It has been estimated that the human brain consumes about a million times less energy than a high-tech silicon computer on quantitatively comparable information processing tasks. \cite{smirnova_organoid_2023}. 

Clearly, the human brain minimizes FE in a very different way than is available in standard optimization toolboxes. In this section, we will argue for developing a FE minimization toolbox based on reactive message passing in a factor graph.   

% Figure environment removed

First, we shortly recapitulate why message passing in factor graphs is an effective inference method for large models. Consider a factorized multivariate function
\begin{align}\label{eq:factorized-model}
p(x_1,&x_2,\ldots,x_7) \notag \\
&= f_a(x_1) f_b(x_2) f_c(x_1,x_2,x_3) f_d(x_4) f_e(x_3,x_4,x_5) f_f(x_6) f_g(x_5,x_6,x_7)
\end{align}
Assume that we are interested in inferring (the so-called marginal distribution) 
\begin{equation}\label{eq:x3-marginal}
  p(x_3) = \sum_{x_1}\sum_{x_2}\sum_{x_4}\sum_{x_5}\sum_{x_6}\sum_{x_7} p(x_1,x_2,\ldots,x_7)  
\end{equation}
If each variable $x_i$ in \eqref{eq:x3-marginal} has about $10$ possible values, then the sum contains about $1$ million terms. However, making use of the factorization \eqref{eq:factorized-model} and the distributive law \cite{noauthor_distributive_2022}, we can rewrite this sum as 
\begin{align}\label{eq:x3-marginal-by-mp}
p(&x_3) = 
\bigg( \overbrace{\sum_{x_1}\sum_{x_2} f_a(x_1) f_b(x_2) f_c(x_1,x_2,x_3)}^{\overrightarrow{\mu}_3(x_3)} \bigg) \cdot \notag \\
&\cdot \bigg( \underbrace{\sum_{x_4} \sum_{x_5} f_d(x_4) f_e(x_3,x_4,x_5) \big( \overbrace{\sum_{x_6} \sum_{x_7} f_f(x_6) f_g(x_5,x_6,x_7)}^{\overleftarrow{\mu}_5(x_5)}\big)}_{\overleftarrow{\mu}_3(x_3)}\bigg)
\end{align}

The computation in \eqref{eq:x3-marginal-by-mp}, which requires only a few hundred summations and multiplications, is clearly preferred from a computational load viewpoint. To execute \eqref{eq:x3-marginal-by-mp}, we need to compute intermediate results $\overrightarrow{\mu}_{i}(x_i)$ and $\overleftarrow{\mu}_{i}(x_i)$ that afford an interpretation of local messages in a Forney-style Factor Graph (FFG) representation of the model, see Fig.~\ref{fig:example-ffg}.

Variational FE minimization can also be executed by message passing in a factor graph. In fact, nearly all known effective variational inference methods on factorized models can be interpreted as minimization of a so-called ``constrained Bethe Free Energy'' (CBFE) functional \cite{senoz_variational_2021}. In this formulation, posterior variational beliefs are factorized into beliefs over both the nodes and the edges of the graph. It is possible to add constraints to these local beliefs such as requiring that a particular variational posterior is expressed by a Gaussian distribution. In general, CBFE minimization by message passing in a factor graph supports local adaptation of a plethora of constraints to optimize accuracy vs resource consumption. \cite{senoz_variational_2021, akbayrak_extended_2021}   

Useful dynamic models for real-time processing of data streams with a large number of latent variables are necessarily sparsely connected because otherwise, real-time inference would not be tractable. In sparse models, the computational complexity of inference can be vastly reduced by message passing in a factor graph representation of the model. In particular, automated CBFE minimization by message passing in a factor graph supports refined optimization of the accuracy vs resource consumption balance.


\subsection{Reactive vs procedural coding style}\label{sec:Reactive-vs-Procedural}

Next, we discuss a key technological component for a synthetic AIF agent, namely the requirement to execute FE minimization through a \emph{reactive} programming paradigm. 

%This is the real issue that hinders progress in applying autonomous AIF agents to serious design tasks. As previously discussed, the potential advantages of automated algorithm design by AIF agents are extremely impressive, but how are we going to implement real-time FE optimization in a high-dimensional AIF agent that samples the world at a sub-millisecond sampling rate?  

%In order to tackle this huge computational challenge, it is important to realize that high-dimensional models are almost per definition sparse. It is certainly true for the brain and most likely also for any useful high-dimensional model. For instance, a first-order Markov assumption in any dynamic model implies that future states are independent of past states, given the present states. Similarly, in an image or video processing model, pixel coloration almost always depends only on a local neighborhood of pixels. If we were to represent such a sparse model by a graph where independent processing modules\footnote{By independent processing modules we mean modules that do not share any variables.} are not connected by an edge, then variational FE minimization in that graph leads always to an inference algorithm that admits an interpretation as Message Passing (MP) between connected modules on that graph. Formally, a graph representation of a factorized model with missing connections between independent processing modules is called a factor graph, and FE minimization on a factor graph leads to well-known inference algorithms such as belief propagation, variational message passing and expectation propagation \parencite{winn_variational_2005,loeliger_factor_2007, chen_factor_2018, senoz_variational_2021}. 

A crucial feature of all MP-based inference is that the inference process consists entirely of a (parallelizable) series of small steps (messages) that individually and independently contribute to FE minimization. As a result, a message passing-based FE minimization process can be interrupted \emph{at any time} without loss of important intermediate computational results.    

In a practical setting, it is very important that an ongoing inference process can be robustly (without crashing) interrupted at any time with a result. These intermediate inference results can only be reliably retrieved if the inference process iteratively updates its beliefs in small steps, or, in other words, by message passing. Moreover, the inference process should not be subject to a prescribed control flow that contains for-loops. Rather, if we were to write code for an anytime-interruptable inference process in a programming language, we should use a \emph{reactive} rather than the more common \emph{procedural} programming style. In a reactively coded inference engine, there is no code for control flow, such as ``do first this, then that'', but instead only a description of how a processing module (a factor graph node) should react to changes in incoming messages. We will call this process \emph{Reactive Message Passing} (RMP) \cite{bagaevReactiveMessagePassing2023}.  In an RMP inference process, there is no prescribed schedule for passing messages such as the Viterbi or Bellman algorithm. Rather, an RMP inference process just \emph{reacts} by FE minimization whenever FE increases due to new observations. 

In Fig.~\ref{fig:AIF-algorithms}, we display the consequences of choosing a reactive programming style for an application engineer like Sarah. The procedural programming style in Algorithm-1 requires Sarah to provide the control flow (the ``procedure'') for the inference process. Sarah needs to write code for when to collect observations, when to update states, etc. The specific control flow in Algorithm-1 is just an example and there exists literature that aims to improve the efficiency of the control flow \cite{champion_realizing_2021, friston_sophisticated_2021}. In order to write an efficient inference control flow recipe for a complex AIF agent, Sarah needs to be an absolute expert in this field. 

Consider in contrast the code for reactive inference in Algorithm-2. In a reactive programming paradigm, there is no control flow. Rather, the only inference instruction is for the agent to react to any opportunity to minimize FE. When FE minimization is executed by a reactive message passing toolbox, the application engineer only needs to specify the model.

Aside from lowering the competence bar for application engineers to design effective AIF agents, the procedural style of implementing FE minimization is fundamentally inappropriate. The control flow in Algorithm-1 necessarily contains many design choices that only become known during deployment. For instance, how far should the agent roll out its model to the future for computing the EFE? This kind of information is highly contextual and not available to the application engineer. In contrast, the application engineer's code for reactive inference ("react to any FEM opportunity") works for any model in any context. In a reactive inference setting, the appropriate planning horizon is going to be continually updated (inferred) with contextual information. In other words, it is the reactive FEM process itself that leads to optimizing the inference control flow.

% Figure environment removed


%These interrupts may be caused by unintentional failures such as a sensor or internal module crash, or they may be of intentional origin. For example, an edge or node in a graph can be pruned for the purpose of model structure adaptation. Alternatively, an ongoing MP-based FE minimization process may be interrupted because new observations change the FE patterns in the graph in such a way that further iterations of the old FE minimization process are no longer appropriate. 



%\footnote{If brain tissue is not fed by sensory signals, it will die \wk{This is quite a strong statement. Do you have a reference?}\bdv{No, I dont. Is it not true?} since the FE functional, which can be decomposed into ``model complexity minus prediction accuracy'', simplifies to the model complexity term when there is no signal to predict. Minimal model complexity is obtained when there is no processing and hence the brain will die.} 
%The actual algorithm that the biological neural network executes, in the form of firing rates of action potentials, %\wk{An action potential is just a digital $1$ (as opposed to "no spike = $0$"). I would relate messages to "firing rate" or "membrane potential oscillation".}\bdv{OK fair point. phrasing updated}
%depends on sensory signals, which are actively selected by the brain's control signals (like turning the head to look into a different direction), which in turn depend on past sensory inputs etc. In other words, the executed inference algorithm in the brain is a result of interactions between the brain and its environment, and \emph{cannot be predicted from a specification of the brain alone}. 

%In Table~\ref{tab:natural-vs-engineered-design}, we included robustness, real-time processing and low power consumption as distinct advantages of natural design systems over engineered systems. We will shortly discuss how these features emerge as a consequence of using RMP for FE minimization. 

\subsection{RMP for robustness}\label{sec:robustness}

Since an AIF agent executes under situated conditions, it must perform the FE minimization process robustly in real-time.  Consider an agent whose computational resources are represented by a graph and FE minimization results from executing MP-based inference on that graph. Any MP schedule that visits the nodes in the graph in a prescribed fixed order (as would be the case in a procedural approach to FE minimization) is vulnerable to malfunction in any of the nodes in the schedule. In principle, the FE minimization process needs to stop after such a malfunction and proceed to compute a new MP schedule. Since FE minimization is the \emph{only} ongoing computational process, the robot basically moves blindfolded after a reset. Clearly, for robustness, we need a system that continues to minimize FE, even after parts of the graph break down over time. In a reactive inference framework, collapse of a component is simply a switch to an alternative model structure. The new model may perform better or worse at FE minimization, but there is no reason to stop processing. %In fact, in this framework, collapsing a tiny fraction of nodes on an ongoing basis may be considered a model exploration feature. 

\subsection{RMP for real-time, situated processing}\label{sec:real-time-processing}

An ongoing RMP process can always be interrupted when computational resources have run out on a given platform. In this way, by trading computational complexity (i.e., the number of messages) for accuracy, any RMP-based inference process can be scaled down to a real-time processing procedure, where of course a prediction accuracy price may have to be paid, depending on the available computational resources. In short, FE minimization in any model can be executed in real-time on any computational platform if we implement inference by RMP in a factor graph. 

%MP is resilient to adversarial attacks and in principle supports both intentional and unintentional interrupts of the inference process.

%Active inference agents process sensory data in real-time. Perceptual processing relates to Bayesian filtering of sensory data following a Kalman filtering update scheme. 


\subsection{RMP for low power consumption}\label{sec:low-power-consumption}

Similarly, an ongoing RMP process can always be terminated if the expected improvement in accuracy does not outweigh the expected computational load that additional messages would incur.\footnote{The computational load and complexity can only be equated in the absence of a Von Neumann bottleneck (i.e., with mortal computation or in-memory processing). This is because energy and time are ‘wasted’ by reading and writing to memory. \label{fnlabel}} Note that, since FE decomposes as computational complexity minus accuracy, interrupting an RMP-based inference process for this reason is fully consistent with the goal of FE minimization.

%As previously discussed, the FE functional decomposes into a computational cost term (often named the ``complexity'', defined as the KLD between posterior and prior distributions over the latent variables) minus prediction accuracy (i.e., the expected log-likelihood). Minimizing FE therefore always aims to maximize data accuracy for any given amount of processing costs. 
Interrupting an ongoing MP process by any of the above-mentioned reasons (e.g., node malfunction, running out of computational resources, expected processing costs outweighing expected accuracy gains, etc.), in principle always leads to sacrificing some prediction accuracy in favor of saving computational costs. Crucially, these interrupts will not cause a system-wide crash in a reactive system.

%The possibility to interrupt the FE minimization process at any time opens the door to solving another serious engineering dilemma. Many complex algorithms are first developed on powerful desktop computers, followed by a ``porting" phase to the actual low-power product platform. Commonly, porting of a complex engineered algorithm to a platform with much lower computational resources requires deep insights into the algorithm itself, since some modules will have to be deleted or re-designed to behave similarly for lower costs. In contrast, if the algorithm would be designed as an inference task in a probabilistic model, then the same RMP-based declarative inference code would work without modifications on both platforms. On the low power platform, the RMP process would, due to power constraints, get interrupted earlier than the RMP process on the high power platform, leading naturally to a solution with lower accuracy. Hence, the same model with the same declarative code for RMP leads to different solutions for different computational platforms. Importantly, in this framework, \emph{porting algorithms  between platforms with very different computational constraints does not require any human expert knowledge}. 

%As a final comment on low power consumption of natural design methods, we note that RMP-based realizations of AIF agents tend to consume less power as time moves on. In the initial stages of algorithm design, the generative model will not yet be very accurate and lots of large prediction errors will be generated. These prediction errors will generate energy-consuming messages to realize FE minimization over states, parameters and even the model structure. As time moves on, the structural and parameter adaptation processes will converge to accurate-enough models and only state inference in the lower layers will be necessary. In the context of learning how to ride a bike, in the beginning stages, full (power-hungry) attention is needed to learn the model, whereas once we have learned a proper model, riding the bike is an almost unconscious exercise that can easily be combined with other cognitive tasks.   

