\section{The Free Energy Principle and Active Inference}\label{sec:FEP-and-AIF}


\subsection{FEP for synthetic AIF agents}

The Free Energy Principle (FEP) describes  self-organizing behavior in persistent natural agents (such as a brain) as the minimization of an information-theoretic functional that is known as the variational Free Energy (FE).\footnote{For reference, we use the following abbreviations in this paper: Active Inference (AIF), Constrained Bethe Free Energy (CBFE), Expected Free Energy (EFE), (variational) Free Energy (FE), Free Energy Principle (FEP), Free Energy Minimization (FEM), Message Passing (MP), Reactive Message Passing (RMP).} Essentially, the FEP is a commitment to describing adaptive behavior by Hamilton's Principle of Least Action \cite{ lanczos_variational_1986}. The process of executing FE minimization in an agent that interacts with its environment through both active and sensory states is called \emph{Active Inference} (AIF). Crucially, the FEP claims that, in natural agents, FE minimization is \emph{all that is going on}. While engineering fields such as signal processing, control, and machine learning are considered different disciplines, in nature these fields all relate to the same computational mechanism, namely FE minimization. 

For an engineer, this is good news. If we wish to design a synthetic AIF agent that learns purposeful behavior solely through self-directed environmental interactions, we can focus on two tasks:
\begin{enumerate}
    \item Specification of the agent's model and inference constraints. This is equivalent to the specification of a (constrained) FE functional. 
    \item A recipe to continually minimize the FE in that model under situated conditions, driven by environmental interactions.
\end{enumerate}

We are interested in the development of an engineering toolbox to support these two tasks. 


%In this context, the FE is a performance function of beliefs, i.e., probabilistic functions over appropriate values for the variables in the model (the ``brain'') under consideration. FE minimization leads to maximizing the performance of the brain's model as a prediction engine for future sensory inputs. The process of FE minimization seeks to maximize both prediction accuracy and model simplicity, thus leading to \emph{inference algorithms that maximize data prediction accuracy while minimizing the amount of required computations}.

%This FE decomposition into accuracy plus simplicity underlies the brain's capacity to attend to many tasks simultaneously at a very low power consumption rate. 

%FE minimization in an AIF agent is equivalent to performing variational Bayesian inference in that agent, which involves updating the agent's beliefs about the state of the world. The brain continually updates beliefs about the state of the world but it will do so with minimal required adaptations to previously held beliefs.  Perception (signal processing in engineering terms) is just FE minimization with respect to synaptic firing rates that represent the state of the world. Learning (in engineering terms called: machine learning or parameter estimation) minimizes FE in relation to the synaptic efficacies, and movement (in engineering terms: control) is due to FE minimization with respect to firing rates in synapses that connect to muscle spindles. Updating beliefs by FE minimization about any aspect of the model is always accomplished continually and simultaneously. Thus, in a FEM agent, we cannot consider signal processing, control, and machine learning separately as they are part of the same process in the same model. \\


%The application of FEP and AIF to algorithm design leads by itself (i.e., independent of the implementation methods) to at least two of the six remarkable benefits of the natural design approach, namely, (1) learning from small data sets, and (2) a one-solution approach to all problems. Next, we shortly recapitulate these ideas.  


\subsection{FEM for simultaneous refinement of problem representation and solution proposal}\label{sec:one-solutiona-approach}

An important quality of the robot will be to define tasks for itself and solve these tasks autonomously. Here, we shortly discuss how the FEP supports this objective.  

Consider a generative model $p(x,s,u)$, where $x$ are observed sensory inputs, $u$ are latent control signals and $s$ are latent internal states. For notational ease, we collect the latent variables by $z = \{s,u\}$. The variational FE for model $p(x,z)$ and variational posterior $q(z)$ is then given by 
\begin{subequations}
\begin{align}
    F[q,p] 
    &= \underbrace{-\log p(x)}_{\text{surprise}} + \underbrace{\sum_z q(z) \log \frac{q(z)}{p(z|x)}}_{\text{bound}}  \label{eq:bound-evidence}\\
    &= \underbrace{\sum_z q(z) \log \frac{q(z)}{p(z)}}_{\text{complexity}} - \underbrace{\sum_z q(z) \log p(x|z)}_{\text{accuracy}} \label{eq:complexity-accuracy}\,.
\end{align}
\end{subequations}
The FE functional in \eqref{eq:bound-evidence} can be interpreted as the sum of surprise (negative log-evidence) and a non-negative bound that is the Kullback-Leibler divergence between the variational and the optimal (Bayesian) posterior. The first term, surprise, can be interpreted as a performance score for the problem representation in the model. This term is completely independent of any inference performance issues. The second term (the bound) scores how well actual solutions are inferred, relative to optimal (Bayesian) inference solutions. In other words, the FE functional is a universal cost function that can be interpreted as the sum of problem representation and solution proposal costs. FE minimization leads toward improving both the problem representation and solving the problem through inference over latent variables. In particular, FE minimization over a particular model structure $p$ should lead to nested sub-models that reflect the causal structure of the sensory data. Sub-tasks are solved by FE minimization in these sub-models. Hence, both creation of subtasks and solving these subtasks are driven solely by FE minimization. 

%serves as a universal cost function that is only low when both the problem representation and the inference process are "good enough".   

In conclusion, a high-end toolbox should be capable to minimize FE both over (beliefs over) latent variables through adaptation of $q(z)$ (leading to better solution proposals for the current model $p$), and over the model structure $p$ (leading to a better problem representation). 

As an aside, an interesting consequence of the FE decomposition into problem plus solution costs is that a relatively poor problem representation with a superior inference process may be preferred (evidenced by lower FE), over a model with a good problem representation (high Bayesian evidence) where inference costs are high. The notion that the model with the largest Bayesian evidence may not be the most useful model in a practical application, casts an interesting light on the common interpretation of FE as a mere upper bound on Bayesian evidence. We argue here that FE is actually a more principled performance score for a model, since in addition to Bayesian model evidence, FE also scores the performance loss in a model due to an inaccurate inference process. 

\subsection{AIF for smart data sets and resource management}

If we want the robot to cope with unknown physical terrain conditions, it is not sufficient to pre-train the robot offline on a large set of relevant examples. The robot must be able to acquire relevant new data and update its model under real-world conditions.  

FE minimization in the generative model's roll-out to the future results in the minimization of a cost functional known as the Expected Free Energy (EFE). It can be shown that the EFE decomposes into a sum of pragmatic (goal-driven, exploitation) and epistemic (information-seeking, exploration) costs \cite{friston_active_2015}. As a result, inferred actions balance the need to acquire informative data (to learn a better predictive model) with the goal to reach desired future behavior. 

In contrast to the current AI direction towards training larger models on larger data sets, an active inference process elicits an optimally informative, small (``smart'') data set for training of just ``good-enough'' models to achieve a desired behavior. AIF agents adapt enough to accomplish the task at hand while minimizing the consumption of resources such as energy, data, and time. The trade-off between data accuracy and resource consumption is driven by the decomposition in \eqref{eq:complexity-accuracy} of FE as a measure of complexity minus accuracy. According to this decomposition, more accurate models are only pursued if the increase in accuracy outweighs the resource consumption costs.   

In short, AIF agents that are driven solely by FE minimization will inherently manage their computational resources. These agents automatically infer actions that elicit appropriately informative data to upgrade their skills toward good-enough performance levels. Since both the agent and environment mutually affect each other in a real-time information processing loop, it would not be possible to acquire the same data set through the sampling of the environment without the agent's participation. 




%, just enough to  FE minimization leads to models that are good enough for the task at hand, adapt in real-time,  drives There is an inherent drive toward This is nature's technological edge that underlies learning from actively in-situ selected smart data sets, rather than training from large data bases in an offline fashion. 
