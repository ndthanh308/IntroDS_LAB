\section{Numerical tests}
\label{sec:results}

In this section, some numerical tests\footnote{The numerical results presented in this section have been obtained using a PC with CPU Intel$^{\mbox{\tiny{\textregistered}}}$ Core\texttrademark\; i5-11400 @ 2.60GHz and 8GB RAM.} are presented to assess the performance of the proposed DD-PGD method, considering three elliptic problems: a diffusion problem with synthetic solution in two subdomains (Sect.~\ref{sec:testAnalytical}), a convection-diffusion problem with parametric geometry and two subdomains in Sect.~\ref{sec:testRozza}, and a \rev{thermal problem featuring multiple subdomains, each with a different bulk conductivity, in Sect.~\ref{sec:testPatera}.}

The local parametric subproblems are solved using the encapsulated PGD toolbox~\cite{Diez:2020:ACME}. Unless otherwise specified, a tolerance $\varepsilon = 10^{-4}$ is selected for the PGD enrichment process and the redundant information is then eliminated by the PGD compression algorithm~\cite{DM-MZH:15} with tolerance $\varepsilon^\star = 10^{-3}$. 

In the offline phase, the meshes employed for the spatial discretization are problem-dependent and they are specified in the corresponding sections. It is worth noticing that, for all considered numerical tests, the local meshes in the spatial subdomains are conforming with the interfaces (i.e., the interfaces do not cut through any elements of the meshes) and they coincide in the overlapping regions. The one-dimensional parametric intervals $\mathcal{I}^p$, $p=1,\ldots,P$ and $\mathcal{J}_i^q$, $q=1,\ldots,N_{\Gamma}$ are discretized using uniform elements.
%
In the online phase, the interface system is solved using GMRES~\cite{Saad:1986:SISSC}, with tolerance $10^{-6}$ on the relative residual. 

\rev{
\begin{rem}
The number of active boundary parameters to be employed in each subproblem is case-dependent.
%
Whilst a large set of active boundary parameters reduces the number of subproblems to be solved, the performance of the PGD-ROM algorithm can be negatively affected in the presence of high-dimensional problems. Indeed, the implementation used to generate the results in this section requires fewer modes to converge when fewer parameters are considered at the interface, thus leading to computational gains when local problems are solved in parallel.
%
In this context, subproblems with only one active boundary parameter can also be devised. This case is equivalent to computing local surrogate models setting unitary Dirichlet boundary conditions at each node of the interface, and appropriately scaling and combining the resulting PGD solutions during the online phase. The two approaches provide comparable solutions in terms of the accuracy of the global surrogate model, the number of PGD modes and the overall computing time, and they are not reported here for brevity.
%
In order to reduce the number of local surrogate models to be determined whilst also avoiding high-dimensional problems, in the rest of this section, problems of type~\eqref{eq:boundaryProbs} are solved with at most three active boundary parameters. 
%
However, further research is required to understand how to optimally choose the sets of active boundary parameters in the DD-PGD framework (see, e.g., optimal port spaces in RBE~\cite{Patera-SP-16}), in order to balance the accuracy of the local surrogate models, the computational complexity of each subproblem and their number. 
\end{rem}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Diffusion problem with synthetic solution in two subdomains}
\label{sec:testAnalytical}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This model problem aims to assess the convergence and accuracy of the proposed method. Let $\Omega = (0, 2) \times (0, 1)$ be the spatial domain and $\mu \in \mathcal{P} = [1,50]$ be a scalar parameter defining a space-dependent conductivity coefficient. The parametric Poisson equation under analysis is: for all $\mu \in \mathcal{P}$, find $u(\mu)$ such that 
%
\begin{equation}
    \label{eq:numTestsModel}
	\begin{array}{rcll}
		- \nabla \cdot ((1 + \mu x) \nabla u(\mu)) &=& s(\mu) & \quad \text{in } \Omega \, ,\\
		u(\mu) &=& 0 & \quad \text{on } \partial\Omega \, ,\\
	\end{array}
\end{equation}
%
where the source term $s(\mu)$, detailed in Appendix~\ref{append:Setup}, is selected such that the analytical solution of the problem is ${u}(\mu) = {u_{\text{ex}}}(\mu)$ with
%
\begin{equation}
    \label{eq:numTests1analytical}
    {u_{\text{ex}}}(\mu) := \sin(2\pi x)\sin(2\pi y) + \frac{\mu}{2}xy(y - 1)(x - 2)\,.
\end{equation}

For the spatial discretization, continuous $\mathbb{Q}_1$ Lagrangian finite elements are employed on a structured grid with local mesh size $\hX = 5 \times 10^{-2}$. The domain $\Omega$ is split into the two overlapping subdomains $\Omega_1 = (0, 1 + n\hX) \times (0, 1)$ and $\Omega_2 = (1 - n\hX, 2) \times (0, 1)$, where $n \in \mathbb{N}$ is a positive value that controls the size $2n\hX$ of the overlap. For instance, if $n = 1$ the width is equal to $2\hX$, that is, the overlap region contains two layers of elements. The mesh of each subdomain contains a total of 420 quadrilateral elements.

Following the definition of the spatial mesh, 19 nodes are considered as auxiliary boundary parameters $\Lambda_i^q$ at the interfaces $\Gamma_1 = \{1+n\hX\}\times(0,1)$ and $\Gamma_2 = \{1-n\hX\}\times(0,1)$,  excluding the nodes at $\Gamma_1 \cap \partial\Omega$ and $\Gamma_2 \cap \partial\Omega$. Since $|u(\mu)|<10$ for all $\mu\in\mathcal{P}$ and for all $(x,y)\in\Omega$, the range of admissible values for $\Lambda_i^q$ is set to $\mathcal{J}_i^q = [-10, 10]$, for $i=1,2$ and for $q=1,\ldots,19$. All parametric domains (i.e., $\mathcal{P}$ and $\mathcal{J}_i^q$, for $i=1,2$ and for $q=1,\ldots,19$) are discretized by equally-spaced nodes, with grid spacing $\hMu = \hLam = 10^{-3}$, leading to $\Nmu = 4.9 \times 10^{4}$ nodes in $\mathcal{P}$ and $\Nlam = 2 \times 10^{4}$ nodes in $\mathcal{J}_i^q$, for all $q$.

Each local problem is thus 22-dimensional, featuring two spatial dimensions, one scalar physical parameter $\mu$ and 19 boundary parameters $\Lambda_i^q$ at the interface. To reduce the dimensionality of the problem, the interface boundary parameters are partitioned into sets of at most \rev{3} active parameters. Hence, for each subdomain, one problem of type~\eqref{eq:sourceProb} of dimension 3 and 6 problems of type~\eqref{eq:boundaryProbs}, with dimension at most \rev{6}, are identified.

The PGD solutions of the local parametric subproblems are formed by at most $116$ modes, and the total computational time of the offline phase is approximately \SI{98}{s}.
%
\rev{More precisely, the convergence of the PGD enrichment is described in Fig.~\ref{fig:PGDAmps}, reporting the amplitude of the computed modes scaled by the amplitude of the first mode, for all subproblems in each subdomain. It is worth noticing that, in each subdomain, subproblem 7  only contains 2 active boundary parameters, whence the significant reduction of required modes with respect to the remaining subproblems. Moreover, the}
%
PGD compression reduces the maximum number of modes required for a local problem to $59$, with an extra execution time in the offline phase of \SI{13}{s}.
%
\rev{
For the sake of brevity, the convergence of the PGD enrichment is only presented for this test case, while for the following problems only the total number of modes required in each subdomain is detailed and the presented results only report the cost of the compressed local PGD solutions.
}

% Figure environment removed

Setting the maximum dimension of the Krylov subspace to 6 in the GMRES algorithm, the DD-PGD strategy is considered to evaluate the global solution for $\mu=3$ and $\mu=30$. The PGD solutions, computed with overlap of width $2\hX$ (i.e., $n=1$), are plotted in Fig.~\ref{fig:testAnalyticalSolutions}.
%
% Figure environment removed

The convergence of the GMRES relative residuals as a function of the iterations is shown in Fig.~\ref{fig:testAnalyticalResidual}, for different widths of the overlapping region. 
%
The results display that fewer iterations are needed when the width of the overlap increases from $2\hX$ (i.e., $n=1$) to $6\hX$ (i.e., $n=3$), a typical behaviour for the one-level Schwarz method, see, e.g.,~\cite{Smith:1996}.

% Figure environment removed

For the overlap width $2\hX$ and for $\mu = 3$, the online phase of the DD-PGD approach requires 17 iterations to achieve the user-defined tolerance (see Fig.~\ref{fig:testAnalyticalResidual}, left), with a computational time of approximately \SI[parse-numbers=false]{4 \times 10^{-2}}{s}. To assess the cost of this procedure, a standard DD-FEM approach is executed using the same overlap width, for 50 random values of the parameter $\mu$. The high-fidelity DD-FEM requires an average of 9 iterations to converge to the specified tolerance, with a mean computing time of \SI{1.85}{s}. 
%
Although the DD-PGD requires a larger number of GMRES iterations to achieve the prescribed tolerance, likely as a consequence of the loss of information occurring during the ROM construction, it is worth noticing that the method is still approximately 46 times faster than the standard DD-FEM procedure, owing to the real-time evaluation of the local surrogate solutions.

Next, the accuracy of the proposed methodology is assessed for the previously selected values of the parameter $\mu$. More precisely, Table~\ref{tab:numTest1Errors} reports the relative error, measured in the $L^2(\Omega)$ norm, between the analytical solution $u_{\text{ex}}$ and the DD-PGD solution $\upgd$, the monolithic PGD solution $\upgd_{\Omega}$ computed on the entire domain $\Omega$ without partitioning it into $\Omega_1$ and $\Omega_2$ and the high-fidelity monolithic finite element solution $u^h_{\Omega}$. 
%
The results clearly display that the DD-PGD strategy provides a solution with global accuracy comparable both to the monolithic ROM and the high-fidelity solution. In particular, it is worth remarking that the monolithic PGD surrogate model $\upgd_{\Omega}$ involves the solution of spatial problems almost twice as large as the ones appearing in the DD-PGD framework, with an overall computing time of \SI[parse-numbers=false]{4.5 \times 10^{-1}}{s}, contrary to the \SI[parse-numbers=false]{4 \times 10^{-2}}{s} of the DD-PGD approach.

Finally, Fig.~\ref{fig:testAnalyticalError} reports the map of the scaled nodal error $|\upgd(\mu) - u_{\text{ex}}(\mu)| / \max_{\Omega} |u_{\text{ex}}(\mu)|$ in the entire domain $\Omega$ for different values of $\mu$, indicating that no loss of accuracy is experienced by the DD-PGD solution in the neighbourhood of the overlapping region.

\begin{table}[h!]
    \centering
    \begin{tabular}{cccc}
        $\mu$ & $\frac{\|\upgd(\mu) - u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}{\|u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}$ &  $\frac{\|\upgd_{\Omega}(\mu) - u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}{\|u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}$ & $\frac{\|u^h_{\Omega}(\mu) - u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}{\|u_{\text{ex}}(\mu)\|_{L^2(\Omega)}}$ \\
        \hline
         3 & $9.27 \times 10^{-3}$ & $9.00 \times 10^{-3}$ &  $9.07 \times 10^{-3}$ \\
         30 & $3.26 \times 10^{-3}$ & $3.27 \times 10^{-3}$  &  $3.27 \times 10^{-3}$ \\
         \hline
    \end{tabular}
    \caption{Relative error in $L^2(\Omega)$ norm between the analytical solution $u_{\text{ex}}(\mu)$ and the DD-PGD solution $\upgd(\mu)$, the monolithic PGD solution $\upgd_{\Omega}(\mu)$ and the monolithic finite element solution $u^h_{\Omega}(\mu)$.}
    \label{tab:numTest1Errors}
\end{table}

% Figure environment removed


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Poiseuilleâ€“Graetz flow in a geometrically parametrized domain}
\label{sec:testRozza}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this section, a convection-diffusion equation in a parametrized domain, presented in~\cite{Pacciarini:2014:CMAME}, is studied. 
%
This benchmark describes a channel, with walls maintained at different temperatures, in which heat convection and conduction phenomena are combined. 

The test features two parameters: a physical one, $\mu_1$, describing the inverse of the diffusion coefficient, and a geometric one, $\mu_2$, controlling the size of the domain $\Omega = (0, 1 + \mu_2) \times (0, 1)$ (see Fig.~\ref{fig:PGflowDom}). 
%
% Figure environment removed

For all $\bmu = (\mu_1, \mu_2) \in \mathcal{P}$, with $\mathcal{P} = [10^4, 2 \times 10^4] \times [0.5, 4]$, a solution $u(\bmu)$ is sought to fulfil the problem
%
\begin{equation}\label{eq:pgProblem}
    \begin{array}{rcll}
    \displaystyle
         -\frac{1}{\mu_1} \,\Delta u(\bmu) + \balpha \cdot \nabla u(\bmu) &=& 0 &\quad \text{in } \Omega(\bmu)\, ,\\
         u(\bmu) &=& 0 &\quad \text{on } \Gamma^{D, 1}(\bmu)\, ,\\
         u(\bmu) &=& 1 &\quad \text{on } \Gamma^{D, 2}(\bmu)\, ,\\
         \displaystyle\frac{1}{\mu_1}\nabla u(\bmu)\cdot \bn(\bmu) &=& 0 &\quad \text{on } \Gamma^N(\bmu) \, ,
    \end{array}
\end{equation}
%
where $\balpha = (\alpha_1,\alpha_2)^T = (4y(1 - y) , 0 )^T$ is a parameter-independent, horizontal velocity field and the boundaries of the parametric domain are such that
%
\begin{align*}
    \Gamma^{D, 1}(\bmu) &= \{0\} \times [0, 1] \cup [0, 1] \times \{0\} \cup [0, 1] \times \{1\} \,,\\
    \Gamma^{D, 2}(\bmu) &= [1, 1 + \mu_2] \times \{0\} \cup [1, 1 + \mu_2] \times \{1\}\, ,\\
    \Gamma^N(\bmu) &= \{1 + \mu_2\} \times [0, 1] \, .
\end{align*}

The domain $\Omega(\bmu)$ is partitioned into two overlapping regions, the parameter-independent subdomain $\Omega_1 = [0, 1.05] \times [0, 1]$ and the parametric subdomain $\Omega_2(\bmu) = [1, 1 + \mu_2] \times [0, 1]$. 
%
In order to devise a surrogate model for the geometrically parametrized subdomain $\Omega_2(\bmu)$, a formulation based on a reference domain configuration~\cite{Ammar-AHCCL-14} is employed. This is achieved by introducing a parameter-independent reference subdomain $\hOmega_2 = [0,1] \times [0,1]$ and an appropriate parametric mapping $\Map : \hOmega_2 \times \mathcal{I}^2 \rightarrow \Omega_2(\bmu)$ to transform the fixed reference subdomain into the parametric physical one.

Structured grids containing 540 and 1,600 quadrilaterals are defined in $\Omega_1$ and $\hOmega_2$, respectively, with non-uniform mesh size near the top and bottom walls, as detailed in Fig.~\ref{fig:meshSize}. 
%
% Figure environment removed

Since the second component of the convective velocity is null, the local P\'eclet number in the horizontal direction (i.e., $\mathrm{Pe} = |\alpha_1| h_{x,1} \mu_1 /2$, $h_{x,1}$ being the local mesh size in the horizontal direction) is displayed in Fig.~\ref{fig:Peclet} for different values of the diffusion coefficient $1/\mu_1$. For all the considered values of the parameter $\mu_1$, problem~\eqref{eq:pgProblem} is convection-dominated. Hence, following~\cite{Huerta-GCCDH-13}, a continuous $\mathbb{Q}_1$ Lagrangian finite element formulation with streamline upwind Petrov-Galerkin (SUPG) stabilization is implemented in the PGD surrogate model. Details of the SUPG formulation on a parameter-independent reference domain are provided in Appendix~\ref{append:Graetz}.

% Figure environment removed

The interface boundary parameter intervals are selected as $\mathcal{J}_i^q = [-5, 5]$, for $i=1,2$ and for all $q$. All parametric intervals are discretized using equidistributed nodes, with spacing $h_{\mu_1} = 10^{-1}$ and $h_{\mu_2} = h_{\Lambda} = 10^{-3}$, leading to $N^{\mu_1} = 10^{5}$, $N^{\mu_2} = 3.5 \times 10^{3}$ and $\Nlam = 10^{4}$ parametric unknowns in $\mathcal{I}^1$, $\mathcal{I}^2$ and $\mathcal{J}_i^q$, respectively.

For each subdomain, one subproblem of type~\eqref{eq:sourceProb} is solved. Moreover, 7 subproblems of type~\eqref{eq:boundaryProbs} are solved in $\Omega_1$, with three active interface boundary parameters, whereas in subdomain $\hOmega_2$, 10 such subproblems are formulated, with at most two active interface boundary parameters. This choice follows from problems in subdomain $\hOmega_2$ being higher dimensional, due to the presence of the geometric parameter $\mu_2$.

The DD-PGD strategy constructs local surrogate models featuring 56 modes in $\Omega_1$ and 619 modes in subdomain $\hOmega_2$. After compression, 38 and 164 modes are obtained for $\Omega_1$ and $\hOmega_2$, respectively, for a total CPU time of approximately \SI[parse-numbers=false]{6.88 \times 10^{3}}{s}.
%
It is worth noticing that the imbalance in the computational effort in $\Omega_1$ and $\hOmega_2$ stems from the second subdomain featuring two parameters, with one of them (i.e., $\mu_2$) controlling the geometric transformation of the domain.
%
Indeed, it is well known that parametric variations of the geometry and the concurrent presence of multiple parameters cause a significant increase in the complexity of parametric problems, thus in the number of terms required in the PGD approximations~\cite{Giacomini-GBSH-21}.
%
In this context, the DD-PGD strategy is particularly appealing as it allows to separately treat the portion of the domain with multiple parameters and complex dynamics by computing a large number of local modes (i.e., 619 in $\hOmega_2$), while only 56 terms need to be determined in the region $\Omega_1$.

\rev{
To assess the capability of the DD-PGD strategy to accurately compute the solution of the parametric problem \eqref{eq:pgProblem}, the online evaluation of the surrogate model $\upgd(\bmu)$ is presented, together with the scaled error map $| \upgd(\bmu) - u_{\Omega}^h(\bmu) | / \max_{\Omega} |u_{\Omega}^h(\bmu)|$, $u_{\Omega}^h(\bmu)$ being the corresponding monolithic finite element solution. Two pairs of values $(\mu_1, \mu_2)$ are tested: Fig.~\ref{fig:PGFlow_12500_3} shows the case $\bmu = (1.25 \times 10^4, 3)$, reproducing the results in~\cite{Pacciarini:2014:CMAME}, while Fig.~\ref{fig:PGFlow_20000_1} the case $\bmu = (2 \times 10^4, 1)$. In both cases, the outcome of the DD-PGD algorithm provides a smooth solution, showing excellent agreement with the high-fidelity simulation. Indeed, the maximum value of the scaled error mentioned above achieves $2 \times 10^{-3}$ for $\bmu = (1.25 \times 10^4, 3)$ and $3 \times 10^{-3}$ for $\bmu = (2 \times 10^4, 1)$.
}

% Figure environment removed

% Figure environment removed

Setting the maximum dimension of the Krylov subspace for the GMRES algorithm to 8, the computed DD-PGD solutions converge in 10 and 12 iterations for $\bmu = (1.25 \times 10^4, 3)$ and $\bmu = (2 \times 10^4, 1)$, respectively, with an average computing time of \SI[parse-numbers=false]{1.45 \times 10^{-1}}{s}.
%
The online scheme accurately glues the local PGD solutions, achieving a maximum difference in $l^\infty$ norm in the overlapping region of the order of $10^{-3}$.
%
For the sake of comparison, the standard DD-FEM approach, executed for 100 random pairs $(\mu_1, \mu_2)$, requires an average of 14 iterations to converge with an average CPU time of \SI{18.13}{s}.
%
Hence, the surrogate-based overlapping Schwarz method is approximately 118 times faster than the standard DD-FEM strategy, showcasing that the DD-PGD method provides a competitive framework, also in the presence of convection-dominated phenomena and geometrically parametrized domains.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Multi-domain thermal problem with discontinuous conductivity}
\label{sec:testPatera}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The last test case discusses the benchmark problem introduced in~\cite{Eftang:2013:IJNME} of the parametric thermal equation
%
\begin{equation}\label{eq:PateraProblem}
    \begin{array}{rcll}
        -\nabla \cdot (\nu(\bmu) \nabla u(\bmu)) &=& 0 & \quad \text{in } \Omega \, ,\\
        u(\bmu) &=& 0 & \quad \text{on } \Gamma_{\text{out}} \, , \\
        \nu(\bmu)\nabla u(\bmu) \cdot \bn &=& 1  & \quad \text{on } \Gamma_{\text{in}} \, ,\\
        \nabla u(\bmu) \cdot \bn &=& 0 & \quad \text{on } \partial\Omega \setminus (\Gamma_{\text{in}} \cup \Gamma_{\text{out}}) \, ,
    \end{array}
\end{equation}
%
where $\Omega$ is the modular structure consisting of 9 subdomains shown in Fig.~\ref{fig:patera9} and $\nu(\bmu)$ denotes the space-dependent thermal conductivity
%
\begin{equation}\label{eq:conductivity}
\nu(\bmu) = \begin{cases}
\mu_i & \text{in $\Omega_i^b$ ($i =1,\ldots,9$)}, \\
1 & \text{otherwise} \, .
\end{cases}
\end{equation}
%
% Figure environment removed

The geometric configuration of each subdomain is reported in Fig.~\ref{fig:pateraMesh}. A structured mesh of 2,080 quadrilateral elements is considered, with uniform size $\hX^b = 5 \times 10^{-2}$ in the bulk region (pink), non-uniform mesh size with horizontal spacing $h_{x,1}^v = 5 \times 10^{-2}$ and vertical spacing $h_{x,2}^v = 1.25 \times 10^{-2}$ in the vertical wings (yellow), whereas in the horizontal wings (blue) the spacing in the $x$ and $y$ directions is given by $h_{x,1}^h = 1.25 \times 10^{-2}$ and $h_{x,2}^h = 5 \times 10^{-2}$, respectively.
%
% Figure environment removed

It is worth noticing that the resulting problem features two spatial dimensions and 9 independent physical parameters, one for each subdomain, leading to a problem of dimension 11. Hence, the construction of a monolithic ROM for equation~\eqref{eq:PateraProblem} is particularly challenging. Nonetheless, as observed in Sect.~\ref{sec:testRozza}, the DD-PGD approach can be utilized to partition the domain into subregions where only a subset of the parameters is relevant. 
%
Following the rationale originally proposed in~\cite{Eftang:2013:IJNME}, the modular nature of the domain is exploited to partition $\Omega$ into 9 regions, each featuring a problem with a unique physical parameter $\hmu$.

The domain $\Omega$ is thus split into 9 overlapping subdomains $\Omega_i$ ($i=1,\ldots,9$) with overlap width equal to $2.5 \times 10^{-2}$. \rev{Note that the overlapping regions are located in the horizontal and vertical wings of the subdomains, where the thermal conductivity is maintained constant and equal to 1, avoiding any possible issue related to overlaps with discontinuous material properties.} Considering the type of conditions imposed on the boundary of each subdomain, four reference subdomains $\hOmega_j$ ($j=1,\ldots,4$) can be identified that, upon appropriate translation and/or rotation, can be used to describe all subdomains $\Omega_i$ ($i=1,\ldots,9$) thus reconstructing the original domain $\Omega$. The reference subdomains are shown in Fig.~\ref{fig:patera9Ref} with the corresponding imposed boundary conditions. When no condition is specified, a homogeneous Neumann boundary condition is applied. Table~\ref{tab:onlineTransformations} reports the transformations that must be applied to $\hOmega_j$ ($j=1,\ldots,4$) to retrieve $\Omega_i$ ($i=1,\ldots,9$).
%
% Figure environment removed

\begin{table}[h!]
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{lccccccccc}
            & $\Omega_1$  & $\Omega_2$  & $\Omega_3$  & $\Omega_4$  & $\Omega_5$  & $\Omega_6$  & $\Omega_7$  & $\Omega_8$  & $\Omega_9$ \\
\hline \\[-1em]
Ref. subdomain & $\hOmega_1$ & $\hOmega_2$ & $\hOmega_2$ & $\hOmega_2$ & $\hOmega_3$ & $\hOmega_2$ & $\hOmega_4$ & $\hOmega_2$ & $\hOmega_1$ \\[0.25em]
Translation & $\begin{pmatrix}0 \\ 0\end{pmatrix}$ & $\begin{pmatrix} 1.5 \\ 0\end{pmatrix}$ & $\begin{pmatrix}3 \\ 0\end{pmatrix}$ & $\begin{pmatrix}0 \\ 1.5 \end{pmatrix}$ & $\begin{pmatrix} 1.5 \\ 1.5 \end{pmatrix}$ & $\begin{pmatrix} 3 \\ 1.5\end{pmatrix}$ & $\begin{pmatrix} 0 \\ 3\end{pmatrix}$ & $\begin{pmatrix} 1.5 \\ 3 \end{pmatrix}$ & $\begin{pmatrix} 3 \\ 3 \end{pmatrix}$ \\[1em]
Rotation    & $\pi$ & $\pi$ & $\displaystyle\frac{3\pi}{2}$ & $\displaystyle\frac{\pi}{2}$ & 0 & $\displaystyle\frac{3\pi}{2}$ & 0 & 0 & 0\\[0.5em]
Case study \#1: $\mu_i=$ & 0.1 & 0.2 & 0.4 & 0.8 & 1.6 & 3.2 & 6.4 & 0.1 & 0.2 \\
Case study \#2: $\mu_i=$ & 4.9 & 4.7 & 4.8 & 5.2 & 5 & 4.9 & 5.5 & 5.3 & 5.1 \\
\hline
\end{tabular}}
\end{center}
\caption{Transformation of the reference subdomains $\hOmega_j$ ($j=1,\ldots,4$) to obtain the physical subdomains $\Omega_i$ ($i=1,\ldots,9$), and conductivity parameters $\mu_i$ in the physical subdomains for two case studies.}
\label{tab:onlineTransformations}
\end{table}

The physical parameter $\mu_i$ is set to vary in the interval $\mathcal{P} = [5 \times 10^{-2}, 10]$, while all interface boundary parameters belong to the interval $\mathcal{J}_i^q = [-5, 5]$, for all $i$ and $q$. The parametric intervals are discretized using equally-space nodes with mesh size $h_{\mu} = \hLam = 10^{-3}$, yielding a total of $N^{\mu} = 9.95 \times 10^3$ and $\Nlam = 10^4$ unknowns in $\mathcal{P}$ and $\mathcal{J}_i^q$, respectively.

The spatial problem is discretized using continuous $\mathbb{Q}_2$ Lagrangian finite element functions. For each subdomain, Table~\ref{tab:Patera} reports the resulting number of finite element degrees of freedom after taking into account the boundary conditions, the number of physical and boundary parameters and the total number of PGD modes computed for the local surrogate models. 
%
Due to the presence of non-homogeneous boundary data only on $\Gamma_{\text{in}}$, the problem of type~\eqref{eq:sourceProb} needs to be formulated solely in the reference subdomain $\hOmega_4$. Concerning the problems of type~\eqref{eq:boundaryProbs}, the active boundary parameters are partitioned to ensure that each subproblem features at most three parameters, yielding 14 subproblems in the reference subdomain $\hOmega_1$, 21 in $\hOmega_2$, 28 in $\hOmega_3$ and 14 in $\hOmega_4$. 
%
The overall computational time of the offline phase is \SI{630}{s}, with additional \SI{228}{s} required to perform the PGD compression.
%
\begin{table}[bht]
\begin{center}
\begin{tabular}{cccccc}
Reference & Finite & Physical & Interface boundary & PGD & Modes after\\
subdomain & element DOFs & parameters & parameters\footnotemark & modes & compression\\
\hline
$\hOmega_1$ & 2,163 & 1 & 42 & 370 & 236\\
$\hOmega_2$ & 2,142 & 1 & 63 & 672 & 386\\
$\hOmega_3$ & 2,121 & 1 & 84 & 930 & 509\\
$\hOmega_4$ & 2,163 & 1 & 42 & 419 & 244\\
\hline
\end{tabular}
\end{center}
\caption{Specifics of the local subproblems and of the computed local PGD surrogate models for each reference subdomain.}
\label{tab:Patera}
\end{table}
%
\footnotetext{When the reference subdomain $\hOmega_2$ is employed for the computation of the physical subdomain $\Omega_3$, only 42 interface boundary parameters are considered, the remaining 21 being fixed by imposing the homogeneous Dirichlet condition on $\Gamma_{\text{out}}$.}

In the online phase, the solution in the global domain $\Omega$ is reconstructed by composing the local PGD surrogate models computed in the reference subdomains $\hOmega_j$ ($j=1,\ldots,4$) with the appropriate translations and rotations described in Table~\ref{tab:onlineTransformations}. In total, there are 12 overlapping regions between the nine subdomains, identified by the dashed lines in Fig.~\ref{fig:patera9}, and the dimension of the resulting interface system is 504.

Two case studies associated with the different values of the conductivity parameters reported in Table~\ref{tab:onlineTransformations} are considered.
%
Case study \#1 reproduces the benchmark in~\cite{Eftang:2013:IJNME}.
%
Setting the maximal dimension of the Krylov subspace equal to 60, GMRES performs 302 iterations, converging in approximately \SI{19.13}{s}. The corresponding DD-FEM solver requires 95 GMRES iterations to converge, with a total CPU time of \SI{414.8}{s}. As already observed in the previous numerical experiments, despite the larger number of GMRES iterations, the DD-PGD method still outperforms the standard DD-FEM procedure in terms of computing time, with an approximate speed-up of 22 times.

Figure~\ref{fig:patera9comparison} displays the temperature distribution $\upgd(\bmu)$ obtained using the DD-PGD strategy and \rev{the scaled  error $| \upgd(\bmu) - u^h(\bmu) | / \max_{\Omega} |u^h(\bmu)|$ for the case study \#1. The maximum value of the scaled error is $2.5 \times 10^{-2}$.}
Moreover, the difference between the local surrogate models in the overlapping region, measured in the $l^\infty$ norm, is equal to $5 \times 10^{-9}$, showcasing the accurate imposition of the continuity of solution in the online phase of the DD-PGD method, even in the case of partitions involving multiple subdomains. 

% Figure environment removed
%
% Figure environment removed

A second test is carried out considering the alternative choice of conductivity parameters identified as case study \#2 in Table~\ref{tab:onlineTransformations}. This choice avoids large variations of the conductivity between adjacent subdomains and guarantees that the diffusion operator is far from becoming singular.
%
In this case, the convergence behaviour of GMRES significantly improves for both the DD-PGD and the DD-FEM approaches. More precisely, for DD-PGD, GMRES converges in 126 iterations with an average CPU time of \SI{8.32}{s}, while, for DD-FEM, convergence is achieved in \SI{249}{s} and 56 iterations. Hence, for case study \#2, the DD-PGD method is approximately 30 times faster than the standard DD-FEM procedure. The comparison of the DD-PGD and DD-FEM temperature distributions for case study \#2 is presented in Fig.~\ref{fig:pateraAlt9comparison}, showing the excellent accuracy of the ROM solution \rev{for which the maximum value of the scaled error $| \upgd(\bmu) - u^h(\bmu) | / \max_{\Omega} |u^h(\bmu)|$ is $1.5 \times 10^{-3}$.}

The improvement in the convergence behaviour observed in case study \#2 suggests that the robustness of the iterative method can be enhanced by introducing \emph{ad-hoc} preconditioning strategies to tackle both the variations in the physical parameters and the presence of multiple subdomains. This optimization of the online phase, which is beyond the contributions of this paper, could be achieved following several approaches from domain decomposition, see, e.g., \cite{Toselli:2005,Dolean-DJN-15} and references therein.