\section{EVALUATION}

\subsection{Experimental Setup}


% Figure environment removed

\begin{table*}[!t]
  \centering
  \caption{Experimental results of \tool. The best results are highlighted in bold. The second-best results are underlined.}
  \renewcommand\arraystretch{0.95}
  \label{tab:exp1}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Acc./emo}} & \multirow{2}{*}{\textbf{Acc./gen}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}}$(MB)} & \multicolumn{3}{c|}{\textbf{\text{Latency}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy}}(mJ)} \\
    \cline{6-11}
    & & & & &
    \textbf{Nano} & \textbf{AGX} & \textbf{Orin} & 
    \textbf{Nano} & \textbf{AGX} & \textbf{Orin} \\
    \midrule
    SISO/img-emo & 62.45 & N/A  & 11.82  & 1003.49 & 6.24 & 7.46 & 2.42 & 49.30 & 298.40 & 134.56  \\
    SISO/img-gen & N/A  & 77.40 & 11.82  & 1000.12 & 6.04  & 7.50 & 2.38 & 47.60  & 298.02 & 131.76 \\
    SISO/aud-emo & 61.81 & N/A  & 11.82  & 983.49 & 3.04 & 4.30 & 1.92 & 24.20  & 171.90 & 71.62  \\
    SISO/aud-gen & N/A  & \textbf{88.24} & 11.82  & 956.92 & 2.98  & 4.16 & 1.88 & 23.62  & 166.42 & 70.12  \\
    MISO/emo & 73.04 & N/A   & 25.51  & 1362.10 & 10.08 & 8.30 & 3.36 & 80.24  & 348.60 & 161.28  \\
    MISO/gen & N/A  & 77.43 & 25.51  & 1342.87 & 9.78  & 8.22 & 3.30 & 77.76  & 343.58 & 158.60  \\
    \midrule
    MIMO & 74.26 & 83.99 & 25.51  & 1364.96 & 5.26 & 4.15 & 2.95 & 39.98  & 175.29 & 81.90  \\
    \midrule
    \textbf{MIMONet (FP32)} & 76.28 & 83.17 & 0.92 & 910.11 & 5.21 & 3.96 & 2.84 & 40.88  & 45.93 & 66.17  \\
    \textbf{MIMONet (FP16)} & \underline{76.29} & 83.16 & 0.92 & 688.15  & 5.15 & 3.72 & 2.81 & 40.42  & 43.15 & 61.54 \\
    \textbf{MIMONet (INT8)} & \textbf{76.40} & 84.48 & \underline{0.92} & \underline{577.17} & \underline{5.11}  & \underline{3.66} & \underline{2.74} & \underline{39.96}  & \underline{41.34} & \underline{59.98} \\
    \textbf{MIMONet (INT4)} & 75.72 & \underline{87.01} & \textbf{0.92} & \textbf{521.68} & \textbf{5.02}  & \textbf{3.61} & \textbf{2.70} & \textbf{39.26}  & \textbf{40.07} & \textbf{59.01}  \\
    \bottomrule
    \end{tabular}
}
\vspace{-3mm}
\end{table*}%
We set up four baselines for comparison with \tool: two SISO models and two MISO models, both using ResNet-18 as the backbone. In MISO models, ResNet-18 extracts features from each input (image and audio), which are then concatenated and passed through three fully connected layers for prediction. \tool differs by having multiple predictors after the fully connected layers, enabling it to generate multiple predictions in a single forward pass. We also apply approximations like half-precision (FP16) and post-training quantization (INT8 and INT4).

We leverage the RAVDESS dataset~\cite{RAVDESS}, with 7356 multimodal clips from 24 actors, to evaluate our method’s performance in multi-input scenarios, specifically for concurrent emotion and gender predictions. Some examples are shown in Fig.~\ref{fig:data_example}. We use 80\% of the data (19 actors) for training and 20\% (the other 5 actors) for testing. All experiments are run three times, with averaged results reported.
Such tasks are pivotal for developing robots designed for human-robot interactions, applicable in healthcare for patient monitoring by interpreting emotions, in customer service for personalized assistance, and in education to tailor teaching strategies based on emotional and demographic insights.

% We choose the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset~\cite{RAVDESS} to evaluate our proposed method in multi-modal input scenarios. There are 7356 video files in the RAVDESS, among which 24 professional actors (12 male and 12 female), are recorded in the database vocalizing two sentences that are lexically similar in a neutral North American accent. Therefore, this dataset is suitable for MIMO scenarios. Specifically, our MIMO model receives images and audio as inputs and generates emotion prediction together with gender prediction as outputs. As shown before, the emotion and gender prediction outputs can be utilized to develop interactive robots that can interpret and respond to human emotions and gender in a more personalized and natural way. Some examples of this dataset are shown in Fig.~\ref{fig:data_example}.


% \begin{table*}[!hbpt]
%   \centering
%   \caption{Experimental results of MIMO model's effectiveness. \zx{describe why parameter number reduces a lot but end-to-end memory consumption reduces not that much: PYTORCH framework memory overhead.}  \zx{describe why after quant int8 the performance even increases:: Regularization Effect: Quantization can introduce a form of noise to the weights and activations of a neural network, which can have a regularizing effect similar to techniques like dropout. This can help the model generalize better to unseen data by reducing overfitting to the training set.
% Clipping Outliers: Quantization reduces the precision of the numerical values in a model by mapping a range of values to a single quantized value. This can effectively clip outlier values that might be due to noise or anomalies in the data, leading to more robust model predictions.
% Improved Noise Robustness: Quantization can make the model less sensitive to small variations in input data, which can be beneficial for noisy datasets, leading to improved performance.}}
%   \renewcommand\arraystretch{1.1}
%   \label{tab:exp1}
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|ccccc|ccc|ccc}
%     \toprule
%     \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Acc./emo}} & \multirow{2}{*}{\textbf{Acc./gen}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}}$(MB)} & \multirow{2}{*}{$\textbf{\text{\# FP}}$} & \multicolumn{3}{c|}{\textbf{\text{Latency/FP}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy/FP}}(J)} \\
%     \cline{7-12}
%     & & & & & &
%     \textbf{Nano} & \textbf{AGX} & \textbf{TX2} & 
%     \textbf{Nano} & \textbf{AGX} & \textbf{TX2} \\
%     \midrule
%     SISO/img-emo & 62.45 & N/A  & 11.82  & 1003.49 & 2 & 329.71 & 100.57 & 303.48 & 1.12 & 1.32 & 1.59 \\
%     SISO/img-gen & N/A  & 77.40 & 11.82  & 1000.12 & 2 & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     SISO/aud-emo & 61.81 & N/A  & 11.82  & 1003.49 & 2 & 329.71 & 100.57 & 303.48 & 1.12 & 1.32 & 1.59 \\
%     SISO/aud-gen & N/A  & \underline{97.41} & 11.82  & 1000.12 & 2  & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     \midrule
%     MISO/emo & 73.04 & N/A   & 25.51  & 1362.10 & 2 & 412.17 & 150.23 & 900.20 & 1.58 & 2.03 & 4.69\\
%     MISO/gen & N/A  & 77.43 & 25.51  & 1342.87 & 2  & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     \midrule
%     \textbf{MIMO (FP32)} & 76.28 & 83.17 & 0.92 & 910.11 & 1 &  289.18 & 82.06 & 160.57 & 0.94 & 1.07 & 0.91\\
%     \textbf{MIMO (FP16)} & 76.29 & 83.16 & 0.92 & 688.15 & 1 & 289.18 & 82.06 & 160.57 & 0.94 & 1.07 & 0.91\\
%     \textbf{MIMO (INT8)} & \textbf{76.40} & 84.48 & 0.92 & 577.17 & 1 & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     \textbf{MIMO (INT4)} & 75.72 & \textbf{87.01} & \textbf{0.92} & \textbf{521.68} & \textbf{1}  & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     \bottomrule
%     \end{tabular}
% }
% \end{table*}%

% \begin{table*}[!hbpt]
%   \centering
%   \vspace{-1mm}
%   \caption{Ablation study results of accuracy and on-device efficiency.} 
%   \renewcommand\arraystretch{1.1}
%   \label{tab:exp2}
%   \resizebox{\textwidth}{!}{
%     \begin{tabular}{l|cccc|ccc|ccc}
%     \toprule
%     \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Acc./emo}} & \multirow{2}{*}{\textbf{Acc./gen}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}}$(MB)} & \multicolumn{3}{c|}{\textbf{\text{Latency}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy}}(J)} \\
%     \cline{6-11}
%     & & & & &
%     \textbf{Nano} & \textbf{AGX} & \textbf{TX2} & 
%     \textbf{Nano} & \textbf{AGX} & \textbf{TX2} \\
%     \midrule
%     \textbf{MIMO} & \textbf{76.40} & \textbf{84.48} & \textbf{0.92} & \textbf{577.17} & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     w/o Pruning & 74.26 & 83.99   & 0.92 & 1964.73 & 289.18 & 82.06 & 160.57 & 0.94 & 1.07 & 0.91\\
%     w/o MTZ & 75.63 & 82.70 & 1.07  & 930.09 & 226.97 & 80.54 & 173.64 & 0.75 & 1.04 & 0.94\\
%     w/o PTQ & 76.28 & 83.17 & 0.92 & 910.11 & 289.18 & 82.06 & 160.57 & 0.94 & 1.07 & 0.91\\
%     % \midrule
%     % \midrule
%     % \textbf{+PTQ} &
%     % 74.55 & 0.92 & 470.29 & x & x & 63.15 & x & x & x & 120.20\\
%     % \textbf{+INT8 Quant} &  x & x & x & x & x & x & x & x & x & x\\
%     \midrule
%     \textbf{MIMO} & \textbf{76.40} & \textbf{84.48} & \textbf{84.48} & \textbf{577.17} & x.xx  & x.xx & x.xx & x.xx  & x.xx & x.xx  \\
%     w/ RandPruning & 12.66  & 51.22 & 0.92  & 579.22 & 288.12  & 82.13 & 166.21 & 0.93 & 1.07 & 0.95  \\
%     \bottomrule
%     \end{tabular}
% }
% \vspace{-5mm}
% \end{table*}%

% \subsubsection{Implementation Details} 
% For the SISO, MISO, and MIMO models without pruning, we set the learning rate to 1e-4 for Adam optimizer~\cite{Adam} and train 2000 epochs to ensure training stability and convergence. All models in this paper are trained from scratch without using extra data for fair comparisons. We adjust the learning hyperparameters of VIB to balance weighted loss values into the same order of magnitude, specifically, we select the Adam optimizer with a weight decay rate of 5e-5 and set the learning rate to 1e-3, IB learning rate to 1e-3, and KL parameter to 1e-7. We set merge degree to 0.9 as default in~\cite{MTZ}. All of our implementations are based on native PyTorch~\cite{pytorch}. Quantization is implemented by referring to native PyTorch dynamic post-training quantization. System performance of NVIDIA devices is measured by tegrastats profiler~\cite{bib:tegrastats}. We train all DNNs on a server with an Intel E5-2650 CPU  and a GeForce RTX 2080 Ti GPU. 
% Inference experiments are conducted on embedded devices based on NVIDIA Jetson Developer Kits (Nano, AGX Xavier, and AGX Orin)
% \footnote{NVIDIA embedded devices equip integrated GPU with different data processing costs from independent GPU on x86 architecture PC or server. To ensure fair comparisons, all inferences experiments are run on integrated GPU with maximal power mode.}
% , which are widely used as the mainboard of various commercial robots, e.g., Duckiebot~\cite{Duckiebot(DB-J)}, SparkFun Jetbot~\cite{SparkFun_JetBot}, Waveshare Jetbot~\cite{Waveshare_JetBot}.

\subsection{Metrics}

We study the following five metrics to evaluate the accuracy and on-device efficiency of \tool. 1) Accuracy: the ratio of correctly identified samples in the test set to the total number of samples in the test set. Note that the accuracies of the two tasks (emotion and gender) are evaluated separately. 2) Parameter number: number of learnable parameters. 3) Memory: required runtime memory for models. 4) Latency: average wall-clock time required to perform one forward prediction. 5) Energy: We measure system-wide energy consumption. For a fair comparison, we calculate the average latency and energy consumption for 10,000 forward passes. We aim to improve on-device efficiency (reduce parameter number, latency, memory, and energy) as much as possible while maintaining the model's effectiveness (accuracy).

\subsection{Effectiveness}
\label{sec:effectiveness}


 % \zx{describe why parameter number reduces a lot but end-to-end memory consumption reduces not that much: PYTORCH framework memory overhead.}  
 % \zx{describe why after quant int8 the performance even increases:: Regularization Effect: Quantization can introduce a form of noise to the weights and activations of a neural network, which can have a regularizing effect similar to techniques like dropout. This can help the model generalize better to unseen data by reducing overfitting to the training set. Clipping Outliers: Quantization reduces the precision of the numerical values in a model by mapping a range of values to a single quantized value. This can effectively clip outlier values that might be due to noise or anomalies in the data, leading to more robust model predictions. Improved Noise Robustness: Quantization can make the model less sensitive to small variations in input data, which can be beneficial for noisy datasets, leading to improved performance.}

 
\begin{table*}[!t]
  \centering
  \vspace{-1mm}
  \caption{Ablation study results of accuracy and on-device efficiency. The best results are in bold.} 
  \renewcommand\arraystretch{0.98}
  \label{tab:exp2}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|cccc|ccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Acc./emo}} & \multirow{2}{*}{\textbf{Acc./gen}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}}$(MB)} & \multicolumn{3}{c|}{\textbf{\text{Latency}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy}}(mJ)} \\
    \cline{6-11}
    & & & & &
    \textbf{Nano} & \textbf{AGX} & \textbf{Orin} & 
    \textbf{Nano} & \textbf{AGX} & \textbf{Orin} \\
    \midrule
    \textbf{MIMONet} & \textbf{76.40} & \textbf{84.48} & \textbf{0.92} & \textbf{577.17} & \textbf{5.11}  & \textbf{3.66} & \textbf{2.74} & \textbf{39.96}  & \textbf{41.34} & \textbf{59.98}  \\
    w/o VIB & 74.26 & 83.99   & 25.51 & 1364.96  & 5.26 & 4.15 & 2.95 & 39.98  & 175.29 & 81.90  \\
    w/o MTZ & 75.63 & 82.70 & 1.07  & 930.09 & 5.15 & 3.98 & 2.80 & 40.28 & 44.23  & 62.34 \\
    w/o PTQ & 76.28 & 83.17 & 0.92 & 910.11 & 5.32 & 3.96 & 2.84 & 40.88 & 45.93 & 66.17 \\
    \midrule
    \textbf{MIMONet} & \textbf{76.40} & \textbf{84.48} & \textbf{0.92} & \textbf{577.17} & \textbf{5.11}  & \textbf{3.66} & \textbf{2.74} & \textbf{39.96}  & \textbf{41.34} & \textbf{59.98}  \\
    w/ RandPruning & 12.66  & 51.22 & 0.92  & 579.22 & 5.15 & 3.71 & 2.77 & 40.43  & 42.23 & 60.12  \\
    \bottomrule
    \end{tabular}
}
\vspace{-6mm}
\end{table*}%

Table~\ref{tab:exp1} provides a comparative analysis of the \tool's performance against its SISO and MISO counterparts on the RAVDESS dataset. Observably, \tool and its variants with different precision (FP32, FP16, INT8, and INT4) consistently outperform SISO and MISO configurations across several metrics:

\begin{itemize}[leftmargin=*]
\item Accuracy: \tool models demonstrate superior performance, with the INT8 variant achieving the highest emotion recognition accuracy (Acc./emo) at 76.40\%, surpassing SISO and MISO models by 3.36\% and a relative gain of 4.6\%. The best gender recognition accuracy (Acc./gen) is 87.01\%, competitive with state-of-the-art results. Although SISO models excel in gender recognition using vocal features alone, adding image inputs can introduce variability, as facial cues are not always distinct. Nevertheless, \tool maintains excellent recognition ability even after compression due to ResNet’s efficient feature extraction and fusion of image and audio inputs. Compression techniques like INT8 further enhance efficiency by reducing resource requirements while preserving high accuracy, showcasing \tool’s adaptability in processing multi-modal data.


\item \textbf{Parameter number (Par.):} \tool largely reduce parameter number via reducing intra- and inter-task redundancies. It can reach a 96.4\% compression rate while achieving very competitive accuracy performance.
\item \textbf{Memory Usage (Mem.):} \tool demonstrates significant memory efficiency for memory-constrained robotic scenarios. For instance, while MISO/emo and MISO/gen configurations require about 1362.10 and 1342.87 MB of memory respectively, the \tool variants operate with markedly less, with the MIMO (INT4) requiring only 521.68 MB. Note that MIMO can operate both tasks simultaneously, unlike the two separate MISO models, which in total consume 2704.97 MB\footnote{The 2704.97 MB memory usage for two MISO models may double-count shared PyTorch framework memory. In practice, models can share some memory, reducing the total. However, \tool still provides substantial memory savings.}, representing approximately up to \textbf{80.7\%} reductions\footnote{Although the parameter count of \tool is reduced by up to 96.4\%, memory consumption is not reduced by as much. This is because the PyTorch framework reserves some internal memory and introduces memory overhead.} in memory requirements compared to MISO models.
\item \textbf{Latency (Latency/FP):} \tool shows better latency performance on all three embedded testbeds. Note that \tool could generate results of two tasks in one forward pass, while SISO or MISO requires two forward passes. Specifically, \tool exhibits speedup compared to MISO models up to 1.98x, 2.29x, and 1.23x on Nano, AGX, and Orin, respectively.
\item \textbf{Energy (Energy/FP):} \tool also demonstrates superior efficiency on all three embedded testbeds. Specifically, \tool exhibits energy saving compared to MISO models up to 2.01x, 8.64x, and 2.71x on Nano, AGX, and Orin, respectively.

\end{itemize}

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \textit{\textbf{Effective and Efficient on-device deployment}}: our experimental results demonstrate that \tool can address all challenges in Sec.~\ref{sec:overview} (limited memory, real-time constraints, and energy constraints) while not sacrificing and often improving accuracy performance.
\end{shaded}
\end{minipage}

\begin{comment}

Table~\ref{tab:exp1} illustrates the effectiveness of the proposed MIMO model from multiple perspectives. Firstly, it can be observed that the MISO model achieves significantly higher accuracy (10.8\% higher) than the Single-Input Single-Output (SISO) model. This finding demonstrates that utilizing information from multiple inputs can significantly improve the model's effectiveness. Secondly, our baseline MIMO model demonstrates a slightly higher accuracy (1.0\%) than the MISO model, indicating the effectiveness and feasibility of our baseline MIMO framework. By rationally designing this framework, we enhanced the model's accuracy. On the other hand, MIMO improves on-device efficiency in inference scenarios by reducing memory overhead and computation costs. Specifically, in our task scenario, MISO requires two passes of forward inference to obtain results consistent with MIMO. However, MIMO achieves the same results with only marginally higher memory, latency, and energy consumption than a single forward inference of MISO.

\end{comment}

% % Figure environment removed

\subsection{Ablation Study}
\label{sec:ablation}

We exhibit the ablation study results in Table~\ref{tab:exp2}. We interpret the results as follows:

\begin{itemize}[leftmargin=*]

\item \textbf{Efficacy of Reducing Intra-Task Redundancy (VIB):} Removing VIB increases memory usage from 577.17 MB to 1364.96 MB and parameters from 0.92 million to 25.51 million, highlighting its effectiveness in reducing intra-task redundancy. Unlike random pruning, which largely degrades accuracy, VIB retains relevant information and reduces noise, making it crucial and particularly work for managing MIMO tasks efficiently.

\item \textbf{Efficacy of Reducing Inter-Task Redundancy (MTZ):} Omitting MTZ reduces accuracy in emotion and gender recognition, increases memory usage, and raises latency slightly, demonstrating its importance in minimizing inter-task redundancy and maintaining accuracy.

\item \textbf{Efficacy of Post-Training Quantization (PTQ):} PTQ significantly reduces the model footprint through low-precision data structures, with minimal impact on accuracy. It decreases latency and energy consumption, enhancing post-training efficiency without major accuracy loss.

\end{itemize}


\begin{comment}
    
\subsubsection{Accuracy} We measure the effectiveness of models by accuracy. It can be observed that applying the extension of VIB and MTZ has almost no degradation in accuracy compared to the baseline MIMO model, outperforming the baseline MIMO model by 2.0\% and 1.0\%, respectively. This demonstrates that our \tool can conduct approximately lossless compression without sacrificing accuracy. This demonstrates that our \tool can conduct approximately lossless compression without sacrificing accuracy.

\subsubsection{On-device efficiency} We measure on-device efficiency by parameter number, memory, latency, and energy. 

\begin{itemize}[leftmargin=*]
   
\item Parameter number: Firstly, the experimental results demonstrate that our VIB extension can significantly reduce the parameter number by 95.8\% compared to our baseline MIMO model. These experimental results show that our single-model compression method yields outstanding performance in the compression of MIMO models.
Furthermore, our proposed~\tool can reduce the parameter number by 14.0\% on top of the VIB extension, reaching 96.4\% overall compression rate compared to the baseline MIMO model. These results demonstrate that optimizing the MIMO network structure for internal characteristics (multi-branching) can reduce cross-model redundancy and thus further improve compression performance.

\item Memory: In our experiments, we measure runtime memory as one metric to reflect on-device efficiency. It can be observed that \tool significantly reduces runtime memory use by 31.1\% compared to the MIMO model. Surprisingly, the MIMO model after compression (VIB extension or \tool) requires even less runtime memory than the SISO model (as shown in Table~\ref{tab:exp1}). These results demonstrate our solution's superiority for on-device deployments, especially for devices with limited memory. For instance, NVIDIA Jetson Nano has only 2 GB runtime memory without enabling swap memory. The limited memory indicates that for SISO, MISO, and baseline MIMO models, only one instance can be deployed on Nano. In contrast, two instances of \tool can be deployed on Nano simultaneously. 

\item Latency: With adding each component of our proposed \tool, latency performance gets significantly improved under all other scenarios over the baseline MIMO model. Specifically, \tool speed ups the baseline MIMO model by 1.44x, 1.80x, and 5.64x, 3.13x on Nano, AGX, TX2, and PC. It is worth noting that our proposed solutions perform better than the baseline MIMO model on cross-platform hardware. For instance, on the NVIDIA Jetson TX2, the baseline MIMO model requires an average of 905.57 milliseconds to perform a single forward inference, while our proposed \tool achieves the same result in just 160.57 milliseconds (5.64x speed-up). On an x86-64 PC, the baseline MIMO model requires an average of 16.91 milliseconds to perform a single forward inference, while our proposed \tool achieves the same result in just 5.40 milliseconds (3.13x speed-up).

\item Energy: \tool embodies superiority in energy saving. Specifically, \tool has 1.69x, 1.90x, and 5.18x energy saving than the baseline MIMO on Nano, AGX, and TX2, respectively. Furthermore, compared to SISO/MISO models that only generate one output per forwarding inference, our \tool framework produces multiple outputs in one forward inference, equivalent to performing multiple inferences by SISO/MISO models. For instance, on AGX, two forward inferences for the MISO model to generate two outputs consume 4.06 joules, while \tool generates two outputs consume 1.07 joules (3.79x energy saving).

\end{itemize}

\end{comment}



\begin{minipage}{0.45\textwidth}
\begin{shaded}
\textit{\textbf{Component Efficacy}}: The ablation study confirms the critical role of each \tool component in ensuring optimal on-device performance.
\end{shaded}
\end{minipage}

% \subsection{Parameter Study}
% We performed a series of parameter studies to assess the impact of different hyperparameters in \tool.
% As shown in Fig.~\ref{fig:param2}, we present scatterplots of accuracy and compression rate for different hyperparameters in \tool. We found that the IB learning rate parameter requires careful selection, with 1e-3 being optimal for this study. For the KL parameter, a lower value setting (less than or equal to 1e-7) produced models with high accuracy but less compression rate, while higher values resulted in higher compression rates but very low accuracy. Therefore, a value of 1e-7 was optimal for the KL parameter in this study. Additionally, we investigated the impact of the merge degree hyperparameter on \tool, and found that it is insensitive to hyperparameters, with nearly lossless compression in all cases. The learnable parameter number linearly decreases with an increase in the merge degree, and model accuracy ranges from 74.21\% to 75.80\%, with model accuracy after finetuning ranging from 75.12\% to 76.67\%.


% % Figure environment removed

\subsection{Case study on TurtleBot3}
\label{sec:case}
We conduct a case study using the TurtleBot3 robot to evaluate the real-world applicability of \tool. Specifically, we replace the TurtleBot3's main control device with a Jetson Nano and install an RGB camera, USB audio card, and speakers. Subsequently, a native speaker was invited to converse with the robot using happy tones while facial videos were captured. The robot will make sounds and specific movements based on the predictions. We evaluate three models (MISO, vanilla MIMO, and \tool) and find that MISO generated wrong results and exhibited the longest response time due to two-pass inferences. In contrast, vanilla MIMO and \tool both yield accurate results, while \tool yields significantly shorter response times (3.00x speedup than vanilla MIMO on average), which aligns with our evaluation (Sec.~\ref{sec:effectiveness} and Sec.~\ref{sec:ablation}). 

\section{Discussion on Future Directions}

Recent research on model compression has primarily focused on simpler models, often neglecting complex architectures like ResNet~\cite{ResNet}, Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, and complex graph-based models~\cite{GCN,SAGE,gatconv,afarin2023commongraph,gao2023mega,dong2022deep,dong2022utility,dong2023graph}. Neural architecture search offers promising methods for creating efficient designs suitable for rapid inference~\cite{zhang2023accuracy}. Our goal is to adapt \tool for these advanced architectures, optimizing them for on-device use by addressing latency, energy, and memory limitations in models~\cite{DBLP:journals/corr/abs-2208-10442,DBLP:journals/corr/abs-2205-06175,gao2024dlora,han2024parameter}. While our approach improves performance on GPU-based devices, integrating FPGAs~\cite{canis2011legup,zhang2021stealing}, autonomous vechicles~\cite{dong2023deep}, and emerging VR technologies~\cite{slocum2023going} into AI-optimized embedded systems highlights a shift towards software-hardware co-design~\cite{zhang2021hardware} for heterogeneous computing, paving the way for broader on-device model deployment. In complex and diverse multi-task models, compression gains might diminish with less overlap between complex multiple tasks. We leave this to future work, where we plan to explore adaptive compression strategies tailored for complex multi-task scenarios.