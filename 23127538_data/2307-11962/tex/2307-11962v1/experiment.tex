\section{EVALUATION}

\subsection{Experimental Setup}

\subsubsection{Baselines and Dataset} We set up four baselines, i.e., a SISO model, a MISO model, a baseline MIMO model, and a VIB-enhanced MIMO model (i.e., applying VIB to the baseline MIMO model). 
For the SISO model, we use one ResNet-18 as the backbone. For the MISO model, we use a ResNet-18 as the backbone to extract features for each input (image and audio) and concatenate features, followed by three fully connected layers to fuse features and generate multiple prediction results (emotion and gender). 
For the baseline MIMO model, we design the loss function as a weighted sum of cross entropy losses for each task and use the same model structure as the MISO model.

We choose the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset~\cite{RAVDESS} to evaluate our proposed method. There are 7356 video files in the RAVDESS (total size: 24.8 GB), among which 24 professional actors, 12 male, and 12 female, are recorded in the database vocalizing two sentences that are lexically similar in a neutral North American accent. Therefore, this dataset is suitable for MIMO scenarios. Specifically, our MIMO model receives images and audio as inputs and generates emotion prediction together with gender prediction as outputs. As shown before, the emotion and gender prediction outputs can be utilized to develop interactive robots that can interpret and respond to human emotions and gender in a more personalized and natural way. We show some example data in Fig.~\ref{fig:data_example} to illustrate the tasks corresponding to this dataset.

% Figure environment removed

\begin{table*}[!hbpt]
  \centering
  \caption{Experimental results of MIMO model's effectiveness.}
  \renewcommand\arraystretch{0.9}
  \label{tab:exp1}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|cccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Accuracy}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}} (\times 10^{6})$MB} & \multicolumn{4}{c|}{\textbf{\text{Latency}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy}}(J)} \\
    \cline{5-8}
    \cline{9-11}
    & & & & 
    \textbf{Nano} & \textbf{AGX} & \textbf{TX2} & \textbf{PC} & 
    \textbf{Nano} & \textbf{AGX} & \textbf{TX2} \\
    \midrule
    \textbf{SISO} & 62.45  & 11.82  & 1003.49 & 329.71 & 100.57 & 303.48 & 4.12 & 1.12 & 1.32 & 1.59 \\
    \textbf{MISO} & 73.22  & 25.51  & 1362.10 & 412.17 & 150.23 & 900.20 & 16.02 & 1.58 & 2.03 & 4.69\\
    \textbf{MIMO} & 74.26  & 25.51  & 1364.20 & 416.01 & 147.58 & 905.57 & 16.91 & 1.59 & 2.03 & 4.71\\
    \bottomrule
    \end{tabular}
}
\end{table*}%

\begin{table*}[!hbpt]
  \centering
  \vspace{-1mm}
  \caption{Ablation study results of accuracy and on-device efficiency. } 
  \renewcommand\arraystretch{0.9}
  \label{tab:exp2}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|ccc|cccc|ccc}
    \toprule
    \multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Accuracy}} & \multirow{2}{*}{$\textbf{\text{Par.}} (\times 10^{6})$} & \multirow{2}{*}{$\textbf{\text{Mem.}} (\times 10^{6})$MB} & \multicolumn{4}{c|}{\textbf{\text{Latency}}(ms)} & \multicolumn{3}{c}{\textbf{\text{Energy}}(J)} \\
    \cline{5-8}
    \cline{9-11}
    & & & & 
    \textbf{Nano} & \textbf{AGX} & \textbf{TX2} & \textbf{PC} & 
    \textbf{Nano} & \textbf{AGX} & \textbf{TX2} \\

    \midrule
    \textbf{MIMO} & 74.26  & 25.51  & 1364.20 & 416.01 & 147.58 & 905.57 & 16.91 & 1.59 & 2.03 & 4.71\\
    \textbf{+VIB} & 76.29 % (76.50)
    & 1.07  & 930.09 & 226.97 & 80.54 & 173.64 & 6.98 & 0.75 & 1.04 & 0.94\\
    \textbf{+MTZ} & % 74.68
    75.51 & 0.92 & 940.57 & 289.18 & 82.06 & 160.57 & 5.40 & 0.94 & 1.07 & 0.91\\
    % \textbf{+PTQ}
    % 74.55 & 0.92 & 470.29 & x & x & 63.15 & x & x & x & 120.20\\
    % \textbf{+INT8 Quant} &  x & x & x & x & x & x & x & x & x & x\\
    \bottomrule
    \end{tabular}
}
\vspace{-5mm}
\end{table*}%

\subsubsection{Implementation Details} 
For the SISO model, MISO model, and the baseline MIMO model, we set the learning rate to 1e-4 for Adam optimizer~\cite{Adam} and train 2000 epochs to ensure the stability and convergence of the training. All models in this paper are trained from scratch for fair comparisons and to eliminate the impact of additional data. For the implementation of VIB~\cite{VIB}, we adjust the original parameters of VIB to scale the two subsets of the weighted loss values into the same order of magnitude, thus achieving a better balance of compression rate and accuracy. Specifically, we select the Adam optimizer with a weight decay rate of 5e-5 and set the learning rate to 1e-3, IB learning rate to 1e-3, and KL parameter to 1e-7. For MTZ, we set merge degree to 0.9 as default in~\cite{MTZ}. All of our implementations are based on PyTorch~\cite{pytorch}. We measure system-level performance metrics on NVIDIA embedded devices by Tegrastats~\cite{bib:tegrastats}.

\subsubsection{Hardware Setup} We train all DNNs on a server with an Intel E5-2650 CPU  and a GeForce RTX 2080 Ti GPU. 
We conduct inference experiments on an x86-based PC with an Intel i7-10700K CPU and arm-based embedded devices. 
For embedded devices, we use NVIDIA Jetson Developer Kits (Nano, AGX Xavier, and TX2) \footnote{NVIDIA embedded devices use unified memory, which can introduce large biases in metrics executed on the GPU due to differences in data transfer cost. To ensure fair comparisons, all inferences are running on CPU.}, which are widely used as the mainboard of various robots, e.g., Duckiebot~\cite{Duckiebot(DB-J)}, SparkFun Jetbot~\cite{SparkFun_JetBot}, Waveshare Jetbot~\cite{Waveshare_JetBot}, etc.

\subsection{Metrics}

We study the following five metrics to evaluate the accuracy and on-device efficiency of \tool. 1) Accuracy: the ratio of correctly identified samples in the test set to the total number of samples in the test set. 2) Parameter number: number of learnable parameters. 3) Memory: required runtime memory for models. 4) Latency: average CPU time required to perform one forward prediction. 5) Energy: We measure system-wide energy consumption. For a fair comparison, we calculate the average energy consumption for each forward inference. We aim to improve on-device efficiency (reduce parameter number, latency, memory, and energy) as much as possible while maintaining the model's effectiveness (accuracy).

\subsection{Effectiveness of MIMO}
\label{sec:effectiveness}

Table~\ref{tab:exp1} illustrates the effectiveness of the proposed MIMO model from multiple perspectives. Firstly, it can be observed that the MISO model achieves significantly higher accuracy (10.8\% higher) than the Single-Input Single-Output (SISO) model. This finding demonstrates that utilizing information from multiple inputs can significantly improve the model's effectiveness. Secondly, our baseline MIMO model demonstrates a slightly higher accuracy (1.0\%) than the MISO model, indicating the effectiveness and feasibility of our baseline MIMO framework. By rationally designing this framework, we enhanced the model's accuracy. On the other hand, MIMO improves on-device efficiency in inference scenarios by reducing memory overhead and computation costs. Specifically, in our task scenario, MISO requires two passes of forward inference to obtain results consistent with MIMO. However, MIMO achieves the same results with only marginally higher memory, latency, and energy consumption than a single forward inference of MISO.

% Figure environment removed

% Figure environment removed

\subsection{Ablation Study}
\label{sec:ablation}
We show the ablation study results in Table~\ref{tab:exp2}. We interpret the results in the following two folds:

\subsubsection{Accuracy} We measure the effectiveness of models by accuracy. It can be observed that applying the extension of VIB and MTZ has almost no degradation in accuracy compared to the baseline MIMO model, outperforming the baseline MIMO model by 2.0\% and 1.0\%, respectively. This demonstrates that our \tool can conduct approximately lossless compression without sacrificing accuracy. This demonstrates that our \tool can conduct approximately lossless compression without sacrificing accuracy.

\subsubsection{On-device efficiency} We measure on-device efficiency by parameter number, memory, latency, and energy. 

\begin{itemize}[leftmargin=*]
   
\item Parameter number: Firstly, the experimental results demonstrate that our VIB extension can significantly reduce the parameter number by 95.8\% compared to our baseline MIMO model. These experimental results show that our single-model compression method yields outstanding performance in the compression of MIMO models.
Furthermore, our proposed~\tool can reduce the parameter number by 14.0\% on top of the VIB extension, reaching 96.4\% overall compression rate compared to the baseline MIMO model. These results demonstrate that optimizing the MIMO network structure for internal characteristics (multi-branching) can reduce cross-model redundancy and thus further improve compression performance.

\item Memory: In our experiments, we measure runtime memory as one metric to reflect on-device efficiency. It can be observed that \tool significantly reduces runtime memory use by 31.1\% compared to the MIMO model. Surprisingly, the MIMO model after compression (VIB extension or \tool) requires even less runtime memory than the SISO model (as shown in Table~\ref{tab:exp1}). These results demonstrate our solution's superiority for on-device deployments, especially for devices with limited memory. For instance, NVIDIA Jetson Nano has only 2 GB runtime memory without enabling swap memory. The limited memory indicates that for SISO, MISO, and baseline MIMO models, only one instance can be deployed on Nano. In contrast, two instances of \tool can be deployed on Nano simultaneously. 

\item Latency: With adding each component of our proposed \tool, latency performance gets significantly improved under all other scenarios over the baseline MIMO model. Specifically, \tool speed ups the baseline MIMO model by 1.44x, 1.80x, and 5.64x, 3.13x on Nano, AGX, TX2, and PC. It is worth noting that our proposed solutions perform better than the baseline MIMO model on cross-platform hardware. For instance, on the NVIDIA Jetson TX2, the baseline MIMO model requires an average of 905.57 milliseconds to perform a single forward inference, while our proposed \tool achieves the same result in just 160.57 milliseconds (5.64x speed-up). On an x86-64 PC, the baseline MIMO model requires an average of 16.91 milliseconds to perform a single forward inference, while our proposed \tool achieves the same result in just 5.40 milliseconds (3.13x speed-up).

\item Energy: \tool embodies superiority in energy saving. Specifically, \tool has 1.69x, 1.90x, and 5.18x energy saving than the baseline MIMO on Nano, AGX, and TX2, respectively. Furthermore, compared to SISO/MISO models that only generate one output per forwarding inference, our \tool framework produces multiple outputs in one forward inference, equivalent to performing multiple inferences by SISO/MISO models. For instance, on AGX, two forward inferences for the MISO model to generate two outputs consume 4.06 joules, while \tool generates two outputs consume 1.07 joules (3.79x energy saving).

\end{itemize}

\begin{minipage}{0.45\textwidth}
\begin{shaded}
    \textit{\textbf{Efficient on-device deployment}}: our experimental results demonstrate that \tool can address all challenges in Sec.~\ref{sec:overview} (limited memory, real-time constraints, and energy constraints) while not sacrificing and often improving accuracy performance.
\end{shaded}
\end{minipage}



\subsection{Parameter Study}
We performed a series of parameter studies to assess the impact of different hyperparameters in \tool. In Fig.~\ref{fig:param1}, we show the changes in compression ratio, training loss, and VIB loss during the training process. We observed that a decrease in the IB learning rate from 1e-1 to 1e-5 resulted in a slower increase in compression rate, as well as a slower decrease in training and VIB loss.

In Fig.\ref{fig:param2}, we present scatterplots of accuracy and compression rate for different hyperparameters in \tool. We found that the IB learning rate parameter requires careful selection, with 1e-3 being optimal for this study. For the KL parameter, a lower value setting (less than or equal to 1e-7) produced models with high accuracy but less compression rate, while higher values resulted in higher compression rates but very low accuracy. Therefore, a value of 1e-7 was optimal for the KL parameter in this study. Additionally, we investigated the impact of the merge degree hyperparameter on \tool~\cite{MTZ}, and found that it is insensitive to hyperparameters, with nearly lossless compression in all cases. The learnable parameter number linearly decreases with an increase in the merge degree, and model accuracy ranges from 74.21\% to 75.80\%, with model accuracy after finetuning ranging from 75.12\% to 76.67\% (baseline MIMO model accuracy is 74.26\%).

\subsection{Case study on TurtleBot3}
\label{sec:case}
We conduct a case study using the TurtleBot3 robot to evaluate the real-world applicability of \tool. Specifically, we replace the TurtleBot3's main control device with a Jetson Nano and install an RGB camera, USB audio card, and speakers. Subsequently, a native speaker was invited to converse with the robot using happy tones while facial videos were captured. The robot will make sounds and specific movements based on the predictions. We evaluate three models (MISO, baseline MIMO, and \tool) and find that MISO generated wrong results and exhibited the longest response time due to two-pass inferences. In contrast, baseline MIMO and \tool both yield accurate results, while \tool yields significantly shorter response times (3.00x speedup than baseline MIMO on average), which aligns with our evaluation (Sec.~\ref{sec:effectiveness} and Sec.~\ref{sec:ablation}). 

\subsection{Limitations and Discussion on Future Directions}

Recent works on model compression have primarily focused on feed-forward models, while ResNet~\cite{ResNet}, Transformer~\cite{DBLP:conf/nips/VaswaniSPUJGKP17}, and GNNs~\cite{GCN,SAGE,gatconv} are more complex network structures that have received less attention. To enable the deployment of state-of-the-art models like BEiT-3 and Gato on on-device scenarios, we intend to explore the application of \tool to these more recent complex network structures. BEiT-3~\cite{DBLP:journals/corr/abs-2208-10442} and Gato~\cite{DBLP:journals/corr/abs-2205-06175} do not currently consider on-device constraints and lack techniques that optimize latency, energy, and memory. Furthermore, while CPU-based embedded devices can benefit from our proposed approach, many AI-oriented embedded devices also have functional computing devices like FPGAs, TPUs, and NPUs that can be utilized to form heterogeneous systems on SoCs. We believe that optimizing the compression strategy considering the different characteristics of heterogeneous computing devices is a promising future direction for enabling on-device deployment for more general embedded devices.