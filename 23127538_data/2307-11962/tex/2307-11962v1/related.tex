

\section{BACKGROUND and RELATED WORK}
\subsection{MIMO Model}

Current multimodal methods that jointly solve different tasks across domains using a shared model are limited to processing only one task or modality at a time, mostly employing multi-input single-output (MISO)~\cite{DBLP:journals/corr/abs-2208-10442,DBLP:journals/corr/abs-2205-06175,DBLP:conf/iccv/HuS21} or single-input single-output (SISO)~\cite{deng2020self, meng2019neural, yang2022instinctive, su2022uncertainty} architectures that are difficult to compress and may cause memory and energy consumption issues when used in on-device embedded systems. Existing state-of-the-art models such as BEiT-3~\cite{DBLP:journals/corr/abs-2208-10442}, Gato~\cite{DBLP:journals/corr/abs-2205-06175}, and UniT~\cite{DBLP:conf/iccv/HuS21} are examples of such models. On the other hand, concurrent multi-input multi-output (MIMO) works in the field include UniAD~\cite{hu2022goal} and BEVFusion~\cite{liu2022bevfusion}, which propose state-of-the-art solutions for multiple autonomous driving tasks and multi-task multi-sensor fusion, respectively. However, these methods have significant memory and computation resource requirements, and BEVFusion addresses performance bottlenecks by proposing an efficient Bird's Eye View (BEV) pooling strategy for sensors. To address these challenges, our work focuses on compressing models to ensure real-world deployability.

\subsection{Model Compression for On-device Scenarios}

Renowned for their computational and memory requirements, modern DNNs, like the VGG-16 with over 130M parameters, can strain resource-constrained devices during inference in MIMO applications \cite{bib:PIEEE20:Deng}. Model compression methods, which reduce the parameter count without affecting accuracy, offer a solution. They can involve removing redundant parameters \cite{VIB, MTZ, bib:NIPS17:Dong, bib:NIPS93:Hassibi, bib:TMC21:Liu} or minimizing redundant precision \cite{bib:NIPS15:Courbariaux}.
Two subclasses of model compression exist single-model pruning and cross-model compression. The former compresses a single network by eliminating unnecessary operations and parameters, while the latter minimizes inter-model redundancy by identifying and removing shared parameters and operations across models \cite{bib:NIPS17:Dong, bib:NIPS93:Hassibi,VIB, bib:CVPR19:Molchanov,bib:PIEEE20:Deng, MTZ, bib:KDD21:He, bib:IJCAI18:Chou}. Notably, the Multi-Task Zipping (MTZ) approach has pioneered in cross-model compression by automating the merging of pre-trained DNNs \cite{MTZ}.
This study leverages the Variational Information Bottleneck (VIB) method, extending it for application to ResNet architecture. It further modifies MTZ to significantly decrease the total parameters in the proposed MIMO model.