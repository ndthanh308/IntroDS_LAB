\section{Conclusion}

In this paper, we proposed \ourmethod{}, an adaptation of diffusion models for motion synthesis which entangles the motion temporal-axis with the diffusion time-axis. This mechanism enables synthesizing arbitrarily long motion sequences in an autoregressive manner using a U-Net architecture. A unique aspect of our work is the notion of a \textit{stationary} motion buffer. Our framework continues to produce clean frames (i.e., progressing along the diffusion-time axis), without \textit{actually} incrementing the diffusion time.
The ability of our pipeline to continually generate motion along the diffusion axis is what enables our framework to robustly and continuously produce novel frames. Interestingly, the ability to naturally use diffusion in such an autoregressive fashion may have implications for other types of sequential data beyond motion, such as audio and video, or modalities where a sequential order can be defined, such as a patch-by-patch order for images.

Our system enables partially-clean-frame to be immediately (or near immediately) popped-off the motion buffer stack. However, a current limitation of our system is that computing a clean from from pure noise requires going through the chain of denoising diffusion.
In the future we are interested in leveraging ideas from DDIM~\cite{song2020denoising} to skip ahead during the denoising process to achieve even lower latency. In addition, our framework may enable future research in long-term text-conditioned motion generation. We are interested in exploring how high-level control may be coupled with low-level user-guidance for the task of long-term generation.

