\section{Experiments}
% Figure environment removed

In this section, we demonstrate the effectiveness of \ourmethod{} on several long-term generation tasks. We show several unique applications of our method, including the ability to plan for upcoming motion using \emph{guided generation}. We also evaluate our method through various comparisons and ablations. For additional qualitative results, please refer to the supplemental video.

% Figure environment removed

% Figure environment removed

% Figure environment removed


% Figure environment removed
\subsection{Implementation details}
Our \ourmethod{} framework is implemented with PyTorch, and the training and inference are done on the NVIDIA A40 GPU. We use Adam as our optimizer. For the training data, we use motions from the CMU motion dataset and downsample them from $120$fps to $30$fps. We then sample windows of $500$ frames with a stride of $100$ frames. The CMU dataset contains frames that are shorter than $500$ frames after downsampling, and those are not used for training. Training takes approximately three days for $500$k iterations. 

\subsection{Long-term Generation}
Our \ourmethod{} framework is able to generate long-term motions conditioned on a clean primer motion which is used to populate the initial motion buffer. 
The model is given as input a primer of $K$ frames $\{\Tilde{f_1}, \Tilde{f_2},\ldots, \Tilde{f_K}\}$ which are progressively noised with a monotonic noise schedule. Our iterative inference strategy can then produce an arbitrarily long sequence of new frames. 
We highlight some of the frames from a long-term sequence generated by our method in Fig. \ref{fig:longterm_generation}. The key to maintaining long-term generation is that at each iteration, the newly sampled noise frame ensures that our "buffer" is able to explore new potential motions in the near future, and the iterative denoising process ensures framewise consistency across the motion. In addition, we show in Fig. \ref{fig:uncond_generation} and \ref{fig:diversity_generation} that our method is capable of generating diverse motion sequences.
Full video results can be found in the supplementary video.





% Figure environment removed


\subsection{Guided generation}

For a character in motion, it is often desired for the character to perform a set of predefined motions which will occur at a point and time in the future. We refer to these frames are \textit{motion guides}. 
Our framework maintains a motion buffer which contains information about the motions to be performed in the future. In order to \emph{influence} the set of currently-generated frames, we directly modify the motion buffer using the motion guide. Specifically, we remove the current set of frames and replace them with a noised version of the motion guide. Then, we discard the predicted denoised frames and replace them with the noised version of the motion guide at the appropriate diffusion time.


Suppose we have a motion buffer of $K$ frames $I = [\tilde{f}_1, \dots, \tilde{f}_K]$ and a set of motion guides $\mathbf{Q}_1, \mathbf{Q}_2, \ldots$ each with length $l_1, l_2, \ldots$ frames that we wish to perform starting at frame number $n_1, n_2, \ldots$, $n_i \geq K$. Assuming we start with the frame number $n=1$ (e.g. the end of the current motion buffer would be frame number $K$), for each predefined motion $\mathbf{Q}_i$, if any of its frames $\mathbf{Q}_{i_j}$, where $ j\in \{1, 2, \ldots, l_i\}$ and $ n_i\leq \mathbf{Q}_{i_j}\leq n_i + l_i$ is such that $n + 5 \leq n_i + j\leq n + K$, then we recursively replace it into the motion buffer. Note that there are five frames right before the start of the motion buffer where we don't recursively replace. This enables the network to smooth out the transitions between the generated frames and the motion guide. We have detailed the procedure for guided generation through recursive replacement in Algorithm \ref{alg:guided_gen}. We demonstrate guided generation in Fig. \ref{fig:guide_generation} and the supplementary material.

\begin{algorithm}
    \caption{Guided generation}
    \label{alg:guided_gen}
    \begin{algorithmic}
        \Require \\
        $M_\theta$: Denoising model\\
        $I = [\tilde{f}_1, \dots, \tilde{f}_K]$: motion buffer \\
        $\{\mathbf{Q}_1, \mathbf{Q}_2, \ldots\}$: motion guides\\
        $\{l_1, l_2, \ldots\}$: motion guide lengths\\
        $\{n_1, n_2, \ldots\}$: starting frame numbers for guidance\\
        $F_{\text{out}} = \varnothing$: ouput frames
        \For{$n$ in $1, 2,\ldots$}
            \State\emph{Evaluate} $M_\theta(I)$
            \For {all frames $Q_{i_j}$}
                \If{$n + 5 \leq n_i + j \leq n + K$}
                    \State $M_\theta(I)_{n_i + j - n} \gets$ $Q_{i_j}$
                \EndIf
            \EndFor
            \State $F_{\text{out}} \gets$ $[F_{\text{out}}, M_\theta(I)_1]$
            \State $\tilde{f}_{i-1} \gets M_\theta(I)_{i}\ \forall i\in \{2, \dots K\}$
            \State $\tilde{f}_K \gets X \sim \mathcal{N}(0, I)$
            \State $I \gets [\tilde{f}_1, \dots, \tilde{f}_K]$
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Trajectory Control}
Our work can be applied to perform trajectory control during inference without additional training. Similar to the mechanism of guided generation in the previous section, trajectory control also utilizes the inpainting strategy by modifying the motion buffer. Specifically, let $I = [\tilde{f}_1, \ldots, \tilde{f}_K]$ be a motion buffer of $K$ frames, and let $\mathbf{P}\in \R^{3\times N}$ be the trajectory information (root displacements with respect to the xz-plane and root height), where $N$ is the desired number of frames to be generated. During inference, we recursively overwrite the trjactory information in the motion buffer with frames in $P$. The detailed procedure is similar to the one presented in Algorithm \ref{alg:guided_gen}. We demonstrate trajectory control generation in Fig. \ref{fig:traj_control}.

\subsection{Comparison and Ablation}
We next evaluate our approach against alternative baselines, and assess our framework through an ablation study. We refer the reader to the supplementary video attached to this work to assess the results qualitatively. For quantitative evaluation, we assess our ability to avoid collapses in the motion sequence by measuring the variance across all generated frames. In order to measure how non-stationary generated motions are, and to detect the time-point where they collapse, we measure the average variance of poses in a local window. 






\subsubsection{Comparison}
\label{section:Comparison}
In this experiment, we focus on comparing our framework to other works on the task of long-term generation. We compare our method with ACRNN ~\cite{zhou2018auto} and the Human Motion Diffusion Model (MDM) ~\cite{tevet2022human}.
In particular, the ACRNN work~\cite{zhou2018auto} is an RNN-based work that receives part of the model's output frames during training, to imitate the inference setting and mitigate motion collapse. MDM is an adaptation of the classic DDPM network for motion generation. While ACRNN is designed to be trained on a subset of samples from the CMU dataset and has long-term generation as default for inference, MDM does not have a default implementation for long term generation. Thus we use a pretrained checkpoint for MDM and implement an inpainting-based scheme to enable long-term generation for MDM. This implementation is the same as the popular "outpainting" technique used in 2D image generation, where we take the latter part of the generated motion and in-paint it to the first part of the generated motion on the next iteration. As in Fig. \ref{fig:quant}, it can be seen that ACRNN is not able to perform well on a large and diverse dataset, producing motions that quickly collapse after initialization. In contrast, \ourmethod{} can produce infinitely long sequences that is robust to collapses. On the other hand, MDM produces significant stitching artifacts along the in-painting boundary. Please refer to the supplemental video for more details.



\subsection{Perceptual Study} We conduct a perceptual study to evaluate the perceived diversity and quality of the generated motions. In addition to MDM and ACRNN, we also add Motion VAE ~\cite{Ling_2020}, a recent autoregressive motion generation model with VAE, as a baseline comparison. Following the setup of DALLE-2 ~\cite{ramesh2022hierarchical}, we show users 3x3 grids of randomly sampled motions from our model, MDM, Motion VAE, and ACRNN, and ask them to choose 1) the set with the most diverse motions and 2) the set with the highest quality motions (only from ours, MDM, or ACRNN). Visual examples of the generated motions in the perceptual study is provided in Appendix \ref{sec:study}

We had 55 respondents for our study, and we report the results in Tab. ~\ref{tab:survey_results}. We conclude from our perceptual study that our method produces motion of equivalent or better quality compared to MDM while significantly outperforming in terms of diversity. 

\begin{table}[htbp]
    \centering
    \begin{tabular}{l|cccc}
    \hline
    & Ours & MDM & ACRNN & MVAE \\
    \hline
    Diversity & \textbf{34} & 12 & 8 & 1 \\
    Quality & \textbf{33} & 17 & 5 & N/A \\
    \hline
    \end{tabular}
    \vspace{5mm}
    \caption{Perceptual study results for our method and baselines.}
    \label{tab:survey_results}
\end{table}


\subsubsection{Ablation}
In Fig. ~\ref{fig:ablation}, we demonstrate the advantage of our training scheme, by training a version of our model with temporally-invariant noise levels. Without temporally varying noise, the network diminishes in both diversity and stability of long-range motion generation.


