\section{Introduction}

Long-term generation of a motion sequence is a difficult and long standing problem in character animation with myriad applications in computer animation, motion control, human-computer interaction, and more. Generating long-term motion entails producing realistic, non-repetitive sequences which avoids degenerate outputs (i.e., frozen motion).

A promising avenue for generating high-quality motion is through Denoising Diffusion Probabilistic Models (DDPM), which have produced unprecedented quality in image synthesis \cite{ho2020denoising} and have been recently adapted to motion synthesis \cite{tevet2022human, zhang2022motiondiffuse,kim2022flame}. 
A typical adaptation of DDPM to motion synthesis generates a fixed-length motion sequence (i.e., a ``motion image'') from randomly sampled Gaussian noise. 

A fixed-length output is limiting in the context of long-term motion synthesis for a couple of reasons. First, there is no satisfactory approach for creating long-sequences from short-sequences outputs. Simply chaining together motions and blending them may create stitching artifacts. 
Second, a typical diffusion process has limited interactive controllability.  Diffusion requires several hundred denoising iterations before producing a short sequence of clean motions. 




We are inspired by the time-dependent nature of the diffusion process, where samples are synthesized from pure noise gradually in small time increments along the diffusion \textit{time-axis}. In this work, we propose to adapt diffusion to the \textit{temporal-axis} of the motion. Our method, referred to as \ourmethod{} (\textbf{T}emporally-\textbf{E}ntangled \textbf{Di}ffusion), extends the DDPM framework by enabling injection of temporally-varying noise levels during each step of the diffusion process, instead of a Gaussian noise with a fixed, temporally-invariant variance.
By \textit{entangling} the \textit{temporal-axis} of the motion sequence with the \textit{time-axis} of the diffusion process, we enable the production of a continuous stream of clean motion frames during each step of the diffusion process.

At the core of our framework lies a \textit{motion buffer}, which encodes noisy future motion frames with varied noise levels. During the training phase, we add temporally varied noise to clean motion sequences, such that each frame has a random level. However, during inference the motion buffer is initialized with a \textit{motion primer} - a sequence of clean motion frames that are being noised with increasing noise levels, such that adjacent frames contains consecutive noise levels.
\ourmethod{} recursively denoises the increasingly-noised future frames. In order to constantly maintain the progressively-noised motion buffer structure during each denoising step, we \textit{insert} a noisy frame at the end of the motion buffer and remove a single clean frame at the beginning. 

This recursive mechanism enables motion sequence frames to be continuously generated, and avoids stitching problems which current motion diffusion models suffer from (see \ref{section:Comparison}).



During inference, we can guide the generation with specific motions by intervening in the process and persistently injecting clean frames, called \textit{guiding motions}. This injection enables us to control and influence the current set of generated frames to \emph{prepare and plan} for the upcoming motion guides. This strategy causes a premeditated and calculated transition between the current frames and the future guiding motions.













Our network continues to denoise an ever-evolving motion buffer, which contains vague information about the future trajectory of the motion sequence. This formulation opens the door to more direct control, and better planning, of the generated motion via manipulation of the motion buffer.
We demonstrate that our framework is capable of producing different types of long motion sequences, and due to its random nature, can provide diverse results even for the same initialization. In addition, we evaluate the model against other long-term generation models. Our experiments show that \ourmethod{} is a natural framework for generating long-term motion sequences.

