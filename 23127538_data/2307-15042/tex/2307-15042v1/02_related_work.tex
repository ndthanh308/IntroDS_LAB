\section{Related Work}

\subsection{Diffusion Models}
Denoising diffusion probalistic models (DDPMs) and its variants \cite{ho2020denoising,dhariwal2021diffusion,ho2022cascaded} have achieved unprecedented quality on conditional and unconditional image generation, generally surpassing GAN-based ~\cite{dhariwal2021diffusion} methods both in visual quality and sampling diversity. In particular, diffusion models have demonstrated remarkable fidelity and semantic control for text-to-image synthesis and editing tasks when large models are trained on text and image pairs \cite{ramesh2022hierarchical,saharia2022photorealistic,rombach2021highresolution,ruiz2022dreambooth,hertz2022prompt}. In addition, diffusion has been successfully applied in adjacent domains such as text-to-video and image-to-image translation \cite{saharia2022palette}. Moreover, diffusion models are beginning to see increased usage in generative tasks with 3D data. Some recent work enable 3D data generation by reducing it to a 2D task, while others directly train the entire diffusion pipeline on 3D data. 
More recently, in the animation domain,  Zhang~\etal~\cite{zhang2022motiondiffuse}, Kim~\etal~\cite{kim2022flame}, Tevet~\etal~\cite{tevet2022human}, and Shafir~\etal~\cite{shafir2023human} have suggested adapting diffusion models for motion generation by directly applying the diffusion framework, namely by treating the entire motion as an image and denoising all frames in parallel. This adaptation can only generate fixed-length motion sequences which makes long-term generation and interactive control infeasible. In contrast, our framework combines the diffusion framework with an auto-regressive generation scheme, thus enabling generation of arbitrary length sequences by design.

\subsection{Deep Motion Synthesis}

Before the advent of modern deep learning architectures, earlier works attempted to model motion and styles of motion with techniques such as restricted Boltzmann machines \citet{taylor2009factored}. Later on, the seminal set of works by Holden \etal~\shortcite{holden2015learning,holden2016deep} applied convolutional neural networks (CNN) to motion data and learned a motion manifold which can then be used to perform motion editing by, for instance, projection onto the motion manifold. Concurrently, \citet{fragkiadaki2015recurrent} chose to use recurrent neural networks (RNN) for motion modeling. RNN based works also succeeded in short-term motion prediction~\cite{fragkiadaki2015recurrent, pavllo2018quaternet}, interactive motion generation~\cite{lee2018interactive}, and music-driven motion synthesis~\cite{aristidou2021rhythm}. 
\citet{holden2017phase} propose phase-functioned neural networks (PFNN) for locomotion generation and introduce \emph{phase} to neural networks for motion synthesis. Similar ideas are used in quadruped motion generation by \citet{zhang2018mode}. \citet{starke2020local} extended phase to local joints to cope with more complex motion generation. \citet{henter2020moglow} proposed another generative model for motion based on normalizing flow. Additionally, deep neural networks have succeeded in a variety of other motion synthesis tasks such as motion retargeting~\cite{villegas2018neural, aberman2020skeleton, aberman2019learning}, motion style transfer~\cite{aberman2020unpaired, mason2022real}, key-frame based motion generation~\cite{harvey2020robust}, motion matching~\cite{holden2020learned}, animation layering~\cite{starke2021neural} and motion synthesis from a single sequence~\cite{li2022ganimator}.




\subsection{Long-Term Motion Synthesis}

Deep learning models for long term motion synthesis are mostly based on RNNs as they naturally enable auto-regressive generation and capture the time dependencies between animation frames. In general, RNNs have shown much success in natural language processing (NLP) for generating text~\cite{sutskever2011generating}, hand written characters \cite{gregor2015draw}, and even captioning images \cite{vinyals2015show}.
They have also been proposed
for spatio-temporal prediction where \cite{ranzato2014video,srivastava2015unsupervised}
integrated 2D convolutions into the recurrent state transitions of a standard LSTM and proposed the
convolutional LSTM network, which can model the spatial correlations and temporal dynamics in
a unified recurrent unit. Wang~\etal~extended convolutional LSTMs with pairwise memory
cells to capture both long-term dependencies and short-term variations to improve the prediction
quality \cite{wang2017predrnn}. 
 Zhou et al. tackled the problem of error accumulation in long-term random generation by alternating the network's output and ground truth as the input of RNN during training \cite{zhou2018auto}.
This method, called acRNN, is able to generate long and stable motion similar to the training set. However, despite the modified training procedure, acRNN still fails to produce very long motions. One speculation is that acRNN, and RNNs in general, rely on a memory component that is being eroded with time. In contrast, our framework explicitly utilizes frames within our context window which only needs to be the same size temporally as the diffusion time-axis, producing the motion autoregressively in small increments that complies with the successful mechanism of the diffusion process.





