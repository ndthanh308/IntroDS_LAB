{
  "title": "Towards a Visual-Language Foundation Model for Computational Pathology",
  "authors": [
    "Ming Y. Lu",
    "Bowen Chen",
    "Drew F. K. Williamson",
    "Richard J. Chen",
    "Ivy Liang",
    "Tong Ding",
    "Guillaume Jaume",
    "Igor Odintsov",
    "Andrew Zhang",
    "Long Phi Le",
    "Georg Gerber",
    "Anil V Parwani",
    "Faisal Mahmood"
  ],
  "submission_date": "2023-07-24T16:13:43+00:00",
  "revised_dates": [
    "2023-07-26T00:44:39+00:00"
  ],
  "abstract": "The accelerated adoption of digital pathology and advances in deep learning have enabled the development of powerful models for various pathology tasks across a diverse array of diseases and patient cohorts. However, model training is often difficult due to label scarcity in the medical domain and the model's usage is limited by the specific task and disease for which it is trained. Additionally, most models in histopathology leverage only image data, a stark contrast to how humans teach each other and reason about histopathologic entities. We introduce CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining. Evaluated on a suite of 13 diverse benchmarks, CONCH can be transferred to a wide range of downstream tasks involving either or both histopathology images and text, achieving state-of-the-art performance on histology image classification, segmentation, captioning, text-to-image and image-to-text retrieval. CONCH represents a substantial leap over concurrent visual-language pretrained systems for histopathology, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.",
  "categories": [
    "cs.CV",
    "cs.AI"
  ],
  "primary_category": "cs.CV",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12914",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 60196710,
  "size_after_bytes": 518122
}