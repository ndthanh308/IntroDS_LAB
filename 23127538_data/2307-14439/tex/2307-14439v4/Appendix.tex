\subsection{Positivity Proof}

To constrain $f$ to be non-negative, we construct an MLP with a composition of $L$ custom layers $\psi_i$:

\begin{align}
    \label{eq:F_custom}
    F_\theta(\vec{x}) &= \psi_L \left(\ldots \psi_2\left(\psi_1(\vec{x})\right)\right) \\
    \psi_i &= \sigma_n \left( \lvert W_i \! \rvert \, \vec{x} + b_i \right)
    \label{eq:psi_custom}
\end{align}

The activation function $\sigma_n$ denotes a custom nonlinearity defined as a function of $n$, the dimension of the input $\vec{x} \in \mathbb{R}^n$:

\begin{equation}
    \sigma_n = \underbrace{\int \int \cdots \int}_{n-1} \frac{\mathrm{erf}(x)+1}{2} \; \underbrace{dx \cdots \, dx \, dx}_{n-1}
    \label{eq:sigma_n}
\end{equation}

Since $f$ is defined implicitly through our parametrisation of $F_\theta$, we must show that the mixed partial of $F_\theta$ is positive to ensure that $f$ is non-negative. From \autoref{eq:f_F}, we can write:

\begin{align}
    \frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} \cdots \frac{\partial}{\partial x_n} F_\theta(\vec{x}) \geq 0 &\implies f(\vec{x}) \geq 0
\end{align}

\vspace{5mm}

\begin{lemma}
    \label{lemma:multilayer}
    If the derivative of each layer $\psi_j$ with respect to some $x_i$ is non-negative, then the derivative of $F_\theta$ is non-negative: $\forall j, \frac{\partial}{\partial x_i} \psi_j(\vec{x}) \geq 0 \implies \frac{\partial}{\partial x_i} F_\theta(\vec{x}) \geq 0$.
\end{lemma}
\begin{proof}
    Using the chain rule, we see that the partial derivative of $F_\theta$ is the product of the partials of each layer $\psi_j$, evaluated at the output of the preceding layer:

    \begin{align}
        \frac{\partial}{\partial x_i} \psi_L \left(\ldots \psi_2\left(\psi_1(\vec{x})\right)\right) = \; &\frac{\partial \psi_L}{\partial x_i} \left(\ldots \psi_2\left(\psi_1(\vec{x})\right)\right) \\
        &\;\;\;\;\;\;\;\;\;\;\vdots \nonumber \\
        & \cdot \frac{\partial \psi_2}{\partial x_i}(\psi_1(\vec{x})) \nonumber \\
        & \cdot \frac{\partial \psi_1}{\partial x_i}(\vec{x}) \nonumber
    \end{align}

    So, if the derivative of each layer $\frac{\partial \psi_j}{\partial x_i}$ is positive, then the derivative of the entire network $\frac{\partial}{\partial x_i} F_\theta$ is positive.
\end{proof}

\begin{theorem}
    Under the parametrisation of $F_\theta$ in \autoref{eq:F_custom}, the function $f$ is non-negative.
\end{theorem}

\begin{proof}
    Consider a single layer of $F_\theta$, denoted $\psi$, given by \autoref{eq:psi_custom}. Furthermore, without loss of generality, let us consider a single dimension of the output $j$, denoted $\psi_j$.

    If we take the partial derivative with respect to input $x_p$, we can apply the chain rule to obtain:

    \begin{align}
        \frac{\partial}{\partial x_p} \psi_j(\vec{x}) &= \frac{\partial}{\partial x_p} \left[ \sigma_n \left( \sum_{i \in [1..n]} w_{ij} \cdot x_i \right) \right] \nonumber \\
         &= \frac{\partial \sigma_n}{\partial x_p} \left( \sum_{i \in [1..n]} w_{ij} \cdot x_i \right) \cdot \frac{\partial}{\partial x_p} \left[ \sum_{i \in [1..n]} w_{ij} \cdot x_i \right] \nonumber \\
         &= \dot{\sigma}_n \left( \sum_{i \in [1..n]} w_{ij} \cdot x_i \right) \cdot w_{pj}
    \end{align}
    
    If we then take the partial derivative with respect to input $x_q$ to form the mixed partial $\frac{\partial^2}{\partial x_p \partial x_q}$, we get:

    \begin{equation}
        \frac{\partial}{\partial x_q} \left(\frac{\partial}{\partial x_p} \psi_j(\vec{x}) \right) = \ddot{\sigma}_n \left( \sum_{i \in [1..n]} w_{ij} \cdot x_i \right) \cdot w_{pj} \cdot w_{qj}
    \end{equation}
    
    Finally, if we take the derivative with respect to 
    \textit{all} inputs, the result is:

    \begin{equation}
        \frac{\partial^n}{\partial x_1 \partial x_2 \ldots \partial x_n} \psi_j(\vec{x}) = \sigma^{(n)}_n \left( \sum_{i \in [1..n]} w_{ij} \cdot x_i \right) \cdot \prod_{i \in [1..n]} w_{ij}
    \end{equation}
    
    We know that our weights are all positive $w_{ij} \geq 0$, and our activation function is parametrised such that it is positive up to the $n$-th derivative (see \autoref{eq:sigma_n}): $\forall k \in [1..n],\forall x \in \mathbb{R}: \sigma^{(k)}_n(x) > 0$. Consequently, the mixed derivative of $\psi_j$ must be positive: $\frac{\partial^n}{\partial x_1 \partial x_2 \ldots \partial x_n} \psi_j(X) \geq 0$.
    
    Furthermore, by applying Lemma \ref{lemma:multilayer}, we can show that since the mixed partial of each layer is positive, the mixed partial of the entire network is also positive:
    
    $$\frac{\partial^n}{\partial x_1 \partial x_2 \ldots \partial x_n} F_\theta(\vec{x}) \geq 0$$
\end{proof}