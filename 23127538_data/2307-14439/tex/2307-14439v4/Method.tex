While it is not tractable to directly compute the integral of a function represented by a neural network, it is straightforward to take the analytical derivative. In this paper, we leverage the fundamental theorem of calculus in order to implicitly learn the integral of a function.

Suppose we wish to learn some function $f: \mathbb{R}^n \mapsto \mathbb{R}$. Instead of directly parametrising $f$, we represent it implicitly by parametrising its indefinite integral $F_\theta$ with a neural network:

\begin{equation}
    F_\theta(\vec{x}) = \int \int \cdots \int f(\vec{x}) \; dx_1 dx_2 \ldots dx_n
\end{equation}

Note that as $f$ is defined implicitly as a function of $F_\theta$, this is not an approximation---it is the \textit{exact} analytical integral. In order to solve for $f$, we must differentiate $F_\theta$:

\begin{equation}
    f(\vec{x}) = \frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} \cdots \frac{\partial}{\partial x_n} F_\theta(\vec{x})
    \label{eq:f_F}
\end{equation}

Although we parametrise its integral $F_\theta$, the function we wish to learn is $f$. Consequently, during the learning process, the loss is applied directly to $f$:

\begin{equation}
    \mathcal{L} = \mathbb{E}\left[ (y - f(\vec{x}))^2 \right]
\end{equation}

\input{schema/eps-fig}

\subsection{Integral Constraints}

Since $f$ is defined as a function of $F_\theta$, we can apply constraints directly to its integral. For example, by using an equality constraint, we can define the class of functions $f$ that integrate to a given value $\epsilon$ over a given domain $\mathcal{D}$. The same principle can be utilised to apply inequality constraints or transformations.

We start by considering rectangular domains defined by intervals $[a_i,b_i]$ for $i \in [1..n]$. The definite integral of $f$ over rectangular domain $\mathcal{D}$ is defined:

\begin{equation}
    F_\theta \Big\vert_\mathcal{D} = \sum_{p_1 \in [a_1, b_1]} \sum_{p_2 \in [a_2,b_2]} \cdots \sum_{p_n \in [a_n,b_n]} (-1)^{^{\sum \mathbbm{1}(p_i=a_i)}} \cdot F_\theta(\langle p_1, p_2, \ldots, p_n \rangle)
    \label{eq:int_eval}
\end{equation}

That is, in order to calculate the definite integral over an $n$-dimensional box, we must evaluate all of its vertices. This is because evaluating $F_\theta$ at a point will yield the ``area'' from negative infinity up to that point, so multiple points must be evaluated to determine the area of a finite region. The $(-1)$ exponent in the equation determines which regions must be subtracted and which must be added in order to determine that area (Figure \ref{fig:int_eval}).

% Figure environment removed

In order to parametrise the class of functions which integrate to $\epsilon$, we start by defining $F'_\theta$, the integral of some unconstrained function $f'$. Then, we define the integral $F_\theta$ of our constrained function $f$ by rescaling $F'_\theta$:

\begin{equation}
    F_\theta(\vec{x}) = \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} F'_\theta(\vec{x})
\end{equation}

Since the term $\frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}}$ is a scalar, we can move it inside the integral:

\begin{align}
    F_\theta(\vec{x}) &= \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} F'_\theta(\vec{x}) \\
    F_\theta(\vec{x}) &= \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} \int \int \cdots \int f'(\vec{x}) \; dx_1 dx_2 \ldots dx_n \\
    F_\theta(\vec{x}) &= \int \int \cdots \int \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} f'(\vec{x}) \; dx_1 dx_2 \ldots dx_n \\
\end{align}

Therefore, we can write the constrained $f$ as a function of the unconstrained integral $F'_\theta$, which carries the learnable parameters:

\begin{align}
    f(\vec{x}) &= \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} f'(\vec{x}) \\
    f(\vec{x}) &= \frac{\epsilon}{F'_\theta \big\vert_\mathcal{D}} \cdot \frac{\partial}{\partial x_1} \frac{\partial}{\partial x_2} \cdots \frac{\partial}{\partial x_n} F'_\theta(\vec{x})
\end{align}


% \subsection{Integration Over Arbitrary Domains}

% In the previous section we focused on the special case of rectangular domains (\textit{i.e.} where the limits of integration are constants). However, it is also possible to integrate over arbitrary domains by reparametrising with $u$-substitution.

% Consider a parametric function $\vec{x} = \vec{r}(\vec{u})$ which defines a transformation from euclidean space to some domain where the limits of integration are constant. Furthermore, as the indefinite integral depends on our choice of $\vec{r}$, we parametrise $F_\theta(\vec{u})$ instead of $F_\theta(\vec{x})$. Simplifying the notation of our iterated integral and applying this reparametrisation, our equation becomes:

% \begin{align}
%     F_\theta(\vec{u}) &= \int f(\vec{r}(\vec{u})) \; \lvert \nabla \vec{r}(\vec{u}) \rvert \, d\vec{u} \\
%     f(\vec{r}(\vec{u})) &= \frac{1}{\lvert \nabla \vec{r}(\vec{u}) \rvert} \cdot \frac{\partial}{\partial u_1} \frac{\partial}{\partial u_2} \cdots \frac{\partial}{\partial u_n} F_\theta(\vec{u}) \\
%     f(\vec{x}) &= \frac{1}{\lvert \nabla \vec{r}(\vec{u}) \rvert} \cdot \frac{\partial}{\partial u_1} \frac{\partial}{\partial u_2} \cdots \frac{\partial}{\partial u_n} F_\theta(\vec{r}^{-1}(\vec{x}))
% \end{align}

% In this formulation, we must select $\vec{r}$ according to our desired domain. For example, if we wish to integrate over the unit circle in $\mathbb{R}^2$, we can select $\langle x_1, x_2 \rangle = \vec{r}(\vec{u}) = \langle u_1 \cos(u_2), u_1 \sin(u_2) \rangle$. The differential after reparametrising with $\vec{r}$ is given by the determinant of the Jacobian $\lvert \nabla \vec{r}(\vec{u}) \rvert$. In this case, the differential is given by $|\nabla \langle u_1 \cos(u_2), u_1 \sin(u_2) \rangle| = \cos(u_2) \cdot u_1 \cos(u_2) - \sin(u_2) \cdot (-u_1 \sin(u_2)) = u_1$. 

\subsection{Positivity Constraint}

In many applications of FINN, it is necessary to constrain $f$ to be non-negative. This is useful in cases where $f$ is only defined in the positive domain (see \autoref{sec:Applications}).

Following from \autoref{eq:f_F}, in order to apply this constraint, we must ensure that the mixed partial of $F_\theta$ is non-negative. To do this, we define a new neural network layer to construct our multi-layer perceptron (MLP):

\begin{equation}
    \sigma_n \left( \lvert W \! \rvert \, \vec{x} + b \right)
\end{equation}

In this layer, we apply an absolute value to the weights (but not the bias), and we use a custom activation function $\sigma_n$, which is conditioned on the dimension of the input. Note that although we wish the mixed partial of $F_\theta$ to be non-negative, it is too constraining to restrict further derivatives to be non-negative as well. The derivative of our function $\dot{f}$ (with respect to any input dimension) should be able to represent positive \textit{or} negative values. To satisfy these criteria, we define the following activation function using the error function $\mathrm{erf}(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-t^2} dt$:

\begin{equation}
    \sigma_n = \underbrace{\int \int \cdots \int}_{n-1} \frac{\mathrm{erf}(x)+1}{2} \; \underbrace{dx \cdots \, dx \, dx}_{n-1}
\end{equation}

For $n=1$, this simplifies to $\frac{\mathrm{erf}(x)+1}{2}$, which closely resembles sigmoid. For $n=2$, it resembles softplus, which is the integral of sigmoid. While they are similar, it is crucial that we use this custom activation instead of sigmoid, because the higher-order integrals of sigmoid evaluate to the polylogarithm, which has no closed-form solution. Conversely, all of the integrals of the error function have analytical solutions in terms of linear compositions of constants, power functions $x^k$, exponentials $e^{-x^2}$, and the error function itself $\mathrm{erf}(x)$ (all of which have efficient implementations). In practice, we use symbolic math to compute the integral once at initialisation time, and then each forward pass evaluates the resulting expression.
