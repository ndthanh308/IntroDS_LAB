There are many possible applications for FINN. In this section, we highlight four use cases, including 1) distance metrics between functions, 2) trajectory optimisation, 3) probability distributions, and 4) optimal bellman updates in reinforcement learning.

\subsection{Distance Metrics}

% Figure environment removed

Given a base function $\psi(x): \mathbb{R}^n \mapsto \mathbb{R}^m$, consider the task of defining a new function $\phi(x): \mathbb{R}^n \mapsto \mathbb{R}^m$ with bounded deviation from $\psi$ (\autoref{fig:metric}). While there are several methods to quantify similarity between functions, we select the ``integral metric'', which takes the integral of a distance metric between the two functions over some domain:

\begin{equation}
    J(\psi, \phi) = \int d(\psi(\vec{x}), \phi(\vec{x})) \; d\vec{x}
\end{equation}

In this case, we select $d(a,b) = ||a-b||$. If we constrain $\phi$ to represent the class of functions such that $J(\phi,\psi) \leq \epsilon$, then it can only place a finite amount of mass across the domain. The constrained function $\phi$ can be implemented by adding a direction function $\nu$ multiplied by a state-dependent scaling factor $f$ defined by FINN to the base function $\psi$:

\begin{equation}
    \phi(\vec{x}) = \psi(\vec{x}) + \frac{f(\vec{x})}{\lvert\lvert \nu(\vec{x}) \rvert\rvert} \cdot \nu(\vec{x})
\end{equation}

Then, if we select a our fixed integral constant to be $\sqrt{\epsilon}$ (using an inequality constraint, and enforcing positivity of $f$), we can show that $J(\psi, \phi) \leq \epsilon$:

\begin{align}
    J(\psi, \phi) &= \int \Big\lvert\Big\lvert \psi(\vec{x}) - (\psi(\vec{x}) + \frac{f(\vec{x})}{\lvert\lvert \nu(\vec{x}) \rvert\rvert} \cdot \nu(\vec{x})) \Big\rvert\Big\rvert \; d\vec{x} \\
     &= \int \Big\lvert\Big\lvert -f(\vec{x}) \cdot \frac{\nu(\vec{x})}{\lvert\lvert \nu(\vec{x})) \rvert\rvert} \Big\rvert\Big\rvert \; d\vec{x} \\
     &= \int f(\vec{x}) \cdot \frac{\lvert\lvert \nu(\vec{x})) \rvert\rvert}{\lvert\lvert \nu(\vec{x})) \rvert\rvert} \; d\vec{x} \\
     &= \int f(\vec{x}) \; d\vec{x} \\
     &\leq (\sqrt{\epsilon})^2 \\
     &\leq \epsilon \\
\end{align}

There are many possible applications for this---for example, it could be used to add a bounded (and controllable) amount of representational complexity on top of an explainable or ground truth model. Such an architecture could mitigate issues with overfitting, forcing a model to learn a representation which distributes its mass in the most efficient manner.

\subsection{Trajectory Optimisation}

% Figure environment removed

Given some physical system in state $x_0$ suppose we wish to define a trajectory $\dot{x}(t)$ to bring the system to state $x_T$. Furthermore, suppose we have access to an inverse dynamics model $u = T(x, \dot{x})$.

If we define $\vec{F}_\theta(t) = \langle F_{\theta_1}(t), F_{\theta_2}(t), F_{\theta_3}(t) \rangle: \mathbb{R} \mapsto \mathbb{R}^3$ with three instances of FINN, then we can parametrise the class of trajectories which end in the desired end state $x_T$. To do this, we set the integral equality constraint $\epsilon$ for each $F_{\theta_i}$ to the corresponding dimension of the terminal state $\langle \epsilon_1, \epsilon_2, \epsilon_3 \rangle = x_T - x_0$. Note that in this application we \textit{do not} apply the positivity constraint, as it is permissible for $\dot{x}(t)$ to assume negative values.

With this parametrisation, we get a trajectory $x(t) = \vec{F}_\theta(t) - \vec{F}_\theta(t_0) + x_0$ and its corresponding first order derivative $\dot{x}(t) = f(t)$ which are constrained to start at $x_0$ and end at $x_T$. Using the inverse dynamics model, an open-loop control trajectory can be defined $u(t) = T(x(t), \dot{x}(t))$.

During the optimisation process the trajectory can be updated, but it will always be constrained to produce $\dot{x}(t)$ which passes through $x_0$ and $x_T$. Since the representation of $x(t)$ and $\dot{x}(t)$ share the same parameters, it is possible to apply loss to either $x(t)$ \textit{or} $\dot{x}(t)$. For example, the derivative $\dot{x}(t)$ can be optimised to satisfy dynamics constraints or minimise energy usage. Alternatively, the trajectory itself $x(t)$ can be optimised to avoid obstacles.

\subsection{Probability Distributions}

% Figure environment removed

As FINN can parametrise the class of positive functions which integrate to $1$, it can be used to represent arbitrary probability distributions. In this context, FINN can be viewed as an alternative to Normalising Flows \cite{normflows}.

The implementation of a probability density function $p: \mathbb{R}^n \mapsto \mathbb{R}$ is fairly straighforward---if we impose the positivity constraint and an integral equality constraint with $\epsilon=1$ over some domain $\mathcal{D}$, then $p(\vec{x}) = f(\vec{x})$. By extension, the integral $F_\theta$ represents the cumulative density function of $p$.

While evaluating the PDF and CDF of the learned probability distribution is straightforward, sampling from the distribution presents a challenge. The standard method for sampling from a multivariate distribution involves sampling from a uniform distribution $t \sim \mathcal{U}[0,1]$, using the inverse CDF to produce a sample in a single dimension, and then repeating the process in each other dimension using conditional probability distributions. Unfortunately, we do not have a representation of the inverse CDF. However, the learned CDF represented by $F_\theta$ is monotonic, so it is straighforward to compute the inverse with binary search or Newton's method.

\subsection{Bellman Optimality Operator}

% Figure environment removed

In reinforcement learning, the Bellman optimality equation defines the value of a given state as the expected discounted sum of future rewards that would be accrued by an optimal policy, starting at that state. The Bellman operator is a dynamic programming update rule which is applied to a value function such that it approaches the optimal value function:

\begin{align}
    \mathcal{B}[Q(s,a)] &= \mathbb{E} \left[ 
r(s,a,s') + \gamma \max_a Q(s',a) \right]
\end{align}

Note that in this equation we must compute the maximum over all actions to compute the value for the next state $V(s') = \max_a Q(s',a)$. While this is possible in discrete settings, it is generally intractable in continuous spaces. Consequently, most approaches sacrifice some optimality for computational feasibility, opting for an on-policy update (where $\pi$ is the current policy):

\begin{align}
    \mathcal{B}[Q(s,a)] &= \mathbb{E} \left[ 
r(s,a,s') + \gamma \, Q(s',\pi(s)) \right]
\end{align}

However, while the first update rule converges to the optimal value function regardless of which actions are chosen, the on-policy approach is biased by the current policy. If the policy takes suboptimal actions, then the value function will be biased towards the values associated with those actions.

However, FINN enables the use of the optimal Bellman operator in continuous space, as it allows us to analytically integrate over actions. Consequently, it converges towards the optimal Q-values regardless of which policy is used.

As a preliminary, note that the $\max$ can be represented with $\mathrm{log\text{-}sum\text{-}exp}$ using temperature parameter $\beta$. This gives rise to the \textit{soft bellman operator} \cite{softbellman}:

\begin{align}
    \mathcal{B}[Q(s,a)] &= \mathbb{E} \left[ 
r(s,a,s') + \gamma \frac{1}{\beta}\log\left( \int 
e^{\beta Q(s,a)} da \right) \right]
\end{align}

If we select $f(s,a) = e^{\beta Q(s',a)}$ and integrate over the actions, we get $F_\theta(s,a) = \int 
e^{\beta Q(s',a)} da$. So, we can represent the value functions as:

\begin{align}
    Q(s,a) &= \frac{1}{\beta} \log(f(s,a)) \\
    V(s) &= \frac{1}{\beta} \log(F_\theta(s,a))
\end{align}

Note that we do not apply an integral constraint, but we do apply a positivity constraint to ensure that $e^{\beta Q(s',a)}$ is positive (and therefore the $\log$ is a real number). However, since we only wish $f$ to be monotonic with respect to the actions $a$ (not the state $s$), we cannot use the standard MLP that we defined in \autoref{sec:Method}. Instead, we use a separate hypernetwork conditioned on $s$ to generate the parameters of our monotonic MLP, allowing $f$ to be non-monotonic with respect to $s$.
