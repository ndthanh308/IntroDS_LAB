
@misc{bayesian,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv:1505.05424 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {“the functional form of a neural network does not lend itself to exact integration”
},
	file = {arXiv.org Snapshot:/Users/ryko/Zotero/storage/JTKIWQ27/1505.html:text/html;Full Text PDF:/Users/ryko/Zotero/storage/3DINWLIA/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf},
}

@inproceedings{monotonic,
	title = {Unconstrained {Monotonic} {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/2a084e55c87b1ebcdaad1f62fdbbac8e-Abstract.html},
	abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.  In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.  We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments.  We also illustrate the ability of UMNNs to improve variational inference.},
	urldate = {2023-06-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Wehenkel, Antoine and Louppe, Gilles},
	year = {2019},
	annote = {represents monotonic neural networks by ensuring that the derivative is positive.
learns the derivative, and integrates numerically
},
	file = {Full Text PDF:/Users/ryko/Zotero/storage/BDRDD335/Wehenkel and Louppe - 2019 - Unconstrained Monotonic Neural Networks.pdf:application/pdf},
}

@inproceedings{hamiltonian,
	title = {Hamiltonian {Neural} {Networks}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/26cd8ecadce0d4efd6cc8a8725cbd1f8-Abstract.html},
	abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
	urldate = {2023-06-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
	year = {2019},
	annote = {parameterises the integral of a desired function (the hamiltonian, which describes the total energy of a system), and computes the loss on the derivative to learn the desired function. because it is the derivative of a function, it must be a conservative vector field, so energy is exactly conserved.
},
	file = {Full Text PDF:/Users/ryko/Zotero/storage/26IB6X94/Greydanus et al. - 2019 - Hamiltonian Neural Networks.pdf:application/pdf},
}

@misc{neuralode,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2023-06-04},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv:1806.07366 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {learns a continuous-time differential equation, and uses and ODE solver to integrate over it
},
	file = {arXiv.org Snapshot:/Users/ryko/Zotero/storage/94HUCMZW/1806.html:text/html;Full Text PDF:/Users/ryko/Zotero/storage/Z8VH9YVI/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf},
}


@article{normflows,
	title = {Normalizing {Flows}: {An} {Introduction} and {Review} of {Current} {Methods}},
	volume = {43},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Normalizing {Flows}},
	url = {http://arxiv.org/abs/1908.09257},
	doi = {10.1109/TPAMI.2020.2992934},
	abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
	number = {11},
	urldate = {2023-06-05},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
	month = nov,
	year = {2021},
	note = {arXiv:1908.09257 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {3964--3979},
	annote = {Comment: This paper appears in: IEEE Transactions on Pattern Analysis and Machine Intelligence On page(s): 1-16 Print ISSN: 0162-8828 Online ISSN: 0162-8828},
	file = {arXiv Fulltext PDF:/Users/ryko/Zotero/storage/6HPNPAIM/Kobyzev et al. - 2021 - Normalizing Flows An Introduction and Review of C.pdf:application/pdf;arXiv.org Snapshot:/Users/ryko/Zotero/storage/LFQJXPNA/1908.html:text/html},
}


@misc{softbellman,
	title = {Reinforcement {Learning} with {Deep} {Energy}-{Based} {Policies}},
	url = {http://arxiv.org/abs/1702.08165},
	doi = {10.48550/arXiv.1702.08165},
	abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
	urldate = {2023-06-05},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
	month = jul,
	year = {2017},
	note = {arXiv:1702.08165 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {soft bellman equation
},
	file = {arXiv Fulltext PDF:/Users/ryko/Zotero/storage/K77VI5SG/Haarnoja et al. - 2017 - Reinforcement Learning with Deep Energy-Based Poli.pdf:application/pdf;arXiv.org Snapshot:/Users/ryko/Zotero/storage/JGTXBIV4/1702.html:text/html},
}


@misc{graphode,
	title = {Graph {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1911.07532},
	abstract = {We introduce the framework of continuous--depth graph neural networks (GNNs). Graph neural ordinary differential equations (GDEs) are formalized as the counterpart to GNNs where the input-output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with various static and autoregressive GNN models. Results prove general effectiveness of GDEs: in static settings they offer computational advantages by incorporating numerical methods in their forward pass; in dynamic settings, on the other hand, they are shown to improve performance by exploiting the geometry of the underlying dynamics.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Poli, Michael and Massaroli, Stefano and Park, Junyoung and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
	month = jun,
	year = {2021},
	note = {arXiv:1911.07532 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {extension of neural odes
},
	file = {arXiv.org Snapshot:/Users/ryko/Zotero/storage/S4PCE8J5/1911.html:text/html;Full Text PDF:/Users/ryko/Zotero/storage/4PRSVSF2/Poli et al. - 2021 - Graph Neural Ordinary Differential Equations.pdf:application/pdf},
}

@misc{ffjord,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2023-06-19},
	publisher = {arXiv},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv:1810.01367 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {extension of neural odes
},
	file = {arXiv.org Snapshot:/Users/ryko/Zotero/storage/EYXS4KUQ/1810.html:text/html;Full Text PDF:/Users/ryko/Zotero/storage/7MRQEX84/Grathwohl et al. - 2018 - FFJORD Free-form Continuous Dynamics for Scalable.pdf:application/pdf},
}
