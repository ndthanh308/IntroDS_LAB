Previous work has shown that integration over learned functions is a powerful tool. It has been used in many disparate applications, from constraining monotonic functions \cite{monotonic} to computing the output of a network over a distribution of parameters \cite{bayesian}. The entire field of Neural ODEs is predicated on the ability to integrate a learned dynamics function over time \cite{neuralode, ffjord, graphode}. However, all of these methods rely on \textit{numerical} integration, which is computationally expensive and not exact.

Another work in the same vein as our method is Hamiltonian Neural Networks \cite{hamiltonian}, which parameterises a vector field which conserves energy by formulating it as the symplectic gradient of an energy function. However, this method is only used as a means of constraining the learned function, not as a technique for integration.