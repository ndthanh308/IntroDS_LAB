\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}

\usepackage{amssymb}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand\xmark{\ding{55}}%
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi

\begin{document}

%%%%%%%%% TITLE
\title{Weakly-supervised 3D Pose Transfer with Keypoints}

\author{
Jinnan Chen\\
National Univeristy of Singapore\\
% jinnan.c@u.nus.edu\\
% {\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Chen Li\\
National Univeristy of Singapore\\
% lichen@u.nus.edu\\\
% {\tt\small secondauthor@i2.org}
\and
Gim Hee Lee\\
National Univeristy of Singapore\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
}

\maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

%%%%%%%%% ABSTRACT
\begin{abstract}
The main challenges of 3D pose transfer are: 1) Lack of paired training data with different characters performing the same pose; 2) Disentangling pose and shape information from the target mesh; 3) Difficulty in applying to meshes with different topologies.
We thus propose a novel weakly-supervised keypoint-based framework to overcome these difficulties. Specifically, we use a topology-agnostic keypoint detector with inverse kinematics to compute transformations between the source and target meshes. Our method only requires supervision on the keypoints, can be applied to meshes with different topologies and is shape-invariant for the target which allows extraction of pose-only information from the target meshes without transferring shape information. We further design a cycle reconstruction to perform self-supervised pose transfer without the need for ground truth deformed mesh with the same pose and shape as the target and source, respectively.
We evaluate our approach on benchmark human and animal datasets, where we achieve superior performance compared to the state-of-the-art unsupervised approaches and even comparable performance with the fully supervised approaches. We test on the more challenging Mixamo dataset to verify our approach's ability in handling meshes with different topologies and complex clothes. Cross-dataset evaluation further shows the strong generalization ability of our approach.
\end{abstract}
%%%%%%%%% BODY TEXT

% Figure environment removed


\section{Introduction}
\label{intro}
% Pose transfer is an essential and challenging task with wide applications in AR/VR. Moreover, pose transfer can also serve as a data augmentation technique for other fundamental downstream 3D tasks like segmentation when only limited data is available.
% %  The task has arisen great research attention in 3D vision areas in recent years, given its wide application in AR/VR, 3D animated movies, and games to generate new poses for existing shapes. 
% Great progress has been made for 3D pose transfer with the deep learning-based methods\cite{disentangle20}\cite{3dpt21}\cite{neuralpt20}\cite{geotrans21}\cite{ieugan21}. However, this is still a very challenging task since it is almost impossible to collect paired data for supervision. Namely, we cannot obtain the data of two people performing exactly the same pose.

% With the help of predefined 3D human model templates like SMPL \cite{smpl15}, the desired posed mesh for different identities could be synthesized as ground truth. Thus the pair-wise data make fully supervised learning possible. Based on this, researchers borrow some ideas from image style transfer. The pose information implicit in the coordinates of the mesh vertices could be treated as style information and could be learned through re-normalization. Although both the qualitative and quantitative results are surprisingly good in the synthetic SMPL-based \cite{smpl15} datasets, there are still several limitations in such methods. 
% The first one is that the template-based synthetic data have a strong bias on both the shape and pose information. Although they are learned from a large amount of real scanned data, they are all naked meshes with some simple poses. Those methods requiring ground truth can only be trained on template-based synthetic datasets which makes the model have a large performance drop on the non-template-based 3D data like the clothed human mesh or the cartoon characters.  On the other hand, the template-based meshes share the same connection structure, therefore, there is natural correspondence for such point clouds. For non-template meshes without ground truth pair data as the supervision, it is almost impossible to embed the source and target point could feature and compute the correspondence directly.

% Thus this leads to a new research problem: how to train a unified model in a weakly-supervised way that could also be applied to non-template stylized 3D meshes rather than only depend on the biased and paired synthetic data.  Based on these observations, we propose a new pipeline to solve this problem with an explicit deformation method in a weakly-supervised learning strategy to alleviate the data scarcity. %Thus we pre-process all the point clouds with Farthest Points Sampling as the keypoints prior and let the model prediction be close to it. 
% Our method is based on keypoints detection and LBS for deformation propagation. We use a simple pointnet\cite{pointnet17} to detect the keypoints with ground truth as supervision.
% This could detect the keypoints in a semantically consistent way. Then we use a differentiable inverse kinematic method to estimate the local transformation at a low cost. Since we do not have the ground truth skinning weights for training, we employ a parametric GMM model to calculate the skinning weights on the fly as pseudo labels to train our skinning weights prediction network to propagate the local transformations to the whole mesh vertices. After that, we add another refinement network to recover the details. In order to make sure the generated mesh contains the desired pose from the target mesh. We enforce pair and cycle consistent loss to reconstruct the input mesh. In this way, our model could generalize well on the unregistered 3D meshes with different topologies without direct supervision. As shown in Figure \ref{fig:teaser}, our method could successfully transfer the complex pose between styled meshes with different topologies.


% We summarize our contributions as follows:
% \begin{itemize}
%     \item Only with keypoints as supervision, we achieve comparable performance on the SMPL-based dataset compared with SOTA fully supervised methods.
%     \item We achieve better performance on FAUST Dataset for pose transfer compared with SOTA unsupervised mesh disentanglement methods.
%     \item We conduct a case study on the unregistered stylized 3D mesh dataset Mixamo. We show our framework could adapt to unregistered and disjoint meshes with different topologies which could not be directly trained on other methods.
% \end{itemize}
3D Pose transfer refers to transferring the pose from a target input to a source input while keeping the identity information of the source at the same time.
Pose transfer is an important research topic in computer vision because of its wide applications in many real-world applications such as augmented/virtual reality (AR/VR), movie making, gaming, metaverse, etc. %Great

Significant progress has been made for 3D pose transfer with the development of deep learning-based methods \cite{disentangle20, 3dpt21, neuralpt20, geotrans21, ieugan21}. However, 3D pose transfer remains a very challenging task due to the lack of paired training data %, namely, where we can not 
since it is difficult to obtain %the 
data of two characters performing the same pose. To alleviate this problem, existing works \cite{neuralpt20,3dpt21,geotrans21} generate such paired data synthetically
 %based on 
using the SMPL \cite{smpl15} and SMAL \cite{smal17} models. %in a synthetic way.
% by sampling the pose and shape parameters from a specific distribution.
The advantage of synthetic data is that it is convenient to generate paired training data by keeping the pose parameter of different meshes the same. %Also
However, the data generated from SMPL and SMAL has a strong bias for both shape and pose information due to the model parameterization. Consequently, a network trained with synthetic data cannot adapt well to real 3D meshes with large shape and pose variations. 
%\ghComment{Furthermore, a real 3D mesh dataset without such paired data for supervision cannot be easily trained.} 
%To solve this problem
Unsupervised approaches are proposed in \cite{disentangle20,ieugan21,limp20} to circumvent the requirement for paired training data. These works adopt an auto-encoder-based framework to learn the shape and pose embeddings implicitly. %and %the
%where
The pose transfer can then be achieved by swapping the pose code between the source and target meshes. Although data-efficient, these works only show results for 3D pose transfer between meshes with the same topology. Furthermore, the shape and pose information are not fully disentangled in \cite{limp20,disentangle20} %because 
due to their implicit representation. %This is evident from the results where the shape of the source mesh is altered after the pose transfer in our experiments.
% especially when the shape of the target mesh has a large shape variation with the source mesh as we will show in the experimental comparison since articulated motion is not considered as the prior and shape preserving is not well-defined.

We propose a 3D pose transfer model weakly-supervised with keypoints to mitigate the limitations of existing works. 
Our method is \textit{weakly-supervised} since we only need the supervision on keypoints instead of ground truth deformed mesh. As shown in Fig.~\ref{fig:teaser}, our approach achieves accurate 3D pose transfer although we do not use ground truth paired data. 
%In this paper, our goal is to design a 3D pose transfer model %
% to mitigate the limitations of the existing works.
% To this end, we design a \ghComment{weakly-supervised} keypoint-based framework with several advantages: (1) keypoints from source and target could be transformed to transformation matrix representation, thus pose information of the target could be better disentangled from the shape information than the implicit methods \cite{disentangle20,limp20}. (2) Keypoints is easy to detect and have correspondence between subjects of the same category. (3) Keypoints ground truth is available for most of the datasets. (4) Processing keypoints with Pointnet \cite{pointnet17} is topology-agnostic, thus it has the potential to do pose transfer across different topologies. 
%Based on these insights, 
Specifically, we first detect keypoints on both source and target meshes with a topology-agnostic Pointnet \cite{pointnet17}. We then compute the transformation matrices between the two sets of keypoints with the differentiable Scalable Inverse and Forward Kinematics (IK/FK) functions, and propagate the transformations to all vertices of the source mesh with Linear Blending Skin (LBS)-based motion propagation. 
%The predicted skinning weights are supervised by the pseudo label from the GMM model to mitigate the lack of ground truth skinning weights. 
To circumvent the lack of the ground truth LBS skinning weights, we also design a Gaussian Mixture Model (GMM)-based pseudo label to supervise
the skinning weights.
% Our combination of keypoint-based motion estimation and differentiable IK helps to filter out non-pose information of the target during pose transfer. 
We choose 
keypoints because it is easy to detect and there are correspondences between subjects of the same category. Ground truth for keypoints is also available for most datasets. Furthermore, in contrast to the implicit methods, our combination of keypoint-based transformation estimation and differentiable IK/FK helps \textit{disentanglement} of the pose from the shape information of the target.

Given that direct supervision for the deformed mesh is not available, we propose a cycle reconstruction that can be trained on realistic stylized meshes without the ground truth deformed mesh: the deformed mesh is exploited as a new target pose to reconstruct the original target mesh. This cycle reconstruction %is able to 
enforces the pose %being transferred
transfer from the target to the source mesh. %Moreover
Since our model operates on %pointcloud
keypoints, it is \textit{topology-agnostic} and thus can be applied to meshes with large shape variations and different topologies.
Furthermore, shape regularizers are also added to enforce the consistency between the deformed and source meshes. 
%The whole network is trained end-to-end with only supervision from the keypoints. 
% As shown in Fig.~\ref{fig:teaser}, our approach achieves accurate 3D pose transfer although we do not use ground truth paired data. 
%Our method is weakly-supervised, and only needs supervision of keypoints rather than ground truth deformed mesh.
We evaluate our approach on the commonly used SMPL-synthetic dataset NPT~\cite{neuralpt20} , SMAL-based~\cite{smal17} animal dataset and FAUST \cite{faust14} from real scans, where we outperform existing unsupervised approaches and even achieve comparable performance with fully-supervised approaches. To further evaluate our method on more complex and diverse topologies, we also collect a new 3D mesh dataset from Mixamo \cite{mixamo22}, where we show better performance than the existing work.
Experiments show the superiority of the proposed method compared to the state-of-the-art 3D pose transfer methods. 

\textbf{Our contribution} can be summarized as: 1) We propose a new 3D pose transfer framework for training data without ground truth supervision on the output deformed mesh. 2) Our approach is the first keypoint-based data-driven method for 3D pose transfer, and achieves better shape and pose disentanglement when combined with IK/FK. 3) Our approach is topology-agnostic, and thus can be applied to meshes with different topologies and non-T-pose source mesh. 4) We achieve superior performance compared to the state-of-the-art unsupervised approaches on FAUST dataset and supervised approaches on Mixamo dataset, and even achieve comparable performance with the fully supervised approaches on the NPT and SMAL datasets.

\section{Related work}
\paragraph{Fully-supervised 3D pose transfer.}
3D pose transfer, also known as deformation transfer has been intensively studied in both computer vision and graphics for a long time. DT~\cite{dt04} is a traditional explicit deformation transfer method for unregistered mesh with different topologies. It requires keypoints annotation and input meshes in the T-pose for optimization which is not always available. 
%
% In computer graphics, hand-crafted rigging and skinning, where users need to design an animation skeleton and propagate the motion through pre-designed skinning in an explicit way, has been the workhorse of articulated character animation in the last decades. With the rapid development of deep neural networks, hand-crafted annotation during inference time becomes unnecessary since deep networks are able to learn the similar mapping from annotated data. This is also known as auto rigging and skinning \cite{rignet20,nbs21}.
% %, which becomes a two-step task. 
% However, this line of methods requires that the source mesh is in the T-pose, and thus limits the application for flexible 3D pose transfer. Moreover, these methods estimate different rigs for different meshes which makes it impossible to apply to the pose transfer task \cite{rignet20}.
%
Recently, some deep learning-based 3D pose transfer methods have been proposed~\cite{neuralpt20, 3dpt21, geotrans21}. These works have achieved promising pose transfer performance by merging source and target information in an implicit way with paired ground truth supervision. NPT~\cite{neuralpt20} is the first end-to-end 3D pose transfer work. They treat the 3D pose transfer as a style transfer problem extended from \cite{ain17} to the point cloud domain with the content as the identity information and the pose as the style information.
% They propose SPAdaIN representing spatially adaptive instance normalization across the spatial dimensions independently for each channel and instance and then modulated with learned scale $\gamma$ and bias $\beta$. 
%Based on that, 
3DPT \cite{3dpt21} is then proposed to directly build the correspondence between source and target vertices by solving an optimal transport problem with the Sinkhorn-Knopp algorithm. Based on the estimated correspondence matrix, a coarse deformed mesh is obtained. 

Inspired by \cite{elaimtrans19} for image style transfer, an elastic instance normalization (ElaIN) module is further proposed to blend the statistics of the original features (coarse results) and the learned parameters from external data (target) elastically. GCT \cite{geotrans21} uses Transformer \cite{trans17} as the more powerful backbone for feature extraction. Furthermore, a direction-aware central geodesic contrastive loss is added to minimize the geodesic features for all the edges between the deformed and the ground truth meshes. However, all of these approaches require strong supervision with paired data, which is hard to obtain in practice. In contrast, we design a weakly-supervised 3D pose transfer approach, which only requires keypoints for supervision. More recently, SKF \cite{sk22} designs a pose transfer framework, which is skeleton-free and able to handle meshes with different topologies. However, T-pose for both source and target meshes is required even during the inference, which is not available for most cases.
% There is also related work on skeleton transfer \cite{skeleton20} which aims to transfer pose between different skeletons, but it cannot be applied directly on 3D mesh pose transfer since propagating such sparse transformation to the whole mesh vertices need extra supervision and careful design. 
On the contrary, our approach can transfer pose from the target mesh to the source mesh with any pose without the requirement of a T-pose shape.
% rely on supervision with synthetic paired data and performs well on synthetic datasets. They show the cross-dataset results, but the test data are registered with the SMPL and have the same mesh topology. In contrast to the supervised methods, our method is weakly supervised and %could
% can handle realistic stylized meshes with different topologies without paired supervision.
% % insert a sentence to highlight our differences/advantages compared to the fully supervised methods

% \toprule
\vspace{-3mm}
\paragraph{Unsupervised 3D pose transfer.}
There are also unsupervised 3D mesh disentangling methods \cite{disentangle20,limp20,neuralmorph21,ieugan21}. These works seek to use an auto-encoder structure to implicitly convert the input mesh into shape and pose latent code without the ground truth paired data as supervision. SPD \cite{disentangle20} designs a novel framework for unsupervised shape and pose disentanglement with latent codes, which can also be applied to 3D pose transfer by swapping the latent code between the source and target mesh. Furthermore, an as-rigid-as-possible constraint is added to the generated meshes with the source meshes to keep the shape information. However, the limitation of this work is that fixed mesh topology is a necessity for the spiral CNN network structure, thus limiting the model generalization ability. LIMP~\cite{limp20} adopts metric preservation to control the amount of geometric distortion incurring in the latent space and a differentiable geodesic loss for intrinsic preservation. 
% Similarly, IEUGAN~\cite{ieugan21} also utilizes a regional geodesic distance loss between the source and generated meshes for intrinsics preserving. However, geodesic computation is typically time-consuming and cannot be applied to stylized meshes with disjoint parts. They show results only on template-based datasets including MANO~\cite{mano17}, DFAUST\cite{df17}, and FAUST~\cite{faust14}. 
In comparison to these methods, our approach is topology-agnostic, where we can handle meshes with different topologies and %even 
disjoint parts. Moreover, our keypoint-based motion representation and propagation help to better disentangle the pose from the shape information during pose transfer. 
% insert a sentence to highlight our differences/advantages compared to the unsupervised methods
\vspace{-3mm}
\paragraph{Keypoints-based deformation.}
Keypoints-based deformation are well-studied in \cite{mvc05,kd21,cage20,handle21}. They achieve high-quality deformation based on keypoints detection and skinning-based motion propagation. The 3d keypoints are detected with the prior from Farthest point sampling in an unsupervised way \cite{kd21} or with pre-processed optimization to find the closest cages \cite{cage20}. For shape-preserving, cage-based \cite{mvc05,cage20} methods use Mean Value Coordinates (MVC) for smooth deformations. Other deformation methods adopt as-rigid-as-possible \cite{arap07} or Laplacian \cite{meshpro06} regularization to preserve shape details \cite{cage20}. However, their focus is on rigid objects without any articulated motion. In comparison, we are the first to apply keypoints-based deformation on the articulated objects for pose transfer.

\vspace{-3mm}
\paragraph{Comparison.}
We show our advantages over other methods in Tab.~\ref{table:adv} in terms of: 1) requirement of ground truth mesh for supervision; 2) requirement of additional T-pose during inference; 3) ability to transfer across different topologies; 4) implicit or explicit disentanglement. Note that our method and \cite{sk22} use transformation matrices as an explicit disentanglement method to exclude shape information being transferred. Implicit methods simply output shape and pose code, thus information tends to entangle together.

\begin{table}[htbp]
\centering
\small
\setlength{\tabcolsep}{0.08cm}
\begin{tabular}{|c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{Method} & \multicolumn{1}{c|}{w/o GT}&\multicolumn {1}{c|}{w/o T-pose}&\multicolumn {1}{c|}{Cross topologies}& \multicolumn {1}{c|}{Disentanglement}\\
 % Method&Train &Test & Per-scene training &Unseen Pose Body& PSNR &SSIM\\
\hline
\cite{dt04}&\cmark &\xmark &\cmark  &Explicit\\
\cite{limp20}&\cmark &\cmark &\cmark  &Implicit\\
\cite{disentangle20}&\cmark &\cmark &\xmark  &Implicit\\
\cite{neuralpt20}&\xmark &\cmark &\xmark  &Implicit\\
\cite{3dpt21}&\xmark&\cmark &\xmark  &Implicit\\
\cite{geotrans21}&\xmark&\cmark &\cmark&Implicit\\
\cite{sk22}&\xmark&\xmark &\cmark &Explicit\\
Ours&\cmark &\cmark &\cmark  &Explicit\\
\hline
\end{tabular} \vspace{2mm}
\caption{Our advantages compared with existing methods.} \vspace{-2mm}
\label{table:adv}
\end{table}

% Figure environment removed

\section{Our Method}
\label{Chapter3}
\paragraph{Problem definition.} Let ${X_{p1,i1}}$ and ${X_{p2,i2}}$ be the source mesh 
% which contains the desired identity information and
and the target mesh, 
% which contains the desired pose information, 
where $p$ represents the pose information and $i$ represents the identity information. 
% Our purpose 
The objective
is to generate 
% the third 
a deformed
mesh ${X_{p2,i1}}$ with the identity information from the source mesh and the pose information from the target mesh. Formally, we aim to learn a general function $\mathcal F(\cdot)$, represented with a deep network, such that:  
\begin{equation}
\label{eq:obj}
\mathcal F(X_{p1,i1},X_{p2,i2}) \mapsto X_{p2,i1}.
\end{equation}
\paragraph{Overview.} The main challenge is to train the network without paired training data, where the ground truth for $X_{p2,i1}$ in Eqn.~\eqref{eq:obj} is not available. We thus introduce a new framework to circumvent this issue. As shown %in 
on the left %part 
of Fig.~\ref{fig:framework}, our proposed framework contains four learnable components: 1) a keypoints detection model; 2) a twist prediction model; 3) a skinning prediction model; 4) a refinement model, two non-learnable parts: the IK and FK functions, and an LBS function.
% and 3) a GMM module for pseudo label generation.
% which aims to generate pseudo labels for skinning weights we do not draw on the Figure
Specifically, our approach learns the 3D pose transfer task in five steps. 1) \textbf{Keypoints Detection.} We start with keypoint detection of the input source and target meshes. 2) \textbf{Scalable Inverse and Forward Kinematics.} We then estimate the relative rotation matrices between the source and target using scalable inverse kinematics based on the corresponding detected keypoints. Forward kinematics is also adopted to compute the global transformation matrix for each bone of the source mesh. 3) \textbf{Motion propagation with GMM-based LBS.} 
Subsequently, we propagate the transformation matrix of each keypoint to all vertices of the source mesh. Since there is no ground truth supervision for the skinning weights, we design a GMM module for pseudo labels. 4) \textbf{Mesh Refinement.} To model the non-linear deformations, we further add a refinement network to model the non-rigid deformation to recover fine-grained details.  5) \textbf{Cycle and Self Reconstruction.} Given that there is no direct supervision for the output, we introduce a cycle and a self reconstruction that can be self-supervised with the input meshes. 

% Our approach consists of five steps. (1) Keypoints Detection, (2) Motion Estimation with Inverse Kinematics, (3) Motion propagation with LBS, (4) Mesh Refinement, and (5) Cycle and self reconstruction. We start with keypoint detection from the input source and target mesh. We then estimate the relative rotation matrix between the source and target based on the corresponding detected keypoints. Forward kinematics is also adopted to compute the transformation matrix for each bone of the source mesh. After that, we propagate the transformation matrix of each keypoint to all vertices of the source mesh with LBS. In order to model the non-linear deformations, we further introduce a refinement network to recover fine-grained details. Given that there is no direct supervision for the output, we adopt the cycle and self reconstruction that can be supervised with the input meshes. 
% The cycle consistency we use here is different from \cite{disentangle20}. For the reconstruction part, we find another mesh with different identity information as the target mesh, which could enforce the network to do shape and pose disentanglement.

\subsection{Keypoints Detection}
We first detect a set of keypoints for both the source and target meshes using a keypoint detector. 
Specifically, we define the keypoints as the joints in the SMPL model for the human shapes and SMAL model for the animal shapes such that the keypoints ground truth can be directly computed using the joint regressor \cite{smpl15}. For the non-template-based 3D meshes, \eg meshes in the Mixamo Dataset, we select the joints that are semantically similar to the SMPL keypoints with annotations in the dataset for keypoint supervision. 
To handle meshes of different topologies, we use a simple Pointnet and MLP as the keypoint detector, which we denote as $\mathcal{F^{K}}(\cdot)$. Given all the vertices of the source and target meshes, represented by $V_{s}$ and $V_{t}$, respectively, the keypoint detector predicts the keypoints as:
\begin{equation}
\centering
\begin{aligned}
k_{s}=\mathcal{F^{K}}(V_{s}), \quad k_{t}=\mathcal{F^{K}}(V_{t}).
\end{aligned} 
\label{kpnet} 
\end{equation}
The keypoint detector is supervised with the $L_2$ distance:
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_{k}=||k_{s}-&k_{s}^{gt}||_{2}+||k_{t}-k_{t}^{gt}||_{2},
\end{aligned} 
\label{kploss} 
\end{equation}
where $k^{gt}$ denotes ground truth keypoints.
% First, rather than estimate the point-to-point correspondence with the learned feature \cite{3dpt21}, we hope to capture correspondence in a more sparse and robust way without full supervision, which is realized by a keypoints detection network shared by the source and target point could. In this way, we make it a more compact model compared with complicated corresponding computing. Our focus is on human-shape meshes with similar structures, so we can pre-define a coherent set of keypoints for better generalization. For SMPL-based synthetic/registered data, we can easily compute the ground truth joints with a simple matrix as the keypoints supervision, for non-template 3D meshes like Mixamo, we use the joints semantically similar to SMPL-defined keypoints as the supervision. Here, we use a simple pointnet and MLP deformer as our keypoints detector. We use $k$ to denote the keypoints detection network.
% We use $P_{S}$ and $P_{T}$ to denote source and target point clouds, $K$ to denote keypoint detection network, $k$ to denote keypoints, and $k^{gt}$ to denote ground truth.
% \begin{equation}
% \centering
% \begin{aligned}
% k_{S},k_{T}&=K(P_{S},P_{T}),\\
% \mathcal{L}_{k}=||(k_{S}-&k_{S}^{gt}||_{2}+||(k_{T}-k_{T}^{gt}||_{2}
% \end{aligned} 
% \end{equation}

\subsection{Scalable Inverse and Forward Kinematics}
Given the keypoints of the source and target meshes in different shapes, we aim to infer the relative motion between them. The most naive way is to directly subtract target keypoints from source keypoints as the motion representation. However, this %will
lead to entanglement of the pose and shape information since different scales of the source and target meshes %will
also contribute to the motion representation. Therefore, we represent the keypoint motion with a transformation matrix instead of the motion vector. %Specifically
Different from the case of the original inverse kinematic \cite{comput85,robo84,leastik05} where the shape keeps fixed, the shape of the source and target meshes are generally different in our case, \eg the bone length. In view of this, we introduce a scalable IK to compute the relative rotations suitable for source and target with different shapes.
% This is similar to the inverse kinematics problem, the difference is that the original IK problem has the condition that the shape of the source and target are the same and the aim is to reconstruct target keypoints with the source keypoints with the derived transformations. However, for our problem, since the shape of the source mesh is different with the target, \ie the length of each bone for connected keypoints is different. Our aim is just to infer the relative rotations rather than reconstruction so we call it scalable IK.
Particularly, we sequentially compute the relative rotation matrices between the source and target keypoints following the kinematic tree by aligning the parent bone and then compute the global transformation matrices based on the pre-defined kinematic tree with the forward kinematics. We use the Twist-and-Swing Decomposition \cite{decomp01,hybrik21} to compute each local relative rotation matrix.
% After getting the source and target keypoints, we want to infer the sparse motion from these.  It is intuitive to directly subtract target keypoints from source keypoints as the motion representation, however, it mixes the target shape information into the deformation. In order to disentangle the pose from the shape, rather than use the motion vector between the source and target keypoints as the motion representation, we represent the kepyoints motion with a global transformation matrix. First, we use inverse kinematics to get the relative rotation matrix and then compute the global transformation matrix along the pre-defined kinematic tree. While the Forward Kinematics problem is well-posed, the inverse kinematics problem is ill-posed because there is either no solution or there are many solutions to fulfill the target joint locations.
% In order to formulate the articulation in a better way, we follow \cite{hybrik21} using Twist-and-Swing Decomposition to regress the local relative rotation matrix. 
% Connecting each keypoint with its parent forms a bone. We denote each of the bone vectors in the source keypoints as $\vec s$ and target keypoints as $\vec t$.
We define a bone as the connection between each keypoint and its parent and denote the bone vectors as $\vec s$ for the source and $\vec t$ for the target.
% Formally, the bone vector is defined as any two connected keypoints in the kinematic tree:
% \begin{equation}
% % \centering
% \label{eq:vector}
% % \begin{aligned}
% \vec v_{i}=k_{i}-k^{pa}_{i}
% % \end{aligned}
% \end{equation}
The relative rotation matrix $\mathcal{R}$ between each set of vectors $\vec s$ and $\vec t$ can be formulated as:
\begin{equation}
% \centering
\label{eq:dec}
% \begin{aligned}
\mathcal{R}=\mathcal{R}^{sw} \mathcal{R}^{tw},
% \end{aligned}
\end{equation}
where $\mathcal{R}^{sw}$,$\mathcal{R}^{tw}$represents the swing and twist component of the rotation matrix. The swing rotation has the axis $\vec n$ that is perpendicular to $\vec s$ and $\vec t$ as:
\begin{equation}
% \centering
\label{eq:perpen}
% \begin{aligned}
\vec n=\frac{\vec s \times \vec t}{||\vec s \times \vec t||}.
% \end{aligned}
\end{equation}
$\mathcal{R}^{sw}$ can then be formulated as: 
\begin{equation}
\centering
\begin{aligned}
& \mathcal{R}^{sw} = \textrm{I} +  \sin \alpha[\vec n]_{\times}  + (1-\cos \alpha) [\vec n]_\times^{2}, \\ 
\end{aligned}
\label{eq:sw}
\end{equation}
where $\cos \alpha=\frac{\vec s \cdot \vec t}{||\vec s|| \cdot ||\vec t||}$ with the swing angle $\alpha$. For the twist angle $\phi$, we use a simple network $\mathcal F^{\phi}(\cdot)$ to estimate the $\cos \phi^{s}$ from source and $\cos \phi^{t}$ from target relative to a reference pose:
\begin{equation}
\centering
\begin{aligned}
\cos \phi^{s}=\mathcal F^{\phi}(V^{s}), \quad \cos \phi^{t}=\mathcal F^{\phi}(V^{t}).
\end{aligned}
\label{eq:nettw}
\end{equation}
Subsequently, $\mathcal{R}^{tw}$ can be analytically computed based on the source keypoints skeleton, \ie: 
%
% \begin{equation}
% \centering
% \begin{aligned}
% \phi^{s},\phi^{t}=F^{\phi}(X^{s},X^{t})
% \end{aligned}
% \label{eq:nettw}
% \end{equation}
% 
\begin{equation}
\centering
\begin{aligned}
\mathcal{R}^{tw} = \textrm{I} + \frac{\sin \phi[\vec s]_{\times}}{||\vec s||^{2}}  +\frac{(1-\cos \phi)}{||\vec s||^{2}} [\vec s]_\times^{2},
% &\text{where $\quad \phi=\phi^{t}-\phi^{s}$}.
% cos \alpha&=\frac{\vec s \cdot \vec t}{||\vec s|| ||\vec t||}\\
\end{aligned}
\label{eq:tw}
\end{equation}
where $\phi=\phi^{t}-\phi^{s}$ represents the relative twist angle. 
$[\vec s]_{\times}$ is the skew-symmetric matrix of $\vec s$. Intuitively, the twist rotation is rotating around $\vec s$ itself, and thus we can determine the twist rotation $\mathcal{R}^{tw}$ according to $\vec s$ and the relative angle $\phi$.
The global transformation matrix for the $k^{th}$ bone $A_{1},\ldots,A_{K}$ can then be analytically computed using forward kinematics with:
\begin{equation}
\centering
\begin{aligned}
{A_{1},\ldots,A_{K}=\mathcal{F}\mathcal{K}(k_{s},\mathcal{R}_{1},\ldots,\mathcal{R}_{K}),}
% cos \alpha&=\frac{\vec s \cdot \vec t}{||\vec s|| ||\vec t||}\\
\end{aligned}
\label{eq:fk}
\end{equation}
where $K$ represents the total number of bones, $\mathcal{F}\mathcal{K}(\cdot)$ represents the whole Forward Kinematics function, $\{\mathcal{R}_1 \ldots, \mathcal{R}_K\}$ represents the relative rotation matrices for all $K$ bones computed from Eqn.~\eqref{eq:dec} and $k_{s}$ represents the source keypoints detected by our keypoints detector from  Eqn.~\eqref{kpnet}. 
% We refer the readers to \cite{hybrik21} for more details about the inverse and forward kinematics.
% In order to handle source and target keypoints in different shapes, we use naive IK rather than adaptive IK \cite{hybrik21} which is used for the case where source and target are from the same characters to reduce the bone length inconsistency error.

The combination of keypoint detection and inverse kinematics (IK) is %quite
crucial for shape-pose disentanglement. This is because the pose of the target mesh is explicitly extracted as the bone transformations, %in explicit way, which 
naturally filters out the shape information of the target mesh. Moreover, the transformation matrix, which only depends on the angle between each pair of bone vectors as shown in Eqn.~\eqref{eq:sw}, is invariant to the target scale. %In this way
As a result, we are able to transfer only the pose information from the target to the source while keeping the shape the same. We will show in the experiments that our formulation is better at disentangling the shape and pose information %,  comparing 
in comparison with existing works \cite{disentangle20,limp20,ieugan21} that enforce the disentanglement in an implicit way.

% This formulation has several advantages. First, the rotation matrix is a proper way to formulate articulation. Then, the combination of keypoints detection and IK(Inverse kinematics) is quite crucial for shape-pose disentanglement since the pose of the target mesh is extracted as the keypoints location and twist angle in an explicit way, so the shape and local details of the target mesh would be naturally discarded. What's more, the output transformation matrix is invariant to the target shape/scale, only affected by the angle between each pair of bone vectors in source and target $\vec s$ and $\vec t$ as shown in Eqn.\eqref{eq:sw}, which is essentially the pose, rather than the bone length $||\vec t||$ which contains target shape information, so the shape information in the target mesh would not be leaked into the deformed mesh.


\subsection{Motion Propagation with GMM-based LBS}
With the transformation matrix for all bones, the next step is to propagate the transformations of the sparse bones to all vertices of the source meshes. We use a network $\mathcal F^{S}(\cdot)$ to predict the skinning weights based on the source point cloud and keypoints:
\begin{equation}
\centering
\label{eq:skinpred}
\begin{aligned}
W=\mathcal F^{S}(V_{s},k_{s}),
\end{aligned}
\end{equation}
where $ W \in \mathbb{R}^{N \times K}$. However, given that the ground truth skinning weights are unknown, we design a distance-based method to compute the pseudo skinning weights as supervision. %To be more specific
Specifically, we model the pseudo skinning weights as a mixture of Gaussians with $K$ bone centers.
% After the transformation matrices for all bones are predicted for all the keypoints, we need to predict the skinning weights for the source mesh to propagate the sparse transformations. We expect the skinning weights to be learned from the data. Since there is no ground truth for the skinning weights, we create the pseudo label for skinning weights based on the distance to the detected keypoints in a fast and reasonable way. Inspired by \cite{lasr21}, we use a parametric GMM to calculate the pseudo label of skinning weights on the fly. To be more specific, we model the pseudo skinning weights as a mixture of Gaussians with $k$ bone centers. 
The probability of assigning the $i^{th}$ vertex to the $k^{th}$ bone is %$\bar w_{ik}$
defined as:
\begin{equation}
\centering
\label{eq:gmm}
\begin{aligned}
\bar w_{ik}= \operatorname{Softmax}\biggl( \exp{ \bigl\{-T (v_{i}-C_{k})Q_{k} (v_{i}-C_{k})\bigr\} } \biggr),
\end{aligned}
\end{equation}
where $C_{k} \in \mathbb{R}^3$ is the center of the $k^{th}$ Gaussian, and $Q^b$ is the corresponding precision matrix that determines the orientation and radius of a Gaussian. We only model the radius of the Gaussian, which is predicted by the network with the source as input. $T$ is the hyper-parameter to control the variance of the weights. We use a softmax function to ensure that the probabilities of assigning a vertex to all Gaussian centers sum up to one. Note that for the same identity, we always use the one common pose to compute the pseudo skinning weights based on the observation that the skinning weights for the same identity should not change too much with different poses which could provide a more stable training signal.
% Moreover, the skinning weight computed from the T-pose shape is less affected by different poses and thus .
% We further notice that skinning weights for the same character in different poses should be similar, thus we use a T-pose shape as the common mesh for each character to compute the skinning weights pseudo label. In this way, it could give more reasonable supervision because in T-pose mesh, the geometry is symmetric and each body part is separated evenly without overlap.
% The input of the skinning prediction network consists of the source mesh and the corresponding keypoints.
We define the skinning weights loss as the $L_{2}$ distance between the network prediction $w_{ik}$ and the pseudo skinning weights $\bar w_{ik}$ as:
% We simply use a pointnet as a feature extractor and use several 1D convolution layers to decode the skinning weights. We define the skinning weights loss as the $L_{2}$ distance between the network prediction $w_{ik}$ and the pseudo label $\bar w_{ik}$ computed by GMM as $\mathcal{L}_{skin}$ in \eqref{eq:skin},
% Note that we use $\bar w_{ik}$ by GMM as the pseudo label and $w_{ik}$ predicted from the network for LBS. 
%
\begin{equation}
\begin{aligned}
\mathcal{L}_{skin}=\frac{1}{NK}\sum_{i=1}^{N} \sum_{k=1}^{K} (||\bar w_{ik}-w_{ik}||)_{2},
\end{aligned}
\label{eq:skin}
\end{equation}
where $w_{ik}$ is an element of the blend weight matrix $W$, representing how much $k^{th}$ bone transformation affects the vertex $i$. $N$ and $K$ represent the number of vertices and bones. With the skinning weights, the transformation matrix of each vertex can be computed with LBS as:
\begin{equation}
\centering
\label{eq:lbs}
\begin{aligned}
G_{i}&=\sum_{k=1}^{K}w_{ik} A_{k},
\end{aligned}
\end{equation}
where $A_{k}$ is the transformation matrix for the $k^{th}$ bone and $G_{i}$ is the transformation matrix for vertex $i$. Finally, a coarse deformed mesh can be obtained by applying the transformation matrix to each vertex of the source mesh:
% Finally, the coarse deformed point cloud should be transformed by applying the earned transformation matrices to all the source vertices using Eqn.\eqref{eq:coarse}, where $V_{i}$ and $V_{i}$ denote the source and deformed vertices before refinement.
\begin{equation}
\begin{aligned}
v^{c}_{i}=G_{i}v^{s}_{i},
\end{aligned}
\label{eq:coarse}
\end{equation}
where $v^{s}_{i}$ and $v^{c}_{i}$ denote the vertices of the source and coarse deformed mesh, respectively.
\subsection{Mesh Refinement}
There are still artifacts in the LBS-based deformed meshes. We further add another refinement network to model the non-linear deformations:
\begin{equation}
\begin{aligned}
\Delta V =\mathcal F^{R}(V_{c},V_{s}), \\
V^{r}=V^{c}+\Delta V.
\end{aligned}
\label{eq:refinement}
\end{equation}
 The input of the refinement network consists of the coarse deformed mesh vertices $V_{c}$ and the source vertices $V_{s}$. $\Delta V$ denotes the predicted deformations and $V_{r}$ the vertices of the refined mesh.
%  Similarly, $v^{c}_{i}$ is an element of $V_{c}$ and $v^{r}_{i}$ is an element of $V_{r}$. 
 As shown in Fig.~\ref{fig:framework}, We first extract the point-wise feature of the source shape as the condition and then feed it together with the coarse mesh into the refinement model. More details about the mesh refinement network are provided in the supplementary. Finally, we regard the output mesh of the refinement network as the final deformed mesh where all the loss terms are enforced.
% With the coarse results deformed by the LBS module, we then add one refinement network to recover the details of the source mesh as well as to model non-rigid motions in a non-linear way using the coarse result and the source mesh as the input. We adopt a simple but effective refinement network to our framework to recover the lost details in the coarse result. To be more specific, 
To enforce the shape consistency between the output deformed mesh and the source mesh, we utilize an edge loss for %shape-preserving
shape preservation. Specifically, we enforce the edge length of the deformed mesh to be the same as the input source mesh. This is based on the prior knowledge that all the edges of a mesh should not be stretched or squeezed too much when re-posed. The 
edge loss is defined as the $L_{2}$ distance of the edge length between the source and final deformed meshes:
% In order to prevent some unnatural artifacts, we utilize edge loss for shape-preserving between the deformed mesh and the source mesh. To be more specific, we minimize the corresponding edge length change for all the edges in the source mesh before and after deformation with the prior observation that most of the edges would not be stretched or squeezed too much. $L_{2}$ loss is applied for all the edges on average. The edge loss is defined in Eqn.\eqref{eq:edge} as following:
\begin{equation}
\centering
\begin{aligned}
\mathcal{L}_{edge}=\sum_{i=1}^{N} \sum_{j=1}^{N(i)} (||\bar e_{ij}-e_{ij}||)_{2},
\end{aligned}
\label{eq:edge}
\end{equation}
where 
% $i$ is the vertex index and $j$ is the vertex index for the neighboring index connected with vertex $i$, 
$\bar e_{ij}$ and $e_{ij}$ represent edges connecting vertex $i$ and $j$ in source and deformed mesh.
\subsection{Cycle and Self Reconstruction}
\label{cylce}
We do not assume the paired ground truth data for training. This means we do not have direct supervision for the output deformed mesh. To further enforce that the pose of the target mesh is transferred to the deformed mesh, we introduce a cycle reconstruction task to reconstruct the input target mesh from the deformed mesh. Specifically, we select three meshes as a triplet: a source mesh ${X_{p1,i1}}$, a target mesh ${X_{p2,i2}}$, and a third mesh with same identity but different pose %with
from the target mesh as ${X_{p3,i2}}$. Note that this type of triplet data is easy to obtain since the only requirement is that meshes with the same identity have different poses, which are very common in the existing datasets.
% Note that here our paired data refer to using two meshes with the same identity commonly existing in datasets.
% While ground truth pair used in supervised methods \cite{neuralpt20},\cite{geotrans21},\cite{3dpt21} refer to two meshes with different identities but exactly the same pose information which only exists in synthetic data.
As shown on the right %part 
of Fig.~\ref{fig:framework}, we first estimate the deformed mesh ${\bar X_{p2,i1}}$ from the source ${X_{p1,i1}}$ and target ${X_{p2,i2}}$ as:
\begin{equation}
\centering
\begin{aligned} 
\bar X_{p2,i1}=F(X_{p1,i1},X_{p2,i2}).
\end{aligned}
% \label{eq:cycle}
\end{equation}
We then use the deformed mesh as the new target mesh and the third mesh ${X_{p3,i2}}$ as the source mesh to reconstruct the original target mesh:
\begin{equation}
\centering
\begin{aligned} 
\bar X_{p2,i2}=F(X_{p3,i2}, \bar X_{p2,i1}).
\end{aligned}
\end{equation}
Intuitively, since the third mesh contains the same identity information as the original target, we can only reconstruct the original target mesh only when the deformed mesh contains the same pose information as the target. Finally, the cycle reconstruction loss is computed as the Point-wise Mesh Euclidean Distance (PMD) between the reconstructed mesh with the target mesh given by:
\begin{equation}
\centering
\begin{aligned} 
\mathcal{L}_{cycle}=\frac{1}{N} \sum_{v=1}^{N}||\bar X_{p2,i2}^{v} -X_{p2,i2}^{v}||_{2}^
{2},
\end{aligned}
\label{eq:cycle}
\end{equation}
where $X^{v}$ represents mesh vertex and $v$ represents index of the vertex. We also adopt the self reconstruction, which transfers pose between the meshes with the same identity, to further enhance the pose transfer. Specifically, we use $X_{p1,i1}$ as the source and $X_{p2,i1}$ as the target to reconstruct $X_{p2,i1}$:
\begin{equation}
\centering
\begin{aligned} 
\bar X_{p2,i1}=F(X_{p1,i1},X_{p2,i1}).
\end{aligned}
% \label{eq:pair}
\end{equation}
There are two benefits of self reconstruction: %are twofold: 
1) This type of data is easy to obtain, and 2) the supervision can be directly applied to the output deformed meshes.
We minimize the PMD between the reconstructed mesh $\bar X_{p2,i1}$ and $X_{p2,i1}$:
\begin{equation}
\centering
\begin{aligned} 
\mathcal{L}_{self}=\frac{1}{N} \sum_{v=1}^{N}||\bar X_{p2,i1}^{v}-X_{p2,i1}^{v}||_{2}^{2}.
\end{aligned}
\label{eq:self}
\end{equation}
In addition to the point-wise distance for both cycle and self reconstruction, we add an edge length loss between the reconstructed and the original meshes.
% Note that both the cycle and self-consistency loss are enforced to the mesh after the refinement network.

\subsection{Total Loss}
The total loss is the weighted summation of all the losses given by:
\begin{equation}
\centering
\begin{aligned} 
\mathcal{L}_{full}=\lambda_{k}\mathcal{L}_{k}+\lambda_{skin}\mathcal{L}_{skin}+\lambda_{cycle}\mathcal{L}_{cycle}\\+\lambda_{self}\mathcal{L}_{self}+\lambda_{edge}\mathcal{L}_{edge},
\end{aligned} 
\label{eq:total}
\end{equation}
where %$\mathcal{L}_{full}$ represents the total loss and 
$\lambda_{k}$, $\lambda_{skin}$, $\lambda_{cycle}$, $\lambda_{self}$, $\lambda_{edge}$ represent the weights for corresponding loss terms.

\section{Experiments}
\subsection{Datasets}
We conduct our experiments on 4 datasets. NPT~\cite{neuralpt20} is an SMPL-based synthetic dataset that consists of 3D human meshes with different shapes and poses sampled from a specific distribution. We follow the training and testing list used in~\cite{neuralpt20}. SMAL Dataset is a synthetic animal dataset generated with SMAL~\cite{smal17} model. FAUST~\cite{faust14} Dataset consists of real 3D human mesh scans with the same vertices and topology as the SMPL model, which contains 100 meshes including 10 different subjects in 10 poses. Following LIMP, we use the same 80 meshes for training and the remaining 20 meshes for testing.

We also collect a stylized character dataset from Mixamo \cite{mixamo22}, which includes 25 characters and up to 2000 motions for each character. The meshes in this dataset have more complicated shapes, such as humans with clothes and stylized characters. Moreover, the poses of each character are also more diverse, including lying down, squatting, dancing, etc. We use this dataset to validate that our approaches can handle more complicated shapes, poses, and even shapes of different topologies.

% identity to validate that our method could also be adapted to non-template meshes with different connectivity and complex structures. 
% The number of frames ranges from around 100 to 300 for each motion sequence. Each character has different topologies, with the number of vertices ranging from 3000 to more than 20,000. We downsample the original meshes to 6890 vertices to keep consistent with the SMPL model. It includes much more complicated shapes like clothed humans and some cartoon characters for both males and females. There are many more complex motions for each identity than in the SMPL model, like lying down, squatting, and dancing.
% We convert the original FBX file, a motion sequence to the OBJ files, which are frame-wise static meshes. For those characters with overlap motions, we collect a small part of these kinds of meshes as ground truth to evaluate our methods.
\subsection{Implementation Details}
We set the hyper-parameters as $\lambda^{k}=2$, $\lambda^{cycle}=1$, $\lambda^{self}=1$, $\lambda^{edge}=0.0005$, which is the same as NPT for all the datasets. The weights for the skinning weights loss $\lambda^{skin}$ are set differently, namely 0.4 for the NPT and SMAL Dataset and 0.1 for the Mixamo and the FAUST Dataset according to the results of the experiments. We use the multi-stage learning rate decay strategy, where the decay rate of 0.3 is applied for 4 times at the 10,000-th, 20,000-th, 30,000-th, and 40,000-th iterations. Details about our network structure are shown in the supplementary.
% We train the whole network end-to-end for 50,000 iterations. 
% The whole training process takes around 12 hours on one GeForce RTX 3090Ti GPU. We use the multi-stage learning rate decay strategy, where the decay rate of 0.3 is applied for 4 times at the 10,000-th, 20,000-th, 30,000-th, and 40,000-th iterations. 

% To form the triplet data for cycle and self reconstruction, we randomly select the source and target with different shapes and poses, and the third mesh with the same shape but a different pose from the target is also selected. 
% We treat training with one triplet data as one iteration and train the entire model end-to-end for 50,000 iterations. 
% We use the multi-stage learning rate decay strategy, where the decay rate of 0.3 is applied for 4 times at the 10,000-th, 20,000-th, 30,000-th, and 40,000-th iterations.

% Figure environment removed

% % Figure environment removed
% Figure environment removed
% % Figure environment removed

% Figure environment removed

% Figure environment removed

% Figure environment removed  
\subsection{Comparison with Supervised Methods}
Supervised methods can only be trained on synthetic datasets, where ground truth meshes can be synthesized. We first compare our method with the existing supervised approaches on the synthetic dataset using the template (SMPL and SMAL): NPT and SMAL Dataset. The commonly used PMD (1e-4)~\cite{neuralpt20} is used as the evaluation metric. We follow the original train and test split in \cite{neuralpt20} and \cite{3dpt21}. 
% The test list contains 72 unseen and 72 seen poses as the target with all unseen shapes as the source. 
% We report the error either by testing with their pre-trained models or using the numbers in their original papers.
The results are shown in Tab.~\ref{npt} and Tab.~\ref{smal}. We can see that our proposed \textit{weakly-supervised} approach achieves comparable or even better performance compared with existing \textit{fully-supervised} approaches on both human and animal datasets. We show qualitative results for the animal dataset in Fig.~\ref{fig:smalvis}.
\begin{table}[htbp]
\centering
\setlength{\tabcolsep}{0.15cm}
\begin{tabular}{lccccc}
\hline
 Method& Ours & 3DPT\cite{3dpt21} &GCT\cite{geotrans21} &NPT\cite{neuralpt20}\\
\hline
PMD (1e-4) $\downarrow$ &1.47& \textbf{1.23} &2.3&5.2\\
% Chamfer Distance &4.33&2.97& -&-\\
\hline
\end{tabular} \vspace{2mm}
\caption{Comparison on NPT Dataset with 3DPT, GCT and NPT. Note that all the other methods are \textit{fully-supervised} with ground truth mesh while we are only \textbf{\textit{weakly-supervised}} by keypoints.}
\label{npt}
\end{table}	

\begin{table}[htbp]
\centering
\begin{tabular}{lccccc}
\hline
 Method& Ours & 3DPT\cite{3dpt21} &NPT\cite{neuralpt20}&SPD\cite{disentangle20}\\
\hline
PMD (1e-4) $\downarrow$ &2.98& \textbf{2.26} &6.75 &25.1\\
% Chamfer Distance &4.33&2.97& -&-\\
\hline
\end{tabular} \vspace{2mm}
\caption{Comparison on the SMAL Dataset with 3DPT, NPT and SPD. Note that 3DPT and NPT are \textit{fully-supervised} with ground truth mesh while we are only \textbf{\textit{weakly-supervised}} by keypoints.} \vspace{-2mm}
\label{smal}
\end{table}	
% We conduct the comparison with supervised methods on NPT Dataset. We use commonly used PMD (1e-4) \cite{neuralpt20} for 3D pose transfer with the ground truth for comparison.
% On NPT Dataset, we use the same training and testing dataset from \cite{neuralpt20} for all the methods in Tab.\ref{npt}. From the table we can see, that even without ground truth supervision, we achieve comparable results with the SOTA method, even better than the rest fully supervised methods.
\subsection{Comparison with Unsupervised Methods}
We test our method on the FAUST Dataset and compare it with existing unsupervised approaches. We do not compare with supervised approaches since the paired training data is not available for this dataset. We use the same training and testing dataset with LIMP. The results of LIMP are obtained by directly testing with their pre-trained model. For SPD, we retrain their method on this dataset until convergence since the original model is not trained with the FAUST Dataset. As shown in Tab.~\ref{table:faust}, we can see our method outperforms the unsupervised method SPD, LIMP by a large margin.
\begin{table}[htbp]
\centering
\begin{tabular}{lccccc}
\hline
 Method& Ours & SPD\cite{disentangle20} & LIMP\cite{limp20}\\
\hline
PMD (1e-4) $\downarrow$ & \textbf{11.70}& 22.31&23.51\\
\hline
\end{tabular} \vspace{2mm}
\caption{Comparison on the FAUST Dataset with SPD and LIMP.}
\label{table:faust}
\end{table}
% We compare unsupervised methods on FAUST Dataset. We use the same training and testing dataset from \cite{limp20} and we use their pre-trained model for comparison. For SPD\cite{disentangle20}, we retrain their method on this dataset with the same data until convergence since their original model is not trained on FAUST Dataset and trained on a larger dataset. From Tab.\ref{table:faust}, we can see our method outperforms the state-of-the-art unsupervised method\cite{disentangle20},\cite{limp20} with a large margin. 
We also show a qualitative comparison on the FAUST Dataset in Fig.~\ref{comp1}. We can see that our results are much better in terms of both shape-preserving and pose transfer. %To be more specific
%Particularly, 
The results of LIMP fail to preserve the shape details, shape information of the source mesh is lost in the generated mesh, \eg the belly of the source mesh disappears in their results. Additionally, the pose is not transferred correctly, \eg the pose of the legs shown in the fourth column second row is wrong.
% Note that the original implementation of LIMP uses the down-sampled meshes with 2100 vertices for both training and testing, so their results look a little corrugated, but the error is computed on average for all the vertices for a fair comparison.
% Figure environment removed


For SPD in the third column, both examples show that the generated mesh does not preserve the source shape well. In comparison, our approach can transfer the pose from the target and keep the shape of the source. We credit this good disentanglement to our keypoint-based motion propagation and scalable inverse kinematics which is shape invariant.
% and the cycle and self-consistency constraints.
% Moreover, although LBS is known to suffer from the candy effects, we do not have this problem since we have considered the non-linear deformations in our refinement network.
% For visual comparison, we show examples with the source, target, and our deformed mesh, as well as the deformed mesh by  SPD\cite{disentangle20} and LIMP\cite{limp20}. From Fig.\ref{comp1}, we can see that compared with the state-of-the-art unsupervised method on FAUST Dataset, we get much better results in terms of shape-preserving and pose transfer. LIMP\cite{limp20} tends to entangle shape from the target mesh into the deformed mesh and SPD\cite{disentangle20} fails to preserve details and produces noisy artifacts. For LIMP\cite{limp20}, it uses both Euclidean and geodesic distance as the metric preserving constraints for pose and shape disentanglement. However, as shown in Fig.\ref{comp1}, the generated results fail to preserve the shape details at all, and the pose is not even transferred correctly in the second-row example. SPD\cite{disentangle20} employ self and cross consistency as the substitution for pair-supervision, however, only trained on the small dataset of FAUST with 80 meshes, it is obvious that deformed mesh tends to entangle target shape in the encoding-decoding process shown in Fig.\ref{comp1} although the pose is transferred successfully. Our methods could transfer the pose from the target mesh while disentangling the shape well. We credit this good disentanglement to our keypoint-based motion propagation and inverse kinematics which is shape invariant. Also, from the visual results, we can see, that our model considers both articulated and non-rigid motion, so the candy effects or some unnatural artifacts at the joints which classic LBS often suffers could be eliminated in our case.

\subsection{Cross Topology Evaluation}
% Note that since our collected mesh contains different topologies, spiral-CNN-based method SPD\cite{disentangle20} could not be applied since the fixed network structure and LIMP\cite{limp20} could not be trained either since the geodesic distance is not available for the disjoint part in this dataset.
\begin{table}[htbp]
\centering
\begin{tabular}{lcccc}
\hline
 Method& Ours&SKF\cite{sk22}\\
\hline
CD (1e-4) $\downarrow$ & \textbf{22.5} & 23.5\\
\hline
\end{tabular} \vspace{2mm}
\caption{Comparison on the Mixamo Dataset with SKF.}
\label{table:com2}
\end{table}	
We further conduct experiments on the Mixamo Dataset to show that our methods can be applied to non-template stylized meshes, \ie meshes with different topologies. We show the results in Tab.~\ref{table:com2}, where we use the Chamfer distance (CD) as the evaluation metric. We compare with SKF \cite{sk22}, which can also handle different topologies. We do not include the supervised approaches 3DPT and NPT since the ground truth is not available for training, we show cross-dataset comparison with them on this dataset in Tab.~\ref{table:cross}.  SPD only works for fixed topology, which is not the case here. We directly take the pre-trained model on the Mixamo Dataset and evaluate it on our test dataset since their test pairs are not available. Note this comparison places our method (Ours) at a disadvantage since SKF is trained with ground truth skinning weights and paired data (when available). Additionally, SKF also requires the T-pose of the driving mesh as input during inference. 
As can be seen from Tab.~\ref{table:com2}, our approach still outperforms SFK although we only use weak keypoint supervision. We also show qualitative comparisons with SKF in Fig.~\ref{mixamo}. As shown in the third column of Fig.~\ref{mixamo}, SKF fails to preserve the geometric details of the source meshes, \eg the unnatural bending at the arms, legs, and hands highlighted in the red box. In comparison, our method successfully transfers the pose from the source to the target and also preserves the shape details of the source mesh. Also, in Fig.~\ref{fig:extreme}, we show our model can transfer pose well in cases with larger shape variation. We show more qualitative results for all the datasets in the supplementary.

\subsection{Cross Dataset Evaluation}
 We show cross-dataset evaluation (PMD $1e^{-3}$) with 3DPT \cite{3dpt21} and NPT\cite{neuralpt20}. 3DPT and NPT are trained on NPT Dataset with ground truth deformed mesh, while ours is waekly-supervised only with keypoints. All methods are tested on the Mixamo Dataset. As shown in Tab~\ref{table:cross}, we can see our method has stronger generalization ability across datasets even with weak supervision.
 \begin{table}[htbp]
\centering
\begin{tabular}{lcccc}
\hline
 Method& Ours&3DPT\cite{3dpt21}&NPT\cite{neuralpt20}\\
\hline
PMD (1e-3) $\downarrow$ & \textbf{15.0} &29.8&23.5\\
\hline
\end{tabular} \vspace{2mm}
\caption{Cross dataset evaluation with 3DPT and NPT.} \vspace{-3mm}
\label{table:cross}
\end{table}	

% For supervised approaches, \ie 3DPT and NPT, there is no ground truth as supervision in the Mixamo training dataset. For the unsupervised approaches, SPD can not be trained on this data since fixed topology is required for their network. LIMP adopts the geodesic distance for shape-preserving during training, which is unavailable for this dataset because of disjoint parts. Thus we compare with SKF \cite{sk22} which is trained on the Mixamo dataset.
% As their methods need source mesh in T-pose, we compare their methods with source meshes in T-pose. Note that our methods could also be applied to non-T-pose source meshes as shown in Fig.~\ref{fig:teaser}. We use their pre-trained model on the Mixamo dataset to evaluate our test dataset since their test pairs are not available. The quantitative results are shown in Tab.~\ref{table:com2}. After down-sampling, the point-wise correspondence between the deformed mesh and the ground truth is lost, so PMD can not be used, so we use Chamfer distance (CD) for evaluation. SKF gets the ground truth skinning weights and paired data for supervised training while ours only use keypoints as supervision. SKF needs additional driving mesh in a T-pose shape as input during inference, while our method does not need such extra data as input but achieves even better results. 
% We also show visual comparisons with SKF in Fig.~\ref{mixamo}. As shown in the third column of Fig.~\ref{mixamo}, SKF fails to preserve the details of the source meshes, \eg the unnatural joints and noise in the hands, while our method successfully retains the details of the source mesh and transfers the source to the target pose.
% The results demonstrate that our methods can be adapted to non-template-based styled meshes successfully.

% Therefore, we compare 3DPT and NPT with our method. For a fair comparison, we use the same training data (without using ground truth pairs) on the NPT Dataset as supervised methods \cite{3dpt21,neuralpt20}. Then we test our model and their pre-trained model on the Mixamo Dataset and the results are shown in Tab.\ref{table:mixamo}. We can see that our approach without finetuning(w/o FT) on the Mixamo Dataset already outperforms existing approaches. The performance is significantly improved when we train our method on this dataset while others can not be trained without paired data. We also show our finetuned qualitative results in Fig.~\ref{exp2}. The results demonstrate that our methods can be adapted to non-template-based styled meshes successfully.

% In order to validate that our methods could be adapted to non-template-based styled meshes. We conduct experiments on Mixamo Dataset. We select 20 characters with different poses for training, note there is only a small overlap of motion for different characters in the training dataset and we do not touch such ground truth pair during training. We also collect a small dataset containing unseen characters in 100 pairs with ground truth in total for testing.
% All the supervised methods can not be trained on this dataset since ground truth is unavailable. For unsupervised methods, SPD\cite{disentangle20} can not be trained or tested on this data since the fixed topology requirement of their network, LIMP needs geodesic distance computing for shape-preserving during training which is unavailable for this dataset because of disjoint parts. Thus we directly test available methods on our collected test datasets. The results are shown in Tab.\ref{table:mixamo}. NF represents no finetuning, directly using our model trained on NPT and testing it on Mixamo which is fair to compare NPT and 3DPT. Our method achieve better performance than NPT and 3DPT. With finetuning, our method achieves much better performance. 
% LIMP trained on FAUST also could not generalize to Mixamo at all. 
% We also show visual results of one character as the source driven by 4 target characters in Fig.\ref{exp2}. The results demonstrate that our methods could be adapted to non-template-based styled meshes successfully and give reasonable deformation in terms of shape-preserving and pose transfer.
\subsection{Skinning weights visualization}
We compare our learned skinning weights with Ground truth skinning weights on NPT and SMAL datasets. As shown in Fig.~\ref{fig:skinning}, we can see our unsupervised-learned skinning weights are reasonable and similar to Ground truth skinning weights.
\subsection{Ablation Study}
We conduct an ablation study for each component of our proposed approach.
% including the refinement network, cycle and self reconstruction loss, edge length loss, and skinning weights loss. 
As seen from Tab.~\ref{table:ablation}, the error gets larger when each component is removed from the full pipeline, especially for the model without the cycle reconstruction loss or the skinning weights loss. We also show a qualitative comparison in Fig.~\ref{abation} and we highlight the part with obvious artifacts in the red box.
% We conduct ablation studies in terms of the effectiveness of several loss terms and network structures on both NPT and FAUST Datasets. More precisely, we consider cycle and self-consistency loss, skinning weights loss supervised by the pseudo label from the GMM model, and edge loss for the shape-preserving and refinement network. The results are shown in Tab.\ref{table:ablation} and Fig.\ref{abation}. From Fig.\ref{abation}.
We can see that GMM-based skinning weights loss and cycle reconstruction loss guarantee accurate pose transfer. Without GMM-based skinning weights as supervision, the pose is not transferred well, \eg the left leg is not correct compared with ground truth as shown in the red box. Without cycle reconstruction loss which serves as a pose constraint, the pose of the deformed mesh is also not correct, \eg both legs are in the wrong positions as shown in the red box. 
Refinement network and edge loss help in shape preserving. Without the refinement network, the details on the leg part are not well-preserved. Without the edge loss, the left thigh is stretched too much as shown in the red box. In comparison, both the pose and shape of the deformed mesh from our full model are closer to the ground truth. 
% while results from our full model look more natural compared with others.  Training with full loss terms and the refinement network could get the best performance in both datasets.
\begin{table}[htbp]
\centering{
\small
\setlength{\tabcolsep}{0.15cm}
\begin{tabular}{ccccc|cc}
\toprule
\multicolumn{5}{c|}{Method} & \multicolumn{2}{c}{PMD (1e-4) $\downarrow$} \\
\cmidrule{1-7}
Refinement& $L_{cycle}$ & $L_{self}$ & $L_{edge}$& $L_{skin}$&  \textit{NPT} & \textit{FAUST}\\
\midrule
\xmark &\cmark & \cmark & \cmark  &\cmark &3.28 &22.3\\
\cmark & \xmark& \cmark & \cmark  &\cmark &6.33 &23.5\\
\cmark & \cmark & \xmark& \cmark  &\cmark &2.44 &18.1\\
\cmark & \cmark & \cmark & \xmark & \cmark &2.40&14.9\\
\cmark & \cmark & \cmark & \cmark & \xmark &6.48&25.6\\
\cmark & \cmark & \cmark & \cmark & \cmark &\textbf{1.47}&\textbf{11.7}\\
\bottomrule
\end{tabular}} \vspace{2mm}
\caption{Ablation studies on the NPT and FAUST Datasets.} \vspace{-2mm}
\label{table:ablation}
\end{table} 


\section{Conclusion}
In this paper, we have proposed a novel keypoint-based framework for 3D pose transfer. A cycle reconstruction constraint is designed to enforce self-supervised pose transfer without ground truth. Combining the keypoint-based motion estimation and Scalable IK, our method is able to disentangle shape and pose information better than existing works. In the absence of skinning weights supervision, we design a GMM module to generate pseudo label as guidance. Our approach is topology-agnostic and pose-agnostic, and therefore can be applied to non-template-based 3D meshes with different topologies and source meshes in non-T-pose. Quantitative and qualitative results on several benchmark datasets show the superiority of our proposed approach compared with existing approaches.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}