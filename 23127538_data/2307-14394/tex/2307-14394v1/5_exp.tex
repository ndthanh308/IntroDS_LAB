\section{Experiments}
In this section, we conduct experiments on seven graph and $12$ hypergraph datasets, which consist of both synthetic and real-world datasets. Additionally, We include an ablation study to compare the two proposed types of hypergraph Weisfeiler-Lehman kernels. Furthermore, we compare the computation runtime and robustness of these methods when facing data noising challenges.

\subsection{Common Settings}
This subsection outlines the standard experimental settings applied across all experiments, which may be overridden in each specified experiment. In the following subsections, the abbreviation \textit{WL} is utilized to represent \textit{Weisfeiler-Lehman}.

% \subsubsection{Compared Methods}
\textbf{Compared Methods. }
Since we focus on pure kernel methods on correlation data, for a fair comparison, we select four typical graph/hypergraph kernel-based methods.
%, including GraphLet Kernel \cite{graphlet}, Graph WL Subtree Kernel \cite{graph_wl_subtree}, Hypergraph Rooted Kernel \cite{hg_root}, and Hypergraph Directed Line Kernel \cite{hg_line}. Hypergraph WL Subtree Kernel and Hypergraph WL Hyperedge Kernel are the proposed methods.  
\textbf{GraphLet Kernel \cite{graphlet}.} A typical sampling-based graph kernel method. It draws and counts small factor graphs from a given graph. 
\textbf{Graph WL Subtree Kernel \cite{graph_wl_subtree}.} The method is naturally generalized from the Graph Weisfiler-Lehman algorithm, which counts the rooted subtrees of a given graph. 
\textbf{Hypergraph Rooted Kernel \cite{hg_root}.} A typical sampling-based hypergraph kernel method. It draws and counts hyper-paths from a given hypergraph.
\textbf{Hypergraph Directed Line Kernel \cite{hg_line}.} Hypergraph Directed Line Kernel is a transformation-based method that transforms the hypergraph into the directed graph. Then, the directed Weisfiler-Lehman algorithm is applied to extract the feature vector of a given hypergraph.



\subsubsection{Training Details}
For a fair comparison, we set the random seed in the $[2021, 2025]$ range, and the 5-fold cross-validation is adopted for each experiment. We report the average results of five folds and five seeds for all methods. Since those kernel methods can only generate the graph/hypergraph features, we utilize the standard SVM \cite{SVM, SVM_lib} as the classifier to compare those methods. Note that the SVM is initialized with the same hyper-parameters for all methods.% More details are provided in Appendix xxx.

\subsubsection{Other Details}
To assess the efficacy of capturing distinct correlation structures, we exclude the original vertex features of all datasets. Each vertex label is initialized based on the vertex degree, and the edge/hyperedge label is initialized according to the edge/hyperedge degree. Note that the edge label in graphs remains constant at two. 

Since traditional graph kernel methods cannot be directly applied to hypergraphs, we employ the common technique of clique expansion \cite{hgnnp} to transform the hypergraphs into graphs. As for the hypergraph kernels confronting the graph datasets, we consider these graphs as 2-uniform hypergraphs directly.

For the single-label classification task, we evaluate the accuracy (Acc) and macro F1 score (F1\_ma) as performance metrics. For the multi-label classification task, we compare the exact match ratio (EMR) and example-based accuracy (EB-Acc) as evaluation measures.


\subsection{Experiments on Graph Datasets}
This subsection presents the experiments on graph datasets, including two synthetic and five real-world graph datasets. The statistics of these datasets are summarized in Table \ref{tab:stat_graph}.


\begin{table}%[!htbp]
\centering
\caption{Statistic information of seven graph datasets.}
\label{tab:stat_graph}
\begin{threeparttable}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\# graphs} & \multirow{2}{*}{\# classes} & Avg. & Avg. \\
 &  &  & \# vertices & \# edges \\
 \midrule
RG-Macro & 1000 & 6 & 27.48 & 47.7 \\
RG-Sub & 1000 & 6 & 27.48 & 47.7 \\
IMDB-BINARY & 1000 & 2 & 19.8 & 96.5 \\
IMDB-MULTI & 1500 & 3 & 13.0 & 65.9 \\
MUTAG & 188 & 2 & 17.9 & 19.8 \\
NCI1 & 4110 & 2 & 29.9 & 32.3 \\
PROTEINS & 1113 & 2 & 39.1 & 72.8 \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item ``\#'' denotes ``number of'', and ``Avg.'' is short for ``Average''.
    \end{tablenotes}
\end{threeparttable}
\end{table}



\subsubsection{Datasets}

\textit{Synthetic Graph Datasets:} We have devised and generated two synthetic graph datasets, RG-Macro and RG-Sub, to test different algorithms' abilities on diverse low-order structures. We employed a two-step approach to create these graphs. First, we handpicked six widely-used graph structures as "subgraph" factors, including complete, bipartite, circle, cube, star, and wheel graphs. Randomly picked constructor parameters generate each graph. As an illustration, a circle graph characterized by a parameter $ \delta = 5 $ comprises a set of five interconnected nodes, with each node being connected to its neighboring node by a single edge. Second, we devise six ways to combine the "subgraph" factor graphs to produce a larger graph, such as chain linking, star linking, circle linking, and more. These linking strategies were recorded as a "macro" structure of the graph. Both datasets contain the same graphs, but the target label is different. The RG-Macro dataset uses the "macro" graph structure as a label, while the RG-Sub dataset tests the ability to differentiate between "subgraph" structures. %For further details and analysis, please see the Appendix xxxx.

\textit{Real-world Graph Datasets:} IMDB-BINARY\cite{IMDB} and IMDB-MULTI\cite{IMDB} are movie collaboration datasets. Each vertex in the two datasets is an actor. Each movie corresponds to a graph and associates a genre as its classification target. If two actors exist simultaneously in another movie, an edge will be created to link them. IMDB-BINARY contains movies from two genres: Action and Romance. IMDB-MULTI is a multi-class version of IMDB-BINARY and contains Comedy, Romance, and Sci-Fi genres. NCI1\cite{NCI1}, MUTAG\cite{MUTAG}, and PROTEINS\cite{PROTEINS} are bioinformatics datasets. NCI1 is collected from the National Cancer Institute (NCI) dataset. It is a balanced subset of chemical compounds screened for suppressing or inhibiting the growth of a panel of human tumor cell lines, with 37 discrete labels. MUTAG is a mutagenic aromatic and heteroaromatic nitro compounds dataset with seven discrete labels. PROTEINS is a dataset where nodes are secondary structure elements (SSEs), and there is an edge between two nodes if they are neighbors in the amino-acid sequence or 3D space. It has three discrete labels, representing helix, sheet, or turn.


\begin{table}%[!htbp]
\caption{Experimental results on synthetic graph datasets.}
\label{tab:graph_synthetic}
\begin{threeparttable}
\begin{tabular}{ccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{RG-Macro} & \multicolumn{2}{c}{RG-Sub} \\
 & Acc & F1\_ma & Acc & F1\_ma \\
 \midrule
Graphlet Kernel & 0.3800 & 0.3195 & 0.6020 & 0.5919 \\
WL Subtree & \textbf{0.6860} & \textbf{0.6702} & 0.9190 & 0.9218 \\
Hypergraph Rooted & 0.1790 & 0.0933 & 0.1850 & 0.0866 \\
Hypergraph Directed Line & 0.6040 & 0.5441 & 0.6820 & 0.6851 \\
\midrule
Hypergraph WL Subtree & \textbf{0.6860} & \textbf{0.6702} & 0.9190 & 0.9218 \\
Hypergraph WL Hyperedge & 0.6490 & 0.6347 & \textbf{0.9250} & \textbf{0.9268} \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item The best results are marked in bold type.
    \end{tablenotes}
\end{threeparttable}
\end{table}

\begin{table*}%[!htbp]
\centering
\caption{Experimental results on real-world graph datasets.}
\label{tab:graph_real}
\begin{threeparttable}
\begin{tabular}{ccccccccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{IMDB-BINARY} & \multicolumn{2}{c}{IMDB-MULTI} & \multicolumn{2}{c}{MUTAG} & \multicolumn{2}{c}{NCI1} & \multicolumn{2}{c}{PROTEINS} \\
 & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma \\
 \midrule
Graphlet & 0.6010 & 0.5959 & 0.3700 & 0.3402 & 0.8087 & 0.7599 & 0.6015 & 0.6003 & 0.6864 & 0.6236 \\
Graph WL Subtree & \textbf{0.7440} & \textbf{0.7245} & \textbf{0.5167} & \textbf{0.5053} & \textbf{0.8245} & \textbf{0.7822} & \textbf{0.7487} & \textbf{0.7486} & \textbf{0.7196} & \textbf{0.6950} \\
Hypergraph Rooted & 0.4950 & 0.4201 & 0.3447 & 0.3244 & 0.6649 & 0.3983 & 0.4947 & 0.4715 & 0.5957 & 0.4451 \\
Hypergraph Directed Line & 0.6370 & 0.6361 & 0.4367 & 0.4161 & 0.6650 & 0.3986 & 0.6591 & 0.6560 & 0.6738 & 0.6493 \\
\midrule
Hypergraph WL Subtree & \textbf{0.7440} & \textbf{0.7245} & \textbf{0.5167} & \textbf{0.5065} & \textbf{0.8245} & \textbf{0.7822} & \textbf{0.7487} & \textbf{0.7486} & \textbf{0.7196} & \textbf{0.6950} \\
Hypergraph WL Hyperedge & 0.7250 & 0.7232 & 0.5153 & 0.5049 & 0.8141 & 0.7738 & 0.7037 & 0.7037 & 0.7071 & 0.6760 \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item The best results are marked in bold type.
    \end{tablenotes}
\end{threeparttable}
\end{table*}


\subsubsection{Experimental Results and Discussions}
Experimental results on seven graph datasets are provided in Table \ref{tab:graph_synthetic} and \ref{tab:graph_real}. We have the following three observations. First, the results show that the hypergraph WL subtree performs similarly to the graph WL Subtree method with the same random seeds and fold splitting. It can be explained by the analysis in Section \ref{sec:relation_graph_wl_and_hypergraph_wl}, which indicates that the hypergraph WL kernel equals the graph WL kernel confronting the low-order graph structure. Second, the two compared hypergraph kernel methods perform worse than the graph kernel methods. This is because the two hypergraph kernel methods only focus on the high-order correlations and are ineffective for the common low-order correlations. For example, the hypergraph directed line kernel embeds the graph/hypergraph by two-step structure transformation, which will produce many redundancy low-order structures. Confronting low-order structures tasks like graph classification, those redundant structures include much noise, which may reduce the quality of the final graph/hypergraph features. Those. Third, the hypergraph WL hyperedge kernel performs passably in those graph datasets compared with the hypergraph/graph WL subtree kernel. The main reason is that the edge in graphs only connects two vertices, which restricts the modeling ability of the hyperedge kernel. In the following hypergraph experiments, we will show the significant performance improvement of the proposed two hypergraph kernel compared with existing graph and hypergraph kernel methods.

\subsection{Experiments on Hypergraph Datasets}
\label{sec:exp:hypergraph}
This subsection presents the experiments results on a collection of hypergraph datasets, including four synthetic and eight real-world hypergraph datasets. Table \ref{tab:stat_hypergraph} provides a summary of the dataset statistics..

\begin{table*}%[!htbp]
\centering
\caption{Statistic information of ten hypergraph datasets.}
\label{tab:stat_hypergraph}
\begin{threeparttable}
\begin{tabular}{ccccccc}
\toprule
Dataset & \# hypergraphs & \# classes & Multi-label & Avg. \# vertices & Avg. \# hyperedges & Avg. hyperedge degree \\
\midrule
RHG-10 & 2000 & 10 & \XSolidBrush & 31.3 & 29.8 & 5.2 \\
RHG-3 & 1500 & 3 & \XSolidBrush & 35.5 & 17.9 & 6.9 \\
RHG-Table & 1000 & 2 & \XSolidBrush & 36.3 & 63.3 & 3.1 \\
RHG-Pyramid & 1000 & 2 & \XSolidBrush & 28.8 & 30.6 & 3.0 \\
\midrule
IMDB-Dir-Form & 1869 & 3 & \XSolidBrush & 15.7 & 39.2 & 3.7 \\
IMDB-Dir-Genre & 3393 & 3 & \XSolidBrush & 17.3 & 36.4 & 3.8 \\
IMDB-Dir-Genre-M & 1554 & 6 & \Checkmark & 15.7 & 40.8 & 3.8 \\
IMDB-Wri-Form & 374 & 4 & \XSolidBrush & 10.1 & 3.7 & 5.0 \\
IMDB-Wri-Genre & 1172 & 6 & \XSolidBrush & 12.8 & 4.4 & 5.2 \\
IMDB-Wri-Genre-M & 344 & 7 & \Checkmark & 10.3 & 3.7 & 5.0 \\
Steam-Player & 2048 & 2 & \XSolidBrush & 13.8 & 46.4 & 4.5 \\
Twitter-Friend & 1310 & 2 & \XSolidBrush & 21.6 & 84.3 & 4.3 \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item ``\#'' denotes ``number of'', and ``Avg.'' is short for ``Average''.
    \end{tablenotes}
\end{threeparttable}
\end{table*}

\subsubsection{Datasets}

\textit{Synthetic Hypergraph Datasets:} To evaluate the performance of isomorphism algorithms in capturing structural information, we created four sets of randomly generated hypergraph datasets: RHG-10, RHG-3, RHG-Table, and RHG-pyramid. By randomly picking constructor parameters, we created a range of diverse hypergraphs for experimentation. The RHG-10 dataset encompasses all ten distinct hypergraph structure factors. Table \ref{tab:hypergraph_syn}
showcases examples of all ten typical hypergraph structures. Among the ten manually selected structures, there are three pairs (six types) of hypergraph structures that are prone to confusion, while the remaining four types are selected from common and classical hypergraph structures. To evaluate the algorithm's ability to recognize significant high-order structures, we randomly generated 1000 hypergraphs for three distinctively various hypergraph structures: hyper-pyramid, hyper-check-table, and hyper-wheel, thus constructing the RHG-3 dataset. Moreover, to validate the fine-grained classification capability of our hypergraph isomorphism algorithm, we created two separate datasets for each pair of easily confusable hypergraph structures: RHG-Pyramid comprises one thousand hypergraphs consisting of Hyper-pyramid and Hyper-firm-pyramid structures; RHG-Table consists of data from Hyper-check-table and Hyper-rot-check-table structures.% More details and analysis are provided in Appendix xxx.



\begin{table*}%[!htbp]
\centering
    \caption{Examples of synthetic factor hypergraphs in RHG datasets.}
    \label{tab:RHG_example}
    \scriptsize
    % \footnotesize
  \begin{tabular}{ |c|c|c|c|c| }
    \hline
        \centering
        Hyper Flower&Hyper Pyramid&Hyper Checked Table& Hyper Wheel&Hyper Lattice\\
        \hline
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    \cr
    \hline
        \centering
        Hyper Windmill&Hyper Firm Pyramid&Hyper Rot Checked Table&Hyper Cycle&Hyper Fern\\
        \hline
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    &
    \begin{minipage}[b]{0.3\columnwidth}
		\centering
		\raisebox{-.5\height}{% Figure removed}
	\end{minipage}
    \\ \hline
  \end{tabular}
\end{table*}



\textit{Real-world Hypergraph Datasets:} In Table \ref{tab:stat_hypergraph}, those datasets that starts with ``IMDB'' is constructed from the original IMDB dataset \footnote{\href{https://developer.imdb.com/non-commercial-datasets/}{https://www.imdb.com/}}. Here, two types of correlations: co-director and co-writer, are adopted for high-order dataset construction. In the dataset name, the ``Dir'' and ``Wri'' denotes that the hypergraph is constructed by the co-director and co-writer relationship, respectively. The staff (director/writer) of each movie is a hypergraph. ``Form'' is included in the dataset's name, indicating that the movie category is identified by its form, like animation, documentary, and drama. ``Genre'' denotes the movie is classified by its genres, like adventure, crime, and family. Suffix ``-M'' in the dataset name denotes that each movie is associated with multiple genre labels, which are collected for the more complex multi-label classification task. The Steam-Player dataset \footnote{\href{https://www.kaggle.com/datasets/antonkozyriev/game-recommendations-on-steam}{https://store.steampowered.com/}} is a player dataset where each player is a hypergraph. The vertex is the games played by the player, and the hyperedge is constructed by linking the games with shared tags. The target of the dataset is to identify each user's preference: single-player-game or multi-player-game. The Twitter-Friend \footnote{\href{https://www.kaggle.com/datasets/hwassner/TwitterFriends}{https://twitter.com/}} dataset is a social media dataset. Each hypergraph is the friends of a specified user. The hyperedge is constructed by linking the users who are friends. The label associated with the hypergraph is to identify whether the user posted the blog about ``national dog day'' or ``respect tyler joseph''.


\subsubsection{Results on Synthetic Hypergraphs and Discussions}
Experimental results on four synthetic hypergraph datasets are provided in Table \ref{tab:hypergraph_syn}. Based on the obtained results, distinct observations were made for each dataset. First, the results obtained from the RHG-10 datasets demonstrated significant improvements in our isomorphism algorithms compared to others. Specifically, there was a $ 25.1\% $ accuracy improvement on Hypergraph WL Subtree and a $ 20.3\% $ accuracy improvement on Hypergraph WL Subtree. Second, the isomorphism results provided by Graphlet exhibited considerably lower accuracy than other algorithms. This discrepancy can be attributed to the heavy reliance of Graphlet on random sampling, which compromises the stability of predictions.

% Figure environment removed

The performance disparity between hypergraph isomorphism and graph isomorphism was evident in the results obtained from the RHG-Table dataset. The need to transform the hypergraph structure into a graph structure through clique expansion when employing the graph isomorphism algorithm revealed that graph structures are less capable of representing complex structures than hypergraphs. Similarly, in the case of RHG-Table and RHG-Pyramid datasets, which consisted of two intricate hypergraph structures, the Hypergraph Rooted algorithm surprisingly exhibited significantly lower recognition ability than other hypergraph isomorphism algorithms. Resembling the performance of the two graph isomorphism algorithms, the Hypergraph Rooted algorithm's accuracy of approximately $ 50\% $ indicated its inability to resolve Hyper-firm-check-table and Hyper-check-table structures. This inferior performance of the Hypergraph Rooted algorithm can be attributed to its fundamental nature as a random walk sampling method, which leads to random traversal when encountering overlapping parts of hyperedges. Consequently, the algorithm lacks a robust judgment of overlapping node groups, common in Hyper Firm Pyramid structures as Figure \ref{fig:pyramid} shows. However, the HG Directed Line algorithm does not suffer from this limitation. The overlapping edges are appropriately reinforced by converting it into a bipartite graph, thereby overcoming the aforementioned issue. Our two algorithms go beyond conventional approaches. They represent a significant advancement by substantially reducing the time needed for extracting structural features from special structures by eliminating the hypergraph-to-graph conversion step and demonstrating a robust recognition ability, particularly in scenarios involving prevalent overlapping hyperedges.



\subsubsection{Results on Real-World Hypergraphs and Discussions}
% 1. 我们方法都高
% 2. 我们方法的两个在不同的数据集高的不一样
% 3. 对比方法分成两类，其中基于结构count的和采样。这两类分别比，超图均高于图。
The Experimental results for eight real-world hypergraph datasets are presented in Tables \ref{tab:hypergraph_real} and \ref{tab:hypergraph_real_m}. Based on those results, we have three observations. Firstly, our proposed two hypergraph WL kernels outperform other compared methods across all real-world hypergraph datasets. This superiority stems from our methods' ability to effectively capture and identify various high-order structures through the two-stage hypergraph WL algorithm. Secondly, we observed that the proposed "HG WL Hyperedge" method outperforms the proposed "HG WL Subtree" method in three IMDB-Wri dataset: IMDB-Wri-Form, IMDB-Wri-Genre, and IMDB-Wri-Genre-M. With further analysis, in Table \ref{tab:stat_hypergraph}, we find that these three hypergraph datasets have lower average numbers of hyperedge ($<= 5.0$) and higher average hyperedge degree ($>=5.0$) compared with other hypergraph datasets. The results indicate that, in comparison to the "HG WL Subtree" method, the "HG WL Hyperedge" method is capable of identifying more complex hyperedges characterized by connecting a greater number of vertices. Thus, the "HG WL Hyperedge" performs better in the three hypergraph datasets. Thirdly, in terms of the compared methods, in most datasets, we observed that, in most datasets, the HG Directed Line method outperforms the Graph WL Subtree method, while the HG Rooted method outperforms the Graphlet method. It is important to note that, to ensure a fair comparison, the Graphlet and HG Rooted methods are sampling-based methods, whereas the other two methods are not. It is evident that the sampling-based methods yield lower performance compared to the other two methods due to the inherent information loss associated with sampling. Within each type of method, the hypergraph kernel approach is able to directly capture and process high-order correlations present in hypergraphs, leading to superior performance. 


\begin{table*}%[!htbp]
\centering
\caption{Experimental results of synthetic hypergraph datasets.}
\label{tab:hypergraph_syn}
\begin{threeparttable}
\begin{tabular}{ccccccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{RHG-10} & \multicolumn{2}{c}{RHG-3} & \multicolumn{2}{c}{RHG-Table} & \multicolumn{2}{c}{RHG-Pyramid} \\
 & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma \\
 \midrule
Graphlet & 0.3925 & 0.3467 & 0.7033 & 0.7027 & 0.4920 & 0.4627 & 0.4700 & 0.4509 \\
Graph WL Subtree & 0.6720 & 0.6483 & 0.9986 & 0.9986 & 0.5070 & 0.4928 & 0.4940 & 0.4737 \\
Hypergraph Rooted & 0.5545 & 0.5118 & 0.9986 & 0.9986 & \textbf{1.0000} & \textbf{1.0000} & 0.5010 & 0.4686 \\
Hypergraph Directed Line & 0.7040 & 0.6743 & 0.9986 & 0.9986 & \textbf{1.0000} & \textbf{1.0000} & 0.9410 & 0.9406 \\
Hypergraph WL Subtree & \textbf{0.9610} & \textbf{0.9606} & \textbf{0.9993} & \textbf{0.9993} & \textbf{1.0000} & \textbf{1.0000} & \textbf{0.9540} & \textbf{0.9540} \\
Hypergraph WL Hyperedge & 0.9030 & 0.9024 & \textbf{0.9993} & \textbf{0.9993} & \textbf{1.0000} & \textbf{1.0000} & \textbf{0.9540} & \textbf{0.9540} \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item ``HG'' is short for ``Hypergraph''. The best results are marked in bold type.
    \end{tablenotes}
\end{threeparttable}
\end{table*}


\begin{table*}%[!htbp]
\centering
\caption{Experimental results of single-label classification on real-world hypergraph datasets.}
% \scriptsize
\footnotesize
\label{tab:hypergraph_real}
\begin{threeparttable}
\begin{tabular}{ccccccccccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{2}{c}{IMDB-Dir-Form} & \multicolumn{2}{c}{IMDB-Dir-Genre} & \multicolumn{2}{c}{IMDB-Wri-Form} & \multicolumn{2}{c}{IMDB-Wri-Genre} & \multicolumn{2}{c}{Steam-Player} & \multicolumn{2}{c}{Twitter-Friend} \\
 & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma & Acc & F1\_ma \\
 \midrule
Graphlet & 0.5265 & 0.3922 & 0.6086 & 0.5058 & 0.4654 & 0.2216 & 0.3353 & 0.2318 & 0.4805 & 0.3440 & 0.5878 & 0.3769 \\
Graph WL Subtree & {0.6453} & {0.5967} & 0.7424 & 0.6619 & 0.4681 & 0.2994 & 0.4846 & 0.4202 & 0.5293 & 0.5189 & 0.5931 & 0.4467 \\
HG Rooted & 0.5944 & 0.5247 & 0.6891 & 0.6095 & 0.4438 & 0.2471 & 0.4215 & 0.3755 & 0.4868 & 0.4855 & 0.5817 & 0.4635 \\
HG Directed Line & 0.6670 & 0.6199 & OOM & OOM & 0.4574 & 0.2604 & 0.4999 & 0.4269 & 0.5312 & 0.5206 & OOM & OOM \\
\midrule
HG WL Subtree & \textbf{0.6741} & \textbf{0.6227} & \textbf{0.7804} & \textbf{0.7379} & 0.4679 & \textbf{0.3335} & 0.5307 & 0.4738 & \textbf{0.5620} & \textbf{0.5474} & \textbf{0.5954} & \textbf{0.4800} \\
HG WL Hyperedge & 0.6688 & 0.6197 & {0.7698} & {0.7165} & \textbf{0.4732} & 0.2990 & \textbf{0.5614} & \textbf{0.4953} & 0.5493 & 0.5467 & 0.5809 & 0.3971 \\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item ``HG'' is short for ``Hypergraph'', and ``OOM'' denotes ``out of memory''. The best results are marked in bold type.
    \end{tablenotes}
\end{threeparttable}
\end{table*}


\begin{table}%[!htbp]
\caption{Experimental results of multi-label classification on real-world hypergraph datasets.}
\scriptsize
% \footnotesize
\centering
\label{tab:hypergraph_real_m}
\begin{threeparttable}
\begin{tabular}{cp{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}p{0.6cm}<{\centering}}
% \begin{tabular}{ccccccc}
\toprule
\multirow{2}{*}{} & \multicolumn{3}{c}{IMDB-Dir-Genre-M} & \multicolumn{3}{c}{IMDB-Wri-Genre-M} \\
 & EMR & EB\_acc & EB\_pre & EMR & EB\_acc & EB\_pre \\
 \midrule
Graphlet & 0.1795 & 0.4008 & 0.5663 & 0.0524 & 0.3306 & 0.5740 \\
Graph WL Subtree & 0.3533 & 0.4823 & 0.5954 & 0.2122 & 0.4327 & 0.6199 \\
HG Rooted & 0.2799 & 0.4159 & 0.5357 & 0.1626 & 0.3789 & 0.5762 \\
HG Directed Line & 0.3410 & 0.4779 & 0.6075 & 0.1684 & 0.4249 & 0.6131 \\
\midrule
HG WL Subtree & \textbf{0.3739} & \textbf{0.5199} & \textbf{0.6500} & 0.2238 & 0.4463 & \textbf{0.6620} \\
HG WL Hyperedge & 0.3539 & 0.5014 & 0.6210 & \textbf{0.2616} & \textbf{0.4668} & 0.6586\\
\bottomrule
\end{tabular}
    \begin{tablenotes}
        \item ``HG'' is short for ``Hypergraph''. The best results are marked in bold type.
    \end{tablenotes}
\end{threeparttable}
\end{table}



\subsection{Runtime Comparison}


% Figure environment removed

% 超图的个数

% 超图的节点数量

% 超边的个数

% 超图的节点度

In this subsection, we perform experiments to evaluate the runtime performance of different methods, as shown in Figure \ref{fig:rt_cmp}. For a fair comparison, we selected the Hypergraph Directed Line method as the benchmark since it is a hypergraph kernel-based method that does not employ sampling strategies. The DeepHyperGraph\footnote{\url{https://github.com/iMoonLab/DeepHypergraph}} library is utilized to generate the synthetic hypergraphs. Those methods are computed on the same machine with Intel i7-10700 @ 2.90GHz$\times$16 CPU and 16G Memory. By default, $500$ hypergraphs with the ``low-order first'' configuration \footnote{\href{https://deephypergraph.readthedocs.io/en/latest/api/random.html\#dhg.random.hypergraph\_Gnm}{hypergraph\_Gnm()}} are randomly generated, and each hypergraph contains $50$ vertices and $250$ hyperedges. For complete runtime comparison, we devise four types of settings of synthetic hypergraphs. The first synthetic hypergraph setting is developed to test the runtime of methods as the number of vertices per hypergraph grows. In this setting, the number of hypergraphs is fixed at $500$. In each hypergraph, the number of hyperedges is five times the number of vertices. The Experimental results for this setting are presented in Figure \ref{fig:rt_nv}. The second synthetic hypergraph setting is developed to test the runtime of methods as the number of hyperedges per hypergraph grows. In this setting, the number of vertices is fixed as $50$. The number of hyperedges varies in the range $[50, 100, 150, 200]$. Experimental results of this setting are shown in Figure \ref{fig:rt_ne}. The third synthetic hypergraph setting is designed to assess the runtime performance of methods as the number of hypergraphs rises. In this setting, the number of vertices and hyperedge is fixed as $50$ and $250$, respectively. The number of hypergraphs varies in the range $[50, 100, 200, 500, 1000, 2000]$. Experimental results of this setting are shown in Figure \ref{fig:rt_hg}. The last synthetic hypergraph setting is developed to test the runtime of methods as the complexity of the hypergraph rises. Compared with an edge in simple graphs, the hyperedge can connect more than two vertices. The complexity of hypergraphs will rise as the degree of hyperedge rises. In this setting, the number of vertices and hyperedges is also fixed as $50$ and $250$, respectively. However, the degree of hyperedges varies. In other words, we randomly generate $500$ $2$-uniform hypergraph, $3$-uniform hypergraphs, $4$-uniform hypergraphs, and $5$-uniform hypergraphs for runtime comparison. Experimental results of this setting are shown in Figure \ref{fig:rt_de}. From the four sets of experimental results, we observed that the proposed two methods run significantly faster ($86\times$) than the compared Hypergraph Directed Line methods. Especially confronting more complex hypergraph datasets (the degree of hyperedge rises), the proposed methods still show robust runtime as shown in Figure \ref{fig:rt_de}. This is because the Hypergraph Directed Line method can not directly process the hypergraph, which transforms hypergraphs into undirected graphs with clique expansion and further generate directed line graphs for kernel feature computation. As the degree of hyperedge rises, the scale of the clique-expanded graphs will sharply increase. In contrast, our proposed methods could be applied in hypergraphs without extra transformation, which is not sensitive to the variation of hyperedge's degree. Besides, we find that the runtime of the proposed Hypergraph WL Hyperedge method is slower than the proposed Hypergraph WL Subtree method. The main reason is that in each iteration, the Hypergraph WL Subtree method directly counts the vertex label for the final feature vector. However, the Hypergraph WL Hyperedge method needs to build hyperedge label from those vertex labels, which will consume extra time for computation. 

