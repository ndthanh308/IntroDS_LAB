{
  "title": "Combating the Curse of Multilinguality in Cross-Lingual WSD by Aligning Sparse Contextualized Word Representations",
  "authors": [
    "GÃ¡bor Berend"
  ],
  "submission_date": "2023-07-25T19:20:50+00:00",
  "revised_dates": [],
  "abstract": "In this paper, we advocate for using large pre-trained monolingual language models in cross lingual zero-shot word sense disambiguation (WSD) coupled with a contextualized mapping mechanism. We also report rigorous experiments that illustrate the effectiveness of employing sparse contextualized word representations obtained via a dictionary learning procedure. Our experimental results demonstrate that the above modifications yield a significant improvement of nearly 6.5 points of increase in the average F-score (from 62.0 to 68.5) over a collection of 17 typologically diverse set of target languages. We release our source code for replicating our experiments at https://github.com/begab/sparsity_makes_sense.",
  "categories": [
    "cs.CL",
    "cs.AI"
  ],
  "primary_category": "cs.CL",
  "doi": "10.18653/v1/2022.naacl-main.176",
  "journal_ref": null,
  "arxiv_id": "2307.13776",
  "pdf_url": null,
  "comment": "Presented at NAACL2022",
  "num_versions": null,
  "size_before_bytes": 322014,
  "size_after_bytes": 83747
}