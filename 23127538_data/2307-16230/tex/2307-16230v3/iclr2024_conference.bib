@article{christ2023undetectable,
  title={Undetectable Watermarks for Language Models},
  author={Christ, Miranda and Gunn, Sam and Zamir, Or},
  journal={arXiv preprint arXiv:2306.09194},
  year={2023}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}


@inproceedings{gangemi2012automatic,
  title={Automatic typing of DBpedia entities},
  author={Gangemi, Aldo and Nuzzolese, Andrea Giovanni and Presutti, Valentina and Draicchio, Francesco and Musetti, Alberto and Ciancarini, Paolo},
  booktitle={The Semantic Web--ISWC 2012: 11th International Semantic Web Conference, Boston, MA, USA, November 11-15, 2012, Proceedings, Part I 11},
  pages={65--81},
  year={2012},
  organization={Springer}
}

@article{kuditipudi2023robust,
  title={Robust Distortion-free Watermarks for Language Models},
  author={Kuditipudi, Rohith and Thickstun, John and Hashimoto, Tatsunori and Liang, Percy},
  journal={arXiv preprint arXiv:2307.15593},
  year={2023}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{lee2023wrote,
  title={Who Wrote this Code? Watermarking for Code Generation},
  author={Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee},
  journal={arXiv preprint arXiv:2305.15060},
  year={2023}
}

@article{zhao2023provable,
  title={Provable Robust Watermarking for AI-Generated Text},
  author={Zhao, Xuandong and Ananth, Prabhanjan and Li, Lei and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2306.17439},
  year={2023}
}

@inproceedings{yoo2023robust,
  title={Robust Multi-bit Natural Language Watermarking through Invariant Features},
  author={Yoo, KiYoon and Ahn, Wonhyuk and Jang, Jiho and Kwak, Nojun},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2092--2115},
  year={2023}
}

@inproceedings{abdelnabi2021adversarial,
  title={Adversarial watermarking transformer: Towards tracing text provenance with data hiding},
  author={Abdelnabi, Sahar and Fritz, Mario},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={121--140},
  year={2021},
  organization={IEEE}
}

@article{chakraborty2023possibilities,
  title={On the possibilities of ai-generated text detection},
  author={Chakraborty, Souradip and Bedi, Amrit Singh and Zhu, Sicheng and An, Bang and Manocha, Dinesh and Huang, Furong},
  journal={arXiv preprint arXiv:2304.04736},
  year={2023}
}

@article{sadasivan2023can,
  title={Can ai-generated text be reliably detected?},
  author={Sadasivan, Vinu Sankar and Kumar, Aounon and Balasubramanian, Sriram and Wang, Wenxiao and Feizi, Soheil},
  journal={arXiv preprint arXiv:2303.11156},
  year={2023}
}
@article{wu2023llmdet,
  title={LLMDet: A Large Language Models Detection Tool},
  author={Wu, Kangxi and Pang, Liang and Shen, Huawei and Cheng, Xueqi and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2305.15004},
  year={2023}
}

@article{hu2023radar,
  title={RADAR: Robust AI-Text Detection via Adversarial Learning},
  author={Hu, Xiaomeng and Chen, Pin-Yu and Ho, Tsung-Yi},
  journal={arXiv preprint arXiv:2307.03838},
  year={2023}
}
@article{su2023detectllm,
  title={DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text},
  author={Su, Jinyan and Zhuo, Terry Yue and Wang, Di and Nakov, Preslav},
  journal={arXiv preprint arXiv:2306.05540},
  year={2023}
}

@article{mireshghallah2023smaller,
  title={Smaller Language Models are Better Black-box Machine-Generated Text Detectors},
  author={Mireshghallah, Fatemehsadat and Mattern, Justus and Gao, Sicun and Shokri, Reza and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2305.09859},
  year={2023}
}

@article{zhan2023g3detector,
  title={G3Detector: General GPT-Generated Text Detector},
  author={Zhan, Haolan and He, Xuanli and Xu, Qiongkai and Wu, Yuxiang and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2305.12680},
  year={2023}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{liu1907roberta,
  title={RoBERTa: a robustly optimized BERT pretraining approach (2019)},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  volume={364},
  year={1907},
  publisher={CoRR}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{pan2023risk,
  title={On the Risk of Misinformation Pollution with Large Language Models},
  author={Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.13661},
  year={2023}
}
@article{chen2023pathway,
  title={A pathway towards responsible ai generated content},
  author={Chen, Chen and Fu, Jie and Lyu, Lingjuan},
  journal={arXiv preprint arXiv:2303.01325},
  year={2023}
}