\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdelnabi \& Fritz(2021)Abdelnabi and Fritz]{abdelnabi2021adversarial}
Sahar Abdelnabi and Mario Fritz.
\newblock Adversarial watermarking transformer: Towards tracing text provenance with data hiding.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pp.\  121--140. IEEE, 2021.

\bibitem[Chakraborty et~al.(2023)Chakraborty, Bedi, Zhu, An, Manocha, and Huang]{chakraborty2023possibilities}
Souradip Chakraborty, Amrit~Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong Huang.
\newblock On the possibilities of ai-generated text detection.
\newblock \emph{arXiv preprint arXiv:2304.04736}, 2023.

\bibitem[Chen et~al.(2023)Chen, Fu, and Lyu]{chen2023pathway}
Chen Chen, Jie Fu, and Lingjuan Lyu.
\newblock A pathway towards responsible ai generated content.
\newblock \emph{arXiv preprint arXiv:2303.01325}, 2023.

\bibitem[Christ et~al.(2023)Christ, Gunn, and Zamir]{christ2023undetectable}
Miranda Christ, Sam Gunn, and Or~Zamir.
\newblock Undetectable watermarks for language models.
\newblock \emph{arXiv preprint arXiv:2306.09194}, 2023.

\bibitem[Costa-juss{\`a} et~al.(2022)Costa-juss{\`a}, Cross, {\c{C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, et~al.]{costa2022no}
Marta~R Costa-juss{\`a}, James Cross, Onur {\c{C}}elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et~al.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock \emph{arXiv preprint arXiv:2207.04672}, 2022.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hu et~al.(2023{\natexlab{a}})Hu, Chen, and Ho]{hu2023radar}
Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho.
\newblock Radar: Robust ai-text detection via adversarial learning.
\newblock \emph{arXiv preprint arXiv:2307.03838}, 2023{\natexlab{a}}.

\bibitem[Hu et~al.(2023{\natexlab{b}})Hu, Chen, Li, Guo, Wen, Yu, and Guo]{hu2023large}
Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip~S Yu, and Zhijiang Guo.
\newblock Do large language models know about facts?
\newblock \emph{arXiv preprint arXiv:2310.05177}, 2023{\natexlab{b}}.

\bibitem[Ji et~al.(2023)Ji, Qiu, Chen, Zhang, Lou, Wang, Duan, He, Zhou, Zhang, et~al.]{ji2023ai}
Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et~al.
\newblock Ai alignment: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2310.19852}, 2023.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirchenbauer et~al.(2023)Kirchenbauer, Geiping, Wen, Katz, Miers, and Goldstein]{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.
\newblock A watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2301.10226}, 2023.

\bibitem[Kuditipudi et~al.(2023)Kuditipudi, Thickstun, Hashimoto, and Liang]{kuditipudi2023robust}
Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang.
\newblock Robust distortion-free watermarks for language models.
\newblock \emph{arXiv preprint arXiv:2307.15593}, 2023.

\bibitem[Lee et~al.(2023)Lee, Hong, Ahn, Hong, Lee, Yun, Shin, and Kim]{lee2023wrote}
Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, and Gunhee Kim.
\newblock Who wrote this code? watermarking for code generation.
\newblock \emph{arXiv preprint arXiv:2305.15060}, 2023.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Liu et~al.(2022)Liu, Yu, Hu, Li, Lin, Ma, Yang, and Wen]{liu2022character}
Aiwei Liu, Honghai Yu, Xuming Hu, Shu'ang Li, Li~Lin, Fukun Ma, Yawen Yang, and Lijie Wen.
\newblock Character-level white-box adversarial attacks against transformers via attachable subwords substitution.
\newblock \emph{arXiv preprint arXiv:2210.17004}, 2022.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Hu, Wen, and Yu]{liu2023comprehensive}
Aiwei Liu, Xuming Hu, Lijie Wen, and Philip~S Yu.
\newblock A comprehensive evaluation of chatgpt's zero-shot text-to-sql capability.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2303, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Pan, Hu, Meng, and Wen]{liu2023semantic}
Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, and Lijie Wen.
\newblock A semantic invariant robust watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2310.06356}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Pan, Lu, Li, Hu, Wen, King, and Yu]{liu2023survey}
Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Lijie Wen, Irwin King, and Philip~S Yu.
\newblock A survey of text watermarking in the era of large language models.
\newblock \emph{arXiv preprint arXiv:2312.07913}, 2023{\natexlab{c}}.

\bibitem[Liu et~al.(1907)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu1907roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: a robustly optimized bert pretraining approach (2019).
\newblock \emph{arXiv preprint arXiv:1907.11692}, 364, 1907.

\bibitem[Mach{\'a}{\v{c}}ek \& Bojar(2014)Mach{\'a}{\v{c}}ek and Bojar]{machavcek2014results}
Matou{\v{s}} Mach{\'a}{\v{c}}ek and Ond{\v{r}}ej Bojar.
\newblock Results of the wmt14 metrics shared task.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine Translation}, pp.\  293--301, 2014.

\bibitem[Mireshghallah et~al.(2023)Mireshghallah, Mattern, Gao, Shokri, and Berg-Kirkpatrick]{mireshghallah2023smaller}
Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor Berg-Kirkpatrick.
\newblock Smaller language models are better black-box machine-generated text detectors.
\newblock \emph{arXiv preprint arXiv:2305.09859}, 2023.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pan et~al.(2023)Pan, Pan, Chen, Nakov, Kan, and Wang]{pan2023risk}
Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William~Yang Wang.
\newblock On the risk of misinformation pollution with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13661}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Sadasivan et~al.(2023)Sadasivan, Kumar, Balasubramanian, Wang, and Feizi]{sadasivan2023can}
Vinu~Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi.
\newblock Can ai-generated text be reliably detected?
\newblock \emph{arXiv preprint arXiv:2303.11156}, 2023.

\bibitem[Su et~al.(2023)Su, Zhuo, Wang, and Nakov]{su2023detectllm}
Jinyan Su, Terry~Yue Zhuo, Di~Wang, and Preslav Nakov.
\newblock Detectllm: Leveraging log rank information for zero-shot detection of machine-generated text.
\newblock \emph{arXiv preprint arXiv:2306.05540}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Yoo et~al.(2023)Yoo, Ahn, Jang, and Kwak]{yoo2023robust}
KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak.
\newblock Robust multi-bit natural language watermarking through invariant features.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  2092--2115, 2023.

\bibitem[Zhan et~al.(2023)Zhan, He, Xu, Wu, and Stenetorp]{zhan2023g3detector}
Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp.
\newblock G3detector: General gpt-generated text detector.
\newblock \emph{arXiv preprint arXiv:2305.12680}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Ananth, Li, and Wang]{zhao2023provable}
Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang.
\newblock Provable robust watermarking for ai-generated text.
\newblock \emph{arXiv preprint arXiv:2306.17439}, 2023.

\end{thebibliography}
