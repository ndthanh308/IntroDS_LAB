\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pan et~al.(2023)Pan, Pan, Chen, Nakov, Kan, and Wang]{pan2023risk}
Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and
  William~Yang Wang.
\newblock On the risk of misinformation pollution with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13661}, 2023.

\bibitem[Chen et~al.(2023)Chen, Fu, and Lyu]{chen2023pathway}
Chen Chen, Jie Fu, and Lingjuan Lyu.
\newblock A pathway towards responsible ai generated content.
\newblock \emph{arXiv preprint arXiv:2303.01325}, 2023.

\bibitem[Christ et~al.(2023)Christ, Gunn, and Zamir]{christ2023undetectable}
Miranda Christ, Sam Gunn, and Or~Zamir.
\newblock Undetectable watermarks for language models.
\newblock \emph{arXiv preprint arXiv:2306.09194}, 2023.

\bibitem[Kirchenbauer et~al.(2023)Kirchenbauer, Geiping, Wen, Katz, Miers, and
  Goldstein]{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom
  Goldstein.
\newblock A watermark for large language models.
\newblock \emph{arXiv preprint arXiv:2301.10226}, 2023.

\bibitem[Zhan et~al.(2023)Zhan, He, Xu, Wu, and Stenetorp]{zhan2023g3detector}
Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp.
\newblock G3detector: General gpt-generated text detector.
\newblock \emph{arXiv preprint arXiv:2305.12680}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Liu et~al.(1907)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu1907roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: a robustly optimized bert pretraining approach (2019).
\newblock \emph{arXiv preprint arXiv:1907.11692}, 364, 1907.

\bibitem[Mireshghallah et~al.(2023)Mireshghallah, Mattern, Gao, Shokri, and
  Berg-Kirkpatrick]{mireshghallah2023smaller}
Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, Reza Shokri, and Taylor
  Berg-Kirkpatrick.
\newblock Smaller language models are better black-box machine-generated text
  detectors.
\newblock \emph{arXiv preprint arXiv:2305.09859}, 2023.

\bibitem[Su et~al.(2023)Su, Zhuo, Wang, and Nakov]{su2023detectllm}
Jinyan Su, Terry~Yue Zhuo, Di~Wang, and Preslav Nakov.
\newblock Detectllm: Leveraging log rank information for zero-shot detection of
  machine-generated text.
\newblock \emph{arXiv preprint arXiv:2306.05540}, 2023.

\bibitem[Hu et~al.(2023)Hu, Chen, and Ho]{hu2023radar}
Xiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho.
\newblock Radar: Robust ai-text detection via adversarial learning.
\newblock \emph{arXiv preprint arXiv:2307.03838}, 2023.

\bibitem[Wu et~al.(2023)Wu, Pang, Shen, Cheng, and Chua]{wu2023llmdet}
Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua.
\newblock Llmdet: A large language models detection tool.
\newblock \emph{arXiv preprint arXiv:2305.15004}, 2023.

\bibitem[Chakraborty et~al.(2023)Chakraborty, Bedi, Zhu, An, Manocha, and
  Huang]{chakraborty2023possibilities}
Souradip Chakraborty, Amrit~Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha,
  and Furong Huang.
\newblock On the possibilities of ai-generated text detection.
\newblock \emph{arXiv preprint arXiv:2304.04736}, 2023.

\bibitem[Sadasivan et~al.(2023)Sadasivan, Kumar, Balasubramanian, Wang, and
  Feizi]{sadasivan2023can}
Vinu~Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and
  Soheil Feizi.
\newblock Can ai-generated text be reliably detected?
\newblock \emph{arXiv preprint arXiv:2303.11156}, 2023.

\bibitem[Abdelnabi and Fritz(2021)]{abdelnabi2021adversarial}
Sahar Abdelnabi and Mario Fritz.
\newblock Adversarial watermarking transformer: Towards tracing text provenance
  with data hiding.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages
  121--140. IEEE, 2021.

\bibitem[Yoo et~al.(2023)Yoo, Ahn, Jang, and Kwak]{yoo2023robust}
KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak.
\newblock Robust multi-bit natural language watermarking through invariant
  features.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2092--2115,
  2023.

\bibitem[Zhao et~al.(2023)Zhao, Ananth, Li, and Wang]{zhao2023provable}
Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang.
\newblock Provable robust watermarking for ai-generated text.
\newblock \emph{arXiv preprint arXiv:2306.17439}, 2023.

\bibitem[Lee et~al.(2023)Lee, Hong, Ahn, Hong, Lee, Yun, Shin, and
  Kim]{lee2023wrote}
Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun,
  Jamin Shin, and Gunhee Kim.
\newblock Who wrote this code? watermarking for code generation.
\newblock \emph{arXiv preprint arXiv:2305.15060}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Gangemi et~al.(2012)Gangemi, Nuzzolese, Presutti, Draicchio, Musetti,
  and Ciancarini]{gangemi2012automatic}
Aldo Gangemi, Andrea~Giovanni Nuzzolese, Valentina Presutti, Francesco
  Draicchio, Alberto Musetti, and Paolo Ciancarini.
\newblock Automatic typing of dbpedia entities.
\newblock In \emph{The Semantic Web--ISWC 2012: 11th International Semantic Web
  Conference, Boston, MA, USA, November 11-15, 2012, Proceedings, Part I 11},
  pages 65--81. Springer, 2012.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\end{thebibliography}
