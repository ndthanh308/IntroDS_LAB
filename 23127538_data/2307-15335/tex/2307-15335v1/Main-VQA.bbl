% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Antol_2015_ICCV}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and
  D.~Parikh, ``Vqa: Visual question answering,'' in \emph{Proceedings of the
  IEEE International Conference on Computer Vision (ICCV)}, December 2015.

\bibitem{qi-etal-2022-dureadervis}
L.~Qi, S.~Lv, H.~Li, J.~Liu, Y.~Zhang, Q.~She, H.~Wu, H.~Wang, and T.~Liu,
  ``$\textrm{DuReader}_{\textrm{vis}}$: A {C}hinese dataset for open-domain
  document visual question answering,'' in \emph{Findings of the Association
  for Computational Linguistics: ACL 2022}.\hskip 1em plus 0.5em minus
  0.4em\relax Dublin, Ireland: Association for Computational Linguistics, May
  2022, pp. 1338--1351.

\bibitem{shimizu-etal-2018-visual}
\BIBentryALTinterwordspacing
N.~Shimizu, N.~Rong, and T.~Miyazaki, ``Visual question answering dataset for
  bilingual image understanding: A study of cross-lingual transfer using
  attention maps,'' in \emph{Proceedings of the 27th International Conference
  on Computational Linguistics (COLING)}.\hskip 1em plus 0.5em minus
  0.4em\relax Santa Fe, New Mexico, USA: Association for Computational
  Linguistics, Aug. 2018, pp. 1918--1928. [Online]. Available:
  \url{https://aclanthology.org/C18-1163}
\BIBentrySTDinterwordspacing

\bibitem{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' in \emph{International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem{pmlr-v139-kim21k}
W.~Kim, B.~Son, and I.~Kim, ``Vilt: Vision-and-language transformer without
  convolution or region supervision,'' in \emph{Proceedings of the 38th
  International Conference on Machine Learning (ICLR)}, ser. Proceedings of
  Machine Learning Research, M.~Meila and T.~Zhang, Eds., vol. 139.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 18--24 Jul 2021, pp. 5583--5594.

\bibitem{pmlr-v139-radford21a}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever, ``Learning
  transferable visual models from natural language supervision,'' in
  \emph{Proceedings of the 38th International Conference on Machine Learning
  (ICLR)}, ser. Proceedings of Machine Learning Research, M.~Meila and
  T.~Zhang, Eds., vol. 139.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 18--24
  Jul 2021, pp. 8748--8763.

\bibitem{li2021align}
J.~Li, R.~R. Selvaraju, A.~D. Gotmare, S.~Joty, C.~Xiong, and S.~Hoi, ``Align
  before fuse: Vision and language representation learning with momentum
  distillation,'' in \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, Eds.,
  2021.

\bibitem{pmlr-v139-touvron21a}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~Jegou,
  ``Training data-efficient image transformers \&amp; distillation through
  attention,'' in \emph{Proceedings of the 38th International Conference on
  Machine Learning}, ser. Proceedings of Machine Learning Research, M.~Meila
  and T.~Zhang, Eds., vol. 139.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  18--24 Jul 2021, pp. 10\,347--10\,357.

\bibitem{devlin-etal-2019-bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``{BERT}: Pre-training of
  deep bidirectional transformers for language understanding,'' in
  \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of
  the Association for Computational Linguistics (NAACL): Human Language
  Technologies, Volume 1}.\hskip 1em plus 0.5em minus 0.4em\relax Minneapolis,
  Minnesota: Association for Computational Linguistics, Jun. 2019, pp.
  4171--4186.

\bibitem{tran-etal-2021-vivqa}
K.~Q. Tran, A.~T. Nguyen, A.~T.-H. Le, and K.~V. Nguyen, ``{V}i{VQA}:
  {V}ietnamese visual question answering,'' in \emph{Proceedings of the 35th
  Pacific Asia Conference on Language, Information and Computation}.\hskip 1em
  plus 0.5em minus 0.4em\relax Shanghai, China: Association for Computational
  Lingustics, 11 2021, pp. 683--691.

\bibitem{nguyen2023vlsp}
N.~L.-T. Nguyen, N.~H. Nguyen, D.~T. Vo, K.~Q. Tran, and K.~Van~Nguyen, ``Vlsp
  2022--evjvqa challenge: Multilingual visual question answering,'' \emph{arXiv
  preprint arXiv:2302.11752}, 2023.

\bibitem{10.5555/3157096.3157129}
J.~Lu, J.~Yang, D.~Batra, and D.~Parikh, ``Hierarchical question-image
  co-attention for visual question answering,'' in \emph{Proceedings of the
  30th International Conference on Neural Information Processing Systems
  (NeurIPS)}, ser. NIPS'16.\hskip 1em plus 0.5em minus 0.4em\relax Red Hook,
  NY, USA: Curran Associates Inc., 2016, p. 289â€“297.

\bibitem{phucvican}
N.~B. Phuc, T.~H. Nguyen, and Q.~T. Tho, ``Vican: Co-attention network for
  vietnamese visual question answering,'' in \emph{Fundamental and Applied
  Information Technology (FAIR)}.\hskip 1em plus 0.5em minus 0.4em\relax
  Publishing House for Science and Technology, 2020.

\bibitem{nam2017dual}
H.~Nam, J.-W. Ha, and J.~Kim, ``Dual attention networks for multimodal
  reasoning and matching,'' in \emph{Proceedings of the IEEE conference on
  computer vision and pattern recognition (CVPR)}, 2017, pp. 299--307.

\bibitem{bao2022beit}
H.~Bao, L.~Dong, S.~Piao, and F.~Wei, ``{BE}it: {BERT} pre-training of image
  transformers,'' in \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem{wang2022image}
W.~Wang, H.~Bao, L.~Dong, J.~Bjorck, Z.~Peng, Q.~Liu, K.~Aggarwal, O.~K.
  Mohammed, S.~Singhal, S.~Som \emph{et~al.}, ``Image as a foreign language:
  Beit pretraining for all vision and vision-language tasks,'' \emph{Accepted
  at the IEEE conference on computer vision and pattern recognition (CVPR)},
  2023.

\bibitem{bartpho}
N.~L. Tran, D.~M. Le, and D.~Q. Nguyen, ``{BARTpho: Pre-trained
  Sequence-to-Sequence Models for Vietnamese},'' in \emph{Proceedings of the
  23rd Annual Conference of the International Speech Communication
  Association}, 2022.

\bibitem{NEURIPS2022_d46662aa}
H.~Bao, W.~Wang, L.~Dong, Q.~Liu, O.~K. Mohammed, K.~Aggarwal, S.~Som, S.~Piao,
  and F.~Wei, ``Vlmo: Unified vision-language pre-training with
  mixture-of-modality-experts,'' in \emph{Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~35.\hskip 1em plus 0.5em minus
  0.4em\relax Curran Associates, Inc., 2022, pp. 32\,897--32\,912.

\bibitem{bao2022vl}
H.~Bao, W.~Wang, L.~Dong, and F.~Wei, ``Vl-beit: Generative vision-language
  pretraining,'' \emph{arXiv preprint arXiv:2206.01127}, 2022.

\bibitem{peng2022beit}
Z.~Peng, L.~Dong, H.~Bao, Q.~Ye, and F.~Wei, ``Beit v2: Masked image modeling
  with vector-quantized visual tokenizers,'' \emph{arXiv preprint
  arXiv:2208.06366}, 2022.

\bibitem{yu2022vectorquantized}
J.~Yu, X.~Li, J.~Y. Koh, H.~Zhang, R.~Pang, J.~Qin, A.~Ku, Y.~Xu, J.~Baldridge,
  and Y.~Wu, ``Vector-quantized image modeling with improved {VQGAN},'' in
  \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{lewis-etal-2020-bart}
M.~Lewis, Y.~Liu, N.~Goyal, M.~Ghazvininejad, A.~Mohamed, O.~Levy, V.~Stoyanov,
  and L.~Zettlemoyer, ``{BART}: Denoising sequence-to-sequence pre-training for
  natural language generation, translation, and comprehension,'' in
  \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics (ACL)}.\hskip 1em plus 0.5em minus 0.4em\relax
  Online: Association for Computational Linguistics, Jul. 2020, pp. 7871--7880.

\bibitem{hendrycks2016gelu}
D.~Hendrycks and K.~Gimpel, ``Gaussian error linear units (gelus),''
  \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{liu-etal-2020-multilingual-denoising}
Y.~Liu, J.~Gu, N.~Goyal, X.~Li, S.~Edunov, M.~Ghazvininejad, M.~Lewis, and
  L.~Zettlemoyer, ``Multilingual denoising pre-training for neural machine
  translation,'' \emph{Transactions of the Association for Computational
  Linguistics}, vol.~8, pp. 726--742, 2020.

\bibitem{nguyen-tuan-nguyen-2020-phobert}
D.~Q. Nguyen and A.~Tuan~Nguyen, ``{P}ho{BERT}: Pre-trained language models for
  {V}ietnamese,'' in \emph{Findings of the Association for Computational
  Linguistics: EMNLP 2020}.\hskip 1em plus 0.5em minus 0.4em\relax Online:
  Association for Computational Linguistics, Nov. 2020, pp. 1037--1042.

\bibitem{sennrich-etal-2016-neural}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' in \emph{Proceedings of the 54th Annual Meeting
  of the Association for Computational Linguistics (ACL)}.\hskip 1em plus 0.5em
  minus 0.4em\relax Berlin, Germany: Association for Computational Linguistics,
  Aug. 2016, pp. 1715--1725.

\bibitem{kudo-richardson-2018-sentencepiece}
T.~Kudo and J.~Richardson, ``{S}entence{P}iece: A simple and language
  independent subword tokenizer and detokenizer for neural text processing,''
  in \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural
  Language Processing (EMNLP): System Demonstrations}.\hskip 1em plus 0.5em
  minus 0.4em\relax Brussels, Belgium: Association for Computational
  Linguistics, Nov. 2018, pp. 66--71.

\bibitem{conneau-etal-2020-unsupervised}
A.~Conneau, K.~Khandelwal, N.~Goyal, V.~Chaudhary, G.~Wenzek, F.~Guzm{\'a}n,
  E.~Grave, M.~Ott, L.~Zettlemoyer, and V.~Stoyanov, ``Unsupervised
  cross-lingual representation learning at scale,'' in \emph{Proceedings of the
  58th Annual Meeting of the Association for Computational Linguistics
  (ACL)}.\hskip 1em plus 0.5em minus 0.4em\relax Online: Association for
  Computational Linguistics, Jul. 2020, pp. 8440--8451.

\bibitem{tuan-nguyen-etal-2020-pilot}
A.~Tuan~Nguyen, M.~H. Dao, and D.~Q. Nguyen, ``A pilot study of text-to-{SQL}
  semantic parsing for {V}ietnamese,'' in \emph{Findings of the Association for
  Computational Linguistics: EMNLP 2020}.\hskip 1em plus 0.5em minus
  0.4em\relax Online: Association for Computational Linguistics, Nov. 2020, pp.
  4079--4085.

\bibitem{DBLP:journals/corr/abs-1301-3781}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean, ``Efficient estimation of word
  representations in vector space,'' in \emph{1st International Conference on
  Learning Representations, {ICLR} 2013, Scottsdale, Arizona, USA, May 2-4,
  2013, Workshop Track Proceedings}, Y.~Bengio and Y.~LeCun, Eds., 2013.

\end{thebibliography}
