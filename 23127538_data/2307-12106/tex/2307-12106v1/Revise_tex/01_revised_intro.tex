    \vspace{-0.5cm}
\section{Introduction}
\label{sec:intro}

% Figure environment removed
Camera-to-robot pose estimation is a crucial task in determining the rigid transformation between the camera space and robot base space in terms of rotation and translation. Accurate estimation of this transformation enables robots to perform downstream tasks autonomously, such as grasping, manipulation, and interaction. Classic camera-to-robot estimation approaches, \textit{e.g.}\cite{fassi2005hand, horaud1995hand, strobl2006optimal}, typically involve attaching augmented reality (AR) tags as markers to the end-effector and directly solving a homogeneous matrix equation to calculate the transformation. However, these approaches have critical drawbacks. Capturing multiple joint configurations and corresponding images is always troublesome, and these methods cannot be used online. These flaws become greatly amplified when downstream tasks require frequent camera position adjustment.

To mitigate this limitation of classic offline hand-eye calibration, some recent works \cite{labbe2021single, lambrecht2021optimizing} introduce vision-based methods to estimate the camera-to-robot pose from a single image, opening the possibility of online hand-eye calibration. Such approaches significantly grant mobile and itinerant autonomous systems the ability to interact with other robots using only visual information in unstructured environments, especially in collaborative robotics \cite{Lee2020CameratoRobotPE}.

Most existing learning-based camera-to-robot pose estimation works\cite{Lee2020CameratoRobotPE, lu2022pose, labbe2021single, Simoni2022SemiPerspectiveDH} focus on single-frame estimation. However, due to the ambiguity of the single-view image, these methods do not perform well when the robotic arm is self-occluded.
Since the camera-to-robot pose is likely invariant during a video sequence and the keypoints are moving continually, one way to tackle this problem is to introduce temporal information. 
However, a crucial technical challenge of estimating camera-to-robot pose temporally is how to fuse temporal information efficiently.
To this end, as shown in Fig.~\ref{teaser}, we propose Structure Prior Guided Temporal Attention for Camera-to-Robot Pose estimation (SGTAPose) from successive frames of an image sequence.
First, we proposed robot structure priors guided feature alignment approach to align the temporal features in two successive frames.
Moreover, we apply a multi-head-cross-attention module to enhance the fusion of features in sequential images.
Then, after a decoder layer, we solve an initial camera-to-robot pose from the 2D projections of detected keypoints and their 3D positions via a PnP solver. 
We lastly reuse the structure priors as an explicit constraint to acquire a refined camera-to-robot pose.

By harnessing the temporal information and the robot structure priors, our proposed method gains significant performance improvement in the accuracy of camera-to-robot pose estimation and is more robust to robot self-occlusion.
We have surpassed previous online camera calibration approaches in synthetic and real-world datasets and show a strong dominance in minimising calibration error compared with traditional hand-eye calibration, where our method could reach the level of 5mm calibration errors via multi-frame PnP solving.
Finally, to test our method's capability in real-world experiments, we directly apply our predicted pose to help implement grasping tasks. We have achieved a fast prediction speed (36FPS) and a high grasping success rate.
Our contributions are summarised as follows:
\begin{itemize}
\vspace{-0.2cm}
\item For the first time, we demonstrate the remarkable performance of camera-to-robot pose estimation from successive frames of a single-view image sequence.
\vspace{-0.2cm}
\item We propose a temporal cross-attention strategy absorbing robot structure priors to efficiently fuse successive frames' features to estimate camera-to-robot pose.
\vspace{-0.2cm}
\item We demonstrate our method's capability of implementing downstream online grasping tasks in the real world with high accuracy and stability, even beyond the performance of classical hand-eye calibration.
\end{itemize}