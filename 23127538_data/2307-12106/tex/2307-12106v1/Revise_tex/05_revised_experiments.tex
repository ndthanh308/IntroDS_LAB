\section{Experiments}
\label{sec:experiments}

\subsection{Datasets}

% Figure environment removed

Our datasets involve one self-generated synthetic training set (Panda Syn Training), one self-generated synthetic testing set (Panda Syn Testing), and three real-world testing sets (Panda 3CAM-RS, Panda 3CAM-AK, and Panda Orb) provided by \cite{Lee2020CameratoRobotPE}.  
Since the training set proposed in \cite{Lee2020CameratoRobotPE} doesn't support temporal images, we thus generate Panda Syn Training/Testing.
The three public real-world sets are generated by an externally mounted camera filming a Franka Emika Panda manipulator according to \cite{Lee2020CameratoRobotPE}.
Panda 3CAM-AK is collected by a Microsoft Azure Kinect camera, and Panda 3CAM-RS/Orb are collected by Intel RealSense D415.
Panda 3CAM-AK and Panda 3CAM-RS are captured from a single fixed view and involve about 6k images respectively, while Panda Orb is captured from 27 different views and involves approximately 32k images.

%Utilising the open source tool Blender 
Following~\cite{dai2022domain}, we construct large-scale synthetic sets Panda Syn Training/Testing (Figure \ref{Dataset}) utilising Blender~\cite{Blender}. 
We generate frames in a video at FPS 30 with a fixed scene and a moving manipulator. 
Several domain randomisations have been used to shorten the sim-to-real gap and will be detailedly explained in supplementary materials.
The synthetic dataset consists of temporal RGB images, 2D/3D predefined keypoints' locations, and part poses.
Overall, Panda Syn Training involves approximately 60k videos which contain 3 successive frames per video (180k images), and Panda Syn Testing involves 347 videos which contain 30 successive frames per video (10k images).


\subsection{Baselines and Evaluations}
\noindent\textbf{Baselines.} 
We compare our framework with previous online keypoint-based camera-to-robot pose estimation approaches and center-based object detection methods via single or multi frames. Besides, we also compare our framework with traditional offline hand-eye calibration.
\textbf{Dream}\cite{Lee2020CameratoRobotPE}: A pioneering approach for estimating the camera-to-robot pose from a single frame by direct 2D keypoints regression and PnP-RANSAC solving.
% \textbf{Robopose}\cite{labbe2021single}: The state-of-the-art approach for estimating the robot pose via 2D render & compare, which can be adapted both in single-frame and multi-frame situations.
\textbf{CenterNet}\cite{CenterNet}: A single-frame object detection approach that models an object as a single point. We adapt CenterNet to estimate 2D keypoints and reserve the heatmap loss and offset loss \cite{CenterNet} during training. 
\textbf{CenterTrack}\cite{CenterTrack}: A multi-frame object detection and tracking approach. We adapt CenterTrack using similar strategies in CenterNet to detect keypoints.

\noindent\textbf{Metrics.} During the inference, we evaluate both 2D and 3D metrics across all datasets.
\textbf{PCK}: The L2 errors between the 2D projections of predicted keypoints and ground truth. 
Only keypoints that exist within the frame will be considered.
% 然后是3D ADD
\textbf{ADD}: The average Euclidean norm between 3D locations of keypoints and corresponding transformed versions, which directly measures the pose estimation accuracy.
% 最后说明，我们统计了它们的AUC曲线，mean值以及median值
For PCK and ADD, we compute the area under the curve (AUC) lower than a fixed threshold (12 pixels and 6cm, respectively) and their median values.

% Due to the page limits, we compare our method with other human pose estimation methods that consider ......... in the supplementary.

\subsection{Results and Analysis}

As can be seen in Table \ref{PCK}, our method has outperformed all other baselines in PCK and ADD metrics across datasets.
Specifically, 
our median PCK, an important measure that reflects the 2D accuracy, is superior to others, indicating that we have achieved better overall precision in predicting the 2D projections.
We also surpass all other baselines in the AUC of ADD, and so is the median. ADD is a more direct indicator of whether the estimated pose is accurate, and we have outperformed the best of others in Panda 3CAM-AK and Panda Orb at 4.9\% and 7.8\%.
Further, our method can reach 9.77mm and 18.12mm in median ADD in Panda 3CAM-RS and Panda Orb, comparable to the accuracy of the traditional hand-eye calibration empirically.

Compared with single-frame methods (CenterNet\cite{CenterNet}, Dream \cite{Lee2020CameratoRobotPE}), our method absorbs temporal information and shows greater robustness to self-occlusion. The analysis of robustness to self-occlusion is discussed in Section \ref{Robustness to Self Occlusion}. 
Meanwhile, compared with the tracking-based method CenterTrack \cite{CenterTrack}, our method owns the robot structure prior guiding feature alignment and temporal cross attention, which we believe facilitates our model's superiority. 
% 而且强调一下其实Median精度也很不错，在rs相机上（目前最常用的相机）能做到1.6cm（单视角）和1.9cm(多视角)的精度水平，其实也是很令人震撼的。

\begin{table*}[]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{Real Data}  &  \multirow{2}{*}{\# Images} & \multirow{2}{*}{\# 6D Poses} & \multirow{2}{*}{Method} & \multicolumn{2}{c}{PCK} & \multicolumn{2}{c}{ADD}  \\ 
\cline{6-9}
    ~ &   ~ &    ~ &  ~ &        ~ & AUC$\uparrow$ ~ & Median@pix$\downarrow$~ & AUC$\uparrow$ ~ & Median@mm$\downarrow$ \\ 
\hline
\multirow{4}{*}{Panda 3CAM-RS\cite{Lee2020CameratoRobotPE}} & \multirow{4}{*}{\checkmark} & \multirow{4}{*}{5944} & \multirow{4}{*}{1}
            & CenterNet\cite{CenterNet}             & 67.38         & 3.51         & 59.26         & 21.25         \\ 
    ~ &   ~ &  ~ &  ~ & CenterTrack\cite{CenterTrack}         & 68.85         & 3.59         & 58.24         & 23.77  \\ 
    ~ &   ~ &  ~ &  ~ &  Dream\cite{Lee2020CameratoRobotPE}    & 64.82         & 3.90         & 58.60         & 24.57  \\ 
    ~ &   ~ &  ~ &  ~ & Ours                                  &\textbf{75.68} &\textbf{2.68} &\textbf{79.89} &\textbf{9.77} \\ \hline
\multirow{4}{*}{Panda 3CAM-AK\cite{Lee2020CameratoRobotPE}} & \multirow{4}{*}{\checkmark} & \multirow{4}{*}{6394}  & \multirow{4}{*}{1}
            & CenterNet\cite{CenterNet}             & 52.38         & 4.90         & 34.07         & 37.56 \\ 
    ~ &   ~ &  ~ &  ~ & CenterTrack\cite{CenterTrack}         & 58.26         & 4.45         & 43.10         & 32.83 \\ 
    ~ &   ~ &  ~ &  ~ &  Dream\cite{Lee2020CameratoRobotPE}    & 52.28         & 4.83         & 44.55        & 33.68   \\ 
    ~ &   ~ &  ~ &  ~ & Ours                                  &\textbf{62.75} &\textbf{3.19} &\textbf{49.42} &\textbf{29.61} \\ \hline
\multirow{4}{*}{Panda Orb\cite{Lee2020CameratoRobotPE}}& \multirow{4}{*}{\checkmark}  & \multirow{4}{*}{32315}  & \multirow{4}{*}{27}
            & CenterNet\cite{CenterNet}             & 60.11         & 3.47         & 50.59         & 24.22 \\ 
    ~ &   ~ &  ~ &  ~ & CenterTrack\cite{CenterTrack}         & 61.03         & 3.73         & 47.67         & 25.13  \\ 
    ~ &   ~ &  ~ &  ~ & Dream\cite{Lee2020CameratoRobotPE}    & 57.44        & 3.73         & 52.56         & 22.53   \\ 
    ~ &   ~ &  ~ &  ~ & Ours        &\textbf{63.28} &\textbf{3.46} &\textbf{60.30} &\textbf{18.12} \\ \hline
\multirow{4}{*}{Panda Syn Testing} & \multirow{4}{*}{$\times$}  & \multirow{4}{*}{10410}  & \multirow{4}{*}{347}      
            & CenterNet\cite{CenterNet}             & 92.60         & 0.89         & 85.97         & 5.79  \\ 
    ~ &   ~ &  ~ &  ~ & CenterTrack\cite{CenterTrack}         & 91.86         & 0.63         & 85.01         & 6.18  \\ 
    ~ &   ~ &  ~ &  ~ & Dream\cite{Lee2020CameratoRobotPE}    & 80.79         & 0.85         & 79.05         & 7.13  \\ 
    ~ &   ~ &  ~ &  ~ & Ours        &\textbf{94.36} &\textbf{0.44} &\textbf{89.62} &\textbf{4.11} \\
\bottomrule
\end{tabular}}
\caption{\textbf{Quantitative comparison with keypoint-based methods.} $\uparrow$ means higher is better, $\downarrow$ means lower is better. The $\checkmark$ and $\times$ in Real Data denote whether the dataset is real-world. \# Images and \# 6D Poses  denote the total amounts of images and 6D poses in the dataset respectively. For a fair comparison, we train all the methods listed in the above table on Panda Syn Training dataset and report the results regarding PCK and ADD across four testing datasets. Obviously, our method is taking the lead in all metrics upon all datasets.}
 \vspace{-4mm}
\label{PCK}
\end{table*}

\subsection{Compare with Classic Hand-Eye Calibration}
We further design an experiment to compare our approach with traditional hand-eye calibration (HEC) methods, implemented via the easy\_handeye ROS package \cite{EasyHandEye}.
We attach an Aruco Fiducial Marker \cite{Marker} to the Franka Emika Panda's gripper and place an external RealSense D415 to take photos.
The manipulator is commanded to move to $L=20$ positions along a predefined trajectory and stays at each position for 1 second.
To assess the accuracy, we consider choosing $l$ positions from the set of $L$ positions and feed all the detection results from the $l$ positions to a single PnP solver.
Specifically, we choose $C_{L}^{l}$ combinations of $l$ images and if $C_{L}^{l} > 2500$, we randomly sample 2500 combinations.
As shown in Fig \ref{Hand_eye_ours}, our approach and traditional HEC solve a more accurate pose with increasing frames. 
However, our approach outperforms HEC in both median and mean ADD, where we can reach the level of 5 mm while HEC reaches 15 mm at last.
Moreover, our approach is more stable than HEC with frames increasing, which can be inferred from the region surrounded by the minimum and maximum ADD.



% Figure environment removed


\subsection{Ablation Study}
%\subsection{}

% \begin{table}[]
% \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc} \toprule
% Ablation Method & PCK(AUC) & PCK(Median)& ADD(AUC) & ADD(Median) \\ \hline
% Ours (B) & 0.776 & 3.597 & 0.598 & 0.039 \\
% Ours (BR) & 0.823 & 3.308 & 0.736 & 0.026 \\
% Ours (BRCA[7,3,1]) & 0.838 & 2.634 & 0.746 & 0.022 \\
% Ours (BRCA[13,7,3]) & 0.834 & 2.865 & 0.789 & 0.018 \\
% Ours (BRCA[17,9,5]) & 0.824 & 2.780 & 0.738& 0.021\\
% Ours (BRCA[7,3,1] + 3D) & 0.838 & 2.634 & 0.776 & 0.019 \\
% Ours (BRCA[13,7,3] + 3D) & 0.834 & 2.865 & 0.816 & 0.016 \\
% Ours (BRCA[17,9,5] + 3D) & 0.824 & 2.780 & 0.754 & 0.019 \\
% \bottomrule
% \end{tabular}}
% \caption{}
% \label{Ablation}
% \end{table}

% 做ablation的目的是什么呢？
We conduct the ablation studies to investigate : 
1) The function of the robot structure prior for guiding feature alignment.
2) The necessity of using a cross attention module for enhancing the fusion of temporal aligned features between successive frames. 
3) The effectiveness of introducing the pose refiner module for updating a more accurate pose.
4) The influence of different window sizes adopted during temporal cross attention. 
To this end, we construct four ablated versions of our model and test their performances on the most diverse real dataset Panda Orb \cite{Lee2020CameratoRobotPE}.

\vspace{-1mm}
\begin{table}[t]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc} \toprule
 \multirow{2}{*}{SGF} & \multirow{2}{*}{TCA}& \multirow{2}{*}{PRF} & \multicolumn{2}{c}{PCK}     & \multicolumn{2}{c}{ADD}   \\
 \cline{4-7}
         ~ &   ~ &  ~        & AUC$\uparrow$ ~ & Median@pix$\downarrow$ ~ & AUC$\uparrow$ ~ & Median@mm$\downarrow$ \\\hline
 & &  & 49.78 & 4.69 & 37.93 & 34.48 \\
$\checkmark$ &  &  & 62.86 & 3.49 & 55.83 & 21.04 \\
$\checkmark$ & $\checkmark$ &  & \textbf{63.28} & \textbf{3.46} & 58.86 & 19.17 \\
$\checkmark$ & $\checkmark$ & $\checkmark$ & \textbf{63.28} & \textbf{3.46} & \textbf{60.30} & \textbf{18.12} \\
\bottomrule
\end{tabular}}
\caption{Ablation studies with different modules. SGF, TCA, and PRF denote the \textbf{Structure Prior Guided Feature Alignment}, \textbf{Temporal Cross Attention Enhanced Fusion}, and \textbf{Pose Refiner} modules, respectively.
$\checkmark$ denotes the corresponding module that has been used. Results show that all the proposed modules benefit camera-to-robot pose estimation.}
\label{Ablation_all}
\vspace{-3mm}
\end{table}

We report the results in Table \ref{Ablation_all}.
For the version without any module, we send $B_{t-1}, I_{t-1}, I_t$ as input with a shared encoder, directly concatenating the multi-scale features.
With the addition of \textbf{Structure Prior Guided Feature Alignment}, we add one more input $\Tilde{B}_t$, and the performance in both PCK and ADD improves greatly.
This fact reveals that the reprojection belief map $\Tilde{B}_t$ provides a confined area rather than the whole image for the network to focus on, which is easier to detect keypoints.
%
Secondly, with the addition of \textbf{Temporal Cross Attention Enhanced Fusion}, we replace the direct concatenation with cross attention.
The performance in ADD improves higher than that in PCK, which shows that the temporal cross attention module facilitates the precision of 3D keypoint predictions.
Thirdly, the final introduction of \textbf{Pose Refiner} witnesses an improvement of 1.5$\%$ in the AUC of ADD and 1mm in median ADD.
This proves that the reweighted optimisation objective (Equation \ref{3D Rf}) succeeds in filtering out the disturbance of outliers and recomputing a better solution.

As shown in Table~\ref{Ablation_ws}, we change the window size during \textbf{Temporal Cross Attention}.
We analyse the average movement amplitude of a manipulator between two successive frames at FPS 30 and determine the window sizes of the first three multi-scale features.
We ultimately select the window size $[13,7,3]$ to ensure most of the keypoints' movement between two frames can be detected within the window. Results show that the larger or smaller window size will degrade the model's performance in both AUC of ADD and PCK.
We believe the reason for the degradation is that a larger window size will contain more redundant information, while a smaller window size might miss some important information during fast movement tracking.
\vspace{-2mm}
\begin{table}[h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc} \toprule
\multirow{2}{*}{Window Size $[d_1, d_2, d_3]$}  & \multicolumn{2}{c}{PCK} & \multicolumn{2}{c}{ADD}  \\
\cline{2-5}
        ~  & AUC$\uparrow$ ~ & Median@pix$\downarrow$ ~ & AUC$\uparrow$ ~   & Median@mm$\downarrow$  \\ \hline
$[7,3,1]$ & 61.29 & \textbf{3.41} & 57.11 & 18.21 \\
$[13,7,3]$ & \textbf{63.28} & 3.46 & \textbf{60.30} & \textbf{18.12} \\
$[17,9,5]$ & 63.01 & 3.53 & 56.54 & 21.45\\
\bottomrule
\end{tabular}}
\caption{Results of our ablation study with different window sizes during \textbf{Temporal Cross Attention}. The $[13,7,3]$ window size performs best among all choices on Panda Orb \cite{Lee2020CameratoRobotPE}.}
\label{Ablation_ws}
\vspace{-5mm}
\end{table}

\subsection{Robustness to Self-Occlusion Scenarios}\label{Robustness to Self Occlusion}
\vspace{-2mm}

% Figure environment removed
%\vspace{-3mm}

\begin{table}[ht]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccc} \toprule
   \multirow{2}{*}{Method} & \multicolumn{2}{c}{PCK} & \multicolumn{2}{c}{ADD}   \\ 
  \cline{2-5}  
  ~ & AUC$\uparrow$ ~ & Median@pix$\downarrow$~ & AUC$\uparrow$ ~ & Median@mm$\downarrow$ \\ \hline
   Dream \cite{Lee2020CameratoRobotPE} & 49.02/59.46 & 4.13/3.67 & 43.87/54.53 & 30.23/21.28  \\
   CenterNet \cite{CenterNet} &  52.86/61.77 & 4.00/3.39 & 39.05/53.21 & 33.73/22.62  \\
   CenterTrack\cite{CenterTrack} & 54.43/62.54  & 4.31/3.62 & 30.37/51.58 & 42.96/22.52  \\
   Ours &  \textbf{60.02}/\textbf{64.03} & \textbf{3.95}/\textbf{3.37} & \textbf{59.37}/\textbf{60.51} & \textbf{20.18}/\textbf{17.69} \\
\bottomrule
\end{tabular}}
\vspace{-3mm}
\caption{Quantitative results of the self-occlusion experiment. The Left and right sides of '/' are the results of severe and no self-occlusion respectively. Results show our method performs robustly when encountering self-occlusion while baselines drop greatly.}
%The left number is the AUC/Median of 31 severe self-occlusion videos, and the right is the other 58 less self-occlusion videos.}
\label{Self-Occlusion-Table}
\vspace{-3mm}
\end{table}
We perform an additional experiment to show our model's robustness to self-occlusion. We divide Panda Orb \cite{Lee2020CameratoRobotPE} into 89 videos, including 31 videos with severe self-occlusion (5971 images) and 58 videos with less or no self-occlusion (26344 images). 
Results in Table \ref{Self-Occlusion-Table} show baselines drop greatly when encountering occlusion, while ours decreases slightly.
Also, from Figure \ref{Self-Occlusion-Figure}, we can see that baselines detect occluded keypoints with significant deviation or even fail. 
In comparison, our model shows more precise predictions. 
The main reason for our model's robustness is that we adopt temporal information fusion and utilise structure priors efficiently rather than estimating pose from a single frame or concatenating temporal features crudely. 

\subsection{Robot Grasping Experiments}
In this section, we construct real-world robotic grasping experiments to demonstrate the performance of our method.

\noindent
\textbf{Experimental Protocol.}
% 我们将我们的模型应用在真机上，可以做到实时标定，并用预测的标定结果来做下游任务。
We perform two experiments using the Franka Emika Panda robot. One of the experiments focuses on robot grasping in a static environment, and another on grasping in a dynamic environment. 
To ensure a fair comparison, GraspNet\cite{Fang2020GraspNet1BillionAL} is used to estimate the robot grasping pose in all experiments, while different methods are employed for camera-to-robot pose estimation. 
In the experiments, all learning-based camera-to-robot pose estimation methods use 30 frames to estimate the camera-to-robot pose, and all the objects are selected from YCB~\cite{calli2015benchmarking} dataset.
For hand-eye calibration, in the static experiment, we use easy\_handeye\cite{EasyHandEye} to acquire the pose. In the dynamic experiment, hand-eye calibration is incapable of online calibration, so their results are none.
% We further adapt our model to a real-time pose estimator and utilise the estimation result with graspnet \cite{Fang2020GraspNet1BillionAL} to implement the downstream grasping task using the Franka Emika Panda robot. We use 30 frames to calculate the robot pose in the real-world experiment.
% 我们基于graspnet的网络结构，利用我们的预测结果，执行了抓取任务。
% 接下来是实验描述了x

In the static experiment, we conduct six scenes, and each scene includes  4-7 randomly chosen objects. During the completion of a grasping task in one scene, the camera remains stationary. After finishing grasping in a particular scene, we move the camera to another position for the next scene. 
In the dynamic experiment, we not only change the camera pose when switching between different scenes but also adjust the camera pose after completing the grasping of each object during the execution of a grasping task in the same scene, which is a tougher setting than the static one.


\noindent
\textbf{Metrics.} To evaluate the performance accurately, we follow the grasping metrics from SuctionNet\cite{cao2021suctionnet}.
We adopt $R_{grasp}$, the
ratio of the number of successful grasps to the number total
grasps, and $R_{object}$,  the ratio of the
number of successfully cleared objects to the number of totals. Moreover, if three consecutive grasping attempts fail in a scene, we consider the experiment for that scene terminates.

\begin{table}[h]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{cccccc}\toprule
Method        & $R_{grasp}\uparrow$   & $R_{object}\uparrow$  \\ \hline
easy\_handeye\cite{EasyHandEye} & 28/52 = 53.8\%       & 28/32 = 87.5\%             \\
DREAM\cite{Lee2020CameratoRobotPE}         & 26/52 = 50.0\%      & 26/32 = 81.2\%             \\
CenterTrack\cite{CenterTrack}   & 26/60 = 43.3\%       & 26/32 = 81.2\%              \\
CenterNet\cite{CenterNet}     & 29/51 = 56.9\%      & 29/32 = 90.6\%              \\
Ours          & \textbf{32/48 = 66.7\%}       & \textbf{32/32 = 100\%}        
\\ 
\bottomrule
\end{tabular}}
%\vspace{-1mm}
\caption{Quantitative comparison of the performance of different methods applied to robot grasping tasks in the static experiment.}
\label{real_static}
\vspace{-0.9mm}
\end{table}

\begin{table}[h]
\centering
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{cccccc}\toprule
Method        & $R_{grasp}\uparrow$   & $R_{object}\uparrow$ \\\hline
easy\_handeye\cite{EasyHandEye} & -              & -                           \\
DREAM\cite{Lee2020CameratoRobotPE}         & 15/38 = 39.5\% & 15/32 = 46.9\% \\
CenterTrack\cite{CenterTrack}   & 24/51 = 47.1\% & 24/32 = 75.0\% \\
CenterNet\cite{CenterNet}     & 16/42 = 38.0\% & 16/32 = 50.0\%  \\
Ours          & \textbf{30/51 = 58.8\%} & \textbf{30/32 = 93.8\%}   
\\ 
\bottomrule
\end{tabular}}
\caption{Quantitative comparison of different methods' performance applied to robot grasping tasks in the dynamic experiment. The blank in the "easy\_handeye" column is because this traditional calibration method cannot be performed online.}
\label{real_dynamic}
\vspace{-4.4mm}
\end{table}

\noindent
\textbf{Results and Analysis.} 
%As shown in Table \ref{real_static} and Table \ref{real_dynamic}, our model surpasses other baselines. This advantage is more obvious as the experiment gets more difficult. As we can see, the performance of single-frame models (DREAM\cite{Lee2020CameratoRobotPE} and CenterNet\cite{CenterNet}) drops drastically when self-occlusion is severe. In contrast, the performances of the multi-frame models, like CenterTrack\cite{CenterTrack} and ours, are not much influenced.
%Due to the page limit, we defer more details to the supplementary materials.
Table \ref{real_static} and Table \ref{real_dynamic} show the results of applying different camera-to-robot pose estimation methods to the grasping experiments in static and dynamic environments, respectively. In the static experiment, our method achieves a 100\% object grasping success rate, outperforming all other baselines, including traditional hand-eye calibration. In the dynamic experiment, since the camera pose changes after each grasping attempt in the same scene, this places higher demands on the robustness and speed of the camera-to-robot pose estimation. Other methods experience significant drops in grasping success rates, while our method maintains a very high success rate with only a slight decrease compared to the static experiment. These experiments demonstrate the accuracy and stability of our method for camera-to-robot pose estimation.



