\section*{Appendix}



\section{A bad case for tangent intersection selection}

It is tempting to consider a variant of \cref{alg:gradient_line_search} where at each iteration we query at the intersection of the tangents of the two external points,
that is,
$x^q_t = \argmin_{(x,y)\in\Delta_t} y$.
This algorithm can be very slow, however.

Consider for example the function
$f(x)=\max\{-x/100, e^{bx}\}$ 
for $b > 0$, with $x^0_1=-100, x^1_1=100$.
It can be verified
that, for this algorithm,
for approximately $100/b$ steps $t$,
$x^0_t=x^0_1$ while $x^1_t \approx 100 - t/b$.
That is, only linear progress is made on the $x$ axis.

By constrast, \nogradalg{} takes less than 15 iterations for $b=10$ for example. 












\section{Numerical stability}\label{sec:stability}

\subsection{Line-line intersection}
Some numerical stability care needs to be taken when implementing the line-line intersection function from 4 points to calculate the $y$-gap.

Empirically, we found that the following formula is more stable than the determinant method.
For 4 points $p^1, p^2, p^3, p^4$, 
with $y^1 \geq y^2$ and $y^3 \leq y^4$,
define 
$a^{12} = \frac{y^2-y^1}{x^2 - x^1}$
and $a^{34} = \frac{y^3-y^4}{x^3 - x^4}$,
and 
$f^{12}(x) = a^{12}(x - x^2) +y^2$ 
and $f^{34}(x) = a^{34}(x - x^3) +y^3$.

Then, for \emph{any} $\hat x\in\Reals$,
the intersection of $f^{12}$ and $f^{34}$ is at 
\begin{align*}
    x^c = \hat x + \frac{f^{12}(\hat x) - f^{34}(\hat x)}{a^{34} - a^{12}}
\end{align*}
First we pick $\hat x=x^2$, then we pick $\hat x = x^c$ and repeat the latter one more time to obtain a more accurate value. 
Finally, we return the last $x^c$, and we take $y^c = \min\{f^{12}(x^c), f^{34}(x^c)\}$
conservatively (for function minimization).
The quantity $|f^{12}(x^c) - f^{34}(x^c)|$ is the approximation error.

Additionally, (possibly numerical) zeros and infinities must be taken care of in a conservative manner (always underestimating $y^c$).


\subsection{Initial gradient for quasi-exact line search}
When using an initial gradient for quasi-exact line search, it is important to keep the line constraint
as a function $\alpha \mapsto f(x) - \alpha \|\nabla f(x)\|^2$ as is done in backtracking line search, rather than the more tempting solution of adding virtual points and letting the 5-point algorithm recover the tangent.














