\section{Table of notation}
\begin{tabular}{c|l}
    $f$ & a convex function to minimize on $[\xleft, \xright]$ \\
    $f'$ & derivative of $f$ \\
    $p^a_t=(x^a_t, y^a_t)$ & a point labelled $a$ at iteration $t$\\
    $f^a_t(x)$ & tangent of $f$ at $x^a_t$ \\
    $f^{ab}_t(x)$ & line going through $p^{a}_t$ and $p^b_t$ \\
    $[\xleft, \xright]$ & initial interval in which to start the search \\
    $\yleft, \yright$ & $f(\xleft), f(\xright)$ \\
    $\xlow, \ylow$ & \XXX \\
    $\xhigh, \yhigh$ & \XXX \\
    $\ytol$ & User-specified tolerance used to terminate the line search \\
    $[x^0_t, x^1_t]$ & interval maintained by the gradient-based algorithms \\
    $x^q_t$ & query coordinate of an algorithm at iteration $t$ \\
    $\Delta_t$ & optimality region for \gradalg{} at step $t$; contains $(x^*, f(x^*))$\\
    $\Delta^x_t, \Delta^y_t$ & $x$ and $y$ coordinates of $\Delta_t$ \\
    $P_t$ & $(f,\xleft, \xright)$-convex set of points maintained by the \nogradalg{} algorithm\\
    $\Delta(P)$ & optimality region for a $(f,\cdot,\cdot)$ set of points $P$ \\
    $\Delta^x(P_t), \Delta^y(P_t)$ & Sets of $x$ and $y$ coordinates of $\Delta(P_t)$ \\
    $x^-, x^+$ & $\inf \Delta(P), \sup \Delta(P)$ \\
    $p^{ij,kl}=(x^{ij,kl}, y^{ij,kl})$ & Intersection point of $f^{ij}$ and $f^{kl}$ \\
\end{tabular}
