%% bare_jrnl.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[journal]{IEEEtran}
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.

% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
\usepackage{diagbox}
\usepackage{bm}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{amsfonts,amssymb}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{bbding}
\usepackage{booktabs}                 % 三线表-短细横线
\usepackage{algorithmic}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
% \usepackage{xparse}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{amssymb,mathtools}

\usepackage{xurl}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}}     %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}    %UseOutput in the format of Algorithm

% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex







\makeatletter
\def\footnoterule{\kern-3\p@
  \hrule \@height 1pt \@width 3.5in \kern 2.6\p@} % the \hrule is .4pt high
\makeatother



\ExplSyntaxOn

% keep the original \footnoterule
\cs_new_eq:NN \guillaume_footnoterule: \footnoterule
% redefine \footnoterule
\cs_set_protected:Npn \footnoterule
 {
  \bool_set_true:N \l_guillaume_footnoterule_bool
  \seq_map_function:NN \g_guillaume_footnoterule_pages_seq \guillaume_test:n
  \bool_if:NT \l_guillaume_footnoterule_bool { \guillaume_footnoterule: }
 }
\cs_new_protected:Nn \guillaume_test:n
 {
  \int_compare:nT { #1 = \value{page} }
   {
    \seq_map_break:n { \bool_set_false:N \l_guillaume_footnoterule_bool }
   }
 }
% save the list of pages where not to have a footnoterule
\seq_new:N \g_guillaume_footnoterule_pages_seq
\NewDocumentCommand{\nofootnoterule}{m}
 {
  \seq_gset_from_clist:Nn \g_guillaume_footnoterule_pages_seq { #1 }
 }
\ExplSyntaxOff

\nofootnoterule{1}




% \makeatletter
% \def\footnoterule{\kern-3\p@
%   \hrule \@height 1pt \@width 3.5in \kern 2.6\p@} % the \hrule is .4pt high
% \makeatother

% \renewcommand{\footnoterule}{%
%   \kern -3pt
%   \hrule width \textwidth height 1pt
%   \kern 2pt
% }

% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "% Figure environment removed

% % Figure environment removed

\textbf{Training process}: In the training process, firstly, we use LSTM to compute the conditioning inputs of the DDPM,
\begin{equation}
    \label{eq:conditioning input}
    \bm {con}_{1:L}, (\bm h_L,\bm c_L) = \mathrm{LSTM_{R}}(\bm x_{1:L}, \bm {cov}_{1:L}),
\end{equation}
where $\bm {cov}_l$ denotes the covariance of the observation at time step $l$. In this paper, the covariance contains temporal information like \cite{ref7}. $(\bm h_L,\bm c_L)$ is the last hidden state of LSTM which will be used in the forecasting model in Section \ref{sec:LSTM Diffusion Model for Forecasting}. For $\mathrm{LSTM_{R}}$ the initial hidden state is $\mathbf{0}$.

Then, we randomly select a diffusion step $n\in \{1,\cdots,N\}$ and generate diffusion samples according to \eqref{eq:Reparameterizing},
\begin{equation}
    \label{eq:diffusion samples}
    \bm x^n_{1:L} = \sqrt{\overline{\alpha}_n}\bm x_{1:L}+\sqrt{1-\overline{\alpha}_n}\bm\epsilon_{1:L},
\end{equation}
where $\bm\epsilon_{l}\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$, and we store $\bm\epsilon_{1:L}$ as labels. 

Subsequently, we estimate $\bm\epsilon_{l}$ using function $\bm \epsilon_{\theta}(\cdot)$,
\begin{equation}
    \label{eq:estimate epsilon}
    \hat{\bm \epsilon}_l = \bm \epsilon_{\theta}(\bm x_l^n,\bm{con}_l,n),\quad l\in \{1,2,\cdots,L\}.
\end{equation}
In this paper, we exploit \textit{DiffWave} \cite{ref6} as $\bm \epsilon_{\theta}(\cdot)$ with limited modification to adjust the model to our data since it has shown great performance when being utilized for DDPM to generate time series data \cite{ref6, ref7}. 

Lastly, we calculate the MSE between $\bm\epsilon_{1:L}$ and $\bm{\hat\epsilon}_{1:L}$ as the optimization objective $\mathcal{L}_R$ of Adam optimizer \cite{ref8},
\begin{equation}
    \label{eq: optimization objective r}
    \mathcal{L}_R = \mathrm{MSE}(\bm\epsilon_{1:L}-\bm{\hat\epsilon}_{1:L}).
\end{equation}
The training process ends when $\mathcal{L}_R$ converges.

\textbf{Inference process}:
In the inference process, we aim to reconstruct $\{\bm x_1^0,\bm x_2^0,\cdots,\bm x_L^0\}$ from $\{\bm x_1^N,\bm x_2^N,\cdots,\bm x_L^N\}$ in which $\bm x_l^N\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$.
Firstly, we initialize $\bm x_{1:L}^N$ and compute conditioning inputs $\bm{con}_{1:L}$ according to \eqref{eq:conditioning input}. For a given denoising step $n$, we use \eqref{eq:estimate epsilon} to estimate the Gaussian noise $\bm \epsilon_{l}^{n-1}$ that is added to $\bm x_l^{n-1}$ at the diffusion step $n-1$. With the estimated Gaussian noise $\bm {\hat\epsilon}_{l}^{n-1}$, we can estimate ${\bm x}_l^{n-1}$ according to \eqref{eq:denoising},
\begin{equation}
    \label{eq:denoising r}
    {\bm x}_l^{n-1} = \frac{1}{\sqrt{\alpha}_n}\left(\bm x_l^n-\frac{\beta_n}{\sqrt{1-\overline{\alpha}_n}}\hat{\bm\epsilon}_l^{n-1} \right) + \sigma_{\theta}\bm z.
\end{equation}
Starting from $\bm x^N_l$, we use \eqref{eq:denoising r} recurrently until we obtain $\bm x_l^0$, which is shown in Fig. \ref{fig:diffusion-r}.
%In this paper, we set $\sigma_{\theta}$ to 0 to avoid multiple samplings.

\subsection{LSTM-DDPM Model for Forecasting}\label{sec:LSTM Diffusion Model for Forecasting}

For forecasting, we aim to exploit DDPM to forecast $\{\bm x_{L+1},\bm x_{L+2},\cdots,\bm x_{L+T}\}$ as $\{\bm{\hat x}_{L+1}^0,\bm{\hat x}_{L+2}^0,\cdots,\bm{\hat x}_{L+T}^0\}$ with a Gaussian random input $\bm x_{L+1:L+T}^N$. Similarly, we use LSTM to compute the conditioning inputs for the DDPM.

\textbf{Training process}:
Similarly with Section \ref{sec:LSTM Diffusion Model for Reconstruction}, in the training process, we first use LSTM to compute the conditioning inputs. Different from \cite{ref7}, in which the predicted $\bm{\hat x}_{L+t}$ is used as input of LSTM to compute conditioning input $\bm{con}_{L+t+1}$ to forecast $\bm{\hat x}_{L+t+1}$, we only use $\bm cov_{L+t+1}$ as input of LSTM to compute $\bm{con}_{L+t+1}$,
\begin{equation}
    \label{eq:conditioning input f}
    \bm {con}_{L+1:L+T} = \mathrm{LSTM_F}(\bm {cov}_{1:L+T}).
\end{equation}
The initial hidden state of $\mathrm{LSTM_F}$ is $(\bm h_L,\bm c_L)$ obtained from \eqref{eq:conditioning input} which is expected to contain all the information of $\bm x_{1:L}$ and $\bm {cov}_{1:L}$. This is the only connection between \textit{LSTMDiff$_R$} and \textit{LSTMDiff$_F$} as seen in Fig. \ref{fig:diffusion comb}.
This modification of the LSTM input can help to mitigate the accumulated error issue of LSTM since in the inference process, the predicted $\bm{\hat x}_{L+t}$ is usually different from $\bm{x}_{L+t}$ that is used for training the model. More importantly, with this modification, we do not need to wait for $\bm{\hat x}_{L+t}$ to compute $\bm{con}_{L+t+1}$. As a result, compared with \cite{ref7}, if we have enough computation resources, the inference speed can increase $T$ times.

Then, we randomly select a diffusion step $n$ and generate diffusion samples according to \eqref{eq:Reparameterizing},
\begin{equation}
    \label{eq:diffusion samples f}
    \bm x^n_{L+1:L+T} = \sqrt{\overline{\alpha}_n}\bm x^0_{L+1:L+T}+\sqrt{1-\overline{\alpha}_n}\bm\epsilon_{L+1:L+T},
\end{equation}
where $\bm\epsilon_{L+t}\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$, and we store $\bm\epsilon_{L+1:L+T}$ as labels. 
Subsequently, we estimate $\bm\epsilon_{L+t}$ using function $\bm \epsilon_{\theta}(\cdot)$,
\begin{equation}
    \label{eq:estimate epsilon f}
    \hat{\bm \epsilon}_{L+t} = \bm \epsilon_{\theta}(\bm x_{L+t}^n,\bm{con}_{L+t},n),\quad t\in \{1,2,\cdots,T\}.
\end{equation}
Lastly, according to \eqref{eq:unified problem}, we calculate the MSE between $\bm\epsilon_{L+1:L+T}$ and $\bm{\hat\epsilon}_{L+1:L+T}$ as the optimization objective $\mathcal{L}_{F}$ of Adam optimizer,
\begin{equation}
    \label{eq:optimization objective f}
    \mathcal{L}_{F} = \mathrm{MSE}(\bm\epsilon_{L+1:L+T}-\bm{\hat\epsilon}_{L+1:L+T}).
\end{equation}
The training process ends when $\mathcal{L}_{F}$ converges.

\textbf{Inference process}:
In the inference process, we aim to calculate $\{\bm x_{L+1}^0,\cdots,\bm x_{L+T}^0\}$ from $\{\bm x_{L+1}^N,\cdots,\bm x_{L+T}^N\}$ where $\bm x_{L+t}^N\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$.
Firstly, we initialize $\bm x_{L+1:L+T}^N$ and compute conditioning inputs $\bm{con}_{L+1:L+T}$ according to \eqref{eq:conditioning input} and \eqref{eq:conditioning input f}. For a given denoising step $n$, we use \eqref{eq:estimate epsilon f} to estimate the Gaussian noise $\bm \epsilon_{L+t}^{n-1}$ that is added to $\bm x_{L+t}^{n-1}$ at the diffusion step $n-1$. With the estimated Gaussian noise $\bm {\hat\epsilon}_{L+t}^{n-1}$, we can estimate ${\bm x}_{L+t}^{n-1}$ according to \eqref{eq:denoising},
\begin{equation}
    \label{eq:denoising f}
    {\bm x}_{L+t}^{n-1} = \frac{1}{\sqrt{\alpha}_n}\left(\bm x_{L+t}^n-\frac{\beta_n}{\sqrt{1-\overline{\alpha}_n}}\hat{\bm\epsilon}_{L+t}^{n-1} \right) + \sigma_{\theta}\bm z.
\end{equation}
Starting from $\bm x^N_{L+t}$, we use \eqref{eq:denoising f} recurrently to get $\bm x_{L+t}^0$. This process is similar to Fig. \ref{fig:diffusion-r}.
%In this paper, we set $\sigma_{\theta}$ to 0 to avoid multiple samplings.


\subsection{Complete \textit{LSTMDiff}} \label{sec:LSTMDiff}
The DDPMs in \textit{LSTMDiff$_R$} and \textit{LSTMDiff$_F$} are the same, which enforces them to generate the same output given the same conditioning input.

\textbf{Training process}: According to \eqref{eq: optimization objective r} and \eqref{eq:optimization objective f}, the optimization objective of \textit{LSTMDiff} is
\begin{equation}
    \label{eq:optimization objective comb}
    \mathcal{L}=\mathcal{L}_{R}+\gamma\mathcal{L}_{F},
\end{equation}
where $\gamma$ is a balancing coefficient, we set it to 1 in this paper. The training process ends when $\mathcal{L}$ converges.

\textbf{Inference process} The inference process of \textit{LSTMDiff} is a simple combination of the models demonstrated in Section \ref{sec:LSTM Diffusion Model for Reconstruction} and Section \ref{sec:LSTM Diffusion Model for Forecasting}. We can generate the reconstruction sequence $\bm{\hat x}_{1:L}^0$ according to \eqref{eq:denoising r} and generate the forecasting sequence $\bm{\hat x}_{L+1:L+T}^0$ according to \eqref{eq:denoising f}. 

The pipeline of the training and inference processes are summarized in Algorithm \ref{al:algorithm 1} and Algorithm \ref{al:algorithm 2}, respectively.

% \begin{algorithm}[htbp]
% \caption{Training Process of LSTMDiff}\label{al:algorithm 1}
% \begin{algorithmic}[1]
% \REQUIRE Initialized $\epsilon_\theta(\cdot)$ and training data
% \ENSURE Trained $\epsilon_\theta(\cdot)$
% \STATE Sample a minibatch of data;
% \STATE Compute conditioning inputs according to \eqref{eq:conditioning input} and \eqref{eq:conditioning input f};
% \STATE Randomly select a diffusion step $n$ and generate diffusion samples according to \eqref{eq:diffusion samples} and \eqref{eq:diffusion samples f};
% \STATE Estimate the diffusion noise according to \eqref{eq:estimate epsilon} and \eqref{eq:estimate epsilon f};
% \STATE Optimize the estimator $\epsilon_\theta(\cdot)$ according to \eqref{eq:optimization objective comb};
% \STATE Repeat the above steps until $\epsilon_\theta(\cdot)$ converges.
% \end{algorithmic}
% \end{algorithm}


% \begin{algorithm}[htbp]
% \caption{Inference Process of LSTMDiff}\label{al:algorithm 2}
% \begin{algorithmic}[1]
% \REQUIRE Trained $\epsilon_\theta(\cdot)$ and an inference sample $\bm x_{1:L}$
% \ENSURE Reconstruction and forecasting result $\bm x_{1:L+T}^0$
% \STATE Given an inference sample $\bm x_{1:L}$, compute its conditioning inputs according to \eqref{eq:conditioning input} and \eqref{eq:conditioning input f} and generate a Gaussian noise sequence $\bm x_{1:L+T}^N\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$;
% \STATE Use $\epsilon_\theta(\cdot)$ to estimate the diffusion noise according to \eqref{eq:estimate epsilon} and \eqref{eq:estimate epsilon f};
% \STATE Use the noise to denoise the current PDF and obtain the PDF of the previous step according to \eqref{eq:denoising r} and \eqref{eq:denoising f};
% \STATE Repeat step 2 and step 3 until we obtain $\bm x_{1:L+T}^0$.
% \end{algorithmic}
% \end{algorithm}

\begin{algorithm}[t!]
\caption{Training Process of \textit{LSTMDiff}}\label{al:algorithm 1}
\begin{algorithmic}[1]
\DontPrintSemicolon
% \small
% \tiny
% \scriptsize	
% \footnotesize
\small
% \scriptsize
\REQUIRE Randomly initialized $\epsilon_\theta(\cdot)$ and training data $X$
\ENSURE Trained $\epsilon_\theta(\cdot)$

\hrule

\For{$epoch$ = $1:max\_epoch$}
{
    
    \For{each $x_{1:L+T}$ in $X$}
    {
        \tcp*[h]{\textit{\footnotesize Conditional inputs for reconstruction}}\;
        \textit{$\bm {con}_{1:L}, (\bm h_L,\bm c_L) = \mathrm{LSTM_{R}}(\bm x_{1:L}, \bm {cov}_{1:L})$ \\}        
        
        \tcp*[h]{\textit{\footnotesize Conditional inputs for forecasting}}\;
        \textit{$\bm {con}_{L+1:L+T} = \mathrm{LSTM_F}(\bm {cov}_{1:L+T})$ \\}

        \tcp*[h]{\footnotesize\textit{Randomly selected n and $\bm\epsilon_{i}\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$}}\;
        \textit{$\bm x^n_{1:L+T} = \sqrt{\overline{\alpha}_n}\bm x_{1:L+T}+\sqrt{1-\overline{\alpha}_n}\bm\epsilon_{1:L+T}$.\\}

        \tcp*[h]{\footnotesize\textit{Estimate $\bm\epsilon_{1:L+T}$ with $\epsilon_\theta(\cdot)$}}\;
        $\hat{\bm \epsilon}_{1:L+T} = \bm \epsilon_{\theta}(\bm x_{1:L+T}^n, \bm{con}_{1:L+T},n)$      

        \tcp*[h]{\footnotesize\textit{Loss function}}\;
        $\mathcal{L} = \mathrm{MSE}(\bm\epsilon_{1:L+T}-\bm{\hat\epsilon}_{1:L+T})$ 
        
        Minimizing $\mathcal{L}$ to optimize $\epsilon_\theta(\cdot)$
    }
}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[t!]
\caption{Inference Process of \textit{LSTMDiff}}\label{al:algorithm 2}
\begin{algorithmic}[1]
\DontPrintSemicolon
% \small
% \tiny
% \scriptsize	
% \footnotesize
\small
% \scriptsize

\REQUIRE Trained $\epsilon_\theta(\cdot)$ and an inference sample $\bm x_{1:L}$
\ENSURE Reconstruction and forecasting result $\bm x_{1:L+T}^0$

\hrule

\tcp*[h]{\footnotesize\textit{Conditional inputs for reconstruction}}\;
\textit{$\bm {con}_{1:L}, (\bm h_L,\bm c_L) = \mathrm{LSTM_{R}}(\bm x_{1:L}, \bm {cov}_{1:L})$ \\}

\tcp*[h]{\footnotesize\textit{Conditional inputs for forecasting}}\;
\textit{$\bm {con}_{L+1:L+T} = \mathrm{LSTM_F}(\bm {cov}_{1:L+T})$ \\}

\For{n=N:1}
{
    
    $\hat{\bm \epsilon}_{1:L+T} = \bm \epsilon_{\theta}(\bm x_{1:L+T}^n,\bm{con}_{1:L+T},n)$

    \tcp*[h]{\footnotesize$\bm x_i^N\sim \mathcal{N}(\mathrm{\mathbf{0}},\mathrm{\mathbf{I}})$}\;
    ${\bm x}_{1:L+t}^{n-1} = \frac{1}{\sqrt{\alpha}_n}\left(\bm x_{1:L+t}^n-\frac{\beta_n}{\sqrt{1-\overline{\alpha}_n}}\hat{\bm\epsilon}_{1:L+t}^{n-1} \right) + \sigma_{\theta}\bm z$
}

\end{algorithmic}
\end{algorithm}

\section{Discussion} \label{sec:4}
In this section, we first give a description of our datasets. Then we evaluate the performance of \textit{LSTMDiff} on the ECF problem. Afterwards, our proposed and baseline ETD methods are introduced. Lastly, we compare the performance of the above methods on the ETD problem.

\subsection{Datasets} \label{sec:datasets}

We employ two datasets to evaluate our proposed scheme. The first is \textit{Electricity}$\footnote{\textbf{Electricity dataset can be accessed through the following link:} \url{https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014}}$ which is a real-world dataset that contains 370 customers’ hourly electricity consumption. The second one is \textit{Electricity-Theft}$\footnote{\textbf{Electricity-Theft dataset can be accessed through the following link:} \url{https://github.com/asr-vip/Electricity-Theft}}$ \cite{ref11}, which is a synthetic 15-minute smart grid dataset generated with the “GridLab-D” simulation tool \cite{ref12}. In our experiments, the reconstruction length and forecasting length are both 24 hours, i.e., $L$ and $T$ are 24 for \textit{Electricity} and 96 for \textit{Electricity-Theft}.

\textbf{\textit{Electricity}:} Since our scheme is user-specific, we select four representative users whose electricity consumptions are around ten (user 2), a hundred (user 1), several hundred (user 3), and several thousand (user 4), to construct our datasets for evaluation. 
% Since a user's behaviour can change over time, we train the model using the most recent data. This can also help to reduce the memory requirement and training time. 
We select the power consumption data from January 1st, 2014, to March 1st, 2014, to construct the dataset. Then we divide the constructed dataset into three non-overlapped datasets, i.e., training (70\%), Validation (10\%), and test (20\%) datasets. To evaluate the capability of our model for the ETD problem, we apply all seven types of attacks only to the test dataset since we don't need attack data to train our model. To verify the general applicability of the proposed scheme, we construct another dataset based on 278 users of \textit{Electricity}, after dropping the users with missing values or too much noise. Since our scheme is user-specific, we need considerable time to train the model 278 times. Instead, we concatenate the data of 278 users so the input space $\mathcal{X} = \mathbb{R}^1$ is changed to $\mathcal{X} = \mathbb{R}^{278}$. Thus, a single \textit{LSTMDiff} can produce the reconstruction and forecasting sequences of 278 users in one inference process. We compute the mean value and the standard deviation value of the training dataset and then use them to normalize the training, validation, and test datasets. 

\textbf{\textit{Electricity-Theft}:} The dataset used in this study comprises of data collected at 15-minute intervals over a span of 31 days. As the dataset does not contain a specific representative user, we randomly select one user to construct the evaluation dataset. In contrast to the conventional dataset, \textit{Electricity}, which solely includes electricity consumption data, the \textit{Electricity-Theft} encompasses additional information such as electricity consumption, voltage, and current. Given the high correlation between voltage, current, and electricity consumption, our constructed dataset incorporates real power, voltage, and current data. To ensure a fair evaluation, we divide the constructed dataset into three non-overlapping subsets: training (70\%), validation (10\%), and test (20\%) datasets. Notably, after normalization, the curves of real power and current exhibit similar shapes. To introduce a greater challenge and avoid data leakage, we apply the same attack to both real power and current, preserving the similarity between their respective curves. Additionally, all seven types of attacks are exclusively applied to the test dataset for evaluation purposes. 
Similarly, we compute the mean values and the standard deviation values of the three variables, i.e., real power, voltage, and current, of the training dataset and then use them to normalize the training, validation, and test datasets. 

\subsection{Experiment results on Electricity Consumption Forecasting} \label{sec:Experiment results on ECF}

In this section, we evaluate our model and the baseline model on the ECF problem. Table \ref{tab:forecasting electricity} shows the MAE of the two models. We can see that \textit{LSTMDiff} shows comparable performance with the \textit{LSTM} model. 

% We show some visualization results on \textit{Electricity} in Fig. \ref{fig:forecasting E}. 
From Table \ref{tab:forecasting electricity}, we can see that both methods cannot perform satisfactorily on User 1. 
%And in the leftmost column of Fig. \ref{fig:forecasting E}, both models tend to forecast higher values. 
This observation is consistent with the data characteristics, i.e., the values of test data of User 1 are consistently smaller than those of training and validation data. In the general time series forecasting area \cite{ref7}, instance normalization is usually used to avoid this problem, i.e., mean shifting between training and test data. However, as mentioned in Section \ref{sec:related work}, `fixed reduction' and `partial reduction' attacks cannot be detected if the input is preprocessed with instance normalization because the normal and artificial sequences will become the same. Thankfully, although the MAE is relatively high on User 1, the shapes of the forecasting curves and the ground truths are similar (See Fig. 3 in Appendix). So, we can distinguish the normal and the attack sequences by the following anomaly score,
\begin{equation}
    \label{eq:error_F_adjust}
    \epsilon_F={\rm mean}(|\hat{\bm x}-\bm x+{\rm mean}(\bm x)-{\rm mean}(\hat{\bm x})|),
\end{equation}
where $\bm x$ and $\bm{\hat{x}}$ denote the real data and forecasting result, respectively. Thus, we can ignore the mean shift and only focus on the shape of the forecasting curve for the ETD problem.

For the ECF problem on \textit{Electricity-Theft}, \textit{LSTMDiff} and the \textit{LSTM} model show different behaviour. \textit{Electricity-Theft} shows high variance on `real power' and `current' data, and the \textit{LSTM} model tries to predict the mean values of the `real power' and `current' of several past time steps while \textit{LSTMDiff} tries to approximate the ground-truth behaviour (See Fig. 4 Appendix). However, the MAE of \textit{LSTMDiff} is also high due to the high variance of this dataset. 
% Thus, we can expect the failure of the FEM based on the \textit{LSTM} model for the ETD problem on \textit{Electricity-Theft}. Theoretically, under a 'fixed reduction' attack, the MAE between the ground truth and the forecasting result of artificial data should be similar to the MAE between the ground truth and the forecasting result of normal data since the forecasting value depends on the mean value of several past time steps and the variance is not changed under a 'fixed reduction' attack. This is consistence with the AUC score of 'LSTM-F', which is around 0.5, in Table \ref{tab:AUC scores Synthetic Smart Grid Data}. Besides, under a 'partial reduction' attack, the MAE between the ground truth and forecasting result of artificial data is expected to be less than the MAE between the ground truth and forecasting result of normal data since the variance of the artificial data becomes smaller. This is consistence with the AUC score of 'LSTM-F', which is less than 0.5, under the 'partial reduction' attack in Table \ref{tab:AUC scores Synthetic Smart Grid Data}. The extreme case is the 'average consumption' attack in which the variance of artificial data is 0. Then the MAE between the forecasting result and artificial ground truth will be much less than the MAE between the forecasting result and normal ground truth. Consistently, the AUC is 0 for 'LSTM-F' in Table \ref{tab:AUC scores Synthetic Smart Grid Data} under the 'average consumption' attack. On the other hand, although \textit{LSTMDiff} cannot forecast precisely on \textit{Electricity-Theft}, it tries to produce similar behaviour. Thus, it can work for the ETD problem using forecasting error as shown in Table \ref{tab:AUC scores Synthetic Smart Grid Data}.



% Figure environment removed 

\begin{table}[htbp]
\renewcommand\arraystretch{1.5}
    \centering
    \caption{MAE of Different ECF Methods}
    \begin{tabular}{c|c|c|c|c|c|c}
    \hline
         {} & \textbf{User 1} & \textbf{User 2}& \textbf{User 3}&\textbf{User 4}&\textbf{Ave}&\textbf{ET}\\
         \hline
         {\textbf{LSTM}\cite{ref38}} &\textbf{0.292}&0.077&\textbf{0.155}&0.083 &\textbf{0.219}&\textbf{0.525}\\
         \hline
         {\textbf{LSTMDiff}} &0.633&\textbf{0.016}&0.203 &\textbf{0.082} &0.241 &0.642\\
         \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item \textbf{User 1-4}: Users from \textit{Electricity}; \textbf{Ave}: Average MAE for 278 users in \textit{Electricity}; \textbf{ET}: A randomly selected user from \textit{Electricity-Theft}.
    \end{tablenotes}
    \label{tab:forecasting electricity}
\end{table}


\subsection{Experiment results on Electricity Theft Detection} \label{sec:Experiment results on ETD}

According to \eqref{eq:lstmdiff}, given an input $\bm x_{1:L}$ that can be either normal data or attack data, \textit{LSTMDiff} will generate $\bm{\hat{x}}_{1:L+T}$. Then we compute the anomaly scores based on reconstruction error \eqref{error_R} and forecasting error \eqref{eq:error_F_adjust} to identify whether the input is normal or not. When evaluating the performance for a given attack, we generate an attack sequence for each normal sequence of the test dataset. Since the number of normal samples and attack samples is the same, there is no need to use the precision-recall curve, which is developed for highly imbalanced test datasets. Instead, we utilize the receiver operating characteristic (ROC) curve as our evaluation metric, in which the y-axis denotes the true positive rate (\textit{TPR}) and the x-axis denotes the false positive rate (\textit{FPR}). 

The area under the ROC curve is the so-called AUC $\in[0,1]$. AUC equal to 1 means that the anomaly score can almost perfectly distinguish between positive and negative samples. AUC around 0.5 means that the anomaly scores of positive and negative samples follow a similar distribution and cannot be distinguished. AUC less than 0.5 means that the anomaly scores of positive samples are usually smaller than those of negative samples. For some general problems, AUC around 0 can be converted to AUC around 1 easily by exchanging the definitions of the positive sample and negative sample. However, the definitions of the positive samples and negative samples of the ETD problem cannot be exchanged according to the assumptions in Section \ref{problem statement}. Thus, an AUC around or less than 0.5 means that the method totally fails.
Now, we introduce our proposed ETD methods, i.e., \textit{LSTMDiff\text{-}R}, \textit{LSTMDiff\text{-}F}, and \textit{LSTMDiff\text{-}E}, and baseline methods, i.e., \textit{LSTM\text{-}R}, \textit{LSTM\text{-}F}, and \textit{FC\text{-}R}.  

In \textit{LSTMDiff\text{-}R}, we use \textit{LSTMDiff} introduced in Section \ref{sec:LSTMDiff} to generate the reconstruction of an input sequence. Then we calculate the reconstruction error \eqref{error_R} as anomaly score. Lastly, we identify input as normal or anomalous according to the anomaly score. 
In \textit{LSTMDiff\text{-}F}, we use \textit{LSTMDiff} to generate the forecasting sequence of an input sequence. Then, we calculate the forecasting error \eqref{eq:error_F_adjust} as anomaly score.
In \textit{LSTMDiff\text{-}E}, we ensemble the ETD results of \textit{LSTMDiff\text{-}R} and \textit{LSTMDiff\text{-}F}, i.e., we identify the input as an anomaly if one of the two methods considers it to be an anomaly. 
In \textit{LSTM\text{-}R}, we reimplement the model described in \cite{ref37} and add two FC layers as latent layers like \cite{ref10} to improve the performance. The model generates the reconstruction of an input sequence. we calculate the reconstruction error as the anomaly score.
In \textit{LSTM\text{-}F}, we reimplement the model described in \cite{ref38} to produce the forecasting sequence given an input sequence. We calculate the forecasting error as the anomaly score. 
In \textit{FC\text{-}R}, we reimplement the model described in \cite{ref9} to produce the reconstruction sequence given an input sequence. We calculate the reconstruction error as the anomaly score.

\subsubsection{\textbf{\textit{Electricity}}}
In this section, we evaluate our ETD methods on the datasets constructed with \textit{Electricity}. To better understand how normal users and dishonest users are distinguished, we illustrate two examples in Fig. \ref{fig:example-etd-electricity}. We employ a fixed reduction attack on the test dataset of user 3 to generate artificial data. Then, we use \textit{LSTMDiff} to produce reconstruction sequence, $\bm{\hat{x}}_{1:L}$, and forecasting sequence, $\bm{\hat{x}}_{L+1:L+T}$. In the upper left figure of Fig. \ref{fig:example-etd-electricity}, we calculate reconstruction error as anomaly score, and the blue bins present the anomaly scores of normal sequences, and the orange bins present the anomaly scores of artificial sequences. Similarly, we calculate forecasting errors as anomaly scores in the bottom left figure. From the figure, we can see that if the distance between the distributions of anomaly scores of normal and artificial sequences is large, we can easily distinguish them. Besides, a larger distance leads to a larger AUC. 

Subsequently, we employ all seven attacks mentioned in Section \ref{sec:attack methods} and generate artificial power consumption sequences for each user. Then, we evaluate the performance of our proposed scheme and baseline methods on different attacks in detail. From Table \ref{tab:AUC scores user 1}, we can see that REMs are better at detecting `FR', `PR', and `RPR', which can be explained by the fact that reconstruction models are sensitive to the input that is rarely met during training. On the contrary, FEMs perform better at `AC', `RAV', and `REV', as the forecasting error \eqref{eq:error_F_adjust} places more emphasis on the shape of forecasting curves. Similar phenomena can be observed from experiments of User 2 to User 4.
%Table \ref{tab:AUC scores user 2} to Table \ref{tab:AUC scores user 4} in Appendix \ref{app:a}. 
From the above experiments, we can also conclude that for REMs, \textit{LSTMDiff} shows better performance than the \textit{LSTM} model and \textit{FC} model. On the contrary, for FEMs, \textit{LSTM} model shows better performance. The average AUC scores of 278 users under different attacks are shown in Table \ref{tab:AUC scores 278 user}. The performance of LSTM-based methods drops a lot under `AC' and `RAC' attacks compared to the models trained for one user. We can attribute this to the fact that the \textit{LSTM} models produce reconstruction and forecasting sequences based on the values of the look-back window. On the contrary, LSTMDiff-based methods try to approximate the ground-truth PDF and do not show any performance decrease.

Although the average AUC scores are satisfactory in Table \ref{tab:AUC scores 278 user}, for a given user, a single method may be insufficient to detect some attacks like User 1 in Table \ref{tab:AUC scores user 1}. Thus, we need an ensemble model to improve the performance further. To show how an ensemble of the \textit{LSTMDiff}-based REM and FEM can improve the performance for ETD problem, we define another evaluation metric, $\alpha$\text{-}\textit{TPR}, that represents the true positive rate with a maximum false positive rate, $\alpha$. In our experiment, we set the maximum \textit{FPR}, $\alpha$, to be 5\% for each method. As shown in Table \ref{tab:AUC scores of single models and the ensemble model}, the actual \textit{FPR}s of \textit{LSTMDiff\text{-}F} and \textit{LSTMDiff\text{-}R} are both 4.9\%. On the other hand, to constrain the \textit{FPR} of the ensemble model within 5\%, we set maximum \textit{FPR} for both the REM and FEM to be 2.5\%, which leads to the overall \textit{FPR} of \textit{LSTMDiff\text{-}E} to be 4.5\%. From Table \ref{tab:AUC scores of single models and the ensemble model}, we can see that the ensemble model improves the performance by a large margin compared to a single REM and FEM. With \textit{FPR} 4.5\%, the ensemble model can detect almost all kinds of attacks satisfactorily.

% \begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
%     \caption{AUC Scores of Different ETD Methods on User 1}
%     \label{tab:AUC scores user 1}
%     \centering
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \hline
%         {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
%          \hline
%          {\textbf{L-R}\cite{ref37}} &1.00&1.00&1.00&1.00&0.00&0.30&0.44&0.68\\
%          \hline
%          {\textbf{L-F}\cite{ref38}} &0.88&0.88&0.97&0.95&0.85&1.00&1.00&\textbf{0.93}\\
%          \hline
%          {\textbf{FC-R}\cite{ref9}} &1.00&1.00&1.00&1.00&0.00&1.00&0.48&0.78\\
%          \hline
%          {\textbf{LD-R}} &0.99&0.98&0.98&1.00&0.13&0.45&0.78&0.76\\
%          \hline
%          {\textbf{LD-F}} &0.82&0.80&0.88&1.00&0.99&1.00&1.00&\textbf{0.93}\\
%          \hline
%     \end{tabular}
%     \begin{tablenotes}
%         \footnotesize
%         \item\textbf{FR}: Fixed reduction; \textbf{PR}: Partial reduction; \textbf{RPR}: Randomly partial reduction; \textbf{SBP}: Selective by-pass;
%         \textbf{AC}: Average consumption; \textbf{RAC}: Randomly average consumption; \textbf{REV}: Reverse; \textbf{Ave}: average.
%         \item\textbf{LD-R}: LSTMDiff-R; \textbf{LD-F}: LSTMDiff-F; \textbf{L-R}: LSTM-R; \textbf{L-F}: LSTM-F.
%     \end{tablenotes}
% \end{table}

\begin{table}[htbp]
\renewcommand\arraystretch{1.2}
    \caption{AUC Scores of Different ETD Methods on User 1-4}
    \label{tab:AUC scores user 1}
    \centering
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
    \multicolumn{9}{c}{\textbf{User 1}} \\
    \hline
        {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
         \hline
         {\textbf{L-R}\cite{ref37}} &1.00&1.00&1.00&1.00&0.00&0.30&0.44&0.68\\
         \hline
         {\textbf{L-F}\cite{ref38}} &0.88&0.88&0.97&0.95&0.85&1.00&1.00&\textbf{0.93}\\
         \hline
         {\textbf{FC-R}\cite{ref9}} &1.00&1.00&1.00&1.00&0.00&1.00&0.48&0.78\\
         \hline
         {\textbf{LD-R}} &0.99&0.98&0.98&1.00&0.13&0.45&0.78&0.76\\
         \hline
         {\textbf{LD-F}} &0.82&0.80&0.88&1.00&0.99&1.00&1.00&\textbf{0.93}\\

    \hline
    \multicolumn{9}{c}{\textbf{User 2}} \\
    \hline
    {\textbf{L-R}\cite{ref37}} &0.99&0.97&1.00&1.00&0.49&1.00&0.59&0.86\\
     \hline
     {\textbf{L-F}\cite{ref38}} &1.00&1.00&1.00&1.00&1.00&1.00&1.00&\textbf{1.00}\\
     \hline
     {\textbf{FC-R}\cite{ref9}} &0.98&098&0.98&1.00&0.00&0.99&0.51&0.78\\
     \hline
     {\textbf{LD-R}} &1.00&1.00&1.00&1.00&1.00&1.00&0.94&0.99\\
     \hline
    {\textbf{LD-F}} &0.91&0.99&1.00&1.00&1.00&1.00&1.00&0.99\\

    \hline
    \multicolumn{9}{c}{\textbf{User 3}} \\
    \hline
    {\textbf{L-R}\cite{ref37}} &0.89&0.40&0.54&1.00&0.30&0.00&0.59&0.53\\
     \hline
     {\textbf{L-F}\cite{ref38}} &0.92&0.73&0.94&1.00&1.00&1.00&1.00&0.94\\
     \hline
     {\textbf{FC-R}\cite{ref9}} &0.93&0.82&0.83&1.00&0.00&0.00&0.48&0.58\\
     \hline
     {\textbf{LD-R}} &0.98&0.95&0.96&1.00&0.97&0.93&0.99&\textbf{0.97}\\
     \hline
     {\textbf{LD-F}} &0.58&0.62&0.83&1.00&1.00&1.00&1.00&0.86\\

    \hline
    \multicolumn{9}{c}{\textbf{User 4}} \\
    \hline
     {\textbf{L-R}\cite{ref37}} &1.00&0.97&0.95&1.00&1.00&1.00&1.00&0.99\\
     \hline
     {\textbf{L-F}\cite{ref38}} &0.94&1.00&1.00&0.99&1.00&1.00&1.00&0.99\\
     \hline
     {\textbf{FC-R}\cite{ref9}} &0.60&0.00&0.00&1.00&0.00&0.00&0.55&0.31\\
     \hline
     {\textbf{LD-R}} &1.00&0.99&0.99&1.00&1.00&0.99&1.00&\textbf{1.00}\\
     \hline
     {\textbf{LD-F}} &0.72&1.00&1.00&1.00&1.00&1.00&1.00&0.96\\
         
         \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item\textbf{FR}: Fixed reduction; \textbf{PR}: Partial reduction; \textbf{RPR}: Random partial reduction; \textbf{SBP}: Selective by-pass;
        \textbf{AC}: Average consumption; \textbf{RAC}: Random average consumption; \textbf{REV}: Reverse; \textbf{Ave}: average.
        \item\textbf{LD-R}: LSTMDiff-R; \textbf{LD-F}: LSTMDiff-F; \textbf{L-R}: LSTM-R; \textbf{L-F}: LSTM-F.
    \end{tablenotes}
\end{table}

\begin{table}[htbp]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{Average AUC Scores among 278 Users}
    \label{tab:AUC scores 278 user}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
        {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
         \hline
         {\textbf{L-R}\cite{ref37}} &0.99&0.96&0.97&1.00&0.13&0.66&0.96&0.81\\
         \hline
         {\textbf{L-F}\cite{ref38}} &0.91&0.86&0.93&0.93&0.00&0.41&0.99&0.59\\
         \hline        
         {\textbf{LD-R}} &0.96&0.92&0.95&0.97&0.99&0.77&0.68&0.89\\
         \hline
         {\textbf{LD-F}} &0.87&0.93&0.96&0.98&0.98&0.99&0.99&\textbf{0.96}\\
         \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Abbreviations are the same as Table \ref{tab:AUC scores user 1}.
    \end{tablenotes}
\end{table}

\begin{table}[htbp]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{5\%-TPR of Different ETD Methods}
    \label{tab:AUC scores of single models and the ensemble model}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
        {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
         \hline
         {\textbf{LD-R}} &0.97 &0.94 &0.95 &1.00 &0.02 &0.02 &0.51 &0.63\\
         \hline
         {\textbf{LD-F}} &0.68 &0.65 &0.68 &1.00 &1.00 &1.00 &1.00 &0.86\\
         \hline
         {\textbf{LD-E}} &0.98 &0.98 &0.98 &1.00 &0.98 &0.99 &1.00 &\textbf{0.99}\\
         \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Abbreviations are the same as Table \ref{tab:AUC scores user 1}; \textbf{LD-E}: LSTMDiff-E.
    \end{tablenotes}
\end{table}
\begin{table}[htbp]
\renewcommand\arraystretch{1.2}
    \centering
    \caption{AUC Scores of Different Methods on \textit{Electricity-Theft}}
    \label{tab:AUC scores Synthetic Smart Grid Data}
    \begin{tabular}{c|c|c|c|c|c|c|c|c}
    \hline
        {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
         \hline
         {\textbf{L-R}\cite{ref37}} &0.55&0.36&0.40&0.59&0.73&0.33&0.59&0.51\\
         \hline
         {\textbf{L-F}\cite{ref38}} &0.56&0.23&0.20&0.49&0&0&0.72&0.31\\
         \hline
         {\textbf{FC-R}\cite{ref9}} &1.00&0.49&0.49&0.08&0.01&0.02&0.53&0.37\\
         \hline
         {\textbf{LD-R}} &1.00&1.00&1.00&1.00&1.00&1.00&1.00&\textbf{1.00}\\
         \hline
         {\textbf{LD-F}} &0.87&0.86&0.86&0.98&1.00&1.00&0.92&0.93\\
         \hline
    \end{tabular}
    \begin{tablenotes}
        \footnotesize
        \item Abbreviations are the same as Table \ref{tab:AUC scores user 1}.
    \end{tablenotes}
\end{table}

\subsubsection{\textbf{\textit{Electricity-Theft}}} \label{sec:Experiment results on Electricity-Theft}
In this section, we evaluate our proposed scheme on the dataset constructed with \textit{Electricity-Theft}. Similar to the previous dataset, we use \textit{LSTMDiff} to produce reconstruction sequence, $\bm{\hat{x}}_{1:L}$, and forecasting sequence, $\bm{\hat{x}}_{L+1:L+T}$. As shown in Fig. 5 in the Appendix, \textit{LSTM\text{-}F} cannot learn the relationship of the three variables and can only produce values around the mean value in the look-back window for the real power and current variables. As shown in Table \ref{tab:AUC scores Synthetic Smart Grid Data}, \textit{LSTM\text{-}R} and \textit{FC\text{-}R} do not work either. The reason for this is similar, \textit{LSTM} and \textit{FC} models cannot learn the pattern of high-variance data nor the relationship between the three variables. Interestingly, \textit{LSTMDiff}-based methods can work well on this ETD problem, though the performance of the \textit{LSTM\text{-}F} is affected by the low forecasting accuracy. The REM, \textit{LSTMDiff\text{-}R}, works perfectly, which indicates that \textit{LSTMDiff} can learn the behaviour pattern of this dataset and the relationship between the three variables. Since \textit{LSTMDiff\text{-}R} shows better or equal performance under all attacks compared to \textit{LSTMDiff\text{-}F}, there is no need to construct an ensemble method for this dataset.


\section{Conclusion} \label{sec:5} 
In this paper, we proposed an unsupervised learning model called \textit{LSTMDiff}," which combines LSTM and DDPM to generate reconstruction and forecasting sequences of the input data. By calculating the reconstruction error and forecasting error, we can detect various energy attacks. For datasets with predictable patterns, such as \textit{Electricity}, we observe that REMs and FEMs exhibit distinct performances under different attack scenarios. In general, these two types of methods are complementary to each other. To leverage their complementarity, we propose an ensemble method that combines reconstruction error and forecasting error, significantly improving the detection performance compared to individual REMs and FEMs. The ensemble method demonstrates effectiveness in detecting various types of attacks discussed in this paper. For datasets with high variances, such as \textit{Electricity-Theft}, the \textit{LSTMDiff}-based methods perform exceptionally well, while the baseline methods fail to yield satisfactory results. Furthermore, based on the observation of high forecasting error, the performance of FEM is relatively lower than that of REM. Consequently, there is no need to construct an ensemble model in this case. Overall, our study demonstrates the effectiveness of the proposed approach in addressing both ETD and ECF problems. The ensemble method shows improved performance by considering both reconstruction error and forecasting error, while the \textit{LSTMDiff}-based methods excel in high-variance datasets without requiring an ensemble model.







% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%




% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% % use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank... 


% % Can use something like this to put references on a page
% % by themselves when using endfloat and the captionsoff option.
% \ifCLASSOPTIONcaptionsoff
%   \newpage
% \fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

% \clearpage
% \clearpage
{\footnotesize
\bibliographystyle{IEEEtran}
\bibliography{reference}
}

% \clearpage
% \appendices
% \section{Supplementary Experimental Results} \label{app:a}
% %In this section, we provide supplementary figures and experimental results to gain a better understanding of this paper. 

% Figure \ref{fig:electricity data} and figure \ref{fig:electricity-theft data} show the characteristics of the two datasets, i.e., \textit{Electricity} and \textit{Electricity-Theft}, and how to split the datasets for training and test of our methods. Figure \ref{fig:forecasting E} shows the forecasting performance of \textit{LSTMDiff} and the \textit{LSTM} model on the 4 users of \textit{Electricity}.  Figure \ref{fig:forecasting ET} shows the forecasting performance of \textit{LSTMDiff} and the \textit{LSTM} model on the user of \textit{Electricity-Theft}. Figure \ref{fig:forecasting average consumption} shows how the \textit{LSTM} model forecasts under average consumption attack.

% Table \ref{tab:AUC scores user 2} to Table \ref{tab:AUC scores user 4} shows the performance of various methods on the ETD task of different users. 

% \begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
%     \centering
%     \caption{AUC Scores of Different ETD Methods on User 2}
%     \label{tab:AUC scores user 2}
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \hline
%         {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
%          \hline
%          {\textbf{LD-R}} &1.00&1.00&1.00&1.00&1.00&1.00&0.94&0.99\\
%          \hline
%         {\textbf{LD-F}} &0.91&0.99&1.00&1.00&1.00&1.00&1.00&0.99\\
%          \hline
%          {\textbf{L-R}\cite{ref37}} &0.99&0.97&1.00&1.00&0.49&1.00&0.59&0.86\\
%          \hline
%          {\textbf{L-F}\cite{ref38}} &1.00&1.00&1.00&1.00&1.00&1.00&1.00&1.00\\
%          \hline
%          {\textbf{FC-R}\cite{ref9}} &0.98&098&0.98&1.00&0.00&0.99&0.51&0.78\\
%          \hline
%     \end{tabular}
%     \begin{tablenotes}
%         \footnotesize
%         \item Abbreviations are the same as Table \ref{tab:AUC scores user 1};
%     \end{tablenotes}
% \end{table}

% \begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
%     \centering
%     \caption{AUC Scores of Different ETD Methods on User 3}
%     \label{tab:AUC scores user 3}
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \hline
%         {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
%          \hline
%          {\textbf{LD-R}} &0.98&0.95&0.96&1.00&0.97&0.93&0.99&0.97\\
%          \hline
%          {\textbf{LD-F}} &0.58&0.62&0.83&1.00&1.00&1.00&1.00&0.86\\
%          \hline
%          {\textbf{L-R}\cite{ref37}} &0.89&0.40&0.54&1.00&0.30&0.00&0.59&0.53\\
%          \hline
%          {\textbf{L-F}\cite{ref38}} &0.92&0.73&0.94&1.00&1.00&1.00&1.00&0.94\\
%          \hline
%          {\textbf{FC-R}\cite{ref9}} &0.93&0.82&0.83&1.00&0.00&0.00&0.48&0.58\\
%          \hline
%     \end{tabular}
%     \begin{tablenotes}
%         \footnotesize
%         \item Abbreviations are the same as Table \ref{tab:AUC scores user 1};
%     \end{tablenotes}
% \end{table}

% \begin{table}[htbp]
% \renewcommand\arraystretch{1.2}
%     \centering
%     \caption{AUC Scores of Different ETD Methods on User 4}
%     \label{tab:AUC scores user 4}
%     \begin{tabular}{c|c|c|c|c|c|c|c|c}
%     \hline
%         {} &\makebox[0.026\textwidth]{\textbf{FR}} & \makebox[0.026\textwidth]{\textbf{PR}} & \makebox[0.026\textwidth]{\textbf{RPR}} & \makebox[0.026\textwidth]{\textbf{SBP}} &\makebox[0.026\textwidth]{\textbf{AC}} & \makebox[0.026\textwidth]{\textbf{RAC}} & \makebox[0.026\textwidth]{\textbf{REV}} & \makebox[0.026\textwidth]{\textbf{Ave}}\\
%          \hline
%          {\textbf{LD-R}} &1.00&0.99&0.99&1.00&1.00&0.99&1.00&1.00\\
%          \hline
%          {\textbf{LD-F}} &0.72&1.00&1.00&1.00&1.00&1.00&1.00&0.96\\
%          \hline
%          {\textbf{L-R}\cite{ref37}} &1.00&0.97&0.95&1.00&1.00&1.00&1.00&0.99\\
%          \hline
%          {\textbf{L-F}\cite{ref38}} &0.94&1.00&1.00&0.99&1.00&1.00&1.00&0.99\\
%          \hline
%          {\textbf{FC-R}\cite{ref9}} &0.60&0.00&0.00&1.00&0.00&0.00&0.55&0.31\\
%          \hline
%     \end{tabular}
%     \begin{tablenotes}
%         \footnotesize
%         \item Abbreviations are the same as Table \ref{tab:AUC scores user 1};
%     \end{tablenotes}
% \end{table}


% % Figure environment removed

% % Figure environment removed


% % Figure environment removed

% % Figure environment removed

% % Figure environment removed

% that's all folks
\end{document}


