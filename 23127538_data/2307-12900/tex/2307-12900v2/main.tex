\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{xfrac}
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{soul}
\usepackage{threeparttable} 
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\sisetup{detect-weight,detect-mode}

\begin{document}

\title{Automotive Object Detection via Learning Sparse Events by Spiking Neurons}

\author{Hu Zhang, Yanchen Li, Luziwei Leng, Kaiwei Che, Qian Liu, Qinghai Guo, Jianxing Liao, and Ran Cheng %,~\IEEEmembership{Senior Member, ~IEEE}
        % <-this % stops a space

        \thanks{Hu Zhang, Yanchen Li, and Ran Cheng are with the Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen 518055, China. Kaiwei Che is with the Department of Electrical and Electronic Engineering, Southern University of Science and Technology, Shenzhen 518055, China.
                (e-mail: ranchengcn@gmail.com)
                }
        \thanks{Luziwei Leng, Qian Liu, Qinghai Guo and Jianxing Liao are with the Advanced Computing and Storage Lab, Huawei Technologies Co., Ltd., Shenzhen 518055, China. (e-mail: lengluziwei@huawei.com) (\emph{Corresponding authors: Luziwei Leng and Ran Cheng.})}

}


% The paper headers
%\markboth{Journal of \LaTeX\ Class Files,~Vol.~14, No.~8, August~2021}%
%{Shell \MakeLowercase{\textit{et al.}}: A Sample Article Using IEEEtran.cls for IEEE Journals}

%\IEEEpubid{\textcolor{red}{TOFIX:~}0000--0000/00\$00.00~\copyright~2023 IEEE}
% Remember, if you use this you must call \IEEEpubidadjcol in the second
% column for its text to clear the IEEEpubid mark.



\maketitle

\begin{abstract}
Event-based sensors, distinguished by their high temporal resolution of \SI{1}{\micro\second} and a dynamic range of \SI{120}{\decibel}, stand out as ideal tools for deployment in fast-paced settings like vehicles and drones. 
Traditional object detection techniques that utilize Artificial Neural Networks (ANNs) face challenges due to the sparse and asynchronous nature of the events these sensors capture. 
In contrast, Spiking Neural Networks (SNNs) offer a promising alternative, providing a temporal representation that is inherently aligned with event-based data. 
This paper explores the unique membrane potential dynamics of SNNs and their ability to modulate sparse events.
We introduce an innovative spike-triggered adaptive threshold mechanism designed for stable training. Building on these insights, we present a specialized spiking feature pyramid network (SpikeFPN) optimized for automotive event-based object detection. 
Comprehensive evaluations demonstrate that SpikeFPN surpasses both traditional SNNs and advanced ANNs enhanced with attention mechanisms. 
Evidently, SpikeFPN achieves a mean Average Precision (mAP) of 0.477 on the {GEN1 Automotive Detection (GAD)} benchmark dataset, marking a significant increase of 9.7\% over the previous best SNN. 
Moreover, the efficient design of SpikeFPN ensures robust performance while optimizing computational resources, attributed to its innate sparse computation capabilities.
\end{abstract}

\begin{IEEEkeywords}
Autonomous driving, object detection, deep learning, spiking neural networks.
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{O}{bject} detection is fundamental in computer vision, with applications ranging from face recognition to vehicle tracking and the identification of smaller objects~\cite{Liu2020-fm, zou2023object}. Traditionally, such tasks rely on frame-based cameras. However, these often produce images that are blurred or poorly exposed, especially in high-speed or challenging lighting conditions, common in scenarios like emergency vehicle detection.
Addressing this gap, the recently advanced Dynamical Vision Sensor (DVS), a.k.a. the event-based sensor~\cite{lichtsteiner2008128, posch2010qvga, chen201164, serrano2013128, brandli2014240, son20174}, offers a compelling alternative. 
Unlike traditional sensors, the DVS draws inspiration from retinal functionalities and is adept at registering pixel intensity variations. 
With its remarkable temporal resolution of \SI{1}{\micro\second} and a dynamic range of \SI{120}{\decibel}, it is able to effectively record asynchronous events instigated by shifts in pixel brightness.

Despite the appealing characteristics, the unique nature of DVS also brings challenges. Its event-driven data is sparse and unpredictable, making traditional object detection methods based on Artificial Neural Networks (ANNs) less effective.
While there have been efforts to preprocess such events using various architectures and algorithms~\cite{perot2020learning, li2022asynchronous}, they often come with substantial computational and latency costs.
By contrast, the Spiking Neural Networks (SNNs)~\cite{doi:10.1142/S0129065709002002, Roy2019-dp} introduce a new paradigm, resembling the brain's functioning by transmitting information through discrete spikes. 
Their inherent efficiency, marked by sparse activations and multiplication-free inferences, makes them ideal for managing the dynamic, sparse data from event-driven sensors.

Recent breakthroughs in surrogate gradient (SG) algorithms \cite{zenke2018superspike, neftci2019surrogate} have paved the way for efficient training of deep SNNs in image classification domains \cite{wu2019direct, rathi2021diet, zheng2021going, li2021differentiable, deng2022temporal}. However, when it comes to more intricate vision tasks like automotive object detection~\cite{cordone2022object, perez2019deep, ulrich2022improved}, SNNs still find themselves trailing behind the established prowess of ANNs. Though significant strides have been made in enhancing ANNs for challenging object detection tasks~\cite{chen2022salient, Brazil_2019_ICCV, Gong_2021_WACV, liang2022anchor, chen2021depth}, directly transposing these refined ANN architectures onto the SNN framework often leads to compromised performance and heightened latency. Such performance deficits are glaring in tasks that necessitate sophisticated neural network adjustments \cite{hagenaars2021self, kim2020spiking, cordone2022object}. Moreover, while current efforts focus on converting ANNs to SNNs \cite{bu2021optimal, li2021free, ding2021optimal, rueckauer2017conversion, diehl2015fast}, these approaches grapple with inherent challenges. These primarily arise when integrating the dynamic nature of SNNs with the foundational attributes of ANNs, which frequently results in the underutilization of the unique temporal and sparse characteristics inherent to SNNs — essential for handling event-driven data efficiently. The inherent differences in data representation and processing between the two neural architectures often result in deviations, undermining both the accuracy and computational performance of the neural system.


In response to these challenges, we delve deeper into the temporal dynamics inherent to spiking neurons, aiming to develop an approach characterized by minimal computational requirements and efficient response times, especially crucial for swiftly varying events. Leveraging our findings, we design a tailored {spiking feature pyramid network (SpikeFPN)} for event-based automotive object detection, which harmoniously blends the unique neuronal properties with surrogate gradient training to achieve optimal performance.
Our primary contributions are delineated as follows:
\begin{enumerate}
    \item We have delved into the inherent temporal dynamics of spiking neurons, particularly focusing on membrane potential dynamics and adaptive thresholds. Our comprehensive study not only reveals their intrinsic strengths in managing rapid event modulations but also underscores their pivotal role in amplifying the robustness of features derived from sparse events. The insights gained ensure stable and efficient training phases, optimizing the network's performance in real-world scenarios.

    \item We have designed an event-driven {SpikeFPN} tailored specifically for automotive object detection, leveraging the self-adaptive mechanism inspired by neocortex neuron adaptation.
    This design foundation captures the unique temporal dynamics of spiking neurons, presenting a novel approach to object detection in automotive scenarios. Moreover, building upon the core principles of our {SpikeFPN}, we have adopted a surrogate gradient training to optimize the encoder's structure. 
   

    \item Our proposed {SpikeFPN} has demonstrated promising performance, surpassing previous SNN models and advanced ANNs with attention mechanisms. It attains a noteworthy {mean Average Precision (mAP) of 0.477} on the {GEN1 Automotive Detection (GAD)} benchmark dataset, marking a substantial advancement by outperforming the previous best SNN by 9.7\%. Additionally, our design is a testament to efficiency—combining a streamlined architecture   with impressive accuracy, all while significantly reducing computation costs.
\end{enumerate}

The structure of this paper is outlined as follows: Section~II provides the foundational background relevant to our study; Section~III delves into the intricacies of the proposed SpikeFPN, detailing its architectural components and elaborating on the neural behavior and adaptive strategies employed; Section~IV showcases our experimental approaches, the methodologies adopted, and the corresponding results, affirming the effectiveness of our design; finally, the paper culminates with conclusions drawn in Section~V.



%Recent advancements in surrogate gradient (SG) algorithms \cite{zenke2018superspike, neftci2019surrogate} have led to the successful training of deep SNNs for image classification tasks~\cite{wu2019direct, rathi2021diet, zheng2021going, li2021differentiable, deng2022temporal}. However, the difference between spiking neurons and real-valued artificial neurons in information representation still results in significant performance drops or increased latency when training SNNs for more challenging tasks, such as dense image prediction or object detection \cite{hagenaars2021self, kim2020spiking, cordone2022object}, especially when their architectures are directly taken from complicated ANNs.

%\textcolor{blue}{TODO - depict the insufficiency of current ANN / SNN on vision tasks and the method to be proposed}

%\textcolor{blue}{TODO - add limition of current spiking FPN which transformed from ANN paradigm. }

%\textcolor{blue}{TODO - to this end, a SNN-suited FPN is proposed, which provides the property of ...}




\section{Background}
%This section introduces the intricacies of event-based object detection, elucidates the principles of deep SNNs for vision tasks, and navigates the architectural marvel of the Feature Pyramid Network (FPN). 

\subsection{Event-based Object Detection}
Event cameras, possessing outstanding temporal resolution and dynamic range, are optimally suited for scenarios demanding rapid object tracking, variable lighting conditions, and minimal latency. 
Nonetheless, the intrinsic dynamism of event camera data clashes with conventional deep learning paradigms grounded in frame-based methodologies. 
Typically, to counter this incongruence, events undergo preprocessing to transform into denser representations before network processing. 
Various methodologies, both handcrafted and automated, have been championed for this transformation, encompassing event frames \cite{lagorce2016hots, maqueda2018event, moeys2016steering, ye1809unsupervised, zhu2018ev, nguyen2019real}, time or event-number stacking \cite{wang2019event}, voxel grids \cite{zhu2019unsupervised}, the event queue approach \cite{tulyakov2019learning}, LSTM grids \cite{cannici2020differentiable}, and discrete time convolutions \cite{zhang2022discrete}. 
For sparse event data, asynchronous convolutions have also been broached \cite{scheerlinck2019asynchronous, messikommer2020event}, with the latter notably applied to the {GEN1 Automotive Detection (GAD) dataset acquired with the GEN1 sensor for event-driven automotive object detection tasks}\cite{de2020large}. 
The work by \cite{perot2020learning} accentuated a convolutional LSTM network coupled with a temporal consistency loss, amplifying training outcomes. 
ASTMNet \cite{li2022asynchronous} showcased an innovative adaptive sampling protocol, fusing temporal attention and memory modules, and clocked unparalleled accuracy levels.



\subsection{Deep SNNs for Vision Tasks}
Neuromorphic hardware evolution has invigorated the enthusiasm for harnessing SNNs in challenging vision-based deep learning tasks \cite{merolla2014million, furber2014spinnaker, leng2014deep, leng2016spiking, leng2018spiking, kungl2019accelerated, leng2020solving, davies2021advancing}.
When amalgamated with event cameras, these neuromorphic ecosystems promise unprecedented power efficiency coupled with minimal latency \cite{roy2019towards}. Training deep SNNs revolves predominantly around two pivotal methodologies: the conversion from ANNs to SNNs and direct training.

The conversion strategy revolves around approximating real-valued activations with spiking dynamics \cite{rueckauer2017conversion, sengupta2019going}. 
While this strategy manifests high accuracy levels, it is weighed down by heightened latency owing to spike rate accumulations, rendering it less effective for rapid event processing. 
\cite{kim2020spiking} introduced a spiking version of YOLO for object detection and achieved a virtually identical performance on the PASCAL VOC \cite{everingham2010pascal} and MS COCO datasets \cite{lin2014microsoft}, albeit demanding thousands of timesteps for convergence. Recent endeavors have managed to curb the latency post conversion \cite{bu2021optimal, li2021free}, yet the scalability of these methodologies to more intricate architectures beyond mere image classification remains nebulous.

Direct training, on the other hand, leverages surrogate gradient (SG) functions to mimic backpropagation gradients \cite{zenke2018superspike, shrestha2018slayer, neftci2019surrogate}.
Owing to advancements like tailored normalization, SG, and loss function crafting \cite{wu2019direct, zhang2020temporal, li2021differentiable, deng2022temporal}, directly trained SNNs have been attaining competitive accuracies on strenuous benchmark tasks like ImageNet, demanding minimal simulation steps for convergence.
These milestones have catalyzed their expansion into other event-centric vision tasks, from optical flow assessment \cite{hagenaars2021self} to video reproduction \cite{zhu2022event} and object recognition \cite{cordone2022object}.
Yet, many of these endeavors remain encumbered by rather simplistic handcrafted architectures and often don't match the accuracy prowess of elite ANNs.
Recent breakthroughs, for instance, those by \cite{che2022differentiable, zhang2023accurate, li2023efficient}, underscore that spike-centric differentiable hierarchical searches can considerably elevate SNN performance across a spectrum of event-driven vision tasks, including arduous ones like deep stereo and semantic segmentation.

%\subsection{\textcolor{blue}{Adaptive Threshold Neuron}}
%\textcolor{blue}{TODO - supplement relevant content}

\subsection{Feature Pyramid Networks}

Feature Pyramid Networks (FPNs) are central components of numerous object detection systems, with their capacity to exploit multi-scale feature information enhancing overall system performance \cite{he2015spatial, lin2017feature, 8627998}. 
The original FPN design was introduced by \cite{Lin_2017_CVPR}, which built on traditional pyramid methods by efficiently leveraging single-scale image input, forming a foundation for subsequent advancements in the field.

Among these advancements, the FaPN by \cite{Huang2021FaPN} incorporated a feature alignment module, achieving a state-of-the-art refinement over the conventional FPN design. 
In contrast to the fusion-based approach, the SSD method, proposed by \cite{Liu_2016}, delegated different stages of feature maps to detect objects of distinct scales. 
Several integrated techniques, such as those found in \cite{ren2016faster, he2018mask, cai2017cascade, lin2018focal, redmon2018yolov3}, fused features from diverse stages, predominantly using a top-down unidirectional fusion method—a prevailing FPN fusion mode in modern object detection models.

The PANet \cite{liu2018path} made strides by pioneering the concept of bottom-up secondary bi-directional fusion, shedding light on the potential benefits of bi-directional fusion. 
Further expanding on the capabilities of FPNs, the ASFF \cite{liu2019learning} incorporated an attention mechanism, while complex bidirectional fusion techniques, such as those in \cite{ghiasi2019nasfpn, tan2020efficientdet}, ventured into more intricate fusion dynamics. 
A novel approach was taken by the Recursive-FPN \cite{qiao2020detectors}, wherein the fused output of the FPN was reintegrated into the backbone, initiating an additional loop and marking a new state-of-the-art. 
Further bridging the gap between conventional neural networks and the emerging field of SNNs, recent work \cite{e24111543} endeavored to combine feature pyramid structures with SNNs.

%On a broader scale, the choice of the encoder network backbone is vital to a detector's overall efficacy. While such encoders form a significant chunk of the network's parameters, their feature extraction prowess is indispensable. A popular choice in this context has been the ResNet \cite{he2016deep} and its derivatives, owing to their proficient feature extraction capabilities. In the domain of SNNS, adaptations of ResNets for spiking mechanisms, as seen in \cite{sengupta2019going, li2021differentiable, deng2022temporal}, have gained traction for image classification tasks. However, an intriguing alternative architecture was recently introduced by \cite{che2022differentiable}, which organized spiking activities into hierarchical directed acyclic graphs, pushing the boundaries of accuracy within deep SNN frameworks.

% \textcolor{blue}{TODO - add additional knowledge, the following is a temporary list for the purpose of collecting FPN-related researchs:
% \begin{itemize}
%     \item FPN\cite{Lin_2017_CVPR}. The original feature pyramid network.
%     \item FaPN\cite{Huang2021FaPN}. A FPN with feature alignment module, SoTA.
%     \item SSD\cite{Liu_2016}. No fusion, different stages of feature maps are responsible for the detection of objects of different scales.
%     \item Faster R-CNN\cite{ren2016faster}, Mask R-CNN\cite{he2018mask}, Cascade R-CNN\cite{cai2017cascade}, RetinaNet\cite{lin2018focal}, YOLOv3\cite{redmon2018yolov3}. Fusion of features from five stages, the commonality is top-down unidirectional fusion method, which is one of the mainstream fusion modes of FPN in current object detection models.
%     \item PANet\cite{liu2018path}. The first model proposed for bottom-up secondary bi-directional fusion, which demonstrates the effectiveness of bi-directional fusion.
%     \item ASFF\cite{liu2019learning}. A FPN with attention mechanism.
%     \item NAS-FPN\cite{ghiasi2019nasfpn}, BiFPN\cite{tan2020efficientdet}. Complex Bidirectional Fusion FPN.
%     \item Recursive-FPN\cite{qiao2020detectors}. The fused output of the FPN was fed back into the backbone for a secondary loop, SoTA.
%     \item Combination of feature pyramid structure and spiking neural network \cite{e24111543}.
% \end{itemize}
% }

% \textcolor{blue}{
% Feature pyramids have been widely used in many object detection systems to provide multi-scale feature information \cite{he2015spatial, lin2017feature, 8627998}.}

% \textcolor{blue}{
% The encoder network typically takes up the largest portion of parameters in a detector and its feature extraction capability is critical to the network's performance. ResNet \cite{he2016deep} and its derivatives are commonly used as backbones in detection networks. In the field of SNNs, spiking ResNets \cite{sengupta2019going, li2021differentiable, deng2022temporal} are currently the norm for image classification. Recently, an alternative architecture was proposed by \cite{che2022differentiable} that achieves higher accuracy in deep SNNs by organizing spiking activities as hierarchical directed acyclic graphs. }


\section{Method}

This section begins with the representation of event data. Subsequently, it introduces the proposed feature-pyramid-centric spiking neural network underpinned by a self-adaptive spiking neuron model. 
This model effectively leverages temporal dynamics, ensuring compatibility with the highly variable event information flow. 
To clarify the network's design, the foundational architecture is elucidated, highlighting modules such as the primary backbone.
The section further presents the formulation of the adaptive spiking neural propagation model intrinsic to the network. 
Ultimately, the strategy for employing approximate information to address the non-differentiability challenge during training is elaborated upon.

\subsection{Event Data Representation}

Traditional cameras capture visual information in the form of images. In contrast, a DVS detects individual pixel changes in their receptive fields, registering changes in luminance and labeling these as ``events''. Typically, an event is recorded in a tuple format \((t,x,y,p)\), where \(t\) represents the timestamp of the event occurrence; \(x,y\) denote the two-dimensional pixel coordinates where the event is registered; and \(p\) indicates the polarity of the event, reflecting the direction of luminance change.

In order to effectively encode the inherently discrete event data while retaining its temporal nuances, we employ the Stacking Based on Time (SBT) method for event preprocessing as presented by Wang {\emph{et al.}} \cite{wang2019event}. The SBT approach, recognized for its real-time processing and low computational overhead, has been integrated into the accumulator modules of mainstream event sensors. Beyond mere data compression, the SBT preserves the granular temporal information of the event stream. It amalgamates events into temporally contiguous frames, facilitating the downstream Spiking Neural Network (SNN) to learn from this rich temporal dataset. Within a single stack input into the detection model, the SBT method partitions the event data uniformly based on the time interval $\Delta t$. It then compresses this data to yield a set of frames. Assuming the event data in the stack can be subdivided into $n$ equal time intervals, the value of each pixel in the $i$-th frame is characterized by the aggregated polarity of events as:
\begin{equation}
    P(x,y) = \mathrm{sign}\Biggr(\sum_{t\in T} p(x,y,t)\Biggr),
    \label{eq:sbt}
\end{equation}
where \(P\) designates the pixel value at coordinates \((x,y)\); \(t\) represents the timestamp; \(p\) denotes the event's polarity; and {\(T \in\ \bigr[\sfrac{(i-1)\Delta t}{n},\sfrac{i\Delta t}{n}\bigr]\)} specifies the timeframe of events consolidated into a single frame.

While the SBT approach offers substantial utility, alternative event data representation techniques exist, some of which closely resemble our encoding method. Although not integrated into our model, these techniques occasionally offer superior performance in specific contexts. For instance, in scenarios characterized by infrequent events during the consolidation interval, the SBT might yield a frame with notably sparse values. In such cases, the Stacking Based on the number of Events (SBE) method, as introduced by Wang \emph{et al.} \cite{wang2019event}, emerges as a more apt choice. The SBE methodology builds a frame using a predetermined number of events, providing an effective solution to the challenge of sparse event representation. To comprehensively evaluate the influence of various event encoding strategies on object detection tasks, we have compared multiple encoding modalities in the ablation studies detailed in Section \ref{subsec:abs}.

\subsection{Network Design}

In response to the unique challenges of event-based object detection, we design a feature-pyramid-centric spiking neural network built upon a threshold-adaptive spiking unit mechanism. The foundational neuron unit, characterized by self-adaptive and spiking attributes, seamlessly adapts to intricate scenarios. Coupled with the network's feature pyramid module, it provides a robust architecture. A multi-head prediction mechanism layered on top further enhances detection accuracy. This section delves into a detailed discussion of the network's overall structure, encapsulating the design intricacies of both the backbone and the feature pyramid module. Subsequently, we illustrate the behavior and propagation mechanisms of the spiking neuron unit incorporated within our approach.

% Figure environment removed

\subsubsection{Network Architecture}

The network adopts a feature pyramid architecture which emanates from a downsampling paradigm, employing foundational cell units to sculpt the encoding backbone. 
Spiking feature maps, derived from the concluding three stages of the backbone, amalgamate to create the spiking feature pyramid—a focal element of the overarching network design as depicted in Fig.~\ref{fig:fig2}.
This component plays a pivotal role in feature extraction. A comprehensive visualization of the backbone's output feature map, in conjunction with the entire network, can be found in Table \ref{tab:SpikeFPN}.
The inaugural two spiking stem layers of our architecture facilitate a twofold downsampling of the feature map. This is succeeded by 10 spiking cells, which together forge a four-stage downsampling lattice. 
Outputs from the terminal cells across stages 2, 3, and 4 are relayed to the feature pyramid. 
The aggregate network achieves a peak downsampling ratio of 32.
In the following, we further elucidate the intricate design elements of the backbone, complemented by the subsequent spiking feature pyramid network.

\begin{table}[htbp]
\caption{Detailed architecture settings of the proposed spiking feature pyramid network. The illustration encompasses the encoder backbone, the feature pyramid, and the multi-head prediction module. Here, \(C\) represents the number of classes, while \(K\) signifies the number of anchors.}
\centering
    \begin{tabular}{|c|c|c|}\hline
    \textbf{Module}&\textbf{Layer}&\makecell[c]{\textbf{Output Feature Map}\\\textbf{\(c \times h \times w\)}}\\
    \hline
    \multirow{12}{*}{\makecell[c]{\textbf{Backbone}}}
    &Stem 0 & \(48 \times 128\times 128\) \\
    &Stem 1 & \(96 \times 64\times 64\) \\
    &Cell 0 & \(96 \times 64\times 64\) \\
    &Cell 1 & \(96 \times 64\times 64\) \\
    &Cell 2 & \(192 \times 32\times 32\) \\
    &Cell 3 & \(192 \times 32\times 32\) \\
    &Cell 4 & \(192 \times 32\times 32\) \\
    &Cell 5 & \(384 \times 16\times 16\) \\
    &Cell 6 & \(384 \times 16\times 16\) \\
    &Cell 7 & \(384 \times 16\times 16\) \\
    &Cell 8 & \(768 \times 8\times 8\) \\
    &Cell 9 & \(768 \times 8\times 8\) \\
    \hline
    \multirow{3}{*}{\textbf{Feature Pyramid}}
    &Cell 4 \(\rightarrow\) p1 & \(96\times32\times 32\) \\
    &Cell 7 \(\rightarrow\) p2 & \(192\times16\times 16\) \\
    &Cell 9 \(\rightarrow\) p3 & \(384\times8\times 8\) \\
    \hline
    \multirow{3}{*}{\textbf{Multi-Head Prediction}}
    &p1 \(\rightarrow\) d1 & \(K\times(C+5)\times32\times 32\) \\
    &p2 \(\rightarrow\) d2 & \(K\times(C+5)\times16\times 16\) \\
    &p3 \(\rightarrow\) d3 & \(K\times(C+5)\times8\times 8\) \\
    \hline
    \end{tabular}
\label{tab:SpikeFPN}
\end{table}

As depicted in Fig.~\ref{fig:fig1}, the encoder backbone adopts a multi-stage downsampling strategy. This architecture consists of an initial two spiking stem layers, followed by a sequence of ten spiking cells. 
Through the downsampling process, the spatial resolution of the feature map is reduced by half, while the channel size experiences a twofold increase. 
Each spiking stem layer comprises a convolutional layer, batch normalization (which can be integrated into convolution weights during test inference, as suggested by \cite{ioffe2015batch}), and a spiking activation function. 
These layers are meticulously crafted to spearhead initial feature extraction and channel modulation. 
Each spiking cell is structured around three spiking nodes, with this arrangement being reiterated across multiple layers. 
The initial pair of nodes procure inputs from the preceding two cells, while the third node assimilates input from its immediate two predecessors. 
The feature maps across all nodes maintain a consistent size. Their respective outputs converge, forming the collective output of the cell. 
Within each node, operations stemming from various edges aggregate at the membrane potential level before undergoing the spiking activation function. 
The interconnections within the spiking cell are heuristically determined to harmonize inference speed with accuracy.

% Figure environment removed

Post the processing through the backbone, fundamental features are discerned. 
However, subsequent operations are requisite to achieve the anticipated results. The lower spatial resolution features from the pyramid undergo upsampling via the nearest interpolation method, which doubles both width and height dimensions, ensuring the maintenance of a binary feature map. 
These are subsequently concatenated with feature maps (of congruent spatial dimensions) originating from the backbone.
Within the network's holistic architecture, the feature pyramid module enhances the features, synergizing the backbone components, thereby fostering superior adaptability for intricate tasks at hand. 
These enhanced features navigate through a \(1\times1\) convolution, followed by batch normalization and a spike activation function, thereby constituting the succeeding level of the feature pyramid. 
spiking feature pyramid then interfaces with a multi-head prediction module through convolutional layers, batch normalization, and spike activation. The resultant features undergo a \(1\times1\) convolution, yielding floating-point outcomes with a shape of \(N\times N \times[3\times (C+5)]\), wherein \(N\) symbolizes the width and height of the pertinent feature map, \(C\) represents the number of classes, and the number 5 corresponds to four bounding box coordinate predictions complemented by a singular confidence prediction. 
The method of non-maximum suppression (NMS) \cite{redmon2017yolo9000,girshick2014rich} is employed to finalize the bounding boxes.

\subsubsection{Neural Behavior and Adaptation}

In the realm of SNNs, the choice of the basic unit emerges as a pivotal aspect in network design. 
In fast-paced, event-based contexts, the adaptability to the conveyed information is our core concern. 
As such, we adopted an adaptively tunable spiking neuron model. 
To amplify its capacity to encapsulate information, we incorporated the iterative Leaky Integrate and Fire (LIF) model \cite{wu2019direct} as the foundational unit. This unit was then augmented with an agile threshold-adaptive mechanism, enhancing its representational capabilities for event data, as discussed below.

Leveraging its inherent temporal dynamics and sparse activation, the LIF neuron model's propagation pattern can be articulated as:
\begin{subequations}
\begin{align}
    u_i^{(t,n)} &= \tau\cdot u_i^{(t-1,n)}\cdot\bigr(1-y_i^{(t-1,n)}\bigr) + I_i^{(t,n)}, \\[1.5ex]
    I_i^{(t,n)} &= \sum_jw_{ji}\cdot y_j^{(t, n-1)}, 
\end{align}
\label{eq1:LIF}
\end{subequations}
with the spike generation process is formulated with:
\begin{equation}
    y_i^{(t,n)} = H\bigr(u_i^{(t,n)}-U_{th}\bigr) =
    \Biggr\{
    \begin{aligned}
    1,~&\text{if}\ u_i^{(t,n)} \geq U_{th} \\\\[-1ex]
    0,~&\text{otherwise.}
    \end{aligned}
\label{eq2:spike}
\end{equation}
where the \(u_i^{(t,n)}\) represents the membrane potential of the neuron \(i\) of the \(n\)-th layer at time \(t\), \(\tau\) is a constant which represents the membrane time attenuation factor consequently configures the membrane leakage of the neuron, \(I_i^{(t,n)}\) is the influx currency, which is essentially the weighted sum of the spiking activation \(y_j^{(t, n-1)}\) generated from the connected neurons from the previous layer \(n-1\). The spiking activation \(y_i^{(t,n)}\) is defined by a Heaviside function \(H(\cdot)\) which is calculated to be \(1\) only when \(u_i^{(t,n)}\) reaches a membrane threshold \(U_{th}\), otherwise remains \(0\).


Drawing inspiration from the adaptive behaviors of neurons in the neocortex \cite{gerstner2014neuronal, gouwens2018systematic}, our design incorporates the self-adaptive mechanism into the LIF spiking neuron. This adaptive threshold mechanism, which has been previously utilized in recurrent SNNs for temporal sequence learning \cite{bellec2018long, bellec2020solution}, introduces the capacity for longer memory retention. Notably, this mechanism has proven its adaptability in several experimental settings.
Specifcially, the adaptive threshold mechanism's propagation pattern can be described as:
\begin{subequations}\label{eq2}
\begin{align}
    y^{(t)} &= H\bigr(u^{(t)}-A_{\mathrm{th}}^{(t)}\bigr), \\[1ex]
    A_{\mathrm{th}}^{(t)} &= U_{\mathrm{th}}+\beta\cdot a^{(t)}, \\[1ex]
    a^{(t)} &= \tau_{a}\cdot a^{(t-1)}+y^{(t-1)},
\end{align}
\end{subequations}
where \(H(\cdot)\) is the Heaviside step function. \(A_{\mathrm{th}}^{(t)}\) signifies the modifiable threshold at time \(t\). Meanwhile, \(a^{(t)}\) represents the evolving threshold increment, which is modulated by the neuron's spiking history. Here, \(\beta\) is a scaling coefficient, and \(\tau_{a}\) is the time constant for \(a\). 

Crucially, the adjustable threshold \(A_{\mathrm{th}}\) dynamically influences the spiking rate. When faced with intensive input, this threshold rises, thereby inhibiting the neuron's firing propensity. 
On the other hand, sparse input leads to a decrease in this threshold, making the neuron more prone to firing. 
By examining the limits of this adaptive mechanism under various input scenarios, we can discern its bounds. 
Under conditions where the neuron remains consistently inactive, as \(t\) approaches infinity, \(a^{(t)}\) converges to 0. This implies that the lowest bound for \(A_{\mathrm{th}}^{(t)}\) is the base threshold \(U_{\mathrm{th}}\). However, in a contrasting scenario where the neuron is incessantly active (with initial conditions \(y^{(0)} = 1\) and \(a^{(0)} = 0\)), the evolution of \(a^{(t)}\) can be portrayed as:
\begin{equation}
a^{(t)} = \sum_{i=1}^{t} \tau_{a}^{(i-1)},
\label{eq:eqac}
\end{equation}
and its upper limit, as \(t\) grows indefinitely, is determined to be \(\sfrac{1}{(1-\tau_{a})}\). Hence, the adaptive threshold \(A_{\mathrm{th}}\) spans from \(U_{\mathrm{th}}\) to \(U_{\mathrm{th}}+\sfrac{\beta}{(1-\tau_{a})}\). 
This event-driven adaptivity augments the network's resilience and stability, especially in the face of swift alterations in event density. 

\subsection{Network Training}
To capitalize on the temporal dynamics of spiking neurons, the initial obstacle is the intricacy of training SNNs. This challenge stems from the discontinuous nature of spike generation, such that the absence of a gradient renders backpropagation unfeasible.

Early strides in the application of deep SNNs to machine learning tasks were made by transferring parameters from ANN-trained models to corresponding SNNs \cite{diehl2015fast,rueckauer2017conversion}. 
However, this method is confined to feed-forward architectures devoid of recurrency, making it suitable only for tasks with limited temporal dependencies. To overcome this limitation, we embrace surrogate gradient training. 
This method approximates the discontinuous activation by smoothening the spike generation function, as denoted in Eq.~(\ref{eq2:spike}). 
This approximation permits gradient computation for backpropagation. During this training phase, temporal neural dynamics, as detailed in Eq.~(\ref{eq1:LIF}), are meticulously modeled, and the resulting errors are backpropagated through time.
Consequently, surrogate gradient approximations not only facilitate the training of spiking neurons while considering their temporal intricacies but are also inherently apt for tasks with pronounced temporal dependencies, like the continuous object detection challenge we address. 
As a stand-in for the gradient of the spiking function, we utilize the Dispike surrogate function \cite{li2021differentiable}, given by:
\begin{subequations}\label{eq3:Dspike}
\begin{align}
\mathrm{Dspike}(u) &= a\cdot\tanh\bigr(b\cdot(u-c)\bigr) + d,&0 \leq u \leq 1, \\[1ex]
\tanh(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}},
\end{align}
\end{subequations}
where \(u\) denotes the membrane potential. The parameters \((a, b, c, d)\) modify the intrinsic hyperbolic tangent function, \(\tanh\), ensuring its output remains bounded within \([0, 1]\), while varying in shape. With a plethora of surrogate functions at our disposal, catering to diverse training needs becomes feasible. The parameter \(b\), termed the temperature factor, regulates the function's smoothness. Typically, \(c\) is fixed at \(0.5\), ensuring symmetrical behavior around \(u=0.5\). For our chosen variant of the Dispike function, it is ensured that both \(c\) and \(d\) are adjusted such that \(\mathrm{Dspike}(0)=0\) and \(\mathrm{Dspike}(1)=1\).

\section{Experiments}

This section delves into a comprehensive examination of our proposed SpikeFPN, underscoring its advantages through rigorous experimentation. Initially, we detail the training regimen of SpikeFPN on the GEN1 Automotive Detection (GAD) dataset\cite{de2020large}, encompassing facets such as data preparation, hyper-parameter optimization, and comparative performance analysis. Subsequently, ablation studies elucidate the design decisions underpinning our model—highlighting the nuances of event data preprocessing and architectural considerations. Finally, capitalizing on the inherent sparse propagation properties of SNNs, we present a comparative analysis of SpikeFPN's computational efficiency and energy consumption.

\subsection{Experiment on GAD}
\label{subsec:impl_details}

A comprehensive procedure encompassing data preparation, training, and testing has been executed on the GAD dataset, yielding noteworthy results. This section delineates the entire experimental procedure, beginning with the details of the input dataset and its encoding settings. This is followed by a discussion on the requisite hyper-parameters and performance evaluation metrics used during the training and testing phases, culminating in an evaluation of the outcomes.

\subsubsection{Input Data Representation}
The GAD dataset, obtained using a Prophesee GEN1 ATIS sensor, is an expansive event-based automotive object detection collection. It consists of over 39 hours of automotive recordings (with a resolution of 304\(\times\)240). Alongside approximately 255,000 manually labeled bounding boxes at \SI{1}{\hertz}, \SI{2}{\hertz}, and \SI{4}{\hertz} denoting cars and pedestrians, the data is divided into training, validation, and test sets. The recordings are segmented into 60-second intervals, with each set containing 1460, 429, and 470 videos, respectively. During this experiment's training and testing phases, the data partitioning in GAD was meticulously followed to ensure accurate representation of results.

In real-world settings, the event data from the sensor is sequential and varies in length. For a more efficient implementation, we processed the variable-length event data through fixed-length segmentation. After encoding the event data, \(S \times C\) frames preceding the label were created to be fed into the network for each sample in the following training. Here, \(S\) represents the number of stacks from the SBT framing process, and \(C\) is the number of frames in each stack. This frame-compression approach corresponds to what is presented in Eq.~(\ref{eq:sbt}). With \(\Delta t = \SI{60}{\milli\second}\), \(n = C = 3\), and \(S = 3\), the segmented event data is transformed into tensor-form data frames, preserving the time information between frames. The minimum temporal resolution here is \(T = \sfrac{\Delta t}{n} = \SI{20}{\milli\second}\).

\subsubsection{Hyper-Parameters Setting}
For the training phase, we employed the AdamW optimizer with an initial learning rate of \(0.001\) and a weight decay of \(0.0005\). Pre-training was deemed unnecessary; hence, all models underwent direct training for 30 epochs with a batch size of 32. A warm-up policy was applied using the first epoch's learning rate, which initially starts at a nominal value and then gradually ascends to the prescribed learning rate. The LIF and ALIF neurons have a membrane time constant \(\tau\) of 0.2 and a membrane threshold \(U_{th}\) of 0.3. The temperature \(b\) of the Dspike function is set at 3. For the ALIF neuron, \(\beta\) is 0.07, while \(\tau_a\) is a learnable parameter constrained to [0.2, 0.4], initialized at 0.3. To assess the network's training error and gauge the model's performance, we opted for the mAP, setting the prediction score to 0.3 as the accuracy metric. For a more comprehensive evaluation, we employed two mAP-based metrics: mAP\(_{50}\) and mAP\(_{50:95}\). The former, mAP\(_{50}\), represents the widely-accepted mean Average Precision with an overlap threshold of 0.5 \cite{lin2014microsoft, Everingham2010-ch}. In contrast, mAP\(_{50:95}\) serves as an alternative metric, indicating the mean Average Precision across 10 IoU ranges ([.50:.05:.95]).


\begin{table*}[t]
\caption{{Comparison of the two mean
Average Precision metrics and the network parameter quantities for different types of networks on the GEN1 Automotive Detection dataset}}
\label{tab:tab1}
\centering
\begin{threeparttable}
\begin{tabular}{|c|c c c c c|}
\hline
Model& Network Type & Params (M) & timestep & mAP\(_{50}\) &mAP\(_{50:95}\) \\
\hline
SparseConv \cite{messikommer2020event}& ANN& 133& -& 0.149& - \\
Events-RetinaNet \cite{perot2020learning}& ANN& 33& -& 0.340& -\\
E2Vid-RetinaNet \cite{perot2020learning}& ANN& 44& -& 0.270& - \\
RED \cite{perot2020learning}& ANN& 24& -& 0.400 & - \\
Gray-RetinaNet \cite{perot2020learning}& ANN& 33& -& 0.440 & - \\
ASTMNet \cite{li2022asynchronous}& ANN& -& -& 0.467& -\\
\hline
VGG-11 + SDD \cite{cordone2022object}& SNN& 13& 5& 0.37\tnote{*} & 0.174 \\
MobileNet-64 + SSD \cite{cordone2022object}& SNN& 24& 5& 0.35\tnote{*} & 0.147 \\
DenseNet121-24 + SSD \cite{cordone2022object}& SNN& 8& 5& 0.38\tnote{*} & 0.189\\
\textbf{SpikeFPN (ours)}& SNN & \textbf{22} & \textbf{3}& \textbf{0.477}& \textbf{0.223} \\
\hline
\end{tabular}
\begin{tablenotes}
  \footnotesize
  \item[*] Provided by the authors.
\end{tablenotes}
\end{threeparttable}
\end{table*}


% Figure environment removed


% Figure environment removed

\subsubsection{Results}
Throughout the training phase, the final stack output from the model serves as the prediction result. This approach ensures the preservation and accumulation of temporal information, crucial for the model's learning process and subsequent calculation of evaluation metrics. For clarity, we juxtaposed our methods against various SNN and ANN models tested on the GAD dataset. These comparative results are tabulated in Table~\ref{tab:tab1}, with values for other models sourced from existing literature. 

From a SNN perspective, our proposed SpikeFPN markedly outperforms three preceding models discussed in \cite{cordone2022object}. It exhibits an improvement ranging from 9.7\% to 12.7\% for mAP\(_{50}\) and between 3.4\% to 7.6\% for mAP\(_{50:95}\). Remarkably, this is achieved with fewer requisite steps to convergence. An examination of the training loss between SNNs, especially when comparing LIF and ALIF neurons in the first layer (depicted in Fig.~\ref{fig:LIF_ALIF}), reveals that the ALIF neuron fosters a more stable training trajectory than its LIF counterpart. Moreover, our SpikeFPN's streamlined architecture modestly outpaces the ASTMNet \cite{li2022asynchronous}, a state-of-the-art ANN incorporating attention mechanisms and bespoke adaptive sampling event preprocessing schemes.

% Figure environment removed

To assess our model's real-time capabilities, we continuously input the entire test split into the network. This process mirrors the length of steps, generating sequential labels. The average inference velocity for a single stack stands at 54 frames per second when operating on a Tesla V100 GPU. Such outcomes underscore both the efficacy of the SpikeFPN and the broader promise of SNNs in handling dynamic events coupled with sparse propagation. For further illustration, a selection of our model's prediction results can be perused in Fig.~\ref{fig:fig4}.


\subsection{Ablation Study}

In this subsection, we conduct ablation experiments from two aspects: the event coding paired with a basic neuron computation scheme, and the structure of the network itself. Through these, we aim to highlight the enhancements the proposed SpikeFPN brings to the table.

\subsubsection{Event Input Configuration}
Our goal here is to gauge the influence of varying preprocessing methods and input configurations on model performance. To this end, we juxtapose results obtained using SBE and SBT, both with LIF neurons. For each method, two input configurations, which are defined by different numbers of stacks (\(S\)) and frames within each stack (\(C\)), are employed: \(S=3,C=3\) and \(S=2,C=5\). With the SBT method, events within \SI{20}{\milli\second} are housed in each frame, while the SBE method's frame encapsulates 5000 events. Additionally, we introduce the Adaptive-LIF (ALIF) neuron to the initial layer in the SBT process, facilitating a comparison of neuron types across various input configurations. The findings are tabulated in Table \ref{tab:different_method}. 

Preliminary observations indicate that the LIF neuron, when coupled with the SBE process, does not fare as well in terms of accuracy compared to its performance with the SBT process. Interestingly, the configurations \(S=3,C=3\) underperform relative to \(S=2,C=5\) for both SBE and SBT. However, when SBT is paired with the ALIF neuron, the performance is optimized at \(S=3,C=3\). In essence, the SBT process outperforms SBE in our model setup, and the ALIF neuron, when presented with a larger stack number, better modulates its threshold in the SBT process.

\begin{table}[htb]
\caption{{Results with different input configurations. \(S\) and \(C\)  represent the number of stacks and the number of frames in each stack respectively.}}
\label{tab:different_method}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
Method& \(S\)& \(C\)& mAP\(_{50}\) & mAP\(_{50:95}\)\\
\hline
SBE + LIF& 2& 5& 0.4356 &0.1970 \\
SBE + LIF& 3& 3& 0.4324 &0.1944 \\
\hline
SBT + LIF& 2& 5& 0.4758& 0.2224 \\
SBT + LIF& 3& 3& 0.4710& 0.2176 \\
\hline
SBT + ALIF& 2& 5& 0.4731& 0.2191 \\
SBT + ALIF& 3& 3& \textbf{0.4770}& \textbf{22.27} \\
\hline
\end{tabular}
\end{table}

\subsubsection{ALIF Performance}
We further delve into the sensitivity of the adaptive threshold in ALIF concerning hyper-parameters and the merits of training \(\tau_a\). Our findings, summarized in Table \ref{tab:hyper}, unequivocally show the robustness of performance across a spectrum of \(\beta\) values. Additionally, networks furnished with a trainable \(\tau_a\) consistently outshine those with fixed counterparts.

\begin{table}[htb]
\caption{Results with different hyperparameter setting of the adaptive threshold. }
\label{tab:hyper}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\(\beta\) & Initial \(\tau_a\) & State of \(\tau_a\)& mAP\(_{50}\) & mAP\(_{50:95}\) \\
\hline
0.07 & 0.3 & non-trainable & 0.470 & 0.219 \\
0.09 & 0.3 & trainable & 0.471 & 0.220 \\
0.05 & 0.3 & trainable & 0.473 & 0.217 \\
0.07 & 0.3 & trainable & 0.477 & 0.223 \\
\hline
\end{tabular}
\end{table}

% Figure environment removed

According to Eq.~(\ref{eq1:LIF}), the membrane time constant, \(\tau\), dictates the proportion of the prior timestep's membrane potential to retain. This attenuated accumulation addresses the sparse propagation issue. Fig.~\ref{fig:alif_rate} depicts a sequence of input event density compared to the first layer activation with different \(\tau\) values. Layers with LIF neurons function akin to low-pass filters, especially when event density experiences sharp variations. In contrast, layers with binary neurons (where \(\tau = 0\)) tend to mirror their input closely, displaying synchronous changes. An exploration into the effect of the membrane time constant was undertaken, testing \(\tau\) values from 0 to 1 at 0.1 intervals.{With a set random seed for LIF neurons, mAP\(_{50}\) values at \(\tau\) of 0, 0.1, 0.2, and 0.3 were 0.4252, 0.4455, 0.4710, and 0.4696, respectively}. This emphasizes the membrane potential constant's impact on overall accuracy. Retaining prior timestep data, attenuated during temporal accumulation, proves beneficial. To corroborate the findings, additional experiments with three random seeds were executed for varying \(\tau\) values. The resultant mAP\(_{50}\) values were averaged and presented in Fig.~\ref{fig:alif_scatter}, underscoring the advantages of self-adaptive thresholds during event encoding.

% Figure environment removed

% Figure environment removed


When using different random seeds, the network with the first layer utilizing ALIF neurons consistently surpassed networks with LIF neurons at 0.3 and 0.4 thresholds, as detailed in Section~\ref{subsec:impl_details}. This accentuates the adaptive threshold's capability in managing dynamic events. In the context of firing rate, the ALIF neuron leads to increased layer sparsity, as seen in Fig.~\ref{fig:lif_spike}, emphasizing the merits of self-adjusting thresholds in event encoding.

\begin{table}[ht]
\caption{Results on the {GAD} dataset when applying different neuron model on the first and all layers of the network.}
\label{tab:act_func_compare}
\centering
\begin{tabular}{|c|c|c|c|}
\hline 
Neuron & Layer& Threshold & mAP\(_{50}\) \\ 
\hline
LIF & All/First & 0.3 & 0.470 \(\pm\) 0.001 \\
LIF & First & 0.4 & 0.470 \\
\hline
ALIF & All & 0.3 & 0.400 \\
ALIF & First & 0.3 & \textbf{0.476 \(\pm\) 0.002}  \\
\hline
\end{tabular}
\end{table}



In pursuit of optimal neuron placement for the adjustable threshold, we compared the performance of models employing LIF and adaptive threshold LIF (ALIF) neurons. Experiments were conducted using ALIF neurons across the entire network and exclusively on the first layer, with subsequent layers using LIF neurons. During these trials, we treated $\tau_a$ as a trainable variable, consistent across layers. Outcomes are summarized in Table \ref{tab:act_func_compare}. However, integrating ALIF neurons throughout the network led to subpar outcomes. Over-reliance on adjustable thresholds might infuse excessive flexibility, thereby hampering training precision. For clarity, Fig.~\ref{fig:sparsity_compare} presents an input event stream sequence by event density, juxtaposed with the first layer's activation using distinct neuron models. The LIF layer with \(U_{th} = 0.4\) exhibits the least activation, whereas the binary neuron layer with \(U_{th} = 0.3\) shows significant fluctuations. In comparison, both LIF and ALIF layers provide a moderating effect over input variations. The ALIF layer's activation resides between the LIF layers' threshold bounds, attributed to the adaptive threshold properties, emphasizing ALIF's potential in enhancing both layer sparsity and accuracy.

\begin{table}[ht]
\caption{{Results on GAD dataset with different backbone architectures. The acronym``ICS" stands for the initial channel size.}}
\label{tab:backbone_compare}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline 
Architecture &ICS & Model size (M) & mAP\(_{50}\) & mAP\(_{50:95}\)\\
\hline
ResNet18 &64 &13.34 &0.4163 &0.1799 \\
ResNet18 &80 &20.82 &0.4208 &0.1896 \\
ResNet18 &96 &29.97 &0.4429 &0.1992 \\
\hline
\textbf{SpikeFPN (ours)} &48 &21.63 &\textbf{0.4770} &\textbf{0.2227} \\
\hline
\end{tabular}
\end{table}

\subsubsection{Spiking ResNet}
\label{subsec:abs}
In this ablation study segment, we designed a spike-based ResNet-18 encoder backbone. Distinctive features include variable initial channel sizes, incorporation of the feature pyramid, a multi-head prediction module, and substitution of all activation functions with the Heaviside function \(H(\cdot)\), referenced in Eq.~(\ref{eq2:spike}). This network, adhering to a 4-stage downsampling blueprint, adopts its residual block from \cite{fang2021deep, deng2022temporal}. Outputs from the last blocks of stages 2, 3, and 4 are relayed to the feature pyramid. The network's maximum downsampling ratio stands at 32. The architecture specifics, when initiated with 80 channels, are tabulated in Table \ref{tab:ResNet_SNN}. Assessing our encoder's design efficacy, we paralleled it with this variant, given its analogous downsampling approach. Different channel sizes were evaluated to represent diverse model capacities, with findings consolidated in Table~\ref{tab:backbone_compare}. Remarkably, when juxtaposed under similar model parameters, ResNet (initialized with 80 channels) marginally underperforms against SpikeFPN. A plausible explanation could be SpikeFPN's extensive intra and inter-cell connections, fostering gradient and information flow throughout the SNN training.

\begin{table}[htbp!]
\caption{Detailed architecture of the Manually constructed spike-based ResNet, including the encoder backbone of ResNet-18, the feature pyramid and the multi-head prediction module. \(C\) denotes the number of classes and \(K\) denotes the number of anchors.}
\centering
    \begin{tabular}{|c|c|c|}\hline
    \textbf{Module}&\textbf{Layer}&\makecell[c]{\textbf{Output Feature Map}\\\textbf{\(c \times h \times w\)}}\\
    \hline
    \multirow{10}{*}{\makecell[c]{\textbf{Backbone}}}
    &Stem 0 & \(80\times128\times128\) \\
    &Stem 1 & \(80\times64\times64\) \\
    &Layer 1-1 & \(80\times64\times64\) \\
    &Layer 1-2 & \(80\times64\times64\) \\
    &Layer 2-1 & \(160\times32\times32\) \\
    &Layer 2-2 & \(160\times32\times32\) \\
    &Layer 3-1 & \(320\times16\times16\) \\
    &Layer 3-2 & \(320\times16\times16\) \\
    &Layer 4-1 & \(640\times8\times8\) \\
    &Layer 4-2 & \(640\times8\times8\) \\
    \hline
    \multirow{3}{*}{\textbf{Feature Pyramid}}
    &Layer 2-2 \(\rightarrow\) p1 & \(80\times32\times32\) \\
    &Layer 3-2 \(\rightarrow\) p2 & \(160\times16\times16\) \\
    &Layer 4-2 \(\rightarrow\) p3 & \(320\times8\times8\) \\
    \hline
    \multirow{3}{*}{\textbf{Multi-Head Prediction}}
    &p1 \(\rightarrow\) d1 & \(K\times(C+5)\times32\times 32\) \\
    &p2 \(\rightarrow\) d2 & \(K\times(C+5)\times16\times 16\) \\
    &p3 \(\rightarrow\) d3 & \(K\times(C+5)\times8\times 8\) \\
    \hline
    \end{tabular}
\label{tab:ResNet_SNN}
\end{table}


\subsection{Network Sparsity and Computation Cost}
SNNs inherently harness the potential of spike-based sparse computations and eschew multiplicative inferences, ensuring a pronounced computational edge over their ANN counterparts, which are predicated on dense matrix multiplication. To elucidate this, we compared the SpikeFPN's computational efficiency against existing ANN and SNN benchmarks.

% Figure environment removed

As illustrated in Fig.~\ref{fig:fig5}, SpikeFPN is characterized by varied sparsity across layers. Comprehensive sparsity metrics and computational costs are detailed in Table \ref{tab:energy}. Evidently, our SpikeFPN outshines contemporaneous spiking neural networks in automotive object detection tasks, achieving greater sparsity and fewer operations.

\begin{table}[htb]
\caption{Comparison of operation number (OP) and estimated energy cost. For SNN number of addition operations is counted and for ANN number of multiplication-addition operations is counted.}
\label{tab:energy}
\centering
\begin{tabular}{|c|r|c|c|}
\hline
Model& {\# OP.} & Sparsity & \makecell[c]{Energy (\SI{}{\milli\joule})}\\ 
\hline
Events-RetinaNet (ANN) & 18.73G  & - & 86.16  \\
\hline
VGG-11 + SDD &12.30G  &22.22$\%$ &11.07  \\
\hline
MobileNet-64 + SSD &6.39G &29.44$\%$ &5.74  \\
\hline
DenseNet121-24 + SSD & 4.33G &37.20$\%$ &3.90  \\
\hline
\textbf{SpikeFPN (ours)} & 2.83G &19.10$\%$ &\textbf{2.55}  \\
\hline
\end{tabular}
\end{table}

Adopting methodologies from \cite{li2021differentiable, rathi2021diet, kim2021beyond}, SNN addition operations are quantified using \(s\times T\times A\), where \(s\) signifies mean sparsity, \(T\) represents timestep, and \(A\) accounts for additions per iteration. Energy estimations are predicated on \cite{horowitz20141}'s examination of \SI{45}{\nano\meter} CMOS technology, as adopted in works like \cite{li2021differentiable, rathi2021diet, che2022differentiable}. Here, SNN addition operations cost \SI{0.9}{\pico\joule}, whereas ANN MAC operations demand \SI{4.6}{\pico\joule}. We benchmarked against Events-RetinaNet \cite{perot2020learning} given its accessible codebase, facilitating a balanced comparison. Notably, compared to Events-RetinaNet, SpikeFPN demonstrates a nearly 6-fold reduction in operations and a 33-fold decrease in energy consumption. Such frugality underscores the SNN's potential in energy-efficient, event-driven vision tasks.

\section{Conclusions}
The presented research introduces the SpikeFPN, a novel architecture that seamlessly combines a spiking feature pyramid network with a self-adaptive spiking neuron. This  design specifically targets event-based automotive object detection tasks by harnessing the unique capability of the self-adaptive spiking neurons to encode sparse data effectively. The adoption of such architecture, an area of burgeoning interest, has delivered impressive outcomes in terms of both detection accuracy and computational efficiency.

Extensive experimentation using the GEN1 Automotive Detection dataset underscores the efficiency and validity of our proposed model. These experimental results depict not only a commendable mAP prediction accuracy, but also the sound reasoning behind the SpikeFPN's design. In drawing comparisons with similar network structures, inclusive of non-adaptive and various downsampling configurations, our SpikeFPN demonstrates consistent robustness across varying problem sizes. Theoretical insights further reveal the benefits of the spiking mechanism-based model, particularly highlighting its low power consumption and reduced computational demands, suggesting its potential utility for applications demanding efficient computational paradigms.

In summation, the integration of self-adaptive spiking mechanisms within the feature pyramid network structure presents a promising avenue for advancing event-based visual tasks. Given the inherent discrete and sparse nature of events, and the added advantage of preserving temporal information, aligning this data format with our unique neuron setup might pave the way for notable performance enhancements. The inherent low power consumption attributed to the sparse propagation properties of spiking neurons, when combined with our experimental outcomes, suggests a possible edge that SNNs could hold over traditional architectures in specific domains. Such potential merits further exploration and research.


\bibliographystyle{IEEEtran}
\bibliography{main}




\vfill

\end{document}


