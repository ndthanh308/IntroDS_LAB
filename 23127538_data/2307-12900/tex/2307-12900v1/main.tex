\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{subfigure}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission
\def\iccvPaperID{9721} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi


%%%%%%%%% TITLE
\title{Automotive Object Detection via Learning Sparse Events by Temporal Dynamics of Spiking Neurons}
% sparse?

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }
\author{
Hu Zhang$^{1,2}$\and
Luziwei Leng$^{2}$\footnotemark[1]\and
Kaiwei Che$^{2,3}$\and
Qian Liu$^{2}$\and
Jie Cheng$^{2}$\and
Qinghai Guo$^{2}$\and
Jiangxing Liao$^{2}$\and
Ran Cheng$^{1}$\footnotemark[1] \\
% \affiliations
% 1. Southern University of Science and Technology, China \\
% 2. ACS Lab, Huawei Technologies, Shenzhen, China 
% \affiliations
% $^1$First Affiliation\\
% $^2$Second Affiliation\\
% $^3$Third Affiliation\\
% $^4$Fourth Affiliation
% \emails
% \{first, second\}@example.com,
% third@other.example.com,
% fourth@example.com
}


\begin{document}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
% \footnotetext[1]{These authors contribute equally to this work.}
\footnotetext[1]{Corresponding author: lengluziwei@huawei.com, ranchengcn@gmail.com. 1. Department of Computer Science and Engineering, Southern University of Science and Technology,  China. 2. ACS Lab, Huawei Technologies, China. 3. Department of Electronic and Electrical Engineering, Southern University of Science and Technology,  China.}
% Remove page # from the first page of camera-ready.
% \ificcvfinal\thispagestyle{empty}\fi
% todo
% estimate ANN MACs..

%%%%%%%%% ABSTRACT
\begin{abstract}
% Spiking neural network (SNN) has been proposed as a low energy candidate for traditional artificial neural network (ANN) 
% With recent progresses in surrogate gradient training methods, SNNs have achieved competing accuracy level as their artificial counterparts in benchmark machine learning tasks such as image classification. 
% In this work, we directly train spiking neural networks for object detection on benchmark RGB datasets including Pascal VOC, COCO and event-based datasets such as Gen1 and 1Mpx. 
% Spiking neural network is a new type of neural network inspired by the brain to propagate information by firing discrete spikes asynchronously.  Different from the general
% ANN, it is more biological, so it has the characteristics of low energy consumption, fast propagation and can save time information. However, due to the limitations of the architecture design and hardware, the current spiking neural network cannot achieve good accuracy in other visual tasks, such as object detection, except that it can be comparable to ANN in image recognition tasks. In this work, we use Neural Network Architecture Search (NAS) to search for SNNs for object detection tasks and directly train SNNs during the retraining process to search for SNN architectures that can achieve good performance. There are many situations where RGB cameras are weak, such as fast moving objects, overexposure, and dark scenes. Event-based cameras can help solve these problems due to their high dynamic range and frame rates, as well as low power consumption, and the events they produce are called Dynamic Vision Sensor (DVS) data. Based on the high matching between SNN network and DVS data, in this work, we mainly use SNN network for object detection on DVS dataset.
Event-based sensors, with their high temporal resolution (1$\mathrm{\mu s}$) and dynamical range (120$\mathrm{dB}$), have the potential to be deployed in high-speed platforms such as vehicles and drones. However, the highly sparse and fluctuating nature of events poses challenges for conventional object detection techniques based on Artificial Neural Networks (ANNs). In contrast, Spiking Neural Networks (SNNs) are well-suited for representing event-based data due to their inherent temporal dynamics. In particular, we demonstrate that the membrane potential dynamics can modulate network activity upon fluctuating events and strengthen features of sparse input. In addition, the spike-triggered adaptive threshold can stabilize training which further improves network performance. Based on this, we develop an efficient spiking feature pyramid network for event-based object detection. Our proposed SNN outperforms previous SNNs and sophisticated ANNs with attention mechanisms, achieving a mean average precision (map50) of 47.7\% on the Gen1 benchmark dataset. This result significantly surpasses the previous best SNN by 9.7\% and demonstrates the potential of SNNs for event-based vision. Our model has a concise architecture while maintaining high accuracy and much lower computation cost as a result of sparse computation.
% To the best of our knowledge, this is the first time SNNs have surpassed ANNs on challenging large-scale event-based automotive object detection tasks. 
Our code will be publicly available.
% Compared to traditional frame-based cameras, event-based sensors have much higher temporal resolution (1$\mathrm{us}$) and dynamical range (120$\mathrm{dB}$), giving them potentials to be deployed in high speed edge platforms such as vehicles and drones. However, highly sparse and fluctuating events also pose challenges to conventional techniques for static image datasets based on artificial neural networks (ANNs). 
%!!!In this work, we demonstrate that inherent temporal dynamics and adaptive threshold mechanism of spiking neurons can be trained to modulate event density which implicitly improve network representation and stability. Based on these neuronal mechanisms, we develop an efficient spiking feature pyramid network for event-based object detection. 
%On the Gen1 benchmark dataset, our proposed SNN achieves 47.7\% map50, significantly surpassing previous best SNN by 9.7\%. With much more concise architecture, our model also outperforms sophisticated ANNs with attention mechanisms on accuracy meanwhile with extremly lower computation cost. To our best knowledge, this is the first time SNNs surpassing ANNs on challenging large-scale event-based automotive object detection tasks, demonstrating the potential of SNNs for event-based vision.
%  To this end, we further develop a self-adaptive events encoder with adaptive-threshold spiking neurons based on its intrinsic dynamics. 
% There are many situations where RGB cameras are weak, such as fast moving objects, overexposure, and dark scenes. Event-based cameras can help solve these problems due to their high dynamic range and frame rates, as well as low power consumption, and the events they produce are called Dynamic Vision Sensor (DVS) data. Based on the high matching between SNN network and DVS data, in this work, we mainly use SNN network for object detection on DVS dataset.
% emphasis application value?? 10ms detection, network speed?
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Object detection is a fundamental and crucial task in computer vision with a wide range of applications, including face recognition and vehicle and small object detection. Typically, object detection tasks rely on images captured by frame-based cameras. However, images produced by these cameras can be too blurry or slow for high-speed scenarios or situations with under/overexposure, such as emergency detection during driving. The recently developed retina-inspired Dynamical Vision Sensor (DVS), also known as an event-based sensor \cite{lichtsteiner2008128, posch2010qvga, chen201164, serrano2013128, brandli2014240, son20174}, solves this problem by recording the intensity changes of pixels at a high temporal resolution of 1 $\mathrm{\mu s}$ with a high dynamical range of 120dB. The sensor generates asynchronous events when the brightness of individual pixels changes, recording the timestamp, polarity, and position of the pixels.

The highly sparse and fluctuating nature of events poses challenges for conventional object detection techniques based on Artificial Neural Networks (ANNs). While recurrent architectural and algorithmic approaches \cite{perot2020learning, li2022asynchronous} have been proposed for preprocessing events, they often come with high computation costs and increased latency.
In contrast, the Spiking Neural Networks (SNNs) are a new type of network inspired by the brain that propagate information through discrete spikes generated from their inherent temporal dynamics. SNNs enable low-cost computation through sparse activation and multiplication-free inference, making them an ideal candidate for processing highly sparse and dynamic events. In addition, SNNs deployed on neuromorphic hardware can also benefit from event-based processing due to the sparse activation.

Recent advancements in surrogate gradient (SG) algorithms \cite{zenke2018superspike, neftci2019surrogate} have led to the successful training of deep SNNs for image classification tasks \cite{wu2019direct, rathi2021diet, zheng2021going, li2021differentiable, deng2022temporal}. However, the difference between spiking neurons and real-valued artificial neurons in information representation still results in significant performance drops or increased latency when training SNNs for more challenging tasks, such as dense image prediction or object detection \cite{hagenaars2021self, kim2020spiking, cordone2022object}, especially when their architectures are directly taken from complicated ANNs.
% that could be suboptimal for spiking neurons.

In this work, we study the potential functionalities of various intrinsic temporal dynamics of spiking neurons to explore a low-computation and low-latency approach for processing highly dynamic events. Based on this study, we design an efficient spiking feature pyramid network for event-based object detection, integrating and optimizing these neuronal mechanisms with surrogate gradient training.

% ckbone where spiking activities are organized as hierarchical directed acyclic graphs, . A spiking feature pyramid is then integrated from the encoder feature maps which subsequently feed into a multi-head prediction module.
% Event streams are directly input into the SNN after simple preprocessing, which preserves temporal information that can be learned by inherent dynamics of the network end-to-end. 
% emphasis real-time!!!
Our main contributions can be summarized as following:
\begin{enumerate}
    \item We explore the functionalities of intrinsic temporal dynamics of spiking neurons, including membrane potential dynamics and adaptive threshold, to modulate highly dynamic events and demonstrate that they can strengthen features from sparse events and stabilize training.
    \item We design an efficient spiking feature pyramid network for event-based automotive object detection with an optimized spiking encoder architecture.
    \item Our proposed SNN outperforms previous SNNs and sophisticated ANNs with attention mechanisms, achieving a mean average precision (map50) of 47.7\% on the Gen1 benchmark dataset, significantly surpassing the previous best SNN by 9.7\%. Meanwhile, our model has a more concise architecture than ANNs while maintaining high accuracy and much lower computation cost.
    % \item compare on Pascal VOC, COCO
\end{enumerate}
% Contributions
% C1. SNN ALIF neuron adaptive sampling and denoising?\\
% C2. design for SNN architecture for event-based detection: including downsampling backbone of SpikeDHS, FPN
% C3. sparsity, energy and speed record..T=2 
% C4. near or surpass ANN SOTA on 1mPX, gen1, ddd17 car
% figures and tables:
% Fig1. network architecture and cell 
% Fig2. ALIF, LIF, AST fmap qualititive compare?
% Fig3. 1mpx, gen1, DDD17 qualitative comparison: input events, detection box
% Fig4. Sparsity and energy statistic (7.26 before)
% !! fig5, compare low pass filter effect of BNN, LIF, ALIF
% structure ablation study, different node number, structure, layer number?
% compare with spiking Yolo?
% table1-3. Gen1, 1mpx, DDD17
% add 3 models of SSD
% table 4. ablation study of ALIF neuron, compare with LIF
% ablation study: resnet-DAG backbone, single-multi head prediction
% table 5. (in supplement?). architecture details
% to do after submission
% check tab cap position !!!
%----------
% memory box, mae for ssl encoder training?
% !!! add resnet backbone experiment, (test directly trained spiking yolo_v2/3 on EbOD?) 
% DGS, temporal Relu, MLP backboneï¼ŸSSD? 
% all 1 input for events and all same weight for alif 
% add 10 ms frame experiment
% add acknowledgement of ASTMNet, ijcnn author for answering question
\section{Related Work}
\subsection{Event-based Object Detection}
% In 2019, Megvii first proposed the neural network architecture search work DetNAS\cite{chen2019detnas} in the field of target detection, which is also a search for the backbone network of target detection. The design of the search space uses a supernet structure. First, the supernet is pre-trained on the ImageNet dataset, and then fine-tuning is performed on target detection datasets such as PASCAL VOC\cite{everingham2010pascal} or MS COCO. Finally, the evolution algorithm is used in the supernet. Find the best subset on the Internet as the searched network architecture, and the result obtained by the search can reach 42.0 mAP (0.5:0.95) on the COCO dataset. In the same year, two works, NAS-FPN\cite{ghiasi2019fpn} and Auto-FPN\cite{xu2019auto}, appeared. NAS-FPN only searches the neck part of the entire network, while Auto-FPN searches the neck and neck of the network at the same time. In the head part, the mAP (0.5:0.95) of the COCO dataset can reach 48.3 for NAS-FPN at a resolution of 1280x1280, while the mAP (0.5:0.95) for Auto-FPN on the COCO dataset can reach 40.5.
% In the follow-up, works such as SpineNet\cite{du2020spinenet}, SM-NAS\cite{yao2020sm}, SP-NAS\cite{jiang2020sp}, Hit-Detector\cite{guo2020hit} and OPANAS\cite{liang2021opanas} appeared one after another. These works have achieved a certain accuracy in the final results, but the search parts of different work are different, and there are also big differences in other parts of the network that do not need to be searched, so in the end, there are large differences between different works, but almost all have the same The similar disadvantage is that the search time is extremely long, from the longest 333 TPU days to the shortest 4 GPU days, it is a long process.
%%%%%%%%%%%%%%%
% Feature Pyramid Networks for Object Detection \cite{lin2017feature} is one of the representative model architectures to generate pyramidal feature representations for object detection. It adopts a backbone model, typically designed for image classification, and builds feature pyramid by sequentially combining two adjacent layers in feature hierarchy in backbone model with top-down and lateral connections. The high-level features, which are semantically strong but lower resolution, are upsampled and combined with higher resolution features to generate feature representations that are both high resolution and semantically strong.
% NAS-FPN \cite{ghiasi2019fpn} aims to search a better feature pyramid network
% DetNas \cite{chen2019detnas} aims to search for a better backbone
% NAS-FCOS? \cite{wang2020fcos}
% \subsection{Event-based Object Detection}
% Compared to frame-based cameras, event-based cameras have much higher temporal resolution (1us) and dynamical range (120dB), giving them potentials to be deployed in high speed edge platforms such as vehicles and drones \cite{chen2020event, zhang2021issafe}. 
% Currently, in most vision tasks, object detection is performed by using models trained on image datasets using deep learning methods.RGB data is still commonly used in target detection, but there are some problems with RGB data. For example, in some special scenes such as high exposure or at night, the obtained image effect is not very good. At the same time, when capturing high-speed moving objects, ordinary RGB cameras also cannot capture clear object images due to their limited frame rate. As a result, event-based DVS (Dynamic Vision Sensor) datasets obtained by special cameras have become popular in recent years, which can solve some of the above problems.
% The feature of the event camera is that since it only outputs events by sensing the change in the degree of light and darkness, the time interval of the event camera is counted in microseconds, and there are only two polarities of 1 and 0 for events, which represent brightness respectively. increase and decrease in degree. When there is no change in brightness, there will be no event output.
% Event-based object detection (EbOD) is an emerging field with few exiting works but rapidly growing interest.
Event cameras, with their high temporal precision and dynamic range, are ideal for scenarios that require high-speed objects, varying lighting conditions, and low latency. However, the dynamic nature of event camera data poses a challenge to traditional deep learning approaches based on frames. To address this challenge, events are usually preprocessed and converted into dense representations before being processed by task networks. There have been several handcrafted methods proposed for this conversion, including event frames \cite{lagorce2016hots, maqueda2018event, moeys2016steering, ye1809unsupervised, zhu2018ev, nguyen2019real}, stacking based on time or number of events \cite{wang2019event}, voxel grids \cite{zhu2019unsupervised}, the event queue method \cite{tulyakov2019learning}, a grid of LSTMs \cite{cannici2020differentiable}, and discrete time convolution \cite{zhang2022discrete}. Special asynchronous convolutions have also been proposed for sparse event data \cite{scheerlinck2019asynchronous, messikommer2020event}. The latter was applied to the Gen1 event-based automotive object detection dataset \cite{de2020large}. \cite{perot2020learning} used a convolutional LSTM network and a temporal consistency loss for improved training. ASTMNet \cite{li2022asynchronous} proposed an adaptive sampling scheme, with temporal attention and memory modules, to encode events and achieved state-of-the-art accuracy.
% however requires empirical hyperparameter tuning. 
% unlike traditional feedforward ANN networks, SNN naturally encodes temporal information..

\subsection{Deep SNNs for Vision Tasks}
%!!! add citation of mixing paper, etc
% The spiking neural network is the earliest biological firing model inspired by the squid neuron system. It is generally believed that for a spiking neuron, when the received input is converted into a membrane voltage and exceeds a certain threshold, the neuron will emit a spike, and at the same time The membrane voltage will reset. The membrane voltage of the neuron is saved, and in different neuron models, the processing of the membrane voltage when it is not firing is different. Currently, SNNs generally use the Leaky Integrate-and-Fire (LIF) neurons as basic units for constructing neural networks. The reason is that LIF neuron\cite{} is a typical synthetic model, which not only has the simplicity of the Integrate-and-Fire (IF) neuron model\cite{}, but also can simulate the abundant physiological characteristics of biological neuron like the Hodgkin-Huxley (HH) neuron model\cite{}. As the equation shows, in the LIF model there will be a time constant or decay coefficient $\tau$ when the neuron does not emit spike, will decay over time, and the degree of decay is related to $\tau$.
% Recent developments of specially designed neuromorphic hardware \cite{merolla2014million, furber2014spinnaker, 2019Towards, kungl2019accelerated, davies2021advancing} have motivated growing efforts to apply SNNs for hard vision tasks in deep learning. Combining with event cameras, there is a potential to create neuromorphic systems realizing extremely low power and low latency sensing \cite{roy2019towards}. Current successful approaches of training deep SNNs can be mainly divided into two ways, i.e, ANN-to-SNN conversion and direct training based on surrogate gradient (SG) algorithms. The former converts an ANN to SNN by closely approximating the real-valued activation function with spiking mechanisms \cite{rueckauer2017conversion, sengupta2019going}, achieving high accuracy but also with high latency since it needs to accumulate spike rates, thus not suitable for processing fast events. In particular, \cite{kim2020spiking} proposed a spiking version of Yolo network for object detection which achieved nearly loss less performance on PASCAL VOC \cite{everingham2010pascal} and MS COCO datasets \cite{lin2014microsoft}, at the cost of thousands of timesteps for convergence. Several recent works have largely reduced the latency after conversion \cite{bu2021optimal, li2021free}, whether these methods applies to other more complicated architectures beyond image classification remains to be seen.\\
% The direct training approach uses SG functions to approximate backpropagation gradients \cite{zenke2018superspike, shrestha2018slayer, neftci2019surrogate}. With improving training techniques such as specialized normalization, SG and loss functions design \cite{wu2019direct, zhang2020temporal, li2021differentiable, deng2022temporal} direct trained SNNs are achieving competitive accuracy on hard benchmark classification tasks like ImageNet meanwhile with only a few simulation steps for convergence. These progresses have motivated their applications on other event-based vision tasks beyond classification, such as optical flow estimation \cite{hagenaars2021self}, video reconstruction \cite{zhu2022event} and object detection \cite{cordone2022object}, etc.
% % deep stereo \cite{ranccon2021stereospike}
% However, constrained by relatively simple handcrafted architectures these works are still suboptimal on accuracy compared to state-of-the-art ANNs. \cite{chedifferentiable} recently proposed a spike-based differentiable hierarchical search framework to optimize the network on both the cell and layer level, and for the first time rivals sophisticated ANNs on event-based deep stereo. 
The development of neuromorphic hardware has led to growing interest in using SNNs for hard vision tasks in deep learning \cite{merolla2014million, furber2014spinnaker, leng2014deep, leng2016spiking, leng2018spiking, kungl2019accelerated, leng2020solving, davies2021advancing}. When combined with event cameras, neuromorphic systems can achieve extremely low power and low latency sensing \cite{roy2019towards}. There are two main approaches for training deep SNNs: ANN-to-SNN conversion and direct training.

The ANN-to-SNN conversion approach approximates real-valued activation functions with spiking mechanisms \cite{rueckauer2017conversion, sengupta2019going}. Although this approach achieves high accuracy, it also has high latency due to the accumulation of spike rates, making it unsuitable for processing fast events. \cite{kim2020spiking} proposed a spiking version of Yolo for object detection, achieving nearly lossless performance on PASCAL VOC \cite{everingham2010pascal} and MS COCO datasets \cite{lin2014microsoft} with thousands of timesteps required for convergence. Recent works have reduced latency after conversion \cite{bu2021optimal, li2021free}, but it is unclear whether these methods can be applied to more complex architectures beyond image classification.

The direct training approach uses surrogate gradient (SG) functions to approximate backpropagation gradients \cite{zenke2018superspike, shrestha2018slayer, neftci2019surrogate}. With improved training techniques such as specialized normalization, SG, and loss function design \cite{wu2019direct, zhang2020temporal, li2021differentiable, deng2022temporal}, directly trained SNNs are achieving competitive accuracy on hard benchmark classification tasks like ImageNet, with only a few simulation steps required for convergence. These advances have encouraged their application to other event-based vision tasks beyond classification, such as optical flow estimation \cite{hagenaars2021self}, video reconstruction \cite{zhu2022event}, and object detection \cite{cordone2022object}. Nevertheless, these works are still limited by relatively simple handcrafted architectures and are suboptimal in terms of accuracy compared to state-of-the-art ANNs. More recently, \cite{che2022differentiable, zhang2023accurate, li2023efficient} demonstrated that by using spike-based differentiable hierarchical search, the performance of SNNs can be further improved on multiple event-based vision tasks including challenging deep stereo and semantic segmentation tasks.
% For dense image prediction problem there are still few existing works, but competitive demonstrations of SNNs are emerging. Early works applying SNNs for image generation used recurrent architectures like Boltzmann machines \cite{neftci2014event, petrovici2016stochastic, leng2018spiking, korcsak2022cortical} as well as Autoencoders \cite{burbank2015mirrored, kamata2022fully}, and explored related functional roles of bio-inspired mechanisms. More recent works adopted deep convolutional layers with an encoder-decoder backbone and applied SNN to large scale benchmark tasks such as event-based optical flow estimation \cite{hagenaars2021self}, stereo matching \cite{ranccon2021stereospike} and video reconstruction \cite{zhu2022event}. Constrained by existing ANN architectures or simple handcrafted networks these SNNs are still suboptimal on accuracy compared to state-of-the-art ANNs. Effective layer level dimension variation has been proven critical for dense prediction task. Inspired from \cite{liu2018darts, liu2019auto, cheng2020hierarchical}, \cite{anonymous2022diff} recently developed a spike-based differentiable hierarchical search (SpikeDHS) method which optimizes deep SNN on both the cell and the layer level, leading to SNNs with efficient and compact architectures outperforming state-of-the-art ANNs on event-based deep stereo.
% Few previous work has applied spike neural networks to object detection. The work spikingyolo\cite{kim2020spiking} is the first, which is based on the YOLO model and obtains the corresponding SNN by means of transformation. The SNN achieves almost the same result as the original ANN on both VOC and COCO datasets. But the transition time step is very large, reaching over 4000, which means a huge time consumption.
% Most of the work uses the transformation method to obtain SNN from ANN, but because of the large time step, the transformation method can not meet the needs of real-time applications. In order to obtain a low-delay SNN network, we pay more attention to the direct training method of SNN.

\section{Event Density Modulation with Temporal Dynamics of Spiking Neurons}
% In automotive scenarios, objects with varying relative speed captured by the event camera often cause event signals with drastic density fluctuation. This has been a challenge for traditional ANNs since they usually deal with images of constant data rate. Spiking neurons are inherited with abundant temporal dynamics due to the continuous evolving membrane potential and discrete synaptic transmission processes. In this section, we investigate potential functionalities of two mechanisms, i.e. membrane potential dynamics and adaptive threshold mechanism in modulating radically fluctuating input events. 
In automotive scenarios, objects with varying relative speeds captured by event cameras often result in event signals with significant density fluctuations. This poses a challenge for traditional ANNs, which typically deal with images with constant data rates. Spiking neurons, on the other hand, possess abundant temporal dynamics, which are derived from the continuous evolution of their membrane potentials and discrete synaptic transmission processes. In this section, we explore the potential capabilities of two mechanisms, i.e., the dynamics of the membrane potential and the adaptive threshold mechanism, in regulating the radically fluctuating input events.

\subsection{Event Representation}
We adopt the simple stacking based on time (SBT) \cite{wang2019event} method for event preprocessing, which is a real-time and low-computation-cost method that is embedded in the accumulator modules of mainstream event sensors. The SBT method combines events into temporally neighboring frames, preserving the rich temporal information of the event stream that can be potentially learned by the downstream SNN with its inherent temporal dynamics.

Within one stack, the SBT method compresses a duration of $\Delta t$ event stream into $n$ frames. The value of each pixel in the $i$th frame is defined as the accumulated polarity of events:
\begin{equation}
    P(x,y) = \textit{sign}(\sum_{t\in T} p(x,y,t))
    \label{eq:sbt}
\end{equation}
where $P$ is the value of the pixel at $(x,y)$, $t$ is the timestamp, $p$ is the polarity of the event, and $T \in [\frac{(i-1)\Delta t}{n},\frac{i\Delta t}{n}]$ is the duration of events merged into one frame. Another simple event encoding method, stacking based on the number of events (SBN) \cite{wang2019event}, is compared to SBT in an ablation study. Other more advanced event encoding methods could also be compatible with our network, but may potentially increase computation cost.

\subsection{Membrane Potential Dynamics}
% % Figure environment removed
% To study the functionality of membrane potential dynamics of spiking neurons in modulating events, for simplicity we use the iterative Leaky Integrate and Fire (LIF) model \cite{wu2019direct}, expressed as
% \begin{equation}
% \begin{split}
%     u_i^{t, n} &= \tau u_i^{t-1,n}(1-y_i^{t-1,n}) + I_i^{t,n} \\
%     I_i^{t,n} &= \sum_jw_{ji}y_j^{t, n-1} 
%     % u^{t} = \tau u^{t-1}(1-y^{t-1}) + I^{t}
% \end{split}
% \label{eq1:LIF}
% \end{equation}
% with the spike generation formulated as
% \begin{equation}
%     y_i^{t,n} = H(u_i^{t, n}-u_\mathrm{th}) =
%     \begin{cases}
%         1,~u_i^{t, n} >= u_\mathrm{th} \\
%         0,~\text{otherwise}
%     \end{cases} 
% \label{eq2:spike}
% \end{equation}
% where $u_i^{t, n}$ represents the membrane potential of the neuron $i$ of the ${n}$th layer at the time ${t}$, $\tau$ represents the membrane time constant which configures the membrane leakage of the neuron, $I_i^{t,n}$ is the influx currency, which is the weighted sum of the spiking activation $y_j^{t, n-1}$ of the connected neurons from the previous layer $n-1$. The spiking activation $y_i^{t, n}$ is defined by a heaviside function $H$ which equals $1$ only when $u_i^{t, n}$ reaches a membrane threshold $u_\mathrm{th}$, otherwise $0$.\\
% When too few events happen during the merging interval, SBT may produce a very sparse event frame. Another simple event encoding method, stacking based on number of events (SBN) \cite{wang2019event} solves this problem by using a fixed number of events to form a frame. However when event density is too high (e.g. fast moving objects) it could lead to over blurred frames and suboptimal network performance (see ablation study section \ref{subsec:abs}). A recent work \cite{li2022asynchronous} proposed an adaptive sampling scheme by iteratively counting an appropriate number of events from temporal bins of adaptive lengths. However in practice this would cause additional computation cost and extra latency. According to Eq. \ref{eq1:LIF}, the membrane time constant $\tau$ controls the proportion of inherited membrane potential from the last time step. This accumulation effect of the membrane potential naturally alleviates the sparse events problem, as shown in Fig. \ref{fig:evdensity}, where we plot the average firing rate of the first layer of SNN using different membrane time constants. Setting $\tau = 0$ leads to a binary neuron (BN) without memory, and its activity is more correlated to the density of the event stream.
% The value of membrane time constant also affects final network performance, as shown in table \ref{tab:tau_compare}. Using binary neuron leads to a drastic reduction of accuracy, demonstrating the critical role of temporal dynamics of the membrane potential. 
% \begin{table}[h]
% \centering
% \begin{tabular}{ccc}
% \toprule 
% Neuron & Membrane time constant $\tau$ & mAP (0.5) \% \\ 
% \midrule
% LIF & 0 & 42.52 \\
% LIF & 0.1 &44.55 \\
% LIF & 0.2 &47.10 \\
% LIF & 0.3 &46.96\\
% \bottomrule
% \end{tabular}
% \caption{Results on the Gen1 dataset when applying different membrane time constant.}
% \label{tab:tau_compare}
% \end{table}
% % Figure environment removed
% Figure environment removed

% % Figure environment removed
To study the functionality of the membrane potential dynamics of spiking neurons in modulating events, we use the iterative Leaky Integrate and Fire (LIF) model \cite{wu2019direct} for simplicity. The LIF model is expressed as:
\begin{equation}
\begin{split}
    u_i^{t, n} &= \tau u_i^{t-1,n}(1-y_i^{t-1,n}) + I_i^{t,n} \\
    I_i^{t,n} &= \sum_jw_{ji}y_j^{t, n-1} 
    % u^{t} = \tau u^{t-1}(1-y^{t-1}) + I^{t}
\end{split}
\label{eq1:LIF}
\end{equation}
with the spike generation formulated as:
\begin{equation}
    y_i^{t,n} = H(u_i^{t, n}-u_\mathrm{th}) =
    \begin{cases}
        1,~u_i^{t, n} >= u_\mathrm{th} \\
        0,~\text{otherwise}
    \end{cases} 
\label{eq2:spike}
\end{equation}
where $u_i^{t, n}$ represents the membrane potential of the neuron $i$ of the $n$th layer at time $t$, $\tau$ represents the membrane time constant which configures the membrane leakage of the neuron, $I_i^{t,n}$ is the influx currency, which is the weighted sum of the spiking activation $y_j^{t, n-1}$ of the connected neurons from the previous layer $n-1$. The spiking activation $y_i^{t, n}$ is defined by a Heaviside function $H$ which equals $1$ only when $u_i^{t, n}$ reaches a membrane threshold $u_{th}$, otherwise $0$.

When there are too few events during the merging interval, the SBT method may produce a very sparse event frame. Another simple event encoding method, stacking based on the number of events (SBN) \cite{wang2019event}, solves this problem by using a fixed number of events to form a frame. However, when the event density is too high (e.g. fast moving objects), it can lead to over-blurred frames and suboptimal network performance (see the ablation study in Section \ref{subsec:abs}). A recent work \cite{li2022asynchronous} proposed an adaptive sampling scheme by iteratively counting an appropriate number of events from temporal bins of adaptive lengths. However, in practice, this would cause additional computation cost and extra latency.

According to Equation \ref{eq1:LIF}, the membrane time constant $\tau$ controls the proportion of the inherited membrane potential from the previous time step. This accumulation effect of the membrane potential naturally alleviates the sparse events problem, as shown in Figure \ref{fig:evdensity}a, where we plot a sample sequence of density of input events alongside with first layer activation using different $\tau$. The layer with LIF neurons acts like a low-pass filter when the event density radically fluctuates, modulating its spiking rate at peaks and valleys of event density. Comparatively, the firing rate of the layer with binary neurons (setting $\tau = 0$) is more influenced by the input and exhibits more correlated fluctuations. The modulation effect is also reflected in the network performance, as shown in Table \ref{tab:tau_compare}. Using a binary neuron leads to a drastic reduction in accuracy, demonstrating the critical role of the temporal dynamics of the membrane potential. In terms of the membrane time constant, we performed experiments with $\tau$ ranging from 0 to 1 with an interval of 0.1. The performance of the network dropped as $\tau$ increased further (Figure \ref{fig:evdensity}b), indicating that $\tau$ needs to be in an appropriate range in order for the SNN to have optimal temporal dynamics for the task.

\begin{table}[h]
\centering
\begin{tabular}{ccc}
\toprule 
Neuron & Membrane time constant $\tau$ & mAP (0.5) \% \\ 
\midrule
LIF & 0 & 42.52 \\
LIF & 0.1 &44.55 \\
LIF & 0.2 &47.10 \\
LIF & 0.3 &46.96\\
\bottomrule
\end{tabular}
\caption{Results on the Gen1 dataset when applying different membrane time constant.}
\label{tab:tau_compare}
\end{table}

\subsection{Adaptive Threshold}
In addition to the passive accumulation mechanism of the membrane potential, we investigate a self-adaptive mechanism for the spiking neuron, called the adaptive threshold. Inspired by neuron adaptation in the neocortex \cite{gerstner2014neuronal, gouwens2018systematic}, the adaptive threshold model has previously been applied to temporal sequence learning in recurrent SNNs \cite{bellec2018long, bellec2020solution} to provide long-term memory. Here, we examine its functionality for short-term modulation of events.
% adaptive leaky-integrate and fire (ALIF) neuron model.
The adaptive threshold is formulated as:
\begin{equation}
    \begin{split}
        % u^{t,n} &= \tau u^{t-1,n}(1-y^{t-1,n})+I^{t,n},I^{t,n}=\Sigma w_{j}y^{t,n-1}_{j} \\
        y^{t} &= H(u^{t}-A^{t})\\
        A^{t} &= u_{\mathrm{th}}+\beta a^{t} \\
        a^{t} &= \tau_{a} a^{t-1}+y^{t-1} \\
        % \rho &= {e}^{-\delta t / \tau_{a}} \\
    \end{split}
    \label{eq2}
\end{equation}
where $H$ is the Heaviside step function, $A^{t}$ is the adjustable threshold at time $t$, $a^{t}$ is the cumulative threshold increment that changes according to the spiking history of the neuron, $\beta$ is a scaling factor, and $\tau_{a}$ is the time constant of $a$. The adjustable threshold $A$ modulates the spiking rate such that the threshold rises during dense input, which suppresses the neuron from firing, and vice versa. The upper and lower bounds of the adaptive threshold can be derived by considering extreme input scenarios. When the neuron is continuously inactive, $a^{t} \rightarrow 0$ as $t \rightarrow \infty$, meaning the lower bound of $A^{t}$ is the original threshold $u_{\mathrm{th}}$. In the extreme case when the neuron is continuously active, with $y^{0} = 1$ and $a^{0} = 0$, $a^{t}$ can be expressed as
\begin{equation}
a^{t} = \sum_{i=1}^{t} \tau_{a}^{i-1}
\label{eq:eqac}
\end{equation}
and its upper bound can be derived as $t \rightarrow \infty$ and equals $1/(1-\tau_{a})$. The range of the adjustable threshold $A$ is therefore $[u_{\mathrm{th}}, u_{\mathrm{th}}+\beta/(1-\tau_{a})]$. This event-triggered, self-adaptive activity improves the network's robustness and stability to rapidly fluctuating event densities, and could further improve the performance of the SNN.

To verify our hypothesis, we compare the performance of a network using LIF neurons with a network using LIF neurons with adaptive threshold (ALIF). We conduct experiments where we use ALIF neurons for the whole network, as well as only for the first layer of the network (with the rest of the network using LIF neurons). During the experiments, we set $\tau_a$ as a trainable variable with each layer sharing the same value. The results are summarized in Table \ref{tab:act_func_compare}.

For multiple random seeds, the network with ALIF neurons applied to the first layer outperforms both networks with LIF neurons, with $u_{\mathrm{th}}$ set to 0.3 and 0.4, which correspond to the lower and upper bounds of the adaptive threshold (refer to section \ref{subsec:impl_details} for details). This demonstrates the effectiveness of the adaptive threshold in processing dynamic events. However, applying ALIF neurons to the whole network results in suboptimal performance. We believe that an excessive use of adjustable thresholds might lead to excessive network flexibility and harm training precision. % The network with ALIF neuron applied at the first layer performs the best, surpassing both LIF neurons with $u_{\mathrm{th}}$ setting to 0.3 and 0.4, which correspond to the lower and upper bounds of the adaptive threshold (see section \ref{subsec:impl_details} for details), demonstrating the effectiveness of the adaptive threshold in processing dynamics events. However applying ALIF neuron for the whole network leads to a suboptimal result. We conjecture that an over use of adjustable thresholds might induce excessive network flexibility which harms the training precision. 
In terms of firing rate, ALIF neuron leads to higher layer sparsity as shown in Figure \ref{fig:evdensity}c, demonstrating the advantage of self-adaptive thresholds for events encoding.
% This modulation effect of the LIF neuron strengthens feature representation during sparse events and leads to a higher average firing rate on the feature map, as shown in Figure \ref{fig:evdensity}b.
% From the training process (Figure \ref{fig:evdensity}c) it can be observed that the ALIF neuron leads to more stable loss reduction and mAP convergence.

\begin{table}[h]
\centering
%  \setlength{\tabcolsep}{1.2mm}{
% \resizebox{\linewidth}{!}{
\begin{tabular}{cccc}
\toprule 
Neuron & Layer& Threshold & mAP (0.5) \%  \\ 
\midrule
% BN & All & 0.3 &42.52 &18,87 \\
% BN & First& 0.3 &46.28 &20.95 \\
LIF & All/First & 0.3 & 47.0 $\pm$ 0.1 \\
LIF & First & 0.4 & 47.0 \\
% \midrule
ALIF & All & 0.3 & 40.0 \\
ALIF & First & 0.3 & \textbf{47.6 $\pm$ 0.2}  \\
\bottomrule
\end{tabular}
% }
\caption{Results on the Gen1 dataset when applying different neuron model on the first and all layers of the network.}
\label{tab:act_func_compare}
\end{table}
% \begin{table}[h]
% \centering
% %  \setlength{\tabcolsep}{1.2mm}{
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule 
% Neuron & Layer& Threshold & \makecell[c]{mAP\\(0.5) \%}  &  \makecell[c]{mAP\\(0.5:0.95) \%}\\ 
% \midrule
% % BN & All & 0.3 &42.52 &18,87 \\
% % BN & First& 0.3 &46.28 &20.95 \\
% LIF & All/First & 0.3 & 47.10 &21.76 \\
% LIF & First & 0.4 & 47.01 & 21.90 \\
% % \midrule
% ALIF & All & 0.3 & 39.66 & 17.38 \\
% ALIF & First & 0.3 & \textbf{47.70} & \textbf{22.27} \\
% \bottomrule
% \end{tabular}
% % }
% \caption{Results on the Gen1 dataset when applying different neuron model on the first and all layers of the network.}
% \label{tab:act_func_compare}
% \end{table}
% By comparing the training loss between SNNs with LIF and ALIF neuron as the first layer, as shown in Figure \ref{fig:LIF_ALIF}, it can be observed that ALIF neuron leads to more stable training process comparing to LIF neuron.
% % Figure environment removed
% % add future use of search algorithms...!!!
% % For better illustration, we plot in Figure \ref{fig:fig3} a sequence of input event streams by event density alongside with activation of the first layer using different neuron models. It can be seen that with a higher threshold, LIF layer of $u_{\mathrm{th}}=0.4$ shows the lowest activation. The activation of BN layer ($u_{\mathrm{th}}=0.3$) fluctuates the most (especially at peak and valley) among neurons with $u_{\mathrm{th}}=0.3$ and correlates most with the event density, due to the lack of intrinsic temporal dynamics. In contrast, LIF and ALIF layers modulates over the input fluctuation exhibiting smoothing effect. The activation of ALIF layer is between the two threshold bounds of LIF layers, owing to the adaptive threshold.\\ !!!
% % cite zhang2022 per channel variable..!!!
% % % Figure environment removed
% % !!! add
% % has a lower sparsity than LIF and BN. This shows that ALIF maintains performance while having lower spike release rates.
% \begin{table}[h]
% \centering
% %  \setlength{\tabcolsep}{1.2mm}{
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{ccccc}
% \toprule 
% Neuron & Layer& Threshold & \makecell[c]{mAP\\(0.5) \%}  &  \makecell[c]{mAP\\(0.5:0.95) \%}\\ 
% \midrule
% % BN & All & 0.3 &42.52 &18,87 \\
% % BN & First& 0.3 &46.28 &20.95 \\
% LIF & All/First & 0.3 & 47.10 &21.76 \\
% LIF & First & 0.4 & 47.01 & 21.90 \\
% % \midrule
% ALIF & All & 0.3 & 39.66 & 17.38 \\
% ALIF & First & 0.3 & \textbf{47.70} & \textbf{22.27} \\
% \bottomrule
% \end{tabular}
% % }
% \caption{Results on the Gen1 dataset when applying different neuron model on the first and all layers of the network.}
% \label{tab:act_func_compare}
% \end{table}

\subsection{Surrogate Gradient Training}
The challenge of training an SNN lies in the discontinuity of spike generation, as no gradient can be applied for backpropagation. The early success of deep SNNs in machine learning tasks was achieved by transferring ANN-trained model parameters to their equivalent SNN~\cite{diehl2015fast,rueckauer2017conversion}. Using rate coding, the activation of an analog neuron is proportional to the firing rate of its corresponding spiking neuron in the converted SNN. However, this approach is usually restricted to feed-forward network architectures without recurrency and is therefore only applicable to tasks with weak temporal dependencies.

On the other hand, surrogate gradient methods approximate the discontinuous activation by smoothing out the spike generation function (such as Equation~\ref{eq2:spike}), allowing for the calculation of gradients for backpropagation. During training, the temporal neural dynamics are precisely modeled (Equation~\ref{eq1:LIF}) and the errors can be backpropagated through time. As a result, surrogate gradient methods not only enable training of spiking neurons considering precise temporal dynamics, but are also naturally suited to solving tasks with temporal dependencies such as the continuous object detection in this paper.

We adopt the Dspike surrogate function proposed by~\cite{li2021differentiable}:
\begin{equation}
\begin{split}
\mathrm{Dspike}(u) &= a\cdot\textit{tanh}(b(u-c)) + d, 0 <= u <= 1\\
\textit{tanh}(x) &= \frac{e^x - e^{-x}}{e^x + e^{-x}},
\label{eq3:Dspike}
\end{split}
\end{equation}
where $u$ is the membrane potential and parameters $(a, b, c, d)$ transform the original hyperbolic tangent function \textit{tanh} to output in the range [0, 1] with different shapes. The wide range of surrogate function choices satisfies various training requirements. Particularly, $b$, the so-called temperature factor, controls the smoothness of the surrogate function. $c$ is usually set to 0.5 to set a symmetry center at $u=0.5$, and parameters $c$ and $d$ are tuned to ensure $\mathrm{Dspike}(0)=0$ and $\mathrm{Dspike}(1)=1$. Other SG functions could also be used for the training.
%Table is misisng for the parameters used for SNN and SG
% \section{Preliminary}
% % emphasis we do detection directly based on sparse data, harnessing SNN temporal dynamics...without attention or LSTM heavy parameters
% \subsection{Surrogate Gradient Training of SNN}
% The training of SNN is divided into conversion and direct training. The former first designs or uses an ANN model for training, and then converts each module or network layer in the ANN into a module or network layer in the spike network. The direct training method is to use the fitting function to replace the spike emission process, and update the weights through the BP of the fitting function to achieve the purpose of directly training the spike neural network.
% \begin{equation}
%     u_i^{t, n} = \tau u_i^{t-1,n}(1-y_i^{t-1,n}) + I_i^{t,n}
%     % u^{t} = \tau u^{t-1}(1-y^{t-1}) + I^{t}
% \label{eq1:LIF}
% \end{equation}
% Considering the LIF model of spiking neurons, during forward propagation, the spike firing and leakage patterns of neurons need to be designed, so we design the initial membrane voltage of neurons to be 0, and then each time there is an output from the upper layer, the same The current membrane voltage of the neuron is summed. If the summed membrane voltage exceeds the threshold of spike emission, the spike will be emitted to the next layer of the network, and the membrane voltage will be set to 0; otherwise, no spike is emitted, and at the next moment, the value of membrane voltage will be attenuated.
% At the moment of the emission spike, its derivative is infinite, which cannot be achieved in backpropagation. Referring to the previous related work, we use the fitting function to replace its backpropagation process. When considering the fitting function used, we choose two fitting functions, one is a simple piecewise function: when the independent variable in the specified region, it returns the original input, and in other regions, it returns 0; another fitting function inspired by the \cite{li2021differentiable} can use hyperparameters to control the shape and steepness of the function.

\section{Spiking Feature Pyramid Network}
Despite recent progress in SNNs for image classification \cite{wu2019direct, rathi2021diet, zheng2021going, li2021differentiable, deng2022temporal}, there are still few examples of SNNs demonstrating competitive precision with ANNs in more challenging vision tasks, such as object detection. Directly applying sophisticated ANN architectures to SNNs often leads to suboptimal performance or extra latency, especially when the task requires efficient structural variation of the network \cite{hagenaars2021self, kim2020spiking, cordone2022object}. While there have been ongoing efforts to improve ANN-to-SNN conversion methods \cite{bu2021optimal, li2021free}, we take a different approach by designing an efficient network specifically for spiking neurons, which we present in this section.
% Despite recent process of SNNs in image classification \cite{wu2019direct, rathi2021diet, zheng2021going, li2021differentiable, deng2022temporal}. There are still rare examples of SNNs demonstrating competitive precision as ANNs in other more challenging vision tasks including object detection. Due to fundamental differences in activation and information representation between spiking and artificial neurons, directly applying sophisticated ANN architectures to SNN often led to suboptimal performances or extra latency, especially when the task requires efficient structural variation of the network \cite{hagenaars2021self, kim2020spiking, cordone2022object}. While continuous efforts have been made to improve ANN-to-SNN conversion methods \cite{bu2021optimal, li2021free}, we take another direction by designing an efficient network for spiking neurons, which we illustrate in this section.
% In particular, \cite{kim2020spiking} proposed a spiking version of Yolo network for object detection which achieved nearly loss less performance on PASCAL VOC \cite{everingham2010pascal} and MS COCO datasets \cite{lin2014microsoft}, at the cost of thousands of timesteps for convergence. Several recent works have largely reduced the latency after conversion  whether these methods applies to other more complicated architectures beyond image classification remains to be seen

% Figure environment removed
% Figure environment removed

% \subsection{Training a zero bias CNN with batch normalization}
% Given input x, the output from batch normalization (BN) can be expressed as:
% \begin{equation}
%     y = \frac{x-E(x)}{\sqrt{Var(x)+\epsilon}}\gamma + \beta
% \end{equation}
% where $\epsilon = 1e-5$ by default in Pytorch. By setting \textit{'affine=False'} in BN of Pytorch, leads to $\gamma=1, \beta=0$.\\
% Conversion of BN parameters into convolution parameters can be expressed as:
% \begin{equation}
%     \tilde{w} = \frac{\gamma}{\sqrt{Var(x)}}w
% \end{equation}
% \begin{equation}
%     \tilde{b} = \frac{\gamma}{\sqrt{Var(x)}}(b-E(x))+\beta
% \end{equation}
% where $w$ and $b$ are original convolution parameters. $Var(x)$ and $E(x)$ can either be an running average of the training process or obtained from 1-batch input of the whole test set $x=x_{\mathrm{test}}$.\\
% The goal is to have $\tilde{b}=0$. One can define an auxiliary loss:
% \begin{equation}
%     L_a = \lambda {||b-E(x))||}^2
% \end{equation}
% where $x$ = $x_{\mathrm{test}}$ or $x$ = $x_{\mathrm{validation}}$.
\subsection{Encoder Backbone}
The encoder network typically takes up the largest portion of parameters in a detector and its feature extraction capability is critical to the network's performance. ResNet \cite{he2016deep} and its derivatives are commonly used as backbones in detection networks. In the field of SNNs, spiking ResNets \cite{sengupta2019going, li2021differentiable, deng2022temporal} are currently the norm for image classification. Recently, an alternative architecture was proposed by \cite{che2022differentiable} that achieves higher accuracy in deep SNNs by organizing spiking activities as hierarchical directed acyclic graphs (HDAGs). We design the encoder backbone following this principle and compare spiking HDAG with spiking ResNet as encoder under similar model capacities in our ablation study.

As shown in Figure \ref{fig:fig1}, the encoder backbone follows a multi-stage downsampling architecture, consisting of two spiking stem layers followed by ten spiking cells. During downsampling, the spatial resolution of the feature map is halved while the channel size is doubled. A spiking stem layer consists of a convolution, batch normalization (which can be later merged into convolution weights during test inference \cite{ioffe2015batch}), and a spiking activation function. These layers are used for initial feature extraction and channel variation. The spiking cell contains three spiking nodes, and its structure is repeated across layers. The first two nodes receive inputs from two previous cells and the third node receives input from the previous two nodes. The feature maps of all nodes share the same size, and their outputs are concatenated to form the output of the cell. Within each node, operations from multiple edges are summed up on the membrane potential level and then passed through a spiking activation function.
The connection of the spiking cell is designed empirically to strike a balance between inference speed and accuracy. Other potential connection topologies can be explored in the future. A detailed architecture of the backbone is provided in the supplement.
% channel split...jump connection conjecture...
% The spiking cell can be formulated as:
% % The expression is as follows:
% % Instead of mixed at spike, we use MM here since no search needed
% \begin{equation}
%   y_j = f(\sum_{i}o^{(i,j)}(y_i))
% \label{eq:node_sum}
% \end{equation}
% where $y_j$ represents the spiking output of node j, $f$ is a spike activation function, $y_i$ denotes the output of a previous node or cell and $o^{(i,j)}$ represents the operation of directed edge $(i\rightarrow j)$.
% The connection of the spiking cell is designed empirically to balance inference speed and accuracy, other potential connection topology can be explored in the future. A detailed architecture of the backbone is in the supplement.
% large number of jump connections both within and across cells potentially help gradient propagation through deep layers, which improves training of the model. 
% Regarding the design of the search space, based on the original Auto-DeepLab search space, we first designed the internal structure of the cell. There are only three candidate operations, one is 3x3 convolution plus BN layer and spiking neurons, the other is 5x5 convolution plus BN layer and spiking neurons, the last one is direct connection, that is, the input and output are unchanged. There are 3 nodes in each cell, each node will accept 2 input connections and the input can only come from the predecessor node, so finally 6 connections need to be searched in the cell.

\subsection{Network Architecture}
% This design is because its task is a semantic segmentation task, and the image needs to be upsampled to restore the original image size after downsampling. However, in the field of object detection, observing most of the previous network models, there is generally only downsampling in the backbone network and no upsampling process. If you continue to use the previous design, it is obviously not suitable for target detection tasks. Therefore, we have modified the connection between cells so that there are only two cases:
% \begin{enumerate}
%     \item When the cell of the previous layer is connected to the cell of this layer, the level level is increased by 1 level, which means that the downsampling multiple is multiplied by 2 times;
%     \item When the cell of the previous layer is connected to the cell of this layer, the level level remains unchanged, which means that the downsampling multiple does not change, and the original multiple is maintained;
% \end{enumerate}
% In this way, there will be no level drop, that is, the process of dividing the downsampling multiple by 2 to produce upsampling. This not only reduces the interference items in the search process to speed up the search, but also more in line with the general situation of the target detection network
% In our backbone network, high-level features have more channels, lower resolution, and richer semantic information, while low-level features have greater resolution but poorer semantic information. If only one layer of features is used for target detection, the useful features and semantic information can not be fully utilized, so FPN combined with multi-layer features for prediction. 
 
 Feature pyramids have been widely used in many object detection systems to provide multi-scale feature information \cite{he2015spatial, lin2017feature, 8627998}. In our approach, we integrate the output spiking feature maps from the last three stages of the backbone to form a spiking feature pyramid, as shown in Figure \ref{fig:fig2}. The features with lower spatial resolution in the pyramid are upsampled by nearest interpolation to double the width and height (to maintain a binary feature map) and concatenated with feature maps of the same spatial dimension from the backbone. These features are then fused through a 1$\times$1 convolution, batch normalization, and spike activation to form the next level of the feature pyramid. The spiking feature pyramid is fed to a multi-head prediction module through convolution, batch normalization, and spike activation. The resulting features are processed by a 1$\times$1 convolution to produce floating-point results of shape $N\times N \times[3\times (C+5)] $, where $N$ represents the width and height of the corresponding feature map, $C$ is the number of categories, and 5 corresponds to 4 box coordinate predictions and 1 confidence prediction. We use non-maximum suppression (NMS) \cite{redmon2017yolo9000,girshick2014rich} to select the final bounding boxes.

% following the approach of Yolo networks \cite{}, where 
% Features from different stages are combined by using a horizontal connection and a vertical connection. The vertical connection uses double nearest neighbor upsampling, that is, the width and height of the resolution are doubled. The horizontal connection first performs concat on features at the current layer and features obtained after upsampling at the upper layer. Then, a 1x1 convolutional layer plus spike function is used to adjust the number of channels to the original number of channels at the current layer. The overall structure is shown in Figure 2.
% Finally, a 3x3 spike convolutional layer is added to the features of each layer of the FPN to smooth the features as output, and these outputs are input to the prediction structure to obtain the prediction frame. Our prediction structure is multi-headed, with each prediction header being a 1x1 convolution layer used to change the number of channels and obtain floating-point output.
% Due to the FPN structure, the entire network has three output feature maps and three prediction boxes at each grid point, so the tensor is 
% Search with integrated feature pyramid network (FPN) and multi-head structure:
% cancel low level jump connection?
\begin{table*}[t]
    \centering
    \begin{tabular}{c|c r c c c }
        \hline
        Model& Type & Params & (T) & mAP(0.5) &mAP(0.5:0.95) \\
        \hline
        % MatrixLSTM \cite{cannici2020differentiable}& ANN & 65M& -& 31.0*& 31.0*& - \\
        SparseConv \cite{messikommer2020event}& ANN& 133M& -& 15.0& - \\
        Events-RetinaNet \cite{perot2020learning}& ANN& 33M& -& 34.0& -\\
        E2Vid-RetinaNet \cite{perot2020learning}& ANN& 44M& -& 27.0& - \\
        RED \cite{perot2020learning}& ANN& 24M& -& 40.0 & - \\
        Gray-RetinaNet \cite{perot2020learning}& ANN& 33M& -& 44.0 & - \\
        ASTMNet \cite{li2022asynchronous}& ANN& -& -& 46.7& -\\
        \hline
        VGG-11+SDD \cite{cordone2022object}& SNN& 13M& 5& 37& 17.4 \\
        MobileNet-64+SSD \cite{cordone2022object}& SNN& 24M& 5& 35& 14.7 \\
        DenseNet121-24+SSD \cite{cordone2022object}& SNN& 8M& 5& 38& 18.9\\
        \textbf{FP-DAGNet (Ours)}& SNN & \textbf{22M} & \textbf{3}& \textbf{47.7}& \textbf{22.3} \\
        \hline
    \end{tabular}
    \caption{Result on Gen1 dataset. - denotes data not available.}
    \label{tab:tab1}
\end{table*}

\section{Experiment}
% \subsection{Object detection on RGB Dataset}
% In experiment,
% Direct training of SNN using ANN architectures (Yolov2, Yolov3)\\
% Architecture search details
% In COCO dataset, train2017=trainval35k=train2014+val2014-minival2014=train2014+val2014-val2017, val2017=minival2014.
% We conduct our experiments on two event-based datasets: Gen1 dataset\cite{de2020large} and 1Mpx dataset\cite{perot2020learning}.
% \subsection{Dataset}
% \subsubsection{Gen1}

We conduct experiments on the widely used Gen1 \cite{de2020large} dataset, a large scale event-based automotive object detection dataset which contains more than 39 hours of automotive recordings acquired with a Prophesee GEN1 ATIS sensor (resolution 304$\times$240), together with more than 255,000 manual bounding box annotations of cars and pedestrians. These bounding boxes are labeled at 1Hz, 2Hz or 4Hz. All data is split into training, validation and test sets where recordings are cut into 60 seconds chunks with each set containing 1460, 429 and 470 videos.
% According to the timestamp of the label, the number of samples in the training set is 72371, the validation set is 20329, and the test set is 30605. On average, each sample has about two bounding boxes.
% For evaluating the accuracy of a detection method, we consider the same metrics used for the COCO dataset and use the released tool code by prophesee. 
% 1Mpx dataset is  the first high-resolution large-scale dataset for object detection. The dataset contains more than 14 hours recordings of a 1 megapixel event camera(resolution 1280$\times$720), in automotive scenarios, together with 25M bounding boxes of cars, pedestrians, and two-wheelers, labeled at 60Hz. The 1MPX is split in 11.19 hours for training, 2.21 hours for validation, and 2.25 hours for testing. 

\subsection{Implementation Details}
\label{subsec:impl_details}
\noindent\textbf{Input Representation}\\
The events generated by the sensor in real-world scenarios are consecutive and have variable lengths. To fully utilize the temporal information of event data, we use $S \times C$ frames ahead of the label timestamp as input for each sample during training. Here, $S$ is the number of SBT stacks and $C$ is the number of frames in each stack. As per Equation \ref{eq:sbt}, we set $\Delta t = 60$ms, $n = C = 3$, and $S = 3$. This results in a minimum temporal resolution of $T = \frac{\Delta t}{n} = 20$ms with a frame-based update scheme.

The output of the network at the last stack is used for prediction. This way, the temporal information can be accumulated and learned by the system for the task.
% !!!and each sample can reuse the previous stack, so that the inference time only needs one time step, that is, the inference time of one stack.

\noindent\textbf{Hyperparameters}\\
During the training phase, we use the Adamw optimizer with an initial learning rate of 1$e^{-3}$ and a weight decay of 5$e^{-4}$. No pre-training is required, all models are directly trained for 30 epochs with a batch size of 32 and use a warm up policy with the learning rate of the first epoch gradually increases from a very low value to the initial learning rate. The LIF and ALIF neurons have a membrane time constant $\tau$ of 0.2 and a membrane threshold $u_{th}$ of 0.3. The temperature $b$ of the Dspike function is set to 3. For the ALIF neuron, we use $\beta=0.07$ and $\tau_a$ is set to a learnable parameter with initial value of 0.3 and limited to [0.2, 0.4]. For event-based object detection, we use COCO mAP \cite{lin2014microsoft} as the metric for accuracy and report both mAP(0.5) and mAP(0.5:0.95). The overlap threshold is set to 0.5 and the predicting score is set to 0.3.
% For test inference, we demonstrate the real-time applicability of our model by feeding all stacks continuously, which evolves an equal length of steps and outputs sequential segmentation results.
% \textbf{Architecture Search and Retrain}: During the search phase, we randomly divide the training dataset in two parts for bi-level optimization like \cite{xu2019auto,anonymous2022diff}. For retraining, the standard training/testing split is used. We use 3 nodes (n4) within one cell and a limited number of candidate operations to reduce search time, which are {conv3Ã—3 with $g(b = 3)$, conv3Ã—3 with $g(b = 5)$, skip connection}, with $g$ = Dspike. For the sign function we use $g(b = 3)$. For the network architecture, we adopt an 10 layer fixed downsampling architecture proposed in [40]. The search phase takes 20 epochs with mini-batch size 64, the first 15 epochs are used to warm up convolution weights. We use SGD optimizer with momentum 0.9 and a learning rate of 0.025. The architecture search takes about 2 GPU day on a single NVIDIA Tesla V100 (32G) GPU. After search, we retrain the model on target datasets with channel expansion for 30 epochs with mini-batch size 32 and learning rate 0.001. We use AdamW optimizer with momentum 0.9.

\subsection{Results}
% Figure environment removed

We compare our methods with previous works of both SNNs and ANNs, the results are reported in Table \ref{tab:tab1}, values of other models are taken from literature. In SNN domain, our network significantly surpasses 3 models in \cite{cordone2022object} by 9.7-12.7 \% on mAP(0.5) and 3.4-7.6 \% on mAP(0.5:0.95) meanwhile with fewer time steps for convergence. Our network also slightly surpasses ASTMNet \cite{li2022asynchronous}, a recently proposed ANN with attention mechanism and handcrafted adaptive sampling event preprocessing schemes, meanwhile with much concise architecture. These results demonstrate the effectiveness of our model and the potential of SNN in processing highly sparse and dynamic events. 
% Note that for the 4 models from \cite{perot2020learning} there are contradicts in literature \cite{li2022asynchronous, cordone2022object} about the metric so we denote corresponding values with *. More discussions are provided in the supplement. 
A few samples from the prediction results of our model are shown in Figure \ref{fig:fig4}. 
% An interesting finding is that our model even successfully detects objects which are not labeled in the ground truth.\\

In real-world scenario, events are generated consecutively by the sensor with flexible lengths. To test the real-time applicability of our model, we feed the entire test split continuously into the model, which evolves an equal length of steps and estimates sequential labels. The average inference speed of one stack reaches 54 FPS (Tesla V100 GPU).
% And ALIF can reduce the sparsity while keeping or improving the accuracy.
% \begin{table*}[htb]
%     \centering
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{|c|c|c c c|c|c|}
%             \hline
%             model& train& params &FLOPS &time steps(T) &mAP &FPS  \\
%             \hline
%             Cascade Eff-B7 NAS-FPN\cite{ghiasi2021simple}& transferred from coco& -& -& -& 89.3&  -\\
%             SSD-512\cite{liu2016ssd}& 07+12+COCO& -& -& -& 83.2& 19(TitanX)\\
%             SSD-512\cite{liu2016ssd}& 07+12& -& -& -& 79.8& 19(TitanX)\\
%             YOLOv2-544\cite{redmon2017yolo9000}& 07+12& -& -& -& 78.6& 40(TitanX)\\
%             \hline
%             SpikingYolo\cite{kim2020spiking}& 07+12& -& -& 8000& 51.83& \\
%             \hline
%             yolov2\_tiny& 07+12& -& -& -& 52.05& \\
%             yolov2\_tiny\_spikeb& 07+12& -& -& 10& 48.82& \\
%             yolov3& 07+12& -& -& -& 81.4& \\
%             yolov3\_spikeb& 07+12& -& -& -& ?& \\
%             \hline
%             snn-darts& -& -& -& -& -& - \\
%             \hline
%         \end{tabular}
%     }
%     \caption{VOC 07+12 result}
%     \label{tab1}
% \end{table*}
% \begin{table*}[htb]
%     \centering
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{|c|c|c c c|c|c c c c c|c|}
%             \hline
%             model& train& params &FLOPS &time steps(T) &mAP &AP$_{50}$ &AP$_{75}$ &AP$_{S}$ &AP$_{M}$ &AP$_{L}$ &FPS\\
%             \hline
%             DINO\cite{zhang2022dino}& train2017& 218M& -& -& 63.2& -& -& -& -& -& -\\
%             YOLOR-D6\cite{wang2021you}& train2017& 151.78M& 936.8G& -& 55.4& 73.5& 60.6& 40.4& 60.1& 68.7& 31(V100) \\
%             \hline
%             SpikingYolo\cite{kim2020spiking}& -& -& -& -& 25.66& -& -& -& -& -& - \\
%             \hline
%             yolov2\_tiny& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov2\_tiny\_spikeb& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov3& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov3\_spikeb& -& -& -& -& -& -& -& -& -& -& - \\
%             \hline
%             snn-darts& -& -& -& -& -& -& -& -& -& -& - \\
%             \hline
%         \end{tabular}
%     }
%     \caption{COCO minival (val2017)result}
%     \label{tab2}
% \end{table*}
% \begin{table*}[htb]
%     \centering
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{|c|c|c c c|c|c c c c c|c|}
%             \hline
%             model& train& params &FLOPS &time steps(T) &mAP &AP$_{50}$ &AP$_{75}$ &AP$_{S}$ &AP$_{M}$ &AP$_{L}$ &FPS\\
%             \hline
%             DINO\cite{zhang2022dino}& train2017& 218M& -& -& 63.3& -& -& -& -& -& -\\
%             YOLOR-D6\cite{wang2021you}& train2017& 152M& 937G& -& 55.4& 73.3& 60.6& 38.0& 59.2& 67.1& 30(V100) \\
%             \hline
%             SpikingYolo\cite{kim2020spiking}& -& -& -& -& 25.66& -& -& -& -& -& - \\
%             \hline
%             yolov2\_tiny& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov2\_tiny\_spikeb& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov3& -& -& -& -& -& -& -& -& -& -& - \\
%             yolov3\_spikeb& -& -& -& -& -& -& -& -& -& -& - \\
%             \hline
%             snn-darts& -& -& -& -& -& -& -& -& -& -& - \\
%             \hline
%         \end{tabular}
%     }
%     \caption{COCO test-dev (upload to server)result}
%     \label{tab3}
% \end{table*}

\subsection{Ablation Study}
\label{subsec:abs}
% % Figure environment removed
\begin{table}[htb]
    \centering
    \begin{tabular}{c c c c c}
        \hline
        Method& S& C& mAP(0.5) & mAP(0.5:0.95)\\
        \hline
        SBN+LIF& 2& 5& 43.56 &19.70 \\
        SBN+LIF& 3& 3& 43.24& 19.44 \\
        \hline
        SBT+LIF& 2& 5& 47.58& 22.24 \\
        SBT+LIF& 3& 3& 47.10& 21.76 \\
        \hline
        SBT+ALIF& 2& 5& 47.31& 21.91 \\
        SBT+ALIF& 3& 3& \textbf{47.70}& \textbf{22.27} \\
        \hline
    \end{tabular}
    \caption{Results with different input configurations. S represents the number of stacks, C represents the number of frames in a stack.}
    \label{tab:different_method}
\end{table}
\noindent\textbf{Input Configuration}\\
\noindent To investigate the impact of different preprocessing methods and input configurations, we compare the results using SBN and SBT with LIF neurons. For both methods, we apply two sets of input configurations with different numbers of stacks and frames: S=3, C=3 and S=2, C=5. For SBT, each frame contains events within 20 ms, whereas for SBN, each frame contains 5000 events. In addition, we apply the ALIF neuron (to the first layer) for SBT to compare the influence of neuron type at different input configurations. The results are shown in Table \ref{tab:different_method}. For the LIF neuron, the overall accuracy of using SBN is lower than that of SBT, and using both SBN and SBT for S=3, C=3 is worse than S=2, C=5. The result of using SBT with ALIF neurons is opposite, with the best result obtained at S=3, C=3. The results demonstrate that SBT is better than SBN, and that the ALIF neuron performs better in the SBT approach with a larger number of stacks, where the neuron has longer time to  adapt its threshold.\\

\noindent\textbf{Adaptive Threshold}\\
We further perform experiments to investigate the hyperparameter sensitivity of adaptive threshold and the effectiveness of training $\tau_a$. The results are summarized in Table \ref{tab:hyper}. The model demonstrates robust performance in a range of different values of $\beta$. In addition, network with a trainable $\tau_a$ performs better than with a fixed value.
\begin{table}[htb]
    \centering
    \begin{tabular}{c c c c}
        \hline
         Initial $\tau_a$ & $\beta$ & mAP(0.5) \% & mAP(0.5:0.95) \% \\
        \hline
        0.3 & 0.05 & 47.3 & 21.7 \\
        0.3 & 0.07 & 47.7 & 22.3 \\
        0.3 (fixed) & 0.07 & 47.0 & 21.9 \\
        0.3 & 0.09 & 47.1 & 22.0 \\
        \hline
    \end{tabular}
    \caption{Results with different hyperparameter setting of the adaptive threshold. $\tau_a$ (fixed) denotes that the variable is not trainable and fixed to a certain value.}
    \label{tab:hyper}
\end{table}

\noindent\textbf{Spiking ResNet}\\
To study the contribution of our specific encoder backbone design on network performance, we implement the backbone with an alternative architecture, i.e. spiking ResNet, which is widely used in image classification. We choose spiking ResNet-18 since it is also a 4-stage downsample structure. For a fair comparison, we select a range of channel sizes for the network covering a range of model capacities. The results are summarized in Table \ref{tab:backbone_compare}. Under similar model size, ResNet with initial channel size 80 performs worse than DAGNet. We conjecture that the large number of inter and intra-cell connections within the DAGNet benefits information flow and gradient propagation in the training of the deep SNN.
% even if the initial channel count was increased to 96. ???
% This demonstrates the effectiveness of our network architecture. 
More details of the architecture of the ResNet are provided in the supplement. 

\begin{table}[h]
\centering
%  \setlength{\tabcolsep}{1.2mm}{
% \resizebox{\linewidth}{!}{
\begin{tabular}{ccccc}
\toprule 
Architecture &ICS & Model size & \makecell[c]{mAP\\(0.5)} &  \makecell[c]{mAP\\(0.5:0.95)}\\
\midrule
ResNet18 &64 &13.34M &41.63 &17.99 \\
ResNet18 &80 &20.82M &42.08 &18.96 \\
ResNet18 &96 &29.97M &44.29 &19.92 \\
\midrule
\textbf{DAGNet} &48 &21.63M &\textbf{47.70} &\textbf{22.27} \\
\bottomrule
\end{tabular}
% }
% \label{tab:tab}
\caption{Results on Gen1 dataset with different backbone architectures. ICS stands for initial channel size.}
\label{tab:backbone_compare}
\end{table}

\subsection{Network Sparsity and Computation Cost}
% \begin{table}[htb]
% \centering
% \label{tab:energy}
% %  \setlength{\tabcolsep}{1.2mm}{
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{crrrr}
% \toprule Model& {\#Add.} &{\#Mul.} &Sparsity & {Energy}\\ 
% \midrule
% SparseConv(ANN)  & 9.322G &9.322G &- &42.88$\mathrm{mJ}$\\
% \midrule
% VGG-11+SDD &11.07G & 0M &22.22$\%$ &11.07 $\mathrm{mJ}$ \\
% MobileNet-64+SSD &4.34G & 0M &29.44$\%$ &5.74 $\mathrm{mJ}$ \\
% DenseNet121-24+SSD & 2.33G & 0M &37.20$\%$ &3.90 $\mathrm{mJ}$  \\
% \textbf{FPN-DAG-ALIF} & 4.94G &	{0M} &19.10$\%$ &\textbf{2.55 $\mathrm{mJ}$}  \\
% \bottomrule
% \end{tabular}
% % }
% % }
% \caption{Comparison of operation number and estimated energy cost.}
% \end{table}

%  \setlength{\tabcolsep}{1.2mm}{
% \resizebox{\linewidth}{!}{
% }
% }
In this section, we compare the computation cost of our methods with previous works of ANN and SNN. Due to spike-based sparse computation and multiplication-free inference, SNNs can achieve significant computation drop compare to ANNs base on dense matrix multiplication. 
% Figure environment removed
As Figure \ref{fig:fig5} shows, our network exhibits an overall sparse activity with varying degrees across layers. 
The average network sparsity and computation cost are summarized in Table \ref{tab:energy}. 
\begin{table}[htb]
\centering
\begin{tabular}{crcr}
\toprule Model& {\# OP.} & Sparsity & \makecell[c]{Energy\\($\mathrm{mJ}$)}\\ 
\midrule
% SparseConv(ANN)  & 9.322G &9.322G &- &42.88$\mathrm{mJ}$\\
% \midrule
%41.26G
Events-RetinaNet (ANN) & 18.73G  & - & 86.16  \\
VGG-11+SDD &12.30G  &22.22$\%$ &11.07  \\
MobileNet-64+SSD &6.39G &29.44$\%$ &5.74  \\
DenseNet121-24+SSD & 4.33G &37.20$\%$ &3.90  \\
\textbf{FP-DAGNet} & 2.83G &19.10$\%$ &\textbf{2.55}  \\
\bottomrule
\end{tabular}
\caption{Comparison of operation number (OP) and estimated energy cost. For SNN number of addition operations is counted and for ANN number of multiplication-addition operations is counted.}
\label{tab:energy}
\end{table}
The spiking FP-DAGNet has much higher sparsity and lower operation number compares with previous SNNs. Following \cite{li2021differentiable, rathi2021diet, kim2021beyond}, we count the number of addition operations of the SNN by $s*T*A$, where $s$ is the mean sparsity, $T$ is the time step and $A$ is the addition number. The energy consumption is estimated following the study of \cite{horowitz20141} on $45 \mathrm{nm}$ CMOS technology adopted by previous works \cite{li2021differentiable, rathi2021diet, che2022differentiable}, with one addition operation in SNN costing $0.9\mathrm{pJ}$ while one multiply-accumulate (MAC) operation in ANN consuming 4.6pJ. For ANN we use Events-RetinaNet \cite{perot2020learning} since there is a lack of detailed architecture descriptions or publicly available code for other works. Compare to Events-RetinaNet, our SNN achieves more than 6 times less operation number and more than 33 times less of estimated energy consumption, demonstrating the great potential of SNNs for low-power event-based vision.
% The operation count of the addition between the membrane potential and convolution input is negligible. 

\section{Conclusion}
In this work, we explore the functionalities of membrane potential dynamics and adaptive threshold of spiking neurons in processing highly sparse and dynamic events. The event feature strengthening and network training stabilization abilities demonstrated by these mechanisms are also of general usage for other event-based vision tasks. Based on these neuronal mechanisms, we develop an efficient spiking feature pyramid network with an optimized spiking encoder backbone. 
% alif neuron
% We construct for the network a multi-stage spiking DAGNet as an efficient encoder backbone and compare with conventional spiking ResNet. 
On the large-scale event-based automotive object detection dataset of Gen1, our SNN significantly outperforms the previous best SNN and also surpasses sophisticated ANNs with attention mechanism, meanwhile with more concise architectures and much lower computation cost, demonstrating the potential of SNN for event-based vision.
% Through ablation studies we analyze and demonstrate the effectiveness of the design of our architecture. 
% Limited experiments are performed for the application of ALIF neuron in the network, potential future works can optimize its usage such as by combining with neural architecture search methods. 
A potential implementation of our model on neuromorphic hardware can further accelerate its inference speed, creating neuromorphic systems realizing extremely low power and low latency sensing. 
% \section{Acknowledgements}
% This is acknowledgements.
% \begin{table}[htbp!]
% % \textcolor{red}{add the details of searched model to supp}
% \label{tab:ddd17-size}
% \centering
%     \begin{tabular}{ccc}\hline
%     \textbf{Module}&\textbf{Layer}&\textbf{Feature map size $c \times h \times w$}\\
%     \hline
%     \multirow{10}{*}{\makecell[c]{\textbf{Feature layers}}}
%     &Stem0 &$ 48 \times 128\times 128$ \\
%     &Stem1 &$ 96 \times 64\times 64$ \\
%     &Cell0 & $ 96 \times 64\times 64$ \\
%     &Cell1 & $ 96 \times 64\times 64$ \\
%     &Cell2 & $ 192 \times 32\times 32$ \\
%     &Cell3 & $ 192 \times 32\times 32$ \\
%     &Cell4 & $ 192 \times 32\times 32$ \\
%     &Cell5 & $ 384 \times 16\times 16$ \\
%     &Cell6 & $ 384 \times 16\times 16$ \\
%     &Cell7 & $ 384 \times 16\times 16$ \\
%     &Cell8 & $ 768 \times 8\times 8$ \\
%     &Cell9 & $ 768 \times 8\times 8$ \\
%     \hline
%     \multirow{3}{*}{\makecell[c]{\textbf{FPN}}}
%     &Cell4${\rightarrow}$p1 &$ 96\times32\times 32$ \\
%     &Cell7${\rightarrow}$p2 &$ 192\times16\times 16$ \\
%     &Cell9${\rightarrow}$p3 &$ 384\times8\times 8$ \\
%     \hline
%     \multirow{3}{*}{\makecell[c]{\textbf{Detect}}}
%     &p1${\rightarrow}$d1 &$ K(C+5)\times32\times 32$ \\
%     &p2${\rightarrow}$d2 &$ K(C+5)\times16\times 16$ \\
%     &p3${\rightarrow}$d3 &$ K(C+5)\times8\times 8$ \\
%     \hline
%     \end{tabular}
% \caption{Detailed architecture of the hand-crafted SNN backbone. C donates number of classes. K donates number of anchors}
% \label{tab:backbone_details}
% \end{table}
%------------------------------------------------------------------------


%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{main}
}

\end{document}