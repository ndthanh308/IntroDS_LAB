\section{Preliminaries} \label{sec:preliminary}
In this section, we first present the problem of detecting OOD samples for data of multiple modalities and then review contrastive learning for multi-modal OOD detection.


\subsection{Problem Statement}
We consider the problem of detecting multi-modal OOD samples under three different scenarios, as mentioned in Section~\ref{sec:intro}. In this paper, we use vision-language modeling as a running problem for multi-modal OOD detection. With a batch of $N$ pairs of images and texts, along with their labels, denoted by $\{(x_n, t_n), y_n\}_{n=1}^N$, there is a very small number of samples, $K$, that are OOD in the three scenarios above and the remaining $N-K$ pairs are ID samples. The goal of this work is to distinguish the OOD samples from the ID ones using weakly-supervised learning.


\subsection{Contrastive Representation Learning}
Contrastive representation learning aims to encode pairs of data samples into latent representations by making similar samples close to each other and dissimilar ones far apart. One well-known method is the vision-language pre-trained model CLIP, which jointly trains an image encoder and a text encoder to learn the latent representations from text paired with images using zero-shot learning. Specifically, it encodes a pair of image $x_n$ and text $t_n$ into the latent representations $\mathcal{I}(x_n) $ and $\mathcal{T}(t_n)$, respectively, and then adopts cosine similarity to minimize the distance of their representations $\mathcal{I}(x_n)$ and $\mathcal{T}(t_n)$. Due to its excellent performance in learning the latent representations of images and texts, a few CLIP-based methods have been developed to detect multi-modal OOD samples. However, existing methods can only detect one specific scenario where a given image and its textual information come from a new domain with a distribution shift. In the following section, we develop a general-purpose OOD detection framework to identify OODs arising from several different scenarios.
