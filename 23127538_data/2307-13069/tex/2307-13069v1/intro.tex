\section{Introduction}\label{sec:intro}
% Anomaly detection aims to identify the patterns in data that do not conform to normal behavior.
Out-of-distribution (OOD) detection~\cite{yang2021generalized,bogdoll2022anomaly,ming2022poem,ruff2021unifying,ma2021comprehensive} aims at identifying whether a test sample differs from the training data. Such detection is crucial for ensuring the safety and reliability of machine learning (ML) systems~\cite{wang2020safety,hussain2018autonomous}, such like autonomous driving and AI diagnosis~\cite{davenport2019potential,han2020bridging,macdonald2022interpretable}. For instance, autonomous driving systems should have the ability to detect unknown or unusual scenes in the real-world and issue an early warning to the driver to take control of the vehicle in a timely fashion to avoid potentially fatal accidents. Recent studies have developed many OOD detection models to enhance the safety and reliability of such ML systems.



Most existing studies focus on single-modal OOD detection, while multi-modal OOD detection is less explored. In real-world applications like autonomous driving, various types of sensors, including cameras, LiDAR, and radar, are used to enhance detection accuracy and enable safe and reliable decision-making. ML systems deployed in an open world, such as autonomous driving, also often encounter different anomaly scenarios caused by multiple factors, including sensor faults, bad weather, and environmental changes. However, recent works~\cite{mingdelving,esmaeilpour2022zero} on multi-modal OOD detection primarily examine one scenario where both an image and its textual description come from a new domain. Specifically, existing studies leverage CLIP-based techniques~\cite{ming2022delving,esmaeilpour2022zero,fort2021exploring} to identify whether a query image matches one of the labels (e.g., road) in the training data. However, these approaches can only detect \textit{visual OODs} rather than those arising from both the image and its corresponding textual information. As a result, they are not applicable to detect anomaly scenarios such as incorrect pairings of an image and its \textit{detailed textual description} (e.g., road is covered with snow).


%%%%%%%
% Figure environment removed


The goal of this work is to develop a general-purpose multi-modal OOD detection model that can identify anomalies in various scenarios in a fine-grained manner. We focus on three different OOD scenarios for multi-sensory data: (1) unaligned pairs of data samples, e.g., an in-distribution (ID) image not aligned with its textual information; (2) aligned pairs of data samples collected from a new domain, e.g., aligned images and text from a new environment with a different distribution from training; (3) the presence of noise in data samples, e.g., samples coming from the same environment but with blurry images due to sensor faults. The primary question is: how can we detect OOD samples from all these OOD scenarios simultaneously? Existing OOD detection approaches, such as the CLIP-based methods~\cite{ming2022delving,esmaeilpour2022zero,fort2021exploring} and weakly-supervised classifications~\cite{ji2020multi,wang2021radar}, only focus on one of these three scenarios, thus failing to generalize to all of them. Fig.~\ref{fig:example} illustrates a motivating example of multi-modal OOD detection on CUB-200 dataset~\cite{wah2011caltech} using a CLIP-based method and a weakly-supervised classifier. We can observe from Fig.~\ref{fig:example} (a) that the CLIP-based contrastive learning can only detect OODs in scenarios 1 and 3, but is not effective in scenario 2. This is because in scenario 2 we deliberately choose the test OOD labelled with ``bird" from MS-COCO data~\cite{lin2014microsoft}, which are similar to those ID samples in the training CUB-200, thus making them more difficult for the CLIP-based method. In contrast, a binary classifier can only identify OOD samples in scenarios 2 and 3, but fails in scenario 1, as illustrated in Fig.~\ref{fig:example} (b). This observation motivates us to develop a new OOD detection model that combines both approaches to complement each other.


In this paper, we propose WOOD, a weakly-supervised multi-modal OOD detection model. As illustrated in Fig.~\ref{fig:wood}, the proposed WOOD model consists of two components: a binary classifier for classifying OOD samples and a contrastive learning module for measuring the similarity scores between multiple data modalities. On the contrastive learning side, we adopt Hinge loss to maximize similarity scores of ID samples and minimize those of OOD samples to better distinguish them. On the binary classifier side, we develop a Feature Sparsity Regularizer to better integrate important features from data of multiple modalities. Then a new scoring metric is designed to fuse the prediction results from these two components. Finally, we evaluate the proposed WOOD model on three real-world benchmark datasets. Experimental results demonstrate that our method can simultaneously detect OOD samples under three different scenarios, which significantly outperforms the CLIP-based baselines.


Contributions of this work are summarized as follows.
\begin{itemize}[noitemsep]
    \item We develop a general-purpose multi-modal OOD detection model that can detect anomalies from three different scenarios. 
    \item We design a new scoring metric that combines the prediction results from both the classifier and contrastive learning to improve detection accuracy.
    \item We adopt the Hinge loss in the contrastive objective to better maximize the difference in latent representations of ID and OOD samples.
    \item We show extensive experimental results to demonstrate that the proposed approach is able to achieve very good performance in  detecting OOD under three different scenarios simultaneously.
\end{itemize}

