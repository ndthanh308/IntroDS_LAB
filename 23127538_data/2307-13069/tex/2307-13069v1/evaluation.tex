\section{Experiments}
In this section, we carry out extensive experimentation to evaluate the performance of the proposed WOOD model on multiple benchmark datasets. Then, we conduct ablation studies to explore how main components in model design and hyperparameters impact OOD detection performance. 

\subsection{Datasets}
We implement experiments on the three real-world datasets: COCO~\cite{lin2014microsoft}, CUB-200~\cite{wah2011caltech}, and MIMIC-CXR~\cite{johnson2019mimic}. COCO and MIMIC-CXR contain images and their corresponding textual descriptions. For CUB-200, the textual information comes from literature~\cite{reed2016learning}.

\noindent
\textbf{Three OOD scenarios}. We generate three different OOD scenarios using the above datasets as follows.
\begin{itemize}[noitemsep,topsep=0.2pt]
    \item \textbf{Scenario 1}. Randomly select a subset of ID images and their textual description from a given dataset and shuffle them so that each image is not aligned with its corresponding textual information. Specifically, select images from one category and the unaligned textual descriptions from another category, ensuring that each pair of OOD image and text are not aligned.  
    \item \textbf{Scenario 2}. Choose OOD samples from another new dataset different from the training data. For instance, when conducting experiments on COCO data, select some OOD pairs of texts and images from Google Conceptual Captions (GCC)~\cite{sharma2018conceptual}.
    \item \textbf{Scenario 3}. Add some Gaussian noise to the ID images such that each image is blurry but its corresponding textual information is correct.
\end{itemize}
Table~\ref{tab:scenario} summarizes the detailed information about generating three OOD scenarios using the above datasets in the experiments.
%%%%------table---
\begin{table}[htb]
\centering
\caption{Detailed Summary of Our Three OOD Scenarios.}\label{tab:scenario}
\begin{adjustbox}{width=0.6\textwidth}
\begin{tabular}{|l|lll|}
\hline
Scenarios  & \multicolumn{1}{l|}{CUB-200}                                 & \multicolumn{1}{l|}{MIMIC-CXR}                                 & COCO                                 \\ \hline
Scenario 1 & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}}For sampled pairs of ID images and texts from \\a given dataset, swap their textual descriptions for \\different images, and then label them as OOD.\end{tabular}} \\ \hline
Scenario 2 & \multicolumn{1}{l|}{OOD from COCO-bird}    & \multicolumn{1}{l|}{ROCO~\cite{pelka2018radiology}}                                          &  GCC~\cite{sharma2018conceptual}                                   \\ \hline
Scenario 3 & \multicolumn{3}{l|}{\begin{tabular}[c]{@{}l@{}}For sampled pairs of ID images and texts, add Gaussian \\noise to each image, and then label them as OOD.\end{tabular}}     \\ \hline
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{Model Configurations}
% \shao{viet write the model parameters}
Following prior CLIP-based detectors, we also use the CLIP model (ViT-B/16~\cite{radford2021learning}) as the backbone of the contrastive learning module. The two encoders are $\text{CLIP}_{\text{image}}$ and $\text{CLIP}_{\text{text}}$, which are pre-trained Transformer models for image and text~\cite{radford2021learning} respectively. We do not change the base encoders but fine-tune them with Hinge loss in Eq. (\ref{eq:contrast}) for both feature alignment and OOD detection. Recall that WOOD also has a Feature Sparsity Regularizer module (Figure \ref{fig:wood}), which is a single projection layer (MLP) with a sigmoid activation. We set its hidden size to 512, the same as the dimensions of the output embeddings from $\text{CLIP}_{\text{image}}$ and $\text{CLIP}_{\text{text}}$.  The Binary Classifier is a 3-layer fully connected network with ReLU activation, which outputs a single probability score for binary OOD classification and the layer hidden size is 1024, 512, and 256 respectively. We train the proposed WOOD model using Adam optimizer \cite{kingma2014adam} with learning rate $1e^{-6}$ (following CLIP \cite{radford2021learning}) and stepped learning rate schedule. Additionally, the batch size is set to 128 in all of the experiments and the margin is $m=0.2$ for all the 3 datasets. Regarding the overall training objective in Eq. (\ref{eq:overall_obj}), we choose $\lambda=0.8$ for COCO and CUB-200, and $\lambda=0.2$ for MIMIC-CXR after grid search. Note that subsequent ablation studies in Section~\ref{sec:ablation} explore the impact of these two hyper-parameters on detection performance. During model training, we choose $1\%$ labeled OOD samples for each scenario in order to improve model performance. During inference, we use the same ratio ($25\%$) of test samples for ID and three OOD scenarios. Finally, following previous research~\cite{mingdelving}, we choose a threshold $\delta$ (e.g., 0.6) so that a high fraction of ID data (e.g., 95\%) is above the threshold. Then OOD samples are identified when $P_{ood}<1-\delta$.

%%--------baseline-------------------
\subsection{Baselines}
We compare the WOOD model with these baselines.
\begin{itemize}[noitemsep,topsep=0.2pt]
    \item \textbf{CLIP-BCE}~\cite{liznerski2022exposing}. This model fine-tunes the pre-trained CLIP with a BCE classifier to maximize the similarity of an image and its label for ID and minimize that for OOD samples.
    \item \textbf{MCM-OOD}~\cite{ming2022delving}. This method uses zero-shot CLIP for multi-modal OOD detection based on Maximum Concept Matching. It can only detect OODs in one type of scenario in which a given image is not aligned with its label in the training data.
    % \item \textbf{CLIP-MSP}~\cite{hendrycksbaseline}. 
    \item \textbf{CLIP-Energy}~\cite{liu2020energy}. We adopt the energy score-based CLIP method for OOD detection.
    \item \textbf{WOOD-CL}. In this method, we only use the contrastive learning part of the proposed WOOD model.
    \item \textbf{WOOD-BC}. This approach uses only the binary classifier in the proposed WOOD detector.
\end{itemize}


%%%---table 2 CUB-200--------
\begin{table*}[!tb]
\centering
\caption{Performance comparison of different methods for OOD detection on CUB-200 dataset averaged over three random seeds. Higher numbers represent better performance. }\label{tab:cub}
\begin{adjustbox}{width=\textwidth}\small
\begin{tabular}{|c|llll|llll|llll|llll|}
\hline
\multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Scenario 1+ID}                                                    & \multicolumn{4}{c|}{Scenario 2+ID}                                                    & \multicolumn{4}{c|}{Scenario 3+ID}                                                    & \multicolumn{4}{c|}{\textbf{Overall} (ID+OOD)}                                                    \\ \cline{2-17} 
 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  \\ \hline
MCM-OOD  & \multicolumn{1}{l|}{56.3} & \multicolumn{1}{l|}{6.4} & \multicolumn{1}{l|}{56.3} & 11.4 & \multicolumn{1}{l|}{79.2} & \multicolumn{1}{l|}{52.0} & \multicolumn{1}{l|}{86.4} & 64.9 & \multicolumn{1}{l|}{81.0} & \multicolumn{1}{l|}{57.1} & \multicolumn{1}{l|}{87.4} & 69.0 & \multicolumn{1}{l|}{55.6} & \multicolumn{1}{l|}{35.5} & \multicolumn{1}{l|}{93.5} & 51.4 \\ \hline
 CLIP-Energy & \multicolumn{1}{l|}{55.9} & \multicolumn{1}{l|}{5.6} & \multicolumn{1}{l|}{47.2} & 10.0 & \multicolumn{1}{l|}{81.8} & \multicolumn{1}{l|}{59.1} & \multicolumn{1}{l|}{87.8} & 70.6 & \multicolumn{1}{l|}{60.5} & \multicolumn{1}{l|}{1.9} & \multicolumn{1}{l|}{18.8} & 3.5 & \multicolumn{1}{l|}{45.8} & \multicolumn{1}{l|}{20.6} & \multicolumn{1}{l|}{89.3} & 33.5 \\ \hline
  CLIP-BCE & \multicolumn{1}{l|}{55.4} &  \multicolumn{1}{l|}{4.4} & \multicolumn{1}{l|}{41.5} & 7.9 & \multicolumn{1}{l|}{65.4} & \multicolumn{1}{l|}{15.0} & \multicolumn{1}{l|}{64.7} & 24.3 & \multicolumn{1}{l|}{84.5} & \multicolumn{1}{l|}{66.3} & \multicolumn{1}{l|}{89.5} & 76.0 & \multicolumn{1}{l|}{49.5} & \multicolumn{1}{l|}{26.2} & \multicolumn{1}{l|}{91.4} & 40.8 \\ \hline
 WOOD-BC & \multicolumn{1}{l|}{56.6} & \multicolumn{1}{l|}{7.3} & \multicolumn{1}{l|}{52.6} & 30.9 & \multicolumn{1}{l|}{\textbf{96.8}} & \multicolumn{1}{l|}{\textbf{99.5}} & \multicolumn{1}{l|}{\textbf{92.4}} & \textbf{95.9} & \multicolumn{1}{l|}{\textbf{97.0}} & \multicolumn{1}{l|}{\textbf{100}} & \multicolumn{1}{l|}{\textbf{92.5}} & \textbf{96.1} & \multicolumn{1}{l|}{73.9} & \multicolumn{1}{l|}{63.0} & \multicolumn{1}{l|}{96.2} & 75.5 \\ \hline
 WOOD-CL & \multicolumn{1}{l|}{79.6} & \multicolumn{1}{l|}{59.6} & \multicolumn{1}{l|}{90.6} & 71.9 & \multicolumn{1}{l|}{67.2} & \multicolumn{1}{l|}{20.0} & \multicolumn{1}{l|}{69.2} & 53.1 & \multicolumn{1}{l|}{96.9} & \multicolumn{1}{l|}{99.9} & \multicolumn{1}{l|}{92.4} & 96.0 & \multicolumn{1}{l|}{71.7} & \multicolumn{1}{l|}{59.8} & \multicolumn{1}{l|}{96.0} & 73.7 \\ \hline

Ours  & \multicolumn{1}{l|}{\textbf{80.3}} & \multicolumn{1}{l|}{\textbf{61.3}} & \multicolumn{1}{l|}{\textbf{90.8}} & \textbf{73.2} & \multicolumn{1}{l|}{96.2} & \multicolumn{1}{l|}{97.8} & \multicolumn{1}{l|}{92.3} & 95.0 & \multicolumn{1}{l|}{\textbf{97.0}} & \multicolumn{1}{l|}{98.7} & \multicolumn{1}{l|}{92.5} & 96.0 & \multicolumn{1}{l|}{\textbf{86.8}} & \multicolumn{1}{l|}{\textbf{82.5}} & \multicolumn{1}{l|}{\textbf{97.1}} & \textbf{89.2} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table*}

%%%---data: MIMIC-CXR---
\begin{table*}[!tb]
\centering
\caption{Performance comparison of different methods for OOD detection on MIMIC-CXR dataset averaged over three random seeds. Higher numbers mean better performance.}\label{tab:MIMIC}
\begin{adjustbox}{width=\textwidth}\small
\begin{tabular}{|c|llll|llll|llll|llll|}
\hline
\multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Scenario 1+ID}                                                    & \multicolumn{4}{c|}{Scenario 2+ID}                                                    & \multicolumn{4}{c|}{Scenario 3+ID}                                                    & \multicolumn{4}{c|}{\textbf{Overall} (ID+OOD)}                                                    \\ \cline{2-17} 
 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  \\ \hline

MCM-OOD  & \multicolumn{1}{l|}{55.8} & \multicolumn{1}{l|}{4.6} & \multicolumn{1}{l|}{43.7} & 8.9 & \multicolumn{1}{l|}{64.7} & \multicolumn{1}{l|}{13.4} & \multicolumn{1}{l|}{61.5} & 22.0 & \multicolumn{1}{l|}{65.2} & \multicolumn{1}{l|}{14.6} & \multicolumn{1}{l|}{63.8} & 23.8 & \multicolumn{1}{l|}{39.1} & \multicolumn{1}{l|}{10.4} & \multicolumn{1}{l|}{80.6} & 18.5 \\ \hline
 CLIP-Energy & \multicolumn{1}{l|}{56.6} & \multicolumn{1}{l|}{6.8} & \multicolumn{1}{l|}{51.7} & 12.1 & \multicolumn{1}{l|}{93.2} & \multicolumn{1}{l|}{89.9} & \multicolumn{1}{l|}{91.5} & 90.7 & \multicolumn{1}{l|}{65.4} & \multicolumn{1}{l|}{15.3} & \multicolumn{1}{l|}{91.5} & 24.7 & \multicolumn{1}{l|}{55.0} & \multicolumn{1}{l|}{34.5} & \multicolumn{1}{l|}{93.2} & 50.3 \\ \hline
CLIP-BCE & \multicolumn{1}{l|}{56.7} &  \multicolumn{1}{l|}{7.1} & \multicolumn{1}{l|}{52.3} & 12.5 & \multicolumn{1}{l|}{87.12} & \multicolumn{1}{l|}{73.6} & \multicolumn{1}{l|}{89.9} & 81.0 & \multicolumn{1}{l|}{93.7} & \multicolumn{1}{l|}{95.4} & \multicolumn{1}{l|}{92.0} & 93.7 & \multicolumn{1}{l|}{67.8} & \multicolumn{1}{l|}{53.9} & \multicolumn{1}{l|}{95.6} &  68.9\\ \hline
 WOOD-BC & \multicolumn{1}{l|}{57.8} & \multicolumn{1}{l|}{9.6} & \multicolumn{1}{l|}{60.1} & 16.6 & \multicolumn{1}{l|}{95.7} & \multicolumn{1}{l|}{93.9} & \multicolumn{1}{l|}{\textbf{94.8}} & 94.0 & \multicolumn{1}{l|}{96.9} & \multicolumn{1}{l|}{100} & \multicolumn{1}{l|}{92.3} & 96.0 & \multicolumn{1}{l|}{74.4} & \multicolumn{1}{l|}{63.9} & \multicolumn{1}{l|}{96.2} & 76.8 \\ \hline
 WOOD-CL & \multicolumn{1}{l|}{81.1} & \multicolumn{1}{l|}{63.1} & \multicolumn{1}{l|}{90.9} & 74.4 & \multicolumn{1}{l|}{91.6} & \multicolumn{1}{l|}{85.7} & \multicolumn{1}{l|}{91.1} & 88.2 & \multicolumn{1}{l|}{96.9} & \multicolumn{1}{l|}{99.9} & \multicolumn{1}{l|}{92.3} & 96.0 & \multicolumn{1}{l|}{85.8} & \multicolumn{1}{l|}{81.0} & \multicolumn{1}{l|}{97.0} & 88.4 \\ \hline
%%%--
Ours  & \multicolumn{1}{l|}{\textbf{81.6}} & \multicolumn{1}{l|}{\textbf{64.4}} & \multicolumn{1}{l|}{\textbf{91.0}} & \textbf{75.3} & \multicolumn{1}{l|}{\textbf{96.6}} & \multicolumn{1}{l|}{\textbf{99.1}} & \multicolumn{1}{l|}{92.3} & \textbf{95.4} & \multicolumn{1}{l|}{\textbf{96.9}} & \multicolumn{1}{l|}{\textbf{100}} & \multicolumn{1}{l|}{\textbf{92.3}} & \textbf{96.0} & \multicolumn{1}{l|}{\textbf{88.8}} & \multicolumn{1}{l|}{\textbf{85.6}} & \multicolumn{1}{l|}{\textbf{97.2}} & \textbf{91.0} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table*}



%%---table COCO dataset----
\begin{table*}[!tb]
\centering
\caption{Performance comparison of different methods for OOD detection on COCO dataset averaged over three random seeds. Higher numbers represent better performance. }\label{tab:coco}
\begin{adjustbox}{width=\textwidth}\small
\begin{tabular}{|c|llll|llll|llll|llll|}
\hline
\multirow{2}{*}{Methods} & \multicolumn{4}{c|}{Scenario 1+ID}                                                    & \multicolumn{4}{c|}{Scenario 2+ID}                                                    & \multicolumn{4}{c|}{Scenario 3+ID}                                                    & \multicolumn{4}{c|}{\textbf{Overall} (ID+OOD)}                                                    \\ \cline{2-17} 
 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1 & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  & \multicolumn{1}{c|}{Accy} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & F1  \\ \hline
MCM-OOD  & \multicolumn{1}{l|}{40.2} & \multicolumn{1}{l|}{4.6} & \multicolumn{1}{l|}{56.7} & 8.3 & \multicolumn{1}{l|}{61.4} & \multicolumn{1}{l|}{22.0} & \multicolumn{1}{l|}{77.6} & 34.3 & \multicolumn{1}{l|}{59.5} & \multicolumn{1}{l|}{73.7} & \multicolumn{1}{l|}{17.9} & 28.8 & \multicolumn{1}{l|}{31.6} & \multicolumn{1}{l|}{11.9} & \multicolumn{1}{l|}{88.4} & 20.9 \\ \hline
 CLIP-Energy & \multicolumn{1}{l|}{40.6} & \multicolumn{1}{l|}{5.0} & \multicolumn{1}{l|}{60.4} & 9.2 & \multicolumn{1}{l|}{60.9} & \multicolumn{1}{l|}{20.4} & \multicolumn{1}{l|}{77.4} & 32.3 & \multicolumn{1}{l|}{51.5} & \multicolumn{1}{l|}{0.2} & \multicolumn{1}{l|}{3.5} & 0.4 & \multicolumn{1}{l|}{28.5} & \multicolumn{1}{l|}{7.8} & \multicolumn{1}{l|}{83.3} & 14.2 \\ \hline
  CLIP-BCE & \multicolumn{1}{l|}{40.2} &  \multicolumn{1}{l|}{4.3} & \multicolumn{1}{l|}{56.4} & 7.9 & \multicolumn{1}{l|}{76.4} & \multicolumn{1}{l|}{54.2} & \multicolumn{1}{l|}{85.3} & 61.2 & \multicolumn{1}{l|}{96.7} & \multicolumn{1}{l|}{98.7} & \multicolumn{1}{l|}{94.4} & 96.5 & \multicolumn{1}{l|}{54.7} & \multicolumn{1}{l|}{42.1} & \multicolumn{1}{l|}{96.5} & 58.1 \\ \hline
 WOOD-BC & \multicolumn{1}{l|}{40.7} & \multicolumn{1}{l|}{5.1} & \multicolumn{1}{l|}{60.8} & 9.4 & \multicolumn{1}{l|}{96.8} & \multicolumn{1}{l|}{98.9} & \multicolumn{1}{l|}{94.3} & 97.3 & \multicolumn{1}{l|}{97.3} & \multicolumn{1}{l|}{100} & \multicolumn{1}{l|}{94.4} & 97.1 & \multicolumn{1}{l|}{64.2} & \multicolumn{1}{l|}{54.6} & \multicolumn{1}{l|}{97.2} & 69.9 \\ \hline
 WOOD-CL & \multicolumn{1}{l|}{\textbf{97.0}} & \multicolumn{1}{l|}{\textbf{98.4}} & \multicolumn{1}{l|}{\textbf{96.8}} & \textbf{97.6} & \multicolumn{1}{l|}{93.2} & \multicolumn{1}{l|}{91.0} & \multicolumn{1}{l|}{93.9} & 92.4 & \multicolumn{1}{l|}{97.2} & \multicolumn{1}{l|}{99.9} & \multicolumn{1}{l|}{94.4} & 97.1 & \multicolumn{1}{l|}{96.4} & \multicolumn{1}{l|}{96.8} & \multicolumn{1}{l|}{98.4} & 97.6 \\ \hline
Ours  & \multicolumn{1}{l|}{96.7} & \multicolumn{1}{l|}{97.8} & \multicolumn{1}{l|}{96.8} & 97.3 & \multicolumn{1}{l|}{\textbf{96.8}} & \multicolumn{1}{l|}{\textbf{98.9}} & \multicolumn{1}{l|}{\textbf{94.4}} & \textbf{96.6} & \multicolumn{1}{l|}{\textbf{97.3}} & \multicolumn{1}{l|}{\textbf{100}} & \multicolumn{1}{l|}{\textbf{94.4}} & \textbf{97.1} & \multicolumn{1}{l|}{\textbf{97.8}} & \multicolumn{1}{l|}{\textbf{98.7}} & \multicolumn{1}{l|}{\textbf{98.5}} & \textbf{98.6} \\ \hline
\end{tabular}
\end{adjustbox}
\end{table*}

%%---table COCO dataset----
%%%%---main result---
\subsection{Main Results}
In this subsection, we analyze the experiments to thoroughly evaluate the detection performance of WOOD. Four commonly used metrics - accuracy, recall, precision, and F1 score - are used to measure the prediction results. 

%%%----
We first evaluate the performance of the proposed WOOD on CUB-200 dataset. Table~\ref{tab:cub} illustrates the comparison results of different OOD detection methods using three random seeds. We can observe from this table that our method is effective in all three OOD scenarios, and its overall performance significantly outperforms the baselines. The reason why WOOD outperforms the baselines is that the binary classifier and the contrastive learning module complement each other for OOD detection. It can be seen that WOOD-CL does not effectively detect OOD in scenario 2 while WOOD-BC can identify OOD with high accuracy. In addition, MCM-OOD does not perform well since it is only designed for detecting visual OOD by querying an image to check whether the returned label from CLIP belongs to training labels. In contrast, our method can detect both visual and textual OODs. Note that the recall for scenario 1 is not very high, since the test samples have very similar images as those in the training data, with the same label ``bird'' but only slightly different textual descriptions. As a result, it is very challenging to identify a pair of OOD samples when an image is only slightly unaligned with its textual information. 

Next, we also show that WOOD can identify OOD samples in multi-modal medical data, MIMIC-CXR. As illustrated in Table~\ref{tab:MIMIC}, it can be observed that the proposed method effectively detects OODs in all three OOD scenarios and its overall performance is better than the baselines. Similar to CUB-200 dataset, the recall of scenario 1 is not very high, since the test images have the same class label ``chest scans'' as those in the training data and only have slightly different textual descriptions. 

% % Figure environment removed

%%%%%%
% Figure environment removed

%%----
Besides, we apply the proposed WOOD to detect OOD samples on COCO datasets. We compare the detection performance of different methods as shown in Table~\ref{tab:coco}. It can be seen that our method is able to detect OODs in all three OOD scenarios simultaneously while the baseline methods can only detect one or two types of OODs.

%%---AUROC---------
Finally, we also compare the AUROC of our method with the baselines on three datasets, as illustrated in Fig.~\ref{fig:auroc}. It can be observed that the proposed WOOD consistently outperforms the baselines in term of AUROC. Based on the above experimental results, we can conclude that WOOD can detect OOD samples arising from different OOD scenarios simultaneously with high accuracy scores.


\subsection{Ablation Study}\label{sec:ablation}
In this section, we conduct ablation studies to investigate the effect of the weight in the objective function, as well as the Hinge loss and its hyperparameter on the performance of OOD detection.


\noindent
\textbf{Effect of Weight in the Objective.} We study the effect of the weight in the objective function ($\lambda$ in Eq. (\ref{eq:overall_obj})) on the performance of OOD detection. We can see from Table~\ref{tab:abs_weight} that the proposed WOOD model has the best performance when $\lambda=0.8$ as it increases from $0.2$ to $1$ on CUB-200 and COCO datasets. Conversely, it performs best when $\lambda=0.2$ on MIMIC-CXR. One possible reason is that the textual description for each image contains about 40 words on average in MIMIC-CXR, which needs more weights in contrastive learning to learn better representations for images and texts.


%%%---hinge loss----
\noindent
\textbf{Effect of Hinge Loss and its Hyperparameter.} We also explore the impact of Hinge loss (in Eq.~\eqref{eq:contrast_id} and Eq.~\eqref{eq:contrast_ood}) on the detection performance. Table~\ref{tab:abs_margin} shows the detection performance under different margin parameters. When margin $m=0$, it means that the proposed WOOD model does not use Hinge loss. We can observe from Table~\ref{tab:abs_margin} that our method has the best performance when $m=0.2$. In addition, our detection method without the Hinge loss does not perform well (when $m=0$). Therefore, we can conclude that this Hinge loss plays an important role in multi-modal OOD detection. 

% \noindent
% \textbf{Effect of Margin Hyperparameter.} We further study the effect of margin value on the detection accuracy.

% \noindent
% \textbf{Effect of Different Components}\viet{Included in baselines?}

% \noindent 

%%%------weight------------
\begin{table*}[!tb]
\centering
\caption{Impact of the weight in the objective on detection performance. We report the results averaged over three random seeds. It can be observed that when $\lambda=0.8$, the proposed model has the best performance on CUB-200 and COCO datasets. For MIMIC-CXR data, it performs best as $\lambda=0.2$. }\label{tab:abs_weight}
\begin{tabular}{|c|cccccccccccc|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{}} & \multicolumn{12}{c|}{Dataset}                                                                                                                                                                                                                                                                                                                        \\ \cline{2-13} 
\multicolumn{1}{|l|}{}                  & \multicolumn{4}{c|}{CUB-200}                                                                                     & \multicolumn{4}{c|}{COCO}                                                                                        & \multicolumn{4}{c|}{MIMIC-CXR}                                                                                 \\ \hline
$\lambda$                               & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1}   & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1}   & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1} \\ \hline
0.2                                     & \multicolumn{1}{c|}{85.6} & \multicolumn{1}{c|}{80.4}   & \multicolumn{1}{c|}{96.8}  & \multicolumn{1}{c|}{87.8} & \multicolumn{1}{c|}{94.3} & \multicolumn{1}{c|}{94.1}   & \multicolumn{1}{c|}{98.4}  & \multicolumn{1}{c|}{96.1} & \multicolumn{1}{c|}{88.7} & \multicolumn{1}{c|}{85.4}   & \multicolumn{1}{c|}{97.1}  & 90.9                    \\ \hline
0.4                                     & \multicolumn{1}{c|}{86.4} & \multicolumn{1}{c|}{82.0}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{88.9} & \multicolumn{1}{c|}{97.0} & \multicolumn{1}{c|}{97.6}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.0} & \multicolumn{1}{c|}{87.9} & \multicolumn{1}{c|}{84.2}   & \multicolumn{1}{c|}{97.1}  & 90.2                    \\ \hline
0.6                                     & \multicolumn{1}{c|}{86.9} & \multicolumn{1}{c|}{82.8}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{89.4} & \multicolumn{1}{c|}{97.9} & \multicolumn{1}{c|}{98.8}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.7} & \multicolumn{1}{c|}{86.7} & \multicolumn{1}{c|}{82.5}   & \multicolumn{1}{c|}{97.0}  & 89.2                    \\ \hline
0.8                                     & \multicolumn{1}{c|}{87.0} & \multicolumn{1}{c|}{82.9}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{89.4} & \multicolumn{1}{c|}{97.9} & \multicolumn{1}{c|}{98.8}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.6} & \multicolumn{1}{c|}{85.6} & \multicolumn{1}{c|}{80.7}   & \multicolumn{1}{c|}{97.0}  & 88.1                    \\ \hline
1.0                                     & \multicolumn{1}{c|}{86.8} & \multicolumn{1}{c|}{82.5}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{89.2} & \multicolumn{1}{c|}{97.8} & \multicolumn{1}{c|}{98.7}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.6} & \multicolumn{1}{c|}{82.7} & \multicolumn{1}{c|}{75.1}   & \multicolumn{1}{c|}{98.5}  & 85.2                    \\ \hline
\end{tabular}
\end{table*}


%%%%---hinge loss---
\begin{table*}[!tb]
\centering
\caption{Impact of Hinge loss in the objective on detection performance. We report the averaged results from three random seeds. It can be seen that the proposed WOOD performs very well when the margin $m=0.2$.}\label{tab:abs_margin}
\begin{tabular}{|c|cccccccccccc|}
\hline
\multicolumn{1}{|l|}{\multirow{2}{*}{}} & \multicolumn{12}{c|}{Dataset}                                                                                                                                                                                                                                                                                                                        \\ \cline{2-13} 
\multicolumn{1}{|l|}{}                  & \multicolumn{4}{c|}{CUB-200}                                                                                     & \multicolumn{4}{c|}{COCO}                                                                                        & \multicolumn{4}{c|}{MIMIC-CXR}                                                                                 \\ \hline
$\mathcal{L}_{cl}$                      & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1}   & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1}   & \multicolumn{1}{c|}{Acc.} & \multicolumn{1}{c|}{Recall} & \multicolumn{1}{c|}{Prec.} & \multicolumn{1}{l|}{F1} \\ \hline
$m=0$                                   & \multicolumn{1}{c|}{71.4} & \multicolumn{1}{c|}{58.5}   & \multicolumn{1}{c|}{95.8}  & \multicolumn{1}{c|}{72.5} & \multicolumn{1}{c|}{65.6}     & \multicolumn{1}{c|}{56.8}       & \multicolumn{1}{c|}{97.4}      & \multicolumn{1}{c|}{71.8}     & \multicolumn{1}{c|}{74.5}   & \multicolumn{1}{c|}{64.0}       & \multicolumn{1}{c|}{96.2}     &  76.9                       \\ \hline
$m=0.1$                                 & \multicolumn{1}{c|}{86.2} & \multicolumn{1}{c|}{81.6}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{88.7} & \multicolumn{1}{c|}{97.5} & \multicolumn{1}{c|}{98.2}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.3} & \multicolumn{1}{c|}{89.3} & \multicolumn{1}{c|}{85.7}   & \multicolumn{1}{c|}{97.2}  & 89.4                    \\ \hline
$m=0.2$                                 & \multicolumn{1}{c|}{86.9} & \multicolumn{1}{c|}{82.8}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{89.4} & \multicolumn{1}{c|}{97.8} & \multicolumn{1}{c|}{98.7}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.6} & \multicolumn{1}{c|}{88.8} & \multicolumn{1}{c|}{85.4}   & \multicolumn{1}{c|}{97.1}  & 89.2                    \\ \hline
$m=0.3$                                 & \multicolumn{1}{c|}{86.1} & \multicolumn{1}{c|}{81.5}   & \multicolumn{1}{c|}{97.1}  & \multicolumn{1}{c|}{88.6} & \multicolumn{1}{c|}{97.7} & \multicolumn{1}{c|}{98.6}   & \multicolumn{1}{c|}{98.5}  & \multicolumn{1}{c|}{98.5} & \multicolumn{1}{c|}{88.0} & \multicolumn{1}{c|}{84.3}   & \multicolumn{1}{c|}{97.1}  & 88.7                    \\ \hline
$m=0.4$                                 & \multicolumn{1}{c|}{85.3} & \multicolumn{1}{c|}{80.3}   & \multicolumn{1}{c|}{97.0}  & \multicolumn{1}{c|}{87.9} & \multicolumn{1}{c|}{95.9} & \multicolumn{1}{c|}{96.2}   & \multicolumn{1}{c|}{98.4}  & \multicolumn{1}{c|}{97.3} & \multicolumn{1}{c|}{87.1} & \multicolumn{1}{c|}{83.1}   & \multicolumn{1}{c|}{97.1}  & 87.7                    \\ \hline
\end{tabular}
\end{table*}
