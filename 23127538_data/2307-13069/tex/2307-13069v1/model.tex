\section{Proposed Model}
In this section, we describe the proposed multi-modal OOD detection framework and propose a new scoring metric for detecting OODs under three different scenarios.

% Figure environment removed
%%---------proposed method---
% \subsection{Proposed WOOD} \wu{constrative -- contrastive}
We propose a weakly-supervised OOD detector, called WOOD, that combines a classifier and contrastive learning to simultaneously detect three different OOD scenarios mentioned in Section~\ref{sec:intro}. Fig.~\ref{fig:wood} illustrates the overall multi-modal OOD detection framework that consists of two components: a contrastive learning module and a binary classifier. The core idea is to use contrastive learning to learn the representations of different data modalities by enforcing the similarity scores of ID pairs to be higher than those of OOD samples. Then WOOD combines the similarity scores from contrastive learning and the prediction results from a binary classifier to identify OOD samples. Below, we detail the two main components in the proposed method.\\

\noindent
\textbf{Contrastive Learning with Hinge Loss}. Inspired by CLIP-based OOD detection methods, we adopt an image encoder and a text encoder to learn the representations of  input pairs (image and text) using contrastive learning. However, unlike existing zero-shot CLIP-based detectors, we add a small number of OOD samples to better separate the representations of ID and OOD samples. Thus, our goal is to maximize the cosine similarity of representations learned from ID samples but minimize those of OOD samples. Let ($x_n$, $t_n$) be a pair of input image and text, and their corresponding representations denote $(\mathcal{I}(x_n), \mathcal{T}(t_n))$. Then, we use the following contrastive loss to minimize the cosine similarity of labeled OOD pairs and maximize that of ID pairs.
\begin{equation}
    \min \mathcal{L}_{1} = -\sum_{n=1}^{N-K} S_{id}(x_n^+,t_n^+) + \sum_{k=1}^{K}  S_{ood}(x_k^-,t_k^-),
\end{equation}
% \begin{equation}
%     \min \mathcal{L}_{1} = \sum_{n=1}^{N-K} S_{id}(x_n^+,t_n^+) - \sum_{k=1}^{K}  S_{ood}(x_k^-,t_k^-),
% \end{equation}

where $S_{id}(x_n^+,t_n^+)=\frac{\mathcal{I}(x_n^+).\mathcal{T}(t_n^+)}{\|\mathcal{I}(x_n^+)\|\|\mathcal{I}(t_n^+)\|}$ and $S_{ood}(x_k^-,t_k^-)=\frac{\mathcal{I}(x_k^-).\mathcal{T}(t_k^-)}{\|\mathcal{I}(x_k^-)\|\|\mathcal{I}(t_k^-)\|}$ represent the cosine similarity between image and text features for ID and OOD pairs, respectively. In addition, $N-K$ and $K$ respectively denote the number of ID and OOD pairs.


In order to further maximize the difference in latent representations between ID and OOD samples, we adopt  Hinge loss to constrain their cosine similarity. In this work, we consider Hinge loss for both ID pairs and labeled OOD pairs in the objective function. 

First, we introduce  Hinge loss for $N-K$ ID samples, as shown in the upper part of the similarity matrix in Fig~\ref{fig:wood}. It is given by
%%%
\begin{equation}\footnotesize \label{eq:contrast_id}
    \mathcal{L}_{id} = \sum_{n=1}^{N-K} \left(\frac{1}{N}\sum_{i=1, i \neq n}^N \max \left(0,m - S_{id}(x_n^+, t_n^+) + S_{id}(x_n^+, t_i^-) \right)\right),
\end{equation}
% \begin{equation}\small
%     \mathcal{L}_{id} = \sum_{n=1}^{N-K}\left(\frac{1}{N}\sum_{i=1, i \neq n}^N\left(m - S_{id}(x_n^+, t_n^+) + S_{id}(x_n^+, t_i^-) \right)_+\right),
% \end{equation}
where $m$ is a margin, $S_{id}(x_n^+,t_n^+)$ represents the cosine similarity of $N-K$  aligned ID pairs and $S_{id}(x_n^+,t_i^-)$ represents the cosine similarity between $N-K$ ID images and all $N$ texts (including OOD samples), where each text either does not align with its corresponding ID image or belongs to an OOD sample. In short, the objective, $\mathcal{L}_{id}$, aims to maximize the difference between the aligned ID pairs and incorrect pairings by ensuring that such difference is larger than a margin $m$.

Second, we introduce Hinge loss for $K$ OOD samples for constraining their cosine similarity to a small value. Hence, we have
\begin{equation}\label{eq:contrast_ood}
    \mathcal{L}_{ood} = \sum_{k=1}^{K}\left(\frac{1}{N}\sum_{i=1}^N \max \left(0,-m + S_{ood}(x_k^-, t_i^-)\right)\right),
\end{equation}
where $S_{ood}(x_k^-,t_i^-)$ represents the cosine similarity between each of $K$ OOD images and all $N$ texts (including OOD samples). By definition, the $K$ OOD images should not align with any of the $N-K$ ID texts. Additionally, each OOD image should not be aligned with its corresponding text as described in scenario 1.

By combining the above loss functions, $\mathcal{L}_{id}$ and $\mathcal{L}_{ood}$, for ID and OOD samples, the overall contrastive loss is given by:
\begin{equation}\label{eq:contrast}
    \mathcal{L}_{cl} = \frac{1}{N}\left({L}_{id}+ {L}_{ood}\right).
\end{equation}


% Given $N$ multimodal samples with visual-text modalities and corresponding OOD labels $\{\{x_n^i, x_n^t\}, y_n\}_{n=1}^N$, we the goal of our multi-modal OOD detector is to construct a mapping between visual-text embeddings obtained from a pre-trained CLIP model $\{\{\mathcal{I}(x_n^i), \mathcal{T}(x_n^t)\}\}_{n=1}^N$ and a scoring function $S(\cdot)$ such that sample $n$ is classified as OOD if $S_n$ is below a decision threshold $\theta$, and ID otherwise. Specifically, to integrate multimodal information and learn the underlying mapping between multimodal data $\{x_n^i, x_n^t\}_{n=1}^N$ and OOD labels, unlike previous models which apply scoring metrics directly on modality-level alignments of pre-trained features $\mathcal{H}_n = \text{similarity}(\mathcal{I}_n, \mathcal{T}_n)$ from CLIP in the zero-shot manner or after fine-tuning, we additionally train a conventional binary classifier $f: \{\mathcal{I}(x^i), \mathcal{T}(x^t)\}\rightarrow y$ to incorporate feature-level OOD information into our scoring function.

% \subsection{Contrastive Representation Learning}
% For better OOD detection performance, the joint representation space $\mathcal{H}$ is supposed to minimize the overlap of the representations of ID and OOD data. Specifically, as we define $\mathcal{H}$ to be the degree of multimodal feature alignment, OOD samples are expected to have lower similarity between corresponding visual-language features than ID samples. In a supervised setting where both ID and OOD data are available in training, it would be easy to obtain such $\mathcal{H}$. However, 
\noindent
\textbf{Binary Classifier}. We further adopt weakly-supervised learning to classify OOD samples since recent studies have illustrated that it can significantly outperform unsupervised learning methods by adding some OOD samples~\cite{tian2020few,sultani2018real,majhi2021weakly}. However, the challenge lies in how to integrate image and text features for improved classification accuracy. One naive method is to concatenate their embeddings directly and then feed them into a classifier. But this simple fusion approach does not perform well since the informativeness of different features may vary for different samples. Motivated by prior work~\cite{han2022multimodal}, we develop a feature sparsity regularizer to select and integrate important features from data of the two modalities, as illustrated in Fig.~\ref{fig:wood}(b). Specifically, we train encoder networks $E^{\mathcal{I}}: \mathcal{I}(x_n)\rightarrow w_n^{\mathcal{I}}$ and $E^{\mathcal{T}}: \mathcal{T}(t_n)\rightarrow w_n^{\mathcal{T}}$ to use features obtained from image encoder and text encoders,  These features are updated by a sigmoid activation $\sigma$ to assign higher weights to informative features and lower weights to the uninformative features. Namely, the weight vectors $\sigma(w_n^{\mathcal{I}})$ and $\sigma(w_n^{\mathcal{T}})$ are multiplied with $\mathcal{I}(x_n)$ and $\mathcal{T}(t_n)$ respectively. After that, we fuse the features from two modalities by concatenation, yielding $h_n=\oplus[\mathcal{I}(x_n)\otimes\sigma(w_n^{\mathcal{I}}), \mathcal{T}(t_n)\otimes\sigma(w_n^{\mathcal{T}})]$.
Next, we adopt a binary classifier to identify ID/OOD samples based on the fused features $h_n$. In order to introduce sparsity in the weight vectors, we use $L_1$ normalization $\|\sigma(w_n^{\mathcal{I}})\|_1$ and $\|\sigma(w_n^{\mathcal{T}})\|_1$ and add them to our binary cross entropy (BCE) loss function, which is given by:
\begin{equation} \label{eq:binary_classifier_loss}
    \mathcal{L}_{bc} = \frac{1}{N}\left(\sum_{i=1}^N \text{BCE}(y_i, \hat{y}_i)+ \|\sigma(w_n^{\mathcal{I}})\|_1 + \|\sigma(w_n^{\mathcal{T}})\|_1 \right)
\end{equation}
where $\text{BCE}(y_i, \hat{y_i})=-\left(y_i\log(\hat{y}_i)+(1-y_i)\log(1-\hat{y}_i)\right)$. Note that here $y_i=0$ means the $i$-th test sample is OOD while it is an ID sample when $y_i=1$.\\

\noindent
\textbf{Overall objective}. Finally, we jointly train the contrastive learning and binary classifier for OOD detection. The overall objective for multi-OOD detection is given by
\begin{equation}\label{eq:overall_obj}
    \mathcal{L} = \mathcal{L}_{cl} + \lambda \mathcal{L}_{bc},
\end{equation}
where $\mathcal{L}_{bc}$ is the binary cross-entropy loss for the classifier, and $\lambda$ is the weight for balancing the two terms.

\subsection{New Scoring Metric}
To improve the performance of OOD detection, we introduce a novel scoring metric that combines predictions from the contrastive learning and the binary classifier. The key insight is that we identify an image and text pair as ID only when both the contrastive learning and binary classifier predict that sample as ID. In all other cases, we identify it as OOD.  We codify this condition with the following scoring metric for identifying OOD samples:
% \begin{equation}
%     P_{id} = P_{bc}P_{cl}
% \end{equation}
% Namely, the scoring metric for detecting OOD samples is
\begin{equation}\label{eq:score}
    P_{ood} =1-P_{bc}P_{cl},
\end{equation}
where $P_{bc}$ and $P_{cl}$ denote the prediction results from the binary classifier and contrastive learning, respectively.  Subsequent sections demonstrate that the new scoring metric can help detect OOD samples in all three OOD scenarios effectively.

\subsection{Summary of Proposed Model}
We summarize the proposed WOOD model in Algorithm~\ref{alg:wood}. The basic idea is to map ID and OOD pairs (image and text) into latent representations, and then calculate their cosine similarity. Then, we use Hinge loss to maximize the difference in similarity scores between ID and OOD samples. Moreover, we feed the fused latent representations into a binary classifier to classify ID or OOD samples. Finally, we jointly train the binary classifier and the contrastive learning component for OOD detection.
%%
\begin{algorithm}[!thb]
\caption{The Proposed WOOD Model}
\label{alg:wood}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} A batch of $N$ pairs of images and texts, with $N-K$ ID pairs and $K$ labeled OOD pairs.
\STATE {\bfseries Output:} OOD or ID samples.
\STATE {Encode ID pairs into $\mathcal{I}(x_n^+)$ and $\mathcal{T}(t_n^+)$.} 
\STATE {Encode OOD pairs into $\mathcal{I}(x_n^-)$ and $\mathcal{T}(t_n^-)$.} 
\STATE {Compute Hinge loss on cosine similarity for ID samples in Eq.~\eqref{eq:contrast_id} and OOD samples in Eq.~\eqref{eq:contrast_ood} in contrastive learning.}
\STATE {Compute the total contrastive loss $\mathcal{L}_{cl}$ in Eq.~\eqref{eq:contrast}.}
\STATE {Compute loss of the binary classifier $\mathcal{L}_{bc}$ in Eq.~\eqref{eq:binary_classifier_loss}.}
\STATE {Jointly train the binary classifier and contrastive learning based on the overall objective in Eq.~\eqref{eq:overall_obj}.}
\STATE {Identify OOD samples based on the designed scoring metric in Eq. ~\eqref{eq:score}.}
\end{algorithmic}
\end{algorithm}
