
\section{Background and Related Work}
\label{sec:background}

This section first positions the proposed on-the-fly formulation in context with existing optimisation techniques for CNNs. We continue with a brief historical context for OVSF codes and how to construct them. Then, we show how orthogonal variable spreading factor (OVSF) codes are used to construct filters in CNNs and, how they are integrated into the training process. This section concludes with an overview of the challenges and opportunities of CNN-targeting single computation engines, which is our paradigm of choice when implementing on-the-fly models on FPGAs.

\subsection{Designing Lightweight Convolutional Neural Networks}

The plethora of existing techniques to modify CNNs for faster inference can be categorised into: pruning~\cite{luo2017thinet, blalock2020state, he2018soft}, which removes redundant parameters; % and often leads to fewer operations; 
quantisation~\cite{krishnamoorthi2018quantizing,jacob2018quantization,dong2019hawq,fernandezmarques2020searching,alizadeh2018a}, which results in compact low-precision models; or, sparsification~\cite{wen2016learning, sparsify2016sensys, gale2020sparse}, which leverages compressed data formats. In addition, a number of frameworks combine several of these techniques. Most notably: Deep Compression~\cite{deepcompression2015iclr} which, given an over-parametrised model, 
applies pruning, quantisation and Huffman encoding;  RedCNN~\cite{pmlr-v70-wang17m} which prunes channels based on an activation overlap metric; %, reducing latency and memory usage; 
and, more recently, APQ~\cite{wang2020apq}, which designs a CNN that meets given computational, memory and latency constraints through a joint optimisation formulation. 

\subsection{On-the-Fly Convolutional Neural Networks}
\label{sec:background_cnn_ovsf}
Orthogonal to these methods, various works have explored ways of obtaining extremely compact model representations by factorising the filters in CNNs or by passing these through a multi-stage compression pipeline. Common to these methods is the need for a \textit{decompressing} stage that generates the filters at run time during inference. 
A selection of such %these on-the-fly 
techniques include: \cite{ha2016hypernetworks} that uses an auxiliary NN to generate each layer's weights in the main network given an embedding of the weights.  
In~\cite{pmlr-v80-qiu18a}, weight filters are constructed as a dense combination of a set of Fourier Bessels bases that are generated deterministically at run time.
Another technique exploiting deterministic bases was presented in~\cite{fernandez2018binarycmd,ijcai2018-380, ovsf2018emdl}, where bases are formed from OVSF binary codes. 
It enables the construction of model weights by learning a linear combination of OVSF bases during training. 

We label these techniques as \emph{on-the-fly} since they \textit{1)} require a single-step decompression stage to obtain the filters, \textit{2)} this process can be done on-demand, \textit{i.e.}~at each layer and for every input, and \textit{3)} such decompression is lightweight, \textit{i.e.}~it does not require multiple inferences to amortise its associated costs. In this work, we focus on on-the-fly methods that use compression as a means to reduce data-movement overheads, balancing the costs of decompressing the model parameters with the latency savings due to reduced off-chip or main memory accesses.

To this end, this work makes use of OVSF codes to compress filters in a CNN and reconstruct them during inference in a lightweight manner. Three additional reasons motivated the choice of this class of codes: 
\textit{1)}~OVSF codes are \textit{binary} and thus can be efficiently stored on-chip~\cite{5291322}; \textit{2)}~their theoretical properties are well studied by the wireless community~\cite{Andreev:2003:OCG:764808.764868,1411169}; \textit{3)}~they offer good compression-accuracy trade-off (i.e. lossy compression) in various AI tasks~\cite{ovsf2018emdl,ijcai2018-380}. 
Nonetheless, \cite{ovcf-fpga}~is the only existing FPGA-based OVSF design, presenting solely a direct implementation specific for communication systems. To effectively use OVSF with CNNs, the underlying hardware design needs to be tailored and optimised for the CNN dataflow.

\subsection{On-the-Fly OVSF CNN Layers}
The chosen OVSF codes are a set of mutually orthogonal binary codes, originally designed to split in the frequency domain signals from different users in W-CDMA-based 3G cellular systems~\cite{wdscdma}. Using them as channelisation codes allowed for communication channels to remain orthogonal in multi-user access scenarios, reducing signal interference while dramatically increasing the system capacity.

These codes can be obtained using Sylvester's construction algorithm for Hadamard matrices. In this way, given $H_0 = [1]$ and $H_2$, subsequent $H_{2^k}$ expansions are defined as
% 
% \vspace{-2mm}
% \begin{equation}
% \resizebox{0.91\linewidth}{!}{
%   {$H_{2}$} =
%   {
%   $
%   \begin{bmatrix}
%     1 & 1 \\
%     1 & -1
%   \end{bmatrix}$
%   }
%   , \hspace{1mm}
%   {$H_{2^k}$} =
%   {$
%   \begin{bmatrix}
%     H_{2^{k-1}} & H_{2^{k-1}} \\
%     H_{2^{k-1}} & -H_{2^{k-1}}
%   \end{bmatrix}$
%   }
%   =
%   {$H_2$ $\otimes$ $H_{2^{k-1}}$}
%  }
% \label{eq:sylvester}
% \end{equation}
\begin{equation}
  {H_{2}} =
  \begin{bmatrix}
    1 & 1 \\
    1 & -1
  \end{bmatrix}
  , \quad
  {H_{2^k}} =
  \begin{bmatrix}
    H_{2^{k-1}} & H_{2^{k-1}} \\
    H_{2^{k-1}} & -H_{2^{k-1}}
  \end{bmatrix}
  =
  {H_2 \otimes H_{2^{k-1}}}
\label{eq:sylvester}
\end{equation}
% 
where $H_{2^k}$ is an $L$$\times$$L$ Hadamard matrix, with $L = 2^k, k\in \mathbb{N}$ and $\otimes$ is the Kronecker product. For $k>1$, each row is an OVSF code fulfilling the properties of being binary and orthogonal to each other. This will enable us to use them as basis for $\mathbb{R}^L$. An alternative formulation~\cite{1997iet} allows for the construction of OVSF codes as a recursive expansion of a perfect binary tree.

By treating the set of $L$ OVSF codes as a basis spanning $\mathbb{R}^L$, we can define the construction process of an arbitrary real-valued vector $\pmb{v}'_i$ as the linear combination of such codes
% 
% Figure environment removed
% 
% 
\begin{equation}
% \resizebox{0.91\linewidth}{!}{
  \pmb{v}'_i =  \sum_{j = 0}^{\lfloor \rho \cdot L \rceil}\alpha^{j}_{i}\bm{b}^{j}_{i}
  ,
  \quad
  E_{i} = \left\|\bm{v}'_i - \bm{v}_i \right \|_2^2 = \left \| \sum_{j = 0}^{\lfloor \rho \cdot L \rceil}\alpha^{j}_{i}\bm{b}^{j}_{i} - \bm{v}_i \right \|^2_2 < \epsilon
%   }
\label{eq:vector_eq}
\end{equation}
\vspace{1mm}
% 
where $\pmb{\alpha}_i = \{\alpha^{0}_i, \alpha^{1}_i, \alpha^{2}_i, ... , \alpha^{L-1}_i\}$ are weighting coefficients, $\pmb{b}^{j}_i$ is the $j$-th OVSF binary code of length $L$ and, $\rho \in [0,1]$ is the ratio of codes to use in order to construct $\pmb{v}_i$. The expression on the right measures the difference between $\pmb{v}'_i$ and a real-valued standard vector of length $L$, $\pmb{v}_i$. Intuitively, $\mathlarger{\epsilon} \to 0$ as we increase the ratio of binary codes used. %, as mathematically presented on the right expression bounding the representation capabilities of OVSF codes.
% Construction and learning (add NICE diagram here)

When constructing matrices from OVSF codes or higher-dimensional tensors, a reshaping stage follows the linear combination shown in Eq.~(\ref{eq:vector_eq}). In this way, if the weights tensor of a given convolutional layer is of shape $N_{\text{out}}$$\times$$N_{\text{in}}$$\times$$K$$\times$$K$, the construction process using OVSF could be framed as the concatenation of $N_{\text{out}}$ $N_{\text{\text{in}}}$$\times$$K$$\times$$K$ filters using codes of length $L=N_{\text{in}}$$K$$K$ and up to $L$ of such codes. Here, $N_{\text{in}}$ and $N_{\text{out}}$ stands for the number of input and output channels in the layer respectively, and $K$$\times$$K$ are the spatial dimensions of the convolutional filter. This scenario is illustrated in Fig.~\ref{fig:ovsf_gen_diagram}. The set of scalars $\{\alpha \}_{k=1}^L$ in each layer are learnt via standard backpropagation and represent the only learnable parameters in an OVSF layer since the OVSF binary codes themselves are fixed. A compressed representation of $f_{i}$ is obtained when $\rho$$<$$1$.
%The accuracy degradation due to compression can be controlled with careful fine-tuning and hyperparameter selection.
Upon deployment, the filters are first generated and then the main inference computation proceeds as normal.

Due to the construction process of OVSF codes (Eq.~\ref{eq:sylvester}), $K$ in OVSF convolutional layers is limited to be a power-of-two number. We address this in Sec.~\ref{sec:OVSF_issues}, where we present two methods to enable the construction of filters with $K\in\mathcal{N}$, for example with the ubiquitous $K=3$.

\subsection{Challenges of FPGA-based CNN Engines}

Until now, a wide array of FPGA-based CNN accelerators have been proposed. In the customisation-programmability spectrum, existing designs span from custom streaming architectures~\cite{streaming2016fpl,fpgaconvnet2019tnnls} and accelerators for quantised~\cite{finn2017fpga,ternaryfpga2018trets,finnr2018trets,multiprec2019fpt,lutnet2020toc,logicnets2020fpl} and sparse CNNs~\cite{cambriconx2016micro,cambircons2018micro,scnn2017isca,sparten2019micro,sparsecnnaccel2019fccm,extensor2019micro,tensaurus2020hpca}, up to instruction-based processors~\cite{snowflake2017iscas,lightopu2020fpga,uniopu2020tvlsi}.
One of the most well adopted paradigms are the single computation engines~\cite{fpdnn2017fccm,dla2018fpl,cascadecnn2020date,caffeine2019tcad,dnnvm2019tcad,cascadecnn2018fpl,fft2020fpga,alamo2020tcad,abdelfattah2020best}, due to their balanced trade-off of programmability and performance. 
Currently, despite the progress in processing unit design, further gain in the attainable performance of such engines is hindered by two main factors: i)~memory-bound layers that are dominated by the communication with the external memory~\cite{mem_req2918iiswc,cascadecnn2018fpl,fft2020fpga,alamo2020tcad}. While embedded platforms provide limited bandwidth~\cite{lostbw2019fpt,divrsemem2019fpt,venieris2018fpl}, \textit{e.g.} less than 4.5~GB/s for Ultra96 and ZC706, sustaining peak bandwidth even on larger devices, such as ZCU104, is nontrivial~\cite{divrsemem2019fpt}. This is aggravated as multiple applications are collocated on a single device~\cite{venieris2018fpl,codesignmultiple2020dac,multinn2020isca}; and ii)~underutilised PEs due to the mismatch of diverse layer shapes~\cite{alamo2020tcad,latency2017fpl,cnnroofline2015fpga,gamma2020iccad,abdelfattah2020best,maestro2020micro}. 

\textbf{Memory-centric Designs.}
The memory bandwidth problem faced by CNN engines has been studied in previous work. 
EIE~\cite{eie2016isca} uses the Deep Compression method~\cite{deepcompression2015iclr} to compress the weights of fully connected (FC) layers. However, as FC layers have been mostly abandoned in modern CNNs, its applicability is limited.
Angel-Eye~\cite{angeleye2018tcad} compresses all layers through precision quantisation. 
Cambricon-X~\cite{cambriconx2016micro} transfers only the non-zero weights, while Cambricon-S~\cite{cambircons2018micro} and Scalpel~\cite{scalpel2017isca} apply coarse weight pruning, but with significant accuracy drop.
CircCNN~\cite{circcnn2017micro} uses block-circulant matrices for weights, but requires complex FFT hardware for efficient execution.
\cite{permdnn2018micro} converts sparse weights to permuted diagonal matrices, but only targets FC layers. 
\cite{escher2017fccm}~exploits large batch sizes to increase weights reuse and, thus, is not suitable for latency-critical applications that cannot tolerate batching~\cite{latency2017fpl}.

Focusing on activations, \cite{fusedlayer2016micro} fuses adjacent layers to cache intermediate activations, while \cite{scnn2017isca} and Eyeriss~\cite{eyeriss2017jssc}, \cite{def2021aspdac} employ encoding schemes to minimise their bandwidth footprint.
Other solutions have either relied on large devices~\cite{opencldla2017fpga} and multiple FPGAs~\cite{brainwave2018isca} to fit all weights on-chip, or utilised highly customised designs to exploit multi-precision cascades~\cite{cascadecnn2020date} or fine-grained pruning~\cite{sparsecnnaccel2019fccm} at the cost of notable accuracy drop.

\textbf{Tackling PE Underutilisation.}
So far, a limited number of designs have focused on ii). \cite{maximising2017isca} addresses underutilisation by grouping CONV layers based on the compatibility of their shapes. 
\cite{streaming2016fpl} maps each layer to a dedicated compute stage, which can be used only for shallower networks, but does not scale to the deeper models of today. Furthermore, a limited number of works rely on FFT-based designs with flexible dataflow~\cite{fft2020fpga} and costly ASIC solutions~\cite{flexflow2017hpca,recpatterns2017tvlsi,maeri2018asplos,polymorph2021tc} with highly flexible PE interconnect.


In contrast to these works, we propose an approach that is independent of the CNN engine by not requiring any modification to the engine architecture itself. \tool can benefit any existing single computation engine by augmenting it with its hardware weights generator and enhancing its PE array with lightweight switches, without affecting the PE's internal processing units. As such, \tool is orthogonal and complementary to quantisation~\cite{angeleye2018tcad}, activations' encoding~\cite{eyeriss2017jssc,scnn2017isca}, fusion~\cite{fusedlayer2016micro} and zero-skipping PEs~\cite{cnvlutin2016isca,pragmatic2017micro,cambircons2018micro,shapeshifter2019micro,sparten2019micro}.
