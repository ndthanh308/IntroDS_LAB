\begin{table}[t]
% 	\vspace{-0.2cm}
	\centering
% 	\vspace{-0.2cm}
        \captionsetup{font=small,labelfont=bf}
	\caption{\footnotesize FPGA platforms used for evaluation.}
	\vspace{-0.2cm}
	\resizebox{0.65\linewidth}{!}{%
		\begin{tabular}{l l r l l r}
			\toprule
			\multicolumn{1}{l}{\multirow{1}{*}{Platform}} & Processor %& FPGA 
			& \multicolumn{1}{c}{\multirow{1}{*}{LUTs}} & Flip-Flops & DSPs & \multicolumn{1}{l}{\multirow{1}{*}{BRAM}} \\
			\midrule
			Zynq 7045 & Arm Cortex A9 %& Kintex-7 
			& 218,600 & 437,200 & 900 & 2.40 MB \\
			UltraScale+ ZU7EV & Arm Cortex A53 %& Artix-7 
			& 230,000 & 461,000 & 1,728 & 4.75 MB \\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:fpgas}
    \vspace{0.2cm}
\end{table}

\section{Evaluation}
\label{sec:eval}


\subsection{Experimental Setup}
\label{sec:exp_setup}

In our experiments, we target two widely used FPGA platforms with varied computational capabilities and memory resources (Table~\ref{tab:fpgas}): the Xilinx ZC706 board mounting the mid-tier Zynq Z7045 and the Xilinx ZCU104 board with the more resource-rich Zynq UltraScale+ ZU7EV. The two platforms are based on the Xilinx Zynq-7000 SoC and UltraScale+ MPSoC architectures, respectively, integrating a dual-core Arm Cortex A9 CPU and a quad-core Arm Cortex A53 CPU, respectively, alongside an FPGA fabric on the same chip. Our hardware designs were synthesised and placed-and-routed with Xilinx Vivado HLS and Vivado Design Suite (v2019.2) and run on both boards, with operating clock frequencies of 150~MHz for ZC706 and 200~MHz for ZCU104, respectively. The achieved clock frequency is currently constrained by the technology of the target device and the use of HLS, which relies on the vendor's toolchain and does not allow for low-level optimisations to shorten the critical path.

The corresponding Arm CPU was used to set up the transactions with the off-chip memory, launch the execution of inference and measure the end-to-end performance of each design. \tool provides support for both custom fixed-point and floating-point precisions. For the evaluation, 16-bit fixed-point precision was used, following the practice of the FPGA works we compare with. The available off-chip memory bandwidth was controlled by using a different number of memory ports and amount of word packing, spanning from 1.1 GB/s (1$\times$) to 13.4 GB/s (12$\times$).

\subsubsection{\textbf{Benchmarks}} 
\label{sec:benchmarks}
We evaluate on CNNs of varying depth, workload and memory footprint. Each CNN has been selected to impose a different design challenge. In particular, we target the widely used family of residual networks~\cite{DBLP:journals/corr/HeZRS15} and map variants of different depths to evaluate the scalability of our design. Concretely, we use ResNet18, ResNet34 and ResNet50 on the ImageNet dataset. In addition to image classification, ResNet models are also found as backbone of other tasks including object detection~\cite{detector2019iros}, super-resolution~\cite{mobisr2019mobicom} and semantic segmentation~\cite{deeplab2018tpami}. We also target SqueezeNet1.1~\cite{iandola2016squeezenet}, to assess \tool's efficacy on a highly optimised network for resource-constrained devices.

\subsubsection{\textbf{Basis Selection and \bm{$3$$\times$$3$} extraction}}
\label{sec:basis_selection_and_3x3_extraction}
The proposed on-the-fly formulation using OVSF codes allows for different strategies for: \textit{i)}~selecting which basis to use when $\rho$$<$$1$; and, \textit{ii)}~extracting $3$$\times$$3$ filters from \emph{true} OVSF filters that are restricted to be of shape $K$$\times$$K$ with $K$ being a power-of-two. In ~\ref{sec:OVSF_issues} we presented two solutions for each of aforementioned considerations. %We therefore consider two approaches to tackle the aforementioned considerations. 
%For i), we consider between \textit{1)}~utilising the first $\lfloor \rho \cdot K^2\rceil$ codes and \textit{2)}~iteratively discarding OVSF codes based on their associated scalar $\alpha$ until the target compression ratio $\rho$ is reached. For ii), we consider \textit{1)}~extracting a $3$$\times$$3$ crop from a $4$$\times$$4$ filter and \textit{2)}~learning a mapping to a $3$$\times$$3$ filter by means of an average pooling layer.
Table~\ref{tab:ovsf_cifar10} shows our analysis of the different approaches on CIFAR-10 with ResNet18/34. For the basis selection strategy \textit{i)}, iteratively dropping bases consistently yields higher-accuracy models. For \textit{ii)}, as the models become more compact (\textit{e.g.}~for OVSF50/25), cropping achieves higher accuracy compared to the average pooling approach.  
Thus, we leverage these findings to inform the parametrisation for ImageNet for the rest of the evaluation.

\subsubsection{\textbf{Training Scheme}}
\label{sec:training_scheme}
We have developed \tool's offline flow on top of \textit{PyTorch} (1.5). To derive the OVSF models, we modified the official \textit{PyTorch}-based ResNet by replacing all $3$$\times$$3$ convolutional layers within residual blocks with their OVSF counterparts. In all our experiments, we employed pre-trained ImageNet models from \textit{torchvision} (0.6.0). After a regression stage that transforms standard models into OVSF ones, the models were fine-tuned for 30 epochs using an Adam optimiser~\cite{kingma2014adam} and learning rate decay every 10 epochs. For each given model, we trained two OVSF variants following different distributions of ratios $\rho$ for layers in each of the four residual blocks. First, OVSF50 with ratios=$[1.0,0.5,0.5,0.5]$; and OVSF25 with ratios=$[1.0,0.4,0.25,0.125]$. 
We follow the same procedure and ratios for SqueezeNet's \textit{Fire} modules.

\subsubsection{\textbf{Baselines}}
\label{sec:baselines}

We introduce two highly optimised single computation engines executing: \textit{a)}~the vanilla CNN and \textit{b)}~pruned variants. For \textit{b)}, we use a state-of-the-art method~\cite{Molchanov_2019} which applies channel pruning based on the first-order Taylor approximation contribution of each filter to the model's loss. This process is carried out iteratively until a target compression ratio is reached. We refer to a pruned model that keeps 82\% of the filters as Tay82 and follow the same naming scheme for other ratios.
The baseline architecture comprises the conventional CNN engine design shown in Fig.~\ref{fig:conventional_cnn_engine}, with the weights transferred from the off-chip memory into the {\small $T_P$$\times$$T_C$} weights buffer, if they do not fit on-chip. Both \textit{a)} and \textit{b)} are parametrised with tile sizes {\small $\left<T_R,T_P,T_C \right>$} and roofline modelling~\cite{cnnroofline2015fpga} is used to obtain the highest throughput configuration for the target \mbox{CNN-FPGA pair}. 


\subsection{Performance Comparison}
\label{sec:perf_comparison}
This section analyses the performance of the proposed framework with respect to both our optimised baselines and existing FPGA work.


\begin{table}[t]
    \normalsize
    % \tablefontsize
    \centering
    \captionsetup{font=small,labelfont=bf}
    \captionof{table}{\footnotesize Impact on accuracy for i)~each basis selection strategy and ii)~method to extract $3$$\times$$3$ filters from $4$$\times$$4$ OVSF filters. Models trained on CIFAR-10, with ResNet18/34 adapted for this dataset adn the much smaller variants ($\dagger$) proposed in~\cite{DBLP:journals/corr/HeZRS15}. Performing an iterative drop of bases, as opposed to taking the first $\lfloor \rho \cdot K^2\rceil$, consistently results in better models. As model size is reduced, taking a $3$$\times$$3$ crop from a $4$$\times$$4$ filter performed better than using an average pooling stage.}
    \vspace{-0.1cm}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{c|c|c|cc|cc|cccc}
   
        \toprule
        Model Arch. & Basis & Filters & \multicolumn{2}{c|}{OVSF100} & \multicolumn{2}{c|}{OVSF50} & \multicolumn{2}{c}{OVSF25}\\
        (baseline) & Strategy & to $3$$\times$$3$ & Param. & Acc. & Param. & Acc. & Param. & Acc.\\ \midrule
        
        \multirow{4}{*}{\parbox{1.9cm}{\centering ResNet18 93.2\% 11.2M}} & \multirow{2}{*}{Sequential} & Crop & \multirow{4}{*}{19.7} & 93.9 &  \multirow{4}{*}{9.1} & 93.7 & \multirow{4}{*}{3.6} & 92.9 \\
                                   & & Adaptive &  & 93.7 & & 93.8 & & 93.0 \\ \cline{2-3} \cline{5-5} \cline{7-7} \cline{9-9}
                                   & \multirow{2}{*}{Iterative} & Crop &  & 94.1 & & 93.6 & & \textbf{93.6} \\
                                   & & Adaptive &  & 94.0 & & 93.8 & & 92.3 \\ \midrule \midrule
                                   
        \multirow{4}{*}{\parbox{1.9cm}{\centering ResNet18$^\dagger$ 91.3\% 0.27M}} & \multirow{2}{*}{Sequential} & Crop & \multirow{4}{*}{0.48} & 90.8 & \multirow{4}{*}{0.25} & 90.8 & \multirow{4}{*}{0.15} & 88.3 \\
                                  & & Adaptive &  & 91.1 & & 91.2 & & 88.5\\ \cline{2-3} \cline{5-5} \cline{7-7} \cline{9-9}
                                  & \multirow{2}{*}{Iterative} & Crop &  & 91.1 & & 91.3 & & \textbf{91.4}\\
                                  & & Adaptive &  & 91.2 & & 91.4 & & 91.0\\ \midrule \midrule
                                   
        \multirow{4}{*}{\parbox{1.9cm}{\centering ResNet34 93.9\% 21.3M }} & \multirow{2}{*}{Sequential} & Crop & \multirow{4}{*}{37.7} & 94.1 &  \multirow{4}{*}{17.6} & 93.9 & \multirow{4}{*}{7.2} & 93.4  \\
                                  & & Adaptive &  & 94.3& & 94.0 & & 93.4 \\ \cline{2-3} \cline{5-5} \cline{7-7} \cline{9-9}
                                  & \multirow{2}{*}{Iterative} & Crop &  & 94.1& & 93.8 & & \textbf{94.3}\\
                                  & & Adaptive &  & 93.8 & & 93.7 & & 93.2\\ \midrule \midrule

        \multirow{4}{*}{\parbox{1.9cm}{\centering ResNet34$^\dagger$ 92.1\% 0.46M}} & \multirow{2}{*}{Sequential} & Crop & \multirow{4}{*}{0.82} & 92.3 & \multirow{4}{*}{0.43} & 91.4 & \multirow{4}{*}{0.26} & 89.3 \\
                                  & & Adaptive &  & 92.2 & & 91.5 & & 89.2\\ \cline{2-3} \cline{5-5} \cline{7-7} \cline{9-9}
                                  & \multirow{2}{*}{Iterative} & Crop &  & 92.3& & 91.8 & & \textbf{92.2}\\
                                  & & Adaptive &  & 92.4 & & 91.7 & & 91.7\\
                                   
    \bottomrule
    \end{tabular}
    }
    
    \label{tab:ovsf_cifar10}
\end{table}

\begin{table}[t]
    \tablefontsize
    \centering
    \captionsetup{font=small,labelfont=bf}
    \caption{\footnotesize Accuracy and number of parameters for ResNet34 models on ImageNet following different compression schemes. Performance measured on ZC706 at different memory bandwidths.}
    \vspace{-0.1cm}
    \resizebox{0.65\columnwidth}{!}{
    \begin{tabular}{l c c c c}
        \toprule
        Model & Compression & Params & Accuracy & Performance (inf/sec) \\
        Arch. & Method & (millions) & (\%) & ($1\times$, $2\times$, $4\times$) \\%, $12\times$)\\
        \midrule
        ResNet34 & - & 21.8 & 73.3 & (8.6, 16.8, 28.7) \\ %, 36.0)\\
        \midrule
        ResNet34 & Tay82 & 17.4 & $72.7$ & (10.7, 21.0, 35.6) \\ %, 44.3) \\
        ResNet34 & Tay72 & 15.1 & $71.9$ & (13.3, 25.8, 44.0) \\ %, 54.7) \\
        ResNet34 & Tay56 & 9.4 & $67.8$ & (18.3, 36.3, 63.8) \\ %, 77.6) \\
        ResNet34 & Tay45 & 6.3 & $63.1$ & (21.8, 43.4, 79.8) \\ %, 97.9) \\
        \midrule
        ResNet34 & OVSF50 & 17.2 & $72.8$ & (18.1, 21.8, 31.1) \\ %, 33.3) \\
        ResNet34 & OVSF25 & 7.2 & $71.5$ & (18.4, 27.3, 33.5) \\ %, 33.7) \\
        \midrule
        ResNet34 & Tay82+OVSF50 & 13.2 & $71.1$ & (18.6, 30.0, 37.3) \\ %, 38.3) \\
        ResNet34 & Tay82+OVSF25 & 6.7 & $70.6$ & (18.8, 31.0, 38.9 )\\ %, 40.5) \\
        ResNet34 & Tay72+OVSF50 & 11.9 & $70.3$ & (18.8, 32.0, 40.2) \\ %, 41.3) \\
        ResNet34 & Tay72+OVSF25 & 4.9 & $68.9$ & (18.9, 33.3, 42.0) \\ %, 43.3) \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:accResultsResnet34}
\end{table}

\begin{table}[t]
    \tablefontsize
    \centering
    \captionsetup{font=small,labelfont=bf}
    \caption{\footnotesize Accuracy and number of parameters for ResNet18 models on ImageNet following different compression schemes. Performance measured on ZC706 at different memory bandwidths.}
    \vspace{-0.1cm}
    \resizebox{0.65\columnwidth}{!}{
    \begin{tabular}{l c c c c}
        \toprule
        Model & Compression & Params & Accuracy & Performance (inf/sec) \\
        Arch. & Method & (millions) & (\%) & ($1\times$, $2\times$, $4\times$) \\ %, $12\times$)\\
        \midrule
        ResNet18 & - & 11.7 & 69.8  & (12.0, 23.5, 40.1)\\ %, 54.5) \\
        \midrule
        ResNet18 & Tay88 & 9.1 & $68.8$ & (14.3, 28.0, 46.4)\\ %, 61.9) \\
        ResNet18 & Tay82 & 7.9 & $67.3$ & (14.3, 27.8, 45.4)\\ %, 61.5) \\
        ResNet18 & Tay72 & 6.0 & $64.8$ & (18.2, 35.3, 57.6)\\ %, 77.3) \\
        ResNet18 & Tay56 & 3.7 & $58.3$ & (23.8, 47.3, 82.2)\\ %, 99.5) \\
        \midrule
        ResNet18 & OVSF50 & 9.1 & $69.2$ & (19.4, 33.8, 49.9)\\ %, 51.9) \\
        ResNet18 & OVSF25 & 4.1 & $67.3$ & (19.4, 34.8, 51.0)\\ %, 52.7) \\
        \midrule
        ResNet18 & Tay82+OVSF50 & 6.3 & $66.2$ & (24.5, 43.2, 57.9)\\ %, 58.5) \\
        ResNet18 & Tay82+OVSF25 & 2.8 & $64.4$ & (24.5, 43.6, 59.7)\\ %, 61.2) \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:accResultsResnet18}
    % \vspace{-0.1cm}
\end{table}




\subsubsection{\textbf{Comparison with Optimised Baselines}}
\label{sec:baseline_comparison}
Tables~\ref{tab:accResultsResnet34} and~\ref{tab:accResultsResnet18} show the achieved validation set accuracy and actual performance of each design as measured on ZC706 under varying bandwidth budget. Across bandwidths (1$\times$/2$\times$/4$\times$ where 4$\times$ is the 4.5 GB/s peak measured bandwidth on ZC706), \tool's OVSF50 and OVSF25 designs outperform the faithful baseline by {\small 2.1$\times$/1.3$\times$/1.1$\times$} and {\small 2.1$\times$/1.6$\times$/1.2$\times$} respectively for ResNet34, and by {\small 1.6$\times$/1.6$\times$/1.24$\times$} and {\small 1.4$\times$/1.5$\times$/1.3$\times$} respectively for ResNet18. As bandwidth availability increases, the baseline becomes less memory-bound and the performance gap closes. 
Table~\ref{tab:accResultsSqueezenet} shows the comparison of \tool with the faithful baseline for SqueezeNet on ZU7EV with peak measured bandwidth of 13.4~GB/s (12$\times$). Both OVSF50 and OVSF25 designs yield increasing throughput gains as the bandwidth becomes more restricted, with OVSF25 sustaining over 57\% speedup for up to 4$\times$ bandwidth.
Under 1$\times$ bandwidth, OVSF25 offers minimal additional gains. This is because, below a compression ratio, even though the memory needs are further reduced, activations begin to dominate I/O, and hence further weights reduction does not provide significant benefits. Activations compression techniques~\cite{eyeriss2017jssc,scnn2017isca} can be orthogonally combined to obtain further gains.

\rev{Based on our evaluation using SqueezeNet, we observe that computation can take place fast due to its lighter workload. As such, the attainable performance depends on how rapidly we can feed the CNN Engine with new inputs. Specifically, for the 4$\times$ bandwidth configuration, all layers of SqueezeNet are memory-bound. On the other hand, at 12$\times$ bandwidth, 88\% of the layers become compute-bound. As such, when there is restricted or medium availability of memory bandwidth, \tool significantly improves performance through our weights generation approach, with 78\%, 74\% and 55\% higher throughput for the 1$\times$, 2$\times$ and 4$\times$ bandwidth configurations, respectively (Table~\ref{tab:accResultsSqueezenet}). This improvement gradually decreases as the available bandwidth increases, with 15\% gain at 12$\times$ bandwidth.}


\begin{table}[t]
    \vspace{-0.2cm}
    \small
    \centering
    \captionsetup{font=small,labelfont=bf}
    \caption{\footnotesize Comparing \tool with faithful baseline on SqueezeNet on ImageNet. Performance measured on the UltraScale+ ZCU104 platform at different memory bandwidths.}
    \vspace{-0.1cm}
    \resizebox{0.65\linewidth}{!}{
    \begin{tabular}{l c c c c}
        \toprule
        Model & Compression & Params & Accuracy & Performance (inf/sec) \\
        Arch. & Method & (millions) & (\%) & ($1\times$, $2\times$, $4\times$, $12\times$)\\
        \midrule
        SqueezeNet & - & 1.24 & 58.2 & (72.9, 145.2, 290.4, 687.4) \\
        % SqueezeNet & - & 1.24 & 58.2 & (54.7, 108.9, 217.8, 515.6) \\
        \midrule
        SqueezeNet & OVSF50 & 1.07 & 57.6 & (129.8, 252.9, 452.1, 792.1) \\
        SqueezeNet & OVSF25 & 0.86 & 57.1 & (129.8, 252.9, 456.8, 800.6) \\
        % SqueezeNet & OVSF50 & 1.07 & 57.6 & (97.4, 189.7, 339.1, 594.1) \\
        % SqueezeNet & OVSF25 & 0.86 & 57.1 & (97.4, 189.7, 342.6, 600.5) \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.2cm}
    \label{tab:accResultsSqueezenet}
\end{table}

\textbf{Comparison with Pruned Baselines.} Compared to the pruned baselines, \tool's OVSF models are more resilient at high compression ratios while resulting in similar accuracy at lower compression ratios. Informed by the analysis in Table~\ref{tab:ovsf_cifar10}, OVSF models are trainined to extract a $3$$\times$$3$ from a $4$$\times$$4$ and, to iteratively discard OVSF basis until the target compression ratio $\rho$ for each layer is reached, as first discussed in Sec.~\ref{sec:basis_selection_and_3x3_extraction}. In terms of throughput, \tool delivers faster processing at more constrained bandwidths. Concretely, ResNet34-OVSF50 is 80\% faster than Tay82 at 1$\times$ bandwidth, with less than 1 percentage point (pp) accuracy drop. Despite being almost identical in terms of model size and accuracy, Tay82's approach, which prioritises the pruning of layers with the least accuracy impact, leads to the pruning of mostly compute-bound layers when targeting ResNet34. On the other hand, ResNet34-OVSF50 compresses more effectively memory-bound layers, leading to significantly higher throughput at low bandwidths.
A similar pattern is observed for ResNet18. At higher compression ratios, ResNet34-OVSF25 yields 3.7 pp higher accuracy than Tay56, despite using 25\% fewer parameters.

To explore the benefits of combining \tool's OVSF execution scheme with channel pruning, we derive, train and map on \tool Tay-OVSF models. 
This results in competitive lightweight models that are not attainable through pruning alone. For instance, ResNet18 with Tay82+OVSF25 is 25\% smaller than ResNet18-Tay56 and achieves 6.1 pp higher accuracy, while achieving 34.6\% and 23.5\% higher throughput over ResNet18-Tay72 with less than 0.5 pp accuracy drop. 


\begin{table*}[t]
	\centering
	\captionsetup{font=small,labelfont=bf}
    \caption{\footnotesize Comparison with prior FPGA work on ResNet18 (4.03 GOps), ResNet34 (7.40 GOps) \& SqueezeNet (0.78 GOps).}
	\vspace{-0.2cm}
	\resizebox{1.0\linewidth}{!}{
	\setlength\tabcolsep{10pt} % default value: 6pt
		\begin{threeparttable}
			%\vspace{-0.1cm}
			\small
			%	\footnotesize
			%	\normalsize
			%	\huge
			%	\large
			%\resizebox{0.5\textwidth}{!}{
			%	\resizebox{\linewidth}{!}{
			%	\resizebox{0.5\textwidth}{!}{
			\begin{tabular}{@{}l l l| l l| l l l l@{}}
				\toprule
				Comparison with: 
				& \multicolumn{2}{l|}{\textbf{Compiler-based Design}}
				& \multicolumn{2}{l|}{\textbf{Compression-based Design}} 
				& \multicolumn{1}{l}{\textbf{Light-CNN-tailored Design}}
				& \multicolumn{2}{l}{\textbf{Multi-Accelerator Designs}}
				\\
				& ResNet18~\cite{snowflake2017compiling} 
				& \begin{tabular}[l]{@{}l@{}} \tool: \\ ResNet18* \end{tabular}
				& \begin{tabular}[l]{@{}l@{}} Sparse ResNet34~\cite{sparsecnnaccel2019fccm} \\ using Deep Compression  \end{tabular}
				& \begin{tabular}[l]{@{}l@{}} \tool: \\ ResNet34* \end{tabular} 
				& SqueezeNet~\cite{lightopu2020fpga} 
				& \multicolumn{2}{l}{SqueezeNet~\cite{maximising2017isca}} 
				& \begin{tabular}[l]{@{}l@{}} \tool:\\ SqueezeNet* \end{tabular}
				\\
				\cmidrule{7-8}
				\midrule
				FPGA  
				& Z7045 
				& Z7045 
				& Z7045 
				& Z7045
				& K325T
				% & \begin{tabular}[l]{@{}l@{}} Kintex-7 \\ 325T \end{tabular}
				& V485T 
				% & \begin{tabular}[l]{@{}l@{}} Virtex-7 \\ 485T \end{tabular}
				& V690T
				% & \begin{tabular}[l]{@{}l@{}} Virtex-7 \\ 690T \end{tabular} 
				& ZU7EV 
				\\
				Clock (MHz) & 250 & 150 & 166	& 150 & 200 & 170 & 170 & 200 \\
				Precision & 16b fixed & 16b fixed & 16b fixed & 16b fixed & 8b fixed & 16b fixed & 16b fixed & 16b fixed \\
				DSPs$^\dagger$ & 900 & 900 & 900 & 900 & 840 & 2800 & 3600 & 1728 \\
				Logic Capacity & 218.6 kLUTs & 218.6 kLUTs & 218.6 kLUTs & 218.6 kLUTs & 203.8 kLUTs & 303.6 kLUTs & 433.2 kLUTs & 230.0 kLUTs \\
				%	Fixed-point DSPs* & 220 & 900 & 220 & 900 \\
				Block RAM & 2.40 MB & 2.40 MB & 2.40 MB & 2.40 MB & 1.95 MB & 4.52 MB & 6.46 MB & 4.75 MB \\
				{\color{black}DSP Util.$^\dagger$} & 28.4\% & 100\% & 56.8\% & 100\% & 83.8\% & 80\% & 80\% & 100\% \\
				
				\begin{tabular}[t]{@{}l@{}} 
					inf/s
				\end{tabular} 
				& 21.38
				& 49.90
				& 27.84
				& 31.1
				& 420.90
				& 913.40
				& 1173.00
				& 792.20
				\\
				\begin{tabular}[t]{@{}l@{}} 
					inf/s/DSP$^\dagger$
				\end{tabular} 
				& 0.0237
				& 0.0576
				& 0.0309
				& 0.0369
				& 0.2505
				& 0.3260
				& 0.3258
				& 0.4584
				\\
				\begin{tabular}[t]{@{}l@{}} 
					inf/s/Logic
				\end{tabular} 
				& 0.0978
				& 0.2282 % 0.2372
				& 0.1273
				& 0.1422 % 0.1521
				& 2.0652
				& 3.0085
				& 2.7077
				& 3.444
				\\
			
				\bottomrule
				
			\end{tabular} 
			\begin{tablenotes}
				\small
				\item * using OVSF50, ** batch size = 1, $\dagger$ 18$\times$18, 19$\times$18 and 25$\times$18 DSP configurations, inf/s/DSP is adjusted based on precision for fair comparison (multiplied by 0.5 for 8b).
			\end{tablenotes}
			
		\end{threeparttable}
	}
	\vspace{-0.25cm}
	\label{tab:comparison_table}
\end{table*}

\begin{table*}[t]
	\centering
	\captionsetup{font=small,labelfont=bf}
    \caption{\footnotesize Comparison with prior FPGA work on ResNet50 (8.41 GOps).}
	\vspace{-0.2cm}
	\resizebox{1.0\linewidth}{!}{
	\setlength\tabcolsep{2pt} % default value: 6pt
		\begin{threeparttable}
			%\vspace{-0.1cm}
			\small
			%	\footnotesize
			%	\normalsize
			%	\huge
			%	\large
			%\resizebox{0.5\textwidth}{!}{
			%	\resizebox{\linewidth}{!}{
			%	\resizebox{0.5\textwidth}{!}{
			\begin{tabular}{l l l | l l l l l l l l l l}
				%\hline
				\toprule
				Comparison with: 
				& \multicolumn{4}{l}{\textbf{Compiler-based Designs}}
				& \multicolumn{2}{l}{\textbf{CNN-to-FPGA Toolflows}}
				& \multicolumn{1}{l}{\textbf{CNN-tailored Designs}} 
				& \multicolumn{1}{l}{\textbf{Overlay Designs}}
				& \multicolumn{1}{l}{\textbf{Cloud-based Designs}}
				& \begin{tabular}[l]{@{}l@{}} \textbf{Interconnect-aware} \\ \textbf{Designs} \end{tabular}
				& \begin{tabular}[l]{@{}l@{}} \textbf{Full-stack-optimised} \\ \textbf{Designs} \end{tabular}
				\\
				
				& Snowflake~\cite{snowflake2017iscas} 
				& \begin{tabular}[l]{@{}l@{}} \tool: \\ ResNet50* \end{tabular}
				& xDNN~\cite{xdnn2020xilinx}
				& DNNVM~\cite{dnnvm2019tcad}
				& \multicolumn{2}{l}{ALAMO~\cite{alamo2020tcad}}
				
				& ResNetAccel~\cite{residaccel2017iscas} 
				& FTDL~\cite{ftdl2020dac}
				& Cloud-DNN~\cite{clouddnn2019fpga}
				& \begin{tabular}[l]{@{}l@{}} Scaling the \\ Cascades~\cite{scaling_the_cascades2019fpl}
				\end{tabular}
				& Full-Stack~\cite{fullstack2021tnnls}
				& \begin{tabular}[l]{@{}l@{}} \tool:\\ ResNet50* \end{tabular}
				\\
				\cmidrule{6-7}
				\midrule
				FPGA  
				& Z7045 
				& Z7045 
				& VU9P 
				& ZU9
				& Arria 10 GX1150
				% & \begin{tabular}[l]{@{}l@{}} Kintex-7 \\ 325T \end{tabular}
				& Stratix 10 GX2800
				& Arria 10 GX1150 
				% & \begin{tabular}[l]{@{}l@{}} Virtex-7 \\ 485T \end{tabular}
				& VU125
				% & \begin{tabular}[l]{@{}l@{}} Virtex-7 \\ 690T \end{tabular} 
                & VU9P
                & VU37P
                & Arria 10 GX1150
                & ZU7EV 
				\\
				Clock (MHz) & 250 & 150 & 500 & 500 & 240 & 150 & 300 & 650 & 125 & 650 & 200 & 200 \\
				Precision & 16b fixed & 16b fixed & 8b fixed & 8b fixed & 16b fixed & 16b fixed & 16b fixed & 16b fixed & 16b fixed & 8b fixed & 8b fixed & 16b fixed \\
				DSPs$^\dagger$ & 900 & 900 & 6840 & 2520 & 3036 & 11,520 & 3036 & 1200 & 3036 & 9024 & 3036 & 1728 \\
				Logic Capacity & 218.6 kLUTs & 218.6 kLUTs & 1182.0 kLUTs & 274.0 kLUTs & 427.2 kALMs & 933.0 kALMs & 427.2 kALMs & 716.0 kLUTs & 1182 kLUTs & 1304 kLUTs & 427.2 kALMs & 230.0 kLUTs \\
				%	Fixed-point DSPs* & 220 & 900 & 220 & 900 \\
				Block RAM & 2.40 MB & 2.40 MB & 9.48 MB & 4.01 MB & 6.60 MB & 28.62 MB & 6.60 MB & 11.075 MB & 43.23 MB & 42.61 MB & 6.60 MB & 4.75 MB \\
				{\color{black}DSP Util.$^\dagger$} & 28.4\% & 100\% & 100\% & 83.8\% & 80\% & 80\% & 56.8\% & 100\% & 80.2\% & 95\% & 97\% & 100\% \\
				
				\begin{tabular}[t]{@{}l@{}} 
					inf/s
				\end{tabular} 
				& 17.7
				& 28.18
				& 153.57
				& 80.95
				& 71.38
				& 77.55
				& 33.93
				& 151.22
				& 71.94
				& 766
				& 197.23
				& 71.71
				\\
				\begin{tabular}[t]{@{}l@{}} 
					inf/s/DSP$^\dagger$
				\end{tabular} 
				& 0.0196
				& 0.0313
				& 0.0112
				& 0.016
				& 0.0235
				& 0.0067
				& 0.0111
				& 0.1260
				& 0.0105
				& 0.0424
				& 0.0324
				& 0.0415
				\\
				\begin{tabular}[t]{@{}l@{}} 
					inf/s/Logic
				\end{tabular} 
				& 0.0809
				& 0.1289
				& 0.0649
				& 0.1477 
				& 0.1671
				& 0.0831
				& 0.0794
				& 0.2112
				& 0.0608
				& 0.5874
				& 0.4616
				& 0.3117
				\\

				\bottomrule
				
			\end{tabular} 
			\begin{tablenotes}
				\normalsize
				\item * using OVSF50, ** batch size = 1, $\dagger$ 18$\times$18, 19$\times$18 and 25$\times$18 DSP configurations, inf/s/DSP is adjusted based on precision for fair comparison (multiplied by 0.5 for 8b).
			\end{tablenotes}
			
		\end{threeparttable}
	}
	\vspace{0.2cm}
	\label{tab:resnet50_comparison_table}
\end{table*}


\subsubsection{\textbf{Comparison with Existing FPGA Designs}}
\label{sec:fpga_comparison}

To assess the performance of the proposed framework with respect to existing FPGA work, we perform a number of comparison with a broad range of state-of-the-art works that optimise CNN inference from different aspects. These span accelerators that aggressively apply compiler techniques~\cite{snowflake2017compiling,snowflake2017iscas,xdnn2020xilinx,dnnvm2019tcad}, the highest performing FPGA-based accelerators for sparse~\cite{sparsecnnaccel2019fccm} and lightweight CNNs~\cite{lightopu2020fpga}, a multi-accelerator design that addresses PE underutilisation for SqueezeNet~\cite{maximising2017isca}, a state-of-the-art CNN-to-FPGA toolflow~\cite{alamo2020tcad}, an optimised overlay architecture~\cite{ftdl2020dac}, a highly customised accelerator for residual networks~\cite{residaccel2017iscas}, a cloud-optimised framework~\cite{clouddnn2019fpga}, a CNN accelerator designed in an interconnect-aware manner~\cite{scaling_the_cascades2019fpl} and an accelerator that applies full-stack optimisations~\cite{fullstack2021tnnls}. 

Table~\ref{tab:comparison_table} lists the performance results for ResNet18/34 and SqueezeNet. On Z7045, \tool achieves 2.33$\times$ and 1.12$\times$ higher throughput than \cite{snowflake2017compiling} and \cite{sparsecnnaccel2019fccm}, respectively. For SqueezeNet, our design delivers 1.83$\times$ and 1.67$\times$ higher performance density in inf/s/DSP and inf/s/Logic than Light-OPU~\cite{lightopu2020fpga}. 
Compared to the multi-accelerator design~\cite{maximising2017isca} that also addresses the PE underutilisation, \tool yields 1.40$\times$ higher inf/s/DSP and 1.14$\times$-1.27$\times$ higher inf/s/Logic despite having the same (V48T-based design~\cite{maximising2017isca}) or 36\% lower (V690T-based design~\cite{maximising2017isca}) on-chip memory budget.

The original ResNet50 reaches 76.15\% accuracy with a model size of 25.56M parameters. With \tool's ResNet50-OVSF50 variant improves accuracy to 76.23\% while having 10\% fewer parameters (22.84M).
Table~\ref{tab:resnet50_comparison_table} presents the measured performance results for ResNet50. On Z7045, \tool outperforms Snowflake by 1.59$\times$ in inf/s. Compared with designs on larger devices, our design achieves higher performance density (inf/s/DSP) by 3.69$\times$, 2.58$\times$, 1.76$\times$-6.16$\times$, 3.17$\times$ and 3.94$\times$ over xDNN, DNNVM, ALAMO, ResNetAccel and Cloud-DNN. The overlay-based FTDL reaches higher inf/s/DSP and 1.47$\times$ lower inf/s/Logic, but targets a platform with 2.33$\times$ larger on-chip memory and 2$\times$ higher bandwidth, both of which substantially reduce the off-chip memory accesses and the associated latency. Similarly, compared to the interconnect-aware design of \cite{scaling_the_cascades2019fpl}, \tool reaches 97.8\% of its inf/s/DSP, despite using a platform with 8.9$\times$ smaller on-chip memory. Finally, \tool outperforms the full-stack-optimised accelerator of \cite{fullstack2021tnnls} by 1.28$\times$ in inf/s/DSP.

\textbf{Discussion.}
Based on the presented evaluation, \tool consistently outperforms a wide range of FPGA-based accelerator designs, in spite of their diverse designs.
As such, our framework delivers an average throughput gain of 2.23$\times$ (2.05$\times$ geo. mean) over designs that aggressively apply compiler optimisations on fixed accelerators~\cite{snowflake2017compiling,snowflake2017iscas,xdnn2020xilinx,dnnvm2019tcad} and, at the same time, achieves an average inf/s/DSP gain of 2.5$\times$ (2.41$\times$ geo. mean) over highly customised CNN-tailored designs~\cite{lightopu2020fpga, residaccel2017iscas} and 3.94$\times$ over the cloud-optimised mapping of Cloud-DNN.
A notable comparison is with the sparse CNN accelerator for ResNet34 presented in~\cite{sparsecnnaccel2019fccm}, with \tool achieving 12\% throughput gain. It should be noted that the sparse CNN accelerator applies Deep Compression~\cite{deepcompression2015iclr} to sparsify the target CNN, employs a specialised dataflow and modifies the underlying PEs in order to extract high performance. 
In contrast, \tool improves the performance of CNN engines while affecting neither the selected dataflow nor the internal design of the PEs, and still delivers 12\% higher throughput than the sparse CNN accelerator. 

\begin{table}[b]
\rev{
	\centering
    \vspace{0.3cm}
    \captionsetup{font=small,labelfont=bf}
	\caption{\rev{\footnotesize Resource usage breakdown of \tool's designs.}}
    \vspace{-0.2cm}
	\resizebox{0.65\linewidth}{!}{%
		\begin{tabular}{l l l l l l}
			\toprule
			\multicolumn{1}{l}{\multirow{1}{*}{\textbf{Design Config.}}} & \textbf{Platform} & \textbf{Resource Type} & \multicolumn{1}{c}{\multirow{1}{*}{\textbf{\texttt{CNN-WGen}}}} & \textbf{CNN Engine} \\
			\midrule
			\multirow{2}{*}{ResNet18-OVSF50} & \multirow{2}{*}{ZC706} 
			& DSPs & 7.5\% & 92.5\% \\
            & & LUTs & 1\% & 74\% \\
	          \midrule		
            \multirow{2}{*}{ResNet34-OVSF50} & \multirow{2}{*}{ZC706}  
			& DSPs & 11.3\% & 88.7\% \\
            & & LUTs & 3\% & 75\% \\
            \midrule
            \multirow{2}{*}{ResNet50-OVSF50} & \multirow{2}{*}{ZC706}  
			& DSPs & 11.1\% & 88.9\% \\
            & & LUTs & 3\% & 75\% \\
			\bottomrule
		\end{tabular}%
	}
	\label{tab:rsc_usage}
    % \vspace{0.2cm}
    }
\end{table}

\subsubsection{\textbf{Resource Usage}}
\label{sec:rsc_usage}
We select \tool and baseline designs with up to 1-pp accuracy drop and compare their post place-and-route resource usage on Z7045, reported in [DSPs, BRAM, LUTs] tuples for 4$\times$ bandwidth.
For ResNet34, the faithful baseline consumes {\small [99\%,83\%,77\%]}, Tay82 {\small [99\%,79\%,77\%]}, OVSF50 {\small [100\%,81\%,78\%]} and Tay82+OVSF50 {\small [100\%,87\%,81\%]}. For ResNet18, the faithful baseline {\small [78\%,99\%,70\%]}, Tay88 {\small [78\%,99\%,69\%]}, OVSF50 {\small [100\%,87\%,75\%]} and Tay82+OVSF50 {\small [100\%,83\%,80\%]}. For ResNet50, OVSF50 on ZU7EV consumes {\small [100\%,87\%,78\%]}. Finally, the input selective PE mechanism adds a minimal LUTs overhead of less than 7\%. \rev{We further report the breakdown of resource consumption between \texttt{CNN-WGen} and the CNN Engine in Table~\ref{tab:rsc_usage}. We observe that, using our performance model, the DSE stage is able to balance the allocation of DSPs between the two modules. Moreover, the LUT overhead of the weights generator is minimal compared to the CNN Engine, providing a beneficial trade-off.}



% Figure environment removed

\subsection{Sensitivity to Off-Chip Memory Bandwidth}
\label{sec:mem_bandwidth_eval}
Fig.~\ref{fig:bw_sensitivity} shows the impact of varying off-chip memory bandwidth over performance on the two target platforms. The figure compares the speedup of \tool and the Tay82 baseline over the vanilla baseline when varying the external memory bandwidth from 1$\times$ to 12$\times$. The bandwidth's impact is most prominent on the larger ZU7EV, where the performance gains are sustained higher across 1$\times$-4$\times$. In the case of the mid-tier Z7045, we observe a sharper drop in the speedup as the bandwidth increases. This is due to the more limited computational resources of Z7045, which makes most CNN layers compute-bound. In contrast, the abundance of computational resources on ZU7EV makes the CNN layers more memory-bound. For instance, at 4$\times$ bandwidth (4.5 GB/s), the vanilla ResNet18 baseline yields DSP utilisation of 71\% on the compute-bound Z7045 and 53\% on the more memory-bound ZU7EV. In this case, \tool significantly improves both cases by mapping ResNet18-OVSF25 with 89\% and 71\% DSP utilisation.
As a result, \tool sustains its gains across a wider range of bandwidths and outperforms Tay82, until the bandwidth-abundant case (12$\times$) where computational resources become the critical factor. In this case, Tay82's lower number of operations due to pruning leads to higher performance.

Across the designs, the input selective PEs contribute an additional speedup of up to 20\%, with varying gains depending on the CNN-FPGA pair and the available bandwidth. For ResNet34-OVSF25 on ZU7EV, disabling this mechanism leads to 0/13.9/3.3/5.9\% lower throughput for the four bandwidths, with a similar pattern observed for the rest of the CNNs. Our input selective PEs effectively improve the performance of suboptimally mapped layers in compute-bound settings, whereas no gain is obtained for the most bandwidth-constrained case (1$\times$) where the designs are severely memory-bound, limiting further improvements through higher PE utilisation.


\subsection{Impact of Input Selective PEs}
\label{sec:input_sel_pes_eval}
Here, we evaluate the impact of input selective PEs on the achieved performance.
This is investigated by implementing \tool's selected hardware design for each of the benchmark CNNs with and without the input selective PEs and comparing the achieved performance, measured on the two target FPGA platforms. When the input selective PEs are omitted, we call the designs \textit{ablated}. 
Table~\ref{tab:input_sel_pes_eval} presents the achieved performance gains between the two designs.


\begin{table}[t]
    % \vspace{-0.2cm}
    \small
    \centering
    \captionsetup{font=small,labelfont=bf}
    \caption{Ablation study of input selective PEs.}
    \vspace{-0.1cm}
    \resizebox{0.5\linewidth}{!}{
    \begin{tabular}{l l l r l c}
        \toprule
        \multicolumn{1}{l}{Model} & & FPGA & \multicolumn{2}{c}{Input Selective PEs} & Performance \\
        \multicolumn{1}{l}{Arch.} & & Platform & \multicolumn{1}{c|}{without} & with & Gain \\
        \midrule
        
        & OVSF50 & Z7045 & 49.9 inf/s & 49.9 inf/s & 1.00$\times$ \\
        \multicolumn{1}{l}{\multirow{1}{*}{ResNet18}} & OVSF25 & Z7045 & 50.4 inf/s & 51.0 inf/s & 1.01$\times$ \\
        \cmidrule{2-6}
        & OVSF50 & ZU7EV & 124.1 inf/s & 124.1 inf/s & 1.00$\times$ \\
        & OVSF25 & ZU7EV & 135.2 inf/s & 135.2 inf/s & 1.00$\times$ \\
        
        \midrule
        
        & OVSF50 & Z7045 & 25.4 inf/s & 31.1 inf/s & 1.22$\times$ \\
        \multicolumn{1}{l}{\multirow{1}{*}{ResNet34}} & OVSF25 & Z7045 & 33.5 inf/s & 33.3 inf/s & \phantom{1}0.6\% \\
        \cmidrule{2-6}
        & OVSF50 & ZU7EV & 81.1 inf/s & 81.1 inf/s & 1.00$\times$ \\
        & OVSF25 & ZU7EV & 72.4 inf/s & 88.0 inf/s & 1.21$\times$ \\
        
        \midrule
        
        & OVSF50 & Z7045 & 23.7 inf/s & 27.0 inf/s & 1.14$\times$ \\
        \multicolumn{1}{l}{\multirow{1}{*}{ResNet50}} & OVSF25 & Z7045 & 23.7 inf/s & 28.1 inf/s & 1.18$\times$ \\
        \cmidrule{2-6}
        & OVSF50 & ZU7EV & 63.1 inf/s & 71.7 inf/s & 1.13$\times$ \\
        & OVSF25 & ZU7EV & 68.5 inf/s & 77.8 inf/s & 1.13$\times$ \\
        
        \midrule
        
        % & OVSF50 & Z7045 & inf/s &  inf/s & $\times$ \\
        % \multicolumn{1}{c}{\multirow{1}{*}{SqueezeNet}} & OVSF25 & Z7045 & inf/s &  inf/s & $\times$ \\
        \multicolumn{1}{l}{\multirow{1}{*}{SqueezeNet}} & OVSF50 & ZU7EV & 724.2 inf/s & 792.2 inf/s & 1.09$\times$ \\
        & OVSF25 & ZU7EV & 731.4 inf/s & 800.6 inf/s & 1.09$\times$ \\
        \midrule
        \midrule
        Average & & & & & 1.12$\times$ \\
        Geo. Mean & & & & & 1.11$\times$ \\
        \bottomrule
    \end{tabular}
    }
    \vspace{0.2cm}
    \label{tab:input_sel_pes_eval}
\end{table}

The PE-enhancing mechanism contributes varying throughput gains, yielding up to 22\% faster inference and an average improvement of 12\% (11\% geo. mean). 
For ResNet18, the ablated designs already sustain high DSP utilisation with ResNet18-OVSF50 and -OVSF25, reaching 90\% and 86.5\% of the theoretical peak performance of Z7045 and ZU7EV, respectively.
On the other hand, the ablated ResNet34-OVSF50 design on Z7045 achieves only 69.6\% of the theoretical peak throughput. Similarly, the ablated ResNet34-OVSF25 design on ZU7EV achieves 77.5\% of the theoretical performance. In both cases, the input selective PEs are able to substantially increase the DSP utilisation, with the enhanced CNN engines achieving 85.1\% and 94.2\% of the peak performance, respectively.

A similar effect is observed for ResNet50 and SqueezeNet. The ablated designs yield 73.8\% of the theoretical peak performance for both OVSF50 and OVSF25 on Z7045, and 76.7\% and 83.3\% for OVSF50 and OVSF25, respectively, on ZU7EV. In this case, our input selective PEs are able to improve the DSP utilisation, achieving 84.1\% and 87.4\% of the peak throughput on Z7045 for OVSF50 and OVSF25, respectively, and 87.2\% and 94.7\% for OVSF50 and OVSF25, respectively, on ZU7EV. Finally, for SqueezeNet, the input selective PEs improve the measured throughput from 73.2\% to 80.1\% of the peak performance for OVSF50 and from 73.9\% to 80.9\% for OVSF25 on ZU7EV. As such, enhancing a CNN engine's PEs with our proposed input selectivity technique alleviates the resource underutilisation due to the diverse layer shapes within a CNN. In the cases where our technique is estimated to provide minimal gains (\textit{i.e.} $<$5\%) and its usage is not justified, \tool opts for omitting it to save LUT resources.


% 691.2
% 518.4
% 270


% Figure environment removed


\subsection{Hardware-Aware vs. Manual Tuning of OVSF Ratios}
\label{sec:eval_hw_aware_ratios}

Next, we evaluate the effectiveness of our hardware-aware tuning of OVSF ratios in yielding designs with improved accuracy-performance trade-off. To this end, we compare against two ratio selection methods: \textit{i)}~\texttt{uniform-$\rho$} which uses the same ratio $\rho$ across all layers, with the exception of the first CONV layer. This baseline represents a brute-force approach of setting the OVSF ratios; and \textit{ii)}~\texttt{manual-OVSF50} and \texttt{manual-OVSF25} which use the manually selected ratios detailed in Section~\ref{sec:training_scheme} to achieve 50\% and 75\% reduction in model size from the original model. This baseline constitutes an optimised hand-engineered method. We perform the comparison by implementing ResNet18 and ResNet34 using both the hardware-aware and the baseline flows for different bandwidth availability and comparing the achieved performance, measured on Z7045.

Figure~\ref{fig:hw_autotune_resnet34_low_bw} shows the achieved accuracy and execution time measured on the target FPGA and depicts how our method, denoted by \texttt{hw-aware autotuning}, yields Pareto-optimal designs that were previously unattainable. For ResNet34, our method sustains the same performance as the fast OVSF25 design across all memory bandwidths. However, it additionally improves OVSF25's accuracy by 0.8pp, thus outputting design that are within 1pp of the original model's accuracy (72.3\% for all three bandwidths vs 73.3\% for the vanilla ResNet34). At the same time, it is consistently faster than the coarse \texttt{uniform-0.5}. 
We obtain similar results for ResNet18, with the same processing speed as OVSF25 and accuracy gains in the range of 0.3pp-1.2pp (0.86pp average gain across bandwidths) over OVSF25. Across all cases, the \texttt{uniform-$\rho$} baselines were either unnecessarily slow (\texttt{uniform-0.5}) or low in accuracy (\texttt{uniform-0.25}), further advocating for a principled method of selecting the OVSF ratios.

By exploiting the bounding factor of each layer, our hardware-aware scheme selectively allows for a longer weights generation stage without affecting the processing speed. As such, we can obtain a better approximation of the weights and sustain high throughput. As shown through our experiments, the hardware-aware methodology yielded competitive designs, performing either better or in par even against highly optimised hand-tuned configurations (OVSF50 and OVSF25).
\stelios{Added this.}


\subsection{Comparison with Embedded GPU}
\label{sec:gpu}

With the majority of CNNs deployed for inference on embedded and mobile devices, our evaluation focuses on the embedded space. In power-constrained applications, the main metrics of interest comprise: \textit{1)}~the absolute power consumption and \textit{2)}~the energy efficiency in performance-per-watt. In this respect, we investigate the energy efficiency of \tool in relation to the widely used high-performance NVIDIA Jetson TX2 platform. In all cases, for \tool we use the OVSF50 variant with less than 1-pp accuracy drop.

For the performance evaluation on TX2, we use NVIDIA TensorRT as supplied by the JetPack~4.1.1 package. TensorRT is run with the NVIDIA cuDNN library and 16-bit half-precision floating-point arithmetic (FP16), which enables the highly optimised execution of layers. Across all platforms, each CNN is run 100 times to obtain the average throughput. Furthermore, power measurements for the GPU and FPGAs are obtained via a power monitor on the corresponding board. In all cases, we subtract the average idle power from the measurement to obtain the power due to benchmark execution. The idle power of the FPGA platforms is measured at the board level with no design programmed in the FPGA fabric, so that the clock tree power and the power leakage of the chip are also included in the run-time power due to benchmark execution.
Across all experiments, we used a batch size of 1, as is typical in mobile and embedded settings.

% Figure environment removed

Tegra X2 mounts a 256-core GPU with native support for FP16 arithmetic which supports a range of operating modes with different clock frequencies and power consumption. To perform a fair comparison with respect to energy efficiency in terms of performance-per-watt, we configure the GPU with the maximum energy efficiency mode (Max-Q) which sets the frequency of the GPU at 850~MHz and configures all components of TX2 to achieve the best power-throughput trade-off. Fig.~\ref{fig:gpu_comparison} presents the conducted comparison. \tool achieves an energy efficiency improvement over TX2 of up to 5.32$\times$ in inf/s/W with an average of 2.57$\times$ (2.31$\times$ geo. mean) across the benchmarks. As a result, \tool consistently demonstrates significant gains in performance-per-watt across the benchmarks over highly optimised embedded GPU implementations.

