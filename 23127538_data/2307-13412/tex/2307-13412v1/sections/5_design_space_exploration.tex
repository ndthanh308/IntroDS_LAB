
\section{Design Space Exploration}
\label{sec:dse}

Based on its parametrisation of the processing engine, buffer sizes and weights generator, \tool defines a particular architectural design space. To estimate the performance and resource usage of different configurations, an analytical modelling framework has been developed. 
At a high-level, the key decisions for yielding a high-performance configuration of the system are: the allocation of the on-chip resources between the CNN engine and the weights generator and, the sizes of the activations buffers.
The design-time tunable parameters comprise \textit{1)}~$M$ that determines the TiWGen's tile size and the size of \texttt{CNN-WGen}'s vector units, \textit{2)}~tile sizes $T_C$ and $T_P$ that determine the number of PEs and MACs per PE, respectively, and $T_R$ affecting the size of the activations buffers.

\subsection{Performance Model}
\label{sec:perf_model}

The workload of a CNN with $N_L$ layers is represented as a sequence of {\small $W_i$$=$$\left<R_i,P_i,C_i\right>$} \textit{workload tuples} with $i~\in~\{1,...,N_L\}$.
Given a design point {\small $\sigma$$=$$\left<M,T_R,T_P,T_C\right>$}, the \texttt{CNN-WGen}'s runtime for generating the i-th layer's weights required 
to compute a {\small $(T_R\times T_C)$} 
output tile is given by
\vspace{-0.1cm}
%
\begin{equation}
    \footnotesize
    \vspace{-1mm}
    % \resizebox{0.7\linewidth}{!}{
    t_{\texttt{CNN-WGen}}^i(\sigma, W_i) = \left\lfloor \rho \cdot l \right\rfloor \cdot \left\lceil \frac{T_P \cdot T_C}{M} \right\rceil \cdot \left\lceil \frac{P_i}{T_P} \right\rceil
    % }
    \vspace{-0.5mm}
    \label{eq:wgen_exec_time}
\end{equation}
% 
where $\rho$ and $l$ are the OVSF ratio and basis length, respectively, and with one factor for each of the pipelined loops in Algorithm~\ref{alg:tiled_weights_gen}.
With $\alpha$ values transferred upfront and the OVSF method generating all weights on-chip, the off-chip memory transfers involve only the input/output activations
% 
\begin{equation}
    \footnotesize
    % \resizebox{0.45\textwidth}{!}{
    t_{\text{mem in}}^i(\sigma, W_i) = \frac{T_R \cdot P \cdot WL}{bw_\text{in}}, \quad t_{\text{mem out}}^i(\sigma, W_i) = \frac{T_R \cdot T_C \cdot WL}{bw_\text{out}}
    % }
    \label{eq:transfer_times}
\end{equation}
% 
where $WL$ is the adopted wordlength, and $bw_{\{\text{in},\text{out}\}}$ are the memory bandwidths for transferring inputs/outputs.


With $T_C$ and $T_P$ dimensions unrolled, computing an output tile %by the accelerator's CNN engine 
requires the pipelined processing of $\frac{P_i}{T_P}$ tiles for each of the $T_R$ rows. Hence, the processing engine's runtime for each output tile is estimated as {\small $t_{\text{eng}}^i(\sigma, W_i) = T_R \left\lceil \frac{P_i}{T_P} \right\rceil$}. With the input selective PEs, the runtime is refined as
% 
\begin{equation}
    \footnotesize
    % \resizebox{0.45\textwidth}{!}{
    t_{\text{eng}^*}^i (\sigma, W_i) = \left( T_C-C_i + \left\lceil \frac{T_R \cdot C_i - (T_C-C_i) \cdot (C_i+1)}{T_C} \right\rceil \right) \cdot \left\lceil \frac{P_i}{T_P} \right\rceil
    % }
    \label{eq:cnn_engine_exec_time_enhanced}
\end{equation}
% 
where dimension $T_R$ is partially unrolled by processing rows of $T_R$ through the underutilised PEs.

Overall, the accelerator forms a pipeline of three coarse stages:
the concurrent input transfer and weights generation, the CNN engine processing and the output transfer. 
In this context, the initiation interval of the architecture is given by the maximum initiation interval of the three-stage pipeline, calculated as
% 
% \vspace{-1mm}
\begin{equation}
    \footnotesize
    \vspace{1mm}
    % \resizebox{0.91\linewidth}{!}{
    II^i(\sigma, W_i) = \max\left( \max\left(t_{\text{mem in}}^i, t_{\texttt{CNN-WGen}}^i \right), t_{\text{eng}^*}^i, t_{\text{mem out}}^i \right)
    % }
    \label{eq:initiation_interval}
\end{equation}
% 
As such, the total runtime for layer $i$ is given by 
\mbox{{\footnotesize $t_\text{total}^i(\sigma, W_i)=II^i(\sigma, W_i) \left\lceil \frac{R_i}{T_R} \right\rceil \left\lceil \frac{C_i}{T_C} \right\rceil$}}.
Thus, for a CNN with $N_L$ layers, the workload tuple is {\footnotesize $W$$=$$\left<W_i ~|~ \forall i \in \{1, ..., N_L \} \right>$} and the throughput in inferences per sec (inf/s) is estimated as \mbox{{\footnotesize $T(\sigma, W) = 1/\sum\limits_{i=1}^{N_L} t_\text{total}^i (\sigma, W_i)$}}.

\subsection{Resource Consumption Model}
The primary factor that constrains the mapping of a CNN engine on a given platform is resource availability. Each candidate configuration has a corresponding resource consumption. We define the \textit{feasible space} of our model as the set of configurations that satisfy all the platform-specific resource constraints. 
In our context, the main design constraints are the DSPs and on-chip RAM blocks of the target FPGA. Assuming that all MAC operators are mapped to DSPs, the values of {\small $
\left<M,T_P,T_C\right>$} are constrained as \mbox{{\footnotesize $D_\text{MAC} \times (M + T_P T_C) \leq D_\text{fpga}$}},
with {\footnotesize $D_{\text{fpga}}$} the available DSPs and {\footnotesize $D_\text{MAC}$} the DSPs/MAC. We consider 16-bit fixed-point precision, where {\footnotesize $D_\text{MAC}$$=$$1$} on the evaluated Xilinx FPGAs.
%In our case, 16-bit fixed-point precision is used, where {\footnotesize $D_\text{MAC}$$=$$1$} on the evaluated Xilinx FPGAs.


% From a resource perspective, the main design constraints are the DSPs and on-chip RAM blocks of the target FPGA. Assuming that all MAC operators are mapped to DSPs, the values of {\small $
% \left<M,T_P,T_C\right>$} are constrained as {\footnotesize $D_\text{MAC} \times (M + T_P T_C) \leq D_\text{fpga}$},
% with {\footnotesize $D_{\text{fpga}}$} the available DSPs and {\footnotesize $D_\text{MAC}$} the DSPs/MAC. In our case, 16-bit fixed-point precision is used, where {\footnotesize $D_\text{MAC}$$=$$1$} on the evaluated Xilinx FPGAs.

In terms of on-chip RAM, the accelerator has the I/O and Alpha buffers with wordlength $WL$ and the binary OVSF FIFO, with a total capacity requirement as given by Eq.~(\ref{eq:onchip_ram}).
% 
% \vspace{-1mm}
\begin{equation}
    % \footnotesize
    \vspace{1mm}
    % \resizebox{0.91\linewidth}{!}{
    \left(2(T_R T_P + T_R T_C) + D^{\text{Alpha}}N_P^{\text{Alpha}}\right) WL + K_\text{max}^2K_\text{max}^2 \leq C_\text{fpga}
    % }
    \label{eq:onchip_ram}
\end{equation}
% 
where the factor of 2 accounts for double-buffering and $C_\text{fpga}$ is the on-chip RAM capacity of the target device.

To further estimate the consumption of look-up tables (LUTs), we used a set of place-and-route measurements and fitted linear regression models as a function of \tool's tunable parameters.
% 
Overall, we formally capture the %on-chip 
resource consumption of a design point $\sigma$ by means of vector $\textbf{\textit{rsc}}(\sigma)$ that holds the utilised amount of DSPs, BRAMs and LUTs. Similarly, we denote the FPGA resource vector by $\textbf{\textit{rsc}}_\text{Avail.}$. 

\subsection{Configuration Optimisation Framework}
To yield the highest performing design for the given CNN-FPGA pair, we cast the DSE task as a constrained optimisation problem that aims to determine the values of the configurable parameters $\left< M, T_R, T_P, T_C \right>$ that achieve the highest performance for the target CNN and available hardware resources. Formally, we express this setup as
% 
\begin{equation}
    \footnotesize
    \min\limits_{\sigma=\left<M,T_R,T_P,T_C\right>} T\left(\sigma, W \right) \quad \text{s.t.} \quad \textbf{\textit{rsc}}(\sigma) \leq \textbf{\textit{rsc}}_\text{Avail.}
    \label{eq:opt}
\end{equation}
%
where $T$, $\textbf{\textit{rsc}}$ and $\textbf{\textit{rsc}}_{\text{Avail.}}$ are the throughput in inferences per second (inf/s), the resource consumption of the current design point $\sigma$ and the resource vector of the target platform, respectively.
% Given a CNN-FPGA pair, we perform exhaustive search, exploring different resource allocations between \texttt{CNN-WGen} and the processing engine. 
Given a CNN-FPGA pair, we perform exhaustive search for different resource allocations between \texttt{CNN-WGen} and the processing engine. 
Designs that violate the resource constraints are pruned as infeasible to accelerate the exploration.