
\section{Introduction}
\label{sec:intro}

The unparalleled accuracy of convolutional neural networks (CNNs) across a broad range of AI inference tasks has led to the development of novel applications. With a large portion deployed across mobile and embedded devices, there is a need for high-performance, energy-efficient implementations that can deliver responsiveness and prolonged battery life. Currently, the conventional computing platforms for CNN inference comprise either CPUs and GPUs~\cite{fb_edge2019hpca,oodin2021smartcomp,embench2019emdl} or custom application-specific integrated circuits (ASICs), such as neural processing units (NPUs)~\cite{embench2019emdl, ai_benchmark2019iccvw,samsung_npu2021isca}. On the one hand, CPU- and GPU-based systems can support diverse CNN models through their programmability, but penalise performance in order to provide this generality~\cite{embench2019emdl}. On the other hand, ASICs provide significant acceleration under a minimal power envelope~\cite{ai_benchmark2019iccvw}. Nevertheless, the benefits of ASICs typically require the functionality to remain fixed after fabrication, leaving no room for applying model-specific optimisations~\cite{residaccel2017iscas,mobilenet_accel2021fpl} or mapping newer CNN models~\cite{fpgaconvnet2019tnnls}.

To provide a balance between flexibility and high performance, numerous CNN accelerators target reconfigurable hardware platforms, such as field-programmable gate arrays (FPGAs). Currently, the FPGA-based accelerator landscape spans a wide spectrum, from flexible CNN-specific processors~\cite{snowflake2017iscas,lightopu2020fpga,uniopu2020tvlsi} to highly customised streaming architectures~\cite{streaming2016fpl,fpgaconvnet2019tnnls,finn2017fpga,lutnet2020toc}. One of the most widely adopted paradigms that lies in the midpoint of the flexibility-customisation spectrum is the single computation engine (SCE)~\cite{fpdnn2017fccm,dla2018fpl,cascadecnn2020date,caffeine2019tcad,dnnvm2019tcad,abdelfattah2020best,gamma2020iccad,cascadecnn2018fpl,fft2020fpga,alamo2020tcad}. Under this paradigm, a powerful processing engine is time-shared to sequentially execute the layers of a CNN. 
% With this approach, the accelerator resources are reused across both layers and CNN models, without the need to reconfigure the fabric upon deployment.
This allows for accelerator's resources to be reused across both layers and CNN models, without the need to reconfigure the fabric.



Despite the flexibility of SCEs, their attainable performance is often bounded by two primary factors:
\textit{1)}~layers with low computation-to-communication ratio that become memory-bound~\cite{cascadecnn2018fpl,fft2020fpga,alamo2020tcad} and \textit{2)}~the suboptimal mapping of diverse layers on the fixed configuration of the SCE that leads to underutilised processing elements (PEs)~\cite{alamo2020tcad,latency2017fpl,cnnroofline2015fpga}. These two obstacles set a hard limit to the actual sustained performance that this family of accelerators can achieve, indicating an emerging need for novel solutions to counteract their impact.

Concurrently with the continuing hardware advances, a growing body of work focuses on compressing CNNs through a class of lossy non-structural methods~\cite{ha2016hypernetworks, pmlr-v80-qiu18a, fernandez2018binarycmd, ijcai2018-380, ovsf2018emdl, Yang2020FSNet}.
% , orthogonal to other model simplification techniques such as pruning or quantisation, focuses on compressing CNNs through lossy non-structural methods~\cite{ha2016hypernetworks, pmlr-v80-qiu18a, fernandez2018binarycmd, ijcai2018-380, ovsf2018emdl, Yang2020FSNet}.
Orthogonally to other model simplification techniques such as pruning or quantisation, this group of methods dictates that the weights of a model are deployed in a compact form and are ``inflated" only at run time. Given that several CNN layers are constrained by the limited off-chip memory bandwidth of the target computing platforms~\cite{latency2017fpl,cascadecnn2018fpl,fft2020fpga,alamo2020tcad}, storing the compressed weights on-chip and reconstructing them on-the-fly can play a decisive role in alleviating the memory-boundedness and enabling the better utilisation of the computational resources.

Nevertheless, the novel dataflow and execution scheme 
of such models brings up a new challenge regarding their optimised mapping.
Existing accelerators have been designed for conventional deep models, adopting either a streaming or layer-by-layer execution~\cite{cnnfpgatoolflows2018csur}.
Hence, despite the significant potential of on-the-fly models, their different execution paradigm renders conventional architectures futile in serving them. 

This paper presents a novel CNN system that overcomes the withstanding limitations of single computation engines and enables the efficient and high-performance execution of on-the-fly models. At the core of the proposed system lies a novel CNN hardware architecture. To alleviate the impact of the memory wall, we introduce a hardware-based weights generator that is responsible for efficiently generating the CNN weights on-the-fly. 
Comprising a custom memory organisation and a highly optimised datapath, the weights generator is scalable, with tunable parameters that allow it to be tailored to the needs of the target application, the workload characteristics of the CNN and, the capabilities of the FPGA device.
To counteract the underutilisation of computational resources due to the suboptimal mapping of diverse layers, we further propose a novel CNN engine design comprising input selective PEs. Under this design, a subset of PEs is enhanced with efficient switches that enable neighbouring PEs to perform load-balancing through seamless work-stealing.
Finally, we present a framework for deriving on-the-fly models from pre-trained CNNs and mapping them on a given FPGA.

The initial work in~\cite{unzipfpga2021fccm} provided a high-level overview of the proposed framework and its on-the-fly weights formulation.
Moreover, the evaluation solely focused on end-to-end performance compared to other FPGA works.
In this paper, we first present the complete framework, encompassing both the algorithmic and hardware aspects (Section~\ref{sec:design_flow}). As such, we provide a detailed description of the on-the-fly weights generation process and its algorithmic underpinning on OVSF codes (Section~\ref{sec:background_cnn_ovsf}), and we position the proposed hardware architecture with respect to the status-quo CNN engines (Section~\ref{sec:arch}).

Moreover, in the initial work, the compression ratios of each CNN layer were manually selected based on heuristics. In this work, we make steps towards automation by introducing a hardware-aware scheme for tuning the per-layer compression ratios of OVSF models (Section~\ref{sec:hw_aware_ratios}). The proposed method exploits the bottleneck characteristics of each layer in order to generate more accurately its weights while sustaining high throughput, leading to a better accuracy-performance trade-off (Section~\ref{sec:eval_hw_aware_ratios}).

Finally, we present for the first time an in-depth evaluation of various critical aspects of the proposed framework. These include the evaluation of two techniques to derive on-the-fly models from vanilla CNNs (Section~\ref{sec:basis_selection_and_3x3_extraction}), a thorough study of the impact of the proposed input selective PEs (Section~\ref{sec:input_sel_pes_eval}), an extensive comparison with highly optimised designs on embedded GPUs (Section~\ref{sec:gpu}) and further comparisons with the most recent state-of-the-art FPGA-based accelerators (Section~\ref{sec:fpga_comparison}).

