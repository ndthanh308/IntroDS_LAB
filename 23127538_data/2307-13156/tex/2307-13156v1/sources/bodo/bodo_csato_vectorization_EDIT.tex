\begin{abstract}
%
% The abstract should briefly summarize the contents of the paper in
% 15--250 words.
%
Interpreted languages are preferred over compiled languages when considering scientific computing -- in this article, we focus on the MATLAB, Python, and Julia languages.
%
Whilst running interpreted code usually means longer runtime, the source code for interpreted languages is usually more compact and easier to understand.
%
A second significant advantage of interpreted languages is that they are easier to learn.
%
These languages, moreover, often provide built-in constructs or third-party libraries for efficient numerical computations.
%
In this course, we give a short introduction to basic machine learning algorithms and show how to implement these algorithms with vectorization in the different languages and modules -- with the scope of creating a more efficient and more understandable code.
%
Besides vectorization, the need for optimized data structures in scientific computing will also be discussed.

\keywords{Code optimization  \and Vectorization \and Data mining.}
\end{abstract}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contents of the lecture}

The motivation of the lecture is the rise in popularity -- and indeed in the added value towards the teaching process -- of the interpreted languages, specifically when the focus of the application is an area related to data analysis, machine learning or -- more broadly -- artificial intelligence.
%
These languages are easier to code, easier to debug, but they are inefficient, for example, when executing loops, typically used when the same operations have to be applied to several elements -- mostly due to the higher level of abstraction due to the presence of the virtual machine that interprets the code \cite{birkbeck2007dimension}.\footnote{Loops are highlighted here because in vectorization we actually optimize cycles, but, of course, this inefficiency applies to almost all language constructs.}
%
In this respect, the compiled versions of the same code -- evidently written in a compiled language -- is at least one order of magnitude faster \cite{aho2007compilers}.

When numerical computations are required, however, some of these languages offer built-in features or libraries to support these.
%
For example, in MATLAB or more recently in the Julia language~\cite{bezanson2017julia,Sherrington2015}, the initial type for all variables is the multidimensional array\footnote{\url{https://www.mathworks.com/help/matlab/learn_matlab/matrices-and-arrays.html}}, and these languages provide specialized operations to make computations on these data structures more efficient than those realized by programmers and using loops.
%
Similarly, Python's NumPy\footnote{\url{https://numpy.org/}} package offers functions to perform optimized matrix computations \cite{van2011numpy}.

In this course, we will discuss the different approaches/methods to rewrite numerical computations using matrices to optimize the program code, if such a rewriting is possible/efficient.
%
This procedure is called \emph{vectorization}.
%
Besides vectorization, the need for using optimized data structures -- e.g. sparse vectors/matrices -- will also be discussed.
% During the lecture we will illustrate the ``classical'' computation of resulting vectors in python, then will switch to the transcription to the

For the sake of example, we consider the problem of computing the Euclidean distance matrix of a set of items in a dataset -- usually needed to calculate nearest neighbors, the RBF kernel matrix when applying kernel methods \cite{scholkopf2018learning}, or in simple clustering algorithms -- i.e. we need to compute a matrix $\mathbf{D}$ of size $N\times N$, containing the pairwise Euclidean distances over data items:
$$
    D_{i,j} = \|\mathbf{x}_{i} - \mathbf{x}_{j}\|_{2}^{2}, \quad i,j\in \{1,2,\ldots,N\}
$$
Suppose the data $\mathbf{x}_{i}\in \mathbb{R}^d$, $i=1,2,\ldots,N$ -- represented as \emph{column} vectors -- is arranged into the $N\times d$ matrix $\mathbf{X}$, as below (left), then with only matrix operations we can compute $\mathbf{D}$ as below (right):
\begin{equation*}
  \mathbf{X} = \begin{bmatrix}
        \mathbf{x}_{1}^{T}\\
        \mathbf{x}_{2}^{T}\\
        \vdots\\
        \mathbf{x}_{N}^{T}
        \end{bmatrix}
  \qquad\qquad\qquad\qquad
  \begin{array}{ll}
    \mathbf{A} = \mathbf{X}\mathbf{X}^{T}\\
    \mathbf{B} = \diag(\diag(\mathbf{A}))\\
    \mathbf{D} = -2\mathbf{X}\mathbf{X}^{T} + \mathbf{B}\mathbf{1}_{NN} + \mathbf{1}_{NN}\mathbf{B},\\
  \end{array}
\end{equation*}
where $\diag(\cdot)$ is the diagonal function, implemented under the same name -- \verb!diag! -- in MATLAB/Octave\footnote{\url{https://www.mathworks.com/help/matlab/ref/diag.html}} which either keeps only the diagonal elements of a matrix -- the inner \verb!diag! function --, or makes a diagonal matrix out of a vector of elements -- the outer function\footnote{In the language Julia there is a distinction: \texttt{diag} takes a matrix and returns the vector of diagonals, for the other direction one uses \texttt{diagm}.}, and $\mathbf{1}_{NN}$ is the matrix containing all ones.

The above simplification of computing the pairwise distances highlights (1) the simplicity of the code -- which is extremely close to the mathematical notations in MATLAB and Julia -- and (2) the efficiency of the implementation, which originates in the optimized versions of the matrix multiplication and the matrix notation itself.
%
Using vectors and matrices in these methods allows the straightforward application of some formulae for more efficient computations when needed (e.g. Searle's set of identities) \cite{matrix_cookbook,golub2013matrix}.

We will explore the approaches to vectorization within the data processing and data analysis methods to
compute dot products, matrix products, weighted averages, etc., and use these in different algorithms.
The examples will be written for real-world data in order to keep the audience motivated.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Intended audience}

The target audience of this course includes everybody who is interested in data mining, applied mathematics, numerical computing, and mathematical modeling within data analysis.
%
In the present, it is inevitable for someone to encounter applications of artificial intelligence, more specifically machine learning methods, since these can be found almost everywhere, from spam filtering, through intelligent image enhancement, to machine translation -- just to name a few.

Consequently, we recommend the course for everyone who is interested in artificial intelligence; specifically in understanding machine learning models and the crux of deep learning methodology.
%
They will acquire knowledge and skills that will help a quicker and ``deeper'' understanding of machine learning methods and their implementation details.
%
The use of vectorization and related concepts will help present and future PhD students improve the quality of their code and produce a more \emph{sustainable} code basis -- in every aspect of the word.
%
Sustainable coding/programming -- from the perspective of scientific computing -- reduces to the following three things: (i) using interpreted languages facilitating fast development and easy debugging; (ii) using optimized data representations, data structures; (iii) exploiting the benefits of vectorization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prerequired knowledge}

The required knowledge to be able to follow the present course efficiently can be summarized as follows:
\begin{itemize}
    \item basic linear algebra knowledge (linear equations, vector spaces, matrix operations);
    \item basic probability theory (probability distributions, conditional probability, Bayes' theorem);
    \item programming skills, not necessarily in the languages mentioned above.
\end{itemize}
However, if some of the above-mentioned knowledge/skill is missing or is only partially available, the participants will be able to fill in the gaps and catch up during the theoretical and practical parts as explanations will be added upon questions from the audience.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Required materials}

As a practical part, we will analyze basic machine learning/data mining algorithms (for example, centroid, $k$-means, kernel $k$-means, $k$-nearest neighbors, naive Bayes \cite{bishop2006pattern}, the PageRank  algorithm \cite{LangvilleMeyer2006}) and implement these using matrix operations, working in Python/Julia/Octave.
%
We will provide the participants with the materials needed for the course, including the presentation slides, bibliography, links to additional materials, datasets, as well as sample code snippets to facilitate the practice.
%
For effective cooperation, however, we would like to ask the participants to have Python\footnote{\url{https://www.python.org/}} and Julia\footnote{\url{https://julialang.org/}} installed on their computers, as well as an integrated development environment or code editor in which they can work comfortably -- we suggest the \emph{Visual Studio Code}\footnote{\url{https://code.visualstudio.com/}} system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Desired outcome}

After finishing the course the participants will have a basic knowledge of machine learning/data mining methods, will understand how to apply these and in which situations and they will gain practical knowledge of how to efficiently implement some of these algorithms using vectorization as well.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Relation to other contributions and lectures}

This lecture is a continuation of the \emph{Sustainable functional programming and applications} lecture given by \emph{Lehel Csat\'o} at the Rijeka Summer school in 2022 -- where the Julia language was used to solve machine learning problems.
%
At the same summer school, the lecture \emph{Applying Soft Computing for Sustainability} by \emph{Goran Mauša} is also related to vectorization; since one computes the same functions for the whole dataset.
%
The 2023 Sust(r)ainable Summer School will host the \emph{Efficient evolutionary computing} lecture by \emph{Goran Mauša}, where the data mining problems require vectorization to function efficiently.

\section{Acknowledgments}

This work acknowledges the support of the ERASMUS+ project ``SusTrainable -- Promoting Sustainability as a Fundamental Driver in Software Development Training and Education'', no. 2020–1–PT01–KA203–078646.

%\bibliographystyle{splncs04}
%\bibliography{bib}

%\end{document}
