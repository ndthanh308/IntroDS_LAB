\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alacaoglu and Malitsky(2021)]{alacaoglu2021stochastic}
Ahmet Alacaoglu and Yura Malitsky.
\newblock Stochastic variance reduction for variational inequality methods.
\newblock \emph{arXiv preprint arXiv:2102.08352}, 2021.

\bibitem[Allen-Zhu(2018)]{allen2018katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha {X}: Simple momentum method for stochastic sum-of-nonconvex
  optimization.
\newblock In \emph{ICML}, 2018.

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{ICML}, 2016.

\bibitem[Allen-Zhu and Yuan(2016)]{allen2016improved}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved svrg for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock In \emph{ICML}, 2016.

\bibitem[Cai et~al.(2019)Cai, Hong, Chen, and Wang]{cai2019global}
Qi~Cai, Mingyi Hong, Yongxin Chen, and Zhaoran Wang.
\newblock On the global convergence of imitation learning: A case for linear
  quadratic regulator.
\newblock \emph{arXiv preprint arXiv:1901.03674}, 2019.

\bibitem[Carmon et~al.(2019)Carmon, Jin, Sidford, and Tian]{carmon2019variance}
Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Variance reduction for matrix games.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Chavdarova et~al.(2019)Chavdarova, Gidel, Fleuret, and
  Lacoste-Julien]{chavdarova2019reducing}
Tatjana Chavdarova, Gauthier Gidel, Fran{\c{c}}ois Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in gan training with variance reduced extragradient.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{NIPS}, 2014.

\bibitem[Doan(2022)]{doan2022convergence}
Thinh Doan.
\newblock Convergence rates of two-time-scale gradient descent-ascent dynamics
  for solving nonconvex min-max problems.
\newblock In \emph{Learning for Dynamics and Control Conference}, 2022.

\bibitem[Du et~al.(2017)Du, Chen, Li, Xiao, and Zhou]{du2017stochastic}
Simon~S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou.
\newblock Stochastic variance reduction methods for policy evaluation.
\newblock In \emph{ICML}, 2017.

\bibitem[Duchi and Namkoong(2019)]{duchi2019variance}
John Duchi and Hongseok Namkoong.
\newblock Variance-based regularization with convex objectives.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2450--2504, 2019.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Guo et~al.(2020)Guo, Liu, Yuan, Shen, Liu, and
  Yang]{guo2020communication}
Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li~Shen, Wei Liu, and Tianbao Yang.
\newblock Communication-efficient distributed stochastic auc maximization with
  deep neural networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Han et~al.(2021)Han, Xie, and Zhang]{han2021lower}
Yuze Han, Guangzeng Xie, and Zhihua Zhang.
\newblock Lower complexity bounds of finite-sum optimization problems: The
  results and construction.
\newblock \emph{arXiv preprint arXiv:2103.08280}, 2021.

\bibitem[Huang et~al.(2022)Huang, Gao, Pei, and Huang]{huang2022accelerated}
Feihu Huang, Shangqian Gao, Jian Pei, and Heng Huang.
\newblock Accelerated zeroth-order and first-order momentum methods from mini
  to minimax optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (36):\penalty0 1--70, 2022.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}. Springer, 2016.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2021page}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt{\'a}rik.
\newblock Page: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{ICML}, 2021.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{NIPS}, 2015.

\bibitem[Lin et~al.(2020{\natexlab{a}})Lin, Jin, and Jordan]{lin2020gradient}
Tianyi Lin, Chi Jin, and Michael~I. Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Lin et~al.(2020{\natexlab{b}})Lin, Jin, and Jordan]{lin2020near}
Tianyi Lin, Chi Jin, and Michael~I. Jordan.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{COLT}, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2022.

\bibitem[Liu et~al.(2019)Liu, Yuan, Ying, and Yang]{liu2019stochastic}
Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang.
\newblock Stochastic auc maximization with deep neural networks.
\newblock \emph{arXiv preprint arXiv:1908.10831}, 2019.

\bibitem[Luo et~al.(2020)Luo, Ye, Huang, and Zhang]{luo2020stochastic}
Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang.
\newblock Stochastic recursive gradient descent ascent for stochastic
  nonconvex-strongly-concave minimax problems.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Luo et~al.(2021)Luo, Xie, Zhang, and Zhang]{luo2021near}
Luo Luo, Guangzeng Xie, Tong Zhang, and Zhihua Zhang.
\newblock Near optimal stochastic algorithms for finite-sum unbalanced
  convex-concave minimax optimization.
\newblock \emph{arXiv preprint arXiv:2106.01761}, 2021.

\bibitem[Mairal(2015)]{mairal2015incremental}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  829--855, 2015.

\bibitem[Meinshausen(2018)]{meinshausen2018causality}
Nicolai Meinshausen.
\newblock Causality from a distributional robustness point of view.
\newblock In \emph{DSW}. IEEE, 2018.

\bibitem[Nash(1953)]{nash1953two}
John Nash.
\newblock Two-person cooperative games.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pages
  128--140, 1953.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Yurii Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{ICML}, 2017.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason~D. Lee, and Meisam
  Razaviyayn.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Palaniappan and Bach(2016)]{palaniappan2016stochastic}
Balamurugan Palaniappan and Francis Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In \emph{NIPS}, 2016.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, and Tran-Dinh]{JMLR:v21:19-248}
Nhan~H. Pham, Lam~M. Nguyen, Dzung~T. Phan, and Quoc Tran-Dinh.
\newblock Proxsarah: An efficient algorithmic framework for stochastic
  composite nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (110):\penalty0 1--48, 2020.

\bibitem[Polyak(1963)]{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Rafique et~al.(2018)Rafique, Liu, Lin, and Yang]{rafique2018non}
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang.
\newblock Non-convex min-max optimization: Provable algorithms and applications
  in machine learning.
\newblock \emph{arXiv preprint:1810.02060}, 2018.

\bibitem[Reddi et~al.(2016)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{ICML}, 2016.

\bibitem[Schmidt et~al.(2017)Schmidt, Le~Roux, and Bach]{schmidt2017minimizing}
Mark Schmidt, Nicolas Le~Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1):\penalty0 83--112,
  2017.

\bibitem[Shalev-Shwartz and Zhang(2013)]{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0 (2), 2013.

\bibitem[Sun et~al.(2018)Sun, Qu, and Wright]{sun2018geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock \emph{Foundations of Computational Mathematics}, 18\penalty0
  (5):\penalty0 1131--1198, 2018.

\bibitem[Tominin et~al.(2021)Tominin, Tominin, Borodich, Kovalev, Gasnikov, and
  Dvurechensky]{vladislav2021accelerated}
Vladislav Tominin, Yaroslav Tominin, Ekaterina Borodich, Dmitry Kovalev,
  Alexander Gasnikov, and Pavel Dvurechensky.
\newblock On accelerated methods for saddle-point problems with composite
  structure.
\newblock \emph{arXiv preprint arXiv:2103.09344}, 2021.

\bibitem[Wai et~al.(2018)Wai, Yang, Wang, and Hong]{wai2018multi}
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong.
\newblock Multi-agent reinforcement learning via double averaging primal-dual
  optimization.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2019spiderboost}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock Spiderboost and momentum: Faster variance reduction algorithms.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Xian et~al.(2021)Xian, Huang, Zhang, and Huang]{xian2021faster}
Wenhan Xian, Feihu Huang, Yanfu Zhang, and Heng Huang.
\newblock A faster decentralized algorithm for nonconvex minimax problems.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Kiyavash, and He]{yang2020global}
Junchi Yang, Negar Kiyavash, and Niao He.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Zhang, Kiyavash, and
  He]{yang2020catalyst}
Junchi Yang, Siqi Zhang, Negar Kiyavash, and Niao He.
\newblock A catalyst framework for minimax optimization.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2022)Yang, Orvieto, Lucchi, and He]{yang2022faster}
Junchi Yang, Antonio Orvieto, Aurelien Lucchi, and Niao He.
\newblock Faster single-loop algorithms for minimax optimization without strong
  concavity.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Ying et~al.(2016)Ying, Wen, and Lyu]{ying2016stochastic}
Yiming Ying, Longyin Wen, and Siwei Lyu.
\newblock Stochastic online {AUC} maximization.
\newblock In \emph{NIPS}, 2016.

\bibitem[Yue et~al.(2023)Yue, Fang, and Lin]{yue2023lower}
Pengyun Yue, Cong Fang, and Zhouchen Lin.
\newblock On the lower bound of minimizing {P}olyak--{\l}ojasiewicz functions.
\newblock In \emph{COLT}, 2023.

\bibitem[Zhang et~al.(2013)Zhang, Mahdavi, and Jin]{zhang2013linear}
Lijun Zhang, Mehrdad Mahdavi, and Rong Jin.
\newblock Linear convergence with condition number independent access of full
  gradients.
\newblock In \emph{NIPS}, 2013.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Yuan, and Feng]{zhou2019faster}
Pan Zhou, Xiao-Tong Yuan, and Jiashi Feng.
\newblock Faster first-order methods for stochastic non-convex optimization on
  riemannian manifolds.
\newblock In \emph{AISTATS}, 2019.

\end{thebibliography}
