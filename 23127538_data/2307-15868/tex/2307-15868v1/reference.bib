@article{han2021lower,
  title={Lower Complexity Bounds of Finite-Sum Optimization Problems: The Results and Construction},
  author={Han, Yuze and Xie, Guangzeng and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2103.08280},
  year={2021}
}


@inproceedings{yang2020global,
  title={Global convergence and variance reduction for a class of nonconvex-nonconcave minimax problems},
  author={Yang, Junchi and Kiyavash, Negar and He, Niao},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{lin2015universal,
  title={A universal catalyst for first-order optimization},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  booktitle={NIPS},
  year={2015}
}

@article{rockafellar1970monotone,
  title={Monotone operators associated with saddle-functions and minimax problems},
  author={Rockafellar, R Tyrrell},
  journal={Nonlinear functional analysis},
  volume={18},
  number={part 1},
  pages={397--407},
  year={1970},
  publisher={Proceedings of Symposia in Pure Mathematics, American Mathematical Society}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii },
  volume={137},
  year={2018},
  publisher={Springer}
}

@inproceedings{nouiehed2019solving,
  title={Solving a class of non-convex min-max games using iterative first order methods},
  author={Nouiehed, Maher and Sanjabi, Maziar and Huang, Tianjian and Lee, Jason D. and Razaviyayn, Meisam},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{xian2021faster,
  title={A faster decentralized algorithm for nonconvex minimax problems},
  author={Xian, Wenhan and Huang, Feihu and Zhang, Yanfu and Huang, Heng},
  booktitle={NeurIPS},
  year={2021}
}

@article{doan2021convergence,
  title={Convergence Rates of Two-Time-Scale Gradient Descent-Ascent Dynamics for Solving Nonconvex Min-Max Problems},
  author={Doan, Thinh T.},
  journal={arXiv preprint arXiv:2112.09579},
  year={2021}
}



@article{el1997robust,
  title={Robust solutions to least-squares problems with uncertain data},
  author={El Ghaoui, Laurent and Lebret, Herv{\'e}},
  journal={SIAM Journal on matrix analysis and applications},
  volume={18},
  number={4},
  pages={1035--1064},
  year={1997},
  publisher={SIAM}
}

@article{CC01a,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}

@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle={ICML},
  year={2018},
}

@article{sun2018geometric,
  title={A geometric analysis of phase retrieval},
  author={Sun, Ju and Qu, Qing and Wright, John},
  journal={Foundations of Computational Mathematics},
  volume={18},
  number={5},
  pages={1131--1198},
  year={2018},
  publisher={Springer}
}

@article{cai2019global,
  title={On the global convergence of imitation learning: A case for linear quadratic regulator},
  author={Cai, Qi and Hong, Mingyi and Chen, Yongxin and Wang, Zhaoran},
  journal={arXiv preprint arXiv:1901.03674},
  year={2019}
}

@article{guo2020fast,
  title={Fast Objective \& Duality Gap Convergence for Nonconvex-Strongly-Concave Min-Max Problems},
  author={Guo, Zhishuai and Yuan, Zhuoning and Yan, Yan and Yang, Tianbao},
  journal={arXiv preprint arXiv:2006.06889},
  year={2020}
}

@inproceedings{guo2020communication,
  title={Communication-efficient distributed stochastic auc maximization with deep neural networks},
  author={Guo, Zhishuai and Liu, Mingrui and Yuan, Zhuoning and Shen, Li and Liu, Wei and Yang, Tianbao},
  booktitle={ICML},
  year={2020},
}


@article{duchi2019variance,
  title={Variance-based regularization with convex objectives},
  author={Duchi, John and Namkoong, Hongseok},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={2450--2504},
  year={2019},
}

%EG 
@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@book{facchinei2003finite,
  title={Finite-dimensional variational inequalities and complementarity problems},
  author={Facchinei, Francisco and Pang, Jong-Shi},
  year={2003},
  publisher={Springer}
}

@inproceedings{gidel2019negative,
  title={Negative momentum for improved game dynamics},
  author={Gidel, Gauthier and Hemmat, Reyhane Askari and Pezeshki, Mohammad and Le Priol, R{\'e}mi and Huang, Gabriel and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={AISTATS},
  year={2019}
}



@article{wang2020improved,
  title={Improved algorithms for convex-concave minimax optimization},
  author={Wang, Yuanhao and Li, Jian},
  journal={NeurIPS},
  year={2020}
}


@inproceedings{kovalev2020don,
  title={Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop},
  author={Kovalev, Dmitry and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter},
  booktitle={ALT},
  year={2020},
}


@inproceedings{carmon2019variance,
  title={Variance reduction for matrix games},
  author={Carmon, Yair and Jin, Yujia and Sidford, Aaron and Tian, Kevin},
  booktitle={NeurIPS},
  year={2019}
}


@inproceedings{daskalakis2019training,
  title={Training {GAN}s with optimism},
  author={Daskalakis, Constantinos and Ilyas, Andrew and Syrgkanis, Vasilis and Zeng, Haoyang},
  booktitle={ICLR},
  year={2019}
}


@article{xiao2014proximal,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}

@inproceedings{du2017stochastic,
  title={Stochastic variance reduction methods for policy evaluation},
  author={Du, Simon S. and Chen, Jianshu and Li, Lihong and Xiao, Lin and Zhou, Dengyong},
  booktitle={ICML},
  year={2017}
}

@inproceedings{wai2018multi,
  title={Multi-agent reinforcement learning via double averaging primal-dual optimization},
  author={Wai, Hoi-To and Yang, Zhuoran and Wang, Zhaoran and Hong, Mingyi},
  booktitle={NeurIPS},
  year={2018}
}

@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@inproceedings{ying2016stochastic,
  title={Stochastic online {AUC} maximization},
  author={Ying, Yiming and Wen, Longyin and Lyu, Siwei},
  booktitle={NIPS},
  year={2016}
}

@article{liu2019stochastic,
  title={Stochastic auc maximization with deep neural networks},
  author={Liu, Mingrui and Yuan, Zhuoning and Ying, Yiming and Yang, Tianbao},
  journal={arXiv preprint arXiv:1908.10831},
  year={2019}
}

@inproceedings{fazel2018global,
  title={Global convergence of policy gradient methods for the linear quadratic regulator},
  author={Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle={ICML},
  year={2018},
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  year={2016},
  organization={Springer}
}


@inproceedings{mertikopoulos2018cycles,
  title={Cycles in adversarial regularized learning},
  author={Mertikopoulos, Panayotis and Papadimitriou, Christos and Piliouras, Georgios},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={2703--2717},
  year={2018},
  organization={SIAM}
}

@inproceedings{bailey2020finite,
  title={Finite regret and cycles with fixed step-size via alternating gradient descent-ascent},
  author={Bailey, James P. and Gidel, Gauthier and Piliouras, Georgios},
  booktitle={COLT},
  year={2020},
}

@inproceedings{meinshausen2018causality,
  title={Causality from a distributional robustness point of view},
  author={Meinshausen, Nicolai},
  booktitle={DSW},
  year={2018},
  organization={IEEE}
}



@article{nguyen2017stochastic,
  title={Stochastic recursive gradient algorithm for nonconvex optimization},
  author={Nguyen, Lam M. and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1705.07261},
  year={2017}
}

@article{bot2022fast,
  title={Fast OGDA in continuous and discrete time},
  author={Bot, Radu Ioan and Csetnek, Ern{\"o} Robert and Nguyen, Dang-Khoa},
  journal={arXiv preprint arXiv:2203.10947},
  year={2022}
}

%% Variance reduction

%SVRG

@inproceedings{zhang2013linear,
  title={Linear convergence with condition number independent access of full gradients},
  author={Zhang, Lijun and Mahdavi, Mehrdad and Jin, Rong},
  booktitle={NIPS},
  year={2013}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={NIPS},
  year={2013}
}

%nonconvex SVRG

@inproceedings{allen2016variance,
  title={Variance reduction for faster non-convex optimization},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  booktitle={ICML},
  year={2016}
}

@inproceedings{reddi2016stochastic,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J. and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={ICML},
  year={2016}
}

%SVRG++ 

@inproceedings{allen2016improved,
  title={Improved SVRG for non-strongly-convex or sum-of-non-convex objectives},
  author={Allen-Zhu, Zeyuan and Yuan, Yang},
  booktitle={ICML},
  year={2016}
}

% SAGA

@inproceedings{defazio2014saga,
  title={{SAGA}: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={NIPS},
  year={2014}
}

% SAG

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

% SDCA
@article{shalev2013stochastic,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={2},
  year={2013}
}

%Finito
@inproceedings{defazio2014finito,
  title={Finito: A faster, permutable incremental gradient method for big data problems},
  author={Defazio, Aaron and Domke, Justin and others},
  booktitle={ICML},
  year={2014}
}

% MISO
@article{mairal2015incremental,
  title={Incremental majorization-minimization optimization with application to large-scale machine learning},
  author={Mairal, Julien},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={2},
  pages={829--855},
  year={2015},
  publisher={SIAM}
}

%Katyusha

@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
}

%KatyushaX

%Spider
@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={NeurIPS},
  year={2018}
}

%SpiderBoost
@inproceedings{wang2019spiderboost,
  title={Spiderboost and momentum: Faster variance reduction algorithms},
  author={Wang, Zhe and Ji, Kaiyi and Zhou, Yi and Liang, Yingbin and Tarokh, Vahid},
  booktitle={NeurIPS},
  year={2019}
}


@inproceedings{zhou2018stochastic,
  title={Stochastic nested variance reduction for nonconvex optimization},
  author={Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  booktitle={NeurIPS},
  year={2018}
}


%SARAG

@article{JMLR:v21:19-248,
  author  = {Nhan H. Pham and Lam M. Nguyen and Dzung T. Phan and Quoc Tran-Dinh},
  title   = {ProxSARAH: An Efficient Algorithmic Framework for Stochastic Composite Nonconvex Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {110},
  pages   = {1-48}
}


@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={ICML},
  year={2017},
}

%SCSG
@inproceddings{lei2017non,
  title={Non-convex finite-sum optimization via scsg methods},
  author={Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I.},
  booktitle={NIPS},
  year={2017}
}

%R-Spider
@inproceedings{zhou2019faster,
  title={Faster first-order methods for stochastic non-convex optimization on Riemannian manifolds},
  author={Zhou, Pan and Yuan, Xiao-Tong and Feng, Jiashi},
  booktitle={AISTATS},
  year={2019},
}


%% Minimax VR technique

%Point-SAGA
@article{luo2019stochastic,
  title={A stochastic proximal point algorithm for saddle-point problems},
  author={Luo, Luo and Chen, Cheng and Li, Yujun and Xie, Guangzeng and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1909.06946},
  year={2019}
}

% SREDA

@inproceedings{luo2020stochastic,
  title={Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems},
  author={Luo, Luo and Ye, Haishan and Huang, Zhichao and Zhang, Tong},
  booktitle={NeurIPS},
  year={2020}
}

@article{huang2022accelerated,
  title={Accelerated zeroth-order and first-order momentum methods from mini to minimax optimization},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={36},
  pages={1--70},
  year={2022}
}



% SVRG/SAGA minimax
@inproceedings{palaniappan2016stochastic,
  title={Stochastic variance reduction methods for saddle-point problems},
  author={Palaniappan, Balamurugan and Bach, Francis},
  booktitle={NIPS},
  year={2016}
}

%SVRE

@inproceedings{chavdarova2019reducing,
  title={Reducing noise in GAN training with variance reduced extragradient},
  author={Chavdarova, Tatjana and Gidel, Gauthier and Fleuret, Fran{\c{c}}ois and Lacoste-Julien, Simon},
  booktitle={NeurIPS},
  year={2019}
}

%Loopness-SVRE
@article{alacaoglu2021stochastic,
  title={Stochastic variance reduction for variational inequality methods},
  author={Alacaoglu, Ahmet and Malitsky, Yura},
  journal={arXiv preprint arXiv:2102.08352},
  year={2021}
}


%% Catalyst yang2020catalyst,luo2021near,lin2020near
@article{vladislav2021accelerated,
  title={On Accelerated Methods for Saddle-Point Problems with Composite Structure},
  author={Tominin, Vladislav and Tominin, Yaroslav  and Borodich, Ekaterina  and Kovalev, Dmitry  and Gasnikov, Alexander and Dvurechensky, Pavel},
  journal={arXiv preprint arXiv:2103.09344},
  year={2021}
}

@article{luo2021near,
  title={Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization},
  author={Luo, Luo and Xie, Guangzeng and Zhang, Tong and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2106.01761},
  year={2021}
}

@inproceedings{yang2020catalyst,
  title={A catalyst framework for minimax optimization},
  author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{lin2020near,
  title={Near-optimal algorithms for minimax optimization},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I.},
  booktitle={COLT},
  year={2020},
}

% OGDA and EG
@inproceedings{mokhtari2020unified,
  title={A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach},
  author={Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020},
}


%%% Applications in introduction

% GAN
@article{creswell2018generative,
  title={Generative adversarial networks: An overview},
  author={Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  journal={IEEE Signal Processing Magazine},
  volume={35},
  number={1},
  pages={53--65},
  year={2018},
  publisher={IEEE}
}

@article{makhzani2015adversarial,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}



%% GDA

% EAG 
@inproceedings{yoon2021accelerated,
  title={Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with $ \mathcal{O} (1/k^2 )$ Rate on Squared Gradient Norm},
  author={Yoon, TaeHo and Ryu, Ernest K.},
  booktitle={ICML},
  year={2021},
}

@article{tran2022connection,
  title={The Connection Between Nesterov's Accelerated Methods and Halpern Fixed-Point Iterations},
  author={Tran-Dinh, Quoc},
  journal={arXiv preprint arXiv:2203.04869},
  year={2022}
}

@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael I.},
  booktitle={ICML},
  year={2020}
}

@article{rafique2018non,
  title={Non-convex min-max optimization: Provable algorithms and applications in machine learning},
  author={Rafique, Hassan and Liu, Mingrui and Lin, Qihang and Yang, Tianbao},
  journal={arXiv preprint:1810.02060},
  year={2018}
}

%OGDA+
@article{bohm2022solving,
  title={Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions},
  author={B{\"o}hm, Axel},
  journal={arXiv preprint arXiv:2201.12247},
  year={2022}
}

%EG+
@inproceedings{diakonikolas2021efficient,
  title={Efficient methods for structured nonconvex-nonconcave min-max optimization},
  author={Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael I.},
  booktitle={AISTATS},
  year={2021},
}

%tw-time-scale GDA
@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={NIPS},
  year={2017}
}

@article{kasai2017sgdlibrary,
  title={SGDLibrary: A MATLAB library for stochastic optimization algorithms},
  author={Kasai, Hiroyuki},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={7942--7946},
  year={2017}
}

@article{nash1953two,
  title={Two-person cooperative games},
  author={Nash, John},
  journal={Econometrica: Journal of the Econometric Society},
  pages={128--140},
  year={1953}
}

@inproceedings{yang2022faster,
  title={Faster Single-loop Algorithms for Minimax Optimization without Strong Concavity},
  author={Yang, Junchi and Orvieto, Antonio and Lucchi, Aurelien and He, Niao},
  booktitle={AISTATS},
  year={2022}
}

@inproceedings{li2021page,
  title={PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization},
  author={Li, Zhize and Bao, Hongyan and Zhang, Xiangliang and Richt{\'a}rik, Peter},
  booktitle={ICML},
  year={2021}
}

@inproceedings{yue2023lower,
  title={On the lower bound of minimizing {P}olyak--{\L}ojasiewicz functions},
  author={Yue, Pengyun and Fang, Cong and Lin, Zhouchen},
  booktitle={COLT},
  year={2023}
}

@inproceedings{doan2022convergence,
  title={Convergence rates of two-time-scale gradient descent-ascent dynamics for solving nonconvex min-max problems},
  author={Doan, Thinh},
  booktitle={Learning for Dynamics and Control Conference},
  year={2022}
}

@inproceedings{allen2018katyusha,
  title={Katyusha {X}: Simple momentum method for stochastic sum-of-nonconvex optimization},
  author={Allen-Zhu, Zeyuan},
  booktitle={ICML},
  year={2018}
}