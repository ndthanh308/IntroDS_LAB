

\section{Some Useful Facts}

In this section, we provide some facts which are useful in the following proofs. 

First of all, we define three notations of optimality. 
\begin{dfn}  \label{dfn: three-points} 
 We say $(x^{\ast},y^{\ast})$ is a saddle point of function $f$, if for all $(x,y)$, it holds that
\begin{align*}
    f(x^{\ast}, y) \le f(x^{\ast},y^{\ast}) \le f(x,y^{\ast}).
\end{align*}
We say $(x^{\ast},y^{\ast})$ is a global minimax point, if for all $x \in \BR^{d_x}, y \in \BR^{d_y} $, it holds that 
\begin{align*}
    f(x^{\ast},y) \le f(x^{\ast}, y^{\ast}) \le \max_{y' \in \BR^{d_y}} f(x,y').
\end{align*}
And we say $(x^{\ast},y^{\ast})$ is a stationary point, if it holds that
\begin{align*}
    \nabla_x f(x^{\ast},y^{\ast}) = \nabla_y f(x^{\ast}, y^{\ast}) = 0.
\end{align*}
\end{dfn}

 For general nonconvex-nonconcave minimax problem, a stationary point or a global minimax point is weaker than a saddle point, i.e. a stationary point or a global minimax point may not  be a saddle point. However, under two-sided PL condition, the above three notations are equivalent.

\begin{lem}[{\citet[Lemma 2.1]{yang2020global}}] \label{lem: three-points-equal}

Under Assumption \ref{asm: two-PL}, it holds that 
\begin{align*}
    (\text{saddle point} ) \Leftrightarrow (\text{global minimax point}) \Leftrightarrow (\text{stationary point} ).
\end{align*}

Further,  if $(x^{\ast}, y^{\ast}) $ is a saddle point of $f$, then 
\begin{align*}
    \max_{y \in \BR^{d_y}} f(x^{\ast}, y) = f(x^{\ast}, y^{\ast}) = \min_{x \in \BR^{d_x}} f(x,y^{\ast}).
\end{align*}
and vice versa.
\end{lem}

 It is well known that weak duality always holds.
 
\begin{lem}[{\citet[Theorem 1.3.1]{nesterov2018lectures}}] \label{lem: minimax}
Given a function $f$, we have 
\begin{align*}
    \max_{y \in \BR^{d_x}}  \min_{x \in \BR^{d_x }} f(x,y) \le  \min_{ x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} f(x,y).
\end{align*}
\end{lem}

It is a standard conclusion that the existence of saddle points implies strong duality. Since strong duality is important for the convergence of Catalyst scheme under PL condition, we present this lemma as follows.

\begin{lem} \label{lem: saddle-point-eq}

If $(x^{\ast}, y^{\ast}) $ is a saddle point of function $f$, then $(x^{\ast}, y^{\ast}) $ is also a global minimax point and stationary point of $f$,  and it holds that
\begin{align*}
     \max_{y \in \BR^{d_y}} \min_{x \in \BR^{d_x}} f(x,y) = f(x^{\ast} ,y^{\ast}) = \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}}  f(x,y).
\end{align*}
\end{lem}

\begin{lem}[{\citet[Lemma A.1]{yang2020global}}] \label{lem: quadratic-growth}

Under Assumption \ref{asm: two-PL}, then $f(x,y)$ also satisfies the following quadratic growth condition, i.e. for all $x \in \BR^{d_x},y \in \BR^{d_y}$, it holds that
\begin{align*}
    f(x,y) - \min_{x \in \BR^{d_x}} f(x,y) &\ge \frac{\mu_x}{2} \Vert x^{\ast}(y) - x \Vert^2, \\
    \max_{y \in \BR^{d_y}} f(x,y) - f(x,y) &\ge \frac{\mu_y}{2} \Vert y^{\ast}(x) - y \Vert^2,
\end{align*}
where $x^{\ast}(y)$ is the projection of $y$ on the set $\argmin_{x \in \BR^{d_x}} f(x,y)$ and $y^{\ast}(x)$ is the projection of $x$ on the set of $\argmax_{y \in \BR^{d_y}} f(x,y)$. 
\end{lem}

Also, we analyze the properties of $g(x)$.
\begin{lem}[{\citet[Lemma 2.1]{yang2020global}}] \label{lem: g(x)-PL}
Under Assumption \ref{asm: two-PL}, then $g(x)$ satisfies $\mu_x$-PL, i.e. for all $x$ we have
\begin{align*}
    \Vert \nabla g(x) \Vert^2 \ge 2 \mu_x (g(x) - g(x^{\ast})).
\end{align*}

\end{lem}

\begin{lem}[{\citet[in the proof of Theorem 3.1]{yang2020global}}] \label{lem: bound-Bk}
Under Assumption \ref{asm: two-PL} and \ref{asm: L-smooth}, then for all $x,y$ it holds true that 
\begin{align*}
    \Vert \nabla_x f(x,y) - \nabla g(x) \Vert^2 &\le \frac{2L^2}{\mu_y} (g(x) - f(x,y)).
\end{align*}
\end{lem}

The above lemma is a direct result of the quadratic growth property implied by PL condition and $L$-smooth property of function $f(x,y)$. Using the definition of $\mu_y$-PL in $y$, we can also show the relationship between $ \Vert \nabla_x f(x,y) - \nabla g(x) \Vert^2 $ and $ \Vert \nabla_y f(x,y) \Vert^2$ as follows.

\begin{lem} \label{lem: new-bound-Bk}

Under Assumption \ref{asm: two-PL} and \ref{asm: L-smooth}, then for all $x,y$ it holds true that 
\begin{align*}
    \Vert \nabla_x f(x,y) - \nabla g(x) \Vert^2 &\le \frac{L^2}{\mu_y^2} \Vert \nabla_y f(x,y) \Vert^2
\end{align*}
\end{lem}

\begin{lem}[{\citet[Lemma A.5]{nouiehed2019solving}}] \label{lem: g(x)-L-smooth}

Under Assumption \ref{asm: one-PL} and \ref{asm: L-smooth}, then $g(x) $ satisfies $(L+{L^2} /{\mu_y})$-smooth,that is, it holds for all $x,x'$ that
\begin{align*}
    \Vert \nabla g(x) - \nabla g(x') \Vert^2 \le \left (L+ \frac{L^2}{\mu_y}\right) \Vert x - x' \Vert^2.
\end{align*}
Further, noting that ${L}/ {\mu_y} \ge 1$, it implies that $g(x)$ is $({2L^2}/ {\mu_y})$-smooth.

\end{lem}

\begin{lem}[{\citet[Modified from Lemma A.3]{nouiehed2019solving}}] \label{lem: prox-argmax}
Under Assumption \ref{asm: two-PL}  and \ref{asm: L-smooth}, suppose $(x^{\ast},y^{\ast})$ is a saddle point of $f$. Denote the operator $y^{\ast}(\cdot)$ as the projection onto the optimal set of $\argmax_{ y \in \BR^{d_y}} f(x,y)$, then it holds true that
\begin{align*}
    \Vert y^{\ast}(x) - y^{\ast} \Vert^2 \le \frac{L^2}{\mu_y^2} \Vert x - x^{\ast} \Vert^2.
\end{align*}
% \begin{proof}
% The proof is similar to the proof under strongly-convex-strongly-concave setting.
% \begin{align*}
%     L^2 \Vert x - x^{\ast} \Vert^2 &\ge \Vert \nabla_y f(x,y^{\ast}) - \nabla_y f(x^{\ast},y^{\ast}) \Vert^2 \\
%     &= \Vert \nabla_y f(x,y^{\ast}) \Vert^2 \\
%     & \ge 2 \mu_y \left (\max_y f(x,y) - f(x,y^{\ast})\right ) \\
%     & \ge \mu_y^2 \Vert y^{\ast}(x) - y^{\ast} \Vert^2,
% \end{align*} 
% where the first inequality is due to $L$-smooth of $f$, and the second line relies on $ \nabla_y f(x^{\ast},y^{\ast}) = 0$. The second inequality relies on the PL condition in $y$; in the last inequality, we use the quadratic growth property by Lemma \ref{lem: quadratic-growth}.
% \end{proof}
\end{lem}


\section{Two-Timescale GDA Matches AGDA}

As a warm-up, we study GDA as well as AGDA with full gradient calculation in this section. After that, it would be easy to extend the analysis to the stochastic setting.

\begin{algorithm*}[htbp] 
\caption{AGDA $(f, (x_0,y_0), K, \tau_x, \tau_y)$} 
\begin{algorithmic}[] \label{alg: AGDA}
    \STATE \textbf{for} $k = 0, 1, \dots, K-1$ \textbf{do}\\[0.15cm]
    \STATE \quad $x_{k+1} = x_k - \tau_x \nabla_x f(x_k,y_k) $ \\[0.15cm]
    \STATE \quad $y_{k+1} = y_k + \tau_y \nabla_y f(x_{k+1},y_k)$ \\[0.15cm]
    \STATE\textbf{end for} \\[0.15cm]
    \STATE \textbf{option I}  (two-sided PL): \textbf{return} $(x_K,y_K)$  \\
    \STATE \textbf{option II} (one-sided PL): \textbf{return} $(\hat x,\hat y)$ chosen uniformly at random from $\{ (x_k,y_k)\}_{k=0}^{K-1}$.
\end{algorithmic}
\end{algorithm*}

\begin{algorithm*}[htbp] 
\caption{GDA $(f, (x_0,y_0), K, \tau_x, \tau_y)$} 
\begin{algorithmic}[] \label{alg: GDA}
    \STATE \textbf{for} $k = 0, 1, \dots, K-1$ \textbf{do}\\[0.15cm]
    \STATE \quad $x_{k+1} = x_k - \tau_x \nabla_x f(x_k,y_k) $ \\[0.15cm]
    \STATE \quad $y_{k+1} = y_k + \tau_y \nabla_y f(x_{k},y_k)$ \\[0.15cm]
    \STATE\textbf{end for} \\[0.15cm]
    \STATE \textbf{option I}  (two-sided PL): \textbf{return} $(x_K,y_K)$  \\
    \STATE \textbf{option II} (one-sided PL): \textbf{return} $(\hat x,\hat y)$ chosen uniformly at random from $\{ (x_k,y_k) \}_{k=0}^{K-1}$
\end{algorithmic}
\end{algorithm*}

\subsection{Convergence under Two-Sided PL condition} \label{sec: GDA}

Under the two-sided PL condition, it is known that AGDA \cite{yang2020global} can find an $\epsilon$-optimal solution with a complexity of $\tilde \fO( n \kappa_x\kappa_y^2 \log ({1}/{\epsilon}))$ when $\kappa_x \gtrsim \kappa_y$. However, the authors left us the question that whether GDA can converge under the same setting. We answer this question affirmatively in this section. We show that the same convergence rate can be achieved by GDA  with simultaneous updates.

We define the following Lyapunov function  suggested by \citet{doan2022convergence}:
\begin{align*}
\mathcal{V}_k &= \mathcal{A}_k + \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_k,
\end{align*}
where $\mathcal{A}_k =  g(x_k ) - g({x}^{\ast})$, $ \mathcal{B}_k = g(x_k) - f(x_k,y_k)$. Then we can obtain the following statement.

\begin{thm} \label{thm: GDA}
Suppose function $f(x,y)$ satisfies $L$-smooth, $\mu_x$-PL in $x$, $\mu_y$-PL in $y$. Let $\tau_y = {1}/{L}$, $\lambda  = {6L^2}/{\mu_y^2}$ and $\tau_x = {\tau_y}/{(22\lambda)}$, then the sequence $\{ (x_k,y_k) \}_{k=1}^{K}$ generated by Algorithm \ref{alg: GDA} satisfies:
\begin{align*}
    \fV_{k+1} \le \left (1 - \frac{\mu_x \tau_x}{2} \right)^k \fV_k.
\end{align*}
\begin{proof}
Since we know that $g$ is $({2L^2}/{\mu_y})$- smooth by Lemma \ref{lem: g(x)-L-smooth}, let $\tau_x \le {\mu_y}/{(2 L^2)}$, we have 
\begin{align} \label{eq: 2-1}
\begin{split}
    g(x_{k+1})  & \le g(x_k ) - g(x^{\ast}) + \nabla g(x_k)^\top(x_{k+1} - x_k) + \frac{L^2}{\mu_y} \Vert x_{k+1} - x_k \Vert^2 \\
    &\le g(x_k)  - \tau_x\nabla g(x_k)^\top \nabla_x f(x_k,y_k) + \frac{\tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2 \\
    &= g(x_k)  - \frac{\tau_x}{2} \Vert \nabla g(x_k)\Vert^2 +\frac{\tau_x}{2} \Vert \nabla  g(x_k ) - \nabla_x f(x_k,y_k) \Vert^2,
\end{split}
\end{align}
which implies 
\begin{align} \label{eq: 2-2}
    \fA_{k+1} \le \fA_k - \frac{\tau_x}{2} \Vert \nabla g(x_k)\Vert^2 +\frac{\tau_x}{2} \Vert \nabla  g(x_k ) - \nabla_x f(x_k,y_k) \Vert^2.
\end{align}
Using the property of $L$-smooth, we know that the difference between $f(x_k,y_k)$ and $f(x_{k+1},y_{k+1})$ can be bounded. Noting that $\tau_x \le {1}/{L}$, we can obtain
\begin{align} \label{eq: 2-3}
\begin{split}
    f(x_k,y_k) - f(x_{k+1}, y_k) &\le  -\nabla_x f(x_k,y_k)^\top(x_{k+1} -x_k) + \frac{L}{2} \Vert x_{k+1} - x_k \Vert^2 \\
& =\tau_x \Vert \nabla_x f(x_k,y_k) \Vert^2 + \frac{ \tau_x^2 L}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2  \\
&\le \frac{3 \tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2.
\end{split}
\end{align}

Let $\tau_y < 1/L$, then we have 
\begin{align} \label{eq: 2-4}
\begin{split}
& f(x_{k+1},y_k) - f(x_{k+1},y_{k+1}) \\
\le& - \nabla_y f(x_{k+1},y_k)^\top(y_{k+1} -y_k) + \frac{L}{2} \Vert y_{k+1} - y_k \Vert^2 \\
\le& - \tau_y \nabla_y f(x_{k+1},y_k)^\top \nabla_y f(x_k,y_k)  + \frac{ \tau_y}{2} \Vert \nabla_y f(x_{k},y_k) \Vert^2 \\
=& - \frac{\tau_y}{2} \Vert \nabla_y f(x_{k+1},y_k) \Vert^2 + \frac{\tau_y}{2} \Vert \nabla_y f(x_k,y_k) - \nabla_y f(x_{k+1},y_k) \Vert^2 \\
\le& -\frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \tau_y \Vert \nabla_y f(x_k,y_k) - \nabla_y f(x_{k+1},y_k) \Vert^2 \\
\le& -\frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \tau_y \tau_x^2 L^2 \Vert \nabla_x f(x_k,y_k) \Vert^2 \\
\le& -\frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \tau_x \Vert \nabla_x f(x_k,y_k) \Vert^2,
\end{split}
\end{align}
where in the first inequality we use $f$ is $L$-smooth, and we use $\tau_y \le {1}/{L}$ in the second one and  Young's inequality of $ -\Vert a- b \Vert^2 \le \frac{1}{2} \Vert a \Vert^2 +\Vert b\Vert^2$ in the third one.

Combing (\ref{eq: 2-3}) and (\ref{eq: 2-5}), we can see that
\begin{align} \label{eq: 2-5}
    f(x_k,y_k) - f(x_{k+1},y_{k+1}) &\le - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{5 \tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2.
\end{align}
Now we can describe how $\fB_{k+1}$ declines compared with $\fB_k$, using (\ref{eq: 2-1}) and (\ref{eq: 2-5}), we have
\begin{align} \label{eq: 2-6}
    \begin{split}
    \fB_{k+1} 
    &= g(x_{k+1}) -g(x_k ) + g(x_k) - f(x_k,y_k) + f(x_k,y_k) - f(x_{k+1},y_{k+1}) \\
&\le \fB_k - \frac{\tau_x}{2} \Vert \nabla g(x_k)\Vert^2 +\frac{\tau_x}{2} \Vert \nabla  g(x_k ) - \nabla_x f(x_k,y_k) \Vert^2   \\
&\quad - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{5 \tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2. 
\end{split}
\end{align}
Using the inequality $\Vert \nabla_x f(x_k,y_k) \Vert^2 \le 2 \Vert \nabla g(x_k) \Vert^2 + 2 \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2$, we have
\begin{align*}
    \fB_{k+1} &\le \fB_k + \frac{9\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{11 \tau_x}{2} \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2.
\end{align*}
By Lemma \ref{lem: g(x)-PL}, Lemma \ref{lem: bound-Bk} and Assumption \ref{asm: two-PL}, we have 
\begin{align} \label{eq: 2-7}
    \begin{split}
        \Vert \nabla g(x_k) \Vert^2 &\ge 2 \mu_x ( g(x_k) - g(x^{\ast})), \\
\Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2 &\le \frac{2L^2}{\mu_y} (g(x_k) - f(x_k,y_k)), \\
\Vert \nabla_y f (x_{k} ,y_k) \Vert^2 &\ge 2 \mu_y (g(x_{k}) - f(x_{k},y_k)).
    \end{split}
\end{align}
Since we let $\tau_y = {1}/ {L}$, $\lambda  = {6L^2} /{\mu_y^2}$ and $\tau_x = {\tau_y} / {(22\lambda)}$, we can obtain
\begin{align} \label{eq: 2-8}
    \begin{split}
        \mathcal{V}_{k+1} &= \mathcal{A}_{k+1} + \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_{k+1} \\
&\le \mathcal{A}_{k} + \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_{k} - \left (1 - \frac{9 \lambda \tau_x}{\tau_y} \right) \frac{\tau_x}{2}\Vert  \nabla g(x_k) \Vert^2 \\
&\quad + \left (1 +\frac{11\lambda \tau_x}{\tau_y} \right )\frac{\tau_x}{2} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k, y_k) \Vert^2 \\
&\le \mathcal{A}_{k}  - \left (1- \frac{9 \lambda \tau_x}{\tau_y} \right ) \tau_x \mu_x \mathcal{A}_k +  \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_{k}+ \left (1 +\frac{11\lambda \tau_x}{\tau_y} \right) \frac{\tau_xL^2 }{\mu_y} \mathcal{B}_k - \frac{\lambda \tau_x \mu_y}{2} \mathcal{B}_k \\
&\le \left (1- \frac{\mu_x \tau_x}{2} \right ) \mathcal{A}_k +  \left (1- \frac{\mu_y \tau_y}{4} \right ) \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_k \\
&\le \left (1- \frac{\mu_x \tau_x}{2} \right)  \mathcal{V}_k,
    \end{split}
\end{align}
where in the second inequality we use $ {11\lambda \tau_x} / {\tau_y}\le {1}/{2}$ by the choices of $\tau_x,\tau_y$ and $\lambda$, while we use the fact that ${ 3\tau_x L^2} / {\mu_y} \le {\lambda \tau_x \mu_y}/{2}$ in the third one and ${\mu_x \tau_x} \le {\mu_y \tau_y} / {2}$ in the last one.

\end{proof}
\end{thm} 

Now we show that the convergence of $\fV_k$ is sufficient to guarantee the convergence to an $\epsilon$-saddle point, defined as follows.

\begin{dfn}
Under Assumption \ref{asm: exist}, we call $(x,y)$ an $\epsilon$-saddle point of problem (\ref{prob:main}) if if holds that $\Vert x - x^* \Vert^2 + \Vert y - y^* \Vert^2 \le \epsilon $ for some saddle point $(x^*,y^*)$.
\end{dfn}

\begin{cor} \label{cor: GDA-saddle}

Suppose  function $f(x,y)$ satisfies $L$-smooth, $\mu_x$-PL in $x$, $\mu_y$-PL in $y$ and  $\kappa_x \gtrsim \kappa_y$. Define $\tau_x,\tau_y$ as in Lemma \ref{thm: GDA} ,then  then the sequence $\{ (x_k,y_k) \}_{k=1}^{K}$ generated by Algorithm \ref{alg: GDA} satisfies:
\begin{align} \label{eq: 11.1}
    \Vert x_k - x^{\ast} \Vert^2 + \Vert y_k - y^{\ast } \Vert^2 \le \frac{2c^k}{(1-\sqrt{c})^2}  \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y} \right\} \fV_0. 
\end{align}
where $c = 1 - {\mu_x \tau_x}/{2}$. Further, Algorithm \ref{alg: GDA} can find an $\epsilon$-saddle point with no more than $\mathcal{O}(n \kappa_x \kappa_y^2 \log ({ \kappa_x \kappa_y}/{\epsilon}))$ stochastic first-order oracle calls.

\begin{proof}

The proof is similar to the proof of Theorem 3.2 in \cite{yang2020global}.

By Lemma \ref{lem: quadratic-growth} and the fact that $2 \tau_x^2 L^2 \le 1$, $\tau_x \le {\mu_y}/{(2L^2)}$ and $\tau_y \le {1}/{L}$ by the choices of $\tau_x,\tau_y$, we can see that
\begin{align} \label{eq: 2-9}
    \begin{split}
        &\quad \Vert x_{k+1} - x_k \Vert^2 + \Vert y_{k+1} - y_k \Vert^2 \\
        &=
        \tau_x^2 \Vert \nabla_x f(x_k,y_k) \Vert^2 + \tau_y^2 \Vert \nabla_y f(x_k,y_k) \Vert^2 \\
        &= \tau_x^2 \Vert \nabla_x f(x_k,y_k) \Vert^2 + \tau_y^2 \Vert \nabla_y f(x_k,y_k) - \nabla_y f(x_k, y^{\ast}(x_k) \Vert^2 \\
        &\le \tau_x^2 \Vert \nabla_x f(x_k,y_k) \Vert^2  + \Vert y_k - y^{\ast}(x_k) \Vert^2 \\
        &\le 2\tau_x^2 \Vert \nabla g(x_k) \Vert^2 + 2 \tau_x^2 \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 +\Vert y_k - y^{\ast}(x_k) \Vert^2 \\
        &\le 2 \Vert x_k -x^{\ast} \Vert^2 + 2 \Vert y_k - y^{\ast}(x_k) - y_k \Vert^2 \\
        &\le \frac{4}{\mu_x} \fA_k + \frac{4}{\mu_y} \fB_k \\
        &\le \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y} \right \} \fV_k \\
        &\le \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y} \right \} \left (1 - \frac{\mu_x \tau_x}{2} \right )^k \fV_0,
    \end{split}
\end{align}
where in the last inequality we use ${\lambda \tau_x}/{\tau_y} = {1}/{22}$. 
Then we have
\begin{align*}
    \Vert x_{k+1} - x_k \Vert + \Vert y_{k+1} - y_k \Vert \le \left (1 - \frac{\mu_x \tau_x}{2} \right)^{k/2}  \sqrt{2 \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y} \right \} \fV_0}.
\end{align*}
For $n \ge k$, we obtain
\begin{align*}
\begin{split}
    \Vert x_n - x_k \Vert + \Vert y_n  - y_k \Vert &\le \sum_{i=k}^{n-1} \Vert x_{i+1} - x_i \Vert^2 + \Vert y_{i+1} - y_i \Vert^2\\
    &\le \sqrt{2 \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y} \right \} \fV_0} \sum_{i=k}^{\infty} \left(1- \frac{\mu_x \tau_x}{2} \right )^{i/2} \\
    &\le \frac{c^{k/2}}{1 - \sqrt{c}} \sqrt{2 \max \left \{ \frac{4}{\mu_x}, \frac{88}{\mu_y}\right \} \fV_0},
\end{split}
\end{align*}
where $c = 1 - {\mu_x \tau_x}/{2}$. We know that when $n \rightarrow \infty$, we have $(x_n,y_n) \rightarrow (x^{\ast}, y^{\ast})$ where $(x^{\ast},y^{\ast})$ is a saddle point, Taking square on both sides completes our proof.

\end{proof}

\end{cor} 

\subsection{Convergence under One-Sided PL condition}

When $f$ is nonconvex in $x$, we have the following theorem for GDA.

\begin{thm} \label{thm: GDA-one-side}

Suppose function $f(x,y)$ satisfies $L$-smooth, $\mu_y$-PL in $y$. Let $\tau_y = {1}/{L}$, $\lambda = {4 L^2}/{\mu_y^2}$ and $\tau_x = {\tau_y}/{(18 \lambda)}$, then the sequence $\{ (x_k,y_k) \}_{k=0}^{K-1}$ generated by Algorithm \ref{alg: GDA} satisfies,~
\begin{align*}
    \frac{1}{K} \sum_{k=0}^{K-1} \Vert \nabla g(x_k) \Vert^2 \le \frac{288 L^3}{K \mu_y^2} \mathcal{V}_0.
\end{align*}
Furthermore, if we choose the output $(\hat x,\hat y)$ uniformly from $\{ (x_k,y_k) \}_{k=0}^{K-1}$, then we can get $\Vert \nabla g(\hat x) \Vert \le \epsilon$ with no more than $\mathcal{O}({n \kappa_y^2 L}/{\epsilon^2} )$ first-order oracle calls. 
\begin{proof}
Using equation (\ref{eq: 2-2}) and Lemma \ref{lem: bound-Bk} that $ \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 \le {2L^2 \fB_k} /{\mu_y} $,we have
\begin{align} \label{eq: 10}
    \fA_{k+1} 
&\le \fA_k - \frac{\tau_x}{2} \Vert \nabla g(x_k)\Vert^2 +\frac{\tau_x L^2}{\mu_y} \mathcal{B}_k.
\end{align}
Further, using equation (\ref{eq: 2-6}), we have 
\begin{align} \label{eq: 11}
\begin{split}
    \fB_{k+1 } &\le \mathcal{B}_k + \frac{9\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{11 \tau_x}{2} \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \\
    &\le \fB_k + \frac{9\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{11 \tau_x L^2}{\mu_y} \fB_k - \frac{\mu_y \tau_y}{2} \fB_k \\
    &\le (1- \frac{\mu_y \tau_y}{4}) \fB_k + \frac{9\tau_x}{2} \Vert \nabla g(x_k) \Vert^2, 
\end{split}
\end{align}
where we use Lemma \ref{lem: bound-Bk} and PL condition in $y$ in the first inequality and ${11 \tau_x L^2}/{\mu_y} \le {\mu_y \tau_y}/{4}$ by the choices of $\tau_x,\tau_y$. Thus,
\begin{align*}
\fB_k \le \left (1 - \frac{\mu_y \tau_y}{4} \right)^k \fB_0 + \frac{9 \tau_x}{2} \sum_{i=0}^{k-1} \left (1 - \frac{\mu_y \tau_y}{4} \right)^{k-i-1} \Vert \nabla g(x_i) \Vert^2.   
\end{align*}
Plugging  into (\ref{eq: 10}), 
{\small
\begin{align*}
    \mathcal{A}_{k+1} &\le \mathcal{A}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x L^2}{\mu_y}\left (1 - \frac{ \mu_y \tau_y}{4} \right)^{k} \mathcal{B}_0  +\frac{9\tau_x^2 L^2}{2\mu_y} \sum_{i=0}^{k-1} \left (1 - \frac{ \mu_y \tau_y}{4} \right)^{k-i-1} \Vert \nabla g(x_i) \Vert^2.
\end{align*}}
Telescoping and noticing that ${18\tau_x^2 L^2}/{\tau_y \mu_y^2} \le {\tau_x}/{4}$ and $\lambda =  {4L^2}/{\mu_y^2}$, we have 
\begin{align*}
    \fA_{K+1} &\le \fA_0 - \frac{\tau_x}{2} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x L^2}{\mu_y} \sum_{k=0}^{K-1} \left (1 - \frac{\mu_y \tau_y}{4}\right )^k \fB_0 \\
    &\quad \quad \quad + \frac{9\tau_x^2 L^2}{2\mu_y} \sum_{k=1}^K \sum_{i=0}^{k-1} \left (1 - \frac{\mu_y \tau_y}{4}\right )^{k-i-1} \Vert \nabla g(x_i) \Vert^2 \\
    &= \fA_0 - \frac{\tau_x}{2} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x L^2}{\mu_y} \sum_{k=0}^{K-1} \left (1 - \frac{\mu_y \tau_y}{4} \right)^k \fB_0 \\
    &\quad \quad \quad + \frac{9\tau_x^2 L^2}{2\mu_y} \sum_{i=0}^{K-1} \sum_{k=i+1}^{K} \left (1 - \frac{\mu_y \tau_y}{4} \right)^{k-i-1} \Vert \nabla g(x_i) \Vert^2 \\
    &\le \fA_0 - \frac{\tau_x}{2} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x L^2}{\mu_y} \sum_{k=0}^{K-1} \left (1 - \frac{\mu_y \tau_y}{4} \right )^k \fB_0 + \frac{18 \tau_x^2 L^2}{\tau_y \mu_y^2} \sum_{i=0}^{K-1} \Vert \nabla g(x_i) \Vert^2 \\
    &\le \fA_0 - \frac{\tau_x}{2} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x L^2}{\mu_y} \sum_{k=0}^{K-1} \left (1 - \frac{\mu_y \tau_y}{4} \right)^k \fB_0 + \frac{18 \tau_x^2 L^2}{\tau_y \mu_y^2} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 \\
    &\le \fA_0 - \frac{\tau_x}{4} \sum_{k=0}^K \Vert \nabla g(x_k) \Vert^2 +  \frac{4 \tau_x L^2}{\tau_y \mu_y^2} \fB_0 \\
    &= \fV_0 - \frac{\tau_x}{4} \sum_{k=0}^K \Vert \nabla g(x_k) \Vert^2.
\end{align*}
Rearranging and noticing that $\fA_{K+1} \ge  0 $, we can see that 
\begin{align*}
    \frac{1}{K+1} \sum_{k=0}^{K} \Vert \nabla g(x_k) \Vert^2 \le \frac{4 \fV_0}{(K+1) \tau_x}, 
\end{align*}
which is equivalent to the desired inequality.
\end{proof}

\end{thm}

\section{Convergence of GDA with SVRG Gradient Estimators} \label{apx: SVRG-GDA}

In this section, we show the convergence rate of GDA with SVRG gradient estimators (Algorithm \ref{alg: SVRG-GDA}) can be $\fO( (n+ n^{2/3} \kappa_x \kappa_y^2 ) \log (1/\epsilon))$, improving the result of $\fO( (n+ n^{2/3} \max\{\kappa_x^3, \kappa_y^3 \} ) \log (1/\epsilon))$  by \citet{yang2020global} with SVRG-AGDA. 

%Plus, we can prove that the same convergence rate can be achieved by AGDA with similar techniques since in our algorithm we set $\tau_x \ll \tau_y$, therefore $x_k$ changes much slower than $y_k$. The reason for unbalanced step sizes lies in that $g(x)$ is $(L+L^2/ \mu_y)$-smooth by Lemma \ref{lem: g(x)-L-smooth}. Thus, we can regard that the condition number of solving problem $\max_{y \in \BR^{d_y}} f(x,y)$ is $\fO(\kappa_y)$ while that of solving $\min_{x \in \BR^{d_x}} g(x) $ is $\fO(\kappa_x \kappa_y)$. Thus, it is reasonable that the total complexity has a factor of $\fO(\kappa_x \kappa_y^2)$.

\begin{algorithm*}[t] 
\caption{SVRG-GDA $(f, (x_0,y_0), T,S,M,B, \tau_x, \tau_y)$} 
\begin{algorithmic}[1] \label{alg: SVRG-GDA}
\STATE $\bar x_0 = x_0, \bar y_0 = y_0$ \\[0.15cm]
\STATE \textbf{for} $t = 0,1,\dots, T-1$ \textbf{do} \\[0.15cm]
\STATE \quad \textbf{for} $s = 0, 1, \dots, S-1$ \textbf{do}\\[0.15cm]
\STATE \quad \quad $x_{s,0} = \bar x_s, y_{s,0} = \bar y_s$ \\[0.15cm]
\STATE \quad \quad compute $\nabla_x f(\bar x_s,\bar y_s) = \dfrac{1}{n} \sum_{i=1}^n \nabla_x f_i(\bar x_s,\bar y_s) $ \\[0.15cm]
\STATE \quad \quad compute $\nabla_y f(\bar x_s,\bar y_s) = \dfrac{1}{n} \sum_{i=1}^n \nabla_y f_i(\bar x_s,\bar y_s) $ \\[0.15cm]
\STATE \quad \quad \textbf{for } $k= 0,1, \dots, M-1$ \\[0.15cm]
\STATE \quad \quad \quad Draw samples $S_x,S_y$ independently with both size $B$. \\[0.15cm]
\STATE \quad \quad \quad  $G_x(x_{s,k},y_{s,k}) =\dfrac{1}{B} \sum_{i \in S_x}[\nabla_x f_{i} (x_{s,k},y_{s,k}) - \nabla_x f_{i} (\bar x_s, \bar y_s) + \nabla_x f(\bar x_s,\bar y_s)]$ \\[0.15cm]
\STATE \quad \quad \quad $G_y(x_{s,k},y_{s,k}) = \dfrac{1}{B} \sum_{i\in S_y}[\nabla_y f_{i} (x_{s,k},y_{s,k}) - \nabla_x f_{i} (\bar x_s, \bar y_s) + \nabla_x f(\bar x_s,\bar y_s)]$ \\[0.15cm]
\STATE \quad \quad \quad $x_{s,k+1} = x_{s,k} - \tau_x G_x(x_{s,k},y_{s,k})$ \\[0.15cm]
\STATE \quad \quad \quad $y_{s,k+1} = y_{s,k} + \tau_y G_y(x_{s,k},y_{s,k})$ \\[0.15cm]
\STATE \quad \quad \textbf{end for} \\[0.15cm]
\STATE \quad \quad $\bar x_{s+1} = x_{s,M}, \bar y_{s+1} = y_{s,M}$ \\[0.15cm]
\STATE \quad \textbf{end for} \\[0.15cm]
\STATE \quad Choose $(x_t,y_t)$ from $\{ \{ (x_{s,k}, y_{s,k} )\}_{k=0}^{M-1} \}_{s=0}^{S-1}$ uniformly at random. \\ [0.15cm]
\STATE \quad $ \bar x_0 = x_t, \bar y_0 = y_t$\\ [0.15cm]
\STATE \textbf{end for} \\[0.15cm]
\STATE \textbf{return} $(x_T,y_T)$
\end{algorithmic}
\end{algorithm*}

For the innermost loop about subscript $k$ when $t$ and $s$ are both fixed,  we define the Lyapunov function:
\begin{align*}
    \mathcal{V}_{s,k} &= \mathcal{A}_{s,k} + \frac{\lambda \tau_x}{ \tau_y} \mathcal{B}_{s,k} + c_{s,k} \Vert x_{s,k} - \bar x_s \Vert^2 + d_{s,k}  \Vert y_k - \bar y_s \Vert^2,
\end{align*}
where $\fA_{s,k} = g(x_{s,k}) - g(x^{\ast}) $ and $\fB_{s,k} = g(x_{s,k}) - f(x_{s,k},y_{s,k})$ and $c_{s,k},d_{s,k}$ will be defined recursively with $c_{s,M} = d_{s,M} = 0$ in our proof. Then we can have the following lemma.

\begin{lem} \label{lem: SVRG-GDA-V_k}

Under Assumption \ref{asm: one-PL} and \ref{asm: L-smooth}, if we let $\tau_y = {\nu}/{(L n^{\alpha})}$, $\lambda  = {14L^2}/{\mu_y^2}$ and $\tau_x = {\tau_y}/{(22 \lambda)}$, where  $\nu = 1/(176(\rm{e}-1)), 0< \alpha \le 1$; let $B = 1, M = \lfloor {n^{3 \alpha/2}}/{(2\nu)} \rfloor$. Then for Algorithm \ref{alg: SVRG-GDA}, the following statement holds true:
\begin{align*}
     \BE[\fV_{s,k+1}] &\le \fV_{s,k} - \frac{\tau_x}{8} \Vert \nabla g(x_{s,k}) \Vert^2 - \frac{\lambda \tau_x}{16} \Vert \nabla_y f(x_{s,k},y_{s,k}) \Vert^2,
\end{align*}
Above, the definitions of $c_{s,k},d_{s,k}$ is given recursively with $c_{s,M} = d_{s,M} = 0$ as:
\begin{align*}
c_{s,k} &= c_{s,k+1}(1+ \tau_x \gamma_1) + \left(c_{s,k+1}\tau_x^2+ \frac{3\tau_x^2 L^2}{ \mu_y}\right ) L^2 + \left (d_{s,k+1}\tau_y^2 +\frac{\lambda \tau_x\tau_y L}{2}\right) L^2, \\
d_{s,k} &= d_{s,k+1}(1+\tau_y \gamma_2) + \left (c_{s,k+1}\tau_x^2+ \frac{3\tau_x^2 L^2}{\mu_y} \right) L^2 + \left (d_{s,k+1}\tau_y^2 +\frac{\lambda \tau_x\tau_y L}{2} \right ) L^2.
\end{align*}

\begin{proof}

Since $s$ is fixed in the lemma, we omit  subscripts of $s$ in the following proofs, then the Lyapunov function can be written as:
\begin{align*}
    \mathcal{V}_{k} &= \mathcal{A}_k + \frac{\lambda \tau_x}{ \tau_y} \mathcal{B}_k + c_{k} \Vert x_k - \bar x \Vert^2 + d_k  \Vert y_k - \bar y \Vert^2.
\end{align*}
Before the formal proof, we present some standard properties of variance reduction. We denote the stochastic gradients as:
\begin{align*}
    G_x(x_k,y_k) &= \frac{1}{B} \sum_{i \in S_x} \big(\nabla_x f_{i} (x_{k},y_{k}) - \nabla_x f_{i} (\bar x, \bar y) + \nabla_x f(\bar x,\bar y)\big)
\end{align*}
and
\begin{align*}    
    G_y(x_k,y_k) &= \frac{1}{B} \sum_{i \in S_y} \big(\nabla_y f_{i} (x_{k},y_{k}) - \nabla_y f_{i} (\bar x, \bar y) + \nabla_y f(\bar x,\bar y)\big).
\end{align*}
Then we know that the stochastic gradients satisfy unbiasedness that 
\begin{align*}
    \BE[G_x(x_k,y_k)] = \nabla_x f(x_k,y_k) \quad\text{and}\quad
    \BE[G_y(x_k,y_k)] = \nabla_y f(x_k,y_k).
\end{align*}
And we  can bound the variance of the stochastic gradients as follows:
\begin{align} \label{eq: bound-var-x}
\begin{split}
&\quad \BE\Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \\
&= \BE\Vert \nabla_x f_{i} (x_{k},y_{k}) - \nabla_x f_{i} (\bar x, \bar y) + \nabla_x f(\bar x,\bar y) - \nabla_x f(x_k,y_k) \Vert^2 \\
&\le \BE\Vert \nabla_x f_{i} (x_{k},y_{k}) - \nabla_x f_{i} (\bar x, \bar y) \Vert^2 \\
&\le  L^2 \BE\Vert x_{k} - \bar x \Vert^2] +L^2 \BE [\Vert y_{k} - \bar y \Vert^2. 
\end{split}
\end{align}
Similarly, we have
\begin{align} \label{eq: bound-var-y}
    \BE\Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 
    &\le  L^2 \BE\Vert x_{k} - \bar x \Vert^2 +L^2 \BE \Vert y_{k} - \bar y \Vert^2.
\end{align}

Equipped with the above properties of SVRG, now we can begin our proof of Lemma \ref{lem: SVRG-GDA-V_k}. 

Since we know that $g$ is $({2L^2}/{\mu_y})$-smooth by Lemma \ref{lem: g(x)-L-smooth} and  $\tau_x \le {\mu_y}/{2 L^2}$, we have
\begin{align} \label{eq: bound-SVRG-Ak}
\begin{split}
   \BE[g(x_{k+1})] 
    & \le \BE\left[g(x_k )  + \nabla g(x_k)^\top(x_{k+1} - x_k) + \frac{2L^2}{\mu_y} \Vert x_{k+1} - x_k \Vert^2\right] \\
    &\le \BE\left[g(x_k)  - \tau_x\nabla g(x_k)^\top G_x(x_k,y_k) + \frac{\tau_x^2 L^2}{\mu_y} \Vert G_x(x_k,y_k) \Vert^2\right] \\
    &\le \BE\left[g(x_k)  - \tau_x\nabla g(x_k)^\top \nabla_x f(x_k,y_k) + \frac{\tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2 \right] \\
    &\quad +\BE\left[ \frac{\tau_x^2 L^2}{\mu_y} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right] \\
    &= \BE\left[g(x_k)  - \frac{\tau_x }{2} \Vert \nabla g(x_k)\Vert^2 +\frac{\tau_x}{2} \Vert \nabla  g(x_k ) - \nabla_x f(x_k,y_k) \Vert^2 \right] \\
    &\quad + \BE \left[\frac{\tau_x^2 L^2}{\mu_y} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right] ,
\end{split}
\end{align}
where we use $ \tau_x^2 L^2 \le \mu_y$ in the third inequality. Also, we have 
\begin{align*}
\mathbb{E}[ f(x_k,y_k) ]&\le \mathbb{E}\left[ f(x_{k+1},y_{k} ) - \nabla_x f(x_k,y_k)^\top(x_{k+1}-x_k) + \frac{L}{2} \Vert x_{k+1} - x_k \Vert^2\right] \\
&= \mathbb{E} \left[ f(x_{k+1},y_k) + \tau_x\nabla_x f(x_k,y_k)^\top G_x(x_k,y_k) + \frac{\tau_x^2L}{2} \Vert G_x(x_k,y_k ) \Vert^2\right] \\
&= \mathbb{E}\left[ f(x_{k+1},y_k)  + \tau_x  \Vert \nabla_x f(x_k,y_k) \Vert^2 + \frac{\tau_x^2L}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2 \right]\\
&\quad + \BE \left[ \frac{\tau_x^2L}{2} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2\right] \\
&\le \mathbb{E} \left[ f(x_{k+1},y_k) + \frac{3\tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2 + \frac{\tau_x^2L}{2} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right],
\end{align*}
where we use the quadratic upper bound implied by $L$-smoothness in the first inequality and $\tau_y \le 1/L$ in the second one. Similarly,  
{\small \begin{align*}
\mathbb{E}[ f(x_{k+1},y_k)] &\le \mathbb{E}\left[ f(x_{k+1},y_{k+1}) - \nabla_y f(x_{k+1},y_k)^\top(y_{k+1} - y_k) + \frac{L}{2} \Vert y_{k+1} - y_k \Vert^2\right] \\
&=\mathbb{E}\left[ f(x_{k+1},y_{k+1} ) - \tau_y \nabla_y f(x_{k+1},y_k)^\top G_y(x_k,y_k) + \frac{\tau_y^2L}{2} \Vert G_y (x_k,y_k) \Vert^2\right] \\
&\le \mathbb{E}\left[ f(x_{k+1},y_{k+1} ) - \tau_y  \nabla_y f(x_{k+1},y_k)^\top  \nabla_y f(x_k,y_k) + \frac{\tau_y}{2} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right]\\
&\quad + \BE \left[ \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right] \\
&= \mathbb{E}\left[ f(x_{k+1},y_{k+1} ) -\frac{\tau_y}{2} \Vert \nabla_y f(x_{k+1},y_{k}) \Vert^2 \right]\\
&\quad + \BE \left[\frac{\tau_y}{2} \Vert \nabla_y f(x_{k+1},y_k) - \nabla_y f(x_k,y_k) \Vert^2 + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right]\\
&\le \mathbb{E}\left[ f(x_{k+1},y_{k+1} ) -\frac{\tau_y}{4} \Vert \nabla_y f(x_{k+1},y_{k}) \Vert^2 \right] \\
&\quad + \BE \left[\tau_y \Vert \nabla_y f(x_{k+1},y_k) - \nabla_y f(x_k,y_k) \Vert^2 + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right]\\
&\le \mathbb{E}\left[ f(x_{k+1},y_{k+1} ) -\frac{\tau_y}{4} \Vert \nabla_y f(x_{k+1},y_{k}) \Vert^2 \right] \\
&\quad + \BE \left[\tau_y \tau_x^2 L^2\Vert G_x(x_k,y_k) \Vert^2 + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right]\\
&\le\mathbb{E}\left[ f(x_{k+1},y_{k+1} ) -\frac{\tau_y}{4} \Vert \nabla_y f(x_{k+1},y_{k}) \Vert^2 + \tau_x \Vert \nabla_x f(x_k,y_k) \Vert^2 \right]\\
&\quad + \BE \left[\tau_x^2  L \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right].
\end{align*}}
Above, the first and fourth inequalities are both due to $L$-smoothness; the second one follows from $\tau_y \le 1/L$; the third one uses the fact that $ -\BE [\Vert a - b \Vert^2] \le -\frac{1}{2} \BE[\Vert a \Vert^2] + \BE [\Vert b \Vert^2]$; the last one relies on $ \BE[ \Vert G_x(x_k,y_k) \Vert^2] = \BE[ \Vert \nabla_x f(x_k,y_k) \Vert^2 + \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2  ] $ and the choices of $\tau_x,\tau_y$.
Summing up the above two inequalities, we obtain 
{\small\begin{align*}
\mathbb{E}[f(x_k,y_k)] &\le \mathbb{E}\left[f(x_{k+1},y_{k+1}) + \frac{5\tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2 + \frac{3\tau_x^2L}{2} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right] \\
&\quad \BE \left[- \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right].
\end{align*}}
Combing with inequality (\ref{eq: bound-SVRG-Ak}), we can see that
\begin{align} \label{eq: bound-SVRG-Bk}
\begin{split}
    \BE[\fB_{k+1}] &\le \BE\left[\fB_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x}{2} \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2\right ] \\
&\quad + \BE\left[  - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{5 \tau_x}{2} \Vert \nabla_x f(x_k,y_k) \Vert^2\right] \\
&\quad + \BE\left[ \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 + \frac{5\tau_x^2 L^2}{2\mu_y} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right] \\
&\le \BE \left[ \fB_k + \frac{9 \tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{11 \tau_x}{2} \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right ] \\
&\quad + \BE\left[ \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 + \frac{5\tau_x^2 L^2}{2\mu_y} \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right],
\end{split}
\end{align}
where we use $\BE\Vert \nabla_x f(x_k,y_k) \Vert^2 \le \BE\Vert \nabla g(x_k) \Vert^2 + \BE\Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2$.
Using Young's inequality as equation (37) and (38) in \cite{yang2020global}, we have 
\begin{align*}
\begin{split}
    \BE\Vert x_{k+1} - \bar x \Vert^2 &\le \BE \left[ \tau_x^2  \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2\right] \\
    & \quad + \BE \left[ (1+ \tau_x \gamma_1) \Vert x_k - \bar x \Vert^2 + \left(\tau_x^2 + \frac{\tau_x}{\gamma_1} \right) \Vert \nabla_x f(x_k,y_k) \Vert^2 \right] ,\\
    \BE\Vert y_{k+1} - \bar y \Vert^2 &\le \BE \left[  \tau_y^2 \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 \right] \\
    &\quad  + \BE \left[ (1+ \tau_y \gamma_2) \Vert y_k - \bar y \Vert^2 + \left(\tau_y^2 + \frac{\tau_y}{\gamma_2} \right) \Vert \nabla_y f(x_k,y_k) \Vert^2 \right],
\end{split}
\end{align*}
where $\gamma_1,\gamma_2$ are two positive constant in Young's inequality  which will be chosen later. 

Then, using equation (\ref{eq: bound-var-x}), (\ref{eq: bound-var-y}), (\ref{eq: bound-SVRG-Ak}), (\ref{eq: bound-SVRG-Bk}),   we have
\begin{align} \label{eq: 18}
\begin{split}
\BE[\mathcal{V}_{k+1}] &= \BE \left[ \fA_{k+1} + \frac{\lambda \tau_x}{\tau_y} \fB_{k+1} +c_{k+1} \Vert x_{k+1} -\bar x \Vert^2 + d_{k+1} \Vert y_{k+1} - \bar y \Vert^2 \right]\\
&\le \mathcal{A}_k + \frac{\lambda \tau_x}{\tau_y} \fB_k -  \left(1 - \frac{9 \lambda \tau_x}{\tau_y}\right ) \frac{\tau_x}{2}\Vert  \nabla g(x_k) \Vert^2 + \\
&\quad + \left (1 +\frac{11\lambda \tau_x}{\tau_y}\right )\frac{\tau_x}{2} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2  \\
&\quad - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{2 \tau_x^2 L^2}{\mu_y} \left(1 + \frac{5\lambda \tau_x}{4\tau_y} \right)\Vert G_x (x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \\
&\quad + \frac{\lambda \tau_x \tau_y L}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\\
&\quad  + \BE [c_{k+1} \Vert x_{k+1} -\bar x \Vert^2 + d_{k+1} \Vert y_{k+1} - \bar y \Vert^2] \\
&= \fV_k - \left(1 - \frac{9 \lambda \tau_x}{\tau_y}\right ) \frac{\tau_x}{2}\Vert  \nabla g(x_k) \Vert^2 + \left (1 +\frac{11\lambda \tau_x}{\tau_y}\right )\frac{\tau_x}{2} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2  \\
&\quad - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k, y_k) \Vert^2+ c_{k+1} \left (\tau_x^2 + \frac{\tau_x}{\gamma_1} \right ) \Vert \nabla_x f(x_k,y_k)  \Vert^2\\
&\quad + d_{k+1}\left ( \tau_y^2+\frac{\tau_y}{\gamma_2} \right ) \Vert \nabla_y f(x_k,y_k) \Vert^2  \\
&\le \fV_k - \left(1 - \frac{9 \lambda \tau_x}{\tau_y}\right ) \frac{\tau_x}{2}\Vert  \nabla g(x_k) \Vert^2 + \left (1 +\frac{11\lambda \tau_x}{\tau_y}\right )\frac{\tau_x}{2} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2  \\
&\quad - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k, y_k) \Vert^2+ d_{k+1}\left ( \tau_y^2+\frac{\tau_y}{\gamma_2} \right ) \Vert \nabla_y f(x_k,y_k) \Vert^2 \\
&\quad +  2c_{k+1} \left(\tau_x^2 + \frac{\tau_x}{\gamma_1}\right ) \Vert \nabla g(x_k) \Vert^2 \\
&\quad + 2c_{k+1} \left (\tau_x^2 + \frac{\tau_x}{\gamma_1} \right ) \Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2 \\ 
&\le \mathcal{V}_k -   \frac{\tau_x}{4}\Vert  \nabla g(x_k) \Vert^2 + \frac{3\tau_x}{4} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k, y_k) \Vert^2 \\
&\quad + d_{k+1}\left ( \tau_y^2+\frac{\tau_y}{\gamma_2} \right ) \Vert \nabla_y f(x_k,y_k) \Vert^2 +  2c_{k+1} \left (\tau_x^2 + \frac{\tau_x}{\gamma_1} \right ) \Vert \nabla g(x_k) \Vert^2 \\
&\quad + 2c_{k+1} \left (\tau_x^2 + \frac{\tau_x}{\gamma_1} \right ) \Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2, 
\end{split}
\end{align}
where the second last inequality relies on 
\begin{align*} \Vert \nabla_x f(x_k,y_k) \Vert^2 \le 2 \Vert \nabla g(x_k) \Vert^2 + 2 \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2;
\end{align*}
in the last inequality we use ${11\lambda \tau_x}/{\tau_y } \le {1}/{2}$ by our choices of $\lambda,\tau_x,\tau_y$; the second equality is due to the definition of $c_{k+1}, d_{k+1}$.

Now we define $e_k = \max\{c_k,d_k \}$ and we bound $e_k$ by letting $ \gamma_1 = {\lambda L}/{n^{\alpha/2}}$ and $\gamma_2 = {L}/{n^{\alpha/2}}$. Then according to  the definition of $c_k,d_k$ given by our definition, we have
\begin{align*}
    e_k &\le (1 + \tau_y \gamma_2 + \tau_y^2 L^2) e_{k+1} + \frac{3 \tau_x^2 L^4}{\mu_y} + \frac{\lambda \tau_x \tau_y L^3}{2} \\ 
    & \le (1 + \tau_y \gamma_2 + \tau_y^2 L^2) e_{k+1} + \tau_y^2L^3 \\
    &= \left (1 + \frac{\nu}{n^{3\alpha/2}} + \frac{\nu^2}{n^{2 \alpha}} \right ) + \frac{L\nu^2 }{n^{2 \alpha}} \\
    &\le \left(1 + \frac{2 \nu}{n^{3\alpha/2}} \right) e_{k+1} + \frac{L\nu^2 }{n^{2 \alpha}},
\end{align*}
where we use  $\tau_x \le \tau_y$ and $\gamma_1 \tau_x \le \gamma_2 \tau_y$ in the first; the second inequality is due to $\tau_x^2 L / \mu_y \le \tau_y^2 /6$ and $\lambda \tau_x \le \tau_y /4$ ; we plug in $\tau_y = {\nu}/{L n^{\alpha}}$ in the third line;  we use $ \nu \le 1$ in the last inequality.

Since  $M = \lfloor {n^{3 \alpha/2}}/{(2\nu)} \rfloor$ and $c_M = d_M  =0$, if we define  $\theta = {2 \nu }/{n^{3 \alpha/2}}$, then
\begin{align} \label{eq: bound-e_k}
    e_0 & \le \frac{ L \nu^2}{n^{2 \alpha}}  \frac{(1 + \theta)^M - 1}{\theta} \le \frac{L \nu ({\rm e}-1)}{ 2n^{\alpha/2}}.
\end{align}
Since $e_{k+1} \le e_k$, we know that $e_k \le e_0$, then 
\begin{align} \label{eq: bound-re-d_k}
\begin{split}
    d_{k+1}\left ( \tau_y^2+\frac{\tau_y}{\gamma_2} \right) & \le e_0 \left (\tau_y + \frac{1}{\gamma_2}\right) \tau_y \\
    &\le \frac{L \nu (\rm{e}-1)}{ 2n^{\alpha/2}}  \left (\tau_y + \frac{1}{\gamma_2}\right) \tau_y \\
    &= \frac{ \nu(\rm{e}-1)}{2} \left ( \frac{\nu}{n^{3\alpha/2}} + 1 \right) \tau_y \\
    &\le \nu(\rm{e}-1) \tau_y,
\end{split}
\end{align}
where we use $d_{k+1} \le e_0$  in the first inequality and (\ref{eq: bound-e_k}) in the second one, and in the third line we plug in $\tau_y = {\nu}/{(L n^{\alpha})}$ and $\gamma_2 = {L}/{n^{\alpha/2}}$. The last inequality follows from ${\nu}/{n^{3\alpha/2}} \le 1$.

Similarly, note that $\gamma_1 \ge \gamma_2$  and $\tau_x \le \tau_y$ by the choices of $\tau_x,\tau_y$, then we have
\begin{align} \label{eq: bound-re-c_k}
c_{k+1}\left (\tau_x^2 + \frac{\tau_x}{\gamma_1} \right) \le e_0 \left (\tau_x + \frac{1}{\gamma_1}\right) \tau_x 
\le e_0\left (\tau_y + \frac{1}{\gamma_2} \right) \tau_x \le3\nu({ \rm e}-1)\tau_x. 
\end{align}
Plugging (\ref{eq: bound-re-d_k}) and (\ref{eq: bound-re-c_k}) into (\ref{eq: 18}), 
\begin{align*} 
\begin{split}
    \BE[\mathcal{V}_{k+1}] 
&\le \mathcal{V}_k -   \frac{\tau_x}{4}\Vert  \nabla g(x_k) \Vert^2 + \frac{3\tau_x}{4} \Vert \nabla_x f(x_k, y_k) - \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k, y_k) \Vert^2 \\
&\quad + \nu({\rm e}-1) \tau_y \Vert \nabla_y f(x_k,y_k) \Vert^2 +  2 \nu(\rm{e}-1) \tau_x \Vert \nabla g(x_k) \Vert^2 \\
&\quad + 2 \nu({\rm e}-1) \tau_x \Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2.
\end{split}
\end{align*}
If we let $\nu \le {1}/{(176{(\rm e}-1))}$, then we can verify that the following statements hold true:
\begin{align*}
     \nu({\rm e}-1) \tau_y &\le \frac{\lambda \tau_x}{8}, \\
    2 \nu({\rm e}-1) \tau_x &\le \frac{\tau_x}{8}.
\end{align*}
Thus,
\begin{align*}
    \BE [\fV_{k+1}] &\le \fV_k - \frac{\tau_x}{8} \Vert \nabla g(x_k) \Vert^2 + \frac{7 \tau_x}{8} \Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{8} \Vert \nabla_y f(x_k,y_k) \Vert^2.
\end{align*}
Using Lemma \ref{lem: new-bound-Bk} and $\mu_y$-PL condition in $y$ and plugging in $\lambda = {14 L^2}/{\mu_y^2}$ yields
\begin{align*}
    \frac{7 \tau_x}{8} \Vert \nabla_x f(x_k,y_k) - \nabla g(x_k) \Vert^2  \le \frac{\lambda \tau_x}{16} \Vert \nabla_y f(x_k,y_k)\Vert^2. 
\end{align*}
Thus, 
\begin{align*}
    \BE[\fV_{k+1}] &\le \fV_k - \frac{\tau_x}{8} \Vert \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{16} \Vert \nabla_y f(x_k,y_k) \Vert^2.
\end{align*}
\end{proof}
\end{lem}

Now it is sufficient to show the convergence of SVRG-GDA.

\begin{thm}  \label{thm: SVRG-GDA}

Under Assumption \ref{asm: two-PL} and  \ref{asm: L-smooth},  if we  let $SM = \lceil {8}/{(\mu_x \tau_x)}  \rceil$, $T = \lceil \log(1/\epsilon) \rceil$ and $M,B,\tau_x,\tau_y$ defined in Lemma \ref{lem: SVRG-GDA-V_k}, then the following statement holds true for Algorithm \ref{alg: SVRG-GDA}:~
\begin{align*}
    \BE \left[\tilde \fA_{t+1}+ \frac{\lambda \tau_x}{\tau_y} \tilde \fB_{t+1} \right]\le \frac{1}{2}\left( \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y} \tilde \fB_t \right),
\end{align*}
where $\tilde \fA_{t} = g(x_{t}) - g(x^{\ast}) $ and $\tilde \fB_{t} = g(x_{t}) - f(x_{t},y_{t})$. Furthermore, let $\alpha = {2}/{3}$, then it requires $\fO((n+  n^{2/3} \kappa_x \kappa_y^2) \log ({1}/{\epsilon}))$ stochastic first-order calls to achieve $g(x_T) - g(x^{\ast}) \le \epsilon$ in expectation.

\begin{proof}
By Lemma \ref{lem: SVRG-GDA-V_k} and Lemma \ref{lem: g(x)-PL} that $g$ satisfies $\mu_x$-PL in $x$ and Assumption \ref{asm: two-PL} that function $f$ satisfies $\mu_y$-PL in $y$, we have
\begin{align*}
    \BE[\mathcal{V}_{s,k+1}] \le \mathcal{V}_{s,k} - \frac{\mu_x \tau_x}{4} \mathcal{A}_{s,k} - \frac{\mu_y \tau_y}{8} \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_{s,k} \le \mathcal{V}_{s,k} - \frac{\mu_x \tau_x}{4} \fV_{s,k},
\end{align*}
where in the last inequality we use $ {\mu_x \tau_x}/{4} \le {\mu_y \tau_y}/{8} $. Telescoping for $k=0,1,\dots,M-1$ and $s = 0,1,\dots,S-1$ and rearranging, we can see that in round $t$, it holds that
\begin{align*}
    \frac{1}{SM} \sum_{s=0}^{S-1} \sum_{k=0}^{M-1} \fV_{s,k} \le \frac{4}{\mu_x \tau_x SM} (\fV_{0,0} - \fV_{S,M}) \le \frac{1}{2} \fV_{0,0},
\end{align*}
where the last inequality is due to the choice of $S$.

The above inequality is exactly equivalent to what we want to prove:
\begin{align*}
    \BE \left[\tilde \fA_{t+1}+ \frac{\lambda \tau_x}{\tau_y} \tilde \fB_{t+1} \right]\le \frac{1}{2}\left( \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y} \tilde \fB_t \right).
\end{align*}
Note that we have $M = \fO(n^{3\alpha/2})$ and $ S = \fO({\kappa_x \kappa_y^2}/{n^{\alpha/2}})$, then the complexity is
\begin{align*}
    \fO\left ( (n+SM + Sn ) \log \left(\frac{1}{\epsilon}\right) \right) = \fO \left( (n+ (n^{\alpha} + n^{1- \alpha/2})\kappa_x \kappa_y^2  ) \log\left(\frac{1}{\epsilon}\right)\right ). 
\end{align*}
Plugging in $\alpha = {2}/{3}$ yields the desired complexity and it can also be seen that it is also the best choice of $\alpha$.

\end{proof}
\end{thm}


\section{Proof of Section \ref{sec: SPIDER-for-PL}} \label{apx: SPIDER}

In this section, we show that SPIDER-type stochastic gradient estimators outperform SVRG-type estimators with complete proofs.
The following lemma controls the variance of gradient estimators using the recursive update formula as SPIDER.
\begin{lem}[{\citet[Modified form Lemma 1]{fang2018spider}}] \label{lem: spider-var}
In Algorithm \ref{alg: SPIDER-GDA},  it holds true that
{\small\begin{align*}
\mathbb{E}[ \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2] &\le \frac{L^2 }{B}\sum_{j=(n_k - 1)M}^k \left(\tau_x^2 \mathbb{E}[\Vert G_x(x_j,y_j) \Vert^2] + \tau_y^2 \mathbb{E}[\Vert G_y(x_j,y_j) \Vert^2] \right),\\
\mathbb{E}[ \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2] &\le \frac{L^2 }{B}\sum_{j=(n_k - 1)M}^k \left(\tau_x^2\mathbb{E}[\Vert G_x(x_j,y_j) \Vert^2] + \tau_y^2 \mathbb{E}[\Vert G_y(x_j,y_j) \Vert^2] \right), 
\end{align*}}
where  $n_k = \lceil k/M \rceil$ and $(n_k - 1)M \le k \le n_k M - 1$.
\end{lem}

We define the following Lyapunov function to measure the progress in each epoch:
\begin{align*}
    \tilde{\mathcal{V}}_{t} = \tilde {\mathcal{A}}_{t} + \frac{\lambda \tau_x}{\tau_y}\tilde{\mathcal{B}}_t,
\end{align*}
where $\tilde{\mathcal{A}}_t = g(\tilde x_t) - g(x^*)  $ and $\tilde{\mathcal{B}}_t = g(\tilde x_t) - f(\tilde x_t, \tilde y_t)$.

The following lemma describes the main convergence property of SPIDER-GDA.
\begin{lem} \label{lem: key-lem-for-Spider}
Under Assumption \ref{asm: one-PL} and \ref{asm: L-smooth}, setting all the parameters as defined in Theorem \ref{thm: SPIDER-GDA}, then it holds true that
\begin{align*}
    \BE \left[ \frac{\tau_x}{2} \Vert \nabla g(\tilde x_{t+1}) \Vert^2 + \frac{\lambda \tau_x}{4} \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 \right] &\le \frac{1}{K}  \BE [ \tilde \fV_t ].
\end{align*}
\begin{proof}
First of all, we fix $t$ and analyze the inner loop. We define 
\begin{align*}
    \fV_{t,k} = \fA_{t,k} + \frac{\lambda \tau_x}{\tau_y} \fB_{t,k},
\end{align*}
where $\fA_{t,k} = g(x_{t,k}) - g(x^{\ast})$ and $\fB_{t,k} = g(x_{t,k}) - f(x_{t,k},y_{t,k}) $.

For simplification, we omit the subscripts $t$ when there is no ambiguity.
Note that $g(x)$ is $(2L^2/\mu_y)$-smooth, we have
\begin{align} \label{eq: bound-Ak-spider}
\begin{split}
    \mathbb{E} [g(x_{k+1}) ] &\le \mathbb{E}\left[ g(x_k) + \nabla g(x_k)^\top(x_{k+1} - x_k) + \frac{L^2}{\mu_y} \Vert x_{k+1} - x_k \Vert^2\right] \\
&= \mathbb{E}\left[ g(x_k)  - \tau_x\nabla g(x_k)^\top G_x(x_k,y_k) + \frac{L^2 \tau_x^2}{\mu_y} \Vert G_x(x_k,y_k) \Vert^2\right] \\
&= \mathbb{E}\left[ g(x_k)  - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 + \frac{\tau_x}{2} \Vert \nabla g(x_k) - G_x(x_k,y_k) \Vert^2 \right] \\
&\quad 
+\left(\frac{L^2 \tau_x^2}{\mu_y} - \frac{\tau_x}{2}\right) \mathbb{E} [\Vert G_x(x_k,y_k) \Vert^2] \\
&\le \mathbb{E} \left[ g(x_k)  - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 - \frac{\tau_x}{4} \Vert G_x(x_k,y_k) \Vert^2 \right] \\
&\quad +\mathbb{E}\left[ \tau_x \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2+ \tau_x \Vert  \nabla_x f(x_k,y_k) - G_x(x_k,y_k )\Vert^2\right].
\end{split}
\end{align}
where the last inequality uses  $\tau_x \le \mu_y / (4L^2)$.
% Similarly,  we can also show that
% \begin{align*}
% &\quad \mathbb{E}[ f(x_k,y_k)  - f(x_{k},y_{k+1})] \\
% &\le  \mathbb{E}\left[ - \nabla_y f(x_k,y_k)^\top (y_{k+1}-y_k) + \frac{L}{2} \Vert y_{k+1} - y_k \Vert^2\right] \\
% &= \mathbb{E}\left[  -\tau_y\nabla_y f(x_k,y_k)^\top G_y(x_k,y_k) + \frac{\tau_y^2 L}{2} \Vert G_y(x_k,y_k) \Vert^2\right] \\
% &= \mathbb{E} \left[  
% -\frac{\tau_y}{2} \Vert \nabla_y f(x_k,y_k) \Vert^2  - \left( \frac{\tau_y}{2} - \frac{\tau_y^2 L}{2} \right) \Vert G_y (x_k,y_k) \Vert^2 \right] \\
% &\quad + \mathbb{E} \left[ \frac{\tau_y}{2} \Vert \nabla_y f(x_k,y_k) - G_y(x_k,y_k) \Vert^2 
% \right ]
% \end{align*}
% and
% \begin{align*}
%     &\quad \mathbb{E} [ f(x_k,y_{k+1}) - f(x_{k+1},y_{k+1})] \\
%     &\le \mathbb{E} \left[ -\nabla_x f(x_{k},y_{k+1})^\top (x_{k+1}-x_k) + \frac{L}{2} \Vert x_{k+1} - x_k \Vert^2 \right] \\
%     &= \mathbb{E} \left[ \tau_x \nabla_x f(x_{k},y_{k+1})^\top G_x(x_k,y_k) + \frac{\tau_x^2 L}{2} \Vert G_x(x_k,y_k) \Vert^2 \right] \\
%     &= \mathbb{E} \left[ \tau_x (\nabla_x f(x_{k},y_{k+1})- G_x(x_k,y_k) )^\top G_x(x_k,y_k) + \left(\frac{\tau_x^2 L}{2} + \tau_x \right) \Vert G_x(x_k,y_k) \Vert^2 \right] \\
%     &\le \mathbb{E} \left[ \frac{\tau_x}{2} \Vert \nabla_x f(x_k,y_{k+1}) - G_x(x_k,y_k) \Vert^2 + \left(\frac{\tau_x^2 L}{2} + \frac{3\tau_x}{2} \right) \Vert G_x(x_k,y_k) \Vert^2  \right]
% \end{align*}
Similarly, we can also show that
\begin{align*}
&\quad \mathbb{E}[ f(x_k,y_k)  - f(x_{k+1},y_k)] \\
&\le  \mathbb{E}\left[ - \nabla_x f(x_k,y_k)^\top (x_{k+1}-x_k) + \frac{L}{2} \Vert x_{k+1} - x_k \Vert^2\right] \\
&= \mathbb{E}\left[  \tau_x\nabla_x f(x_k,y_k)^\top G_x(x_k,y_k) + \frac{\tau_x^2 L}{2} \Vert G_x(x_k,y_k) \Vert^2\right] \\
&= \mathbb{E}\left[  \tau_x (\nabla_x f(x_k, y_k) - G_x(x_k,y_k))^\top G_x(x_k,y_k) +  \left(\frac{\tau_x^2L}{2} +\tau_x\right) \Vert G_x(x_k,y_k) \Vert^2\right] \\
&\le \mathbb{E}\left[ \frac{\tau_x}{2} \Vert  \nabla_x f(x_k,y_k)  - G_x(x_k,y_k) \Vert^2  +2 \tau_x \Vert G_x(x_k,y_k) \Vert^2\right],
\end{align*}
and 
\begin{align*}
&\quad\mathbb{E}[ f(x_{k+1},y_k) - f(x_{k+1},y_{k+1})] \\
&\le \mathbb{E}\left[ - \nabla_y f(x_{k+1},y_k)^\top (y_{k+1} - y_k) + \frac{L}{2} \Vert y_{k+1} - y_k \Vert^2\right] \\
&= \mathbb{E}\left[ - \tau_y\nabla_y f(x_{k+1},y_k)^\top G_y(x_k,y_k) + \frac{\tau_y^2L}{2} \Vert G_y(x_k,y_k) \Vert^2\right] \\
&=  \mathbb{E}\left[ -
\frac{\tau_y}{2} \Vert \nabla_y f(x_{k+1},y_k) \Vert^2 + \frac{\tau_y}{2} \Vert \nabla_y f(x_{k+1},y_k) - G_y(x_k,y_k) \Vert^2 \right] \\
&\quad 
 -\left( \frac{\tau_y}{2} - \frac{\tau_y^2L}{2} \right) \mathbb{E} \left[ \Vert G_y(x_k,y_k) \Vert^2\right] \\
&\le \mathbb{E}\left[ -
\frac{\tau_y}{4} \Vert \nabla_y f(x_{k},y_k) \Vert^2 -\left( \frac{\tau_y}{2} - \frac{\tau_y^2L}{2} \right)  \Vert G_y(x_k,y_k) \Vert^2  \right] \\
&\quad + \mathbb{E} \left[ \frac{\tau_y}{2} \Vert \nabla_y f(x_{k},y_k) - G_y(x_k,y_k) \Vert^2 + 2\tau_y \Vert \nabla_y f(x_{k+1},y_k) - \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\le \mathbb{E}\left[ -
\frac{\tau_y}{4} \Vert \nabla_y f(x_{k},y_k) \Vert^2 -\left( \frac{\tau_y}{2} - \frac{\tau_y^2L}{2} \right)  \Vert G_y(x_k,y_k) \Vert^2  \right] \\
&\quad + \mathbb{E} \left[ \frac{\tau_y}{2} \Vert \nabla_y f(x_{k},y_k) - G_y(x_k,y_k) \Vert^2 + 2\tau_y \tau_x^2 L^2 \Vert  G_x(x_k,y_k) \Vert^2 \right]
\end{align*}
% where we use $\tau_y \le 1/(2L)$ and $\tau_x \le 1/L$ and Young's inequality.

Summing up the above two inequalities, we have
\begin{align*}
&\quad \mathbb{E} [f(x_k,y_k) - f(x_{k+1},y_{k+1})] \\
&\le \mathbb{E} \left[ - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 - \frac{\tau_y}{4} \Vert G_y(x_k,y_k) \Vert^2 + 3 \tau_x \Vert G_x (x_k,y_k) \Vert^2 \right] \\
&\quad + \mathbb{E} \left[ \frac{\tau_y}{2} \Vert \nabla_y f(x_k,y_k) - G_y(x_k,y_k) \Vert^2 + \frac{\tau_x}{2} \Vert \nabla_x f(x_k,y_k) - G_x(x_k,y_k) \Vert^2  \right] \\.
\end{align*}
Combing with inequality (\ref{eq: bound-Ak-spider}), it can be seen that
\begin{align*}
\mathbb{E}[\mathcal{B}_{k+1} ] &= \mathbb{E} [ g(x_{k+1}) -  f(x_{k+1},y_{k+1})] \\
&= \mathbb{E} [ g(x_{k+1} )- g(x_k) + g(x_k)  -f(x_k,y_k) + f(x_k,y_k)- f(x_{k+1},y_{k+1})] \\
&= \mathbb{E} \left[ \mathcal{B}_k   - \frac{\tau_y}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 - \frac{\tau_y}{4} \Vert G_y(x_k,y_k) \Vert^2 \right] \\
&\quad +\mathbb{E}  \left[  \tau_x \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2 +\frac{3\tau_x}{2} \Vert G_x(x_k,y_k )- \nabla_x f(x_k,y_k) \Vert^2 \right] \\
&\quad + \mathbb{E} \left[ \frac{\tau_y}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 + 3 \tau_x \Vert G_x(x_k,y_k) \Vert^2\right].
\end{align*}
Therefore, using $24\lambda \tau_x \le \tau_y$ and inequality (\ref{eq: bound-Ak-spider}) again, we obtain
\begin{align*}
\mathbb{E}[ \fV_{k+1} ] &= \mathbb{E}\left[ \mathcal{A}_{k+1} + \frac{\lambda \tau_x}{\tau_y} \mathcal{B}_{k+1}\right] \\
&\le \BE\left[ \fA_k + \frac{\lambda \tau_x}{\tau_y} \fB_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2  \right] \\
&\quad + \mathbb{E}\left[ \left(  \frac{3\lambda \tau_x^2}{\tau_y} - \frac{\tau_x}{4} \right)  \Vert G_x(x_k,y_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert G_y (x_k,y_k) \Vert^2  \right] \\
&\quad + \left( \tau_x + \frac{\lambda \tau_x}{\tau_y}\right) \mathbb{E} \left[ \Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2\right] \\
&\quad + \left( \tau_x + \frac{3 \lambda \tau_x^2 }{2 \tau_y}  \right) \BE \left[ \Vert G_x(x_k,y_k) - \nabla_x f(x_k,y_k) \Vert^2 \right]\\
&\quad +  \frac{\lambda \tau_x}{2} \mathbb{E}\left[ \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\le \mathbb{E}\left[ \mathcal{A}_k + \frac{\lambda \tau_x }{ \tau_y}\mathcal{B}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2\right]\\
&\quad +  2 \tau_x \mathbb{E} \left[\Vert \nabla g(x_k) - \nabla_x f(x_k,y_k) \Vert^2\right] \\
&\quad +  \mathbb{E}\left[\frac{5\tau_x}{2} \Vert G_x(x_k,y_k )- \nabla_x f(x_k,y_k) \Vert^2 - \frac{\tau_x}{8} \Vert G_x(x_k,y_k) \Vert^2  \right] \\
&\quad + \mathbb{E}\left[ \frac{\lambda \tau_x}{2} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert G_y(x_k,y_k) \Vert^2
\right]. 
\end{align*}
Furthermore,
\begin{align*}
\mathbb{E}[\mathcal{V}_{k+1} ] &\le 
\mathbb{E}\left[ \mathcal{V}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 + \frac{2L^2 \tau_x}{\mu_y^2} \Vert \nabla_y f(x_k,y_k) \Vert^2\right]  \\
&\quad +  \mathbb{E}\left[\frac{5\tau_x}{2} \Vert G_x(x_k,y_k )- \nabla_x f(x_k,y_k) \Vert^2 - \frac{\tau_x}{8} \Vert G_x(x_k,y_k) \Vert^2\right] \\
&\quad + \mathbb{E}\left[ \lambda \tau_x \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert G_y(x_k,y_k) \Vert^2\right] \\
&\le \mathbb{E} \left[ \mathcal{V}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad - \mathbb{E}\left[ \frac{\tau_x}{8} \Vert G_x(x_k,y_k) \Vert^2 + \frac{\lambda \tau_x}{8} \Vert G_y(x_k,y_k) \Vert^2 \right] \\
&\quad +  \mathbb{E}\left[  \frac{5\tau_x}{2} \Vert G_x(x_k,y_k )- \nabla_x f(x_k,y_k) \Vert^2 + \frac{9\lambda \tau_x}{8} \Vert G_y(x_k,y_k) - \nabla_y f(x_k,y_k) \Vert^2\right].
\end{align*}
Above, the first inequality follows from Lemma \ref{lem: new-bound-Bk} and the second inequality uses Young's inequality that $ \BE [\Vert a - b \Vert^2] \le \BE [ \Vert a \Vert^2 + \Vert b \Vert^2]  $ and  $\lambda = 32L^2 / \mu_y^2$.

Plug in the variance bound by Lemma \ref{lem: spider-var} and  $B  = M$, we have
\begin{align*}
\mathbb{E}[\mathcal{V}_{k+1}] &\le \mathbb{E}\left[ \mathcal{V}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad + \mathbb{E} \left[ \left(\frac{5 }{2} + \frac{9\lambda }{8}\right) \frac{\tau_x^3 L^2}{M} \sum_{j=(n_k-1)M}^k \Vert G_x(x_j,y_j) \Vert^2  - \frac{\tau_x}{8} \Vert G_x(x_k,y_k) \Vert^2\right] \\
&\quad + \mathbb{E}\left[ \left(\frac{5 }{2} + \frac{9\lambda }{8}\right) \frac{\tau_x \tau_y^2 L^2}{M} \sum_{j=(n_k-1)M}^k\Vert G_y(x_j,y_j) \Vert^2  - \frac{\lambda \tau_x}{8} \Vert G_y(x_k,y_k) \Vert^2\right] \\
&\le   \mathbb{E}\left[ \mathcal{V}_k - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad+ \mathbb{E} \left[ \frac{5\lambda \tau_x^3  L^2}{4M} \sum_{j=(n_k-1)M}^k  \Vert G_x(x_j,y_j) \Vert^2  - \frac{\tau_x}{8} \Vert G_x(x_k,y_k) \Vert^2\right] \\
&\quad + \mathbb{E}\left[ \frac{5\lambda \tau_x \tau_y^2 L^2}{4M} \sum_{j=(n_k-1)M}^k \Vert G_y(x_j,y_j) \Vert^2  - \frac{\lambda \tau_x}{8} \Vert G_y(x_k,y_k) \Vert^2\right].
\end{align*}

Now we telescope for $i = (n_k-1)M, \cdots, k$.
\begin{align*}
\mathbb{E} [ \fV_{k+1}] &\le \mathbb{E}\left[ \mathcal{V}_{(n_k-1)M} - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad+ \BE \left[ \sum_{i= (n_k-1)M}^k \left( \sum_{j=(n_k-1)M}^i \frac{5\lambda \tau_x^3  L^2}{4M}  \Vert G_x(x_j,y_j) \Vert^2  - \frac{\tau_x}{8} \Vert G_x(x_i,y_i) \Vert^2\right)\right] \\
&\quad + \mathbb{E}\left[  \sum_{i= (n_k-1)M}^k \left(  \sum_{j=(n_k-1)M}^i \frac{5\lambda \tau_x \tau_y^2 L^2}{4M} \Vert G_y(x_j,y_j) \Vert^2  - \frac{\lambda \tau_x}{8} \Vert G_y(x_i,y_i) \Vert^2\right)\right] \\
&\le \mathbb{E}\left[ \mathcal{V}_{(n_k-1)M} - \frac{\tau_x}{2} \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad + \mathbb{E}  \left[\sum_{j=(n_k-1)M}^k  \left( \frac{5\lambda \tau_x^3  L^2}{4}  \Vert G_x(x_j,y_j) \Vert^2  - \frac{\tau_x}{8} \Vert G_x(x_j,y_j) \Vert^2\right)\right] \\
&\quad + \mathbb{E}\left[   \sum_{j=(n_k-1)M}^k  \left( \frac{5\lambda \tau_x \tau_y^2 L^2}{4}\Vert G_y(x_j,y_j) \Vert^2  - \frac{\lambda \tau_x}{8} \Vert G_y(x_j,y_j) \Vert^2\right)\right] \\
&\le \BE \left[\fV_{(n_k-1)M} - \frac{\tau_x}{2} \sum_{j=(n_k-1)M}^k \Vert \nabla g(x_k) \Vert^2 -\frac{\lambda \tau_x}{4} \sum_{j=(n_k-1)M}^k \Vert \nabla_y f(x_k,y_k) \Vert^2 \right] \\
&\quad 
- \mathbb{E} \left[\frac{\tau_x}{16} \sum_{j=(n_k-1)M}^k \Vert G_x(x_j,y_j) \Vert^2 +\frac{\lambda \tau_x}{16} \sum_{j=(n_k-1)M}^k \Vert G_y(x_j,y_j) \Vert^2 \right],
\end{align*}
where we use $ \lambda \tau_x^2L^2 \le 1/20 $ and $\tau_y^2L^2 \le 1/20$ in the last inequality.

From now on, we need to write down the subscripts with respect to $t$. Telescope for $k = 0,\cdots,K-1$ and drop the negative terms containing $ \Vert G_x (x_j,y_j) \Vert^2$ and $ \Vert G_y (x_j,y_j) \Vert^2$, we have
\begin{align*}
    \BE[\fV_{t,K}] \le \mathbb{E}\left[ \mathcal{V}_{t,0}   - \frac{\tau_x}{2} \sum_{k=0}^{K-1} \Vert \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \sum_{k=0}^{K-1} \Vert \nabla_y f(x_k,y_k) \Vert^2\right].
\end{align*}
Rearranging this inequality concludes the proof.
% Noting that we choose $(\tilde x_{t+1},\tilde y_{t+1})$ from $\{  (x_{t,k}, y_{t,k} )\}_{k=0}^{K-1} $ uniformly at random, we have
% \begin{align} \label{eq: spider-mid-eq}
%     \BE \left[ \frac{\tau_x}{16} \Vert G_x(\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2 + \frac{\lambda \tau_x}{16} \Vert G_y(\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2 \right] &\le \frac{1}{K}  \BE \left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y}\tilde \fB_t\right].
% \end{align}

% Additionally, denote random variable $\xi_t$ as the index from $k = 0,1,\cdots,K-1$ that is chosen as $(\tilde x_{t+1},\tilde y_{t+1})$, it holds that
% \begin{align*}
% &\quad \mathbb{E}[ \Vert G_x (\tilde x_{t+1}, \tilde y_{t+1}) - \nabla_x f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 ] \\
% &\le \mathbb{E} \left[\frac{L^2 \tau_x^2}{B}\sum_{j=(n_{\xi_t} - 1)M}^{\xi_t} \Vert G_x(x_j,y_j) \Vert^2\right] \\
% &\le \frac{L^2 \tau_x^2 M}{K B} \sum_{j=0}^{K-1} \mathbb{E}\Vert G_x (x_j, y_j) \Vert^2 \\
% &= \frac{L^2 \tau_x^2}{K} \sum_{j=0}^{K-1} \mathbb{E}\Vert G_x (x_j, y_j) \Vert^2 \\
% &= L^2 \tau_x^2 \mathbb{E}\Vert G_x (\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2, 
% \end{align*}
% where we use Lemma \ref{lem: spider-var} again in the first inequality; the second inequality holds because the probability that $n_{\xi_t} = 1, 2, \cdots, n_K$ is less than or equal to $M /K$. Similarly,
% \begin{align*}
% \mathbb{E}\Vert G_x (\tilde x_{t+1}, \tilde y_{t+1}) - \nabla_x f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 
% &\le L^2 \tau_x^2 \mathbb{E}\Vert G_x (\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2. 
% \end{align*}
% Now it is sufficient to show that
% \begin{align*}
% &\quad\mathbb{E}[ \Vert \nabla_x f(\tilde x_{t+1},\tilde y_{t+1} ) \Vert^2 + \lambda \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2] \\
% &\le \mathbb{E}[ 2 \Vert \nabla_x f(\tilde x_{t+1},\tilde y_{t+1})  - G_x(\tilde x_{t+1},\tilde y_{t+1})\Vert^2 +2 \Vert G_x(\tilde x_{t+1},\tilde y_{t+1} )\Vert^2 ] \\
% &\quad + \mathbb{E}[ 2 \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1})  - G_y(\tilde x_{t+1},\tilde y_{t+1})\Vert^2 +2 \Vert G_y(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 ] \\
% &\le 2( 1+ L^2 \tau_x^2) \mathbb{E}[ \Vert G_x (\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2] +  2 \lambda(1+L^2 \tau_y^2) \mathbb{E}[ \Vert G_y (\tilde x_{t+1},\tilde y_{t+1}) \Vert^2] \\
% &\le 4 \mathbb{E} \Vert G_x(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 + 4 \lambda \mathbb{E}\Vert G_y (\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 \\
% &\le \frac{64}{\tau_x K} \mathbb{E}\left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y} \tilde \fB_t \right],
% \end{align*}
% where we use inequality (\ref{eq: spider-mid-eq}) in the last line.

\end{proof}
\end{lem}

Equipped with the above lemma, we can easily prove Theorem \ref{thm: SPIDER-GDA}.

\subsection{Proof of Theorem \ref{thm: SPIDER-GDA}}

\begin{proof}

Recall that $g(\,\cdot\,)$ is $\mu_x$-PL and $-f(x,\,\cdot\,)$ is $\mu_y$-PL, it holds that
\begin{align*}
    \Vert \nabla g(x) \Vert^2 &\ge 2 \mu_x ( g(x) - g(x^*) ) \\
    \Vert \nabla_y f(x,y) \Vert^2 &\ge 2 \mu_y (g(x) - f(x,y) ).
\end{align*}
Then Lemma \ref{lem: key-lem-for-Spider} implies
\begin{align*}
    \mu_x \tau_x  \BE \left[\tilde \fA_{t+1} +  \frac{\lambda \mu_y}{2 \mu_x} \tilde \fB_{t+1}\right]  \le \frac{1}{K} \BE\left[\tilde \fA_t + \frac{\lambda\tau_x}{\tau_y} \tilde \fB_t\right].
\end{align*}
By $ \mu_y \tau_y \ge 2 \mu_x \tau_x$, we further have
\begin{align*}
      \BE \left[\tilde \fA_{t+1} +  \frac{\lambda \tau_x}{\tau_y} \tilde \fB_{t+1}\right] \le \frac{1}{\mu_x \tau_x K} \BE \left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y} \fB_t \right] \le \frac{1}{2}  \BE \left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y} \fB_t \right].
\end{align*}
%\begin{align*}
% &\quad \mathbb{E}\left[ \tilde \fA_{t+1} + \frac{\lambda\tau_x}{\tau_y} \tilde \fB_{t+1}\right] \\
% &\le \mathbb{E}\left[ \frac{1}{2\mu_x} \Vert \nabla g(\tilde x_{t+1}) \Vert^2 +  \frac{\lambda \tau_x}{2 \mu_y \tau_y}  \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2\right] \\
% &\le \mathbb{E}\left[ \frac{1}{2\mu_x} \Vert \nabla g(\tilde x_{t+1}) \Vert^2 +  \frac{1}{48 \mu_y }  \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2\right] \\
% &\le \BE\left[ \frac{1}{\mu_x} \Vert \nabla_x f(\tilde x_{t+1}, y_{t+1} ) \Vert^2 + \frac{1}{\mu_x} \Vert \nabla_x f(\tilde x_{t+1}, \tilde y_{t+1}) - \nabla g(\tilde x_{t+1}) \Vert^2 \right] \\
% &\quad  + \BE \left[\frac{1}{48 \mu_y }  \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2\right] \\
% &\le \mathbb{E}\left[ \frac{1}{\mu_x} \Vert \nabla_x f(\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2 + \left( \frac{1}{48 \mu_y } + \frac{L^2}{\mu_x \mu_y^2} \right)\Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2\right] \\
% &\le \BE \left[\frac{1}{\mu_x} \Vert \nabla_x f(\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2 + \frac{33 L^2}{32\mu_x \mu_y^2} \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 \right] \\
% &\le \frac{33}{\mu_x}  \BE \left[ \Vert \nabla_x f(\tilde x_{t+1}, \tilde y_{t+1}) \Vert^2 + \lambda \Vert \nabla_y f(\tilde x_{t+1},\tilde y_{t+1}) \Vert^2 \right] \\
% &\le \frac{2112}{\mu_x \tau_xK} \BE \left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y}\tilde \fB_t \right] \\
% &\le \frac{1}{2} \BE \left[ \tilde \fA_t + \frac{\lambda \tau_x}{\tau_y}\tilde \fB_t \right].
% \end{align*}
% Above, in the first inequality we use the definition of PL condition; in the the second we plug in $\lambda \tau_x/\tau_y = 1/24$; the third one is due to Young's inequality; the fourth one follows from Lemma \ref{lem: new-bound-Bk}; the fifth  and sixth ones are both trivial; in the second last one we use Lemma \ref{lem: key-lem-for-Spider} and in the last one we plug in our choice of $K$. 

Therefore, to find $\hat x$ such that $g(\hat x) - g(x^{\ast}) \le \epsilon$ and $ g(\hat x) - f(\hat x,\hat y) \le 24\epsilon$ in expectation,  the complexity is
\begin{align*}
    \mathcal{O}\left( (n+MK ) \log \left(\frac{1}{\epsilon}\right)\right) = \mathcal{O} \left( (n+K \sqrt{n})\log\left( \frac{1}{\epsilon}\right)\right) = \mathcal{O} \left((n + \sqrt{n} \kappa_x \kappa_y^2) \log \left(\frac{1}{\epsilon}\right) \right)
\end{align*}
\end{proof}


\section{Proof of Section \ref{sec: Catalyst-for-acc}}

In this section, we present the convergence results of AccSPIDER-GDA when $\gamma = 0$, now  $F_k$ can be written as: 
\begin{align*}
    \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} F_k(x,y) \triangleq f(x,y) + \frac{\beta}{2} \Vert x - x_k \Vert^2.
\end{align*}


First of all, we take a closer look at $F_k$. The regularization term $\beta$ transforms the condition number of the problem.
\begin{lem} \label{lem: sub-probelm}
Given $\beta > L$, the sub-problem  $F_k(x,y)$ is $(\beta-L)$-PL in $x$ , $\mu_y$-PL in $y$ and $(\beta+L)$-smooth if $f$ satisfies $L$-smmoth and $\mu_x$-PL in $x$.
\end{lem}

Strong duality also holds for sub-problem $F_k(x,y)$ since its saddle point exist.
\begin{lem} \label{lem: dual-Fk}
Under Assumption \ref{asm: one-PL} and \ref{asm:uni}, given $\beta > L$, the sub-problem $F_k(x,y)$ has a unique saddle point.
\begin{proof}
Denote $G_k(x) \triangleq \max_{y \in \BR^{d_y}} F_k(x,y)$. According to Assumption \ref{asm:uni}, the inner  problem $\max_{ y \in \BR^{d_y}} F_k(x,y)$ has a unique solution $y^*(x)$. 


Additionally, it is clear that $F_k(x,y)$ is strongly convex in $x$ for $\beta >L$. Hence, we know that $G_k(x)$ is strongly convex since taking the supremum is an operation that preserve (strong) convexity and . In this case, the outer problem $\min_{x \in \BR^{d_x}}G_k(x)$ also has a unique solution $x^*$.

Above, the point $(x^*,y^*)$ is a unique global minimax point of $F_k(x,y)$. And the global minimax point of $F_k(x,y)$ is equivalent to a saddle point of $F_k(x,y)$ by Lemma \ref{lem: three-points-equal}.
\end{proof}
\end{lem}

From now on, throughout this section, we always $(\tilde x_{k}, \tilde y_k)$ be the saddle point of $F_{k-1}$ for all $k \ge 1$.

Next, we study the error brought by the inexact solution to the sub-problem. The idea is that when we can controls the precision of $F_k$ with a global constant $\delta$, then the algorithm will be close to the exact proximal point algorithm. We omit the notation of expectation when no ambiguity arises.

\begin{lem} \label{lem: approx-F_k}

Suppose $x_{k+1}$ satisfies $\BE[\Vert x_{k+1} - \tilde x_{k+1} \Vert^2] \le \delta $ for any saddle point $(\tilde x_{k+1}, \tilde y_{k+1})$ of $F_k$, then it holds that 
\begin{align*}
    \BE \left[F_k(x_{k+1}, \tilde y_{k+1}) - F_k(\tilde x_{k+1}, \tilde y_{k+1})\right] \le \frac{(\beta+L)^2 \delta}{2 (\beta-L)}.
\end{align*}
\begin{proof}

Lemma \ref{lem: minimax} tells us that $F_k(\tilde x_{k+1}, \tilde y_{k+1}) = \min_{x \in \BR^{d_x}} F_k(x,\tilde y_{k+1})$, thus, we have 
\begin{align*}
&\quad \BE[F_k(x_{k+1}, \tilde y_{k+1}) - F_k(\tilde x_{k+1}, \tilde y_{k+1})] \\
&= \BE[F_k(x_{k+1},\tilde y_{k+1}) - \min_{x \in \BR^{d_x}} F_k(x, \tilde y_{k+1})] \\
&\le \frac{1}{2 (\beta-L)} \BE[\Vert \nabla_x F_k(x_{k+1}, \tilde y_{k+1}) \Vert^2] \\
&= \frac{1}{2(\beta-L)} \BE[\Vert \nabla_x F_k(x_{k+1},\tilde y_{k+1}) - \nabla_x F_k(\tilde x_{k+1}, \tilde y_{k+1}) \Vert^2] \\
&\le \frac{(\beta+L)^2}{2 (\beta-L)} \BE[\Vert x_{k+1} - \tilde x_{k+1} \Vert^2] \\
&\le \frac{(\beta+L)^2 \delta}{2 (\beta-L)},
\end{align*}    
where  the first and third inequalities rely on Lemma \ref{lem: sub-probelm} that $F_k(x,y)$ is $(\beta-L)$-PL in $x$ and $(\beta+L)$-smooth and  the second equality is dependent on the fact that $\nabla_x F_k(\tilde x_{k+1}, \tilde y_{k+1}) = 0$.

\end{proof}

\end{lem}

We can see that when we can find a $\delta$-saddle point of $F_k$, then we can approximate $g(x)$ well. 

\begin{lem} \label{lem: approx-g(x)}

Suppose $x_{k+1}$ satisfies $\BE \Vert x_{k+1} - \tilde x_{k+1} \Vert^2  \le \delta $ for any saddle point $(\tilde x_{k+1}, \tilde y_{k+1})$ of $F_k$, then it holds that 
\begin{align*}
    \BE \vert g(x_{k+1}) - g(\tilde x_{k+1}) \vert 
    &\le \left (\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right ) \delta.
\end{align*}
\begin{proof}

By definition we know the relationship between $g(x) $ and $F_k(x,y)$ is given by: 
\begin{align*}
    g(x) = \max_{y \in \BR^{d_y}} f(x,y) = F_k( x,\tilde y_{k+1}) - \frac{\beta}{2} \Vert x - x_k \Vert^2. 
\end{align*}
Thus,
\begin{align*}
&\quad \BE\vert g(x_{k+1}) - g(\tilde x_{k+1}) \vert \\
&= \BE\left \vert \left(F_k( x_{k+1},\tilde y_{k+1}) - \frac{\beta}{2} \Vert x_{k+1} -x_k \Vert^2 \right) - \left(F_k(\tilde x_{k+1}, \tilde y_{k+1}) - \frac{\beta}{2} \Vert \tilde x_{k+1} - x_k \Vert^2\right) \right\vert \\
&\le \BE \left[F_k(x_{k+1}, \tilde y_{k+1}) - F_k(\tilde x_{k+1}, \tilde y_{k+1}) + \frac{\beta}{2} \Vert x_{k+1 } - \tilde x_{k+1} \Vert^2\right] \\
&\le \left (\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right ) \delta,
\end{align*}
where the second inequality follows from the triangle inequality of distance and the third inequality follows from  Lemma \ref{lem: approx-F_k}. 

\end{proof}
\end{lem}

When the sub-problem is solved precisely enough, we can show that $g(x)$ decreases in each iteration.

\begin{lem} \label{lem:g(x)-decrease}
Suppose $x_{k+1}$ satisfies $\BE \Vert x_{k+1} - \tilde x_{k+1} \Vert^2 \le \delta $ for any saddle point $(\tilde x_{k+1}, \tilde y_{k+1})$ of $F_k$, then it holds that 
\begin{align*} 
\BE[g(x_{k+1}) - g(x^{\ast})]  &\le \BE \left[g(x_k) - g(x^{\ast}) -  \frac{\beta}{2} \Vert x_{k+1} - x_k \Vert^2 + \frac{(\beta+L)^2 \delta}{2(\beta-L)}\right], 
\end{align*}
\end{lem}

\begin{proof}
Consider the following inequalities:
\begin{align*} 
\BE[g(x_{k+1}) - g(x^{\ast})] &=  \BE \left[F_k(x_{k+1}, \tilde y_{k+1}) - g(x^{\ast}) - \frac{\beta}{2} \Vert x_{k+1} - x_k \Vert^2\right]  \\
&\le \BE \left[F_k(\tilde x_{k+1}, \tilde y_{k+1}) - g(x^{\ast})- \frac{\beta}{2} \Vert x_{k+1} - x_k \Vert^2 + \frac{(\beta+L)^2 \delta}{2(\beta-L)}\right] \\
&\le \BE \left[g(x_k) - g(x^{\ast}) -  \frac{\beta}{2} \Vert x_{k+1} - x_k \Vert^2 + \frac{(\beta+L)^2 \delta}{2(\beta-L)}\right], 
\end{align*}
where in the first inequality we use Lemma \ref{lem: approx-F_k}, the second inequality is because we know it holds that  $F_k(\tilde x_{k+1}, \tilde y_{k+1}) \le F_k(x_k, \tilde y_{k+1}) = g(x_k)$.
\end{proof}
Now, we consider how $g(x_k)$ converge to $g(x^{\ast})$ when precision $\delta$ is obtained.
\begin{lem}  \label{lem: g(x)-convergence}
Suppose $x_{k+1}$ satisfies $\BE\Vert x_{k+1} - \tilde x_{k+1} \Vert^2 \le \delta $ for any saddle point $(\tilde x_{k+1}, \tilde y_{k+1})$ of $F_k$, then it holds that 
\begin{align*}
    \BE[g(x_{k+1}) - g(x^{\ast})] \le \BE \left[g(x_k) - g(x^{\ast}) - \frac{1}{4 \beta} \Vert \nabla g(\tilde x_{k+1}) \Vert^2 + \left (\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta\right]. 
\end{align*}
\begin{proof}

The proof is based on Lemma \ref{lem:g(x)-decrease}
\begin{align*}
    \BE[g(x_{k+1}) - g(x^{\ast})] 
    &\le \BE \left[g(x_k) - g(x^{\ast}) -  \frac{\beta}{2} \Vert x_{k+1} - x_k \Vert^2 + \frac{(\beta+L)^2 \delta}{2(\beta-L)}\right] \\
    &\le \BE \left[g(x_k) - g(x^{\ast}) - \frac{\beta}{4} \Vert \tilde x_{k+1} - x_k \Vert^2 + \frac{\beta}{2} \Vert x_{k+1} - \tilde  x_{k+1}\Vert^2 + \frac{(\beta+L)^2 \delta}{2(\beta-L)}\right] \\
    &= \BE \left[g(x_k) - g(x^{\ast}) - \frac{1}{4 \beta} \Vert \nabla g(\tilde x_{k+1}) \Vert^2 + \left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta\right], 
\end{align*}
where the second inequality relies on the fact that $- \Vert a - b\Vert^2 \le \frac{1}{2} \Vert a \Vert^2 + \Vert b \Vert^2$. In the last equality we use the fact that $(\tilde x_{k+1}, \tilde y_{k+1})$ is also a stationary point by Lemma \ref{lem: three-points-equal}, which implies that $ \nabla g( \tilde x_{k+1}) + \beta ( \tilde x_{k+1} - x_k ) = 0$. 


\end{proof}

\end{lem}


\subsection{Proof of Lemma \ref{lem: outer-converge}}

\begin{proof}
Noting that $g(x)$ satisfies $\mu_x$-PL by Lemma \ref{lem: g(x)-PL} and using the result of Lemma \ref{lem: g(x)-convergence}, we can see that 
\begin{align*}
    &\quad \BE[g(x_{k+1}) - g(x^{\ast})] \\
    &\le \BE \left[g(x_k) - g(x^{\ast}) - \frac{1}{4 \beta} \Vert \nabla g(\tilde x_{k+1}) \Vert^2 + \left ( \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right ) \delta\right] \\
    &\le \BE \left[g(x_k) - g(x^{\ast}) - \frac{\mu_x}{2 \beta} (g(\tilde x_{k+1}) - g(x^{\ast})) +  \left ( \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta\right]. 
\end{align*}
Using Lemma \ref{lem: approx-g(x)}, we obtain 
\begin{align*}
    &\quad \BE[g(x_{k+1}) - g(x^{\ast})] \\ 
    &\le \BE \left[g(x_k) - g(x^{\ast}) - \frac{\mu_x}{2 \beta}(g(x_{k+1}) - g(x^{\ast})) +\left(1 + \frac{\mu_x}{2 \beta}\right) \left ( \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right ) \delta\right].
\end{align*}
Rearranging,
\begin{align*}
    &\quad \BE [g(x_{k+1}) - g(x^{\ast})] \\ &\le \BE \left[\left(1 - \frac{\mu_x}{2 \beta + \mu_x}\right) (g(x_k) - g(x^{\ast})) + \left (\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right ) \delta\right]. 
\end{align*}
Let $q \triangleq {\mu_x}/{(2\beta+ \mu_x)}$ and telescope, then we can obtain that
\begin{align*}
    &\quad \BE[g(x_k) - g(x^{\ast})] \\
    &\le (1- q)^k (g(x_0) - g(x^{\ast})) + \left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right) \delta  \sum_{i=0}^{k-1}(1-q)^i \\
    &\le (1-q)^k (g(x_0) - g(x^{\ast})) + \left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right) \frac{\delta}{q}.
\end{align*}

Plugging in $\beta, \delta$ yields the desired statement,
\end{proof}


Now we show that how we can control the precision of the sub-problem $\delta$ recursively to satisfy the condition of Lemma \ref{lem: outer-converge} that $\BE[\Vert x_{k} - \tilde x_{k} \Vert^2 + \Vert y_k - \tilde y_k \Vert^2] \le \delta$ holds for all $k \ge 1$.

Before that, we need the following lemma showing that when $\Vert  x_k - x_{k+1} \Vert $ is small, then the distance between the saddle points of $F_k$ and $F_{k+1}$ will be also small. Denote $(\tilde x_k,\tilde y_k) $ be a saddle point of sub-problem $F_{k-1}$ and $(\tilde x_{k+1}, \tilde y_{k+1})$ be a saddle point of sub-problem $F_k$. 

\begin{lem} \label{lem: bound-tilde-xy}

If we let $\beta  > L$, then it holds true that
\begin{align*}
    \Vert \tilde x_{k+1} - \tilde x_k \Vert^2 + \Vert \tilde y_{k+1} - \tilde y_k \Vert^2 &\le \frac{4 \beta^2}{(\beta-L) \mu_y} \Vert x_k - x_{k-1} \Vert^2.
\end{align*}

\begin{proof}

Noting that $F_{k-1}$ is  $(\beta-L)$-PL in $x$ and $\mu_y$-PL in $y$ by Lemma \ref{lem: sub-probelm} and using the quadratic growth condition by Lemma \ref{lem: quadratic-growth}, we have 
\begin{align*}
    \frac{\beta-L}{2} \Vert  \tilde x_{k+1} - \tilde x_k \Vert^2 &\le F_{k-1}(\tilde x_{k+1},\tilde y_k) - \min_{x \in \BR^{d_x}} F_{k-1}(x, \tilde y_k) = F_{k-1}(\tilde x_{k+1},\tilde y_k) - F_{k-1}(\tilde x_k, \tilde y_k), \\
    \frac{\mu_y}{2} \Vert \tilde y_{k+1} - \tilde y_k \Vert^2 &\le \max_{y \in \BR^{d_y}} F_{k-1}(\tilde x_k, y) - F_{k-1}(\tilde x_k, \tilde y_{k+1}) = F_{k-1}(\tilde x_k, \tilde y_k) - F_{k-1}(\tilde x_k, \tilde y_{k+1}). 
\end{align*}
Combining the above two inequalities, we can see that 
\begin{align*}
    &\quad \frac{\beta-L}{2} \Vert  \tilde x_{k+1} - \tilde x_k \Vert^2 + \frac{\mu_y}{2} \Vert\tilde y_{k+1} - \tilde y_k \Vert^2 \\
    &\le F_{k-1}(\tilde x_{k+1} , \tilde y_k) - F_{k-1}(\tilde x_k ,\tilde y_{k+1}) \\
    &= F_k(\tilde x_{k+1},\tilde y_k) + \frac{\beta}{2} \Vert \tilde x_{k+1} - x_{k-1} \Vert^2 - \frac{\beta}{2} \Vert \tilde x_{k+1} - x_k \Vert^2 \\
    &\quad - F_k(\tilde x_k, \tilde y_{k+1}) - \frac{\beta}{2} \Vert \tilde x_k- x_{k-1} \Vert^2 + \frac{\beta}{2} \Vert \tilde x_k  - x_k \Vert^2 \\
    &\le \frac{\beta}{2} \Vert \tilde x_{k+1} - x_{k-1} \Vert^2 - \frac{\beta}{2} \Vert \tilde x_{k+1} - x_k \Vert^2 - \frac{\beta}{2} \Vert \tilde x_k- x_{k-1} \Vert^2 + \frac{\beta}{2} \Vert \tilde x_k  - x_k \Vert^2 \\
    &= \beta (\tilde x_{k+1} - \tilde x_k)^\top(x_k - x_{k-1}) \\
    &\le \frac{\beta-L}{4} \Vert \tilde x_{k+1} - \tilde x_k \Vert^2 + \frac{\beta^2}{\beta-L} \Vert x_k - x_{k-1} \Vert^2,  
\end{align*}
where the second inequality is based on $F_k(\tilde x_{k+1}, \tilde y_k) \le F_k(\tilde x_{k+1}, \tilde y_{k+1}) \le F_k(\tilde x_k ,\tilde y_{k+1})$ by $(\tilde x_{k+1}, \tilde y_{k+1}) $ is a saddle point of $F_k$. In the last inequality we use Young's inequality. Rearranging,
\begin{align*}
    \frac{\beta-L}{4} \Vert \tilde x_{k+1} - \tilde x_k \Vert^2 + \frac{\mu_y}{2} \Vert \tilde y_{k+1} - \tilde y_k \Vert^2 &\le \frac{\beta^2}{\beta-L} \Vert x_k - x_{k-1} \Vert^2. 
\end{align*}
Since we have ${(\beta - L)}/{\mu_y} \ge {L}/{4} \ge {\mu_y}/{4}$, we can obtain that 
\begin{align*}
    \Vert \tilde x_{k+1} - \tilde x_k \Vert^2 + \Vert \tilde y_{k+1} - \tilde y_k \Vert^2 &\le \frac{4 \beta^2}{(\beta-L) \mu_y} \Vert x_k - x_{k-1} \Vert^2,
\end{align*}
\end{proof}
\end{lem}

An additional bound is for the use of the base case, i.e. $k=0$.
\begin{lem} \label{lem:induction-base}
If we let $\beta  > L$, then it holds true that
\begin{align*}
    \Vert x_0 - \tilde x_1 \Vert^2 + \Vert y_0 - \tilde y_1 \Vert^2 \le \frac{2}{\mu_y} (g(x_0) - g(x^*))
\end{align*}
\end{lem}

\begin{proof}
Note that $F_1$ satisfies $(\beta-L) $-PL in $x$ and $\mu_y$-PL in $y$. We can  bound $ \Vert x_0 - \tilde x_1 \Vert^2 + \Vert y_0 - \tilde y_1 \Vert^2 $ as follows:
\begin{align*}
    \frac{\beta-L}{2} \Vert x_0 - \tilde x_1 \Vert^2 &\le F_0(x_0, \tilde y_1) - \min_{x \in \BR^{d_x}} F_0(x, \tilde y_1) = F_0(x_0, \tilde x_1) - F_0(\tilde x_1, \tilde y_1), \\
    \frac{\mu_y}{2} \Vert y_0 - \tilde y_1 \Vert^2 &\le \max_{y \in \BR^{d_y}} F_0(\tilde x_1, y) - F_0(\tilde x_1, y_0) = F_0(\tilde x_1, \tilde y_1) - F_0(\tilde x_1, y_0).
\end{align*}
Combining the above two inequalities, we have
\begin{align*}
    &\quad \frac{\beta-L}{2} \Vert x_0 - \tilde x_1 \Vert^2 + \frac{\mu_y}{2} \Vert y_0 - \tilde y_1 \Vert^2 \\
    &\le F_0(x_0,\tilde y_1) - F_0(\tilde x_1, y_0) \\
    &= f(x_0, \tilde y_1) - f(\tilde x_1, y_0) - \frac{\beta}{2} \Vert  x_0 - \tilde x_1 \Vert^2 \\
    &\le f(x_0, \tilde y_1) - f(\tilde x_1, y_0) \\
    &\le \max_{y \in \BR^{d_y}} f(x_0, y) - \min_{x \in \BR^{d_x}} f(x, y_0) \\
    &= \max_{y \in \BR^{d_y}} f(x_0,y) - \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} f(x,y) + \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} f(x,y) - \min_{x \in \BR^{d_x}} f(x,y_0)  \\
    &\le g(x_0) - g(x^{\ast}),
\end{align*}
where we use the definition of $g$ and the fact that $ \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} f(x,y) \ge \min_{x \in \BR^{d_x}} f(x,y_0) $ in the last inequality. Rearranging and noting that $\beta- L \ge L \ge \mu_y$, we finish the proof.
\end{proof}
\subsection{The Proof of Lemma \ref{sub:eps-saddle}}

\begin{proof}
Recall we  solve the sub-problem:
\begin{align*}
    \max_{y \in \BR^{d_y}} \min_{x \in \BR^{d_x}} F_k(x,y) = - \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} \{ - F_k(x,y) \}.
\end{align*}
It is $\mu_y$-PL in $y$ and $L$-strongly-convex in $x$ and thus clearly satisfies $L$-PL in $x$.

We define  $H_k(y) = \min_{x \in \BR_{d_x}} F_k(x,y)$. By Lemma \ref{lem: g(x)-L-smooth} and \ref{lem: g(x)-PL}, we know that $H_k(y)$ is also $\mu_y$-PL in $y$ and it is $12L$-smooth since $F_k$ is $3L$-smooth.

According to Theorem \ref{thm: SPIDER-GDA}, We know that SPIDER-GDA makes sure
\begin{align*}
&\quad \underbrace{\mathbb{E}\left[  H_{k+1}(\tilde y_{k+1}) - H_{k+1}( y_{k+1}) + \frac{1}{24} \left( F_{k+1}(x_{k+1},y_{k+1}) -H_{k+1}(y_{k+1}) \right) \right]}_{\rm LHS} \\
&\le \delta_k \underbrace{\mathbb{E} \left[H_{k+1}(\tilde y_{k+1}) - H_{k+1}( y_{k}) + \frac{1}{24} \left( F_{k+1}(x_{k},y_{k}) -H_{k+1}(y_{k}) \right)\right]}_{\rm RHS}.
\end{align*}
For the left hand side (LHS), we bound it according to
\begin{align} \label{ieq:sum1}
\mathbb{E}\Vert y_{k+1} - \tilde y_{k+1} \Vert^2 \le \frac{2}{\mu_y} \mathbb{E}[ H_{k+1}(\tilde y_{k+1}) - H_{k+1} (y_{k+1}) ] 
\end{align}
(where we use the $\mu_y$-PL condition in $y$) and
\begin{align} \label{ieq:sum2}
\begin{split}
&\quad \mathbb{E}\Vert x_{k+1} - \tilde x_{k+1} \Vert^2  \\
&\le 2\mathbb{E} [\Vert x_{k+1} - x^*(y_{k+1}) \Vert^2 +  \Vert x^*(y_{k+1}) - \tilde x_{k+1} \Vert^2 ] \\
&= 2\mathbb{E} [\Vert x_{k+1} - x^*(y_{k+1}) \Vert^2 +  \Vert x^*(y_{k+1}) - x^*(\tilde y_{k+1}) \Vert^2 ] \\
&\le 2\mathbb{E} [\Vert x_{k+1} - x^*(y_{k+1}) \Vert^2  + 18 \mathbb{E}[\Vert y_{k+1} - \tilde y_{k+1} \Vert^2 ] \\
&\le \frac{4}{L} \mathbb{E}[  F_{k+1} (x_{k+1},y_{k+1}) - H_{k+1}(y_{k+1}) ] + \frac{36}{\mu_y} \mathbb{E}[ H_{k+1}(\tilde y_{k+1}) - H_{k+1}(y_{k+1})) ],
\end{split}
\end{align}
where we use Lemma \ref{lem: prox-argmax} in the second last inequality and the PL condition of $F_k(x,y)$ and $H_k(y)$ in the last one.  Summing up (\ref{ieq:sum1}) and (\ref{ieq:sum2}), we have
\begin{align*}
&\quad \mathbb{E}[ \Vert y_{k+1} - \tilde y_{k+1} \Vert^2 + \Vert x_{k+1} - \tilde x_{k+1} \Vert^2 ] \\
&\le \frac{4}{L} \mathbb{E}[  F_{k+1} (x_{k+1},y_{k+1}) - H_{k+1}(y_{k+1}) ] + \frac{38}{\mu_y} \mathbb{E}[ H_{k+1}(\tilde y_{k+1}) - H_{k+1}(y_{k+1})) ] \\
&\le \frac{96}{\mu_y} \times {\rm LHS} \\
&\le \frac{96 \delta_k}{\mu_y} \times {\rm RHS}.
\end{align*}

For the right hand side (RHS), we bound it using 
\begin{align*}
{\rm RHS} &= H_{k+1}(\tilde y_{k+1}) - H_{k+1}( y_{k}) + \frac{1}{24} \left( F_{k+1}(x_{k},y_{k}) -H_{k+1}(y_{k}) \right) \\
&\le \frac{1}{2 \mu_y} \Vert \nabla H_{k+1}(y_k) \Vert^2 + \frac{1}{48 L} \Vert \nabla_x F_{k+1}(x_k,y_k) \Vert^2 \\
& = \frac{1}{2 \mu_y} \Vert \nabla H_{k+1}(y_k) - \nabla H_{k+1}(\tilde y_{k+1}) \Vert^2 + \frac{1}{48L} \Vert \nabla_x F_{k+1}(x_k,y_k) - \nabla_x F_{k+1}(x^*(y_k),y_k) \Vert^2 \\
&\le \frac{72 L^2}{\mu_y} \Vert y_k - \tilde y_{k+1} \Vert^2 + \frac{3L}{16} \Vert x_k - x^*(y_k) \Vert^2 \\
&\le \frac{72 L^2}{\mu_y} \Vert y_k - \tilde y_{k+1} \Vert^2 + \frac{3L}{8} \Vert x_k - \tilde x_{k+1} \Vert^2 + \frac{3L}{8} \Vert  x^*(\tilde y_{k+1}) - x^*(y_k) \Vert^2 \\
&\le \frac{72 L^2}{\mu_y} \Vert y_k - \tilde y_{k+1} \Vert^2 + \frac{L}{24} \Vert x_k - \tilde x_{k+1} \Vert^2 + \frac{27 L}{8} \Vert y_k - \tilde y_{k+1}\Vert^2.
\end{align*}
Above, the first inequality is due to the PL condition of $F_k(x,y)$ and $H_k(y)$; the second inequality follows from $H_k(y)$ is $12L$-smooth and $F_k(x,y)$ is $3L$-smooth; the third one directly follows from the Young's inequality; the fourth inequality uses Lemma \ref{lem: prox-argmax} and the fact that $F_k(x,y)$ is $L$-PL in $x$ and $3L$-smooth.

Therefore, we obtain
\begin{align*}
\mathbb{E}[ \Vert y_{k+1} - \tilde y_{k+1} \Vert^2 + \Vert x_{k+1} - \tilde x_{k+1} \Vert^2 ] \le \delta_k'( \Vert y_k - \tilde y_{k+1} \Vert^2 +  \Vert x_k - \tilde x_{k+1} \Vert^2),
\end{align*}
where
\begin{align} \label{rela:delta-prime}
\delta_k' = 7236 \kappa_y^2 \delta_k.
\end{align}
\end{proof}


Now it is sufficient to control $\delta$ recursively.

\begin{lem} \label{lem: two-side-error}
If we solve each sub-problem $F_k$ with precision $\delta_k$ as defined in Theorem \ref{thm: Catalyst-GDA}, then for all $k$ it holds true that
\begin{align*}
    \BE \Vert x_k - \tilde x_k \Vert^2 + \Vert y_k - \tilde y_k  \Vert^2\le \delta. 
\end{align*}

\begin{proof}

We prove by induction. 
Suppose we the following statement holds true for all $1 \le k' \le k$ that we have 
\begin{align*}
\BE \Vert x_{k'} - \tilde x_{k'} \Vert^2 + \Vert y_{k'} - \tilde y_{k'} \Vert^2  & \le \delta, 
\end{align*}
Then, by Lemma \ref{sub:eps-saddle} we have
\begin{align*}
&\quad \BE [\Vert x_{k+1} - \tilde x_{k+1} \Vert^2 + \Vert y_{k+1} - \tilde y_{k+1} \Vert^2] \\
&\le \delta_k' (\Vert x_k - \tilde x_{k+1} \Vert^2 + \Vert y_k - \tilde y_{k+1} \Vert^2) \\
&\le 2 \delta_k'( \Vert x_k - \tilde x_k \Vert^2 + \Vert y_k - \tilde y_k \Vert^2) + 2 \delta_k'(\Vert \tilde x_{k+1} - \tilde x_k \Vert^2 + \Vert \tilde y_{k+1} - \tilde y_k \Vert^2 )\\
&\le 2 \delta_k' \delta + \frac{8 \beta^2 \delta_k'}{(\beta-L)\mu_y} \Vert x_k - x_{k-1} \Vert^2,
\end{align*}
where $\delta_k' $ follows from (\ref{rela:delta-prime}) and we use the induction hypothesis and  Lemma \ref{lem: bound-tilde-xy} in the third inequality. 
Note that our choice of $\delta_k$ and the relationship between $\delta_k$ and $\delta_k'$ satisfy
\begin{align*}
    \max \left\{2 \delta_k' \delta , \frac{8 \beta^2 \delta_k' \Vert x_k - x_{k-1} \Vert^2 }{(\beta-L)\mu_y} \right \} \le \frac{\delta}{2}.
\end{align*}
Therefore, we can see that 
\begin{align*}
    \BE[\Vert x_{k+1} - \tilde x_{k+1} \Vert^2 + \Vert y_{k+1} - \tilde y_{k+1} \Vert^2] &\le \delta,
\end{align*}
which completes the induction from $k$ to $k+1$. For the induction base, using Lemma \ref{lem:induction-base} we have  
\begin{align*}
    &\quad \BE[\Vert x_1 - \tilde x_1 \Vert^2 + \Vert y_1 - \tilde y_1 \Vert^2] \\
    &\le \delta_0' ( \Vert x_0 - \tilde x_1 \Vert^2 + \Vert y_0 - \tilde y_1 \Vert^2 ) \\
    &\le \frac{2 \delta_0'}{ \mu_y} (g(x_0) - g^{\ast}) \\ 
    &\le \delta.
\end{align*}
\end{proof}

\end{lem}



\subsection{Proof of Theorem \ref{thm: Catalyst-GDA}}

Combing Lemma \ref{lem: outer-converge}, Lemma \ref{sub:eps-saddle} and Lemma \ref{lem: two-side-error}, we can easily prove Theorem \ref{thm: Catalyst-GDA}.

\begin{proof}
Note that each sub-problem $F_k$ is $3L$-smooth, $L$-PL in $x$ and $\mu_y$-PL in $y$ for $\beta  =2L$. Now if we choose  
\begin{align*}
    K = \left \lceil ((2 \beta+ \mu_x)/ \mu_x) \log (2/\epsilon) \right \rceil = \fO( \kappa_x \log (1/\epsilon) ),
\end{align*} 
then by Lemma \ref{lem: outer-converge} it is sufficient to guarantee that $\BE[g(x_K) - g(x^*)] \le \epsilon$,  while solving each sub-problem $F_k$ requires no more than $T_k \le a (n+ \sqrt{n } \kappa_y )  \log({\kappa_y}/{\delta_k})$ first-order oracle calls in expectation by Lemma \ref{sub:eps-saddle}, where $a$ is an independent positive constant. 

Now we telescope the inequality in Lemma \ref{lem:g(x)-decrease} and we can obtain
\begin{align} \label{eq: one-side-tele}
    \sum_{k=0}^{K-1} \BE \left[\Vert x_{k+1} - x_k \Vert^2\right] &\le \frac{2}{\beta} (g(x_0) - g^{\ast}) + \frac{(\beta+L)^2 \delta}{\beta (\beta-L)}.
\end{align}
Note that we have
\begin{align*}
    \frac{1}{\delta_k} \le \omega \times \max\left\{ 4, \frac{16 \kappa_y \Vert x_k - x_{k-1} \Vert^2}{\delta } \right\} \le \omega \times \left(4 + \frac{16 \kappa_y \Vert x_k - x_{k-1} \Vert^2}{\delta }\right),
\end{align*}
by the choice of $\delta_k$ for all $k \ge 1$ (\ref{dfn:delta_k}), where  $\omega = 7236 \kappa_y^2$. Denote $C = a(n+ \sqrt{n}\kappa)$, then
{\small
\begin{align}
\begin{split} \label{SFO:xk12}
&\quad \sum_{k=0}^{K} T_k  \\
&= T_0 + \sum_{k=1}^{K} T_k \\
 &\le C  \log \left( \omega \times \frac{2 \kappa_y (g(x_0) - g^{\ast})}{ \delta \mu_y}\right ) + C \sum_{k=1}^{K} \log \left( \frac{\kappa_y}{\delta_k} \right) \\
&\le C  \log \left(  \omega \times \frac{2 \kappa_y (g(x_0) - g^{\ast})}{ \delta \mu_y}\right ) + C \sum_{k=1}^{K} \log \left( \omega \times\left(  4 \kappa_y + \frac{16 \kappa_y^2 \Vert x_k - x_{k-1} \Vert^2}{\delta} \right)\right)\\
&\le  C  \log \left( \omega \times \frac{2\kappa_y(g(x_0) - g^{\ast})}{ \delta \mu_y}\right ) +    C K  \log \left( \omega \times\sum_{k=1}^K \left(  4 \kappa_y + \frac{16 \kappa_y^2 \Vert x_k - x_{k-1} \Vert^2}{\delta} \right)\right)   \\
&= C  \log \left(  \omega \times \frac{2\kappa_y(g(x_0) - g^{\ast})}{ \delta \mu_y}\right ) +    C K  \log \left(\omega \times \sum_{k=0}^{K-1} \left(  4 \kappa_y + \frac{16 \kappa_y^2 \Vert x_k - x_{k-1} \Vert^2}{\delta} \right)\right).
\end{split}
\end{align}}
 Above, the second inequality relies on the choice of $\delta_k$  and the third inequality is  due to the fact that $(\prod _{i=1}^n x_i)^{{1}/{n}} \le \frac{1}{n} \sum_{i=1}^n x_i$, which implies that  $ \sum_{i=1}^n \log x_i \le n \log \left(  \frac{1}{n} \sum_{i=1}^n x_i \right) $.
 
Lastly, we use (\ref{eq: one-side-tele}) and notice that $\delta$ (\ref{dfn:delta-two}) is dependent on $\epsilon, \kappa_x$  and  $\omega$ is dependent on $\kappa_y$ to show the SFO complexity of the order
\begin{align*}
    \fO ( (n \kappa_x + \sqrt{n} \kappa_x \kappa_y) \log (1/\epsilon) \log(\kappa_x \kappa_y / \epsilon)).
\end{align*}

\end{proof}


\section{Proof of Section \ref{sec: extension}} \label{apx: one-side}

First of all, we show the convergence of SVRG-GDA and SPIDER-GDA under one-sided PL condition as studied in Section \ref{sec: extension}. We reuse the lemmas under two-sided PL condition. It is worth noticing that we can discard the outermost loop with respect to restart strategy for both SVRG-GDA and SPIDER-GDA, i.e we set $T=1$ in this setting.

\subsection{SVRG-GDA under one-sided PL condition}

For SVRG-GDA, we have the following theorem. 
\begin{thm} \label{thm: SVRG-GDA-one}
Under Assumption \ref{asm: one-PL} and \ref{asm: L-smooth}, let $T = 1$ and $M,\tau_x,\tau_y,\lambda$  defined in Lemma \ref{lem: SVRG-GDA-V_k}; $\alpha = {2}/{3}$, $ SM = \lceil {8 }/{(\tau_x \epsilon^2)} \rceil$. Algorithm \ref{alg: Catalyst-GDA} can guarantee the output $\hat x$ to satisfy  $\Vert \nabla g(\hat x) \Vert^2 \le \epsilon$ in expectation with no more than $\fO(n+ { n^{2/3} \kappa_y^2 L}{\epsilon^{-2}})$ stochastic first-order oracle calls.

\begin{proof}

Telescoping for $k = 0,\dots, M-1$ and $s= 0,\dots, S-1$  for the inequality in Lemma \ref{lem: SVRG-GDA-V_k}:
\begin{align*}
    \frac{1}{SM} \sum_{s=0}^{S-1} \sum_{k=0}^{M-1} \BE [\Vert \nabla g(x_{s,k}) \Vert^2] \le \frac{8 \fV_{0,0}}{\tau_x SM}.
\end{align*}
Note that we have $M = \fO(n^{3\alpha/2})$ and if we let $ SM = \lceil {8 }/{(\tau_x \epsilon^2)} \rceil$, then $S = \fO( {L \kappa_y^2 }/({n^{\alpha/2}\epsilon^2)})$, so the complexity is
\begin{align*}
    \fO( n+ SM + Sn) = \fO\left( n+ \frac{\kappa_y^2 L ( n^{\alpha} + n^{1- \alpha/2})}{\epsilon^2} \right).
\end{align*}
Plugging in $\alpha = 2/3$ yields the desired complexity.
\end{proof}

\end{thm}

\subsection{Proof of Theorem \ref{thm: SPIDER-GDA-one}}

Similarly to SVRG-GDA, we can also analyze the convergence of SPIDER-GDA.
\begin{proof}
By Lemma \ref{lem: key-lem-for-Spider}, the output satisfies $ \BE[ \Vert \nabla g(\hat x) \Vert] \le \epsilon$ under our choice of parameters.
%Denote $(\hat x,\hat y)$ the output, then
% \begin{align*}
%     \BE  \Vert \nabla g(\hat x) \Vert^2 &\le 2\BE\big[\Vert  \nabla_x f(\hat x,\hat y) \Vert^2 + \Vert \nabla_x f(\hat x,\hat y) - \nabla g(\hat x) \Vert^2\big] \\
%     &\le 2\BE\left[\Vert  \nabla_x f(\hat x,\hat y) \Vert^2 + \frac{L^2}{\mu_y^2} \Vert \nabla_y f(\hat x,\hat y)  \Vert^2\right] \\
%     &\le 2 \BE[ \Vert \nabla_x f(\hat x,\hat y) \Vert^2 + \lambda \Vert \nabla_y f(\hat x,\hat y) \Vert^2] \\
%     &\le \frac{32}{\tau_x K} \BE\left[ \tilde \fA_0 + \tilde \fB_ 0\right],
% \end{align*}
% where the first inequality follows from Young's inequality; the second one relies on Lemma \ref{lem: new-bound-Bk}; the third one uses the definition of $\lambda$ and the last one uses Lemma \ref{lem: key-lem-for-Spider}.

Since $\tau_x = \fO(1/ (\kappa_y^2L) )$ and $M = B = \sqrt{n}$, the complexity becomes:
\begin{align*}
\fO\left( n + \frac{\sqrt{n}}{\tau_x \epsilon^2}\right)    = \fO\left( n + \frac{\sqrt{n} \kappa_y^2L}{\epsilon^2}\right).
\end{align*}
\end{proof}

Above, we have show that the complexity of SVRG-GDA is $ \fO( n+ n^{2/3} \kappa_y^2 L \epsilon^{-2}) $ and the complexity of SPIDER-GDA is $\fO(n + \sqrt{n} \kappa_y^2 L \epsilon^{-2})$ \footnote{ To be more precise, our theorem only suits the case when $ 1/ (\kappa_y^2 L \epsilon^2 ) > \sqrt{n}$ for SPIDER-GDA. If not, we can directly set $K = 2M$ to achieve the same convergence result.}. Thus, we can come to the conclusion that SPIDER-GDA strictly outperforms SVRG-GDA under both  two-sided and one-sided PL conditions. In the rest of this section, we mainly focus on the complexity of AccSPIDER-GDA under one-sided PL condition. 

In the following lemma, we show that AccSPIDER-GDA converge when we can control the precision of solving each sub-problem with a global constant $\delta$.

\subsection{Proof of Lemma \ref{lem: outer-convergence-one-side}}
\begin{proof}
Similar to the proof under two-sided PL condition, we begin our proof with Lemma \ref{lem: g(x)-convergence}. We can see that
\begin{align*}
    \BE[g(x_{k+1})] 
    &\le \BE \left[g(x_k)  - \frac{1}{4 \beta} \Vert \nabla g(\tilde x_{k+1}) \Vert^2 + \left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta \right]\\ 
    &\le \BE \left[g(x_k)  - \frac{1}{8 \beta} \Vert \nabla g(x_{k+1}) \Vert^2\right] \\
    &\quad +  \BE \left[\frac{1}{4 \beta} \Vert \nabla g(\tilde x_{k+1}) - \nabla g(x_{k+1}) \Vert^2 +\left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right) \delta\right] \\
    &\le \BE \left[ g(x_k)  - \frac{1}{8 \beta} \Vert \nabla g(x_{k+1}) \Vert^2 \right]\\
    &\quad + \BE \left[\frac{L^2}{2 \mu_y \beta} \Vert \tilde x_{k+1} - x_{k+1} \Vert^2 + \left(\frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta\right] \\
    &\le \BE \left[g(x_k)  - \frac{1}{8 \beta} \Vert \nabla g(x_{k+1}) \Vert^2 + \left (\frac{L^2}{2 \mu_y \beta}  + \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2}\right) \delta\right],
\end{align*}
where we use the fact that $-\Vert a - b \Vert^2 \le \frac{1}{2} \Vert a \Vert^2 + \Vert b \Vert^2$ in the second inequality, $g(x)$ is $({2L^2}/{\mu_y})$-smooth in the third one and $\Vert \tilde x_{k+1} - x_{k+1} \Vert^2 \le \delta $ in the last one.
Telescoping for $k = 0,1,2,...K-1$, we can see that 
\begin{align*}
    \frac{1}{8 \beta} \sum_{k=0}^{K-1} \BE \left[\Vert \nabla g(x_k) \Vert^2\right] &\le \BE \left[g(x_0) - g(x_K) + \left(\frac{L^2}{2 \mu_y \beta}  + \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) K \delta\right] \\
    &\le  \BE \left[g(x_0) - g^{\ast} + \left(\frac{L^2}{2 \mu_y \beta}  + \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) K \delta\right].
\end{align*}
Divide  both sides by $K$, then
\begin{align*}
    \frac{1}{K} \sum_{k=0}^{K-1} \BE \left[ \Vert \nabla g(x_k) \Vert^2\right] 
    &\le  \BE \left[\frac{8 \beta(g(x_0) - g^{\ast})}{K} + 8 \beta\left (\frac{L^2}{2 \mu_y \beta}  +  \frac{(\beta+L)^2 }{2 (\beta-L)} + \frac{\beta}{2} \right) \delta\right].
\end{align*}

Plugging the choice of $\delta$ yields the desired inequality.
\end{proof}


\subsection{Proof of Theorem \ref{thm: Catalyst-GDA-one-side}}

Combing Lemma \ref{lem: outer-convergence-one-side}, Lemma \ref{sub:eps-saddle} and Lemma \ref{lem: two-side-error}, we can easily prove Theorem \ref{thm: Catalyst-GDA-one-side}. We remark that both the proof Lemma \ref{sub:eps-saddle} and Lemma \ref{lem: two-side-error} only uses the PL property in the direction of $y$, so they can both be directly applied to the one-sided PL case.

\begin{proof}
Note that each sub-problem $F_k$ is $3L$-smooth, $L$-PL in $x$ and $\mu_y$-PL in $y$ for $\beta  =2L$. Now if we choose  
\begin{align*}
    K = \left \lceil 16 \beta (g(x_0) - g^*)/ \epsilon^2 \right \rceil = \fO(L \epsilon^{-2} ),
\end{align*} 
then by Lemma \ref{lem: outer-convergence-one-side} it is sufficient to guarantee that $\BE[ \Vert g(\hat x) \Vert] \le \epsilon$,  while solving each sub-problem $F_k$ requires no more than $T_k \le a (n+ \sqrt{n } \kappa_y )  \log({\kappa_y}/{\delta_k})$ first-order oracle calls in expectation by Lemma \ref{sub:eps-saddle}, where $a$ is an independent positive constant. 

Therefore,  using (\ref{eq: one-side-tele}), (\ref{SFO:xk12}) and noticing that $\delta$ (\ref{dfn:delta-one}) is dependent on $\epsilon, \kappa_y$  and  $\omega$ in (\ref{SFO:xk12}) is dependent on $\kappa_y$ to show the SFO complexity of the order
\begin{align*}
    \fO ( (n  + \sqrt{n}  \kappa_y) L \epsilon^{-2} \log(\kappa_y / \epsilon)).
\end{align*}

\end{proof}


\iffalse
\begin{lem}
Let $f(x,y)$ be $L$-smooth and $\mu$-PL in $y$.
Suppose for any $x$, there exist $y^*(x)\in\argmax_y f(x,y)$. 
Then $F_k(x,y)=f(x,y)+L\norm{x-u_k}^2$ has a saddle point.
\begin{proof}
Let $g(x)=\max_y f(x,y)$ and $G_k(x)=\max_y F_k(x,y)$, then
\begin{align*}
y^*(x)\in\argmax_y f(x,y) 
= \argmax_y f(x,y) + L\norm{x-u_k}^2
= \argmax_y F_k(x,y).
\end{align*}
The smoothness of $f$ means $F_k(x,y)$ is $L$-strongly-convex in $x$. Then for any $t\in[0,1]$ and $y$, we have
\begin{align*}
&  G_k(tx_1+(1-t)x_2) \\
= & F_k(tx_1+(1-t)x_2,y^*(tx_1+(1-t)x_2)) \\
\leq & tF_k(x_1,y^*(tx_1+(1-t)x_2)) + (1-t)F_k(x_2,y^*(tx_1+(1-t)x_2)) - \frac{1}{2}Lt(1-t)\norm{x_1-x_2}^2 \\
\leq & tF_k(x_1,y^*(x_1)) + (1-t)F_k(x_2,y^*(x_2)) - \frac{1}{2}Lt(1-t)\norm{x_1-x_2}^2 \\
\leq & tG_k(x_1) + (1-t)G_k(x_2) - \frac{1}{2}Lt(1-t)\norm{x_1-x_2}^2,
\end{align*}
which means $G_k(x)$ is $L$-strongly-convex. Let 
\begin{align*}
x_k^*=\argmin_x G_k(x)=\argmin_x F_k(x,y^*(x))    
\end{align*}
and 
\begin{align*}
y_k^*\in\argmax_y F_k(x_k^*,y).     
\end{align*}
Then $(x_k^*,y_k^*)$ is a global minimax point of $G_k(x,y)$ since we have
\begin{align*}
F_k(x_k^*,y) \leq F_k(x_k^*,y_k^*) = G_k(x_k^*) \leq G_k(x) = \max_{y'} F_k(x,y').
\end{align*}
Hence, it is also a saddle point due to $G_k(x,y)$ is two-sided PL.
\end{proof}
\end{lem}
\fi

