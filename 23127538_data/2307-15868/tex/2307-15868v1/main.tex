\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2022

% ready for submission
 \PassOptionsToPackage{numbers, compress, sort}{natbib}
\usepackage[final]{neurips_2022}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{mathtools}
\input{math.tex}

\title{Faster Stochastic Algorithms for Minimax Optimization under Polyak--{\L}ojasiewicz Conditions}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
	Lesi Chen \\
	School of Data Science \\ 
	Fudan University \\
	\texttt{lschen19@fudan.edu.cn}
	\And
 	Boyuan Yao\\
 	School of Data Science \\ 
	Fudan University \\
 	\texttt{byyao19@fudan.edu.cn}
 	\And
	Luo Luo\thanks{The corresponding author} \\
	School of Data Science \\ 
	Fudan University \\ 
	\texttt{luoluo@fudan.edu.cn}
}

\begin{document}

\maketitle

\begin{abstract}

This paper considers stochastic first-order algorithms for minimax optimization under Polyak--{\L}ojasiewicz (PL) conditions. 
We propose SPIDER-GDA for solving the finite-sum problem of the form $\min_x \max_y f(x,y)\triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y)$, where the objective function $f(x,y)$ is $\mu_x$-PL in $x$ and $\mu_y$-PL in $y$; and each $f_i(x,y)$ is $L$-smooth. We prove SPIDER-GDA could find an $\epsilon$-optimal solution within ${\mathcal O}\left((n + \sqrt{n}\,\kappa_x\kappa_y^2)\log (1/\epsilon)\right)$ stochastic first-order oracle (SFO) complexity, which is better than the state-of-the-art method whose SFO upper bound is ${\mathcal O}\big((n + n^{2/3}\kappa_x\kappa_y^2)\log (1/\epsilon)\big)$, where $\kappa_x\triangleq L/\mu_x$ and $\kappa_y\triangleq L/\mu_y$.
For the ill-conditioned case, we provide an accelerated algorithm to reduce the computational cost further. It achieves $\tilde{{\mathcal O}}\big((n+\sqrt{n}\,\kappa_x\kappa_y)\log^2 (1/\epsilon)\big)$ SFO upper bound when $\kappa_y \gtrsim \sqrt{n}$. Our ideas also can be applied to the more general setting that the objective function only satisfies PL condition for one variable. Numerical experiments validate the superiority of proposed methods.


\end{abstract}




\section{Introduction}
 
This paper focuses on the smooth minimax optimization problem of the form
\begin{align}\label{prob:main}
    \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} f(x,y) \triangleq \frac{1}{n} \sum_{i=1}^n f_i(x,y),
\end{align}
which covers a lot of important applications in machine learning such as reinforcement learning~\cite{du2017stochastic,wai2018multi}, AUC maximization~\cite{guo2020communication,liu2019stochastic,ying2016stochastic}, 
imitation learning~\cite{cai2019global,nouiehed2019solving}, robust optimization ~\cite{duchi2019variance}, causal inference~\cite{meinshausen2018causality}, game theory~\cite{carmon2019variance,nash1953two} and so on.

We are interested in the minimax problems under PL conditions~\cite{nouiehed2019solving,yang2020global,doan2022convergence,yue2023lower}.
The PL condition~\cite{polyak1963gradient} was originally proposed to relax the strong convexity in minimization problem that is sufficient for achieving the global linear convergence rate for first-order methods. 
This condition has been successfully used to analyze the convergence behavior for overparameterized neural networks~\cite{liu2022loss}, robust phase retrieval~\cite{sun2018geometric}, and many other machine learning models~\cite{karimi2016linear}. 
There are many popular minimax formulations that only satisfy PL conditions but lack strong convexity (or strong concavity).
The examples include PL-game~\cite{nouiehed2019solving}, robust least square~\cite{yang2020global}, deep AUC maximization~\cite{liu2019stochastic} and generative adversarial imitation learning of LQR~\cite{cai2019global,nouiehed2019solving}. 

\citet{yang2020global} showed that the alternating gradient descent ascent
(AGDA) algorithm linearly converges to the saddle point when the objective function satisfies the two-sided PL condition. 
They also proposed the SVRG-AGDA method for the finite-sum problem (\ref{prob:main}), which could find $\epsilon$-optimal solution within $\fO\big((n+ n^{2/3} \kappa_x\kappa_y^2)\log(1/\epsilon)\big)$ stochastic first-order oracle (SFO) calls,\footnote{The original analysis ~\cite{yang2020global} provided an SFO upper bound $\fO\big((n+ n^{2/3} \max \{\kappa_x^3, \kappa_y^3 \})\log(1/\epsilon)\big)$, which can be refined to $\fO\big((n+ n^{2/3} \kappa_x\kappa_y^2)\log(1/\epsilon)\big)$ by some little modification in the proof.} where $\kappa_x$ and $\kappa_y$ are the condition numbers with respect to PL condition for $x$ and $y$ respectively. The variance reduced technique in the SVRG-AGDA leads to better a convergence rate than full batch AGDA whose SFO complexity is $\fO\big(n\kappa_x\kappa_y^2\log(1/\epsilon)\big)$.
However, there are still some open questions left. 
Firstly, \citet{yang2020global}'s theoretical analysis heavily relies on the alternating update rules. It remains interesting whether a simultaneous version of GDA (or its stochastic variants) also has similar convergence results. 
Secondly, it is unclear whether the SFO upper bound obtained by SVRG-AGDA can be improved by designing more efficient algorithms. 


Under the one-sided PL condition, we desire to find the stationary point of $g(x)\triangleq \max_{y\in\BR^{d_y}}f(x,y)$, since the saddle point may not exist. \citet{nouiehed2019solving} proposed the Multi-Step GDA that achieves the $\epsilon$-stationary point within $\fO(\kappa_y^2 L \epsilon^{-2} \log(\kappa_y/\epsilon))$  %\footnote{Notation $\tilde \fO(\cdot)$ hides the logarithmic terms with respect to $\kappa_x,\kappa_y$ and $\epsilon$.}
numbers of full gradient iterations. A similar complexity is also proved by AGDA~\cite{yang2020global}.
Recently, \citet{yang2022faster} proposed the Smoothed-AGDA that improves the upper bound into $\fO(\kappa_y L \epsilon^{-2})$.
Both the Multi-Step GDA~\cite{nouiehed2019solving} and Smoothed-AGDA~\cite{yang2020catalyst} can be extended to the online setting, where we only assume the existence of an unbiased stochastic gradient with bounded variance.  But to the best of our knowledge, the formulation (\ref{prob:main}) with finite-sum structure has not been explored by prior works.

%Additionally, \citet{doan2021convergence} studied the continuous-time GDA dynamics and they found that continuous time GDA converges faster than discrete time GDA or AGDA 

%Additionally, \citet{nouiehed2019solving} proposed multi-step GDA algorithms for the problem with one-sided PL condition and \citet{doan2021convergence} studied the continuous-time GDA dynamics.

%\citet{nouiehed2019solving} have analyzed a GDmax type algorithm when one of direction satisfies PL condition. Later, in order to solve the problem when $f(x,y)$ is $L$-smooth, $\mu_x$-PL in $x$ and $\mu_y$-PL in $y$, \citet{yang2020global} have shown that full batch GDA with alternating updates (AGDA) converges linearly while we can only guarantee a sublinear convergence when  stochastic gradient descent (Stoc-AGDA) is used. Fortunately, integrated with SVRG gradient estimators, \citet{yang2020global} proposed  SVRG-AGDA \footnote{They called this method VR-AGDA (variance-reduced-AGDA )in their original paper, however, we call it SVRG-AGDA to distinguish different variance reduced techniques.} which can find an $\epsilon$-approximate solution within $\fO((n+ n^{2/3} \max \{\kappa_x, \kappa_y \}^3)\log(1/\epsilon))$ stochastic first-order oracle calls, where $\kappa_x \triangleq L /\mu_x, \kappa_y \triangleq L /\mu_y$. The reason why they have incorporated AGDA instead of GDA results from their belief that AGDA is more stable than GDA in some cases \cite{gidel2019negative,bailey2020finite}. We can see that the complexity of SVRG-AGDA has a high order dependency on both sample size and condition numbers. The authors have left us two questions. Firstly, their convergence analysis relies heavily on the alternating updates. It remains interesting whether a simultaneous version of GDA also converges under such setting. Another question that arises naturally is that whether acceleration is possible like convex-concave settings \cite{yoon2021accelerated,bot2022fast,tran2022connection}. It is also worth mentioning that \citet{doan2021convergence} have derived that  two-time-scale GDA dynamics converge with a rate $\fO(n \kappa_x \kappa_y \log (1/\epsilon))$. However their analysis does not hold for discrete time GDA or AGDA. For AGDA the best convergence rate we know is $\fO(n \kappa_x \kappa_y^2 \log(1/\epsilon))$  \cite{yang2020global} and for GDA the rate is not known yet. 


In this paper, we introduce a variance reduced first-order method, called SPIDER-GDA, which constructs the gradient estimator by stochastic recursive gradient~\cite{fang2018spider}, and the iterations are based on simultaneous gradient descent ascent~\cite{lin2020gradient}.
We prove that SPIDER-GDA could achieve $\epsilon$-optimal solution of the two-sided PL problem of the form (\ref{prob:main}) within $\fO\big((n+ \sqrt{n}\,\kappa_x\kappa_y^2)\log(1/\epsilon)\big)$ SFO calls, 
which has better dependency on $n$ than SVRG-AGDA~\cite{yang2020global}. We also provide an acceleration framework to improve first-order methods for solving ill-conditioned minimax problems under PL conditions. 
The accelerated SPIDER-GDA (AccSPIDER-GDA) could achieve $\epsilon$-optimal solution within $\tilde\fO\big((n+ \sqrt{n}\,\kappa_x\kappa_y)\log^2(1/\epsilon)\big)$ SFO calls\footnote{In this paper, we ues the notation $\tilde \fO(\cdot)$ to hide the logarithmic factors of $\kappa_x,\kappa_y$ but not $1/\epsilon$. } when $\kappa_y \gtrsim \sqrt{n}$, which is the best known SFO upper bound for this problem.
We summarize our main results and compare them with related work in Table~\ref{tbl: two-side-PL}. Without loss of generality, we always suppose $\kappa_x \gtrsim \kappa_y$.
Furthermore, the proposed algorithms also work for minimax problem with one-sided PL condition. We present the results for this case in Table~\ref{tbl: one-side-PL}.

\begin{table}[t] 
\centering
\caption{We present the comparison of SFO complexities under two-sided PL condition. Note that \citet{yang2020global} named their stochastic algorithm as variance-reduced-AGDA (VR-AGDA). Here we call it SVRG-AGDA to distinguish with other variance reduced algorithms.}
\label{tbl: two-side-PL}
\begin{tabular}{ccc} 
\hline
Algorithm & Complexity & Reference \\
\hline\addlinespace
GDA/AGDA & $\fO\left(n\kappa_x \kappa_y^2 \log\left( 1/\epsilon\right)\right)$ & Theorem \ref{thm: GDA}, \cite{yang2020global} \\\addlinespace
SVRG-AGDA & $\fO\left( (n+ n^{2/3} \kappa_x \kappa_y^2) \log \left( 1/\epsilon\right)\right)$ &  \cite{yang2020global} \\\addlinespace
SVRG-GDA & $\fO\left( (n+ n^{2/3} \kappa_x \kappa_y^2) \log \left( 1/\epsilon\right)\right)$ & Theorem \ref{thm: SVRG-GDA} \\\addlinespace
SPIDER-GDA & $\fO\left( (n+ \sqrt{n} \kappa_x \kappa_y^2) \log \left( 1/\epsilon\right)\right)$ & Theorem \ref{thm: SPIDER-GDA}  \\\addlinespace
AccSPIDER-GDA & 
$
{\begin{cases}
\tilde \fO\left(\sqrt{n} \kappa_x \kappa_y \log^2 \left(1/\epsilon\right)\right), &  \sqrt{n} \lesssim \kappa_y; \\[0.1cm]
\tilde \fO\left(n \kappa_x \log^2 \left(1/\epsilon\right) \right), & \kappa_y \lesssim \sqrt{n} \lesssim \kappa_x \kappa_y; \\[0.1cm]
\fO\left((n+  \sqrt{n} \kappa_x \kappa_y^2) \log \left(1/\epsilon\right)\right), & \kappa_x \kappa_y \lesssim \sqrt{n}. 
\end{cases}}
$
& Theorem \ref{thm: Catalyst-GDA} \\\addlinespace
\hline
\end{tabular}
\end{table}


\begin{table}[t]
\centering
\caption{We present the comparison of SFO complexities under one-sided PL condition.} \label{tbl: one-side-PL}
\begin{tabular}{ccc} 
\hline
Algorithm & Complexity & Reference \\
\hline\addlinespace
Multi-Step GDA & $\fO(n \kappa_y^2 L \epsilon^{-2} \log(\kappa_y/\epsilon))$ & \cite{nouiehed2019solving} \\ \addlinespace
GDA/AGDA & $\fO\left(n\kappa_y^2L\epsilon^{-2}\right) $ & Theorem \ref{thm: GDA-one-side}, \cite{yang2020global} \\\addlinespace
Smooothed-AGDA & $\fO\left(n \kappa_y L\epsilon^{-2}\right)$ & \cite{yang2022faster} \\ \addlinespace 
SVRG-GDA & $\fO\left(n+  n^{2/3}\kappa_y^2L\epsilon^{-2} \right) $ & Theorem \ref{thm: SVRG-GDA-one}  \\\addlinespace
SPIDER-GDA & $ \fO\left(n+ \sqrt{n} \kappa_y^2L \epsilon^{-2} \right)$ & Theorem \ref{thm: SPIDER-GDA-one}  \\\addlinespace
AccSPIDER-GDA & 
$
\begin{cases}
\fO\left(\sqrt{n} \kappa_y L \epsilon^{-2} \log(\kappa_y/\epsilon)\right), & \sqrt{n} \lesssim \kappa_y; \\[0.15cm]
\fO\left(n L\epsilon^{-2} \log(\kappa_y/\epsilon)\right), & \kappa_y \lesssim \sqrt{n} \lesssim \kappa_y^2; \\[0.15cm]
\fO\left(n+\sqrt{n}\kappa_y^2L\epsilon^{-2}\right), & \kappa_y^2  \lesssim \sqrt{n}. \end{cases}
$
& Theorem \ref{thm: Catalyst-GDA-one-side} \\\addlinespace
\hline
\end{tabular}
\end{table}

\section{Related Work}

The minimax optimization problem (\ref{prob:main}) can be viewed as the following minimization problem
\begin{align*}
    \min_{x\in\BR^{d_x}} \bigg\{g(x)\triangleq \max_{y\in\BR^{d_y}}f(x,y)\bigg\}.
\end{align*}
A natural way to solve this problem is the Multi-step GDA~\cite{lin2020gradient,nouiehed2019solving,rafique2018non,luo2020stochastic} that contains double-loop iterations in which the outer loop can be regarded as running inexact gradient descent on $g(x)$ and the inner loop finds the approximate solution to $\max_{y\in\BR^{d_y}} f(x,y)$ for a given $x$. Another class of methods is the two-timescale (alternating) GDA algorithm~\cite{lin2020gradient,yang2020global,xian2021faster,doan2022convergence} that only has single-loop iterations which updates two variables with different stepsizes. The two-timescale GDA can be implemented more easily and typically performs better than Multi-Step GDA empirically~\cite{lin2020gradient}.
Its convergence rate also can be established by analyzing function $g(x)$ but the analysis is more challenging than the Multi-Step GDA.  

The variance reduction is a popular technique to improve the efficiency of stochastic optimization algorithms~\cite{schmidt2017minimizing,defazio2014saga,allen2016improved,johnson2013accelerating,shalev2013stochastic,mairal2015incremental,allen2018katyusha,fang2018spider,JMLR:v21:19-248,wang2019spiderboost,reddi2016stochastic,allen2016variance,palaniappan2016stochastic,chavdarova2019reducing,zhou2018stochastic,zhang2013linear,huang2022accelerated,nguyen2017sarah,li2021page}. 
It is shown that solving nonconvex minimization problems with stochastic recursive gradient estimator~\cite{fang2018spider,JMLR:v21:19-248,wang2019spiderboost,zhou2019faster,huang2022accelerated} has the optimal SFO complexity.  
In the context of minimax optimization, the variance reduced algorithms also obtain the best-known SFO complexities in several settings~\cite{luo2021near,han2021lower,alacaoglu2021stochastic,luo2020stochastic,yang2020global,vladislav2021accelerated}. Specifically, the (near) optimal SFO algorithm for several convex-concave minimax problem has been proposed~\cite{luo2021near,han2021lower}, but the optimality for the more general case is still unclear~\cite{luo2020stochastic,yang2020global}. 

The Catalyst acceleration~\cite{lin2015universal} is a useful approach to reduce the computational cost of ill-conditioned optimization problems, which is based on a sequence of inexact proximal point iterations. 
\citet{lin2020near} first introduced Catalyst into minimax optimization. Later, \citet{yang2020catalyst,luo2021near,vladislav2021accelerated} designed the accelerated stochastic algorithms for convex-concave and nonconvex-concave problems. Concurrently with our work, \citet{yang2022faster} also applied this technique to the one-sided PL setting.

\section{Notation and Preliminaries} \label{sec: set-up}

First of all, we present the definition of saddle point.
\begin{dfn}
We say $(x^{\ast},y^{\ast})\in\BR^{d_x}\times\BR^{d_y}$ is a saddle point of function $f:\BR^{d_x}\times\BR^{d_y}\to\BR$ if it holds that
$f(x^*,y) \leq f(x^*,y^*) \leq f(x,y^*)$
for any $x\in\BR^{d_x}$ and $y\in\BR^{d_y}$.
\end{dfn}

Then we formally define the Polyak--{\L}ojasiewicz (PL) condition~\cite{polyak1963gradient} as follows. 

\begin{dfn}\label{asm: PL}
We say a differentiable function $h:\BR^d\to\BR$ satisfies $\mu$-PL for some $\mu>0$ if
$\Vert \nabla h(z) \Vert^2 \ge 2 \mu \big(h(z) - \min_{z'\in\BR^d} h(z')\big)$
holds for any $z\in\BR^d$.
\end{dfn}


Note that the PL condition does not require strong convexity and it can be satisfied even if the function is nonconvex. For instance, the function $h(z) = z^2 + 3 \sin^2(z)$ ~\cite{karimi2016linear}.

%We define two-sided PL condition as follows.

We are interested in the finite-sum minimax optimization problem (\ref{prob:main}) under following assumptions.

\begin{asm} \label{asm: L-smooth}
We suppose each component $f_i:\BR^{d_x}\times\BR^{d_y}\to\BR$ is $L$-smooth, i.e., there exists a constant $L>0$ such that
$\Vert \nabla f_i(x,y) - \nabla f_i(x',y') \Vert^2 \le L^2\big(\Vert x - x' \Vert^2 + \Vert y - y' \Vert^2\big)$ holds for any $x,x' \in \BR^{d_x}$ and $y,y' \in \BR^{d_y}$.
\end{asm}

\begin{asm} \label{asm: two-PL}
We suppose the differentiable function $f:\BR^{d_x}\times\BR^{d_y} \rightarrow \BR$ satisfies two-sided PL condition, i.e., there exist constants $\mu_x>0$ and $\mu_y>0$ such that $f(\cdot, y)$ is $\mu_x$-PL for any $y\in\BR^{d_y}$ and $-f(x,\cdot)$ is $\mu_y$-PL for any $x\in\BR^{d_x}$.
\end{asm}

%{\color{red} For the ease of presentation, suppose each $f_i$ is $L$-smooth.}

%Note that Assumption \ref{asm: L-smooth} implies $f(x,y)\triangleq\frac{1}{n}\sum_{i=1}^n f(x,y)$ is also $L$-smooth.
Under Assumption \ref{asm: L-smooth} and \ref{asm: two-PL}, we define the condition numbers of problem (\ref{prob:main}) with respect to PL conditions for $x$ and $y$ as $\kappa_x \triangleq {L}/{\mu_x}$ and $\kappa_y \triangleq {L}/{\mu_y}$ 
respectively. 

We also introduce the following assumption for the existence of saddle points.

\begin{asm}[\citet{yang2020global}] \label{asm: exist}
We suppose the function $f:\BR^{d_x}\times\BR^{d_y} \rightarrow \BR$ has at least one saddle point $(x^*,y^*)$. We also suppose that for any fixed $y\in\BR^{d_y}$, the problem $\min_{x \in \BR^{d_x}} f(x, y)$ has a nonempty solution set and a finite optimal value; and for any fixed $x\in\BR^{d_x}$, the problem $\max_{y \in \BR^{d_y}} f(x, y)$ has a nonempty solution set and a finite optimal value.
\end{asm}

The goal of solving minimax optimization under the two-sided PL condition is finding an $\epsilon$-optimal solution or $\epsilon$-saddle point that is defined as follows.

\begin{dfn} \label{dfn: epsilon-appox}
We say $x$ is an $\epsilon$-optimal solution of problem (\ref{prob:main}) if it holds that $g(x)-g(x^{\ast})\le \epsilon$, where $g(x)=\max_{y\in\BR^{d_y}}f(x,y)$.
\end{dfn}


We do not assume the existence of saddle points for the problems under the one-sided PL condition. 
In such case, it is guaranteed that $g(x)\triangleq \max_{y\in\BR^{d_y}}f(x,y)$ is differentiable \cite[Lemma A.5]{nouiehed2019solving} and we target to find an $\epsilon$-stationary point of $g(x)$.

%\begin{asm}
%We suppose the function $g:\BR^{d_x}\to\BR$ is lower bounded, i.e., it holds that $g^*=\inf_{x\in\BR^{d_x}} g(x)>-\infty$.
%\end{asm}

\begin{dfn} \label{dfn: epsilon-stationary}
If the function $g:\BR^{d_x}\to\BR$ is differentiable, we say $x$ is an $\epsilon$-stationary point of $g$ if it holds that $\Vert\nabla g(x)\Vert\leq\epsilon$.
\end{dfn}

%\begin{lem}[{Lemma A.5 of \citet{nouiehed2019solving}}]
%Under Assumption~\ref{asm: L-smooth}, we suppose $-f(x,\cdot)$ is $\mu_y$-PL for any $x\in\BR^d$. Then the function $g(x)\triangleq \max_{y\in\BR^{d_y}}f(x,y)$ is $(L+L^2/\mu_y^2)$-smooth and $\nabla g(x)=\nabla_x f(x,y^*(x))$ for any $x\in\BR^{d_x}$ and $y^*(x)\in\argmax_{y\in\BR^{d_y}}f(x,y)$.
%\end{lem}


\section{A Faster Algorithm for the Two-Sided PL Condition} \label{sec: SPIDER-for-PL}

We first consider the two-sided PL conditioned minimax problem of the finite-sum form (\ref{prob:main}) under Assumption~\ref{asm: L-smooth}, \ref{asm: two-PL} and \ref{asm: exist}.
We propose a novel stochastic algorithm, which we refer to as SPIDER-GDA. The detailed procedure of our method is presented in Algorithm~\ref{alg: SPIDER-GDA}. SPIDER-GDA constructs the stochastic recursive gradient estimators~\cite{fang2018spider,nguyen2017sarah} as follows:
\begin{align*}
G_x(x_{t,k},y_{t,k}) =& \frac{1}{B } \sum_{i \in S_x} \big(\nabla_x f_i(x_{t,k},y_{t,k}) - \nabla_x f_i(x_{t,k-1}, y_{t,k-1})  +G_x(x_{t,k-1},y_{t,k-1})\big), \\
G_y(x_{t,k},y_{t,k}) =& \frac{1}{B} \sum_{i \in S_y} \big(\nabla_y f_i(x_{t,k},y_{t,k}) - \nabla_y f_i(x_{t,k-1}, y_{t,k-1}) +G_y(x_{t,k-1},y_{t,k-1})\big).
\end{align*}
It simultaneously updates two variables $\vx$ and $\vy$ by estimators $G_x$ and $G_y$ with different stepsizes $\tau_x=\Theta(1/(\kappa_y^2L))$ and $\tau_y=\Theta(1/L)$ respectively. \citet{luo2020stochastic,xian2021faster} have studied the SPIDER-type algorithm for nonconvex-strongly-concave problem and showed it converges to the stationary point of $g(x)\triangleq \max_{y\in\BR^{d_y}}f(x,y)$ sublinearly. However, solving the problem minimax problems under the two-sided PL condition desires a stronger linear convergence rate, which leads to our theoretical analysis being different from previous works.

%We provide the sketch of our proofs to clarify why SPIDER may be more suitable for this problem .

We measure the convergence of SPIDER-GDA by the following Lyapunov function
%\begin{align*}
%\fV_{k,t} = \underbrace{g(x_{k,t}) - g(x^{\ast})}_{\fA_{k,t}} +  \frac{\lambda \tau_x}{\tau_y} \underbrace{\big(g(x_{k,t}) - f(x_{k,t},y_{k,t}) \big)}_{\fB_{k,t}},
%\end{align*}
\begin{align*}
\fV(x,y) \triangleq g(x) - g(x^{\ast}) + \frac{\lambda \tau_x}{\tau_y}\big(g(x) - f(x,y) \big),
\end{align*}
where $x^*\in\argmin_{x\in\BR^{d_x}}g(x)$ and $\lambda=\Theta(\kappa_y^2)$.
In Lemma \ref{lem: key-lem-for-Spider} we establish recursion for $\fV_{t,k}$ as
\begin{align*}
    \BE[\fV (x_{t,K}, y_{t,K})] \le \mathbb{E}\left[ \mathcal{V} (x_{t,0},y_{t,0})   - \frac{\tau_x}{2} \sum_{k=0}^{K-1} \Vert \nabla g(x_k) \Vert^2 - \frac{\lambda \tau_x}{4} \sum_{k=0}^{K-1} \Vert \nabla_y f(x_k,y_k) \Vert^2\right]
\end{align*}
by setting $M =B = \sqrt{n}$. Using the facts that $g(\,\cdot\,)$ is $\mu_x$-PL (Lemma \ref{lem: g(x)-PL}) and that $-f(x,\,\cdot\,)$ is $\mu_y$-PL (Assumption \ref{asm: two-PL}), we can show by setting $K = \Theta(\kappa_x \kappa_y^2)$, the value of $\tilde \fV(\tilde x_t,\tilde y_t)$ would shrink by $1/2$ each time the restart mechanism is triggered (Line 16 in Algorithm \ref{alg: SPIDER-GDA}).
%Using some calculus, we can show that (Appendix \ref{apx: SPIDER})
% \begin{align*}
% \text{\small$\displaystyle{
% \BE[\fV_{t,K}] \le \mathbb{E}\left[ \mathcal{V}_{t,0}   
% -\frac{\tau_x}{16} \left( 2 - \frac{M}{B}\right)  \sum_{k=0}^{K-1} \Vert G_x(x_{t,k},y_{t,k}) \Vert^2 - \frac{\lambda \tau_x}{16} \left( 2 - \frac{M}{B}\right) \sum_{k=0}^{K-1} \Vert G_y(x_{t,k},y_{t,k}) \Vert^2\right].}$}
% \end{align*}
% Using the above inequality by setting $M=B=\sqrt{n}\,$ leads to the  estimators $G_x(\tilde x_t,\tilde y_t)$ and $G_y(\tilde x_t,\tilde y_t)$ be sufficiently close to the exact gradient and converge to zero linearly, which indicates $g(\tilde x_t)$ also converges to $g(x^*)$ linearly. 
Below, we formally provide the convergence result for SPIDER-GDA, and its detailed proof is shown in Appendix \ref{apx: SPIDER}.


\begin{thm} \label{thm: SPIDER-GDA} Under Assumption \ref{asm: L-smooth}, \ref{asm: two-PL} and \ref{asm: exist}, we run Algorithm \ref{alg: SPIDER-GDA} with $M= B = \sqrt{n}$ , $\tau_y = 1/(5L), \lambda = 32 L^2 / \mu_y^2$,  $\tau_x = \tau_y /  (24 \lambda) $, $ K = \lceil 2/ (\mu_x \tau_x) \rceil$ and $T = \lceil \log(1/\epsilon)\rceil$. Then the output $(\tilde x_T, \tilde y_T)$ satisfies $g(\tilde x_T) - g(x^{\ast}) \le \epsilon$ and $g(\tilde x_T) - f(\tilde x_T,\tilde y_T) \le 24 \epsilon$ in expectation; and it takes no more than $\fO\big((n + \sqrt{n} \kappa_x \kappa_y^2) \log(1/\epsilon)\big)$ SFO calls.
\end{thm}

Our results provide an SFO upper bound of $\fO( (n + \sqrt{n} \kappa_x \kappa_y^2) \log(1/\epsilon))$ for finding an $\eps$-optimal solution that is better than the complexity $\fO((n + n^{2/3}\kappa_x \kappa_y^2) \log(1/\epsilon))$ derived from SVRG-AGDA~\cite{yang2020global}.
It is possible to use SVRG-type~\cite{johnson2013accelerating,zhang2013linear} estimators to replace the stochastic recursive estimators in Algorithm~\ref{alg: SPIDER-GDA}, which yields the algorithm SVRG-GDA.
We can prove that SVRG-GDA also has $\fO((n + n^{2/3}\kappa_x \kappa_y^2) \log(1/\epsilon))$ SFO upper bound that matches the theoretical result of SVRG-AGDA.
We provide the details in Appendix \ref{apx: SVRG-GDA}.

% We remark that although Theorem \ref{thm: SPIDER-GDA} only shows the convergence to an $\epsilon$-approximate solution (Definition \ref{dfn: epsilon-appox}), the convergence to an $\epsilon$-saddle point (Definition \ref{dfn: epsilon-saddle}) can also be achieved with additionally multiplicative $ \log(1/\epsilon)$ factors in the complexity by following similar analysis 

\begin{algorithm*}[t]  
\caption{SPIDER-GDA $(f, (x_0,y_0), T,K,M, B, \tau_x, \tau_y)$} 
\begin{algorithmic}[1] \label{alg: SPIDER-GDA}
\STATE $\tilde x_0 = x_0, \tilde y_t = y_0 $\\[0.15cm]
\STATE \textbf{for} $t = 0,1,\dots, T-1$ \textbf{do} \\[0.15cm]
\STATE \quad $ x_{t,0} =  \tilde x_t, y_{t,0} = \tilde y_t$ \\[0.15cm]
\STATE \quad \textbf{for} $k = 0, 1, \dots, K-1$ \textbf{do}\\[0.15cm]
\STATE \quad \quad \textbf{if} $\mod(k,M) =0$ \textbf{then} \\[0.15cm]
\STATE \quad \quad \quad $G_x(x_{t,k},y_{t,k}) = \nabla_x f(x_{t,k},y_{t,k})$ \\[0.15cm]
\STATE \quad \quad \quad $G_y(x_{t,k},y_{t,k}) = \nabla_y f(x_{t,k},y_{t,k})$ \\[0.15cm]
\STATE \quad \quad \textbf{else} \\[0.15cm]
\STATE \quad \quad \quad Draw mini-batches $S_x$ and $S_y$ independently with both sizes of $B$. \\[0.15cm]
\STATE \quad \quad \quad {\small$G_x(x_{t,k},y_{t,k}) =\dfrac{1}{B } \sum_{i \in S_x} [\nabla_x f_i(x_{t,k},y_{t,k})- \nabla_x f_i(x_{t,k-1} , y_{t,k-1} )  +G_x(x_{t,k-1},y_{t,k-1})]$}\\[0.15cm]
\STATE \quad \quad \quad {\small$G_y(x_{t,k},y_{t,k}) = \dfrac{1}{B} \sum_{i \in S_y} [\nabla_y f_i(x_{t,k},y_{t,k})- \nabla_y f_i(x_{t,k-1} , y_{t,k-1} ) +G_y(x_{t,k-1},y_{t,k-1})]$} \\[0.15cm]
\STATE \quad \quad \textbf{end if} \\[0.15cm]
\STATE \quad \quad $x_{t,k+1} = x_{t,k} - \tau_x G_x(x_{t,k},y_{t,k})$ \\[0.15cm]
\STATE \quad \quad $y_{t,k+1} = x_{y,k} + \tau_y G_y(x_{t,k},y_{t,k})$ \\[0.15cm]
\STATE \quad  \textbf{end for} \\[0.15cm]
\STATE \quad Choose $(\tilde x_{t+1},\tilde y_{t+1})$ from $\{  (x_{t,k}, y_{t,k} )\}_{k=0}^{K-1} $ uniformly at random. \\ [0.15cm]
\STATE \textbf{end for}  \\[0.15cm]
\STATE \textbf{return} $(\tilde x_T, \tilde y_T)$ 
\end{algorithmic}
\end{algorithm*}

\section{Further Acceleration with Catalyst} \label{sec: Catalyst-for-acc}

\begin{algorithm*}[t]  
\caption{AccSPIDER-GDA} 
\begin{algorithmic}[1] \label{alg: Catalyst-GDA}
    \STATE $u_ 0  = x_0$ \\[0.15cm]
    \STATE \textbf{for} $k = 0, 1, \dots, K-1$ \textbf{do}\\[0.15cm]
    \STATE \quad $(x_{k+1}, y_{k+1}) = \text{SPIDER-GDA}\left( f(x,y) + \dfrac{\beta}{2} \Vert x - u_k \Vert^2,(x_k,y_k),T_k,K,M,B,\tau_x,\tau_y\right)$ \\[0.15cm]
    \STATE $\quad u_{k+1} = x_{k+1} + \gamma(x_{k+1} - x_k)$ \\ [0.15cm]
    \STATE\textbf{end for} \\[0.15cm]
    \STATE \textbf{Option I}  (two-sided PL): \textbf{return} $(x_K,y_K)$  \\[0.15cm]
    \STATE \textbf{Option II} (one-sided PL): \textbf{return} $(\hat x,\hat y)$ chosen uniformly at random from $\{(x_k,y_k)\}_{k=0}^{K-1}$
\end{algorithmic}
\end{algorithm*}

Both the proposed SPIDER-GDA (Algorithm~\ref{alg: SPIDER-GDA}) and existing SVRG-AGDA~\cite{yang2020global} have complexities that more heavily depend on the condition number of $y$ than the condition number of $x$. It is natural to ask whether we can make the dependency of two condition numbers balanced like the results in the strongly-convex-strongly-concave case~\cite{lin2020near,luo2020stochastic,vladislav2021accelerated}. In this section, we show it is possible by introducing the Catalyst acceleration.
To make the acceleration possible, we further assume the uniqueness of the optimal set for the inner problem. 

%{\color{red} rebision: add an assumption}
\begin{asm} \label{asm:uni}
We assume the inner problem $\max_{y \in \BR^{d_y}} f(x,y)$ has a unique solution.
\end{asm}

We proposed the accelerated SPIDER-GDA (AccSPIDER-GDA) in Algorithm \ref{alg: Catalyst-GDA} for reducing the computational cost further.
Each iteration of the algorithm solves the following sub-problem
\begin{align}\label{prob:sub}
    \min_{x \in \BR^{d_x}} \max_{y \in \BR^{d_y}} F_k(x,y) \triangleq \min_{x \in \BR^{d_x}} \left\{g(x) + \frac{\beta}{2} \Vert x - u_k \Vert_2^2\right\}.
\end{align}
by SPIDER-GDA (Algorithm \ref{alg: SPIDER-GDA}). AccSPIDER-GDA has the following convergence result if the sub-problem can attain the required accuracy.

\begin{lem}\label{lem: outer-converge}
Under Assumption  \ref{asm: L-smooth}, \ref{asm: two-PL} and \ref{asm: exist}, we run Algorithm~\ref{alg: Catalyst-GDA} by $\beta = 2L$, $\gamma = 0$ and the appropriate setting for the sub-problem solver such that $\BE [\Vert x_{k} - \tilde x_{k} \Vert^2 + \Vert y_{k} - \tilde y_{k} \Vert^2] \le \delta $, where $(\tilde x_{k}, \tilde y_{k})$ is a saddle point of $F_{k-1}$ $(k \ge 1)$ and we set the precision
\begin{align} \label{dfn:delta-two}
    \delta = \frac{\mu_x \epsilon}{11(\mu_x+ 4L)L}
\end{align}
Then it holds that
\begin{align*}
    \BE[g(x_k)  - g(x^{\ast})]
    &\le \left(1-\frac{\mu_x}{2\beta+ \mu_x} \right)^k \big(g(x_0) - g(x^{\ast})\big) + \frac{\epsilon}{2}.
\end{align*}
\end{lem}
The setting $\beta = \Theta(L)$ in Lemma \ref{lem: outer-converge} guarantees the sub-problem (\ref{prob:sub}) has condition number of the order $\fO(1)$ for $x$. It is more well-conditioned on $x$, we prefer to address the following equivalent problem
\begin{align} \label{sub-minimax}
    \max_{y \in \BR^{d_y}} \min_{ x \in \BR^{d_x}} F_k(x,y) = - \min_{y \in \BR^{d_y}} \max_{ x \in \BR^{d_x}} \left\{ -F_k(x,y)\right\}.
\end{align}

%{\color{red} revision: the uniqueness is necessary}

Since (\ref{sub-minimax}) is a minimax problem satisfying the two-sided PL condition, we can apply SPIDER-GDA to solve it. And we can show that under Assumption \ref{asm:uni}, the saddle point $(\tilde x_k,\tilde y_k)$ of each  $F_{k-1}(k\ge 1)$ is unique (see Lemma \ref{lem: dual-Fk} in appendix) and we are able to obtain a good approximation to it. 

\begin{lem} \label{sub:eps-saddle}
Under Assumption  \ref{asm: L-smooth}, \ref{asm: two-PL} and \ref{asm: exist}, if we use Algorithm \ref{alg: SPIDER-GDA} to solve each sub-problem $\max_{y \in \BR^{d_y}} \min_{ x \in \BR^{d_x}} F_k(x,y) $ (\ref{prob:sub}) with $\beta = 2L$,
$M = B = \sqrt{n}$, $\tau_x = 1/(15L)$, $\lambda = 288$, $\tau_y = \tau_x / (24 \lambda)$, $K = \lceil 2 / (\mu_y \tau_y) \rceil$, $T_k = \lceil \log(1/\delta_k) \rceil$, then it holds that
\begin{align*}
    \BE[ \Vert x_{k+1} - \tilde x_{k+1} \Vert^2 + \Vert y_{k+1} - \tilde y_{k+1} \Vert^2 ] \le 7236 \kappa_y^2 \delta_k \BE[ \Vert x_k - \tilde x_k \Vert^2 + \Vert y_k - \tilde y_k \Vert^2],
\end{align*}
where $(\tilde x_k,\tilde y_k)$ is the unique saddle point of $F_{k-1}(k \ge 1)$.
\end{lem}


%Using Corollary \ref{thm: find-epsilon-two} (in the view of changing the roles of $x$ and $y$), we can find a $\delta$-approximate saddle point of $-F_k(x,y)$ within $\fO( (n+ \sqrt{n}\kappa_y \tilde \kappa_x^2) \log(1/\delta)) = \fO(( n+ \sqrt{n}\kappa_y) \log(1/\delta))$ SFO calls in expectation. 


For a short summary, Lemma \ref{lem: outer-converge} means Algorithm \ref{alg: Catalyst-GDA} requires $\fO(\kappa_x \log(1/\epsilon))$ numbers of inexact proximal point iterations to find an $\epsilon$-optimal solution of the problem. And Lemma \ref{lem: sub-probelm} tells us that each sub-problem can be solved within an SFO complexity of $\fO\left(n + \sqrt{n} \kappa_y) \log(1/\delta_k)\right)$. Thus, the total complexity for AccSPIDER-GDA becomes $\fO( (n \kappa_x + \sqrt{n} \kappa_x \kappa_y ) \log(1/\epsilon) \log(1/\delta_k))$. Our next step is to specify $\delta_k$ which would lead to the total SFO complexity of the algorithm.

\begin{thm} \label{thm: Catalyst-GDA}
Under Assumption  \ref{asm: L-smooth}, \ref{asm: two-PL}, \ref{asm: exist} and \ref{asm:uni} if we  let $\gamma = 0, \beta = 2L$ and use Algorithm \ref{alg: SPIDER-GDA} to solve each sub-problem $\max_{y \in \BR^{d_y}} \min_{ x \in \BR^{d_x}} F_k(x,y)$ (\ref{prob:sub}) with
$M, B, \tau_x, \tau_y,K $ defined as Lemma \ref{sub:eps-saddle} and $T_k = \lceil \log(1/\delta_k) \rceil$, where 
%\begin{align} \label{dfn:delta_k}
%    \delta_k =  \frac{1}{7236 \kappa_y^2 } \times
%    \begin{cases}
%    \min \left\{ \frac{1}{4}, \frac{(\beta-L) \mu_y \delta}{16 \beta^2 \Vert x_k - x_{k-1} \Vert^2} \right \}, & k \ge 1;\\[0.25cm]
%    \frac{\delta \mu_y}{2(g(x_0) - g(x^{\ast}))}, & k =0,
%    \end{cases}
%\end{align}
\begin{align} \label{dfn:delta_k}
    \delta_k = 
    \begin{cases}
    \dfrac{1}{7236 \kappa_y^2 } \min \left\{ \dfrac{1}{4}, \frac{(\beta-L) \mu_y \delta}{16 \beta^2 \Vert x_k - x_{k-1} \Vert^2} \right \}, & k \ge 1;\\[0.25cm]
    \dfrac{\delta \mu_y}{14472 \kappa_y^2 (g(x_0) - g(x^{\ast}))}, & k =0,
    \end{cases}
\end{align}
and $\delta$ is followed by the definition in (\ref{dfn:delta-two}). Then Algorithm \ref{alg: Catalyst-GDA} can return $x_K$ such that $g(x_K) - g(x^{\ast}) \le \epsilon$ in expectation with no more than $\fO( (n \kappa_x + \sqrt{n} \kappa_x \kappa_y) \log(1/\epsilon) \log( \kappa_x \kappa_y /\epsilon))$ SFO calls.
\end{thm}

Lemma \ref{lem: outer-converge} does not rely on the choice of sub-problem solver, we can apply the acceleration framework in Algorithm~\ref{alg: Catalyst-GDA} by replacing SPIDER-GDA with other algorithms. We summarize the SFO complexities for the acceleration of different algorithms in Table \ref{tbl: diff-acc}. 
\begin{table}[t]
\centering
\caption{Accelerated results for different methods under two-sided PL condition.}
{\small
\begin{tabular}{ccc}
\hline 
Method & Before Acceleration & After Acceleration   \\
\hline \addlinespace
GDA & $ \fO(n \kappa_x \kappa_y^2 \log(1/\epsilon))$  & $\tilde \fO\left(n \kappa_x \kappa_y \log^2(1/\epsilon)\right)$\\[0.3cm]
SVRG-GDA & $\fO((n + n^{2/3}\kappa_x \kappa_y^2) \log(1/\epsilon))$  & 
$
\begin{cases}
\tilde \fO\left(n^{2/3} \kappa_x \kappa_y \log^2(1/\epsilon)\right), & n^{1/3} \lesssim \kappa_y; \\[0.15cm]
\tilde \fO \left(n \kappa_x \log^2(1/\epsilon)\right), & \kappa_y \lesssim n^{1/3} \lesssim \kappa_x \kappa_y; \\[0.15cm]
\text{no acceleration}, & \kappa_x \kappa_y \lesssim n^{1/3}.
\end{cases}
$
\\\addlinespace
SPIDER-GDA & $\fO\left((n + \sqrt{n} \kappa_x \kappa_y^2) \log(1/\epsilon)\right)$ & $
\begin{cases}
\tilde \fO\left(\sqrt{n} \kappa_x \kappa_y \log^2(1/\epsilon)\right), &  \sqrt{n} \lesssim \kappa_y; \\[0.15cm]
\tilde \fO \left( n \kappa_x \log^2(1/\epsilon)\right), & \kappa_y \lesssim \sqrt{n} \lesssim \kappa_x \kappa_y; \\[0.15cm]
\text{no acceleration}, & \kappa_x \kappa_y \lesssim \sqrt{n}. 
\end{cases}
$  \\ \addlinespace
 \hline
\end{tabular}}
\label{tbl: diff-acc}
\end{table}

\section{Extension to One-Sided PL Condition} \label{sec: extension}
In this section, we show the idea that SPIDER-GDA and its  Catalyst acceleration also work for one-sided PL conditions. 
We relax Assumption \ref{asm: two-PL} and \ref{asm: exist} to the following one.

\begin{asm} \label{asm: one-PL}
We suppose that $-f(x,\cdot)$ is $\mu_y$-PL for any $x\in\BR^{d_x}$;
the problem $\max_{y\in\BR^{d_y}}f(x,y)$ has a nonempty solution set and an optimal value ; $g(x) \triangleq \max_{y\in\BR^{d_y}}f(x,y)$ is lower bounded, i.e., we have $g^{\ast} = \inf_{x \in \BR^{d_x}} g(x) > -\infty$.
\end{asm}

We first show that the SFO complexity of SPIDER-GDA outperforms SVRG-GDA \footnote{The complexity for finding an $\epsilon$-stationary point of SVRG-GDA is presented in Appendix \ref{apx: one-side}.}  by a factor of $\mathcal{O}(n^{1/6})$ in Theorem \ref{thm: SPIDER-GDA-one}. 
 
\begin{thm} \label{thm: SPIDER-GDA-one}
Under Assumption \ref{asm: L-smooth} and \ref{asm: one-PL} , Let $T = 1$ and $M,B,\tau_x,\tau_y,\lambda$ as defined in Theorem~\ref{thm: SPIDER-GDA} and $ K = \lceil 64 / (\tau_x \epsilon^2 )\rceil$, then  Algorithm \ref{alg: SPIDER-GDA} can guarantee the output $\hat x$ to satisfy  $\Vert \nabla g(\hat x) \Vert \le \epsilon$ in expectation with no more than $\fO(n+ { \sqrt{n} \kappa_y^2 L}{\epsilon^{-2}})$ SFO calls.
\end{thm}

The AccSPIDER-GDA also performs better than SPIDER-GDA under one-sided PL conditions for ill-conditioned problems. In the following lemma, we show that AccSPIDER-GDA could find an approximate stationary point if we solve the sub-problem sufficiently accurately.

\begin{lem} \label{lem: outer-convergence-one-side}
Under Assumption \ref{asm: L-smooth} and \ref{asm: one-PL}, if  it holds true that  $\BE[\Vert x_{k} - \tilde x_{k} \Vert^2 + \Vert y_{k} - \tilde y_{k} \Vert^2] \le \delta $ for some saddle point $(\tilde x_{k}, \tilde y_{k})$ of $F_{k-1}$ $(k \ge 1)$, where  
\begin{align} \label{dfn:delta-one}
    \delta = \frac{\epsilon^2}{8L \kappa_y(  22 \mu_y +1 )}.
\end{align}
 Let $\beta=2L$, then for the output $(\hat x,\hat y)$ of Algorithm \ref{alg: Catalyst-GDA}, it holds true that
\begin{align*}
    \BE \Vert \nabla g(\hat x) \Vert^2
    \le  \frac{8 \beta(g(x_0) - g^{\ast})}{K} +\frac{\epsilon^2}{2}.
\end{align*}
\end{lem}

Compared with SPIDER-GDA, the analysis of AccSPIDER-GDA is more complicated since the precision $\delta_k$ at each round is different. By choosing the parameters of the algorithm carefully, we obtain the following result.

\begin{thm} \label{thm: Catalyst-GDA-one-side}
Under Assumption \ref{asm: L-smooth}, \ref{asm: one-PL} and \ref{asm:uni}, if we run Algorithm~\ref{alg: Catalyst-GDA} by $\gamma = 0, \beta = 2L$  and use Algorithm \ref{alg: SPIDER-GDA} to solve each sub-problem $\max_{y \in \BR^{d_y}} \min_{ x \in \BR^{d_x}} F_k(x,y)$ (\ref{prob:sub}) with $M,B,\tau_x,\tau_y,\lambda,K$ and $T_k$ (dependent on $\delta$) as in Theorem \ref{thm: Catalyst-GDA}
and $\delta$ is followed by the definition in Lemma \ref{lem: outer-convergence-one-side}, then Algorithm \ref{alg: Catalyst-GDA} can find $\hat x$ such that $\Vert \nabla g(\hat x) \Vert \le \epsilon$ in expectation within $\fO( (n  + \sqrt{n} \kappa_y) L \epsilon^{-2} \log(\kappa_y/\epsilon) )$ SFO calls.
\end{thm}

We can directly set $\beta=0$ for Algorithm~\ref{alg: Catalyst-GDA} in the case of very large $n$, which makes AccSPIDER-GDA reduce to SPIDER-GDA. The summary and comparison of the complexities under the one-sided PL condition are both shown in Table \ref{tbl: one-side-PL}.
Besides, the algorithms of GDA and SVRG-GDA also can be accelerated with the Catalyst framework and we present the corresponding results in Table \ref{tbl: diff-acc-one}.

\begin{table}[t]
\centering
\caption{Acceleration for different methods under one-sided PL condition. }
{\small\begin{tabular}{ccc}
\hline
Method & Before Acceleration & After Acceleration   \\
\hline \addlinespace
GDA & $\fO\left(n \kappa_y^2L\epsilon^{-2}\right)$  & $\fO\left(n \kappa_y L\epsilon^{-2} \log(\kappa_y/\epsilon)\right)$\\[0.2cm]
SVRG-GDA & $ \fO\left(n + n^{2/3} \kappa_y^2 L\epsilon^{-2} \right)$  & 
$
\begin{cases}
 \fO\left(n^{2/3} \kappa_y L\epsilon^{-2}  \log(\kappa_y/\epsilon) \right), &  n^{1/3} \lesssim \kappa_y; \\[0.15cm]
 \fO \left( n L\epsilon^{-2}  \log(\kappa_y/\epsilon) \right), & \kappa_y \lesssim n^{1/3} \lesssim \kappa_y^2; \\[0.15cm]
\text{no acceleration}, & \kappa_y^2 \lesssim n^{1/3}. 
\end{cases}
$
\\ \addlinespace
SPIDER-GDA & $\fO\left(n + {\sqrt{n} \kappa_y^2 L}{\epsilon^{-2}}\right)$ & $
\begin{cases}
\fO\left({\sqrt{n} \kappa_y L}{\epsilon^{-2}}  \log(\kappa_y/\epsilon) \right), & \sqrt{n} \lesssim \kappa_y; \\[0.15cm]
\fO \left( {n L}{\epsilon^{-2}}  \log(\kappa_y/\epsilon)\right), & \kappa_y \lesssim \sqrt{n} \lesssim \kappa_y^2; \\[0.15cm]
\text{no acceleration}, & \kappa_y^2  \lesssim \sqrt{n}. 
\end{cases}
$  \\ \addlinespace
 \hline
\end{tabular}}
\label{tbl: diff-acc-one}
\end{table}

\section{Experiments} \label{sec: exp}

In this section, we conduct the numerical experiments to show the advantage of proposed algorithms and the source code is available\footnote{~\url{https://github.com/TrueNobility303/SPIDER-GDA}}. We consider the following two-player Polyak--{\L}ojasiewicz game:
\begin{align*}
\min_{x\in\BR^{d}}\max_{y\in\BR^{d}} f(x,y) \triangleq \frac{1}{2} x^\top P x  - \frac{1}{2}y^\top Q  y + x^\top R  y,
\end{align*}
where
\begin{align*}
P = \frac{1}{n}\sum_{i=1}^n p_i p_i^\top, \quad
Q = \frac{1}{n}\sum_{i=1}^n q_i q_i^\top \quad \text{and} \quad
R = \frac{1}{n}\sum_{i=1}^n r_i r_i^\top.
\end{align*}
We independently sample $p_i$, $q_i$ and $r_i$ from $\fN(0, \Sigma_P) $, $\fN(0, \Sigma_Q)$ and $\fN(0, \Sigma_R)$ respectively. We set the covariance matrix $\Sigma_P$ as the form of $U D U^\top$ such that $U\in\BR^{d\times r}$ is column orthogonal matrix and $D\in\BR^{r\times r}$ is diagonal with $r<d$. The diagonal elements of $D$ are distributed uniformly in the interval $[\mu, L]$ with $0<\mu<L$. The matrix $\Sigma_Q$ is set in a similar way to $\Sigma_P$. We also let $\Sigma_R = 0.1 V V^\top$, where each element of $V\in\BR^{d\times d}$ is sampled from $\fN(0, 1)$ independently. Since the covariance matrices $\Sigma_P$ and $\Sigma_Q$ are rank-deficient, it is guaranteed that both $P$ and $Q$ are singular. Hence, the objective function is not strongly-convex nor strongly-concave, but it satisfies the two-sided PL-condition~\cite{karimi2016linear}. We set $n = 6000, d= 10$, $r=5$, $L=1$ for all experiments; and let $\mu$ be $10^{-5}$ and $10^{-9}$ for two different settings.

We compare the proposed SPIDER-GDA (Algorithm \ref{alg: SPIDER-GDA}) and AccSPIDER-GDA (Algorithm \ref{alg: Catalyst-GDA}) with the baseline algorithm SVRG-AGDA~\cite{yang2020global}. We let $B=1$ and $M=n$ for all of these algorithms and both of the stepsizes for $x$ and $y$ are tuned from $\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}\}$. For AccSPIDER, we set $\beta = L/(20n)$ and $\gamma = 0.999$. 
We present the results of the number of SFO calls against the norm of gradient and the distance to the saddle point in Figure \ref{fig: dataset-1} and Figure \ref{fig: dataset-2}. It is clear that our algorithms outperform baselines.

% condition number of 1e5
% Figure environment removed

% condition number of 1e9
% Figure environment removed

\section{Conclusion and Future Work} \label{sec: discucssion}


In this paper, we have investigated stochastic optimization for PL conditioned minimax problem with the finite-sum objective. 
We have proposed the SPIDER-GDA algorithm, which reduces the dependency of the sample numbers in SFO complexity. Moreover, we have introduced a Catalyst scheme to accelerate our algorithm for solving ill-conditioned problems. 
We improve the SFO upper bound of the state-of-the-art algorithms for both two-sided and one-sided PL conditions.

However, the optimality of SFO algorithms for the PL conditioned minimax problem is still unclear. It is interesting to construct the lower bound for verifying the tightness of our results. It is also possible to extend our algorithm to the online setting.


\section*{Acknowledgements}
The authors would like to thank Yunyan Bai for pointing out some mistakes in the proof of Theorem \ref{thm: SPIDER-GDA}.
This work is supported by the National Natural Science Foundation of China (No. 62206058) and the Shanghai Sailing Program (22YF1402900).

\bibliographystyle{plainnat}
\bibliography{reference}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{Checklist}


%%% BEGIN INSTRUCTIONS %%%

\iffalse
The checklist follows the references.  Please
read the checklist guidelines carefully for information on how to answer these
questions.  For each question, change the default \answerTODO{} to \answerYes{},
\answerNo{}, or \answerNA{}.  You are strongly encouraged to include a {\bf
justification to your answer}, either by referencing the appropriate section of
your paper or providing a brief inline description.  For example:
\begin{itemize}
  \item Did you include the license to the code and datasets? \answerYes{See Section~\ref{gen_inst}.}
  \item Did you include the license to the code and datasets? \answerNo{The code and the data are proprietary.}
  \item Did you include the license to the code and datasets? \answerNA{}
\end{itemize}
Please do not modify the questions and only use the provided macros for your
answers.  Note that the Checklist section does not count towards the page
limit.  In your paper, please delete this instructions block and only keep the
Checklist section heading above along with the questions/answers below.
\fi
%%% END INSTRUCTIONS %%%


\begin{enumerate}


\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work?
    \answerYes{See Section \ref{sec: discucssion}.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerNo{The purpose of this work is for to provide a better understanding of GDA on a class of nonconvex-nonconcave minimax optimization.} 
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerYes{See Section \ref{sec: set-up}.}
        \item Did you include complete proofs of all theoretical results?
    \answerYes{See Appendix for details.}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{We include the codes In the supplemental materials.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{See Section \ref{sec: exp}.}
\item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerNo{We wants to compare the training dynamic, and different trials may cause different numerical results, which can not be observed clearly in one graph.}
    
    \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerNo{The experiments is certainly simple
and easy to run under CPUs.}

\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{}
  \item Did you mention the license of the assets?
    \answerYes{}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNo{These datasets are common.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNo{These datasets are common.}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\newpage
\appendix
\input{appendix.tex}


\end{document} 