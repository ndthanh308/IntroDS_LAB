\documentclass[11pt,a4paper]{amsart}
\usepackage[foot]{amsaddr}
\setlength\marginparwidth{2cm}
\usepackage{ifxetex}
\ifxetex
  \usepackage[no-math]{fontspec}
\else
\fi
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{cleveref}
\usepackage{fullpage}
\usepackage{microtype}
%\usepackage{charter}
\ifxetex % fix font incompatibility
  \usepackage[libertine]{newtxmath}
\else
  \usepackage{newtxmath}
\fi
\usepackage[tt=false]{libertine} %% tt is ugly 
\usepackage{caption}
\usepackage{tcolorbox}
\usepackage{bbm}
\usepackage{hyperref, color}
\hypersetup{colorlinks=true,citecolor=blue, linkcolor=blue, urlcolor=blue}
\usepackage[linesnumbered,boxed,ruled,vlined]{algorithm2e}
\usepackage{bm}
\usepackage{bbm}
\usepackage[numbers]{natbib}
\usepackage{xcolor}
\usepackage{enumerate} 
\usepackage{enumitem}
\usepackage{ragged2e}
\usepackage{tabularx}
\usepackage{array}
\usepackage{longtable}
\usepackage{hhline}
\usepackage{multirow}
\usepackage{cleveref}
%\usepackage{enumitem}
\newcolumntype{L}[1]{>{\raggedright\arraybackslash}p{#1}}
\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}p{#1}}
\renewcommand\arraystretch{1.5}  
%\renewcommand\arraystretch{2}
\usepackage{makecell}
%\usepackage{ulem}
%\usepackage[T1]{fontenc}
\usepackage{footnote}
\usepackage{float}
\usepackage{xifthen}
\makesavenoteenv{tabular}

\newcommand{\TODO}[1]{\typeout{TODO: \the\inputlineno: #1}\textbf{{\color{red}[[[ #1 ]]]}}}


\newcommand{\mgn}[1]{\tilde{{#1}}}
\newcommand{\MC}[1]{\mathfrak{{#1}}}
\newcommand{\MSC}[1]{\mathscr{{#1}}}
\newcommand{\sample}{\textnormal{\textsf{{\color{red}old-MarginalSampler}}}}
\newcommand{\factor}{\textnormal{\textsf{Factorize}}}
\newcommand{\bsample}{\textnormal{\textsf{BFSampler}}}
\newcommand{\fix}{\textnormal{\textsf{Fix}}}
\newcommand{\refactor}{\textnormal{\textsf{Recursive-Factorize}}}
\newcommand{\rejsamp}{\textnormal{\textsf{RejectionSampling}}}
\newcommand{\magsample}{\textnormal{\textsf{MarginSample}}}
\newcommand{\magsampleh}{\textnormal{\textsf{MarginSample-hardcore}}}
\newcommand{\subsampleone}{\textnormal{\textsf{SubSampler1}}}
\newcommand{\subsampletwo}{\textnormal{\textsf{SubSampler2}}}
\newcommand{\pathsample}{\textnormal{\textsf{PathSampler}}}
\newcommand{\newsample}{\textnormal{\textsf{NewSampler}}}
\newcommand{\newsampleh}{\textnormal{\textsf{NewSampler-hardcore}}}
\newcommand{\bpathsample}{\textnormal{\textsf{BoundedPathSampler}}}
\newcommand{\pth}{\textnormal{\textsf{Path}}}
\newcommand{\bpth}{\textnormal{\textsf{BPath}}}
\newcommand{\lf}{\textnormal{\textsf{Leaf}}}
\newcommand{\blf}{\textnormal{\textsf{BLeaf}}}
\newcommand{\simulator}{\textnormal{\textsf{Simulate}}}

\newcommand{\DS}[1]{\mathds{{#1}}}
\newcommand{\exenum}{\textnormal{\textsf{ExhaustiveEnumeration}}}
\newcommand{\gassign}{\textnormal{\textsf{GuidingAssignment}}}
\newcommand{\consor}{\textnormal{\textsf{ConsOracle}}}
\newcommand{\rsor}{\textnormal{\textsf{RSOracle}}}
\newcommand{\revbf}{\textnormal{\textsf{ReverseBF}}}
\newcommand{\linbf}{\textnormal{\textsf{LinearBF}}}
\newcommand{\berrace}{\textnormal{\textsf{BernoulliRace}}}
\newcommand{\subbf}{\textnormal{\textsf{SubtractBF}}}
\newcommand{\sub}{\textnormal{\textsf{Sub}}}
\newcommand{\eval}{\textnormal{\textsf{Eval}}}
\newcommand{\checkf}{\textnormal{\textsf{Frozen}}}
\newcommand{\asgn}{\textnormal{\textsf{Assign}}}
\newcommand{\revoke}{\textnormal{\textsf{Revoke}}}
\newcommand{\qvar}{\textnormal{\textsf{Findvar}}}
\newcommand{\qvarr}{\textnormal{\textsf{Findvar2}}}
\newcommand{\qvbl}{\textnormal{\textsf{Findvbl}}}
\newcommand{\res}{\textnormal{\textsf{Res}}}
\newcommand{\var}{\textnormal{\textsf{vbl}}}
\newcommand{\qcc}{\textnormal{\textsf{Component}}}
\newcommand{\upd}{\textnormal{\textsf{UpdTime}}}
\newcommand{\decomp}{\textnormal{\textsf{Decomposition}}}

\newcommand{\recsample}{\textnormal{\textsf{MarginOverflow}}}
\newcommand{\gd}{\textnormal{\textsf{GlauberDynamics}}}
\newcommand{\gdss}{\textnormal{\textsf{GlauberDynamics(Systematic Scan)}}}
\newcommand{\mgdss}{\textnormal{\textsf{BoundedMarginalGDsampler}}}
\newcommand{\imgdss}{\textnormal{\textsf{MarginalGDsampler}}}
\newcommand{\recsampleh}{\textnormal{\textsf{MarginOverflow-hardcore}}}
\newcommand{\vrecsample}{\textnormal{\textsf{constraintRecursiveSampler}}}
\newcommand{\recalc}{\textnormal{\textsf{RecursiveApproximator}}}
\newcommand{\firstcalc}{\textnormal{\textsf{FirstApproximator}}}
\newcommand{\calc}{\textnormal{\textsf{MarginalApproximator}}}
\newcommand{\enu}{\textnormal{\textsf{Enumerator}}}

\newcommand{\tsamp}{\overline{t}_\textnormal{\textsf{MS}}}
\newcommand{\rtsamp}{T_\textnormal{\textsf{MS}}}
\newcommand{\trecor}{\overline{t}_\textnormal{\textsf{MO}}}
\newcommand{\tbf}{T_\textnormal{\textsf{BFS}}}
\newcommand{\tbfup}{\overline{t}_\textnormal{\textsf{BF}}}
\newcommand{\trecalc}{T_\textnormal{\textsf{ReAppr}}}
\newcommand{\texe}{T_\textnormal{\textsf{EXE}}}
\newcommand{\tenum}{T_\textnormal{\textsf{Enum}}}
\newcommand{\tenumup}{T^{\textnormal{\textsf{up}}}_\textnormal{\textsf{Enum}}}
\newcommand{\tma}{T_\textnormal{\textsf{MAppr}}}
\newcommand{\tass}{T_\textnormal{\textsf{Guide}}}
\newcommand{\tcmain}{T_\textnormal{\textsf{Count}}}
\newcommand{\trej}{T_\textnormal{\textsf{Rej}}}
\newcommand{\tmarg}{T_\textnormal{\textsf{Mag}}}
\newcommand{\teval}{T_\textnormal{\textsf{Eval}}}
\newcommand{\tcheck}{T_\textnormal{\textsf{Check}}}
\newcommand{\tvar}{t_\textnormal{{\textsf{var}}}}
\newcommand{\dtv}{d_{\rm TV}}
\def\Pr{\mathop{\mathbf{Pr}}\nolimits}

\newcommand{\at}{A_\textnormal{{\textsf{time}}}}
\newcommand{\ac}{A_\textnormal{{\textsf{check}}}}
\newcommand{\aev}{A_\textnormal{{\textsf{eval}}}}


%\newcommand{\bcalc}{\textnormal{\textsf{BFCalculator}}}

\newcommand{\Mod}[3]{{#1}_{{#2}\gets{#3}}}
\newcommand{\vbl}{\mathsf{vbl}}
\newcommand{\True}{\mathtt{True}}
\newcommand{\False}{\mathtt{False}}
\newcommand{\tp}{\tuple}
% \renewcommand{\mid}{\;\middle\vert\;}
\newcommand{\cmid}{\,:\,}
\newcommand{\pin}{\mathsf{Pin}}
\newcommand{\numP}{\#\mathbf{P}} \renewcommand{\P}{\mathbf{P}}
\renewcommand{\d}{\,\-d}
\newcommand{\ol}{\overline}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\ctp}[1]{\left\lceil#1\right\rceil}
\newcommand{\ftp}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\vstar}[1]{V^{#1}_{\star}}

\SetKwRepeat{Do}{do}{while}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{example}[theorem]{Example}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{Condition}{Condition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem*{remark*}{Remark}
\newtheorem{assumption}{Assumption}

\renewcommand{\Pr}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathop{\mathbf{Pr}}\left[#2\right]} {\mathop{\mathbf{Pr}}\limits_{#1}\left[#2\right]} }
\newcommand{\E}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathop{\mathbf{E}}\left[#2\right]}
  {\mathop{\mathbf{E}}\limits_{#1}\left[#2\right]} }
\newcommand{\Var}[2][]{ \ifthenelse{\isempty{#1}}
  {\mathbf{\mathbf{Var}}\left[#2\right]}
  {\mathbf{\mathbf{Var}}_{#1}\left[#2\right]} }
\newcommand{\poly}{{\rm poly}}  
%\newcommand{\cfrozen}{\+{C}_\text{frozen}}
\newcommand{\nextvar}[1]{{{\mathsf{NextVar}}}\left({#1}\right)} 

\newcommand{\cfrozen}[1]{\+{C}^{#1}_{\mathsf{frozen}}}
\newcommand{\vfrozen}[1]{V^{#1}_{\mathsf{frozen}}}
\newcommand{\vfix}[1]{V^{#1}_{\mathsf{fix}}}
\newcommand{\vsfix}[1]{V^{#1}_{\star{\mathsf{\text{-}fix}}}}
\newcommand{\hfix}[1]{H^{#1}_{\mathsf{fix}}}
\newcommand{\vinf}[1]{V^{#1}_{\star{\mathsf{\text{-}inf}}}}
\newcommand{\cfix}[1]{\+{C}^{#1}_{\mathsf{fix}}}
\newcommand{\ccon}[1]{\+{C}^{#1}_{\star{\mathsf{\text{-}con}}}}
\newcommand{\csfix}[1]{\+{C}^{#1}_{\star{\mathsf{\text{-}fix}}}}
\newcommand{\vrec}[1]{V^{#1}_{\mathsf{recur}}}
\newcommand{\vcon}[1]{V^{#1}_{\star{\mathsf{\text{-}con}}}}
\newcommand{\crec}[1]{\+{C}^{#1}_{\star}}
\newcommand{\vst}[1]{V^{#1}_{\star}}
\newcommand{\csfrozen}[1]{\+{C}^{#1}_{\star{\mathsf{\text{-}frozen}}}}
\newcommand{\cbad}[1]{\+{C}^{#1}_{\star{\mathsf{\text{-}bad}}}}
\newcommand{\ccrec}[1]{\+{C}^{#1}_{\star{\mathsf{-ass}}}}
\newcommand{\ccbad}[1]{\+{C}^{#1}_{{\mathsf{bad}}}}
\newcommand{\csat}[1]{\+{C}^{#1}_{{\mathsf{sat}}}}
\newcommand{\qs}{\+{Q}^*}
\newcommand{\qst}{\+{Q}^{\star}}
\newcommand{\os}{\Omega^{\ast}}
\newcommand{\tv}{d_{\rm TV}}
\newcommand{\IET}[1]{\mathscr{T}^{IET}_{#1}}

\newcommand{\Formal}[1]{\underline{\mathbf{#1}}}

%\newcommand{\Formal}[1]{%
%    \IfEqCase{#1}{%
%        {x}{\underline{\mathbf{x}}}%
%        {y}{\underline{\mathbf{y}}}%
%        % you can add more cases here as desired
%    }%
%}%

\newcommand{\D}[1]{\+{D}{#1}}
\newcommand{\Set}[1]{\+{S}{#1}}


\usepackage[textsize=tiny]{todonotes}
\newcommand{\wcytodo}[1]{\todo[color=green!30]{wcy: #1}}
\linepenalty=1000



\newcommand{\one}[1]{\mathbbm{1}\left[#1\right]}
\def\^#1{\mathbb{#1}} % Use \*A for \mathbf{A}
\def\*#1{\mathbf{#1}} % Use \*A for \mathbf{A}
\def\+#1{\mathcal{#1}} % Use \+A for \mathcal{A}
\def\-#1{\mathrm{#1}} % Use \-A for \mathrm{A}
\def\=#1{\boldsymbol{#1}} % Use \=A for \boldsymbol{A}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\edge}{\+{E}}
\newcommand{\defeq}{\triangleq}

\usepackage{amssymb}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{xcolor}
\usepackage{graphicx}
\newcommand\openbigstar[1][0.7]{%
  \scalerel*{%
    \stackinset{c}{-.125pt}{c}{}{\scalebox{#1}{\color{white}{$\bigstar$}}}{%
      $\bigstar$}%
  }{\bigstar}
}

\newcommand{\hollowstar}{\text{$\scriptstyle\openbigstar[.7]$}}


\newbool{doubleblind}
\setbool{doubleblind}{false}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SET THE TITLE
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TITLE:
\title{Correlation decay up to the sampling threshold\\ in the local lemma regime}
% AUTHORS:

\ifdoubleblind
  \author{Author(s)}
\else
\author{Chunyang Wang, Yitong Yin}
\address[Chunyang Wang, Yitong Yin]{State Key Laboratory for Novel Software Technology, Nanjing University, 163 Xianlin Avenue, Nanjing, Jiangsu Province, 210023, China. \textnormal{E-mail: \url{wcysai@smail.nju.edu.cn}, \url{yinyt@nju.edu.cn}}}
\fi

    
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BODY OF THE DOCUMENT
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\allowdisplaybreaks
\maketitle
\begin{comment}

\section{Outline for intro}

\begin{enumerate}
    \item  Introduce LLL and CSP background
    \item  Introduce current state of sampling LLL
    \begin{itemize}
        \item $p\Delta^5\lesssim 1$, while using the freezing technique and encounters a bottleneck for the use of $\{2,3\}$-trees
        \item compare with algorithmic LLL, where exactly the same technique of freezing was used and the bottleneck from $\{2,3\}$-tree was confronted
        \begin{itemize}
            \item And then Moser-Tardos algorithm comes, with a very different method, achieving the optimal regime
        \end{itemize}
    \end{itemize}
    \item Introduce the correlation decay method
    \begin{itemize}
        \item The notion of correlation decay (WSM,SSM) and its use for spin systems
        \begin{itemize}
            \item the recent ``spectral independence'' method where the largest eigenvalue of pairwise influence matrix under abitrary pinning was analyzed
        \end{itemize}
        \item Correlation decay under LLL regime
        \begin{itemize}
            \item All previous notions fail(WSM,SSM) by that the LLL condition is not self-reducible
            \begin{itemize}
            \item Draw a picture on this for better illustration
            \end{itemize}
            \item A special form of correlation decay persists
            \begin{itemize}
                \item Moitra's approach starts with analyzing a coupling of $\mu(\cdot\mid x=T)$ and $\mu(\cdot\mid x=F)$, also mentioned it in the \hyperlink{https://www.youtube.com/watch?v=oErCGdM-YDc&t=3491s}{video}(35:21) as ``an average case of correlation decay''
                \item In ~\cite{guo2019counting} they call this ``a special form of the spatial mixing
property''
                    \item This implies the largest eigenvalue of pairwise influence matrix (with no pinning!) is bounded
                    \item All current algorithmic results for sampling/counting LLL implies and utilizes this kind of property
                    \item In hardcore model, this implies spectral independence
            \end{itemize}
        \end{itemize}
        \item Our results
        \begin{itemize}
            \item This ``special form'' the correlation decay holds for atomic constraints when \eqref{eq:correlation-decay-condition} holds. This regime approaches $p\Delta^2\lesssim 1$ when $q$ becomes large.
            \item Conjecture1: Hypergraph coloring is tractable under the $p\Delta^2\lesssim 1$ regime
            \item Conjecture2 (Much more wild!): The phase transition of computational tractability for occurs around the phase transition of such kind of correlation decay
            \begin{itemize}
                \item How to back this up?
                \item Missing piece: such  ``special form'' of correlation decay does not hold when $p\Delta^2\gtrsim 1$.
                \begin{itemize}
                    \item Can construct instance from hypergraph independent set(group $k$ variables together to get domain size $2^k$),
                    \item What about hypergraph coloring?
                \end{itemize}
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{enumerate}
    
\end{comment}
\begin{abstract}
We study the decay of correlation between locally constrained independent random variables in the local lemma regimes.
%the distribution defined by constraint satisfaction problems (CSPs) in the local lemma regime. 
%
For atomically constrained independent random variables of sufficiently large domains,
we show that a decay of correlation property holds up to the local lemma condition $pD^{2+o(1)}\lesssim 1$,
asymptotically matching the sampling threshold for constraint satisfaction solutions~\cite{BGG19,galanis2021inapproximability}. 
%
This provides evidence for the conjectured $pD^2\lesssim 1$ threshold for the ``\emph{sampling Lov\'{a}sz local lemma}''. 
%

We use a recursively-constructed coupling to bound the correlation decay. Our approach completely dispenses with the ``freezing'' paradigm originated from Beck~\cite{beck1991algorithmic}, which was commonly used to deal with the non-self-reducibility of the local lemma regimes, and hence can bypass the current technical barriers due to the use of $\{2,3\}$-trees.
%Our approach bounds the correlation decay in a recursively-constructed coupling 
%through the ``freezing'' paradigm of Beck~\cite{beck1991algorithmic},
%which is capable of dealing with the non-self-reducibility of the local lemma condition, 
%and hence can bypass the current technical barriers from using the $\{2,3\}$-trees.
\end{abstract}




\section{Introduction}\label{sec:intro}

Constraint satisfaction problems (CSPs) are ubiquitous in Computer Science, and the analysis of their solution spaces has always been a subject of great interest. 
%
A CSP is a collection of constraints defined on a set of variables whose solution is an assignment of variables such that all constraints are satisfied. 
%
A powerful tool closely related to the solution space of CSP is the celebrated \emph{Lov\'{a}sz local lemma}~\cite{LocalLemma}, which establishes the following sufficient condition for the existence of a CSP solution by interpreting the space of assignments as a product probability space and the violation of each constraint as a ``bad event'':
\begin{equation}\label{eq:LLL-condition}
\mathrm{e}p(D+1)\leq 1,
\end{equation}
where $p$ is the maximum violation probability of each constraint, and $D$ is the maximum number of other constraints that a constraint can share variables with. 
This condition~\eqref{eq:LLL-condition} was later shown to be essentially tight~\cite{She85}. Subsequent work on the \emph{algorithmic Lov\'{a}sz local lemma} seeks to give algorithms for finding a CSP solution efficiently. 
This leads to a long line of research~\cite{beck1991algorithmic,Alon91,MR98,CS00,Sri08,moser2009constructive,moser2010constructive}, culminating in the breakthrough of Moser and Tardos~\cite{moser2010constructive}, which gives an algorithm for efficiently finding a CSP solution up to the condition in \eqref{eq:LLL-condition}. 
These together beautifully establish a sharp threshold for the existence/construction of CSP solutions given $p$ and $D$.

On the other hand, a considerable amount of work has been focused on the \emph{sampling Lov\'{a}sz local lemma} ~\cite{BGG19,HSZ19,Moi19,guo2019counting,FGYZ20,feng2021sampling,Vishesh21sampling,Vishesh21towards,HSW21,galanis2021inapproximability,qiu2022perfect,feng2022improved,he2022sampling,qiu2022inapproximability,he2022counting}, which seeks to characterize a local-lemma type regime under which the problem of (almost) uniformly sampling CSP solutions is tractable. The hardness results in ~\cite{BGG19,galanis2021inapproximability} show that the tractability of the sampling variant of LLL requires a strictly stronger condition  $pD^2\lesssim 1$, where $\lesssim$ hides lower-order factors and constants.
And this is true even restricted to some sub-classes of CSPs, e.g.~$k$-CNFs or hypergraph colorings. 
For upper bounds, the state-of-the-art work~\cite{he2022counting} gives an efficient algorithm for sampling CSP solutions under the condition $pD^5\lesssim 1$. %And for the restricted subclass of hypergraph colorings, the current best algorithmic result works under $pD^3\lesssim 1$ instead of $pD^2\lesssim 1$~\cite{Vishesh21sampling,HSW21}. 
It is not yet clear what is the correct threshold for the sampling LLL. Henceforth, the following open problem is fundamental for understanding sampling CSP solutions:
\begin{equation}\label{eq:question}
\textit{Is } pD^2\lesssim 1 \textit{ the correct threshold for the sampling LLL?} 
\end{equation}




\textbf{Correlation decay in the LLL regime}\,\,
As an attempt to answer~\eqref{eq:question}, we consider a key property known as ``correlation decay'', 
which has been playing a crucial role in efficient sampling.  
The correlation decay property asks that the correlations between random variables, captured by the influences on the marginal distributions, decay as the distances between the random variables grow.
The correlation decay property has been a key to the rapid mixing since the classic work of Dobrushin~\cite{dobrushin70prescribing}, 
and for two-spin systems (i.e.~the pairwise constrained Boolean-valued random variables), it has been proved that the correlation decay property captures the tractability of sampling and the rapid mixing of Markov chains~\cite{weitz06counting,sinclair12approximation,li2013correlation,Sly2012computational,galanis2016inapproximability,anari2020spectral,chen2020rapid,chen2021optimal,CFYZ2021,anari2022entropic,CE22,CFYZ2022}. 
For sampling CSP solutions,
the correlation decay properties have also been used explicitly or implicitly in various previous algorithms~\cite{HSZ19,Moi19,galanis2019counting,guo2019counting,FGYZ20,feng2021sampling,Vishesh21sampling,Vishesh21towards,HSW21,qiu2022perfect,feng2022improved,he2022sampling,galanis2022fast,he2022counting,feng2023towards,chen2023from,he2023improved}.

%There, the term ``correlation decay'' is often associated with forms of ``weak/strong spatial mixing'', which essentially says the effect on the marginal probability of a single variable of any boundary condition decays as the distance grows. 
%A fascinating connection has been established between strong spatial mixing and the computational phase transition for sampling/counting on spin systems, disclosing that this form of correlation decay occurs up to the uniqueness regime~\cite{weitz06counting,sinclair12approximation,li2013correlation}, which is exactly the threshold up to which the problem of sampling/counting on spin systems stays tractable~\cite{Sly2012computational,galanis2016inapproximability}. Unfortunately, this ``weak/strong spatial mixing'' does not hold in the local lemma regime. This is generally due to the fact that the local- lemma-type condition is \emph{not self-reducible}, as pinning variables will lead to a degradation in the  condition~\cite{BGG19,Moi19}.

%Recent developments on the algorithmic side of the sampling LLL has revealed a special kind of correlation decay property in the local lemma regime. In Moitra's seminal work~\cite{Moi19}, an innovative algorithm was introduced for efficiently counting $k$-SAT solutions when $pD^{60}\lesssim 1$. The algorithm relies on a carefully-constructed coupling, which shows the influence of discrepancy at certain variables decays as the distance grows and only affects the distribution in a logarithmic-sized connected component with high probability under such a weakened local lemma condition. This property was also identified by ~\cite{guo2019counting} as a ``special kind of spatial mixing''. Though probably not formally defined or stated, this property was also used for analyzing the mixing time of Markov chains~\cite{FGYZ20,feng2021sampling,Vishesh21sampling,HSW21,galanis2022fast,chen2023from} or other algorithms~\cite{he2022sampling,he2022counting,he2023improved} for the sampling LLL.  Yet, in these works, this special kind of correlation decay has direct algorithmic implications, and it is natural to conjecture the correct threshold of the sampling LLL coincides with the existence of such kind of correlation decay.  This calls for a definition and characterization of the special form of correlation decay that occurs in the local lemma regime.

We consider a notion of correlation decay characterized through the \emph{pairwise influence matrix}, which was introduced in~\cite{anari2020spectral} for Boolean domains.
It has been used in many recent works on rapid mixing of Markov chains~\cite{anari2020spectral,chen2020rapid,chen2021optimal,CFYZ2021,anari2022entropic,CE22,CFYZ2022}, and was extended beyond the Boolean domains~\cite{chen2021rapid,feng2021rapid}. For any distribution $\mu$ over $[q]^V$, its pairwise influence matrix $\Psi_{\mu}$ is defined as in~\cite{feng2021rapid}:
\begin{equation}\label{eq:definition-influence}
\Psi_{\mu}(u,v)\defeq \max\limits_{i,j\in Q_u}\dtv(\mu^{u\gets i}_v,\mu^{u\gets j}_v),
\end{equation}
where for any $c\in Q_u$, we use %$\mu^{u\gets c}$ to denote the distribution $\mu$ conditioning on $u$ being fixed to $c$, and 
$\mu^{u\gets c}_v$ to denote the marginal distribution  induced from $\mu$ on $v$ conditioning on $u$ being fixed to $c$.
Thus $\Psi_{\mu}(u,v)$ gives the maximum influence on $v$ caused by a  disagreement on $u$. It holds that $\Psi_{\mu}(v,v)\defeq 0$ for any $v\in V$.
%We remark that this definition is not restricted to distributions defined over spin systems and can be directly applied to the uniform distribution over solutions to CSPs with boolean domains.

The induced $1$-norm/$\infty$-norm of the pairwise influence matrix correspond to the \emph{all-to-one/one-to-all total influence}, respectively:
\begin{equation*}
\Vert\Psi_{\mu}\Vert_1\defeq \max\limits_{v\in V}\sum\limits_{u\in V}\Psi_{\mu}(u,v)\quad\text{ and }\quad \Vert\Psi_{\mu}\Vert_{\infty}\defeq \max\limits_{u\in V}\sum\limits_{v\in V}\Psi_{\mu}(u,v).
\end{equation*}
The total influence is a standard tool for establishing the \emph{spectral independence}~\cite{anari2020spectral}, which holds if the maximum eigenvalue of the pairwise influence matrix $\lambda_{\max}(\Psi_{\mu^{\sigma}})$ is finitely bounded for all distributions $\mu^{\sigma}$ induced from $\mu$ by ``pinning'' arbitrary feasible partial assignments $\sigma\in[q]^{\Lambda}$ for $\Lambda\subseteq V$. 
Here, because of the non-self-reducibility of the local lemma regimes, we consider a weak version of spectral independence without pinning.
Specifically, we are interested in the locally bounded total influences, namely the total influences whose growth do not depend on $n=|V|$. 

%A distribution $\mu$ over $[q]^V$ is said to be \emph{spectrally independent}, if the maximum eigenvalue of the pairwise influence matrix $\lambda_{\max}(\Psi_{\mu^{\sigma}})$ is finitely bounded for all distributions $\mu^{\sigma}$ induced from $\mu$ by conditioning on arbitrary feasible partial assignment $\sigma\in[q]^{V}$ for $\Lambda\subseteq V$. The maximum eigenvalue of a pairwise influence matrix is  upper bounded by its 


%We say a quantity is locally bounded if it doesn't depend on the size of the variable set $|V|$. The local boundedness of all-to-one/one-to-all total influence can be viewed as a certain kind of correlation decay, roughly suggesting that the influence at any variable decays rapidly as the distance grows. We draw inspiration from this definition and use it to analyze the correlation decay in the local lemma regime. Since the local lemma regime is not self-reducible, it is reasonable only to consider the pairwise influence matrix for $\mu$, not all its induced conditional distributions. 



\subsection{Our results}

In this work, we characterize the condition under which the one-to-all total influence of distribution defined by atomic CSPs is locally bounded. We show that the local boundedness of the one-to-all total influence undergoes a phase transition under a local lemma regime that approaches $pD^2\lesssim 1$ as the minimum domain size grows, therefore providing evidence to a positive answer of~\eqref{eq:question}. 


We start by considering uniform distribution over CSP solutions.
Let $V$ be a set of $n$ mutually independent random variables, where each $v\in V$ corresponds to a value drawn uniformly from the domain $[q]$, where $q\ge 2$. 
Let $C$ be a collection of local constraints over $V$, such that each  $c\in C$ is a constraint function $c:[q]^{\vbl(c)}\to\{\True,\False\}$ defined on a subset $\vbl(c)\subseteq V$ of variables. 
A constraint is called satisfied by an assignment if it evaluates to $\True$ or violated otherwise. 
An assignment in $[q]^V$ is said to be satisfying if it satisfies all constraints. 
A CSP instance is represented by $\Phi=(V,[q],C)$. 
We denote the uniform distribution over all satisfying assignments by $\mu=\mu_{\Phi}$. 
%We are focused on the problem of sampling from $\mu=\mu_{\Phi}$,  the uniform distribution over all satisfying assignments. 
%We remark this assumption of identical domains and uniform distributions is quite common, and it includes many important subclasses such as $k$-SAT and hypergraph colorings.


A CSP instance is called \emph{atomic} if each constraint $c\in C$ is violated by exactly one configuration $\sigma_{c}\in [q]^{\vbl(c)}$, i.e., $|c^{-1}(\False)|=1$. The atomicity of constraints is common for many classical constraint satisfaction problems, including $k$-SAT, and is a natural assumption in previous studies of LLL~\cite{achlioptas2014random,HV15,kolmogorov2016commutativity,harris2017algorithmic,HS17,HS19,AIS19,harris2021oblivious,feng2021sampling,Vishesh21sampling,HSW21}.

Some key parameters for a  CSP $\Phi=(V,[q],C)$ are listed below. Let $\+P$ denote the uniform distribution over all possible assignments in $[q]^V$.




\begin{itemize}
    \item \emph{width} $k= \max\limits_{c\in C}\abs{ {\vbl}(c)}$;
%    \item \emph{variable degree} $d=d_\Phi\triangleq\max\limits_{v\in V}\vert \{c\in \+{C}\vert v\in \text{vbl}(c) \}\vert$;
    \item \emph{dependency degree} $D=\max_{c\in C}\{c'\in C\setminus \{c\}\mid \var(c)\cap \var(c')\neq \emptyset\}$
    %\footnote{Here $\Delta$ is one plus the maximum degree of the dependency graph of $\Phi$ since $c\in \{c'\in \+{C}\vert \text{vbl}(c)\cap\text{vbl}(c')\neq\emptyset\}$.}
    \item \emph{maximum violation probability} $p=\max\limits_{c\in C}\Pr[\+P]{\neg c}$
   
\end{itemize}


%We safely assume that $q\geq 2$ and $k,D\geq 1$. 
%We also need to specify the definition of pairwise influence when each variable is not restricted to boolean domains, which varies in different works~\cite{feng2021rapid,chen2021rapid}.  Let $\mu$ be a probability distribution over all assignments $\+Q=\bigotimes_{v\in V}Q_v$ such that $\mu(v=i)>0$ for any $v\in V$ and any $i\in Q_v$. For any two distinct variables $u,v\in V$, the following definition of the pairwise influence $\Psi_\mu(u,v)$ of $u$ on $v$ is from ~\cite{feng2021rapid}:





%We prove that under certain LLL regimes, the one-to-all total influence of the distribution defined by identical uniform atomic CSPs can be locally bounded.

The following main theorem bounds the one-to-all total influence of the uniform distribution over all CSP solutions under an LLL condition $pD^{2+o_q(1)}\lesssim 1$.

\begin{theorem}[locally bounded total influence of uniform atomic CSP]\label{thm:correlation-decay-uniform-distribution}
Let $\Phi=(V,[q],C)$ be an atomic CSP instance satisfying
\begin{equation*}
    60q^3\cdot p\cdot (D+1)^{2+\zeta}\leq 1,
\end{equation*}
where
\[
\zeta %= \frac{2}{2-\frac{\ln{(2q-1)}}{\ln{q}}}
=\frac{2\ln(2-1/q)}{\ln q-\ln(2-1/q)}.
\]
Then it holds for the uniform distribution $\mu=\mu_{\Phi}$ over all satisfying assignment of $\Phi$ that 
\[
\Vert\Psi_{\mu}\Vert_{\infty}\leq k(D+1)^2.
\] 
\end{theorem}

Note that $\zeta$ approaches zero and the regime in \Cref{thm:correlation-decay-uniform-distribution} approaches $pD^2\lesssim 1$ as $q$ grows to infinity, which matches the hardness result for sampling LLL~\cite{BGG19,galanis2021inapproximability}.  We also have $\zeta \approx 2.819$ and $pD^{4.819}\lesssim 1$ for the Boolean domain case, which is slightly better than the current best regime $pD^5\lesssim 1$ for sampling algorithm~\cite{he2022counting}. We further remark that large $q$ does not trivialize the problem, as the current best bound for hypergraph colorings is $pD^3\lesssim 1$ for arbitrarily large $q$~\cite{Vishesh21sampling, HSW21}.  



Our results also apply to the more general setting where the independent random variables are generally distributed.  
Each random variable $v\in V$ is endowed with 
 a finite domain $Q_v$ with $\abs{Q_v}\ge 2$ and a probability distribution $\+D_v$ over $Q_v$. By interpreting the violation of each constraint as a bad event, such an instance $\Phi=(V,(Q_v,\+D_v)_{v\in V},C)$ naturally specifies a distribution $\mu= \mu_{\Phi}$ over all satisfying assignments induced by the product distribution $\+P\defeq \prod_{v\in V}\+D_v$ conditioning on that all constraints are satisfied. This distribution $\mu$ is called the \emph{LLL distribution}~\cite{harris2020new}. 

 
 We additionally specify the following parameters for the CSP $\Phi=(V,(\+D_v,Q_v)_{v\in V},C)$ under this more general setting:
\begin{itemize}
\item \emph{minimum distortion} $\chi_{\min}=\min\limits_{v\in V}\min\limits_{x\in Q_v}{\+D_v(x)}^{-1}$  
  \item \emph{maximum distortion} $\chi_{\max}=\max\limits_{v\in V}\max\limits_{x\in Q_v}{\+D_v(x)}^{-1}$  
\end{itemize}

The next theorem generalizes \Cref{thm:correlation-decay-uniform-distribution} to this generally distributed random variables setting.
  
\begin{theorem}[locally bounded total influence of general atomic CSP]\label{thm:correlation-decay}
Let $\Phi=(V,(Q_v,\+D_v)_{v\in V},C)$ be an atomic CSP instance satisfying
\begin{equation}\label{eq:correlation-decay-condition}
    (2\mathrm{e})^{1+\frac{\zeta}{2}}\cdot \chi_{\max}^3\cdot p\cdot (D+1)^{2+\zeta} \leq 1,
\end{equation}
where
\[
\zeta= \frac{2}{2-\frac{\ln{(2\chi_{\min}-1)}}{\ln{\chi_{\min}}}}
=2+\frac{2\ln(2-1/\chi_{\min})}{\ln\chi_{\min}-\ln(2-1/\chi_{\min})}.
\]
Then it holds for the LLL distribution $\mu=\mu_\Phi$ that \[
\Vert\Psi_{\mu}\Vert_{\infty}\leq k(D+1)^2.
\] 
\end{theorem}

It is not hard to verify that $\chi_{\min}=\chi_{\max}=q$ for $\Phi=(V,[q],C)$ with uniform random variables, therefore  \Cref{thm:correlation-decay-uniform-distribution} immediately follows from \Cref{thm:correlation-decay} as $q\geq 2$. Similar to the case of \Cref{thm:correlation-decay-uniform-distribution}, the regime in \Cref{thm:correlation-decay} approaches $pD^2\lesssim 1$ as $\chi_{\text{min}}$ grows to infinity.


\begin{remark}
Note that there is a lower-order factor $\chi_{\max}$ in \eqref{eq:correlation-decay-condition}, which appears mainly because of an artifact, namely the choice of ``one-to-all total influence'' as our form of correlation decay property: 
Pairwise influence is defined by the influence on the distribution after fixing the value of a certain variable, and fixing a value will lead to a degradation of $\chi_{\max}$ in the local lemma condition due to its non-self-reducibility. 
We also have another $\chi_{\max}^2$ degradation when transforming the correlation decay property in the form of ``a locally bounded coupling'' into ``locally bounded one-to-all total influence'', as we will see later in \Cref{cor:disc-upper-infl}. In \Cref{lemma:couple-dis}, we show that another form of correlation decay occurs under a regime without the lower-order factor $\chi_{\max}$ when we only need to analyze the influence on the distribution when some constraint is added/removed.
\end{remark}

We also prove a lower bound that, even for arbitrarily large $\chi_{\min}$, the one-to-all total influence can be unbounded locally when $pD^2\gtrsim 1$. 


\begin{theorem}[lower bound for total influence of atomic CSP]\label{thm:lower-bound}
For any real $\delta>1$, there exists $D(\delta)\geq 2$, such that when $D>D(\delta)$ and
\begin{equation*}
    pD^2\geq 4,
\end{equation*}
$\Vert\Psi_{\mu}\Vert_{\infty}$ is locally unbounded for distribution $\mu$ defined by atomic CSPs with  $\chi_{\min}\geq \delta$.
\end{theorem}

Note that \Cref{thm:correlation-decay} and \Cref{thm:lower-bound} together show a phase transition of the locally bounded one-to-all total influence of the LLL distributions of atomic CSPs at the critical threshold $pD^2\lesssim 1$, matching the previous hardness result on sampling LLL~\cite{BGG19,galanis2021inapproximability}. 
%Combining with 
%\begin{enumerate}
%    \item the prevalence of correlation-decay property in recent algorithmic results for the sampling LLL;
%    \item the deep connection between the correlation-decay property and the computational hardness in the literature of counting and sampling,
%\end{enumerate}
%we are tempted to produce the following conjecture:

%\begin{conjecture}
%The problem of sampling from distributions $\mu$ defined by atomic CSPs undergoes a computational phase transition at the  $pD^2\lesssim 1$ regime, under/beyond which the one-to-all total influence of $\mu$ is locally bounded/unbounded.     
%\end{conjecture}


\subsection{Technique overview}

A particular technical highlight in our method of proving \Cref{thm:correlation-decay} is that we dispense with Beck's ``freezing'' paradigm~\cite{beck1991algorithmic}, which is a substantial departure from previous works of the sampling LLL and seems to be the key of achieving the $pD^2\lesssim 1$ regime. Beck's ``freezing'' paradigm was introduced to deal with the non-self-reducibility of the local lemma condition. First applied in the algorithmic LLL, this ``freezing'' paradigm eventually leads to a non-optimal $pD^4\lesssim 1$ regime~\cite{Sri08} after a long line of improvements~\cite{Alon91,MR98,Sri08}, with the additional $D^3$ slackness from the use of a certain structure named $\{2,3\}$-trees.  In comparison, the subsequent breakthrough by Moser and Tardos, which achieves the optimal $pD\lesssim 1$ regime for the constructive local lemma, completely dispenses with this method. On the side of the sampling LLL, Beck's technique is also highly prevalent. In Moitra's seminal work~\cite{Moi19} for counting $k$-SAT solutions, the idea of ``mark/unmark'' was introduced. The heart of this idea was to turn slackness into a worst-case local lemma condition that preserves under arbitrary pinning and can be viewed as a static version of the ``freezing'' paradigm. Even though this static version of ``freezing'' made possible several Markov chain Monte Carlo approaches~\cite{FGYZ20,feng2021sampling,Vishesh21sampling,HSW21,galanis2022fast,chen2023from} where fast sampling takes place,  the current best regime $pD^5\lesssim 1$ for sampling general CSPs still applies the idea of ``freezing'', and the slackness is from the use of a structure called generalized $\{2,3\}$-trees~\cite{he2022counting}, which is a very similar source as in ~\cite{Sri08}.  The above literature suggests that the current direction of the sampling LLL based on Beck's technique cannot lead to the optimal threshold from the algorithmic side. In contrast, our method for showing the correlation decay property in a local lemma regime of $pD^2\lesssim 1$ is unaccompanied by Beck's technique, and we deal with non-self-reducibility simply by projecting back all possibilities of our coupling algorithm onto independent samples from the original local lemma distribution.


\begin{comment}
\textbf{Challenges towards the sharp threshold of the sampling LLL}\,\,
Despite the current gap in the exponent of the current upper bound and lower bound of the conditions for the sampling LLL is arguably small, new ideas and techniques are probably required to close such a gap from the algorithmic side. We will review the techniques used in the existing algorithms for the sampling LLL, and compare them with prior developments in the algorithmic LLL to show why this is the case from a technical point of view.

A major distinction between distributions under local lemma conditions and other well-studied distributions, such as those defined over spin systems, is that the local lemma condition is \emph{not self-reducible}. Pinning a variable to take some certain value will cause a degradation in the local lemma condition. To circumvent this issue, Beck~\cite{beck1991algorithmic} cleverly introduced the technique of ``freezing'' constraints with a high violation probability, to guarantee a weakened worst-case local lemma condition under certain pinnings. Given enough slackness in the required local lemma condition, this technique made it much easier to manipulate distributions under the local lemma regime. However, after a long line of improvement of this method  for the algorithmic LLL ~\cite{Alon91,MR98,Sri08}, it eventually leads to a non-optimal $pD^4\lesssim 1$ regime~\cite{Sri08}, with the additional $D^3$ slackness from the use of a certain structure named $\{2,3\}$-trees. In comparison, The subsequent breakthrough by Moser and Tardos, which achieves the optimal $pD\lesssim 1$ regime for the algorithmic lemma, completely dispenses with this method. 

On the side of the sampling LLL, Beck's technique is also highly prevalent. In Moitra's seminal work~\cite{Moi19} for counting $k$-SAT solutions, the paradigm of ``mark/unmark'' was introduced. The heart of this paradigm was also to turn slackness in the local lemma condition into a worst-case local lemma condition that preserves under arbitrary pinning and can be viewed as a static version of the ``freezing'' technique. Even though this static version of ``freezing'' made possible several Markov chain Monte Carlo approaches~\cite{FGYZ20,feng2021sampling,Vishesh21sampling,HSW21,galanis2022fast,chen2023from} where fast sampling takes place,  the best regime $pD^5\lesssim 1$ for general CSPs achieved so far still applies the idea of ``freezing''~\cite{he2022counting}, and the slackness is from the use of a structure called ``generalized'' $\{2,3\}$-trees, which is a very similar source as in ~\cite{Sri08}.  The above literature strongly suggests that the current direction of sampling LLL based on Beck's technique cannot lead to the optimal threshold from the algorithmic side and hence poses a great challenge for this problem. New ideas are needed to resolve this problem.

Confronted with such a great challenge, our work, instead of directly tackling the problem from the algorithmic side, takes a detour and focuses on the correlation decay phenomenon that is considered to be highly relevant to the existence of efficient algorithms. We show that there exists a phase transition of the boundedness of a certain quantity called ``one-to-all total influence'', which is recognized as a special form of correlation decay, at the hardness threshold $pD^2\lesssim 1$. This provides strong evidence that the correct threshold for tractability of sampling LLL happens exactly at $pD^2\lesssim 1$, where the correlation decay property occurs/vanishes. Our method completely dispenses with Beck's technique and hence achieves such a bound.
Before formally presenting our results, we briefly introduce the correlation decay property in the local lemma regime.
% A major difficulty that was posed to uniformly sampling CSP solutions is that, even under the LLL regime, the solution space may be disconnected. Currently, all existing algorithmic results  
\end{comment}


\begin{comment}
Aside from providing strong evidence towards a positive answer of \eqref{eq:question}, our work may also of technical significance. Prior approaches~\cite{Moi19,guo2019counting,FGYZ20,feng2021sampling,Vishesh21towards,HSW21,he2022sampling,he2022counting} towards the sampling LLL exploits variations of Beck's ``freezing'' technique~\cite{beck1991algorithmic} from the algorithmic LLL to deal with the non-self-reducibility in the local lemma regime. This technique, however, possibly encounters a technical bottleneck towards the optimal threshold as it introduces additional slackness.  In the algorithmic LLL, Beck's technique eventually leads to a non-optimal $pD^4\lesssim 1$ regime~\cite{Sri08}. In comparison, the subsequent breakthrough by Moser and Tardos, which achieves the optimal $pD\lesssim 1$ regime for the constructive local lemma, completely dispenses with this method.  Our method for analyzing the correlation decay property is also unaccompanied by Beck's technique, and we deal with non-self-reducibility by projecting back all possibilities of our coupling algorithm onto independent samples from the original local lemma distribution with no pinnings. 
\end{comment}

We then outline our method. To bound the one-to-all total influence in the local lemma regime, it suffices to construct a coupling $\+C:(X, Y)$ between the two distributions $\mu^{u\gets i}$ and $\mu^{u\gets j}$ for each $u\in V$ and $i,j\in Q_u$ such that 
\[
\E[\+C]{d_{\textrm{Ham}}(X,Y)}=O_{\abs{V}}(1),
\]

Let $S, T\subseteq C$ be the set of (simplified) constraints not satisfied conditioning on fixing $u$ to $i$ and $j$, respectively. What we are trying to couple is the two induced distributions $\+P_{V\setminus \{u\}}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)$ and  $\+P_{V\setminus \{u\}}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$ over the subset of variables $U=V\setminus \{u\}$. 

The idea is as follows. If $S=T$, the two distributions are identical, and we can couple them perfectly. Otherwise, without loss of generality, we can assume $T\not\subseteq S$, or we can swap $S$ and $T$. We choose an arbitrary constraint $c^*\in T\setminus S$ and try adding $c^*$ to $S$ to reduce the discrepancy between $S$ and $T$. We decompose the first distribution using the chain rule 
\begin{equation}\label{eq:decompose-S}
\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)=\Pr[\+P]{ c^*\mid \bigwedge\limits_{c\in S} c}\cdot \+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S\cup \{c^*\}} c\right)+\Pr[\+P]{\neg c^*\mid \bigwedge\limits_{c\in S} c}\cdot \+P_{U}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right),
\end{equation}

and write the second distribution as follows.
\begin{equation}\label{eq:decompose-T}
\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)=\Pr[\+P]{ c^*\mid \bigwedge\limits_{c\in S} c}\cdot \+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)+\Pr[\+P]{\neg c^*\mid \bigwedge\limits_{c\in S} c}\cdot \+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right).
\end{equation}

\sloppy
Therefore, with probability $p_{1}=\Pr[\+P]{\neg c^*\mid \bigwedge\limits_{c\in S} c}$, we can reduce our problem to coupling $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S\cup \{c^*\}} c\right)$ and $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$, as desired, at a cost of needing to couple $\+P_{U}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right)$ and $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$ with probability $1-p_1$, which is indeed much more trickier to handle. In this case, we further decompose the two distributions by the chain rule: we ``discard" the discrepancy on the variable set $\vbl(c^*)$, sample $X_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right)$ and $Y_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$. It then suffices to finish the coupling by recursively on the remaining variables and constraints.

This recursively-constructed coupling may be reminiscent of the recursive coupling technique that Goldberg, Martin, and Mike used to prove strong spatial mixing results of graph coloring for lattice graphs~\cite{goldberg2005strong}. However, there is a major difference between our recursive coupling procedure and that of Goldberg et al., generally because the LLL regime is not self-reducible. In ~\cite{goldberg2005strong}, self-reducibility plays a great role in the design of the recursive coupling so that in each recursive step, after assigning value to some variables,  one is faced with the same problem of coupling two distributions of the graph coloring problem with distinct boundary configurations, which allows one to use path coupling technique~\cite{Bubley97pathcoupling:} and only analyze a one-step worst-case contraction result. However, 
in local lemma regimes, self-reducibility is not at our disposal, and the condition degrades after assigning value to some variables, so we keep track of the whole execution of the algorithm and apply a percolation-style analysis.  An intriguing finding is that the random choices involved in this recursively-constructed coupling procedure can be projected onto two independent samples from the two distributions $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)$, $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$, which greatly simplifies the analysis and may be of independent interest.

We use reductions from hardcore distributions to prove the lower bound. Results in ~\cite{Sly2010computation, galanis2016inapproximability} indicates that the one-to-all total influence is unbounded on the infinite $\Delta$-regular tree in the non-uniqueness regime. Based on this result, we construct a class of Gibbs distributions on finite graphs where the one-to-all total influence is locally unbounded. We then derive the lower bound by interpreting the constructed Gibbs distribution as a distribution defined by atomic CSPs.

\subsection{Organization} The organization of the paper is as follows.

In \Cref{sec:prelim}, we introduce some preliminaries and notations. 

In \Cref{sec:alg}, we present the coupling procedure for proving the upper bound result (\Cref{thm:correlation-decay}) and show its correctness. 

In \Cref{sec:discrepancy}, we analyze the discrepancy produced by the coupling procedure and formally prove \Cref{thm:correlation-decay-uniform-distribution} and \Cref{thm:correlation-decay}. 

In \Cref{sec:lower-bound}, we prove the lower bound result (\Cref{thm:lower-bound}).

In \Cref{sec:conclusions}, we summarize the contributions of the paper and discuss some possible future directions. 
\newpage
\section{Preliminaries and notations}\label{sec:prelim}

\subsection{CSP formulas defined by atomic bad events}
A CSP is described by a collection of constraints defined on a set of variables. 
%
Formally, an instance of a constraint satisfaction problem, 
called a \emph{CSP formula}, 
is denoted by $\Phi=(V,(\+D_v,Q_v)_{v\in V},C)$.
Here, $V$ is a set of $n=|V|$ random variables, where each random variable $v\in V$ is endowed with 
 a  finite domain $Q_v$ of size $q_v\triangleq\abs{Q_v}\ge 2$ and a probability distribution $\+D_v$ over $Q_v$;
and $C$ gives a collection of local constraints, 
such that each  $c\in C$ is a constraint function $c:\bigotimes_{v\in \vbl(c)}Q_v\to\{\True,\False\}$ defined on a subset of variables, denoted by $\vbl(c)\subseteq V$.
%
An assignment $\={x}\in \+Q$ is called \emph{satisfying} for $\Phi$ if %$\Phi(\={x})=\True$, where
\[
\Phi(\={x})\triangleq\bigwedge\limits_{c\in C} c\left(\={x}_{\vbl(c)}\right)=\True.
\]
%where $\={x}_{\vbl(c)}$ denotes the restriction of $\={x}$ on $\vbl(c)$.
%
%
%We use $\mathbb{P}=\mathbb{P}_{\Phi}$ to denote the law for the uniform product distribution over $\+{Q}=\bigotimes_{v\in V}Q_v$.
%
%



 In the context of LLL, each constraint $c$ can be interpreted as a bad event  $A_c$, which happens when the assignment on $\var(c)$ violates $c$. We say a  CSP formula $\Phi=(V,C)$ is defined by \emph{atomic bad events}, or simply, \emph{atomic}, if each constraint $c\in C$ is violated by exactly one configuration $\sigma_{c}\in \bigotimes_{v\in \vbl(c)}Q_v$.

We use $\+P=\prod\limits_{v\in V}\+D_v$ to denote the product distribution over the space $\+Q\defeq\bigotimes\limits_{v\in V}Q_v$. For any subset of variables $S\subseteq V$, let $\+P_S$ denote the induced distribution of $\+P$ on $S$. For ease of notation, we assume the probability space is $\+P$ for the rest of the paper by default if without further specification. 
We say a set of constraints $S$ is satisfiable if $\Pr{ \bigwedge\limits_{c\in S}c}>0$.

Let $\mu=\mu_{\Phi}$ denote the distribution over all satisfying assignments of $\Phi$ induced  by $\+P$, i.e.
\[
    \mu_{\Phi}\defeq \+P\left(\cdot \mid \bigwedge\limits_{c\in C}c\right)
\]

For a subset of variables $\Lambda\subseteq V$ and an assignment $X\in \+Q_{\Lambda}\triangleq \bigotimes_{v\in \Lambda}Q_v$, the simplification of $\Phi=(V,C)$ under $X$, denoted by $\Phi^X=(V^X,C^X)$, 
is a new CSP formula such that $V^X=V\setminus \Lambda$, 
and the $C^X$ is obtained from $C$ by: 
\begin{enumerate}
\item
removing all the constraints that have already been satisfied
 by $X$;
 %\footnote{Recall that a constraint $c$ is satisfied by a partial assignment $\sigma$ if $c$ is satisfied by all full assignments that extend $\sigma$.} 
\item
for  the remaining constraints, %not yet satisfied by $\sigma$, 
replacing the variables $v\in \Lambda$ with their values $X(v)$.
\end{enumerate}
It is easy to see that $\mu_{\Phi^X}=\mu^{X}_{V\setminus\Lambda}$. Moreover, if $\Phi$ is atomic, then $\Phi^X$ is atomic, and each of the remaining constraints $c'\in C^{X}$ simplified from some constraint $c\in C$ can be uniquely determined by identifying the unassigned subset of variables $\var(c')=(V\setminus \Lambda)\cap \var(c)\subseteq \var(c)$. We say $C^{X}$ is the set of simplified constraints of $C$ under $X$. 



For any two assignments $X\in \+Q_{\Lambda},Y\in \+Q_{\Lambda'}$ defined over two disjoint subset of variables $\Lambda,\Lambda'\subseteq V$, we define $X\oplus Y\in \+Q_{\Lambda\cup \Lambda'}$ as the concatenation of $X$ and $Y$ such that for any $v\in \Lambda\cup \Lambda'$,
\[
(X\oplus Y)(v)=\begin{cases}X(v) & v\in \Lambda\\ Y(v)& v\in \Lambda'\end{cases}
\]




\subsection{Lov\'{a}sz Local lemma}
  The celebrated Lov\'{a}sz local lemma gives a sufficient criterion for a CSP solution to exist:



\begin{theorem}[\cite{LocalLemma}]\label{locallemma}
    Given a CSP formula $\Phi=(V,C)$, if the following holds
    %there is a function $x : \+{C} \rightarrow (0, 1) $ such that for any $c \in \+{C}$,\\
    \begin{align}\label{llleq}
    \exists x\in (0, 1)^{C}\quad \text{ s.t.}\quad \forall c \in C:\quad
        {\Pr{\neg c}\leq x(c)\prod_{\substack{c'\in C\setminus \{c\}\\ \var(c)\cap\var(c')\neq \emptyset}}(1-x(c'))},
    \end{align}
    then  
    $$
        {\Pr{ \bigwedge\limits_{c\in C} c}\geq \prod\limits_{c\in C}(1-x(c))>0},
    $$
\end{theorem}

When the condition \eqref{llleq} is satisfied, 
the probability of any event in the uniform distribution $\mu$ over all satisfying assignments can be well approximated by the probability of the event in the product distribution.
%

\begin{theorem}[\text{\cite[Theorem 2.1]{haeupler2011new}}]\label{HSS}
Given a CSP formula $\Phi=(V,C)$, if $\eqref{llleq}$ holds, 
then for any event $A$ that is determined by the assignment on a subset of variables $\var(A)\subseteq V$,
\[
   \Pr{A\mid \bigwedge\limits_{c\in C}  c}\leq \Pr{A}\prod_{\substack{c\in C\\ \var(c)\cap\var(A)\neq \emptyset}}(1-x(c))^{-1}.
\]
\end{theorem}







\subsection{Coupling and one-to-all total influence}

 Let $\mu$ and $\nu$ be two probability distributions
over the same state space $\Omega$. Their total variation distance is defined by

\[
\dtv(\mu,\nu)=\frac{1}{2}\sum\limits_{x\in \Omega}|\mu(x)-\nu(x)|,
\]

A coupling $\+C$ of two distributions $\mu$ and $\nu$ is a joint distribution over $\Omega\times \Omega$ whose projection on the first (or second)
coordinate is $\mu$ (or $\nu$). The well-known coupling lemma is given as follows.

\begin{lemma}[\text{\cite[Proposition 4.7]{levin2017markov}}]\label{lemma:coupling-lemma}
   Let $\+C:(X,Y)$ be any coupling of $\mu$ and $\nu$, then
   \[
   \dtv(\mu,\nu)\leq \Pr[\+C]{X\neq Y}
   \]
\end{lemma}



The following is a simple corollary of the coupling lemma.

\begin{corollary}[Expected discrepancy upper bounds one-to-all total influence]\label{cor:disc-upper-infl}
    Let $\Phi=(V,C)$ be a CSP formula.  Suppose there exists some value $A$, such that for any $u\in V$ and $i,j\in Q_u$ there is  a coupling $\+C^u_{i,j}$ of $\mu^{u\gets i}(\cdot)$ and $\mu^{u\gets j}(\cdot)$ satisfying that
    \[
        \E[\+C^u_{i,j}:(X,Y)]{d_{\textrm{Ham}}(X,Y)}\leq A,
    \]
    then
    \[
        \Vert\Psi_{\mu}\Vert_{\infty}\leq \chi_{\max}^2A,
    \]
where $d_{\textrm{Ham}}(X,Y)$ denotes the Hamming distance between two assignments $X,Y\in \Omega$.
\end{corollary}
\begin{proof}
\begin{equation*}
    \begin{aligned}
        &\Vert\Psi_{\mu}\Vert_{\infty}\\
        = &\max_{u\in V}\sum\limits_{v\in V\setminus \{u\}}\Psi_{\mu}(u,v)\\
        =&\max_{u\in V}\sum\limits_{v\in V\setminus \{u\}}\max\limits_{i,j\in Q_u}\dtv(\mu^{u\gets i}_v,\mu^{u\gets j}_v)\\
        \leq &\max_{u\in V}\sum\limits_{v\in V\setminus \{u\}}\sum\limits_{i,j\in Q_u}\dtv(\mu^{u\gets i}_v,\mu^{u\gets j}_v)\\
     (\text{by \Cref{lemma:coupling-lemma}})\quad   \leq&\max_{u\in V}\sum\limits_{v\in V\setminus \{u\}}\sum\limits_{i,j\in Q_u}\Pr[\+C^u_{i,j}:(X,Y)]{X(v)\neq Y(v)}\\
     =&\max_{u\in V}\sum\limits_{i,j\in Q_u}\E[\+C^u_{i,j}:(X,Y)]{d_{\textrm{Ham}}(X,Y)}\\
(\text{by }\chi_{\max}\geq |Q_u|)\quad     \leq& \chi_{\max}^2A.
    \end{aligned}
\end{equation*}
\end{proof}

\newpage
\section{The recursive coupling procedure}\label{sec:alg}


In this section, we present our recursive-constructed coupling procedure for bounding the one-to-all total influence in the local lemma regime. For ease of notation, throughout this section and the next section, i.e.,  \Cref{sec:discrepancy}, we fix a set of random variables $V$ where each random variable $v\in V$ is endowed with a finite domain $Q_v$ of size $q_v\triangleq\abs{Q_v}\ge 2$ and a probability distribution $\+D_v$ over $Q_v$.

Our coupling procedure  takes as input a subset of variables $U\subseteq V$, two sets of satisfiable atomic constraints $S,T$ defined over $U$, and outputs a pair of assignments $(X^{\+C},Y^{\+C})\in \+Q_U\defeq\bigotimes _{v\in V}Q_v$ distributed under a coupling of the two distributions $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)$ and $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$. We assume an arbitrary ordering on all constraints in $S\cup T$ and their (possible) simplifications. The coupling procedure is formally presented as \Cref{Alg:couple}. 



\begin{algorithm}[H]
\caption{$\+C(U,S,T)$} \label{Alg:couple}
  \SetKwInput{KwPar}{Parameter}
\KwIn {a subset of variables $U\subseteq V$, two sets of satisfiable atomic constraints $S,T$ defined over $U$} 
\KwOut{a pair of assignments $(X^{\+C},Y^{\+C})\in \+Q_U$}
\If{$S=T$\label{Line:couple-return-cond}}{
    \Return $(X^{\+C},Y^{\+C})$ distributed under the perfect coupling of $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)$ and  $\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$\;\label{Line:couple-return}
}
\If{$T\not\subseteq S$ \label{Line:couple-swap-cond}}{
   Choose the smallest $c^*\in T\setminus S$\; \label{Line:couple-choose}
Sample a random number $r\in [0,1]$ uniformly at random\; \label{Line:couple-rand}
Let $p_{\mathsf{sat}}=\Pr{c^*\mid \bigwedge\limits_{c\in S} c}$\;  \label{Line:couple-psat}
\If{$r<p_{\mathsf{sat}}$ \label{Line:couple-psat-cond}}{
    \Return $\+C(U,S\cup\{c^*\},T)$\; \label{Line:couple-psat-return}
}
\Else{
    Sample $X_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right)$ and $Y_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$\; \label{Line:couple-sample}
    Let the set of simplified constraints of $S$ under $X_{\vbl(c^*)}$ be $S^*$, and $T^*$ defined analogously for $T$ and $Y_{\vbl(c^*)}$\;  \label{Line:couple-simplify}
    $(X_{U\setminus \vbl(c^*)},Y_{U\setminus \vbl(c^*)})\gets \+C(U\setminus \vbl(c^*),S^*,T^*)$\; \label{Line:couple-assign}
    \Return $(X,Y)$\; \label{Line:couple-assign-return}
}
}
\Else{
Choose the smallest $c^*\in S\setminus T$ \; \label{Line:couple-choose-1}
Sample a random number $r\in [0,1]$ uniformly at random\; \label{Line:couple-rand-1}
Let $p_{\mathsf{sat}}=\Pr{c^*\mid \bigwedge\limits_{c\in T} c}$\;  \label{Line:couple-psat-1}
\If{$r<p_{\mathsf{sat}}$ \label{Line:couple-psat-cond-1}}{
    \Return $\+C(U,S,T\cup \{c^*\})$\; \label{Line:couple-psat-return-1}
}
\Else{
    Sample $X_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in S} c\right)$ and $Y_{\vbl(c^*)}\sim \+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in T} c\right)\land \neg c^*\right)$\; \label{Line:couple-sample-1}
    Let the set of simplified constraints of $S$ under $X_{\vbl(c^*)}$ be $S^*$, and $T^*$ defined analogously for $T$ and $Y_{\vbl(c^*)}$\;  \label{Line:couple-simplify-1}
    $(X_{U\setminus \vbl(c^*)},Y_{U\setminus \vbl(c^*)})\gets \+C(U\setminus \vbl(c^*),S^*,T^*)$\; \label{Line:couple-assign-1}
    \Return $(X,Y)$\; \label{Line:couple-assign-return-1}
}
}


\end{algorithm}

\Cref{Alg:couple} implements the algorithm outlined in the technique overview. It tries to reduce the discrepancy of the two sets of constraint $S,T$ by decomposing the two distributions as described in \eqref{eq:decompose-S} and \eqref{eq:decompose-T}, or in the other way around when $T\subseteq S$. Therefore, with certain marginal probability over some chosen constraint $c^*$, this trial succeeds, the discrepancy between the two sets of constraints is reduced by one, and we couple them recursively, as in \Cref{Line:couple-psat-return} and \Cref{Line:couple-psat-return-1}; or the trial fails, and we need to compensate it by sampling the values of all variables on $c^*$ for both distributions and still need to couple the distributions conditioning on the sampled values by recursive calls as in \Cref{Line:couple-assign-return} and \Cref{Line:couple-assign-return-1}. This recursive procedure finally ends when the two sets of constraints become the same, which means the distributions they represent can be perfectly coupled, as in \Cref{Line:couple-return}.

We remark that \Cref{Alg:couple} only serves the purpose of analyzing the correlation decay property in the local lemma regime and cannot be realized efficiently, as the algorithm involves estimating/sampling from some nontrivial marginal distributions. 

For the rest of this section, we show the correctness of \Cref{Alg:couple} through the following lemma. 
 
 \begin{lemma}\label{lemma:couple-cor}
 Given a subset of variables $U\subseteq V$, and two sets of satisfiable atomic constraints $S,T$ defined over $U$. $\+C(U,S,T)$ terminates with probability $1$ and returns a pair of assignments $(X^{\+C},Y^{\+C})$ such that $X^{\+C}\sim \+P_U\left(\cdot\mid \bigwedge\limits_ {c\in S} c\right)$ and $Y^{\+C}\sim \+P_U\left(\cdot\mid \bigwedge\limits_{c\in T} c\right)$.
 \end{lemma}
\begin{proof}

Let the set of all possible simplified constraints from $C$ be $C^{\text{si}}$. We define the following binary relation $<_{\+C}:(2^V,2^{C^{\text{si}}},2^{C^{\text{si}}})^2\to \{0,1\}$ such that $(U,S,T)<_{\+C} (U',S',T')$ if and only if one of the following holds:
\begin{enumerate}[label=(\alph*)]
    \item $\abs{U}< \abs{U'}$,  \label{couple-cor-a}
    \item $\abs{U}=\abs{U'}$ and $\abs{S}+\abs{T}>\abs{S'}+\abs{T'}$,  \label{couple-cor-b}
\end{enumerate}

It is straightforward to verify that $<_{\+C}$ is a strict total order on  $(2^V,2^{C^{\text{si}}},2^{C^{\text{si}}})$. We then claim that for all recursive calls $\+C(U',S',T')$ of $\+C(U,S,T)$, it must follow that $(U',S',T')<_{\+C} (U,S,T)$. It suffices to prove the claim for recursive calls at Lines \ref{Line:couple-psat-return} and \ref{Line:couple-assign-return}. The recursive calls at Lines \ref{Line:couple-psat-return-1} and \ref{Line:couple-assign-return-1} follow analogously.
\begin{itemize}
    \item For the recursive call at \Cref{Line:couple-psat-return}, we have $U'=U$, $S'=S\cup \{c^*\}$ and $T'=T$ for some constraint $c^*$. This shows that $\abs{U'}=\abs{U}$ and $\abs{S'}+\abs{T'}>\abs{S}+\abs{T}$. Therefore \Cref{couple-cor-b} is satisfied and we have  $(U',S',T')<_{\+C} (U,S,T)$.
    \item For the recursive call at \Cref{Line:couple-assign-return}, we have $U'=U\setminus \vbl(c^*)$  for some constraint $c^*$ defined over $U$ and hence $\abs{U'}=\abs{U\setminus \vbl(c^*)}<\abs{U}$. Therefore \Cref{couple-cor-a} is satisfied and we have  $(U',S',T')<_{\+C} (U,S,T)$.
\end{itemize}
The claim is proved.

Note that the set $(2^V,2^{C^{\text{si}}},2^{C^{\text{si}}})$ is finite. We then induct on this strict total order $<_{\+C}$  to show the lemma holds.

The base case is when $(U,S,T)$ is a minimal element defined on $(2^V,2^{C^{\text{si}}},2^{C^{\text{si}}})$ with respect to $<_{\+C}$, that is, $U=\emptyset,S=T=\emptyset$, then the condition at \Cref{Line:couple-return-cond} is satisfied and a pair of empty assignments is directly returned. In this case, the lemma holds by convention.

For the induction step, we have the lemma satisfied for all $(U',S',T')\in (2^V,2^{C^{\text{si}}},2^{C^{\text{si}}})$ such that $(U',S',T')<_{\+C}(U,S,T)$. By $S$ and $T$ are satisfiable we have the two distributions $ \+P_U(\cdot\mid \bigwedge\limits_ {c\in S} c)$ and $\+P_U(\cdot\mid \bigwedge\limits_{c\in T} c)$ are well-defined. We verify several cases  as follows:
\begin{itemize}
\item  If $S=T$, then the condition at \Cref{Line:couple-return-cond} is satisfied. In this case $\+C(U,S,T)$ terminates at \Cref{Line:couple-return}, no recursive call is incurred and ${\+C} (U',S',T')$ terminates with probability $1$. We also have  $X^{\+C}\sim \+P_U(\cdot\mid \bigwedge\limits_ {c\in S} c)$ and $Y^{\+C}\sim \+P_U(\cdot\mid \bigwedge\limits_{c\in T} c)$ immediately by \Cref{Line:couple-return},
\item Otherwise, we only prove the case when $T\not\subseteq S$. The case when $T\subseteq S$ follows analogously. In this case the conditions at  \Cref{Line:couple-return-cond} and \Cref{Line:couple-swap-cond} are not satisfied. Let $c^*\in T\setminus S$ be the constraint chosen at \Cref{Line:couple-choose}. Let $p_{\mathsf{sat}}=\Pr{c^*\mid \bigwedge\limits_{c\in S} c}$ as defined in \Cref{Line:couple-psat}. From Lines \ref{Line:couple-choose}-\ref{Line:couple-assign-return}, we have the following two cases:
\begin{itemize}
\item With probability $p_{\mathsf{sat}}$, the condition at \Cref{Line:couple-psat-cond} is satisfied. In this case $p_{\mathsf{sat}}>0$ and $\+C(U,S,T)$ terminates at \Cref{Line:couple-psat-return}, with a recursive call of $\+C(U,S\cup \{c^*\},T)$ at \Cref{Line:couple-psat-return}. By assumption on the input we have both $S$ and $T$ are satisfiable, therefore $\Pr{ \bigwedge\limits_{c\in S}c}>0$. By $p_{\mathsf{sat}}=\Pr{c^*\mid \bigwedge\limits_{c\in S} c}>0$, we have 
\[
\Pr{ \bigwedge\limits_{c\in S\cup \{c^*\}}c}=\Pr{\bigwedge\limits_{c\in S}c}\cdot \Pr{ c^*\mid \bigwedge\limits_{c\in S} c}>0,
\]
hence $S\cup \{c^*\}$ is also satisfiable, and the input condition is satisfied by the tuple $(U,S\cup \{c^*\},T)$. By the induction hypothesis, we have $\+C(U,S\cup \{c^*\},T)$ terminates with probability $1$, and the pair of assignments $(X_1,Y_1)$ returned by $\+C(U,S\cup \{c^*\},T)$ at \Cref{Line:couple-psat-return} follows the distribution
\[X_1\sim \+P_U\left(\cdot\mid \bigwedge\limits_ {c\in S\cup \{c^*\}} c\right),\quad Y_1\sim \+P_U\left(\cdot\mid \bigwedge\limits_{c\in T} c\right).\]

\item With probability $1-p_{\mathsf{sat}}$, the condition at \Cref{Line:couple-psat-cond} is not satisfied. In this case $p_{\mathsf{sat}}<1$ and $\+C(U,S,T)$ terminates at \Cref{Line:couple-assign-return}, with a recursive call of $\+C(U\setminus \var(c^*),S^*,T^*)$ at \Cref{Line:couple-assign}.  Here by Lines \ref{Line:couple-sample}-\ref{Line:couple-assign} we have $S^*$ is the set of simplified constraints of $S$ under $X_{\vbl(c^*)}$, with $X_{\vbl(c^*)}$ sampled according to $\+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right)$. $T^*$ is defined as the set of simplified constraints of $T$ under $Y_{\vbl(c^*)}$, with $Y_{\vbl(c^*)}$ sampled according to $\+P_{\vbl(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in T} c\right)$. 

\sloppy
We claim that for each possible recursive call of $\+C(U\setminus \var(c^*),S^*,T^*)$, both $S^*$ and $T^*$ are satisfiable. Note that for each possible outcome of $X_{\var(c^*)}$, it must hold that $c^*$ is not satisfied by $X_{\var(c^*)}$. Fix any possible outcome $\hat{X} \in \+Q_{\vbl(c^*)}$ of $X_{\var(c^*)}$. Let $S^*$ be the set of simplified constraints of $S$ under $\hat{X} $. Note that by $S$ is satisfiable and $p_{\text{sat}}<1$ we have $\Pr{\left(\bigwedge \limits_{c\in S}c\right)\land \neg c^*}>0$. Combining with $X_{\vbl(c^*)}$ is sampled according to $\+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in S} c\right)\land \neg c^*\right)$, we have $\Pr{\left(\bigwedge \limits_{c\in S}c\right)\land \neg c^*\land \hat{X}}>0$.
Therefore
\begin{align*}
&\Pr{\bigwedge \limits_{c\in S^*}c}\\
(\text{by $S^*$ is simplified from $S$ under $\hat{X}$})\quad=&\Pr{\bigwedge \limits_{c\in S}c\mid \hat{X} }\\
 (\text{by $c^*$ is not satisfied by $\hat{X}$})\quad=&\Pr{\left(\bigwedge \limits_{c\in S}c\right)\land \neg c^*\mid \hat{X} }\\
 (\text{by chain rule})\quad=&\frac{\Pr{\left(\bigwedge \limits_{c\in S}c\right)\land \neg c^*\land \hat{X} }}{\Pr{\hat{X} }}>0.
\end{align*}
Hence $S^*$ is satisfiable. Similarly, for each possible outcome of $Y_{\var(c^*)}$, it must hold that $c^*$ is satisfied by $Y_{\var(c^*)}$. Fix any possible outcome $\hat{Y} \in \+Q_{\vbl(c^*)}$ of $Y_{\var(c^*)}$. Let $T^*$ be the set of simplified constraints of $T$ under $\hat{Y}$. Note that by $T$ is satisfiable we have $\Pr{\bigwedge \limits_{c\in T}c}>0$. Combining with $Y_{\vbl(c^*)}$ is sampled according to $\+P_{\vbl(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in T} c\right)\right)$ we have $\Pr{\left(\bigwedge \limits_{c\in T}c\right)\land \neg c^*\land \hat{Y}}>0$.
Therefore
\begin{align*}
&\Pr{\bigwedge \limits_{c\in T^*}c}\\
(\text{by $T^*$ is simplified from $T$ under $\hat{Y}$})\quad=&\Pr{\bigwedge \limits_{c\in T}c\mid \hat{Y}}\\
 (\text{by chain rule})\quad=&\frac{\Pr{\left(\bigwedge \limits_{c\in T}c\right)\land \hat{Y}}}{\Pr{{\hat{Y}}}}>0.
\end{align*}
Hence $T^*$ is also satisfiable, and the claim is proved.
Therefore, by the induction hypothesis, we have $\+C(U\setminus \var(c^*),S^*,T^*)$ terminates with probability $1$. It then immediately follows that $\+C(U,S, T)$ terminates with probability $1$ as all other steps take a finite time.   Let $(X',Y')$be the pair of assignments returned by $\+C(U\setminus \var(c^*),S^*,T^*)$, then we have
\[
X'\sim \+P_{U\setminus \var(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in S^*}c\right)=\+P_{U\setminus \var(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in S}c\right)\land \neg c^*\land X^*_{\vbl(c)}\right)
\]
and
\[
Y'\sim \+P_{U\setminus \var(c^*)}\left(\cdot \mid \bigwedge\limits_{c\in T^*}c\right)=\+P_{U\setminus \var(c^*)}\left(\cdot \mid \left(\bigwedge\limits_{c\in T}c\right)\land Y^*_{\vbl(c)}\right),
\]
Hence by chain rule, in this case, the pair of assignments $(X_2,Y_2)$ returned at \Cref{Line:couple-assign-return} follows the distribution 
\[X_2\sim \+P_{U}\left(\cdot \mid \left(\bigwedge\limits_{c\in S}c\right)\land \neg c^*\right), \quad Y_2\sim \+P_{U}\left(\cdot \mid\bigwedge\limits_{c\in T}c\right).\]
\end{itemize}
Combining the two possibilities of whether $r<p_{\text{sat}}$, the pair of assignments $(X,Y)$ returned at \Cref{Line:couple-assign-return} follows the distribution 
\begin{equation*}
    \begin{aligned}
    X\sim & p_{\text{sat}}\cdot \+P_U\left(\cdot\mid \bigwedge\limits_ {c\in S\cup \{c^*\}} c\right)+(1-p_{\text{sat}})\+P_{U}\left(\cdot \mid \left(\bigwedge\limits_{c\in S}c\right)\land \neg c^*\right)\\
    =&\Pr{c^*\mid \bigwedge\limits_{c\in S} c}\cdot \+P_U\left(\cdot\mid \bigwedge\limits_ {c\in S\cup \{c^*\}} c\right)+\Pr{\neg c^*\mid \bigwedge\limits_{c\in S} c}\+P_{U}\left(\cdot \mid \left(\bigwedge\limits_{c\in S}c\right)\land \neg c^*\right)\\
    =&\+P_U\left(\cdot\mid \bigwedge\limits_ {c\in S}c\right)
    \end{aligned}
\end{equation*}
and
\[
Y\sim p_{\text{sat}}\cdot \+P_U\left(\cdot\mid \bigwedge\limits_ {c\in T} c\right)+\left(1-p_{\text{sat}}\right)\+P_{U}\left(\cdot \mid \bigwedge\limits_{c\in T}c\right)=\+P_U\left(\cdot\mid \bigwedge\limits_ {c\in T}c\right),
\]
This finishes the last case of the induction step and the proof of the lemma.
\end{itemize}
\end{proof}

\newpage

\section{Expected discrepancy of the coupling procedure}\label{sec:discrepancy}
In this section, we analyze the expected discrepancy of the output of \Cref{Alg:couple}. Specifically, we fix an initial input tuple $(V,S^{\text{in}},T^{\text{in}})$ of \Cref{Alg:couple} and analyze $\E[\+C]{d_{\textrm{Ham}}(X^{\+C},Y^{\+C})}$, where $(X^{\+C},Y^{\+C})$ is the pair of assignments returned by $\+C(V,S^{\text{in}},T^{\text{in}})$. 


\begin{definition}[Parameters for the input tuple $(V,S^{\text{in}},T^{\text{in}})$]\label{definition:parameters-input}

We specify some further notations for the parameters of the input tuples.
    \begin{itemize}
        \item Let $p=\max\limits_{c\in S^{\text{in}}\cup T^{\text{in}}}\Pr[\+P]{\neg c}$ be the maximum violation probability in $S^{\text{in}}\cup T^{\text{in}}$. 
        \item Let $k= \max\limits_{c\in S^{\text{in}}\cup T^{\text{in}}}$ be the maximum width of a constraint in $S^{\text{in}}\cup T^{\text{in}}$. 
        \item Let $G_\+C$ be the dependency graph with vertex set $S^{\text{in}}\cup T^{\text{in}}$, such that any two distinct constraints $c, c' \in S^{\text{in}}\cup T^{\text{in}}$ are adjacent in $G_{\+C}$ if $\var(c)\cap \var(c')\neq \emptyset$. Also for any $Z\subseteq S^{\text{in}}\cup T^{\text{in}}$, we use $G_{\+C}(Z)$ to denote the subgraph of $G_\+C$ induced by $Z$.
         \item Let $D= \max\limits_{c\in S^{\text{in}}\cup T^{\text{in}}}\abs{\{c'\in (S^{\text{in}}\cup T^{\text{in}})\setminus \{c\}\mid \vbl(c)\cap \vbl(c')\neq\emptyset\}}$ be the maximum degree of $G_{\+C}$.
          \item Let $\chi_{\min}=\min\limits_{v\in V}\min\limits_{x\in Q_v}{\+D_v(x)}^{-1}$  be the minimum 
distortion in $S^{\text{in}}\cup T^{\text{in}}$.
    \end{itemize}
\end{definition}





We remark that \Cref{definition:parameters-input} is defined only for the analysis of \Cref{Alg:couple} in this section, with notations chosen to align with the ones defined for CSPs in \Cref{sec:intro}. To avoid confusion of notations, we will add subscripts and use notations like $p_{\Phi}$ or $k_{\Phi}$ to refer to the parameters of some CSP $\Phi$.


\begin{comment}
\begin{theorem}\label{lemma:couple-dis}
Let $(V,S^{\text{in}},T^{\text{in}})$ be the input of \Cref{Alg:couple} and let  $(X^{\+C},Y^{\+C})$ be its output.


If \eqref{eq:correlation-decay-condition} holds,
then
\[
\E[\+C]{d_{\textrm{Ham}}(X^{\+C},Y^{\+C})}\leq \frac{k\Delta}{2}\cdot \abs{S^{\text{in}}\ominus T^{\text{in}}},
\] 
where $S^{\text{in}}\ominus T^{\text{in}}$ is the symmetric difference between $S^{\text{in}}$ and $T^{\text{in}}$.
\end{theorem}

    
\end{comment}


At the end of this section, we will prove the following main technical lemma.

\begin{lemma}\label{lemma:couple-dis}
Let $(V,S^{\text{in}},T^{\text{in}})$ be the input of \Cref{Alg:couple} and let  $(X^{\+C},Y^{\+C})$ be its output.
Let $\delta\geq 1$ be any real. 

If $D\geq 1$ and
\begin{equation}\label{eq:correlation-decay-condition-couple}
    (2\mathrm{e})^{1+\frac{\zeta}{2}}\cdot \delta\cdot p\cdot (D+1)^{2+\zeta}\leq 1,
\end{equation}
where
\[
\zeta=\frac{2\ln(2-1/\chi_{\min})}{\ln\chi_{\min}-\ln(2-1/\chi_{\min})},
\]

then
\[
\E[\+C]{d_{\textrm{Ham}}(X^{\+C},Y^{\+C})}\leq \frac{k(D+1)}{2\delta}\cdot \abs{S^{\text{in}}\ominus T^{\text{in}}},
\] 
where $S^{\text{in}}\ominus T^{\text{in}}$ is the symmetric difference between $S^{\text{in}}$ and $T^{\text{in}}$.
\end{lemma}


Assuming \Cref{lemma:couple-dis}, we can already prove \Cref{thm:correlation-decay}.


\begin{proof}[Proof of \Cref{thm:correlation-decay}]


We claim that for each $u\in V$ and $i,j\in Q_u$, there exists a coupling $\+C^u_{i,j}:(X,Y)$ of $\mu^{u\gets i}$ and $\mu^{u\gets j}$ such that
\[
\E[\+C^u_{i,j}]{d_{\textrm{Ham}}(X,Y)}\leq \frac{k_{\Phi}(D_{\Phi}+1)^2}{\left(\chi_{\max}\right)_{\Phi}^2},
\]
then the theorem directly follows from \Cref{cor:disc-upper-infl}.

To show the claim, we set $S^{\text{in}}$ to be the set of simplified constraints of $C$ after assigning $i$ to $u$, and  set $T^{\text{in}}$ to be the set of simplified constraints of $C$ after assigning $j$ to $u$, and the coupling $\+C^u_{i,j}$ is constructed using $\+C(V,S^{\text{in}},T^{\text{in}})$ through \Cref{Alg:couple}. The correctness follows from \Cref{lemma:couple-cor}.

We then clearly have  $D\leq D_{\Phi}$, $\chi_{\min}\geq \left(\chi_{\min}\right)_{\Phi}$, $p\leq  p_{\Phi}\cdot \left(\chi_{\max}\right)_{\Phi}$, $\abs{S^{\text{in}}\ominus T^{\text{in}}}\leq D_{\Phi}+1$  and  $k\leq k_{\Phi}$, therefore \eqref{eq:correlation-decay-condition} implies \eqref{eq:correlation-decay-condition-couple} with $\delta=\left(\chi_{\max}\right)_{\Phi}^2$, and the theorem follows from \Cref{lemma:couple-dis}.

\end{proof}

\begin{comment}
\begin{lemma}\label{lemma:couple-dis}
Let $(V,S^{\text{in}},T^{\text{in}})$ be the input of \Cref{Alg:couple} and let  $(X^{\+C},Y^{\+C})$ be its output.

If \eqref{eq:correlation-decay-condition} holds,
then for any $v\in V$, 
\[
\Pr[\+C]{X^{\+C}(v)\neq Y^{\+C}(v)}\leq |S^{\text{in}}\ominus T^{\text{in}}|\cdot \mathrm{e}^{-\lceil\frac{d(v)}{2}\rceil}.
\] 
\end{lemma}

Assuming \Cref{lemma:couple-dis}, we can already prove \Cref{thm:correlation-decay}.

\begin{proof}[Proof of \Cref{thm:correlation-decay}]

For each $u,v\in V$, define $D(u,v)$ as the minimum length of the sequence of constraints $(c_1,c_2,\dots,c_{\ell})$ satisfying that 
\begin{enumerate}
    \item $u\in \var(c_1),v\in \var(c_{\ell})$.
    \item $\var(c_i)\cap \var(c_{i+1})\neq \varnothing$ for $1\leq i<\ell$,
\end{enumerate} 
If no such sequence of constraints exists, define $D(u,v)=\infty$.

we claim that $\Psi_\mu(u,v)\leq \Delta\cdot \mathrm{e}^{-\lceil\frac{D(v)}{2}\rceil}$. Fix any $u\in V$. Note that for any integer $i\geq 1$, it holds that $|\{v\in V,D(u,v)\leq i\}|\leq k\cdot \Delta^i$. Then we have
\begin{equation*}
    \begin{aligned}
        &\lambda_{\max}\Psi_{\mu}\\
        \leq &\sum\limits_{v\in V}\Psi_{\mu}(u,v)\\
        \leq &\sum\limits_{i=1}^{\infty}\sum\limits_{\substack{v\in V\\ D(u,v)=2i-1\text{ or }2i}}\Psi_{\mu}(u,v)\\
        \leq &\sum\limits_{i=1}^{\infty}\sum\limits_{\substack{v\in V\\ D(u,v)=2i-1\text{ or }2i}}\left(\Delta\cdot \mathrm{e}^{-i}\right)\\
        \leq &\sum\limits_{i=1}^{\infty}\left(k\cdot (\Delta^2)^i\left(\Delta\cdot \mathrm{e}^{-i}\right)\right)\\
    \end{aligned}
\end{equation*}

Now we prove the claim. The claim clearly holds when $D(u,v)=\infty$ by the conditional independence property of Gibbs distribution. 

\end{proof}

\end{comment}

\newpage
\subsection{Execution log and bad constraints}

\subsubsection{Execution log}
To begin our analysis, we introduce the definition of execution log, which captures the execution process of $\+C(V,S^{\text{in}},T^{\text{in}})$. 
\begin{definition}[Execution log]\label{definition:execution-log}
Suppose the $\+C(V,S^{\text{in}},T^{\text{in}})$ is called for some tuple $(V,S^{\text{in}},T^{\text{in}})$ satisfying the input condition of \Cref{Alg:couple}, its \emph{execution log}  is defined as the random sequence of $6$-tuples
\[
 \Lambda(V,S^{\text{in}},T^{\text{in}})\defeq (E_0,E_1,\dots,E_{\ell})\quad \text{where }\forall 0\leq i\leq \ell, E_i\defeq (U_{i},S_{i},T_{i},c_{i},X_{i},Y_{i})
\]
generated from the execution of $\+C(V,S^{\text{in}},T^{\text{in}})$ as follows: Initially set $U_0=V,S_0=S^{\text{in}},T_0=T^{\text{in}}$ and $\Lambda(V,S^{\text{in}},T^{\text{in}})=E_0=(U_0,S_0,T_0,\emptyset,\emptyset,\emptyset)$. For $i=0,1,\dots $,
\begin{enumerate}
    \item If $\+C(U_i,S_i,T_i)$ terminates at \Cref{Line:couple-return}, the process ends. \label{item:exlog-1}
    \item Otherwise we append a new $6$-tuple $E_{i+1}$ to $\Lambda(V,S^{\text{in}},T^{\text{in}})$ where the $(U_{i+1},S_{i+1},T_{i+1})$ is generated from $(U_{i},S_{i},T_{i})$ such that $\+C(U_{i+1},S_{i+1},T_{i+1})$ is recursively called within $\+C(U_{i},S_{i},T_{i})$. The $c_{i+1},X_{i+1}$ and $Y_{i+1}$ are set according to the following rule during the call of $\+C(U_{i},S_{i},T_{i})$: \label{item:exlog-2}
    \begin{enumerate}
        \item $c_{i+1}=c^*$ is the (simplified) constraint chosen at \Cref{Line:couple-choose}  or \Cref{Line:couple-choose-1}, depending on which line is executed.
        \item If $\+C(U_i,S_i,T_i)$ terminates at \Cref{Line:couple-psat-return} or  \Cref{Line:couple-psat-return-1}, then $X_{i+1}=X_{i},Y_{i+1}=Y_i$; otherwise $X_{i+1}=X_i\oplus X_{\var(c^*)},Y_{i+1}=Y_i\oplus Y_{\var(c^*)}$, where  $X_{\var(c^*)}$ and $Y_{\var(c^*)}$ are the two partial assignments sampled at \Cref{Line:couple-sample} or \Cref{Line:couple-sample-1}, depending on which line is executed.
    \end{enumerate}
\end{enumerate}
Note that  during $\+C(U,S,T)$, either the algorithm terminates immediately at \Cref{Line:couple-return}, or exactly one recursive call of $\+C(U',S',T')$ is induced, so this sequence is well-defined.

Moreover, we say a sequence $(E_0,E_1,\dots,E_{\ell})$ is a \emph{proper execution log} (with respect to $(V,S^{\text{in}},T^{\text{in}})$) if
\[
\Pr[\+C]{ \Lambda(V,S^{\text{in}},T^{\text{in}})=(E_0,E_1,\dots,E_{\ell})}>0,
\]
where the subscript $\+C$ means the probability is taken over the randomness of the coupling procedure $\+C(V,S^{\text{in}},T^{\text{in}})$.
\end{definition}

\begin{remark}[Meaning of each entry in the execution log]
Here in \Cref{definition:execution-log}, each entry $E_i$ in the execution log is a $6$-tuple $(U_i,S_i,T_i,c_i,X_i,Y_i)$. Here the first three entries $(U_i,S_i,T_i)$ represent the arguments passed to each level of recursion, the fourth entry $c_i$ represents the (simplified) constraint chosen at each level of recursion, the fifth and sixth entry $X_i$ and $Y_i$ represent the (cumulative) partial assignment for either distribution up to the current recursion level.  
\end{remark}

The length $\ell(V,S^{\text{in}},T^{\text{in}})$ of $\Lambda(V,S^{\text{in}},T^{\text{in}}) = \left(E_0,E_1,\dots,E_{\ell}\right)$ is a random variable whose distribution is determined by $(V,S^{\text{in}},T^{\text{in}})$. 
%
We simply write $\ell=\ell(V,S^{\text{in}},T^{\text{in}})$ and $\Lambda(V,S^{\text{in}},T^{\text{in}}) = \left(E_0,E_1,\dots,E_{\ell}\right)$ if $(V,S^{\text{in}},T^{\text{in}})$ is clear from the context.
%
It is obvious from \Cref{definition:execution-log} that $\Lambda(V,S^{\text{in}},T^{\text{in}})$ satisfies the Markov property. 


We first record some basic facts about any proper execution log.
\begin{lemma}\label{lemma:facts}
Given any proper execution log $L=(E_0,E_1,\dots,E_{\ell})$, where $E_i=(U_i,S_i,T_i,c_i,X_i,Y_i)$ for each $0\leq i\leq \ell$. The following holds.
\begin{enumerate}
    \item $S_0=S^{\text{in}},T_0=T^{\text{in}},U_0=V,c_0=X_0=Y_0=\emptyset$.\label{item:facts-1}
    \item $S_{\ell}=T_{\ell}$ and $S_i\neq T_i$ for each $0\leq i<\ell$.\label{item:facts-2}
    \item For each $0\leq i\leq \ell$, $X_i$ and $Y_i$ are partial assignments defined over  $V\setminus U_i$.\label{item:facts-3}
    \item For each $1\leq i\leq \ell$, $\var(c_i)\subseteq U_{i-1}$. \label{item:facts-4}
    \item For each $0\leq i\leq \ell$, the event $X_i\land \left(\bigwedge\limits_{c\in S_i}c\right)$ implies $\left(\bigwedge\limits_{c\in S^{\text{in}}}c\right)$ and the event $Y_i\land \left(\bigwedge\limits_{c\in T_i}c\right)$ implies $\left(\bigwedge\limits_{c\in T^{\text{in}}}c\right)$.  \label{item:facts-5} 
\end{enumerate}
\end{lemma}

\begin{proof}
  \Cref{lemma:facts}-(\ref{item:facts-1}) is immediate by \Cref{definition:execution-log}.
  
  \Cref{lemma:facts}-(\ref{item:facts-2}) is immediate by \Cref{definition:execution-log}-(\ref{item:exlog-1}) and Lines \ref{Line:couple-return-cond}-\ref{Line:couple-psat-return} of \Cref{Alg:couple}.  

  
  \Cref{lemma:facts}-(\ref{item:facts-3}) holds by a simple induction, where the induction step is through comparing Lines \ref{Line:couple-sample} and \ref{Line:couple-sample-1} of \Cref{Alg:couple} with \Cref{definition:execution-log}.

  \Cref{lemma:facts}-(\ref{item:facts-4}) is simply by \Cref{definition:execution-log} and the input condition of \Cref{Alg:couple}.

 We prove \Cref{lemma:facts}-(\ref{item:facts-5}) by an induction on $i$ from $0$ to $\ell$. The base case is when $i=0$. In this case, $X_i=Y_i=\emptyset$ and $S_i=S^{\text{in}},T_i=T^{\text{in}}$, and the base case is immediate. 

 For the induction step,  we assume that $0<i\leq \ell$.  Note that by \Cref{definition:execution-log}-(\ref{item:exlog-1}) we have $S_{i-1}\neq T_{i-1}$ and the condition at \Cref{Line:couple-return-cond} is not satisfied in $\+C(U_{i-1},S_{i-1},T_{i-1})$. 
 We then assume that $T_{i-1}\not\subseteq S_{i-1}$ and that the condition at \Cref{Line:couple-swap-cond} is not satisfied in $\+C(U_{i-1},S_{i-1},T_{i-1})$, the case when $T_{i-1}\subseteq S_{i-1}$ follows analogously. Let $c^*$ be the constraint chosen at \Cref{Line:couple-choose}. By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log we have either $U_i=U_{i-1}$ or $U_i=U_{i-1}\setminus \var(c^*)$.

\begin{itemize}
    \item Suppose $U_i=U_{i-1}$. By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1},S_{i}=S_{i-1}\cup \{c^*\},T_{i}=T_{i-1},c_{i}=c^*,X_{i}=X_{i-1}$ and $Y_{i}=Y_{i-1}$. In this case, the lemma holds from the induction hypothesis.
    \item Otherwise $U_i=U_{i-1}\setminus \var(c^*)$. By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1}\setminus \var(c^*),S_{i}=S^*_{i-1},T_{i}=T^*_{i-1},c_{i}=c^*,X_{i}=X_{i-1}\oplus X^*_{\var(c^*)}$ and $Y_{i}=Y_{i-1}\oplus Y^*_{\var(c^*)}$, where $X^*_{\var(c^*)}$ and $Y^*_{\var(c^*)}$ are two partial assignments over $\var(c^*)$, $S^*_{i-1}$  is the simplification of $S_{i-1}$ under $X^*_{\var(c^*)}$ and $T^*_{i-1}$ is the simplification of $T_{i-1}$ under $Y^*_{\var(c^*)}$. In this case, the lemma holds from the induction hypothesis and the definition of simplification.
\end{itemize}


\end{proof}
%\begin{proof}
%\Cref{item:facts-1} is immediate by \Cref{definition:execution-log}. \Cref{item:facts-2} is immediate by \Cref{definition:execution-log}-(\ref{item:exlog-1}) and \Cref{Line:couple-return-cond} of \Cref{Alg:couple}.

%\end{proof}

 The following lemma characterizes the probability a certain proper execution log occurs.
\begin{lemma}\label{lemma:log-prob}
Given any proper execution log $L=(E_0,E_1,\dots,E_{\ell})$, where $E_i=(U_i,S_i,T_i,c_i,X_i,Y_i)$ for each $0\leq i\leq \ell$. It holds that
\begin{equation*}
\Pr[\+C]{ \Lambda(V,S^{\text{in}},T^{\text{in}})=L}= \Pr{X_{\ell}\land \left(\bigwedge\limits_{c\in S_{\ell}}c \right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{\ell}\land \left(\bigwedge\limits_{c\in T_{\ell}}c \right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}.
\end{equation*}
\end{lemma}
\begin{proof}
Suppose $\Lambda(V,S^{\text{in}},T^{\text{in}})=(E'_0,E'_1,\dots,E'_{t})$ where $t=\ell(V,S^{\text{in}},T^{\text{in}})$. For any non-negative integer $i\geq 0$, we define 
\begin{equation}\label{eq:log-prob-2}
\Lambda(V,S^{\text{in}},T^{\text{in}},i)\defeq \begin{cases}(E'_0,E'_1,\dots,E'_{i}) & i\leq t\\
\perp & i>t
\end{cases}
\end{equation}
as the prefix containing the first $i+1$ terms of $\Lambda(V,S^{\text{in}},T^{\text{in}})$ if $i\leq t$ and $\perp$ otherwise.

For each $0\leq i\leq \ell$, define the following event
\[
\+E_i: \Lambda(V,S^{\text{in}},T^{\text{in}},i)=(E_0,E_1,\dots,E_i).
\]

We then claim that for each $0\leq i\leq \ell$,

\begin{equation}\label{eq:log-prob-3}
\Pr[\+C]{\+E_{i}}=\Pr{X_{i}\land\left(\bigwedge\limits_{c\in S_{i}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i}\land\left(\bigwedge\limits_{c\in T_{i}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}.
\end{equation}

 Note that by \eqref{eq:log-prob-2} we have the event $ \Lambda(V,S^{\text{in}},T^{\text{in}})=L$ implies $\+E_{\ell}$.  By \Cref{lemma:facts}-(\ref{item:facts-2}) and $L$ is a proper execution log we have $S_{\ell}=T_{\ell}$, combining with \Cref{definition:execution-log}-(\ref{item:exlog-1})   we have\[
 \Lambda(V,S^{\text{in}},T^{\text{in}})=L \Longleftrightarrow  \+E_{\ell},
\]
therefore \eqref{eq:log-prob-3} already proves the lemma.  We then prove \eqref{eq:log-prob-3} by an induction on $i$ from $0$ to $\ell$.



The base case is when $i=0$. Note that by \Cref{lemma:facts}-(\ref{item:facts-1}) and $L$ is a proper execution log we have $E_0=(V,S^{\text{in}},T^{\text{in}},\emptyset,\emptyset,\emptyset)$ and $\Pr[\+C]{\+E_0}=1$.  Also by $X_0=Y_0=\emptyset,S_0=S^{\text{in}},T_0=T^{\text{in}}$ it is straightforward to verify both sides of \eqref{eq:log-prob-3} equal to $1$. The base case is proved.

For the induction step, we assume that $0<i\leq \ell$.  Note that by \Cref{definition:execution-log}-(\ref{item:exlog-1}) we have $S_{i-1}\neq T_{i-1}$ and the condition at \Cref{Line:couple-return-cond} is not satisfied in $\+C(U_{i-1},S_{i-1},T_{i-1})$. 
 We then assume that $T_{i-1}\not\subseteq S_{i-1}$ and that the condition at \Cref{Line:couple-swap-cond} is not satisfied in $\+C(U_{i-1},S_{i-1},T_{i-1})$, the case when $T_{i-1}\subseteq S_{i-1}$ follows analogously. Let $c^*$ be the constraint chosen at \Cref{Line:couple-choose}. By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log we have either $U_i=U_{i-1}$ or $U_i=U_{i-1}\setminus \var(c^*)$.
 \begin{itemize}
 \item Suppose $U_i=U_{i-1}$.  By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1},S_{i}=S_{i-1}\cup \{c^*\},T_{i}=T_{i-1},c_{i}=c^*,X_{i}=X_{i-1}$ and $Y_{i}=Y_{i-1}$. In this case, by the Markov property of $\Lambda(V,S^{\text{in}},T^{\text{in}})$,  $\+E_{i}$ happens if and only if both the following two events happen:
 \begin{enumerate}
     \item $\+E_{i-1}$ happens;
     \item the condition at \Cref{Line:couple-psat-cond} of $\+C(U_{i-1},S_{i-1},T_{i-1})$ is satisfied, which happens independently with probability $\Pr{c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}$.
 \end{enumerate} 
 
 Therefore, in this case, we have
 \begin{equation}\label{eq:log-prob-4}
     \begin{aligned}
 &\Pr[\+C]{\+E_i}\\
 =&\Pr[\+C]{\+E_{i-1}}\cdot  \Pr{c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}\\
=& \Pr{X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}\cdot \Pr{c^*\mid \bigwedge\limits_{c\in S_{i-1}} c},
\end{aligned} 
 \end{equation}

where the last equality is by induction hypothesis. Note that we further have
\begin{equation*}
    \Pr{c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}= \Pr{c^*\mid X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right) }=  \Pr{c^*\mid X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land \left(\bigwedge\limits_{c\in S^{\text{in}}}c\right)},
\end{equation*}
where the first equality is by \Cref{lemma:facts}-(\ref{item:facts-3}) and \Cref{lemma:facts}-(\ref{item:facts-4}), and the second equality is by \Cref{lemma:facts}-(\ref{item:facts-5}). Combining with \eqref{eq:log-prob-4} and applying chain rule, we finally have

\begin{align*}
&\Pr[\+C]{\+E_i}\\
=& \Pr{X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land c^*\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}\\
=& \Pr{X_{i}\land\left(\bigwedge\limits_{c\in S_{i}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i}\land\left(\bigwedge\limits_{c\in T_{i}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c},
 \end{align*}
 where the second equality  is by that in this case $S_{i}=S_{i-1}\cup \{c^*\},T_{i}=T_{i-1},c_{i}=c^*,X_{i}=X_{i-1}$ and $Y_{i}=Y_{i-1}$.
 \item Otherwise $U_i=U_{i-1}\setminus \var(c^*)$.  By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1}\setminus \var(c^*),S_{i}=S^*_{i-1},T_{i}=T^*_{i-1},c_{i}=c^*,X_{i}=X_{i-1}\oplus X^*_{\var(c^*)}$ and $Y_{i}=Y_{i-1}\oplus Y^*_{\var(c^*)}$, where $X^*_{\var(c^*)}$ and $Y^*_{\var(c^*)}$ are two partial assignments over $\var(c^*)$, $S^*_{i-1}$  is the simplification of $S_{i-1}$ with respect to $X^*_{\var(c^*)}$ and $T^*_{i-1}$ is the simplification of $T_{i-1}$ with respect to $Y^*_{\var(c^*)}$. In this case, by the Markov property of $\Lambda(V,S^{\text{in}},T^{\text{in}})$, $\+E_{i}$ happens if and only if the following three events happen:
 \begin{enumerate}
 \item $\+E_{i-1}$ happens;\label{item:prob-1}
 \item  The condition at \Cref{Line:couple-psat-cond} of $\+C(U_{i-1},S_{i-1},T_{i-1})$ is not satisfied, which happens independently with probability $\Pr{\neg c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}$;\label{item:prob-2}
 \item Let $X_{\var(c^*)}$ and $Y_{\var(c^*)}$ be the two partial assignments sampled at \Cref{Line:couple-sample}, then $X_{\var(c^*)}=X^*_{\var(c^*)}$ and $X_{\var(c^*)}=Y^*_{\var(c^*)}$. This happens with probability 
 \[
 \Pr{X^*_{\var(c^*)}\mid \left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land\neg c^*}\cdot \Pr{Y^*_{\var(c^*)}\mid \bigwedge\limits_{c\in T_{i-1}}c}.
 \]
 conditioning on the former two events happen.
 \end{enumerate}
Therefore, in this case, we have

\begin{equation}\label{eq:log-prob-5}
   \begin{aligned}
    &\Pr[\+C]{\+E_i}\\
 =&\Pr[\+C]{\+E_{i-1}}\cdot  \Pr{\neg c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}\cdot  \Pr{X^*_{\var(c^*)}\mid \left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land\neg c^*}\cdot \Pr{Y^*_{\var(c^*)}\mid \bigwedge\limits_{c\in T_{i-1}}c}\\
=& \Pr{X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}\\
\cdot& \Pr{\neg c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}\cdot  \Pr{X^*_{\var(c^*)}\mid \left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land\neg c^*}\cdot \Pr{Y^*_{\var(c^*)}\mid \bigwedge\limits_{c\in T_{i-1}}c},
   \end{aligned}
\end{equation}


where the last equality is by induction hypothesis. Note that we further have

\begin{align*}
&\Pr{\neg c^*\mid \bigwedge\limits_{c\in S_{i-1}} c}\cdot \Pr{X^*_{\var(c^*)}\mid \left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land\neg c^*}\cdot \Pr{Y^*_{\var(c^*)}\mid \bigwedge\limits_{c\in T_{i-1}}c}\\ 
=&  \Pr{\neg c^*\mid X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right) }\cdot  \Pr{X^*_{\var(c^*)}\mid X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land\neg c^*  }\cdot \Pr{Y^*_{\var(c^*)}\mid Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)}\\
=&  \Pr{\neg c^*\mid X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right) \land \left(\bigwedge\limits_{c\in S^{\text{in}}}c\right) }
\cdot  \Pr{X^*_{\var(c^*)}\mid X_{i-1}\land \left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land   \neg c^*\land \left(\bigwedge\limits_{c\in S^{\text{in}}}c\right)}\\
\cdot & \Pr{Y^*_{\var(c^*)}\mid Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)\land \left(\bigwedge\limits_{c\in T^{\text{in}}}c\right)},
\end{align*}
where the first equality is by \Cref{lemma:facts}-(\ref{item:facts-3}) and \Cref{lemma:facts}-(\ref{item:facts-4}), and the second equality is by \Cref{lemma:facts}-(\ref{item:facts-5}). Combining with \eqref{eq:log-prob-5} and applying chain rule, we finally have
\begin{align*}
&\Pr[\+C]{\+E_i}\\
=& \Pr{X_{i-1}\land\left(\bigwedge\limits_{c\in S_{i-1}}c\right)\land X^*_{\var(c^*)}\land \neg c\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i-1}\land\left(\bigwedge\limits_{c\in T_{i-1}}c\right)\land Y^*_{\var(c^*)}\mid \bigwedge\limits_{c\in T^{\text{in}}}c}\\
=& \Pr{X_{i}\land\left(\bigwedge\limits_{c\in S_{i}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i}\land \left(\bigwedge\limits_{c\in T_{i}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c},
 \end{align*}
 where the second equality is by that $X^*_{\var(c^*)}$ implies $\neg c^*$ and that in this case $S_{i}=S^*_{i-1},T_{i}=T^*_{i-1},c_{i}=c^*,X_{i}=X_{i-1}\oplus X^*_{\var(c^*)}$ and $Y_{i}=Y_{i-1}\oplus Y^*_{\var(c^*)}$, where $S^*_{i-1}$  is the simplification of $S_{i-1}$ with respect to $X^*_{\var(c^*)}$  and $T^*_{i-1}$ is the simplification of $T_{i-1}$ with respect to $Y^*_{\var(c^*)}$.

\end{itemize}

This finishes the induction step and the proof.
\end{proof}



\subsubsection{Set of bad constraints}
To upper bound the expected discrepancy of the output produced by the coupling procedure, we introduce the set of bad constraints to serve as a witness of large discrepancy. Before formally introducing the definition, we make some additional specifications.

Note that in \Cref{Alg:couple} we manipulate the set of (simplified) constraints. Although we can safely assume that the initial set of constraints in $S^{\text{in}}$ or $T^{\text{in}}$ are distinct, it is possible that after assigning values to some variables, the two constraints $c'_1$ and $c'_2$ simplified from some $c_1,c_2\in S^{\text{in}}$ become the same. Therefore when we say at some step, the algorithm picks a (simplified) constraint $c$, it is not immediately clear which constraint we are referring to. To deal with this issue, we provide the following succinct representation of (simplified) constraints, which is identifying each simplified constraint by a pair of its original constraint and the subset of variables it is specified on.

\begin{definition}
[succinct representation of (simplified) constraints]\label{remark:succinct}
Each (simplified) constraint is written in the form of $(c,Z)$ where $c\in S^{\text{in}}\cup T^{\text{in}}$ is some original constraint, and $Z\subseteq \var(c)$ is a subset of variables, denoting the variables appearing in the (simplified) constraint. Note that this representation uniquely specifies any (simplified) constraint that could possibly appear in \Cref{Alg:couple}. Also, for each simplified constraint $(c,Z)$, we denote its original constraint in $S^{\text{in}}\cup T^{\text{in}}$ as $\+F((c,Z))=c$.
\end{definition}

The set of bad constraints is then defined as the set of constraints in $S^{\text{in}}\cup T^{\text{in}}$, whose simplification, when chosen by \Cref{Alg:couple}, leads to an assignment of values of variables.

\begin{definition}[set of bad constraints]\label{definition:bad-constraint}
Let $\Lambda(V,S^{\text{in}},T^{\text{in}})=(E_0,E_{1},\dots,E_{\ell})$ be a proper execution log generated from calling $\+C(V,S^{\text{in}},T^{\text{in}})$, we define its associated set of \emph{bad constraints} $B(V,S^{\text{in}},T^{\text{in}})\subseteq S^{\text{in}}\cup T^{\text{in}}$ is defined as the random set of (original) constraints constructed from $\Lambda(V,S^{\text{in}},T^{\text{in}})$ as follows. Initially $B(V,S^{\text{in}},T^{\text{in}})=\emptyset$. For $i=1,\dots,\ell$,
\begin{enumerate}
    \item If $X_{i}=X_{i-1}$, do nothing.
    \item Otherwise, add $\+F(c_i)$ into $B(V,S^{\text{in}},T^{\text{in}})$, where $\+F(c_i)$ denote the original constraint of $c_i$ as in \Cref{remark:succinct}.
\end{enumerate}
\end{definition}

From \Cref{definition:execution-log} and \Cref{definition:bad-constraint} one can see that each execution of \Cref{Alg:couple} corresponds to one proper execution log and one set of bad constraints. However, we remark that one set of bad constraints may correspond to multiple possible executions of \Cref{Alg:couple}. 

We finish this subsection by showing that we can actually give an upper bound of the discrepancy introduced by the coupling procedure by the size of the set of bad constraints.

\begin{lemma}\label{lemma:disc-bound}
Let $(X^{\+C},Y^{\+C})$ be the output of $\+C(V,S^{\text{in}},T^{\text{in}})$. Then
\[d_{\text{Ham}}(X^{\+C},Y^{\+C})\leq k\cdot \abs{B(V,S^{\text{in}},T^{\text{in}})}.\]
\end{lemma}
\begin{proof}
Let $\Lambda(V,S^{\text{in}},T^{\text{in}})=(E_0,\dots,E_{\ell})$ as in \Cref{definition:execution-log}. By \Cref{Line:couple-return} of \Cref{Alg:couple} and \Cref{definition:execution-log} we have $d_{\text{Ham}}(X^{\+C},Y^{\+C})\leq \abs{V\setminus U_{\ell}}$. Then the lemma follows by \Cref{lemma:facts}-(\ref{item:facts-3}) and \Cref{definition:bad-constraint}.
\end{proof}

\begin{comment}
\begin{lemma}\label{lemma:disc-cover}
Let $(X^{\+C},Y^{\+C})$ be the output of $\+C(V,S^{\text{in}},T^{\text{in}})$. Then for any $v\in V$,
\[
X^{\+C}(v)\neq Y^{\+C}(v)\implies \exists c\in B(V,S^{\text{in}},T^{\text{in}}) \text{ s.t. } v\in \var(c)
\]
\end{lemma}
\begin{proof}
Let $\Lambda(V,S^{\text{in}},T^{\text{in}})=(E_0,E_1,\dots,E_{\ell})$, where $E_i=(U_i,S_i,T_i,c_i,X_i,Y_i)$ for each $0\leq i\leq \ell$. Note that by \Cref{Line:couple-return} of \Cref{Alg:couple} and \Cref{definition:execution-log} we have $d_{\textrm{Ham}}X^{\+C}(v)\neq Y^{\+C}(v)\implies v \in \var(c_i)$ for some $1\leq i\leq \ell$. Then the lemma follows by  \Cref{definition:bad-constraint}.
\end{proof}
\end{comment}


\subsection{Refutation of bad constraints}


In this subsection, we will bound the probability that a certain set of bad constraints appears, which is done by showing that a bad constraint actually enforces some of the assignment on variables, which combined with \Cref{lemma:log-prob} gives an upper bound.

\begin{lemma}\label{lemma:disjoint-prob}
Assume the conditions of \Cref{lemma:couple-dis}. For a set of disjoint constraints $A\subseteq S^{\text{in}}\cup T^{\text{in}}$,
\[
\Pr[\+C]{A\subseteq B(V,S^{\text{in}},T^{\text{in}})}\leq p^{\frac{2|A|}{2+\zeta}}\cdot (1-\mathrm{e}p)^{-2(D+1)|A|}.
\]
\end{lemma}


\subsubsection{Explicit randomness for the coupling procedure}


To prove \Cref{lemma:disjoint-prob}, we need the following alternative coupling procedure $\+C'$ of \Cref{Alg:couple} where all randomness sources comes from two independent samples $X^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in S^{\text{in}}}c\right),Y^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in T^{\text{in}}}c\right)$. It is presented as \Cref{Alg:couple-1}. 

\newpage

\begin{algorithm}[H]
\caption{$\+C'(U,S,T)$} \label{Alg:couple-1}
  \SetKwInput{KwPar}{Parameter}
\KwIn{ a set of variables $V$ where each random variable $v\in V$ is endowed with  a finite domain $Q_v$ and a probability distribution $\+D_v$ over $Q_v$, a subset of variables $U\subseteq V$, two sets of satisfiable atomic constraints $S,T$ defined over $U$, two assignments $X^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in S^{\text{in}}}c\right),Y^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in T^{\text{in}}}c\right)$} 
\KwOut{a pair of assignments $(X^{\+C'},Y^{\+C'})\in \{0,1\}^U$}
\If{$S=T$\label{Line:couple-1-return-cond}}{
    \Return $(X^{\+C'},Y^{\+C'})$ distributed under the perfect coupling of $\+P_{U}(\cdot \mid \bigwedge\limits_{c\in S} c)$ and  $\+P_{U}(\cdot \mid \bigwedge\limits_{c\in T} c)$\;\label{Line:couple-1-return}
}
\If{$T\not\subseteq S$ \label{Line:couple-1-swap-cond}}{
   Choose the smallest $c^*\in T\setminus S$\; \label{Line:couple-1-choose}
\If{$c^*$ is satisfied by $X^{\text{samp}}$ \label{Line:couple-1-psat-cond}}{
    \Return $\+C'(U,S\cup\{c^*\},T)$\; \label{Line:couple-1-psat-return}
}
\Else{
   $X_{\vbl(c^*)}\gets X^{\text{samp}}_{\vbl(c^*)}$ and $Y_{\vbl(c^*)}\gets Y^{\text{samp}}_{\vbl(c^*)}$\; \label{Line:couple-1-sample}
    Let the set of simplified constraints of $S$ under $X_{\vbl(c^*)}$ be $S^*$, and $T^*$ defined analogously for $T$ and $Y_{\vbl(c^*)}$\;  \label{Line:couple-1-simplify}
    $(X_{U\setminus \vbl(c^*)},Y_{U\setminus \vbl(c^*)})\gets \+C'(U\setminus \vbl(c^*),S^*,T^*)$\; \label{Line:couple-1-assign}
    \Return $(X,Y)$\; \label{Line:couple-1-assign-return}
}
}
\Else{
Choose the smallest $c^*\in S\setminus T$ \; \label{Line:couple-1-choose-1}
\If{$c^*$ is satisfied by $Y^{\text{samp}}$ \label{Line:couple-1-psat-cond-1}}{
    \Return $\+C'(U,S,T\cup \{c^*\})$\; \label{Line:couple-1-psat-return-1}
}
\Else{
    $X_{\vbl(c^*)}\gets X^{\text{samp}}_{\vbl(c^*)}$ and $Y_{\vbl(c^*)}\gets Y^{\text{samp}}_{\vbl(c^*)}$\; \label{Line:couple-1-sample-1}
    Let the set of simplified constraints of $S$ under $X_{\vbl(c^*)}$ be $S^*$, and $T^*$ defined analogously for $T$ and $Y_{\vbl(c^*)}$\;  \label{Line:couple-1-simplify-1}
    $(X_{U\setminus \vbl(c^*)},Y_{U\setminus \vbl(c^*)})\gets \+C'(U\setminus \vbl(c^*),S^*,T^*)$\; \label{Line:couple-1-assign-1}
    \Return $(X,Y)$\; \label{Line:couple-1-assign-return-1}
}
}


\end{algorithm}

\begin{remark}[Differences between \Cref{Alg:couple} and \Cref{Alg:couple-1}]
One can observe that the transitions of states in \Cref{Alg:couple} and \Cref{Alg:couple-1} are the same, and the only difference between \Cref{Alg:couple} and \Cref{Alg:couple-1} is the randomness used in determining which transition to choose:
\begin{itemize}
\item In \Cref{Alg:couple}, we each time use fresh randomness to sample with the (conditional) probability that some (simplified) constraint is satisfied, and the outcome of the variables.
\item In \Cref{Alg:couple-1}, we look at the two assignments $X^{\text{samp}}$ and $Y^{\text{samp}}$ sampled prior to the execution of the algorithm, i.e., the transitions are uniquely determined by $X^{\text{samp}}$ and $Y^{\text{samp}}$.
\end{itemize}
\end{remark}

We can similarly define the execution log $\Lambda'(V,S^{\text{in}},T^{\text{in}})$ and the set of bad constraints $B'(V,S^{\text{in}},T^{\text{in}})$ for the execution of $\+C'(V,S^{\text{in}},T^{\text{in}})$ as in \Cref{definition:execution-log} and \Cref{definition:bad-constraint}. Note that the properties in \Cref{lemma:facts} also holds for any proper execution log $\Lambda'$ produced from the execution of $\+C'(V,S^{\text{in}},T^{\text{in}})$ by similarly going through the proofs. We will show in the next lemma that the distribution of the output/the execution log/the set of bad constraints produced by the two algorithms is actually identical. 

\begin{lemma}\label{lemma:alter-equiv}
The execution log $\Lambda(V,S^{\text{in}},T^{\text{in}})$ and $\Lambda'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed. Furthermore, 
\begin{itemize}
    \item the set of bad constraints $B(V,S^{\text{in}},T^{\text{in}})$ and $B'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed.
    \item the output of $\+C(V,S^{\text{in}},T^{\text{in}})$ and $\+C'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed.
\end{itemize}
\end{lemma}
\begin{proof}

 
By \Cref{lemma:log-prob}, it is sufficient to prove that given any proper execution log $L=(E_0,E_1,\dots,E_{\ell})$, where $E_i=(U_i,S_i,T_i,X_i,Y_i)$ for each $0\leq i\leq \ell$. It holds that
\begin{equation}\label{eq:alter-equiv-1}
\Pr[\+C']{ \Lambda'(V,S^{\text{in}},T^{\text{in}})=L}= \Pr{X_{\ell}\land \left(\bigwedge\limits_{c\in S_{\ell}}c \right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{ Y_{\ell}\land \left(\bigwedge\limits_{c\in T_{\ell}}c \right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c},
\end{equation}
where $\Lambda'(V,S^{\text{in}},T^{\text{in}})$ is the execution log produced from the execution of $\+C'(V,S^{\text{in}},T^{\text{in}})$.
We then prove \eqref{eq:alter-equiv-1} using a similar method as in the proof of \Cref{lemma:log-prob}. Suppose $\Lambda'(V,S^{\text{in}},T^{\text{in}})=(E'_0,E'_1,\dots,E'_{t})$ where $t=\ell(V,S^{\text{in}},T^{\text{in}})$. For any non-negative integer $i\geq 0$, we define 
\begin{equation*}
\Lambda'(V,S^{\text{in}},T^{\text{in}},i)\defeq \begin{cases}(E'_0,E'_1,\dots,E'_{i}) & i\leq t\\
\perp & i>t
\end{cases}
\end{equation*}
as the prefix containing the first $i+1$ terms of $\Lambda'(V,S^{\text{in}},T^{\text{in}})$ if $i\leq t$ and $\perp$ otherwise. For each $0\leq i\leq \ell$, define the following event
\[
\+E_i: \Lambda'(V,S^{\text{in}},T^{\text{in}},i)=(E_0,E_1,\dots,E_i)
\]

We then claim that for each $0\leq i\leq \ell$,

\begin{equation}\label{eq:alter-equiv-3}
\Pr[\+C']{\+E_{i}}=\Pr{X_{i}\land\left(\bigwedge\limits_{c\in S_{i}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i}\land\left(\bigwedge\limits_{c\in T_{i}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c}
\end{equation}

 Note that by \eqref{eq:alter-equiv-1} we have the event $ \Lambda'(V,S^{\text{in}},T^{\text{in}})=L$ implies $\+E_{\ell}$.  By \Cref{lemma:facts}-(\ref{item:facts-2}) and $L$ is a proper execution log we have $S_{\ell}=T_{\ell}$, combining with \Cref{definition:execution-log}-(\ref{item:exlog-1})   we have\[
 \Lambda'(V,S^{\text{in}},T^{\text{in}})=L \Longleftrightarrow  \+E_{\ell},
\]
therefore \eqref{eq:alter-equiv-3} already proves \eqref{eq:alter-equiv-1} and the lemma.  For each $0\leq i\leq \ell$, define the following event:
\[
\+E^{\text{samp}}_i: =X^{\text{samp}}_{V\setminus U_i}=X_i\land \left(\bigwedge\limits_{c\in S_i}c\right)\text{ is satisfied by }X^{\text{samp}}\land Y^{\text{samp}}_{V\setminus U_i}=Y_i\land \left(\bigwedge\limits_{c\in T_i}c\right)\text{ is satisfied by }Y^{\text{samp}}.
%\left(X_i\land \left(\bigwedge\limits_{c\in S_i}c\right)\right)\land \left(Y_i\land \left(\bigwedge\limits_{c\in T_i}c\right)\right)
\]
We claim that for each $0\leq i\leq \ell$,
\begin{equation}\label{eq:alter-equiv-4}
\+E^{\text{samp}}_i\Longleftrightarrow  \+E_{i}.
\end{equation}
Note that  $X^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in S^{\text{in}}}c\right)$, $Y^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in T^{\text{in}}}c\right)$, hence we directly have
\[
\Pr[\+C']{\+E_{i}}=\Pr[\+C']{\+E^{\text{samp}}_i}=\Pr{X_{i}\land\left(\bigwedge\limits_{c\in S_{i}}c\right)\mid \bigwedge\limits_{c\in S^{\text{in}}}c}\cdot \Pr{Y_{i}\land\left(\bigwedge\limits_{c\in T_{i}}c\right)\mid \bigwedge\limits_{c\in T^{\text{in}}}c},
\]
therefore \eqref{eq:alter-equiv-4} directly proves \eqref{eq:alter-equiv-3} and the lemma. 
We then prove \eqref{eq:alter-equiv-4} by an induction on $i$ from $0$ to $\ell$.

The base case is when $i=0$. Note that by \Cref{lemma:facts}-(\ref{item:facts-1}) and $L$ is a proper execution log we have $E_0=(V,S^{\text{in}},T^{\text{in}},\emptyset,\emptyset,\emptyset)$ and $\Pr[\+C']{\+E_0}=1$.  Also by $X_0=Y_0=\emptyset,S_0=S^{\text{in}},T_0=T^{\text{in}}$ and that $X^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in S^{\text{in}}}c\right),Y^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in T^{\text{in}}}c\right)$ we have $\Pr[\+C']{\+E^{\text{samp}}_0}=1$ . The base case is proved.

For the induction step, we assume that $0<i\leq \ell$.  Note that by \Cref{definition:execution-log}-(\ref{item:exlog-1}) we have $S_{i-1}\neq T_{i-1}$ and the condition at \Cref{Line:couple-1-return-cond} is not satisfied in $\+C'(U_{i-1},S_{i-1},T_{i-1})$. 
 We then assume that $T_{i-1}\not\subseteq S_{i-1}$ and that the condition at \Cref{Line:couple-1-swap-cond} is not satisfied in $\+C'(U_{i-1},S_{i-1},T_{i-1})$, the case when $T_{i-1}\subseteq S_{i-1}$ follows analogously. Let $c^*$ be the constraint chosen at \Cref{Line:couple-1-choose}. By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log we have either $U_i=U_{i-1}$ or $U_i=U_{i-1}\setminus \var(c^*)$.
\begin{itemize}
 \item Suppose $U_i=U_{i-1}$.  By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1},S_{i}=S_{i-1}\cup \{c^*\},T_{i}=T_{i-1},c_{i}=c^*,X_{i}=X_{i-1}$ and $Y_{i}=Y_{i-1}$. In this case $\+E_{i}$ happens if and only if both $\+E_{i-1}$ happens and the condition at \Cref{Line:couple-1-psat-cond} of $\+C'(U_{i-1},S_{i-1},T_{i-1})$ is satisfied. By induction hypothesis, this is equivalent to both $\+E^{\text{samp}}_{i-1}$ happens and $c^*$ is satisfied by $X^{\text{samp}}$. By the definition of $\+E^{\text{samp}}_{i}$ and $S_{i}=S_{i-1}\cup \{c^*\}$ we have this is equivalent to $\+E^{\text{samp}}_{i}$ happens and the claim holds in this case.
 \item  Otherwise $U_i=U_{i-1}\setminus \var(c^*)$.  By \Cref{definition:execution-log}-(\ref{item:exlog-2}) and $L$ is a proper execution log it must follow that  $U_{i}=U_{i-1}\setminus \var(c^*),S_{i}=S^*_{i-1},T_{i}=T^*_{i-1},c_{i}=c^*,X_{i}=X_{i-1}\oplus X^*_{\var(c^*)}$ and $Y_{i}=Y_{i-1}\oplus Y^*_{\var(c^*)}$, where $X^*_{\var(c^*)}$ and $Y^*_{\var(c^*)}$ are two partial assignments over $\var(c^*)$, $S^*_{i-1}$  is the simplification of $S_{i-1}$ with respect to $X^*_{\var(c^*)}$ and $T^*_{i-1}$ is the simplification of $T_{i-1}$ with respect to $Y^*_{\var(c^*)}$. Here it must satisfy that $c^*$ is not satisfied by $X^*_{\var(c^*)}$ and $c^*$ is satisfied by $Y^*_{\var(c^*)}$ by combining Lines \ref{Line:couple-1-psat-cond} and \ref{Line:couple-1-sample} of \Cref{Alg:couple-1} and that $L$ is a proper execution log. In this case $\+E_{i}$ happens if and only if the following happens:
 \begin{enumerate}
 \item $\+E_{i-1}$ happens;\label{item:equiv-1}
 \item  The condition at \Cref{Line:couple-1-psat-cond} of $\+C'(U_{i-1},S_{i-1},T_{i-1})$ is not satisfied, meaning $c^*$ is not satisfied by $X^{\text{samp}}$; \label{item:equiv-2}
 \item $X^*_{\var(c^*)}=X^{\text{samp}}_{\var(c^*)}$ and $Y_{\var(c^*)}=Y^{\text{samp}}_{\var(c^*)}$.\label{item:equiv-3}
 \end{enumerate}
 Note that here \Cref{item:equiv-2} is already implied by \Cref{item:equiv-3} since $c^*$ is not satisfied by $X^*_{\var(c^*)}$. By induction hypothesis, this is equivalent to both $\+E^{\text{samp}}_{i-1}$ happens, $X^*_{\var(c^*)}=X^{\text{samp}}_{\var(c^*)}$ and $Y_{\var(c^*)}=Y^{\text{samp}}_{\var(c^*)}$. By the definition of $\+E^{\text{samp}}_{i}$ and $S_{i}=S^*_{i-1},T_{i}=T^*_{i-1}$ where $S^*_{i-1}$  is the simplification of $S_{i-1}$ with respect to $X^*_{\var(c^*)}$ and $T^*_{i-1}$ is the simplification of $T_{i-1}$ with respect to $Y^*_{\var(c^*)}$, we have this is equivalent to $\+E^{\text{samp}}_{i}$ happens and the claim holds in this case.
\end{itemize}
This finishes the induction step. Hence we have proved that $\Lambda(V,S^{\text{in}},T^{\text{in}})$ and $\Lambda'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed.


By comparing \Cref{Alg:couple} and \Cref{Alg:couple-1}, it is straightforward by \Cref{definition:execution-log} that the output of $\+C(V,S^{\text{in}},T^{\text{in}})$ and $\+C'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed. Also, by \Cref{definition:bad-constraint}, both sets of bad constraints are uniquely determined by their corresponding execution log through the same process. We then have  $B(V,S^{\text{in}},T^{\text{in}})$ and $B'(V,S^{\text{in}},T^{\text{in}})$ are identically distributed.
\end{proof}


We can now prove \Cref{lemma:disjoint-prob}.


\begin{proof}[Proof of \Cref{lemma:disjoint-prob}]

By \Cref{lemma:alter-equiv}, it is sufficient to prove that 
\[
\Pr[\+C']{A\subseteq B'(V,S^{\text{in}},T^{\text{in}})}\leq p^{\frac{2|A|}{2+\zeta}}\cdot (1-\mathrm{e}p)^{-2(D+1)|A|}.
\]

Recall that for each atomic constraint $c$, we use $\sigma_c\in \bigotimes_{v\in \var(c)}Q_v$ to represent the unique assignment that violates it. We claim that for each $c\in A$, $c\in B'(V,S^{\text{in}},T^{\text{in}})$ implies the following event 
$$\+E_c: \text{For each $v\in \var(c)$, either $X^{\text{samp}}(v)=\sigma_c(v)$ or $Y^{\text{samp}}(v)=\sigma_c(v)$}.$$

To prove the claim, we suppose for the sake of contradiction that there exists some $v\in \vbl(c)$ such that both 
$X^{\text{samp}}(v)\neq \sigma_c(v)$ and $Y^{\text{samp}}(v)\neq \sigma_c(v)$. Recall the succinct representation of simplified constraints in \Cref{remark:succinct}. Let $(c,Z)$ be the simplified constraint when $c$ is added into $B'(V,S^{\text{in}},T^{\text{in}})$. It must hold that $v\in Z$, as otherwise $c$ is both satisfied in $S$ and $T$, and could not have been chosen by the algorithm. However, $v\in Z$ is also not possible as $(c,Z)$ is not satisfied by $X^{\text{samp}}$ or $Y^{\text{samp}}$ by $c\in B'(V,S^{\text{in}},T^{\text{in}})$, \Cref{definition:bad-constraint} and \Cref{Alg:couple-1}. So we have a contradiction, and the claim is proved.
 
 Then we have
 \begin{equation}\label{eq:disjoint-prob-1}
\begin{aligned}
\Pr[\+C']{A\subseteq B(V,S^{\text{in}},T^{\text{in}})}\leq & \Pr[\+C']{\bigwedge\limits_{c\in A}\+E_c}\\
(\text{by $A$ is disjoint})\quad\leq &\prod\limits_{c\in A}\Pr[\+C']{\+E_c}\\
\leq & \prod\limits_{c\in A}\left(\prod\limits_{v\in \var(c)}\left(2\+D_v(\sigma_c(v))-\+D_v^2(\sigma_c(v))\right)\cdot (1-\mathrm{e}p)^{-2(D+1)}\right).
\end{aligned}
 \end{equation}

Here, the third inequality is by interpreting the probability space for generating $X^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in S^{\text{in}}}c\right)$ and $Y^{\text{samp}}\sim \+P\left(\cdot \mid \bigwedge\limits_{c\in T^{\text{in}}}c\right)$ as the product space over two copies of distribution $\+P$, conditioning on all constraints in $S^{\text{in}}$ are satisfied in the first product space, and all constraints in $T^{\text{in}}$ are satisfied in the second product space. Note that this can be viewed as an LLL distribution with dependency degree at most $D$ and violation probability of each bad event at most $p$. Also, each event $\+E_c$ is mutually dependent on all but $2(D+1)$ bad events. Therefore, setting $x(c)=\mathrm{e}p$ for each bad event $c$ and applying \Cref{HSS} leads to the third equality. 

Note that by $S^{\text{in}}$ and $T^{\text{in}}$ are both atomic constraints we have for each $c\in S^{\text{in}}\cup T^{\text{in}}$,
\[
    \prod\limits_{v\in \var(c)}\+D_v(\sigma_c(v))\leq p, 
\]
and for any $v\in \var(c)$
\[
    \frac{\ln \left(2\+D_v(\sigma_c(v))-\+D_v^2(\sigma_c(v))\right)}{\ln \left(\+D_v(\sigma_c(v))\right)}=2-\frac{\ln(2\+D_v^{-1}(\sigma_c(v))-1)}{\ln(\+D_v^{-1}(\sigma_c(v)))}\geq 2-\frac{\ln(2\chi_{\min}-1)}{\ln \chi_{\min}}=\frac{2}{2+\zeta},
\]
where the second-to-last inequality is by that $\frac{\ln(2x-1)}{\ln x}$ is a monotone decreasing function for $x>1$ and $\chi_{\min}$ lower bounds $\+D_v^{-1}(\sigma_c(v))$.

Hence, combining with \eqref{eq:disjoint-prob-1} we have
\[
\Pr[\+C']{A\subseteq B(V,S^{\text{in}},T^{\text{in}})}\leq \prod\limits_{c\in A}\left(p^{\frac{2}{2+\zeta}}\cdot (1-\mathrm{e}p)^{-2(D+1)}\right)=p^{\frac{2|A|}{2+\zeta}}\cdot (1-\mathrm{e}p)^{-2(D+1)|A|},
\]
completing the proof.
\end{proof}



Recall the definition of the dependency graph $G_\+C$ in \Cref{definition:parameters-input}. The next lemma states that each connected component of bad constraints in $G_\+C$ includes some discrepancy in the initial set of constraints.


\begin{lemma}\label{lemma:comp-disc}
Each connected component of $G_{\+C}(B(V,S^{\text{in}},T^{\text{in}}))$ contains at least one $c\in S^{\text{in}}\ominus T^{\text{in}}$.
\end{lemma}
\begin{proof}
For any proper execution log $L=(E_0,E_1\dots,E_{\ell})$ where $E_j=(U_j,S_j,T_j,c_j,X_j,Y_j)$ for each $0\leq j\leq \ell$,  and any $0\leq i\leq \ell$, let $B(L,i)\subseteq S^{\text{in}}\ominus T^{\text{in}}$ be the set of constraints constructed from $L$ as follows. Initially $B(L,i)=\emptyset$. For $j=1,\dots,i$,
\begin{enumerate}
    \item If $X_{j}=X_{j-1}$, do nothing.
    \item Otherwise, add $\+F(c_j)$ into $B(L,i)$, where $\+F(c_j)$ denote the original constraint of $c_j$ as defined in \Cref{remark:succinct}.
\end{enumerate}
We claim that each connected component of $G_{\+C}(B(L,i))$ contains at least one $c\in S^{\text{in}}\ominus T^{\text{in}}$, then the lemma immediately follows by comparing the above process with the process in \Cref{definition:bad-constraint}.

We then prove the claim by an induction on $i$ from $0$ to $\ell$. The base case is when $i=0$, and the claim trivially holds as $B(L,i)=\emptyset$.

For the induction step, we assume $i>0$. If $X_i=X_{i-1}$, then $B(L,i)=B(L,i-1)$ and the claim holds. 

Otherwise if $c_i=\+F(c_i)$, then it must follow that  $c_i\in S_{i-1}\ominus T_{i-1}$ by \Cref{definition:execution-log} and the conditions in Lines \ref{Line:couple-choose} and \ref{Line:couple-choose-1}. Also by \Cref{definition:execution-log} we further have $c_i\in S^{\text{in}}\ominus T^{\text{in}}$, and the claim holds in this case.

Otherwise $c_i\neq \+F(c_i)$, then by \Cref{Alg:couple} and \Cref{definition:execution-log} we have $\+F(c_i)$ must share variables with some $c\in B(L,i-1)$. By induction hypothesis and \Cref{definition:parameters-input} we have the claim also holds in this case. This finishes the induction step and the proof of the lemma.
\end{proof}

We also need the following notion of $2$-trees, which dates back to the work of Alon~\cite{Alon91}. 

\begin{definition}[$2$-tree]\label{definition:2-tree}
Let $G=(V,E)$ be a graph and $\text{dist}_G(\cdot,\cdot)$ denote the shortest path distance in $G$.
A \emph{$2$-tree} in $G$ is a subset of vertices $T\subseteq V$ such that: 
\begin{itemize}
    \item  for any $u,v\in T$, $\text{dist}_G(u,v)\geq2$; 
    \item  $T$ is connected if an edge is added between every $u,v\in T$ such that $\text{dist}_G(u,v)=2$.
\end{itemize}
\end{definition}


The following two lemmas regarding properties of $2$-trees are known.


\begin{lemma}[\text{\cite[Corollary 5.7]{FGYZ20}}]\label{lemma:num-2-tree}
Let $G = (V, E)$ be a graph with maximum degree $d$ and $v \in V$ be a vertex. Then the
number of $2$-trees in $G$ of size $\ell$ containing $v$ is at most $\frac{\left(\mathrm{e}d^2\right)^{\ell-1}}{2}$.
\end{lemma}

\begin{lemma}[\text{\cite[Lemma 4.5]{Vishesh21sampling}}]\label{lemma:2-tree}
Let $G = (V, E)$ be a graph with maximum degree $d$. Let $H = (V (H), E')$ be a
connected subgraph of $G$ and let $v\in V (H)$. Then, there exists a $2$-tree T with $v \in T \subseteq V (H)$ such
that $\abs{T} \geq \frac{\abs{V (H)}}{d + 1}$.
\end{lemma}




We are finally ready to prove \Cref{lemma:couple-dis}.
\begin{proof}[Proof of \Cref{lemma:couple-dis}]
For each $c\in S^{\text{in}}\cup  T^{\text{in}}$, we use $\textsf{Comp}(c)$ to denote the set of constraints in the same connected component with $c$ in $G_{\+C}(B(V,S^{\text{in}},T^{\text{in}}))$. If $c\notin B(V,S^{\text{in}},T^{\text{in}})$, then $\textsf{Comp}(c)=\emptyset$. By \Cref{lemma:comp-disc} we have
\begin{equation}\label{eq:couple-dis-1}
 \abs{B(V,S^{\text{in}},T^{\text{in}})}\leq \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\abs{\textsf{Comp}(c)}.
\end{equation}
Therefore
\begin{equation*}
\begin{aligned}
&\E[\+C]{d_{\textrm{Ham}}(X^{\+C},Y^{\+C})}\\
(\text{by \Cref{lemma:disc-bound}})\quad\leq &  k\cdot \E[\+C]{\abs{B(V,S^{\text{in}},T^{\text{in}})}} \\
(\text{by \eqref{eq:couple-dis-1}})\quad\leq &  k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\E{\abs{\textsf{Comp}(c)}}\\
\leq &  k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=0}^{\infty}\left((D+1)\cdot\sum\limits_{i=1}^{\infty}\Pr{\abs{\textsf{Comp}(c)}\geq i (D+1)+1}\right)\\
\end{aligned}
\end{equation*}
For each $c\in S^{\text{in}}\cup T^{\text{in}}$ and $i\geq 1$, we use $\MSC{T}_c^i$ denote the set of all $2$-trees in $G_{\+C}(S^{\text{in}}\cup T^{\text{in}})$ of size $i$ containing $c$. Then we have
\begin{equation*}
\begin{aligned}
&\E[\+C]{d_{\textrm{Ham}}(X^{\+C},Y^{\+C})}\\
\leq &k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=0}^{\infty}\left((D+1)\cdot\sum\limits_{i=1}^{\infty}\Pr{\abs{\textsf{Comp}(c)}\geq i (D+1)+1}\right)\\
(\text{by \Cref{lemma:2-tree}} )\quad\leq &  k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=1}^{\infty}\left((D+1)\cdot\sum\limits_{A\in \MSC{T}_c^i}\Pr{A\subseteq B(V,S^{\text{in}},T^{\text{in}})}\right)\\
(\text{by \Cref{lemma:disjoint-prob}} )\quad\leq & k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=1}^{\infty}\left((D+1)\cdot\sum\limits_{A\in \MSC{T}_c^i}\left(p^{\frac{2i}{2+\zeta}}(1-\mathrm{e}p)^{-2(D+1) i}\right)\right)\\
(\text{by \Cref{lemma:num-2-tree}} )\quad\leq & k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=1}^{\infty}\left((D+1)\cdot \frac{\left(\mathrm{e}D^2\right)^{i-1}}{2}\left(p^\frac{2}{2+\zeta}\right)^{ i}(1-\mathrm{e}p)^{-2(D+1) i}\right)\\
(\text{by \eqref{eq:correlation-decay-condition-couple}} )\quad\leq & k\cdot \sum\limits_{c\in S^{\text{in}}\ominus  T^{\text{in}}}\sum\limits_{i=1}^{\infty}\left((D+1)\cdot \frac{\left(\mathrm{e}D^2\right)^{i-1}}{2}\cdot\left(4\mathrm{e}D^2\delta\right)^{- i}\cdot 2^{i}\right)\\
\leq &\frac{k(D+1)}{2\delta} \cdot \abs{S^{\text{in}}\ominus  T^{\text{in}}}.
\end{aligned}
\end{equation*}

\end{proof}

\begin{comment}
\begin{proof}[Proof of \Cref{lemma:couple-dis}]

For the proof of the theorem, we first claim that the event $X^{\+C}(v)\neq Y^{\+C}(v)$ implies there exists a sequence of disjoint constraints $A=(c_1,c_2,\dots,c_{t})$ such that 

\begin{enumerate}
    \item $c_1\in S^{\text{in}}\ominus T^{\text{in}}$
    \item $ t=\lceil\frac{d(v)}{2}\rceil$
    \item $\text{dist}_{G}(c_i,c_{i+1})\leq 2$ for each $1\leq i<t$
\end{enumerate}


To prove the claim, let $(c'_1,c'_2,\dots,c'_{\ell} )$ be the sequence of constraints with \emph{minimum length} (breaking ties arbitrarily) satisfying that 
\begin{enumerate} 
    \item $c'_i\in B(V,S^{\text{in}},T^{\text{in}})$ for $1\leq i\leq \ell$
    \item $c'_1\in S^{\text{in}}\ominus T^{\text{in}}$, $v\in \var(c'_{\ell})$
    \item $\var(c'_i)\cap \var(c'_{i+1})\neq \varnothing$ for $1\leq i<\ell$.
\end{enumerate} 

The existence of such a sequence of constraints is by \Cref{lemma:disc-cover} and \Cref{lemma:comp-disc}. 


We further claim that $(c'_1,c'_2,\dots,c'_{\ell} )$ forms an induced path in $G(B(V,S^{\text{in}},T^{\text{in}}))$, that is, for $1\leq i,j\leq \ell$ such that $i+1<j$, it must hold that $\var(c_i)\cap \var(c_j)\neq \varnothing$, because otherwise $(c_1,\dots,c_i,c_j,\dots,c_{\ell} )$ is still a valid sequence with smaller length, contradicting the minimality assumption. Note that by comparing with the definition of $d(v)$ we have $\ell\geq d(v)$. It is then straightforward to check that the sequence of constraints with the first $\lceil\frac{d(v)}{2}\rceil$ odd indices $(c_1,c_3,\dots,c_{2\lceil\frac{d(v)}{2}\rceil-1} )$ satisfies the properties. This proves the initial claim.

For each $i\geq 1$, let $P^{i}$ denote the set containing all sequences of disjoint constraints $(c_1,c_2,\dots,c_i)$ such that $c_1\in S^{\text{in}}\ominus T^{\text{in}}$ and $\text{dist}_{G}(c_i,c_{i+1})\leq 2$ for each $1\leq j<i$. It is immediate that $|P^{i}|\leq |S^{\text{in}}\ominus T^{\text{in}}|\cdot (\Delta^2)^{i-1}$. Let $\alpha=\lceil\frac{d(v)}{2}\rceil$. Then we have
\begin{equation*}
\begin{aligned}
&\Pr[\+C]{X^{\+C}(v)\neq Y^{\+C}(v)}\\
\leq &  \sum\limits_{A\in P^{\alpha}}\Pr{A\subseteq B(V,S^{\text{in}},T^{\text{in}})}\\
(\text{by \Cref{lemma:disjoint-prob}})\quad\leq &  \sum\limits_{A\in P^{\alpha}}\left(p^{\frac{2\alpha}{2+\zeta}}(1-\mathrm{e}p)^{-2\alpha\Delta}\right)\\
\leq &  |S^{\text{in}}\ominus T^{\text{in}}|\cdot (\Delta^2)^{\alpha-1}\cdot  \left(p^{\frac{2}{2+\zeta}}\right)^{\alpha}\cdot \left((1-\mathrm{e}p)^{-2\Delta}\right)^{\alpha}\\
 (\text{by \eqref{eq:correlation-decay-condition}} )\quad\leq & |S^{\text{in}}\ominus T^{\text{in}}|\cdot (\Delta^2)^{\alpha-1} \cdot (2\mathrm{e}\Delta^2)^{-\alpha}\cdot 2^{\alpha}\\
 \leq & |S^{\text{in}}\ominus T^{\text{in}}|\cdot \mathrm{e}^{-\alpha},
\end{aligned}
\end{equation*}

which finishes the proof.

\end{proof}
\end{comment}

\newpage
\section{Lower bounds for correlation decay in the LLL regime}\label{sec:lower-bound}

In this section, we prove \Cref{thm:lower-bound}. We will construct our hard instance using properties of the free Gibbs distribution on hardcore models on infinite regular trees in the non-uniqueness regime. Generally, we will first show the local unboundedness of one-to-all total influence of the hardcore distribution on infinite regular trees in the non-uniqueness regime, and use this result to motivate the construction of the hard instance.

\subsection{Pairwise influence for the hardcore model in the non-uniqueness regime}

In this subsection, we will derive some hardness results from the hardcore model. Specifically, we will prove that $\Vert\Psi_{\mu}\Vert_{\infty}$ is unbounded for hardcore models on infinite regular trees in the non-uniqueness regime. 

A hardcore model is specified by an undirected graph $G = ( V, E)$ and a fugacity parameter $\lambda \geq 0$.
The Gibbs distribution over a hardcore model is a distribution over independent sets $I$ of $G$ weighted as $\frac{\lambda^{|I|}}{Z}$ where $Z$ is the normalizing constant called the partition function. When the distribution is clear, for each vertex $v\in V$, we simply write $v=1/v=0$ to denote $v$ is in/out of the independent set. 




We consider the hardcore model on trees. It has been known~\cite{Kelly85stochastic} that there exists a critical threshold $\lambda_c(\Delta)=\frac{(\Delta-1)^{(\Delta-1)}}{(\Delta-2)^{\Delta}}$ such that the Gibbs distribution is unique on the infinite $\Delta$-regular tree $\mathbb{T}_{\Delta}$ if and only if $\lambda<\lambda_c(\Delta)$. This is referred to as ``the uniqueness regime''.  It has been shown in the literature (e.g., see ~\cite{galanis2016inapproximability}, Section 3) that at the non-uniqueness regime of $\mathbb{T}_{\Delta}$, there exists a unique translation invariant Gibbs measure $\mu^*$ (which is referred to as the free Gibbs distribution) and two semi-translation invariant measures $\mu^+$ and $\hat{\mu}^-$. At the end of this subsection, we will prove the following lemma. 

\begin{lemma}\label{lemma:gibbs-free-unbounded}
If $\lambda>\lambda_c(\Delta)=\frac{(\Delta-1)^{(\Delta-1)}}{(\Delta-2)^{\Delta}}$, then $\Vert\Psi_{\mu^*}\Vert_{\infty}$ is unbounded.
\end{lemma}

We remark that by symmetry, all rows of $\Psi_{\mu^*}$ have the same sum, therefore \Cref{lemma:gibbs-free-unbounded} implies that the maximum eigenvalue of the pairwise influence matrix $\lambda_{\max}(\Psi_{\mu^*})$ is  also unbounded.

A key tool to analyze the  behavior of the hardcore model on trees is tree recurrence for occupancy ratios. For some tree $T$ and some vertex $v\in T$, let $p_{T,v}$ be the marginal probability that $v$ is occupied under the distribution of the hardcore model on $T$. We also define the occupancy ratio as $R_{T,v}\defeq \frac{p_{T,v}}{1-p_{T,v}}$. Fix some tree $T$ with root $r$. For some vertex $v\in V$, we write $T_v$ as the subtree of $T$ rooted at $v$. We write $u\sim v$ to represent the vertex $v$ is a child of the vertex $u$ in $T$. We then have the following tree recurrence:
\begin{equation}\label{eq:general-recurrence}
    R_{T,r}=\lambda\prod\limits_{r\sim v}\frac{1}{R_{T_v,v}+1}
\end{equation}


To study the behavior of the hardcore distribution on $\Delta$-regular trees, it is then helpful to consider the following univariate recurrence:
\begin{equation}\label{eq:univariate-recurrence}
    f(R)=\lambda\left(\frac{1}{R+1}\right)^{\Delta-1}
\end{equation}

As $f(0)=\lambda$ and $f$ is monotone decreasing in $[0,+\infty)$, $f$ has a unique fixed point $R^*$ for all $\lambda>0$. 

We also consider the hardcore model on the infinite $(\Delta-1)$-ary tree $\hat{\mathbb{T}}_{\Delta}$, which has exactly the same uniqueness threshold as on $\mathbb{T}_{\Delta}$. Note that the only difference between  $\mathbb{T}_{\Delta}$ and $\hat{\mathbb{T}}_{\Delta}$ is the degree of the root. Denote by $\hat{\mu}^*,$ on $\hat{\mathbb{T}}_{\Delta}$ as the analog of measures $\mu^*$ on  $\mathbb{T}_{\Delta}$.

Denote $q^*=\hat{\mu}^*(r=1)$ as the marginal probability for the root to be in the independent set on $\hat{\mathbb{T}}_{\Delta}$ under the free Gibbs measure. It is direct to see that
\[
R^*=\frac{q^*}{1-q^*}.
\]



The following lemma from \cite{galanis2016inapproximability} characterizes the occupancy ratio $R^*$ on the hardcore model on $(\Delta-1)$-regular trees at the non-uniqueness regime.

\begin{lemma}[\text{\cite[Lemma 8]{galanis2016inapproximability}}]\label{lemma:hardcore-nonuniqueness-bound}
In the non-uniqueness regime of hardcore model on $\hat{\mathbb{T}}_{\Delta}$, it holds that
\[
(\Delta-1)\cdot \frac{R^*}{1+R^*}>1.
\]
\end{lemma}




For any two vertices $u,v$ and any distribution $\mu$, let 
\begin{equation*}
    \Psi^{+}_{\mu}(u, v)\defeq \Pr[\mu]{v=1\mid u=1}-\Pr[\mu]{v=1\mid u=0}
\end{equation*}
be the signed influence of $u$ to $v$. Comparing with \eqref{eq:definition-influence} we immediately have $\Psi_{\mu}(u,v)=|\Psi^{+}_{\mu}(u,v)|$ in the hardcore model. The following lemma states a crucial property of signed influence on trees. 

\begin{lemma}[\text{\cite[Lemma B.2]{anari2020spectral}}]\label{lemma:influence-tree}
Let $\mu$ be a Gibbs distribution with Boolean domains on some tree $T$.  Let $u, v, w$ be distinct vertices in $T$ such that $w$ is on the unique path from $u$ to $v$. Then \[\Psi^{+}_{\mu}(u, v) = \Psi_{\mu}^{+}(u, w) \cdot \Psi_{\mu}^{+}(w, v),\]
\end{lemma}





\begin{corollary}
\label{cor:gibbs-free-entry-bound}
Suppose $\lambda$ is in the non-uniqueness regime of $\hat{\mathbb{T}}_{\Delta}$. For any $u\sim v$ in $\hat{\mathbb{T}}_{\Delta}$, it holds that
\[
-\frac{1}{\Delta-1}<\Psi_{\hat{\mu}^*}^+(u,v)<0.
\]
\end{corollary}

\begin{proof}
    Note that 
    \[\Psi_{\hat{\mu}^*}^+(u,v)=\Pr[\hat{\mu}^*]{v=1\mid u=1}-\Pr[\hat{\mu}^*]{v=1\mid u=0}=-\Pr[\hat{\mu}^*]{v=1\mid u=0}=-q^*=-\frac{R^*}{1+R^*},\]
     Therefore the result directly follows from \Cref{lemma:hardcore-nonuniqueness-bound}.
\end{proof}


Now we can prove \Cref{lemma:gibbs-free-unbounded}.
\begin{proof}[Proof of \Cref{lemma:gibbs-free-unbounded}]
Let $S$ be the set of all vertices in $\mathbb{T}_{\Delta}$. For any two vertices $u,v\in S$, let $\text{dist}(u,v)$ be the shortest path distance between $u$ and $v$. Note that $\Psi^+_{\hat{\mu}^*}(u,v)$ is the same for any $u\sim v$ in $\hat{\mathbb{T}}_{\Delta}$ by symmetry, and we can then use $\alpha$ to denote this quantity. Since the only difference between  $\mathbb{T}_{\Delta}$ and $\hat{\mathbb{T}}_{\Delta}$ is the degree of the root, we also have $\Psi^+_{\mu^*}(u,v)=\alpha$ for any $u\sim v$ in $\mathbb{T}_{\Delta}$. Then we have
\begin{equation*}
    \begin{aligned}
    &\Vert\Psi_{\mu^*}\Vert_{\infty}\\
    \geq& \sum\limits_{v\in S\setminus \{r\}}\Psi_{\mu^*}(r,v)\\
    =&\sum\limits_{i=1}^{\infty}\sum\limits_{\substack{v\in S\setminus \{r\}\\ \text{dist}(r,v)=i}}\Psi_{\mu^*}(r,v)\\
    (\text{by \Cref{lemma:influence-tree}})\quad=&\sum\limits_{i=1}^{\infty}\sum\limits_{\substack{v\in S\setminus \{r\}\\ \text{dist}(r,v)=i}}\left(-\alpha\right)^i\\
    =&\frac{\Delta}{\Delta-1}\sum\limits_{i=1}^{\infty}\left((\Delta-1)\cdot (-\alpha)\right)^i\\
    (\text{by \Cref{cor:gibbs-free-entry-bound}})\quad=&+\infty
    \end{aligned}
\end{equation*}
\end{proof}


\subsection{Recovering free Gibbs distribution in the non-uniqueness regime on finite trees}



Fix any $\Delta\geq 3$ and any $\lambda>\lambda_c(\Delta)$, let $R^*$ be the unique fixed point of \eqref{eq:univariate-recurrence} and $q^*=R^*/(1+R^*)$. For any integer $n\geq 2$, we let $\mathbb{T}_{\Delta}(n)$ be the first $n$ levels of $\mathbb{T}_{\Delta}$.  We define $\mu_{n}$ as the following distribution over independent sets $I$ of $\mathbb{T}_{\Delta}$:
\begin{equation}\label{eq:definition-mu-n}
\mu_n(I)\propto \lambda^{|I\setminus L(I)|}\cdot (q^*)^{|L(I)|},
\end{equation}
where $L(I)$ denotes the set of leaf nodes in $I$, i.e. the set of nodes at level $n$ in $I$. This distribution can be viewed as the Gibbs distribution constructed from the hardcore model on $\mathbb{T}_{\Delta}(n)$ by changing the external field on leaf nodes from $\lambda$ to $q^*$. 

The following lemma characterizes the pairwise influence of the distribution $\mu_{n}$ between adjacent vertices.


\begin{lemma}\label{lemma:finite-gibbs-free-entry-bound}
Suppose $\lambda>\lambda_c(\Delta)$. For any $n\geq 2$ and any $u\sim v$ in $\mathbb{T}_{\Delta}(n)$, it holds that
\[
\Psi_{\mu_n}^+(u,v)=\Psi_{\mu_n}^+(v,u)=-\frac{R^*}{1+R^*}
\]
\end{lemma}
\begin{proof}
    Note that   
    \[\Psi_{\mu_n}^+(u,v)=\Pr[\mu_n]{v=1\mid u=1}-\Pr[\mu_n]{v=1\mid u=0}=-\Pr[\mu_n]{v=1\mid u=0}=-\frac{R^*}{1+R^*}.
    \]
    
    Here, the last equality is by that after setting $u=0$, the marginal probability of $v$ is equal to the marginal probability of the root of some complete $(\Delta-1)$-ary tree, which can be calculated using \eqref{eq:general-recurrence}. Note that by \eqref{eq:definition-mu-n}, the leaves have marginal probability $q^*$ and occupancy ratio $R^*$ at their respective subtrees. Then by $R^*$ is the fixed point of \eqref{eq:univariate-recurrence} we have the marginal probability at $v$ is also $q^*=\frac{R^*}{1+R^*}$.

    Similarly, we have
     \[\Psi_{\mu_n}^+(v,u)=\Pr[\mu_n]{u=1\mid v=1}-\Pr[\mu_n]{u=1\mid v=0}=-\Pr[\mu_n]{u=1\mid v=0}=-\frac{R^*}{1+R^*}.
    \]
    Here, the last equality is due to a similar reason: after setting $v=0$, we can remove the subtree of $v$ when computing the marginal probability of $u$. Consider $u$ as the root in the remaining tree, where each vertex has $(\Delta-1)$ children. By \eqref{eq:definition-mu-n}, the leaves have marginal probability $q^*$ and occupancy ratio $R^*$ at their respective subtrees. Therefore, the marginal probability at $u$ is $q^*=\frac{R^*}{1+R^*}$.
\end{proof}


Finally, we can show that the one-to-all total influence on $\mu_n$ is locally unbounded.

\begin{lemma}\label{lemma:gibbs-finite-unbounded}
Suppose $\lambda>\lambda_c(\Delta)$. For any $n\geq 2$, it holds that
\begin{equation*}
\Vert\Psi_{\mu_n}\Vert_{\infty} \geq \frac{\Delta}{\Delta-1}\cdot \left(1+\delta\right)^{n-1},
\end{equation*}
where $\delta=(\Delta-1)\cdot \left(-\frac{R^*}{1+R^*}\right)-1>0$ by \Cref{cor:gibbs-free-entry-bound}.
\end{lemma}
\begin{proof}
Let $S$ denote the set of all vertices in $\mathbb{T}_{\Delta}(n)$. We then have
\begin{equation*}
    \begin{aligned}
    &\Vert\Psi_{\mu_n}\Vert_{\infty}\\
    \geq &\sum\limits_{v\in S\setminus \{r\}}\Psi_{\mu_n}(r,v)\\
    =&\sum\limits_{i=1}^{\infty}\sum\limits_{\substack{v\in S\setminus \{r\}\\ \text{dist}(r,v)=i}}\Psi_{\mu_n}(r,v)\\
    (\text{by \Cref{lemma:influence-tree} and \Cref{lemma:finite-gibbs-free-entry-bound}})\quad=&\sum\limits_{i=1}^{n-1}\sum\limits_{\substack{v\in S\setminus \{r\}\\ \text{dist}(r,v)=i}}\left(-\frac{R^*}{1+R^*}\right)^i\\
    \geq & \frac{\Delta}{\Delta-1}\sum\limits_{i=1}^{n-1}\left((\Delta-1)^i\cdot \left(-\frac{R^*}{1+R^*}\right)^i\right)\\
    \geq & \frac{\Delta}{\Delta-1}\cdot \left(1+\delta\right)^{n-1}.
    \end{aligned}
\end{equation*} 

\end{proof}

\subsection{Reduction from the distribution defined by atomic CSPs}

Note that there is a natural interpretation of the Gibbs distribution $\mu_n$ described in \eqref{eq:definition-mu-n} as a distribution specified by an atomic CSP $\Phi=(V, C)$: each vertex in $\mathbb{T}_{\Delta}(n)$ corresponds to a variable in $V$, each variable has domain $\{0,1\}$, where the underlying distribution differs for variables corresponding to leaf and non-leaf vertices: variables corresponding to non-leaf vertices take value $1$ with probability $\frac{\lambda}{1+\lambda}$, while variables corresponding to leaf vertices take value $1$ with probability $q^*$. The independent set restriction translates to a constraint for each $e\in E$ that forbids the both-$1$ assignment on variables corresponding to incident vertices. It is straightforward to verify that this gives a distribution defined by an atomic CSP, with maximum violation probability $p=\frac{\lambda^2}{(1+\lambda)^2}$ (note that $f(\lambda)<\lambda$, therefore $R^*<\lambda$ and $q^*<\frac{\lambda}{1+\lambda}$) and dependency degree $D=2(\Delta-1)$. Hence the non-uniqueness condition in \Cref{lemma:gibbs-finite-unbounded} translates to the condition in the following lemma.

\begin{lemma}\label{lemma:gibbs-to-LLL}
    For any $0<p<1$, and positive even integer $D\geq 4$ satisfying 
    \[
        pD^2\geq 4,
    \]
    then the (one-to-all) total influence $\Vert\Psi_{\mu}\Vert_{\infty}$ is locally unbounded for distribution $\mu$ defined by atomic CSPs with parameters $p,D$.
\end{lemma}
\begin{proof}
    Interpret the $\mu_n$ described in \eqref{eq:definition-mu-n} as a distribution defined by an atomic CSP as stated above. We then have the following equality:
    \[
    p=\frac{\lambda^2}{(1+\lambda)^2}, \quad D=2(\Delta-1).
    \]
     The condition translates to $\Delta\geq p^{-\frac{1}{2}}+1\geq \frac{1}{\lambda}+2$, we then have that
    \[
        \lambda_{c}(\Delta)=\frac{(\Delta-1)^{(\Delta-1)}}{(\Delta-2)^{\Delta}}=\frac{1}{\left(1-\frac{1}{\Delta-1}\right)^{\Delta-1}}\cdot \frac{1}{\Delta-2}< \frac{1}{\Delta-2}\leq \lambda,
    \]
    hence by \Cref{lemma:gibbs-finite-unbounded}, the (one-to-all) total influence $\Vert\Psi_{\mu}\Vert_{\infty}$ is locally unbounded.
\end{proof}

Now notice that we can arbitrarily split the domain with value $0$ of each variable in the LLL instance we constructed in \Cref{lemma:gibbs-to-LLL} to make it satisfy $\chi_{\min}= 1+\frac{1}{\lambda}$. Then $D\geq \frac{2}{\lambda}+4$ satisfies all conditions in \Cref{lemma:gibbs-to-LLL}. Hence, we can take $D(\chi_{\min})=2\chi_{\min}+2$ and \Cref{thm:lower-bound} directly follows from \Cref{lemma:gibbs-to-LLL}.

\begin{comment}
    
However, we haven't finished because  the construction in \Cref{lemma:gibbs-to-LLL} has domain size $2$, while our upper bound on the exponent only approaches $2$ when the domain size is large. Therefore we apply the tensorization technique to transform the hard instance constructed above to work when the domain size is large, by splitting each variable into copies of variables with underlying product maps to the same bad event with the same probability. 

\begin{definition}[Domain tensorization for LLL distribution with atomic bad events]
    Let $\Phi=(V,\+A)$ be an atomic LLL instance, for any $k\geq 1$, we define its $k$-domain tensorized LLL instance $\Phi_k=(V_k,\+A_k)$ as follows:
    \begin{itemize}
        \item $V_k=\bigcup\limits_{v\in V}\{u_{v,1},u_{v,2},\dots,u_{v,k}\}$
    \end{itemize}
\end{definition}

\end{comment}




\section{Conclusion and future directions}\label{sec:conclusions}

In this work, we study the correlation decay property on distributions defined by atomic CSPs in the local lemma regime through one-to-all total influences. We present both an upper and lower bound for the regimes where the one-to-all total influence is bounded/unbounded, showing that the gap closes at the threshold $pD^2\lesssim 1$ when a special distortion parameter $\chi_{\min}$ grows to infinity.

Beyond characterizing the threshold up to which the correlation decay property occurs, our threshold $pD^2\lesssim 1$ coincides with the lower bound for the tractability of sampling LLL~\cite{BGG19,galanis2021inapproximability}. This provides evidence that the correct threshold for tractability of sampling LLL is $pD^2\lesssim 1$, where the correlation decay property occurs/vanishes.

Our work also suggests several possible future directions:
\begin{itemize}
    \item Our current upper bound (\Cref{thm:correlation-decay}) for bounded one-to-all total influence only approaches the regime $pD^2\lesssim 1$ when the special parameter $\chi_{\min}$ grows to infinity. Is it possible to show bounded one-to-all total influence in the same regime without this restriction?
    \item Our work focuses on atomic CSPs, a commonly-studied subclass of general CSPs. Is it possible to extend the result to general CSPs?
    \item Most importantly, does the result of bounded one-to-all total influence have any algorithmic implications under the optimal local lemma regime $pD^2\lesssim 1$?
\end{itemize}

\bibliographystyle{alpha}
\bibliography{references} 
\clearpage
\end{document}