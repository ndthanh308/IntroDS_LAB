%!TEX root = MST_brownian.tex

In this section we prove the lower bound on the Hausdorff dimension of $\sM$. We use the mass distribution principle using the mass measure $\mu$ that is defined in Section~\ref{sub:mass_measure}. The asymptotics for the $\mu$-mass of small balls are provided in Section~\ref{sub:evolution_mass_ball} and relies heavily on the growth process defined in Section~\ref{sec:compactness}.

\subsection{The mass measure} % (fold)
\label{sub:mass_measure}

We start with the construction of the mass measure $\mu$ on $\sM$. The measures in this section will always be seen as Borel measures on $\sM$, the completion of $\R_+$ with respect to $d$. For $t\in \R_+$ we let $\mu_t$ be the rescaled Lebesgue measure on $[0,t]$: $\mu_t(A)=t^{-1} \Leb(A\cap [0,t])$. For each $t$, $\mu_t$ is a probability measure on $\sM$ which charges only a subtree containing the root $0$ (it is easy to see that $[0,t]$ is connected). Let $\cL$ be the set of leaves of $\sM$, that is the set of points $x$ such that $\sM\setminus \{x\}$ is connected. Our aim in this section is the following
\begin{prop}\label{prop:mass_measure}
With probability one, as $t\to\infty$, $\mu_t$ converges weakly to a limit probability measure that we denote by $\mu$ and call the mass measure on $\sM$. Furthermore $\mu(\R_+)=0$ so $\mu(\cL)=1$.
\end{prop} 

Recall the notation in Section~\ref{sub:the_growth_process}. For a subset $S$ of the tree $\sM$ and $x\in \R_+$, we define the projection of $x$ onto $S$ as the point of $S$ that is closest to $x$. Fix $\lambda_0\in \R$. We are interested in the projection onto the subset of $\sM$ consisting of the points $[0,R_{\lambda_0}]$. For $x\in \sM$, we let $[x]_{\lambda_0}$ denote the corresponding point. Observe that, with the notation of the previous section, a.s.\ $[x]_{\lambda_0}=\sup\{p^n(x) \cap [0,R_{\lambda_0}], n\ge 0\}$.


Even though $[0,R_{\lambda_0}]$ is not closed in $\sM$, we will always have $[x]_{\lambda_0}\in [0,R_{\lambda_0})$ for the points $x$ we consider. Rather than working with the measures $\mu_t$, $t\in \R_+$, it will be more convenient to work with $\bar \mu_\lambda := \mu_{R_\lambda}$ for $\lambda \in \R$; Lemma~\ref{lem:position_Hlambda} which says that $R_\lambda\to\infty$ guarantees that taking the limits as $t\to\infty$ or $\lambda\to\infty$ is equivalent.

We define the following process: for a Borel set $S\subseteq [0,R_{\lambda_0}]$ and $\lambda\ge \lambda_0$, 
\begin{equation}\label{eq:def_martingale}
M_\lambda=M_\lambda(S):=\bar \mu_\lambda(\{x\le R_\lambda: [x]_{\lambda_0} \in S\})\,.
\end{equation}
We will consider only the randomness coming from $\bU$ and study $M_\lambda$ conditionally on $\sigma(R_\lambda:\lambda\ge \lambda_0)$. We let $\bP^{\shortdownarrow}$ and $\bE^\shortdownarrow$ be the corresponding probability and expectation. 

\begin{lem}\label{lem:mass_martingale}
The process $(M_\lambda, \lambda \ge \lambda_0)$ is almost surely a supermartingale under $\bP^\shortdownarrow$.
\end{lem}
\begin{proof}For each $\lambda\in \Lambda^\ssp$, $\lambda>\lambda_0$, all the points $x\in q_\lambda$ have the same projection on $[0,R_{\lambda_0}]$ since $p(x)=\xi(l_\lambda)$. Furthermore, in order to determine where an interval $q_\lambda$ projects onto $H_{\lambda_0}$ it suffices to follow the sequence of random projections/jumps $p^n(x)$, $n\ge 1$. Almost surely, $\inf\{n\ge 0: p^n(l_\lambda)\in H_{\lambda_0}\}<\infty$ and the point $[l_\lambda]_{\lambda_0}$ is uniformly random in $H_{\lambda_0}$. In the following, $\lambda_0$ being fixed, we use $\phi(\lambda)$ as a short-hand for $[l_\lambda]_{\lambda_0}$. The points $\phi(\lambda)$, $\lambda>\lambda_0$, are of course not independent because of the coalescence of the trajectories. Then,
\begin{equation}\label{eq:mass_martingale}
M_\lambda = \frac 1 {R_\lambda} \left[{M_{\lambda_0}\cdot R_{\lambda_0}} + \sum_{\lambda_0<\lambda'\le \lambda } m_{\lambda'} \I{\phi(\lambda') \in S}\right].
\end{equation}
It follows that, writing $\cF_\lambda$ for the sigma-algebra generated by $(\ju(l_{\lambda'}): \lambda
'\le \lambda)$, the random variable $M_\lambda$ is bounded and $\cF_\lambda$-measurable. The expression in \eqref{eq:mass_martingale} is amenable to a simple evaluation of the conditional expectations: for $h\ge 0$,
\begin{align*}
\bE^\shortdownarrow[M_{\lambda+h}~|~\cF_\lambda]
  & = \frac 1 {R_{\lambda+h}} \cdot \bE^\shortdownarrow \Bigg[M_{\lambda_0}\cdot R_{\lambda_0} + \sum_{\lambda_0< \lambda'\le \lambda + h} m_{\lambda'}\I{\phi(\lambda') \in S}~\Bigg|~\cF_\lambda \Bigg]\\
  & = \frac 1 {R_{\lambda+h}} \Bigg(R_\lambda \cdot M_\lambda + \sum_{\lambda<\lambda'\le \lambda+h} m_{\lambda'} \bP^\shortdownarrow (\phi(\lambda')\in S~|~\cF_\lambda)\Bigg)\,.
\end{align*}

However, almost surely conditionaly on $\cF_\lambda$, $\phi(\lambda')\in S$ if and only if the first point of the sequence $(p^i(l_{\lambda'}))_{i\ge 1}$ that falls in $H_\lambda$ lies in some interval $q_{\lambda''}$ itself such that $\phi(\lambda'')\in S$. By definition, such a point is the projection of $l_{\lambda'}$ on $H_\lambda$, and is uniform in $H_\lambda \subset [0,R_\lambda]$ and therefore $\bP^\shortdownarrow(\phi(\lambda')\in S~|~\cF_\lambda)\le M_\lambda$. It follows that 
\begin{align*}
\bE^\shortdownarrow [M_{\lambda+h}~|~\cF_\lambda]
& \le \frac{M_\lambda} {R_{\lambda+h}} \cdot \Bigg(R_\lambda + \sum_{\lambda<\lambda'\le \lambda+h} m_{\lambda'}\Bigg)  = M_\lambda\,,
\end{align*}
which completes the proof.
\end{proof}

% \begin{alert}I wrote on some notes ``Need to be careful about the continuity of $\cF_\lambda$'', but not clear why anymore. 
% \end{alert}






\begin{proof}[Proof of Proposition~\ref{prop:mass_measure}]
Since $\sM$ is compact by Proposition~\ref{pro:space_compact}, the collection of measures $(\mu_t)_{t> 0}$ is tight. We prove that it is Cauchy for the Prohorov metric using the super-martingales $M_\lambda$ we have just introduced. Recall that, for two Borel measures $\nu$ and $\nu'$ on $(\sM,d)$, the Prohorov distance is given by 
\[\dP(\nu,\nu')=\inf\{\epsilon>0: \nu(A)\le \nu'(A^\epsilon)+\epsilon, \nu'(A)\le \nu(A^\epsilon)+\epsilon \text{ for all Borel sets } A\}\,\]
where $A^\epsilon=\{x: d(x,A)<\epsilon\}$. 

The arguments for compactness in Section~\ref{sec:compactness} show that for any $\epsilon>0$, there exists a $\lambda_0\in \R_+$ such that $\sup_x d(x,[0,R_\lambda])<\epsilon$ for all $\lambda\ge \lambda_0$. With this choice for $\lambda_0$, it follows that, for any $\lambda>\lambda_0$, 
\[\dP(\mu_\lambda,[\mu_\lambda]_{\lambda_0})<\epsilon\,,\] 
where $[\mu_\lambda]_{\lambda_0}$ denotes the image of $\mu_\lambda$ by the projection onto $[0,R_{\lambda_0}]$. Therefore, for any $\lambda,\lambda'\ge \lambda_0$,
\[\dP(\mu_\lambda,\mu_{\lambda'})\le \dP([\mu_\lambda]_{\lambda_0}, [\mu_{\lambda'}]_{\lambda_0}) + 2 \epsilon\,.\]
To complete the proof, cover $[0,R_{\lambda_0}]$ with finitely many balls of diameter $\epsilon$, say $B_1,B_2,\dots, B_k$. Then, by definition of $M_\lambda(S)$, we can construct a coupling $(X,Y)$ with $X\sim [\mu_\lambda]_{\lambda_0}$ and $Y\sim [\mu_{\lambda'}]_{\lambda_0}$ such that $(X,Y)\not \in \cup_{i} B_i \times B_i$ with probability at most $\sum_{i} |M_\lambda(B_i)-M_{\lambda'}(B_i)|$. Since the diameter of $B_i$ is at most $\epsilon$, the cost of the coupling on $B_i\times B_i$ is at most $\epsilon$. It follows that 
\[\dP([\mu_\lambda]_{\lambda_0},[\mu_{\lambda'}]_{\lambda_0}) \le \sum_{i=1}^k |M_{\lambda}(B_i)-M_{\lambda'}(B_i)| + \epsilon \,,\]
which is at most $2\epsilon$ for all $\lambda,\lambda'$ large enough because of the convergence of the mass super-martingales of Lemma~\ref{lem:mass_martingale}. This completes the proof of convergence.

The two additional properties are straightforward from the definition. First, for any interval $[i,i+1)$, $\mu([i,i+1))=\lim_\lambda \mu_\lambda([i,i+1])= 0$ since $R_\lambda\to \infty$, and thus $\mu(\R_+)=0$. The completion of $\R_+$ with respect to $d$ only adds leaves, so $\sM\setminus \R_+\subseteq \cL$, and therefore $\mu(\cL)=1$. 
\end{proof}


% subsection mass_martingales (end)


\subsection{The mass of balls around zero} % (fold)
\label{sub:evolution_mass_ball}

It is proved in \cite{AdBrGoMi2013a} that the Minkowski dimension of $\sM$ is almost surely equal to 3, and we thus only need to find a lower bound. For this, we aim at using the mass distribution principle with the mass measure. In this direction, one needs to upper bound the $\mu$-mass of balls centered at points with distribution $\mu$. In general, this might be delicate since we need to identify the balls around these points, and they are almost surely not in $\R_+$ (Proposition~\ref{sub:mass_measure}). This is why the following result is crucial; the intuition should be intuitively clear from the discrete setting, where the point $1$ can be replaced in Prim's algorithm by a uniformly random point in $[n]$ without altering the distributions. For a point $x\in \sM$ and $r> 0$ we let $B_x(r)$ denote the open ball of radius $r$ centered at $x$ (for the metric $d$).

\begin{lem}\label{lem:uniform_point}Let $\zeta$ be a point of $\sM$ with distribution $\mu$. Then, the processes $(\mu(B_0(r)))_{r\ge 0}$ has the same distribution as $(\mu(B_\zeta(r)))_{r\ge 0}$. 
\end{lem}
\begin{proof}
For $n\ge 1$, recall from Section~\ref{sec:motivation_and_history} that $v_1,v_2,\dots, v_n$ denote the Prim order on $[n]$ on the complete graph with edge weights $(w_e)$, $e\in E^n$; in what follows, we will occasionally write $v(i)$ instead of $v_i$. 
For $n\ge 1$, let $V^n_\lambda$ be the collection of vertices connected to $v(\lfloor n^{2/3}\rfloor)$ in the random graph with edge weights at most $p_n(\lambda)$, and let $H^n_\lambda$ denote the collection of their Prim ranks, and let $L^n_\lambda=\min H^n_\lambda$. Let $\hat \mu^n_\lambda$ denote the uniform probability distribution on $H^n_\lambda$. 
By Lemma~\ref{lem:dist_left-most-node}, conditionally on $V^n_\lambda$, the vertex $v(L^n_\lambda)$ is uniformly random in $V^n_\lambda$, and independent of the random variables $w_{e}$ associated to the edges with end points in $V^n_\lambda$. 
It follows in particular that, for any $r\ge 0$, 
\begin{align}\label{eq:mass_discrete_ball}
  \mu^n_\lambda(\{u \in H^n_\lambda: d^n_\lambda(L^n_\lambda, u)\le r n^{1/3}\}) 
\eqdist
\mu^n_\lambda(\{u \in H^n_\lambda: d^n_\lambda(\zeta^n_\lambda, u)\le r n^{1/3}\}) \,,
\end{align}
where $\zeta^n_\lambda$ denotes an independent point with distribution $\hat \mu^n_\lambda$ (uniform in $H^n_\lambda$).
By Proposition~\ref{pro:convergence_distances}, $(H^n_\lambda, d^n_\lambda, \mu^n_\lambda, L^n_\lambda, \zeta^n_\lambda)$ converges in distribution in the sense of Gromov--Prokhorov to $(H_\lambda, d, \hat \mu_\lambda, L_\lambda, \zeta_\lambda)$, where $\zeta_\lambda$ is an independent point with distribution $\mu_\lambda$. This implies the convergence of the random variables in \eqref{eq:mass_discrete_ball} as $n\to\infty$ towards
\[
\hat\mu_\lambda(\{u\in H_\lambda: d(L_\lambda,u)\le r\})
\eqdist
\hat\mu_\lambda(\{u\in H_\lambda: d(\zeta_\lambda,u)\le r\})\,.
\]
Now, $d(0,L_\lambda)\le \diam(\sP_\lambda)\to 0$ as $\lambda\to \infty$ by Proposition~\ref{pro:left-end}. Note also that a straightforward coupling yields $\dP(\hat \mu_\lambda, \mu_\lambda)\le L_\lambda/R_\lambda \le L_\lambda$ (independently of the metric since we can match the points exactly on a set of probability $1-L_\lambda/R_\lambda$). Taking the limit as $\lambda\to\infty$, Lemma~\ref{lem:position_Hlambda} and Proposition~\ref{prop:mass_measure} yield the claim for every fixed $r\ge 0$. This is easily extended to the joint convergence for finitely many values $r_1<r_2<\dots<r_k$, which completes the proof.
\end{proof}

In order to upper bound $\mu(B_0(r))$ we will proceed in two steps: we will first upper bound $\bar \mu_\lambda(B_0(r))$ showing that it is of the correct order of magnitude, that is roughly $r^{3}$ for some well-chosen $\lambda$ depending on $r$ ($\lambda$ of order $1/r$); we will then rely on the concentration for the mass supermartingales of the previous section, which controls the evolution of the mass as $\lambda$ increases, to show that $\mu(B_0(r))=\lim_\lambda \bar \mu_\lambda(B_0(r))$ remains of order $r^3$. Once we have the relevant upper bound for a fixed $r$, the proof is easily completed using routine arguments (taking a suitable subsequence and the Borel--Cantelli lemma). 

\begin{prop}\label{pro:mass_distribution-lambda}There exists a constant $c>0$ such that, for any $\epsilon\in (0,1)$ and every $r>0$ small enough,
\[\pc{\bar \mu_{r^{\epsilon-1}}(B_0(r)) > r^{3-\epsilon}} \le r^{c\epsilon}\,.\]
\end{prop}
\begin{proof}Observe that if $d(0,H_\lambda)>r$ then no point of $H_\lambda$ lies within $B_0(r)$, so that $\bar\mu_\lambda(B_0(r))$ is at most $L_\lambda/R_\lambda$. Using this with $\lambda=r^{\epsilon-1}$, it follows that
\begin{align}\label{eq:bound_mubar_start}
\pc{\bar \mu_{r^{\epsilon-1}}(B_0(r)) > r^{3-6\epsilon}}
& \le \p{\bar \mu_{r^{\epsilon-1}}(B_0(r)) > \frac{L_{r^{\epsilon-1}}}{R_{r^{\epsilon-1}}}} + \p{\frac{L_{r^{\epsilon-1}}}{R_{r^{\epsilon-1}}} > r^{3-6\epsilon} } \notag \\
& \le \pc{d(0,H_{r^{\epsilon-1}}) \le r } + \pc{L_{r^{\epsilon-1}} > r^{2-4\epsilon}} + \pc{R_{r^{\epsilon-1}} \le r^{-1+2\epsilon}}\notag \\ 
& \le \pc{d(0,H_{r^{\epsilon-1}}) \le r } + \exp(-r^{-\epsilon})\,,
\end{align}
where the last line follows, for all $r>0$ small enough, from the bounds in Lemma~\ref{lem:position_Hlambda}. 

Most of the work now consists in bounding the first term in \eqref{eq:bound_mubar_start} above. Observe that the geodesic from $H_{r^{\epsilon-1}}$ to $0$ must cross every single one of the metric spaces induced by $\sM$ on the intervals $q_\lambda\subseteq [0,1]$ with $\lambda>r^{\epsilon-1}$. Furthermore, with the notation of Section~\ref{sub:distances_in_swallowed_components}, $d(0,H_{r^{\epsilon-1}})$ decomposes as follows: since here the portion of path in $q_\lambda$ is precisely between $\ju(\sup q_\lambda)\in q_\lambda$ a.s. and $\inf q_\lambda$; we have 
\begin{align*}
d(0,H_{r^{\epsilon-1}}) 
%= \sum_{\lambda\in \Lambda^\ssm} d(\ell_\lambda, \zeta_\lambda) 
= \sum_{\lambda\in \Lambda^\ssm} Y_{\lambda} \I{\lambda > r^{\epsilon-1}}
\ge_{st} \sum_{\lambda\in \Lambda^\ssm} Y^\star_{\lambda} \cdot m_{\lambda}^{1/2} \cdot \I{\lambda> r^{\epsilon-1}}\,,
\end{align*}
where the last inequality is a stochastic minoration that relies on Proposition~\ref{pro:diameter_small}: the $m_\lambda$, $\lambda \in \Lambda^\ssm$, are the sizes of the jumps of $L_\lambda$, and the $Y^\star_\lambda$ are conditionally independent from the entire collection $(m_\lambda: \lambda\in \Lambda^\ssm)$. In order to lower bound $d(0,H_{r^{\epsilon-1}})$ it suffices to focus on a single term of the sum in the right-hand side: if any of those terms is greater than $r$, then $d(0,H_{r^{\epsilon-1}})>r$ as well, thus
\begin{align}\label{eq:bound_mubar}
\pc{d(0,H_{r^{\epsilon-1}}) \le r} 
%& \le \p{\sum Y_\lambda^\star m_\lambda^{1/2} \I{\lambda r^{\epsilon-1}>1} \le r} \notag\\ 
& \le \p{\#\{\lambda>r^{1-\epsilon}: \lambda\in \Lambda^\ssm, Y^\star_\lambda m_\lambda^{1/2} > r\}=0} \notag\\ 
& \le \p{\sup\{m_\lambda: \lambda\in \Lambda^\ssm, \lambda>r^{1-\epsilon}\}\le r^{2-\epsilon}} + \pc{Y^\star \le r^{\epsilon/2}}\,.
\end{align}
The second term is at most $r^{c\epsilon}$ by Proposition~\ref{pro:diameter_small} \emph{ii)} and Markov's inequality. The $(m_\lambda)$ are also the lengths of the faces of the convex minorant of $X$ on the interval $[0,1]$ by Lemma~\ref{lem:fragment_of_x}, and to deal with the first term, we relate it to the convex minorant of a standard Brownian motion $W$. 

Let $(Q_t)_{t\ge 0}$ be defined by 
\begin{align*}
Q_t
&=\exp\left(\int_0^t s dW_s - \frac 1 2 \int_0^t s^2 ds\right)\,,
\end{align*}
Then by the Cameron--Martin--Girsanov formula (Theorem 38.5 of \cite{RoWi2000}), the laws of $X$ and $W$ are related by a change of measure whose density is given by the martingale $Q_t$. For a function $\omega \in \cC(\R_+, \R)$, we consider the convex minorant of $\omega$ on $[0,1]$ and we let $\chi_r(\omega)$ denote the indicator that the longest face with slope (strictly) smaller than $-r^{\epsilon-1}$ has length at most $r^{2-\epsilon/2}$. Then, by Lemma~\ref{lem:fragment_of_x} we have
\begin{align}\label{eq:convex_minorant_CS}
\p{\sup\{m_\lambda: \lambda\in \Lambda^\ssm, \lambda>r^{1-\epsilon}\}\le r^{2-\epsilon}}
& = \Ec{\chi_r(X^0)} \notag \\ 
& = \Ec{\chi_r(W) \cdot Q_1} \notag \\ 
& \le \Ec{\chi_r(W)^2}^{1/2} \cdot \Ec{Q_1^2}^{1/2}\,,
\end{align}
by the Cauchy--Schwarz inequality. Observe that in the right-hand side above, $\Ec{Q_1^2}\le \Ec{\exp(2 \overline{W}_1)}$ is finite and independent of $r$ thanks to the Gaussian tails of $\overline W_1$.

The first factor in \eqref{eq:convex_minorant_CS} can be estimated using the results of Pitman and Ross \cite[Theorem~1]{PiRo2011a} and Brownian scaling. Let $(x_i,s_i)_{i}$ be the points of a Poisson point process with intensity $(2\pi x)^{-1/2} \cdot \exp(-(2+s^2)x/2)dxds$ on $\R_+\times \R$. Then $(x_i,s_i)$ are the lengths and slopes of the faces of the convex minorant of $W$ on the interval $[0,E]$ where $E$ is an independent exponential random variable with mean one, and here $E=\sum x_j$. Therefore, by Brownian scaling,
\begin{align*}
\Ec{\chi_r(W)}
&= \p{\sup\{ x_i : s_i \sqrt E <-r^{\epsilon-1}\} \le E \cdot r^{2-\epsilon}}\\ 
& \le \p{A^c} + \p{\sup\{x_i: s_i r^{1-5\epsilon/6}<-1\} \le r^{2-4\epsilon/3}}\,,
\end{align*}
where $A$ denotes the event that $E=\sum x_j \in [r^{\epsilon/3}, r^{-\epsilon/3}]$.
Since $E$ is exponential with mean one, we have $\pc{A^c}\le 2r^{\epsilon/3}$ for all $r>0$ small enough. We claim that there exists a constant $c>0$ such that the second term above is no larger than $r^{c\epsilon}$, and in order the complete the proof, it suffices to justify that claim. We slightly change the scaling and write $\delta=r^{1-5\epsilon/6}$ and $\gamma=\epsilon/3$ to lighten the notation. Then $r^{2-4\epsilon/3}\le \delta^{2+\gamma}$, and we focus on 
\begin{align}\label{eq:around_zero_main}
\p{\sup\{x_i: s_i \delta <-1\} \le \delta^{2+\gamma}}
% & \le \p{\#\{i: x_i> r^{2+2\delta}, s_i r< - 1\} = 0 } \\
& = \exp\left(-\int\!\!\!\!\int \frac{e^{-(2+s^2)x/2}}{\sqrt{2\pi x}} \I{x\ge \delta^{2+\gamma}, s\delta<-1} ds dx\right)\,.
\end{align}
We just need to lower bound the integral in the right-hand side: consider the subregion $\Sigma$ of $[\delta^{2+\gamma}, \infty)\times (-\infty, -1/\delta]$ where $u=xs^2\le 1$, that is $\Sigma:=\{(x,s): \delta^{2+\gamma}\le x\le \delta^2, -x^{-1/2}\le s\le -1/\delta\}$:
\begin{align*}
\int\!\!\!\!\int \frac{e^{-(2+s^2)x/2}}{\sqrt{2\pi x}} \I{x\ge \delta^{2+\gamma}, s \delta<-1} ds dx 
& \ge \int\!\!\!\!\int_\Sigma \frac{e^{-(s^2+2)x/2}}{\sqrt{2\pi x}}dsdx\\ 
&\ge e^{-\delta^2} \int\!\!\!\!\int_\Sigma \frac{e^{-s^2x/2}}{\sqrt{2\pi x}}dsdx \\ 
% & = \frac 1 {2 \sqrt{2\pi}} \int_{r^{2+2\epsilon}}^{r^2} \frac{\Delta s(x)}{\sqrt x} dx \\ 
& \ge \frac {e^{-\delta^2-1/2}} {\sqrt{2\pi}} \int_{\delta^{2+\gamma}}^{\delta^2}\left[\frac 1 x - \frac 1{\delta\sqrt x} \right] dx \\ 
& \ge \frac {-\gamma \log \delta e^{-\delta^2-1/2}} { \sqrt{2\pi}} \,,
\end{align*}
for all $\delta>0$ small enough.
It follows easily that, there exists a constant $c>0$ such that for all $\delta>0$ small enough the right-hand side of \eqref{eq:around_zero_main} is at most $\delta^{c\gamma}$, which translated into the original parameters yields a bound of $r^{c'\epsilon}$ for the right-hand side of \eqref{eq:convex_minorant_CS}, and in turn for \eqref{eq:bound_mubar} and \eqref{eq:bound_mubar_start}. This completes the proof.
\end{proof}

Finally, using Lemma~\ref{lem:uniform_point}, the following proposition completes the proof of Theorem~\ref{thm:compact_dimH}

\begin{prop}\label{pro:mass_distribution}Let $\zeta$ be a point of $\sM$ with distribution $\mu$. Then for every $\epsilon\in (0,1)$, almost surely, for all $r>0$ small enough we have
\[\mu(B_\zeta(r)) \le r^{3-\epsilon}\,.\]
As a consequence $\DimH(\sM)\ge 3$.
\end{prop}
\begin{proof}By Lemma~\ref{lem:uniform_point}, it suffices to prove the bound for $\mu(B_0(r))$. Fix $\lambda_0=1/r$, set $S=B_0(r)$ and recall the process $M_\lambda=\bar\mu_\lambda(\{x\le R_\lambda: d(0,[x]_{\lambda_0}) \le r \})$ of \eqref{eq:def_martingale}. Then, for any $\lambda\ge \lambda_0$, $\bar\mu_\lambda(B_0(r))$ is no larger than the $\bar\mu_\lambda$-mass of the excursions which are grafted within distance $r$ or the origin: 
we have 
\[\bar\mu_\lambda(B_0(r)) \le M_\lambda\,.\]
Since $M_\lambda$ is bounded, Lemma~\ref{lem:mass_martingale} implies that $M_\lambda$ converges almost surely as $\lambda\to\infty$, but it also implies some concentration results since the increments of $M_\lambda$ are bounded by the $m_\lambda$, $\lambda \in \Lambda^\ssp$. By the Azuma--Hoeffding inequality \cite{Azuma1967,Hoeffding1963,BoLuMa2012a}, for any $x>0$, we have, conditionally on $(m_\lambda, \lambda\in \Lambda)$,
\begin{align}\label{eq:azuma_mass-martingale}
\p{M_\lambda - M_{\lambda_0}>x~|~m_\lambda, \lambda>\lambda_0} 
%&\le \exp\bigg(-\frac{x^2}{2 \sum_{\lambda_0<\lambda'\le \lambda} m_{\lambda'}^2}\bigg)\notag \\ 
&\le \exp\bigg(-\frac{x^2}{2 \sum_{\lambda_0<\lambda'} m_{\lambda'}^2}\bigg)\,.
\end{align}

Bounding the $\ell^2$-norm of $(m_\lambda)_{\lambda>\lambda_0}$ is routine using Lemma~\ref{lem:sizes_fragments-k}. Indeed, for any $k\ge 1$, 
\begin{align*}
\E{\sum_\lambda m_\lambda^2 \cdot \I{\lambda \in \Lambda_k}} 
&\le 8k^5\int_0^\infty \frac{t^2}{\sqrt{2\pi t^3}} e^{-k^6 t/8} dt
%&= \frac{8 k^5 }{\sqrt{2\pi}} \int_0^\infty t^{1/2} e^{-k^6t/8} dt\\
%&= \frac{8^{5/2} k^{-4} }{\sqrt{2\pi}} \int_0^\infty u^{1/2} e^{-u} du 
= 64 \cdot k^{-4}\,,
\end{align*}
and Markov's inequality then yields, for some constant $K$,  
\begin{equation}\label{eq:tail_masses}
  \p{\sum_{\lambda: \lambda r>1} m_\lambda^2 \ge r^{3-\epsilon}} \le r^{\epsilon-3} \cdot \E{\sum_\lambda m_\lambda^2 \cdot \I{\lambda >1/r}} \le K r^\epsilon\,.
\end{equation}
Since $M_{\lambda_0}=\bar\mu_{\lambda_0}(B_0(r))$, it follows from Proposition~\ref{pro:mass_distribution-lambda} and \eqref{eq:azuma_mass-martingale}--\eqref{eq:tail_masses} that 
\begin{align*}
\pc{\mu(B_0(r) \ge 2 r^{3-\epsilon})} 
& \le \pc{\mu_{\lambda_0}(B_0(r)) \ge r^{3-\epsilon}} + \pc{M_\infty - M_{\lambda_0} \ge r^{3-\epsilon}} \\
& \le r^{c\epsilon} + \exp(-c r^{-\epsilon}) + K r^\epsilon\,.
\end{align*}

From there, completing the proof is standard: take a subsequence $r_i=2^{-i}$, $i\ge 1$; the Borel--Cantelli implies that for all but finitely many values of $i\ge 1$, we have $\mu(B_0(r_i))\le 2 r_i^{3-\epsilon}$, and thus $\mu(B_0(r))\le 16 r^{3-\epsilon}$ for all $r>0$ small enough. As a consequence, the mass distribution principle (see, e.g., Proposition~4.9 of \cite{Falconer1990a}) implies that $\DimH(\sM)\ge 3-\epsilon$, which completes the proof since $\epsilon>0$ was arbitrary.
\end{proof}


% subsection subsection_name (end)


% subsection the_mass_measure (end)
