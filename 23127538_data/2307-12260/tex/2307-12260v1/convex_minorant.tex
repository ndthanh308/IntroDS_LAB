%!TEX root = MST_brownian.tex

\subsection{Convex minorants of continuous functions}
\label{sec:convex_minorants_continuous}

Let $D\subseteq \R$ be an interval containing $0$ that will in general be $[0,1]$ or $\R_+$ in the sequel. Let $\cC(D,\R)$ be the set of continuous functions on $D$ equipped with the uniform distance. 
For $\omega\in \cC(D,\R)$ such that $\omega(0)=0$ and $x\in D$, the (greatest) convex minorant of $\omega$ on $[0,x]$ is the maximum convex function $c_x(\cdot, \omega)$ defined on $[0,x]$ such that $c_x(t,\omega)\le \omega(t)$ for all $t\in [0,x]$. We let $\cV_x(\omega)=\{t\in [0,x]: c_x(t,\omega)=\omega(t)\}$, and call the elements of $\cV_x(\omega)\setminus\{x\}$ the vertices of the convex minorant of $\omega$ on $[0,x]$. Observe that $0\in \cV_x(\omega)$. We shall see shortly why the extremity $x$ ought to be treated differently. 

Up to now, the literature has mainly focused on properties of the convex minorant of a function on a fixed domain (for a fixed $x$). Of prime importance to us, is instead the structure of the different convex minorants $c_x(\cdot, \omega)$ of a fixed function $\omega$ as $x\in D$ varies. We start with the following straightforward (deterministic) geometric observation: 

\begin{lem}\label{lem:convex_minorants_intersection}
Let $\omega\in \cC(D,\R)$ be such that $\omega(0)=0$ and let $x,y\in D$ with $0\le y<x$. For any $t\in \cV_x(\omega) \cap [0,y]$, we have $\cV_y(\omega) \cap [0,t] = \cV_x(\omega) \cap [0,t]$.
\end{lem}
In other words, traversing them from the left to the right, the convex minorants $c_x(\cdot, \omega)$ and $c_{y}(\cdot, \omega)$ on $[0,x]$ and $[0,y]$ coincide on a non-empty closed interval, and then split for good. This induces a natural branching structure for $\{\cV_x(\omega), x\in D\}$ that is depicted in Figure~\ref{fig:CM_branching} that is central to the paper. This also justifies that for $t\in \cV_x(\omega)$, the slope of the convex minorant $c_x(\cdot, \omega)$ to the left of $t$, defined by  
\[\slo(t,\omega)=\sup\bigg\{\frac{c_x(t) - c_x(t-s)}{s}: t-s\in \cV_x(\omega) \bigg\}\,,\]
is well-defined intrinsically since for any $x'$ such that $t\in \cV_{x'}(\omega)$ would yield the same value. 
If $t\in \cV_x(\omega)$ for some $x$, let 
\[\zi(t,\omega)=\inf\{s>t: \omega(s) \le \omega(t) +\slo(t,\omega) (s-t)\}\,.\]
We call $\zi(t,\omega)$ the \emph{intercept associated to $t$}; this is defined independently of the choice of $x$ for which $t\in \cV_x(\omega)$. (The notation $\zi(t,\omega)$ comes from ``right''.) The following is clear by construction: 
\begin{lem}\label{lem:intercept}Suppose that $t\in \cV_x(\omega)$ for some $x\in D$. 
\begin{compactitem}[i)]
    \item If $y\in [t,\zi(t,\omega)]$, then $t\in \cV_y(\omega)$.
    \item If $y<t$ or $y>\zi(t,\omega)$ then $t\not\in \cV_y(\omega)$.
\end{compactitem}
\end{lem}
Observe that if $\sL(\omega)$ denotes the set of local minima of $\omega$, then $\slo(t,\omega)$ and $\zi(t,\omega)$ are well-defined for every $t\in \sL(\omega)\setminus \{0\}$. 

% Figure environment removed

\subsection{Convex minorants of Brownian paths} % (fold)
\label{sec:convex_minorant_of_brownian_paths}

We are interested in convex minorants of various Brownian-like paths such as Brownian motion or the Brownian excursion, the latter being more essential because of classical path decompositions. Such convex minorants have been studied for instance by \citet{Groeneboom1983a} and \citet{Pitman1982a}; in the following, we will mostly rely on the work of \citet{PiRo2011a} that provides means to do explicit calculations; more information about related studies and references can be found there. We therefore now focus on these cases.

Since we should focus on the structure of $\cV_x(\omega)$ as $x$ varies, the following lemma is crucial. 
%{\nic Let $\bW$ be the Wiener measure on $\cC(\R_+,\R)$.}

\begin{lem}[No exceptional point]\label{lem:no_exception_convex}There exists a Borel set $\Omega^\star$ of $\cC(\R_+,\R)$ with $\bW(\Omega^\star)=1$, such that if $\omega\in \Omega^\star$, then for every $x\in \R_+$, it holds that: 
\begin{compactenum}[i)]
  \item $\cV_x(\omega)$ is countable; 
  \item %if $\sup \cV_x < x$ then $\cV_x$ is finite; equivalently, 
  $\cV_x(\omega)$ has no accumulation point in $(0,x)$; 
  \item the elements of $\cV_x(\omega)\setminus \{0,x\}$ are all local minima;
  \item the slopes $\slo(t,\omega)$ at the points $t\in \cV_x(\omega)\setminus \{0,x\}$ are all distinct.
\end{compactenum}
\end{lem}
% \begin{alert}  
% \JF{Question: je ne comprends pas pourquoi il y a un prime, à $c$. Il faut le supprimer, je pense}

% Réponse: C'est bien une pente, il faut une dérivée
% \end{alert}

\begin{rem}Note that a version of Lemma~\ref{lem:no_exception_convex} holds for a Brownian excursion $\exc$ on $[0,1]$ instead of a Brownian motion: in this case, the claims in \emph{i)--iv)} also hold, even with $[0,x)$ instead of $(0,x)$ in \emph{ii)}. 
\end{rem}

Let $(W_t)_{t\ge 0}$ be a Brownian motion on $\R_+$. The results of \cite{PiRo2011a,Groeneboom1983a} imply that for $W$, the set of exceptional points $x$, for which one of the properties in \emph{i)}--\emph{iii)} might fail has Lebesgue measure zero. We verify that with probability one, there is no exceptional point by showing that if there were an exceptional point, then a.s.\ the set of such points would be of positive Lebesgue measure. In the following, we drop the dependence in $W$. 

\begin{proof}[Proof of Lemma~\ref{lem:no_exception_convex}]
\emph{i)} Suppose that, with positive probability, there is some $x$ is such that $\cV_x$ is uncountable. Since there are countably many intervals $[a,b]$ with rational endpoints $0\le a<b<x$, one of them must be such that $\cV_x\cap [a,b]$ is uncountable. By Lemma~\ref{lem:convex_minorants_intersection}, for any point $y\in [b,x]$ we have $\cV_x\cap [a,b] \subseteq \cV_y$, so that the set of exceptional points would have positive Lebesgue measure. 

\emph{ii)} Suppose that, with positive probability, there exists some $x\in \R_+$ such that $\cV_x$ has a an accumulation point $y$ in $(0,x)$. By construction, for any $w\in (y,x]$, we have $\cV_x \cap [0,y] \subseteq \cV_{w} \cap [0,y]$, so that the set of points $w$ for which \emph{ii)} fails has positive Lebesgue measure, a contradition.

\emph{iii)} Finally, suppose that there is some $x\in \R_+$ and $t\in \cV_x\setminus \{0,x\}$ which is not a local minimum. Then, by Lemma~\ref{lem:convex_minorants_intersection}, for any $y\in [t,x]$ we have $t\in \cV_y$, and the proof is complete since $t<x$.

\emph{iv)} If the slopes at $t$ and $t'$ such that $t'<t<x$ are identical, then the same holds for the convex minorants $c_y$ on the intervals $[0,y]$ for every $y\in [t,x]$, so that the set of exceptional points has positive Lebesgue measure.
\end{proof}

The typical situation is that $\cV_x(W)$ has accumulation points at both $0$ and $x$, but it may also happen that $x$ is not an accumulation point: this happens for instance when $x$ is a local minimum or $x=\zi(t,W)$ for some local minimum $t$. 
Let $(t_i)_{i\in \Z}=(t_i(x))_{i\in \Z}$ denote the vertices in $\cV_x(W)\setminus \{x\}$, indexed in such a way that $t_i\le t_{i+1}$ and $t_0=\arg\min\{W_s:s \in [0,x]\}$. In the case where $x$ is not an accumulation point of $\cV_x(W)$, it is understood that the sequence is only defined for $i\le k$ for some $k \ge 0$. The intervals $[t_i,t_{i+1}]$ where the slope of $c_x(W)$ is constant are called the \emph{faces} of the convex minorant. Let $\gamma_i$ denote the slope of the convex minorant on $[t_i,t_{i+1}]$, and $z_i$ be the intercept associated to $t_i$:
\[\gamma_i=\slo(t_{i+1},W)=\frac{W(t_{i+1})-W(t_i)}{t_{i+1}-t_i}
\quad \text{and} \quad
z_i=\zi(t_i,W)=\inf\{s>t_i:W_s\le W_{t_i}+\gamma_{i-1} (s-t_i)\}\,.
\]
It is possible that $z_i=\infty$ for some $i\le 0$, but a.s.\ $z_i<\infty$ for $i\ge 1$. For every $i\ge 1$, such that $z_i<\infty$, and for $s\ge 0$ let 
\begin{equation}\label{eq:decomp_in_excursions}
g_i(s):=(W_{t_i+s}-W_{t_i}-\gamma_i s) \I{t_i+s\le t_{i+1}}
\quad \text{and}\quad 
h_i(s)=(W_{t_i+s}-W_{t_i}-\gamma_{i-1} s ) \I{t_i+s\le z_i}
\,.
\end{equation}
Let $\bn_\sigma$ be the law of a Brownian excursion of duration $\sigma>0$. The following decomposition lemma is straighforward from Theorem~2.2 of \cite{Groeneboom1983a} (see also Theorem~2 and Corollary~2 of \cite{PiRo2011a}):
\begin{lem}\label{lem:W_decomp}For any $i\in \Z$ such that $z_i<\infty$, conditionally on $(t_j,\gamma_j)_{j<i}$, and $(t_i,z_i)$, the collection of functions $g_j$, $j<i$, and $h_i$ form an independent family with law given respectively by $\bn_{t_{j+1}-t_j}$, $j<i$, and $\bn_{z_i-t_i}$.
\end{lem}


Together with the previous considerations about the decomposition, we are thus let to studying convex minorants of Brownian excursions, which is the subject of the next section. 


\subsection{Convex minorants of a Brownian excursion} % (fold)
\label{sub:convex_minorant_of_a_brownian_excursion}

% Figure environment removed

In this section, we consider a Brownian excursion $\exc$ on $[0,1]$. We use the notation of the previous section with $\omega=\exc$, up to the obvious modifications:
The vertices of the convex minorant of $\exc$ on $[0,x]$ are denoted by $\cV_x=\cV_x(\exc)$, and can be enumerated in increasing order as $(t_i)_{i\ge 0}$ with $t_0=0$, where $t_i=t_i(x)=t_i(x,\exc)$. The slopes $\gamma_i=\gamma_i(x,\exc)$ are defined as before. For each $i\ge 1$, let $z_i=z_i(x)=z_i(x,\exc)=\inf\{s>t_i: \exc_s=\exc_{t_i}+\gamma_{i-1} (s-t_i)\}$. We define $z_0=1$ for convenience. See Figure~\ref{fig:convex_minorant_excursion}. 
 
%The following decomposition of the convex minorant at the first vertex/intercepts, is more essential than the one we have seen before. 
A simple induction yields the description of the restriction of the excursion $\exc$ to the interval $[0,x]$ as a collection of Brownian excursions above the graph of $c_x(\cdot, \exc)$.

\begin{lem}\label{lem:convex_minorant_law1}Let $\exc$ be a standard Brownian excursion on $[0,1]$, and let $x=V$ be an independent random variable uniform on $[0,1]$. Then consider the convex minorant $c_V(\cdot, \exc)$ of $\exc$ on $[0,V]$, with vertices $(t_i)_{i\ge 0}$. Define the functions $\exc_0$ and $\exc_1$ by
\[\exc_0(s):=(\exc(s)-s\cdot \exc(t_1))\I{s\le t_1} 
\qquad \text{and} \qquad 
\exc_1(s):=(\exc(t_1+s)-\exc(t_1)-s\cdot \exc(t_1))\I{t_1+s\le z_1}\,.\]
Then $(t_1,z_1-t_1, 1-z_1)$ is a Dirichlet$(\tfrac 12, \tfrac 12, \tfrac 12)$ random vector,
and conditionally on $(t_1,z_1)$, $(\exc_0,\exc_1, V)$ are independent, $\exc_0$ and $\exc_1$ are Brownian excursions of durations $t_1$ and $z_1-t_1$, respectively, and $V$ is uniform on $(t_1,z_1)$.
\end{lem}
\begin{proof}The claimed properties follow from the decomposition of the Brownian excursion $\exc$ using the line linking $(0,0)$ to  $(t_1,\exc(t_1))$ (notice that $0<t_1<1$ a.s.). For $s\ge 0$, consider the straight line $\{(t,st): t\in [0,1]\}$, and increase the value of $s$ from $0$ until the first value $s=\gamma_1$ at which the location of the first intersection $i(s)=\inf\{t\ge 0: \exc(t)=st\}$ is at most $V$: $\gamma_1=\inf\{s\ge 0: i(s)\le V\}$, $t_1=i(\gamma_1)$. Now, by strong Markov property, $(\exc_{1-t}, z_1\leq t \leq 1)$ is a Brownian meander of duration $1-z_1$ conditioned to end at $\exc_{z_1}$; the path $\exc_0$ coincides with the path above the first face, and is a Brownian excursion; the path $\exc_1$ is the path above the same line, between $t_1$ and $z_1$. Let $\varphi_t(x)=e^{-x^2/(2t)}/\sqrt{t2\pi}$. The vector $(\gamma_1,t_1,z_1,V)$ has a distribution which is absolutely continuous with respect to Lebesgue measure on the set $D:=\R_+\times \{(t,z,u): 0<t<v<z<1\}$, with density $f(s,t,z,v)$ given by 
\begin{align*}
f(s,x,z,v)
&= \I{(s,t,z,v)\in D} \cdot 
       \frac{\varphi_t(ts)}t \cdot  % # 1ere excu
       \frac{\varphi_{z-t}( (z-t) s)}{z-t} \cdot  %# 2eme excu
       \frac{\varphi_{1-z}(z s) zs}{1-z} \cdot  %#   meandre final
         %# masse correspondant à la présence de l'uniforme dans [x,z] ENLEVE POUR avoir la loi de U
t   %# vitesse à laquelle monte la droite penchée à l'abscisse x (quand on augmente de dtheta).
\sqrt{2\pi} %# normalisation
\\
&=\I{(s,t,z,v)\in D}\cdot 
\frac{sze^{- \frac{s^2}{2(1-z)/z}}}{1-z} \times \frac{1} {2\pi\sqrt{t(z-t)(1-z)}} \times \frac 1 {z-t}\,.
\end{align*}  
Integrating for $s\in \R_+$, this yields the claimed distribution.
\end{proof}

\begin{rem}We point out that the distribution of $(t_1,z_1-t_1,1-z_1)$ may also be obtained, without any calculation, using the correspondence with the cut tree and the decomposition of a Brownian continuum random tree into tree pieces that is induced by removing the branch point at the intersection of the geodesics between three random points (see~Section~\ref{sec:dynamics} and \cite{Aldous1994a}). 
\end{rem}

A straightforward induction yields the distribution of the vector of lengths of the faces of the convex minorant $c_V(\cdot, \exc)$ of $\exc$ on $[0,V]$ for an independent uniform point $V$ in $[0,1]$:
\begin{lem}\label{lem:convex_minorant_law2}Let $\exc$ be a standard Brownian excursion on $[0,1]$, and let $x=V$ be an independent random variable uniform on $[0,1]$. Let $(\Delta_{i,1}, \Delta_{i,2}, \Delta_{i,3})_{i\ge 0}$ denote a family of independent Dirichlet$(\tfrac 12, \tfrac 12, \tfrac 12)$ random vectors. Then, for $(t_i)_{i\ge 0}=(t_i(V))_{i\ge 0}$ the sequence of vertices, we have
\[(t_{i}-t_{i-1})_{i\ge 1} \eqdist \bigg(\Delta_{i,1} \cdot \prod_{1\le j<i} \Delta_{j,2}\bigg)_{i\ge 1}\,.\]
\end{lem}


\subsection{Recursive convex minorants of a Brownian excursion} % (fold)
\label{sub:recursive_convex_minorants}

The results of the previous section point out the recursive structure of convex minorants of a Brownian excursion. Here we will use it to construct the tree $\CMT(\exc,\bU)$. This is the first building block of our construction of $\CMT(X,\bU)$ the scaling limit of the minimum spanning tree, and it already reveals some of the main ingredients. Before proceeding to the details, let us explain roughly the strategy: 
\begin{itemize}
  \item for all $x,y\in [0,1]$, we define the set $\llb x,y\rrb$ which is meant to be the collection of points used to go from $x$ to $y$ (somewhat pre-arcs or pre-branches);
  \item we also show that it is possible to assign a ``measure'' $d(x,y)$ to $\llb x,y\rrb$ that induces a $0$-hyperbolic metric space.
\end{itemize}
We will see that the metric space induced by $d$ on $[0,1]$ is connected if we restrict our attention to points at finite distance from $0$, so that the subset of $[0,1]$ with this property, endowed with $d$ is thus an $\R$-tree (in the sense of Section~2.2 of \cite{AdBrGoMi2013a}). Later on, we will show that the metric completion of $([0,1],d)$ is compact, so that no point is put aside. 

The definition of $\llb x,y\rrb$ will be done in stages: first $\llb 0,x\rrb$ with $x$ restricted to some suitable dense subset of $[0,1]$; then, we extend the definition of $\llb 0, x\rrb$ to all $x\in [0,1]$; finally, $\llb x,y\rrb$ is defined in Section~\ref{sec:the_branching_structure} using a notion of common ancestor of $x$ and $y$.

\begin{rem}\label{rem:association_map}
Let $\bn_\sigma$ denote the law of a standard Brownian excursion of duration $\sigma>0$. We will define $\CMT(\exc,\bU)$ as a proper random variable for $\bn_1$-almost every function $\omega$, and almost all sequences $\bU=(U_1,U_2,\dots)$ of independent random variables, uniform on $[0,1]$. For this, the components of $\bU$ are associated to the local minima of $\exc$. This can be done by defining a canonical bijection between $\N$ and the set $\sL(\exc)$ of local minima of $\exc$. For instance, consider an enumeration $I=(I_j,j\geq 0)$ of the (countable) set of all intervals with rational extremities on $[0,1]$. Since a.s.~each local minimum of $\exc$ is a global minimum on at least one interval of $I$, associate with each local minimum $t \in \sL$ the index $j(t)$ of the first interval of $I$ on which $t$ is a global minimum; after that associate with $t$, the uniform random variable $U_{j(t)}$. In the sequel, $j$ is called the association map of $\exc$. The proofs of convergence in Section~\ref{sec:coupling} will need a different, more complex association, but we believe it is not necessary until then. 
\end{rem}

Let $\cU=\bigcup_{n\ge 0} \N^n$ (see Section~\ref{sec:notation}). For any $x\in \sL=\sL(\exc)$, we define recursively a collection $(t_u,\xi_u,\gamma_u, e_u)$, $u\in \cU$, that a priori depends on $x$. Lemma~\ref{lem:no_exception_convex} ensures that, with probability one, the following definition makes sense for all $x\in \sL$.

We first let $t_\varnothing = 0$, $\xi_\varnothing=x$, $\gamma_\varnothing=0$, and $e_\varnothing = \exc$. Almost surely, there are only finitely many vertices of the convex minorant of $\exc$ on $[0,x]$, and they are all elements of $\sL$ and denoted by $t_0=0<t_1< t_2 < \dots < t_k = x$ for some $k\in \N$. For each $i=0,\dots, k-1$, let $\xi_i=t_i + U_{t_{i+1}} |t_{i+1}-t_i|$, $\gamma_i=\slo(t_{i+1}, \exc)=(\exc(t_{i+1})-\exc(t_i))/|t_{i+1}-t_i|$ and let $e_i$ be defined by, for $s\ge 0$,
\[e_i(s)=(\exc(t_i+s)-\exc(t_i)-s\cdot \gamma_i)\I{t_i+s\le t_{i+1}}\,.\]

More generally, suppose now that we have defined $(t_u,\xi_u, \gamma_u, e_u)$ for some $u \in \N^n$, $n\ge 1$. Let $\theta_0^u=0<\theta_1^u<\theta_2^u<\dots$ be the vertices of the convex minorant of $e_u$ on the interval $[0,\xi_u-t_u]$, and set $t_{ui}=t_u+\theta_i^u$ for all $i\ge 0$; observe that the $t_{ui}=t_u+\theta_i^{u}$, $i\ge 0$, are precisely the elements of $(t_j(\xi_u))_{j\ge 0}$ lying in $[t_u, \xi_u]$. Then let $\varphi^u_i=(e_u(\theta^u_{i+1})-e_u(\theta^u_i))/|\theta^u_{i+1}-\theta^u_i|$ be the slope of the convex minorant of $e_u$ on $[\theta^u_i,\theta^u_{i+1}]$. For each $i\in \N$, we let $m_{ui}=|\theta^u_{i+1}-\theta^u_i|=|t_{u(i+1)}-t_{ui}|$, $\xi_{ui}=t_{ui} + U_{t_{u(i+1)}} m_{ui}$, $\gamma_{ui}=(e(t_{u(i+1)})-e(t_{ui}))/m_{ui}=\gamma_u+\varphi_i^u$ and define the function $e_{ui}:[0,m_{ui}]\to \R_+$ by 
\[e_{ui}(s)=(\exc(t_{ui}+s)-\exc(t_{ui})-s\cdot \gamma_{ui})\I{t_{ui}+s\le t_{u(i+1)}}\,.\]
We then define
\begin{equation}
\label{def:0x}\llb 0,x\rrb:=\{x\} \cup \bigcap_{n\ge 0} \overline {\bigcup_{|u|=n} [t_u,\xi_u]}\,,
\end{equation}
which is then a non-empty closed subset of $[0,x]$. 
For each $n\ge 1$, we also let 
\begin{equation}\label{def:distance} d_n(0,x):=\sqrt{\frac \pi 2} \cdot \sum_{|u|=n} m_u^{1/2} \qquad \text{and} \qquad d(0,x):=\limsup_{n\to\infty} d_n(0,x)\,.\end{equation}
\begin{lem}\label{lem:dist_martingale}For each $x\in \sL$, the sequence $(d_n(0,x))_{n\ge 1}$ is a non-negative martingale. As a consequence, with probability one, the sequences $d_n(0,x)$ converge for all $x\in \sL$ to finite limits $d(0,x)$. 
\end{lem}
\begin{proof}Fix $x\in \sL$. Let $\cF_n$ denote the sigma-algebra generated by the random variables $\{(t_u, \gamma_u), |u|\le n\}$; in particular, $(m_u)_{|u|\le n}$ is $\cF_n$-measurable. Conditionally on $\cF_n$, the functions $e_u$, $u\in \N^n$, are independent Brownian excursions of respective durations $m_u$. It follows that 
\begin{equation}\label{eq:recursive_convex_martingale}
d_{n+1}(0,x)=\E{\sum_{|u|=n+1} m_u^{1/2}~\bigg|~\cF_n}=\sum_{|u|=n} m_u^{1/2} \cdot \E{\sum_{i\ge 0}(m_{ui}/m_u)^{1/2}~\Bigg|~\cF_n}\,.
\end{equation}
Let $(\Delta_{i,1},\Delta_{i,2},\Delta_{i,3})_{i\ge 0}$ be iid Dirichlet$(\tfrac 12, \tfrac 12, \tfrac 12)$ random vectors. Then, by Lemma~\ref{lem:convex_minorant_law2}, we have conditionally on $m_u$: 
\begin{equation}\label{eq:dn_conditional}
\Big(\frac{m_{ui}}{m_u}\Big)_{i\ge 0} \eqdist \bigg(\prod_{1\le j<i} \Delta_{j,2} \cdot \Delta_{i,1}\bigg)_{i\ge 0}\end{equation}
From there, it is straightforward to verify by induction that, since $\Ec{\Delta_{i,1}^{1/2} + \Delta_{i,2}^{1/2}}=1$, for each $i\ge 1$, the expectation of the square root of the right-hand side of \eqref{eq:dn_conditional} equals $2^{-i}$. As a consequence, the conditional expectations in the right-hand side of \eqref{eq:recursive_convex_martingale} all equal one almost surely, so that $d_n(0,x)$ is indeed a martingale.
Since $\sL$ is countable, the convergence is almost surely for all $x\in \sL$. 
\end{proof}

The function $d(0, \cdot)$ can be extented to $[0,1]$ as follows. First let $\llb 0,0\rrb = \{0\}$ and $d(0,0)=0$. Then, for each point $x\in (0,1]$, any $t\in \cV_x\setminus \{0,x\}$ is a local minimum, and therefore $\llb0,t\rrb$ and $d(0,t)$ has already been defined in \eqref{def:0x} and \eqref{def:distance}, respectively.  We rely on those to define, for $x\in (0,1]$, 
\begin{equation}\label{eq:def_geodesic_to_zero}
  \llb 0,x\rrb = \overline{\bigcup_{t\in \cV_x\cap \sL} \llb 0,t\rrb} \cup \{x\} \qquad \text{and } \qquad d(0,x)= \sup_{t\in \cV_x\cap \sL} d(0,t)\in \R_+ \cup \{\infty\}\,.
\end{equation}

\begin{rem}\label{rem:points_absolute}
 {\bf i)} The slight subtlety in the definition in \eqref{eq:def_geodesic_to_zero}, where the union is taken on $t\in \cV_x\cap \sL$ rather than $\cV_x\setminus \{x\}$ is to ensure that the definion in \eqref{eq:def_geodesic_to_zero} is consistent with the one in \eqref{def:0x} in the case that $x\in \sL$. For instance, if $t\in \sL$ and $x=\zi(t)$ then $x\in \cV_x$ but almost surely not in $\sL$. 

\noindent {\bf ii)} The recursive construction yields a collection of ``join points'' associated to the local minima. First there is a well-defined face to the left of $t$: almost surely for $t\in [0,1]$, $\li(t)= \sup \cV_t < t$, and $\li(t)\in \sL$, so that $[\li(t), t]$ is a face of the convex minorant $c_t$. The slope $\slo(t)$ is precisely the slope of this face. Furthermore, $U_t$ is used to define a uniform random point in $[\li(t), t]$ that we denote by $\ju(t)$. In the previous decomposition, for any $x\in [0,1]$ and any $u\in \cU$, $i\in \N$ such that $t_{u(i+1)}=t$, we have $t_{ui}=\li(t)$ and $\xi_{ui}=\ju(t)$. 
\end{rem}

Before going further, let us prove the following lemma, that will be useful later (Lemma~\ref{lem:geodesic_restriction}). Observe first that \eqref{eq:def_geodesic_to_zero} allows to extend the definition of $t_u(x), \xi_u(x), m_u(x)$, $u\in \cU$, to all $x\in [0,1]$: let $t_u(x),\xi_u(x)$ and $m_u(x)$ coincide with $t_u(t_i),\xi_u(t_i)$, $m_u(t_i)$ for all $u$ of the form $u=jv$, with $j<i$ and $v\in \cU$. When $\cV_x$ is a finite set, it is understood $m_{jv}(x)$ is only defined for the relevant values of $j$.  

\begin{lem}\label{lem:geodesic_to_zero}Almost surely, for every $x\in [0,1]$ we have $\cV_x\subseteq \llb 0, x\rrb$ and furthermore:
\begin{compactenum}[i)] {}
    \item for every $u\in \cU$, $t_u(x),\xi_u(x)\in \llb 0,x\rrb$,
    \item $\sup_{|u|=n} m_u(x) \to 0$, as $n\to\infty$, and thus
    \item $\llb 0,x\rrb$ is the closure of $\{t_u(x): u\in \cU\}\cup \{x\}$, in particular, if $x=\ri(t)$, then $\llb 0,x\rrb = \llb 0,t\rrb \cup \{x\}$.
\end{compactenum}
\end{lem}
\begin{proof}The first claim is clear from \eqref{eq:def_geodesic_to_zero}. We first prove \emph{i)} for $x\in \sL$. 
For $u\in \cU$, $t_u\in \llb 0,x\rrb$ by definition: indeed, for each $v\in \cU$, $t_{v0}=t_v$, and thus $t_u\in \cap_{n\ge |u|} \cup_{|v|=n}[t_v, \xi_v]$. For $\xi_u$, note that, almost surely $\cV_{\xi_u}$ has an accumulation point at $\xi_u$. It follows that $\xi_u$ lies in the closure of $\{t_{uk}: k\ge 0\}$. By the previous argument, all these points lie in $\llb 0, x\rrb$ which is closed, and thus $\xi_u\in \llb0,x\rrb$ as well. Now, since $\cU$ is countable, this is true for every $u$, and, hence, for every $x\in \sL$. Finally, this is true for all $x\in [0,1]$ by definition of $\llb 0, x\rrb$ in \eqref{eq:def_geodesic_to_zero}. 

\emph{ii)} We restrict our attention to the set of probability one where $m_u(y)\to 0$ as $|u|\to\infty$ for all $y\in \sL$. Fix any $x\in [0,1]$ and $\epsilon>0$. There is an $i\in \N$ large enough that $\sup \cV_x\cap \sL \le t_i+\epsilon$. Then, for any $u$ of the form $jv$ with $j\ge i$ and $v\in \cU$ either $m_{jv}(x)\le \epsilon$, or $m_{jv}(x)$ is not defined. On the other hand, for $u$ of the form $jv$ with $j<i$ and $v\in \cU$, we have $m_{jv}(x)=m_{jv}(t_i)$. It follows that $\sup\{m_u(x): |u|=n\}\le \epsilon$, which completes the proof since $\epsilon>0$ was arbitrary. 

\emph{iii)} follows readily from \emph{i)}, \emph{ii)} and the definition.
\end{proof}

\subsection{The branching structure and the convex minorant tree} % (fold)
\label{sec:the_branching_structure}

We now move on to the branching structure. Let $x,x'\in [0,1]$. With the ultimate objective of defining $d(x,x')$ we first define $x\wedge x':=\sup (\llb 0,x\rrb \cap \llb 0,x'\rrb)$. It should be understood as the closest common ancestor of $x$ and $x'$, when $0$ is seen as the root. It follows readily that the definition that the sets $\llb 0,x\rrb$ enjoy the following restriction property:
\begin{lem}\label{lem:geodesic_restriction}
Almost surely, for any $x\in [0,1]$ and $y\in \llb 0,x\rrb$, we have $y=x\wedge y$, $\llb0,y\rrb = \llb 0,x\rrb \cap [0,y]$, and $d(0,y)\le d(0,x)$
\end{lem}
\begin{proof}We restrict our attention to the set of probability one on which the events of Lemma~\ref{lem:geodesic_to_zero} all occur. 
If $y=x$, the claim is clear, so suppose that $y<x$, which implies that $y\le \sup(\cV_x\cap \sL)$. If $y=\sup(\cV_x \cap \sL)$ then $\llb 0,y\rrb$ is the closure of $\bigcup_{i\ge 1}\llb 0,t_i(x)\rrb$, so that the claim holds by \eqref{eq:def_geodesic_to_zero}. Finally consider the last case $y<\sup(\cV_x\cap \sL)$, and let $t_u=t_u(x)$, and $m_u=m_u(x)$, $u\in \cU$, defined in the previous section. For any $n\ge 1$ there exists some $u\in \cU$ with $|u|=n$ such that $y\in [t_u,t_u+m_u]$. It follows easily that $\llb 0,x\rrb \cap [0,t_u] \subseteq \llb 0,y\rrb$. Since $m_u\to 0$ by Lemma~\ref{lem:geodesic_to_zero}, we have $\llb 0,y\rrb=\llb 0,x\rrb \cap [0,y]$. The claim about the distance follows readily. 
\end{proof}

The extension of $\llb \cdot, \cdot\rrb$ and $d(\cdot,\cdot)$ to $[0,1]^2$ will require the following lemma, that will allow us to bring the (nice) points of $\sL$ back in the game:
\begin{lem}\label{lem:path_to_ancestors}
With probability one, for every $x,y\in [0,1]$ with $x>y$, there exists some $t\in \cV_x\cap \sL$ such that $x\wedge y = t \wedge y \in \llb 0, t\rrb$. 
\end{lem}
\begin{proof}We work on set $\Omega^\star$ of probability one where all the events of Lemma~\ref{lem:no_exception_convex} all occur.
Since $x>y$, let $(t_i)_{i\ge 0}$ be the vertices of $\cV_x\setminus \{x\}$, which might be a finite sequence. Then $[t_i,t_{i+1})$, $i\ge 0$, together with $[\sup_i t_i, x)$ forms a partition of $[0,x)$. On the event $\Omega^\star$, it suffices to consider the following two cases. (a) If $\sup_i t_i=x$, then there exists some $i\in \N$ for which $y\in [t_i, t_{i+1})$. By definition, $x\wedge y = t_{i+1} \wedge y \in \llb 0, t_{i+1}\rrb$, and $t_{i+1}\in \sL$. (b) Otherwise there are only finitely many vertices $t_0, t_1,\dots, t_k$, all of which are in $\sL$; then since $y<x$, we have $x\wedge y = t_k \wedge y \in \llb 0,t_k\rrb$. 
\end{proof}

The following lemma makes formal the branching structure of the sets $\llb 0,x\rrb$, $x\in [0,1]$.

\begin{lem}\label{lem:intersection_arcs}
There exists a set of probability one on which for any $x,x'\in (0,1)$, $x\wedge x'>0$ and 
\[\llb 0,x\rrb \cap [0, x\wedge x' ] = \llb 0, x'\rrb \cap [0,x\wedge x'] 
\qquad \text{and} \qquad 
\llb 0,x\rrb \cap \llb 0,x'\rrb \cap (x\wedge x',1] = \varnothing\,.\]
\end{lem} 
\begin{proof}
We work on a set $\Omega^\star$ of probability one where the events of Lemmas~\ref{lem:no_exception_convex} and~\ref{lem:geodesic_to_zero} all occur.
Let $(t_u)_{u\in \cU}$ and $(t'_u)_{u\in \cU}$ denote the recursive collections of points introduced before, for the points $x$ and $x'$, respectively. For $n\ge 1$, define 
\[x\wedge_n x':=\sup\{\{t_u: |u|\le n\}\cap \{t_u': |u|\le n\}\}\ge 0\,.\] 
Then Lemma~\ref{lem:geodesic_to_zero} \emph{iii)} implies that for every $n\ge 1$ and all $y<x\wedge_n x'$, we have $y\in \llb 0,x\rrb$ if and only if $y\in \llb 0,x'\rrb$. The sequence $(x\wedge_n x')_{n\ge 1}$ is non-decreasing and taking the limit as $n\to\infty$, it follows that $\llb 0, x\rrb$ and $\llb 0,x'\rrb$ coincide on $[0,\sup_n x\wedge_n x')$.

On the other hand, by definition of $x\wedge x'=\sup \llb 0,x\rrb \cap \llb 0,x'\rrb$, the sets $\llb 0,x\rrb$ and $\llb 0,x'\rrb$ are disjoint on $(x\wedge x', \infty)$. So to complete the proof, it suffices to prove that $\sup_n x\wedge_n x'=x\wedge x'$. For every $n\ge 1$, we have $x\wedge_n x'\in \llb 0,x\rrb \cap \llb 0,x'\rrb$ so that $x\wedge_n x' \le x\wedge x'$. To prove the converse inequality, consider an arbitrary point $y\in \llb 0,x\rrb \cap \llb 0,x'\rrb$, and observe that for any $n\ge 1$,  
there exists $u,v\in \cU$ with $|u|=|v|=n$ such that $y\in [t_u,\xi_u]$ and $y\in [t'_v, \xi'_v]$, and necessarily $t_u,t'_v\le x\wedge_n x'$. It follows that 
\[y\le x\wedge_n x' + \sup\{m_u(x): |u|=n \}\,.\] 
It follows that from Lemma~\ref{lem:geodesic_to_zero} that $y\le \sup_n x\wedge_n x'$. Since $y\in \llb 0,x\rrb \cap \llb 0,x'\rrb$ was arbitrary, we may take it as close to $x\wedge x'$ as we want, which proves that $\sup_n x \wedge_n x' = x\wedge x'$.  

Finally, we show that $x\wedge x'>0$. Without loss of generality, we assume that $x'<x$. If $x'\ge t_1$, then $x\wedge x'\ge t_1>t_0=0$. More generally, for any $n\ge 1$, if $x'\ge t_{0^{(n)}1}$, then $x'\wedge x \ge t_{0^{(n)}1}$, where $0^{(n)}1$ is the sequence formed by $n$ consecutive $0$ followed by a $1$. But for every $n\ge 1$, $t_{0^{(n)}1}$ is distributed like $m_1\times \prod_{2\le i\le n} \Delta_i$, where $(\Delta_i)_{i\ge 2}$ is a family of i.i.d.\ random variables with distribution Beta$(\tfrac 12,1)$; as a consequence, $t_{0^{(n)}1}>0$ a.s.\ for every $n\ge 1$ and $t_{0^{(n)}1}\le \sup\{m_u(x): |u|=n+1\} \to0$ by Lemma~\ref{lem:geodesic_to_zero}. Since $x'>0$, there is some $n\ge 1$ for which $0<t_{0^{(n)}1}\le x'$ which proves that $x\wedge x'>0$. The latter decomposition depends on $x$, but either $x'\ge t_1(x)>0$, or $x\wedge x'=t_1(x) \wedge x'$ so that it suffices to consider the decomposition at the set of local minima, which is countable; it follows that, almost surely, for every $x,x'\in (0,1)$, $x\wedge x'>0$.
\end{proof}

We are now ready to define $\llb x,y\rrb$ and $d(x,y)$ for all $x,y\in [0,1]$. Observe first that, by Lemma~\ref{lem:path_to_ancestors}, almost surely, for all $x\ne y$, we have $x\wedge y \in \llb 0,t\rrb$ for some $t\in \sL$, so that $d(0,x\wedge y)<\infty$. Now, if $x=y$, we set $d(x,y)=0$, and otherwise
\begin{equation}\label{eq:def_distance_xy}
d(x,y):=d(0,x) + d(0,y) - 2 d(0,x\wedge y) 
\qquad \text{and} \qquad 
\llb x,y\rrb := (\llb 0,x \rrb \cup \llb 0, y \rrb) \cap [x\wedge y, 1]\,.
\end{equation}
By the previous remark, both $d(\cdot, \cdot)$ and $\llb \cdot, \cdot\rrb$ are well-defined and symmetric on $[0,1]^2$. When necessary, we write $\llb x,y\llb=\llb x, y\rrb \setminus \{y\}$; $\rrb x,y\rrb$ and $\rrb x,y\llb$ are defined similarly. 

Finally, we verify now that $d$ induces a metric space that has the topology of a tree. In the following, we let $(x\cdot y)_0:=\tfrac 1 2 (d(0,x)+d(0,y)-d(x,y))$. Observe that, by definition, we have $(x\cdot y)_0 = d(0,x\wedge y)$. 

\begin{lem}[Triangle inequality and four-point condition]\label{lem:pseudo_metric}
A.s., for every $x,y,z\in [0,1]$, we have 
\begin{compactenum}[i)]
  \item $0\le d(x,y)\le d(x,z)+d(z,y)$, and 
  \item $(x\cdot y)_0 \ge \min \{(x\cdot z)_0, (z\cdot y)_0\}$.
\end{compactenum}
\end{lem}
\begin{proof}We prove \emph{i)} and \emph{ii)} simultaneously. Note that $d(x,y)\ge 0$ by Lemma~\ref{lem:geodesic_restriction}.
By definition, $x\wedge z\in \llb 0, x\rrb$. Suppose first that $x\wedge z\in \llb 0,x\wedge y\llb$. Then, $z\wedge y\in \llb 0, x\wedge y\llb$ as well by Lemma~\ref{lem:intersection_arcs}. It follows readily that $d(0,x\wedge y) \ge d(0,x \wedge z), d(0,y\wedge z)$. Furthermore, by definition,
\[d(x,y)=d(x,x\wedge y)+ d(x\wedge y, y)\le d(x,x\wedge z)+d(x\wedge y,y)\le d(x,z)+d(z,y)\,.\]
If on the other hand, we have $x\wedge z \in \llb x\wedge y, x\rrb$, then Lemma~\ref{lem:intersection_arcs} implies that $z\wedge y = x \wedge y$. As a consequence, we have $d(0,x\wedge y)=d(0,z\wedge y) = \min \{d(0,x\wedge z), d(0,z\wedge y)\}$. Moreover
\begin{equation}\label{eq:triangle_ineq_case2}
d(x,y)=d(x,x\wedge y)+ d(x\wedge y,y) = d(x,x\wedge z)+d(x\wedge z, x\wedge y)+ d(x\wedge y, y)\, , 
\end{equation}
and 
\begin{align*}
  d(x,z)+d(z,y)
  &= d(x,x\wedge z)+d(x\wedge z,z) + d(z,y\wedge z) + d(y\wedge z, y) \\
  &= d(x,x\wedge z)+d(x\wedge z,z) + d(z,x\wedge y) + d(x\wedge y,y)\,,
\end{align*}
which is easily seen to be at least as large as the right-hand side of \eqref{eq:triangle_ineq_case2}.
\end{proof}

By Lemma~\ref{lem:pseudo_metric}, $d$ satisfies the triangle inequality and thus induces a metric on the quotient space: Let $x\sim y$ if $d(x,y)=0$. Let $\sT^\circ:=\{x\in [0,1]: d(0,x)<\infty\}$, and write $\sT$ for the metric completion of the quotient $\sT^\circ/_\sim$; we still write $d$ for the induced metric on $\sT$. Writing $\pi$ for the canonical projection, we let $\rho=\pi(0)$ be the root of $\sT$ and $\mu$ be the push-foward of the Lebesgue measure on $[0,1]$ by $\pi$. We define $\CMT(\exc,\bU)$ as $\frak T:=(\sT,d,\mu, \rho)$.

We will later on identify exactly the distribution of $\CMT(\exc,\bU)$ the Brownian CRT (Theorem~\ref{thm:limit_mst_surplus}); however since the proof requires to introduce a number of additional concepts, it is interesting to first verify that:

\begin{prop}\label{pro:cmt_real-tree}
With probability one, the metric space $(\sT,d)$ is a real tree.
\end{prop}

\begin{rem}
{\bf i)} We define $\sT^\circ$ to ensure that $(\sT,d)$ is connected. We will see later that a.s.~$\sT^\circ=[0,1]$.

\noindent {\bf ii)} It is plausible that $(\sT^\circ,d)$ is already complete; we do not have a short argument for either direction, and we did not try to investigate further since there is no real influence on what follows.
\end{rem}

\begin{proof}[Proof of Proposition~\ref{pro:cmt_real-tree}] By the four-point condition in Lemma~\ref{lem:pseudo_metric} \emph{ii)} and Lemma 3.10 of \cite{Evans2005}, $\sT$ is $0$-hyperbolic \cite[see also][]{Chiswell2001}. Then, by Theorem 3.40 of \cite{Evans2005}, it suffices to prove that $\sT$ is connected to complete the proof.

We show that $\sT$ is path-connected; this relies on the fact, proved in Section~\ref{sub:geodesics_length}, that there exists a measure $\ell$ on $\sT$ such that for all $x,y\in [0,1]$ we have $d(x,y)=\ell(\llb x,y\rrb)$. Let $\pi$ denote the canonical projection from $[0,1]$ onto $\sT$. For any $r\in [0,d(0,x)]$, let $x_r:=\sup\{s\in \llb 0,x\rrb: d(0,s)\le r\}$. Then we claim that the map $\phi$ given by $\phi(r)=\pi(x_r)$ is an isometry from $[0,d(0,x)]$ to $\sT$. To see this, note first that since $d(0,s)$ is non-decreasing for $s\in \llb 0,x\rrb$, and the set  
$\{s\in \llb 0,x\rrb: d(0,s)\le r\}$ is closed, we have $d(0,\phi(r))=d(0,x_r)\le r$. On the other hand, for $s\in \llb 0,x\rrb$, we have $d(0,s)=\ell(\llb 0,s\rrb) = \ell(\llb 0,x\rrb \cap [0,s])$; since $\ell(\llb 0,x\rrb)<\infty$ the right-hand side is continuous if we consider $s\in [0,1]$. 
It follows that $d(0,\phi(r))=r$, and that $\phi$ is an isometry. Therefore, for every $x$, there is a geodesic from $0$ to $x$, and $\sT$ is path-connected and then connected.
\end{proof}

The following consistency property will be useful. It implies in particular that the pairwise distances may be defined using only certain suitable sub-excursions of $\exc$. 

\begin{lem}[Restriction and consistency]\label{lem:def_consistency}
For $x\in [0,1]$, let $(t_i)_{i\ge 0}$ denote the vertices of $\cV_x\cap \sL$, and $z_i=z_i(x)$, $i\ge 0$, the corresponding intercepts. Then
\begin{compactenum}[i)]
  \item for any $i\ge 0$, we have 
  \[\llb 0,x\rrb = \bigcup_{0\le j<i} \llb t_j,t_{j+1}\rrb \cup \llb t_i,x\rrb 
  \qquad \text{and} \qquad 
  d(0,x)=\sum_{0\le j<i} d(t_j,t_{j+1})+ d(t_i,x)\,.\]
  \item for every $i\ge 0$ there exists a vector $\bU_i$ constructed from $\bU$ such that, almost surely, the restriction of $\CMT(\exc, \bU)$ to $\pi([t_i,z_i])$ is isometric to $\CMT(h_i, \bU_i)$, where $h_i$ is the excursion defined in \eqref{eq:decomp_in_excursions}. 

  % we have $y\in [t_i,z_i]$ then $\llb x,y\rrb$ is measurable with respect to the values of $\exc$ on the open interval $(t_i,z_i)$, and of the corresponding points of $\bU$. Furthermore, 
  %   \[d(x,y)=d(x,t_i)+d(y,t_i)-2d(t_i,x\wedge y)\,.\]
\end{compactenum}
\end{lem}
\begin{proof}\emph{i)} By definition, $t_i\in \llb 0,x\rrb$ so that decomposing $\llb 0,x\rrb$ on $[0,t_i] \cup [t_i,x]$, it follows immediately that $\llb 0,x\rrb= \llb 0, t_i\rrb \cup \llb t_i, x\rrb$. A straightforward induction yields the claim. 

\emph{ii)} By Lemma~\ref{lem:intercept}, $t_i\in \cV_y$ so that $t_i\in \llb 0,y\rrb\cap \llb 0, x\rrb$. It follows that $t_i\le x\wedge y$. For the interval $[t_i,z_i]\subseteq [0,1]$, we now define a sequence $\bU_i$ from $\bU$ as follows. Recall Remark~\ref{rem:association_map} about the association map, and let $(I_j)_{j\ge 1}$ the enumeration of the intervals with rational end points there. Recall also the definition of $h_i$ in \eqref{eq:decomp_in_excursions}. We denote by $j(\exc, \cdot)$ and $j(h_i, \cdot)$ the association maps of $\exc$ and $h_i$ respectively. For every $t\in \sL(\exc)$, then $t-t_i\in \sL(h_i)$ (the set of local minima is a.s.\ preserved by the removal of a linear drift). Let 
\[
U_{i,k}:=
\left\{
\begin{array}{ll}
U_{j(\exc,t+t_i)} & \text{ if } k=j(h_i,t) \text{ for some } t\in \sL(h_i)\\
0            & \text{ otherwise}\,.     
\end{array}  
\right.
\]
Let $\bU_i=(U_{i,k})_{k\ge 1}$. Then, the restriction of $\CMT(\exc,\bU)$ to $\pi([t_i,z_i])$ is isometric to $\CMT(h_i, \bU_i)$. Note that the components of $\bU_i$ that we have set to $0$ above are never used in the construction; if one wants to enforce that $\bU_i$ has the same distribution as $\bU$, one can instead use independent uniform random variables to complete the definition of $\bU_i$.  
\end{proof}

\subsection{Geodesics and the length measure} % (fold)
\label{sub:geodesics_length}

In this section, we show that the distance $d(x,y)$ is actually a measurable function of the set $\llb x,y\rrb$. Let $\psi$ be the function defined by $\psi(r)=\sqrt{r|\log|\log r||}$ for $r>0$, and let $m^\psi$ denote the Hausdorff measure constructed on $\R$ using $\psi$ as a gauge function. Recall that the $\psi$-Hausdorff measure $m^\psi$ of a Borel set $E\subseteq \R$ is defined by \cite{Falconer1986,Falconer1990a,Mattila1999a}
\[m^\psi(E):=\lim_{\delta\to 0+} \inf\bigg\{\sum_{i\ge 1} \psi(A_i): E\subseteq \bigcup_{i\ge 1} A_i, |A_i|<\delta\bigg\}\,,\]
where the $A_i$ are intervals and $|A_i|$ are their lengths. 

For any $x,y\in [0,1]$, the distances between pairs of points of $\llb x,y\rrb$ naturally define a measure as follows: for any $z,t\in \llb x,y\rrb$, we have $\llb z,t\rrb\subseteq \llb x,y\rrb$ and we let $\ell_{x,y}(\llb z,t\rrb)=d(z,t)$. More generally, for any compact interval $A\subseteq [0,1]$, we let $\ell^\circ_{\llb x,y\rrb}(A)=\ell^\circ_{\llb x,y\rrb}(\llb x,y\rrb \cap A)=d(\inf \llb x,y\rrb \cap A, \sup \llb x,y\rrb \cap A)$. This defines $\ell^\circ_{\llb x,y\rrb}$ uniquely as a Borel measure on $[0,1]$. 

\begin{lem}\label{lem:measure_geodesic}Let $V$ be a random variable with uniform distribution independent of $(\exc,\bU)$. There exists a constant $a>0$ such that,
 with probability one, for any Borel set $A\subseteq [0,1]$, we have $\ell_{\llb 0,V\rrb}(A)=a \cdot m^\psi(A \cap \llb 0,V\rrb)$. In particular, $d(0,V)=a \cdot m^\psi(\llb 0,V\rrb)$.
\end{lem}

\begin{rem}It would be possible to identify the constant $a$ using Theorem~1 of \citet{Perkins1981a} who strengthened the results of \citet{TaWe1966a} by (among others) identifying the multiplicative constant between the $\psi$-Hausdorff measure and the local time for the zero set of Brownian motion. However, we did not pursue this further.
\end{rem}

\begin{proof}For $V$ uniform on $[0,1]$, the Cantor set $\llb 0,V\rrb$ has a recursive structure that is tractable with the tools developed by \citet*{GrMaWi1988a} and \citet{MaWi1986a}, which will allow us to compare $m^\psi(\llb 0,V\rrb)$ and $d(0,V)$.

Let $t_1=t_1(V)$ be the location of the first vertex of the convex minorant of $\exc$ on the interval $[0,V]$, and let $z_1=z_1(V)$. Then, by Lemma~\ref{lem:convex_minorant_law1}, 
\[(t_1,z_1-t_1, 1-z_1) \sim \text{Dirichlet}(\tfrac 12, \tfrac 12, \tfrac 12)\,,\]
so that, conditionally on $(t_1,z_1)$, $V$ is uniform in $(t_1,z_1)$. On the other hand, the random jump $\xi_1=\ju(t_1)$ is uniform in $(0,t_1)$ and independent of the rest. This implies that the random Cantor set $\llb 0,V \rrb$ has the same distribution as $C$ constructed as follows. 
Let $\cU_2:=\bigcup_{n\ge 0}\{1,2\}^n$, and let $(\Delta_1(u),\Delta_2(u), \Delta_3(u))$, $u\in \cU_2$, be i.i.d.\ copies of a Dirichlet$(\tfrac 12, \tfrac 12, \tfrac 12)$ random vector $(\Delta_1,\Delta_2,\Delta_3)$.  Set $C_\varnothing=[0,1]$ and, for each $u\in \cU_2$, let 
\[C_{u1}:=[\inf C_u, \inf C_u + |C_u|\cdot \Delta_1(u)],
\qquad \text{and} \qquad 
C_{u2}:= [\sup C_{u1}, \sup C_{u1}+ |C_u| \cdot \Delta_2(u)] \,.
\]

Observe that, for each $u$, $C_{u1}$ and $C_{u2}$ are two intervals in $C_u$, with disjoint interior. Then, for $n\ge 0$, we set $C^n=\bigcup_{|u|=n} C_u$ and $C=\bigcap_{n\ge 0} C^n$. Note in particular that no additional randomness is needed, that would correspond to the point $V$: with this definition $\sup C$ is uniformly distributed on $[0,1]$. 

The law of $(\Delta_1,\Delta_2)$ is explicit and its density $\rho(x_1,x_2)$ is given by
\[\rho(x_1,x_2)=x_1^{-1/2}x_2^{-1/2}(1-x_1-x_2)^{-1/2} \frac{\Gamma(3/2)}{\Gamma(1/2)^3} \cdot \I{x_1+x_2\le 1}\,.\] 
Theorem~5.1 of \cite{GrMaWi1988a} applies: one easily verifies that for $\alpha=1/2$ we have $\Ec{\Delta_1^\alpha+\Delta_2^\alpha}=1$, $\pc{\Delta_1^\alpha+\Delta_2^\alpha = 1} =0$, and $\Ec{1/\min\{\Delta_1^\nu,\Delta_2^\nu\}}\le 2 \Ec{\Delta_1^{-\nu}}<\infty$ for all $\nu\in (0,\tfrac 12)$. Furthermore, Condition (5.1) of \cite{GrMaWi1988a} is satisfied for the point $(x_1,x_2)=(\tfrac 12, \tfrac 12)$, since the density $\rho$ is bounded away from zero uniformly. It follows that with probability one, $m^\psi(C)\in (0,\infty)$. Now, Theorem~5.5 there does not directly apply since the $C_{u1}$ and $C_{u2}$ intersect for every $u$, but this is only at one point, and one easily verifies that the proof there still holds since $m^\psi$ assigns measure zero to any countable collection of points. We conclude that there exists a constant $a>0$ such that $d(0,V) = a \cdot m^\psi(C)$. 

Furthermore, the measure $a \cdot m^\psi(\,\cdot \cap C)$ coincides with the construction measure $\nu$ of \citet{MaWi1986a}, which is easily seen to correspond here to the measure $\ell^\circ_{\llb 0,V\rrb}$. 
% Almost surely, for every continuous function $f$ on $[0,1]$, the sequence
% \[F_n(f):=\lim_{n\to\infty} \sum_{|u|=n} f(\inf C_u) \cdot |C_u|^{1/2}\,,\]
For an interval $A\subseteq [0,1]$, the sequence 
\[\nu_n(A):=\sum_{|u|=n, C_u\cap A\neq \varnothing } |C_u|^{1/2}\]
almost surely converges to a limit value $\nu(A)$. This defines the Borel measure $\nu$ on $[0,1]$ of total mass $d(0,V)$.
%  and satisfies 
% \[\nu(A)=\lim_{n\to \infty} \sum_{\substack{|u|=n\\ C_u\cap A\ne \varnothing}} C_u^{1/2}\,.\]
The fact that $\nu(A)=\ell^\circ_{\llb 0,V\rrb}(A)$ should by now be straightforward. 
\end{proof}

The measures $\ell^\circ_{\llb x,y\rrb}$, $x,y\in [0,1]$, are actually the restrictions of a general measure on $[0,1]$ which projects to the length measure on the convex minorant tree. There is a pre-skeleton on $[0,1]$ which is defined by $\Skel([0,1])=\cup_{x\in \sL}\rrb 0,x\llb = \cup_{x,y\in \sL} \rrb x,y\llb$. 
Let $\ell^\circ$ be the Borel sigma-finite measure on $[0,1]$ uniquely defined by
\begin{enumerate}[i)]
  \item $\ell^\circ(\Skel([0,1])^c)=0$, and 
  \item for every $x,y\in \sL$, and every interval $A$ of $[0,1]$, $\ell^\circ(A \cap \llb x,y\rrb)=\ell^\circ_{\llb x,y\rrb}(A)$.
\end{enumerate}
Then the push-foward measure $\ell=\pi_* \ell^\circ$ is the length measure on the convex minorant tree $\CMT(\exc,\bU)$. Finally, we verify that this corresponds to the push-forward of $m^\psi$ (up to a multiplicative constant). This is essentially just the fact that the skeleton $\Skel([0,1])$ is a countable union of segments, that we can rewrite in terms of a sequence of i.i.d.\ uniform points on $[0,1]$

\begin{prop}\label{pro:length_measure}
Let $(V_i)_{i\ge 1}$ be i.i.d.\ uniform on $[0,1]$, also independent of $(\exc,\bU)$. Then
\begin{compactenum}[i)]
  \item $\skel([0,1]) \subseteq \cup_{i\ge 1} \llb 0, V_i\rrb$, and 
  \item with the constant $a>0$ of Lemma~\ref{lem:measure_geodesic},  the measures $\ell^\circ$ and $a\cdot m^\psi$ almost surely coincide.
\end{compactenum}
\end{prop}
\begin{proof}
\emph{i)} Fix any $x\in \sL$. Almost surely, there is an $i\in \N$, such that $x=t_i(x)$ and $z_i(x)>t_i(x)$. For any $y\in [t_i,z_i)$ we have $x=t_i(x) = t_i(y)$. In particular, a.s.\ there exist infinitely many $n\ge 1$ such that $x=t_i(V_n)$ for some $i\ge 1$. It follows that $\Skel([0,1])\subset \cup_{n\ge 1} \llb 0, V_n\rrb$. 

\emph{ii)} For each $n\ge 1$, let $B_n := \llb 0, V_n\rrb \setminus \bigcup_{1\le j<n} \llb 0,V_j\rrb$. Then, $\Skel([0,1])$ is contained in the union of the $B_n$, $n\ge 1$, which are disjoint sets, and for any interval $A\subseteq [0,1]$, we have
\begin{align*}
\ell^\circ(A) = \sum_{n\ge 1} \ell^\circ(A \cap B_n) = \sum_{n\ge 1} a\cdot m^\psi(A \cap B_n) = m^\psi(A)\,,
\end{align*}
so that the measures $\ell^\circ$ and $a\cdot m^\psi$ indeed concide.
\end{proof}

\begin{rem}\label{rem:one-point_function}
We note the decomposition for the distance $d(0,V)$ identifies its distribution: indeed, by Brownian scaling if $D$denotes the random variable $d(0,V)$, and $D_{1}$ and $D_2$ are two independent copies of $D$, then we have 
\[D \eqdist \sqrt{\Delta_1} D_1 + \sqrt{\Delta_2} D_2\,\]
which implies that, up to a deterministic multiplicative constant, $D$ has the Rayleigh distribution (see for instance Proposition~2.1 of \cite{AlGo2015a}). This can be seen as a first step towards the identification of the law of $\CMT(\exc,\bU)$; see Section~\ref{sec:a_new_point_of_view_on_the_continuum_random_tree} for a full proof of this fact.
\end{rem}

% subsection geodesics_and_the_length_measure (end)subs

\subsection{Recursive convex minorants of Brownian motion with parabolic drift} % (fold)
\label{sub:recursive_convex_minorants_BPT}

We now move on to the definition of the main object of the paper, the tree $\CMT(X,\bU)$. A straightforward application of the Girsanov Theorem shows that, for any $x\in \R_+$, the law of $(X_{s\in [0,x]})$ is absolutely continuous with respect to that of $(W_s)_{s\in [0,x]}$.
As a consequence, ``local properties'' that hold almost surely for $W$ also hold almost surely for $X$ as well.
Since we are only interested in a definition in this section, we may focus on the case of a Brownian motion. In the following, we use the same notation as for the Brownian excursion, we believe that it should not cause any confusion. 


\medskip
\noindent
\noindent\textbf{The convex minorant tree associated with a Brownian motion.} We consider $(W_s)_{s\ge 0}$ a standard Brownian motion. 
Fix $x\in \R_+$ and consider the recursive convex minorants of $W$ on $[0,x]$. Recall that $\cV_x(W)$ a.s.\ has an accumulation point at $0$; let $(t_i)_{i\in \Z}=(t_i(x,W))_{i\in \Z}$ as defined in Section~\ref{sec:convex_minorant_of_brownian_paths}. 
The sequence $(t_i)_{i\in \Z}$ is bi-infinite, but one can write for a fixed $i$
\begin{equation}\label{eq:def_distance_brownian-motion}
\llb 0,x\rrb := \{0\} \cup \bigcup_{j:j\le i} \llb t_{j-1},t_{j}\rrb \cup \llb t_i,x\rrb\,.  
\end{equation}
By Lemma~\ref{lem:W_decomp}, the sets $\llb t_{j-1},t_j\rrb$, for $j$ such that $j\le i$ and $\llb t_i,x\rrb$ are well-defined by the construction of Section~\ref{sec:recursive_convex_minorants} (for the Brownian excursion). Furthermore, Lemma~\ref{lem:def_consistency} ensures that the value of $\llb 0,x\rrb$ is independent of $i\in \Z$, so that $\llb 0,x\rrb$ is well-defined as well. 
To define the distance $d(0,x)=d_\subW(0,x)$, we shall verify that the sum of distances given by the decomposition in \eqref{eq:def_distance_brownian-motion} converges (a priori, 0 could be at infinite distance from every point $x>0$). Observe that, still from Lemma~\ref{lem:W_decomp}, conditionally on $(t_i)_{i\in \Z}$, for any $i\in \Z$, $d(t_i,t_{i+1})$ is distributed like $|t_{i+1}-t_i|^{1/2}$ times the distance between $0$ and $1$ in $\CMT(\exc,\bU)$. Also by Theorem~1, Corollary~1 of \cite{PiRo2011a} and Brownian scaling, $(|t_{i+1}-t_i|)_{i \in \Z}$ has the same distribution as $(x_i/\sum_{j\in \mathbb{Z}} x_j)_{i\in \Z}$,  where $(x_j)_{j\in \Z}$ denote the points of a Poisson point process of intensity $e^{-x}/x dx$  on $\R_+$.
Straightforward calculation shows that $\pc{\sum_i x_i >0} = 1$ and $\Ec{\sum_i x_i^p}=\int_{0}^{+\infty} x^{p-1}e^{-x} dx<\infty$ for $p\in(0,1]$ so that a.s. $\sum_i \sqrt{x_i}<+\infty$ and  $\sum_i {x_i}<+\infty$ which implies that $\sum_{i} |t_{i+1}-t_i|^{1/2} < \infty$ almost surely. 

So, for the distance we may define $d(0,x)=d_\subW(0,x)$ by 
\[d_\subW(0,x):= \sum_{j\le i} d(t_{j-i}, t_j) + d(t_i,x) = \sum_{j\in \Z} d(t_{j-1},t_j)\,,\]
which is almost surely finite for almost all $x\in \R_+$. In particular, with probability one $d_\subW(0,x)<\infty$ for every $x\in \sL(W)$ by Lemma~\ref{lem:no_exception_convex}. %\JF{en contradiction avec ce qui suit, en magenta}

Finally, for any $x,y>0$, we can define $d(x,y)=d_\subW(x,y)$ as follows. Writing $x\wedge y=\sup \llb 0,x\rrb \cap \llb 0,y\rrb$ as before, we have $x\wedge y \in \llb0,t\rrb$ for some $t\in \sL$, by the obvious extension of Lemma~\ref{lem:path_to_ancestors} to the case of Brownian motion. Therefore, $d(0,x\wedge y)<\infty$. We may thus define
\[d_\subW(x,y):=d_\subW(0,x)+d_\subW(0,y)-2 d_\subW(0,x\wedge y)\,,\]
and we will prove that it is a.s.\ finite for all $x,y$. 
Note also that, assuming without loss of generality that $y<x$, there exists some $i\in \Z$ such that $y\in [t_i,t_{i+1})$. In particular, $x,y\in [t_i,z_i]$ and we may equivalently define $d(x,y)$ by
\[d(t_i,x)+d(t_i,y)- 2d(t_i,x\wedge y)\,,\]
and any $i$ of which $t_i<y$ would yield the exact same value. 

\medskip
\noindent\textbf{The convex minorant tree associated with $X$: The Brownian parabolic tree. } 
At last, we consider $X$, the Brownian motion with parabolic drift. 
By absolute continuity, the sets $\llb x,y\rrb=\llb x,y\rrb_\subX$ and $d(x,y)=d_\subX(x,y)$ are also well-defined for every $x,y\in \R_+$. The triangle inequality and four-point condition are satisfied by construction (Lemma~\ref{lem:pseudo_metric}). Let $x\sim y$ if $d_\subX(x,y)=0$, and let $(\sM,d)$ denote the metric completion of the quotient metric space; define $\rho=\pi(0)$. For the mass measure, one needs some rescaling and we shall admit for now that the collection of measures $(x^{-1} \pi_* \Leb|_{[0,x]})_{x>0}$ conve{}rges weakly with probability one to a probability measure $\mu$. The proof of this fact is the topic of Section~\ref{sub:mass_measure}. Finally, we let $\CMT(X,\bU)$ denote the pointed measured complete metric space $\frak M:=(\sM,d,\mu,\rho)$, and we call it the Brownian parabolic tree.



% subsection recursive_convex_minorants_BPT (end)


