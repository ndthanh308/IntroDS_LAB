
@article{zhang_multi-agent_2021,
	title = {Multi-{Agent} {Reinforcement} {Learning}: {A} {Selective} {Overview} of {Theories} and {Algorithms}},
	shorttitle = {Multi-{Agent} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1911.10635},
	abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
	urldate = {2021-06-09},
	journal = {arXiv:1911.10635 [cs, stat]},
	author = {Zhang, Kaiqing and Yang, Zhuoran and Başar, Tamer},
	month = apr,
	year = {2021},
	note = {arXiv: 1911.10635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RATXJ4RH/Zhang et al. - 2021 - Multi-Agent Reinforcement Learning A Selective Ov.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/372KIZVM/1911.html:text/html},
}

@article{desjardins_natural_2015,
	title = {Natural {Neural} {Networks}},
	url = {http://arxiv.org/abs/1507.00210},
	abstract = {We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.},
	urldate = {2021-06-09},
	journal = {arXiv:1507.00210 [cs, stat]},
	author = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
	month = jul,
	year = {2015},
	note = {arXiv: 1507.00210},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NGBMVJHP/Desjardins et al. - 2015 - Natural Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/5DHJIT7R/1507.html:text/html},
}

@article{du_plessis_online_2015,
	title = {Online {Direct} {Density}-{Ratio} {Estimation} {Applied} to {Inlier}-{Based} {Outlier} {Detection}},
	volume = {27},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/NECO_a_00761},
	doi = {10.1162/NECO_a_00761},
	abstract = {Many machine learning problems, such as nonstationarity adaptation, outlier detection, dimensionality reduction, and conditional density estimation, can be effectively solved by using the ratio of probability densities. Since the naive two-step procedure of first estimating the probability densities and then taking their ratio performs poorly, methods to directly estimate the density ratio from two sets of samples without density estimation have been extensively studied recently. However, these methods are batch algorithms that use the whole data set to estimate the density ratio, and they are inefficient in the online setup, where training samples are provided sequentially and solutions are updated incrementally without storing previous samples. In this letter, we propose two online density-ratio estimators based on the adaptive regularization of weight vectors. Through experiments on inlier-based outlier detection, we demonstrate the usefulness of the proposed methods.},
	number = {9},
	urldate = {2021-06-09},
	journal = {Neural Computation},
	author = {du Plessis, Marthinus Christoffel and Shiino, Hiroaki and Sugiyama, Masashi},
	month = sep,
	year = {2015},
	pages = {1899--1914},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/N2Z8LDNN/du Plessis et al. - 2015 - Online Direct Density-Ratio Estimation Applied to .pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/NHD99MVR/Online-Direct-Density-Ratio-Estimation-Applied-to.html:text/html},
}

@article{xiao_disentangling_2020,
	title = {Disentangling {Trainability} and {Generalization} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1912.13053},
	abstract = {A longstanding goal in the theory of deep learning is to characterize the conditions under which a given neural network architecture will be trainable, and if so, how well it might generalize to unseen data. In this work, we provide such a characterization in the limit of very wide and very deep networks, for which the analysis simplifies considerably. For wide networks, the trajectory under gradient descent is governed by the Neural Tangent Kernel (NTK), and for deep networks the NTK itself maintains only weak data dependence. By analyzing the spectrum of the NTK, we formulate necessary conditions for trainability and generalization across a range of architectures, including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). We identify large regions of hyperparameter space for which networks can memorize the training set but completely fail to generalize. We find that CNNs without global average pooling behave almost identically to FCNs, but that CNNs with pooling have markedly different and often better generalization performance. These theoretical results are corroborated experimentally on CIFAR10 for a variety of network architectures and we include a colab notebook that reproduces the essential results of the paper.},
	urldate = {2021-06-09},
	journal = {arXiv:1912.13053 [cs, stat]},
	author = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S.},
	month = jul,
	year = {2020},
	note = {arXiv: 1912.13053},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EW77V5KM/Xiao et al. - 2020 - Disentangling Trainability and Generalization in D.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ED4MTICS/1912.html:text/html},
}

@article{pascanu_difficulty_2013,
	title = {On the difficulty of training {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1211.5063},
	abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	urldate = {2021-06-09},
	journal = {arXiv:1211.5063 [cs]},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	month = feb,
	year = {2013},
	note = {arXiv: 1211.5063},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/22EJEVFL/Pascanu et al. - 2013 - On the difficulty of training Recurrent Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LUQGGHJK/1211.html:text/html},
}

@article{shaheen_continual_2021,
	title = {Continual {Learning} for {Real}-{World} {Autonomous} {Systems}: {Algorithms}, {Challenges} and {Frameworks}},
	shorttitle = {Continual {Learning} for {Real}-{World} {Autonomous} {Systems}},
	url = {http://arxiv.org/abs/2105.12374},
	abstract = {Continual learning is essential for all real-world applications, as frozen pre-trained models cannot effectively deal with non-stationary data distributions. The purpose of this study is to review the state-of-the-art methods that allow continuous learning of computational models over time. We primarily focus on the learning algorithms that perform continuous learning in an online fashion from considerably large (or infinite) sequential data and require substantially low computational and memory resources. We critically analyze the key challenges associated with continual learning for autonomous real-world systems and compare current methods in terms of computations, memory, and network/model complexity. We also briefly describe the implementations of continuous learning algorithms under three main autonomous systems, i.e., self-driving vehicles, unmanned aerial vehicles, and robotics. The learning methods of these autonomous systems and their strengths and limitations are extensively explored in this article.},
	urldate = {2021-06-09},
	journal = {arXiv:2105.12374 [cs]},
	author = {Shaheen, Khadija and Hanif, Muhammad Abdullah and Hasan, Osman and Shafique, Muhammad},
	month = may,
	year = {2021},
	note = {arXiv: 2105.12374},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6L3XC5BX/Shaheen et al. - 2021 - Continual Learning for Real-World Autonomous Syste.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F6MNX5KN/2105.html:text/html},
}

@article{vorontsov_orthogonality_2017,
	title = {On orthogonality and learning recurrent networks with long term dependencies},
	url = {http://arxiv.org/abs/1702.00071},
	abstract = {It is well known that it is challenging to train deep neural networks and recurrent neural networks for tasks that exhibit long term dependencies. The vanishing or exploding gradient problem is a well known issue associated with these challenges. One approach to addressing vanishing and exploding gradients is to use either soft or hard constraints on weight matrices so as to encourage or enforce orthogonality. Orthogonal matrices preserve gradient norm during backpropagation and may therefore be a desirable property. This paper explores issues with optimization convergence, speed and gradient stability when encouraging or enforcing orthogonality. To perform this analysis, we propose a weight matrix factorization and parameterization strategy through which we can bound matrix norms and therein control the degree of expansivity induced during backpropagation. We find that hard constraints on orthogonality can negatively affect the speed of convergence and model performance.},
	urldate = {2021-06-09},
	journal = {arXiv:1702.00071 [cs]},
	author = {Vorontsov, Eugene and Trabelsi, Chiheb and Kadoury, Samuel and Pal, Chris},
	month = oct,
	year = {2017},
	note = {arXiv: 1702.00071},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7SWP2Q9R/Vorontsov et al. - 2017 - On orthogonality and learning recurrent networks w.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/7J2PFB28/1702.html:text/html},
}

@article{huang_orthogonal_2017,
	title = {Orthogonal {Weight} {Normalization}: {Solution} to {Optimization} over {Multiple} {Dependent} {Stiefel} {Manifolds} in {Deep} {Neural} {Networks}},
	shorttitle = {Orthogonal {Weight} {Normalization}},
	url = {http://arxiv.org/abs/1709.06079},
	abstract = {Orthogonal matrix has shown advantages in training Recurrent Neural Networks (RNNs), but such matrix is limited to be square for the hidden-to-hidden transformation in RNNs. In this paper, we generalize such square orthogonal matrix to orthogonal rectangular matrix and formulating this problem in feed-forward Neural Networks (FNNs) as Optimization over Multiple Dependent Stiefel Manifolds (OMDSM). We show that the rectangular orthogonal matrix can stabilize the distribution of network activations and regularize FNNs. We also propose a novel orthogonal weight normalization method to solve OMDSM. Particularly, it constructs orthogonal transformation over proxy parameters to ensure the weight matrix is orthogonal and back-propagates gradient information through the transformation during training. To guarantee stability, we minimize the distortions between proxy parameters and canonical weights over all tractable orthogonal transformations. In addition, we design an orthogonal linear module (OLM) to learn orthogonal filter banks in practice, which can be used as an alternative to standard linear module. Extensive experiments demonstrate that by simply substituting OLM for standard linear module without revising any experimental protocols, our method largely improves the performance of the state-of-the-art networks, including Inception and residual networks on CIFAR and ImageNet datasets. In particular, we have reduced the test error of wide residual network on CIFAR-100 from 20.04\% to 18.61\% with such simple substitution. Our code is available online for result reproduction.},
	urldate = {2021-06-09},
	journal = {arXiv:1709.06079 [cs]},
	author = {Huang, Lei and Liu, Xianglong and Lang, Bo and Yu, Adams Wei and Wang, Yongliang and Li, Bo},
	month = nov,
	year = {2017},
	note = {arXiv: 1709.06079},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/238PNFFY/Huang et al. - 2017 - Orthogonal Weight Normalization Solution to Optim.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GH7RCK7I/1709.html:text/html},
}

@article{liu_pay_2021,
	title = {Pay {Attention} to {MLPs}},
	url = {http://arxiv.org/abs/2105.08050},
	abstract = {Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute.},
	urldate = {2021-06-09},
	journal = {arXiv:2105.08050 [cs]},
	author = {Liu, Hanxiao and Dai, Zihang and So, David R. and Le, Quoc V.},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.08050},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SX64HYUA/Liu et al. - 2021 - Pay Attention to MLPs.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/85KU8HFN/2105.html:text/html},
}

@article{mcrae_catastrophic_1993,
	title = {Catastrophic {Interference} is {Eliminated} in {Pretrained} {Networks}},
	abstract = {this article, we outline the major cause of catastrophic interference in standard networks, describe recent approaches to the problem, and present a novel approach. In contrast to previous work on the sequential learning problem that has manipulated network parameters and architecture (e.g., Hinton \& Plaut, 1987; Kortge, 1990; Kruschke, 1992; Lewandowsky, 1991; Sloman \& Rumelhart, 1992) we attempt to make the simulation more similar to the human situation that is being modeled. When this is done, catastrophic interference is eliminated.},
	author = {McRae, Ken},
	month = mar,
	year = {1993},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/EZUZS6WT/McRae - 1993 - Catastrophic Interference is Eliminated in Pretrai.pdf:application/pdf},
}

@article{carpenter_distributed_1994,
	title = {A distributed outstar network for spatial pattern learning},
	volume = {7},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0893608094900647},
	doi = {10.1016/0893-6080(94)90064-7},
	language = {en},
	number = {1},
	urldate = {2021-06-01},
	journal = {Neural Networks},
	author = {Carpenter, Gail A.},
	month = jan,
	year = {1994},
	pages = {159--168},
	file = {Carpenter - 1994 - A distributed outstar network for spatial pattern .pdf:/Users/tokio/Zotero/storage/D4MJ9CLR/Carpenter - 1994 - A distributed outstar network for spatial pattern .pdf:application/pdf},
}

@article{jacot_neural_2020,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/1806.07572},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	urldate = {2021-06-01},
	journal = {arXiv:1806.07572 [cs, math, stat]},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	month = feb,
	year = {2020},
	note = {arXiv: 1806.07572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8DMBSVIT/Jacot et al. - 2020 - Neural Tangent Kernel Convergence and Generalizat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ACDJ4V6Z/1806.html:text/html},
}

@article{jianping_zhou_special_2006,
	title = {Special paraunitary matrices, {Cayley} transform, and multidimensional orthogonal filter banks},
	volume = {15},
	issn = {1057-7149},
	url = {https://ieeexplore.ieee.org/document/1576824},
	doi = {10.1109/TIP.2005.863046},
	abstract = {We characterize and design multidimensional orthogonal ﬁlter banks using special paraunitary matrices and the Cayley transform. Orthogonal ﬁlter banks are represented by paraunitary matrices in the polyphase domain. We deﬁne special paraunitary matrices as paraunitary matrices with unit determinant. We show that every paraunitary matrix can be characterized by a special paraunitary matrix and a phase factor. Therefore the design of paraunitary matrices (and thus of orthogonal ﬁlter banks) becomes the design of special paraunitary matrices, which requires a smaller set of nonlinear equations. Moreover, we provide a complete characterization of special paraunitary matrices in the Cayley domain, which converts nonlinear constraints into linear constraints. Our method greatly simpliﬁes the design of multidimensional orthogonal ﬁlter banks and leads to complete characterizations of such ﬁlter banks.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {IEEE Transactions on Image Processing},
	author = {{Jianping Zhou} and Do, M.N. and Kovacevic, J.},
	month = feb,
	year = {2006},
	pages = {511--519},
	file = {Jianping Zhou et al. - 2006 - Special paraunitary matrices, Cayley transform, an.pdf:/Users/tokio/Zotero/storage/DCGNZGLK/Jianping Zhou et al. - 2006 - Special paraunitary matrices, Cayley transform, an.pdf:application/pdf},
}

@article{xie_all_2017,
	title = {All {You} {Need} is {Beyond} a {Good} {Init}: {Exploring} {Better} {Solution} for {Training} {Extremely} {Deep} {Convolutional} {Neural} {Networks} with {Orthonormality} and {Modulation}},
	shorttitle = {All {You} {Need} is {Beyond} a {Good} {Init}},
	url = {http://arxiv.org/abs/1703.01827},
	abstract = {Deep neural network is difficult to train and this predicament becomes worse as the depth increases. The essence of this problem exists in the magnitude of backpropagated errors that will result in gradient vanishing or exploding phenomenon. We show that a variant of regularizer which utilizes orthonormality among different filter banks can alleviate this problem. Moreover, we design a backward error modulation mechanism based on the quasi-isometry assumption between two consecutive parametric layers. Equipped with these two ingredients, we propose several novel optimization solutions that can be utilized for training a specific-structured (repetitively triple modules of Conv-BNReLU) extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/ identity mappings from scratch. Experiments show that our proposed solutions can achieve distinct improvements for a 44-layer and a 110-layer plain networks on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train plain CNNs to match the performance of the residual counterparts. Besides, we propose new principles for designing network structure from the insights evoked by orthonormality. Combined with residual structure, we achieve comparative performance on the ImageNet dataset.},
	urldate = {2021-06-01},
	journal = {arXiv:1703.01827 [cs]},
	author = {Xie, Di and Xiong, Jiang and Pu, Shiliang},
	month = apr,
	year = {2017},
	note = {arXiv: 1703.01827},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/T4CK4CRC/Xie et al. - 2017 - All You Need is Beyond a Good Init Exploring Bett.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/AZMGYWKD/1703.html:text/html},
}

@article{coop_ensemble_2013,
	title = {Ensemble {Learning} in {Fixed} {Expansion} {Layer} {Networks} for {Mitigating} {Catastrophic} {Forgetting}},
	volume = {24},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/6544273/},
	doi = {10.1109/TNNLS.2013.2264952},
	abstract = {Catastrophic forgetting is a well-studied attribute of most parameterized supervised learning systems. A variation of this phenomenon, in the context of feedforward neural networks, arises when nonstationary inputs lead to loss of previously learned mappings. The majority of the schemes proposed in the literature for mitigating catastrophic forgetting were not data driven and did not scale well. We introduce the ﬁxed expansion layer (FEL) feedforward neural network, which embeds a sparsely encoding hidden layer to help mitigate forgetting of prior learned representations. In addition, we investigate a novel framework for training ensembles of FEL networks, based on exploiting an information-theoretic measure of diversity between FEL learners, to further control undesired plasticity. The proposed methodology is demonstrated on a basic classiﬁcation task, clearly emphasizing its advantages over existing techniques. The architecture proposed can be enhanced to address a range of computational intelligence tasks, such as regression problems and system control.},
	language = {en},
	number = {10},
	urldate = {2021-06-01},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Coop, Robert and Mishtal, Aaron and Arel, Itamar},
	month = oct,
	year = {2013},
	pages = {1623--1634},
	file = {Coop et al. - 2013 - Ensemble Learning in Fixed Expansion Layer Network.pdf:/Users/tokio/Zotero/storage/UTYJ7SAB/Coop et al. - 2013 - Ensemble Learning in Fixed Expansion Layer Network.pdf:application/pdf},
}

@article{watanabe_divergence_2011,
	title = {Divergence measures and a general framework for local variational approximation},
	volume = {24},
	issn = {08936080},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089360801100164X},
	doi = {10.1016/j.neunet.2011.06.004},
	abstract = {The local variational method is a technique to approximate an intractable posterior distribution in Bayesian learning. This article formulates a general framework for local variational approximation and shows that its objective function is decomposable into the sum of the Kullback information and the expected Bregman divergence from the approximating posterior distribution to the Bayesian posterior distribution. Based on a geometrical argument in the space of approximating posteriors, we propose an efficient method to evaluate an upper bound of the marginal likelihood. Moreover, we demonstrate that the variational Bayesian approach for the latent variable models can be viewed as a special case of this general framework.},
	language = {en},
	number = {10},
	urldate = {2021-06-01},
	journal = {Neural Networks},
	author = {Watanabe, Kazuho and Okada, Masato and Ikeda, Kazushi},
	month = dec,
	year = {2011},
	pages = {1102--1109},
	file = {Watanabe et al. - 2011 - Divergence measures and a general framework for lo.pdf:/Users/tokio/Zotero/storage/CEUBRNZ3/Watanabe et al. - 2011 - Divergence measures and a general framework for lo.pdf:application/pdf},
}

@article{lin_why_2017,
	title = {Why {Does} {Deep} and {Cheap} {Learning} {Work} {So} {Well}?},
	volume = {168},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-017-1836-5},
	doi = {10.1007/s10955-017-1836-5},
	abstract = {We show how the success of deep learning could depend not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can frequently be approximated through “cheap learning” with exponentially fewer parameters than generic ones. We explore how properties frequently encountered in physics such as symmetry, locality, compositionality, and polynomial log-probability translate into exceptionally simple neural networks. We further argue that when the statistical process generating the data is of a certain hierarchical form prevalent in physics and machine learning, a deep neural network can be more efﬁcient than a shallow one. We formalize these claims using information theory and discuss the relation to the renormalization group. We prove various “no-ﬂattening theorems” showing when efﬁcient linear deep networks cannot be accurately approximated by shallow ones without efﬁciency loss; for example, we show that n variables cannot be multiplied using fewer than 2n neurons in a single hidden layer.},
	language = {en},
	number = {6},
	urldate = {2021-05-30},
	journal = {Journal of Statistical Physics},
	author = {Lin, Henry W. and Tegmark, Max and Rolnick, David},
	month = sep,
	year = {2017},
	pages = {1223--1247},
	file = {,DanaInfo=link.springer.com,SSL+s10955-017-1836-5.pdf:/Users/tokio/Zotero/storage/RS2S7X6V/,DanaInfo=link.springer.com,SSL+s10955-017-1836-5.pdf:application/pdf},
}

@article{french_catastrophic_1999,
	title = {Catastrophic forgetting in connectionist networks},
	volume = {3},
	language = {en},
	number = {4},
	journal = {Trends in Cognitive Sciences},
	author = {French, Robert M},
	year = {1999},
	pages = {8},
	file = {French - 1999 - Catastrophic forgetting in connectionist networks.pdf:/Users/tokio/Zotero/storage/ANCTEIW9/French - 1999 - Catastrophic forgetting in connectionist networks.pdf:application/pdf},
}

@article{noauthor_catastrophic_1999,
	title = {Catastrophic forgetting in connectionist networks},
	volume = {3},
	issn = {1364-6613},
	url = {https://gateway2.itc.u-tokyo.ac.jp:11002/science/article/pii/S1364661399012942},
	doi = {10.1016/S1364-6613(99)01294-2},
	abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition shoul…},
	language = {en},
	number = {4},
	urldate = {2021-05-30},
	journal = {Trends in Cognitive Sciences},
	month = apr,
	year = {1999},
	note = {Publisher: Elsevier Current Trends},
	pages = {128--135},
	file = {Snapshot:/Users/tokio/Zotero/storage/9XUYGNNT/S1364661399012942.html:text/html},
}

@article{lee_deep_2018,
	title = {Deep {Neural} {Networks} as {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1711.00165},
	abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
	urldate = {2021-05-30},
	journal = {arXiv:1711.00165 [cs, stat]},
	author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
	month = mar,
	year = {2018},
	note = {arXiv: 1711.00165},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KJKQEP4Z/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FDENF9G2/1711.html:text/html},
}

@article{bietti_inductive_2019,
	title = {On the {Inductive} {Bias} of {Neural} {Tangent} {Kernels}},
	url = {http://arxiv.org/abs/1905.12173},
	abstract = {State-of-the-art neural networks are heavily over-parameterized, making the optimization algorithm a crucial ingredient for learning predictive models with good generalization properties. A recent line of work has shown that in a certain over-parameterized regime, the learning dynamics of gradient descent are governed by a certain kernel obtained at initialization, called the neural tangent kernel. We study the inductive bias of learning in such a regime by analyzing this kernel and the corresponding function space (RKHS). In particular, we study smoothness, approximation, and stability properties of functions with finite norm, including stability to image deformations in the case of convolutional networks, and compare to other known kernels for similar architectures.},
	urldate = {2021-05-30},
	journal = {arXiv:1905.12173 [cs, stat]},
	author = {Bietti, Alberto and Mairal, Julien},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.12173},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RK4ZUE36/Bietti and Mairal - 2019 - On the Inductive Bias of Neural Tangent Kernels.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3GFPGFGQ/1905.html:text/html},
}

@article{chaudhari_entropy-sgd_2017,
	title = {Entropy-{SGD}: {Biasing} {Gradient} {Descent} {Into} {Wide} {Valleys}},
	shorttitle = {Entropy-{SGD}},
	url = {http://arxiv.org/abs/1611.01838},
	abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
	urldate = {2021-05-30},
	journal = {arXiv:1611.01838 [cs, stat]},
	author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.01838},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VVPBZB5J/Chaudhari et al. - 2017 - Entropy-SGD Biasing Gradient Descent Into Wide Va.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BQ3XMRLX/1611.html:text/html},
}

@article{chizat_global_2018,
	title = {On the {Global} {Convergence} of {Gradient} {Descent} for {Over}-parameterized {Models} using {Optimal} {Transport}},
	url = {http://arxiv.org/abs/1805.09545},
	abstract = {Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.},
	urldate = {2021-05-30},
	journal = {arXiv:1805.09545 [cs, math, stat]},
	author = {Chizat, Lenaic and Bach, Francis},
	month = oct,
	year = {2018},
	note = {arXiv: 1805.09545},
	keywords = {Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KVU5GKNB/Chizat and Bach - 2018 - On the Global Convergence of Gradient Descent for .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/8444D2ZD/1805.html:text/html},
}

@article{le_learning_2019,
	title = {Learning to {Remember} {More} with {Less} {Memorization}},
	url = {http://arxiv.org/abs/1901.01347},
	abstract = {Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks.},
	urldate = {2021-05-30},
	journal = {arXiv:1901.01347 [cs, stat]},
	author = {Le, Hung and Tran, Truyen and Venkatesh, Svetha},
	month = mar,
	year = {2019},
	note = {arXiv: 1901.01347},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EDT2NSH3/Le et al. - 2019 - Learning to Remember More with Less Memorization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UW2D3HHZ/1901.html:text/html},
}

@article{gregor_finding_2020,
	title = {Finding online neural update rules by learning to remember},
	url = {http://arxiv.org/abs/2003.03124},
	abstract = {We investigate learning of the online local update rules for neural activations (bodies) and weights (synapses) from scratch. We represent the states of each weight and activation by small vectors, and parameterize their updates using (meta-) neural networks. Different neuron types are represented by different embedding vectors which allows the same two functions to be used for all neurons. Instead of training directly for the objective using evolution or long term back-propagation, as is commonly done in similar systems, we motivate and study a different objective: That of remembering past snippets of experience. We explain how this objective relates to standard back-propagation training and other forms of learning. We train for this objective using short term back-propagation and analyze the performance as a function of both the different network types and the difficulty of the problem. We find that this analysis gives interesting insights onto what constitutes a learning rule. We also discuss how such system could form a natural substrate for addressing topics such as episodic memories, meta-learning and auxiliary objectives.},
	urldate = {2021-05-30},
	journal = {arXiv:2003.03124 [cs, stat]},
	author = {Gregor, Karol},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03124},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/H82H642K/Gregor - 2020 - Finding online neural update rules by learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DK4CT6SE/2003.html:text/html},
}

@article{schoenholz_deep_2017,
	title = {Deep {Information} {Propagation}},
	url = {http://arxiv.org/abs/1611.01232},
	abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
	urldate = {2021-05-30},
	journal = {arXiv:1611.01232 [cs, stat]},
	author = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.01232},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/JR97MVTQ/Schoenholz et al. - 2017 - Deep Information Propagation.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FINNTBU2/1611.html:text/html},
}

@inproceedings{karakida_universal_2019,
	title = {Universal {Statistics} of {Fisher} {Information} in {Deep} {Neural} {Networks}: {Mean} {Field} {Approach}},
	shorttitle = {Universal {Statistics} of {Fisher} {Information} in {Deep} {Neural} {Networks}},
	url = {http://proceedings.mlr.press/v89/karakida19a.html},
	abstract = {The Fisher information matrix (FIM) is a fundamental quantity to represent the characteristics of a stochastic model, including deep neural networks (DNNs). The present study reveals novel statisti...},
	language = {en},
	urldate = {2021-05-29},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1032--1041},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/YC5SZBP3/Karakida et al. - 2019 - Universal Statistics of Fisher Information in Deep.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/CMGYYHU2/karakida19a.html:text/html},
}

@inproceedings{arpit_closer_2017,
	title = {A {Closer} {Look} at {Memorization} in {Deep} {Networks}},
	url = {http://proceedings.mlr.press/v70/arpit17a.html},
	abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our resu...},
	language = {en},
	urldate = {2021-05-29},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {233--242},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZUW6XIRY/Arpit et al. - 2017 - A Closer Look at Memorization in Deep Networks.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/IJSIRTAA/arpit17a.html:text/html},
}

@article{poole_exponential_2016,
	title = {Exponential expressivity in deep neural networks through transient chaos},
	url = {http://arxiv.org/abs/1606.05340},
	abstract = {We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.},
	urldate = {2021-05-29},
	journal = {arXiv:1606.05340 [cond-mat, stat]},
	author = {Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.05340},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/48JLFQY8/Poole et al. - 2016 - Exponential expressivity in deep neural networks t.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RW7BLQ4P/1606.html:text/html},
}

@article{ramasesh_anatomy_2020,
	title = {Anatomy of {Catastrophic} {Forgetting}: {Hidden} {Representations} and {Task} {Semantics}},
	shorttitle = {Anatomy of {Catastrophic} {Forgetting}},
	url = {http://arxiv.org/abs/2007.07400},
	abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting: a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
	urldate = {2021-05-28},
	journal = {arXiv:2007.07400 [cs, stat]},
	author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.07400},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EJJQW5X5/Ramasesh et al. - 2020 - Anatomy of Catastrophic Forgetting Hidden Represe.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KK6KIS74/2007.html:text/html},
}

@article{lopez-paz_gradient_2017,
	title = {Gradient {Episodic} {Memory} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1706.08840},
	abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
	urldate = {2021-05-28},
	journal = {arXiv:1706.08840 [cs]},
	author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
	month = nov,
	year = {2017},
	note = {arXiv: 1706.08840},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/PC3TRJNH/Lopez-Paz and Ranzato - 2017 - Gradient Episodic Memory for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WB4FCS5Y/1706.html:text/html},
}

@article{caccia_online_2021,
	title = {Online {Fast} {Adaptation} and {Knowledge} {Accumulation}: a {New} {Approach} to {Continual} {Learning}},
	shorttitle = {Online {Fast} {Adaptation} and {Knowledge} {Accumulation}},
	url = {http://arxiv.org/abs/2003.05856},
	abstract = {Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new (out-of-distribution) tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.},
	urldate = {2021-05-28},
	journal = {arXiv:2003.05856 [cs]},
	author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexandre and Vazquez, David and Charlin, Laurent},
	month = jan,
	year = {2021},
	note = {arXiv: 2003.05856},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/S33VV3CS/Caccia et al. - 2021 - Online Fast Adaptation and Knowledge Accumulation.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KWMXM9XA/2003.html:text/html},
}

@article{rebuffi_icarl_2017,
	title = {{iCaRL}: {Incremental} {Classifier} and {Representation} {Learning}},
	shorttitle = {{iCaRL}},
	url = {http://arxiv.org/abs/1611.07725},
	abstract = {A major open problem on the road to artificial intelligence is the development of incrementally learning systems that learn about more and more concepts over time from a stream of data. In this work, we introduce a new training strategy, iCaRL, that allows learning in such a class-incremental way: only the training data for a small number of classes has to be present at the same time and new classes can be added progressively. iCaRL learns strong classifiers and a data representation simultaneously. This distinguishes it from earlier works that were fundamentally limited to fixed data representations and therefore incompatible with deep learning architectures. We show by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can learn many classes incrementally over a long period of time where other strategies quickly fail.},
	urldate = {2021-05-28},
	journal = {arXiv:1611.07725 [cs, stat]},
	author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H.},
	month = apr,
	year = {2017},
	note = {arXiv: 1611.07725},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NND88ICX/Rebuffi et al. - 2017 - iCaRL Incremental Classifier and Representation L.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YA96LJNT/1611.html:text/html},
}

@article{toneva_empirical_2019,
	title = {An {Empirical} {Study} of {Example} {Forgetting} during {Deep} {Neural} {Network} {Learning}},
	url = {http://arxiv.org/abs/1812.05159},
	abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a related phenomenon occurs when data does not undergo a clear distributional shift. We define a `forgetting event' to have occurred when an individual training example transitions from being classified correctly to incorrectly over the course of learning. Across several benchmark data sets, we find that: (i) certain examples are forgotten with high frequency, and some not at all; (ii) a data set's (un)forgettable examples generalize across neural architectures; and (iii) based on forgetting dynamics, a significant fraction of examples can be omitted from the training data set while still maintaining state-of-the-art generalization performance.},
	urldate = {2021-05-28},
	journal = {arXiv:1812.05159 [cs, stat]},
	author = {Toneva, Mariya and Sordoni, Alessandro and Combes, Remi Tachet des and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J.},
	month = nov,
	year = {2019},
	note = {arXiv: 1812.05159},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/66QSWBTA/Toneva et al. - 2019 - An Empirical Study of Example Forgetting during De.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/5WT9E9CJ/1812.html:text/html},
}

@article{diaz-rodriguez_dont_2018,
	title = {Don't forget, there is more than forgetting: new metrics for {Continual} {Learning}},
	shorttitle = {Don't forget, there is more than forgetting},
	url = {http://arxiv.org/abs/1810.13166},
	abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
	urldate = {2021-05-28},
	journal = {arXiv:1810.13166 [cs]},
	author = {Díaz-Rodríguez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.13166},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition, 68T05, cs.LG, cs.AI, cs.CV, cs.NE, stat.ML},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QSPI734S/Díaz-Rodríguez et al. - 2018 - Don't forget, there is more than forgetting new m.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/78QTEFDY/1810.html:text/html},
}

@article{nguyen_toward_2019,
	title = {Toward {Understanding} {Catastrophic} {Forgetting} in {Continual} {Learning}},
	url = {http://arxiv.org/abs/1908.01091},
	abstract = {We study the relationship between catastrophic forgetting and properties of task sequences. In particular, given a sequence of tasks, we would like to understand which properties of this sequence influence the error rates of continual learning algorithms trained on the sequence. To this end, we propose a new procedure that makes use of recent developments in task space modeling as well as correlation analysis to specify and analyze the properties we are interested in. As an application, we apply our procedure to study two properties of a task sequence: (1) total complexity and (2) sequential heterogeneity. We show that error rates are strongly and positively correlated to a task sequence's total complexity for some state-of-the-art algorithms. We also show that, surprisingly, the error rates have no or even negative correlations in some cases to sequential heterogeneity. Our findings suggest directions for improving continual learning benchmarks and methods.},
	urldate = {2021-05-28},
	journal = {arXiv:1908.01091 [cs, stat]},
	author = {Nguyen, Cuong V. and Achille, Alessandro and Lam, Michael and Hassner, Tal and Mahadevan, Vijay and Soatto, Stefano},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.01091},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6MFSJITL/Nguyen et al. - 2019 - Toward Understanding Catastrophic Forgetting in Co.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/C796KQVV/1908.html:text/html},
}

@inproceedings{kurle_continual_2019,
	title = {Continual {Learning} with {Bayesian} {Neural} {Networks} for {Non}-{Stationary} {Data}},
	url = {https://openreview.net/forum?id=SJlsFpVtDB},
	abstract = {This work addresses continual learning for non-stationary data, using Bayesian neural networks and memory-based online variational Bayes.},
	language = {en},
	urldate = {2021-05-28},
	author = {Kurle, Richard and Cseke, Botond and Klushyn, Alexej and Smagt, Patrick van der and Günnemann, Stephan},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/IDK9WMB9/Kurle et al. - 2019 - Continual Learning with Bayesian Neural Networks f.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/FYE9VQFZ/forum.html:text/html},
}

@inproceedings{he_overcoming_2018,
	title = {Overcoming {Catastrophic} {Interference} using {Conceptor}-{Aided} {Backpropagation}},
	url = {https://openreview.net/forum?id=B1al7jg0b},
	abstract = {We propose a variant of the backpropagation algorithm, in which gradients are shielded by conceptors against degradation of previously learned tasks.},
	language = {en},
	urldate = {2021-05-28},
	author = {He, Xu and Jaeger, Herbert},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/DU3FEBCH/He and Jaeger - 2018 - Overcoming Catastrophic Interference using Concept.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/SE9KJJ73/forum.html:text/html},
}

@article{neal_mcmc_2012,
	title = {{MCMC} using {Hamiltonian} dynamics},
	url = {http://arxiv.org/abs/1206.1901},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	urldate = {2021-05-25},
	journal = {arXiv:1206.1901 [physics, stat]},
	author = {Neal, Radford M.},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.1901},
	keywords = {Physics - Computational Physics, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5G8YF99N/Neal - 2012 - MCMC using Hamiltonian dynamics.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PGTWSEGN/1206.html:text/html},
}

@article{watanabe_widely_nodate,
	title = {A {Widely} {Applicable} {Bayesian} {Information} {Criterion}},
	abstract = {A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive deﬁnite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is deﬁned by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold.},
	language = {en},
	author = {Watanabe, Sumio},
	pages = {31},
	file = {Watanabe - A Widely Applicable Bayesian Information Criterion.pdf:/Users/tokio/Zotero/storage/HVRVW85C/Watanabe - A Widely Applicable Bayesian Information Criterion.pdf:application/pdf},
}

@article{ma_hsic_2019,
	title = {The {HSIC} {Bottleneck}: {Deep} {Learning} without {Back}-{Propagation}},
	shorttitle = {The {HSIC} {Bottleneck}},
	url = {http://arxiv.org/abs/1908.01580},
	abstract = {We introduce the HSIC (Hilbert-Schmidt independence criterion) bottleneck for training deep neural networks. The HSIC bottleneck is an alternative to the conventional cross-entropy loss and backpropagation that has a number of distinct advantages. It mitigates exploding and vanishing gradients, resulting in the ability to learn very deep networks without skip connections. There is no requirement for symmetric feedback or update locking. We find that the HSIC bottleneck provides performance on MNIST/FashionMNIST/CIFAR10 classification comparable to backpropagation with a cross-entropy target, even when the system is not encouraged to make the output resemble the classification labels. Appending a single layer trained with SGD (without backpropagation) to reformat the information further improves performance.},
	urldate = {2021-05-24},
	journal = {arXiv:1908.01580 [cs, stat]},
	author = {Ma, Wan-Duo Kurt and Lewis, J. P. and Kleijn, W. Bastiaan},
	month = dec,
	year = {2019},
	note = {arXiv: 1908.01580},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/69P72EPB/Ma et al. - 2019 - The HSIC Bottleneck Deep Learning without Back-Pr.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TE6MWYUL/1908.html:text/html},
}

@article{sun_learning_2019,
	title = {Learning {Sparse} {Sharing} {Architectures} for {Multiple} {Tasks}},
	url = {http://arxiv.org/abs/1911.05034},
	abstract = {Most existing deep multi-task learning models are based on parameter sharing, such as hard sharing, hierarchical sharing, and soft sharing. How choosing a suitable sharing mechanism depends on the relations among the tasks, which is not easy since it is difficult to understand the underlying shared factors among these tasks. In this paper, we propose a novel parameter sharing mechanism, named {\textbackslash}emph\{Sparse Sharing\}. Given multiple tasks, our approach automatically finds a sparse sharing structure. We start with an over-parameterized base network, from which each task extracts a subnetwork. The subnetworks of multiple tasks are partially overlapped and trained in parallel. We show that both hard sharing and hierarchical sharing can be formulated as particular instances of the sparse sharing framework. We conduct extensive experiments on three sequence labeling tasks. Compared with single-task models and three typical multi-task learning baselines, our proposed approach achieves consistent improvement while requiring fewer parameters.},
	urldate = {2021-05-24},
	journal = {arXiv:1911.05034 [cs]},
	author = {Sun, Tianxiang and Shao, Yunfan and Li, Xiaonan and Liu, Pengfei and Yan, Hang and Qiu, Xipeng and Huang, Xuanjing},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.05034},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6D97TRLC/Sun et al. - 2019 - Learning Sparse Sharing Architectures for Multiple.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YLLRKKW5/1911.html:text/html},
}

@article{zhang_multimodal_2020,
	title = {Multimodal {Intelligence}: {Representation} {Learning}, {Information} {Fusion}, and {Applications}},
	volume = {14},
	issn = {1932-4553, 1941-0484},
	shorttitle = {Multimodal {Intelligence}},
	url = {http://arxiv.org/abs/1911.03977},
	doi = {10.1109/JSTSP.2020.2987728},
	abstract = {Deep learning methods have revolutionized speech recognition, image recognition, and natural language processing since 2010. Each of these tasks involves a single modality in their input signals. However, many applications in the artificial intelligence field involve multiple modalities. Therefore, it is of broad interest to study the more difficult and complex problem of modeling and learning across multiple modalities. In this paper, we provide a technical review of available models and learning methods for multimodal intelligence. The main focus of this review is the combination of vision and natural language modalities, which has become an important topic in both the computer vision and natural language processing research communities. This review provides a comprehensive analysis of recent works on multimodal deep learning from three perspectives: learning multimodal representations, fusing multimodal signals at various levels, and multimodal applications. Regarding multimodal representation learning, we review the key concepts of embedding, which unify multimodal signals into a single vector space and thereby enable cross-modality signal processing. We also review the properties of many types of embeddings that are constructed and learned for general downstream tasks. Regarding multimodal fusion, this review focuses on special architectures for the integration of representations of unimodal signals for a particular task. Regarding applications, selected areas of a broad interest in the current literature are covered, including image-to-text caption generation, text-to-image generation, and visual question answering. We believe that this review will facilitate future studies in the emerging field of multimodal intelligence for related communities.},
	number = {3},
	urldate = {2021-05-24},
	journal = {IEEE Journal of Selected Topics in Signal Processing},
	author = {Zhang, Chao and Yang, Zichao and He, Xiaodong and Deng, Li},
	month = mar,
	year = {2020},
	note = {arXiv: 1911.03977},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	pages = {478--493},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/R8LHV47N/Zhang et al. - 2020 - Multimodal Intelligence Representation Learning, .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/7XKNVKYZ/1911.html:text/html},
}

@article{geiger_information_2020,
	title = {On {Information} {Plane} {Analyses} of {Neural} {Network} {Classifiers} -- {A} {Review}},
	url = {http://arxiv.org/abs/2003.09671},
	abstract = {We review the current literature concerned with information plane analyses of neural network classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in information planes is not necessarily information-theoretic, but is rather often compatible with geometric compression of the latent representations. This insight gives the information plane a renewed justification. Aside from this, we shed light on the problem of estimating mutual information in deterministic neural networks and its consequences. Specifically, we argue that even in feed-forward neural networks the data processing inequality need not hold for estimates of mutual information. Similarly, while a fitting phase, in which the mutual information between the latent representation and the target increases, is necessary (but not sufficient) for good classification performance, depending on the specifics of mutual information estimation such a fitting phase need not be visible in the information plane.},
	urldate = {2021-05-24},
	journal = {arXiv:2003.09671 [cs, math, stat]},
	author = {Geiger, Bernhard C.},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.09671},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KNVTKCCP/Geiger - 2020 - On Information Plane Analyses of Neural Network Cl.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/JYVTTKV6/2003.html:text/html},
}

@article{wortsman_supermasks_2020,
	title = {Supermasks in {Superposition}},
	url = {http://arxiv.org/abs/2006.14769},
	abstract = {We present the Supermasks in Superposition (SupSup) model, capable of sequentially learning thousands of tasks without catastrophic forgetting. Our approach uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask) that achieves good performance. If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. In practice we find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. We also showcase two promising extensions. First, SupSup models can be trained entirely without task identity information, as they may detect when they are uncertain about new data and allocate an additional supermask for the new training distribution. Finally the entire, growing set of supermasks can be stored in a constant-sized reservoir by implicitly storing them as attractors in a fixed-sized Hopfield network.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.14769 [cs, stat]},
	author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.14769},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XVCGFCCI/Wortsman et al. - 2020 - Supermasks in Superposition.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RHIWU2VW/2006.html:text/html},
}

@article{ye_good_2020,
	title = {Good {Subnetworks} {Provably} {Exist}: {Pruning} via {Greedy} {Forward} {Selection}},
	shorttitle = {Good {Subnetworks} {Provably} {Exist}},
	url = {http://arxiv.org/abs/2003.01794},
	abstract = {Recent empirical works show that large deep neural networks are often highly redundant and one can find much smaller subnetworks without a significant drop of accuracy. However, most existing methods of network pruning are empirical and heuristic, leaving it open whether good subnetworks provably exist, how to find them efficiently, and if network pruning can be provably better than direct training using gradient descent. We answer these problems positively by proposing a simple greedy selection approach for finding good subnetworks, which starts from an empty network and greedily adds important neurons from the large network. This differs from the existing methods based on backward elimination, which remove redundant neurons from the large network. Theoretically, applying the greedy selection strategy on sufficiently large \{pre-trained\} networks guarantees to find small subnetworks with lower loss than networks directly trained with gradient descent. Our results also apply to pruning randomly weighted networks. Practically, we improve prior arts of network pruning on learning compact neural architectures on ImageNet, including ResNet, MobilenetV2/V3, and ProxylessNet. Our theory and empirical results on MobileNet suggest that we should fine-tune the pruned subnetworks to leverage the information from the large model, instead of re-training from new random initialization as suggested in {\textbackslash}citet\{liu2018rethinking\}.},
	urldate = {2021-05-24},
	journal = {arXiv:2003.01794 [cs, stat]},
	author = {Ye, Mao and Gong, Chengyue and Nie, Lizhen and Zhou, Denny and Klivans, Adam and Liu, Qiang},
	month = oct,
	year = {2020},
	note = {arXiv: 2003.01794},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/L5C6CDWJ/Ye et al. - 2020 - Good Subnetworks Provably Exist Pruning via Greed.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/EUTSICFK/2003.html:text/html},
}

@article{riemer_learning_2019,
	title = {Learning to {Learn} without {Forgetting} by {Maximizing} {Transfer} and {Minimizing} {Interference}},
	url = {http://arxiv.org/abs/1810.11910},
	abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
	urldate = {2021-05-24},
	journal = {arXiv:1810.11910 [cs, stat]},
	author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
	month = may,
	year = {2019},
	note = {arXiv: 1810.11910},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/K65D58G2/Riemer et al. - 2019 - Learning to Learn without Forgetting by Maximizing.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/J4PSDC8H/1810.html:text/html},
}

@article{masse_alleviating_2018,
	title = {Alleviating catastrophic forgetting using context-dependent gating and synaptic stabilization},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/1802.01569},
	doi = {10.1073/pnas.1803839115},
	abstract = {Humans and most animals can learn new tasks without forgetting old ones. However, training artificial neural networks (ANNs) on new tasks typically cause it to forget previously learned tasks. This phenomenon is the result of "catastrophic forgetting", in which training an ANN disrupts connection weights that were important for solving previous tasks, degrading task performance. Several recent studies have proposed methods to stabilize connection weights of ANNs that are deemed most important for solving a task, which helps alleviate catastrophic forgetting. Here, drawing inspiration from algorithms that are believed to be implemented in vivo, we propose a complementary method: adding a context-dependent gating signal, such that only sparse, mostly non-overlapping patterns of units are active for any one task. This method is easy to implement, requires little computational overhead, and allows ANNs to maintain high performance across large numbers of sequentially presented tasks when combined with weight stabilization. This work provides another example of how neuroscience-inspired algorithms can benefit ANN design and capability.},
	number = {44},
	urldate = {2021-05-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Masse, Nicolas Y. and Grant, Gregory D. and Freedman, David J.},
	month = oct,
	year = {2018},
	note = {arXiv: 1802.01569},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {E10467--E10475},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5M4P764Q/Masse et al. - 2018 - Alleviating catastrophic forgetting using context-.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TM37KY8X/1802.html:text/html},
}

@article{tishby_deep_2015,
	title = {Deep {Learning} and the {Information} {Bottleneck} {Principle}},
	url = {http://arxiv.org/abs/1503.02406},
	abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
	urldate = {2021-05-24},
	journal = {arXiv:1503.02406 [cs]},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02406},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QYBXY8QQ/Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/2QLMSVWY/1503.html:text/html},
}

@article{yu_gradient_2020,
	title = {Gradient {Surgery} for {Multi}-{Task} {Learning}},
	url = {http://arxiv.org/abs/2001.06782},
	abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multi-task architectures for enhanced performance.},
	urldate = {2021-05-24},
	journal = {arXiv:2001.06782 [cs, stat]},
	author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
	month = dec,
	year = {2020},
	note = {arXiv: 2001.06782},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4VN4Y8KW/Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/USP5AWLS/2001.html:text/html},
}

@article{dascoli_triple_2020,
	title = {Triple descent and the two kinds of overfitting: {Where} \& why do they appear?},
	shorttitle = {Triple descent and the two kinds of overfitting},
	url = {http://arxiv.org/abs/2006.03509},
	abstract = {A recent line of research has highlighted the existence of a "double descent" phenomenon in deep learning, whereby increasing the number of training examples \$N\$ causes the generalization error of neural networks to peak when \$N\$ is of the same order as the number of parameters \$P\$. In earlier works, a similar phenomenon was shown to exist in simpler models such as linear regression, where the peak instead occurs when \$N\$ is equal to the input dimension \$D\$. Since both peaks coincide with the interpolation threshold, they are often conflated in the litterature. In this paper, we show that despite their apparent similarity, these two scenarios are inherently different. In fact, both peaks can co-exist when neural networks are applied to noisy regression tasks. The relative size of the peaks is then governed by the degree of nonlinearity of the activation function. Building on recent developments in the analysis of random feature models, we provide a theoretical ground for this sample-wise triple descent. As shown previously, the nonlinear peak at \$N{\textbackslash}!={\textbackslash}!P\$ is a true divergence caused by the extreme sensitivity of the output function to both the noise corrupting the labels and the initialization of the random features (or the weights in neural networks). This peak survives in the absence of noise, but can be suppressed by regularization. In contrast, the linear peak at \$N{\textbackslash}!={\textbackslash}!D\$ is solely due to overfitting the noise in the labels, and forms earlier during training. We show that this peak is implicitly regularized by the nonlinearity, which is why it only becomes salient at high noise and is weakly affected by explicit regularization. Throughout the paper, we compare analytical results obtained in the random feature model with the outcomes of numerical experiments involving deep neural networks.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.03509 [cond-mat, stat]},
	author = {d'Ascoli, Stéphane and Sagun, Levent and Biroli, Giulio},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.03509},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FH2QV6RT/d'Ascoli et al. - 2020 - Triple descent and the two kinds of overfitting W.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FKPST5GM/2006.html:text/html},
}

@article{dhar_learning_2019,
	title = {Learning without {Memorizing}},
	url = {http://arxiv.org/abs/1811.08051},
	abstract = {Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incremental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called `Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (\$L\_\{AD\}\$), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding \$L\_\{AD\}\$ to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.},
	urldate = {2021-05-24},
	journal = {arXiv:1811.08051 [cs]},
	author = {Dhar, Prithviraj and Singh, Rajat Vikram and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
	month = apr,
	year = {2019},
	note = {arXiv: 1811.08051},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5M9MSWYJ/Dhar et al. - 2019 - Learning without Memorizing.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/573YANUA/1811.html:text/html},
}

@article{xu_open-world_2019,
	title = {Open-world {Learning} and {Application} to {Product} {Classification}},
	url = {http://arxiv.org/abs/1809.06004},
	doi = {10.1145/3308558.3313644},
	abstract = {Classic supervised learning makes the closed-world assumption, meaning that classes seen in testing must have been seen in training. However, in the dynamic world, new or unseen class examples may appear constantly. A model working in such an environment must be able to reject unseen classes (not seen or used in training). If enough data is collected for the unseen classes, the system should incrementally learn to accept/classify them. This learning paradigm is called open-world learning (OWL). Existing OWL methods all need some form of re-training to accept or include the new classes in the overall model. In this paper, we propose a meta-learning approach to the problem. Its key novelty is that it only needs to train a meta-classifier, which can then continually accept new classes when they have enough labeled data for the meta-classifier to use, and also detect/reject future unseen classes. No re-training of the meta-classifier or a new overall classifier covering all old and new classes is needed. In testing, the method only uses the examples of the seen classes (including the newly added classes) on-the-fly for classification and rejection. Experimental results demonstrate the effectiveness of the new approach.},
	urldate = {2021-05-24},
	journal = {The World Wide Web Conference on   - WWW '19},
	author = {Xu, Hu and Liu, Bing and Shu, Lei and Yu, P.},
	year = {2019},
	note = {arXiv: 1809.06004},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	pages = {3413--3419},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7ZNPFSMI/Xu et al. - 2019 - Open-world Learning and Application to Product Cla.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/G2CSB5XE/1809.html:text/html},
}

@article{hayes_lifelong_2020,
	title = {Lifelong {Machine} {Learning} with {Deep} {Streaming} {Linear} {Discriminant} {Analysis}},
	url = {http://arxiv.org/abs/1909.01520},
	abstract = {When an agent acquires new information, ideally it would immediately be capable of using that information to understand its environment. This is not possible using conventional deep neural networks, which suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. A variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, where a model learns from a series of large collections of labeled samples. However, in this setting, inference is only possible after a batch has been accumulated, which prohibits many applications. An alternative paradigm is online learning in a single pass through the training dataset on a resource constrained budget, which is known as streaming learning. Streaming learning has been much less studied in the deep learning community. In streaming learning, an agent learns instances one-by-one and can be tested at any time, rather than only after learning a large batch. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet ILSVRC-2012 and CORe50, a dataset that involves learning to classify from temporally ordered samples.},
	urldate = {2021-05-24},
	journal = {arXiv:1909.01520 [cs, stat]},
	author = {Hayes, Tyler L. and Kanan, Christopher},
	month = apr,
	year = {2020},
	note = {arXiv: 1909.01520},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3QIAPEB4/Hayes and Kanan - 2020 - Lifelong Machine Learning with Deep Streaming Line.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/V7UQVUW4/1909.html:text/html},
}

@article{zhang_survey_2021,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	url = {http://arxiv.org/abs/1707.08114},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	urldate = {2021-05-24},
	journal = {arXiv:1707.08114 [cs]},
	author = {Zhang, Yu and Yang, Qiang},
	month = mar,
	year = {2021},
	note = {arXiv: 1707.08114},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GA2T94GL/Zhang and Yang - 2021 - A Survey on Multi-Task Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3UV3KXY3/1707.html:text/html},
}

@article{hung_compacting_2019,
	title = {Compacting, {Picking} and {Growing} for {Unforgetting} {Continual} {Learning}},
	url = {http://arxiv.org/abs/1910.06562},
	abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
	urldate = {2021-05-24},
	journal = {arXiv:1910.06562 [cs, stat]},
	author = {Hung, Steven C. Y. and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.06562},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VQZJM5HI/Hung et al. - 2019 - Compacting, Picking and Growing for Unforgetting C.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IB5YZ9Q8/1910.html:text/html},
}

@article{parisi_continual_2019,
	title = {Continual {Lifelong} {Learning} with {Neural} {Networks}: {A} {Review}},
	volume = {113},
	issn = {08936080},
	shorttitle = {Continual {Lifelong} {Learning} with {Neural} {Networks}},
	url = {http://arxiv.org/abs/1802.07569},
	doi = {10.1016/j.neunet.2019.01.012.},
	abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
	urldate = {2021-05-24},
	journal = {Neural Networks},
	author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
	month = may,
	year = {2019},
	note = {arXiv: 1802.07569},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	pages = {54--71},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FY2WXHXF/Parisi et al. - 2019 - Continual Lifelong Learning with Neural Networks .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F3ZESW68/1802.html:text/html},
}

@article{parisi_lifelong_2018,
	title = {Lifelong {Learning} of {Spatiotemporal} {Representations} with {Dual}-{Memory} {Recurrent} {Self}-{Organization}},
	volume = {12},
	issn = {1662-5218},
	url = {http://arxiv.org/abs/1805.10966},
	doi = {10.3389/fnbot.2018.00078},
	abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios},
	urldate = {2021-05-24},
	journal = {Frontiers in Neurorobotics},
	author = {Parisi, German I. and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
	month = nov,
	year = {2018},
	note = {arXiv: 1805.10966},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {78},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RR7LL98B/Parisi et al. - 2018 - Lifelong Learning of Spatiotemporal Representation.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CZUE9V5T/1805.html:text/html},
}

@article{pennington_spectrum_2018,
	title = {The {Spectrum} of the {Fisher} {Information} {Matrix} of a {Single}-{Hidden}-{Layer} {Neural} {Network}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/18bb68e2b38e4a8ce7cf4f6b2625768c-Abstract.html},
	language = {en},
	urldate = {2021-05-24},
	journal = {Advances in Neural Information Processing Systems},
	author = {Pennington, Jeffrey and Worah, Pratik},
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/AV9G6C3L/Pennington and Worah - 2018 - The Spectrum of the Fisher Information Matrix of a.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/NBQPAY5F/18bb68e2b38e4a8ce7cf4f6b2625768c-Abstract.html:text/html},
}

@article{hospedales_meta-learning_2020,
	title = {Meta-{Learning} in {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Meta-{Learning} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2004.05439},
	abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
	urldate = {2021-05-24},
	journal = {arXiv:2004.05439 [cs, stat]},
	author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
	month = nov,
	year = {2020},
	note = {arXiv: 2004.05439},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/A4QUBMI7/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F5XXFBGL/2004.html:text/html},
}

@article{lesort_continual_2019,
	title = {Continual {Learning} for {Robotics}: {Definition}, {Framework}, {Learning} {Strategies}, {Opportunities} and {Challenges}},
	shorttitle = {Continual {Learning} for {Robotics}},
	url = {http://arxiv.org/abs/1907.00182},
	abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective changes through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge. Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier.},
	urldate = {2021-05-24},
	journal = {arXiv:1907.00182 [cs]},
	author = {Lesort, Timothée and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and Díaz-Rodríguez, Natalia},
	month = nov,
	year = {2019},
	note = {arXiv: 1907.00182},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/579NLZF2/Lesort et al. - 2019 - Continual Learning for Robotics Definition, Frame.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CYVEI49W/1907.html:text/html},
}

@article{lee_estimating_2020,
	title = {Estimating {Model} {Uncertainty} of {Neural} {Networks} in {Sparse} {Information} {Form}},
	url = {http://arxiv.org/abs/2006.11631},
	abstract = {We present a sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. The key insight of our work is that the information matrix, i.e. the inverse of the covariance matrix tends to be sparse in its spectrum. Therefore, dimensionality reduction techniques such as low rank approximations (LRA) can be effectively exploited. To achieve this, we develop a novel sparsification algorithm and derive a cost-effective analytical sampler. As a result, we show that the information form can be scalably applied to represent model uncertainty in DNNs. Our exhaustive theoretical analysis and empirical evaluations on various benchmarks show the competitiveness of our approach over the current methods.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.11631 [cs, stat]},
	author = {Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Triebel, Rudolph},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.11631},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4U9NLSQG/Lee et al. - 2020 - Estimating Model Uncertainty of Neural Networks in.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PD98W4C7/2006.html:text/html},
}

@article{jain_attention_2019,
	title = {Attention is not {Explanation}},
	url = {http://arxiv.org/abs/1902.10186},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
	urldate = {2021-05-24},
	journal = {arXiv:1902.10186 [cs]},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = may,
	year = {2019},
	note = {arXiv: 1902.10186},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/X3HQFQS8/Jain and Wallace - 2019 - Attention is not Explanation.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/A3YLARTX/1902.html:text/html},
}

@article{rodriguez_regularizing_2017,
	title = {Regularizing {CNNs} with {Locally} {Constrained} {Decorrelations}},
	url = {http://arxiv.org/abs/1611.01967},
	abstract = {Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.},
	urldate = {2021-05-24},
	journal = {arXiv:1611.01967 [cs]},
	author = {Rodríguez, Pau and Gonzàlez, Jordi and Cucurull, Guillem and Gonfaus, Josep M. and Roca, Xavier},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.01967},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VL93HQP2/Rodríguez et al. - 2017 - Regularizing CNNs with Locally Constrained Decorre.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CWVGJ8L6/1611.html:text/html},
}

@article{arpit_normalization_2016,
	title = {Normalization {Propagation}: {A} {Parametric} {Technique} for {Removing} {Internal} {Covariate} {Shift} in {Deep} {Networks}},
	shorttitle = {Normalization {Propagation}},
	url = {http://arxiv.org/abs/1603.01431},
	abstract = {While the authors of Batch Normalization (BN) identify and address an important problem involved in training deep networks-- Internal Covariate Shift-- the current solution has certain drawbacks. Specifically, BN depends on batch statistics for layerwise input normalization during training which makes the estimates of mean and standard deviation of input (distribution) to hidden layers inaccurate for validation due to shifting parameter values (especially during initial training epochs). Also, BN cannot be used with batch-size 1 during training. We address these drawbacks by proposing a non-adaptive normalization technique for removing internal covariate shift, that we call Normalization Propagation. Our approach does not depend on batch statistics, but rather uses a data-independent parametric estimate of mean and standard-deviation in every layer thus being computationally faster compared with BN. We exploit the observation that the pre-activation before Rectified Linear Units follow Gaussian distribution in deep networks, and that once the first and second order statistics of any given dataset are normalized, we can forward propagate this normalization without the need for recalculating the approximate statistics for hidden layers.},
	urldate = {2021-05-24},
	journal = {arXiv:1603.01431 [cs, stat]},
	author = {Arpit, Devansh and Zhou, Yingbo and Kota, Bhargava U. and Govindaraju, Venu},
	month = jul,
	year = {2016},
	note = {arXiv: 1603.01431},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZETA5M36/Arpit et al. - 2016 - Normalization Propagation A Parametric Technique .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TQHUHA36/1603.html:text/html},
}

@article{bansal_can_2018,
	title = {Can {We} {Gain} {More} from {Orthogonality} {Regularizations} in {Training} {Deep} {CNNs}?},
	url = {http://arxiv.org/abs/1810.09102},
	abstract = {This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available: https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality.},
	urldate = {2021-05-24},
	journal = {arXiv:1810.09102 [cs, stat]},
	author = {Bansal, Nitin and Chen, Xiaohan and Wang, Zhangyang},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.09102},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WI4TRL6U/Bansal et al. - 2018 - Can We Gain More from Orthogonality Regularization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SI8ADQGJ/1810.html:text/html},
}

@article{santurkar_how_2019,
	title = {How {Does} {Batch} {Normalization} {Help} {Optimization}?},
	url = {http://arxiv.org/abs/1805.11604},
	abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
	urldate = {2021-05-24},
	journal = {arXiv:1805.11604 [cs, stat]},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	month = apr,
	year = {2019},
	note = {arXiv: 1805.11604},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ND45WN9W/Santurkar et al. - 2019 - How Does Batch Normalization Help Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/M9J7C5QA/1805.html:text/html},
}

@article{zeno_task_2019,
	title = {Task {Agnostic} {Continual} {Learning} {Using} {Online} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1803.10123},
	abstract = {Catastrophic forgetting is the notorious vulnerability of neural networks to the change of the data distribution while learning. This phenomenon has long been considered a major obstacle for allowing the use of learning agents in realistic continual learning settings. A large body of continual learning research assumes that task boundaries are known during training. However, research for scenarios in which task boundaries are unknown during training has been lacking. In this paper we present, for the first time, a method for preventing catastrophic forgetting (BGD) for scenarios with task boundaries that are unknown during training --- task-agnostic continual learning. Code of our algorithm is available at https://github.com/igolan/bgd.},
	urldate = {2021-05-24},
	journal = {arXiv:1803.10123 [cs, stat]},
	author = {Zeno, Chen and Golan, Itay and Hoffer, Elad and Soudry, Daniel},
	month = feb,
	year = {2019},
	note = {arXiv: 1803.10123},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/22L6AXBL/Zeno et al. - 2019 - Task Agnostic Continual Learning Using Online Vari.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SFUCR5VN/1803.html:text/html},
}

@inproceedings{ritter_scalable_2018,
	title = {A {Scalable} {Laplace} {Approximation} for {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Skdvd2xAZ},
	abstract = {We construct a Kronecker factored Laplace approximation for neural networks that leads to an efficient matrix normal distribution over the weights.},
	language = {en},
	urldate = {2021-05-24},
	author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FSAQE8VP/Ritter et al. - 2018 - A Scalable Laplace Approximation for Neural Networ.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/TZ5LSD9K/forum.html:text/html},
}

@article{caccia_online_2020,
	title = {Online {Learned} {Continual} {Compression} with {Adaptive} {Quantization} {Modules}},
	url = {http://arxiv.org/abs/1911.08019},
	abstract = {We introduce and study the problem of Online Continual Compression, where one attempts to simultaneously learn to compress and store a representative dataset from a non i.i.d data stream, while only observing each sample once. A naive application of auto-encoders in this setting encounters a major challenge: representations derived from earlier encoder states must be usable by later decoder states. We show how to use discrete auto-encoders to effectively address this challenge and introduce Adaptive Quantization Modules (AQM) to control variation in the compression ability of the module at any given stage of learning. This enables selecting an appropriate compression for incoming samples, while taking into account overall memory constraints and current progress of the learned compression. Unlike previous methods, our approach does not require any pretraining, even on challenging datasets. We show that using AQM to replace standard episodic memory in continual learning settings leads to significant gains on continual learning benchmarks. Furthermore we demonstrate this approach with larger images, LiDAR, and reinforcement learning environments.},
	urldate = {2021-05-24},
	journal = {arXiv:1911.08019 [cs, stat]},
	author = {Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Pineau, Joelle},
	month = aug,
	year = {2020},
	note = {arXiv: 1911.08019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KSW22LGW/Caccia et al. - 2020 - Online Learned Continual Compression with Adaptive.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ZKHTS8AW/1911.html:text/html},
}

@article{lee_wide_2020,
	title = {Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear} {Models} {Under} {Gradient} {Descent}},
	volume = {2020},
	issn = {1742-5468},
	url = {http://arxiv.org/abs/1902.06720},
	doi = {10.1088/1742-5468/abc62b},
	abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
	number = {12},
	urldate = {2021-05-24},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	month = dec,
	year = {2020},
	note = {arXiv: 1902.06720},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {124002},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6GIXRLBG/Lee et al. - 2020 - Wide Neural Networks of Any Depth Evolve as Linear.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/8LF5EJI9/1902.html:text/html},
}

@article{farajtabar_orthogonal_2019,
	title = {Orthogonal {Gradient} {Descent} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1910.07104},
	abstract = {Neural networks are achieving state of the art and sometimes super-human performance on learning tasks across a variety of domains. Whenever these problems require learning in a continual or sequential manner, however, neural networks suffer from the problem of catastrophic forgetting; they forget how to solve previous tasks after being trained on a new task, despite having the essential capacity to solve both tasks if they were trained on both simultaneously. In this paper, we propose to address this issue from a parameter space perspective and study an approach to restrict the direction of the gradient updates to avoid forgetting previously-learned data. We present the Orthogonal Gradient Descent (OGD) method, which accomplishes this goal by projecting the gradients from new tasks onto a subspace in which the neural network output on previous task does not change and the projected gradient is still in a useful direction for learning the new task. Our approach utilizes the high capacity of a neural network more efficiently and does not require storing the previously learned data that might raise privacy concerns. Experiments on common benchmarks reveal the effectiveness of the proposed OGD method.},
	urldate = {2021-05-24},
	journal = {arXiv:1910.07104 [cs, stat]},
	author = {Farajtabar, Mehrdad and Azizan, Navid and Mott, Alex and Li, Ang},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.07104},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3N4UTGV3/Farajtabar et al. - 2019 - Orthogonal Gradient Descent for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CXCL8CT6/1910.html:text/html},
}

@article{doan_theoretical_2021,
	title = {A {Theoretical} {Analysis} of {Catastrophic} {Forgetting} through the {NTK} {Overlap} {Matrix}},
	url = {http://arxiv.org/abs/2010.04003},
	abstract = {Continual learning (CL) is a setting in which an agent has to learn from an incoming stream of data during its entire lifetime. Although major advances have been made in the field, one recurring problem which remains unsolved is that of Catastrophic Forgetting (CF). While the issue has been extensively studied empirically, little attention has been paid from a theoretical angle. In this paper, we show that the impact of CF increases as two tasks increasingly align. We introduce a measure of task similarity called the NTK overlap matrix which is at the core of CF. We analyze common projected gradient algorithms and demonstrate how they mitigate forgetting. Then, we propose a variant of Orthogonal Gradient Descent (OGD) which leverages structure of the data through Principal Component Analysis (PCA). Experiments support our theoretical findings and show how our method can help reduce CF on classical CL datasets.},
	urldate = {2021-05-24},
	journal = {arXiv:2010.04003 [cs, stat]},
	author = {Doan, Thang and Bennani, Mehdi and Mazoure, Bogdan and Rabusseau, Guillaume and Alquier, Pierre},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.04003},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TQD98H6P/Doan et al. - 2021 - A Theoretical Analysis of Catastrophic Forgetting .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FMKXI4Y9/2010.html:text/html},
}

@article{cardot_online_2015,
	title = {Online {Principal} {Component} {Analysis} in {High} {Dimension}: {Which} {Algorithm} to {Choose}?},
	shorttitle = {Online {Principal} {Component} {Analysis} in {High} {Dimension}},
	url = {http://arxiv.org/abs/1511.03688},
	abstract = {In the current context of data explosion, online techniques that do not require storing all data in memory are indispensable to routinely perform tasks like principal component analysis (PCA). Recursive algorithms that update the PCA with each new observation have been studied in various fields of research and found wide applications in industrial monitoring, computer vision, astronomy, and latent semantic indexing, among others. This work provides guidance for selecting an online PCA algorithm in practice. We present the main approaches to online PCA, namely, perturbation techniques, incremental methods, and stochastic optimization, and compare their statistical accuracy, computation time, and memory requirements using artificial and real data. Extensions to missing data and to functional data are discussed. All studied algorithms are available in the R package onlinePCA on CRAN.},
	urldate = {2021-05-24},
	journal = {arXiv:1511.03688 [cs, stat]},
	author = {Cardot, Hervé and Degras, David},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.03688},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/LY6J232V/Cardot and Degras - 2015 - Online Principal Component Analysis in High Dimens.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ZKT4DMAJ/1511.html:text/html},
}

@article{ding_generalization_2020,
	title = {Generalization {Guarantees} for {Sparse} {Kernel} {Approximation} with {Entropic} {Optimal} {Features}},
	url = {http://arxiv.org/abs/2002.04195},
	abstract = {Despite their success, kernel methods suffer from a massive computational cost in practice. In this paper, in lieu of commonly used kernel expansion with respect to \$N\$ inputs, we develop a novel optimal design maximizing the entropy among kernel features. This procedure results in a kernel expansion with respect to entropic optimal features (EOF), improving the data representation dramatically due to features dissimilarity. Under mild technical assumptions, our generalization bound shows that with only \$O(N{\textasciicircum}\{{\textbackslash}frac\{1\}\{4\}\})\$ features (disregarding logarithmic factors), we can achieve the optimal statistical accuracy (i.e., \$O(1/{\textbackslash}sqrt\{N\})\$). The salient feature of our design is its sparsity that significantly reduces the time and space cost. Our numerical experiments on benchmark datasets verify the superiority of EOF over the state-of-the-art in kernel approximation.},
	urldate = {2021-05-24},
	journal = {arXiv:2002.04195 [cs, stat]},
	author = {Ding, Liang and Tuo, Rui and Shahrampour, Shahin},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.04195},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/BFP3HDHX/Ding et al. - 2020 - Generalization Guarantees for Sparse Kernel Approx.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DGI4LCMU/2002.html:text/html},
}

@article{wenzel_how_2020,
	title = {How {Good} is the {Bayes} {Posterior} in {Deep} {Neural} {Networks} {Really}?},
	url = {http://arxiv.org/abs/2002.02405},
	abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
	urldate = {2021-05-24},
	journal = {arXiv:2002.02405 [cs, stat]},
	author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and Świątkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
	month = jul,
	year = {2020},
	note = {arXiv: 2002.02405},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9UEZYGEV/Wenzel et al. - 2020 - How Good is the Bayes Posterior in Deep Neural Net.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/Z4VAN6XN/2002.html:text/html},
}

@article{adams_bayesian_2007,
	title = {Bayesian {Online} {Changepoint} {Detection}},
	url = {http://arxiv.org/abs/0710.3742},
	abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
	urldate = {2021-05-24},
	journal = {arXiv:0710.3742 [stat]},
	author = {Adams, Ryan Prescott and MacKay, David J. C.},
	month = oct,
	year = {2007},
	note = {arXiv: 0710.3742},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UY8RWKJQ/Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/G8D54LYS/0710.html:text/html},
}

@article{lu_mean-field_2020,
	title = {A {Mean}-field {Analysis} of {Deep} {ResNet} and {Beyond}: {Towards} {Provable} {Optimization} {Via} {Overparameterization} {From} {Depth}},
	shorttitle = {A {Mean}-field {Analysis} of {Deep} {ResNet} and {Beyond}},
	url = {http://arxiv.org/abs/2003.05508},
	abstract = {Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a mean-field analysis of deep residual networks, based on a line of works that interpret the continuum limit of the deep residual network as an ordinary differential equation when the network capacity tends to infinity. Specifically, we propose a new continuum limit of deep residual networks, which enjoys a good landscape in the sense that every local minimizer is global. This characterization enables us to derive the first global convergence result for multilayer neural networks in the mean-field regime. Furthermore, without assuming the convexity of the loss landscape, our proof relies on a zero-loss assumption at the global minimizer that can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble, i.e. a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to apply existing mean-field analyses of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on the new continuous model, including one training procedure that switches the order of the residual blocks and results in strong empirical performance on the benchmark datasets.},
	urldate = {2021-05-24},
	journal = {arXiv:2003.05508 [cs, math, stat]},
	author = {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.05508},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4VT6YI8F/Lu et al. - 2020 - A Mean-field Analysis of Deep ResNet and Beyond T.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/5WCBT29S/2003.html:text/html},
}

@article{eynard_random_2018,
	title = {Random matrices},
	url = {http://arxiv.org/abs/1510.04430},
	abstract = {We provide a self-contained introduction to random matrices. While some applications are mentioned, our main emphasis is on three different approaches to random matrix models: the Coulomb gas method and its interpretation in terms of algebraic geometry, loop equations and their solution using topological recursion, orthogonal polynomials and their relation with integrable systems. Each approach provides its own definition of the spectral curve, a geometric object which encodes all the properties of a model. We also introduce the two peripheral subjects of counting polygonal surfaces, and computing angular integrals.},
	urldate = {2021-05-24},
	journal = {arXiv:1510.04430 [cond-mat, physics:hep-th, physics:math-ph]},
	author = {Eynard, Bertrand and Kimura, Taro and Ribault, Sylvain},
	month = jul,
	year = {2018},
	note = {arXiv: 1510.04430},
	keywords = {Condensed Matter - Statistical Mechanics, High Energy Physics - Theory, Mathematical Physics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RA2A8FV2/Eynard et al. - 2018 - Random matrices.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/MWPRVZ5K/1510.html:text/html},
}

@article{hayase_spectrum_2021,
	title = {The {Spectrum} of {Fisher} {Information} of {Deep} {Networks} {Achieving} {Dynamical} {Isometry}},
	url = {http://arxiv.org/abs/2006.07814},
	abstract = {The Fisher information matrix (FIM) is fundamental to understanding the trainability of deep neural nets (DNN), since it describes the parameter space's local metric. We investigate the spectral distribution of the conditional FIM, which is the FIM given a single sample, by focusing on fully-connected networks achieving dynamical isometry. Then, while dynamical isometry is known to keep specific backpropagated signals independent of the depth, we find that the parameter space's local metric linearly depends on the depth even under the dynamical isometry. More precisely, we reveal that the conditional FIM's spectrum concentrates around the maximum and the value grows linearly as the depth increases. To examine the spectrum, considering random initialization and the wide limit, we construct an algebraic methodology based on the free probability theory. As a byproduct, we provide an analysis of the solvable spectral distribution in two-hidden-layer cases. Lastly, experimental results verify that the appropriate learning rate for the online training of DNNs is in inverse proportional to depth, which is determined by the conditional FIM's spectrum.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.07814 [cs, math, stat]},
	author = {Hayase, Tomohiro and Karakida, Ryo},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.07814},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability, 68T07, 46L54, 60B20, 62E20, G.3},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/43DRMJWX/Hayase and Karakida - 2021 - The Spectrum of Fisher Information of Deep Network.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/AQR2S7W3/2006.html:text/html},
}

@article{kemker_measuring_2017,
	title = {Measuring {Catastrophic} {Forgetting} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1708.02072},
	abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem has yet to be solved.},
	urldate = {2021-05-24},
	journal = {arXiv:1708.02072 [cs]},
	author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler and Kanan, Christopher},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.02072},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/G3UIH5BV/Kemker et al. - 2017 - Measuring Catastrophic Forgetting in Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/9HL55MIS/1708.html:text/html},
}

@article{ball_elementary_nodate,
	title = {An {Elementary} {Introduction} to {Modern} {Convex} {Geometry}},
	language = {en},
	number = {1},
	author = {Ball, Keith},
	pages = {58},
	file = {Ball - An Elementary Introduction to Modern Convex Geomet.pdf:/Users/tokio/Zotero/storage/QRKRH33I/Ball - An Elementary Introduction to Modern Convex Geomet.pdf:application/pdf},
}

@article{achille_where_2020,
	title = {Where is the {Information} in a {Deep} {Neural} {Network}?},
	url = {http://arxiv.org/abs/1905.12213},
	abstract = {Whatever information a deep neural network has gleaned from training data is encoded in its weights. How this information affects the response of the network to future data remains largely an open question. Indeed, even defining and measuring information entails some subtleties, since a trained network is a deterministic map, so standard information measures can be degenerate. We measure information in a neural network via the optimal trade-off between accuracy of the response and complexity of the weights, measured by their coding length. Depending on the choice of code, the definition can reduce to standard measures such as Shannon Mutual Information and Fisher Information. However, the more general definition allows us to relate information to generalization and invariance, through a novel notion of effective information in the activations of a deep network. We establish a novel relation between the information in the weights and the effective information in the activations, and use this result to show that models with low (information) complexity not only generalize better, but are bound to learn invariant representations of future inputs. These relations hinge not only on the architecture of the model, but also on how it is trained, highlighting the complex inter-dependency between the class of functions implemented by deep neural networks, the loss function used for training them from finite data, and the inductive bias implicit in the optimization.},
	urldate = {2021-05-24},
	journal = {arXiv:1905.12213 [cs, math, stat]},
	author = {Achille, Alessandro and Paolini, Giovanni and Soatto, Stefano},
	month = jun,
	year = {2020},
	note = {arXiv: 1905.12213},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GJSH3ADB/Achille et al. - 2020 - Where is the Information in a Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BTF5JIT4/1905.html:text/html},
}

@article{nguyen_explaining_2020,
	title = {Explaining {How} {Deep} {Neural} {Networks} {Forget} by {Deep} {Visualization}},
	url = {http://arxiv.org/abs/2005.01004},
	abstract = {Explaining the behaviors of deep neural networks, usually considered as black boxes, is critical especially when they are now being adopted over diverse aspects of human life. Taking the advantages of interpretable machine learning (interpretable ML), this paper proposes a novel tool called Catastrophic Forgetting Dissector (or CFD) to explain catastrophic forgetting in continual learning settings. We also introduce a new method called Critical Freezing based on the observations of our tool. Experiments on ResNet articulate how catastrophic forgetting happens, particularly showing which components of this famous network are forgetting. Our new continual learning algorithm defeats various recent techniques by a significant margin, proving the capability of the investigation. Critical freezing not only attacks catastrophic forgetting but also exposes explainability.},
	urldate = {2021-05-24},
	journal = {arXiv:2005.01004 [cs]},
	author = {Nguyen, Giang and Chen, Shuan and Jun, Tae Joon and Kim, Daeyoung},
	month = oct,
	year = {2020},
	note = {arXiv: 2005.01004},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8Z8GGL6Q/Nguyen et al. - 2020 - Explaining How Deep Neural Networks Forget by Deep.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NLZJIHQD/2005.html:text/html},
}

@article{li_measuring_2018,
	title = {Measuring the {Intrinsic} {Dimension} of {Objective} {Landscapes}},
	url = {http://arxiv.org/abs/1804.08838},
	abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
	urldate = {2021-05-24},
	journal = {arXiv:1804.08838 [cs, stat]},
	author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08838},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZF5IC95G/Li et al. - 2018 - Measuring the Intrinsic Dimension of Objective Lan.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/2FCQKTIA/1804.html:text/html},
}

@article{nguyen_variational_2018,
	title = {Variational {Continual} {Learning}},
	url = {http://arxiv.org/abs/1710.10628},
	abstract = {This paper develops variational continual learning (VCL), a simple but general framework for continual learning that fuses online variational inference (VI) and recent advances in Monte Carlo VI for neural networks. The framework can successfully train both deep discriminative models and deep generative models in complex continual learning settings where existing tasks evolve over time and entirely new tasks emerge. Experimental results show that VCL outperforms state-of-the-art continual learning methods on a variety of tasks, avoiding catastrophic forgetting in a fully automatic way.},
	urldate = {2021-05-24},
	journal = {arXiv:1710.10628 [cs, stat]},
	author = {Nguyen, Cuong V. and Li, Yingzhen and Bui, Thang D. and Turner, Richard E.},
	month = may,
	year = {2018},
	note = {arXiv: 1710.10628},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MWJMZWR7/Nguyen et al. - 2018 - Variational Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/2L46H8EN/1710.html:text/html},
}

@article{kuroki_combinatorial_2020,
	title = {Combinatorial {Pure} {Exploration} with {Full}-bandit {Feedback} and {Beyond}: {Solving} {Combinatorial} {Optimization} under {Uncertainty} with {Limited} {Observation}},
	shorttitle = {Combinatorial {Pure} {Exploration} with {Full}-bandit {Feedback} and {Beyond}},
	url = {http://arxiv.org/abs/2012.15584},
	abstract = {Combinatorial optimization is one of the fundamental research fields that has been extensively studied in theoretical computer science and operations research. When developing an algorithm for combinatorial optimization, it is commonly assumed that parameters such as edge weights are exactly known as inputs. However, this assumption may not be fulfilled since input parameters are often uncertain or initially unknown in many applications such as recommender systems, crowdsourcing, communication networks, and online advertisement. To resolve such uncertainty, the problem of combinatorial pure exploration of multi-armed bandits (CPE) and its variants have recieved increasing attention. Earlier work on CPE has studied the semi-bandit feedback or assumed that the outcome from each individual edge is always accessible at all rounds. However, due to practical constraints such as a budget ceiling or privacy concern, such strong feedback is not always available in recent applications. In this article, we review recently proposed techniques for combinatorial pure exploration problems with limited feedback.},
	urldate = {2021-05-24},
	journal = {arXiv:2012.15584 [cs, stat]},
	author = {Kuroki, Yuko and Honda, Junya and Sugiyama, Masashi},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.15584},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HG64WY86/Kuroki et al. - 2020 - Combinatorial Pure Exploration with Full-bandit Fe.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/EV3LIZ99/2012.html:text/html},
}

@article{guedj_primer_2019,
	title = {A {Primer} on {PAC}-{Bayesian} {Learning}},
	url = {http://arxiv.org/abs/1901.05353},
	abstract = {Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.},
	urldate = {2021-05-24},
	journal = {arXiv:1901.05353 [cs, stat]},
	author = {Guedj, Benjamin},
	month = may,
	year = {2019},
	note = {arXiv: 1901.05353},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YSASP67A/Guedj - 2019 - A Primer on PAC-Bayesian Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UEABFTGT/1901.html:text/html},
}

@article{pfulb_comprehensive_2019,
	title = {A comprehensive, application-oriented study of catastrophic forgetting in {DNNs}},
	url = {http://arxiv.org/abs/1905.08101},
	abstract = {We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremental) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.},
	urldate = {2021-05-24},
	journal = {arXiv:1905.08101 [cs, stat]},
	author = {Pfülb, B. and Gepperth, A.},
	month = may,
	year = {2019},
	note = {arXiv: 1905.08101},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MG5TG39G/Pfülb and Gepperth - 2019 - A comprehensive, application-oriented study of cat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/R7ZMF89H/1905.html:text/html},
}

@article{paik_overcoming_2019,
	title = {Overcoming {Catastrophic} {Forgetting} by {Neuron}-level {Plasticity} {Control}},
	url = {http://arxiv.org/abs/1907.13322},
	abstract = {To address the issue of catastrophic forgetting in neural networks, we propose a novel, simple, and effective solution called neuron-level plasticity control (NPC). While learning a new task, the proposed method preserves the knowledge for the previous tasks by controlling the plasticity of the network at the neuron level. NPC estimates the importance value of each neuron and consolidates important {\textbackslash}textit\{neurons\} by applying lower learning rates, rather than restricting individual connection weights to stay close to certain values. The experimental results on the incremental MNIST (iMNIST) and incremental CIFAR100 (iCIFAR100) datasets show that neuron-level consolidation is substantially more effective compared to the connection-level consolidation approaches.},
	urldate = {2021-05-24},
	journal = {arXiv:1907.13322 [cs]},
	author = {Paik, Inyoung and Oh, Sangjun and Kwak, Tae-Yeong and Kim, Injung},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.13322},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/L36593LD/Paik et al. - 2019 - Overcoming Catastrophic Forgetting by Neuron-level.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/75SI7W66/1907.html:text/html},
}

@article{loo_generalized_2020,
	title = {Generalized {Variational} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2011.12328},
	abstract = {Continual learning deals with training models on new tasks and datasets in an online fashion. One strand of research has used probabilistic regularization for continual learning, with two of the main approaches in this vein being Online Elastic Weight Consolidation (Online EWC) and Variational Continual Learning (VCL). VCL employs variational inference, which in other settings has been improved empirically by applying likelihood-tempering. We show that applying this modification to VCL recovers Online EWC as a limiting case, allowing for interpolation between the two approaches. We term the general algorithm Generalized VCL (GVCL). In order to mitigate the observed overpruning effect of VI, we take inspiration from a common multi-task architecture, neural networks with task-specific FiLM layers, and find that this addition leads to significant performance gains, specifically for variational methods. In the small-data regime, GVCL strongly outperforms existing baselines. In larger datasets, GVCL with FiLM layers outperforms or is competitive with existing baselines in terms of accuracy, whilst also providing significantly better calibration.},
	urldate = {2021-05-24},
	journal = {arXiv:2011.12328 [cs, stat]},
	author = {Loo, Noel and Swaroop, Siddharth and Turner, Richard E.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.12328},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZBEJFWWN/Loo et al. - 2020 - Generalized Variational Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YAGIC2SI/2011.html:text/html},
}

@article{pan_continual_2021,
	title = {Continual {Deep} {Learning} by {Functional} {Regularisation} of {Memorable} {Past}},
	url = {http://arxiv.org/abs/2004.14070},
	abstract = {Continually learning new skills is important for intelligent systems, yet standard deep learning methods suffer from catastrophic forgetting of the past. Recent works address this with weight regularisation. Functional regularisation, although computationally expensive, is expected to perform better, but rarely does so in practice. In this paper, we fix this issue by using a new functional-regularisation approach that utilises a few memorable past examples crucial to avoid forgetting. By using a Gaussian Process formulation of deep networks, our approach enables training in weight-space while identifying both the memorable past and a functional prior. Our method achieves state-of-the-art performance on standard benchmarks and opens a new direction for life-long learning where regularisation and memory-based methods are naturally combined.},
	urldate = {2021-05-24},
	journal = {arXiv:2004.14070 [cs, stat]},
	author = {Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E. and Khan, Mohammad Emtiyaz},
	month = jan,
	year = {2021},
	note = {arXiv: 2004.14070},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3S8MIQX8/Pan et al. - 2021 - Continual Deep Learning by Functional Regularisati.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DGIRQJNH/2004.html:text/html},
}

@article{goldt_gaussian_2021,
	title = {The {Gaussian} equivalence of generative models for learning with shallow neural networks},
	url = {http://arxiv.org/abs/2006.14709},
	abstract = {Understanding the impact of data structure on the computational tractability of learning is a key challenge for the theory of neural networks. Many theoretical works do not explicitly model training data, or assume that inputs are drawn component-wise independently from some simple probability distribution. Here, we go beyond this simple paradigm by studying the performance of neural networks trained on data drawn from pre-trained generative models. This is possible due to a Gaussian equivalence stating that the key metrics of interest, such as the training and test errors, can be fully captured by an appropriately chosen Gaussian model. We provide three strands of rigorous, analytical and numerical evidence corroborating this equivalence. First, we establish rigorous conditions for the Gaussian equivalence to hold in the case of single-layer generative models, as well as deterministic rates for convergence in distribution. Second, we leverage this equivalence to derive a closed set of equations describing the generalisation performance of two widely studied machine learning problems: two-layer neural networks trained using one-pass stochastic gradient descent, and full-batch pre-learned features or kernel methods. Finally, we perform experiments demonstrating how our theory applies to deep, pre-trained generative models. These results open a viable path to the theoretical study of machine learning models with realistic data.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.14709 [cond-mat, stat]},
	author = {Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and Mézard, Marc and Zdeborová, Lenka},
	month = may,
	year = {2021},
	note = {arXiv: 2006.14709},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/THQETYJS/Goldt et al. - 2021 - The Gaussian equivalence of generative models for .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/G2FRTKR3/2006.html:text/html},
}

@article{benjamin_measuring_2019,
	title = {Measuring and regularizing networks in function space},
	url = {http://arxiv.org/abs/1805.08289},
	abstract = {To optimize a neural network one often thinks of optimizing its parameters, but it is ultimately a matter of optimizing the function that maps inputs to outputs. Since a change in the parameters might serve as a poor proxy for the change in the function, it is of some concern that primacy is given to parameters but that the correspondence has not been tested. Here, we show that it is simple and computationally feasible to calculate distances between functions in a \$L{\textasciicircum}2\$ Hilbert space. We examine how typical networks behave in this space, and compare how parameter \${\textbackslash}ell{\textasciicircum}2\$ distances compare to function \$L{\textasciicircum}2\$ distances between various points of an optimization trajectory. We find that the two distances are nontrivially related. In particular, the \$L{\textasciicircum}2/{\textbackslash}ell{\textasciicircum}2\$ ratio decreases throughout optimization, reaching a steady value around when test error plateaus. We then investigate how the \$L{\textasciicircum}2\$ distance could be applied directly to optimization. We first propose that in multitask learning, one can avoid catastrophic forgetting by directly limiting how much the input/output function changes between tasks. Secondly, we propose a new learning rule that constrains the distance a network can travel through \$L{\textasciicircum}2\$-space in any one update. This allows new examples to be learned in a way that minimally interferes with what has previously been learned. These applications demonstrate how one can measure and regularize function distances directly, without relying on parameters or local approximations like loss curvature.},
	urldate = {2021-05-24},
	journal = {arXiv:1805.08289 [cs, stat]},
	author = {Benjamin, Ari S. and Rolnick, David and Kording, Konrad},
	month = jun,
	year = {2019},
	note = {arXiv: 1805.08289},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GJJPFUKL/Benjamin et al. - 2019 - Measuring and regularizing networks in function sp.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/V8477ZU6/1805.html:text/html},
}

@article{aljundi_selfless_2019,
	title = {Selfless {Sequential} {Learning}},
	url = {http://arxiv.org/abs/1806.05421},
	abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a scenario with fixed model capacity, and postulate that the learning process should not be selfish, i.e. it should account for future tasks to be added and thus leave enough capacity for them. To achieve Selfless Sequential Learning we study different regularization strategies and activation functions. We find that imposing sparsity at the level of the representation (i.e.{\textasciitilde}neuron activations) is more beneficial for sequential learning than encouraging parameter sparsity. In particular, we propose a novel regularizer, that encourages representation sparsity by means of neural inhibition. It results in few active neurons which in turn leaves more free neurons to be utilized by upcoming tasks. As neural inhibition over an entire layer can be too drastic, especially for complex tasks requiring strong representations, our regularizer only inhibits other neurons in a local neighbourhood, inspired by lateral inhibition processes in the brain. We combine our novel regularizer, with state-of-the-art lifelong learning methods that penalize changes to important previously learned parts of the network. We show that our new regularizer leads to increased sparsity which translates in consistent performance improvement \%over alternative regularizers we studied on diverse datasets.},
	urldate = {2021-05-24},
	journal = {arXiv:1806.05421 [cs, stat]},
	author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.05421},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UIWGIGXE/Aljundi et al. - 2019 - Selfless Sequential Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/A6D353AJ/1806.html:text/html},
}

@article{immer_improving_2021,
	title = {Improving predictions of {Bayesian} neural nets via local linearization},
	url = {http://arxiv.org/abs/2008.08400},
	abstract = {The generalized Gauss-Newton (GGN) approximation is often used to make practical Bayesian deep learning approaches scalable by replacing a second order derivative with a product of first order derivatives. In this paper we argue that the GGN approximation should be understood as a local linearization of the underlying Bayesian neural network (BNN), which turns the BNN into a generalized linear model (GLM). Because we use this linearized model for posterior inference, we should also predict using this modified model instead of the original one. We refer to this modified predictive as "GLM predictive" and show that it effectively resolves common underfitting problems of the Laplace approximation. It extends previous results in this vein to general likelihoods and has an equivalent Gaussian process formulation, which enables alternative inference schemes for BNNs in function space. We demonstrate the effectiveness of our approach on several standard classification datasets as well as on out-of-distribution detection. We provide an implementation at https://github.com/AlexImmer/BNN-predictions.},
	urldate = {2021-05-24},
	journal = {arXiv:2008.08400 [cs, stat]},
	author = {Immer, Alexander and Korzepa, Maciej and Bauer, Matthias},
	month = feb,
	year = {2021},
	note = {arXiv: 2008.08400},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/THRUYAJB/Immer et al. - 2021 - Improving predictions of Bayesian neural nets via .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YPEZGB69/2008.html:text/html},
}

@article{schwarz_progress_2018,
	title = {Progress \& {Compress}: {A} scalable framework for continual learning},
	shorttitle = {Progress \& {Compress}},
	url = {http://arxiv.org/abs/1805.06370},
	abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to preserve performance on previously encountered tasks while accelerating learning progress on subsequent problems. This is achieved by training a network with two components: A knowledge base, capable of solving previously encountered problems, which is connected to an active column that is employed to efficiently learn the current task. After learning a new task, the active column is distilled into the knowledge base, taking care to protect any previously acquired skills. This cycle of active learning (progression) followed by consolidation (compression) requires no architecture growth, no access to or storing of previous data or tasks, and no task-specific parameters. We demonstrate the progress \& compress approach on sequential classification of handwritten alphabets as well as two reinforcement learning domains: Atari games and 3D maze navigation.},
	urldate = {2021-05-24},
	journal = {arXiv:1805.06370 [cs, stat]},
	author = {Schwarz, Jonathan and Luketina, Jelena and Czarnecki, Wojciech M. and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	month = jul,
	year = {2018},
	note = {arXiv: 1805.06370},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/Z8J5HE82/Schwarz et al. - 2018 - Progress & Compress A scalable framework for cont.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DL7LQAGR/1805.html:text/html},
}

@article{ebrahimi_uncertainty-guided_2020,
	title = {Uncertainty-guided {Continual} {Learning} with {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.02425},
	abstract = {Continual learning aims to learn new tasks without forgetting previously learned ones. This is especially challenging when one cannot access data from previous tasks and when the model has a fixed capacity. Current regularization-based continual learning algorithms need an external representation and extra computation to measure the parameters' {\textbackslash}textit\{importance\}. In contrast, we propose Uncertainty-guided Continual Bayesian Neural Networks (UCB), where the learning rate adapts according to the uncertainty defined in the probability distribution of the weights in networks. Uncertainty is a natural way to identify {\textbackslash}textit\{what to remember\} and {\textbackslash}textit\{what to change\} as we continually learn, and thus mitigate catastrophic forgetting. We also show a variant of our model, which uses uncertainty for weight pruning and retains task performance after pruning by saving binary masks per tasks. We evaluate our UCB approach extensively on diverse object classification datasets with short and long sequences of tasks and report superior or on-par performance compared to existing approaches. Additionally, we show that our model does not necessarily need task information at test time, i.e. it does not presume knowledge of which task a sample belongs to.},
	urldate = {2021-05-24},
	journal = {arXiv:1906.02425 [cs, stat]},
	author = {Ebrahimi, Sayna and Elhoseiny, Mohamed and Darrell, Trevor and Rohrbach, Marcus},
	month = feb,
	year = {2020},
	note = {arXiv: 1906.02425},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FSL4SHB2/Ebrahimi et al. - 2020 - Uncertainty-guided Continual Learning with Bayesia.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/J3R6NUWY/1906.html:text/html},
}

@article{jospin_hands-bayesian_2020,
	title = {Hands-on {Bayesian} {Neural} {Networks} -- a {Tutorial} for {Deep} {Learning} {Users}},
	url = {http://arxiv.org/abs/2007.06823},
	abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks.},
	urldate = {2021-05-24},
	journal = {arXiv:2007.06823 [cs, stat]},
	author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.06823},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, G.3, 62-02 (Primary), I.2.6},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MQDVG6EQ/Jospin et al. - 2020 - Hands-on Bayesian Neural Networks -- a Tutorial fo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ZZSSSFIR/2007.html:text/html},
}

@article{lee_neural_2020,
	title = {A {Neural} {Dirichlet} {Process} {Mixture} {Model} for {Task}-{Free} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2001.00689},
	abstract = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
	urldate = {2021-05-24},
	journal = {arXiv:2001.00689 [cs, stat]},
	author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.00689},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/PUD3LUJJ/Lee et al. - 2020 - A Neural Dirichlet Process Mixture Model for Task-.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/VIRAZA52/2001.html:text/html},
}

@article{kapoor_variational_2021,
	title = {Variational {Auto}-{Regressive} {Gaussian} {Processes} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2006.05468},
	abstract = {Through sequential construction of posteriors on observing data online, Bayes' theorem provides a natural framework for continual learning. We develop Variational Auto-Regressive Gaussian Processes (VAR-GPs), a principled posterior updating mechanism to solve sequential tasks in continual learning. By relying on sparse inducing point approximations for scalable posteriors, we propose a novel auto-regressive variational distribution which reveals two fruitful connections to existing results in Bayesian inference, expectation propagation and orthogonal inducing points. Mean predictive entropy estimates show VAR-GPs prevent catastrophic forgetting, which is empirically supported by strong performance on modern continual learning benchmarks against competitive baselines. A thorough ablation study demonstrates the efficacy of our modeling choices.},
	urldate = {2021-11-02},
	journal = {arXiv:2006.05468 [cs, stat]},
	author = {Kapoor, Sanyam and Karaletsos, Theofanis and Bui, Thang D.},
	month = jun,
	year = {2021},
	note = {arXiv: 2006.05468},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/U5JYTF68/Kapoor et al. - 2021 - Variational Auto-Regressive Gaussian Processes for.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/INLEZLNP/2006.html:text/html},
}

@article{derakhshani_kernel_2021,
	title = {Kernel {Continual} {Learning}},
	url = {http://arxiv.org/abs/2107.05757},
	abstract = {This paper introduces kernel continual learning, a simple but effective variant of continual learning that leverages the non-parametric nature of kernel methods to tackle catastrophic forgetting. We deploy an episodic memory unit that stores a subset of samples for each task to learn task-specific classifiers based on kernel ridge regression. This does not require memory replay and systematically avoids task interference in the classifiers. We further introduce variational random features to learn a data-driven kernel for each task. To do so, we formulate kernel continual learning as a variational inference problem, where a random Fourier basis is incorporated as the latent variable. The variational posterior distribution over the random Fourier basis is inferred from the coreset of each task. In this way, we are able to generate more informative kernels specific to each task, and, more importantly, the coreset size can be reduced to achieve more compact memory, resulting in more efficient continual learning based on episodic memory. Extensive evaluation on four benchmarks demonstrates the effectiveness and promise of kernels for continual learning.},
	urldate = {2021-11-02},
	journal = {arXiv:2107.05757 [cs]},
	author = {Derakhshani, Mohammad Mahdi and Zhen, Xiantong and Shao, Ling and Snoek, Cees G. M.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.05757},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NFZ342M8/Derakhshani et al. - 2021 - Kernel Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NX558NAP/2107.html:text/html},
}

@article{dedeoglu_continual_2021,
	title = {Continual {Learning} of {Generative} {Models} with {Limited} {Data}: {From} {Wasserstein}-1 {Barycenter} to {Adaptive} {Coalescence}},
	shorttitle = {Continual {Learning} of {Generative} {Models} with {Limited} {Data}},
	url = {http://arxiv.org/abs/2101.09225},
	abstract = {Learning generative models is challenging for a network edge node with limited data and computing power. Since tasks in similar environments share model similarity, it is plausible to leverage pre-trained generative models from the cloud or other edge nodes. Appealing to optimal transport theory tailored towards Wasserstein-1 generative adversarial networks (WGAN), this study aims to develop a framework which systematically optimizes continual learning of generative models using local data at the edge node while exploiting adaptive coalescence of pre-trained generative models. Specifically, by treating the knowledge transfer from other nodes as Wasserstein balls centered around their pre-trained models, continual learning of generative models is cast as a constrained optimization problem, which is further reduced to a Wasserstein-1 barycenter problem. A two-stage approach is devised accordingly: 1) The barycenters among the pre-trained models are computed offline, where displacement interpolation is used as the theoretic foundation for finding adaptive barycenters via a "recursive" WGAN configuration; 2) the barycenter computed offline is used as meta-model initialization for continual learning and then fast adaptation is carried out to find the generative model using the local samples at the target edge node. Finally, a weight ternarization method, based on joint optimization of weights and threshold for quantization, is developed to compress the generative model further.},
	urldate = {2021-10-28},
	journal = {arXiv:2101.09225 [cs, eess]},
	author = {Dedeoglu, Mehmet and Lin, Sen and Zhang, Zhaofeng and Zhang, Junshan},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.09225},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SWVIB34U/Dedeoglu et al. - 2021 - Continual Learning of Generative Models with Limit.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SKKBN3VL/2101.html:text/html},
}

@article{li_continuous_2020,
	title = {Continuous {Regularized} {Wasserstein} {Barycenters}},
	url = {http://arxiv.org/abs/2008.12534},
	abstract = {Wasserstein barycenters provide a geometrically meaningful way to aggregate probability distributions, built on the theory of optimal transport. They are difficult to compute in practice, however, leading previous work to restrict their supports to finite sets of points. Leveraging a new dual formulation for the regularized Wasserstein barycenter problem, we introduce a stochastic algorithm that constructs a continuous approximation of the barycenter. We establish strong duality and use the corresponding primal-dual relationship to parametrize the barycenter implicitly using the dual potentials of regularized transport problems. The resulting problem can be solved with stochastic gradient descent, which yields an efficient online algorithm to approximate the barycenter of continuous distributions given sample access. We demonstrate the effectiveness of our approach and compare against previous work on synthetic examples and real-world applications.},
	urldate = {2021-10-28},
	journal = {arXiv:2008.12534 [cs, stat]},
	author = {Li, Lingxiao and Genevay, Aude and Yurochkin, Mikhail and Solomon, Justin},
	month = oct,
	year = {2020},
	note = {arXiv: 2008.12534},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8YBGQ35P/Li et al. - 2020 - Continuous Regularized Wasserstein Barycenters.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FELJ7QXZ/2008.html:text/html},
}

@article{yaida_non-gaussian_2020,
	title = {Non-{Gaussian} processes and neural networks at finite widths},
	url = {http://arxiv.org/abs/1910.00019},
	abstract = {Gaussian processes are ubiquitous in nature and engineering. A case in point is a class of neural networks in the infinite-width limit, whose priors correspond to Gaussian processes. Here we perturbatively extend this correspondence to finite-width neural networks, yielding non-Gaussian processes as priors. The methodology developed herein allows us to track the flow of preactivation distributions by progressively integrating out random variables from lower to higher layers, reminiscent of renormalization-group flow. We further develop a perturbative procedure to perform Bayesian inference with weakly non-Gaussian priors.},
	urldate = {2021-10-26},
	journal = {arXiv:1910.00019 [cond-mat, physics:hep-th, stat]},
	author = {Yaida, Sho},
	month = aug,
	year = {2020},
	note = {arXiv: 1910.00019},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, High Energy Physics - Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/K5F8F6HE/Yaida - 2020 - Non-Gaussian processes and neural networks at fini.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CSNR5SYY/1910.html:text/html},
}

@article{phillips_coresets_2016,
	title = {Coresets and {Sketches}},
	url = {http://arxiv.org/abs/1601.00617},
	abstract = {Geometric data summarization has become an essential tool in both geometric approximation algorithms and where geometry intersects with big data problems. In linear or near-linear time large data sets can be compressed into a summary, and then more intricate algorithms can be run on the summaries whose results approximate those of the full data set. Coresets and sketches are the two most important classes of these summaries. We survey five types of coresets and sketches: shape-fitting, density estimation, high-dimensional vectors, high-dimensional point sets / matrices, and clustering.},
	urldate = {2021-10-22},
	journal = {arXiv:1601.00617 [cs]},
	author = {Phillips, Jeff M.},
	month = jun,
	year = {2016},
	note = {arXiv: 1601.00617},
	keywords = {Computer Science - Computational Geometry},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8LZFLWS6/Phillips - 2016 - Coresets and Sketches.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QZC3YXE9/1601.html:text/html},
}

@article{chen_single-timescale_2021,
	title = {A {Single}-{Timescale} {Stochastic} {Bilevel} {Optimization} {Method}},
	url = {http://arxiv.org/abs/2102.04671},
	abstract = {Stochastic bilevel optimization generalizes the classic stochastic optimization from the minimization of a single objective to the minimization of an objective function that depends the solution of another optimization problem. Recently, stochastic bilevel optimization is regaining popularity in emerging machine learning applications such as hyper-parameter optimization and model-agnostic meta learning. To solve this class of stochastic optimization problems, existing methods require either double-loop or two-timescale updates, which are sometimes less efficient. This paper develops a new optimization method for a class of stochastic bilevel problems that we term Single-Timescale stochAstic BiLevEl optimization (STABLE) method. STABLE runs in a single loop fashion, and uses a single-timescale update with a fixed batch size. To achieve an \${\textbackslash}epsilon\$-stationary point of the bilevel problem, STABLE requires \$\{{\textbackslash}cal O\}({\textbackslash}epsilon{\textasciicircum}\{-2\})\$ samples in total; and to achieve an \${\textbackslash}epsilon\$-optimal solution in the strongly convex case, STABLE requires \$\{{\textbackslash}cal O\}({\textbackslash}epsilon{\textasciicircum}\{-1\})\$ samples. To the best of our knowledge, this is the first bilevel optimization algorithm achieving the same order of sample complexity as the stochastic gradient descent method for the single-level stochastic optimization.},
	urldate = {2021-10-22},
	journal = {arXiv:2102.04671 [cs, math, stat]},
	author = {Chen, Tianyi and Sun, Yuejiao and Yin, Wotao},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.04671},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7IKZB8IR/Chen et al. - 2021 - A Single-Timescale Stochastic Bilevel Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/EMGZQRAQ/2102.html:text/html},
}

@article{bang_rainbow_2021,
	title = {Rainbow {Memory}: {Continual} {Learning} with a {Memory} of {Diverse} {Samples}},
	shorttitle = {Rainbow {Memory}},
	url = {http://arxiv.org/abs/2103.17230},
	abstract = {Continual learning is a realistic learning scenario for AI models. Prevalent scenario of continual learning, however, assumes disjoint sets of classes as tasks and is less realistic rather artificial. Instead, we focus on 'blurry' task boundary; where tasks shares classes and is more realistic and practical. To address such task, we argue the importance of diversity of samples in an episodic memory. To enhance the sample diversity in the memory, we propose a novel memory management strategy based on per-sample classification uncertainty and data augmentation, named Rainbow Memory (RM). With extensive empirical validations on MNIST, CIFAR10, CIFAR100, and ImageNet datasets, we show that the proposed method significantly improves the accuracy in blurry continual learning setups, outperforming state of the arts by large margins despite its simplicity. Code and data splits will be available in https://github.com/clovaai/rainbow-memory.},
	urldate = {2021-10-22},
	journal = {arXiv:2103.17230 [cs]},
	author = {Bang, Jihwan and Kim, Heesu and Yoo, YoungJoon and Ha, Jung-Woo and Choi, Jonghyun},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.17230},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TQM7GKF9/Bang et al. - 2021 - Rainbow Memory Continual Learning with a Memory o.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/AQHRGWGE/2103.html:text/html},
}

@inproceedings{huang_novel_2021,
	title = {A {Novel} {Sequential} {Coreset} {Method} for {Gradient} {Descent} {Algorithms}},
	url = {https://proceedings.mlr.press/v139/huang21b.html},
	abstract = {A wide range of optimization problems arising in machine learning can be solved by gradient descent algorithms, and a central question in this area is how to efficiently compress a large-scale dataset so as to reduce the computational complexity. Coreset is a popular data compression technique that has been extensively studied before. However, most of existing coreset methods are problem-dependent and cannot be used as a general tool for a broader range of applications. A key obstacle is that they often rely on the pseudo-dimension and total sensitivity bound that can be very high or hard to obtain. In this paper, based on the “locality” property of gradient descent algorithms, we propose a new framework, termed “sequential coreset”, which effectively avoids these obstacles. Moreover, our method is particularly suitable for sparse optimization whence the coreset size can be further reduced to be only poly-logarithmically dependent on the dimension. In practice, the experimental results suggest that our method can save a large amount of running time compared with the baseline algorithms.},
	language = {en},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Jiawei and Huang, Ruomin and Liu, Wenjie and Freris, Nikolaos and Ding, Hu},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4412--4422},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KVNWHL5R/Huang et al. - 2021 - A Novel Sequential Coreset Method for Gradient Des.pdf:application/pdf},
}

@article{isele_selective_2018,
	title = {Selective {Experience} {Replay} for {Lifelong} {Learning}},
	url = {http://arxiv.org/abs/1802.10269},
	abstract = {Deep reinforcement learning has emerged as a powerful tool for a variety of learning tasks, however deep nets typically exhibit forgetting when learning multiple tasks in sequence. To mitigate forgetting, we propose an experience replay process that augments the standard FIFO buffer and selectively stores experiences in a long-term memory. We explore four strategies for selecting which experiences will be stored: favoring surprise, favoring reward, matching the global training distribution, and maximizing coverage of the state space. We show that distribution matching successfully prevents catastrophic forgetting, and is consistently the best approach on all domains tested. While distribution matching has better and more consistent performance, we identify one case in which coverage maximization is beneficial - when tasks that receive less trained are more important. Overall, our results show that selective experience replay, when suitable selection algorithms are employed, can prevent catastrophic forgetting.},
	urldate = {2021-10-19},
	journal = {arXiv:1802.10269 [cs]},
	author = {Isele, David and Cosgun, Akansel},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.10269},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HZ69ES7M/Isele and Cosgun - 2018 - Selective Experience Replay for Lifelong Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ZN2SJWNM/1802.html:text/html},
}

@article{schaul_prioritized_2016,
	title = {Prioritized {Experience} {Replay}},
	url = {http://arxiv.org/abs/1511.05952},
	abstract = {Experience replay lets online reinforcement learning agents remember and reuse experiences from the past. In prior work, experience transitions were uniformly sampled from a replay memory. However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance. In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently. We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
	urldate = {2021-10-19},
	journal = {arXiv:1511.05952 [cs]},
	author = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
	month = feb,
	year = {2016},
	note = {arXiv: 1511.05952},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8SVKVRMV/Schaul et al. - 2016 - Prioritized Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/5BCMRU4K/1511.html:text/html},
}

@article{kaplanis_continual_2018,
	title = {Continual {Reinforcement} {Learning} with {Complex} {Synapses}},
	url = {http://arxiv.org/abs/1802.07239},
	abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \& Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
	urldate = {2021-10-19},
	journal = {arXiv:1802.07239 [cs]},
	author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.07239},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/222CZBTT/Kaplanis et al. - 2018 - Continual Reinforcement Learning with Complex Syna.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TREMNTHI/1802.html:text/html},
}

@inproceedings{milan_forget-me-not_2016,
	title = {The {Forget}-me-not {Process}},
	volume = {29},
	url = {https://papers.nips.cc/paper/2016/hash/f26dab9bf6a137c3b6782e562794c2f2-Abstract.html},
	urldate = {2021-10-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Milan, Kieran and Veness, Joel and Kirkpatrick, James and Bowling, Michael and Koop, Anna and Hassabis, Demis},
	year = {2016},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/HKZMKWK9/Milan et al. - 2016 - The Forget-me-not Process.pdf:application/pdf},
}

@article{blakeman_complementary_2019,
	title = {A {Complementary} {Learning} {Systems} {Approach} to {Temporal} {Difference} {Learning}},
	url = {http://arxiv.org/abs/1905.02636},
	abstract = {Complementary Learning Systems (CLS) theory suggests that the brain uses a 'neocortical' and a 'hippocampal' learning system to achieve complex behavior. These two systems are complementary in that the 'neocortical' system relies on slow learning of distributed representations while the 'hippocampal' system relies on fast learning of pattern-separated representations. Both of these systems project to the striatum, which is a key neural structure in the brain's implementation of Reinforcement Learning (RL). Current deep RL approaches share similarities with a 'neocortical' system because they slowly learn distributed representations through backpropagation in Deep Neural Networks (DNNs). An ongoing criticism of such approaches is that they are data inefficient and lack flexibility. CLS theory suggests that the addition of a 'hippocampal' system could address these criticisms. In the present study we propose a novel algorithm known as Complementary Temporal Difference Learning (CTDL), which combines a DNN with a Self-Organising Map (SOM) to obtain the benefits of both a 'neocortical' and a 'hippocampal' system. Key features of CTDL include the use of Temporal Difference (TD) error to update a SOM and the combination of a SOM and DNN to calculate action values. We evaluate CTDL on grid worlds and the Cart-Pole environment, and show several benefits over the classic Deep Q-Network (DQN) approach. These results demonstrate (1) the utility of complementary learning systems for the evaluation of actions, (2) that the TD error signal is a useful form of communication between the two systems and (3) the biological plausibility of the proposed approach.},
	urldate = {2021-10-19},
	journal = {arXiv:1905.02636 [cs, q-bio]},
	author = {Blakeman, Sam and Mareschal, Denis},
	month = may,
	year = {2019},
	note = {arXiv: 1905.02636},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FP9YTGB5/Blakeman and Mareschal - 2019 - A Complementary Learning Systems Approach to Tempo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3W3EL9L7/1905.html:text/html},
}

@article{chizat_convergence_2021,
	title = {Convergence {Rates} of {Gradient} {Methods} for {Convex} {Optimization} in the {Space} of {Measures}},
	url = {http://arxiv.org/abs/2105.08368},
	abstract = {We study the convergence rate of Bregman gradient methods for convex optimization in the space of measures on a d-dimensional manifold. Under basic regularity assumptions, we show that the suboptimality gap at iteration k is in O(log(k)k{\textasciicircum}\{--1\}) for multiplicative updates, while it is in O(k{\textasciicircum}\{--q/(d+q)\}) for additive updates for some q = \{1, 2, 4\} determined by the structure of the objective function. Our flexible proof strategy, based on approximation arguments, allows to painlessly cover all Bregman Proximal Gradient Methods (PGM) and their acceleration (APGM) under various geometries such as the hyperbolic entropy and L{\textasciicircum}p divergences. We also prove the tightness of our analysis with matching lower bounds and confirm the theoretical results with numerical experiments on low dimensional problems. Note that all these optimization methods must additionally pay the computational cost of discretization, which can be exponential in d.},
	urldate = {2021-10-17},
	journal = {arXiv:2105.08368 [math]},
	author = {Chizat, Lénaïc},
	month = may,
	year = {2021},
	note = {arXiv: 2105.08368},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/LM29KGML/Chizat - 2021 - Convergence Rates of Gradient Methods for Convex O.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/B3PTXITK/2105.html:text/html},
}

@inproceedings{evans_coordinated_2019,
	title = {Coordinated hippocampal-entorhinal replay as structural inference},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/aa68c75c4a77c87f97fb686b2f068676-Abstract.html},
	urldate = {2021-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Evans, Talfan and Burgess, Neil},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/E6QMCJQJ/Evans and Burgess - 2019 - Coordinated hippocampal-entorhinal replay as struc.pdf:application/pdf},
}

@article{eysenbach_search_2019,
	title = {Search on the {Replay} {Buffer}: {Bridging} {Planning} and {Reinforcement} {Learning}},
	shorttitle = {Search on the {Replay} {Buffer}},
	url = {http://arxiv.org/abs/1906.05253},
	abstract = {The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.},
	urldate = {2021-10-17},
	journal = {arXiv:1906.05253 [cs]},
	author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.05253},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/JNRA9LFP/Eysenbach et al. - 2019 - Search on the Replay Buffer Bridging Planning and.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/JJ3ZJZ3Z/1906.html:text/html},
}

@inproceedings{fang_curriculum-guided_2019,
	title = {Curriculum-guided {Hindsight} {Experience} {Replay}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/83715fd4755b33f9c3958e1a9ee221e1-Abstract.html},
	urldate = {2021-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fang, Meng and Zhou, Tianyi and Du, Yali and Han, Lei and Zhang, Zhengyou},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZVEXPG9G/Fang et al. - 2019 - Curriculum-guided Hindsight Experience Replay.pdf:application/pdf},
}

@article{daley_reconciling_2020,
	title = {Reconciling \${\textbackslash}lambda\$-{Returns} with {Experience} {Replay}},
	url = {http://arxiv.org/abs/1810.09967},
	abstract = {Modern deep reinforcement learning methods have departed from the incremental learning required for eligibility traces, rendering the implementation of the \${\textbackslash}lambda\$-return difficult in this context. In particular, off-policy methods that utilize experience replay remain problematic because their random sampling of minibatches is not conducive to the efficient calculation of \${\textbackslash}lambda\$-returns. Yet replay-based methods are often the most sample efficient, and incorporating \${\textbackslash}lambda\$-returns into them is a viable way to achieve new state-of-the-art performance. Towards this, we propose the first method to enable practical use of \${\textbackslash}lambda\$-returns in arbitrary replay-based methods without relying on other forms of decorrelation such as asynchronous gradient updates. By promoting short sequences of past transitions into a small cache within the replay memory, adjacent \${\textbackslash}lambda\$-returns can be efficiently precomputed by sharing Q-values. Computation is not wasted on experiences that are never sampled, and stored \${\textbackslash}lambda\$-returns behave as stable temporal-difference (TD) targets that replace the target network. Additionally, our method grants the unique ability to observe TD errors prior to sampling; for the first time, transitions can be prioritized by their true significance rather than by a proxy to it. Furthermore, we propose the novel use of the TD error to dynamically select \${\textbackslash}lambda\$-values that facilitate faster learning. We show that these innovations can enhance the performance of DQN when playing Atari 2600 games, even under partial observability. While our work specifically focuses on \${\textbackslash}lambda\$-returns, these ideas are applicable to any multi-step return estimator.},
	urldate = {2021-10-17},
	journal = {arXiv:1810.09967 [cs, stat]},
	author = {Daley, Brett and Amato, Christopher},
	month = jan,
	year = {2020},
	note = {arXiv: 1810.09967},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VJVHLZXB/Daley and Amato - 2020 - Reconciling \$lambda\$-Returns with Experience Repl.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/L67XZ2Z2/1810.html:text/html},
}

@inproceedings{rajasegaran_random_2019,
	title = {Random {Path} {Selection} for {Continual} {Learning}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/83da7c539e1ab4e759623c38d8737e9e-Abstract.html},
	urldate = {2021-10-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rajasegaran, Jathushan and Hayat, Munawar and Khan, Salman H and Khan, Fahad Shahbaz and Shao, Ling},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/N3NHIHWI/Rajasegaran et al. - 2019 - Random Path Selection for Continual Learning.pdf:application/pdf},
}

@article{aljundi_online_2019,
	title = {Online {Continual} {Learning} with {Maximally} {Interfered} {Retrieval}},
	url = {http://arxiv.org/abs/1908.04742},
	abstract = {Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or "single-pass through the data" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal. In this work, we consider a controlled sampling of memories for replay. We retrieve the samples which are most interfered, i.e. whose prediction will be most negatively impacted by the foreseen parameters update. We show a formulation for this sampling criterion in both the generative replay and the experience replay setting, producing consistent gains in performance and greatly reduced forgetting. We release an implementation of our method at https://github.com/optimass/Maximally\_Interfered\_Retrieval.},
	urldate = {2021-10-17},
	journal = {arXiv:1908.04742 [cs, stat]},
	author = {Aljundi, Rahaf and Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Lin, Min and Charlin, Laurent and Tuytelaars, Tinne},
	month = oct,
	year = {2019},
	note = {arXiv: 1908.04742},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XMN6DZK8/Aljundi et al. - 2019 - Online Continual Learning with Maximally Interfere.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ADNRKI8F/1908.html:text/html},
}

@article{aljundi_gradient_2019,
	title = {Gradient based sample selection for online continual learning},
	url = {http://arxiv.org/abs/1903.08671},
	abstract = {A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.},
	urldate = {2021-10-17},
	journal = {arXiv:1903.08671 [cs, stat]},
	author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
	month = oct,
	year = {2019},
	note = {arXiv: 1903.08671},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZNG5NBAF/Aljundi et al. - 2019 - Gradient based sample selection for online continu.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/2LIIQGU9/1903.html:text/html},
}

@article{jerfel_reconciling_2019,
	title = {Reconciling meta-learning and continual learning with online mixtures of tasks},
	url = {http://arxiv.org/abs/1812.06080},
	abstract = {Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.},
	urldate = {2021-10-17},
	journal = {arXiv:1812.06080 [cs, stat]},
	author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Thomas L. and Heller, Katherine},
	month = jun,
	year = {2019},
	note = {arXiv: 1812.06080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/P9DDAKPS/Jerfel et al. - 2019 - Reconciling meta-learning and continual learning w.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/J892M9RV/1812.html:text/html},
}

@article{rao_continual_2019,
	title = {Continual {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/1910.14481},
	abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
	urldate = {2021-10-17},
	journal = {arXiv:1910.14481 [cs, stat]},
	author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.14481},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/2F5VGI9H/Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PHLQJ9VA/1910.html:text/html},
}

@article{javed_meta-learning_2019,
	title = {Meta-{Learning} {Representations} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1905.12588},
	abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite---they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. We release an implementation of our method at https://github.com/khurramjaved96/mrcl .},
	urldate = {2021-10-17},
	journal = {arXiv:1905.12588 [cs, stat]},
	author = {Javed, Khurram and White, Martha},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.12588},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FZQ5I3BS/Javed and White - 2019 - Meta-Learning Representations for Continual Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/VMHK9GTF/1905.html:text/html},
}

@article{rolnick_experience_2019,
	title = {Experience {Replay} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/1811.11682},
	abstract = {Continual learning is the problem of learning new tasks or knowledge while protecting old knowledge and ideally generalizing from old experience to learn new tasks faster. Neural networks trained by stochastic gradient descent often degrade on old tasks when trained successively on new tasks with different data distributions. This phenomenon, referred to as catastrophic forgetting, is considered a major hurdle to learning with non-stationary data or sequences of new tasks, and prevents networks from continually accumulating knowledge and skills. We examine this issue in the context of reinforcement learning, in a setting where an agent is exposed to tasks in a sequence. Unlike most other work, we do not provide an explicit indication to the model of task boundaries, which is the most general circumstance for a learning agent exposed to continuous experience. While various methods to counteract catastrophic forgetting have recently been proposed, we explore a straightforward, general, and seemingly overlooked solution - that of using experience replay buffers for all past events - with a mixture of on- and off-policy learning, leveraging behavioral cloning. We show that this strategy can still learn new tasks quickly yet can substantially reduce catastrophic forgetting in both Atari and DMLab domains, even matching the performance of methods that require task identities. When buffer storage is constrained, we confirm that a simple mechanism for randomly discarding data allows a limited size buffer to perform almost as well as an unbounded one.},
	urldate = {2021-10-17},
	journal = {arXiv:1811.11682 [cs, stat]},
	author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy P. and Wayne, Greg},
	month = nov,
	year = {2019},
	note = {arXiv: 1811.11682},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WBV7YWTA/Rolnick et al. - 2019 - Experience Replay for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/Q7I9IWCC/1811.html:text/html},
}

@article{xu_reinforced_2018,
	title = {Reinforced {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.12369},
	abstract = {Most artificial intelligence models have limiting ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.},
	urldate = {2021-10-17},
	journal = {arXiv:1805.12369 [cs, stat]},
	author = {Xu, Ju and Zhu, Zhanxing},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GT8BQRHJ/Xu and Zhu - 2018 - Reinforced Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LP3W8FYB/1805.html:text/html},
}

@article{wu_memory_2019,
	title = {Memory {Replay} {GANs}: learning to generate images from new categories without forgetting},
	shorttitle = {Memory {Replay} {GANs}},
	url = {http://arxiv.org/abs/1809.02058},
	abstract = {Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.},
	urldate = {2021-10-17},
	journal = {arXiv:1809.02058 [cs]},
	author = {Wu, Chenshen and Herranz, Luis and Liu, Xialei and Wang, Yaxing and van de Weijer, Joost and Raducanu, Bogdan},
	month = sep,
	year = {2019},
	note = {arXiv: 1809.02058},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YC53WG5F/Wu et al. - 2019 - Memory Replay GANs learning to generate images fr.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6TCYBK7G/1809.html:text/html},
}

@article{huo_training_2019,
	title = {Training {Neural} {Networks} {Using} {Features} {Replay}},
	url = {http://arxiv.org/abs/1807.04511},
	abstract = {Training a neural network using backpropagation algorithm requires passing error gradients sequentially through the network. The backward locking prevents us from updating network layers in parallel and fully leveraging the computing resources. Recently, there are several works trying to decouple and parallelize the backpropagation algorithm. However, all of them suffer from severe accuracy loss or memory explosion when the neural network is deep. To address these challenging issues, we propose a novel parallel-objective formulation for the objective function of the neural network. After that, we introduce features replay algorithm and prove that it is guaranteed to converge to critical points for the non-convex problem under certain conditions. Finally, we apply our method to training deep convolutional neural networks, and the experimental results show that the proposed method achieves \{faster\} convergence, \{lower\} memory consumption, and \{better\} generalization error than compared methods.},
	urldate = {2021-10-17},
	journal = {arXiv:1807.04511 [cs, stat]},
	author = {Huo, Zhouyuan and Gu, Bin and Huang, Heng},
	month = may,
	year = {2019},
	note = {arXiv: 1807.04511},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3VG2ID6V/Huo et al. - 2019 - Training Neural Networks Using Features Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SWAQUZ46/1807.html:text/html},
}

@article{andrychowicz_hindsight_2018,
	title = {Hindsight {Experience} {Replay}},
	url = {http://arxiv.org/abs/1707.01495},
	abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.},
	urldate = {2021-10-17},
	journal = {arXiv:1707.01495 [cs]},
	author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
	month = feb,
	year = {2018},
	note = {arXiv: 1707.01495},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XZN9S6PE/Andrychowicz et al. - 2018 - Hindsight Experience Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GTCD92R7/1707.html:text/html},
}

@article{neu_information-theoretic_2021,
	title = {Information-{Theoretic} {Generalization} {Bounds} for {Stochastic} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/2102.00931},
	abstract = {We study the generalization properties of the popular stochastic optimization method known as stochastic gradient descent (SGD) for optimizing general non-convex loss functions. Our main contribution is providing upper bounds on the generalization error that depend on local statistics of the stochastic gradients evaluated along the path of iterates calculated by SGD. The key factors our bounds depend on are the variance of the gradients (with respect to the data distribution) and the local smoothness of the objective function along the SGD path, and the sensitivity of the loss function to perturbations to the final output. Our key technical tool is combining the information-theoretic generalization bounds previously used for analyzing randomized variants of SGD with a perturbation analysis of the iterates.},
	urldate = {2021-10-17},
	journal = {arXiv:2102.00931 [cs, stat]},
	author = {Neu, Gergely and Dziugaite, Gintare Karolina and Haghifam, Mahdi and Roy, Daniel M.},
	month = aug,
	year = {2021},
	note = {arXiv: 2102.00931},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9TXVIA7B/Neu et al. - 2021 - Information-Theoretic Generalization Bounds for St.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/7SQ34GKV/2102.html:text/html},
}

@article{chaloner_bayesian_1995,
	title = {Bayesian {Experimental} {Design}: {A} {Review}},
	volume = {10},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Bayesian {Experimental} {Design}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-10/issue-3/Bayesian-Experimental-Design-A-Review/10.1214/ss/1177009939.full},
	doi = {10.1214/ss/1177009939},
	abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision-theoretic approach. This framework casts criteria from the Bayesian literature of design as part of a single coherent approach. The decision-theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility function are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion.},
	number = {3},
	urldate = {2021-10-16},
	journal = {Statistical Science},
	author = {Chaloner, Kathryn and Verdinelli, Isabella},
	month = aug,
	year = {1995},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {decision theory, hierarchical linear models, logistic regression, nonlinear design, nonlinear models, optimal design, optimality criteria, utility functions},
	pages = {273--304},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/K8R8AFYU/Chaloner and Verdinelli - 1995 - Bayesian Experimental Design A Review.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/KHDKIBQD/1177009939.html:text/html},
}

@article{wu_phase_2020,
	title = {Phase {Transitions} for the {Information} {Bottleneck} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/2001.01878},
	abstract = {In the Information Bottleneck (IB), when tuning the relative strength between compression and prediction terms, how do the two terms behave, and what's their relationship with the dataset and the learned representation? In this paper, we set out to answer these questions by studying multiple phase transitions in the IB objective: \${\textbackslash}text\{IB\}\_{\textbackslash}beta[p(z{\textbar}x)] = I(X; Z) - {\textbackslash}beta I(Y; Z)\$ defined on the encoding distribution p(z{\textbar}x) for input \$X\$, target \$Y\$ and representation \$Z\$, where sudden jumps of \$dI(Y; Z)/d {\textbackslash}beta\$ and prediction accuracy are observed with increasing \${\textbackslash}beta\$. We introduce a definition for IB phase transitions as a qualitative change of the IB loss landscape, and show that the transitions correspond to the onset of learning new classes. Using second-order calculus of variations, we derive a formula that provides a practical condition for IB phase transitions, and draw its connection with the Fisher information matrix for parameterized models. We provide two perspectives to understand the formula, revealing that each IB phase transition is finding a component of maximum (nonlinear) correlation between \$X\$ and \$Y\$ orthogonal to the learned representation, in close analogy with canonical-correlation analysis (CCA) in linear settings. Based on the theory, we present an algorithm for discovering phase transition points. Finally, we verify that our theory and algorithm accurately predict phase transitions in categorical datasets, predict the onset of learning new classes and class difficulty in MNIST, and predict prominent phase transitions in CIFAR10.},
	urldate = {2021-10-12},
	journal = {arXiv:2001.01878 [cs, math, stat]},
	author = {Wu, Tailin and Fischer, Ian},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.01878},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HWRHRAVG/Wu and Fischer - 2020 - Phase Transitions for the Information Bottleneck i.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DQ5VQNLD/2001.html:text/html},
}

@article{wen_characterizing_2021,
	title = {Characterizing the {Gap} {Between} {Actor}-{Critic} and {Policy} {Gradient}},
	url = {http://arxiv.org/abs/2106.06932},
	abstract = {Actor-critic (AC) methods are ubiquitous in reinforcement learning. Although it is understood that AC methods are closely related to policy gradient (PG), their precise connection has not been fully characterized previously. In this paper, we explain the gap between AC and PG methods by identifying the exact adjustment to the AC objective/gradient that recovers the true policy gradient of the cumulative reward objective (PG). Furthermore, by viewing the AC method as a two-player Stackelberg game between the actor and critic, we show that the Stackelberg policy gradient can be recovered as a special case of our more general analysis. Based on these results, we develop practical algorithms, Residual Actor-Critic and Stackelberg Actor-Critic, for estimating the correction between AC and PG and use these to modify the standard AC algorithm. Experiments on popular tabular and continuous environments show the proposed corrections can improve both the sample efficiency and final performance of existing AC methods.},
	urldate = {2021-10-12},
	journal = {arXiv:2106.06932 [cs]},
	author = {Wen, Junfeng and Kumar, Saurabh and Gummadi, Ramki and Schuurmans, Dale},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.06932},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/AL6D9CUV/Wen et al. - 2021 - Characterizing the Gap Between Actor-Critic and Po.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KKFFHEG5/2106.html:text/html},
}

@inproceedings{nietert_smooth_2021,
	title = {Smooth \$p\$-{Wasserstein} {Distance}: {Structure}, {Empirical} {Approximation}, and {Statistical} {Applications}},
	shorttitle = {Smooth \$p\$-{Wasserstein} {Distance}},
	url = {https://proceedings.mlr.press/v139/nietert21a.html},
	abstract = {Discrepancy measures between probability distributions, often termed statistical distances, are ubiquitous in probability theory, statistics and machine learning. To combat the curse of dimensionality when estimating these distances from data, recent work has proposed smoothing out local irregularities in the measured distributions via convolution with a Gaussian kernel. Motivated by the scalability of this framework to high dimensions, we investigate the structural and statistical behavior of the Gaussian-smoothed ppp-Wasserstein distance W(σ)pWp(σ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\}, for arbitrary p≥1p≥1p{\textbackslash}geq 1. After establishing basic metric and topological properties of W(σ)pWp(σ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\}, we explore the asymptotic statistical properties of W(σ)p(μ{\textasciicircum}n,μ)Wp(σ)(μ{\textasciicircum}n,μ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\}({\textbackslash}hat\{{\textbackslash}mu\}\_n,{\textbackslash}mu), where μ{\textasciicircum}nμ{\textasciicircum}n{\textbackslash}hat\{{\textbackslash}mu\}\_n is the empirical distribution of nnn independent observations from μμ{\textbackslash}mu. We prove that W(σ)pWp(σ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\} enjoys a parametric empirical convergence rate of n−1/2n−1/2n{\textasciicircum}\{-1/2\}, which contrasts the n−1/dn−1/dn{\textasciicircum}\{-1/d\} rate for unsmoothed {\textbackslash}Wp{\textbackslash}Wp{\textbackslash}Wp when d≥3d≥3d {\textbackslash}geq 3. Our proof relies on controlling W(σ)pWp(σ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\} by a pppth-order smooth Sobolev distance d(σ)pdp(σ){\textbackslash}mathsf\{d\}\_p{\textasciicircum}\{({\textbackslash}sigma)\} and deriving the limit distribution of n−−√d(σ)p(μ{\textasciicircum}n,μ)ndp(σ)(μ{\textasciicircum}n,μ){\textbackslash}sqrt\{n\}{\textbackslash},{\textbackslash}mathsf\{d\}\_p{\textasciicircum}\{({\textbackslash}sigma)\}({\textbackslash}hat\{{\textbackslash}mu\}\_n,{\textbackslash}mu) for all dimensions ddd. As applications, we provide asymptotic guarantees for two-sample testing and minimum distance estimation using W(σ)pWp(σ){\textbackslash}mathsf\{W\}\_p{\textasciicircum}\{({\textbackslash}sigma)\}, with experiments for p=2p=2p=2 using a maximum mean discrepancy formulation of d(σ)2d2(σ){\textbackslash}mathsf\{d\}\_2{\textasciicircum}\{({\textbackslash}sigma)\}.},
	language = {en},
	urldate = {2021-10-12},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nietert, Sloan and Goldfeld, Ziv and Kato, Kengo},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {8172--8183},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JHT9K26L/Nietert et al. - 2021 - Smooth \$p\$-Wasserstein Distance Structure, Empiri.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/QXFVRY3K/Nietert et al. - 2021 - Smooth \$p\$-Wasserstein Distance Structure, Empiri.pdf:application/pdf},
}

@article{arora_dropout_2020,
	title = {Dropout: {Explicit} {Forms} and {Capacity} {Control}},
	shorttitle = {Dropout},
	url = {http://arxiv.org/abs/2003.03397},
	abstract = {We investigate the capacity control provided by dropout in various machine learning problems. First, we study dropout for matrix completion, where it induces a data-dependent regularizer that, in expectation, equals the weighted trace-norm of the product of the factors. In deep learning, we show that the data-dependent regularizer due to dropout directly controls the Rademacher complexity of the underlying class of deep neural networks. These developments enable us to give concrete generalization error bounds for the dropout algorithm in both matrix completion as well as training deep neural networks. We evaluate our theoretical findings on real-world datasets, including MovieLens, MNIST, and Fashion-MNIST.},
	urldate = {2021-10-12},
	journal = {arXiv:2003.03397 [cs, stat]},
	author = {Arora, Raman and Bartlett, Peter and Mianjy, Poorya and Srebro, Nathan},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.03397},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3S7IYIPN/Arora et al. - 2020 - Dropout Explicit Forms and Capacity Control.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WPI3JGCZ/2003.html:text/html},
}

@article{arbel_annealed_2021,
	title = {Annealed {Flow} {Transport} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/2102.07501},
	abstract = {Annealed Importance Sampling (AIS) and its Sequential Monte Carlo (SMC) extensions are state-of-the-art methods for estimating normalizing constants of probability distributions. We propose here a novel Monte Carlo algorithm, Annealed Flow Transport (AFT), that builds upon AIS and SMC and combines them with normalizing flows (NFs) for improved performance. This method transports a set of particles using not only importance sampling (IS), Markov chain Monte Carlo (MCMC) and resampling steps - as in SMC, but also relies on NFs which are learned sequentially to push particles towards the successive annealed targets. We provide limit theorems for the resulting Monte Carlo estimates of the normalizing constant and expectations with respect to the target distribution. Additionally, we show that a continuous-time scaling limit of the population version of AFT is given by a Feynman--Kac measure which simplifies to the law of a controlled diffusion for expressive NFs. We demonstrate experimentally the benefits and limitations of our methodology on a variety of applications.},
	urldate = {2021-10-12},
	journal = {arXiv:2102.07501 [cond-mat, stat]},
	author = {Arbel, Michael and Matthews, Alexander G. D. G. and Doucet, Arnaud},
	month = jul,
	year = {2021},
	note = {arXiv: 2102.07501},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WLRZF77J/Arbel et al. - 2021 - Annealed Flow Transport Monte Carlo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/99H9JJ39/2102.html:text/html},
}

@article{alvarez-melis_dataset_2021,
	title = {Dataset {Dynamics} via {Gradient} {Flows} in {Probability} {Space}},
	url = {http://arxiv.org/abs/2010.12760},
	abstract = {Various machine learning tasks, from generative modeling to domain adaptation, revolve around the concept of dataset transformation and manipulation. While various methods exist for transforming unlabeled datasets, principled methods to do so for labeled (e.g., classification) datasets are missing. In this work, we propose a novel framework for dataset transformation, which we cast as optimization over data-generating joint probability distributions. We approach this class of problems through Wasserstein gradient flows in probability space, and derive practical and efficient particle-based methods for a flexible but well-behaved class of objective functions. Through various experiments, we show that this framework can be used to impose constraints on classification datasets, adapt them for transfer learning, or to re-purpose fixed or black-box models to classify -- with high accuracy -- previously unseen datasets.},
	urldate = {2021-10-12},
	journal = {arXiv:2010.12760 [cs, stat]},
	author = {Alvarez-Melis, David and Fusi, Nicolò},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.12760},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/2DESDS88/Alvarez-Melis and Fusi - 2021 - Dataset Dynamics via Gradient Flows in Probability.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3QYQ2QKS/2010.html:text/html},
}

@article{pedregosa_hyperparameter_2016,
	title = {Hyperparameter optimization with approximate gradient},
	url = {http://arxiv.org/abs/1602.02355},
	abstract = {Most models in machine learning contain at least one hyperparameter to control for model complexity. Choosing an appropriate set of hyperparameters is both crucial in terms of model accuracy and computationally challenging. In this work we propose an algorithm for the optimization of continuous hyperparameters using inexact gradient information. An advantage of this method is that hyperparameters can be updated before model parameters have fully converged. We also give sufficient conditions for the global convergence of this method, based on regularity conditions of the involved functions and summability of errors. Finally, we validate the empirical performance of this method on the estimation of regularization constants of L2-regularized logistic regression and kernel Ridge regression. Empirical benchmarks indicate that our approach is highly competitive with respect to state of the art methods.},
	urldate = {2021-10-08},
	journal = {arXiv:1602.02355 [cs, math, stat]},
	author = {Pedregosa, Fabian},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.02355},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8FXFY4CM/Pedregosa - 2016 - Hyperparameter optimization with approximate gradi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YY9WYMWV/1602.html:text/html},
}

@article{shin_continual_2017,
	title = {Continual {Learning} with {Deep} {Generative} {Replay}},
	url = {http://arxiv.org/abs/1705.08690},
	abstract = {Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model ("generator") and a task solving model ("solver"). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.},
	urldate = {2021-10-06},
	journal = {arXiv:1705.08690 [cs]},
	author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
	month = dec,
	year = {2017},
	note = {arXiv: 1705.08690},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9Y7F336W/Shin et al. - 2017 - Continual Learning with Deep Generative Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/G6YNZQQ2/1705.html:text/html},
}

@article{fujimoto_equivalence_2020,
	title = {An {Equivalence} between {Loss} {Functions} and {Non}-{Uniform} {Sampling} in {Experience} {Replay}},
	url = {http://arxiv.org/abs/2007.06049},
	abstract = {Prioritized Experience Replay (PER) is a deep reinforcement learning technique in which agents learn from transitions sampled with non-uniform probability proportionate to their temporal-difference error. We show that any loss function evaluated with non-uniformly sampled data can be transformed into another uniformly sampled loss function with the same expected gradient. Surprisingly, we find in some environments PER can be replaced entirely by this new loss function without impact to empirical performance. Furthermore, this relationship suggests a new branch of improvements to PER by correcting its uniformly sampled loss function equivalent. We demonstrate the effectiveness of our proposed modifications to PER and the equivalent loss function in several MuJoCo and Atari environments.},
	urldate = {2021-10-01},
	journal = {arXiv:2007.06049 [cs, stat]},
	author = {Fujimoto, Scott and Meger, David and Precup, Doina},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.06049},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/M2J4DBBV/Fujimoto et al. - 2020 - An Equivalence between Loss Functions and Non-Unif.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/9SEPJHH2/2007.html:text/html},
}

@article{xu_continual_2020,
	title = {Continual {Learning} of {Control} {Primitives}: {Skill} {Discovery} via {Reset}-{Games}},
	shorttitle = {Continual {Learning} of {Control} {Primitives}},
	url = {http://arxiv.org/abs/2011.05286},
	abstract = {Reinforcement learning has the potential to automate the acquisition of behavior in complex settings, but in order for it to be successfully deployed, a number of practical challenges must be addressed. First, in real world settings, when an agent attempts a task and fails, the environment must somehow "reset" so that the agent can attempt the task again. While easy in simulation, this could require considerable human effort in the real world, especially if the number of trials is very large. Second, real world learning often involves complex, temporally extended behavior that is often difficult to acquire with random exploration. While these two problems may at first appear unrelated, in this work, we show how a single method can allow an agent to acquire skills with minimal supervision while removing the need for resets. We do this by exploiting the insight that the need to "reset" an agent to a broad set of initial states for a learning task provides a natural setting to learn a diverse set of "reset-skills". We propose a general-sum game formulation that balances the objectives of resetting and learning skills, and demonstrate that this approach improves performance on reset-free tasks, and additionally show that the skills we obtain can be used to significantly accelerate downstream learning.},
	urldate = {2021-10-01},
	journal = {arXiv:2011.05286 [cs]},
	author = {Xu, Kelvin and Verma, Siddharth and Finn, Chelsea and Levine, Sergey},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.05286},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5QZZS7MW/Xu et al. - 2020 - Continual Learning of Control Primitives Skill Di.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/47DC6K4P/2011.html:text/html},
}

@article{joseph_meta-consolidation_2020,
	title = {Meta-{Consolidation} for {Continual} {Learning}},
	url = {http://arxiv.org/abs/2010.00352},
	abstract = {The ability to continuously learn and adapt itself to new tasks, without losing grasp of already acquired knowledge is a hallmark of biological learning systems, which current deep learning systems fall short of. In this work, we present a novel methodology for continual learning called MERLIN: Meta-Consolidation for Continual Learning. We assume that weights of a neural network \${\textbackslash}boldsymbol {\textbackslash}psi\$, for solving task \${\textbackslash}boldsymbol t\$, come from a meta-distribution \$p({\textbackslash}boldsymbol\{{\textbackslash}psi{\textbar}t\})\$. This meta-distribution is learned and consolidated incrementally. We operate in the challenging online continual learning setting, where a data point is seen by the model only once. Our experiments with continual learning benchmarks of MNIST, CIFAR-10, CIFAR-100 and Mini-ImageNet datasets show consistent improvement over five baselines, including a recent state-of-the-art, corroborating the promise of MERLIN.},
	urldate = {2021-10-01},
	journal = {arXiv:2010.00352 [cs]},
	author = {Joseph, K. J. and Balasubramanian, Vineeth N.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.00352},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FZZE88JJ/Joseph and Balasubramanian - 2020 - Meta-Consolidation for Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DCV5YSSY/2010.html:text/html},
}

@article{chaudhry_continual_2020,
	title = {Continual {Learning} in {Low}-rank {Orthogonal} {Subspaces}},
	url = {http://arxiv.org/abs/2010.11635},
	abstract = {In continual learning (CL), a learner is faced with a sequence of tasks, arriving one after the other, and the goal is to remember all the tasks once the continual learning experience is finished. The prior art in CL uses episodic memory, parameter regularization or extensible network structures to reduce interference among tasks, but in the end, all the approaches learn different tasks in a joint vector space. We believe this invariably leads to interference among different tasks. We propose to learn tasks in different (low-rank) vector subspaces that are kept orthogonal to each other in order to minimize interference. Further, to keep the gradients of different tasks coming from these subspaces orthogonal to each other, we learn isometric mappings by posing network training as an optimization problem over the Stiefel manifold. To the best of our understanding, we report, for the first time, strong results over experience-replay baseline with and without memory on standard classification benchmarks in continual learning. The code is made publicly available.},
	urldate = {2021-10-01},
	journal = {arXiv:2010.11635 [cs]},
	author = {Chaudhry, Arslan and Khan, Naeemullah and Dokania, Puneet K. and Torr, Philip H. S.},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.11635},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SZ5MZUCX/Chaudhry et al. - 2020 - Continual Learning in Low-rank Orthogonal Subspace.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IB579TCH/2010.html:text/html},
}

@article{gupta_-maml_2020,
	title = {La-{MAML}: {Look}-ahead {Meta} {Learning} for {Continual} {Learning}},
	shorttitle = {La-{MAML}},
	url = {http://arxiv.org/abs/2007.13904},
	abstract = {The continual learning problem involves training models with limited capacity to perform well on a set of an unknown number of sequentially arriving tasks. While meta-learning shows great potential for reducing interference between old and new tasks, the current training procedures tend to be either slow or offline, and sensitive to many hyper-parameters. In this work, we propose Look-ahead MAML (La-MAML), a fast optimisation-based meta-learning algorithm for online-continual learning, aided by a small episodic memory. Our proposed modulation of per-parameter learning rates in our meta-learning update allows us to draw connections to prior work on hypergradients and meta-descent. This provides a more flexible and efficient way to mitigate catastrophic forgetting compared to conventional prior-based methods. La-MAML achieves performance superior to other replay-based, prior-based and meta-learning based approaches for continual learning on real-world visual classification benchmarks. Source code can be found here: https://github.com/montrealrobotics/La-MAML},
	urldate = {2021-10-01},
	journal = {arXiv:2007.13904 [cs, stat]},
	author = {Gupta, Gunshi and Yadav, Karmesh and Paull, Liam},
	month = nov,
	year = {2020},
	note = {arXiv: 2007.13904},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SK3Y7ADK/Gupta et al. - 2020 - La-MAML Look-ahead Meta Learning for Continual Le.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KIXFKKZ4/2007.html:text/html},
}

@article{buzzega_dark_2020,
	title = {Dark {Experience} for {General} {Continual} {Learning}: a {Strong}, {Simple} {Baseline}},
	shorttitle = {Dark {Experience} for {General} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2004.07211},
	abstract = {Continual Learning has inspired a plethora of approaches and evaluation settings; however, the majority of them overlooks the properties of a practical scenario, where the data stream cannot be shaped as a sequence of tasks and offline training is not viable. We work towards General Continual Learning (GCL), where task boundaries blur and the domain and class distributions shift either gradually or suddenly. We address it through mixing rehearsal with knowledge distillation and regularization; our simple baseline, Dark Experience Replay, matches the network's logits sampled throughout the optimization trajectory, thus promoting consistency with its past. By conducting an extensive analysis on both standard benchmarks and a novel GCL evaluation setting (MNIST-360), we show that such a seemingly simple baseline outperforms consolidated approaches and leverages limited resources. We further explore the generalization capabilities of our objective, showing its regularization being beneficial beyond mere performance.},
	urldate = {2021-10-01},
	journal = {arXiv:2004.07211 [cs, stat]},
	author = {Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
	month = oct,
	year = {2020},
	note = {arXiv: 2004.07211},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HZ7AZRIQ/Buzzega et al. - 2020 - Dark Experience for General Continual Learning a .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/U3FNGZWW/2004.html:text/html},
}

@article{lee_overcoming_2018,
	title = {Overcoming {Catastrophic} {Forgetting} by {Incremental} {Moment} {Matching}},
	url = {http://arxiv.org/abs/1703.08475},
	abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
	urldate = {2021-10-01},
	journal = {arXiv:1703.08475 [cs]},
	author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.08475},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HSID3HEE/Lee et al. - 2018 - Overcoming Catastrophic Forgetting by Incremental .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/MNK3M4U9/1703.html:text/html},
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2021-09-24},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EHXW4VJZ/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DDWMTXJ2/1701.html:text/html},
}

@techreport{poggio_stable_2020,
	type = {Technical {Report}},
	title = {Stable {Foundations} for {Learning}: a foundational framework for learning theory in both the classical and modern regime.},
	shorttitle = {Stable {Foundations} for {Learning}},
	url = {https://dspace.mit.edu/handle/1721.1/124343},
	abstract = {We consider here the class of supervised learning algorithms known as Empirical Risk Minimization (ERM). The classical theory by Vapnik and others characterize universal consistency of ERM in the classical regime in which the architecture of the learning network is fixed and n, the number of training examples, goes to infinity. According to the classical theory, the minimizer of the empirical risk is consistent if the hypothesis space has finite complexity. We do not have a similar general theory for the modern regime of interpolating regressors and over-parameterized deep networks, in which d {\textgreater} n and 𝑑/n remains constant as n goes to infinity. 
 
In this note I propose the outline of such a theory based on the specific notion of CVloo stability of the 
learning algorithm with respect to perturbations of the training set. The theory shows that for interpolating regressors and separating classifiers (either kernel machines or deep RELU networks) 
   1. minimizing CVloo stability minimizes the expected error  
   2. the most stable solutions are minimum norm solutions 
The hope is that this approach may lead to a unified theory encompassing both the modern regime and the classical one.},
	language = {en},
	urldate = {2021-09-24},
	institution = {Center for Brains, Minds and Machines (CBMM)},
	author = {Poggio, Tomaso},
	month = mar,
	year = {2020},
	note = {Accepted: 2020-03-25T20:02:45Z},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/TVGL4G6U/Poggio - 2020 - Stable Foundations for Learning a foundational fr.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/H7IQEE7S/124343.html:text/html},
}

@article{poggio_double_2020,
	title = {Double descent in the condition number},
	url = {http://arxiv.org/abs/1912.06190},
	abstract = {In solving a system of \$n\$ linear equations in \$d\$ variables \$Ax=b\$, the condition number of the \$n,d\$ matrix \$A\$ measures how much errors in the data \$b\$ affect the solution \$x\$. Estimates of this type are important in many inverse problems. An example is machine learning where the key task is to estimate an underlying function from a set of measurements at random points in a high dimensional space and where low sensitivity to error in the data is a requirement for good predictive performance. Here we discuss the simple observation, which is known but surprisingly little quoted (see Theorem 4.2 in {\textbackslash}cite\{Brgisser:2013:CGN:2526261\}): when the columns of \$A\$ are random vectors, the condition number of \$A\$ is highest if \$d=n\$, that is when the inverse of \$A\$ exists. An overdetermined system (\$n{\textgreater}d\$) as well as an underdetermined system (\$n{\textless}d\$), for which the pseudoinverse must be used instead of the inverse, typically have significantly better, that is lower, condition numbers. Thus the condition number of \$A\$ plotted as function of \$d\$ shows a double descent behavior with a peak at \$d=n\$.},
	urldate = {2021-09-24},
	journal = {arXiv:1912.06190 [cs, stat]},
	author = {Poggio, Tomaso and Kur, Gil and Banburski, Andrzej},
	month = apr,
	year = {2020},
	note = {arXiv: 1912.06190},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/69T4HSTH/Poggio et al. - 2020 - Double descent in the condition number.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BBFALWRR/1912.html:text/html},
}

@article{foong_-between_2019,
	title = {'{In}-{Between}' {Uncertainty} in {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.11537},
	abstract = {We describe a limitation in the expressiveness of the predictive uncertainty estimate given by mean-field variational inference (MFVI), a popular approximate inference method for Bayesian neural networks. In particular, MFVI fails to give calibrated uncertainty estimates in between separated regions of observations. This can lead to catastrophically overconfident predictions when testing on out-of-distribution data. Avoiding such overconfidence is critical for active learning, Bayesian optimisation and out-of-distribution robustness. We instead find that a classical technique, the linearised Laplace approximation, can handle 'in-between' uncertainty much better for small network architectures.},
	urldate = {2021-08-29},
	journal = {arXiv:1906.11537 [cs, stat]},
	author = {Foong, Andrew Y. K. and Li, Yingzhen and Hernández-Lobato, José Miguel and Turner, Richard E.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11537},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UTU9JXV3/Foong et al. - 2019 - 'In-Between' Uncertainty in Bayesian Neural Networ.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QQCRNU8L/1906.html:text/html},
}

@article{ash_deep_2020,
	title = {Deep {Batch} {Active} {Learning} by {Diverse}, {Uncertain} {Gradient} {Lower} {Bounds}},
	url = {http://arxiv.org/abs/1906.03671},
	abstract = {We design a new algorithm for batch active learning with deep neural network models. Our algorithm, Batch Active learning by Diverse Gradient Embeddings (BADGE), samples groups of points that are disparate and high-magnitude when represented in a hallucinated gradient space, a strategy designed to incorporate both predictive uncertainty and sample diversity into every selected batch. Crucially, BADGE trades off between diversity and uncertainty without requiring any hand-tuned hyperparameters. We show that while other approaches sometimes succeed for particular batch sizes or architectures, BADGE consistently performs as well or better, making it a versatile option for practical active learning problems.},
	urldate = {2021-08-25},
	journal = {arXiv:1906.03671 [cs, stat]},
	author = {Ash, Jordan T. and Zhang, Chicheng and Krishnamurthy, Akshay and Langford, John and Agarwal, Alekh},
	month = feb,
	year = {2020},
	note = {arXiv: 1906.03671},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RYYBEDKK/Ash et al. - 2020 - Deep Batch Active Learning by Diverse, Uncertain G.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UY4UFJ78/1906.html:text/html},
}

@article{kulesza_k-dpps_nodate,
	title = {k-{DPPs}: {Fixed}-{Size} {Determinantal} {Point} {Processes}},
	abstract = {Determinantal point processes (DPPs) have recently been proposed as models for set selection problems where diversity is preferred. For example, they can be used to select diverse sets of sentences to form document summaries, or to ﬁnd multiple nonoverlapping human poses in an image. However, DPPs conﬂate the modeling of two distinct characteristics: the size of the set, and its content. For many realistic tasks, the size of the desired set is known up front; e.g., in search we may want to show the user exactly ten results. In these situations the eﬀort spent by DPPs modeling set size is not only wasteful, but actually introduces unwanted bias into the modeling of content. Instead, we propose the k-DPP, a conditional DPP that models only sets of cardinality k. In exchange for this restriction, k-DPPs oﬀer greater expressiveness and control over content, and simpliﬁed integration into applications like search. We derive algorithms for eﬃciently normalizing, sampling, and marginalizing kDPPs, and propose an experts-style algorithm for learning combinations of k-DPPs. We demonstrate the usefulness of the model on an image search task, where k-DPPs signiﬁcantly outperform MMR as judged by human annotators.},
	language = {en},
	author = {Kulesza, Alex and Taskar, Ben},
	pages = {8},
	file = {Kulesza and Taskar - k-DPPs Fixed-Size Determinantal Point Processes.pdf:/Users/tokio/Zotero/storage/3KS9AYHI/Kulesza and Taskar - k-DPPs Fixed-Size Determinantal Point Processes.pdf:application/pdf},
}

@article{derezinski_reverse_2018,
	title = {Reverse iterative volume sampling for linear regression},
	url = {http://arxiv.org/abs/1806.01969},
	abstract = {We study the following basic machine learning task: Given a fixed set of \$d\$-dimensional input points for a linear regression problem, we wish to predict a hidden response value for each of the points. We can only afford to attain the responses for a small subset of the points that are then used to construct linear predictions for all points in the dataset. The performance of the predictions is evaluated by the total square loss on all responses (the attained as well as the hidden ones). We show that a good approximate solution to this least squares problem can be obtained from just dimension \$d\$ many responses by using a joint sampling technique called volume sampling. Moreover, the least squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal solution based on all n responses. This unbiasedness is a desirable property that is not shared by other common subset selection techniques. Motivated by these basic properties, we develop a theoretical framework for studying volume sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which are of importance not only to least squares regression but also to numerical linear algebra in general. Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient algorithms for volume sampling which make this technique a practical tool in the machine learning toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.},
	urldate = {2021-08-25},
	journal = {arXiv:1806.01969 [cs, stat]},
	author = {Dereziński, Michał and Warmuth, Manfred K.},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.01969},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9NMT2SGM/Dereziński and Warmuth - 2018 - Reverse iterative volume sampling for linear regre.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KN29QE46/1806.html:text/html},
}

@article{milne_fields_nodate,
	title = {Fields and {Galois} {Theory}},
	language = {en},
	author = {Milne, J S},
	pages = {142},
	file = {Milne - Fields and Galois Theory.pdf:/Users/tokio/Zotero/storage/ZWNK5X9V/Milne - Fields and Galois Theory.pdf:application/pdf},
}

@article{eynard_algebraic_2008,
	title = {Algebraic methods in random matrices and enumerative geometry},
	url = {http://arxiv.org/abs/0811.3531},
	abstract = {We review the method of symplectic invariants recently introduced to solve matrix models loop equations, and further extended beyond the context of matrix models. For any given spectral curve, one defined a sequence of differential forms, and a sequence of complex numbers Fg . We recall the definition of the invariants Fg, and we explain their main properties, in particular symplectic invariance, integrability, modularity,... Then, we give several example of applications, in particular matrix models, enumeration of discrete surfaces (maps), algebraic geometry and topological strings, non-intersecting brownian motions,...},
	urldate = {2021-08-23},
	journal = {arXiv:0811.3531 [hep-th, physics:math-ph]},
	author = {Eynard, Bertrand and Orantin, Nicolas},
	month = nov,
	year = {2008},
	note = {arXiv: 0811.3531},
	keywords = {High Energy Physics - Theory, Mathematical Physics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RL5TID9Q/Eynard and Orantin - 2008 - Algebraic methods in random matrices and enumerati.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UNESWUFE/0811.html:text/html},
}

@article{khan_approximate_2020,
	title = {Approximate {Inference} {Turns} {Deep} {Networks} into {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1906.01930},
	abstract = {Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result, we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly, the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.},
	urldate = {2021-08-23},
	journal = {arXiv:1906.01930 [cs, stat]},
	author = {Khan, Mohammad Emtiyaz and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
	month = jul,
	year = {2020},
	note = {arXiv: 1906.01930},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DV2FHSLY/Khan et al. - 2020 - Approximate Inference Turns Deep Networks into Gau.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/T9VYG2F9/1906.html:text/html},
}

@article{launay_exact_2020,
	title = {Exact {Sampling} of {Determinantal} {Point} {Processes} without {Eigendecomposition}},
	volume = {57},
	issn = {0021-9002, 1475-6072},
	url = {http://arxiv.org/abs/1802.08429},
	doi = {10.1017/jpr.2020.56},
	abstract = {Determinantal point processes (DPPs) enable the modeling of repulsion: they provide diverse sets of points. The repulsion is encoded in a kernel \$K\$ that can be seen as a matrix storing the similarity between points. The diversity comes from the fact that the inclusion probability of a subset is equal to the determinant of a submatrice of \$K\$. The exact algorithm to sample DPPs uses the spectral decomposition of \$K\$, a computation that becomes costly when dealing with a high number of points. Here, we present an alternative exact algorithm in the discrete setting that avoids the eigenvalues and the eigenvectors computation. Instead, it relies on Cholesky decompositions. This is a two steps strategy: first, it samples a Bernoulli point process with an appropriate distribution, then it samples the target DPP distribution through a thinning procedure. Not only is the method used here innovative, but this algorithm can be competitive with the original algorithm or even faster for some applications specified here.},
	number = {4},
	urldate = {2021-08-19},
	journal = {Journal of Applied Probability},
	author = {Launay, Claire and Galerne, Bruno and Desolneux, Agnès},
	month = dec,
	year = {2020},
	note = {arXiv: 1802.08429},
	keywords = {Statistics - Machine Learning},
	pages = {1198--1221},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UZ2AFSIF/Launay et al. - 2020 - Exact Sampling of Determinantal Point Processes wi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/52LPABSL/1802.html:text/html},
}

@article{calandriello_sampling_2020,
	title = {Sampling from a \$k\$-{DPP} without looking at all items},
	url = {http://arxiv.org/abs/2006.16947},
	abstract = {Determinantal point processes (DPPs) are a useful probabilistic model for selecting a small diverse subset out of a large collection of items, with applications in summarization, stochastic optimization, active learning and more. Given a kernel function and a subset size \$k\$, our goal is to sample \$k\$ out of \$n\$ items with probability proportional to the determinant of the kernel matrix induced by the subset (a.k.a. \$k\$-DPP). Existing \$k\$-DPP sampling algorithms require an expensive preprocessing step which involves multiple passes over all \$n\$ items, making it infeasible for large datasets. A na{\textbackslash}"ive heuristic addressing this problem is to uniformly subsample a fraction of the data and perform \$k\$-DPP sampling only on those items, however this method offers no guarantee that the produced sample will even approximately resemble the target distribution over the original dataset. In this paper, we develop an algorithm which adaptively builds a sufficiently large uniform sample of data that is then used to efficiently generate a smaller set of \$k\$ items, while ensuring that this set is drawn exactly from the target distribution defined on all \$n\$ items. We show empirically that our algorithm produces a \$k\$-DPP sample after observing only a small fraction of all elements, leading to several orders of magnitude faster performance compared to the state-of-the-art.},
	urldate = {2021-08-18},
	journal = {arXiv:2006.16947 [cs, stat]},
	author = {Calandriello, Daniele and Dereziński, Michał and Valko, Michal},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.16947},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/R29ZUVAJ/Calandriello et al. - 2020 - Sampling from a \$k\$-DPP without looking at all ite.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6DHAPSFJ/2006.html:text/html},
}

@article{courrieu_fast_2008,
	title = {Fast {Computation} of {Moore}-{Penrose} {Inverse} {Matrices}},
	url = {http://arxiv.org/abs/0804.4809},
	abstract = {Many neural learning algorithms require to solve large least square systems in order to obtain synaptic weights. Moore-Penrose inverse matrices allow for solving such systems, even with rank deficiency, and they provide minimum-norm vectors of synaptic weights, which contribute to the regularization of the input-output mapping. It is thus of interest to develop fast and accurate algorithms for computing Moore-Penrose inverse matrices. In this paper, an algorithm based on a full rank Cholesky factorization is proposed. The resulting pseudoinverse matrices are similar to those provided by other algorithms. However the computation time is substantially shorter, particularly for large systems.},
	urldate = {2021-08-18},
	journal = {arXiv:0804.4809 [cs]},
	author = {Courrieu, Pierre},
	month = apr,
	year = {2008},
	note = {arXiv: 0804.4809},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/75RKWVLB/Courrieu - 2008 - Fast Computation of Moore-Penrose Inverse Matrices.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KUV8ZQ9H/0804.html:text/html},
}

@article{gartrell_low-rank_2016,
	title = {Low-{Rank} {Factorization} of {Determinantal} {Point} {Processes} for {Recommendation}},
	url = {http://arxiv.org/abs/1602.05436},
	abstract = {Determinantal point processes (DPPs) have garnered attention as an elegant probabilistic model of set diversity. They are useful for a number of subset selection tasks, including product recommendation. DPPs are parametrized by a positive semi-definite kernel matrix. In this work we present a new method for learning the DPP kernel from observed data using a low-rank factorization of this kernel. We show that this low-rank factorization enables a learning algorithm that is nearly an order of magnitude faster than previous approaches, while also providing for a method for computing product recommendation predictions that is far faster (up to 20x faster or more for large item catalogs) than previous techniques that involve a full-rank DPP kernel. Furthermore, we show that our method provides equivalent or sometimes better predictive performance than prior full-rank DPP approaches, and better performance than several other competing recommendation methods in many cases. We conduct an extensive experimental evaluation using several real-world datasets in the domain of product recommendation to demonstrate the utility of our method, along with its limitations.},
	urldate = {2021-08-18},
	journal = {arXiv:1602.05436 [cs, stat]},
	author = {Gartrell, Mike and Paquet, Ulrich and Koenigstein, Noam},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.05436},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TTUHNU6H/Gartrell et al. - 2016 - Low-Rank Factorization of Determinantal Point Proc.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6XQR6YLY/1602.html:text/html},
}

@article{dupuy_learning_2016,
	title = {Learning {Determinantal} {Point} {Processes} in {Sublinear} {Time}},
	url = {http://arxiv.org/abs/1610.05925},
	abstract = {We propose a new class of determinantal point processes (DPPs) which can be manipulated for inference and parameter learning in potentially sublinear time in the number of items. This class, based on a specific low-rank factorization of the marginal kernel, is particularly suited to a subclass of continuous DPPs and DPPs defined on exponentially many items. We apply this new class to modelling text documents as sampling a DPP of sentences, and propose a conditional maximum likelihood formulation to model topic proportions, which is made possible with no approximation for our class of DPPs. We present an application to document summarization with a DPP on \$2{\textasciicircum}\{500\}\$ items.},
	urldate = {2021-08-18},
	journal = {arXiv:1610.05925 [cs, stat]},
	author = {Dupuy, Christophe and Bach, Francis},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.05925},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MNZVBP5B/Dupuy and Bach - 2016 - Learning Determinantal Point Processes in Sublinea.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NS8YI6DL/1610.html:text/html},
}

@inproceedings{anari_monte_2016,
	title = {Monte {Carlo} {Markov} {Chain} {Algorithms} for {Sampling} {Strongly} {Rayleigh} {Distributions} and {Determinantal} {Point} {Processes}},
	url = {http://proceedings.mlr.press/v49/anari16.html},
	language = {en},
	urldate = {2021-08-14},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Anari, Nima and Gharan, Shayan Oveis and Rezaei, Alireza},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {103--115},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/L96PL47B/Anari et al. - 2016 - Monte Carlo Markov Chain Algorithms for Sampling S.pdf:application/pdf},
}

@inproceedings{bhaskara_online_2020,
	title = {Online {MAP} {Inference} of {Determinantal} {Point} {Processes}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/23378a2d0a25c6ade2c1da1c06c5213f-Abstract.html},
	urldate = {2021-08-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bhaskara, Aditya and Karbasi, Amin and Lattanzi, Silvio and Zadimoghaddam, Morteza},
	year = {2020},
	pages = {3419--3429},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/96IEPVZG/Bhaskara et al. - 2020 - Online MAP Inference of Determinantal Point Proces.pdf:application/pdf},
}

@article{cao_towards_2020,
	title = {Towards {Understanding} the {Spectral} {Bias} of {Deep} {Learning}},
	url = {http://arxiv.org/abs/1912.01198},
	abstract = {An intriguing phenomenon observed during training neural networks is the spectral bias, which states that neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain efforts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks. Finally, we provide numerical experiments to demonstrate the correctness of our theory. Our experimental results also show that our theory can tolerate certain model misspecification in terms of the input data distribution.},
	urldate = {2021-08-14},
	journal = {arXiv:1912.01198 [cs, stat]},
	author = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
	month = oct,
	year = {2020},
	note = {arXiv: 1912.01198},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XZNQZ3GX/Cao et al. - 2020 - Towards Understanding the Spectral Bias of Deep Le.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YC4RJVRK/1912.html:text/html},
}

@article{nguyen_tight_2021,
	title = {Tight {Bounds} on the {Smallest} {Eigenvalue} of the {Neural} {Tangent} {Kernel} for {Deep} {ReLU} {Networks}},
	url = {http://arxiv.org/abs/2012.11654},
	abstract = {A recent line of work has analyzed the theoretical properties of deep neural networks via the Neural Tangent Kernel (NTK). In particular, the smallest eigenvalue of the NTK has been related to the memorization capacity, the global convergence of gradient descent algorithms and the generalization of deep nets. However, existing results either provide bounds in the two-layer setting or assume that the spectrum of the NTK matrices is bounded away from 0 for multi-layer networks. In this paper, we provide tight bounds on the smallest eigenvalue of NTK matrices for deep ReLU nets, both in the limiting case of infinite widths and for finite widths. In the finite-width setting, the network architectures we consider are fairly general: we require the existence of a wide layer with roughly order of \$N\$ neurons, \$N\$ being the number of data samples; and the scaling of the remaining layer widths is arbitrary (up to logarithmic factors). To obtain our results, we analyze various quantities of independent interest: we give lower bounds on the smallest singular value of hidden feature matrices, and upper bounds on the Lipschitz constant of input-output feature maps.},
	urldate = {2021-08-14},
	journal = {arXiv:2012.11654 [cs, stat]},
	author = {Nguyen, Quynh and Mondelli, Marco and Montufar, Guido},
	month = jun,
	year = {2021},
	note = {arXiv: 2012.11654},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7PCLE465/Nguyen et al. - 2021 - Tight Bounds on the Smallest Eigenvalue of the Neu.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/56P4MZM8/2012.html:text/html},
}

@article{belabbas_spectral_2009,
	title = {Spectral methods in machine learning and new strategies for very large datasets},
	volume = {106},
	copyright = {© 2009 by The National Academy of Sciences of the USA},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/106/2/369},
	doi = {10.1073/pnas.0810600105},
	abstract = {Spectral methods are of fundamental importance in statistics and machine learning, because they underlie algorithms from classical principal components analysis to more recent approaches that exploit manifold structure. In most cases, the core technical problem can be reduced to computing a low-rank approximation to a positive-definite kernel. For the growing number of applications dealing with very large or high-dimensional datasets, however, the optimal approximation afforded by an exact spectral decomposition is too costly, because its complexity scales as the cube of either the number of training examples or their dimensionality. Motivated by such applications, we present here 2 new algorithms for the approximation of positive-semidefinite kernels, together with error bounds that improve on results in the literature. We approach this problem by seeking to determine, in an efficient manner, the most informative subset of our data relative to the kernel approximation task at hand. This leads to two new strategies based on the Nyström method that are directly applicable to massive datasets. The first of these—based on sampling—leads to a randomized algorithm whereupon the kernel induces a probability distribution on its set of partitions, whereas the latter approach—based on sorting—provides for the selection of a partition in a deterministic way. We detail their numerical implementation and provide simulation results for a variety of representative problems in statistical data analysis, each of which demonstrates the improved performance of our approach relative to existing methods.},
	language = {en},
	number = {2},
	urldate = {2021-08-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belabbas, Mohamed-Ali and Wolfe, Patrick J.},
	month = jan,
	year = {2009},
	pmid = {19129490},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {kernel methods, low-rank approximation, statistical data analysis},
	pages = {369--374},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/VF6FKLUV/Belabbas and Wolfe - 2009 - Spectral methods in machine learning and new strat.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/VEVBBUKU/369.html:text/html},
}

@inproceedings{ferrari-trecate_finite-dimensional_1999,
	title = {Finite-{Dimensional} {Approximation} of {Gaussian} {Processes}},
	volume = {11},
	url = {https://papers.nips.cc/paper/1998/hash/55c567fd4395ecef6d936cf77b8d5b2b-Abstract.html},
	urldate = {2021-08-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ferrari-Trecate, Giancarlo and Williams, Christopher and Opper, Manfred},
	year = {1999},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/H8BN7E6I/Ferrari-Trecate et al. - 1999 - Finite-Dimensional Approximation of Gaussian Proce.pdf:application/pdf},
}

@inproceedings{van_de_ven_class-incremental_2021,
	title = {Class-{Incremental} {Learning} {With} {Generative} {Classifiers}},
	url = {https://openaccess.thecvf.com/content/CVPR2021W/CLVision/html/van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.html},
	language = {en},
	urldate = {2021-08-11},
	author = {van de Ven, Gido M. and Li, Zhe and Tolias, Andreas S.},
	year = {2021},
	pages = {3611--3620},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/BFEDDDLN/van de Ven et al. - 2021 - Class-Incremental Learning With Generative Classif.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/EGLCLXA7/van_de_Ven_Class-Incremental_Learning_With_Generative_Classifiers_CVPRW_2021_paper.html:text/html},
}

@inproceedings{kumar_bayesian_2021,
	title = {Bayesian {Structural} {Adaptation} for {Continual} {Learning}},
	url = {http://proceedings.mlr.press/v139/kumar21a.html},
	language = {en},
	urldate = {2021-08-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kumar, Abhishek and Chatterjee, Sunabha and Rai, Piyush},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5850--5860},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KESAJKI4/Kumar et al. - 2021 - Bayesian Structural Adaptation for Continual Learn.pdf:application/pdf},
}

@incollection{boutsidis_online_2014,
	series = {Proceedings},
	title = {Online {Principal} {Components} {Analysis}},
	isbn = {978-1-61197-374-7},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611973730.61},
	abstract = {We consider the online version of the well known Principal Component Analysis (PCA) problem. In standard PCA, the input to the problem is a set of d-dimensional vectors X = [x1, …, xn] and a target dimension k {\textless} d; the output is a set of k-dimensional vectors Y = [y1, …, yn] that minimize the reconstruction error: . Here, Φ ∊ ℝd × k is restricted to being isometric. The global minimum of this quantity, OPTk, is obtainable by offline PCA.

In online PCA (OPCA) the setting is identical except for two differences: i) the vectors xt are presented to the algorithm one by one and for every presented xt the algorithm must output a vector yt before receiving xt+1; ii) the output vectors yt are ℓ dimensional with ℓ ≥ k to compensate for the handicap of operating online. To the best of our knowledge, this paper is the first to consider this setting of OPCA. Our algorithm produces yt ∊ ℝℓ with ℓ = O(k · poly(1/ε)) such that .},
	urldate = {2021-08-07},
	booktitle = {Proceedings of the 2015 {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms} ({SODA})},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Boutsidis, Christos and Garber, Dan and Karnin, Zohar and Liberty, Edo},
	month = dec,
	year = {2014},
	doi = {10.1137/1.9781611973730.61},
	pages = {887--901},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/RK5UYXYJ/Boutsidis et al. - 2014 - Online Principal Components Analysis.pdf:application/pdf},
}

@article{warmuth_randomized_nodate,
	title = {Randomized {Online} {PCA} {Algorithms} with {Regret} {Bounds} that are {Logarithmic} in the {Dimension}},
	abstract = {We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic.},
	language = {en},
	author = {Warmuth, Manfred K and Kuzmin, Dima},
	pages = {34},
	file = {Warmuth and Kuzmin - Randomized Online PCA Algorithms with Regret Bound.pdf:/Users/tokio/Zotero/storage/5NH8VII4/Warmuth and Kuzmin - Randomized Online PCA Algorithms with Regret Bound.pdf:application/pdf},
}

@inproceedings{kuzmin_online_2007,
	address = {Corvalis, Oregon},
	title = {Online kernel {PCA} with entropic matrix updates},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273555},
	doi = {10.1145/1273496.1273555},
	abstract = {A number of updates for density matrices have been developed recently that are motivated by relative entropy minimization problems. The updates involve a softmin calculation based on matrix logs and matrix exponentials. We show that these updates can be kernelized. This is important because the bounds provable for these algorithms are logarithmic in the feature dimension (provided that the 2-norm of feature vectors is bounded by a constant). The main problem we focus on is the kernelization of an online PCA algorithm which belongs to this family of updates.},
	language = {en},
	urldate = {2021-08-06},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning - {ICML} '07},
	publisher = {ACM Press},
	author = {Kuzmin, Dima and Warmuth, Manfred K.},
	year = {2007},
	pages = {465--472},
	file = {Kuzmin and Warmuth - 2007 - Online kernel PCA with entropic matrix updates.pdf:/Users/tokio/Zotero/storage/IQZHAZ6R/Kuzmin and Warmuth - 2007 - Online kernel PCA with entropic matrix updates.pdf:application/pdf},
}

@article{fezai_online_2018,
	title = {Online reduced kernel principal component analysis for process monitoring},
	volume = {61},
	issn = {09591524},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959152417302007},
	doi = {10.1016/j.jprocont.2017.10.010},
	language = {en},
	urldate = {2021-08-06},
	journal = {Journal of Process Control},
	author = {Fezai, Radhia and Mansouri, Majdi and Taouali, Okba and Harkat, Mohamed Faouzi and Bouguila, Nasreddine},
	month = jan,
	year = {2018},
	pages = {1--11},
	file = {Fezai et al. - 2018 - Online reduced kernel principal component analysis.pdf:/Users/tokio/Zotero/storage/NPCVENV3/Fezai et al. - 2018 - Online reduced kernel principal component analysis.pdf:application/pdf},
}

@article{martens_new_2020,
	title = {New insights and perspectives on the natural gradient method},
	url = {http://arxiv.org/abs/1412.1193},
	abstract = {Natural gradient descent is an optimization method traditionally motivated from the perspective of information geometry, and works well for many applications as an alternative to stochastic gradient descent. In this paper we critically analyze this method and its properties, and show how it can be viewed as a type of 2nd-order optimization method, with the Fisher information matrix acting as a substitute for the Hessian. In many important cases, the Fisher information matrix is shown to be equivalent to the Generalized Gauss-Newton matrix, which both approximates the Hessian, but also has certain properties that favor its use over the Hessian. This perspective turns out to have significant implications for the design of a practical and robust natural gradient optimizer, as it motivates the use of techniques like trust regions and Tikhonov regularization. Additionally, we make a series of contributions to the understanding of natural gradient and 2nd-order methods, including: a thorough analysis of the convergence speed of stochastic natural gradient descent (and more general stochastic 2nd-order methods) as applied to convex quadratics, a critical examination of the oft-used "empirical" approximation of the Fisher matrix, and an analysis of the (approximate) parameterization invariance property possessed by natural gradient methods (which we show also holds for certain other curvature, but notably not the Hessian).},
	urldate = {2021-07-24},
	journal = {arXiv:1412.1193 [cs, stat]},
	author = {Martens, James},
	month = sep,
	year = {2020},
	note = {arXiv: 1412.1193},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/PYGWV5TD/Martens - 2020 - New insights and perspectives on the natural gradi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/A3WLXPQD/1412.html:text/html},
}

@article{martyn_grand_2021,
	title = {A {Grand} {Unification} of {Quantum} {Algorithms}},
	url = {http://arxiv.org/abs/2105.02859},
	abstract = {Quantum algorithms offer significant speedups over their classical counterparts for a variety of problems. The strongest arguments for this advantage are borne by algorithms for quantum search, quantum phase estimation, and Hamiltonian simulation, which appear as subroutines for large families of composite quantum algorithms. A number of these quantum algorithms were recently tied together by a novel technique known as the quantum singular value transformation (QSVT), which enables one to perform a polynomial transformation of the singular values of a linear operator embedded in a unitary matrix. In the seminal GSLW'19 paper on QSVT [Gily{\textbackslash}'en, Su, Low, and Wiebe, ACM STOC 2019], many algorithms are encompassed, including amplitude amplification, methods for the quantum linear systems problem, and quantum simulation. Here, we provide a pedagogical tutorial through these developments, first illustrating how quantum signal processing may be generalized to the quantum eigenvalue transform, from which QSVT naturally emerges. Paralleling GSLW'19, we then employ QSVT to construct intuitive quantum algorithms for search, phase estimation, and Hamiltonian simulation, and also showcase algorithms for the eigenvalue threshold problem and matrix inversion. This overview illustrates how QSVT is a single framework comprising the three major quantum algorithms, thus suggesting a grand unification of quantum algorithms.},
	urldate = {2021-07-19},
	journal = {arXiv:2105.02859 [quant-ph]},
	author = {Martyn, John M. and Rossi, Zane M. and Tan, Andrew K. and Chuang, Isaac L.},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02859},
	keywords = {Quantum Physics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IXTXRH4Q/Martyn et al. - 2021 - A Grand Unification of Quantum Algorithms.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SUSBC3EU/2105.html:text/html},
}

@article{matthews_sparse_2015,
	title = {On {Sparse} variational methods and the {Kullback}-{Leibler} divergence between stochastic processes},
	url = {http://arxiv.org/abs/1504.07027},
	abstract = {The variational framework for learning inducing variables (Titsias, 2009a) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximating and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. In this paper we give a substantial generalization of the literature on this topic. We give a new proof of the result for infinite index sets which allows inducing points that are not data points and likelihoods that depend on all function values. We then discuss augmented index sets and show that, contrary to previous works, marginal consistency of augmentation is not enough to guarantee consistency of variational inference with the original model. We then characterize an extra condition where such a guarantee is obtainable. Finally we show how our framework sheds light on interdomain sparse approximations and sparse approximations for Cox processes.},
	urldate = {2021-07-03},
	journal = {arXiv:1504.07027 [stat]},
	author = {Matthews, Alexander G. de G. and Hensman, James and Turner, Richard E. and Ghahramani, Zoubin},
	month = dec,
	year = {2015},
	note = {arXiv: 1504.07027},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VAK2BHJB/Matthews et al. - 2015 - On Sparse variational methods and the Kullback-Lei.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RQGYH9B7/1504.html:text/html},
}

@article{burt_rates_2019,
	title = {Rates of {Convergence} for {Sparse} {Variational} {Gaussian} {Process} {Regression}},
	url = {http://arxiv.org/abs/1903.03571},
	abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the \${\textbackslash}mathcal\{O\}{\textbackslash}left(N{\textasciicircum}3{\textbackslash}right)\$ scaling with dataset size \$N\$. They reduce the computational cost to \${\textbackslash}mathcal\{O\}{\textbackslash}left(NM{\textasciicircum}2{\textbackslash}right)\$, with \$M{\textbackslash}ll N\$ being the number of inducing variables, which summarise the process. While the computational cost seems to be linear in \$N\$, the true complexity of the algorithm depends on how \$M\$ must increase to ensure a certain quality of approximation. We address this by characterising the behavior of an upper bound on the KL divergence to the posterior. We show that with high probability the KL divergence can be made arbitrarily small by growing \$M\$ more slowly than \$N\$. A particular case of interest is that for regression with normally distributed inputs in D-dimensions with the popular Squared Exponential kernel, \$M={\textbackslash}mathcal\{O\}({\textbackslash}log{\textasciicircum}D N)\$ is sufficient. Our results show that as datasets grow, Gaussian process posteriors can truly be approximated cheaply, and provide a concrete rule for how to increase \$M\$ in continual learning scenarios.},
	urldate = {2021-06-30},
	journal = {arXiv:1903.03571 [cs, stat]},
	author = {Burt, David R. and Rasmussen, Carl E. and van der Wilk, Mark},
	month = sep,
	year = {2019},
	note = {arXiv: 1903.03571},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TXDQYLZ6/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SJ26UDY3/1903.html:text/html},
}

@article{zaidi_adaptive_2020,
	title = {Adaptive {Forgetting} {Curves} for {Spaced} {Repetition} {Language} {Learning}},
	url = {http://arxiv.org/abs/2004.11327},
	abstract = {The forgetting curve has been extensively explored by psychologists, educationalists and cognitive scientists alike. In the context of Intelligent Tutoring Systems, modelling the forgetting curve for each user and knowledge component (e.g. vocabulary word) should enable us to develop optimal revision strategies that counteract memory decay and ensure long-term retention. In this study we explore a variety of forgetting curve models incorporating psychological and linguistic features, and we use these models to predict the probability of word recall by learners of English as a second language. We evaluate the impact of the models and their features using data from an online vocabulary teaching platform and find that word complexity is a highly informative feature which may be successfully learned by a neural network model.},
	urldate = {2021-06-30},
	journal = {arXiv:2004.11327 [cs]},
	author = {Zaidi, Ahmed and Caines, Andrew and Moore, Russell and Buttery, Paula and Rice, Andrew},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11327},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XKUBDVZX/Zaidi et al. - 2020 - Adaptive Forgetting Curves for Spaced Repetition L.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/HUE42TEC/2004.html:text/html},
}

@article{tang_graph-based_2021,
	title = {Graph-{Based} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2007.04813},
	abstract = {Despite significant advances, continual learning models still suffer from catastrophic forgetting when exposed to incrementally available data from non-stationary distributions. Rehearsal approaches alleviate the problem by maintaining and replaying a small episodic memory of previous samples, often implemented as an array of independent memory slots. In this work, we propose to augment such an array with a learnable random graph that captures pairwise similarities between its samples, and use it not only to learn new tasks but also to guard against forgetting. Empirical results on several benchmark datasets show that our model consistently outperforms recently proposed baselines for task-free continual learning.},
	urldate = {2021-06-30},
	journal = {arXiv:2007.04813 [cs, stat]},
	author = {Tang, Binh and Matteson, David S.},
	month = feb,
	year = {2021},
	note = {arXiv: 2007.04813},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IW2PPHFI/Tang and Matteson - 2021 - Graph-Based Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NUDFQTX9/2007.html:text/html},
}

@article{nitanda_stochastic_2017,
	title = {Stochastic {Particle} {Gradient} {Descent} for {Infinite} {Ensembles}},
	url = {http://arxiv.org/abs/1712.05438},
	abstract = {The superior performance of ensemble methods with infinite models are well known. Most of these methods are based on optimization problems in infinite-dimensional spaces with some regularization, for instance, boosting methods and convex neural networks use \$L{\textasciicircum}1\$-regularization with the non-negative constraint. However, due to the difficulty of handling \$L{\textasciicircum}1\$-regularization, these problems require early stopping or a rough approximation to solve it inexactly. In this paper, we propose a new ensemble learning method that performs in a space of probability measures, that is, our method can handle the \$L{\textasciicircum}1\$-constraint and the non-negative constraint in a rigorous way. Such an optimization is realized by proposing a general purpose stochastic optimization method for learning probability measures via parameterization using transport maps on base models. As a result of running the method, a transport map to output an infinite ensemble is obtained, which forms a residual-type network. From the perspective of functional gradient methods, we give a convergence rate as fast as that of a stochastic optimization method for finite dimensional nonconvex problems. Moreover, we show an interior optimality property of a local optimality condition used in our analysis.},
	urldate = {2021-06-30},
	journal = {arXiv:1712.05438 [cs, math, stat]},
	author = {Nitanda, Atsushi and Suzuki, Taiji},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.05438},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/V27LUNEH/Nitanda and Suzuki - 2017 - Stochastic Particle Gradient Descent for Infinite .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KJEUXHSI/1712.html:text/html},
}

@article{hensman_scalable_nodate,
	title = {Scalable {Variational} {Gaussian} {Process} {Classiﬁcation}},
	abstract = {Gaussian process classiﬁcation is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, outperforming the state of the art on benchmark datasets. Importantly, the variational formulation can be exploited to allow classiﬁcation in problems with millions of data points, as we demonstrate in experiments.},
	language = {en},
	author = {Hensman, James and Ghahramani, Zoubin},
	pages = {10},
	file = {Hensman and Ghahramani - Scalable Variational Gaussian Process Classiﬁcatio.pdf:/Users/tokio/Zotero/storage/WQCFVZ4J/Hensman and Ghahramani - Scalable Variational Gaussian Process Classiﬁcatio.pdf:application/pdf},
}

@article{titsias_variational_nodate,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are deﬁned to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis K},
	pages = {8},
	file = {Titsias - Variational Learning of Inducing Variables in Spar.pdf:/Users/tokio/Zotero/storage/3AP8GL2X/Titsias - Variational Learning of Inducing Variables in Spar.pdf:application/pdf},
}

@article{hensman_gaussian_2013,
	title = {Gaussian {Processes} for {Big} {Data}},
	url = {http://arxiv.org/abs/1309.6835},
	abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be vari- ationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our ap- proach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
	urldate = {2021-06-15},
	journal = {arXiv:1309.6835 [cs, stat]},
	author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D.},
	month = sep,
	year = {2013},
	note = {arXiv: 1309.6835},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4AMY5Y84/Hensman et al. - 2013 - Gaussian Processes for Big Data.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IY9ZZKNS/1309.html:text/html},
}

@article{hensman_variational_2017,
	title = {Variational {Fourier} features for {Gaussian} processes},
	url = {http://arxiv.org/abs/1611.06740},
	abstract = {This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Matern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the dataset, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(NM2) (for a sparse Gaussian process) to O(NM) per iteration, where N is the number of data and M is the number of features.},
	urldate = {2021-06-15},
	journal = {arXiv:1611.06740 [stat]},
	author = {Hensman, James and Durrande, Nicolas and Solin, Arno},
	month = nov,
	year = {2017},
	note = {arXiv: 1611.06740},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MK7HCJ8L/Hensman et al. - 2017 - Variational Fourier features for Gaussian processe.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F3YCH3TB/1611.html:text/html},
}

@article{liao_approximate_2018,
	title = {Approximate {Fisher} {Information} {Matrix} to {Characterise} the {Training} of {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1810.06767},
	abstract = {In this paper, we introduce a novel methodology for characterising the performance of deep learning networks (ResNets and DenseNet) with respect to training convergence and generalisation as a function of mini-batch size and learning rate for image classification. This methodology is based on novel measurements derived from the eigenvalues of the approximate Fisher information matrix, which can be efficiently computed even for high capacity deep models. Our proposed measurements can help practitioners to monitor and control the training process (by actively tuning the mini-batch size and learning rate) to allow for good training convergence and generalisation. Furthermore, the proposed measurements also allow us to show that it is possible to optimise the training process with a new dynamic sampling training approach that continuously and automatically change the mini-batch size and learning rate during the training process. Finally, we show that the proposed dynamic sampling training approach has a faster training time and a competitive classification accuracy compared to the current state of the art.},
	urldate = {2021-06-14},
	journal = {arXiv:1810.06767 [cs]},
	author = {Liao, Zhibin and Drummond, Tom and Reid, Ian and Carneiro, Gustavo},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.06767},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TV6RHCIR/Liao et al. - 2018 - Approximate Fisher Information Matrix to Character.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IVXMRH2H/1810.html:text/html},
}

@article{titsias_functional_2020,
	title = {Functional {Regularisation} for {Continual} {Learning} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1901.11356},
	abstract = {We introduce a framework for Continual Learning (CL) based on Bayesian inference over the function space rather than the parameters of a deep neural network. This method, referred to as functional regularisation for Continual Learning, avoids forgetting a previous task by constructing and memorising an approximate posterior belief over the underlying task-specific function. To achieve this we rely on a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Then, the training algorithm sequentially encounters tasks and constructs posterior beliefs over the task-specific functions by using inducing point sparse Gaussian process methods. At each step a new task is first learnt and then a summary is constructed consisting of (i) inducing inputs -- a fixed-size subset of the task inputs selected such that it optimally represents the task -- and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms. Our method thus unites approaches focused on (pseudo-)rehearsal with those derived from a sequential Bayesian inference perspective in a principled way, leading to strong results on accepted benchmarks.},
	urldate = {2021-06-14},
	journal = {arXiv:1901.11356 [cs, stat]},
	author = {Titsias, Michalis K. and Schwarz, Jonathan and Matthews, Alexander G. de G. and Pascanu, Razvan and Teh, Yee Whye},
	month = feb,
	year = {2020},
	note = {arXiv: 1901.11356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NJAE4UKW/Titsias et al. - 2020 - Functional Regularisation for Continual Learning w.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FJR5WAD8/1901.html:text/html},
}

@article{leibfried_tutorial_2021,
	title = {A {Tutorial} on {Sparse} {Gaussian} {Processes} and {Variational} {Inference}},
	url = {http://arxiv.org/abs/2012.13962},
	abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as interdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
	urldate = {2021-06-13},
	journal = {arXiv:2012.13962 [cs, stat]},
	author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
	month = jun,
	year = {2021},
	note = {arXiv: 2012.13962},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QG57UVQ6/Leibfried et al. - 2021 - A Tutorial on Sparse Gaussian Processes and Variat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/H3EP53BY/2012.html:text/html},
}

@article{ridgeway_learning_2018,
	title = {Learning {Deep} {Disentangled} {Embeddings} with the {F}-{Statistic} {Loss}},
	url = {http://arxiv.org/abs/1802.05312},
	abstract = {Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the \$F\$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@\$k\$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.},
	urldate = {2021-06-13},
	journal = {arXiv:1802.05312 [cs, stat]},
	author = {Ridgeway, Karl and Mozer, Michael C.},
	month = may,
	year = {2018},
	note = {arXiv: 1802.05312},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GSL6R8VF/Ridgeway and Mozer - 2018 - Learning Deep Disentangled Embeddings with the F-S.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/N4BR9UXM/1802.html:text/html},
}

@article{wen_beneficial_2021,
	title = {Beneficial {Perturbation} {Network} for {Designing} {General} {Adaptive} {Artificial} {Intelligence} {Systems}},
	issn = {2162-237X, 2162-2388},
	url = {https://ieeexplore.ieee.org/document/9356334/},
	doi = {10.1109/TNNLS.2021.3054423},
	abstract = {The human brain is the gold standard of adaptive learning. It not only can learn and beneﬁt from experience, but also can adapt to new situations. In contrast, deep neural networks only learn one sophisticated but ﬁxed mapping from inputs to outputs. This limits their applicability to more dynamic situations, where the input to output mapping may change with different contexts. A salient example is continual learninglearning new independent tasks sequentially without forgetting previous tasks. Continual learning of multiple tasks in artiﬁcial neural networks using gradient descent leads to catastrophic forgetting, whereby a previously learned mapping of an old task is erased when learning new mappings for new tasks. Herein, we propose a new biologically plausible type of deep neural network with extra, out-of-network, task-dependent biasing units to accommodate these dynamic situations. This allows, for the ﬁrst time, a single network to learn potentially unlimited parallel input to output mappings, and to switch on the ﬂy between them at runtime. Biasing units are programed by leveraging beneﬁcial perturbations (opposite to well-known adversarial perturbations) for each task. Beneﬁcial perturbations for a given task bias the network toward that task, essentially switching the network into a different mode to process that task. This largely eliminates catastrophic interference between tasks. Our approach is memory-efﬁcient and parameter-efﬁcient, can accommodate many tasks, and achieves the state-of-the-art performance across different tasks and domains.},
	language = {en},
	urldate = {2021-06-13},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wen, Shixian and Rios, Amanda and Ge, Yunhao and Itti, Laurent},
	year = {2021},
	pages = {1--14},
	file = {Wen et al. - 2021 - Beneficial Perturbation Network for Designing Gene.pdf:/Users/tokio/Zotero/storage/GSGS9ZR7/Wen et al. - 2021 - Beneficial Perturbation Network for Designing Gene.pdf:application/pdf},
}

@article{eastwood_framework_2018,
	title = {A {FRAMEWORK} {FOR} {THE} {QUANTITATIVE} {EVALUATION} {OF} {DISENTANGLED} {REPRESENTATIONS}},
	abstract = {Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent deﬁnitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly deﬁned and quantiﬁed to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by recent state-of-the-art models.},
	language = {en},
	author = {Eastwood, Cian and Williams, Christopher K I},
	year = {2018},
	pages = {15},
	file = {Eastwood and Williams - 2018 - A FRAMEWORK FOR THE QUANTITATIVE EVALUATION OF DIS.pdf:/Users/tokio/Zotero/storage/G3IYUBHF/Eastwood and Williams - 2018 - A FRAMEWORK FOR THE QUANTITATIVE EVALUATION OF DIS.pdf:application/pdf},
}

@misc{noauthor_beneficial_nodate,
	title = {Beneficial {Perturbation} {Network} for {Designing} {General} {Adaptive} {Artificial} {Intelligence} {Systems}},
	url = {https://gateway2.itc.u-tokyo.ac.jp:11028/document/9356334/},
	abstract = {The human brain is the gold standard of adaptive learning. It not only can learn and benefit from experience, but also can adapt to new situations. In contrast, deep neural networks only learn one sophisticated but fixed mapping from inputs to outputs. This limits their applicability to more dynamic situations, where the input to output mapping may change with different contexts. A salient example is continual learning-learning new independent tasks sequentially without forgetting previous tasks. Continual learning of multiple tasks in artificial neural networks using gradient descent leads to catastrophic forgetting, whereby a previously learned mapping of an old task is erased when learning new mappings for new tasks. Herein, we propose a new biologically plausible type of deep neural network with extra, out-of-network, task-dependent biasing units to accommodate these dynamic situations. This allows, for the first time, a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly between them at runtime. Biasing units are programed by leveraging beneficial perturbations (opposite to well-known adversarial perturbations) for each task. Beneficial perturbations for a given task bias the network toward that task, essentially switching the network into a different mode to process that task. This largely eliminates catastrophic interference between tasks. Our approach is memory-efficient and parameter-efficient, can accommodate many tasks, and achieves the state-of-the-art performance across different tasks and domains.},
	language = {en-US},
	urldate = {2021-06-13},
}

@article{deng_non-convex_2021,
	title = {Non-convex {Learning} via {Replica} {Exchange} {Stochastic} {Gradient} {MCMC}},
	url = {http://arxiv.org/abs/2008.05367},
	abstract = {Replica exchange Monte Carlo (reMC), also known as parallel tempering, is an important technique for accelerating the convergence of the conventional Markov Chain Monte Carlo (MCMC) algorithms. However, such a method requires the evaluation of the energy function based on the full dataset and is not scalable to big data. The na{\textbackslash}"ive implementation of reMC in mini-batch settings introduces large biases, which cannot be directly extended to the stochastic gradient MCMC (SGMCMC), the standard sampling method for simulating from deep neural networks (DNNs). In this paper, we propose an adaptive replica exchange SGMCMC (reSGMCMC) to automatically correct the bias and study the corresponding properties. The analysis implies an acceleration-accuracy trade-off in the numerical discretization of a Markov jump process in a stochastic environment. Empirically, we test the algorithm through extensive experiments on various setups and obtain the state-of-the-art results on CIFAR10, CIFAR100, and SVHN in both supervised learning and semi-supervised learning tasks.},
	urldate = {2021-06-13},
	journal = {arXiv:2008.05367 [cs, math, stat]},
	author = {Deng, Wei and Feng, Qi and Gao, Liyao and Liang, Faming and Lin, Guang},
	month = mar,
	year = {2021},
	note = {arXiv: 2008.05367},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ERWJBU3Z/Deng et al. - 2021 - Non-convex Learning via Replica Exchange Stochasti.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/JNI6L4KP/2008.html:text/html},
}

@article{borsos_coresets_2020,
	title = {Coresets via {Bilevel} {Optimization} for {Continual} {Learning} and {Streaming}},
	url = {http://arxiv.org/abs/2006.03875},
	abstract = {Coresets are small data summaries that are sufficient for model training. They can be maintained online, enabling efficient handling of large data streams under resource constraints. However, existing constructions are limited to simple models such as k-means and logistic regression. In this work, we propose a novel coreset construction via cardinality-constrained bilevel optimization. We show how our framework can efficiently generate coresets for deep neural networks, and demonstrate its empirical benefits in continual learning and in streaming settings.},
	urldate = {2021-06-13},
	journal = {arXiv:2006.03875 [cs, stat]},
	author = {Borsos, Zalán and Mutný, Mojmír and Krause, Andreas},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.03875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4AEJWFF7/Borsos et al. - 2020 - Coresets via Bilevel Optimization for Continual Le.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/M47CLCNL/2006.html:text/html},
}

@article{yoon_federated_2020,
	title = {Federated {Continual} {Learning} with {Weighted} {Inter}-client {Transfer}},
	url = {http://arxiv.org/abs/2003.03196},
	abstract = {There has been a surge of interest in continual learning and federated learning, both of which are important in deep neural networks in real-world scenarios. Yet little research has been done regarding the scenario where each client learns on a sequence of tasks from a private local data stream. This problem of federated continual learning poses new challenges to continual learning, such as utilizing knowledge from other clients, while preventing interference from irrelevant knowledge. To resolve these issues, we propose a novel federated continual learning framework, Federated Weighted Inter-client Transfer (FedWeIT), which decomposes the network weights into global federated parameters and sparse task-specific parameters, and each client receives selective knowledge from other clients by taking a weighted combination of their task-specific parameters. FedWeIT minimizes interference between incompatible tasks, and also allows positive knowledge transfer across clients during learning. We validate our {\textbackslash}emph\{FedWeIT\}{\textasciitilde}against existing federated learning and continual learning methods under varying degrees of task similarity across clients, and our model significantly outperforms them with a large reduction in the communication cost.},
	urldate = {2021-06-13},
	journal = {arXiv:2003.03196 [cs, stat]},
	author = {Yoon, Jaehong and Jeong, Wonyong and Lee, Giwoong and Yang, Eunho and Hwang, Sung Ju},
	month = dec,
	year = {2020},
	note = {arXiv: 2003.03196},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NGVIFNN3/Yoon et al. - 2020 - Federated Continual Learning with Weighted Inter-c.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/VJPRVQC5/2003.html:text/html},
}

@article{kaplanis_continual_2020,
	title = {Continual reinforcement learning with memory at multiple timescales},
	copyright = {Creative Commons Attribution NonCommercial ShareAlike Licence},
	url = {http://spiral.imperial.ac.uk/handle/10044/1/82257},
	doi = {10.25560/82257},
	abstract = {In the past decade, with increased availability of computational resources and several improvements in training techniques, artificial neural networks (ANNs) have been rediscovered as a powerful class of machine learning methods, featuring in several groundbreaking applications of artificial intelligence. Most of these successes have been achieved in stationary, confined domains, such as a game playing and image recognition, but, ultimately, we want to apply artificial intelligence to problems that require it to interact with the real world, which is both vast and nonstationary. Unfortunately, ANNs have long been known to suffer from the phenomenon of catastrophic forgetting, whereby, in a setting where the data distribution is changing over time, new learning can lead to an abrupt erasure of previously acquired knowledge. The resurgence of ANNs has led to an increased urgency to solve this problem and endow them with the capacity for continual learning, which refers to the ability to build on their knowledge over time in environments that are constantly evolving. The most common setting for evaluating continual learning approaches to date has been in the context of training on a number of distinct tasks in sequence, and as a result many of them use the knowledge of task boundaries to consolidate knowledge during training. In the real world, however, the changes to the distribution may occur more gradually and at times that are not known in advance.  
 
The goal of this thesis has been to develop continual learning approaches that can cope with both discrete and continuous changes to the data distribution, without any prior knowledge of the nature or timescale of the changes. I present three new methods, all of which involve learning at multiple timescales, and evaluate them in the context of deep reinforcement learning, a paradigm that combines reinforcement learning with neural networks, which provides a natural testbed for continual learning as (i) it involves interacting with an environment, and (ii) it can feature non-stationarity at unpredictable timescales during training of a single task. The first method is inspired by the process of synaptic consolidation in the brain and involves multi-timescale memory at the level of the parameters of the network; the second extends the first by directly consolidating the agent's policy over time, rather than its individual parameters; finally, the third approach extends the experience replay database, which typically maintains a buffer of the agent's most recent experiences in order to decorrelate them during training, by enabling it to store data over multiple timescales.},
	language = {en-US-GB},
	urldate = {2021-06-13},
	author = {Kaplanis, Christos},
	month = apr,
	year = {2020},
	note = {Accepted: 2020-09-08T14:51:18Z
Publisher: Imperial College London},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/T4VAYRHN/Kaplanis - 2020 - Continual reinforcement learning with memory at mu.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/X7I6GDBQ/82257.html:text/html},
}

@article{xu_task-agnostic_2020,
	title = {Task-{Agnostic} {Online} {Reinforcement} {Learning} with an {Infinite} {Mixture} of {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2006.11441},
	abstract = {Continuously learning to solve unseen tasks with limited experience has been extensively pursued in meta-learning and continual learning, but with restricted assumptions such as accessible task distributions, independently and identically distributed tasks, and clear task delineations. However, real-world physical tasks frequently violate these assumptions, resulting in performance degradation. This paper proposes a continual online model-based reinforcement learning approach that does not require pre-training to solve task-agnostic problems with unknown task boundaries. We maintain a mixture of experts to handle nonstationarity, and represent each different type of dynamics with a Gaussian Process to efficiently leverage collected data and expressively model uncertainty. We propose a transition prior to account for the temporal dependencies in streaming data and update the mixture online via sequential variational inference. Our approach reliably handles the task distribution shift by generating new models for never-before-seen dynamics and reusing old models for previously seen dynamics. In experiments, our approach outperforms alternative methods in non-stationary tasks, including classic control with changing dynamics and decision making in different driving scenarios.},
	urldate = {2021-06-13},
	journal = {arXiv:2006.11441 [cs, stat]},
	author = {Xu, Mengdi and Ding, Wenhao and Zhu, Jiacheng and Liu, Zuxin and Chen, Baiming and Zhao, Ding},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.11441},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SAU7YNJE/Xu et al. - 2020 - Task-Agnostic Online Reinforcement Learning with a.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NC8XVZJP/2006.html:text/html},
}

@article{wu_comprehensive_2021,
	title = {A {Comprehensive} {Survey} on {Graph} {Neural} {Networks}},
	volume = {32},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1901.00596},
	doi = {10.1109/TNNLS.2020.2978386},
	abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
	number = {1},
	urldate = {2021-06-09},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
	month = jan,
	year = {2021},
	note = {arXiv: 1901.00596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {4--24},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VV5DZ4P4/Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/C3R9NKA9/1901.html:text/html},
}

@article{beltagy_longformer_2020,
	title = {Longformer: {The} {Long}-{Document} {Transformer}},
	shorttitle = {Longformer},
	url = {http://arxiv.org/abs/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
	urldate = {2021-06-09},
	journal = {arXiv:2004.05150 [cs]},
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.05150},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DI6D6BUS/Beltagy et al. - 2020 - Longformer The Long-Document Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/4524E4VS/2004.html:text/html},
}

@article{kemker_fearnet_2018,
	title = {{FearNet}: {Brain}-{Inspired} {Model} for {Incremental} {Learning}},
	shorttitle = {{FearNet}},
	url = {http://arxiv.org/abs/1711.10563},
	abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
	urldate = {2021-06-09},
	journal = {arXiv:1711.10563 [cs]},
	author = {Kemker, Ronald and Kanan, Christopher},
	month = feb,
	year = {2018},
	note = {arXiv: 1711.10563},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3KRPKXKB/Kemker and Kanan - 2018 - FearNet Brain-Inspired Model for Incremental Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TIGKK4TG/1711.html:text/html},
}

@inproceedings{oono_graph_2020,
	title = {Graph {Neural} {Networks} {Exponentially} {Lose} {Expressive} {Power} for {Node} {Classification}},
	url = {https://openreview.net/forum?id=S1ldO2EFPr},
	abstract = {Graph Neural Networks (graph NNs) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph NNs via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network (GCN), which is a popular graph NN variant, as a specific dynamical system. In the case of a GCN, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of GCNs with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of GCNs on the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph. We show that when the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph is sufficiently dense and large, a broad range of GCNs on it suffers from the ``information loss" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph NNs. We experimentally confirm that the proposed weight scaling enhances the predictive performance of GCNs in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.},
	language = {en},
	urldate = {2023-03-08},
	author = {Oono, Kenta and Suzuki, Taiji},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JEA9N6LJ/Oono and Suzuki - 2020 - Graph Neural Networks Exponentially Lose Expressiv.pdf:application/pdf},
}

@inproceedings{keriven_convergence_2020,
	title = {Convergence and {Stability} of {Graph} {Convolutional} {Networks} on {Large} {Random} {Graphs}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f5a14d4963acf488e3a24780a84ac96c-Abstract.html},
	abstract = {We study properties of Graph Convolutional Networks (GCNs) by analyzing their behavior on standard models of random graphs, where nodes are represented by random latent variables and edges are drawn according to a similarity kernel.  This allows us to overcome the difficulties of dealing with discrete notions such as isomorphisms on very large graphs, by considering instead more natural geometric aspects. We first study the convergence of GCNs to their continuous counterpart as the number of nodes grows. Our results are fully non-asymptotic and are valid for relatively sparse graphs with an average degree that grows logarithmically with the number of nodes. We then analyze the stability of GCNs to small deformations of the random graph model. In contrast to previous studies of stability in discrete settings, our continuous setup allows us to provide more intuitive deformation-based metrics for understanding stability, which have proven useful for explaining the success of convolutional representations on Euclidean domains.},
	urldate = {2023-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Keriven, Nicolas and Bietti, Alberto and Vaiter, Samuel},
	year = {2020},
	pages = {21512--21523},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZVQY5Y83/Keriven et al. - 2020 - Convergence and Stability of Graph Convolutional N.pdf:application/pdf},
}

@inproceedings{selsam_learning_2022,
	title = {Learning a {SAT} {Solver} from {Single}-{Bit} {Supervision}},
	url = {https://openreview.net/forum?id=HJMC_iA5tm},
	abstract = {We present NeuroSAT, a message passing neural network that learns to solve SAT problems after only being trained as a classifier to predict satisfiability. Although it is not competitive with state-of-the-art SAT solvers, NeuroSAT can solve problems that are substantially larger and more difficult than it ever saw during training by simply running for more iterations. Moreover, NeuroSAT generalizes to novel distributions; after training only on random SAT problems, at test time it can solve SAT problems encoding graph coloring, clique detection, dominating set, and vertex cover problems, all on a range of distributions over small random graphs.},
	language = {en},
	urldate = {2023-03-07},
	author = {Selsam, Daniel and Lamm, Matthew and B{\textbackslash}"\{u\}nz, Benedikt and Liang, Percy and Moura, Leonardo de and Dill, David L.},
	month = feb,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/QIVFQ25X/Selsam et al. - 2022 - Learning a SAT Solver from Single-Bit Supervision.pdf:application/pdf},
}

@inproceedings{esser_learning_2021,
	title = {Learning {Theory} {Can} ({Sometimes}) {Explain} {Generalisation} in {Graph} {Neural} {Networks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/e34376937c784505d9b4fcd980c2f1ce-Abstract.html},
	abstract = {In recent years, several results in the supervised learning setting suggested that classical statistical learning-theoretic measures, such as VC dimension, do not adequately explain the performance of deep learning models which prompted a slew of work in the infinite-width and iteration regimes. However, there is little theoretical explanation for the success of neural networks beyond the supervised setting. In this paper we argue that, under some distributional assumptions, classical learning-theoretic measures can sufficiently explain generalization for graph neural networks in the transductive setting. In particular, we provide a rigorous analysis of the performance of neural networks in the context of transductive inference, specifically by analysing the generalisation properties of graph convolutional networks for the problem of node classification. While VC-dimension does result in trivial generalisation error bounds in this setting as well, we show that transductive Rademacher complexity can explain the generalisation properties of graph convolutional networks for stochastic block models. We further use the generalisation error bounds based on transductive Rademacher complexity to demonstrate the role of graph convolutions and network architectures in achieving smaller generalisation error and provide insights into when the graph structure can help in learning. The findings of this paper could re-new the interest in studying generalisation in neural networks in terms of learning-theoretic measures, albeit in specific problems.},
	urldate = {2023-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Esser, Pascal and Chennuru Vankadara, Leena and Ghoshdastidar, Debarghya},
	year = {2021},
	pages = {27043--27056},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/DAJWBDMX/Esser et al. - 2021 - Learning Theory Can (Sometimes) Explain Generalisa.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KJ6VFZ9K/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CPZXSNV4/2106.html:text/html},
}

@misc{takase_lessons_2022,
	title = {Lessons on {Parameter} {Sharing} across {Layers} in {Transformers}},
	url = {http://arxiv.org/abs/2104.06022},
	doi = {10.48550/arXiv.2104.06022},
	abstract = {We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Takase, Sho and Kiyono, Shun},
	month = apr,
	year = {2022},
	note = {arXiv:2104.06022 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/BDZ2T2NM/Takase and Kiyono - 2022 - Lessons on Parameter Sharing across Layers in Tran.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/S2VGJ5RR/2104.html:text/html},
}

@misc{liu_understanding_2020,
	title = {Understanding the {Difficulty} of {Training} {Transformers}},
	url = {http://arxiv.org/abs/2004.08249},
	doi = {10.48550/arXiv.2004.08249},
	abstract = {Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding designing cutting-edge optimizers and learning rate schedulers carefully (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand \${\textbackslash}textit\{what complicates Transformer training\}\$ from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially -- for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (\${\textbackslash}textbf\{Ad\}\$aptive \${\textbackslash}textbf\{m\}\$odel \${\textbackslash}textbf\{in\}\$itialization) to stabilize stabilize the early stage's training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance. Implementations are released at: https://github.com/LiyuanLucasLiu/Transforemr-Clinic.},
	urldate = {2023-03-06},
	publisher = {arXiv},
	author = {Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
	month = sep,
	year = {2020},
	note = {arXiv:2004.08249 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/H4ZBNZZS/Liu et al. - 2020 - Understanding the Difficulty of Training Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/YX9JKNQF/2004.html:text/html},
}

@misc{verma_graphmix_2020,
	title = {{GraphMix}: {Improved} {Training} of {GNNs} for {Semi}-{Supervised} {Learning}},
	shorttitle = {{GraphMix}},
	url = {http://arxiv.org/abs/1909.11715},
	doi = {10.48550/arXiv.1909.11715},
	abstract = {We present GraphMix, a regularization method for Graph Neural Network based semi-supervised object classification, whereby we propose to train a fully-connected network jointly with the graph neural network via parameter sharing and interpolation-based regularization. Further, we provide a theoretical analysis of how GraphMix improves the generalization bounds of the underlying graph neural network, without making any assumptions about the "aggregation" layer or the depth of the graph neural networks. We experimentally validate this analysis by applying GraphMix to various architectures such as Graph Convolutional Networks, Graph Attention Networks and Graph-U-Net. Despite its simplicity, we demonstrate that GraphMix can consistently improve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets: Cora-Full, Co-author-CS and Co-author-Physics.},
	urldate = {2023-02-23},
	publisher = {arXiv},
	author = {Verma, Vikas and Qu, Meng and Kawaguchi, Kenji and Lamb, Alex and Bengio, Yoshua and Kannala, Juho and Tang, Jian},
	month = oct,
	year = {2020},
	note = {arXiv:1909.11715 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/E4592FW4/Verma et al. - 2020 - GraphMix Improved Training of GNNs for Semi-Super.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IL3GCU7A/1909.html:text/html},
}

@inproceedings{chamberlain_grand_2021,
	title = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle = {{GRAND}},
	url = {https://proceedings.mlr.press/v139/chamberlain21a.html},
	abstract = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
	language = {en},
	urldate = {2023-02-23},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chamberlain, Ben and Rowbottom, James and Gorinova, Maria I. and Bronstein, Michael and Webb, Stefan and Rossi, Emanuele},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {1407--1418},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/TF7CDR6C/Chamberlain et al. - 2021 - GRAND Graph Neural Diffusion.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/TQ9LLBUS/Chamberlain et al. - 2021 - GRAND Graph Neural Diffusion.pdf:application/pdf},
}

@inproceedings{bevilacqua_size-invariant_2021,
	title = {Size-{Invariant} {Graph} {Representations} for {Graph} {Classification} {Extrapolations}},
	url = {https://proceedings.mlr.press/v139/bevilacqua21a.html},
	abstract = {In general, graph representation learning methods assume that the train and test data come from the same distribution. In this work we consider an underexplored area of an otherwise rapidly developing field of graph representation learning: The task of out-of-distribution (OOD) graph classification, where train and test data have different distributions, with test data unavailable during training. Our work shows it is possible to use a causal model to learn approximately invariant representations that better extrapolate between train and test data. Finally, we conclude with synthetic and real-world dataset experiments showcasing the benefits of representations that are invariant to train/test distribution shifts.},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bevilacqua, Beatrice and Zhou, Yangze and Ribeiro, Bruno},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {837--851},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FNJDBJWM/Bevilacqua et al. - 2021 - Size-Invariant Graph Representations for Graph Cla.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/J9J77QYA/Bevilacqua et al. - 2021 - Size-Invariant Graph Representations for Graph Cla.pdf:application/pdf},
}

@inproceedings{han_g-mixup_2022,
	title = {G-{Mixup}: {Graph} {Data} {Augmentation} for {Graph} {Classification}},
	shorttitle = {G-{Mixup}},
	url = {https://proceedings.mlr.press/v162/han22c.html},
	abstract = {This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.},
	language = {en},
	urldate = {2023-02-22},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Hu, Xia},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {8230--8248},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PKXGBC2B/Han et al. - 2022 - G-Mixup Graph Data Augmentation for Graph Classif.pdf:application/pdf},
}

@inproceedings{neyshabur_pac-bayesian_2018,
	title = {A {PAC}-{Bayesian} {Approach} to {Spectrally}-{Normalized} {Margin} {Bounds} for {Neural} {Networks}},
	url = {https://openreview.net/forum?id=Skz_WfbCZ},
	abstract = {We present a generalization bound for feedforward neural networks in terms of the product of the spectral norm of the layers and the Frobenius norm of the weights. The generalization bound is derived using a PAC-Bayes analysis.},
	language = {en},
	urldate = {2023-02-17},
	author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
	month = feb,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/UG6QLVA9/Neyshabur et al. - 2018 - A PAC-Bayesian Approach to Spectrally-Normalized M.pdf:application/pdf},
}

@inproceedings{ma_subgroup_2021,
	title = {Subgroup {Generalization} and {Fairness} of {Graph} {Neural} {Networks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/08425b881bcde94a383cd258cea331be-Abstract.html},
	abstract = {Despite enormous successful applications of graph neural networks (GNNs), theoretical understanding of their generalization ability, especially for node-level tasks where data are not independent and identically-distributed (IID), has been sparse. The theoretical investigation of the generalization performance is beneficial for understanding fundamental issues (such as fairness) of GNN models and designing better learning methods. In this paper, we present a novel PAC-Bayesian analysis for GNNs under a non-IID semi-supervised learning setup. Moreover, we analyze the generalization performances on different subgroups of unlabeled nodes, which allows us to further study an accuracy-(dis)parity-style (un)fairness of GNNs from a theoretical perspective. Under reasonable assumptions, we demonstrate that the distance between a test subgroup and the training set can be a key factor affecting the GNN performance on that subgroup, which calls special attention to the training node selection for fair learning. Experiments across multiple GNN models and datasets support our theoretical results.},
	urldate = {2023-02-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ma, Jiaqi and Deng, Junwei and Mei, Qiaozhu},
	year = {2021},
	pages = {1048--1061},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Z8ZFQZ2B/Ma et al. - 2021 - Subgroup Generalization and Fairness of Graph Neur.pdf:application/pdf},
}

@misc{ju_generalization_2023,
	title = {Generalization in {Graph} {Neural} {Networks}: {Improved} {PAC}-{Bayesian} {Bounds} on {Graph} {Diffusion}},
	shorttitle = {Generalization in {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2302.04451},
	doi = {10.48550/arXiv.2302.04451},
	abstract = {Graph neural networks are widely used tools for graph prediction tasks. Motivated by their empirical performance, prior works have developed generalization bounds for graph neural networks, which scale with graph structures in terms of the maximum degree. In this paper, we present generalization bounds that instead scale with the largest singular value of the graph neural network's feature diffusion matrix. These bounds are numerically much smaller than prior bounds for real-world graphs. We also construct a lower bound of the generalization gap that matches our upper bound asymptotically. To achieve these results, we analyze a unified model that includes prior works' settings (i.e., convolutional and message-passing networks) and new settings (i.e., graph isomorphism networks). Our key idea is to measure the stability of graph neural networks against noise perturbations using Hessians. Empirically, we find that Hessian-based measurements correlate with the observed generalization gaps of graph neural networks accurately; Optimizing noise stability properties for fine-tuning pretrained graph neural networks also improves test performance on several graph-level classification tasks.},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Ju, Haotian and Li, Dongyue and Sharma, Aneesh and Zhang, Hongyang R.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04451 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/K6HZVFAS/Ju et al. - 2023 - Generalization in Graph Neural Networks Improved .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PIQRWM37/2302.html:text/html},
}

@inproceedings{liao_pac-bayesian_2022,
	title = {A {PAC}-{Bayesian} {Approach} to {Generalization} {Bounds} for {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=TR-Nj6nFx42},
	abstract = {In this paper, we derive generalization bounds for two primary classes of graph neural networks (GNNs), namely graph convolutional networks (GCNs) and message passing GNNs (MPGNNs), via a PAC-Bayesian approach. Our result reveals that the maximum node degree and the spectral norm of the weights govern the generalization bounds of both models. We also show that our bound for GCNs is a natural generalization of the results developed in {\textbackslash}citep\{neyshabur2017pac\} for fully-connected and convolutional neural networks. For MPGNNs, our PAC-Bayes bound improves over the Rademacher complexity based bound {\textbackslash}citep\{garg2020generalization\}, showing a tighter dependency on the maximum node degree and the maximum hidden dimension. The key ingredients of our proofs are a perturbation analysis of GNNs and the generalization of PAC-Bayes analysis to non-homogeneous GNNs. We perform an empirical study on several synthetic and real-world graph datasets and verify that our PAC-Bayes bound is tighter than others.},
	language = {en},
	urldate = {2023-02-17},
	author = {Liao, Renjie and Urtasun, Raquel and Zemel, Richard},
	month = feb,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/72V6KVQV/Liao et al. - 2022 - A PAC-Bayesian Approach to Generalization Bounds f.pdf:application/pdf},
}

@misc{arvind_weisfeiler-leman_2019,
	title = {On {Weisfeiler}-{Leman} {Invariance}: {Subgraph} {Counts} and {Related} {Graph} {Properties}},
	shorttitle = {On {Weisfeiler}-{Leman} {Invariance}},
	url = {http://arxiv.org/abs/1811.04801},
	doi = {10.48550/arXiv.1811.04801},
	abstract = {The \$k\$-dimensional Weisfeiler-Leman algorithm (\$k\$-WL) is a fruitful approach to the Graph Isomorphism problem. 2-WL corresponds to the original algorithm suggested by Weisfeiler and Leman over 50 years ago. 1-WL is the classical color refinement routine. Indistinguishability by \$k\$-WL is an equivalence relation on graphs that is of fundamental importance for isomorphism testing, descriptive complexity theory, and graph similarity testing which is also of some relevance in artificial intelligence. Focusing on dimensions \$k=1,2\$, we investigate subgraph patterns whose counts are \$k\$-WL invariant, and whose occurrence is \$k\$-WL invariant. We achieve a complete description of all such patterns for dimension \$k=1\$ and considerably extend the previous results known for \$k=2\$.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Arvind, V. and Fuhlbrück, Frank and Köbler, Johannes and Verbitsky, Oleg},
	month = apr,
	year = {2019},
	note = {arXiv:1811.04801 [cs]},
	keywords = {Computer Science - Discrete Mathematics, Computer Science - Computational Complexity, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WT82MTD9/Arvind et al. - 2019 - On Weisfeiler-Leman Invariance Subgraph Counts an.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LJIPQ67E/1811.html:text/html},
}

@inproceedings{chen_can_2020,
	title = {Can {Graph} {Neural} {Networks} {Count} {Substructures}?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/75877cb75154206c4e65e76b88a12712-Abstract.html},
	abstract = {The ability to detect and count certain substructures in graphs is important for solving many tasks on graph-structured data, especially in the contexts of computational chemistry and biology as well as social network analysis. Inspired by this, we propose to study the expressive power of graph neural networks (GNNs) via their ability to count attributed graph substructures, extending recent works that examine their power in graph isomorphism testing and function approximation. We distinguish between two types of substructure counting: induced-subgraph-count and subgraph-count, and establish both positive and negative answers for popular GNN architectures. Specifically, we prove that Message Passing Neural Networks (MPNNs), 2-Weisfeiler-Lehman (2-WL) and 2-Invariant Graph Networks (2-IGNs) cannot perform induced-subgraph-count of substructures consisting of 3 or more nodes, while they can perform subgraph-count of star-shaped substructures. As an intermediary step, we prove that 2-WL and 2-IGNs are equivalent in distinguishing non-isomorphic graphs, partly answering an open problem raised in Maron et al. (2019). We also prove positive results for k-WL and k-IGNs as well as negative results for k-WL with a finite number of iterations. We then conduct experiments that support the theoretical results for MPNNs and 2-IGNs. Moreover, motivated by substructure counting and inspired by Murphy et al. (2019), we propose the Local Relational Pooling model and demonstrate that it is not only effective for substructure counting but also able to achieve competitive performance on molecular prediction tasks.},
	urldate = {2023-02-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
	year = {2020},
	pages = {10383--10395},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MA4IA63Q/Chen et al. - 2020 - Can Graph Neural Networks Count Substructures.pdf:application/pdf},
}

@inproceedings{garg_generalization_2020,
	title = {Generalization and {Representational} {Limits} of {Graph} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v119/garg20c.html},
	abstract = {We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties, e.g., shortest/longest cycle, diameter, or certain motifs, cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.},
	language = {en},
	urldate = {2023-02-13},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Garg, Vikas and Jegelka, Stefanie and Jaakkola, Tommi},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3419--3430},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/EZ77BXC8/Garg et al. - 2020 - Generalization and Representational Limits of Grap.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/68UZKR46/Garg et al. - 2020 - Generalization and Representational Limits of Grap.pdf:application/pdf},
}

@inproceedings{thiede_autobahn_2021,
	title = {Autobahn: {Automorphism}-based {Graph} {Neural} {Nets}},
	volume = {34},
	shorttitle = {Autobahn},
	url = {https://proceedings.neurips.cc/paper/2021/hash/faf02b2358de8933f480a146f4d2d98e-Abstract.html},
	abstract = {We introduce Automorphism-based graph neural networks (Autobahn), a new family of graph neural networks. In an Autobahn, we decompose the graph into a collection of subgraphs and apply local convolutions that are equivariant to each subgraph's automorphism group. Specific choices of local neighborhoods and subgraphs recover existing architectures such as message passing neural networks. Our formalism also encompasses novel architectures: as an example, we introduce a graph neural network that decomposes the graph into paths and cycles. The resulting convolutions reflect the natural way that parts of the graph can transform, preserving the intuitive meaning of convolution without sacrificing global permutation equivariance. We validate our approach by applying Autobahn to molecular graphs, where it achieves results competitive with state-of-the-art message passing algorithms.},
	urldate = {2023-02-08},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Thiede, Erik and Zhou, Wenda and Kondor, Risi},
	year = {2021},
	pages = {29922--29934},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JIRP4MHQ/Thiede et al. - 2021 - Autobahn Automorphism-based Graph Neural Nets.pdf:application/pdf},
}

@inproceedings{topping_understanding_2022,
	title = {Understanding over-squashing and bottlenecks on graphs via curvature},
	url = {https://openreview.net/forum?id=7UmjRGzp-A},
	abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.},
	language = {en},
	urldate = {2023-02-08},
	author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZENUEWLM/Topping et al. - 2022 - Understanding over-squashing and bottlenecks on gr.pdf:application/pdf},
}

@inproceedings{satorras_en_2021,
	title = {E(n) {Equivariant} {Graph} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v139/satorras21a.html},
	abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
	language = {en},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Satorras, Vı́ctor Garcia and Hoogeboom, Emiel and Welling, Max},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9323--9332},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FDY82CT4/Satorras et al. - 2021 - E(n) Equivariant Graph Neural Networks.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/VIP8HL2V/Satorras et al. - 2021 - E(n) Equivariant Graph Neural Networks.pdf:application/pdf},
}

@article{boguna_network_2021,
	title = {Network {Geometry}},
	volume = {3},
	issn = {2522-5820},
	url = {http://arxiv.org/abs/2001.03241},
	doi = {10.1038/s42254-020-00264-4},
	abstract = {Real networks are finite metric spaces. Yet the geometry induced by shortest path distances in a network is definitely not its only geometry. Other forms of network geometry are the geometry of latent spaces underlying many networks, and the effective geometry induced by dynamical processes in networks. These three approaches to network geometry are all intimately related, and all three of them have been found to be exceptionally efficient in discovering fractality, scale-invariance, self-similarity, and other forms of fundamental symmetries in networks. Network geometry is also of great utility in a variety of practical applications, ranging from the understanding how the brain works, to routing in the Internet. Here, we review the most important theoretical and practical developments dealing with these approaches to network geometry in the last two decades, and offer perspectives on future research directions and challenges in this novel frontier in the study of complexity.},
	number = {2},
	urldate = {2023-02-08},
	journal = {Nature Reviews Physics},
	author = {Boguna, Marian and Bonamassa, Ivan and De Domenico, Manlio and Havlin, Shlomo and Krioukov, Dmitri and Serrano, M. Angeles},
	month = jan,
	year = {2021},
	note = {arXiv:2001.03241 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Computer Science - Social and Information Networks, Physics - Physics and Society},
	pages = {114--135},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QMLVRNFQ/Boguna et al. - 2021 - Network Geometry.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BJT6Y2U6/2001.html:text/html},
}

@article{vaart_information_2011,
	title = {Information {Rates} of {Nonparametric} {Gaussian} {Process} {Methods}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/vandervaart11a.html},
	abstract = {We consider the quality of learning a response function by a nonparametric Bayesian approach using a Gaussian process (GP) prior on the response function. We upper bound the quadratic risk of the learning procedure, which in turn is an upper bound on the Kullback-Leibler information between the predictive and true data distribution. The upper bound is expressed in small ball probabilities and concentration measures of the GP prior. We illustrate the computation of the upper bound for the Matérn and squared exponential kernels. For these priors the risk, and hence the information criterion, tends to zero for all continuous response functions. However, the rate at which this happens depends on the combination of true response function and Gaussian prior, and is expressible in a certain concentration function. In particular, the results show that for good performance, the regularity of the GP prior should match the regularity of the unknown response function.},
	number = {60},
	urldate = {2023-02-08},
	journal = {Journal of Machine Learning Research},
	author = {Vaart, Aad van der and Zanten, Harry van},
	year = {2011},
	pages = {2095--2119},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/57DL6NBB/Vaart and Zanten - 2011 - Information Rates of Nonparametric Gaussian Proces.pdf:application/pdf},
}

@misc{li_out--distribution_2022,
	title = {Out-{Of}-{Distribution} {Generalization} on {Graphs}: {A} {Survey}},
	shorttitle = {Out-{Of}-{Distribution} {Generalization} on {Graphs}},
	url = {http://arxiv.org/abs/2202.07987},
	doi = {10.48550/arXiv.2202.07987},
	abstract = {Graph machine learning has been extensively studied in both academia and industry. Although booming with a vast number of emerging methods and techniques, most of the literature is built on the in-distribution hypothesis, i.e., testing and training graph data are identically distributed. However, this in-distribution hypothesis can hardly be satisfied in many real-world graph scenarios where the model performance substantially degrades when there exist distribution shifts between testing and training graph data. To solve this critical problem, out-of-distribution (OOD) generalization on graphs, which goes beyond the in-distribution hypothesis, has made great progress and attracted ever-increasing attention from the research community. In this paper, we comprehensively survey OOD generalization on graphs and present a detailed review of recent advances in this area. First, we provide a formal problem definition of OOD generalization on graphs. Second, we categorize existing methods into three classes from conceptually different perspectives, i.e., data, model, and learning strategy, based on their positions in the graph machine learning pipeline, followed by detailed discussions for each category. We also review the theories related to OOD generalization on graphs and introduce the commonly used graph datasets for thorough evaluations. Finally, we share our insights on future research directions. This paper is the first systematic and comprehensive review of OOD generalization on graphs, to the best of our knowledge.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Li, Haoyang and Wang, Xin and Zhang, Ziwei and Zhu, Wenwu},
	month = dec,
	year = {2022},
	note = {arXiv:2202.07987 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KIP788U6/Li et al. - 2022 - Out-Of-Distribution Generalization on Graphs A Su.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/B2FLPKSZ/2202.html:text/html},
}

@misc{azangulov_stationary_2023,
	title = {Stationary {Kernels} and {Gaussian} {Processes} on {Lie} {Groups} and their {Homogeneous} {Spaces} {II}: non-compact symmetric spaces},
	shorttitle = {Stationary {Kernels} and {Gaussian} {Processes} on {Lie} {Groups} and their {Homogeneous} {Spaces} {II}},
	url = {http://arxiv.org/abs/2301.13088},
	doi = {10.48550/arXiv.2301.13088},
	abstract = {Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is split into two parts, each involving different technical considerations: part I studies compact spaces, while part II studies non-compact spaces possessing certain structure. Our contributions make the non-Euclidean Gaussian process models we study compatible with well-understood computational techniques available in standard Gaussian process software packages, thereby making them accessible to practitioners.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Azangulov, Iskander and Smolensky, Andrei and Terenin, Alexander and Borovitskiy, Viacheslav},
	month = jan,
	year = {2023},
	note = {arXiv:2301.13088 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MT7EQPGS/Azangulov et al. - 2023 - Stationary Kernels and Gaussian Processes on Lie G.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WS263W6T/2301.html:text/html},
}

@article{nikolentzos_graph_2022,
	title = {Graph {Kernels}: {A} {Survey}},
	volume = {72},
	issn = {1076-9757},
	shorttitle = {Graph {Kernels}},
	url = {https://doi.org/10.1613/jair.1.13225},
	doi = {10.1613/jair.1.13225},
	abstract = {Graph kernels have attracted a lot of attention during the last decade, and have evolved into a rapidly developing branch of learning on structured data. During the past 20 years, the considerable research activity that occurred in the field resulted in the development of dozens of graph kernels, each focusing on specific structural properties of graphs. Graph kernels have proven successful in a wide range of domains, ranging from social networks to bioinformatics. The goal of this survey is to provide a unifying view of the literature on graph kernels. In particular, we present a comprehensive overview of a wide range of graph kernels. Furthermore, we perform an experimental evaluation of several of those kernels on publicly available datasets, and provide a comparative study. Finally, we discuss key applications of graph kernels, and outline some challenges that remain to be addressed.},
	urldate = {2023-02-03},
	journal = {Journal of Artificial Intelligence Research},
	author = {Nikolentzos, Giannis and Siglidis, Giannis and Vazirgiannis, Michalis},
	month = jan,
	year = {2022},
	keywords = {data mining, machine learning},
	pages = {943--1027},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/E2BNALHA/Nikolentzos et al. - 2022 - Graph Kernels A Survey.pdf:application/pdf},
}

@inproceedings{xu_how_2023,
	title = {How {Powerful} are {Graph} {Neural} {Networks}?},
	url = {https://openreview.net/forum?id=ryGs6iA5Km},
	abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
	language = {en},
	urldate = {2023-02-03},
	author = {Xu*, Keyulu and Hu*, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	month = jan,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/4Z4T398F/Xu et al. - 2023 - How Powerful are Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{grunwald_pac-bayes_2021,
	title = {{PAC}-{Bayes}, {MAC}-{Bayes} and {Conditional} {Mutual} {Information}: {Fast} rate bounds that handle general {VC} classes},
	shorttitle = {{PAC}-{Bayes}, {MAC}-{Bayes} and {Conditional} {Mutual} {Information}},
	url = {https://proceedings.mlr.press/v134/grunwald21a.html},
	abstract = {We give a novel, unified derivation of conditional PAC-Bayesian and mutual information (MI) generalization bounds. We derive conditional MI bounds as an instance, with special choice of prior, of conditional MAC-Bayesian (Mean Approximately Correct) bounds, itself derived from conditional PAC-Bayesian bounds, where ‘conditional’ means that one can use priors conditioned on a joint training and ghost sample.  This allows us to get nontrivial PAC-Bayes and MI-style bounds for general VC classes, something recently shown to be impossible with standard PAC-Bayesian/MI bounds. Second, it allows us to get fast rates of order \$O(({\textbackslash}text\{KL\}/n){\textasciicircum}\{{\textbackslash}gamma\}\$ for \${\textbackslash}gamma {\textgreater} 1/2\$ if a Bernstein condition holds and for exp-concave losses (with \${\textbackslash}gamma=1\$), which is impossible with both standard PAC-Bayes generalization and MI bounds. Our work extends the recent work by Steinke and Zakynthinou (2020) who handle MI with VC but neither PAC-Bayes nor fast rates and Mhammedi et al. (2019) who initiated fast rate PAC-Bayes generalization error bounds but handle neither MI nor general VC classes.},
	language = {en},
	urldate = {2023-01-17},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Grünwald, Peter and Steinke, Thomas and Zakynthinou, Lydia},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2217--2247},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/4HWVWTCW/Grunwald et al. - 2021 - PAC-Bayes, MAC-Bayes and Conditional Mutual Inform.pdf:application/pdf},
}

@article{imaizumi_generalization_2023,
	title = {On generalization bounds for deep networks based on loss surface implicit regularization},
	volume = {69},
	issn = {0018-9448, 1557-9654},
	url = {http://arxiv.org/abs/2201.04545},
	doi = {10.1109/TIT.2022.3215088},
	abstract = {The classical statistical learning theory implies that fitting too many parameters leads to overfitting and poor performance. That modern deep neural networks generalize well despite a large number of parameters contradicts this finding and constitutes a major unsolved problem towards explaining the success of deep learning. While previous work focuses on the implicit regularization induced by stochastic gradient descent (SGD), we study here how the local geometry of the energy landscape around local minima affects the statistical properties of SGD with Gaussian gradient noise. We argue that under reasonable assumptions, the local geometry forces SGD to stay close to a low dimensional subspace and that this induces another form of implicit regularization and results in tighter bounds on the generalization error for deep neural networks. To derive generalization error bounds for neural networks, we first introduce a notion of stagnation sets around the local minima and impose a local essential convexity property of the population risk. Under these conditions, lower bounds for SGD to remain in these stagnation sets are derived. If stagnation occurs, we derive a bound on the generalization error of deep neural networks involving the spectral norms of the weight matrices but not the number of network parameters. Technically, our proofs are based on controlling the change of parameter values in the SGD iterates and local uniform convergence of the empirical loss functions based on the entropy of suitable neighborhoods around local minima.},
	number = {2},
	urldate = {2023-01-24},
	journal = {IEEE Transactions on Information Theory},
	author = {Imaizumi, Masaaki and Schmidt-Hieber, Johannes},
	month = feb,
	year = {2023},
	note = {arXiv:2201.04545 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1203--1223},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GRJ2GAH8/Imaizumi and Schmidt-Hieber - 2023 - On generalization bounds for deep networks based o.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/JNYRIU4L/2201.html:text/html},
}

@article{shervashidze_weisfeiler-lehman_2011,
	title = {Weisfeiler-{Lehman} {Graph} {Kernels}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/shervashidze11a.html},
	abstract = {In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.},
	number = {77},
	urldate = {2023-01-23},
	journal = {Journal of Machine Learning Research},
	author = {Shervashidze, Nino and Schweitzer, Pascal and Leeuwen, Erik Jan van and Mehlhorn, Kurt and Borgwardt, Karsten M.},
	year = {2011},
	pages = {2539--2561},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PS59EUWV/Shervashidze et al. - 2011 - Weisfeiler-Lehman Graph Kernels.pdf:application/pdf},
}

@article{brown_winning_nodate,
	title = {Winning {Agent} from the 2016 {Annual} {Computer} {Poker} {Competition}},
	abstract = {Imperfect-information games, where players have private information, pose a unique challenge in artiﬁcial intelligence. In recent years, Heads-Up NoLimit Texas Hold’em poker, a popular version of poker, has emerged as the primary benchmark for evaluating game-solving algorithms for imperfectinformation games. We demonstrate a winning agent from the 2016 Annual Computer Poker Competition, Baby Tartanian8.},
	language = {en},
	author = {Brown, Noam and Sandholm, Tuomas},
	file = {Brown and Sandholm - Winning Agent from the 2016 Annual Computer Poker .pdf:/Users/tokio/Zotero/storage/LVNHHVY3/Brown and Sandholm - Winning Agent from the 2016 Annual Computer Poker .pdf:application/pdf},
}

@inproceedings{lanctot_monte_2009,
	title = {Monte {Carlo} {Sampling} for {Regret} {Minimization} in {Extensive} {Games}},
	volume = {22},
	url = {https://papers.nips.cc/paper/2009/hash/00411460f7c92d2124a67ea0f4cb5f85-Abstract.html},
	abstract = {Sequential decision-making with multiple agents and imperfect information is commonly modeled as an extensive game.  One efficient method for computing Nash equilibria in large, zero-sum, imperfect information games is counterfactual regret minimization (CFR). In the domain of poker, CFR has proven effective, particularly when using a domain-specific augmentation involving chance outcome sampling.  In this paper, we describe a general family of domain independent CFR sample-based algorithms called Monte Carlo counterfactual regret minimization (MCCFR) of which the original and poker-specific versions are special cases. We start by showing that MCCFR performs the same regret updates as CFR on expectation. Then, we introduce two sampling schemes: \{{\textbackslash}it outcome sampling\} and \{{\textbackslash}it external sampling\}, showing that both have bounded overall regret with high  probability. Thus, they can compute an approximate equilibrium using self-play. Finally, we prove a new tighter bound on the regret for the original CFR algorithm and relate this new bound to MCCFRs bounds. We show empirically that, although the sample-based algorithms require more iterations, their lower cost per iteration can lead to dramatically faster convergence in various games.},
	urldate = {2023-01-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lanctot, Marc and Waugh, Kevin and Zinkevich, Martin and Bowling, Michael},
	year = {2009},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/9EQYXKG2/Lanctot et al. - 2009 - Monte Carlo Sampling for Regret Minimization in Ex.pdf:application/pdf},
}

@misc{brown_solving_2019,
	title = {Solving {Imperfect}-{Information} {Games} via {Discounted} {Regret} {Minimization}},
	url = {http://arxiv.org/abs/1809.04040},
	doi = {10.48550/arXiv.1809.04040},
	abstract = {Counterfactual regret minimization (CFR) is a family of iterative algorithms that are the most popular and, in practice, fastest approach to approximately solving large imperfect-information games. In this paper we introduce novel CFR variants that 1) discount regrets from earlier iterations in various ways (in some cases differently for positive and negative regrets), 2) reweight iterations in various ways to obtain the output strategies, 3) use a non-standard regret minimizer and/or 4) leverage "optimistic regret matching". They lead to dramatically improved performance in many settings. For one, we introduce a variant that outperforms CFR+, the prior state-of-the-art algorithm, in every game tested, including large-scale realistic settings. CFR+ is a formidable benchmark: no other algorithm has been able to outperform it. Finally, we show that, unlike CFR+, many of the important new variants are compatible with modern imperfect-information-game pruning techniques and one is also compatible with sampling in the game tree.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Brown, Noam and Sandholm, Tuomas},
	month = feb,
	year = {2019},
	note = {arXiv:1809.04040 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/B6EPJBS3/Brown and Sandholm - 2019 - Solving Imperfect-Information Games via Discounted.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/I8GRAC5X/1809.html:text/html},
}

@article{shen_molecular_2019,
	series = {Artificial {Intelligence}},
	title = {Molecular property prediction: recent trends in the era of artificial intelligence},
	volume = {32-33},
	issn = {1740-6749},
	shorttitle = {Molecular property prediction},
	url = {https://www.sciencedirect.com/science/article/pii/S1740674920300032},
	doi = {10.1016/j.ddtec.2020.05.001},
	abstract = {Artificial intelligence (AI) has become a powerful tool in many fields, including drug discovery. Among various AI applications, molecular property prediction can have more significant immediate impact to the drug discovery process since most algorithms and methods use predicted properties to evaluate, select, and generate molecules. Herein, we provide a brief review of the state-of-art molecular property prediction methodologies and discuss examples reported recently. We highlight key techniques that have been applied to molecular property prediction such as learned representation, multi-task learning, transfer learning, and federated learning. We also point out some critical but less discussed issues such as data set quality, benchmark, model performance evaluation, and prediction confidence quantification.},
	language = {en},
	urldate = {2023-01-22},
	journal = {Drug Discovery Today: Technologies},
	author = {Shen, Jie and Nicolaou, Christos A.},
	month = dec,
	year = {2019},
	pages = {29--36},
	file = {ScienceDirect Full Text PDF:/Users/tokio/Zotero/storage/2NUMJ5ZY/Shen and Nicolaou - 2019 - Molecular property prediction recent trends in th.pdf:application/pdf;ScienceDirect Snapshot:/Users/tokio/Zotero/storage/F94KZWX3/S1740674920300032.html:text/html},
}

@inproceedings{xu_information-theoretic_2017,
	title = {Information-theoretic analysis of generalization capability of learning algorithms},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html},
	abstract = {We derive upper bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and output. The bounds provide an information-theoretic understanding of generalization in learning problems, and give theoretical guidelines for striking the right balance between data fit and generalization by controlling the input-output mutual information.  We propose a number of methods for this purpose, among which are algorithms that regularize the ERM algorithm with relative entropy or with random noise. Our work extends and leads to nontrivial improvements on the recent results of Russo and Zou.},
	urldate = {2023-01-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Xu, Aolin and Raginsky, Maxim},
	year = {2017},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/8V952RRP/Xu and Raginsky - 2017 - Information-theoretic analysis of generalization c.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
	urldate = {2023-01-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MN4D7PNY/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf},
}

@article{erven_fast_2015,
	title = {Fast {Rates} in {Statistical} and {Online} {Learning}},
	volume = {16},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v16/vanerven15a.html},
	abstract = {The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning --- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for `proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.},
	number = {54},
	urldate = {2023-01-17},
	journal = {Journal of Machine Learning Research},
	author = {Erven, Tim van and Grünwald, Peter D. and Mehta, Nishant A. and Reid, Mark D. and Williamson, Robert C.},
	year = {2015},
	pages = {1793--1861},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ACFZ8TYS/Erven et al. - 2015 - Fast Rates in Statistical and Online Learning.pdf:application/pdf},
}

@article{perez-ortiz_tighter_2021,
	title = {Tighter {Risk} {Certificates} for {Neural} {Networks}},
	volume = {22},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v22/20-879.html},
	abstract = {This paper presents an empirical study regarding training probabilistic neural networks using training objectives derived from PAC-Bayes bounds. In the context of probabilistic neural networks, the output of training is a probability distribution over network weights. We present two training objectives, used here for the first time in connection with training neural networks. These two training objectives are derived from tight PAC-Bayes bounds. We also re-implement a previously used training objective based on a classical PAC-Bayes bound, to compare the properties of the predictors learned using the different training objectives. We compute risk certificates for the learnt predictors, based on part of the data used to learn the predictors. We further experiment with different types of priors on the weights (both data-free and data-dependent priors) and neural network architectures. Our experiments on MNIST and CIFAR-10 show that our training methods produce competitive test set errors and non-vacuous risk bounds with much tighter values than previous results in the literature, showing promise not only to guide the learning algorithm through bounding the risk but also for model selection. These observations suggest that the methods studied here might be good candidates for self-certified learning, in the sense of using the whole data set for learning a predictor and certifying its risk on any unseen data (from the same distribution as the training data) potentially without the need for holding out test data.},
	number = {227},
	urldate = {2023-01-17},
	journal = {Journal of Machine Learning Research},
	author = {Pérez-Ortiz, María and Rivasplata, Omar and Shawe-Taylor, John and Szepesvári, Csaba},
	year = {2021},
	pages = {1--40},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/S2DZNBTQ/Pérez-Ortiz et al. - 2021 - Tighter Risk Certificates for Neural Networks.pdf:application/pdf;Source Code:/Users/tokio/Zotero/storage/KV6N283N/PBB.html:text/html},
}

@inproceedings{mhammedi_pac-bayes_2019,
	title = {{PAC}-{Bayes} {Un}-{Expected} {Bernstein} {Inequality}},
	volume = {32},
	url = {https://papers.nips.cc/paper/2019/hash/3dea6b598a16b334a53145e78701fa87-Abstract.html},
	urldate = {2023-01-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mhammedi, Zakaria and Grünwald, Peter and Guedj, Benjamin},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/REDASHX5/Mhammedi et al. - 2019 - PAC-Bayes Un-Expected Bernstein Inequality.pdf:application/pdf},
}

@inproceedings{livni_limitation_2020,
	title = {A {Limitation} of the {PAC}-{Bayes} {Framework}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/ec79d4bed810ed64267d169b0d37373e-Abstract.html},
	urldate = {2023-01-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Livni, Roi and Moran, Shay},
	year = {2020},
	pages = {20543--20553},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/E72XBCXB/Livni and Moran - 2020 - A Limitation of the PAC-Bayes Framework.pdf:application/pdf},
}

@article{grunwald_fast_2022,
	title = {Fast rates for general unbounded loss functions: from {ERM} to generalized {Bayes}},
	volume = {21},
	issn = {1532-4435},
	shorttitle = {Fast rates for general unbounded loss functions},
	abstract = {We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to η-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate η is set correctly. For general loss functions, our bounds rely on two separate conditions: the v-GRIP (generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter v in the v-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the v-GRIP conditions generalize; favorable v in combination with small model complexity leads to Õ(1/n) rates. The witness condition allows us to connect the excess risk to an "annealed" version thereof, by which we generalize several previous results connecting Hellinger and Rényi divergence to KL divergence.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Grünwald, Peter D. and Mehta, Nishant A.},
	month = jun,
	year = {2022},
	keywords = {fast rates, generalized Bayes, misspecification, PAC-Bayes, statistical learning theory},
	pages = {56:2040--56:2119},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ANANXAU5/Grünwald and Mehta - 2022 - Fast rates for general unbounded loss functions f.pdf:application/pdf},
}

@article{dziugaite_computing_2017,
	title = {Computing {Nonvacuous} {Generalization} {Bounds} for {Deep} ({Stochastic}) {Neural} {Networks} with {Many} {More} {Parameters} than {Training} {Data}},
	url = {http://auai.org/uai2017/proceedings/papers/173.pdf},
	abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
	urldate = {2022-03-07},
	journal = {Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	month = oct,
	year = {2017},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DPVINPWQ/Dziugaite and Roy - 2017 - Computing Nonvacuous Generalization Bounds for Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RMW6NTRA/1703.html:text/html},
}

@article{catoni_pac-bayesian_2007,
	title = {Pac-{Bayesian} {Supervised} {Classification}: {The} {Thermodynamics} of {Statistical} {Learning}},
	volume = {56},
	shorttitle = {Pac-{Bayesian} {Supervised} {Classification}},
	url = {http://arxiv.org/abs/0712.0248},
	doi = {10.1214/074921707000000391},
	abstract = {This monograph deals with adaptive supervised classification, using tools borrowed from statistical mechanics and information theory, stemming from the PACBayesian approach pioneered by David McAllester and applied to a conception of statistical learning theory forged by Vladimir Vapnik. Using convex analysis on the set of posterior probability measures, we show how to get local measures of the complexity of the classification model involving the relative entropy of posterior distributions with respect to Gibbs posterior measures. We then discuss relative bounds, comparing the generalization error of two classification rules, showing how the margin assumption of Mammen and Tsybakov can be replaced with some empirical measure of the covariance structure of the classification model.We show how to associate to any posterior distribution an effective temperature relating it to the Gibbs prior distribution with the same level of expected error rate, and how to estimate this effective temperature from data, resulting in an estimator whose expected error rate converges according to the best possible power of the sample size adaptively under any margin and parametric complexity assumptions. We describe and study an alternative selection scheme based on relative bounds between estimators, and present a two step localization technique which can handle the selection of a parametric model from a family of those. We show how to extend systematically all the results obtained in the inductive setting to transductive learning, and use this to improve Vapnik's generalization bounds, extending them to the case when the sample is made of independent non-identically distributed pairs of patterns and labels. Finally we review briefly the construction of Support Vector Machines and show how to derive generalization bounds for them, measuring the complexity either through the number of support vectors or through the value of the transductive or inductive margin.},
	urldate = {2021-10-06},
	journal = {Institute of Mathematical Statistics Lecture Notes - Monograph Series},
	author = {Catoni, Olivier},
	month = dec,
	year = {2007},
	note = {arXiv: 0712.0248},
	keywords = {Statistics - Machine Learning, 62H30, 68T05, 62B10 (Primary)},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ABGCXELH/Catoni - 2007 - Pac-Bayesian Supervised Classification The Thermo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PJRV5XIR/0712.html:text/html},
}

@article{vapnik_uniform_1971,
	title = {On the {Uniform} {Convergence} of {Relative} {Frequencies} of {Events} to {Their} {Probabilities}},
	volume = {16},
	issn = {0040-585X},
	url = {https://epubs.siam.org/doi/10.1137/1116025},
	doi = {10.1137/1116025},
	number = {2},
	urldate = {2023-01-16},
	journal = {Theory of Probability \& Its Applications},
	author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
	month = jan,
	year = {1971},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {264--280},
}

@article{tsybakov_optimal_2004,
	title = {Optimal aggregation of classifiers in statistical learning},
	volume = {32},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-32/issue-1/Optimal-aggregation-of-classifiers-in-statistical-learning/10.1214/aos/1079120131.full},
	doi = {10.1214/aos/1079120131},
	abstract = {Classification can be considered as nonparametric estimation of sets, where the risk is defined by means of a specific distance between sets associated with misclassification error. It is shown that the rates of convergence of classifiers depend on two parameters: the complexity of the class of candidate sets and the margin parameter. The dependence is explicitly given, indicating that optimal fast rates approaching \$O(n{\textasciicircum}\{-1\})\$ can be attained, where n is the sample size, and that the proposed classifiers have the property of robustness to the margin. The main result of the paper concerns optimal aggregation of classifiers: we suggest a classifier that automatically adapts both to the complexity and to the margin, and attains the optimal fast rates, up to a logarithmic factor.},
	number = {1},
	urldate = {2023-01-16},
	journal = {The Annals of Statistics},
	author = {Tsybakov, Alexander B.},
	month = feb,
	year = {2004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G07, 62G08, 62H30, 68T10, aggregation of classifiers, ‎classification‎, complexity of classes of sets, Empirical processes, margins, Optimal rates, Statistical learning},
	pages = {135--166},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/BIAXI53X/Tsybakov - 2004 - Optimal aggregation of classifiers in statistical .pdf:application/pdf},
}

@inproceedings{bu_tightening_2019,
	title = {Tightening {Mutual} {Information} {Based} {Bounds} on {Generalization} {Error}},
	doi = {10.1109/ISIT.2019.8849590},
	abstract = {A mutual information based upper bound on the generalization error of a supervised learning algorithm is derived in this paper. The bound is constructed in terms of the mutual information between each individual training sample and the output of the learning algorithm, which requires weaker conditions on the loss function, but provides a tighter characterization of the generalization error than existing studies. Examples are further provided to demonstrate that the bound derived in this paper is tighter, and has a broader range of applicability. Application to noisy and iterative algorithms, e.g., stochastic gradient Langevin dynamics (SGLD), is also studied, where the constructed bound provides a tighter characterization of the generalization error than existing results.},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V.},
	month = jul,
	year = {2019},
	note = {ISSN: 2157-8117},
	keywords = {Mutual information, Random variables, Sociology, Supervised learning, Training, Training data, Upper bound},
	pages = {587--591},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/C9HRHIL6/8849590.html:text/html},
}

@inproceedings{szegedy_going_2015,
	title = {Going deeper with convolutions},
	doi = {10.1109/CVPR.2015.7298594},
	abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Computer architecture, Computer vision, Convolutional codes, Neural networks, Object detection, Sparse matrices, Visualization},
	pages = {1--9},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/AJPIWPDR/7298594.html:text/html;報告したバージョン:/Users/tokio/Zotero/storage/5SZ3D962/Szegedy et al. - 2015 - Going deeper with convolutions.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2023-01-16},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9PU3Y5TY/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RL48XPIF/2203.html:text/html},
}

@inproceedings{dosovitskiy_image_2022,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {https://openreview.net/forum?id=YicbFdNTTy},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	language = {en},
	urldate = {2023-01-16},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/6R5KVY4Q/Dosovitskiy et al. - 2022 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf},
}

@inproceedings{nachum_direct_2018,
	title = {A {Direct} {Sum} {Result} for the {Information} {Complexity} of {Learning}},
	url = {https://proceedings.mlr.press/v75/nachum18a.html},
	abstract = {How many bits of information are required to PAC learn a class of hypotheses of VC dimension ddd? The mathematical setting we follow is that of Bassily et al., where the value of interest is the mutual information I(S;A(S))I(S;A(S)){\textbackslash}mathrm\{I\}(S;A(S)) between the input sample SSS and the hypothesis outputted by the learning algorithm AAA. We introduce a class of functions of VC dimension ddd over the domain XX{\textbackslash}mathcal\{X\} with information complexity at least Ω(dloglog{\textbar}X{\textbar}d)Ω(dlog⁡log⁡{\textbar}X{\textbar}d){\textbackslash}Omega {\textbackslash}left(d{\textbackslash}log {\textbackslash}log {\textbackslash}frac\{{\textbar}{\textbackslash}mathcal\{X\}{\textbar}\}\{d\}{\textbackslash}right) bits for any consistent and proper algorithm (deterministic or random). Bassily et al. proved a similar (but quantitatively weaker) result for the case d=1d=1d=1. The above result is in fact a special case of a more general phenomenon we explore. We define the notion of \{{\textbackslash}em information complexity\} of a given class of functions {\textbackslash}cH{\textbackslash}cH{\textbackslash}cH. Intuitively, it is the minimum amount of information that an algorithm for XX{\textbackslash}mathcal\{X\} must retain about its input to ensure consistency and properness. We prove a direct sum result for information complexity in this context; roughly speaking, the information complexity sums when combining several classes.},
	language = {en},
	urldate = {2023-01-16},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Nachum, Ido and Shafer, Jonathan and Yehudayoff, Amir},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1547--1568},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JCTYG6IH/Nachum et al. - 2018 - A Direct Sum Result for the Information Complexity.pdf:application/pdf},
}

@article{bassily_learners_2017,
	title = {Learners that {Use} {Little} {Information}},
	url = {https://arxiv.org/abs/1710.05233v3},
	doi = {10.48550/arXiv.1710.05233},
	abstract = {We study learning algorithms that are restricted to using a small amount of information from their input sample. We introduce a category of learning algorithms we term \$d\$-bit information learners, which are algorithms whose output conveys at most \$d\$ bits of information of their input. A central theme in this work is that such algorithms generalize. We focus on the learning capacity of these algorithms, and prove sample complexity bounds with tight dependencies on the confidence and error parameters. We also observe connections with well studied notions such as sample compression schemes, Occam's razor, PAC-Bayes and differential privacy. We discuss an approach that allows us to prove upper bounds on the amount of information that algorithms reveal about their inputs, and also provide a lower bound by showing a simple concept class for which every (possibly randomized) empirical risk minimizer must reveal a lot of information. On the other hand, we show that in the distribution-dependent setting every VC class has empirical risk minimizers that do not reveal a lot of information.},
	language = {en},
	urldate = {2023-01-16},
	author = {Bassily, Raef and Moran, Shay and Nachum, Ido and Shafer, Jonathan and Yehudayoff, Amir},
	month = oct,
	year = {2017},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/HICYFIEJ/Bassily et al. - 2017 - Learners that Use Little Information.pdf:application/pdf},
}

@article{mammen_smooth_1999,
	title = {Smooth discrimination analysis},
	volume = {27},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-27/issue-6/Smooth-discrimination-analysis/10.1214/aos/1017939240.full},
	doi = {10.1214/aos/1017939240},
	abstract = {Discriminant analysis for two data sets in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ with probability densities \$f\$ and \$g\$ can be based on the estimation of the set \$G = {\textbackslash}\{x: f(x) {\textbackslash}geq g(x){\textbackslash}\}\$. We consider applications where it is appropriate to assume that the region \$G\$ has a smooth boundary or belongs to another nonparametric class of sets. In particular, this assumption makes sense if discrimination is used as a data analytic tool. Decision rules based on minimization of empirical risk over the whole class of sets and over sieves are considered. Their rates of convergence are obtained. We show that these rules achieve optimal rates for estimation of \$G\$ and optimal rates of convergence for Bayes risks. An interesting conclusion is that the optimal rates for Bayes risks can be very fast, in particular, faster than the “parametric” root-\$n\$ rate. These fast rates cannot be guaranteed for plug-in rules.},
	number = {6},
	urldate = {2023-01-15},
	journal = {The Annals of Statistics},
	author = {Mammen, Enno and Tsybakov, Alexandre B.},
	month = dec,
	year = {1999},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Optimal rates, 62G05, 62G20, Bayes risk, Discrimination analysis, Empirical risk, sieves},
	pages = {1808--1829},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/5IUB5K3X/Mammen and Tsybakov - 1999 - Smooth discrimination analysis.pdf:application/pdf},
}

@article{zhang_information_2006,
	title = {Information {Theoretical} {Upper} and {Lower} {Bounds} for {Statistical} {Estimation}},
	volume = {52},
	abstract = {We establish upper and lower bounds for some statistical estimation problems through concise information theoretical arguments. Our upper bound analysis is based on a simple yet general inequality which we call the information exponential inequality. We show that this inequality naturally leads to a general randomized estimation method, for which performance upper bounds can be obtained. The lower bounds, applicable for all statistical estimators, are obtained by original applications of some well known information theoretical inequalities, and approximately match the obtained upper bounds for various important problems. Moreover, our framework can be regarded as a natural generalization of the standard minimax framework, in that we allow the performance of the estimator to vary for different possible underlying distributions according to a pre-deﬁned prior.},
	language = {en},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Zhang, Tong},
	year = {2006},
	pages = {1307--1321},
	file = {Zhang - Information Theoretical Upper and Lower Bounds for.pdf:/Users/tokio/Zotero/storage/ES26KUJC/Zhang - Information Theoretical Upper and Lower Bounds for.pdf:application/pdf},
}

@article{zhang_epsilon-entropy_2007,
	title = {From \${\textbackslash}epsilon\$-entropy to {KL}-entropy: {Analysis} of minimum information complexity density estimation},
	shorttitle = {From \${\textbackslash}epsilon\$-entropy to {KL}-entropy},
	url = {https://ui.adsabs.harvard.edu/abs/2007math......2653Z},
	abstract = {We consider an extension of \${\textbackslash}epsilon\$-entropy to a KL-divergence based complexity measure for randomized density estimation methods. Based on this extension, we develop a general information-theoretical inequality that measures the statistical complexity of some deterministic and randomized density estimators. Consequences of the new inequality will be presented. In particular, we show that this technique can lead to improvements of some classical results concerning the convergence of minimum description length and Bayesian posterior distributions. Moreover, we are able to derive clean finite-sample convergence bounds that are not obtainable using previous approaches.},
	urldate = {2023-01-15},
	journal = {arXiv Mathematics e-prints},
	author = {Zhang, Tong},
	month = feb,
	year = {2007},
	note = {ADS Bibcode: 2007math......2653Z},
	keywords = {62C10, 62G07 (Primary), Mathematics - Statistics},
	pages = {math/0702653},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/N4CGCWHL/Zhang - 2007 - From \$epsilon\$-entropy to KL-entropy Analysis of.pdf:application/pdf},
}

@inproceedings{grunwald_tight_2019,
	title = {A tight excess risk bound via a unified {PAC}-{Bayesian}–{Rademacher}–{Shtarkov}–{MDL} complexity},
	url = {https://proceedings.mlr.press/v98/grunwald19a.html},
	abstract = {We present a novel notion of complexity that interpolates between and generalizes some classic complexity notions in learning theory: for empirical risk minimization (ERM) with arbitrary bounded loss, it is upper bounded in terms of data-independent Rademacher complexity; for generalized Bayesian estimators, it is upper bounded by the data-dependent information (KL) complexity. For ERM, the new complexity reduces to normalized maximum likelihood complexity, i.e., a minimax log-loss individual sequence regret.  Our first main result bounds excess risk in terms of the new complexity. Our second main result links the new complexity to L2(P)L2(P)L\_2(P) entropy via Rademacher complexity, generalizing earlier results of Opper, Haussler, Lugosi, and Cesa-Bianchi who covered the log-loss case with L∞L∞L\_{\textbackslash}infty entropy. Together, these results recover optimal bounds for VC-type and large (polynomial entropy) classes, replacing local Rademacher complexities by a simpler analysis which almost completely separates the two aspects that determine the achievable rates: ‘easiness’ (Bernstein) conditions and model complexity.},
	language = {en},
	urldate = {2023-01-15},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Algorithmic} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Grünwald, Peter D. and Mehta, Nishant A.},
	month = mar,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {433--465},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/T5QAYG9K/Grünwald and Mehta - 2019 - A tight excess risk bound via a unified PAC-Bayesi.pdf:application/pdf},
}

@article{bartlett_model_2002,
	title = {Model {Selection} and {Error} {Estimation}},
	volume = {48},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1013999503812},
	doi = {10.1023/A:1013999503812},
	abstract = {We study model selection strategies based on penalized empirical loss minimization. We point out a tight relationship between error estimation and data-based complexity penalization: any good error estimate may be converted into a data-based penalty function and the performance of the estimate is governed by the quality of the error estimate. We consider several penalty functions, involving error estimates on independent test data, empirical VC dimension, empirical VC entropy, and margin-based quantities. We also consider the maximal difference between the error on the first half of the training data and the second half, and the expected maximal discrepancy, a closely related capacity estimate that can be calculated by Monte Carlo integration. Maximal discrepancy penalty functions are appealing for pattern classification problems, since their computation is equivalent to empirical risk minimization over the training data with some labels flipped.},
	language = {en},
	number = {1},
	urldate = {2023-01-15},
	journal = {Machine Learning},
	author = {Bartlett, Peter L. and Boucheron, Stéphane and Lugosi, Gábor},
	month = jul,
	year = {2002},
	keywords = {concentration inequalities, empirical penalties, model selection, penalization},
	pages = {85--113},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/2US44EJV/Bartlett et al. - 2002 - Model Selection and Error Estimation.pdf:application/pdf},
}

@article{bartlett_convexity_2006,
	title = {Convexity, {Classification}, and {Risk} {Bounds}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000907},
	doi = {10.1198/016214505000000907},
	abstract = {Many of the classiﬁcation algorithms developed in the machine learning literature, including the support vector machine and boosting, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. The convexity makes these algorithms computationally eﬃcient. The use of a surrogate, however, has statistical consequences that must be balanced against the computational virtues of convexity. To study these issues, we provide a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial upper bounds on excess risk under the weakest possible condition on the loss function: that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a simple variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled convex hulls of a ﬁnite-dimensional base class, with a variety of commonly used loss functions.},
	language = {en},
	number = {473},
	urldate = {2023-01-15},
	journal = {Journal of the American Statistical Association},
	author = {Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
	month = mar,
	year = {2006},
	pages = {138--156},
	file = {Bartlett et al. - 2006 - Convexity, Classification, and Risk Bounds.pdf:/Users/tokio/Zotero/storage/EMTYGGX4/Bartlett et al. - 2006 - Convexity, Classification, and Risk Bounds.pdf:application/pdf},
}

@inproceedings{koolen_combining_2016,
	title = {Combining {Adversarial} {Guarantees} and {Stochastic} {Fast} {Rates} in {Online} {Learning}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/db116b39f7a3ac5366079b1d9fe249a5-Abstract.html},
	abstract = {We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.},
	urldate = {2023-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Koolen, Wouter M and Grünwald, Peter and van Erven, Tim},
	year = {2016},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/7CCPK77T/Koolen et al. - 2016 - Combining Adversarial Guarantees and Stochastic Fa.pdf:application/pdf},
}

@article{seeger_pac-bayesian_2002,
	title = {{PAC}-{Bayesian} {Generalisation} {Error} {Bounds} for {Gaussian} {Process} {Classification}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {https://www.jmlr.org/papers/v3/seeger02a.html},
	abstract = {Approximate Bayesian Gaussian process (GP) classification techniques are powerful non-parametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques.},
	number = {Oct},
	urldate = {2023-01-15},
	journal = {Journal of Machine Learning Research},
	author = {Seeger, Matthias},
	year = {2002},
	pages = {233--269},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/I6E3B9MH/Seeger - 2002 - PAC-Bayesian Generalisation Error Bounds for Gauss.pdf:application/pdf},
}

@inproceedings{langford_pac-bayes_2002,
	title = {{PAC}-{Bayes} \& {Margins}},
	volume = {15},
	url = {https://proceedings.neurips.cc/paper/2002/hash/68d309812548887400e375eaa036d2f1-Abstract.html},
	urldate = {2023-01-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Langford, John and Shawe-Taylor, John},
	year = {2002},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KIU5GCSD/Langford and Shawe-Taylor - 2002 - PAC-Bayes & Margins.pdf:application/pdf},
}

@inproceedings{russo_controlling_2016,
	title = {Controlling {Bias} in {Adaptive} {Data} {Analysis} {Using} {Information} {Theory}},
	url = {https://proceedings.mlr.press/v51/russo16.html},
	abstract = {Modern big data settings often involve messy, high-dimensional data, where it is not clear a priori what are the right questions to ask. To extract the most insights from a dataset, the analyst typically needs to engage in an iterative process of adaptive data analysis. The choice of analytics to be performed next depends on the results of the previous analyses on the same data. It is commonly recognized that such adaptivity (also called researcher degrees of freedom), even if well-intentioned, can lead to false discoveries, contributing to the crisis of reproducibility in science. In this paper, we propose a general information-theoretic framework to quantify and provably bound the bias of arbitrary adaptive analysis process. We prove that our mutual information based bound is tight in natural models. We show how this framework can give rigorous insights into when commonly used feature selection protocols (e.g. rank selection) do and do not lead to biased estimation. We also show how recent insights from differential privacy emerge from this framework when the analyst is assumed to be adversarial, though our bounds applies in more general settings. We illustrate our results with simple simulations.},
	language = {en},
	urldate = {2023-01-15},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Russo, Daniel and Zou, James},
	month = may,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {1232--1240},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/CNWFNAHS/Russo and Zou - 2016 - Controlling Bias in Adaptive Data Analysis Using I.pdf:application/pdf},
}

@article{catoni_pac-bayesian_2004,
	title = {A {PAC}-{Bayesian} approach to adaptive classiﬁcation},
	language = {en},
	author = {Catoni, Olivier},
	year = {2004},
	file = {Catoni - 2004 - A PAC-Bayesian approach to adaptive classiﬁcation.pdf:/Users/tokio/Zotero/storage/94BYKMLX/Catoni - 2004 - A PAC-Bayesian approach to adaptive classiﬁcation.pdf:application/pdf},
}

@misc{van_erven_pac-bayes_2014,
	title = {{PAC}-{Bayes} {Mini}-tutorial: {A} {Continuous} {Union} {Bound}},
	shorttitle = {{PAC}-{Bayes} {Mini}-tutorial},
	url = {http://arxiv.org/abs/1405.1580},
	doi = {10.48550/arXiv.1405.1580},
	abstract = {When I first encountered PAC-Bayesian concentration inequalities they seemed to me to be rather disconnected from good old-fashioned results like Hoeffding's and Bernstein's inequalities. But, at least for one flavour of the PAC-Bayesian bounds, there is actually a very close relation, and the main innovation is a continuous version of the union bound, along with some ingenious applications. Here's the gist of what's going on, presented from a machine learning perspective.},
	urldate = {2023-01-13},
	publisher = {arXiv},
	author = {van Erven, Tim},
	month = may,
	year = {2014},
	note = {arXiv:1405.1580 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/66A7DHV9/van Erven - 2014 - PAC-Bayes Mini-tutorial A Continuous Union Bound.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6WF94TKX/1405.html:text/html},
}

@inproceedings{shawe-taylor_pac_1997,
	address = {New York, NY, USA},
	series = {{COLT} '97},
	title = {A {PAC} analysis of a {Bayesian} estimator},
	isbn = {978-0-89791-891-6},
	url = {https://doi.org/10.1145/267460.267466},
	doi = {10.1145/267460.267466},
	urldate = {2023-01-13},
	booktitle = {Proceedings of the tenth annual conference on {Computational} learning theory},
	publisher = {Association for Computing Machinery},
	author = {Shawe-Taylor, John and Williamson, Robert C.},
	month = jul,
	year = {1997},
	pages = {2--9},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/9L8XV52T/Shawe-Taylor and Williamson - 1997 - A PAC analysis of a Bayesian estimator.pdf:application/pdf},
}

@article{donsker_asymptotic_1983,
	title = {Asymptotic evaluation of certain markov process expectations for large time. {IV}},
	volume = {36},
	issn = {0010-3640},
	url = {http://www.scopus.com/inward/record.url?scp=84990587390&partnerID=8YFLogxK},
	doi = {10.1002/cpa.3160360204},
	number = {2},
	urldate = {2023-01-13},
	journal = {Communications on Pure and Applied Mathematics},
	author = {Donsker, M. D. and Varadhan, S. R.S.},
	month = mar,
	year = {1983},
	pages = {183--212},
}

@article{valiant_theory_1984,
	title = {A theory of the learnable},
	volume = {27},
	issn = {0001-0782},
	url = {https://doi.org/10.1145/1968.1972},
	doi = {10.1145/1968.1972},
	number = {11},
	urldate = {2023-01-03},
	journal = {Communications of the ACM},
	author = {Valiant, L. G.},
	month = nov,
	year = {1984},
	keywords = {inductive inference, probabilistic models of learning, propositional expressions},
	pages = {1134--1142},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JJVA5REH/Valiant - 1984 - A theory of the learnable.pdf:application/pdf},
}

@article{oneto_randomized_2020,
	title = {Randomized learning and generalization of fair and private classifiers: {From} {PAC}-{Bayes} to stability and differential privacy},
	volume = {416},
	copyright = {open},
	shorttitle = {Randomized learning and generalization of fair and private classifiers},
	url = {https://doi.org/10.1016/j.neucom.2019.12.137},
	abstract = {We address the problem of randomized learning and generalization of fair and private classifiers. From one side we want to ensure that sensitive information does not unfairly influence the outcome of a classifier. From the other side we have to learn from data while preserving the privacy of individual observations. We initially face this issue in the PAC-Bayes framework presenting an approach which trades off and bounds the risk and the fairness of the randomized (Gibbs) classifier. Our new approach is able to handle several different state-of-the-art fairness measures. For this purpose, we further develop the idea that the PAC-Bayes prior can be defined based on the data-generating distribution without actually knowing it. In particular, we define a prior and a posterior which give more weight to functions with good generalization and fairness properties. Furthermore, we will show that this randomized classifier possesses interesting stability properties using the algorithmic distribution stability theory. Finally, we will show that the new posterior can be exploited to define a randomized accurate and fair algorithm. Differential privacy theory will allow us to derive that the latter algorithm has interesting privacy preserving properties ensuring our threefold goal of good generalization, fairness, and privacy of the final model.},
	language = {eng},
	urldate = {2022-12-11},
	journal = {Neurocomputing},
	author = {Oneto, L. and Donini, M. and Pontil, M. and Shawe-Taylor, J.},
	month = nov,
	year = {2020},
	pages = {231--243},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ED346C6P/Oneto et al. - 2020 - Randomized learning and generalization of fair and.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/ATE2TVSK/10097480.html:text/html},
}

@misc{hellstrom_new_2022,
	title = {A {New} {Family} of {Generalization} {Bounds} {Using} {Samplewise} {Evaluated} {CMI}},
	url = {http://arxiv.org/abs/2210.06422},
	doi = {10.48550/arXiv.2210.06422},
	abstract = {We present a new family of information-theoretic generalization bounds, in which the training loss and the population loss are compared through a jointly convex function. This function is upper-bounded in terms of the disintegrated, samplewise, evaluated conditional mutual information (CMI), an information measure that depends on the losses incurred by the selected hypothesis, rather than on the hypothesis itself, as is common in probably approximately correct (PAC)-Bayesian results. We demonstrate the generality of this framework by recovering and extending previously known information-theoretic bounds. Furthermore, using the evaluated CMI, we derive a samplewise, average version of Seeger's PAC-Bayesian bound, where the convex function is the binary KL divergence. In some scenarios, this novel bound results in a tighter characterization of the population loss of deep neural networks than previous bounds. Finally, we derive high-probability versions of some of these average bounds. We demonstrate the unifying nature of the evaluated CMI bounds by using them to recover average and high-probability generalization bounds for multiclass classification with finite Natarajan dimension.},
	urldate = {2022-12-05},
	publisher = {arXiv},
	author = {Hellström, Fredrik and Durisi, Giuseppe},
	month = oct,
	year = {2022},
	note = {arXiv:2210.06422 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/PWW329CK/Hellström and Durisi - 2022 - A New Family of Generalization Bounds Using Sample.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IFGY8JUN/2210.html:text/html},
}

@article{kutin_almost-everywhere_nodate,
	title = {Almost-everywhere algorithmic stability and generalization error},
	abstract = {We introduce a new notion of algorithmic stability, which we call training stability. We show that training stability is suﬃcient for good bounds on generalization error. These bounds hold even when the learner has inﬁnite VC dimension. In the PAC setting, training stability gives necessary and suﬃcient conditions for exponential convergence, and thus serves as a distribution-dependent analog to VC dimension.},
	language = {en},
	author = {Kutin, Samuel and Niyogi, Partha},
	pages = {43},
	file = {Kutin and Niyogi - Almost-everywhere algorithmic stability and genera.pdf:/Users/tokio/Zotero/storage/F9G2PU5W/Kutin and Niyogi - Almost-everywhere algorithmic stability and genera.pdf:application/pdf},
}

@article{bousquet_stability_nodate,
	title = {Stability and {Generalization}},
	abstract = {We deﬁne notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classiﬁcation one when the classiﬁer is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hilbert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classiﬁcation.},
	language = {en},
	author = {Bousquet, Olivier and Elisseeﬀ, Andre},
	pages = {28},
	file = {Bousquet and Elisseeﬀ - Stability and Generalization.pdf:/Users/tokio/Zotero/storage/2YEHF8YX/Bousquet and Elisseeﬀ - Stability and Generalization.pdf:application/pdf},
}

@inproceedings{kontorovich_concentration_2014,
	title = {Concentration in unbounded metric spaces and algorithmic stability},
	url = {https://proceedings.mlr.press/v32/kontorovicha14.html},
	abstract = {We prove an extension of McDiarmid’s inequality for metric spaces with unbounded diameter.  To this end, we introduce the notion of the {\textbackslash}em subgaussian diameter,  which is a distribution-dependent refinement of the metric diameter.  Our technique provides an alternative approach to that of Kutin and Niyogi’s   method of weakly difference-bounded functions, and yields nontrivial,   dimension-free results in some interesting cases where the former does not.  As an application, we give apparently the first generalization bound in the  algorithmic stability setting that holds for unbounded loss functions.  This yields a novel risk bound for some regularized metric regression algorithms.  We give two extensions of the basic concentration result.  The first enables one to replace the independence assumption by appropriate strong mixing.  The second generalizes the subgaussian technique to other Orlicz norms.},
	language = {en},
	urldate = {2022-11-02},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kontorovich, Aryeh},
	month = jun,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {28--36},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/SVZ7JBDD/Kontorovich - 2014 - Concentration in unbounded metric spaces and algor.pdf:application/pdf},
}

@misc{yehudai_local_2021,
	title = {From {Local} {Structures} to {Size} {Generalization} in {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.08853},
	doi = {10.48550/arXiv.2010.08853},
	abstract = {Graph neural networks (GNNs) can process graphs of different sizes, but their ability to generalize across sizes, specifically from small to large graphs, is still not well understood. In this paper, we identify an important type of data where generalization from small to large graphs is challenging: graph distributions for which the local structure depends on the graph size. This effect occurs in multiple important graph learning domains, including social and biological networks. We first prove that when there is a difference between the local structures, GNNs are not guaranteed to generalize across sizes: there are "bad" global minima that do well on small graphs but fail on large graphs. We then study the size-generalization problem empirically and demonstrate that when there is a discrepancy in local structure, GNNs tend to converge to non-generalizing solutions. Finally, we suggest two approaches for improving size generalization, motivated by our findings. Notably, we propose a novel Self-Supervised Learning (SSL) task aimed at learning meaningful representations of local structures that appear in large graphs. Our SSL task improves classification accuracy on several popular datasets.},
	urldate = {2022-10-28},
	publisher = {arXiv},
	author = {Yehudai, Gilad and Fetaya, Ethan and Meirom, Eli and Chechik, Gal and Maron, Haggai},
	month = jul,
	year = {2021},
	note = {arXiv:2010.08853 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/CLEYE97R/Yehudai et al. - 2021 - From Local Structures to Size Generalization in Gr.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/Q8IKP6WV/2010.html:text/html},
}

@misc{foti_survey_2012,
	title = {A survey of non-exchangeable priors for {Bayesian} nonparametric models},
	url = {http://arxiv.org/abs/1211.4798},
	abstract = {Dependent nonparametric processes extend distributions over measures, such as the Dirichlet process and the beta process, to give distributions over collections of measures, typically indexed by values in some covariate space. Such models are appropriate priors when exchangeability assumptions do not hold, and instead we want our model to vary fluidly with some set of covariates. Since the concept of dependent nonparametric processes was formalized by MacEachern [1], there have been a number of models proposed and used in the statistics and machine learning literatures. Many of these models exhibit underlying similarities, an understanding of which, we hope, will help in selecting an appropriate prior, developing new models, and leveraging inference techniques.},
	urldate = {2022-10-16},
	publisher = {arXiv},
	author = {Foti, Nicholas J. and Williamson, Sinead},
	month = nov,
	year = {2012},
	note = {arXiv:1211.4798 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/AYYCG8KK/Foti and Williamson - 2012 - A survey of non-exchangeable priors for Bayesian n.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/2IVJY272/1211.html:text/html},
}

@inproceedings{dziugaite_role_2021,
	title = {On the {Role} of {Data} in {PAC}-{Bayes} {Bounds}},
	url = {https://proceedings.mlr.press/v130/karolina-dziugaite21a.html},
	abstract = {The dominant term in PAC-Bayes bounds is often the Kullback-Leibler divergence between the posterior and prior. For so-called linear PAC-Bayes risk bounds based on the empirical risk of a fixed posterior kernel, it is possible to minimize the expected value of the bound by choosing the prior to be the expected posterior, which we call the oracle prior on the account that it is distribution dependent. In this work, we show that the bound based on the oracle prior can be suboptimal: In some cases, a stronger bound is obtained by using a data-dependent oracle prior, i.e., a conditional expectation of the posterior, given a subset of the training data that is then excluded from the empirical risk term. While using data to learn a prior is a known heuristic, its essential role in optimal bounds is new. In fact, we show that using data can mean the difference between vacuous and nonvacuous bounds. We apply this new principle in the setting of nonconvex learning, simulating data-dependent oracle priors on MNIST and Fashion MNIST with and without held-out data, and demonstrating new nonvacuous bounds in both cases.},
	language = {en},
	urldate = {2022-10-06},
	booktitle = {Proceedings of {The} 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Dziugaite, Gintare Karolina and Hsu, Kyle and Gharbieh, Waseem and Arpino, Gabriel and Roy, Daniel},
	month = mar,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {604--612},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZDGXAQWS/Dziugaite et al. - 2021 - On the Role of Data in PAC-Bayes Bounds.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/NPIRDW5R/Dziugaite et al. - 2021 - On the Role of Data in PAC-Bayes Bounds.pdf:application/pdf},
}

@inproceedings{rivasplata_pac-bayes_2020,
	title = {{PAC}-{Bayes} {Analysis} {Beyond} the {Usual} {Bounds}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/c3992e9a68c5ae12bd18488bc579b30d-Abstract.html},
	abstract = {We focus on a stochastic learning model where the learner observes a finite set of training examples and the output of the learning process is a data-dependent distribution over a space of hypotheses. The learned data-dependent distribution is then used to make randomized predictions, and the high-level theme addressed here is guaranteeing the quality of predictions on examples that were not seen during training, i.e. generalization. In this setting the unknown quantity of interest is the expected risk of the data-dependent randomized predictor, for which upper bounds can be derived via a PAC-Bayes analysis, leading to PAC-Bayes bounds.},
	urldate = {2022-10-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rivasplata, Omar and Kuzborskij, Ilja and Szepesvari, Csaba and Shawe-Taylor, John},
	year = {2020},
	pages = {16833--16845},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/T66RRCXV/Rivasplata et al. - 2020 - PAC-Bayes Analysis Beyond the Usual Bounds.pdf:application/pdf},
}

@article{hennig_probabilistic_2015,
	title = {Probabilistic {Numerics} and {Uncertainty} in {Computations}},
	volume = {471},
	issn = {1364-5021, 1471-2946},
	url = {http://arxiv.org/abs/1506.01326},
	doi = {10.1098/rspa.2015.0142},
	abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data has led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimisers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
	number = {2179},
	urldate = {2022-09-30},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
	month = jul,
	year = {2015},
	note = {arXiv:1506.01326 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Mathematics - Numerical Analysis},
	pages = {20150142},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/CH7AXXWL/Hennig et al. - 2015 - Probabilistic Numerics and Uncertainty in Computat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/A4KX6G9L/1506.html:text/html},
}

@misc{schober_probabilistic_2017,
	title = {A probabilistic model for the numerical solution of initial value problems},
	url = {http://arxiv.org/abs/1610.05261},
	doi = {10.48550/arXiv.1610.05261},
	abstract = {Like many numerical methods, solvers for initial value problems (IVPs) on ordinary differential equations estimate an analytically intractable quantity, using the results of tractable computations as inputs. This structure is closely connected to the notion of inference on latent variables in statistics. We describe a class of algorithms that formulate the solution to an IVP as inference on a latent path that is a draw from a Gaussian process probability measure (or equivalently, the solution of a linear stochastic differential equation). We then show that certain members of this class are connected precisely to generalized linear methods for ODEs, a number of Runge--Kutta methods, and Nordsieck methods. This probabilistic formulation of classic methods is valuable in two ways: analytically, it highlights implicit prior assumptions favoring certain approximate solutions to the IVP over others, and gives a precise meaning to the old observation that these methods act like filters. Practically, it endows the classic solvers with `docking points' for notions of uncertainty and prior information about the initial value, the value of the ODE itself, and the solution of the problem.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Schober, Michael and Särkkä, Simo and Hennig, Philipp},
	month = aug,
	year = {2017},
	note = {arXiv:1610.05261 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/S7S42AT9/Schober et al. - 2017 - A probabilistic model for the numerical solution o.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RNLDH9U4/1610.html:text/html},
}

@inproceedings{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	language = {en},
	urldate = {2022-09-29},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228},
	pages = {2256--2265},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ATGEAHH7/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf},
}

@misc{song_improved_2020,
	title = {Improved {Techniques} for {Training} {Score}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2006.09011},
	doi = {10.48550/arXiv.2006.09011},
	abstract = {Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.},
	urldate = {2022-09-29},
	publisher = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv:2006.09011 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WZGJ5G2F/Song and Ermon - 2020 - Improved Techniques for Training Score-Based Gener.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QXK36ERG/2006.html:text/html},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
	urldate = {2022-09-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/98GEBYDW/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@inproceedings{kramer_probabilistic_2022,
	title = {Probabilistic {ODE} {Solutions} in {Millions} of {Dimensions}},
	url = {https://proceedings.mlr.press/v162/kramer22b.html},
	abstract = {Probabilistic solvers for ordinary differential equations (ODEs) have emerged as an efficient framework for uncertainty quantification and inference on dynamical systems. In this work, we explain the mathematical assumptions and detailed implementation schemes behind solving high-dimensional ODEs with a probabilistic numerical algorithm. This has not been possible before due to matrix-matrix operations in each solver step, but is crucial for scientifically relevant problems—most importantly, the solution of discretised partial differential equations. In a nutshell, efficient high-dimensional probabilistic ODE solutions build either on independence assumptions or on Kronecker structure in the prior model. We evaluate the resulting efficiency on a range of problems, including the probabilistic numerical simulation of a differential equation with millions of dimensions.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Krämer, Nicholas and Bosch, Nathanael and Schmidt, Jonathan and Hennig, Philipp},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {11634--11649},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/NV74H9WM/Krämer et al. - 2022 - Probabilistic ODE Solutions in Millions of Dimensi.pdf:application/pdf},
}

@misc{jing_torsional_2022,
	title = {Torsional {Diffusion} for {Molecular} {Conformer} {Generation}},
	url = {http://arxiv.org/abs/2206.01729},
	doi = {10.48550/arXiv.2206.01729},
	abstract = {Molecular conformer generation is a fundamental task in computational chemistry. Several machine learning approaches have been developed, but none have outperformed state-of-the-art cheminformatics methods. We propose torsional diffusion, a novel diffusion framework that operates on the space of torsion angles via a diffusion process on the hypertorus and an extrinsic-to-intrinsic score model. On a standard benchmark of drug-like molecules, torsional diffusion generates superior conformer ensembles compared to machine learning and cheminformatics methods in terms of both RMSD and chemical properties, and is orders of magnitude faster than previous diffusion-based models. Moreover, our model provides exact likelihoods, which we employ to build the first generalizable Boltzmann generator. Code is available at https://github.com/gcorso/torsional-diffusion.},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Jing, Bowen and Corso, Gabriele and Chang, Jeffrey and Barzilay, Regina and Jaakkola, Tommi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.01729 [physics, q-bio]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GH2UZ5V8/Jing et al. - 2022 - Torsional Diffusion for Molecular Conformer Genera.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QUW62W74/2206.html:text/html},
}

@inproceedings{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	volume = {34},
	url = {https://papers.nips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html},
	urldate = {2022-09-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dhariwal, Prafulla and Nichol, Alexander},
	year = {2021},
	pages = {8780--8794},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/4WGPJUP4/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ACRM6IJ2/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/XZLBTLEF/1505.html:text/html},
}

@inproceedings{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} {With} {Latent} {Diffusion} {Models}},
	url = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html},
	language = {en},
	urldate = {2022-09-16},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	year = {2022},
	pages = {10684--10695},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/SEF4N9WM/Rombach et al. - 2022 - High-Resolution Image Synthesis With Latent Diffus.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/ZDKVMNUG/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html:text/html},
}

@misc{aivodji_fairwashing_2019,
	title = {Fairwashing: the risk of rationalization},
	shorttitle = {Fairwashing},
	url = {http://arxiv.org/abs/1901.09749},
	doi = {10.48550/arXiv.1901.09749},
	abstract = {Black-box explanation is the problem of explaining how a machine learning model -- whose internal logic is hidden to the auditor and generally complex -- produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Aïvodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, Sébastien and Hara, Satoshi and Tapp, Alain},
	month = may,
	year = {2019},
	note = {arXiv:1901.09749 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/P6NZ2SYD/Aïvodji et al. - 2019 - Fairwashing the risk of rationalization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/8PG8TA9T/1901.html:text/html},
}

@inproceedings{aivodji_characterizing_2022,
	title = {Characterizing the risk of fairwashing},
	url = {https://openreview.net/forum?id=9PnKduzf-FT},
	abstract = {Fairwashing refers to the risk that an unfair black-box model can be explained by a fairer model through post-hoc explanation manipulation. In this paper, we investigate the capability of...},
	language = {en},
	urldate = {2022-07-21},
	author = {Aïvodji, Ulrich and Arai, Hiromi and Gambs, Sébastien and Hara, Satoshi},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/CTXQRGNY/Aïvodji et al. - 2022 - Characterizing the risk of fairwashing.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/6KDZC3XX/forum.html:text/html},
}

@misc{tay_efficient_2022,
	title = {Efficient {Transformers}: {A} {Survey}},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	doi = {10.48550/arXiv.2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = mar,
	year = {2022},
	note = {arXiv:2009.06732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ABWGWU2V/Tay et al. - 2022 - Efficient Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/8WJRQVLF/2009.html:text/html},
}

@misc{hestness_deep_2017,
	title = {Deep {Learning} {Scaling} is {Predictable}, {Empirically}},
	url = {http://arxiv.org/abs/1712.00409},
	doi = {10.48550/arXiv.1712.00409},
	abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
	month = dec,
	year = {2017},
	note = {arXiv:1712.00409 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MKPUHRSC/Hestness et al. - 2017 - Deep Learning Scaling is Predictable, Empirically.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/P82545CV/1712.html:text/html},
}

@misc{viering_shape_2021,
	title = {The {Shape} of {Learning} {Curves}: a {Review}},
	shorttitle = {The {Shape} of {Learning} {Curves}},
	url = {http://arxiv.org/abs/2103.10948},
	doi = {10.48550/arXiv.2103.10948},
	abstract = {Learning curves provide insight into the dependence of a learner's generalization performance on the training set size. This important tool can be used for model selection, to predict the effect of more training data, and to reduce the computational complexity of model training and hyperparameter tuning. This review recounts the origins of the term, provides a formal definition of the learning curve, and briefly covers basics such as its estimation. Our main contribution is a comprehensive overview of the literature regarding the shape of learning curves. We discuss empirical and theoretical evidence that supports well-behaved curves that often have the shape of a power law or an exponential. We consider the learning curves of Gaussian processes, the complex shapes they can display, and the factors influencing them. We draw specific attention to examples of learning curves that are ill-behaved, showing worse learning performance with more training data. To wrap up, we point out various open problems that warrant deeper empirical and theoretical investigation. All in all, our review underscores that learning curves are surprisingly diverse and no universal model can be identified.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Viering, Tom and Loog, Marco},
	month = mar,
	year = {2021},
	note = {arXiv:2103.10948 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZWDDGCIF/Viering and Loog - 2021 - The Shape of Learning Curves a Review.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/W6HVTI25/2103.html:text/html},
}

@article{jones_introduction_2003,
	title = {An introduction to power and sample size estimation},
	volume = {20},
	issn = {1472-0205, 1472-0213},
	url = {https://emj.bmj.com/lookup/doi/10.1136/emj.20.5.453},
	doi = {10.1136/emj.20.5.453},
	language = {en},
	number = {5},
	urldate = {2022-07-21},
	journal = {Emergency Medicine Journal},
	author = {Jones, S R},
	month = sep,
	year = {2003},
	pages = {453--458},
	file = {Jones - 2003 - An introduction to power and sample size estimatio.pdf:/Users/tokio/Zotero/storage/P6BTP8EV/Jones - 2003 - An introduction to power and sample size estimatio.pdf:application/pdf},
}

@article{figueroa_predicting_2012,
	title = {Predicting sample size required for classification performance},
	volume = {12},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/1472-6947-12-8},
	doi = {10.1186/1472-6947-12-8},
	abstract = {Supervised learning methods need annotated data in order to generate efficient models. Annotated data, however, is a relatively scarce resource and can be expensive to obtain. For both passive and active learning methods, there is a need to estimate the size of the annotated sample required to reach a performance target.},
	number = {1},
	urldate = {2022-07-21},
	journal = {BMC Medical Informatics and Decision Making},
	author = {Figueroa, Rosa L. and Zeng-Treitler, Qing and Kandula, Sasikiran and Ngo, Long H.},
	month = feb,
	year = {2012},
	keywords = {Active Learning, Annotate Data, Learning Curve, Mean Absolute Error, Root Mean Square Error},
	pages = {8},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3PWEBS6F/Figueroa et al. - 2012 - Predicting sample size required for classification.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/8E7PQS29/1472-6947-12-8.html:text/html},
}

@misc{zhai_scaling_2022,
	title = {Scaling {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2106.04560},
	doi = {10.48550/arXiv.2106.04560},
	abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy of the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45\% top-1 accuracy. The model also performs well for few-shot transfer, for example, reaching 84.86\% top-1 accuracy on ImageNet with only 10 examples per class.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
	month = jun,
	year = {2022},
	note = {arXiv:2106.04560 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FTDRSU68/Zhai et al. - 2022 - Scaling Vision Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BDW5TRNP/2106.html:text/html},
}

@inproceedings{sun_revisiting_2017,
	address = {Venice},
	title = {Revisiting {Unreasonable} {Effectiveness} of {Data} in {Deep} {Learning} {Era}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237359/},
	doi = {10.1109/ICCV.2017.97},
	abstract = {The success of deep learning in vision can be attributed to: (a) models with high capacity; (b) increased computational power; and (c) availability of large-scale labeled data. Since 2012, there have been signiﬁcant advances in representation capabilities of the models and computational capabilities of GPUs. But the size of the biggest dataset has surprisingly remained constant. What will happen if we increase the dataset size by 10× or 100×? This paper takes a step towards clearing the clouds of mystery surrounding the relationship between ‘enormous data’ and visual deep learning. By exploiting the JFT-300M dataset which has more than 375M noisy labels for 300M images, we investigate how the performance of current vision tasks would change if this data was used for representation learning. Our paper delivers some surprising (and some expected) ﬁndings. First, we ﬁnd that the performance on vision tasks increases logarithmically based on volume of training data size. Second, we show that representation learning (or pretraining) still holds a lot of promise. One can improve performance on many vision tasks by just training a better base model. Finally, as expected, we present new state-of-theart results for different vision tasks including image classiﬁcation, object detection, semantic segmentation and human pose estimation. Our sincere hope is that this inspires vision community to not undervalue the data and develop collective efforts in building larger datasets.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
	month = oct,
	year = {2017},
	pages = {843--852},
	file = {Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in D.pdf:/Users/tokio/Zotero/storage/XGMAA5W5/Sun et al. - 2017 - Revisiting Unreasonable Effectiveness of Data in D.pdf:application/pdf},
}

@inproceedings{frey_modeling_1999,
	title = {Modeling decision tree performance with the power law},
	url = {https://proceedings.mlr.press/r2/frey99a.html},
	abstract = {This paper discusses the use of a power law to predict decision tree performance. Power laws are fit to learning curves of decision trees trained on data sets from the UCI repository. The learning ...},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Seventh {International} {Workshop} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Frey, Lewis J. and Fisher, Douglas H.},
	month = jan,
	year = {1999},
	note = {ISSN: 2640-3498},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/YUVPD2FS/Frey and Fisher - 1999 - Modeling decision tree performance with the power .pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/MPBS2H42/frey99a.html:text/html},
}

@misc{bisla_theoretical-empirical_2021,
	title = {A {Theoretical}-{Empirical} {Approach} to {Estimating} {Sample} {Complexity} of {DNNs}},
	url = {http://arxiv.org/abs/2105.01867},
	doi = {10.48550/arXiv.2105.01867},
	abstract = {This paper focuses on understanding how the generalization error scales with the amount of the training data for deep neural networks (DNNs). Existing techniques in statistical learning require computation of capacity measures, such as VC dimension, to provably bound this error. It is however unclear how to extend these measures to DNNs and therefore the existing analyses are applicable to simple neural networks, which are not used in practice, e.g., linear or shallow ones or otherwise multi-layer perceptrons. Moreover, many theoretical error bounds are not empirically verifiable. We derive estimates of the generalization error that hold for deep networks and do not rely on unattainable capacity measures. The enabling technique in our approach hinges on two major assumptions: i) the network achieves zero training error, ii) the probability of making an error on a test point is proportional to the distance between this point and its nearest training point in the feature space and at a certain maximal distance (that we call radius) it saturates. Based on these assumptions we estimate the generalization error of DNNs. The obtained estimate scales as O(1/({\textbackslash}delta N{\textasciicircum}\{1/d\})), where N is the size of the training data and is parameterized by two quantities, the effective dimensionality of the data as perceived by the network (d) and the aforementioned radius ({\textbackslash}delta), both of which we find empirically. We show that our estimates match with the experimentally obtained behavior of the error on multiple learning tasks using benchmark data-sets and realistic models. Estimating training data requirements is essential for deployment of safety critical applications such as autonomous driving etc. Furthermore, collecting and annotating training data requires a huge amount of financial, computational and human resources. Our empirical estimates will help to efficiently allocate resources.},
	urldate = {2022-07-21},
	publisher = {arXiv},
	author = {Bisla, Devansh and Saridena, Apoorva Nandini and Choromanska, Anna},
	month = may,
	year = {2021},
	note = {arXiv:2105.01867 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/2G6RNLFJ/Bisla et al. - 2021 - A Theoretical-Empirical Approach to Estimating Sam.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QTNF62SL/2105.html:text/html},
}

@inproceedings{hoiem_learning_2021,
	title = {Learning {Curves} for {Analysis} of {Deep} {Networks}},
	url = {https://proceedings.mlr.press/v139/hoiem21a.html},
	abstract = {Learning curves model a classifier’s test error as a function of the number of training samples. Prior works show that learning curves can be used to select model parameters and extrapolate performance. We investigate how to use learning curves to evaluate design choices, such as pretraining, architecture, and data augmentation. We propose a method to robustly estimate learning curves, abstract their parameters into error and data-reliance, and evaluate the effectiveness of different parameterizations. Our experiments exemplify use of learning curves for analysis and yield several interesting observations.},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hoiem, Derek and Gupta, Tanmay and Li, Zhizhong and Shlapentokh-Rothman, Michal},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {4287--4296},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/63BWGXP2/Hoiem et al. - 2021 - Learning Curves for Analysis of Deep Networks.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/X82AP2L3/Hoiem et al. - 2021 - Learning Curves for Analysis of Deep Networks.pdf:application/pdf},
}

@inproceedings{rosenfeld_constructive_2022,
	title = {A {Constructive} {Prediction} of the {Generalization} {Error} {Across} {Scales}},
	url = {https://openreview.net/forum?id=ryenvpEKDr},
	abstract = {We predict the generalization error and specify the model which attains it across model/data scales.},
	language = {en},
	urldate = {2022-07-21},
	author = {Rosenfeld, Jonathan S. and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
	month = feb,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/XCMFYVBB/Rosenfeld et al. - 2022 - A Constructive Prediction of the Generalization Er.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/SGGYWAZY/forum.html:text/html},
}

@inproceedings{abnar_exploring_2022,
	title = {Exploring the {Limits} of {Large} {Scale} {Pre}-training},
	url = {https://openreview.net/forum?id=V3C8p78sDa},
	abstract = {Recent developments in large-scale machine learning suggest that by scaling up data, model size and training time properly, one might  observe that improvements in pre-training would transfer...},
	language = {en},
	urldate = {2022-07-21},
	author = {Abnar, Samira and Dehghani, Mostafa and Neyshabur, Behnam and Sedghi, Hanie},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/F9D4YF6D/Abnar et al. - 2022 - Exploring the Limits of Large Scale Pre-training.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/YAUCPQZZ/forum.html:text/html},
}

@misc{lu_dpm-solver_2022,
	title = {{DPM}-{Solver}: {A} {Fast} {ODE} {Solver} for {Diffusion} {Probabilistic} {Model} {Sampling} in {Around} 10 {Steps}},
	shorttitle = {{DPM}-{Solver}},
	url = {http://arxiv.org/abs/2206.00927},
	doi = {10.48550/arXiv.2206.00927},
	abstract = {Diffusion probabilistic models (DPMs) are emerging powerful generative models. Despite their high-quality generation performance, DPMs still suffer from their slow sampling as they generally need hundreds or thousands of sequential function evaluations (steps) of large neural networks to draw a sample. Sampling from DPMs can be viewed alternatively as solving the corresponding diffusion ordinary differential equations (ODEs). In this work, we propose an exact formulation of the solution of diffusion ODEs. The formulation analytically computes the linear part of the solution, rather than leaving all terms to black-box ODE solvers as adopted in previous works. By applying change-of-variable, the solution can be equivalently simplified to an exponentially weighted integral of the neural network. Based on our formulation, we propose DPM-Solver, a fast dedicated high-order solver for diffusion ODEs with the convergence order guarantee. DPM-Solver is suitable for both discrete-time and continuous-time DPMs without any further training. Experimental results show that DPM-Solver can generate high-quality samples in only 10 to 20 function evaluations on various datasets. We achieve 4.70 FID in 10 function evaluations and 2.87 FID in 20 function evaluations on the CIFAR10 dataset, and a \$4{\textbackslash}sim 16{\textbackslash}times\$ speedup compared with previous state-of-the-art training-free samplers on various datasets.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
	month = jun,
	year = {2022},
	note = {arXiv:2206.00927 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/JV6VS2I7/Lu et al. - 2022 - DPM-Solver A Fast ODE Solver for Diffusion Probab.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RLJI3YC9/2206.html:text/html},
}

@misc{mahmood_how_2022,
	title = {How {Much} {More} {Data} {Do} {I} {Need}? {Estimating} {Requirements} for {Downstream} {Tasks}},
	shorttitle = {How {Much} {More} {Data} {Do} {I} {Need}?},
	url = {http://arxiv.org/abs/2207.01725},
	doi = {10.48550/arXiv.2207.01725},
	abstract = {Given a small training data set and a learning algorithm, how much more data is necessary to reach a target validation or test performance? This question is of critical importance in applications such as autonomous driving or medical imaging where collecting data is expensive and time-consuming. Overestimating or underestimating data requirements incurs substantial costs that could be avoided with an adequate budget. Prior work on neural scaling laws suggest that the power-law function can fit the validation performance curve and extrapolate it to larger data set sizes. We find that this does not immediately translate to the more difficult downstream task of estimating the required data set size to meet a target performance. In this work, we consider a broad class of computer vision tasks and systematically investigate a family of functions that generalize the power-law function to allow for better estimation of data requirements. Finally, we show that incorporating a tuned correction factor and collecting over multiple rounds significantly improves the performance of the data estimators. Using our guidelines, practitioners can accurately estimate data requirements of machine learning systems to gain savings in both development time and data acquisition costs.},
	urldate = {2022-07-19},
	publisher = {arXiv},
	author = {Mahmood, Rafid and Lucas, James and Acuna, David and Li, Daiqing and Philion, Jonah and Alvarez, Jose M. and Yu, Zhiding and Fidler, Sanja and Law, Marc T.},
	month = jul,
	year = {2022},
	note = {arXiv:2207.01725 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KRREE6DQ/Mahmood et al. - 2022 - How Much More Data Do I Need Estimating Requireme.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/866VZNC8/2207.html:text/html},
}

@misc{rackauckas_universal_2021,
	title = {Universal {Differential} {Equations} for {Scientific} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2001.04385},
	doi = {10.48550/arXiv.2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	month = nov,
	year = {2021},
	note = {arXiv:2001.04385 [cs, math, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6JVZNSCZ/Rackauckas et al. - 2021 - Universal Differential Equations for Scientific Ma.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/W9QDS7B8/2001.html:text/html},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2022-07-09},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv:1806.07366 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5KNSYTYD/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ZYXLSSH9/1806.html:text/html},
}

@misc{shokri_membership_2017,
	title = {Membership {Inference} {Attacks} against {Machine} {Learning} {Models}},
	url = {http://arxiv.org/abs/1610.05820},
	doi = {10.48550/arXiv.1610.05820},
	abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
	month = mar,
	year = {2017},
	note = {arXiv:1610.05820 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/PLHKR238/Shokri et al. - 2017 - Membership Inference Attacks against Machine Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CHEUL44Z/1610.html:text/html},
}

@misc{ginart_making_2019,
	title = {Making {AI} {Forget} {You}: {Data} {Deletion} in {Machine} {Learning}},
	shorttitle = {Making {AI} {Forget} {You}},
	url = {http://arxiv.org/abs/1907.05012},
	doi = {10.48550/arXiv.1907.05012},
	abstract = {Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100X improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.},
	urldate = {2022-07-04},
	publisher = {arXiv},
	author = {Ginart, Antonio and Guan, Melody Y. and Valiant, Gregory and Zou, James},
	month = nov,
	year = {2019},
	note = {arXiv:1907.05012 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/2DVH9KGL/Ginart et al. - 2019 - Making AI Forget You Data Deletion in Machine Lea.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TLVVL73A/1907.html:text/html},
}

@article{alquier_pac-bayesian_2008,
	title = {{PAC}-{Bayesian} {Bounds} for {Randomized} {Empirical} {Risk} {Minimizers}},
	volume = {17},
	issn = {1066-5307, 1934-8045},
	url = {http://arxiv.org/abs/0712.1698},
	doi = {10.3103/S1066530708040017},
	abstract = {The aim of this paper is to generalize the PAC-Bayesian theorems proved by Catoni in the classification setting to more general problems of statistical inference. We show how to control the deviations of the risk of randomized estimators. A particular attention is paid to randomized estimators drawn in a small neighborhood of classical estimators, whose study leads to control the risk of the latter. These results allow to bound the risk of very general estimation procedures, as well as to perform model selection.},
	number = {4},
	urldate = {2022-07-01},
	journal = {Mathematical Methods of Statistics},
	author = {Alquier, Pierre},
	month = dec,
	year = {2008},
	note = {arXiv:0712.1698 [math, stat]},
	keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory},
	pages = {279--304},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/9IW5EQ5E/Alquier - 2008 - PAC-Bayesian Bounds for Randomized Empirical Risk .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/77V9FFRE/0712.html:text/html},
}

@misc{kingma_variational_2022,
	title = {Variational {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2107.00630},
	doi = {10.48550/arXiv.2107.00630},
	abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum. Code is available at https://github.com/google-research/vdm .},
	urldate = {2022-06-30},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
	month = jun,
	year = {2022},
	note = {arXiv:2107.00630 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7XF6CDKA/Kingma et al. - 2022 - Variational Diffusion Models.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QY6V9YCP/2107.html:text/html},
}

@inproceedings{jagielski_auditing_2020,
	title = {Auditing {Differentially} {Private} {Machine} {Learning}: {How} {Private} is {Private} {SGD}?},
	volume = {33},
	shorttitle = {Auditing {Differentially} {Private} {Machine} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fc4ddc15f9f4b4b06ef7844d6bb53abf-Abstract.html},
	abstract = {We investigate whether Differentially Private SGD offers better privacy in practice than what is guaranteed by its state-of-the-art analysis. We do so via novel data poisoning attacks, which we show correspond to realistic privacy attacks. While previous work (Ma et al., arXiv 2019) proposed this connection between differential privacy and data poisoning as a defense against data poisoning, our use as a tool for understanding the privacy of a specific mechanism is new. More generally, our work takes a quantitative, empirical approach to understanding the privacy afforded by specific implementations of differentially private algorithms that we believe has the potential to complement and influence analytical work on differential privacy.},
	urldate = {2022-06-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Jagielski, Matthew and Ullman, Jonathan and Oprea, Alina},
	year = {2020},
	pages = {22205--22216},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FRGQZKAH/Jagielski et al. - 2020 - Auditing Differentially Private Machine Learning .pdf:application/pdf},
}

@inproceedings{bassily_model-agnostic_2018,
	title = {Model-{Agnostic} {Private} {Learning}},
	volume = {31},
	url = {https://papers.nips.cc/paper/2018/hash/aa97d584861474f4097cf13ccb5325da-Abstract.html},
	urldate = {2022-06-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bassily, Raef and Thakkar, Om and Guha Thakurta, Abhradeep},
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/5HI3MHZ8/Bassily et al. - 2018 - Model-Agnostic Private Learning.pdf:application/pdf},
}

@inproceedings{papernot_scalable_2022,
	title = {Scalable {Private} {Learning} with {PATE}},
	url = {https://openreview.net/forum?id=rkZB1XbRZ},
	abstract = {The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal...},
	language = {en},
	urldate = {2022-06-30},
	author = {Papernot, Nicolas and Song, Shuang and Mironov, Ilya and Raghunathan, Ananth and Talwar, Kunal and Erlingsson, Ulfar},
	month = feb,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/DGGLJU2P/Papernot et al. - 2022 - Scalable Private Learning with PATE.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/YAP5KE3Z/forum.html:text/html},
}

@misc{papernot_tempered_2020,
	title = {Tempered {Sigmoid} {Activations} for {Deep} {Learning} with {Differential} {Privacy}},
	url = {http://arxiv.org/abs/2007.14191},
	doi = {10.48550/arXiv.2007.14191},
	abstract = {Because learning sometimes involves sensitive data, machine learning algorithms have been extended to offer privacy for training data. In practice, this has been mostly an afterthought, with privacy-preserving models obtained by re-running training with a different optimizer, but using the model architectures that already performed well in a non-privacy-preserving setting. This approach leads to less than ideal privacy/utility tradeoffs, as we show here. Instead, we propose that model architectures are chosen ab initio explicitly for privacy-preserving training. To provide guarantees under the gold standard of differential privacy, one must bound as strictly as possible how individual training points can possibly affect model updates. In this paper, we are the first to observe that the choice of activation function is central to bounding the sensitivity of privacy-preserving deep learning. We demonstrate analytically and experimentally how a general family of bounded activation functions, the tempered sigmoids, consistently outperform unbounded activation functions like ReLU. Using this paradigm, we achieve new state-of-the-art accuracy on MNIST, FashionMNIST, and CIFAR10 without any modification of the learning procedure fundamentals or differential privacy analysis.},
	urldate = {2022-06-30},
	publisher = {arXiv},
	author = {Papernot, Nicolas and Thakurta, Abhradeep and Song, Shuang and Chien, Steve and Erlingsson, Úlfar},
	month = jul,
	year = {2020},
	note = {arXiv:2007.14191 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/K33JBN9D/Papernot et al. - 2020 - Tempered Sigmoid Activations for Deep Learning wit.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/L2G7N6AS/2007.html:text/html},
}

@inproceedings{zhang_how_2022,
	title = {How to {Robustify} {Black}-{Box} {ML} {Models}? {A} {Zeroth}-{Order} {Optimization} {Perspective}},
	shorttitle = {How to {Robustify} {Black}-{Box} {ML} {Models}?},
	url = {https://openreview.net/forum?id=W9G_ImpHlQd},
	abstract = {The lack of adversarial robustness has been recognized as an important issue for state-of-the-art machine learning (ML) models, e.g., deep neural networks (DNNs). Thereby, robustifying ML models...},
	language = {en},
	urldate = {2022-06-28},
	author = {Zhang, Yimeng and Yao, Yuguang and Jia, Jinghan and Yi, Jinfeng and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MZTYUVDK/Zhang et al. - 2022 - How to Robustify Black-Box ML Models A Zeroth-Ord.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/9DBYRX5C/forum.html:text/html},
}

@inproceedings{chen_bridging_2022,
	title = {On {Bridging} {Generic} and {Personalized} {Federated} {Learning} for {Image} {Classification}},
	url = {https://openreview.net/forum?id=I1hQbx10Kxn},
	abstract = {Federated learning is promising for its capability to collaboratively train models with multiple clients without accessing their data, but vulnerable when clients' data distributions diverge from...},
	language = {en},
	urldate = {2022-06-28},
	author = {Chen, Hong-You and Chao, Wei-Lun},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FSYRETW9/Chen and Chao - 2022 - On Bridging Generic and Personalized Federated Lea.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/5U33D22F/forum.html:text/html},
}

@inproceedings{miao_continual_2022,
	title = {Continual {Learning} with {Filter} {Atom} {Swapping}},
	url = {https://openreview.net/forum?id=metRpM4Zrcb},
	abstract = {Continual learning has been widely studied in recent years to resolve the catastrophic forgetting of deep neural networks. In this paper, we first enforce a low-rank filter subspace by decomposing...},
	language = {en},
	urldate = {2022-06-28},
	author = {Miao, Zichen and Wang, Ze and Chen, Wei and Qiu, Qiang},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/EQ77NDJU/Miao et al. - 2022 - Continual Learning with Filter Atom Swapping.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/3SD3FG2R/forum.html:text/html},
}

@inproceedings{salimans_progressive_2022,
	title = {Progressive {Distillation} for {Fast} {Sampling} of {Diffusion} {Models}},
	url = {https://openreview.net/forum?id=TIdIXIpzhoI},
	abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their...},
	language = {en},
	urldate = {2022-06-28},
	author = {Salimans, Tim and Ho, Jonathan},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/GPCTR7HB/Salimans and Ho - 2022 - Progressive Distillation for Fast Sampling of Diff.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/4XEBXC39/forum.html:text/html},
}

@inproceedings{papernot_hyperparameter_2022,
	title = {Hyperparameter {Tuning} with {Renyi} {Differential} {Privacy}},
	url = {https://openreview.net/forum?id=-70L8lpp9DF},
	abstract = {For many differentially private algorithms, such as the prominent noisy stochastic gradient descent (DP-SGD), the analysis needed to bound the privacy leakage of a single training run is well...},
	language = {en},
	urldate = {2022-06-28},
	author = {Papernot, Nicolas and Steinke, Thomas},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/B7ALKDEZ/Papernot and Steinke - 2022 - Hyperparameter Tuning with Renyi Differential Priv.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/Z3SPTITA/forum.html:text/html},
}

@inproceedings{li_large_2022,
	title = {Large {Language} {Models} {Can} {Be} {Strong} {Differentially} {Private} {Learners}},
	url = {https://openreview.net/forum?id=bVuP3ltATMz},
	abstract = {Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient...},
	language = {en},
	urldate = {2022-06-28},
	author = {Li, Xuechen and Tramer, Florian and Liang, Percy and Hashimoto, Tatsunori},
	month = mar,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZDD4E6PL/Li et al. - 2022 - Large Language Models Can Be Strong Differentially.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/L5A6TVS6/forum.html:text/html},
}

@inproceedings{zhang_reinforcement_2022,
	title = {Reinforcement {Learning} under a {Multi}-agent {Predictive} {State} {Representation} {Model}: {Method} and {Theory}},
	shorttitle = {Reinforcement {Learning} under a {Multi}-agent {Predictive} {State} {Representation} {Model}},
	url = {https://openreview.net/forum?id=PLDOnFoVm4},
	abstract = {We study reinforcement learning for partially observable multi-agent systems where each agent only has access to its own observation and reward and aims to maximize its cumulative rewards. To...},
	language = {en},
	urldate = {2022-06-28},
	author = {Zhang, Zhi and Yang, Zhuoran and Liu, Han and Tokekar, Pratap and Huang, Furong},
	month = may,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/UVZVM4FS/Zhang et al. - 2022 - Reinforcement Learning under a Multi-agent Predict.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/KXFCUAZP/forum.html:text/html},
}

@misc{ramachandran_searching_2017,
	title = {Searching for {Activation} {Functions}},
	url = {http://arxiv.org/abs/1710.05941},
	abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
	month = oct,
	year = {2017},
	note = {Number: arXiv:1710.05941
arXiv:1710.05941 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8WC97NMT/Ramachandran et al. - 2017 - Searching for Activation Functions.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3ZQRLTAF/1710.html:text/html},
}

@inproceedings{mcallester_pac-bayesian_1999,
	address = {Santa Cruz, California, United States},
	title = {{PAC}-{Bayesian} model averaging},
	isbn = {978-1-58113-167-3},
	url = {http://portal.acm.org/citation.cfm?doid=307400.307435},
	doi = {10.1145/307400.307435},
	abstract = {PAC-Bayesian learning methods combine the informative priors of Bayesian methods with distribution-free PAC guarantees. Building on earlier methods for PAC-Bayesian model selection, this paper presents a method for PACBayesian model averaging. The method constructs an optimized weighted mixture of concepts analogous to a Bayesian posterior distribution. Although the main result is stated for bounded loss, a preliminary analysis for unbounded loss is also given.},
	language = {en},
	urldate = {2022-05-28},
	booktitle = {Proceedings of the twelfth annual conference on {Computational} learning theory  - {COLT} '99},
	publisher = {ACM Press},
	author = {McAllester, David A.},
	year = {1999},
	pages = {164--170},
	file = {McAllester - 1999 - PAC-Bayesian model averaging.pdf:/Users/tokio/Zotero/storage/6KWEQQ5S/McAllester - 1999 - PAC-Bayesian model averaging.pdf:application/pdf},
}

@inproceedings{mcallester_pac-bayesian_1998,
	address = {Madison, Wisconsin, United States},
	title = {Some {PAC}-{Bayesian} theorems},
	isbn = {978-1-58113-057-7},
	url = {http://portal.acm.org/citation.cfm?doid=279943.279989},
	doi = {10.1145/279943.279989},
	abstract = {This paper gives PAC guarantees for “Bayesian” algorithms—algorithms that optimize risk minimization expressions involving a prior probability and a likelihood for the training data. PAC-Bayesian algorithms are motivated by a desire to provide an informative prior encoding information about the expected experimental setting but still having PAC performance guarantees over all IID settings. The PAC-Bayesian theorems given here apply to an arbitrary prior measure on an arbitrary concept space. These theorems provide an alternative to the use of VC dimension in proving PAC bounds for parameterized concepts.},
	language = {en},
	urldate = {2022-05-28},
	booktitle = {Proceedings of the eleventh annual conference on {Computational} learning theory  - {COLT}' 98},
	publisher = {ACM Press},
	author = {McAllester, David A.},
	year = {1998},
	pages = {230--234},
	file = {McAllester - 1998 - Some PAC-Bayesian theorems.pdf:/Users/tokio/Zotero/storage/ITTJH99N/McAllester - 1998 - Some PAC-Bayesian theorems.pdf:application/pdf},
}

@techreport{grunwald_tight_2017,
	title = {A {Tight} {Excess} {Risk} {Bound} via a {Unified} {PAC}-{Bayesian}-{Rademacher}-{Shtarkov}-{MDL} {Complexity}},
	url = {http://arxiv.org/abs/1710.07732},
	abstract = {We present a novel notion of complexity that interpolates between and generalizes some classic existing complexity notions in learning theory: for estimators like empirical risk minimization (ERM) with arbitrary bounded losses, it is upper bounded in terms of data-independent Rademacher complexity; for generalized Bayesian estimators, it is upper bounded by the data-dependent information complexity (also known as stochastic or PAC-Bayesian, \${\textbackslash}mathrm\{KL\}({\textbackslash}text\{posterior\} {\textbackslash}operatorname\{{\textbackslash}{\textbar}\} {\textbackslash}text\{prior\})\$ complexity. For (penalized) ERM, the new complexity reduces to (generalized) normalized maximum likelihood (NML) complexity, i.e. a minimax log-loss individual-sequence regret. Our first main result bounds excess risk in terms of the new complexity. Our second main result links the new complexity via Rademacher complexity to \$L\_2(P)\$ entropy, thereby generalizing earlier results of Opper, Haussler, Lugosi, and Cesa-Bianchi who did the log-loss case with \$L\_{\textbackslash}infty\$. Together, these results recover optimal bounds for VC- and large (polynomial entropy) classes, replacing localized Rademacher complexity by a simpler analysis which almost completely separates the two aspects that determine the achievable rates: 'easiness' (Bernstein) conditions and model complexity.},
	number = {arXiv:1710.07732},
	urldate = {2022-05-27},
	institution = {arXiv},
	author = {Grünwald, Peter D. and Mehta, Nishant A.},
	month = oct,
	year = {2017},
	doi = {10.48550/arXiv.1710.07732},
	note = {arXiv:1710.07732 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WLS48NQ2/Grünwald and Mehta - 2017 - A Tight Excess Risk Bound via a Unified PAC-Bayesi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/7B7TDNU6/1710.html:text/html},
}

@article{barnett_introduction_nodate,
	title = {Introduction to {Quantum} {Information}},
	language = {en},
	author = {Barnett, Stephen M},
	pages = {64},
	file = {Barnett - Introduction to Quantum Information.pdf:/Users/tokio/Zotero/storage/IDUZPVEW/Barnett - Introduction to Quantum Information.pdf:application/pdf},
}

@techreport{suzuki_generalization_2020,
	title = {Generalization bound of globally optimal non-convex neural network training: {Transportation} map estimation by infinite dimensional {Langevin} dynamics},
	shorttitle = {Generalization bound of globally optimal non-convex neural network training},
	url = {http://arxiv.org/abs/2007.05824},
	abstract = {We introduce a new theoretical framework to analyze deep learning optimization with connection to its generalization error. Existing frameworks such as mean field theory and neural tangent kernel theory for neural network optimization analysis typically require taking limit of infinite width of the network to show its global convergence. This potentially makes it difficult to directly deal with finite width network; especially in the neural tangent kernel regime, we cannot reveal favorable properties of neural networks beyond kernel methods. To realize more natural analysis, we consider a completely different approach in which we formulate the parameter training as a transportation map estimation and show its global convergence via the theory of the infinite dimensional Langevin dynamics. This enables us to analyze narrow and wide networks in a unifying manner. Moreover, we give generalization gap and excess risk bounds for the solution obtained by the dynamics. The excess risk bound achieves the so-called fast learning rate. In particular, we show an exponential convergence for a classification problem and a minimax optimal rate for a regression problem.},
	number = {arXiv:2007.05824},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Suzuki, Taiji},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2007.05824},
	note = {arXiv:2007.05824 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7SZ68KK2/Suzuki - 2020 - Generalization bound of globally optimal non-conve.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LDRT7DNH/2007.html:text/html},
}

@techreport{rodriguez-galvez_tighter_2022,
	title = {Tighter expected generalization error bounds via {Wasserstein} distance},
	url = {http://arxiv.org/abs/2101.09315},
	abstract = {This work presents several expected generalization error bounds based on the Wasserstein distance. More specifically, it introduces full-dataset, single-letter, and random-subset bounds, and their analogues in the randomized subsample setting from Steinke and Zakynthinou [1]. Moreover, when the loss function is bounded and the geometry of the space is ignored by the choice of the metric in the Wasserstein distance, these bounds recover from below (and thus, are tighter than) current bounds based on the relative entropy. In particular, they generate new, non-vacuous bounds based on the relative entropy. Therefore, these results can be seen as a bridge between works that account for the geometry of the hypothesis space and those based on the relative entropy, which is agnostic to such geometry. Furthermore, it is shown how to produce various new bounds based on different information measures (e.g., the lautum information or several \$f\$-divergences) based on these bounds and how to derive similar bounds with respect to the backward channel using the presented proof techniques.},
	number = {arXiv:2101.09315},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Rodríguez-Gálvez, Borja and Bassi, Germán and Thobaben, Ragnar and Skoglund, Mikael},
	month = mar,
	year = {2022},
	doi = {10.48550/arXiv.2101.09315},
	note = {arXiv:2101.09315 [cs, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/54YX8MZA/Rodríguez-Gálvez et al. - 2022 - Tighter expected generalization error bounds via W.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UMX9SAS8/2101.html:text/html},
}

@article{guedj_still_2021,
	title = {Still no free lunches: the price to pay for tighter {PAC}-{Bayes} bounds},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {Still no free lunches},
	url = {http://arxiv.org/abs/1910.04460},
	doi = {10.3390/e23111529},
	abstract = {"No free lunch" results state the impossibility of obtaining meaningful bounds on the error of a learning algorithm without prior assumptions and modelling. Some models are expensive (strong assumptions, such as as subgaussian tails), others are cheap (simply finite variance). As it is well known, the more you pay, the more you get: in other words, the most expensive models yield the more interesting bounds. Recent advances in robust statistics have investigated procedures to obtain tight bounds while keeping the cost minimal. The present paper explores and exhibits what the limits are for obtaining tight PAC-Bayes bounds in a robust setting for cheap models, addressing the question: is PAC-Bayes good value for money?},
	number = {11},
	urldate = {2022-05-23},
	journal = {Entropy},
	author = {Guedj, Benjamin and Pujol, Louis},
	month = nov,
	year = {2021},
	note = {arXiv:1910.04460 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	pages = {1529},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/72RS36IN/Guedj and Pujol - 2021 - Still no free lunches the price to pay for tighte.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/B36ECEP8/1910.html:text/html},
}

@article{wang_pac-bayes_2022,
	title = {{PAC}-{Bayes} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/2109.14509},
	abstract = {Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the compression of information stored in weights (IIW) is proved to play a key role in NNs generalization based on the PAC-Bayes theorem. However, no solution of IIW has ever been provided, which builds a barrier for further investigation of the IIW's property and its potential in practical deep learning. In this paper, we propose an algorithm for the efficient approximation of IIW. Then, we build an IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the fitting to compressing phase transition during NNs' training and the concrete connection between the IIW compression and the generalization. Besides, we verify that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, over-parameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which fulfills the potential of IIW in enhancing NNs in practice.},
	urldate = {2022-04-22},
	journal = {arXiv:2109.14509 [cs]},
	author = {Wang, Zifeng and Huang, Shao-Lun and Kuruoglu, Ercan E. and Sun, Jimeng and Chen, Xi and Zheng, Yefeng},
	month = mar,
	year = {2022},
	note = {arXiv: 2109.14509},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FD22HD2S/Wang et al. - 2022 - PAC-Bayes Information Bottleneck.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QSWHQ4RS/2109.html:text/html},
}

@article{zhou_individually_2020,
	title = {Individually {Conditional} {Individual} {Mutual} {Information} {Bound} on {Generalization} {Error}},
	url = {http://arxiv.org/abs/2012.09922},
	abstract = {We propose a new information-theoretic bound on generalization error based on a combination of the error decomposition technique of Bu et al. and the conditional mutual information (CMI) construction of Steinke and Zakynthinou. In a previous work, Haghifam et al. proposed a different bound combining the two aforementioned techniques, which we refer to as the conditional individual mutual information (CIMI) bound. However, in a simple Gaussian setting, both the CMI and the CIMI bounds are order-wise worse than that by Bu et al.. This observation motivated us to propose the new bound, which overcomes this issue by reducing the conditioning terms in the conditional mutual information. In the process of establishing this bound, a conditional decoupling lemma is established, which also leads to a meaningful dichotomy and comparison among these information-theoretic bounds.},
	urldate = {2022-04-22},
	journal = {arXiv:2012.09922 [cs, math, stat]},
	author = {Zhou, Ruida and Tian, Chao and Liu, Tie},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.09922},
	keywords = {Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/7PGCSNFQ/Zhou et al. - 2020 - Individually Conditional Individual Mutual Informa.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/SKX6DSTM/2012.html:text/html},
}

@article{zhivotovskiy_localization_2017,
	title = {Localization of {VC} {Classes}: {Beyond} {Local} {Rademacher} {Complexities}},
	shorttitle = {Localization of {VC} {Classes}},
	url = {http://arxiv.org/abs/1606.00922},
	abstract = {In this paper we introduce an alternative localization approach for binary classification that leads to a novel complexity measure: fixed points of the local empirical entropy. We show that this complexity measure gives a tight control over complexity in the upper bounds. Our results are accompanied by a novel minimax lower bound that involves the same quantity. In particular, we practically answer the question of optimality of ERM under bounded noise for general VC classes.},
	urldate = {2022-04-06},
	journal = {arXiv:1606.00922 [math, stat]},
	author = {Zhivotovskiy, Nikita and Hanneke, Steve},
	month = dec,
	year = {2017},
	note = {arXiv: 1606.00922},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/CNNYDEIB/Zhivotovskiy and Hanneke - 2017 - Localization of VC Classes Beyond Local Rademache.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/E3SQ8BXR/1606.html:text/html},
}

@article{foong_how_2022,
	title = {How {Tight} {Can} {PAC}-{Bayes} be in the {Small} {Data} {Regime}?},
	url = {http://arxiv.org/abs/2106.03542},
	abstract = {In this paper, we investigate the question: Given a small number of datapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be made? For such small datasets, test set bounds adversely affect generalisation performance by withholding data from the training procedure. In this setting, PAC-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes theorem of Germain et al. While their theorem is known to recover many existing PAC-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. For a fixed learning algorithm and dataset, we show that the tightest possible bound coincides with a bound considered by Catoni; and, in the more natural case of distributions over datasets, we establish a lower bound on the best bound achievable in expectation. Interestingly, this lower bound recovers the Chernoff test set bound if the posterior is equal to the prior. Moreover, to illustrate how tight these bounds can be, we study synthetic one-dimensional classification tasks in which it is feasible to meta-learn both the prior and the form of the bound to numerically optimise for the tightest bounds possible. We find that in this simple, controlled scenario, PAC-Bayes bounds are competitive with comparable, commonly used Chernoff test set bounds. However, the sharpest test set bounds still lead to better guarantees on the generalisation error than the PAC-Bayes bounds we consider.},
	urldate = {2022-04-05},
	journal = {arXiv:2106.03542 [cs, math, stat]},
	author = {Foong, Andrew Y. K. and Bruinsma, Wessel P. and Burt, David R. and Turner, Richard E.},
	month = jan,
	year = {2022},
	note = {arXiv: 2106.03542},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/M2U3R2LE/Foong et al. - 2022 - How Tight Can PAC-Bayes be in the Small Data Regim.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RA34CHJR/2106.html:text/html},
}

@article{alquier_properties_2016,
	title = {On the properties of variational approximations of {Gibbs} posteriors},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-290.html},
	abstract = {The PAC-Bayesian approach is a powerful set of techniques to derive non-asymptotic risk bounds for random estimators. The corresponding optimal distribution of estimators, usually called the Gibbs posterior, is unfortunately often intractable. One may sample from it using Markov chain Monte Carlo, but this is usually too slow for big datasets. We consider instead variational approximations of the Gibbs posterior, which are fast to compute. We undertake a general study of the properties of such approximations. Our main finding is that such a variational approximation has often the same rate of convergence as the original PAC-Bayesian procedure it approximates. In addition, we show that, when the risk function is convex, a variational approximation can be obtained in polynomial time using a convex solver. We give finite sample oracle inequalities for the corresponding estimator. We specialize our results to several learning tasks (classification, ranking, matrix completion), discuss how to implement a variational approximation in each case, and illustrate the good properties of said approximation on real datasets.},
	number = {236},
	urldate = {2022-04-05},
	journal = {Journal of Machine Learning Research},
	author = {Alquier, Pierre and Ridgway, James and Chopin, Nicolas},
	year = {2016},
	pages = {1--41},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/QQSG2PYC/Alquier et al. - 2016 - On the properties of variational approximations of.pdf:application/pdf},
}

@article{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2202.02435},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
	urldate = {2022-04-05},
	journal = {arXiv:2202.02435 [cs, math, stat]},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.02435},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Classical Analysis and ODEs},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/79TH8CG2/Kidger - 2022 - On Neural Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WTBEAFDR/2202.html:text/html},
}

@article{haddouche_pac-bayes_2021,
	title = {{PAC}-{Bayes} unleashed: generalisation bounds with unbounded losses},
	volume = {23},
	issn = {1099-4300},
	shorttitle = {{PAC}-{Bayes} unleashed},
	url = {http://arxiv.org/abs/2006.07279},
	doi = {10.3390/e23101330},
	abstract = {We present new PAC-Bayesian generalisation bounds for learning problems with unbounded loss functions. This extends the relevance and applicability of the PAC-Bayes learning framework, where most of the existing literature focuses on supervised learning problems with a bounded loss function (typically assumed to take values in the interval [0;1]). In order to relax this assumption, we propose a new notion called HYPE (standing for {\textbackslash}emph\{HYPothesis-dependent rangE\}), which effectively allows the range of the loss to depend on each predictor. Based on this new notion we derive a novel PAC-Bayesian generalisation bound for unbounded loss functions, and we instantiate it on a linear regression problem. To make our theory usable by the largest audience possible, we include discussions on actual computation, practicality and limitations of our assumptions.},
	number = {10},
	urldate = {2022-03-30},
	journal = {Entropy},
	author = {Haddouche, Maxime and Guedj, Benjamin and Rivasplata, Omar and Shawe-Taylor, John},
	month = oct,
	year = {2021},
	note = {arXiv: 2006.07279},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	pages = {1330},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/72VPSBFT/Haddouche et al. - 2021 - PAC-Bayes unleashed generalisation bounds with un.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WPXFT7ER/2006.html:text/html},
}

@article{lever_tighter_2013,
	title = {Tighter {PAC}-{Bayes} bounds through distribution-dependent priors},
	volume = {473},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397512009346},
	doi = {10.1016/j.tcs.2012.10.013},
	abstract = {We further develop the idea that the PAC-Bayes prior can be informed by the datagenerating distribution. We use this framework to prove sharp risk bounds for stochastic exponential weights algorithms, and develop insights into controlling function class complexity in this method. In particular we consider controlling capacity with respect to the unknown geometry defined by the data-generating distribution. We also use the method to obtain new bounds for RKHS regularization schemes such as SVMs.},
	language = {en},
	urldate = {2022-03-28},
	journal = {Theoretical Computer Science},
	author = {Lever, Guy and Laviolette, François and Shawe-Taylor, John},
	month = feb,
	year = {2013},
	pages = {4--28},
	file = {Lever et al. - 2013 - Tighter PAC-Bayes bounds through distribution-depe.pdf:/Users/tokio/Zotero/storage/36P4GJ69/Lever et al. - 2013 - Tighter PAC-Bayes bounds through distribution-depe.pdf:application/pdf},
}

@article{hellstrom_generalization_2020,
	title = {Generalization {Bounds} via {Information} {Density} and {Conditional} {Information} {Density}},
	volume = {1},
	issn = {2641-8770},
	url = {http://arxiv.org/abs/2005.08044},
	doi = {10.1109/JSAIT.2020.3040992},
	abstract = {We present a general approach, based on an exponential inequality, to derive bounds on the generalization error of randomized learning algorithms. Using this approach, we provide bounds on the average generalization error as well as bounds on its tail probability, for both the PAC-Bayesian and single-draw scenarios. Specifically, for the case of subgaussian loss functions, we obtain novel bounds that depend on the information density between the training data and the output hypothesis. When suitably weakened, these bounds recover many of the information-theoretic available bounds in the literature. We also extend the proposed exponential-inequality approach to the setting recently introduced by Steinke and Zakynthinou (2020), where the learning algorithm depends on a randomly selected subset of the available training data. For this setup, we present bounds for bounded loss functions in terms of the conditional information density between the output hypothesis and the random variable determining the subset choice, given all training data. Through our approach, we recover the average generalization bound presented by Steinke and Zakynthinou (2020) and extend it to the PAC-Bayesian and single-draw scenarios. For the single-draw scenario, we also obtain novel bounds in terms of the conditional \${\textbackslash}alpha\$-mutual information and the conditional maximal leakage.},
	number = {3},
	urldate = {2022-03-27},
	journal = {IEEE Journal on Selected Areas in Information Theory},
	author = {Hellström, Fredrik and Durisi, Giuseppe},
	month = nov,
	year = {2020},
	note = {arXiv: 2005.08044},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	pages = {824--839},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IKQGLQMZ/Hellström and Durisi - 2020 - Generalization Bounds via Information Density and .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/S7PML9IN/2005.html:text/html},
}

@article{asadi_chaining_2019,
	title = {Chaining {Mutual} {Information} and {Tightening} {Generalization} {Bounds}},
	url = {http://arxiv.org/abs/1806.03803},
	abstract = {Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm's input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou '15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine the chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley's inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.},
	urldate = {2022-03-27},
	journal = {arXiv:1806.03803 [cs, math, stat]},
	author = {Asadi, Amir R. and Abbe, Emmanuel and Verdú, Sergio},
	month = jul,
	year = {2019},
	note = {arXiv: 1806.03803},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Probability, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/FFGPXUVL/Asadi et al. - 2019 - Chaining Mutual Information and Tightening General.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GS4M8U7Y/1806.html:text/html},
}

@incollection{hutter_tuning_2007,
	address = {Berlin, Heidelberg},
	title = {Tuning {Bandit} {Algorithms} in {Stochastic} {Environments}},
	volume = {4754},
	isbn = {978-3-540-75224-0 978-3-540-75225-7},
	url = {http://link.springer.com/10.1007/978-3-540-75225-7_15},
	abstract = {Algorithms based on upper-conﬁdence bounds for balancing exploration and exploitation are gaining popularity since they are easy to implement, eﬃcient and eﬀective. In this paper we consider a variant of the basic algorithm for the stochastic, multi-armed bandit problem that takes into account the empirical variance of the diﬀerent arms. In earlier experimental works, such algorithms were found to outperform the competing algorithms. The purpose of this paper is to provide a theoretical explanation of these ﬁndings and provide theoretical guidelines for the tuning of the parameters of these algorithms. For this we analyze the expected regret and for the ﬁrst time the concentration of the regret. The analysis of the expected regret shows that variance estimates can be especially advantageous when the payoﬀs of suboptimal arms have low variance. The risk analysis, rather unexpectedly, reveals that except some very special bandit problems, for upper conﬁdence bound based algorithms with standard bias sequences, the regret concentrates only at a polynomial rate. Hence, although these algorithms achieve logarithmic expected regret rates, they seem less attractive when the risk of achieving much worse than logarithmic cumulative regret is also taken into account.},
	language = {en},
	urldate = {2022-03-27},
	booktitle = {Algorithmic {Learning} {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Audibert, Jean-Yves and Munos, Rémi and Szepesvári, Csaba},
	editor = {Hutter, Marcus and Servedio, Rocco A. and Takimoto, Eiji},
	year = {2007},
	doi = {10.1007/978-3-540-75225-7_15},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {150--165},
	file = {Audibert et al. - 2007 - Tuning Bandit Algorithms in Stochastic Environment.pdf:/Users/tokio/Zotero/storage/ZDQ4VR9V/Audibert et al. - 2007 - Tuning Bandit Algorithms in Stochastic Environment.pdf:application/pdf},
}

@inproceedings{mnih_empirical_2008,
	address = {Helsinki, Finland},
	title = {Empirical {Bernstein} stopping},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390241},
	doi = {10.1145/1390156.1390241},
	abstract = {Sampling is a popular way of scaling up machine learning algorithms to large datasets. The question often is how many samples are needed. Adaptive stopping algorithms monitor the performance in an online fashion and they can stop early, saving valuable resources. We consider problems where probabilistic guarantees are desired and demonstrate how recently-introduced empirical Bernstein bounds can be used to design stopping rules that are eﬃcient. We provide upper bounds on the sample complexity of the new rules, as well as empirical results on model selection and boosting in the ﬁltering setting.},
	language = {en},
	urldate = {2022-03-27},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Mnih, Volodymyr and Szepesvári, Csaba and Audibert, Jean-Yves},
	year = {2008},
	pages = {672--679},
	file = {Mnih et al. - 2008 - Empirical Bernstein stopping.pdf:/Users/tokio/Zotero/storage/E5HCYVBX/Mnih et al. - 2008 - Empirical Bernstein stopping.pdf:application/pdf},
}

@article{maurer_empirical_2009,
	title = {Empirical {Bernstein} {Bounds} and {Sample} {Variance} {Penalization}},
	url = {http://arxiv.org/abs/0907.3740},
	abstract = {We give improved constants for data dependent and variance sensitive confidence bounds, called empirical Bernstein bounds, and extend these inequalities to hold uniformly over classes of functionswhose growth function is polynomial in the sample size n. The bounds lead us to consider sample variance penalization, a novel learning method which takes into account the empirical variance of the loss function. We give conditions under which sample variance penalization is effective. In particular, we present a bound on the excess risk incurred by the method. Using this, we argue that there are situations in which the excess risk of our method is of order 1/n, while the excess risk of empirical risk minimization is of order 1/sqrt/\{n\}. We show some experimental results, which confirm the theory. Finally, we discuss the potential application of our results to sample compression schemes.},
	urldate = {2022-03-27},
	journal = {arXiv:0907.3740 [stat]},
	author = {Maurer, Andreas and Pontil, Massimiliano},
	month = jul,
	year = {2009},
	note = {arXiv: 0907.3740},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EC4898JZ/Maurer and Pontil - 2009 - Empirical Bernstein Bounds and Sample Variance Pen.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F6NM6B3Q/0907.html:text/html},
}

@article{seldin_pac-bayesian_2012,
	title = {{PAC}-{Bayesian} {Inequalities} for {Martingales}},
	url = {http://arxiv.org/abs/1110.6886},
	abstract = {We present a set of high-probability inequalities that control the concentration of weighted averages of multiple (possibly uncountably many) simultaneously evolving and interdependent martingales. Our results extend the PAC-Bayesian analysis in learning theory from the i.i.d. setting to martingales opening the way for its application to importance weighted sampling, reinforcement learning, and other interactive learning domains, as well as many other domains in probability theory and statistics, where martingales are encountered. We also present a comparison inequality that bounds the expectation of a convex function of a martingale difference sequence shifted to the [0,1] interval by the expectation of the same function of independent Bernoulli variables. This inequality is applied to derive a tighter analog of Hoeffding-Azuma's inequality.},
	urldate = {2022-03-26},
	journal = {arXiv:1110.6886 [cs, math, stat]},
	author = {Seldin, Yevgeny and Laviolette, François and Cesa-Bianchi, Nicolò and Shawe-Taylor, John and Auer, Peter},
	month = jul,
	year = {2012},
	note = {arXiv: 1110.6886},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/XR8F42WZ/Seldin et al. - 2012 - PAC-Bayesian Inequalities for Martingales.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/H7IJSYWT/1110.html:text/html},
}

@inproceedings{steinke_reasoning_2020,
	title = {Reasoning {About} {Generalization} via {Conditional} {Mutual} {Information}},
	url = {https://proceedings.mlr.press/v125/steinke20a.html},
	abstract = {We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify how well the input (i.e., the training data) can be recognized given the output (i.e., the trained model) of the learning algorithm. We show that bounds on CMI can be obtained from VC dimension, compression schemes, differential privacy, and other methods. We then show that bounded CMI implies various forms of generalization.},
	language = {en},
	urldate = {2022-03-26},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Steinke, Thomas and Zakynthinou, Lydia},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3437--3452},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/UHIPEBP9/Steinke and Zakynthinou - 2020 - Reasoning About Generalization via Conditional Mut.pdf:application/pdf},
}

@inproceedings{ambroladze_tighter_2006,
	title = {Tighter {PAC}-{Bayes} {Bounds}},
	volume = {19},
	url = {https://proceedings.neurips.cc/paper/2006/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
	urldate = {2022-03-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ambroladze, Amiran and Parrado-hernández, Emilio and Shawe-taylor, John},
	year = {2006},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/IJUN266B/Ambroladze et al. - 2006 - Tighter PAC-Bayes Bounds.pdf:application/pdf},
}

@inproceedings{tolstikhin_pac-bayes-empirical-bernstein_2013,
	title = {{PAC}-{Bayes}-{Empirical}-{Bernstein} {Inequality}},
	volume = {26},
	url = {https://papers.nips.cc/paper/2013/hash/a97da629b098b75c294dffdc3e463904-Abstract.html},
	urldate = {2022-03-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Tolstikhin, Ilya O and Seldin, Yevgeny},
	year = {2013},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/4PI2DNQ8/Tolstikhin and Seldin - 2013 - PAC-Bayes-Empirical-Bernstein Inequality.pdf:application/pdf},
}

@article{bartlett_empirical_2006,
	title = {Empirical minimization},
	volume = {135},
	issn = {0178-8051, 1432-2064},
	url = {http://link.springer.com/10.1007/s00440-005-0462-3},
	doi = {10.1007/s00440-005-0462-3},
	abstract = {We investigate the behavior of the empirical minimization algorithm using various methods. We ﬁrst analyze it by comparing the empirical, random, structure and the original one on the class, either in an additive sense, via the uniform law of large numbers, or in a multiplicative sense, using isomorphic coordinate projections. We then show that a direct analysis of the empirical minimization algorithm yields a signiﬁcantly better bound, and that the estimates we obtain are essentially sharp. The method of proof we use is based on Talagrand’s concentration inequality for empirical processes.},
	language = {en},
	number = {3},
	urldate = {2022-03-26},
	journal = {Probability Theory and Related Fields},
	author = {Bartlett, Peter L. and Mendelson, Shahar},
	month = jul,
	year = {2006},
	pages = {311--334},
	file = {Bartlett and Mendelson - 2006 - Empirical minimization.pdf:/Users/tokio/Zotero/storage/IUTGQKAZ/Bartlett and Mendelson - 2006 - Empirical minimization.pdf:application/pdf},
}

@article{yang_fast-rate_2019,
	title = {Fast-rate {PAC}-{Bayes} {Generalization} {Bounds} via {Shifted} {Rademacher} {Processes}},
	url = {http://arxiv.org/abs/1908.07585},
	abstract = {The developments of Rademacher complexity and PAC-Bayesian theory have been largely independent. One exception is the PAC-Bayes theorem of Kakade, Sridharan, and Tewari (2008), which is established via Rademacher complexity theory by viewing Gibbs classifiers as linear operators. The goal of this paper is to extend this bridge between Rademacher complexity and state-of-the-art PAC-Bayesian theory. We first demonstrate that one can match the fast rate of Catoni's PAC-Bayes bounds (Catoni, 2007) using shifted Rademacher processes (Wegkamp, 2003; Lecu{\textbackslash}'\{e\} and Mitchell, 2012; Zhivotovskiy and Hanneke, 2018). We then derive a new fast-rate PAC-Bayes bound in terms of the "flatness" of the empirical risk surface on which the posterior concentrates. Our analysis establishes a new framework for deriving fast-rate PAC-Bayes bounds and yields new insights on PAC-Bayesian theory.},
	urldate = {2022-03-25},
	journal = {arXiv:1908.07585 [cs, math, stat]},
	author = {Yang, Jun and Sun, Shengyang and Roy, Daniel M.},
	month = dec,
	year = {2019},
	note = {arXiv: 1908.07585},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YMGL4GMG/Yang et al. - 2019 - Fast-rate PAC-Bayes Generalization Bounds via Shif.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DQKSCDTC/1908.html:text/html},
}

@article{mai_pac-bayesian_2022,
	title = {{PAC}-{Bayesian} {Matrix} {Completion} with a {Spectral} {Scaled} {Student} {Prior}},
	url = {http://arxiv.org/abs/2104.08191},
	abstract = {We study the problem of matrix completion in this paper. A spectral scaled Student prior is exploited to favour the underlying low-rank structure of the data matrix. We provide a thorough theoretical investigation for our approach through PAC-Bayesian bounds. More precisely, our PAC-Bayesian approach enjoys a minimax-optimal oracle inequality which guarantees that our method works well under model misspecification and under general sampling distribution. Interestingly, we also provide efficient gradient-based sampling implementations for our approach by using Langevin Monte Carlo. More specifically, we show that our algorithms are significantly faster than Gibbs sampler in this problem. To illustrate the attractive features of our inference strategy, some numerical simulations are conducted and an application to image inpainting is demonstrated.},
	urldate = {2022-03-18},
	journal = {arXiv:2104.08191 [cs, stat]},
	author = {Mai, The Tien},
	month = jan,
	year = {2022},
	note = {arXiv: 2104.08191},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4FFUQRWP/Mai - 2022 - PAC-Bayesian Matrix Completion with a Spectral Sca.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TWBAM3ZR/2104.html:text/html},
}

@article{dziugaite_data-dependent_2019,
	title = {Data-dependent {PAC}-{Bayes} priors via differential privacy},
	url = {http://arxiv.org/abs/1802.09583},
	abstract = {The Probably Approximately Correct (PAC) Bayes framework (McAllester, 1999) can incorporate knowledge about the learning algorithm and (data) distribution through the use of distribution-dependent priors, yielding tighter generalization bounds on data-dependent posteriors. Using this flexibility, however, is difficult, especially when the data distribution is presumed to be unknown. We show how an \{{\textbackslash}epsilon\}-differentially private data-dependent prior yields a valid PAC-Bayes bound, and then show how non-private mechanisms for choosing priors can also yield generalization bounds. As an application of this result, we show that a Gaussian prior mean chosen via stochastic gradient Langevin dynamics (SGLD; Welling and Teh, 2011) leads to a valid PAC-Bayes bound given control of the 2-Wasserstein distance to an \{{\textbackslash}epsilon\}-differentially private stationary distribution. We study our data-dependent bounds empirically, and show that they can be nonvacuous even when other distribution-dependent bounds are vacuous.},
	urldate = {2022-03-18},
	journal = {arXiv:1802.09583 [cs, stat]},
	author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
	month = apr,
	year = {2019},
	note = {arXiv: 1802.09583},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DZVLPJPX/Dziugaite and Roy - 2019 - Data-dependent PAC-Bayes priors via differential p.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/55YKMJL4/1802.html:text/html},
}

@article{germain_pac-bayesian_2017,
	title = {{PAC}-{Bayesian} {Theory} {Meets} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/1605.08636},
	abstract = {We exhibit a strong link between frequentist PAC-Bayesian risk bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization risk bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam's razor criteria, under the assumption that the data is generated by an i.i.d distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.},
	urldate = {2022-03-18},
	journal = {arXiv:1605.08636 [cs, stat]},
	author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.08636},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/58F7USUE/Germain et al. - 2017 - PAC-Bayesian Theory Meets Bayesian Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/Y94DKYJE/1605.html:text/html},
}

@inproceedings{germain_pac-bayesian_2009,
	address = {Montreal, Quebec, Canada},
	title = {{PAC}-{Bayesian} learning of linear classifiers},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553419},
	doi = {10.1145/1553374.1553419},
	abstract = {We present a general PAC-Bayes theorem from which all known PAC-Bayes risk bounds are obtained as particular cases. We also propose diﬀerent learning algorithms for ﬁnding linear classiﬁers that minimize these bounds. These learning algorithms are generally competitive with both AdaBoost and the SVM.},
	language = {en},
	urldate = {2022-03-17},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Germain, Pascal and Lacasse, Alexandre and Laviolette, François and Marchand, Mario},
	year = {2009},
	pages = {1--8},
	file = {Germain et al. - 2009 - PAC-Bayesian learning of linear classifiers.pdf:/Users/tokio/Zotero/storage/R3U767AM/Germain et al. - 2009 - PAC-Bayesian learning of linear classifiers.pdf:application/pdf},
}

@article{bilodeau_relaxing_2021,
	title = {Relaxing the {I}.{I}.{D}. {Assumption}: {Adaptively} {Minimax} {Optimal} {Regret} via {Root}-{Entropic} {Regularization}},
	shorttitle = {Relaxing the {I}.{I}.{D}. {Assumption}},
	url = {http://arxiv.org/abs/2007.06552},
	abstract = {We consider sequential prediction with expert advice when data are generated from distributions varying arbitrarily within an unknown constraint set. We quantify relaxations of the classical i.i.d. assumption in terms of these constraint sets, with i.i.d. sequences at one extreme and adversarial mechanisms at the other. The Hedge algorithm, long known to be minimax optimal in the adversarial regime, was recently shown to be minimax optimal for i.i.d. data. We show that Hedge with deterministic learning rates is suboptimal between these extremes, and present a new algorithm that adaptively achieves the minimax optimal rate of regret with respect to our relaxations of the i.i.d. assumption, and does so without knowledge of the underlying constraint set. We analyze our algorithm using the follow-the-regularized-leader framework, and prove it corresponds to Hedge with an adaptive learning rate that implicitly scales as the square root of the entropy of the current predictive distribution, rather than the entropy of the initial predictive distribution.},
	urldate = {2022-03-15},
	journal = {arXiv:2007.06552 [cs, stat]},
	author = {Bilodeau, Blair and Negrea, Jeffrey and Roy, Daniel M.},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.06552},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HZMP8WLD/Bilodeau et al. - 2021 - Relaxing the I.I.D. Assumption Adaptively Minimax.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TLMQU87E/2007.html:text/html},
}

@article{maurer_note_2004,
	title = {A {Note} on the {PAC} {Bayesian} {Theorem}},
	url = {http://arxiv.org/abs/cs/0411099},
	abstract = {We prove general exponential moment inequalities for averages of [0,1]-valued iid random variables and use them to tighten the PAC Bayesian Theorem. The logarithmic dependence on the sample count in the enumerator of the PAC Bayesian bound is halved.},
	urldate = {2022-03-08},
	journal = {arXiv:cs/0411099},
	author = {Maurer, Andreas},
	month = nov,
	year = {2004},
	note = {arXiv: cs/0411099},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, I.5.1},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/AZQFHXQJ/Maurer - 2004 - A Note on the PAC Bayesian Theorem.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PWPEHVFW/0411099.html:text/html},
}

@article{wang_generalization_2019,
	title = {On the {Generalization} {Gap} in {Reparameterizable} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1905.12654},
	abstract = {Understanding generalization in reinforcement learning (RL) is a significant challenge, as many common assumptions of traditional supervised learning theory do not apply. We focus on the special class of reparameterizable RL problems, where the trajectory distribution can be decomposed using the reparametrization trick. For this problem class, estimating the expected return is efficient and the trajectory can be computed deterministically given peripheral random variables, which enables us to study reparametrizable RL using supervised learning and transfer learning theory. Through these relationships, we derive guarantees on the gap between the expected and empirical return for both intrinsic and external errors, based on Rademacher complexity as well as the PAC-Bayes bound. Our bound suggests the generalization capability of reparameterizable RL is related to multiple factors including "smoothness" of the environment transition, reward and agent policy function class. We also empirically verify the relationship between the generalization gap and these factors through simulations.},
	urldate = {2022-03-07},
	journal = {arXiv:1905.12654 [cs, stat]},
	author = {Wang, Huan and Zheng, Stephan and Xiong, Caiming and Socher, Richard},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12654},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/LS54DLP3/Wang et al. - 2019 - On the Generalization Gap in Reparameterizable Rei.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/AHA2Q9G5/1905.html:text/html},
}

@article{alquier_user-friendly_2021,
	title = {User-friendly introduction to {PAC}-{Bayes} bounds},
	url = {http://arxiv.org/abs/2110.11216},
	abstract = {Aggregated predictors are obtained by making a set of basic predictors vote according to some weights, that is, to some probability distribution. Randomized predictors are obtained by sampling in a set of basic predictors, according to some prescribed probability distribution. Thus, aggregated and randomized predictors have in common that they are not defined by a minimization problem, but by a probability distribution on the set of predictors. In statistical learning theory, there is a set of tools designed to understand the generalization ability of such procedures: PAC-Bayesian or PAC-Bayes bounds. Since the original PAC-Bayes bounds of D. McAllester, these tools have been considerably improved in many directions (we will for example describe a simplified version of the localization technique of O. Catoni that was missed by the community, and later rediscovered as "mutual information bounds"). Very recently, PAC-Bayes bounds received a considerable attention: for example there was workshop on PAC-Bayes at NIPS 2017, "(Almost) 50 Shades of Bayesian Learning: PAC-Bayesian trends and insights", organized by B. Guedj, F. Bach and P. Germain. One of the reason of this recent success is the successful application of these bounds to neural networks by G. Dziugaite and D. Roy. An elementary introduction to PAC-Bayes theory is still missing. This is an attempt to provide such an introduction.},
	urldate = {2022-03-07},
	journal = {arXiv:2110.11216 [cs, math, stat]},
	author = {Alquier, Pierre},
	month = nov,
	year = {2021},
	note = {arXiv: 2110.11216},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/G4K7QIKC/Alquier - 2021 - User-friendly introduction to PAC-Bayes bounds.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RN8UL3LS/2110.html:text/html},
}

@article{takada_transfer_2020,
	title = {Transfer {Learning} via \${\textbackslash}ell\_1\$ {Regularization}},
	url = {http://arxiv.org/abs/2006.14845},
	abstract = {Machine learning algorithms typically require abundant data under a stationary environment. However, environments are nonstationary in many real-world applications. Critical issues lie in how to effectively adapt models under an ever-changing environment. We propose a method for transferring knowledge from a source domain to a target domain via \${\textbackslash}ell\_1\$ regularization. We incorporate \${\textbackslash}ell\_1\$ regularization of differences between source parameters and target parameters, in addition to an ordinary \${\textbackslash}ell\_1\$ regularization. Hence, our method yields sparsity for both the estimates themselves and changes of the estimates. The proposed method has a tight estimation error bound under a stationary environment, and the estimate remains unchanged from the source estimate under small residuals. Moreover, the estimate is consistent with the underlying function, even when the source estimate is mistaken due to nonstationarity. Empirical results demonstrate that the proposed method effectively balances stability and plasticity.},
	urldate = {2022-02-14},
	journal = {arXiv:2006.14845 [cs, stat]},
	author = {Takada, Masaaki and Fujisawa, Hironori},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.14845},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DQBV2VG2/Takada and Fujisawa - 2020 - Transfer Learning via \$ell_1\$ Regularization.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/D6PNGTHR/2006.html:text/html},
}

@article{sugiyama_active_nodate,
	title = {Active {Learning} in {Approximately} {Linear} {Regression} {Based} on {Conditional} {Expectation} of {Generalization} {Error}},
	abstract = {The goal of active learning is to determine the locations of training input points so that the generalization error is minimized. We discuss the problem of active learning in linear regression scenarios. Traditional active learning methods using least-squares learning often assume that the model used for learning is correctly speciﬁed. In many practical situations, however, this assumption may not be fulﬁlled. Recently, active learning methods using “importance”-weighted least-squares learning have been proposed, which are shown to be robust against misspeciﬁcation of models. In this paper, we propose a new active learning method also using the weighted least-squares learning, which we call ALICE (Active Learning using the Importance-weighted least-squares learning based on Conditional Expectation of the generalization error). An important difference from existing methods is that we predict the conditional expectation of the generalization error given training input points, while existing methods predict the full expectation of the generalization error. Due to this difference, the training input design can be ﬁne-tuned depending on the realization of training input points. Theoretically, we prove that the proposed active learning criterion is a more accurate predictor of the single-trial generalization error than the existing criterion. Numerical studies with toy and benchmark data sets show that the proposed method compares favorably to existing methods.},
	language = {en},
	author = {Sugiyama, Masashi and Jp, Cs Titech Ac},
	pages = {26},
	file = {Sugiyama and Jp - Active Learning in Approximately Linear Regression.pdf:/Users/tokio/Zotero/storage/8XB28LGT/Sugiyama and Jp - Active Learning in Approximately Linear Regression.pdf:application/pdf},
}

@article{ren_survey_2021,
	title = {A {Survey} of {Deep} {Active} {Learning}},
	url = {http://arxiv.org/abs/2009.00236},
	abstract = {Active learning (AL) attempts to maximize the performance gain of the model by marking the fewest samples. Deep learning (DL) is greedy for data and requires a large amount of data supply to optimize massive parameters, so that the model learns how to extract high-quality features. In recent years, due to the rapid development of internet technology, we are in an era of information torrents and we have massive amounts of data. In this way, DL has aroused strong interest of researchers and has been rapidly developed. Compared with DL, researchers have relatively low interest in AL. This is mainly because before the rise of DL, traditional machine learning requires relatively few labeled samples. Therefore, early AL is difficult to reflect the value it deserves. Although DL has made breakthroughs in various fields, most of this success is due to the publicity of the large number of existing annotation datasets. However, the acquisition of a large number of high-quality annotated datasets consumes a lot of manpower, which is not allowed in some fields that require high expertise, especially in the fields of speech recognition, information extraction, medical images, etc. Therefore, AL has gradually received due attention. A natural idea is whether AL can be used to reduce the cost of sample annotations, while retaining the powerful learning capabilities of DL. Therefore, deep active learning (DAL) has emerged. Although the related research has been quite abundant, it lacks a comprehensive survey of DAL. This article is to fill this gap, we provide a formal classification method for the existing work, and a comprehensive and systematic overview. In addition, we also analyzed and summarized the development of DAL from the perspective of application. Finally, we discussed the confusion and problems in DAL, and gave some possible development directions for DAL.},
	urldate = {2022-01-13},
	journal = {arXiv:2009.00236 [cs, stat]},
	author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
	month = dec,
	year = {2021},
	note = {arXiv: 2009.00236},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/B6TMJ2BT/Ren et al. - 2021 - A Survey of Deep Active Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KBEMG4TM/2009.html:text/html},
}

@article{even-dar_online_2009,
	title = {Online {Markov} {Decision} {Processes}},
	volume = {34},
	issn = {0364-765X},
	url = {http://www.jstor.org/stable/40538442},
	abstract = {We consider a Markov decision process (MDP) setting in which the reward function is allowed to change after each time step (possibly in an adversarial manner), yet the dynamics remain fixed. Similar to the experts setting, we address the question of how well an agent can do when compared to the reward achieved under the best stationary policy over time. We provide efficient algorithms, which have regret bounds with no dependence on the size of state space. Instead, these bounds depend only on a certain horizon time of the process and logarithmically on the number of actions.},
	number = {3},
	urldate = {2021-12-09},
	journal = {Mathematics of Operations Research},
	author = {Even-Dar, Eyal and Kakade, Sham. M. and Mansour, Yishay},
	year = {2009},
	note = {Publisher: INFORMS},
	pages = {726--736},
	file = {JSTOR Full Text PDF:/Users/tokio/Zotero/storage/Q8KC96FB/Even-Dar et al. - 2009 - Online Markov Decision Processes.pdf:application/pdf},
}

@article{zhang_bayesian_2021,
	title = {Bayesian {Coresets}: {Revisiting} the {Nonconvex} {Optimization} {Perspective}},
	shorttitle = {Bayesian {Coresets}},
	url = {http://arxiv.org/abs/2007.00715},
	abstract = {Bayesian coresets have emerged as a promising approach for implementing scalable Bayesian inference. The Bayesian coreset problem involves selecting a (weighted) subset of the data samples, such that the posterior inference using the selected subset closely approximates the posterior inference using the full dataset. This manuscript revisits Bayesian coresets through the lens of sparsity constrained optimization. Leveraging recent advances in accelerated optimization methods, we propose and analyze a novel algorithm for coreset selection. We provide explicit convergence rate guarantees and present an empirical evaluation on a variety of benchmark datasets to highlight our proposed algorithm's superior performance compared to state-of-the-art on speed and accuracy.},
	urldate = {2021-12-09},
	journal = {arXiv:2007.00715 [cs, stat]},
	author = {Zhang, Jacky Y. and Khanna, Rajiv and Kyrillidis, Anastasios and Koyejo, Oluwasanmi},
	month = feb,
	year = {2021},
	note = {arXiv: 2007.00715},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DUKP8GU4/Zhang et al. - 2021 - Bayesian Coresets Revisiting the Nonconvex Optimi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NF9Y742H/2007.html:text/html},
}

@article{campbell_sparse_2019,
	title = {Sparse {Variational} {Inference}: {Bayesian} {Coresets} from {Scratch}},
	shorttitle = {Sparse {Variational} {Inference}},
	url = {http://arxiv.org/abs/1906.03329},
	abstract = {The proliferation of automated inference algorithms in Bayesian statistics has provided practitioners newfound access to fast, reproducible data analysis and powerful statistical models. Designing automated methods that are also both computationally scalable and theoretically sound, however, remains a significant challenge. Recent work on Bayesian coresets takes the approach of compressing the dataset before running a standard inference algorithm, providing both scalability and guarantees on posterior approximation error. But the automation of past coreset methods is limited because they depend on the availability of a reasonable coarse posterior approximation, which is difficult to specify in practice. In the present work we remove this requirement by formulating coreset construction as sparsity-constrained variational inference within an exponential family. This perspective leads to a novel construction via greedy optimization, and also provides a unifying information-geometric view of present and past methods. The proposed Riemannian coreset construction algorithm is fully automated, requiring no problem-specific inputs aside from the probabilistic model and dataset. In addition to being significantly easier to use than past methods, experiments demonstrate that past coreset constructions are fundamentally limited by the fixed coarse posterior approximation; in contrast, the proposed algorithm is able to continually improve the coreset, providing state-of-the-art Bayesian dataset summarization with orders-of-magnitude reduction in KL divergence to the exact posterior.},
	urldate = {2021-12-09},
	journal = {arXiv:1906.03329 [cs, stat]},
	author = {Campbell, Trevor and Beronov, Boyan},
	month = oct,
	year = {2019},
	note = {arXiv: 1906.03329},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DWEMVWMR/Campbell and Beronov - 2019 - Sparse Variational Inference Bayesian Coresets fr.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PHK9WRN2/1906.html:text/html},
}

@article{huggins_coresets_2017,
	title = {Coresets for {Scalable} {Bayesian} {Logistic} {Regression}},
	url = {http://arxiv.org/abs/1605.06423},
	abstract = {The use of Bayesian methods in large-scale data settings is attractive because of the rich hierarchical models, uncertainty quantification, and prior specification they provide. Standard Bayesian inference algorithms are computationally expensive, however, making their direct application to large datasets difficult or infeasible. Recent work on scaling Bayesian inference has focused on modifying the underlying algorithms to, for example, use only a random data subsample at each iteration. We leverage the insight that data is often redundant to instead obtain a weighted subset of the data (called a coreset) that is much smaller than the original dataset. We can then use this small coreset in any number of existing posterior inference algorithms without modification. In this paper, we develop an efficient coreset construction algorithm for Bayesian logistic regression models. We provide theoretical guarantees on the size and approximation quality of the coreset -- both for fixed, known datasets, and in expectation for a wide class of data generative models. Crucially, the proposed approach also permits efficient construction of the coreset in both streaming and parallel settings, with minimal additional effort. We demonstrate the efficacy of our approach on a number of synthetic and real-world datasets, and find that, in practice, the size of the coreset is independent of the original dataset size. Furthermore, constructing the coreset takes a negligible amount of time compared to that required to run MCMC on it.},
	urldate = {2021-12-09},
	journal = {arXiv:1605.06423 [cs, stat]},
	author = {Huggins, Jonathan H. and Campbell, Trevor and Broderick, Tamara},
	month = feb,
	year = {2017},
	note = {arXiv: 1605.06423},
	keywords = {Statistics - Machine Learning, Statistics - Computation, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/JJBJDEYI/Huggins et al. - 2017 - Coresets for Scalable Bayesian Logistic Regression.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BBK9K9KA/1605.html:text/html},
}

@article{campbell_automated_2019,
	title = {Automated {Scalable} {Bayesian} {Inference} via {Hilbert} {Coresets}},
	url = {http://arxiv.org/abs/1710.05053},
	abstract = {The automation of posterior inference in Bayesian data analysis has enabled experts and nonexperts alike to use more sophisticated models, engage in faster exploratory modeling and analysis, and ensure experimental reproducibility. However, standard automated posterior inference algorithms are not tractable at the scale of massive modern datasets, and modifications to make them so are typically model-specific, require expert tuning, and can break theoretical guarantees on inferential quality. Building on the Bayesian coresets framework, this work instead takes advantage of data redundancy to shrink the dataset itself as a preprocessing step, providing fully-automated, scalable Bayesian inference with theoretical guarantees. We begin with an intuitive reformulation of Bayesian coreset construction as sparse vector sum approximation, and demonstrate that its automation and performance-based shortcomings arise from the use of the supremum norm. To address these shortcomings we develop Hilbert coresets, i.e., Bayesian coresets constructed under a norm induced by an inner-product on the log-likelihood function space. We propose two Hilbert coreset construction algorithms---one based on importance sampling, and one based on the Frank-Wolfe algorithm---along with theoretical guarantees on approximation quality as a function of coreset size. Since the exact computation of the proposed inner-products is model-specific, we automate the construction with a random finite-dimensional projection of the log-likelihood functions. The resulting automated coreset construction algorithm is simple to implement, and experiments on a variety of models with real and synthetic datasets show that it provides high-quality posterior approximations and a significant reduction in the computational cost of inference.},
	urldate = {2021-12-09},
	journal = {arXiv:1710.05053 [cs, stat]},
	author = {Campbell, Trevor and Broderick, Tamara},
	month = feb,
	year = {2019},
	note = {arXiv: 1710.05053},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EAF7KPBH/Campbell and Broderick - 2019 - Automated Scalable Bayesian Inference via Hilbert .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GG3PHSJK/1710.html:text/html},
}

@article{campbell_bayesian_2018,
	title = {Bayesian {Coreset} {Construction} via {Greedy} {Iterative} {Geodesic} {Ascent}},
	url = {http://arxiv.org/abs/1802.01737},
	abstract = {Coherent uncertainty quantification is a key strength of Bayesian methods. But modern algorithms for approximate Bayesian posterior inference often sacrifice accurate posterior uncertainty estimation in the pursuit of scalability. This work shows that previous Bayesian coreset construction algorithms---which build a small, weighted subset of the data that approximates the full dataset---are no exception. We demonstrate that these algorithms scale the coreset log-likelihood suboptimally, resulting in underestimated posterior uncertainty. To address this shortcoming, we develop greedy iterative geodesic ascent (GIGA), a novel algorithm for Bayesian coreset construction that scales the coreset log-likelihood optimally. GIGA provides geometric decay in posterior approximation error as a function of coreset size, and maintains the fast running time of its predecessors. The paper concludes with validation of GIGA on both synthetic and real datasets, demonstrating that it reduces posterior approximation error by orders of magnitude compared with previous coreset constructions.},
	urldate = {2021-12-08},
	journal = {arXiv:1802.01737 [cs, stat]},
	author = {Campbell, Trevor and Broderick, Tamara},
	month = may,
	year = {2018},
	note = {arXiv: 1802.01737},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WY4DI3P9/Campbell and Broderick - 2018 - Bayesian Coreset Construction via Greedy Iterative.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/P5MT7IKK/1802.html:text/html},
}

@article{chen_facilitating_2019,
	title = {Facilitating {Bayesian} {Continual} {Learning} by {Natural} {Gradients} and {Stein} {Gradients}},
	url = {http://arxiv.org/abs/1904.10644},
	abstract = {Continual learning aims to enable machine learning models to learn a general solution space for past and future tasks in a sequential manner. Conventional models tend to forget the knowledge of previous tasks while learning a new task, a phenomenon known as catastrophic forgetting. When using Bayesian models in continual learning, knowledge from previous tasks can be retained in two ways: 1). posterior distributions over the parameters, containing the knowledge gained from inference in previous tasks, which then serve as the priors for the following task; 2). coresets, containing knowledge of data distributions of previous tasks. Here, we show that Bayesian continual learning can be facilitated in terms of these two means through the use of natural gradients and Stein gradients respectively.},
	urldate = {2021-12-08},
	journal = {arXiv:1904.10644 [cs, stat]},
	author = {Chen, Yu and Diethe, Tom and Lawrence, Neil},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.10644},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YGDL9CPQ/Chen et al. - 2019 - Facilitating Bayesian Continual Learning by Natura.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PL65EL9L/1904.html:text/html},
}

@article{beronov_sequential_nodate,
	title = {Sequential {Core}-{Set} {Monte} {Carlo}},
	abstract = {Sequential Monte Carlo (SMC) is a generalpurpose methodology for recursive Bayesian inference, and is widely used in state space modeling and probabilistic programming. Its resamplemove variant reduces the variance of posterior estimates by interleaving Markov chain Monte Carlo (MCMC) steps for particle “rejuvenation”; but this requires accessing all past observations and leads to linearly growing memory size and quadratic computation cost. Under the assumption of exchangeability, we introduce sequential core-set Monte Carlo (SCMC), which achieves constant space and linear time by rejuvenating based on sparse, weighted subsets of past data. In contrast to earlier approaches, which uniformly subsample or throw away observations, SCMC uses a novel online version of a state-of-the-art Bayesian core-set algorithm to incrementally construct a nonparametric, data- and model-dependent variational representation of the unnormalized target density. Experiments demonstrate signiﬁcantly reduced approximation errors at negligible additional cost.},
	language = {en},
	author = {Beronov, Boyan and Weilbach, Christian and Wood, Frank and Campbell, Trevor},
	pages = {11},
	file = {Beronov et al. - Sequential Core-Set Monte Carlo.pdf:/Users/tokio/Zotero/storage/8TYNVDUD/Beronov et al. - Sequential Core-Set Monte Carlo.pdf:application/pdf},
}

@phdthesis{yang_information_nodate,
	address = {Ann Arbor, United States},
	type = {Ph.{D}.},
	title = {Information {Geometry} and {Optimal} {Transport}},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/pqdtglobal/docview/2386300824/abstract/D5A13D3E48724900PQ/1},
	abstract = {This thesis is devoted to building the connection of information geometry and optimal transport. In- formation geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics. The modern theory of information geometry is largely initiated and developed by the Japanese mathematician Shun-ichi Amari. His books [1] and [2] summarize well the techniques and applications of information geometry. Optimal transport is a larger area with longer history than information geometry. The original problem, optimizing the transportation of earthworks (”des rem- blais” in French), was formalized by the French mathematician Gaspard Monge in 1781. Modern theory of optimal transport started with the generalization by the Soviet mathematician and economist Leonid Kantorovich[3] and consequently the optimal transport problem is also named as Monge–Kantorovich transportation problem.
In this thesis, we show that the dualistic geometric structure, a core concept of information geometry, can be encoded from a geometric structure in optimal transport theory. With the help of this connection, we can have a novel understanding on the classic problems in information geometry from the point of view from optimal transport. Moreover, we study various divergences and show that their geometry structure are equivalent.},
	language = {英語},
	urldate = {2021-12-06},
	author = {Yang, Jiaowen},
	note = {ISBN: 9781658458306},
	keywords = {Bregman divergence, Divergence, Information geometry, L alpha divergence, Optimal transport, Pseudo Riemannian framework},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KRXCZV33/Yang - Information Geometry and Optimal Transport.pdf:application/pdf},
}

@article{foster_variational_2020,
	title = {Variational {Bayesian} {Optimal} {Experimental} {Design}},
	url = {http://arxiv.org/abs/1903.05480},
	abstract = {Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.},
	urldate = {2021-11-27},
	journal = {arXiv:1903.05480 [cs, stat]},
	author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah},
	month = jan,
	year = {2020},
	note = {arXiv: 1903.05480},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/W8PPUV9Q/Foster et al. - 2020 - Variational Bayesian Optimal Experimental Design.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/VKGVMZCB/1903.html:text/html},
}

@article{kleinegesse_gradient-based_2021,
	title = {Gradient-based {Bayesian} {Experimental} {Design} for {Implicit} {Models} using {Mutual} {Information} {Lower} {Bounds}},
	url = {http://arxiv.org/abs/2105.04379},
	abstract = {We introduce a framework for Bayesian experimental design (BED) with implicit models, where the data-generating distribution is intractable but sampling from it is still possible. In order to find optimal experimental designs for such models, our approach maximises mutual information lower bounds that are parametrised by neural networks. By training a neural network on sampled data, we simultaneously update network parameters and designs using stochastic gradient-ascent. The framework enables experimental design with a variety of prominent lower bounds and can be applied to a wide range of scientific tasks, such as parameter estimation, model discrimination and improving future predictions. Using a set of intractable toy models, we provide a comprehensive empirical comparison of prominent lower bounds applied to the aforementioned tasks. We further validate our framework on a challenging system of stochastic differential equations from epidemiology.},
	urldate = {2021-11-27},
	journal = {arXiv:2105.04379 [cs, stat]},
	author = {Kleinegesse, Steven and Gutmann, Michael U.},
	month = may,
	year = {2021},
	note = {arXiv: 2105.04379},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology, 62K05,},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/BNFREPQB/Kleinegesse and Gutmann - 2021 - Gradient-based Bayesian Experimental Design for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/I94TTU84/2105.html:text/html},
}

@article{amzal_bayesian-optimal_2006,
	title = {Bayesian-{Optimal} {Design} via {Interacting} {Particle} {Systems}},
	volume = {101},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/27590734},
	abstract = {We propose a new stochastic algorithm for Bayesian-optimal design in nonlinear and high-dimensional contexts. Following Peter Müller, we solve an optimization problem by exploring the expected utility surface through Markov chain Monte Carlo simulations. The optimal design is the mode of this surface considered a probability distribution. Our algorithm relies on a "particle" method to efficiently explore high-dimensional multimodal surfaces, with simulated annealing to concentrate the samples near the modes. We first test the method on an optimal allocation problem for which the explicit solution is available, to compare its efficiency with a simpler algorithm. We then apply our method to a challenging medical case study in which an optimal protocol treatment needs to be determined. For this case, we propose a formalization of the problem in the framework of Bayesian decision theory, taking into account physicians' knowledge and motivations. We also briefly review further improvements and alternatives.},
	number = {474},
	urldate = {2021-11-27},
	journal = {Journal of the American Statistical Association},
	author = {Amzal, Billy and Bois, Frédéric Y. and Parent, Eric and Robert, Christian P.},
	year = {2006},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {773--785},
	file = {JSTOR Full Text PDF:/Users/tokio/Zotero/storage/KPTJU8N8/Amzal et al. - 2006 - Bayesian-Optimal Design via Interacting Particle S.pdf:application/pdf},
}

@article{foster_unified_2020,
	title = {A {Unified} {Stochastic} {Gradient} {Approach} to {Designing} {Bayesian}-{Optimal} {Experiments}},
	url = {http://arxiv.org/abs/1911.00294},
	abstract = {We introduce a fully stochastic gradient based approach to Bayesian optimal experimental design (BOED). Our approach utilizes variational lower bounds on the expected information gain (EIG) of an experiment that can be simultaneously optimized with respect to both the variational and design parameters. This allows the design process to be carried out through a single unified stochastic gradient ascent procedure, in contrast to existing approaches that typically construct a pointwise EIG estimator, before passing this estimator to a separate optimizer. We provide a number of different variational objectives including the novel adaptive contrastive estimation (ACE) bound. Finally, we show that our gradient-based approaches are able to provide effective design optimization in substantially higher dimensional settings than existing approaches.},
	urldate = {2021-11-27},
	journal = {arXiv:1911.00294 [cs, stat]},
	author = {Foster, Adam and Jankowiak, Martin and O'Meara, Matthew and Teh, Yee Whye and Rainforth, Tom},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.00294},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VLCV528H/Foster et al. - 2020 - A Unified Stochastic Gradient Approach to Designin.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/82CRALTP/1911.html:text/html},
}

@article{prangle_bayesian_2021,
	title = {Bayesian experimental design without posterior calculations: an adversarial approach},
	shorttitle = {Bayesian experimental design without posterior calculations},
	url = {http://arxiv.org/abs/1904.05703},
	abstract = {Most computational approaches to Bayesian experimental design require making posterior calculations repeatedly for a large number of potential designs and/or simulated datasets. This can be expensive and prohibit scaling up these methods to models with many parameters, or designs with many unknowns to select. We introduce an efficient alternative approach without posterior calculations, based on optimising the expected trace of the Fisher information, as discussed by Walker (2016). We illustrate drawbacks of this approach, including lack of invariance to reparameterisation and encouraging designs in which one parameter combination is inferred accurately but not any others. We show these can be avoided by using an adversarial approach: the experimenter must select their design while a critic attempts to select the least favourable parameterisation. We present theoretical properties of this approach and show it can be used with gradient based optimisation methods to find designs efficiently in practice.},
	urldate = {2021-11-27},
	journal = {arXiv:1904.05703 [stat]},
	author = {Prangle, Dennis and Harbisher, Sophie and Gillespie, Colin S.},
	month = nov,
	year = {2021},
	note = {arXiv: 1904.05703},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YTMGTMQW/Prangle et al. - 2021 - Bayesian experimental design without posterior cal.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QIXN6Y5C/1904.html:text/html},
}

@article{aletti_properties_2012,
	title = {Properties of the {KL}-optimality criterion},
	volume = {26},
	doi = {10.1007/s11222-014-9515-8},
	abstract = {In this paper some new properties and computational tools for finding KL-optimum designs are provided. KL-optimality is a general criterion useful to select the best experimental conditions to discriminate between statistical models. A KL-optimum design is obtained from a minimax optimization problem, which is defined on a infinite-dimensional space. In particular, continuity of the KL-optimality criterion is proved under mild conditions; as a consequence, the first-order algorithm converges to the set of KL-optimum designs for a large class of models. It is also shown that KL-optimum designs are invariant to any scale-position transformation. Some examples are given and discussed, together with some practical implications for numerical computation purposes.},
	journal = {Statistics and Computing},
	author = {Aletti, Giacomo and May, Caterina and Tommasi, Chiara},
	month = dec,
	year = {2012},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/4XAQ5XZW/Aletti et al. - 2012 - Properties of the KL-optimality criterion.pdf:application/pdf},
}

@article{huang_optimal_2019,
	title = {Optimal {Design} of {Experiments} for {Nonlinear} {Response} {Surface} {Models}},
	volume = {68},
	issn = {0035-9254, 1467-9876},
	url = {http://arxiv.org/abs/1803.06536},
	doi = {10.1111/rssc.12313},
	abstract = {Many chemical and biological experiments involve multiple treatment factors and often it is convenient to fit a nonlinear model in these factors. This nonlinear model can be mechanistic, empirical or a hybrid of the two. Motivated by experiments in chemical engineering, we focus on D-optimal design for multifactor nonlinear response surfaces in general. In order to find and study optimal designs, we first implement conventional point and coordinate exchange algorithms. Next, we develop a novel multiphase optimisation method to construct D-optimal designs with improved properties. The benefits of this method are demonstrated by application to two experiments involving nonlinear regression models. The designs obtained are shown to be considerably more informative than designs obtained using traditional design optimality algorithms.},
	number = {3},
	urldate = {2021-11-27},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Huang, Yuanzhi and Gilmour, Steven and Mylona, Kalliopi and Goos, Peter},
	month = apr,
	year = {2019},
	note = {arXiv: 1803.06536},
	keywords = {Statistics - Computation, 62K05, Statistics - Applications},
	pages = {623--640},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ZCHQQBVZ/Huang et al. - 2019 - Optimal Design of Experiments for Nonlinear Respon.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3SVZAPBG/1803.html:text/html},
}

@article{kleinegesse_bayesian_2020,
	title = {Bayesian {Experimental} {Design} for {Implicit} {Models} by {Mutual} {Information} {Neural} {Estimation}},
	url = {http://arxiv.org/abs/2002.08129},
	abstract = {Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.},
	urldate = {2021-11-27},
	journal = {arXiv:2002.08129 [cs, stat]},
	author = {Kleinegesse, Steven and Gutmann, Michael U.},
	month = aug,
	year = {2020},
	note = {arXiv: 2002.08129},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Computation, Statistics - Methodology, G.3, 62K05 (Primary)},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/RV4XJ3JU/Kleinegesse and Gutmann - 2020 - Bayesian Experimental Design for Implicit Models b.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/TBDEPJMC/2002.html:text/html},
}

@article{duarte_optimal_2020,
	title = {Optimal {Design} of {Experiments} for {Implicit} {Models}},
	doi = {10.1080/01621459.2020.1862670},
	abstract = {Explicit models representing the response variables as functions of the control variables are standard in virtually all scientific fields. For these models there is a vast literature on the optimal design of experiments to provide good estimates of the parameters with the use of minimal resources. Contrarily, the optimal design of experiments for implicit models is more complex and has not been systematically addressed. Nevertheless, there are practical examples where the models relating the response variables, the parameters and the factors are implicit or hardly convertible into an explicit form.
We propose a general formulation for developing the theory of the optimal design of experiments (ODoE) for implicit algebraic models to specifically find continuous local designs. The treatment relies on converting the ODoE problem into an optimization problem of the Nonlinear Programming class which includes the construction of the parameter sensitivities and the Cholesky decomposition of the Fisher Information Matrix. The Nonlinear Programming problem generated has multiple local optima, and we use global solvers, combined with an equivalence theorem from the theory of ODoE, to ensure the global optimality of our continuous optimal designs. We consider D– and A–optimality criteria and apply the approach to five examples of practical interest in chemistry and thermodynamics.},
	journal = {Journal of the American Statistical Association},
	author = {Duarte, Belmiro and Atkinson, Anthony and Granjo, Jose and Oliveira, Nuno},
	month = dec,
	year = {2020},
	pages = {1--38},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PYZHTQ2B/Duarte et al. - 2020 - Optimal Design of Experiments for Implicit Models.pdf:application/pdf},
}

@article{feldman_introduction_2020,
	title = {Introduction to {Core}-sets: an {Updated} {Survey}},
	shorttitle = {Introduction to {Core}-sets},
	url = {http://arxiv.org/abs/2011.09384},
	abstract = {In optimization or machine learning problems we are given a set of items, usually points in some metric space, and the goal is to minimize or maximize an objective function over some space of candidate solutions. For example, in clustering problems, the input is a set of points in some metric space, and a common goal is to compute a set of centers in some other space (points, lines) that will minimize the sum of distances to these points. In database queries, we may need to compute such a some for a specific query set of \$k\$ centers. However, traditional algorithms cannot handle modern systems that require parallel real-time computations of infinite distributed streams from sensors such as GPS, audio or video that arrive to a cloud, or networks of weaker devices such as smartphones or robots. Core-set is a "small data" summarization of the input "big data", where every possible query has approximately the same answer on both data sets. Generic techniques enable efficient coreset {\textbackslash}changed\{maintenance\} of streaming, distributed and dynamic data. Traditional algorithms can then be applied on these coresets to maintain the approximated optimal solutions. The challenge is to design coresets with provable tradeoff between their size and approximation error. This survey summarizes such constructions in a retrospective way, that aims to unified and simplify the state-of-the-art.},
	urldate = {2021-11-27},
	journal = {arXiv:2011.09384 [cs]},
	author = {Feldman, Dan},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.09384},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Data Structures and Algorithms, Computer Science - Computational Geometry},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TEZ79E3V/Feldman - 2020 - Introduction to Core-sets an Updated Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GFB9WEKT/2011.html:text/html},
}

@inproceedings{clarkson_coresets_nodate,
	title = {Coresets, sparse greedy approximation and the {Frank}-{Wolfe} algorithm},
	abstract = {The problem of maximizing a concave function f(x) in a simplex S can be solved approximately by a simple greedy algorithm. For given k, the algorithm can find a point x(k) on a k-dimensional face of S, such that f(x(k))  ≥ f(x∗)  − O(1/k). Here f(x∗) is the maximum value of f in S. This algorithm and analysis were known before, and related to problems of statistics and machine learning, such as boosting, regression, and density mixture estimation. In other work, coming from computational geometry, the existence of ɛ-coresets was shown for the minimum enclosing ball problem, by means of a simple greedy algorithm. Similar greedy algorithms, that are special cases of the Frank-Wolfe algorithm, were described for other enclosure problems. Here these results are tied together, stronger convergence results are reviewed, and several coreset bounds are generalized or strengthened.},
	booktitle = {Proceedings of the 19th {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	author = {Clarkson, Kenneth L.},
	pages = {922--931},
	file = {Citeseer - Full Text PDF:/Users/tokio/Zotero/storage/R9GYSHVB/Clarkson - Coresets, sparse greedy approximation and the Fran.pdf:application/pdf;Citeseer - Snapshot:/Users/tokio/Zotero/storage/5LUSKPK9/download.html:text/html},
}

@article{alexanderian_bayesian_2016,
	title = {On {Bayesian} {A}- and {D}-{Optimal} {Experimental} {Designs} in {Infinite} {Dimensions}},
	volume = {11},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-11/issue-3/On-Bayesian-A--and-D-Optimal-Experimental-Designs-in/10.1214/15-BA969.full},
	doi = {10.1214/15-BA969},
	abstract = {We consider Bayesian linear inverse problems in infinite-dimensional separable Hilbert spaces, with a Gaussian prior measure and additive Gaussian noise model, and provide an extension of the concept of Bayesian D-optimality to the infinite-dimensional case. To this end, we derive the infinite-dimensional version of the expression for the Kullback–Leibler divergence from the posterior measure to the prior measure, which is subsequently used to derive the expression for the expected information gain. We also study the notion of Bayesian A-optimality in the infinite-dimensional setting, and extend the well known (in the finite-dimensional case) equivalence of the Bayes risk of the MAP estimator with the trace of the posterior covariance, for the Gaussian linear case, to the infinite-dimensional Hilbert space case.},
	number = {3},
	urldate = {2021-11-24},
	journal = {Bayesian Analysis},
	author = {Alexanderian, Alen and Gloor, Philip J. and Ghattas, Omar},
	month = sep,
	year = {2016},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Bayes risk, Bayesian inference in Hilbert space, Bayesian optimal experimental design, expected information gain, Gaussian measure, Kullback–Leibler divergence},
	pages = {671--695},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ALUGTMJZ/Alexanderian et al. - 2016 - On Bayesian A- and D-Optimal Experimental Designs .pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/6ZMPALAQ/15-BA969.html:text/html},
}

@article{liu_stein_2017,
	title = {Stein {Variational} {Gradient} {Descent} as {Gradient} {Flow}},
	url = {http://arxiv.org/abs/1704.07520},
	abstract = {Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on an efficient gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD, discussing its weak convergence properties and showing that its asymptotic behavior is captured by a gradient flow of the KL divergence functional under a new metric structure induced by Stein operator. We also provide a number of results on Stein operator and Stein's identity using the notion of weak derivative, including a new proof of the distinguishability of Stein discrepancy under weak conditions.},
	urldate = {2021-11-19},
	journal = {arXiv:1704.07520 [stat]},
	author = {Liu, Qiang},
	month = nov,
	year = {2017},
	note = {arXiv: 1704.07520},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/X8QMM5II/Liu - 2017 - Stein Variational Gradient Descent as Gradient Flo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/MPNY4GDK/1704.html:text/html},
}

@article{chen_unified_2018,
	title = {A {Unified} {Particle}-{Optimization} {Framework} for {Scalable} {Bayesian} {Sampling}},
	url = {http://arxiv.org/abs/1805.11659},
	abstract = {There has been recent interest in developing scalable Bayesian sampling methods such as stochastic gradient MCMC (SG-MCMC) and Stein variational gradient descent (SVGD) for big-data analysis. A standard SG-MCMC algorithm simulates samples from a discrete-time Markov chain to approximate a target distribution, thus samples could be highly correlated, an undesired property for SG-MCMC. In contrary, SVGD directly optimizes a set of particles to approximate a target distribution, and thus is able to obtain good approximations with relatively much fewer samples. In this paper, we propose a principle particle-optimization framework based on Wasserstein gradient flows to unify SG-MCMC and SVGD, and to allow new algorithms to be developed. Our framework interprets SG-MCMC as particle optimization on the space of probability measures, revealing a strong connection between SG-MCMC and SVGD. The key component of our framework is several particle-approximate techniques to efficiently solve the original partial differential equations on the space of probability measures. Extensive experiments on both synthetic data and deep neural networks demonstrate the effectiveness and efficiency of our framework for scalable Bayesian sampling.},
	urldate = {2021-11-19},
	journal = {arXiv:1805.11659 [cs, stat]},
	author = {Chen, Changyou and Zhang, Ruiyi and Wang, Wenlin and Li, Bai and Chen, Liqun},
	month = jul,
	year = {2018},
	note = {arXiv: 1805.11659},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/Y4AX74PY/Chen et al. - 2018 - A Unified Particle-Optimization Framework for Scal.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/GGFSMK5I/1805.html:text/html},
}

@article{duncan_geometry_2019,
	title = {On the geometry of {Stein} variational gradient descent},
	url = {http://arxiv.org/abs/1912.00894},
	abstract = {Bayesian inference problems require sampling or approximating high-dimensional probability distributions. The focus of this paper is on the recently introduced Stein variational gradient descent methodology, a class of algorithms that rely on iterated steepest descent steps with respect to a reproducing kernel Hilbert space norm. This construction leads to interacting particle systems, the mean-field limit of which is a gradient flow on the space of probability distributions equipped with a certain geometrical structure. We leverage this viewpoint to shed some light on the convergence properties of the algorithm, in particular addressing the problem of choosing a suitable positive definite kernel function. Our analysis leads us to considering certain nondifferentiable kernels with adjusted tails. We demonstrate significant performs gains of these in various numerical experiments.},
	urldate = {2021-11-18},
	journal = {arXiv:1912.00894 [cs, math, stat]},
	author = {Duncan, A. and Nuesken, N. and Szpruch, L.},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.00894},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory, Mathematics - Analysis of PDEs},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QVBSZHC7/Duncan et al. - 2019 - On the geometry of Stein variational gradient desc.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/63GEEY83/1912.html:text/html},
}

@article{chu_equivalence_2020,
	title = {The equivalence between {Stein} variational gradient descent and black-box variational inference},
	url = {http://arxiv.org/abs/2004.01822},
	abstract = {We formalize an equivalence between two popular methods for Bayesian inference: Stein variational gradient descent (SVGD) and black-box variational inference (BBVI). In particular, we show that BBVI corresponds precisely to SVGD when the kernel is the neural tangent kernel. Furthermore, we interpret SVGD and BBVI as kernel gradient flows; we do this by leveraging the recent perspective that views SVGD as a gradient flow in the space of probability distributions and showing that BBVI naturally motivates a Riemannian structure on that space. We observe that kernel gradient flow also describes dynamics found in the training of generative adversarial networks (GANs). This work thereby unifies several existing techniques in variational inference and generative modeling and identifies the kernel as a fundamental object governing the behavior of these algorithms, motivating deeper analysis of its properties.},
	urldate = {2021-11-18},
	journal = {arXiv:2004.01822 [cs, stat]},
	author = {Chu, Casey and Minami, Kentaro and Fukumizu, Kenji},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.01822},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/CBYKYI8M/Chu et al. - 2020 - The equivalence between Stein variational gradient.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NWHMR5JC/2004.html:text/html},
}

@article{ma_variational_2019,
	title = {Variational {Implicit} {Processes}},
	url = {http://arxiv.org/abs/1806.02390},
	abstract = {We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over functions, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates. This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes.},
	urldate = {2021-11-17},
	journal = {arXiv:1806.02390 [cs, stat]},
	author = {Ma, Chao and Li, Yingzhen and Hernández-Lobato, José Miguel},
	month = may,
	year = {2019},
	note = {arXiv: 1806.02390},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/HJJIFM36/Ma et al. - 2019 - Variational Implicit Processes.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CFBPEPFX/1806.html:text/html},
}

@article{wang_function_2019,
	title = {Function {Space} {Particle} {Optimization} for {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.09754},
	abstract = {While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and over-parameterized nature. To address this issue, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as such methods have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.},
	urldate = {2021-11-17},
	journal = {arXiv:1902.09754 [cs, stat]},
	author = {Wang, Ziyu and Ren, Tongzheng and Zhu, Jun and Zhang, Bo},
	month = may,
	year = {2019},
	note = {arXiv: 1902.09754},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8SV49CLC/Wang et al. - 2019 - Function Space Particle Optimization for Bayesian .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/S3TV2EAC/1902.html:text/html},
}

@article{shi_spectral_2018,
	title = {A {Spectral} {Approach} to {Gradient} {Estimation} for {Implicit} {Distributions}},
	url = {http://arxiv.org/abs/1806.02925},
	abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr{\textbackslash}"om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr{\textbackslash}"om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
	urldate = {2021-11-17},
	journal = {arXiv:1806.02925 [cs, stat]},
	author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.02925},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VQIL6XHM/Shi et al. - 2018 - A Spectral Approach to Gradient Estimation for Imp.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CC7MU5YQ/1806.html:text/html},
}

@article{liu_stein_2019,
	title = {Stein {Variational} {Gradient} {Descent}: {A} {General} {Purpose} {Bayesian} {Inference} {Algorithm}},
	shorttitle = {Stein {Variational} {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1608.04471},
	abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
	urldate = {2021-11-17},
	journal = {arXiv:1608.04471 [cs, stat]},
	author = {Liu, Qiang and Wang, Dilin},
	month = sep,
	year = {2019},
	note = {arXiv: 1608.04471},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/G2UGGUX6/Liu and Wang - 2019 - Stein Variational Gradient Descent A General Purp.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RDPE4EHK/1608.html:text/html},
}

@article{roeder_sticking_2017,
	title = {Sticking the {Landing}: {Simple}, {Lower}-{Variance} {Gradient} {Estimators} for {Variational} {Inference}},
	shorttitle = {Sticking the {Landing}},
	url = {http://arxiv.org/abs/1703.09194},
	abstract = {We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.},
	urldate = {2021-11-13},
	journal = {arXiv:1703.09194 [cs, stat]},
	author = {Roeder, Geoffrey and Wu, Yuhuai and Duvenaud, David},
	month = may,
	year = {2017},
	note = {arXiv: 1703.09194},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ANHU4J36/Roeder et al. - 2017 - Sticking the Landing Simple, Lower-Variance Gradi.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PETF78YM/1703.html:text/html},
}

@article{sun_functional_2019,
	title = {Functional {Variational} {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1903.05779},
	abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
	urldate = {2021-11-12},
	journal = {arXiv:1903.05779 [cs, stat]},
	author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.05779},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/DN3EWNRP/Sun et al. - 2019 - Functional Variational Bayesian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BMSS8QMY/1903.html:text/html},
}

@article{lloyd_variational_2015,
	title = {Variational {Inference} for {Gaussian} {Process} {Modulated} {Poisson} {Processes}},
	url = {http://arxiv.org/abs/1411.0254},
	abstract = {We present the first fully variational Bayesian inference scheme for continuous Gaussian-process-modulated Poisson processes. Such point processes are used in a variety of domains, including neuroscience, geo-statistics and astronomy, but their use is hindered by the computational cost of existing inference schemes. Our scheme: requires no discretisation of the domain; scales linearly in the number of observed events; and is many orders of magnitude faster than previous sampling based approaches. The resulting algorithm is shown to outperform standard methods on synthetic examples, coal mining disaster data and in the prediction of Malaria incidences in Kenya.},
	urldate = {2021-11-12},
	journal = {arXiv:1411.0254 [stat]},
	author = {Lloyd, Chris and Gunter, Tom and Osborne, Michael A. and Roberts, Stephen J.},
	month = jul,
	year = {2015},
	note = {arXiv: 1411.0254},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GTVTN9SX/Lloyd et al. - 2015 - Variational Inference for Gaussian Process Modulat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/DIHSV4IL/1411.html:text/html},
}

@inproceedings{rudner_rethinking_2020,
	title = {Rethinking {Function}-{Space} {Variational} {Inference} in {Bayesian} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=KtY5qphxnCv},
	abstract = {The paper proposes a scalable approach to function-space variational inference in Bayesian neural networks.},
	language = {en},
	urldate = {2021-11-12},
	author = {Rudner, Tim G. J. and Chen, Zonghao and Gal, Yarin},
	month = nov,
	year = {2020},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3CGXL8WH/Rudner et al. - 2020 - Rethinking Function-Space Variational Inference in.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/NX74KVBJ/forum.html:text/html},
}

@article{burt_understanding_2020,
	title = {Understanding {Variational} {Inference} in {Function}-{Space}},
	url = {http://arxiv.org/abs/2011.09421},
	abstract = {Recent work has attempted to directly approximate the `function-space' or predictive posterior distribution of Bayesian models, without approximating the posterior distribution over the parameters. This is appealing in e.g. Bayesian neural networks, where we only need the former, and the latter is hard to represent. In this work, we highlight some advantages and limitations of employing the Kullback-Leibler divergence in this setting. For example, we show that minimizing the KL divergence between a wide class of parametric distributions and the posterior induced by a (non-degenerate) Gaussian process prior leads to an ill-defined objective function. Then, we propose (featurized) Bayesian linear regression as a benchmark for `function-space' inference methods that directly measures approximation quality. We apply this methodology to assess aspects of the objective function and inference scheme considered in Sun, Zhang, Shi, and Grosse (2018), emphasizing the quality of approximation to Bayesian inference as opposed to predictive performance.},
	urldate = {2021-11-12},
	journal = {arXiv:2011.09421 [cs, stat]},
	author = {Burt, David R. and Ober, Sebastian W. and Garriga-Alonso, Adrià and van der Wilk, Mark},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.09421},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5U8CA7QP/Burt et al. - 2020 - Understanding Variational Inference in Function-Sp.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/IUJ8VDEX/2011.html:text/html},
}

@article{hernandez-lobato_probabilistic_2015,
	title = {Probabilistic {Backpropagation} for {Scalable} {Learning} of {Bayesian} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1502.05336},
	abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
	urldate = {2021-11-06},
	journal = {arXiv:1502.05336 [stat]},
	author = {Hernández-Lobato, José Miguel and Adams, Ryan P.},
	month = jul,
	year = {2015},
	note = {arXiv: 1502.05336},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MGBTXNIG/Hernández-Lobato and Adams - 2015 - Probabilistic Backpropagation for Scalable Learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LNNG2CI5/1502.html:text/html},
}

@article{moore_gaussian_2015,
	title = {Gaussian {Process} {Random} {Fields}},
	url = {http://arxiv.org/abs/1511.00054},
	abstract = {Gaussian processes have been successful in both supervised and unsupervised machine learning tasks, but their computational complexity has constrained practical applications. We introduce a new approximation for large-scale Gaussian processes, the Gaussian Process Random Field (GPRF), in which local GPs are coupled via pairwise potentials. The GPRF likelihood is a simple, tractable, and parallelizeable approximation to the full GP marginal likelihood, enabling latent variable modeling and hyperparameter selection on large datasets. We demonstrate its effectiveness on synthetic spatial data as well as a real-world application to seismic event location.},
	urldate = {2021-11-04},
	journal = {arXiv:1511.00054 [cs, stat]},
	author = {Moore, David A. and Russell, Stuart J.},
	month = oct,
	year = {2015},
	note = {arXiv: 1511.00054},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/BJVCJNV5/Moore and Russell - 2015 - Gaussian Process Random Fields.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QNP2H4MC/1511.html:text/html},
}

@article{gal_improving_2015,
	title = {Improving the {Gaussian} {Process} {Sparse} {Spectrum} {Approximation} by {Representing} {Uncertainty} in {Frequency} {Inputs}},
	url = {http://arxiv.org/abs/1503.02424},
	abstract = {Standard sparse pseudo-input approximations to the Gaussian process (GP) cannot handle complex functions well. Sparse spectrum alternatives attempt to answer this but are known to over-fit. We suggest the use of variational inference for the sparse spectrum approximation to avoid both issues. We model the covariance function with a finite Fourier series approximation and treat it as a random variable. The random covariance function has a posterior, on which a variational distribution is placed. The variational distribution transforms the random covariance function to fit the data. We study the properties of our approximate inference, compare it to alternative ones, and extend it to the distributed and stochastic domains. Our approximation captures complex functions better than standard approaches and avoids over-fitting.},
	urldate = {2021-11-04},
	journal = {arXiv:1503.02424 [stat]},
	author = {Gal, Yarin and Turner, Richard},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.02424},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/BLFDWHK6/Gal and Turner - 2015 - Improving the Gaussian Process Sparse Spectrum App.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/HIX5MBEX/1503.html:text/html},
}

@article{sukhbaatar_not_2021,
	title = {Not {All} {Memories} are {Created} {Equal}: {Learning} to {Forget} by {Expiring}},
	shorttitle = {Not {All} {Memories} are {Created} {Equal}},
	url = {http://arxiv.org/abs/2105.06548},
	abstract = {Attention mechanisms have shown promising results in sequence modeling tasks that require long-term memory. Recent work investigated mechanisms to reduce the computational cost of preserving and storing memories. However, not all content in the past is equally important to remember. We propose Expire-Span, a method that learns to retain the most important information and expire the irrelevant information. This forgetting of memories enables Transformers to scale to attend over tens of thousands of previous timesteps efficiently, as not all states from previous timesteps are preserved. We demonstrate that Expire-Span can help models identify and retain critical information and show it can achieve strong performance on reinforcement learning tasks specifically designed to challenge this functionality. Next, we show that Expire-Span can scale to memories that are tens of thousands in size, setting a new state of the art on incredibly long context tasks such as character-level language modeling and a frame-by-frame moving objects task. Finally, we analyze the efficiency of Expire-Span compared to existing approaches and demonstrate that it trains faster and uses less memory.},
	urldate = {2021-11-02},
	journal = {arXiv:2105.06548 [cs]},
	author = {Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.06548},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/LCVZPW2G/Sukhbaatar et al. - 2021 - Not All Memories are Created Equal Learning to Fo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/G6L6CWQD/2105.html:text/html},
}

@article{lazic_improved_2021,
	title = {Improved {Regret} {Bound} and {Experience} {Replay} in {Regularized} {Policy} {Iteration}},
	url = {http://arxiv.org/abs/2102.12611},
	abstract = {In this work, we study algorithms for learning in infinite-horizon undiscounted Markov decision processes (MDPs) with function approximation. We first show that the regret analysis of the Politex algorithm (a version of regularized policy iteration) can be sharpened from \$O(T{\textasciicircum}\{3/4\})\$ to \$O({\textbackslash}sqrt\{T\})\$ under nearly identical assumptions, and instantiate the bound with linear function approximation. Our result provides the first high-probability \$O({\textbackslash}sqrt\{T\})\$ regret bound for a computationally efficient algorithm in this setting. The exact implementation of Politex with neural network function approximation is inefficient in terms of memory and computation. Since our analysis suggests that we need to approximate the average of the action-value functions of past policies well, we propose a simple efficient implementation where we train a single Q-function on a replay buffer with past data. We show that this often leads to superior performance over other implementation choices, especially in terms of wall-clock time. Our work also provides a novel theoretical justification for using experience replay within policy iteration algorithms.},
	urldate = {2021-11-02},
	journal = {arXiv:2102.12611 [cs, stat]},
	author = {Lazic, Nevena and Yin, Dong and Abbasi-Yadkori, Yasin and Szepesvari, Csaba},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.12611},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4TBR6GV9/Lazic et al. - 2021 - Improved Regret Bound and Experience Replay in Reg.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/58A45IVJ/2102.html:text/html},
}

@article{jiang_prioritized_2021,
	title = {Prioritized {Level} {Replay}},
	url = {http://arxiv.org/abs/2010.03934},
	abstract = {Environments with procedurally generated content serve as important benchmarks for testing systematic generalization in deep reinforcement learning. In this setting, each level is an algorithmically created environment instance with a unique configuration of its factors of variation. Training on a prespecified subset of levels allows for testing generalization to unseen levels. What can be learned from a level depends on the current policy, yet prior work defaults to uniform sampling of training levels independently of the policy. We introduce Prioritized Level Replay (PLR), a general framework for selectively sampling the next training level by prioritizing those with higher estimated learning potential when revisited in the future. We show TD-errors effectively estimate a level's future learning potential and, when used to guide the sampling procedure, induce an emergent curriculum of increasingly difficult levels. By adapting the sampling of training levels, PLR significantly improves sample efficiency and generalization on Procgen Benchmark--matching the previous state-of-the-art in test return--and readily combines with other methods. Combined with the previous leading method, PLR raises the state-of-the-art to over 76\% improvement in test return relative to standard RL baselines.},
	urldate = {2021-11-02},
	journal = {arXiv:2010.03934 [cs]},
	author = {Jiang, Minqi and Grefenstette, Edward and Rocktäschel, Tim},
	month = jun,
	year = {2021},
	note = {arXiv: 2010.03934},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8TT5LJ3H/Jiang et al. - 2021 - Prioritized Level Replay.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LC6QQF82/2010.html:text/html},
}

@article{nekoei_continuous_2021,
	title = {Continuous {Coordination} {As} a {Realistic} {Scenario} for {Lifelong} {Learning}},
	url = {http://arxiv.org/abs/2103.03216},
	abstract = {Current deep reinforcement learning (RL) algorithms are still highly task-specific and lack the ability to generalize to new environments. Lifelong learning (LLL), however, aims at solving multiple tasks sequentially by efficiently transferring and using knowledge between tasks. Despite a surge of interest in lifelong RL in recent years, the lack of a realistic testbed makes robust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the other hand, can be seen as a natural scenario for lifelong RL due to its inherent non-stationarity, since the agents' policies change over time. In this work, we introduce a multi-agent lifelong learning testbed that supports both zero-shot and few-shot settings. Our setup is based on Hanabi -- a partially-observable, fully cooperative multi-agent game that has been shown to be challenging for zero-shot coordination. Its large strategy space makes it a desirable environment for lifelong RL tasks. We evaluate several recent MARL methods, and benchmark state-of-the-art LLL algorithms in limited memory and computation regimes to shed light on their strengths and weaknesses. This continual learning paradigm also provides us with a pragmatic way of going beyond centralized training which is the most commonly used training protocol in MARL. We empirically show that the agents trained in our setup are able to coordinate well with unseen agents, without any additional assumptions made by previous works. The code and all pre-trained models are available at https://github.com/chandar-lab/Lifelong-Hanabi.},
	urldate = {2021-11-02},
	journal = {arXiv:2103.03216 [cs]},
	author = {Nekoei, Hadi and Badrinaaraayanan, Akilesh and Courville, Aaron and Chandar, Sarath},
	month = jun,
	year = {2021},
	note = {arXiv: 2103.03216},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/B5IDP732/Nekoei et al. - 2021 - Continuous Coordination As a Realistic Scenario fo.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/9IIM996R/2103.html:text/html},
}

@article{xie_deep_2020,
	title = {Deep {Reinforcement} {Learning} amidst {Lifelong} {Non}-{Stationarity}},
	url = {http://arxiv.org/abs/2006.10701},
	abstract = {As humans, our goals and our environment are persistently changing throughout our lifetime based on our experiences, actions, and internal and external drives. In contrast, typical reinforcement learning problem set-ups consider decision processes that are stationary across episodes. Can we develop reinforcement learning algorithms that can cope with the persistent change in the former, more realistic problem settings? While on-policy algorithms such as policy gradients in principle can be extended to non-stationary settings, the same cannot be said for more efficient off-policy algorithms that replay past experiences when learning. In this work, we formalize this problem setting, and draw upon ideas from the online learning and probabilistic inference literature to derive an off-policy RL algorithm that can reason about and tackle such lifelong non-stationarity. Our method leverages latent variable models to learn a representation of the environment from current and past experiences, and performs off-policy RL with this representation. We further introduce several simulation environments that exhibit lifelong non-stationarity, and empirically find that our approach substantially outperforms approaches that do not reason about environment shift.},
	urldate = {2021-11-02},
	journal = {arXiv:2006.10701 [cs, stat]},
	author = {Xie, Annie and Harrison, James and Finn, Chelsea},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.10701},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/6MEJP45C/Xie et al. - 2020 - Deep Reinforcement Learning amidst Lifelong Non-St.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F5F3ZCF8/2006.html:text/html},
}

@article{lee_continual_2021,
	title = {Continual {Learning} in the {Teacher}-{Student} {Setup}: {Impact} of {Task} {Similarity}},
	shorttitle = {Continual {Learning} in the {Teacher}-{Student} {Setup}},
	url = {http://arxiv.org/abs/2107.04384},
	abstract = {Continual learning-the ability to learn many tasks in sequence-is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forgetting, where learning new tasks erases knowledge of earlier tasks. While catastrophic forgetting labels the problem, the theoretical reasons for interference between tasks remain unclear. Here, we attempt to narrow this gap between theory and practice by studying continual learning in the teacher-student setup. We extend previous analytical work on two-layer networks in the teacher-student setup to multiple teachers. Using each teacher to represent a different task, we investigate how the relationship between teachers affects the amount of forgetting and transfer exhibited by the student when the task switches. In line with recent work, we find that when tasks depend on similar features, intermediate task similarity leads to greatest forgetting. However, feature similarity is only one way in which tasks may be related. The teacher-student approach allows us to disentangle task similarity at the level of readouts (hidden-to-output weights) and features (input-to-hidden weights). We find a complex interplay between both types of similarity, initial transfer/forgetting rates, maximum transfer/forgetting, and long-term transfer/forgetting. Together, these results help illuminate the diverse factors contributing to catastrophic forgetting.},
	urldate = {2021-11-02},
	journal = {arXiv:2107.04384 [cond-mat, stat]},
	author = {Lee, Sebastian and Goldt, Sebastian and Saxe, Andrew},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.04384},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Statistical Mechanics},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/A3G3YP4A/Lee et al. - 2021 - Continual Learning in the Teacher-Student Setup I.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RMQVM45D/2107.html:text/html},
}

@article{blundell_weight_2015,
	title = {Weight {Uncertainty} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1505.05424},
	abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
	urldate = {2021-05-24},
	journal = {arXiv:1505.05424 [cs, stat]},
	author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
	month = may,
	year = {2015},
	note = {arXiv: 1505.05424},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IRV3WLTN/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/XX769C9K/1505.html:text/html},
}

@article{labach_survey_2019,
	title = {Survey of {Dropout} {Methods} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1904.13310},
	abstract = {Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.},
	urldate = {2021-05-24},
	journal = {arXiv:1904.13310 [cs]},
	author = {Labach, Alex and Salehinejad, Hojjat and Valaee, Shahrokh},
	month = oct,
	year = {2019},
	note = {arXiv: 1904.13310},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TQX7FBIK/Labach et al. - 2019 - Survey of Dropout Methods for Deep Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/N5M6YDYR/1904.html:text/html},
}

@article{gal_uncertainty_nodate,
	title = {Uncertainty in {Deep} {Learning}},
	language = {en},
	author = {Gal, Yarin},
	pages = {174},
	file = {Gal - Uncertainty in Deep Learning.pdf:/Users/tokio/Zotero/storage/5YZRB4Z9/Gal - Uncertainty in Deep Learning.pdf:application/pdf},
}

@article{gal_concrete_2017,
	title = {Concrete {Dropout}},
	url = {http://arxiv.org/abs/1705.07832},
	abstract = {Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary - a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.},
	urldate = {2021-05-24},
	journal = {arXiv:1705.07832 [stat]},
	author = {Gal, Yarin and Hron, Jiri and Kendall, Alex},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07832},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5XDA6UZY/Gal et al. - 2017 - Concrete Dropout.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UJB689W9/1705.html:text/html},
}

@article{gal_dropout_2016,
	title = {Dropout as a {Bayesian} {Approximation}: {Representing} {Model} {Uncertainty} in {Deep} {Learning}},
	shorttitle = {Dropout as a {Bayesian} {Approximation}},
	url = {http://arxiv.org/abs/1506.02142},
	abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
	urldate = {2021-05-24},
	journal = {arXiv:1506.02142 [cs, stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = oct,
	year = {2016},
	note = {arXiv: 1506.02142},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NRAZZGC2/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/5DMYN5VF/1506.html:text/html},
}

@article{welling_bayesian_nodate,
	title = {Bayesian {Learning} via {Stochastic} {Gradient} {Langevin} {Dynamics}},
	abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overﬁtting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a “sampling threshold” and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
	language = {en},
	author = {Welling, Max and Teh, Yee Whye},
	pages = {8},
	file = {Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf:/Users/tokio/Zotero/storage/88MBM8D9/Welling and Teh - Bayesian Learning via Stochastic Gradient Langevin.pdf:application/pdf},
}

@article{serra_overcoming_2018,
	title = {Overcoming catastrophic forgetting with hard attention to the task},
	url = {http://arxiv.org/abs/1801.01423},
	abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
	urldate = {2021-05-24},
	journal = {arXiv:1801.01423 [cs, stat]},
	author = {Serrà, Joan and Surís, Dídac and Miron, Marius and Karatzoglou, Alexandros},
	month = may,
	year = {2018},
	note = {arXiv: 1801.01423},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MN3FTVG5/Serrà et al. - 2018 - Overcoming catastrophic forgetting with hard atten.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/RVP6XNEE/1801.html:text/html},
}

@article{mirzadeh_understanding_2020,
	title = {Understanding the {Role} of {Training} {Regimes} in {Continual} {Learning}},
	url = {http://arxiv.org/abs/2006.06958},
	abstract = {Catastrophic forgetting affects the training of neural networks, limiting their ability to learn multiple tasks sequentially. From the perspective of the well established plasticity-stability dilemma, neural networks tend to be overly plastic, lacking the stability necessary to prevent the forgetting of previous knowledge, which means that as learning progresses, networks tend to forget previously seen tasks. This phenomenon coined in the continual learning literature, has attracted much attention lately, and several families of approaches have been proposed with different degrees of success. However, there has been limited prior work extensively analyzing the impact that different training regimes -- learning rate, batch size, regularization method-- can have on forgetting. In this work, we depart from the typical approach of altering the learning algorithm to improve stability. Instead, we hypothesize that the geometrical properties of the local minima found for each task play an important role in the overall degree of forgetting. In particular, we study the effect of dropout, learning rate decay, and batch size, on forming training regimes that widen the tasks' local minima and consequently, on helping it not to forget catastrophically. Our study provides practical insights to improve stability via simple yet effective techniques that outperform alternative baselines.},
	urldate = {2021-05-24},
	journal = {arXiv:2006.06958 [cs, stat]},
	author = {Mirzadeh, Seyed Iman and Farajtabar, Mehrdad and Pascanu, Razvan and Ghasemzadeh, Hassan},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.06958},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/QJEH5CFR/Mirzadeh et al. - 2020 - Understanding the Role of Training Regimes in Cont.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/L4ZJA4J6/2006.html:text/html},
}

@article{lakshminarayanan_simple_2017,
	title = {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using {Deep} {Ensembles}},
	url = {http://arxiv.org/abs/1612.01474},
	abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
	urldate = {2021-05-24},
	journal = {arXiv:1612.01474 [cs, stat]},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	month = nov,
	year = {2017},
	note = {arXiv: 1612.01474},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/KG5AFSPQ/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6Y246XSK/1612.html:text/html},
}

@article{watanabe_alternative_2012,
	title = {An alternative view of variational {Bayes} and asymptotic approximations of free energy},
	volume = {86},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-011-5264-5},
	doi = {10.1007/s10994-011-5264-5},
	abstract = {Bayesian learning, widely used in many applied data-modeling problems, is often accomplished with approximation schemes because it requires intractable computation of the posterior distributions. In this study, we focus on two approximation methods, variational Bayes and local variational approximation. We show that the variational Bayes approach for statistical models with latent variables can be viewed as a special case of local variational approximation, where the log-sum-exp function is used to form the lower bound of the log-likelihood. The minimum variational free energy, the objective function of variational Bayes, is analyzed and related to the asymptotic theory of Bayesian learning. This analysis additionally implies a relationship between the generalization performance of the variational Bayes approach and the minimum variational free energy.},
	language = {en},
	number = {2},
	urldate = {2021-05-24},
	journal = {Machine Learning},
	author = {Watanabe, Kazuho},
	month = feb,
	year = {2012},
	pages = {273--293},
	file = {Springer Full Text PDF:/Users/tokio/Zotero/storage/RACENLGF/Watanabe - 2012 - An alternative view of variational Bayes and asymp.pdf:application/pdf},
}

@article{he_bayesian_2020,
	title = {Bayesian {Deep} {Ensembles} via the {Neural} {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/2007.05864},
	abstract = {We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks.},
	urldate = {2021-05-24},
	journal = {arXiv:2007.05864 [cs, stat]},
	author = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
	month = oct,
	year = {2020},
	note = {arXiv: 2007.05864},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GYC9WMVZ/He et al. - 2020 - Bayesian Deep Ensembles via the Neural Tangent Ker.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6MYMCQX4/2007.html:text/html},
}

@article{mianjy_convergence_2020,
	title = {On {Convergence} and {Generalization} of {Dropout} {Training}},
	url = {http://arxiv.org/abs/2010.12711},
	abstract = {We study dropout in two-layer neural networks with rectified linear unit (ReLU) activations. Under mild overparametrization and assuming that the limiting kernel can separate the data distribution with a positive margin, we show that dropout training with logistic loss achieves \${\textbackslash}epsilon\$-suboptimality in test error in \$O(1/{\textbackslash}epsilon)\$ iterations.},
	urldate = {2021-05-24},
	journal = {arXiv:2010.12711 [cs, stat]},
	author = {Mianjy, Poorya and Arora, Raman},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.12711},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/N3BIIE65/Mianjy and Arora - 2020 - On Convergence and Generalization of Dropout Train.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/KAGD3RJD/2010.html:text/html},
}

@article{mundt_wholistic_2020,
	title = {A {Wholistic} {View} of {Continual} {Learning} with {Deep} {Neural} {Networks}: {Forgotten} {Lessons} and the {Bridge} to {Active} and {Open} {World} {Learning}},
	shorttitle = {A {Wholistic} {View} of {Continual} {Learning} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2009.01797},
	abstract = {Current deep learning research is dominated by benchmark evaluation. A method is regarded as favorable if it empirically performs well on the dedicated test set. This mentality is seamlessly reflected in the resurfacing area of continual learning, where consecutively arriving sets of benchmark data are investigated. The core challenge is framed as protecting previously acquired representations from being catastrophically forgotten due to the iterative parameter updates. However, comparison of individual methods is nevertheless treated in isolation from real world application and typically judged by monitoring accumulated test set performance. The closed world assumption remains predominant. It is assumed that during deployment a model is guaranteed to encounter data that stems from the same distribution as used for training. This poses a massive challenge as neural networks are well known to provide overconfident false predictions on unknown instances and break down in the face of corrupted data. In this work we argue that notable lessons from open set recognition, the identification of statistically deviating data outside of the observed dataset, and the adjacent field of active learning, where data is incrementally queried such that the expected performance gain is maximized, are frequently overlooked in the deep learning era. Based on these forgotten lessons, we propose a consolidated view to bridge continual learning, active learning and open set recognition in deep neural networks. Our results show that this not only benefits each individual paradigm, but highlights the natural synergies in a common framework. We empirically demonstrate improvements when alleviating catastrophic forgetting, querying data in active learning, selecting task orders, while exhibiting robust open world application where previously proposed methods fail.},
	urldate = {2021-05-24},
	journal = {arXiv:2009.01797 [cs, stat]},
	author = {Mundt, Martin and Hong, Yong Won and Pliushch, Iuliia and Ramesh, Visvanathan},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.01797},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/B7WERD3I/Mundt et al. - 2020 - A Wholistic View of Continual Learning with Deep N.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/F7VVVG2Z/2009.html:text/html},
}

@article{hein_why_2019,
	title = {Why {ReLU} networks yield high-confidence predictions far away from the training data and how to mitigate the problem},
	url = {http://arxiv.org/abs/1812.05720},
	abstract = {Classifiers used in the wild, in particular for safety-critical systems, should not only have good generalization properties but also should know when they don't know, in particular make low confidence predictions far away from the training data. We show that ReLU type neural networks which yield a piecewise linear classifier function fail in this regard as they produce almost always high confidence predictions far away from the training data. For bounded domains like images we propose a new robust optimization technique similar to adversarial training which enforces low confidence predictions far away from the training data. We show that this technique is surprisingly effective in reducing the confidence of predictions far away from the training data while maintaining high confidence predictions and test error on the original classification task compared to standard training.},
	urldate = {2021-05-24},
	journal = {arXiv:1812.05720 [cs, stat]},
	author = {Hein, Matthias and Andriushchenko, Maksym and Bitterwolf, Julian},
	month = may,
	year = {2019},
	note = {arXiv: 1812.05720},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UXKHUWK5/Hein et al. - 2019 - Why ReLU networks yield high-confidence prediction.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/79FLQBH3/1812.html:text/html},
}

@article{mirzadeh_dropout_2020,
	title = {Dropout as an {Implicit} {Gating} {Mechanism} {For} {Continual} {Learning}},
	url = {http://arxiv.org/abs/2004.11545},
	abstract = {In recent years, neural networks have demonstrated an outstanding ability to achieve complex learning tasks across various domains. However, they suffer from the "catastrophic forgetting" problem when they face a sequence of learning tasks, where they forget the old ones as they learn new tasks. This problem is also highly related to the "stability-plasticity dilemma". The more plastic the network, the easier it can learn new tasks, but the faster it also forgets previous ones. Conversely, a stable network cannot learn new tasks as fast as a very plastic network. However, it is more reliable to preserve the knowledge it has learned from the previous tasks. Several solutions have been proposed to overcome the forgetting problem by making the neural network parameters more stable, and some of them have mentioned the significance of dropout in continual learning. However, their relationship has not been sufficiently studied yet. In this paper, we investigate this relationship and show that a stable network with dropout learns a gating mechanism such that for different tasks, different paths of the network are active. Our experiments show that the stability achieved by this implicit gating plays a very critical role in leading to performance comparable to or better than other involved continual learning algorithms to overcome catastrophic forgetting.},
	urldate = {2021-05-24},
	journal = {arXiv:2004.11545 [cs, stat]},
	author = {Mirzadeh, Seyed-Iman and Farajtabar, Mehrdad and Ghasemzadeh, Hassan},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11545},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NE6U75IQ/Mirzadeh et al. - 2020 - Dropout as an Implicit Gating Mechanism For Contin.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/MCSVRE3P/2004.html:text/html},
}

@article{ahn_uncertainty-based_2019,
	title = {Uncertainty-based {Continual} {Learning} with {Adaptive} {Regularization}},
	url = {http://arxiv.org/abs/1905.11614},
	abstract = {We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.},
	urldate = {2021-05-24},
	journal = {arXiv:1905.11614 [cs, stat]},
	author = {Ahn, Hongjoon and Cha, Sungmin and Lee, Donggyu and Moon, Taesup},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.11614},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/TFB2542T/Ahn et al. - 2019 - Uncertainty-based Continual Learning with Adaptive.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/MBLVC4FS/1905.html:text/html},
}

@article{molchanov_variational_2017,
	title = {Variational {Dropout} {Sparsifies} {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1701.05369},
	abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
	urldate = {2021-05-24},
	journal = {arXiv:1701.05369 [cs, stat]},
	author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
	month = jun,
	year = {2017},
	note = {arXiv: 1701.05369},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MVU4MKTJ/Molchanov et al. - 2017 - Variational Dropout Sparsifies Deep Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/P6YIBJLK/1701.html:text/html},
}

@article{jung_continual_2020,
	title = {Continual {Learning} with {Node}-{Importance} based {Adaptive} {Group} {Sparse} {Regularization}},
	url = {http://arxiv.org/abs/2003.13726},
	abstract = {We propose a novel regularization-based continual learning method, dubbed as Adaptive Group Sparsity based Continual Learning (AGS-CL), using two group sparsity-based penalties. Our method selectively employs the two penalties when learning each node based its the importance, which is adaptively updated after learning each new task. By utilizing the proximal gradient descent method for learning, the exact sparsity and freezing of the model is guaranteed, and thus, the learner can explicitly control the model capacity as the learning continues. Furthermore, as a critical detail, we re-initialize the weights associated with unimportant nodes after learning each task in order to prevent the negative transfer that causes the catastrophic forgetting and facilitate efficient learning of new tasks. Throughout the extensive experimental results, we show that our AGS-CL uses much less additional memory space for storing the regularization parameters, and it significantly outperforms several state-of-the-art baselines on representative continual learning benchmarks for both supervised and reinforcement learning tasks.},
	urldate = {2021-05-24},
	journal = {arXiv:2003.13726 [cs, stat]},
	author = {Jung, Sangwon and Ahn, Hongjoon and Cha, Sungmin and Moon, Taesup},
	month = dec,
	year = {2020},
	note = {arXiv: 2003.13726},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/5SRNNQ8D/Jung et al. - 2020 - Continual Learning with Node-Importance based Adap.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NTIE9MU7/2003.html:text/html},
}

@article{adel_continual_2020,
	title = {Continual {Learning} with {Adaptive} {Weights} ({CLAW})},
	url = {http://arxiv.org/abs/1911.09514},
	abstract = {Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.},
	urldate = {2021-05-24},
	journal = {arXiv:1911.09514 [cs, stat]},
	author = {Adel, Tameem and Zhao, Han and Turner, Richard E.},
	month = jun,
	year = {2020},
	note = {arXiv: 1911.09514},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ASBLYNHV/Adel et al. - 2020 - Continual Learning with Adaptive Weights (CLAW).pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/STCH3IFL/1911.html:text/html},
}

@article{farquhar_towards_2019,
	title = {Towards {Robust} {Evaluations} of {Continual} {Learning}},
	url = {http://arxiv.org/abs/1805.09733},
	abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
	urldate = {2021-05-24},
	journal = {arXiv:1805.09733 [cs, stat]},
	author = {Farquhar, Sebastian and Gal, Yarin},
	month = jun,
	year = {2019},
	note = {arXiv: 1805.09733},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NYAVLFBC/Farquhar and Gal - 2019 - Towards Robust Evaluations of Continual Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QI6L5TCZ/1805.html:text/html},
}

@inproceedings{yun_small_2019,
	title = {Small {ReLU} networks are powerful memorizers: a tight analysis of memorization capacity},
	volume = {32},
	shorttitle = {Small {ReLU} networks are powerful memorizers},
	url = {https://proceedings.neurips.cc/paper/2019/hash/dbea3d0e2a17c170c412c74273778159-Abstract.html},
	urldate = {2023-03-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/FTRZ9CK8/Yun et al. - 2019 - Small ReLU networks are powerful memorizers a tig.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/7PXIE6BZ/main_memorize-13-28.pdf:application/pdf},
}

@inproceedings{ying_transformers_2022,
	title = {Do {Transformers} {Really} {Perform} {Badly} for {Graph} {Representation}?},
	url = {https://openreview.net/forum?id=OeWooOxFwDa},
	abstract = {The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer. The code and models of Graphormer will be made publicly available at {\textbackslash}url\{https://github.com/Microsoft/Graphormer\}.},
	language = {en},
	urldate = {2023-03-09},
	author = {Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZLEHNFQ5/Ying et al. - 2022 - Do Transformers Really Perform Badly for Graph Rep.pdf:application/pdf;Supplementary material:/Users/tokio/Zotero/storage/9X54XGRZ/777_do_transformers_really_perform-Supplementary Material.pdf:application/pdf},
}

@inproceedings{sicilia_pac-bayesian_2022,
	title = {{PAC}-{Bayesian} domain adaptation bounds for multiclass learners},
	url = {https://proceedings.mlr.press/v180/sicilia22a.html},
	abstract = {Multiclass neural networks are a common tool in modern unsupervised domain adaptation, yet an appropriate theoretical description for their non-uniform sample complexity is lacking in the adaptation literature. To fill this gap, we propose the first PAC-Bayesian adaptation bounds for multiclass learners. We facilitate practical use of our bounds by also proposing the first approximation techniques for the multiclass distribution divergences we consider. For divergences dependent on a Gibbs predictor, we propose additional PAC-Bayesian adaptation bounds which remove the need for inefficient Monte-Carlo estimation. Empirically, we test the efficacy of our proposed approximation techniques as well as some novel design-concepts which we include in our bounds. Finally, we apply our bounds to analyze a common adaptation algorithm that uses neural networks.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Sicilia, Anthony and Atwell, Katherine and Alikhani, Malihe and Hwang, Seong Jae},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {1824--1834},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZCNHMKUL/Sicilia et al. - 2022 - PAC-Bayesian domain adaptation bounds for multicla.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/XHIXIDIF/Sicilia et al. - 2022 - PAC-Bayesian domain adaptation bounds for multicla.pdf:application/pdf},
}

@inproceedings{kim_provable_2023,
	title = {Provable {Memorization} {Capacity} of {Transformers}},
	url = {https://openreview.net/forum?id=8JCg5xJCTPR},
	abstract = {Quantifying memorization capacity is essential for understanding the expressiveness and generalizability of deep learning model architectures. However, the memorization capacity of the Transformer architecture has yet to be explored. In this work, we present the first study of the memorization capacity of the Transformer architecture. We prove that Transformers are capable of memorizing \$N\$ sequence-to-sequence mappings of length \$n\$ with \$d\$-dimensional input tokens using \${\textbackslash}tilde\{O\}(d + n + {\textbackslash}sqrt\{nN\})\$ parameters. Our theory supports memorization both with and without permutation equivariance, utilizing positional encodings in the latter case. Building on our theory, we also analyze the memorization capacity of Transformers in the sequence classification and language modeling tasks. To verify these theoretical findings, we conduct experiments analyzing the memorization capacity of Transformers in the natural language domain.},
	language = {en},
	urldate = {2023-03-09},
	author = {Kim, Junghwan and Kim, Michelle and Mozafari, Barzan},
	month = feb,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/8Z7D3VDJ/Kim et al. - 2023 - Provable Memorization Capacity of Transformers.pdf:application/pdf},
}

@inproceedings{dong_attention_2021,
	title = {Attention is not all you need: pure attention loses rank doubly exponentially with depth},
	shorttitle = {Attention is not all you need},
	url = {https://proceedings.mlr.press/v139/dong21a.html},
	abstract = {Attention-based architectures have become ubiquitous in machine learning. Yet, our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers. Using this path decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dong, Yihe and Cordonnier, Jean-Baptiste and Loukas, Andreas},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {2793--2803},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/U7W5XWXF/Dong et al. - 2021 - Attention is not all you need pure attention lose.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/KFYRVMYR/Dong et al. - 2021 - Attention is not all you need pure attention lose.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-074.html},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus”, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {140},
	urldate = {2023-03-09},
	journal = {Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	pages = {1--67},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/2SAF6GEZ/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf;Source Code:/Users/tokio/Zotero/storage/7T3M249L/text-to-text-transfer-transformer.html:text/html},
}

@inproceedings{zhu_beyond_2020,
	title = {Beyond {Homophily} in {Graph} {Neural} {Networks}: {Current} {Limitations} and {Effective} {Designs}},
	volume = {33},
	shorttitle = {Beyond {Homophily} in {Graph} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/58ae23d878a47004366189884c2f8440-Abstract.html},
	abstract = {We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Many popular GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs—ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations—that boost learning from the graph structure under heterophily. We combine them into a graph neural network, H2GCN, which we use as the base method to empirically evaluate the effectiveness of the identified designs. Going beyond the traditional benchmarks with strong homophily, our empirical analysis shows that the identified designs increase the accuracy of GNNs by up to 40\% and 27\% over models without them on synthetic and real networks with heterophily, respectively, and yield competitive performance under homophily.},
	urldate = {2023-03-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and Koutra, Danai},
	year = {2020},
	pages = {7793--7804},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/LFFV9JI9/Zhu et al. - 2020 - Beyond Homophily in Graph Neural Networks Current.pdf:application/pdf;NeurIPS-2020-beyond-homophily-in-graph-neural-networks-current-limitations-and-effective-designs-Supplemental.pdf:/Users/tokio/Zotero/storage/4XMWVHXS/NeurIPS-2020-beyond-homophily-in-graph-neural-networks-current-limitations-and-effective-designs-Supplemental.pdf:application/pdf},
}

@inproceedings{defferrard_convolutional_2016,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/04df4d434d481c5bb723be1b6df1ee65-Abstract.html},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words’ embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2023-03-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	year = {2016},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/EDLWRPWU/Defferrard et al. - 2016 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;supplement.pdf:/Users/tokio/Zotero/storage/IBPJVVJI/supplement.pdf:application/pdf},
}

@inproceedings{akyurek_what_2023,
	title = {​​{What} learning algorithm is in-context learning? {Investigations} with linear models},
	shorttitle = {​​{What} learning algorithm is in-context learning?},
	url = {https://openreview.net/forum?id=0g0X4H8yN4I},
	abstract = {Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples \$(x, f(x))\$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding context-specific parametric models in their hidden representations, and updating these implicit models as new examples appear in the context. Using linear regression as a model problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form computation of regression parameters. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners' late layers encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may work by rediscovering standard estimation algorithms.},
	language = {en},
	urldate = {2023-03-14},
	author = {Akyürek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
	month = feb,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/X4E3JKWG/Akyürek et al. - 2023 - What learning algorithm is in-context learning .pdf:application/pdf},
}

@misc{hu_open_2021,
	title = {Open {Graph} {Benchmark}: {Datasets} for {Machine} {Learning} on {Graphs}},
	shorttitle = {Open {Graph} {Benchmark}},
	url = {http://arxiv.org/abs/2005.00687},
	doi = {10.48550/arXiv.2005.00687},
	abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .},
	urldate = {2023-03-15},
	publisher = {arXiv},
	author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
	month = feb,
	year = {2021},
	note = {arXiv:2005.00687 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WFRNWMHV/Hu et al. - 2021 - Open Graph Benchmark Datasets for Machine Learnin.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CW2PJST7/2005.html:text/html},
}

@inproceedings{shaw_self-attention_2018,
	address = {New Orleans, Louisiana},
	title = {Self-{Attention} with {Relative} {Position} {Representations}},
	url = {https://aclanthology.org/N18-2074},
	doi = {10.18653/v1/N18-2074},
	abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
	urldate = {2023-03-15},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 2 ({Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
	month = jun,
	year = {2018},
	pages = {464--468},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/8H4KUM7M/Shaw et al. - 2018 - Self-Attention with Relative Position Representati.pdf:application/pdf},
}

@inproceedings{yun_are_2023,
	title = {Are {Transformers} universal approximators of sequence-to-sequence functions?},
	url = {https://openreview.net/forum?id=ByxRM0Ntvr},
	abstract = {Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other simpler alternatives to self-attention layers and empirically evaluate them.},
	language = {en},
	urldate = {2023-03-15},
	author = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
	month = dec,
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/VGFXPA7U/Yun et al. - 2023 - Are Transformers universal approximators of sequen.pdf:application/pdf},
}

@inproceedings{wang_equivariant_2022,
	title = {Equivariant and {Stable} {Positional} {Encoding} for {More} {Powerful} {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=e95i1IHcWj},
	abstract = {Graph neural networks (GNN) have shown great advantages in many graph-based learning tasks but often fail to predict accurately for a task-based on sets of nodes such as link/motif prediction and so on. Many works have recently proposed to address this problem by using random node features or node distance features. However, they suffer from either slow convergence, inaccurate prediction, or high complexity. In this work, we revisit GNNs that allow using positional features of nodes given by positional encoding (PE) techniques such as Laplacian Eigenmap, Deepwalk, etc. GNNs with PE often get criticized because they are not generalizable to unseen graphs (inductive) or stable. Here, we study these issues in a principled way and propose a provable solution, a class of GNN layers termed PEG with rigorous mathematical analysis. PEG uses separate channels to update the original node features and positional features. PEG imposes permutation equivariance w.r.t. the original node features and rotation equivariance w.r.t. the positional features simultaneously. Extensive link prediction experiments over 8 real-world networks demonstrate the advantages of PEG in generalization and scalability. Code is available at https://github.com/Graph-COM/PEG.},
	language = {en},
	urldate = {2023-03-16},
	author = {Wang, Haorui and Yin, Haoteng and Zhang, Muhan and Li, Pan},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/6ZYA72II/Wang et al. - 2022 - Equivariant and Stable Positional Encoding for Mor.pdf:application/pdf},
}

@inproceedings{ke_rethinking_2023,
	title = {Rethinking {Positional} {Encoding} in {Language} {Pre}-training},
	url = {https://openreview.net/forum?id=09-528y2Fgf},
	abstract = {In this work, we investigate the positional encoding methods used in language pre-training (e.g., BERT) and identify several problems in the existing formulations. First, we show that in the absolute positional encoding, the addition operation applied on positional embeddings and word embeddings brings mixed correlations between the two heterogeneous information resources. It may bring unnecessary randomness in the attention and further limit the expressiveness of the model. Second, we question whether treating the position of the symbol {\textbackslash}texttt\{[CLS]\} the same as other words is a reasonable design, considering its special role (the representation of the entire sentence) in the downstream tasks. Motivated from above analysis, we propose a new positional encoding method called {\textbackslash}textbf\{T\}ransformer with {\textbackslash}textbf\{U\}ntied {\textbackslash}textbf\{P\}ositional {\textbackslash}textbf\{E\}ncoding (TUPE). In the self-attention module, TUPE computes the word contextual correlation and positional correlation separately with different parameterizations and then adds them together. This design removes the mixed and noisy correlations over heterogeneous embeddings and offers more expressiveness by using different projection matrices. Furthermore, TUPE unties the {\textbackslash}texttt\{[CLS]\} symbol from other positions, making it easier to capture information from all positions. Extensive experiments and ablation studies on GLUE benchmark demonstrate the effectiveness of the proposed method. Codes and models are released at {\textbackslash}url\{https://github.com/guolinke/TUPE\}.},
	language = {en},
	urldate = {2023-03-16},
	author = {Ke, Guolin and He, Di and Liu, Tie-Yan},
	month = jan,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KEAI5VUS/Ke et al. - 2023 - Rethinking Positional Encoding in Language Pre-tra.pdf:application/pdf},
}

@inproceedings{olatunji_membership_2021,
	title = {Membership {Inference} {Attack} on {Graph} {Neural} {Networks}},
	doi = {10.1109/TPSISA52974.2021.00002},
	abstract = {Graph Neural Networks (GNNs), which generalize traditional deep neural networks on graph data, have achieved state-of-the-art performance on several graph analytical tasks. We focus on how trained GNN models could leak information about the member nodes that they were trained on. We introduce two realistic settings for performing a membership inference (MI) attack on GNNs. While choosing the simplest possible attack model that utilizes the posteriors of the trained model (black-box access), we thoroughly analyze the properties of GNNs and the datasets which dictate the differences in their robustness towards MI attack. While in traditional machine learning models, overfitting is considered the main cause of such leakage, we show that in GNNs the additional structural information is the major contributing factor. We support our findings by extensive experiments on four representative GNN models. To prevent MI attacks on GNN, we propose two effective defenses that significantly decreases the attacker's inference by up to 60\% without degradation to the target model's performance. Our code is available at https://github.com/iyempissy/rebMIGraph.},
	booktitle = {2021 {Third} {IEEE} {International} {Conference} on {Trust}, {Privacy} and {Security} in {Intelligent} {Systems} and {Applications} ({TPS}-{ISA})},
	author = {Olatunji, Iyiola E. and Nejdl, Wolfgang and Khosla, Megha},
	month = feb,
	year = {2021},
	keywords = {Analytical models, Graph neural networks, Graph Neural Networks, Market research, Measurement, Membership Inference, Perturbation methods, Privacy, Privacy leakage, Robustness},
	pages = {11--20},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/EUDQTVPG/9750191.html:text/html;報告したバージョン:/Users/tokio/Zotero/storage/AZFNEURW/Olatunji et al. - 2021 - Membership Inference Attack on Graph Neural Networ.pdf:application/pdf},
}

@inproceedings{hussain_global_2022,
	address = {New York, NY, USA},
	series = {{KDD} '22},
	title = {Global {Self}-{Attention} as a {Replacement} for {Graph} {Convolution}},
	isbn = {978-1-4503-9385-0},
	url = {https://doi.org/10.1145/3534678.3539296},
	doi = {10.1145/3534678.3539296},
	abstract = {We propose an extension to the transformer neural network architecture for general-purpose graph learning by adding a dedicated pathway for pairwise structural information, called edge channels. The resultant framework - which we call Edge-augmented Graph Transformer (EGT) - can directly accept, process and output structural information of arbitrary form, which is important for effective learning on graph-structured data. Our model exclusively uses global self-attention as an aggregation mechanism rather than static localized convolutional aggregation. This allows for unconstrained long-range dynamic interactions between nodes. Moreover, the edge channels allow the structural information to evolve from layer to layer, and prediction tasks on edges/links can be performed directly from the output embeddings of these channels. We verify the performance of EGT in a wide range of graph-learning experiments on benchmark datasets, in which it outperforms Convolutional/Message-Passing Graph Neural Networks. EGT sets a new state-of-the-art for the quantum-chemical regression task on the OGB-LSC PCQM4Mv2 dataset containing 3.8 million molecular graphs. Our findings indicate that global self-attention based aggregation can serve as a flexible, adaptive and effective replacement of graph convolution for general-purpose graph learning. Therefore, convolutional local neighborhood aggregation is not an essential inductive bias.},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the 28th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Hussain, Md Shamim and Zaki, Mohammed J. and Subramanian, Dharmashankar},
	month = aug,
	year = {2022},
	keywords = {graph neural networks, graph representation learning, self-attention},
	pages = {655--665},
	file = {受け入れたバージョン:/Users/tokio/Zotero/storage/GAI7ID2K/Hussain et al. - 2022 - Global Self-Attention as a Replacement for Graph C.pdf:application/pdf},
}

@inproceedings{edelman_inductive_2022,
	title = {Inductive {Biases} and {Variable} {Creation} in {Self}-{Attention} {Mechanisms}},
	url = {https://proceedings.mlr.press/v162/edelman22a.html},
	abstract = {Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm Transformer networks "create sparse variables": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers.},
	language = {en},
	urldate = {2023-03-16},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Edelman, Benjamin L. and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {5793--5831},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JKWGYTL2/Edelman et al. - 2022 - Inductive Biases and Variable Creation in Self-Att.pdf:application/pdf},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	number = {4},
	urldate = {2023-03-17},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	note = {arXiv:1906.02691 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {307--392},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/WXRLTVX5/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/3CDE9ZBZ/1906.html:text/html},
}

@inproceedings{luo_your_2022,
	title = {Your {Transformer} {May} {Not} be as {Powerful} as {You} {Expect}},
	url = {https://openreview.net/forum?id=NQFFNdsOGD},
	abstract = {Relative Positional Encoding (RPE), which encodes the relative distance between any pair of tokens, is one of the most successful modifications to the original Transformer. As far as we know, theoretical understanding of the RPE-based Transformers is largely unexplored. In this work, we mathematically analyze the power of RPE-based Transformers regarding whether the model is capable of approximating any continuous sequence-to-sequence functions. One may naturally assume the answer is in the affirmative---RPE-based Transformers are universal function approximators. However, we present a negative result by showing there exist continuous sequence-to-sequence functions that RPE-based Transformers cannot approximate no matter how deep and wide the neural network is. One key reason lies in that most RPEs are placed in the softmax attention that always generates a right stochastic matrix. This restricts the network from capturing positional information in the RPEs and limits its capacity. To overcome the problem and make the model more powerful, we first present sufficient conditions for RPE-based Transformers to achieve universal function approximation. With the theoretical guidance, we develop a novel attention module, called Universal RPE-based (URPE) Attention, which satisfies the conditions. Therefore, the corresponding URPE-based Transformers become universal function approximators. Extensive experiments covering typical architectures and tasks demonstrate that our model is parameter-efficient and can achieve superior performance to strong baselines in a wide range of applications. The code will be made publicly available at https://github.com/lsj2408/URPE.},
	language = {en},
	urldate = {2023-03-22},
	author = {Luo, Shengjie and Li, Shanda and Zheng, Shuxin and Liu, Tie-Yan and Wang, Liwei and He, Di},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/JSN9LHRW/Luo et al. - 2022 - Your Transformer May Not be as Powerful as You Exp.pdf:application/pdf},
}

@inproceedings{ying_hierarchical_2018,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	volume = {31},
	url = {https://papers.nips.cc/paper_files/paper/2018/hash/e77dbaf6759253c7c6d0efc5690369c7-Abstract.html},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark datasets.},
	urldate = {2023-03-22},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ying, Zhitao and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, Will and Leskovec, Jure},
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/N4W5B63L/Ying et al. - 2018 - Hierarchical Graph Representation Learning with Di.pdf:application/pdf},
}

@article{sontag_shattering_1997,
	title = {Shattering {All} {Sets} of \textit{‘k’} {Points} in “{General} {Position}” {Requires} ( \textit{k} — 1)/2 {Parameters}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/2/337-348/6035},
	doi = {10.1162/neco.1997.9.2.337},
	abstract = {For classes of concepts defined by certain classes of analytic functions depending on n parameters, there are nonempty open sets of samples of length 2n + 2 that cannot be shattered. A slighly weaker result is also proved for piecewise-analytic functions. The special case of neural networks is discussed.},
	language = {en},
	number = {2},
	urldate = {2023-03-23},
	journal = {Neural Computation},
	author = {Sontag, Eduardo D.},
	month = feb,
	year = {1997},
	pages = {337--348},
	file = {Sontag - 1997 - Shattering All Sets of ‘k’ Points in “Gener.pdf:/Users/tokio/Zotero/storage/SJRL8CRI/Sontag - 1997 - Shattering All Sets of ‘k’ Points in “Gener.pdf:application/pdf},
}

@inproceedings{vardi_optimal_2022,
	title = {On the {Optimal} {Memorization} {Power} of {ReLU} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=MkTPtnjeYTV},
	abstract = {We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any \$N\$ points that satisfy a mild separability assumption using \${\textbackslash}tilde\{O\}{\textbackslash}left({\textbackslash}sqrt\{N\}{\textbackslash}right)\$ parameters. Known VC-dimension upper bounds imply that memorizing \$N\$ samples requires \${\textbackslash}Omega({\textbackslash}sqrt\{N\})\$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by \$1 {\textbackslash}leq L {\textbackslash}leq {\textbackslash}sqrt\{N\}\$, for memorizing \$N\$ samples using \${\textbackslash}tilde\{O\}(N/L)\$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.},
	language = {en},
	urldate = {2023-03-28},
	author = {Vardi, Gal and Yehudai, Gilad and Shamir, Ohad},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/RDMJAKY5/Vardi et al. - 2022 - On the Optimal Memorization Power of ReLU Neural N.pdf:application/pdf},
}

@inproceedings{hamilton_inductive_2017,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/5dd9db5e033da9c6fb5ba83c7a7ebea9-Abstract.html},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings.  Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2023-04-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	year = {2017},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/NLNAUYAV/Hamilton et al. - 2017 - Inductive Representation Learning on Large Graphs.pdf:application/pdf},
}

@article{yang_breaking_2022,
	title = {Breaking the {Expression} {Bottleneck} of {Graph} {Neural} {Networks}},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2022.3168070},
	abstract = {Recently, the Weisfeiler-Lehman (WL) graph isomorphism test was used to measure the expressiveness of graph neural networks (GNNs), showing that the neighborhood aggregation GNNs were at most as powerful as 1-WL test in distinguishing graph structures. There were also improvements proposed in analogy to k-WL test (\$k{\textgreater}1\$). However, the aggregations in these GNNs are far from injective as required by the WL test, and suffer from weak distinguishing strength, making it become the expression bottleneck. In this paper, we improve the expressiveness by exploring powerful aggregations. We reformulate an aggregation with the corresponding aggregation coefficient matrix, and then systematically analyze the requirements on this matrix for building more powerful and even injective aggregations. We also show the necessity of applying nonlinear units ahead of aggregations, which is different from most existing GNNs. Based on our theoretical analysis, we develop ExpandingConv. Experimental results show that our model significantly boosts performance, especially for large and densely connected graphs.},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Yang, Mingqi and Wang, Renjian and Shen, Yanming and Qi, Heng and Yin, Baocai},
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Graph neural networks, Graph Neural Networks, Buildings, Convolution, Deep Learning, Graph Representation Learning, Power measurement, Representation learning, Systematics, Task analysis},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/XAXY7HA6/9759979.html:text/html;IEEE Xplore Full Text PDF:/Users/tokio/Zotero/storage/53IU5G6J/Yang et al. - 2022 - Breaking the Expression Bottleneck of Graph Neural.pdf:application/pdf},
}

@article{morris_weisfeiler_2019,
	title = {Weisfeiler and {Leman} {Go} {Neural}: {Higher}-{Order} {Graph} {Neural} {Networks}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	shorttitle = {Weisfeiler and {Leman} {Go} {Neural}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4384},
	doi = {10.1609/aaai.v33i01.33014602},
	abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically—showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
	language = {en},
	number = {01},
	urldate = {2023-04-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {4602--4609},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/XQAE7276/Morris et al. - 2019 - Weisfeiler and Leman Go Neural Higher-Order Graph.pdf:application/pdf},
}

@misc{morris_weisfeiler_2023,
	title = {Weisfeiler and {Leman} go {Machine} {Learning}: {The} {Story} so far},
	shorttitle = {Weisfeiler and {Leman} go {Machine} {Learning}},
	url = {http://arxiv.org/abs/2112.09992},
	doi = {10.48550/arXiv.2112.09992},
	abstract = {In recent years, algorithms and neural architectures based on the Weisfeiler-Leman algorithm, a well-known heuristic for the graph isomorphism problem, have emerged as a powerful tool for machine learning with graphs and relational data. Here, we give a comprehensive overview of the algorithm's use in a machine-learning setting, focusing on the supervised regime. We discuss the theoretical background, show how to use it for supervised graph and node representation learning, discuss recent extensions, and outline the algorithm's connection to (permutation-)equivariant neural architectures. Moreover, we give an overview of current applications and future directions to stimulate further research.},
	urldate = {2023-04-06},
	publisher = {arXiv},
	author = {Morris, Christopher and Lipman, Yaron and Maron, Haggai and Rieck, Bastian and Kriege, Nils M. and Grohe, Martin and Fey, Matthias and Borgwardt, Karsten},
	month = mar,
	year = {2023},
	note = {arXiv:2112.09992 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3IG3CFVJ/Morris et al. - 2023 - Weisfeiler and Leman go Machine Learning The Stor.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/Y5KEUKX5/2112.html:text/html},
}

@article{cordella_subgraph_2004,
	title = {A (sub)graph isomorphism algorithm for matching large graphs},
	volume = {26},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2004.75},
	abstract = {We present an algorithm for graph isomorphism and subgraph isomorphism suited for dealing with large graphs. A first version of the algorithm has been presented in a previous paper, where we examined its performance for the isomorphism of small and medium size graphs. The algorithm is improved here to reduce its spatial complexity and to achieve a better performance on large graphs; its features are analyzed in detail with special reference to time and memory requirements. The results of a testing performed on a publicly available database of synthetically generated graphs and on graphs relative to a real application dealing with technical drawings are presented, confirming the effectiveness of the approach, especially when working with large graphs.},
	number = {10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Cordella, L.P. and Foggia, P. and Sansone, C. and Vento, M.},
	month = oct,
	year = {2004},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Algorithm design and analysis, Application software, attributed relational graphs., Index Terms- Graph-subgraph isomorphism, large graphs, NP-complete problem, Pattern analysis, Pattern matching, Pattern recognition, Performance analysis, Performance evaluation, Relational databases, Testing},
	pages = {1367--1372},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/ETBEFTSV/1323804.html:text/html;IEEE Xplore Full Text PDF:/Users/tokio/Zotero/storage/L5ZANTTE/Cordella et al. - 2004 - A (sub)graph isomorphism algorithm for matching la.pdf:application/pdf},
}

@inproceedings{carletti_introducing_2017,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Introducing {VF3}: {A} {New} {Algorithm} for {Subgraph} {Isomorphism}},
	isbn = {978-3-319-58961-9},
	shorttitle = {Introducing {VF3}},
	doi = {10.1007/978-3-319-58961-9_12},
	abstract = {Several graph-based applications require to detect and locate occurrences of a pattern graph within a larger target graph. Subgraph isomorphism is a widely adopted formalization of this problem. While subgraph isomorphism is NP-Complete in the general case, there are algorithms that can solve it in a reasonable time on the average graphs that are encountered in specific real-world applications. In 2015 we introduced one such algorithm, VF2Plus, that was specifically designed for the large graphs encountered in bioinformatics applications. VF2Plus was an evolution of VF2, which had been considered for many years one of the fastest available algorithms. In turn, VF2Plus proved to be significantly faster than its predecessor, and among the fastest algorithms on bioinformatics graphs. In this paper we propose a further evolution, named VF3, that adds new improvements specifically targeted at enhancing the performance on graphs that are at the same time large and dense, that are currently the most problematic case for the state-of-the-art algorithms. The effectiveness of VF3 has been experimentally validated using several publicly available datasets, showing a significant speedup with respect to its predecessor and to the other most advanced state-of-the-art algorithms.},
	language = {en},
	booktitle = {Graph-{Based} {Representations} in {Pattern} {Recognition}},
	publisher = {Springer International Publishing},
	author = {Carletti, Vincenzo and Foggia, Pasquale and Saggese, Alessia and Vento, Mario},
	editor = {Foggia, Pasquale and Liu, Cheng-Lin and Vento, Mario},
	year = {2017},
	pages = {128--139},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/A5PUK4YJ/Carletti et al. - 2017 - Introducing VF3 A New Algorithm for Subgraph Isom.pdf:application/pdf},
}

@techreport{hamilton_representation_2017,
	title = {Representation {Learning} on {Graphs}: {Methods} and {Applications}},
	shorttitle = {Representation {Learning} on {Graphs}},
	url = {https://ui.adsabs.harvard.edu/abs/2017arXiv170905584H},
	abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
	urldate = {2023-04-24},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2017},
	doi = {10.48550/arXiv.1709.05584},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2017arXiv170905584H
Type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/5TEL8IXR/Hamilton et al. - 2017 - Representation Learning on Graphs Methods and App.pdf:application/pdf},
}

@inproceedings{bulatov_recurrent_2022,
	title = {Recurrent {Memory} {Transformer}},
	url = {https://openreview.net/forum?id=Uynr3iPhksa},
	abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
	language = {en},
	urldate = {2023-04-27},
	author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MSQ4LHPZ/Bulatov et al. - 2022 - Recurrent Memory Transformer.pdf:application/pdf},
}

@inproceedings{wu_memformer_2022,
	address = {Online only},
	title = {Memformer: {A} {Memory}-{Augmented} {Transformer} for {Sequence} {Modeling}},
	shorttitle = {Memformer},
	url = {https://aclanthology.org/2022.findings-aacl.29},
	abstract = {Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared against the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.},
	urldate = {2023-04-27},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {AACL}-{IJCNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Qingyang and Lan, Zhenzhong and Qian, Kun and Gu, Jing and Geramifard, Alborz and Yu, Zhou},
	month = jan,
	year = {2022},
	pages = {308--318},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/SNBGP555/Wu et al. - 2022 - Memformer A Memory-Augmented Transformer for Sequ.pdf:application/pdf},
}

@inproceedings{hu_strategies_2020,
	title = {Strategies for {Pre}-training {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=HJlWWJSFDH},
	abstract = {Many applications of machine learning require a model to make accurate pre-dictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that naïve strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4\% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.},
	language = {en},
	urldate = {2023-04-27},
	author = {Hu*, Weihua and Liu*, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
	month = mar,
	year = {2020},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/AU6XICIH/Hu et al. - 2020 - Strategies for Pre-training Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{zaheer_big_2020,
	title = {Big {Bird}: {Transformers} for {Longer} {Sequences}},
	volume = {33},
	shorttitle = {Big {Bird}},
	url = {https://papers.nips.cc/paper/2020/hash/c8512d142a2d849725f31a9a7a361ab9-Abstract.html},
	urldate = {2023-05-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	year = {2020},
	pages = {17283--17297},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/LMZZ3RV3/Zaheer et al. - 2020 - Big Bird Transformers for Longer Sequences.pdf:application/pdf;NeurIPS-2020-big-bird-transformers-for-longer-sequences-Supplemental.pdf:/Users/tokio/Zotero/storage/CT9S7IAF/NeurIPS-2020-big-bird-transformers-for-longer-sequences-Supplemental.pdf:application/pdf},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
	urldate = {2023-05-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/QM8KPV25/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@inproceedings{garg_what_2022,
	title = {What {Can} {Transformers} {Learn} {In}-{Context}? {A} {Case} {Study} of {Simple} {Function} {Classes}},
	shorttitle = {What {Can} {Transformers} {Learn} {In}-{Context}?},
	url = {https://openreview.net/forum?id=flNZJ2eOet},
	abstract = {In-context learning is the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To investigate this, we consider the problem of training a model to in-context learn a function class (e.g., linear functions): given data derived from some functions in the class, can we train a model (e.g., a Transformer) to in-context learn most functions from that class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions---that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the Transformer and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes: sparse linear functions where the model outperforms least squares and nearly matches the performance of Lasso, and two-layer neural networks where the model performs comparably to neural networks trained on in-context examples using gradient descent.},
	language = {en},
	urldate = {2023-05-11},
	author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/DFDFS8G7/Garg et al. - 2022 - What Can Transformers Learn In-Context A Case Stu.pdf:application/pdf},
}

@inproceedings{zaheer_deep_2017,
	title = {Deep {Sets}},
	volume = {30},
	url = {https://papers.nips.cc/paper_files/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
	abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	urldate = {2023-05-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	year = {2017},
	file = {deepsets-app.pdf:/Users/tokio/Zotero/storage/WV6AX5EW/deepsets-app.pdf:application/pdf;Full Text PDF:/Users/tokio/Zotero/storage/PFCZ9ID7/Zaheer et al. - 2017 - Deep Sets.pdf:application/pdf},
}

@inproceedings{noci_signal_2022,
	title = {Signal {Propagation} in {Transformers}: {Theoretical} {Perspectives} and the {Role} of {Rank} {Collapse}},
	shorttitle = {Signal {Propagation} in {Transformers}},
	url = {https://openreview.net/forum?id=FxVH7iToXS},
	abstract = {Transformers have achieved remarkable success in several domains, ranging from natural language processing to computer vision. Nevertheless, it has been recently shown that stacking self-attention layers — the distinctive architectural component of Transformers — can result in rank collapse of the tokens’ representations at initialization. The question of if and how rank collapse affects training is still largely unanswered, and its investigation is necessary for a more comprehensive understanding of this architecture. In this work, we shed new light on the causes and the effects of this phenomenon. First, we show that rank collapse of the tokens’ representations hinders training by causing the gradients of the queries and keys to vanish at initialization. Furthermore, we provide a thorough description of the origin of rank collapse and discuss how to prevent it via an appropriate depth-dependent scaling of the residual branches. Finally, our analysis unveils that specific architectural hyperparameters affect the gradients of queries, keys and values differently, leading to disproportionate gradient norms. This suggests an explanation for the widespread use of adaptive methods for Transformers' optimization.},
	language = {en},
	urldate = {2023-05-16},
	author = {Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
	month = oct,
	year = {2022},
	file = {7389_signal_propagation_in_transfor-Supplementary Material.pdf:/Users/tokio/Zotero/storage/9J6N3BQF/7389_signal_propagation_in_transfor-Supplementary Material.pdf:application/pdf;Full Text PDF:/Users/tokio/Zotero/storage/7JLRR23S/Noci et al. - 2022 - Signal Propagation in Transformers Theoretical Pe.pdf:application/pdf},
}

@inproceedings{rahaman_spectral_2019,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v97/rahaman19a.html},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions – i.e. functions that vary globally without local fluctuations – which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {5301--5310},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/N2ISW2NS/Rahaman et al. - 2019 - On the Spectral Bias of Neural Networks.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/BS7WSMMY/Rahaman et al. - 2019 - On the Spectral Bias of Neural Networks.pdf:application/pdf},
}

@misc{gui_g-adapter_2023,
	title = {G-{Adapter}: {Towards} {Structure}-{Aware} {Parameter}-{Efficient} {Transfer} {Learning} for {Graph} {Transformer} {Networks}},
	shorttitle = {G-{Adapter}},
	url = {http://arxiv.org/abs/2305.10329},
	doi = {10.48550/arXiv.2305.10329},
	abstract = {It has become a popular paradigm to transfer the knowledge of large-scale pre-trained models to various downstream tasks via fine-tuning the entire model parameters. However, with the growth of model scale and the rising number of downstream tasks, this paradigm inevitably meets the challenges in terms of computation consumption and memory footprint issues. Recently, Parameter-Efficient Fine-Tuning (PEFT) (e.g., Adapter, LoRA, BitFit) shows a promising paradigm to alleviate these concerns by updating only a portion of parameters. Despite these PEFTs having demonstrated satisfactory performance in natural language processing, it remains under-explored for the question of whether these techniques could be transferred to graph-based tasks with Graph Transformer Networks (GTNs). Therefore, in this paper, we fill this gap by providing extensive benchmarks with traditional PEFTs on a range of graph-based downstream tasks. Our empirical study shows that it is sub-optimal to directly transfer existing PEFTs to graph-based tasks due to the issue of feature distribution shift. To address this issue, we propose a novel structure-aware PEFT approach, named G-Adapter, which leverages graph convolution operation to introduce graph structure (e.g., graph adjacent matrix) as an inductive bias to guide the updating process. Besides, we propose Bregman proximal point optimization to further alleviate feature distribution shift by preventing the model from aggressive update. Extensive experiments demonstrate that G-Adapter obtains the state-of-the-art performance compared to the counterparts on nine graph benchmark datasets based on two pre-trained GTNs, and delivers tremendous memory footprint efficiency compared to the conventional paradigm.},
	urldate = {2023-05-21},
	publisher = {arXiv},
	author = {Gui, Anchun and Ye, Jinqiang and Xiao, Han},
	month = may,
	year = {2023},
	note = {arXiv:2305.10329 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IVNWGSAM/Gui et al. - 2023 - G-Adapter Towards Structure-Aware Parameter-Effic.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WR4CLKFI/2305.html:text/html},
}

@misc{bianchi_expressive_2023,
	title = {The expressive power of pooling in {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2304.01575},
	doi = {10.48550/arXiv.2304.01575},
	abstract = {In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. Considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, while a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically-grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.},
	urldate = {2023-05-21},
	publisher = {arXiv},
	author = {Bianchi, Filippo Maria and Lachi, Veronica},
	month = apr,
	year = {2023},
	note = {arXiv:2304.01575 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/622WKQR8/Bianchi and Lachi - 2023 - The expressive power of pooling in Graph Neural Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NV4RQZLZ/2304.html:text/html},
}

@inproceedings{perez_turing_2018,
	title = {On the {Turing} {Completeness} of {Modern} {Neural} {Network} {Architectures}},
	url = {https://openreview.net/forum?id=HyGBdo0qFm},
	abstract = {Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser \& Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.},
	language = {en},
	urldate = {2023-05-21},
	author = {Pérez, Jorge and Marinković, Javier and Barceló, Pablo},
	month = dec,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/RP4WVUWJ/Pérez et al. - 2018 - On the Turing Completeness of Modern Neural Networ.pdf:application/pdf},
}

@inproceedings{levine_inductive_2022,
	title = {The {Inductive} {Bias} of {In}-{Context} {Learning}: {Rethinking} {Pretraining} {Example} {Design}},
	shorttitle = {The {Inductive} {Bias} of {In}-{Context} {Learning}},
	url = {https://openreview.net/forum?id=lnEaqbTJIRz},
	abstract = {Pretraining Neural Language Models (NLMs) over a large corpus involves chunking the text into training examples, which are contiguous text segments of sizes processable by the neural architecture. We highlight a bias introduced by this common practice: we prove that the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples. This intuitive result has a twofold role. First, it formalizes the motivation behind a broad line of recent successful NLM training heuristics, proposed for the pretraining and fine-tuning stages, which do not necessarily appear related at first glance. Second, our result clearly indicates further improvements to be made in NLM pretraining for the benefit of Natural Language Understanding tasks. As an example, we propose ``kNN-Pretraining": we show that including semantically related non-neighboring sentences in the same pretraining example yields improved sentence representations and open domain question answering abilities. This theoretically motivated degree of freedom for pretraining example design indicates new training schemes for self-improving representations.},
	language = {en},
	urldate = {2023-05-22},
	author = {Levine, Yoav and Wies, Noam and Jannai, Daniel and Navon, Dan and Hoshen, Yedid and Shashua, Amnon},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/24MNIJBE/Levine et al. - 2022 - The Inductive Bias of In-Context Learning Rethink.pdf:application/pdf},
}

@misc{levine_depth--width_2021,
	title = {The {Depth}-to-{Width} {Interplay} in {Self}-{Attention}},
	url = {http://arxiv.org/abs/2006.12467},
	doi = {10.48550/arXiv.2006.12467},
	abstract = {Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.},
	urldate = {2023-05-22},
	publisher = {arXiv},
	author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
	month = jan,
	year = {2021},
	note = {arXiv:2006.12467 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/JJA5TB3U/Levine et al. - 2021 - The Depth-to-Width Interplay in Self-Attention.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/W39T4QPC/2006.html:text/html},
}

@misc{dwivedi_generalization_2021,
	title = {A {Generalization} of {Transformer} {Networks} to {Graphs}},
	url = {http://arxiv.org/abs/2012.09699},
	doi = {10.48550/arXiv.2012.09699},
	abstract = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
	urldate = {2023-05-26},
	publisher = {arXiv},
	author = {Dwivedi, Vijay Prakash and Bresson, Xavier},
	month = jan,
	year = {2021},
	note = {arXiv:2012.09699 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/R93S42I4/Dwivedi and Bresson - 2021 - A Generalization of Transformer Networks to Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UCIUJXDW/2012.html:text/html},
}

@inproceedings{rawat_sampled_2019,
	title = {Sampled {Softmax} with {Random} {Fourier} {Features}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/e43739bba7cdb577e9e3e4e42447f5a5-Abstract.html},
	abstract = {The computational cost of training with softmax cross entropy loss grows linearly with the number of classes. For the settings where a large number of classes are involved, a common method to speed up training is to sample a subset of classes and utilize an estimate of the loss gradient based on these classes, known as the sampled softmax method. However, the sampled softmax provides a biased estimate of the gradient unless the samples are drawn from the exact softmax distribution, which is again expensive to compute. Therefore, a widely employed practical approach involves sampling from a simpler distribution in the hope of approximating the exact softmax distribution. In this paper, we develop the first theoretical understanding of the role that different sampling distributions play in determining the quality of sampled softmax. Motivated by our analysis and the work on kernel-based sampling, we propose the Random Fourier Softmax (RF-softmax) method that utilizes the powerful Random Fourier Features to enable more efficient and accurate sampling from an approximate softmax distribution. We show that RF-softmax leads to low bias in estimation in terms of both the full softmax distribution and the full softmax gradient. Furthermore, the cost of RF-softmax scales only logarithmically with the number of classes.},
	urldate = {2023-06-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rawat, Ankit Singh and Chen, Jiecao and Yu, Felix Xinnan X and Suresh, Ananda Theertha and Kumar, Sanjiv},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/SGEMHG84/Rawat et al. - 2019 - Sampled Softmax with Random Fourier Features.pdf:application/pdf;RF-softmax-camera-ready-supplementary.pdf:/Users/tokio/Zotero/storage/77NCUDPA/RF-softmax-camera-ready-supplementary.pdf:application/pdf},
}

@inproceedings{qin_devil_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {The {Devil} in {Linear} {Transformer}},
	url = {https://aclanthology.org/2022.emnlp-main.473},
	abstract = {Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .},
	urldate = {2023-06-02},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Qin, Zhen and Han, Xiaodong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
	month = feb,
	year = {2022},
	pages = {7025--7041},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Q76Y6249/Qin et al. - 2022 - The Devil in Linear Transformer.pdf:application/pdf},
}

@inproceedings{bello_lambdanetworks_2021,
	title = {{LambdaNetworks}: {Modeling} long-range {Interactions} without {Attention}},
	shorttitle = {{LambdaNetworks}},
	url = {https://openreview.net/forum?id=xTJEN-ggl1b},
	abstract = {We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. In large-scale semi-supervised training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to 86.7\% ImageNet accuracy while being 9.5x faster than EfficientNet NoisyStudent and 9x faster than a Vision Transformer with comparable accuracies.},
	language = {en},
	urldate = {2023-06-02},
	author = {Bello, Irwan},
	month = jan,
	year = {2021},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3R4U53QI/Bello - 2021 - LambdaNetworks Modeling long-range Interactions w.pdf:application/pdf},
}

@inproceedings{mialon_trainable_2021,
	title = {A {Trainable} {Optimal} {Transport} {Embedding} for {Feature} {Aggregation} and its {Relationship} to {Attention}},
	url = {https://openreview.net/forum?id=ZK6vTvb84s},
	abstract = {We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.},
	language = {en},
	urldate = {2023-06-11},
	author = {Mialon, Grégoire and Chen, Dexiong and d'Aspremont, Alexandre and Mairal, Julien},
	month = jan,
	year = {2021},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/A6GPVMVC/Mialon et al. - 2021 - A Trainable Optimal Transport Embedding for Featur.pdf:application/pdf},
}

@inproceedings{cuturi_differentiable_2019,
	title = {Differentiable {Ranking} and {Sorting} using {Optimal} {Transport}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/d8c24ca8f23c562a5600876ca2a550ce-Abstract.html},
	urldate = {2023-06-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/UNT2RZ5B/Cuturi et al. - 2019 - Differentiable Ranking and Sorting using Optimal T.pdf:application/pdf},
}

@inproceedings{qin_cosformer_2022,
	title = {{cosFormer}: {Rethinking} {Softmax} {In} {Attention}},
	shorttitle = {{cosFormer}},
	url = {https://openreview.net/forum?id=Bl8CQrx2Up4},
	abstract = {Transformer has shown great successes in natural language processing, computer vision, and audio processing. As one of its core components, the softmax attention helps to capture long-range dependencies yet prohibits its scale-up due to the quadratic space and time complexity to the sequence length. Kernel methods are often adopted to reduce the complexity by approximating the softmax operator. Nevertheless, due to the approximation errors, their performances vary in different tasks/corpus and suffer crucial performance drops when compared with the vanilla softmax attention. In this paper, we propose a linear transformer called cosFormer that can achieve comparable or better accuracy to the vanilla transformer in both casual and cross attentions. cosFormer is based on two key properties of softmax attention: i). non-negativeness of the attention matrix; ii). a non-linear re-weighting scheme that can concentrate the distribution of the attention matrix. As its linear substitute, cosFormer fulfills these properties with a linear operator and a cosine-based distance re-weighting mechanism. Extensive experiments on language modeling and text understanding tasks demonstrate the effectiveness of our method. We further examine our method on long sequences and achieve state-of-the-art performance on the Long-Range Arena benchmark. The source code is available at https://github.com/OpenNLPLab/cosFormer.},
	language = {en},
	urldate = {2023-06-14},
	author = {Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/YC8HBBJX/Qin et al. - 2022 - cosFormer Rethinking Softmax In Attention.pdf:application/pdf},
}

@misc{gong_vision_2021,
	title = {Vision {Transformers} with {Patch} {Diversification}},
	url = {http://arxiv.org/abs/2104.12753},
	doi = {10.48550/arXiv.2104.12753},
	abstract = {Vision transformer has demonstrated promising performance on challenging computer vision tasks. However, directly training the vision transformers may yield unstable and sub-optimal results. Recent works propose to improve the performance of the vision transformers by modifying the transformer structures, e.g., incorporating convolution layers. In contrast, we investigate an orthogonal approach to stabilize the vision transformer training without modifying the networks. We observe the instability of the training can be attributed to the significant similarity across the extracted patch representations. More specifically, for deep vision transformers, the self-attention blocks tend to map different patches into similar latent representations, yielding information loss and performance degradation. To alleviate this problem, in this work, we introduce novel loss functions in vision transformer training to explicitly encourage diversity across patch representations for more discriminative feature extraction. We empirically show that our proposed techniques stabilize the training and allow us to train wider and deeper vision transformers. We further show the diversified features significantly benefit the downstream tasks in transfer learning. For semantic segmentation, we enhance the state-of-the-art (SOTA) results on Cityscapes and ADE20k. Our code is available at https://github.com/ChengyueGongR/PatchVisionTransformer.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Gong, Chengyue and Wang, Dilin and Li, Meng and Chandra, Vikas and Liu, Qiang},
	month = jun,
	year = {2021},
	note = {arXiv:2104.12753 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EMPJZ9V7/Gong et al. - 2021 - Vision Transformers with Patch Diversification.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ML8DB5N2/2104.html:text/html},
}

@inproceedings{shi_revisiting_2022,
	title = {Revisiting {Over}-smoothing in {BERT} from the {Perspective} of {Graph}},
	url = {https://openreview.net/forum?id=dUV91uaXm3},
	abstract = {Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.},
	language = {en},
	urldate = {2023-06-14},
	author = {Shi, Han and Gao, Jiahui and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng and Lee, Stephen M. S. and Kwok, James},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3G84AANA/Shi et al. - 2022 - Revisiting Over-smoothing in BERT from the Perspec.pdf:application/pdf},
}

@misc{gao_properties_2018,
	title = {On the {Properties} of the {Softmax} {Function} with {Application} in {Game} {Theory} and {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1704.00805},
	doi = {10.48550/arXiv.1704.00805},
	abstract = {In this paper, we utilize results from convex analysis and monotone operator theory to derive additional properties of the softmax function that have not yet been covered in the existing literature. In particular, we show that the softmax function is the monotone gradient map of the log-sum-exp function. By exploiting this connection, we show that the inverse temperature parameter determines the Lipschitz and co-coercivity properties of the softmax function. We then demonstrate the usefulness of these properties through an application in game-theoretic reinforcement learning.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Gao, Bolin and Pavel, Lacra},
	month = aug,
	year = {2018},
	note = {arXiv:1704.00805 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/NIG2YVUX/Gao and Pavel - 2018 - On the Properties of the Softmax Function with App.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/ELY2URUF/1704.html:text/html},
}

@misc{wang_linformer_2020,
	title = {Linformer: {Self}-{Attention} with {Linear} {Complexity}},
	shorttitle = {Linformer},
	url = {http://arxiv.org/abs/2006.04768},
	doi = {10.48550/arXiv.2006.04768},
	abstract = {Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses \$O(n{\textasciicircum}2)\$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from \$O(n{\textasciicircum}2)\$ to \$O(n)\$ in both time and space. The resulting linear transformer, the {\textbackslash}textit\{Linformer\}, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
	urldate = {2023-06-14},
	publisher = {arXiv},
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	month = jun,
	year = {2020},
	note = {arXiv:2006.04768 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/IDWDKPL8/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/BBTR38JE/2006.html:text/html},
}

@article{yang_tackling_2023,
	title = {Tackling {Over}-{Smoothing} in {Graph} {Convolutional} {Networks} {With} {EM}-{Based} {Joint} {Topology} {Optimization} and {Node} {Classification}},
	volume = {9},
	issn = {2373-776X},
	doi = {10.1109/TSIPN.2023.3244112},
	abstract = {Over-smoothing has emerged as a severe obstacle to node classification with message passing based graph convolutional networks (GCNs). Classification performance dramatically deteriorates for deep GCNs, as message passing over the observed noisy graph topology cannot adequately propagate intra-class information and over-mix the features of nodes from different communities (classes). Existing topology optimization methods for GCNs cannot sufficiently exploit the underlying ground-truth community structure to distinguish nodes from different communities. In this paper, we propose a novel method, termed EM-GCN, to address this issue by employing the Expectation Maximization (EM) algorithm to simultaneously achieve community-enhanced topology optimization and learn desirable node representations for classification. EM-GCN represents the underlying community structure with a latent adjacency matrix parameterized by an assortative-constrained stochastic block model, and consequently, explicitly enhances intra-class connection and suppresses inter-class interaction in the observed noisy graph. In the inference procedure (E-step), a graph inference model shared across all the node pairs is learned from node embeddings to approximate the posterior distribution of the latent adjacency matrix and optimize the graph topology. In the learning procedure (M-step), node representations are learned using GCNs based on the refined graph topology for the downstream classification task. EM-GCN is a general and flexible method that leverages approximate posterior and arbitrary GCNs for overcoming over-smoothing with topology optimization. Experimental results on synthetic and real-world datasets demonstrate that EM-GCN outperforms existing strategies for tackling over-smoothing and optimizing graph topology in node classification.},
	journal = {IEEE Transactions on Signal and Information Processing over Networks},
	author = {Yang, Rui and Dai, Wenrui and Li, Chenglin and Zou, Junni and Xiong, Hongkai},
	year = {2023},
	note = {Conference Name: IEEE Transactions on Signal and Information Processing over Networks},
	keywords = {expectation maximization, Graph convolutional networks, Information processing, Message passing, Network topology, node classification, Noise measurement, Optimization, over-smoothing, Stochastic processes, Topology, topology optimization},
	pages = {123--139},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/92PWII23/10048549.html:text/html;IEEE Xplore Full Text PDF:/Users/tokio/Zotero/storage/49QZ8VZT/Yang et al. - 2023 - Tackling Over-Smoothing in Graph Convolutional Net.pdf:application/pdf},
}

@inproceedings{wu_non-asymptotic_2023,
	title = {A {Non}-{Asymptotic} {Analysis} of {Oversmoothing} in {Graph} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=CJd-BtnwtXq},
	abstract = {Oversmoothing is a central challenge of building more powerful Graph Neural Networks (GNNs). While previous works have only demonstrated that oversmoothing is inevitable when the number of graph convolutions tends to infinity, in this paper, we precisely characterize the mechanism behind the phenomenon via a non-asymptotic analysis. Specifically, we distinguish between two different effects when applying graph convolutions—an undesirable mixing effect that homogenizes node representations in different classes, and a desirable denoising effect that homogenizes node representations in the same class. By quantifying these two effects on random graphs sampled from the Contextual Stochastic Block Model (CSBM), we show that oversmoothing happens once the mixing effect starts to dominate the denoising effect, and the number of layers required for this transition is \$O({\textbackslash}log N/{\textbackslash}log ({\textbackslash}log N))\$ for sufficiently dense graphs with \$N\$ nodes. We also extend our analysis to study the effects of Personalized PageRank (PPR), or equivalently, the effects of initial residual connections on oversmoothing. Our results suggest that while PPR mitigates oversmoothing at deeper layers, PPR-based architectures still achieve their best performance at a shallow depth and are outperformed by the graph convolution approach on certain graphs. Finally, we support our theoretical results with numerical experiments, which further suggest that the oversmoothing phenomenon observed in practice can be magnified by the difficulty of optimizing deep GNN models.},
	language = {en},
	urldate = {2023-06-15},
	author = {Wu, Xinyi and Chen, Zhengdao and Wang, William Wei and Jadbabaie, Ali},
	month = feb,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/SZPD8ZF4/Wu et al. - 2023 - A Non-Asymptotic Analysis of Oversmoothing in Grap.pdf:application/pdf},
}

@inproceedings{prillo_softsort_2020,
	title = {{SoftSort}: {A} {Continuous} {Relaxation} for the argsort {Operator}},
	shorttitle = {{SoftSort}},
	url = {https://proceedings.mlr.press/v119/prillo20a.html},
	abstract = {While sorting is an important procedure in computer science, the argsort operator - which takes as input a vector and returns its sorting permutation - has a discrete image and thus zero gradients almost everywhere. This prohibits end-to-end, gradient-based learning of models that rely on the argsort operator. A natural way to overcome this problem is to replace the argsort operator with a continuous relaxation. Recent work has shown a number of ways to do this, but the relaxations proposed so far are computationally complex. In this work we propose a simple continuous relaxation for the argsort operator which has the following qualities: it can be implemented in three lines of code, achieves state-of-the-art performance, is easy to reason about mathematically - substantially simplifying proofs - and is faster than competing approaches. We open source the code to reproduce all of the experiments and results.},
	language = {en},
	urldate = {2023-06-15},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Prillo, Sebastian and Eisenschlos, Julian},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {7793--7802},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/XQ6HMLDH/Prillo and Eisenschlos - 2020 - SoftSort A Continuous Relaxation for the argsort .pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/7XY29SYL/Prillo and Eisenschlos - 2020 - SoftSort A Continuous Relaxation for the argsort .pdf:application/pdf},
}

@article{wagstaff_universal_2022,
	title = {Universal {Approximation} of {Functions} on {Sets}},
	volume = {23},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v23/21-0730.html},
	abstract = {Modelling functions of sets, or equivalently, permutation-invariant functions, is a long-standing challenge in machine learning. Deep Sets is a popular method which is known to be a universal approximator for continuous set functions. We provide a theoretical analysis of Deep Sets which shows that this universal approximation property is only guaranteed if the model's latent space is sufficiently high-dimensional. If the latent space is even one dimension lower than necessary, there exist piecewise-affine functions for which Deep Sets performs no better than a naïve constant baseline, as judged by worst-case error. Deep Sets may be viewed as the most efficient incarnation of the Janossy pooling paradigm. We identify this paradigm as encompassing most currently popular set-learning methods. Based on this connection, we discuss the implications of our results for set learning more broadly, and identify some open questions on the universality of Janossy pooling in general.},
	number = {151},
	urldate = {2023-06-22},
	journal = {Journal of Machine Learning Research},
	author = {Wagstaff, Edward and Fuchs, Fabian B. and Engelcke, Martin and Osborne, Michael A. and Posner, Ingmar},
	year = {2022},
	pages = {1--56},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Y6BLDM5U/Wagstaff et al. - 2022 - Universal Approximation of Functions on Sets.pdf:application/pdf},
}

@inproceedings{petersen_monotonic_2022,
	title = {Monotonic {Differentiable} {Sorting} {Networks}},
	url = {https://openreview.net/forum?id=IcUWShptD7d},
	abstract = {Differentiable sorting algorithms allow training with sorting and ranking supervision, where only the ordering or ranking of samples is known. Various methods have been proposed to address this challenge, ranging from optimal transport-based differentiable Sinkhorn sorting algorithms to making classic sorting networks differentiable. One problem of current differentiable sorting methods is that they are non-monotonic. To address this issue, we propose a novel relaxation of conditional swap operations that guarantees monotonicity in differentiable sorting networks. We introduce a family of sigmoid functions and prove that they produce differentiable sorting networks that are monotonic. Monotonicity ensures that the gradients always have the correct sign, which is an advantage in gradient-based optimization. We demonstrate that monotonic differentiable sorting networks improve upon previous differentiable sorting methods.},
	language = {en},
	urldate = {2023-06-22},
	author = {Petersen, Felix and Borgelt, Christian and Kuehne, Hilde and Deussen, Oliver},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/S6WDZU44/Petersen et al. - 2022 - Monotonic Differentiable Sorting Networks.pdf:application/pdf},
}

@inproceedings{asadi_alternative_2017,
	title = {An {Alternative} {Softmax} {Operator} for {Reinforcement} {Learning}},
	url = {https://proceedings.mlr.press/v70/asadi17a.html},
	abstract = {A softmax operator applied to a set of values acts somewhat like the maximization function and somewhat like an average. In sequential decision making, softmax is often used in settings where it is necessary to maximize utility but also to hedge against problems that arise from putting all of one’s weight behind a single maximum utility decision. The Boltzmann softmax operator is the most commonly used softmax operator in this setting, but we show that this operator is prone to misbehavior. In this work, we study a differentiable softmax operator that, among other properties, is a non-expansion ensuring a convergent behavior in learning and planning. We introduce a variant of SARSA algorithm that, by utilizing the new operator, computes a Boltzmann policy with a state-dependent temperature parameter. We show that the algorithm is convergent and that it performs favorably in practice.},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Asadi, Kavosh and Littman, Michael L.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {243--252},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/F3IRRDP2/Asadi and Littman - 2017 - An Alternative Softmax Operator for Reinforcement .pdf:application/pdf},
}

@misc{cai_note_2020,
	title = {A {Note} on {Over}-{Smoothing} for {Graph} {Neural} {Networks}},
	url = {https://arxiv.org/abs/2006.13318v1},
	abstract = {Graph Neural Networks (GNNs) have achieved a lot of success on graph-structured data. However, it is observed that the performance of graph neural networks does not improve as the number of layers increases. This effect, known as over-smoothing, has been analyzed mostly in linear cases. In this paper, we build upon previous results {\textbackslash}cite\{oono2019graph\} to further analyze the over-smoothing effect in the general graph neural network architecture. We show when the weight matrix satisfies the conditions determined by the spectrum of augmented normalized Laplacian, the Dirichlet energy of embeddings will converge to zero, resulting in the loss of discriminative power. Using Dirichlet energy to measure "expressiveness" of embedding is conceptually clean; it leads to simpler proofs than {\textbackslash}cite\{oono2019graph\} and can handle more non-linearities.},
	language = {en},
	urldate = {2023-06-22},
	journal = {arXiv.org},
	author = {Cai, Chen and Wang, Yusu},
	month = jun,
	year = {2020},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ZTC2W7W9/Cai and Wang - 2020 - A Note on Over-Smoothing for Graph Neural Networks.pdf:application/pdf},
}

@inproceedings{xie_explanation_2021,
	title = {An {Explanation} of {In}-context {Learning} as {Implicit} {Bayesian} {Inference}},
	url = {https://openreview.net/forum?id=RdJVFCHjUMI},
	abstract = {Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.},
	language = {en},
	urldate = {2023-06-26},
	author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/6YT8INPT/Xie et al. - 2021 - An Explanation of In-context Learning as Implicit .pdf:application/pdf},
}

@misc{likhosherstov_expressive_2021,
	title = {On the {Expressive} {Power} of {Self}-{Attention} {Matrices}},
	url = {http://arxiv.org/abs/2106.03764},
	doi = {10.48550/arXiv.2106.03764},
	abstract = {Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. In practice, this matrix typically exhibits two properties: (1) it is sparse, meaning that each token only attends to a small subset of other tokens; and (2) it changes dynamically depending on the input to the module. With these considerations in mind, we ask the following question: Can a fixed self-attention module approximate arbitrary sparse patterns depending on the input? How small is the hidden size \$d\$ required for such approximation? We make progress in answering this question and show that the self-attention matrix can provably approximate sparse matrices, where sparsity is in terms of a bounded number of nonzero elements in each row and column. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. Our proof is constructive, enabling us to propose an algorithm for finding adaptive inputs and fixed self-attention parameters in order to approximate a given matrix. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, \$d\$ grows only logarithmically with the sequence length \$L\$ (i.e. \$d = O({\textbackslash}log L)\$).},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Likhosherstov, Valerii and Choromanski, Krzysztof and Weller, Adrian},
	month = jun,
	year = {2021},
	note = {arXiv:2106.03764 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/33J6I57E/Likhosherstov et al. - 2021 - On the Expressive Power of Self-Attention Matrices.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UFD9USVM/2106.html:text/html},
}

@misc{vuckovic_mathematical_2020,
	title = {A {Mathematical} {Theory} of {Attention}},
	url = {http://arxiv.org/abs/2007.02876},
	doi = {10.48550/arXiv.2007.02876},
	abstract = {Attention is a powerful component of modern neural networks across a wide variety of domains. However, despite its ubiquity in machine learning, there is a gap in our understanding of attention from a theoretical point of view. We propose a framework to fill this gap by building a mathematically equivalent model of attention using measure theory. With this model, we are able to interpret self-attention as a system of self-interacting particles, we shed light on self-attention from a maximum entropy perspective, and we show that attention is actually Lipschitz-continuous (with an appropriate metric) under suitable assumptions. We then apply these insights to the problem of mis-specified input data; infinitely-deep, weight-sharing self-attention networks; and more general Lipschitz estimates for a specific type of attention studied in concurrent work.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Vuckovic, James and Baratin, Aristide and Combes, Remi Tachet des},
	month = jul,
	year = {2020},
	note = {arXiv:2007.02876 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/2KQQIWTK/Vuckovic et al. - 2020 - A Mathematical Theory of Attention.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/UIPXKX4G/2007.html:text/html},
}

@inproceedings{bhojanapalli_low-rank_2020,
	title = {Low-{Rank} {Bottleneck} in {Multi}-head {Attention} {Models}},
	url = {https://proceedings.mlr.press/v119/bhojanapalli20a.html},
	abstract = {Attention based Transformer architecture has enabled significant advances in the field of natural language processing. In addition to new pre-training techniques, recent improvements crucially rely on working with a relatively larger embedding dimension for tokens. Unfortunately, this leads to models that are prohibitively large to be employed in the downstream tasks. In this paper we identify one of the important factors contributing to the large embedding size requirement. In particular, our analysis highlights that the scaling between the number of heads and the size of each head in the current architecture gives rise to a low-rank bottleneck in attention heads, causing this limitation. We further validate this in our experiments. As a solution we propose to set the head size of an attention unit to input sequence length, and independent of the number of heads, resulting in multi-head attention layers with provably more expressive power. We empirically show that this allows us to train models with a relatively smaller embedding dimension and with better performance scaling.},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {864--873},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Q973MY7F/Bhojanapalli et al. - 2020 - Low-Rank Bottleneck in Multi-head Attention Models.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/WVHRZPPT/Bhojanapalli et al. - 2020 - Low-Rank Bottleneck in Multi-head Attention Models.pdf:application/pdf},
}

@inproceedings{dehghani_universal_2018,
	title = {Universal {Transformers}},
	url = {https://openreview.net/forum?id=HyzdRiR9Y7},
	abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
	language = {en},
	urldate = {2023-06-27},
	author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/XY3JQIRJ/Dehghani et al. - 2018 - Universal Transformers.pdf:application/pdf},
}

@inproceedings{kim_lipschitz_2021,
	title = {The {Lipschitz} {Constant} of {Self}-{Attention}},
	url = {https://proceedings.mlr.press/v139/kim21i.html},
	abstract = {Lipschitz constants of neural networks have been explored in various contexts in deep learning, such as provable adversarial robustness, estimating Wasserstein distance, stabilising training of GANs, and formulating invertible neural networks. Such works have focused on bounding the Lipschitz constant of fully connected or convolutional networks, composed of linear maps and pointwise non-linearities. In this paper, we investigate the Lipschitz constant of self-attention, a non-linear neural network module widely used in sequence modelling. We prove that the standard dot-product self-attention is not Lipschitz for unbounded input domain, and propose an alternative L2 self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant of L2 self-attention and provide empirical evidence for its asymptotic tightness. To demonstrate the practical relevance of our theoretical work, we formulate invertible self-attention and use it in a Transformer-based architecture for a character-level language modelling task.},
	language = {en},
	urldate = {2023-06-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kim, Hyunjik and Papamakarios, George and Mnih, Andriy},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {5562--5571},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/S2UGQERZ/Kim et al. - 2021 - The Lipschitz Constant of Self-Attention.pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/TGUKGT3H/Kim et al. - 2021 - The Lipschitz Constant of Self-Attention.pdf:application/pdf},
}

@misc{mahdavi_memorization_2023,
	title = {Memorization {Capacity} of {Multi}-{Head} {Attention} in {Transformers}},
	url = {http://arxiv.org/abs/2306.02010},
	doi = {10.48550/arXiv.2306.02010},
	abstract = {In this paper, we investigate the memorization capabilities of multi-head attention in Transformers, motivated by the central role attention plays in these models. Under a mild linear independence assumption on the input data, we present a theoretical analysis demonstrating that an \$H\$-head attention layer with a context size \$n\$, dimension \$d\$, and \$O(Hd{\textasciicircum}2)\$ parameters can memorize \$O(Hn)\$ examples. We conduct experiments that verify our assumptions on the image classification task using Vision Transformer. To validate our theoretical findings, we perform synthetic experiments and show a linear relationship between memorization capacity and the number of attention heads.},
	urldate = {2023-06-27},
	publisher = {arXiv},
	author = {Mahdavi, Sadegh and Liao, Renjie and Thrampoulidis, Christos},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02010 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/UGII57RC/Mahdavi et al. - 2023 - Memorization Capacity of Multi-Head Attention in T.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/FMWIGR53/2306.html:text/html},
}

@article{oswald_transformers_2023,
	title = {Transformers {Learn} {In}-{Context} by {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=tHvXrFQma5},
	abstract = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers.},
	language = {en},
	urldate = {2023-07-03},
	author = {Oswald, Johannes Von and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Joao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
	month = jun,
	year = {2023},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MNTNSTKI/Oswald et al. - 2023 - Transformers Learn In-Context by Gradient Descent.pdf:application/pdf},
}

@inproceedings{irie_dual_2022,
	title = {The {Dual} {Form} of {Neural} {Networks} {Revisited}: {Connecting} {Test} {Time} {Predictions} to {Training} {Patterns} via {Spotlights} of {Attention}},
	shorttitle = {The {Dual} {Form} of {Neural} {Networks} {Revisited}},
	url = {https://proceedings.mlr.press/v162/irie22a.html},
	abstract = {Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.},
	language = {en},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {9639--9659},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/ABMQBYPQ/Irie et al. - 2022 - The Dual Form of Neural Networks Revisited Connec.pdf:application/pdf},
}

@misc{pires_multiclass_2016,
	title = {Multiclass {Classification} {Calibration} {Functions}},
	url = {http://arxiv.org/abs/1609.06385},
	abstract = {In this paper we refine the process of computing calibration functions for a number of multiclass classification surrogate losses. Calibration functions are a powerful tool for easily converting bounds for the surrogate risk (which can be computed through well-known methods) into bounds for the true risk, the probability of making a mistake. They are particularly suitable in non-parametric settings, where the approximation error can be controlled, and provide tighter bounds than the common technique of upper-bounding the 0-1 loss by the surrogate loss. The abstract nature of the more sophisticated existing calibration function results requires calibration functions to be explicitly derived on a case-by-case basis, requiring repeated efforts whenever bounds for a new surrogate loss are required. We devise a streamlined analysis that simplifies the process of deriving calibration functions for a large number of surrogate losses that have been proposed in the literature. The effort of deriving calibration functions is then surmised in verifying, for a chosen surrogate loss, a small number of conditions that we introduce. As case studies, we recover existing calibration functions for the well-known loss of Lee et al. (2004), and also provide novel calibration functions for well-known losses, including the one-versus-all loss and the logistic regression loss, plus a number of other losses that have been shown to be classification-calibrated in the past, but for which no calibration function had been derived.},
	urldate = {2023-07-03},
	publisher = {arXiv},
	author = {Pires, Bernardo Ávila and Szepesvári, Csaba},
	month = sep,
	year = {2016},
	note = {arXiv:1609.06385 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/tokio/Zotero/storage/NA7Y567M/1609.html:text/html;Full Text PDF:/Users/tokio/Zotero/storage/VATBZ9M7/Pires and Szepesvári - 2016 - Multiclass Classification Calibration Functions.pdf:application/pdf},
}

@inproceedings{park_provable_2021,
	title = {Provable {Memorization} via {Deep} {Neural} {Networks} using {Sub}-linear {Parameters}},
	url = {https://proceedings.mlr.press/v134/park21a.html},
	abstract = {It is known that 𝑂(𝑁)O(N)O(N) parameters are sufficient for neural networks to memorize arbitrary 𝑁NN input-label pairs. By exploiting depth, we show that 𝑂(𝑁2/3)O(N2/3)O(N{\textasciicircum}\{2/3\}) parameters suffice to memorize 𝑁NN pairs, under a mild condition on the separation of input points. In particular, deeper networks (even with width 3) are shown to memorize more pairs than shallow networks, which also agrees with the recent line of works on the benefits of depth for function approximation. We also provide empirical results that support our theoretical findings.},
	language = {en},
	urldate = {2023-07-06},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Park, Sejun and Lee, Jaeho and Yun, Chulhee and Shin, Jinwoo},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3627--3661},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/RDS3FSF7/Park et al. - 2021 - Provable Memorization via Deep Neural Networks usi.pdf:application/pdf},
}

@inproceedings{brandstetter_message_2021,
	title = {Message {Passing} {Neural} {PDE} {Solvers}},
	url = {https://openreview.net/forum?id=vSix3HPYKSU},
	abstract = {The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, discretization, etc. in 1D and 2D. Our model outperforms state-of-the-art numerical solvers in the low resolution regime in terms of speed, and accuracy.},
	language = {en},
	urldate = {2023-07-07},
	author = {Brandstetter, Johannes and Worrall, Daniel E. and Welling, Max},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Y6DADP5E/Brandstetter et al. - 2021 - Message Passing Neural PDE Solvers.pdf:application/pdf},
}

@inproceedings{hardt_identity_2016,
	title = {Identity {Matters} in {Deep} {Learning}},
	url = {https://openreview.net/forum?id=ryxB0Rtxx},
	abstract = {An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as batch normalization, but was also key to the immense success of residual networks. In this work, we put the principle of identity parameterization on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size. Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.},
	language = {en},
	urldate = {2023-07-11},
	author = {Hardt, Moritz and Ma, Tengyu},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/8Z5TMKIJ/Hardt and Ma - 2016 - Identity Matters in Deep Learning.pdf:application/pdf},
}

@article{baum_capabilities_1988,
	title = {On the capabilities of multilayer perceptrons},
	volume = {4},
	issn = {0885-064X},
	url = {https://www.sciencedirect.com/science/article/pii/0885064X88900209},
	doi = {10.1016/0885-064X(88)90020-9},
	abstract = {What is the smallest multilayer perceptron able to compute arbitrary and random functions? Previous results show that a net with one hidden layer containing N − 1 threshold units is capable of implementing an arbitrary dichotomy of N points. A construction is presented here for implementing an arbitrary dichotomy with one hidden layer containing [Nd] units, for any set of N points in general position in d dimensions. This is in fact the smallest such net as dichotomies which cannot be implemented by any net with fewer units are described. Several constructions are presented of one-hidden-layer nets implementing arbitrary functions into the e-dimensional hypercube. One of these has only [4Nd][e[log2(Nd)]] units in its hidden layer. Arguments based on a function counting theorem of Cover establish that any net implementing arbitrary functions must have at least Nelog2(N) weights, so that no net with one hidden layer containing less than Ne/(d log2(N)) units will suffice. Simple counts also show that if the weights are only allowed to assume one of ng possible values, no net with fewer than Nelog2(ng) weights will suffice. Thus the gain coming from using real valued synapses appears to be only logarithmic. The circuit implementing functions into the e hypercube realizes such logarithmic gains. Since the counting arguments limit below only the number of weights, the possibility is suggested that, if suitable restrictions are imposed on the input vector set to avoid topological obstructions, two-hidden-layer nets with O(N) weights but only O(√N) threshold units might suffice for arbitrary dichotomies. Interesting and potentially sufficient restrictions include (a) if the vectors are binary, i.e., lie on the d hypercube or (b) if they are randomly and uniformly selected from a bounded region.},
	language = {en},
	number = {3},
	urldate = {2023-07-11},
	journal = {Journal of Complexity},
	author = {Baum, Eric B},
	month = sep,
	year = {1988},
	pages = {193--215},
	file = {ScienceDirect Full Text PDF:/Users/tokio/Zotero/storage/83X2WEQR/Baum - 1988 - On the capabilities of multilayer perceptrons.pdf:application/pdf;ScienceDirect Snapshot:/Users/tokio/Zotero/storage/9HQ2RBNG/0885064X88900209.html:text/html},
}

@inproceedings{zhang_understanding_2016,
	title = {Understanding deep learning requires rethinking generalization},
	url = {https://openreview.net/forum?id=Sy8gdB9xx},
	abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
	language = {en},
	urldate = {2023-07-11},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/U6S2276V/Zhang et al. - 2016 - Understanding deep learning requires rethinking ge.pdf:application/pdf},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	language = {en},
	number = {4},
	urldate = {2023-07-11},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {Neural networks, Approximation, Completeness},
	pages = {303--314},
	file = {報告したバージョン:/Users/tokio/Zotero/storage/85TCR6CJ/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf:application/pdf},
}

@inproceedings{carroll_construction_1989,
	title = {Construction of neural nets using the radon transform},
	doi = {10.1109/IJCNN.1989.118639},
	abstract = {The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.{\textless}{\textgreater}},
	booktitle = {International 1989 {Joint} {Conference} on {Neural} {Networks}},
	author = {{Carroll} and {Dickinson}},
	year = {1989},
	keywords = {Neural networks, Approximation methods},
	pages = {607--611 vol.1},
	file = {IEEE Xplore Abstract Record:/Users/tokio/Zotero/storage/AMDW4Z8L/118639.html:text/html},
}

@inproceedings{lu_expressive_2017,
	title = {The {Expressive} {Power} of {Neural} {Networks}: {A} {View} from the {Width}},
	volume = {30},
	shorttitle = {The {Expressive} {Power} of {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html},
	abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
	urldate = {2023-07-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
	year = {2017},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/DY49BR4A/Lu et al. - 2017 - The Expressive Power of Neural Networks A View fr.pdf:application/pdf},
}

@inproceedings{lin_resnet_2018,
	title = {{ResNet} with one-neuron hidden layers is a {Universal} {Approximator}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/hash/03bfc1d4783966c69cc6aef8247e0103-Abstract.html},
	abstract = {We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. {\textbackslash}ell\_1(R{\textasciicircum}d). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21,11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.},
	urldate = {2023-07-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lin, Hongzhou and Jegelka, Stefanie},
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/VGNALPP3/Lin and Jegelka - 2018 - ResNet with one-neuron hidden layers is a Universa.pdf:application/pdf},
}

@inproceedings{bubeck_network_2020,
	title = {Network size and size of the weights in memorization with two-layers neural networks},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/34609bdc08a07ace4e1526bbb1777673-Abstract.html},
	urldate = {2023-07-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bubeck, Sebastien and Eldan, Ronen and Lee, Yin Tat and Mikulincer, Dan},
	year = {2020},
	pages = {4977--4986},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/C9P5QLMU/Bubeck et al. - 2020 - Network size and size of the weights in memorizati.pdf:application/pdf},
}

@inproceedings{rajput_exponential_2021,
	title = {An {Exponential} {Improvement} on the {Memorization} {Capacity} of {Deep} {Threshold} {Networks}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/69dd2eff9b6a421d5ce262b093bdab23-Abstract.html},
	urldate = {2023-07-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Rajput, Shashank and Sreenivasan, Kartik and Papailiopoulos, Dimitris and Karbasi, Amin},
	year = {2021},
	pages = {12674--12685},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/KRYPA6CK/Rajput et al. - 2021 - An Exponential Improvement on the Memorization Cap.pdf:application/pdf},
}

@inproceedings{nakkiran_deep_2019,
	title = {Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data} {Hurt}},
	shorttitle = {Deep {Double} {Descent}},
	url = {https://openreview.net/forum?id=B1g5sA4twr},
	abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity, and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
	language = {en},
	urldate = {2023-07-11},
	author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
	month = sep,
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/MQIQR6I5/Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/ES4ALE9P/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/CDDPZ7MF/1810.html:text/html},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/T3998WZ5/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/6W5ZTXTX/1907.html:text/html},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2023-07-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/VS3XPWXH/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/Users/tokio/Zotero/storage/RJYVXQDZ/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@article{radford_improving_nodate,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/Users/tokio/Zotero/storage/JHQ4EWS8/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@misc{lialin_stack_2023,
	title = {Stack {More} {Layers} {Differently}: {High}-{Rank} {Training} {Through} {Low}-{Rank} {Updates}},
	shorttitle = {Stack {More} {Layers} {Differently}},
	url = {http://arxiv.org/abs/2307.05695},
	doi = {10.48550/arXiv.2307.05695},
	abstract = {Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.},
	urldate = {2023-07-21},
	publisher = {arXiv},
	author = {Lialin, Vladislav and Shivagunde, Namrata and Muckatira, Sherin and Rumshisky, Anna},
	month = jul,
	year = {2023},
	note = {arXiv:2307.05695 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/VY2B3FAC/Lialin et al. - 2023 - Stack More Layers Differently High-Rank Training .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/79A8J6KF/2307.html:text/html},
}

@inproceedings{aghajanyan_intrinsic_2021,
	address = {Online},
	title = {Intrinsic {Dimensionality} {Explains} the {Effectiveness} of {Language} {Model} {Fine}-{Tuning}},
	url = {https://aclanthology.org/2021.acl-long.568},
	doi = {10.18653/v1/2021.acl-long.568},
	abstract = {Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90\% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.},
	urldate = {2023-07-25},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Aghajanyan, Armen and Gupta, Sonal and Zettlemoyer, Luke},
	month = aug,
	year = {2021},
	pages = {7319--7328},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/BW74XPZJ/Aghajanyan et al. - 2021 - Intrinsic Dimensionality Explains the Effectivenes.pdf:application/pdf},
}

@inproceedings{clark_what_2019,
	address = {Florence, Italy},
	title = {What {Does} {BERT} {Look} at? {An} {Analysis} of {BERT}'s {Attention}},
	shorttitle = {What {Does} {BERT} {Look} at?},
	url = {https://aclanthology.org/W19-4828},
	doi = {10.18653/v1/W19-4828},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
	urldate = {2023-07-25},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = aug,
	year = {2019},
	pages = {276--286},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/K6UFLMZI/Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT's Atte.pdf:application/pdf},
}

@article{kratsios_small_2023,
	title = {Small {Transformers} {Compute} {Universal} {Metric} {Embeddings}},
	volume = {24},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v24/22-1246.html},
	abstract = {We study representations of data from an arbitrary metric space 
X
 in the space of univariate Gaussian mixtures equipped with a transport metric (Delon and Desolneux 2020). We prove embedding guarantees for feature maps implemented by small neural networks called probabilistic transformers. Our guarantees are of memorization type: we prove that a probabilistic transformer of depth about nlog(n)
n
log
⁡
(
n
)
 and width about n2
n
2
 can bi-H{\textbackslash}"older embed any n
n
-point dataset from 
X
 with low metric distortion, thus avoiding the curse of dimensionality. We further derive probabilistic bi-Lipschitz guarantees, which trade off the amount of distortion and the probability that a randomly chosen pair of points embeds with that distortion. If the geometry of 
X
 is sufficiently regular, we obtain stronger bi-Lipschitz guarantees for all points. As applications, we derive neural embedding guarantees for datasets from Riemannian manifolds, metric trees, and certain types of combinatorial graphs. When instead embedding into multivariate Gaussian mixtures, we show that probabilistic transformers compute bi-Hölder embeddings with arbitrarily small distortion. Our results show that any finite metric dataset, from vertices on a graph to functions a function space, can be faithfully represented in a single representation space, and that the representation can be implemented by a simple transformer architecture. Thus one may only need a modular set of machine learning tools compatible with this one representation space, many of which already exist, for downstream supervised and unsupervised learning from a great variety of data types.},
	number = {170},
	urldate = {2023-07-31},
	journal = {Journal of Machine Learning Research},
	author = {Kratsios, Anastasis and Debarnot, Valentin and Dokmanić, Ivan},
	year = {2023},
	pages = {1--48},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/QLJ7KA5R/Kratsios et al. - 2023 - Small Transformers Compute Universal Metric Embedd.pdf:application/pdf;Source Code:/Users/tokio/Zotero/storage/GRVG6MR4/Universal-Embeddings.html:text/html},
}

@inproceedings{mireshghallah_empirical_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {An {Empirical} {Analysis} of {Memorization} in {Fine}-tuned {Autoregressive} {Language} {Models}},
	url = {https://aclanthology.org/2022.emnlp-main.119},
	abstract = {Large language models are shown to present privacy risks through memorization of training data, andseveral recent works have studied such risks for the pre-training phase. Little attention, however, has been given to the fine-tuning phase and it is not well understood how different fine-tuning methods (such as fine-tuning the full model, the model head, and adapter) compare in terms of memorization risk. This presents increasing concern as the “pre-train and fine-tune” paradigm proliferates. In this paper, we empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that fine-tuning the head of the model has the highest susceptibility to attacks, whereas fine-tuning smaller adapters appears to be less vulnerable to known extraction attacks.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Mireshghallah, Fatemehsadat and Uniyal, Archit and Wang, Tianhao and Evans, David and Berg-Kirkpatrick, Taylor},
	month = feb,
	year = {2022},
	pages = {1816--1826},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/9DC8G3PM/Mireshghallah et al. - 2022 - An Empirical Analysis of Memorization in Fine-tune.pdf:application/pdf},
}

@article{wei_statistically_2022,
	title = {Statistically {Meaningful} {Approximation}: a {Case} {Study} on {Approximating} {Turing} {Machines} with {Transformers}},
	volume = {35},
	shorttitle = {Statistically {Meaningful} {Approximation}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/4ebf1d74f53ece08512a23309d58df89-Abstract-Conference.html},
	language = {en},
	urldate = {2023-07-31},
	journal = {Advances in Neural Information Processing Systems},
	author = {Wei, Colin and Chen, Yining and Ma, Tengyu},
	month = dec,
	year = {2022},
	pages = {12071--12083},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3LIRICRN/Wei et al. - 2022 - Statistically Meaningful Approximation a Case Stu.pdf:application/pdf},
}

@article{hahn_theoretical_2020,
	title = {Theoretical {Limitations} of {Self}-{Attention} in {Neural} {Sequence} {Models}},
	volume = {8},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00306},
	doi = {10.1162/tacl_a_00306},
	abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
	urldate = {2023-07-31},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Hahn, Michael},
	month = jan,
	year = {2020},
	pages = {156--171},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/2285TNH8/Hahn - 2020 - Theoretical Limitations of Self-Attention in Neura.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/DGMINYLP/Theoretical-Limitations-of-Self-Attention-in.html:text/html},
}

@article{merrill_saturated_2022,
	title = {Saturated {Transformers} are {Constant}-{Depth} {Threshold} {Circuits}},
	volume = {10},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00493},
	doi = {10.1162/tacl_a_00493},
	abstract = {Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize.},
	urldate = {2023-07-31},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Merrill, William and Sabharwal, Ashish and Smith, Noah A.},
	month = aug,
	year = {2022},
	pages = {843--856},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/YV7ETTBR/Merrill et al. - 2022 - Saturated Transformers are Constant-Depth Threshol.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/JVAP5RTZ/Saturated-Transformers-are-Constant-Depth.html:text/html},
}

@article{hao_formal_2022,
	title = {Formal {Language} {Recognition} by {Hard} {Attention} {Transformers}: {Perspectives} from {Circuit} {Complexity}},
	volume = {10},
	issn = {2307-387X},
	shorttitle = {Formal {Language} {Recognition} by {Hard} {Attention} {Transformers}},
	url = {https://doi.org/10.1162/tacl_a_00490},
	doi = {10.1162/tacl_a_00490},
	abstract = {This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn’s (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.},
	urldate = {2023-07-31},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Hao, Yiding and Angluin, Dana and Frank, Robert},
	month = jul,
	year = {2022},
	pages = {800--810},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/Z6V2MFWY/Hao et al. - 2022 - Formal Language Recognition by Hard Attention Tran.pdf:application/pdf;Snapshot:/Users/tokio/Zotero/storage/PTTGI268/Formal-Language-Recognition-by-Hard-Attention.html:text/html},
}

@inproceedings{yao_self-attention_2021,
	address = {Online},
	title = {Self-{Attention} {Networks} {Can} {Process} {Bounded} {Hierarchical} {Languages}},
	url = {https://aclanthology.org/2021.acl-long.292},
	doi = {10.18653/v1/2021.acl-long.292},
	abstract = {Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
	month = aug,
	year = {2021},
	pages = {3770--3785},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/3FA5A7JK/Yao et al. - 2021 - Self-Attention Networks Can Process Bounded Hierar.pdf:application/pdf},
}

@inproceedings{chiang_overcoming_2022,
	address = {Dublin, Ireland},
	title = {Overcoming a {Theoretical} {Limitation} of {Self}-{Attention}},
	url = {https://aclanthology.org/2022.acl-long.527},
	doi = {10.18653/v1/2022.acl-long.527},
	abstract = {Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions get closer and closer to random guessing (that is, a cross-entropy of 1) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation implied by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chiang, David and Cholak, Peter},
	month = may,
	year = {2022},
	pages = {7654--7664},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/NPFAHQI6/Chiang and Cholak - 2022 - Overcoming a Theoretical Limitation of Self-Attent.pdf:application/pdf},
}

@inproceedings{merrill_effects_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Effects of {Parameter} {Norm} {Growth} {During} {Transformer} {Training}: {Inductive} {Bias} from {Gradient} {Descent}},
	shorttitle = {Effects of {Parameter} {Norm} {Growth} {During} {Transformer} {Training}},
	url = {https://aclanthology.org/2021.emnlp-main.133},
	doi = {10.18653/v1/2021.emnlp-main.133},
	abstract = {The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude ({\textbackslash}ell\_2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such “saturated” networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Merrill, William and Ramanujan, Vivek and Goldberg, Yoav and Schwartz, Roy and Smith, Noah A.},
	month = jan,
	year = {2021},
	pages = {1766--1781},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PVYY46A7/Merrill et al. - 2021 - Effects of Parameter Norm Growth During Transforme.pdf:application/pdf},
}

@article{likhosherstov_expressive_2023,
	title = {On the {Expressive} {Flexibility} of {Self}-{Attention} {Matrices}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26055},
	doi = {10.1609/aaai.v37i7.26055},
	abstract = {Transformer networks are able to capture patterns in data coming from many domains (text, images, videos, proteins, etc.) with little or no change to architecture components. We perform a theoretical analysis of the core component responsible for signal propagation between elements, i.e. the self-attention matrix. We ask the following question: Can self-attention matrix approximate arbitrary patterns? How small is the query dimension d required for such approximation? Our first result shows that the task of deciding whether approximation of a given pattern is possible or not is NP-hard for a fixed d greater than one. In practice, self-attention matrix typically exhibits two properties: it is sparse, and it changes dynamically depending on the input to the module. Motivated by this observation, we show that the self-attention matrix can provably approximate sparse matrices. While the parameters of self-attention are fixed, various sparse matrices can be approximated by only modifying the inputs. Our proof is based on the random projection technique and uses the seminal Johnson-Lindenstrauss lemma. In particular, we show that, in order to approximate any sparse matrix up to a given precision defined in terms of preserving matrix element ratios, d grows only logarithmically with the sequence length n.},
	language = {en},
	number = {7},
	urldate = {2023-07-31},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Likhosherstov, Valerii and Choromanski, Krzysztof and Weller, Adrian},
	month = jun,
	year = {2023},
	note = {Number: 7},
	keywords = {ML: Kernel Methods},
	pages = {8773--8781},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/26FFVT5Z/Likhosherstov et al. - 2023 - On the Expressive Flexibility of Self-Attention Ma.pdf:application/pdf},
}

@misc{sanford_representational_2023,
	title = {Representational {Strengths} and {Limitations} of {Transformers}},
	url = {http://arxiv.org/abs/2306.02896},
	doi = {10.48550/arXiv.2306.02896},
	abstract = {Attention layers, as commonly used in transformers, form the backbone of modern deep learning, yet there is no mathematical description of their benefits and deficiencies as compared with other architectures. In this work we establish both positive and negative results on the representation power of attention layers, with a focus on intrinsic complexity parameters such as width, depth, and embedding dimension. On the positive side, we present a sparse averaging task, where recurrent networks and feedforward networks all have complexity scaling polynomially in the input size, whereas transformers scale merely logarithmically in the input size; furthermore, we use the same construction to show the necessity and role of a large embedding dimension in a transformer. On the negative side, we present a triple detection task, where attention layers in turn have complexity scaling linearly in the input size; as this scenario seems rare in practice, we also present natural variants that can be efficiently solved by attention layers. The proof techniques emphasize the value of communication complexity in the analysis of transformers and related models, and the role of sparse averaging as a prototypical attention task, which even finds use in the analysis of triple detection.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
	month = jun,
	year = {2023},
	note = {arXiv:2306.02896 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/EV4EBTXD/Sanford et al. - 2023 - Representational Strengths and Limitations of Tran.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/QEXCG7IB/2306.html:text/html},
}

@misc{gurevych_rate_2021,
	title = {On the rate of convergence of a classifier based on a {Transformer} encoder},
	url = {http://arxiv.org/abs/2111.14574},
	doi = {10.48550/arXiv.2111.14574},
	abstract = {Pattern recognition based on a high-dimensional predictor is considered. A classifier is defined which is based on a Transformer encoder. The rate of convergence of the misclassification probability of the classifier towards the optimal misclassification probability is analyzed. It is shown that this classifier is able to circumvent the curse of dimensionality provided the aposteriori probability satisfies a suitable hierarchical composition model. Furthermore, the difference between Transformer classifiers analyzed theoretically in this paper and Transformer classifiers used nowadays in practice are illustrated by considering classification problems in natural language processing.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Gurevych, Iryna and Kohler, Michael and Sahin, Gözde Gül},
	month = nov,
	year = {2021},
	note = {arXiv:2111.14574 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/GRS5GKXA/Gurevych et al. - 2021 - On the rate of convergence of a classifier based o.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/LXRQQLR6/2111.html:text/html},
}

@misc{tarzanagh_max-margin_2023,
	title = {Max-{Margin} {Token} {Selection} in {Attention} {Mechanism}},
	url = {http://arxiv.org/abs/2306.13596},
	doi = {10.48550/arXiv.2306.13596},
	abstract = {Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model \$f({\textbackslash}boldsymbol\{X\})={\textbackslash}langle {\textbackslash}boldsymbol\{Xv\}, {\textbackslash}texttt\{softmax\}({\textbackslash}boldsymbol\{XWp\}){\textbackslash}rangle\$, where \${\textbackslash}boldsymbol\{X\}\$ is the token sequence and \$({\textbackslash}boldsymbol\{v\},{\textbackslash}boldsymbol\{W\},{\textbackslash}boldsymbol\{p\})\$ are trainable parameters. We prove that running gradient descent on \${\textbackslash}boldsymbol\{p\}\$, or equivalently \${\textbackslash}boldsymbol\{W\}\$, converges in direction to a max-margin solution that separates \${\textbackslash}textit\{locally-optimal\}\$ tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize \${\textbackslash}textit\{optimality\}\$ of tokens in terms of the value embeddings \${\textbackslash}boldsymbol\{Xv\}\$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing \${\textbackslash}boldsymbol\{v\}\$ and \${\textbackslash}boldsymbol\{p\}\$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where \${\textbackslash}boldsymbol\{v\}\$ separates the input features based on their labels. Interestingly, the SVM formulation of \${\textbackslash}boldsymbol\{p\}\$ is influenced by the support vector geometry of \${\textbackslash}boldsymbol\{v\}\$. Finally, we verify our theoretical findings via numerical experiments and provide insights.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Tarzanagh, Davoud Ataee and Li, Yingcong and Zhang, Xuechen and Oymak, Samet},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13596 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/26D4WK6R/Tarzanagh et al. - 2023 - Max-Margin Token Selection in Attention Mechanism.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/NVBLGFZQ/2306.html:text/html},
}

@misc{fu_what_2023,
	title = {What can a {Single} {Attention} {Layer} {Learn}? {A} {Study} {Through} the {Random} {Features} {Lens}},
	shorttitle = {What can a {Single} {Attention} {Layer} {Learn}?},
	url = {http://arxiv.org/abs/2307.11353},
	doi = {10.48550/arXiv.2307.11353},
	abstract = {Attention layers -- which map a sequence of inputs to a sequence of outputs -- are core building blocks of the Transformer architecture which has achieved significant breakthroughs in modern artificial intelligence. This paper presents a rigorous theoretical study on the learning and generalization of a single multi-head attention layer, with a sequence of key vectors and a separate query vector as input. We consider the random feature setting where the attention layer has a large number of heads, with randomly sampled frozen query and key matrices, and trainable value matrices. We show that such a random-feature attention layer can express a broad class of target functions that are permutation invariant to the key vectors. We further provide quantitative excess risk bounds for learning these target functions from finite samples, using random feature attention with finitely many heads. Our results feature several implications unique to the attention structure compared with existing random features theory for neural networks, such as (1) Advantages in the sample complexity over standard two-layer random-feature networks; (2) Concrete and natural classes of functions that can be learned efficiently by a random-feature attention layer; and (3) The effect of the sampling distribution of the query-key weight matrix (the product of the query and key matrix), where Gaussian random weights with a non-zero mean result in better sample complexities over the zero-mean counterpart for learning certain natural target functions. Experiments on simulated data corroborate our theoretical findings and further illustrate the interplay between the sample size and the complexity of the target function.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Fu, Hengyu and Guo, Tianyu and Bai, Yu and Mei, Song},
	month = jul,
	year = {2023},
	note = {arXiv:2307.11353 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/YKTH9V57/Fu et al. - 2023 - What can a Single Attention Layer Learn A Study T.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/N5QZS6NK/2307.html:text/html},
}

@misc{geshkovski_emergence_2023,
	title = {The emergence of clusters in self-attention dynamics},
	url = {http://arxiv.org/abs/2305.05465},
	doi = {10.48550/arXiv.2305.05465},
	abstract = {Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. Cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by Transformers. Using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. [VSP'17] that leaders appear in a sequence of tokens when processed by Transformers.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
	month = may,
	year = {2023},
	note = {arXiv:2305.05465 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Analysis of PDEs},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/3T5SGXRZ/Geshkovski et al. - 2023 - The emergence of clusters in self-attention dynami.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WWFFBXZL/2305.html:text/html},
}

@misc{jiang_approximation_2023,
	title = {Approximation theory of transformer networks for sequence modeling},
	url = {http://arxiv.org/abs/2305.18475},
	doi = {10.48550/arXiv.2305.18475},
	abstract = {The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Jiang, Haotian and Li, Qianxiao},
	month = may,
	year = {2023},
	note = {arXiv:2305.18475 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8XD2SE9P/Jiang and Li - 2023 - Approximation theory of transformer networks for s.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/V7VND9RD/2305.html:text/html},
}

@inproceedings{takakura_approximation_2023,
	title = {Approximation and {Estimation} {Ability} of {Transformers} for {Sequence}-to-{Sequence} {Functions} with {Infinite} {Dimensional} {Input}},
	url = {https://proceedings.mlr.press/v202/takakura23a.html},
	abstract = {Despite the great success of Transformer networks in various applications such as natural language processing and computer vision, their theoretical aspects are not well understood. In this paper, we study the approximation and estimation ability of Transformers as sequence-to-sequence functions with infinite dimensional inputs. Although inputs and outputs are both infinite dimensional, we show that when the target function has anisotropic smoothness, Transformers can avoid the curse of dimensionality due to their feature extraction ability and parameter sharing property. In addition, we show that even if the smoothness changes depending on each input, Transformers can estimate the importance of features for each input and extract important features dynamically. Then, we proved that Transformers achieve similar convergence rate as in the case of the fixed smoothness. Our theoretical results support the practical success of Transformers for high dimensional data.},
	language = {en},
	urldate = {2023-07-31},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Takakura, Shokichi and Suzuki, Taiji},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {33416--33447},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/QZWIAU6M/Takakura and Suzuki - 2023 - Approximation and Estimation Ability of Transforme.pdf:application/pdf},
}

@misc{zhang_analysis_2023,
	title = {An {Analysis} of {Attention} via the {Lens} of {Exchangeability} and {Latent} {Variable} {Models}},
	url = {http://arxiv.org/abs/2212.14852},
	doi = {10.48550/arXiv.2212.14852},
	abstract = {With the attention mechanism, transformers achieve significant empirical successes. Despite the intuitive understanding that transformers perform relational inference over long sequences to produce desirable representations, we lack a rigorous theory on how the attention mechanism achieves it. In particular, several intriguing questions remain open: (a) What makes a desirable representation? (b) How does the attention mechanism infer the desirable representation within the forward pass? (c) How does a pretraining procedure learn to infer the desirable representation through the backward pass? We observe that, as is the case in BERT and ViT, input tokens are often exchangeable since they already include positional encodings. The notion of exchangeability induces a latent variable model that is invariant to input sizes, which enables our theoretical analysis. - To answer (a) on representation, we establish the existence of a sufficient and minimal representation of input tokens. In particular, such a representation instantiates the posterior distribution of the latent variable given input tokens, which plays a central role in predicting output labels and solving downstream tasks. - To answer (b) on inference, we prove that attention with the desired parameter infers the latent posterior up to an approximation error, which is decreasing in input sizes. In detail, we quantify how attention approximates the conditional mean of the value given the key, which characterizes how it performs relational inference over long sequences. - To answer (c) on learning, we prove that both supervised and self-supervised objectives allow empirical risk minimization to learn the desired parameter up to a generalization error, which is independent of input sizes. Particularly, in the self-supervised setting, we identify a condition number that is pivotal to solving downstream tasks.},
	urldate = {2023-07-31},
	publisher = {arXiv},
	author = {Zhang, Yufeng and Liu, Boyi and Cai, Qi and Wang, Lingxiao and Wang, Zhaoran},
	month = jan,
	year = {2023},
	note = {arXiv:2212.14852 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/V6G6HIQT/Zhang et al. - 2023 - An Analysis of Attention via the Lens of Exchangea.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/EAEP7SXA/2212.html:text/html},
}

@article{papyan_prevalence_2020,
	title = {Prevalence of neural collapse during the terminal phase of deep learning training},
	volume = {117},
	url = {https://www.pnas.org/doi/10.1073/pnas.2015509117},
	doi = {10.1073/pnas.2015509117},
	abstract = {Modern practice for training classification deepnets involves a terminal phase of training (TPT), which begins at the epoch where training error first vanishes. During TPT, the training error stays effectively zero, while training loss is pushed toward zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call neural collapse (NC), involving four deeply interconnected phenomena. (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class means. (NC2) The class means collapse to the vertices of a simplex equiangular tight frame (ETF). (NC3) Up to rescaling, the last-layer classifiers collapse to the class means or in other words, to the simplex ETF (i.e., to a self-dual configuration). (NC4) For a given activation, the classifier’s decision collapses to simply choosing whichever class has the closest train class mean (i.e., the nearest class center [NCC] decision rule). The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
	number = {40},
	urldate = {2023-08-01},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
	month = oct,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {24652--24663},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/J59I7HH7/Papyan et al. - 2020 - Prevalence of neural collapse during the terminal .pdf:application/pdf},
}

@inproceedings{voita_analyzing_2019,
	address = {Florence, Italy},
	title = {Analyzing {Multi}-{Head} {Self}-{Attention}: {Specialized} {Heads} {Do} the {Heavy} {Lifting}, the {Rest} {Can} {Be} {Pruned}},
	shorttitle = {Analyzing {Multi}-{Head} {Self}-{Attention}},
	url = {https://aclanthology.org/P19-1580},
	doi = {10.18653/v1/P19-1580},
	abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
	urldate = {2023-08-02},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
	month = jul,
	year = {2019},
	pages = {5797--5808},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PE2542MR/Voita et al. - 2019 - Analyzing Multi-Head Self-Attention Specialized H.pdf:application/pdf},
}

@inproceedings{bhattamishra_simplicity_2023,
	address = {Toronto, Canada},
	title = {Simplicity {Bias} in {Transformers} and their {Ability} to {Learn} {Sparse} {Boolean} {Functions}},
	url = {https://aclanthology.org/2023.acl-long.317},
	abstract = {Despite the widespread success of Transformers on NLP tasks, recent works have found that they struggle to model several formal languages when compared to recurrent models. This raises the question of why Transformers perform well in practice and whether they have any properties that enable them to generalize better than recurrent models. In this work, we conduct an extensive empirical study on Boolean functions to demonstrate the following: (i) Random Transformers are relatively more biased towards functions of low sensitivity. (ii) When trained on Boolean functions, both Transformers and LSTMs prioritize learning functions of low sensitivity, with Transformers ultimately converging to functions of lower sensitivity. (iii) On sparse Boolean functions which have low sensitivity, we find that Transformers generalize near perfectly even in the presence of noisy labels whereas LSTMs overfit and achieve poor generalization accuracy. Overall, our results provide strong quantifiable evidence that suggests differences in the inductive biases of Transformers and recurrent models which may help explain Transformer's effective generalization performance despite relatively limited expressiveness.},
	urldate = {2023-08-02},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bhattamishra, Satwik and Patel, Arkil and Kanade, Varun and Blunsom, Phil},
	month = jul,
	year = {2023},
	pages = {5767--5791},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/K589HZNB/Bhattamishra et al. - 2023 - Simplicity Bias in Transformers and their Ability .pdf:application/pdf},
}

@inproceedings{bhattamishra_ability_2020,
	address = {Online},
	title = {On the {Ability} and {Limitations} of {Transformers} to {Recognize} {Formal} {Languages}},
	url = {https://aclanthology.org/2020.emnlp-main.576},
	doi = {10.18653/v1/2020.emnlp-main.576},
	abstract = {Transformers have supplanted recurrent models in a large number of NLP tasks. However, the differences in their abilities to model different syntactic properties remain largely unknown. Past works suggest that LSTMs generalize very well on regular languages and have close connections with counter languages. In this work, we systematically study the ability of Transformers to model such languages as well as the role of its individual components in doing so. We first provide a construction of Transformers for a subclass of counter languages, including well-studied languages such as n-ary Boolean Expressions, Dyck-1, and its generalizations. In experiments, we find that Transformers do well on this subclass, and their learned mechanism strongly correlates with our construction. Perhaps surprisingly, in contrast to LSTMs, Transformers do well only on a subset of regular languages with degrading performance as we make languages more complex according to a well-known measure of complexity. Our analysis also provides insights on the role of self-attention mechanism in modeling certain behaviors and the influence of positional encoding schemes on the learning and generalization abilities of the model.},
	urldate = {2023-08-03},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
	month = jan,
	year = {2020},
	pages = {7096--7116},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/7MRBLUXE/Bhattamishra et al. - 2020 - On the Ability and Limitations of Transformers to .pdf:application/pdf},
}

@inproceedings{shi_sparsebert_2021,
	title = {{SparseBERT}: {Rethinking} the {Importance} {Analysis} in {Self}-attention},
	shorttitle = {{SparseBERT}},
	url = {https://proceedings.mlr.press/v139/shi21a.html},
	abstract = {Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.},
	language = {en},
	urldate = {2023-08-04},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shi, Han and Gao, Jiahui and Ren, Xiaozhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kwok, James Tin-Yau},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {9547--9557},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/L4QMJWAP/Shi et al. - 2021 - SparseBERT Rethinking the Importance Analysis in .pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/7SEBJHCK/Shi et al. - 2021 - SparseBERT Rethinking the Importance Analysis in .pdf:application/pdf},
}

@inproceedings{yun_on_2020,
	title = {O(n) {Connections} are {Expressive} {Enough}: {Universal} {Approximability} of {Sparse} {Transformers}},
	volume = {33},
	shorttitle = {O(n) {Connections} are {Expressive} {Enough}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html},
	urldate = {2023-08-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
	year = {2020},
	pages = {13783--13794},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/RYFMQLSY/Yun et al. - 2020 - O(n) Connections are Expressive Enough Universal .pdf:application/pdf},
}

@misc{chiang_tighter_2023,
	title = {Tighter {Bounds} on the {Expressivity} of {Transformer} {Encoders}},
	url = {http://arxiv.org/abs/2301.10743},
	doi = {10.48550/arXiv.2301.10743},
	abstract = {Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform \$TC{\textasciicircum}0\$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Chiang, David and Cholak, Peter and Pillay, Anand},
	month = may,
	year = {2023},
	note = {arXiv:2301.10743 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Logic in Computer Science, Computer Science - Formal Languages and Automata Theory},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/MRLHGYKK/Chiang et al. - 2023 - Tighter Bounds on the Expressivity of Transformer .pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/L2WMTY9D/2301.html:text/html},
}

@misc{zhai_stabilizing_2023,
	title = {Stabilizing {Transformer} {Training} by {Preventing} {Attention} {Entropy} {Collapse}},
	url = {http://arxiv.org/abs/2303.06296},
	doi = {10.48550/arXiv.2303.06296},
	abstract = {Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as \${\textbackslash}textit\{entropy collapse\}\$. As a remedy, we propose \${\textbackslash}sigma\$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that \${\textbackslash}sigma\$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with \${\textbackslash}sigma\$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that \${\textbackslash}sigma\$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer \{to competitive performance\} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at {\textbackslash}url\{https://github.com/apple/ml-sigma-reparam\}.},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Josh},
	month = jul,
	year = {2023},
	note = {arXiv:2303.06296 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/SW78PAHG/Zhai et al. - 2023 - Stabilizing Transformer Training by Preventing Att.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/WKHNU4LM/2303.html:text/html},
}

@inproceedings{chen_when_2021,
	title = {When {Vision} {Transformers} {Outperform} {ResNets} without {Pre}-training or {Strong} {Data} {Augmentations}},
	url = {https://openreview.net/forum?id=LtKcMgGOeLt},
	abstract = {Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pre-training and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rates). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3{\textbackslash}\% and +11.0{\textbackslash}\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pre-training or strong data augmentations. Model checkpoints are available at {\textbackslash}url\{https://github.com/google-research/vision\_transformer\}.},
	language = {en},
	urldate = {2023-08-04},
	author = {Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
	month = oct,
	year = {2021},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/5RB8JPIS/Chen et al. - 2021 - When Vision Transformers Outperform ResNets withou.pdf:application/pdf},
}

@inproceedings{li_theoretical_2022,
	title = {A {Theoretical} {Understanding} of {Shallow} {Vision} {Transformers}: {Learning}, {Generalization}, and {Sample} {Complexity}},
	shorttitle = {A {Theoretical} {Understanding} of {Shallow} {Vision} {Transformers}},
	url = {https://openreview.net/forum?id=jClGv3Qjhb},
	abstract = {Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, the theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a three-layer ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs.},
	language = {en},
	urldate = {2023-08-04},
	author = {Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/6TYM9N85/Li et al. - 2022 - A Theoretical Understanding of Shallow Vision Tran.pdf:application/pdf},
}

@inproceedings{gruver_lie_2022,
	title = {The {Lie} {Derivative} for {Measuring} {Learned} {Equivariance}},
	url = {https://openreview.net/forum?id=JL7Va5Vy15J},
	abstract = {Equivariance guarantees that a model's predictions capture key symmetries in data. When an image is translated or rotated, an equivariant model's representation of that image will translate or rotate accordingly. The success of convolutional neural networks has historically been tied to translation equivariance directly encoded in their architecture. The rising success of vision transformers, which have no explicit architectural bias towards equivariance, challenges this narrative and suggests that augmentations and training data might also play a significant role in their performance. In order to better understand the role of equivariance in recent vision models, we apply the Lie derivative, a method for measuring equivariance with strong mathematical foundations and minimal hyperparameters. Using the Lie derivative, we study the equivariance properties of hundreds of pretrained models, spanning CNNs, transformers, and Mixer architectures. The scale of our analysis allows us to separate the impact of architecture from other factors like model size or training method. Surprisingly, we find that many violations of equivariance can be linked to spatial aliasing in ubiquitous network layers, such as pointwise non-linearities, and that as models get larger and more accurate they tend to display more equivariance, regardless of architecture. For example, transformers can be more equivariant than convolutional neural networks after training.},
	language = {en},
	urldate = {2023-08-04},
	author = {Gruver, Nate and Finzi, Marc Anton and Goldblum, Micah and Wilson, Andrew Gordon},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/YRDGIG64/Gruver et al. - 2022 - The Lie Derivative for Measuring Learned Equivaria.pdf:application/pdf},
}

@inproceedings{choromanski_rethinking_2020,
	title = {Rethinking {Attention} with {Performers}},
	url = {https://openreview.net/forum?id=Ua6zuk0WRH},
	abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
	language = {en},
	urldate = {2023-08-04},
	author = {Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared Quincy and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David Benjamin and Colwell, Lucy J. and Weller, Adrian},
	month = oct,
	year = {2020},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/VCCAIDYD/Choromanski et al. - 2020 - Rethinking Attention with Performers.pdf:application/pdf},
}

@inproceedings{reif_visualizing_2019,
	title = {Visualizing and {Measuring} the {Geometry} of {BERT}},
	volume = {32},
	url = {https://papers.nips.cc/paper_files/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html},
	abstract = {Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
	urldate = {2023-09-04},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
	year = {2019},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/L9YUP6CG/Reif et al. - 2019 - Visualizing and Measuring the Geometry of BERT.pdf:application/pdf},
}

@misc{muckley_interpretable_2022,
	title = {Interpretable models for extrapolation in scientific machine learning},
	url = {http://arxiv.org/abs/2212.10283},
	doi = {10.48550/arXiv.2212.10283},
	abstract = {Data-driven models are central to scientific discovery. In efforts to achieve state-of-the-art model accuracy, researchers are employing increasingly complex machine learning algorithms that often outperform simple regressions in interpolative settings (e.g. random k-fold cross-validation) but suffer from poor extrapolation performance, portability, and human interpretability, which limits their potential for facilitating novel scientific insight. Here we examine the trade-off between model performance and interpretability across a broad range of science and engineering problems with an emphasis on materials science datasets. We compare the performance of black box random forest and neural network machine learning algorithms to that of single-feature linear regressions which are fitted using interpretable input features discovered by a simple random search algorithm. For interpolation problems, the average prediction errors of linear regressions were twice as high as those of black box models. Remarkably, when prediction tasks required extrapolation, linear models yielded average error only 5\% higher than that of black box models, and outperformed black box models in roughly 40\% of the tested prediction tasks, which suggests that they may be desirable over complex algorithms in many extrapolation problems because of their superior interpretability, computational overhead, and ease of use. The results challenge the common assumption that extrapolative models for scientific machine learning are constrained by an inherent trade-off between performance and interpretability.},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Muckley, Eric S. and Saal, James E. and Meredig, Bryce and Roper, Christopher S. and Martin, John H.},
	month = dec,
	year = {2022},
	note = {arXiv:2212.10283 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/4G5UMLCS/Muckley et al. - 2022 - Interpretable models for extrapolation in scientif.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/AC7CVK2Y/2212.html:text/html},
}

@misc{liang_drugchat_2023,
	title = {{DrugChat}: {Towards} {Enabling} {ChatGPT}-{Like} {Capabilities} on {Drug} {Molecule} {Graphs}},
	shorttitle = {{DrugChat}},
	url = {http://arxiv.org/abs/2309.03907},
	doi = {10.48550/arXiv.2309.03907},
	abstract = {A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at {\textbackslash}url\{https://github.com/UCSD-AI4H/drugchat\}},
	urldate = {2023-09-14},
	publisher = {arXiv},
	author = {Liang, Youwei and Zhang, Ruiyi and Zhang, Li and Xie, Pengtao},
	month = may,
	year = {2023},
	note = {arXiv:2309.03907 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	file = {arXiv Fulltext PDF:/Users/tokio/Zotero/storage/8VEJTNDH/Liang et al. - 2023 - DrugChat Towards Enabling ChatGPT-Like Capabiliti.pdf:application/pdf;arXiv.org Snapshot:/Users/tokio/Zotero/storage/PMR4DY2Q/2309.html:text/html},
}

@inproceedings{huang_improving_2020,
	title = {Improving {Transformer} {Optimization} {Through} {Better} {Initialization}},
	url = {https://proceedings.mlr.press/v119/huang20f.html},
	abstract = {The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously difficult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we first investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justification, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without difficulty. Code for this work is available here: {\textbackslash}url\{https://github.com/layer6ai-labs/T-Fixup\}.},
	language = {en},
	urldate = {2023-09-18},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {4475--4483},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/BV5YLQPT/Huang et al. - 2020 - Improving Transformer Optimization Through Better .pdf:application/pdf;Supplementary PDF:/Users/tokio/Zotero/storage/RYD6BM8S/Huang et al. - 2020 - Improving Transformer Optimization Through Better .pdf:application/pdf},
}

@inproceedings{tjong_kim_sang_introduction_2003,
	title = {Introduction to the {CoNLL}-2003 {Shared} {Task}: {Language}-{Independent} {Named} {Entity} {Recognition}},
	shorttitle = {Introduction to the {CoNLL}-2003 {Shared} {Task}},
	url = {https://aclanthology.org/W03-0419},
	urldate = {2023-09-25},
	booktitle = {Proceedings of the {Seventh} {Conference} on {Natural} {Language} {Learning} at {HLT}-{NAACL} 2003},
	author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	year = {2003},
	pages = {142--147},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/PWJE2MR2/Tjong Kim Sang and De Meulder - 2003 - Introduction to the CoNLL-2003 Shared Task Langua.pdf:application/pdf},
}

@inproceedings{wang_glue_2018,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {https://openreview.net/forum?id=rJ4km2R5t7},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2023-09-25},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = sep,
	year = {2018},
	file = {Full Text PDF:/Users/tokio/Zotero/storage/38Q72XLC/Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform.pdf:application/pdf},
}
