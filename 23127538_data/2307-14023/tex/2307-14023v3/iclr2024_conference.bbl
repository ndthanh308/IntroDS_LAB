\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, and
  Zettlemoyer]{aghajanyan_intrinsic_2021}
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
\newblock Intrinsic {Dimensionality} {Explains} the {Effectiveness} of
  {Language} {Model} {Fine}-{Tuning}.
\newblock In \emph{Proceedings of the 59th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} and the 11th {International}
  {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long}
  {Papers})}, pp.\  7319--7328, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.568}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.568}.

\bibitem[Asadi \& Littman(2017)Asadi and Littman]{asadi_alternative_2017}
Kavosh Asadi and Michael~L. Littman.
\newblock An {Alternative} {Softmax} {Operator} for {Reinforcement} {Learning}.
\newblock In \emph{Proceedings of the 34th {International} {Conference} on
  {Machine} {Learning}}, pp.\  243--252. PMLR, July 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/asadi17a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett_nearly_2019}
Peter~L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (63):\penalty0 1--17, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/17-612.html}.

\bibitem[Baum(1988)]{baum_capabilities_1988}
Eric~B Baum.
\newblock On the capabilities of multilayer perceptrons.
\newblock \emph{Journal of Complexity}, 4\penalty0 (3):\penalty0 193--215,
  September 1988.
\newblock ISSN 0885-064X.
\newblock \doi{10.1016/0885-064X(88)90020-9}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0885064X88900209}.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{belkin_reconciling_2019}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias–variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.
\newblock \doi{10.1073/pnas.1903070116}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.1903070116}.

\bibitem[Bhattamishra et~al.(2020)Bhattamishra, Ahuja, and
  Goyal]{bhattamishra_ability_2020}
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
\newblock On the {Ability} and {Limitations} of {Transformers} to {Recognize}
  {Formal} {Languages}.
\newblock In \emph{Proceedings of the 2020 {Conference} on {Empirical}
  {Methods} in {Natural} {Language} {Processing} ({EMNLP})}, pp.\  7096--7116,
  Online, January 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.576}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.576}.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and
  Kumar]{bhojanapalli_low-rank_2020}
Srinadh Bhojanapalli, Chulhee Yun, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Low-{Rank} {Bottleneck} in {Multi}-head {Attention} {Models}.
\newblock In \emph{Proceedings of the 37th {International} {Conference} on
  {Machine} {Learning}}, pp.\  864--873. PMLR, November 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/bhojanapalli20a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and
  Vandenberghe]{BoydVandenbergheConvex2004}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock {Cambridge University Press}, 2004.
\newblock URL \url{https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown_language_2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Bubeck et~al.(2020)Bubeck, Eldan, Lee, and
  Mikulincer]{bubeck_network_2020}
Sebastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and size of the weights in memorization with two-layers
  neural networks.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  4977--4986. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/hash/34609bdc08a07ace4e1526bbb1777673-Abstract.html}.

\bibitem[{Carroll} \& {Dickinson}(1989){Carroll} and
  {Dickinson}]{carroll_construction_1989}
{Carroll} and {Dickinson}.
\newblock Construction of neural nets using the radon transform.
\newblock In \emph{International 1989 {Joint} {Conference} on {Neural}
  {Networks}}, pp.\  607--611 vol.1, 1989.
\newblock \doi{10.1109/IJCNN.1989.118639}.

\bibitem[Chiang \& Cholak(2022)Chiang and Cholak]{chiang_overcoming_2022}
David Chiang and Peter Cholak.
\newblock Overcoming a {Theoretical} {Limitation} of {Self}-{Attention}.
\newblock In \emph{Proceedings of the 60th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} ({Volume} 1: {Long}
  {Papers})}, pp.\  7654--7664, Dublin, Ireland, May 2022. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.527}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.527}.

\bibitem[Chiang et~al.(2023)Chiang, Cholak, and Pillay]{chiang_tighter_2023}
David Chiang, Peter Cholak, and Anand Pillay.
\newblock Tighter {Bounds} on the {Expressivity} of {Transformer} {Encoders},
  May 2023.
\newblock URL \url{http://arxiv.org/abs/2301.10743}.
\newblock arXiv:2301.10743 [cs].

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{choromanski_rethinking_2020}
Krzysztof~Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared~Quincy Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David~Benjamin Belanger, Lucy~J. Colwell, and
  Adrian Weller.
\newblock Rethinking {Attention} with {Performers}.
\newblock October 2020.
\newblock URL \url{https://openreview.net/forum?id=Ua6zuk0WRH}.

\bibitem[Cybenko(1989)]{cybenko_approximation_1989}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0
  (4):\penalty0 303--314, December 1989.
\newblock ISSN 1435-568X.
\newblock \doi{10.1007/BF02551274}.
\newblock URL \url{https://doi.org/10.1007/BF02551274}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for
  {Language} {Understanding}, May 2019.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.
\newblock arXiv:1810.04805 [cs].

\bibitem[Dosovitskiy et~al.(2022)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy_image_2022}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image}
  {Recognition} at {Scale}.
\newblock March 2022.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman_inductive_2022}
Benjamin~L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive {Biases} and {Variable} {Creation} in {Self}-{Attention}
  {Mechanisms}.
\newblock In \emph{Proceedings of the 39th {International} {Conference} on
  {Machine} {Learning}}, pp.\  5793--5831. PMLR, June 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/edelman22a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Funahashi(1989)]{funahashi_on_1989}
Ken-Ichi Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock \emph{Neural Networks}, 2\penalty0 (3):\penalty0 183--192, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(89)90003-8}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0893608089900038}.

\bibitem[Gurevych et~al.(2021)Gurevych, Kohler, and Sahin]{gurevych_rate_2021}
Iryna Gurevych, Michael Kohler, and Gözde~Gül Sahin.
\newblock On the rate of convergence of a classifier based on a {Transformer}
  encoder, November 2021.
\newblock URL \url{http://arxiv.org/abs/2111.14574}.
\newblock arXiv:2111.14574 [cs, math, stat].

\bibitem[Hahn(2020)]{hahn_theoretical_2020}
Michael Hahn.
\newblock Theoretical {Limitations} of {Self}-{Attention} in {Neural}
  {Sequence} {Models}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 156--171, January 2020.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00306}.
\newblock URL \url{https://doi.org/10.1162/tacl_a_00306}.

\bibitem[Hao et~al.(2022)Hao, Angluin, and Frank]{hao_formal_2022}
Yiding Hao, Dana Angluin, and Robert Frank.
\newblock Formal {Language} {Recognition} by {Hard} {Attention} {Transformers}:
  {Perspectives} from {Circuit} {Complexity}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 800--810, July 2022.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00490}.
\newblock URL \url{https://doi.org/10.1162/tacl_a_00490}.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt_identity_2016}
Moritz Hardt and Tengyu Ma.
\newblock Identity {Matters} in {Deep} {Learning}.
\newblock November 2016.
\newblock URL \url{https://openreview.net/forum?id=ryxB0Rtxx}.

\bibitem[Hornik(1991)]{hornik_approximation_1991}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(91)90009-T}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/089360809190009T}.

\bibitem[Huang(2003)]{huang_learning_2003}
Guang-Bin Huang.
\newblock Learning capability and storage capacity of two-hidden-layer
  feedforward networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 14\penalty0
  (2):\penalty0 274--281, 2003.
\newblock \doi{10.1109/TNN.2003.809401}.

\bibitem[Huang \& Babri(1998)Huang and Babri]{huang_upper_1998}
Guang-Bin Huang and H.A. Babri.
\newblock Upper bounds on the number of hidden neurons in feedforward networks
  with arbitrary bounded nonlinear activation functions.
\newblock \emph{IEEE Transactions on Neural Networks}, 9\penalty0 (1):\penalty0
  224--229, 1998.
\newblock \doi{10.1109/72.655045}.

\bibitem[Jiang \& Li(2023)Jiang and Li]{jiang_approximation_2023}
Haotian Jiang and Qianxiao Li.
\newblock Approximation theory of transformer networks for sequence modeling,
  May 2023.
\newblock URL \url{http://arxiv.org/abs/2305.18475}.
\newblock arXiv:2305.18475 [cs].

\bibitem[Kim et~al.(2023)Kim, Kim, and Mozafari]{kim_provable_2023}
Junghwan Kim, Michelle Kim, and Barzan Mozafari.
\newblock Provable {Memorization} {Capacity} of {Transformers}.
\newblock February 2023.
\newblock URL \url{https://openreview.net/forum?id=8JCg5xJCTPR}.

\bibitem[Kratsios et~al.(2021)Kratsios, Zamanlooy, Liu, and
  Dokmanić]{kratsios_universal_2021}
Anastasis Kratsios, Behnoosh Zamanlooy, Tianlin Liu, and Ivan Dokmanić.
\newblock Universal {Approximation} {Under} {Constraints} is {Possible} with
  {Transformers}.
\newblock October 2021.
\newblock URL \url{https://openreview.net/forum?id=JGO8CvG5S9}.

\bibitem[Lialin et~al.(2023)Lialin, Shivagunde, Muckatira, and
  Rumshisky]{lialin_stack_2023}
Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky.
\newblock Stack {More} {Layers} {Differently}: {High}-{Rank} {Training}
  {Through} {Low}-{Rank} {Updates}, July 2023.
\newblock URL \url{http://arxiv.org/abs/2307.05695}.
\newblock arXiv:2307.05695 [cs].

\bibitem[Likhosherstov et~al.(2023)Likhosherstov, Choromanski, and
  Weller]{likhosherstov_expressive_2023}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the {Expressive} {Flexibility} of {Self}-{Attention} {Matrices}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  37\penalty0 (7):\penalty0 8773--8781, June 2023.
\newblock ISSN 2374-3468.
\newblock \doi{10.1609/aaai.v37i7.26055}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/26055}.
\newblock Number: 7.

\bibitem[Lin \& Jegelka(2018)Lin and Jegelka]{lin_resnet_2018}
Hongzhou Lin and Stefanie Jegelka.
\newblock {ResNet} with one-neuron hidden layers is a {Universal}
  {Approximator}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/hash/03bfc1d4783966c69cc6aef8247e0103-Abstract.html}.

\bibitem[Littman(1996)]{littman_algorithms_1996}
Michael~Lederman Littman.
\newblock \emph{Algorithms for Sequential Decision-Making}.
\newblock PhD thesis, USA, 1996.
\newblock AAI9709069.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu_roberta_2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining}
  {Approach}, July 2019.
\newblock URL \url{http://arxiv.org/abs/1907.11692}.
\newblock arXiv:1907.11692 [cs].

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu_expressive_2017}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The {Expressive} {Power} of {Neural} {Networks}: {A} {View} from the
  {Width}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html}.

\bibitem[Luo et~al.(2022)Luo, Li, Zheng, Liu, Wang, and He]{luo_your_2022}
Shengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di~He.
\newblock Your {Transformer} {May} {Not} be as {Powerful} as {You} {Expect}.
\newblock October 2022.
\newblock URL \url{https://openreview.net/forum?id=NQFFNdsOGD}.

\bibitem[Mahdavi et~al.(2023)Mahdavi, Liao, and
  Thrampoulidis]{mahdavi_memorization_2023}
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis.
\newblock Memorization {Capacity} of {Multi}-{Head} {Attention} in
  {Transformers}, June 2023.
\newblock URL \url{http://arxiv.org/abs/2306.02010}.
\newblock arXiv:2306.02010 [cs].

\bibitem[Merrill et~al.(2022)Merrill, Sabharwal, and
  Smith]{merrill_saturated_2022}
William Merrill, Ashish Sabharwal, and Noah~A. Smith.
\newblock Saturated {Transformers} are {Constant}-{Depth} {Threshold}
  {Circuits}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 843--856, August 2022.
\newblock ISSN 2307-387X.
\newblock \doi{10.1162/tacl_a_00493}.
\newblock URL \url{https://doi.org/10.1162/tacl_a_00493}.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran_deep_2019}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data}
  {Hurt}.
\newblock September 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g5sA4twr}.

\bibitem[Park et~al.(2021)Park, Lee, Yun, and Shin]{park_provable_2021}
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin.
\newblock Provable {Memorization} via {Deep} {Neural} {Networks} using
  {Sub}-linear {Parameters}.
\newblock In \emph{Proceedings of {Thirty} {Fourth} {Conference} on {Learning}
  {Theory}}, pp.\  3627--3661. PMLR, July 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/park21a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Radford et~al.({\natexlab{a}})Radford, Narasimhan, Salimans, and
  Sutskever]{radford_improving_nodate}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving {Language} {Understanding} by {Generative}
  {Pre}-{Training}.
\newblock {\natexlab{a}}.

\bibitem[Radford et~al.({\natexlab{b}})Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford_language_nodate}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language {Models} are {Unsupervised} {Multitask} {Learners}.
\newblock {\natexlab{b}}.

\bibitem[Rajput et~al.(2021)Rajput, Sreenivasan, Papailiopoulos, and
  Karbasi]{rajput_exponential_2021}
Shashank Rajput, Kartik Sreenivasan, Dimitris Papailiopoulos, and Amin Karbasi.
\newblock An {Exponential} {Improvement} on the {Memorization} {Capacity} of
  {Deep} {Threshold} {Networks}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~34, pp.\  12674--12685. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/hash/69dd2eff9b6a421d5ce262b093bdab23-Abstract.html}.

\bibitem[Reif et~al.(2019)Reif, Yuan, Wattenberg, Viegas, Coenen, Pearce, and
  Kim]{reif_visualizing_2019}
Emily Reif, Ann Yuan, Martin Wattenberg, Fernanda~B Viegas, Andy Coenen, Adam
  Pearce, and Been Kim.
\newblock Visualizing and {Measuring} the {Geometry} of {BERT}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://papers.nips.cc/paper_files/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html}.

\bibitem[Sontag(1997)]{sontag_shattering_1997}
Eduardo~D. Sontag.
\newblock Shattering {All} {Sets} of \textit{‘k’} {Points} in “{General}
  {Position}” {Requires} ( \textit{k} — 1)/2 {Parameters}.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 337--348,
  February 1997.
\newblock ISSN 0899-7667, 1530-888X.
\newblock \doi{10.1162/neco.1997.9.2.337}.
\newblock URL \url{https://direct.mit.edu/neco/article/9/2/337-348/6035}.

\bibitem[Takakura \& Suzuki(2023)Takakura and
  Suzuki]{takakura_approximation_2023}
Shokichi Takakura and Taiji Suzuki.
\newblock Approximation and {Estimation} {Ability} of {Transformers} for
  {Sequence}-to-{Sequence} {Functions} with {Infinite} {Dimensional} {Input}.
\newblock In \emph{Proceedings of the 40th {International} {Conference} on
  {Machine} {Learning}}, pp.\  33416--33447. PMLR, July 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/takakura23a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Tjong Kim~Sang \& De~Meulder(2003)Tjong Kim~Sang and
  De~Meulder]{tjong_kim_sang_introduction_2003}
Erik~F. Tjong Kim~Sang and Fien De~Meulder.
\newblock Introduction to the {CoNLL}-2003 {Shared} {Task}:
  {Language}-{Independent} {Named} {Entity} {Recognition}.
\newblock In \emph{Proceedings of the {Seventh} {Conference} on {Natural}
  {Language} {Learning} at {HLT}-{NAACL} 2003}, pp.\  142--147, 2003.
\newblock URL \url{https://aclanthology.org/W03-0419}.

\bibitem[Vardi et~al.(2022)Vardi, Yehudai, and Shamir]{vardi_optimal_2022}
Gal Vardi, Gilad Yehudai, and Ohad Shamir.
\newblock On the {Optimal} {Memorization} {Power} of {ReLU} {Neural}
  {Networks}.
\newblock January 2022.
\newblock URL \url{https://openreview.net/forum?id=MkTPtnjeYTV}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani_attention_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is {All} you {Need}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Vershynin(2020)]{vershynin_memory_2020}
Roman Vershynin.
\newblock Memory capacity of neural networks with threshold and rectified
  linear unit activations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1004--1033, 2020.
\newblock \doi{10.1137/20M1314884}.
\newblock URL \url{https://doi.org/10.1137/20M1314884}.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang_linformer_2020}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: {Self}-{Attention} with {Linear} {Complexity}, June 2020.
\newblock URL \url{http://arxiv.org/abs/2006.04768}.
\newblock arXiv:2006.04768 [cs, stat].

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and
  Narasimhan]{yao_self-attention_2021}
Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan.
\newblock Self-{Attention} {Networks} {Can} {Process} {Bounded} {Hierarchical}
  {Languages}.
\newblock In \emph{Proceedings of the 59th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} and the 11th {International}
  {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long}
  {Papers})}, pp.\  3770--3785, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.292}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.292}.

\bibitem[Ying et~al.(2022)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying_Transformers_2022}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do {Transformers} {Really} {Perform} {Badly} for {Graph}
  {Representation}?
\newblock January 2022.
\newblock URL \url{https://openreview.net/forum?id=OeWooOxFwDa}.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{yun_small_2019}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small {ReLU} networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~32. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/dbea3d0e2a17c170c412c74273778159-Abstract.html}.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun_are_2023}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are {Transformers} universal approximators of sequence-to-sequence
  functions?
\newblock December 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Yun et~al.(2020)Yun, Chang, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun_on_2020}
Chulhee Yun, Yin-Wen Chang, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank
  Reddi, and Sanjiv Kumar.
\newblock O(n) {Connections} are {Expressive} {Enough}: {Universal}
  {Approximability} of {Sparse} {Transformers}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  13783--13794. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/hash/9ed27554c893b5bad850a422c3538c15-Abstract.html}.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang_understanding_2016}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock November 2016.
\newblock URL \url{https://openreview.net/forum?id=Sy8gdB9xx}.

\end{thebibliography}
