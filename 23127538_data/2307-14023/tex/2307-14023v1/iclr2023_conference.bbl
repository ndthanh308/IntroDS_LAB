\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, and
  Zettlemoyer]{aghajanyan_intrinsic_2021}
Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer.
\newblock Intrinsic {Dimensionality} {Explains} the {Effectiveness} of
  {Language} {Model} {Fine}-{Tuning}.
\newblock In \emph{Proceedings of the 59th {Annual} {Meeting} of the
  {Association} for {Computational} {Linguistics} and the 11th {International}
  {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long}
  {Papers})}, pp.\  7319--7328, Online, August 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.568}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.568}.

\bibitem[Asadi \& Littman(2017)Asadi and Littman]{asadi_alternative_2017}
Kavosh Asadi and Michael~L. Littman.
\newblock An {Alternative} {Softmax} {Operator} for {Reinforcement} {Learning}.
\newblock In \emph{Proceedings of the 34th {International} {Conference} on
  {Machine} {Learning}}, pp.\  243--252. PMLR, July 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/asadi17a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Baum(1988)]{baum_capabilities_1988}
Eric~B Baum.
\newblock On the capabilities of multilayer perceptrons.
\newblock \emph{Journal of Complexity}, 4\penalty0 (3):\penalty0 193--215,
  September 1988.
\newblock ISSN 0885-064X.
\newblock \doi{10.1016/0885-064X(88)90020-9}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0885064X88900209}.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{belkin_reconciling_2019}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias–variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.
\newblock \doi{10.1073/pnas.1903070116}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.1903070116}.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and
  Kumar]{bhojanapalli_low-rank_2020}
Srinadh Bhojanapalli, Chulhee Yun, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Low-{Rank} {Bottleneck} in {Multi}-head {Attention} {Models}.
\newblock In \emph{Proceedings of the 37th {International} {Conference} on
  {Machine} {Learning}}, pp.\  864--873. PMLR, November 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/bhojanapalli20a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown_language_2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language {Models} are {Few}-{Shot} {Learners}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}.

\bibitem[Bubeck et~al.(2020)Bubeck, Eldan, Lee, and
  Mikulincer]{bubeck_network_2020}
Sebastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and size of the weights in memorization with two-layers
  neural networks.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33, pp.\  4977--4986. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/hash/34609bdc08a07ace4e1526bbb1777673-Abstract.html}.

\bibitem[{Carroll} \& {Dickinson}(1989){Carroll} and
  {Dickinson}]{carroll_construction_1989}
{Carroll} and {Dickinson}.
\newblock Construction of neural nets using the radon transform.
\newblock In \emph{International 1989 {Joint} {Conference} on {Neural}
  {Networks}}, pp.\  607--611 vol.1, 1989.
\newblock \doi{10.1109/IJCNN.1989.118639}.

\bibitem[Cybenko(1989)]{cybenko_approximation_1989}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0
  (4):\penalty0 303--314, December 1989.
\newblock ISSN 1435-568X.
\newblock \doi{10.1007/BF02551274}.
\newblock URL \url{https://doi.org/10.1007/BF02551274}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin_bert_2019}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for
  {Language} {Understanding}, May 2019.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.
\newblock arXiv:1810.04805 [cs].

\bibitem[Dosovitskiy et~al.(2022)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy_image_2022}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image}
  {Recognition} at {Scale}.
\newblock March 2022.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman_inductive_2022}
Benjamin~L. Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive {Biases} and {Variable} {Creation} in {Self}-{Attention}
  {Mechanisms}.
\newblock In \emph{Proceedings of the 39th {International} {Conference} on
  {Machine} {Learning}}, pp.\  5793--5831. PMLR, June 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/edelman22a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Funahashi(1989)]{funahashi_on_1989}
Ken-Ichi Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock \emph{Neural Networks}, 2\penalty0 (3):\penalty0 183--192, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(89)90003-8}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0893608089900038}.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt_identity_2016}
Moritz Hardt and Tengyu Ma.
\newblock Identity {Matters} in {Deep} {Learning}.
\newblock November 2016.
\newblock URL \url{https://openreview.net/forum?id=ryxB0Rtxx}.

\bibitem[Hornik(1991)]{hornik_approximation_1991}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(91)90009-T}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/089360809190009T}.

\bibitem[Huang(2003)]{huang_learning_2003}
Guang-Bin Huang.
\newblock Learning capability and storage capacity of two-hidden-layer
  feedforward networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 14\penalty0
  (2):\penalty0 274--281, 2003.
\newblock \doi{10.1109/TNN.2003.809401}.

\bibitem[Huang \& Babri(1998)Huang and Babri]{huang_upper_1998}
Guang-Bin Huang and H.A. Babri.
\newblock Upper bounds on the number of hidden neurons in feedforward networks
  with arbitrary bounded nonlinear activation functions.
\newblock \emph{IEEE Transactions on Neural Networks}, 9\penalty0 (1):\penalty0
  224--229, 1998.
\newblock \doi{10.1109/72.655045}.

\bibitem[Kim et~al.(2023)Kim, Kim, and Mozafari]{kim_provable_2023}
Junghwan Kim, Michelle Kim, and Barzan Mozafari.
\newblock Provable {Memorization} {Capacity} of {Transformers}.
\newblock February 2023.
\newblock URL \url{https://openreview.net/forum?id=8JCg5xJCTPR}.

\bibitem[Likhosherstov et~al.(2021)Likhosherstov, Choromanski, and
  Weller]{likhosherstov_expressive_2021}
Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller.
\newblock On the {Expressive} {Power} of {Self}-{Attention} {Matrices}, June
  2021.
\newblock URL \url{http://arxiv.org/abs/2106.03764}.
\newblock arXiv:2106.03764 [cs].

\bibitem[Lin \& Jegelka(2018)Lin and Jegelka]{lin_resnet_2018}
Hongzhou Lin and Stefanie Jegelka.
\newblock {ResNet} with one-neuron hidden layers is a {Universal}
  {Approximator}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2018/hash/03bfc1d4783966c69cc6aef8247e0103-Abstract.html}.

\bibitem[Littman(1996)]{littman_algorithms_1996}
Michael~Lederman Littman.
\newblock \emph{Algorithms for Sequential Decision-Making}.
\newblock PhD thesis, USA, 1996.
\newblock AAI9709069.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu_roberta_2019}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining}
  {Approach}, July 2019.
\newblock URL \url{http://arxiv.org/abs/1907.11692}.
\newblock arXiv:1907.11692 [cs].

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu_expressive_2017}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The {Expressive} {Power} of {Neural} {Networks}: {A} {View} from the
  {Width}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/hash/32cbf687880eb1674a07bf717761dd3a-Abstract.html}.

\bibitem[Mahdavi et~al.(2023)Mahdavi, Liao, and
  Thrampoulidis]{mahdavi_memorization_2023}
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis.
\newblock Memorization {Capacity} of {Multi}-{Head} {Attention} in
  {Transformers}, June 2023.
\newblock URL \url{http://arxiv.org/abs/2306.02010}.
\newblock arXiv:2306.02010 [cs].

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran_deep_2019}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep {Double} {Descent}: {Where} {Bigger} {Models} and {More} {Data}
  {Hurt}.
\newblock September 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g5sA4twr}.

\bibitem[Park et~al.(2021)Park, Lee, Yun, and Shin]{park_provable_2021}
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin.
\newblock Provable {Memorization} via {Deep} {Neural} {Networks} using
  {Sub}-linear {Parameters}.
\newblock In \emph{Proceedings of {Thirty} {Fourth} {Conference} on {Learning}
  {Theory}}, pp.\  3627--3661. PMLR, July 2021.
\newblock URL \url{https://proceedings.mlr.press/v134/park21a.html}.
\newblock ISSN: 2640-3498.

\bibitem[Radford et~al.({\natexlab{a}})Radford, Narasimhan, Salimans, and
  Sutskever]{radford_improving_nodate}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving {Language} {Understanding} by {Generative}
  {Pre}-{Training}.
\newblock {\natexlab{a}}.

\bibitem[Radford et~al.({\natexlab{b}})Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford_language_nodate}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language {Models} are {Unsupervised} {Multitask} {Learners}.
\newblock {\natexlab{b}}.

\bibitem[Rajput et~al.(2021)Rajput, Sreenivasan, Papailiopoulos, and
  Karbasi]{rajput_exponential_2021}
Shashank Rajput, Kartik Sreenivasan, Dimitris Papailiopoulos, and Amin Karbasi.
\newblock An {Exponential} {Improvement} on the {Memorization} {Capacity} of
  {Deep} {Threshold} {Networks}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~34, pp.\  12674--12685. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/hash/69dd2eff9b6a421d5ce262b093bdab23-Abstract.html}.

\bibitem[Sontag(1997)]{sontag_shattering_1997}
Eduardo~D. Sontag.
\newblock Shattering {All} {Sets} of \textit{‘k’} {Points} in “{General}
  {Position}” {Requires} ( \textit{k} — 1)/2 {Parameters}.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 337--348,
  February 1997.
\newblock ISSN 0899-7667, 1530-888X.
\newblock \doi{10.1162/neco.1997.9.2.337}.
\newblock URL \url{https://direct.mit.edu/neco/article/9/2/337-348/6035}.

\bibitem[Vardi et~al.(2022)Vardi, Yehudai, and Shamir]{vardi_optimal_2022}
Gal Vardi, Gilad Yehudai, and Ohad Shamir.
\newblock On the {Optimal} {Memorization} {Power} of {ReLU} {Neural}
  {Networks}.
\newblock January 2022.
\newblock URL \url{https://openreview.net/forum?id=MkTPtnjeYTV}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani_attention_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Łukasz Kaiser, and Illia Polosukhin.
\newblock Attention is {All} you {Need}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}.

\bibitem[Vershynin(2020)]{vershynin_memory_2020}
Roman Vershynin.
\newblock Memory capacity of neural networks with threshold and rectified
  linear unit activations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1004--1033, 2020.
\newblock \doi{10.1137/20M1314884}.
\newblock URL \url{https://doi.org/10.1137/20M1314884}.

\bibitem[Ying et~al.(2022)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying_Transformers_2022}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do {Transformers} {Really} {Perform} {Badly} for {Graph}
  {Representation}?
\newblock January 2022.
\newblock URL \url{https://openreview.net/forum?id=OeWooOxFwDa}.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun_small_2019}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small {ReLU} networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}},
  volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/hash/dbea3d0e2a17c170c412c74273778159-Abstract.html}.

\bibitem[Yun et~al.(2023)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun_are_2023}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are {Transformers} universal approximators of sequence-to-sequence
  functions?
\newblock January 2023.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang_understanding_2016}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock November 2016.
\newblock URL \url{https://openreview.net/forum?id=Sy8gdB9xx}.

\end{thebibliography}
