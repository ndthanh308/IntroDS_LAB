
\documentclass{article} % For LaTeX2e

\usepackage{natbib}
\usepackage[margin=40truemm]{geometry}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{amssymb}

\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{cleveref}
\usepackage{url}
\usepackage[whole]{bxcjkjatype}

\usepackage{amsthm}

\usepackage{comment}

\usepackage{nomencl}
\makenomenclature

\usepackage{etoolbox}
\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{A}{Numbers and Arrays}{%
  \ifstrequal{#1}{B}{Sets}{%
  \ifstrequal{#1}{C}{Indexing}{
  \ifstrequal{#1}{F}{Functions}{}}}}%
]}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}
\newtheorem{pro}{Proposition}


\theoremstyle{definition}
\newtheorem{dfn}{Definition}

\theoremstyle{remark}
\newtheorem{rem}{Remark}

\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\crefname{cor}{Corollary}{Corollaries}
\crefname{pro}{Proposition}{Propositions}

\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\DeclareMathOperator{\boltz}{\mathbf{boltz}}
\DeclareMathOperator{\quant}{\mathbf{quant}}
\DeclareMathOperator{\penal}{\mathbf{penalty}}
\DeclareMathOperator{\bump}{\mathbf{bump}}
\DeclareMathOperator{\dist}{\mathbf{d}}
\date{}

\title{Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?}

\usepackage{authblk}
\author[1]{Tokio Kajitsuka\thanks{kajitsuka-tokio@g.ecc.u-tokyo.ac.jp}}
\author[1]{Issei Sato \thanks{sato@g.ecc.u-tokyo.ac.jp}}
\affil[1]{Department of Computer Science, The University of Tokyo}


\begin{document}

\maketitle

\begin{abstract}
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. 
This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function.
By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.
As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
\end{abstract}

\section{Introduction}
The Transformer model has been ubiquitously used in deep learning since its proposal by \citet{vaswani_attention_2017}.  
Its widespread application spans several domains, not only revolutionizing Natural Language Processing (NLP) through models like BERT \citep{devlin_bert_2019, liu_roberta_2019} and GPT \citep{brown_language_2020, radford_improving_nodate, radford_language_nodate} but also making significant advancements in image and graph processing as an alternative to conventional models like convolutional neural networks (CNNs) and graph neural networks (GNNs) \citep{dosovitskiy_image_2022, ying_Transformers_2022}.

One of the key reasons behind the success of the Transformer model is its ability to represent a wide range of functions. Various studies have been conducted to investigate this aspect, including the universal approximation theorem for Transformer models and its memorization capacity \citet{yun_are_2023, kim_provable_2023, mahdavi_memorization_2023, edelman_inductive_2022, likhosherstov_expressive_2021}.

The main challenge in proving the universal approximation theorem for Transformer models lies in the fact that the Transformer needs to account for the context of the entire input sequence. Unlike feed-forward neural networks where each input is processed independently, the self-attention mechanism in Transformer models must take into account the dependencies between all elements in each input sequence. In constructive proofs \citep{edelman_inductive_2022, yun_are_2023, kim_provable_2023}, these dependencies are often aggregated into a scalar value, which we call a ``context id'' here and is calculated by a self-attention mechanism.

The drawback of existing analyses is that it requires excessively deep layers for data memorization \citep{yun_are_2023, kim_provable_2023}, which leads to a discrepancy with Transformers being deployed in practice.
This discrepancy primarily arises from the interpretation of the softmax function as an approximation of the hardmax function. Consequently, to compute the ``context id'' within the self-attention mechanism, the requirement for self-attention blocks scales linearly with the length of an input sequence.

In this work, we address this gap by closely examining the softmax function itself. First, we show that it is impossible to output the ``context id'' using just one layer of self-attention with the hardmax function. At the same time, we also demonstrate that a single layer of one-head and softmax-based self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.
This result implies that the Transformer with one self-attention layer is a universal approximator by using two feed-forward neural networks connected before and after the self-attention mechanism.

Our contributions are summarized as follows.
\begin{enumerate}
    \item We show that one layer self-attention with the hardmax function is not a contextual mapping; that is, one layer Transformer has no memorization capacity.

    \item In contrast, 
    we provide a framework for constructing a context mapping with one layer of self-attention using the softmax function.

    \item As a result, we prove that one layer Transformer has a memorization capacity, and Transformers with one self-attention layer is universal approximators.
\end{enumerate}

\subsection{Related Works}
\textbf{Universal approximation theorems.} 
The history of the universal approximation theorem begins around 1990 \citep{cybenko_approximation_1989, carroll_construction_1989, hornik_approximation_1991,funahashi_on_1989}. In particular, \citet{cybenko_approximation_1989} analyzed the ability of one-hidden-layer neural networks with step function to approximate continuous functions,  and later \citet{hornik_approximation_1991} extended the analysis to bounded activation functions.
Recent studies on this topic include analyses of how network width and depth affect the expressive power \citep{lu_expressive_2017}, and proofs of the universal approximation theorems for specific architectures \citep{lin_resnet_2018}.
In parallel with studies on the universal approximation theorem, there have also been analyses of memorization capacity of models, i.e., the number of parameters required to memorize a finite number of samples perfectly \citep{baum_capabilities_1988, huang_upper_1998}.
This research topic is similar to the proof of universal approximation theorem, but the focus of the memorization capacity is mainly on the analysis of parameter efficiency for storing finite samples \citep{huang_learning_2003, vershynin_memory_2020, park_provable_2021, vardi_optimal_2022, yun_small_2019, bubeck_network_2020, hardt_identity_2016, rajput_exponential_2021, zhang_understanding_2016}.
Notably, \citet{zhang_understanding_2016} demonstrates that a neural network of the size used in practice can perfectly memorize a randomly labeled data set.
In addition, \citet{belkin_reconciling_2019, nakkiran_deep_2019} have pointed out that the minimum number of parameters required to memorize a dataset is related to the double descent threshold.

\textbf{Expressive capacity of Transformer.} 
Ever since \citet{vaswani_attention_2017} first proposed the Transformer architecture, there have been various theoretical analyses on its expressive capacity.
\citet{yun_are_2023} proved for the first time the universal approximation theorem for the Transformer, showing that a continuous function can be approximated with arbitrary precision if the number of Transformer blocks is on the order of the power of $n$, where $n$ is the length of each input sequence.
Later, \citet{kim_provable_2023} developed a more efficient way to construct contextual mapping, showing that $2n$ self-attention blocks are sufficient for the memorization of finite samples.
Since the studies of \citet{yun_are_2023} and \citet{kim_provable_2023} are closely related to our paper, we discuss the details in more depth in \Secref{sec:background} later.
Their results were based on the assumption that the inputs are separated to some extent, which is an assumption we also make in this paper. Alternatively, under the assumption that input sequences are linearly independent, \citet{mahdavi_memorization_2023} recently showed that a one-layer $H$-head the self-attention mechanism can memorize $O(Hn)$ samples.
Relatedly, \citet{edelman_inductive_2022} investigated the inductive bias of self-attention mechanism and demonstrated that the bounded self-attention head is capable of expressing a sparse Boolean function while obtaining an upper bound on the covering number of self-attention.
In an approach opposite to ours, in which inputs are assumed to be given, \citet{likhosherstov_expressive_2021} showed that, given parameters, there exists an input such that self-attention approximates an arbitrary sparse pattern.
While \citet{bhojanapalli_low-rank_2020} proved that Transformers with small head size, which is typical for multi-head self-attention, cannot express certain  positive column-stochastic matrix, \citet{aghajanyan_intrinsic_2021} demonstrated empirically that pre-trained Transformers have a very low intrinsic dimension.

\section{Preliminaries}
\subsection{Notation}
We use bold lowercase letters to represent vectors and bold uppercase letters to represent matrices.
For any vector $\vv \in \R^a$, we denote by $v_i$ the $i$-the element of $\vv$.
For any matrix $\mA \in \R^{a \times b}$, we denote its $i$-th row by $\mA_{i,:}$, its $k$-th column by $\mA_{:,k}$ and the element at its $i$-th row and $k$-th column by $A_{i,k}$.
For any positive integer $m \in \mathbb{N}_+$, $[m]$ represents the set $\{1,\dots,m\}$. 
For any real numbers $a < b$, $[a,b]$ represents the interval $\{x \in \R \mid a \leq x \leq b\}$, $(-\infty, a)$ represents $\{x \in \R \mid x < a\}$ and $(b,\infty)$ represents $\{x \in \R \mid x > b\}$.
%$\sigma_S[\vv]$ and $\sigma_H[\vv]$ for any input vector $\vv$ are 
Let $\sigma_S[\vv]$ and $\sigma_H[\vv]$ for any input vector $\vv$ be the softmax function and hardmax function, respectively. Note that when there are multiple indices with maximum values, the hardmax function is defined such that the sum of the values at these indices equals one.
By abuse of notation, for any input matrix $\mA$, $\sigma_S\left[\mA\right]$ and $\sigma_H\left[\mA\right]$ are defined as column-wise softmax and column-wise hardmax, respectively.
We denote the ReLU activation function by $\sigma_R$. Unlike $\sigma_S$ and $\sigma_H$, $\sigma_R$ is always an element-wise operator, regardless of whether the input is a vector or a matrix.
Let $\|\cdot\|$ be the $\ell^2$ norm and $\|\cdot\|_p~(1 \leq p<\infty)$ be the $\ell^p$ norm.
We define the distance between two functions $f_1, f_2: \mathbb{R}^{d \times n} \rightarrow \mathbb{R}^{d \times n}$ by
$$
\dist_p\left(f_1, f_2\right):=\left(\int\left\|f_1(\mathbf{X})-f_2(\mathbf{X})\right\|_p^p \mathrm{d} \mathbf{X}\right)^{1 / p}.
$$
In this paper, $n$ denotes the length of an input sequence, $N$ the number of input sequences, $C$ the number of output classes, and $d$ the embedding dimension.
In addition, $i,j$ are basically used for the indices of finite samples and $k,l$ are used for the indices in each input sequence.


\subsection{Transformer Block}
Transformer was first introduced in \citet{vaswani_attention_2017}. Here we follow the definitions adopted in \citet{kim_provable_2023}:
the Transformer block is composed of the self-attention mechanism and the feed-forward neural network, each accompanied by a skip connection.
Given an input sequence $\mZ \in \R^{d \times n}$, composed of $n$ tokens each with an embedding dimension of size $d$, a dot-product self-attention mechanism with $h$ heads outputs the following values:
\begin{align*}
    \mathcal{F}^{(SA)}_S(\mZ)
    = \mZ + \sum_{i=1}^h \mW_{l,i}^{(O)}
    \left(\mW_{l,i}^{(V)}\mZ \right)
    \sigma_S \left[
    \left(\mW_{l,i}^{(K)}\mZ \right)^\top
    \left(\mW_{l,i}^{(Q)}\mZ \right)
    \right]
    \in \R^{d \times n},
\end{align*}
where $\mW_{l,i}^{(O)} \in \R^{d \times s}$ and $\mW_{l,i}^{(V)},\, \mW_{l,i}^{(K)}\, \mW_{l,i}^{(Q)} \in \R^{s \times d}$ are the weight matrices, and $s$ is the head size.
Note that here, as with \citet{yun_are_2023} and \citet{kim_provable_2023}, we adopt the definition of the self-attention mechanism, which excludes layer normalization from the original definition of \citet{vaswani_attention_2017} for the sake of simplicity.

In contrast, given an input $\mH \in \R^{d \times n}$, the output of feed-forward neural network with a skip connection at index $k \in [n]$ is
\begin{align*}
    \mathcal{F}^{(FF)}\left(\mH\right)_{:,k}
    = \mH_{:,k} + \mW^{(2)}\sigma_R\left[
    \mW^{(1)}\mH_{:,k} + \vb^{(1)}
    \right] + \vb^{(2)} \in \R^d,
\end{align*}
where $q$ is the hidden dimension, $\mW^{(1)} \in \R^{q \times d}$ and $\mW^{(2)} \in \R^{d \times q}$ are weight matrices, and $\vb^{(1)} \in \R^q$ and $\vb^{(2)}$ are bias terms.

On the basis of the above definition, the Transformer block is represented as a combination of a self-attention mechanism and a feed-forward neural network: for any input sequence $\mZ \in \R^{d \times n}$, composed of $n$ tokens each with an embedding dimension of size $d$, the Transformer block $\mathcal{F}:\R^{d \times n} \to \R^{d \times n}$ outputs
\begin{align*}
    \mathcal{F}\left(\mZ\right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mZ\right)
    \right).
\end{align*}
From the above definition, we see that the interaction of each token occurs only in the self-attention mechanism.

\section{Attention is a Contextual Mapping}
\subsection{Problem Setting}
Let $(\mX^{(1)}, \mY^{(1)}),\dots,(\mX^{(1)}, \mY^{(1)}) \subset \R^{d \times n} \times [C]^{1 \times n}$ be a $N$ sequence of input-output pairs, each of which consists of an $n$ sequence $\mX^{(i)}$ of tokens with embedding dimension $d$, and an output $\mY^{(i)}$, where $\mY^{(i)}_{:,}$ corresponds to the label of the token $\mX^{(i)}_{:,k}$ at index $k$.
In addition, we define the $i$-th vocabulary set for $i \in [N]$ by $\mathcal{V}^{(i)} = \bigcup_{k \in [n]} \mX^{(i)}_{:,k} \subset \R^{d}$, and the whole vocabulary set $\mathcal{V}$ is defined by $\mathcal{V} = \bigcup_{i \in [N]} \mathcal{V}^{(i)} \subset \R^d$.

In order to analyze the memorization capacity and universal approximation theorem in the following sections without contradiction, we impose the following natural consistency on the data. 
\begin{ass}[Consistency]
The $N$ sequence of input-output pairs \\
$(\mX^{(1)}, \mY^{(1)}),\dots,(\mX^{(1)}, \mY^{(1)}) \subset \R^{d \times n} \times [C]^{1 \times n}$ satisfies the following consistency condition: for any $i, j \in [N]$ and $k,l \in [n]$ \begin{align*}
    \mY^{(i)}_{:,k}
    =
    \mY^{(j)}_{:,l}
\end{align*}
holds if $\mathcal{V}^{(i)} = \mathcal{V}^{(j)}$ and $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ are satisfied.
\end{ass}
\begin{rem}
It is important to note that if general input-output pairs are to be considered, the condition in the above definition of consistency should not be defined as $\mathcal{V}^{(i)} = \mathcal{V}^{(j)}$ but rather as a condition where the \textit{multiset} of tokens in each sentence matches, and simultaneously the tokens in the position of interest are the same.
However, when considering consistency in this paper, it is only for input sequences that do not contain duplicate tokens, so matching the vocabulary set as described above is sufficient.
\end{rem}

\subsection{Background}\label{sec:background}
\citet{yun_are_2023} proved affirmatively one of the most fundamental questions on the expressive capacity of Transformer models, namely, whether universal approximation theorem for Transformer models hold.
Their proof approach is to quantize the input domain and reduce the universal approximation theorem to an analysis of memorization of finite samples, i.e., the construction of a model that achieves zero loss for a finite number of training data, which was also analyzed later by \citet{kim_provable_2023}.
In the analysis of memorization capacity, assumptions are usually made on the inputs in order to perform a meaningful analysis beyond the lower bound of \citet{sontag_shattering_1997}, and here, as with the assumptions adopted by \citet{yun_are_2023, kim_provable_2023}, we assume that the input tokens are separated by a certain distance:

\begin{dfn}[Tokenwise Separatedness]
Let $m \in \mathbb{N}$ and $\mZ^{(1)},\dots,\mZ^{(N)} \in \R^{m \times n}$ be input sequences. Then, $\mZ^{(1)},\dots,\mZ^{(N)}$ are called tokenwise $(r_{\min}, r_{\max},\delta)$-separated if the following three conditions hold.
\begin{enumerate}
    \item For any $i \in [N]$ and $k \in [n]$, $\left\|\mZ^{(i)}_{:,k}\right\| > r_{\min}$ holds.
    
    \item For any $i \in [N]$ and $k \in [n]$, $\left\|\mZ^{(i)}_{:,k}\right\| < r_{\max}$ holds.
    
    \item For any $i,j \in [N]$ and $k,l \in [n]$ with $\mZ^{(i)}_{:,k} \neq \mZ^{(j)}_{:,l}$, $\left\|\mZ^{(i)}_{:,k} - \mZ^{(j)}_{:,l}\right\| > \epsilon$ holds.
\end{enumerate}
Note that we refer to $\mZ^{(1)},\dots,\mZ^{(N)}$ as tokenwise $(r_{\max}, \epsilon)$-separated instead if the sequences satisfy the conditions $2$ and $3$.
\end{dfn}

The achievement of \citet{yun_are_2023} was not only to prove the universal approximation theorem for Transformers, but also to clarify the difficulties in the analysis of this kind of expressive capacity of Transformers and elucidate an approach to establishing the proof. 
Namely, what makes Transformers' memorization different from that of feed-forward neural networks is that Transformers need to capture the context of each input sequence as a whole, rather than simply associating each token with a label. 

Remarkably, \citet{yun_are_2023, kim_provable_2023} formulated this concept as a contextual mapping, which assigns a unique id to a pair of input sequences and each of their tokens. We define it here using the notion of $(r,\delta)$-separatedness.
\begin{dfn}[Contextual Mapping]
    Let $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ be input sequences. Then, a map $q: \R^{d \times n} \to \R^{d \times n}$ is called an $(r,\delta)$-contextual mapping if the following two condition holds:
    \begin{enumerate}
        \item For any $i \in [N]$ and $k \in [n]$, $\left\|q\left(\mX^{(i)}\right)_{:,k}\right\| \leq r$ holds.
        
        \item For any $i,j \in [N]$ and $k,l \in [n]$ with $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$ or $\mX^{(i)}_{:,k} \neq \mX^{(j)}_{:,l}$, \\
        $\left\|q\left(\mX^{(i)}\right)_{:,k} - q\left(\mX^{(j)}\right)_{:,l}\right\| > \delta$ holds.
    \end{enumerate}
    In particular, $q(\mX^{(i)})$ for $i \in [N]$ is called a context id of $\mX^{(i)}$.
\end{dfn}

If we have such a contextual mapping, a label sequence can be associated with a unique id for each input sequence using the existing analysis of memorization in feed-forward neural networks.

So the central question is: how to construct a contextual mapping in Transformer models?
The only place in Transformer models where interaction between tokens can be taken into account is the self-attention mechanism, and therefore the self-attention mechanism must be used to construct the contextual mapping.
\citet{yun_are_2023} first constructed a contextual mapping by using $|\mathcal{V}|^d + 1$ self-attention layers\footnote{To be precise, when the continuous input range is quantized into $1/\delta$ pieces for some $0 < \delta < 1$, they demonstrated that there exists a contextual mapping composed of $\delta^{-d}$ self-attention layers.}, and later \citet{kim_provable_2023} improved it to $2n$ self-attention layers.
However, this is still far from the practical implementation of Transformers, and it remains unclear whether a reasonably-sized Transformer would possess memorization capacity or if the universal approximation theorem would hold.
This leads to the following question.

\textbf{How many self-attention layers are both necessary and sufficient to construct a contextual mapping?}

We first point out the reason for requiring a significant number of self-attention layers in the construction of contextual mapping in the analyses of \citet{yun_are_2023, kim_provable_2023}.
Their approach entails interpreting the softmax function in the self-attention mechanism as an approximation of the hardmax function, which also hindered the detailed analysis of the specific properties of the softmax function.
As evidence of this, we illustrate in \Secref{sec:self_attention_with_hardmax} that using a single layer of self-attention with the hardmax function does not suffice to construct a contextual mapping.

Next, in \Secref{sec:self_attention_with_softmax}, we demonstrate that a contextual mapping can be constructed by using only $1$ self-attention layer with the softmax function.
This is somewhat surprising because this implies that it is possible to fully capture the context of each input sequence only through the attention coefficients computed by the pairwise dot-product of the softmax function and its weighted average.


\subsection{Self-attention with hardmax}\label{sec:self_attention_with_hardmax}
In previous studies analyzing the memorization capacity of the Transformer \citep{yun_are_2023,kim_provable_2023}, softmax is taken to be an approximation of hardmax.
However, we show here that the attention block with hardmax is not a contextual mapping.

First we define the attention block with hardmax: for an input sequence $\mZ \in \R^{d \times n}$, the attention with hardmax is calculated as
\begin{align}
    \mathcal{F}^{(SA)}_H(\mZ)
    = \mZ + \sum_{i=1}^h \mW_{l,i}^{(O)}
    \left(\mW_{l,i}^{(V)}\mZ \right)
    \sigma_H \left[
    \left(\mW_{l,i}^{(K)}\mZ \right)^\top
    \left(\mW_{l,i}^{(Q)}\mZ \right)
    \right],
\end{align}
where $\mW_{l,i}^{(O)} \in \R^{d \times s}$ and $\mW_{l,i}^{(V)},\, \mW_{l,i}^{(K)}\, \mW_{l,i}^{(Q)} \in \R^{s \times d}$ are the weight matrices

The following theorem holds for such a model. The proof is in Appendix \ref{sec: proof_of_hardmax}.
\begin{thm}\label{thm:hardmax_is_not_contextual_mapping}
$1$-layer multi-head self-attention $\mathcal{F}^{(SA)}_H$ with the hardmax function cannot be a contextual mapping.
\end{thm}
Since the self-attention mechanism is the only place in Transformer models where interaction between tokens can be considered, this theorem indicates that one-layer Transformer does not have a memorization capacity.


\subsection{Self-attention with softmax}\label{sec:self_attention_with_softmax}
In this subsection, we show that $1$-layer attention block with softmax is a contextual mapping for almost all input sequences.

\begin{thm}\label{thm:softmax_is_contextual_mapping}
    Let $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ be input sequences with no duplicate word token in each sequence, that is,
    \begin{align*}
        \mX^{(i)}_{:,k} \neq \mX^{(i)}_{:,l} \numberthis
    \end{align*}
    for any $i \in [N]$ and $k,l \in [n]$.
    Also assume that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated.
    Then, there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{V}, \mW^{K}, \mW^{Q} \in \R^{s \times d}$ such that the ranks of $\mW^{V}, \mW^{K}$ and $\mW^{Q}$ are all $1$, and $1$-layer single head attention with softmax, i.e., $\mathcal{F}^{(SA)}_S$ with $h=1$ is an $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ with $r$ and $\delta$ defined by
    \begin{align}
        r &= r_{\max} + \frac{\epsilon}{4}, \\
        \delta &= \frac{\epsilon r_{\min} \log n}{r_{\max}^2 (|\mathcal{V}|+1)^4 \pi d \cdot \max \left(2+e, 6\log n\right)} \nonumber\\
        &\quad\quad\quad\quad
        \cdot \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{\pi d r_{\max}^2 \cdot \max \left(2+e, 6\log n\right)}{4\epsilon r_{\min}}\right).
    \end{align}
\end{thm}

Here we provide a simple proof sketch.
The full proof can be found in Appendix \ref{sec: proof_of_contextual_mapping}.
\begin{proof}[Proof Overview]
For simplicity, we here assume $s = 1$.
If we have a unique id, i.e., sequence id, corresponding to each input sequence $\mX^{(i)}$ for $i \in [N]$, a context id can be constructed from a suitable linear combination of sequence id and the value of each token. Since this procedure of calculating the linear combination can be achieved by the output projection matrix $\mW^{(O)}$ and skip connection, the problem is how to configure weight parameters $\mW^{(V)}, \mW^{(K)}, \mW^{(Q)} \in \R^{1 \times d}$ so that each row of values' softmax weighted average,
\begin{equation*}
    \left(\mW^{(V)}\mX^{(i)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(i)} \right)^\top
    \left(\mW^{(Q)}\mX^{(i)} \right)
    \right]
    \in \R^{1 \times n}, \numberthis
\end{equation*}
outputs the unique sequence id of $\mX^{(i)}$.

Actually, an even weaker condition is sufficient for an attention block to be a contextual mapping: there is no need to have just one unique sequence id for each input sequence. In fact, it is possible to construct a contextual mapping, provided that for each token $\vv \in \mathcal{V}$, input sequences in which the token appears can be identified by some $v$-specific sequence ids. This condition can be expressed in a mathematical form as follows: for any distinct $i,j \in [N]$ and any $k,l \in [n]$ such that $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$, what we have to show is to construct weight matrices $\mW^{(V)},\mW^{(K)},\mW^{(Q)} \in \R^{1 \times d}$ such that
\begin{align*}
    &\left(\mW^{(V)}\mX^{(i)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(i)} \right)^\top
    \left(\mW^{(Q)}\mX^{(i)}_{:,k} \right)
    \right] \\
    &\quad\quad\quad\quad-
    \left(\mW^{(V)}\mX^{(j)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(j)} \right)^\top
    \left(\mW^{(Q)}\mX^{(j)}_{:,l} \right)
    \right]
    > \epsilon \numberthis
\end{align*}
holds for some $\epsilon > 0$.

For simplicity, we choose $\mW^{(V)}=\mW^{(K)}=\mW^{(Q)} = \vw^\top$ \footnote{In our actual proof, there exist unit vectors $\vv,\vv' \in \R^d$ such that $\mW^{(V)},\mW^{(K)}$ and $\mW^{(Q)}$ may be defined by $\mW^{(V)}=\vu'' \vv^\top, \mW^{(K)}=\vu' \vv^\top$ and $\mW^{(Q)}=\vu \vv'^\top$ for arbitrary vectors $\vu,\vu',\vu'' \in \R^s$ satisfying certain constraints.} such that the linear operator $\vw \in \R^d$ projects each token to a scalar value while approximately preserving distance between each pair of tokens: for any pair of tokens $\vv_a,\vv_b \in \mathcal{V}$,
\begin{align*}
    C\|\vv_a - \vv_b\|
    \leq \left|\vw^\top \vv_a - \vw^\top \vv_b\right|
    \leq \|\vv_a - \vv_b\| \numberthis
\end{align*}
with some constant $0 < C < 1$. Then, by using the assumption $\vt = \mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ for some token $\vt \in \R^d$, we have
\begin{align*}
    &\left|
    \vw^\top \vt
    \right|
    \cdot 
    \left|
    \left(\vw^\top\mX^{(i)} \right)
    \sigma_S \left[
    \left(\vw^\top\mX^{(i)} \right)^\top
    \left(\vw^\top \vt \right)
    \right]
    -
    \left(\vw^\top\mX^{(j)} \right)
    \sigma_S \left[
    \left(\vw^\top\mX^{(j)} \right)^\top
    \left(\vw^\top \vt \right)
    \right]
    \right| \\
    &\geq
    \left|
    \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
    -
    \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
    \right|, \numberthis
\end{align*}
where we denote $\va^{(i)} = \left(\vw^\top\mX^{(i)} \right)^\top
    \left(\vw^\top \vt \right) \in \R^n$ and $\va^{(j)} = \left(\vw^\top\mX^{(j)} \right)^\top
    \left(\vw^\top \vt \right) \in \R^n$.
Therefore, in order to prove that a self-attention block serves as a contextual mapping, we only have to focus on the separability of the function 
\begin{align*}
    \boltz: \mathbb{R}^n \to \mathbb{R}, \va \mapsto \va^\top \sigma_S[\va], \numberthis
\end{align*}
which is known as the Boltzmann operator \citep{littman_algorithms_1996, asadi_alternative_2017}.

The following lemma shows that the Boltzmann operator is a mapping that projects input sequences to scalar values while preserving some distance, and is central to our proof that the self-attention function is a contextual mapping.
\begin{lem}\label{lem: boltzmann_separation}
Let $\va^{(1)},\dots,\va^{(m)} \in \R^n$ be tokenwise $(r,\delta)$-separated vectors with no duplicate element in each vector and
\begin{equation*}
    \delta = \max(2+e, 6\log n). \numberthis
\end{equation*}
Then, the outputs of the Boltzmann operator are $(r,\delta')$-separated, that is,
\begin{align}
    \left|\boltz(\va^{(i)})\right| &\leq r \\
    \left|\boltz(\va^{(i)}) - \boltz(\va^{(j)})\right|
    &> \delta'
    = \frac{\log n}{2}e^{-2r}
\end{align}
hold for each $i,j \in [m]$ with $\va^{(i)} \neq \va^{(j)}$.
\end{lem}
Taking into account the above arguments, this separability of the Boltzmann operator allows us to construct one self-attention layer to be a contextual mapping.
\end{proof}

\section{Applications of Contextual Mapping}
\subsection{Memorization capacity of one-layer Transformer}
As a first application of \cref{thm:softmax_is_contextual_mapping}, we prove that a 1-layer Transformer can completely memorize finite samples, each of which has no duplicate token.
This result especially indicates that in contrast to the proof of \citet{kim_provable_2023}, which required $2n$ self-attention layers for Transformer memorization, one layer of self-attention is actually sufficient.
In addition, it is worth referring to the fact that the hardmax-based Transformers do not have a memorization capacity, which is implied straightforwardly from \cref{thm:hardmax_is_not_contextual_mapping}.

\begin{cor}[Memorization capacity of one-layer Transformer]\label{cor:memorization_of_one_layer_Transformer}
    Let $\epsilon > 0, r_{\max} > r_{\min} > 0$ and $(\mX^{(1)}, \mY^{(1)}),\dots, (\mX^{(N)}, \mY^{(N)}) \subset \R^{d \times n} \times |C|^{1 \times n}$ be sequences of input-output-pairs such that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated input sequences with no duplicate token in each sentence.
    Then, there exist weight parameters such that for any $i \in [N]$
    \begin{align*}
        \mathcal{F}\left(\mX^{(i)} \right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mX^{(i)}\right)
    \right)
        = \mY^{(i)} \numberthis
    \end{align*}
    holds.
\end{cor}

In addition, it is straightforward to show that a $1$-layer Transformer with trainable positional encodings has a memorization capacity for arbitrary input sequences possibly with duplicate tokens.

\begin{cor}[Memorization capacity of one-layer Transformer with positional encodings]\label{cor:memorization_of_one_layer_Transformer_with_positional_encodings}
    Let $\epsilon > 0, r_{\max} > r_{\min} > 0$ and $(\mX^{(1)}, \mY^{(1)}),\dots, (\mX^{(N)}, \mY^{(N)}) \subset \R^{d \times n} \times |C|^{1 \times n}$ be sequences of input-output-pairs such that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated input sequences.
    Then, there exist weight parameters and positional encodings $\mE \in \R^{d \times n}$ such that for any $i \in [N]$
    \begin{align*}
        \mathcal{F}\left(\mX^{(i)} + \mE\right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mX^{(i)} + \mE\right)
    \right)
        = \mY^{(i)} \numberthis
    \end{align*}
    holds.
\end{cor}

\subsection{Transformers with one self-attention layer are universal approximators}
As a further application of \cref{thm:softmax_is_contextual_mapping}
we here provide a proof that Transformer with one self-attention layer is a universal approximator.
More precisely, let $\mathcal{F}_{\mathrm{PE}}$ be the set of all permutation equivariant continuous functions that take values on a compact domain in $\R^{d \times n}$, and let $\mathcal{T}_2$ be the set of all two layer Transformers with one self-attention layer, that is,
\begin{align*}
    \mathcal{T}_2
    = \left\{
    \mathcal{F}^{(FF)}_2
    \circ
    \mathcal{F}^{(SA)}_S
    \circ
    \mathcal{F}^{(FF)}_1
    : \R^{n \times d} \to \R^{n \times d}
    \right\}, \numberthis
\end{align*}
where $\mathcal{F}^{(FF)}_1,\mathcal{F}^{(FF)}_2$ and $\mathcal{F}^{(SA)}_S$ are feed-forward neural network layers and a self-attention layer with the softmax function, respectively.
Then the following proposition holds:
\begin{pro}[Transformers with one layer self-attention are universal approximators]\label{pro:Transformers_are_universal_approximator}
Let $1 \leq p < \infty$. Then, for any $f \in \mathcal{F}_{\mathrm{PE}}$ and $\epsilon > 0$, there exists a Transformer $g \in \mathcal{T}_2$  with one layer self-attention such that
\begin{align*}
    \dist_p(f,g)
    = \left(\int_{\R^{d \times n}} \left\|f(\mZ) - g(\mZ)\right\|^p_p \mathrm{d}\mZ\right)^{1/p}
    < \epsilon.
\end{align*}
holds.
\end{pro}

\section{Conclusions}
We demonstrated that a contextual mapping can be implemented in one layer self-attention with low-rank matrices, by clarifying the connection between a self-attention mechanism and the Boltzmann operator. This particularly indicates that one layer Transformers have a memorization capacity for finite samples, and Transformers with one self-attention layer are universal approximators for continuous functions on a compact domain.
Our proof of the universal approximation theorem requires one feed-forward neural network layer before the self-attention layer to quantize continuous inputs.
We leave it as future work to clarify whether the one-layer Transformers without such a quantization layer are universal approximators or not.

\clearpage

\bibliography{iclr2023_conference,zotero2023_7_26}

\bibliographystyle{iclr2023_conference}

\appendix
\renewcommand{\nomname}{Notation Table}
\nomenclature[A, 03]{\(\mA\)}{A matrix}
\nomenclature[A, 02]{\(\va\)}{A vector}
\nomenclature[A, 01]{\(a\)}{A scalar}
\nomenclature[A, 08]{\(\mX^{(i)}\)}{$i$-th input sequence, consisting of $n$ tokens of embedding dimension $d$}
\nomenclature[A, 04]{$n$}{The length of an input sequence}
\nomenclature[A, 05]{$N$}{The number of input sequences}
\nomenclature[A, 06]{$C$}{The number of output classes}
\nomenclature[A, 07]{$d$}{Embedding dimension}

\nomenclature[B, 03]{\([a,b]\)}{Closed interval from $a$ to $b$}
\nomenclature[B, 02]{\([m]\)}{Set of all integers from $1$ to $m$}
\nomenclature[B, 01]{\(\R\)}{Set of real numbers}
\nomenclature[B, 04]{\(\mathcal{V}^{(i)}\)}{$i$-th vocabulary set}

\nomenclature[C, 02]{\(A_{i,j}\)}{Element $i, j$ of matrix $\mA$}
\nomenclature[C, 01]{\(a_i\)}{Element $i$ of vector $\va$, with indexing starting at $1$}
\nomenclature[C, 03]{\(\mA_{i, :}\)}{Row $i$ of matrix $\mA$}
\nomenclature[C, 03]{\(\mA_{:, i}\)}{Column $i$ of matrix $\mA$}

\nomenclature[F, 02]{$\lVert\vx\rVert$}{$\ell^2$ norm of $\vx$}
\nomenclature[F, 03]{$\lVert\vx\rVert_p$}{$\ell^p$ norm of $\vx$}
\nomenclature[F, 04]{\(\1_\mathrm{condition}\)}{is 1 if the condition is true, 0 otherwise}
\nomenclature[F, 05]{$\dist_p(f_1,f_2)$}{$\left(\int\left\lVert f_1(\mathbf{X})-f_2(\mathbf{X})\right\rVert_p^p \mathrm{d} \mathbf{X}\right)^{1 / p}$}
\nomenclature[F, 06]{$\sigma_S$}{Softmax function}
\nomenclature[F, 07]{$\sigma_H$}{Hardmax function}
\nomenclature[F, 08]{$\sigma_R$}{ReLU activation function}
\nomenclature[F, 09]{$\mathcal{F}^{(SA)}_H$}{Hardmax-based self-attention mechanism with a skip-connection}
\nomenclature[F, 10]{$\mathcal{F}^{(SA)}_S$}{Softmax-based self-attention mechanism with a skip-connection}
\nomenclature[F, 11]{$\mathcal{F}^{(FF)}$}{Feed-forward neural network with a skip-connection}
\nomenclature[F, 12]{$\boltz$}{Boltzmann opeartor}

\printnomenclature


\section{Proof of Main Results}

First, we introduce the Boltzmann operator, which frequently appear in our proofs.
\begin{dfn}(Boltzmann operator)
The Boltzmann operator is defined by
\begin{align*}
    \boltz: \R^m \to \R,\,
    \va \mapsto \va^\top \sigma_S[\va]. \numberthis
\end{align*}
By abuse of notation, we use the same notation $\boltz$ for various dimension $m$.
\end{dfn}

\subsection{\texorpdfstring{Proof of \cref{thm:hardmax_is_not_contextual_mapping}}{}}\label{sec: proof_of_hardmax}
\begin{proof}
Let $\vv \in \R^d$ be an arbitrary nonzero vector, and consider the situation that all input tokens can be written as
\begin{equation}
    \mathcal{V} = \{\alpha_1 \vv, \alpha_2 \vv, \alpha_3 \vv, \alpha_4 \vv\} \subset \R^d
\end{equation}
for some scalars $\alpha_1 < \alpha_2 < \alpha_3 < \alpha_4$. 
Then, the attention matrix inside the hardmax function at head $i$ can be expressed as
\begin{align}
    \left(\mW_{i}^{(K)}\vv \va^\top \right)^\top
    \left(\mW_{i}^{(Q)}\vv \va^\top \right)
    = \va \left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \va^\top 
\end{align}
with input coefficients $\va \in \{\alpha_1,\alpha_2,\alpha_3,\alpha_4\}^n \subset \R^n$.
In particular, when we focus on a certain index, at which the token is, e.g., $\alpha_2 \vv$, the above expression can further be written as
\begin{align*}
    \left(\mW_{i}^{(K)}\vv \va^\top \right)^\top
    \left(\mW_{i}^{(Q)}\alpha_2\vv \right)
    &= \va \left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \alpha_2 \\
    &= \underbrace{\left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \alpha_2}_{\in \R} \cdot \va \numberthis
\end{align*}
The right-hand side is a vector $\va$ multiplied by some scalar. So it is evident that the maximum value of the vector on the right-hand side is achieved only at the indices where the values of the input sequence $\vv \va^\top$ are $\alpha_1$ or $\alpha_4$.
This implies that a self-attention with the hardmax function invariably gets distracted by the indices where $\alpha_1$ or $\alpha_4$ are present, thereby overlooking information from other tokens in the input sequence.
As a result, no matter how many heads there are, one-layer self-attention with the hardmax function cannot distinguish input sequences, e.g., $(\alpha_1 \vv, \alpha_2 \vv, \alpha_4 \vv)$ and $(\alpha_1 \vv, \alpha_3 \vv, \alpha_4 \vv)$.
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{thm:softmax_is_contextual_mapping}}{}} \label{sec: proof_of_contextual_mapping}
\begin{proof}[Proof of \cref{thm:softmax_is_contextual_mapping}]
    Recall that a softmax-based self-attention function $\mathcal{F}^{(SA)}_S:\R^{d \times n} \to \R^{d \times n}$ with $h = 1$ is defined as
    \begin{align}
        \mathcal{F}^{(SA)}_{S}\left(\mZ\right)
        = \mZ + \mW^{(O)}\left(\mW^{(V)}\mZ\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mZ\right)^\top
        \left(\mW^{(Q)}\mZ\right)
        \right],
    \end{align}
    where $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)},\mW^{(K)},\mW^{(Q)} \in R^{s \times d}$ are weight matrices.
    
    We construct a softmax-based self-attention function $\mathcal{F}^{(SA)}_S$ with the property that
    \begin{align}
        \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| < \frac{\epsilon}{4}
    \end{align}
    holds for any input sequence $i \in [N]$ and $k \in [n]$.
    When this property is fulfilled, it is easy to show that
    \begin{align}
        \left\|\mathcal{F}^{(SA)}_{S}\left(\mX^{(i)}\right)_{:,k}\right\|
        &\leq
        \left\|\mX^{(i)}_{:,k}\right\|
        + \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \nonumber \\
        &< r_{\max} + \frac{\epsilon}{4} \label{eq:contextual_mapping_condition1}
    \end{align}
    holds for any $i \in [N]$ and $k \in [n]$, and also
    \begin{align}
        &\left\|
        \mathcal{F}^{(SA)}_{S}\left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_{S}\left(\mX^{(j)}\right)_{:,l}
        \right\| \nonumber\\
        &\geq \left\|\mX^{(i)}_{:,k} - \mX^{(j)}_{:,l}\right\|
        - \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \nonumber\\
        &\quad\quad\quad\quad -
        \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(j)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(j)}\right)^\top
        \left(\mW^{(Q)}\mX^{(j)}_{:,l}\right)
        \right]
        \right\| \nonumber\\
        &> \epsilon - \frac{\epsilon}{4} - \frac{\epsilon}{4} 
        = \frac{\epsilon}{2} \label{eq:contextual_mapping_condition2}
    \end{align}
    for any $i,j \in [N]$ and $k,l \in [n]$ such that $\mX^{(i)}_{:,k} \neq \mX^{(j)}_{:,l}$. So all that remains to prove is to construct a self-attention function $\mathcal{F}^{(SA)}$ that has the properties described above and can also distinguish input tokens $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ such that $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.
    
    Let $\delta = \max(2 + e, 6\log n)$ and fix any vectors $\vu,\vu' \in \R^s$ with 
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}. \label{eq:setting_of_u}
    \end{align}
    Then, according to \cref{lem:attention_projection} with $\delta = \max(2 + e, 6\log n)$, we see that there exists a unit vector $\vv \in \R^d$ such that 
    \begin{gather}
        \left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right|
        > \delta, \label{eq:attention_projection_delta} \\
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\| \label{eq:attention_projection_min_max}
    \end{gather}
    for any $\vv_a,\vv_b,\vv_c \in \mathcal{V}$ with $\vv_a \neq \vv_b$, where $\mW^{(K)} = \vu \vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu' \vv^\top \in \R^{s \times d}$.

    Furthermore, we configure $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)} \in \R^{s \times d}$ to be $\mW^{(V)} = \vu'' \vv^\top$ for any nonzero vector $\vu'' \in \R^s$ such that 
    \begin{align}
        \left\|\mW^{(O)} \vu''\right\|
        = \frac{\epsilon}{4 r_{\max}} \label{eq:condition_on_w_o}
    \end{align}
    holds. This can be accomplished, e.g., $\mW^{(O)} = \vu''' \vu''^\top$ for any vector $\vu''' \in \R^d$ which satisfies $\left\|\vu'''\right\| = \epsilon/(4r_{\max}\left\|\vu''\right\|^2)$.
    In this case, the value of the self-attention without a skip-connection is upper-bounded by
    \begin{align*}
        &\left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \\
        &= \left\|
        \sum_{k'=1}^n s_{k'}^k\mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\|
        \quad\text{with } s^k_{k'} = \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]_{k'} \\
        &\leq \sum_{k'=1}^n s_{k'}^k\left\|
         \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\| \\
        &\leq \max_{k' \in [n]} \left\|
         \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\| \quad (\text{from} \sum_{k'=1}^n s^k_{k'} = 1) \\
        &=  \max_{k' \in [n]} \left\|\mW^{(O)} \vu'' \vv^\top \mX^{(i)}_{:,k'}\right\|\\
        &=  \left\|\mW^{(O)} \vu''\right\| \cdot \max_{k' \in [n]} \left| \vv^\top \mX^{(i)}_{:,k'}\right|\\
        &\leq \frac{\epsilon}{4 r_{\max}} \cdot 
        \max_{k' \in [n]} \left\|\mX^{(i)}_{:,k'}\right\|
        \quad (\text{from \eqref{eq:attention_projection_min_max} and \eqref{eq:condition_on_w_o}}) \numberthis \label{eq:output_norm_of_self_attention}\\
        &< \frac{\epsilon}{4}, \numberthis 
    \end{align*}
    which means that \eqref{eq:contextual_mapping_condition1} and \eqref{eq:contextual_mapping_condition2} are satisfied with the weight matrices defined above.

    Now, we see that the weight matrices $\mW^{(O)},\mW^{(V)},\mW^{(K)},\mW^{(Q)}$ configured above can distinguish the most subtle pattern of input tokens, i.e. $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ with $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.

    Pick up any $i,j \in [N]$ and $k,l \in [n]$ such that $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ and $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$. In addition, we define $\va^{(i)}, \va^{(j)}$ by
    \begin{align}
        \va^{(i)}
        &= \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right) \in \R^n, \\
        \va^{(j)}
        &= \left(\mW^{(K)}\mX^{(j)}\right)^\top
        \left(\mW^{(Q)}\mX^{(j)}_{:,l}\right) \in \R^n.
    \end{align}
    Then, \eqref{eq:attention_projection_delta} and \eqref{eq:attention_projection_min_max} imply that $\va^{(i)}$ and $\va^{(j)}$ are tokenwise $(r,\delta)$-separated, where $r$ is defined by
    \begin{align}
        r = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta r_{\max}^2}{\epsilon r_{\min}},
    \end{align}
    because for any $k' \in [n]$, we have
    \begin{align*}
        \left|a^{(i)}_{k'}\right|
        &= \left|\left(\mW^{(K)}\mX^{(i)}_{:,k'}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)\right| \\
        &= \left|\left(\vv^\top \mX^{(i)}_{:,k'}\right)^\top \right|
        \cdot
        \left|\vu^\top \vu'^\top \right|
        \cdot
        \left| \left(\vv^\top \mX^{(i)}_{:,k}\right)\right| \\
        &\leq \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}r_{\max}^2 \quad (\text{from \eqref{eq:setting_of_u} and \eqref{eq:attention_projection_min_max}}), \numberthis
    \end{align*}
    and the same upper-bound also holds for $\va^{(j)}$.

    Since $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$ and there exists no duplicate token in $\mX^{(i)}$ and $\mX^{(j)}$ respectively, it follows from \cref{lem: boltzmann_separation} that
    \begin{align}
        \left|\boltz(\va^{(i)}) - \boltz(\va^{(j)})\right|
        &> \delta'
        = \frac{\log n}{2}e^{-2r},
    \end{align}
    that is,
    \begin{align}
        \left|
        \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
        -
        \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
        \right| > \delta'. \label{eq:a_i_minus_a_j}
    \end{align}
    Since $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ by assumption, \eqref{eq:a_i_minus_a_j} are further expanded as
    \begin{align*}
        \delta'
        &< \left|
        \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
        -
        \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &= \left|
        \left(\mX^{(i)}_{:,k}\right)^\top \left(\mW^{(Q)}\right)^\top \mW^{(K)}
        \left(
        \mX^{(i)}
        \sigma_S\left[\va^{(i)}\right]
        -
        \mX^{(j)}
        \sigma_S\left[\va^{(j)}\right]
        \right)
        \right| \\
        &= \left|
        \left(\mX^{(i)}_{:,k}\right)^\top \vv \vu'^\top \vu \vv^\top
        \left(
        \mX^{(i)}
        \sigma_S\left[\va^{(i)}\right]
        -
        \mX^{(j)}
        \sigma_S\left[\va^{(j)}\right]
        \right)
        \right| \\
        &= \left|\vv^\top \mX^{(i)}_{:,k}\right|
        \cdot
        \left|\vu^\top \vu'\right|
        \cdot
        \left|
        \left(\vv^\top\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &\leq r_{\max}
        \cdot
        \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}
        \cdot
        \left|
        \left(\vv^\top\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right|, \numberthis
    \end{align*}
    where the last inequality follows from \eqref{eq:setting_of_u} and \eqref{eq:attention_projection_min_max}.

    Therefore, the gap between the outputs of th self-attention function for $\mX^{(i)}$ and $\mX^{(j)}$ are lower-bounded as follows:
    \begin{align*}
        &\left\|
        \mathcal{F}^{(SA)}_S \left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_S \left(\mX^{(j)}\right)_{:,l}
        \right\| \\
        &= \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \mW^{(O)}\left(\mW^{(V)}\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right\|
        \quad (\because \mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}) \\
        &= \left\|\mW^{(O)} \vu''\right\| \cdot 
        \left|
        \left(\vv^\top \mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top \mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &> \frac{\epsilon}{4r_{\max}} \cdot \frac{\delta'}{(|\mathcal{V}|+1)^4} \frac{8\epsilon r_{\min}}{\pi d \delta r_{\max}}, \numberthis
    \end{align*}
    where $\delta$ and $\delta'$ are defined respectively as
    \begin{align}
        \delta
        &= \max \left(2+e, 6\log n\right), \\
        \delta'
        &= \frac{\log n}{2} e^{-2r} \quad \text{with} \quad r = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta r_{\max}^2}{\epsilon r_{\min}}.
    \end{align}
    By plugging $\delta$ and $\delta'$, the above inequality is simplified as
    \begin{align*}
        &\left\|
        \mathcal{F}^{(SA)}_S \left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_S \left(\mX^{(j)}\right)_{:,l}
        \right\| \\
        &> \frac{\epsilon r_{\min} \log n}{r_{\max}^2 (|\mathcal{V}|+1)^4 \pi d \cdot \max \left(2+e, 6\log n\right)} \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{\pi d r_{\max}^2 \cdot \max \left(2+e, 6\log n\right)}{4\epsilon r_{\min}}\right). \numberthis
    \end{align*}
\end{proof} 

\subsection{\texorpdfstring{Proof of \cref{cor:memorization_of_one_layer_Transformer}}{}}
\begin{proof}
    According to \cref{thm:softmax_is_contextual_mapping}, we can construct such self-attention to be contextual mapping, that is,
    there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{V}, \mW^{K}, \mW^{Q} \in \R^{s \times d}$ such that $\mathcal{F}^{(SA)}_S$ with $h=1$ is a $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)},\dots,\mX^{(N)}$ with $r$ and $\delta$ defined by
    \begin{align*}
        r &= r_{\max} + \frac{\epsilon}{4}, \numberthis\\
        \delta &= \frac{\epsilon r_{\min} \log n}{r_{\max}^2 (|\mathcal{V}|+1)^4 \pi d \cdot \max \left(2+e, 6\log n\right)} \\
        &\quad\quad\quad\quad
        \cdot \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{\pi d r_{\max}^2 \cdot \max \left(2+e, 6\log n\right)}{4\epsilon r_{\min}}\right). \numberthis
    \end{align*}
    So what remains to do is to associate each context id with the corresponding output label using a feed-forward neural network $\mathcal{F}^{(FF)}$.
    Construction of such a network is a typical memorization task of a one-hidden-layer feed-forward neural network. See \citet{zhang_understanding_2016}, for example. 
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{cor:memorization_of_one_layer_Transformer_with_positional_encodings}}{}}
\begin{proof}
First, we define the positional encoding matrix $\mE \in \R^{d \times n}$ as follows:
\begin{align*}
    \mE
    =
    \left(
    \begin{array}{cccc}
    2r_{\max} & 4r_{\max} & \dots & 2nr_{\max} \\
    \vdots & \vdots & \ddots & \vdots \\
    2r_{\max} & 4r_{\max} & \dots & 2nr_{\max}
    \end{array}
    \right). \numberthis
\end{align*}
Then, $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ are tokenwise $(r_{\max}, (2n+1)r_{\max}, \epsilon)$-separated, and each sentence has no duplicate token.

From \cref{thm:softmax_is_contextual_mapping},
there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{V}, \mW^{K}, \mW^{Q} \in \R^{s \times d}$ such that $\mathcal{F}^{(SA)}_S$ with $h=1$ is a $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ with $r$ and $\delta$ defined by
\begin{align*}
    r &= (2n+1)r_{\max} + \frac{\epsilon}{4}, \numberthis\\
    \delta
    &= \frac{\epsilon \log n}{(2n+1)^ 2r_{\max} (nN+1)^4 \pi d \cdot \max \left(2+e, 6\log n\right)} \\
    &\quad\quad\quad\quad
    \cdot
    \exp \left(-(2n+1)^2\left(nN+1\right)^4
        \frac{\pi d r_{\max} \cdot \max \left(2+e, 6\log n\right)}{4\epsilon}\right), \numberthis
\end{align*}
because the size of the vocabulary set of $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ is at most $nN$.
Hence, hereafter we do the same thing as in the proof of \cref{cor:memorization_of_one_layer_Transformer}, that is, implementing a feed-forward neural network $\mathcal{F}^{(FF)}$ which associates each context id with the corresponding label. 
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{pro:Transformers_are_universal_approximator}}{}}
\begin{proof}
We show the proposition by the same steps as in \citet{yun_are_2023}. Namely,
\begin{enumerate}
    \item First, given a permutation equivariant continuous function $f \in \mathcal{F}_{\mathrm{PE}}$ defined on a compact set, it follows from typical analysis that $f$ can be approximated by a step function with arbitrary precision in terms of $p$-norm. Therefore, to show a universal approximation theorem, it is sufficient to show that such a step function can be approximated by a Transformer with one self-attention layer.

    \item Second, we use a first feed-forward neural network layer $\mathcal{F}^{(FF)}_1$ to quantize the input domain, reducing the problem to memorization of finite samples.

    \item Then, by a similar analysis as in \cref{cor:memorization_of_one_layer_Transformer}, it can be shown that a combination of the self-attention layer $\mathcal{F}^{(SA)}$ and $\mathcal{F}^{(FF)}_2$ can memorize the step function almost everywhere, in the sense that quantized input domains corresponding to sentences with duplicate tokens are negligibly small. 
\end{enumerate}
We hereafter provide rough proofs of the three steps outlined above.

First, without loss of generality, we ignore skip-connections in $\mathcal{F}^{(FF)}_1$ and $\mathcal{F}^{(FF)}_2$.
\begin{enumerate}
    \item Since $f$ is a continuous function on a compact set, $f$ has maximum and minimum values on the domain.
    By scaling with $\mathcal{F}^{(FF)}_1$ and $\mathcal{F}^{(FF)}_2$, $f$ is assumed to be normalized without loss of generality: for any $\mZ \in \R^{d \times n} \setminus [0,1]^{d \times n}$
    \begin{align}
        f(\mZ) = 0,
    \end{align}
    and for any $\mZ \in [-1,1]^{d\ \times n}$
    \begin{align*}
        -1 \leq f(\mZ) \leq 1. \numberthis
    \end{align*}
    Let $D \in \mathbb{N}$ be the granularity of a grid
    \begin{align}
        \mathbb{G}_D
        = \{1/D, 2/D, \dots, 1\}^{d \times n} \subset \R^{d \times n}
    \end{align}
    such that a piece-wise constant approximation
    \begin{align*}
        \overline{f}(\mZ)
        = \sum_{\mL \in \mathbb{G}_D} f\left(\mL\right) \1_{\mZ \in \mL + [-1/D,0)^{d \times n}} \numberthis
    \end{align*}
    satisfies 
    \begin{align}
        \dist_p(f, \overline{f}) < \epsilon/3.
        \label{eq:estimate_of_step_function}
    \end{align}
    Such a $D$ always exists because of uniform continuity of $f$.

    \item We use $\mathcal{F}^{(FF)}_1$ to quantize the input domain into $\mathbb{G}_D$.

    For any small $\delta > 0$, the following $\delta$-approximated step function can be constructed with one-hidden-layer feed-forward neural network: for any $z \in \R$
    \begin{align}
        \frac{
        \sigma_R\left[z/\delta\right]
        -
        \sigma_R\left[z/\delta - 1\right]
        }{D}
        = \begin{cases}
            0 & z < 0 \\
            z/\delta D & 0 \leq z < \delta \\
            1/D & \delta \leq z
        \end{cases}.
    \end{align}
    By shifting and stacking this step function, we have an approximated multiple-step function
    \begin{align*}
        &\sum_{t = 0}^{D-1} \frac{\sigma_R\left[z/\delta-t/\delta D\right]
        -
        \sigma_R\left[z/\delta - 1-t/\delta D\right]
        }{D}\\
        &\approx \quant_D(z)
        =
        \begin{cases}
            0 & z < 0 \\
            1/D & 0\leq z < 1/D \\
            \vdots & \vdots \\
            1 & 1-1/D \leq z
        \end{cases}, \numberthis
    \end{align*}
    and subtracting the last step function from it,
    \begin{align*}
        \sum_{t = 1}^{D} \frac{\sigma_R\left[z/\delta-t/\delta D\right]
        -
        \sigma_R\left[z/\delta - 1-t/\delta D\right]
        }{D}
        - \left(\sigma_R\left[z/\delta - 1/\delta\right]
        -
        \sigma_R\left[z/\delta - 1 - 1/\delta\right]\right)
    \end{align*}
    approximately quantize $[0,1]$ into $\{1/D,\dots,1\}$, while it projects $\R \setminus [0,1]$ to $0$.

    These operations can be realized by one-hidden-layer neural network, and it is straightforward to approximate its extension $\quant_D$ to dimension $d \times n$, which we denote $\quant_D^{d \times n}: \R^{d \times n} \to \R^{d \times n}$.
    
    In addition to that, we also add a penalty term, with which we identify whether an input sequence is in $[0,1]^{d \times n}$ or not.
    This is defined by
    \begin{align*}
        &
        \sigma_R\left[(z-1)/\delta\right]
        -
        \sigma_R\left[(z-1)/\delta - 1\right]
        -
        \sigma_R\left[-z/\delta\right]
        -
        \sigma_R\left[-z/\delta - 1\right] \\
        &\approx\penal(z)
        =
        \begin{cases}
            -1 & z \leq 0 \\
            0 & 0 < z \leq 1 \\
            -1 & 1 < z
        \end{cases}, \numberthis
    \end{align*}
    which can also be implemented by one-hidden-layer feed-forward neural network.

    Combining these components together, the first feed-forward neural network layer $\mathcal{F}^{(FF)}_1$ approximates the following function:
    \begin{align*}
        \overline{\mathcal{F}}^{(FF)}_1(\mZ) 
        = \quant_D^{d \times n}(\mZ)
        + \sum_{t=1}^d \sum_{k=1}^n \penal(\mZ_{t,k}) \numberthis
    \end{align*} 
    Note that this function quantizes inputs in $[0,1]^{d \times n}$ with granularity $D$, while every element of the output is non-positive for inputs outside $[0,1]^{d \times n}$.
    In particular, the norm of the output is upper-bounded by
    \begin{align}
        \max_{\mZ \in \R^{d \times n}}\left\|\mathcal{F}^{(FF)}_1(\mZ)_{:,k}\right\|
        = dn \cdot \sqrt{d} \label{eq:maximum_norm_of_quantization}
    \end{align}
    for any $k \in [n]$.

    \item Let $\tilde{\mathbb{G}}_D \subset \mathbb{G}_D$ be a sub-grid
    \begin{align}
        \tilde{\mathbb{G}}_D
        = \left\{
        \mL \in \mathbb{G}_D
        \relmiddle|
        \forall k,l \in [n],\ \mL_{:,k} \neq \mL_{:,l}
        \right\},
    \end{align}
    and consider memorization of $\tilde{\mathbb{G}}_D$ with its labels given by $f(\mL)$ for each $\mL \in \tilde{\mathbb{G}}_D$.
    Note that the label sets are consistent because $f$ is a permutation equivariant function.
    Then, \cref{thm:softmax_is_contextual_mapping} allows us to construct a self-attention $\mathcal{F}^{(SA)}$ to be a contextual mapping for such input sequences, because $ \tilde{\mathbb{G}}_D$ can be regarded as tokenwise $(1/D, \sqrt{d}, 1/D)$-separated input sequences, each of which has no duplicate token by definition. The idea is that when the granularity $D$ of $\mathbb{G}_D$ is sufficiently large, the number of cells with duplicate tokens, that is, $|\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D|$ is negligible compared to the total number $|\mathbb{G}_D|$ of cells, and thus the memorization of $\tilde{\mathbb{G}}_D$ suffices for universal approximation theorem.

    From the way the self-attention $\mathcal{F}^{(SA)}$ is constructed, we have
    \begin{align}
        \left\|\mathcal{F}^{(SA)}_S(\mZ)_{:,k} - \mZ_{:,k}\right\|
        < \frac{1}{4\sqrt{d}D}\max_{k' \in [n]}\left\|\mZ_{:,k'}\right\|
    \end{align}
    for any $k \in [n]$ and $\mZ \in \R^{d \times n}$.
    This follows from the fact that  $\mX^{(i)}$ in \cref{eq:output_norm_of_self_attention} may actually be replaced with any input sequence $\mZ$, because $\vv$ in \cref{eq:output_norm_of_self_attention} is a unit vector.
    In particular, combining this upper-bound with \eqref{eq:maximum_norm_of_quantization}, we have
    \begin{align*}
        \left\|\mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ_{:,k}\right) - \mathcal{F}^{(FF)}\left(\mZ_{:,k}\right)\right\|
        < \frac{dn}{4D}. \numberthis
    \end{align*}
    Thus, if we take large enough $D$, every element of the output for $\mZ \in \R^{d \times n} \setminus [0,1]^{d \times n}$ is upper-bounded by
    \begin{align}
        \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ\right)_{t,k} < \frac{1}{4D}
        \quad (\forall t \in [d],\ k \in [n]),
    \end{align}
    while the output for $\mZ \in [0,1]^{d \times n}$ is lower-bounded by
    \begin{align}
        \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ\right)_{t,k}
        > \frac{3}{4D}
        \quad (\forall t \in [d],\ k \in [n]).
    \end{align}
    
    Therefore, what remains to show is construct a feed-forward neural network $\mathcal{F}^{(FF)}_2$ which associates the context id of each $\mL \in \tilde{\mathbb{G}}_D \subset (3/4D, \infty)^{d \times n}$ to its corresponding label, while it outputs $0$ for any input matrix $\mZ \in (-\infty, 1/4D)^{d \times n}$.
    This can be accomplished by usual bump-functions.
    Precisely, construct a bump function of scale $R > 0$
    \begin{align}
        \bump_R(\mZ)
        &=\frac{f(\mL)}{dn}\sum_{t=1}^d\sum_{k=1}^n (\sigma_R\left[R(Z_{t,k}-L_{t,k})-1\right]
        - \sigma_R\left[R(Z_{t, k}-L_{t,k})\right] \\
        &\quad\quad\quad\quad\quad\quad\quad\quad
        + \sigma_R\left[R(Z_{t,k}-L_{t,k})+1\right])
    \end{align}
    for each input sequence $\mL \in \tilde{\mathbb{G}}_D$ and add up these functions to implement $\mathcal{F}^{(FF)}_2$.

    For large enough $R > 0$, $\mathcal{F}^{(FF)}_2$ maps each input sequence $\mL \in \tilde{\mathbb{G}}_D$ to its labels $f(\mL)$ and $\mZ \in (-\infty, 1/4D)^{d \times n}$ to 0.
    In addition, the value of $\mathcal{F}^{(FF)}_2$ is always bounded: $0 \leq \mathcal{F}^{(FF)}_2 \leq 1$. Thus, by taking sufficiently small $\delta > 0$, we have
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1, \mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1\right) < \frac{\epsilon}{3}.
        \label{eq:estimate_of_quantization}
    \end{align}
    Lastly, there are $|\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D| = O\left(D^{-d}\left|\mathbb{G}_D\right|\right)$ input sequences with duplicate tokens, each corresponding to a cell of area $D^{-d}$. Thus, by taking sufficiently large $D$, areas of $\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D$ becomes negligible and
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1,
        \overline{f}\right) < \frac{\epsilon}{3}. \label{eq:estimate_of_duplicate_area}
    \end{align}
    Combining \eqref{eq:estimate_of_step_function}, \eqref{eq:estimate_of_quantization} and \eqref{eq:estimate_of_duplicate_area} together, we get the approximation error of the Transformer:
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1,
        f\right) < \epsilon. 
    \end{align}
\end{enumerate}
\end{proof}

\section{Technical Lemmas}
We cite Lemma 13 in \citet{park_provable_2021}, with which we configure weight matrices of a self-attention mechanism.
\begin{lem}[\citet{park_provable_2021}]\label{lem:projection_into_scalar}
Let $d \in \mathbb{N}$. Then, for any finite subset $\mathcal{X} \subset \R^d$, there exists a unit vector $\vv \in \R^d$ such that
\begin{align}\label{eq:projection_into_scalar}
    \frac{1}{\left|\mathcal{X}\right|^2}\sqrt{\frac{8}{\pi d}} \left\|\vx - \vx'\right\|
    \leq \left|\vv^\top \left(\vx - \vx'\right)\right|
    \leq \left\|\vx - \vx'\right\|
\end{align}
holds for any $\vx,\vx' \in \mathcal{X}$.
\end{lem}

The following lemma follows immediately from \cref{lem:projection_into_scalar}. We use $\mW^{(K)}$ and $\mW^{(Q)}$ in the lemma as low-rank weight matrices.\footnote{It is easy to show that different unit vectors $\vv, \vv' \in \R^d$ may be used for $\mW^{(K)}$ and $\mW^{(Q)}$, respectively, that is, $\mW^{(K)} = \vu \vv^\top$ and $\mW^{(Q)} = \vu' {\vv'}^\top$, as long as $\vv$ and $\vv'$ satisfy \cref{eq:projection_into_scalar}.}
\begin{lem}\label{lem:attention_projection}
    Given $(r_{\min},r_{\max},\epsilon)$-separated finite vocabulary $\mathcal{V} \subset \R^{d}$ with $r_{\min} > 0$. Then, for any $\delta > 0$, there exists a unit vector $\vv \in \R^d$ such that for any vectors $\vu,\vu' \in \R^s$ with 
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}},
    \end{align}
    we have
    \begin{gather}
        \left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right|
        > \delta, \\
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\|
    \end{gather}
    for any $\vv_a, \vv_b, \vv_c \in \mathcal{V}$ with $\vv_a \neq \vv_b$, where $\mW^{(K)} = \vu\vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu'\vv^\top \in \R^{s \times d}$.
\begin{comment}
    there exist weight matrices $\mW^{(K)}, \mW^{(Q)} \in \R^{1 \times d}$ with $\mW^{(K)} = \mW^{(Q)}$ such that for any $V_a,V_b,V_c \in \mathcal{V}$ with $V_a \neq V_b$, the following inequalities hold:
    \begin{align*}
        \left|
        \left(\mW^{(K)}V_a\right)^\top
        \left(\mW^{(Q)}V_c\right)
        -
        \left(\mW^{(K)}V_b\right)^\top
        \left(\mW^{(Q)}V_c\right)
        \right|
        > \delta, \\
        \sqrt{\frac{\delta}{\epsilon r_{\min}}}\left\|\vv_c\right\|
        \leq
        \left|\mW^{(K)}\vv_c\right|
        =
        \left|\mW^{(Q)}\vv_c\right|
        \leq (|\mathcal{V}|+1)^2 \sqrt{\frac{\pi d \delta}{8\epsilon r_{\min}}}\left\|\vv_c\right\|.
    \end{align*}
\end{comment}
\end{lem}
\begin{proof}
    By applying \cref{lem:projection_into_scalar} to $\mathcal{V} \cup \{0\}$, we know that there exists a unit vector $\vv \in \R^d$ such that for any $\vv_a,\vv_b \in \mathcal{V} \cup \{0\}$ such that $\vv_a \neq \vv_b$, we have
    \begin{align}
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_a - \vv_b\right\|
        \leq
        \left|\vv^\top \left(\vv_a - \vv_b\right)\right|
        \leq 
        \left\|\vv_a - \vv_b\right\|.
    \end{align}
    In particular, this means that for any $\vv_c \in \mathcal{V}$
    \begin{align}
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\|
    \end{align}
    holds. Thus, pick up arbitrary vectors $\vu, \vu' \in \R^s$ with
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}},
    \end{align}
    and by setting $\mW^{(K)} = \vu\vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu'\vv^\top \in \R^{s \times d}$, we have
    \begin{align*}
        &\left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right| \\
        &=\left|
        \left(\vv_a - \vv_b\right)^\top \left(\mW^{(k)}\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right| \\
        &= \left|\left(\vv_a - \vv_b\right)^\top \vv\right|
        \cdot
        \left|\vu^\top \vu' \right|
        \cdot
        \left|\vv^\top \vv_c\right| \\
        &\geq \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}} \left\|\vv_a - \vv_b\right\|
        \cdot
        \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}
        \cdot
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\| \\
        &> \delta \numberthis,
    \end{align*}
    where the last inequality follows from $(r_{\min}, r_{\max}, \epsilon)$-separatedness of $\mathcal{V}$.
\end{proof}

The following lemma shows that the Boltzmann operator is monotonically decreasing when the maximum and the rest of the arguments are far enough apart.
\begin{lem}[Monotonicity]\label{lem:monotonicity_of_boltzmann}
    Given a sequence $\va = (a_1,\dots,a_n) \in \R^n$ such that for all $k = 2,\dots,n$
    \begin{equation*}
        a_1 - a_k > \delta = \max\left(2,\log n\right). \numberthis
    \end{equation*}
    Then, the derivative of the Boltzmann operator $\boltz\left(\va\right) = \va^\top \sigma_S\left[\va\right]$ with respect to coordinates $k=2,\dots,n$ is negative, that is,
    \begin{equation*}
        \frac{\partial}{\partial a_k} \boltz\left(\va\right)
        < 0. \numberthis
    \end{equation*}
\end{lem}

\begin{proof}
    Fix $k = 2,\dots,n$ and let $\alpha_k$ and $\beta_k$ be
    \begin{align*}
        \alpha_k = \sum_{\substack{l=1 \\ l \neq k}}^n a_l e^{a_l}, \quad
        \beta_k = \sum_{\substack{l=1 \\ l \neq k}}^n e^{a_l}. \numberthis
    \end{align*}
    Then, the derivative of $\va^\top \sigma_S\left[\va\right]$ with respect to $a_i$ can be written as
    \begin{align}\label{eq:derivative_of_boltzmann}
        \frac{\partial}{\partial a_k} \va^\top \sigma_S[\va] 
        &= \frac{\partial}{\partial a_k} \frac{\alpha_k + a_k e^{a_k}}{\beta_k + e^{a_k}} \nonumber\\
        &= \frac{e^{a_k}\left(1+a_k\right)\left(\beta_k + e^{a_k}\right) - \left(\alpha_k + a_ke^{a_k}\right)e^{a_k}}{\left(\beta_k + e^{a_k}\right)^2} \nonumber\\
        &= \frac{e^{a_k}\left(\beta_k + e^{a_k} + a_k \beta_k - \alpha_k\right)}{\left(\beta_k + e^{a_k}\right)^2},
    \end{align}
    and we see that we only need to focus on the sign of $\beta_k + e^{a_k} + a_k \beta_k - \alpha_k$. Expanding $\alpha_k$ and $\beta_k$, it becomes
    \begin{align*}
        \beta_k + e^{a_k} + a_k \beta_k - \alpha_k
        &= \sum_{\substack{l=1 \\ l \neq k}}^n e^{a_l}
        + e^{a_k} 
        + a_k\sum_{\substack{l=1 \\ l \neq k}}^n e^{a_l}
        - \sum_{\substack{l=1 \\ l \neq k}}^n a_l e^{a_l} \\
        &= \sum_{l=1}^n \left(1+a_k-a_l\right) e^{a_l} \\
        &< \left(1+a_k-a_1\right) e^{a_1} + \sum_{l=2}^n \left(1+a_1-\delta-a_l\right) e^{a_l} \quad
        (\because a_k < a_1 - \delta)\\
        &\leq \left(1+a_k-a_1\right) e^{a_1} + (n-1)e^{a_1 - \delta}, \numberthis
    \end{align*}
    where the last inequality follows from the fact 
    $1 + a_1 - \delta - a_j \leq e^{a_1 - \delta - a_l}$ and thus
    \begin{align*}
        \left(1+a_1-\delta-a_l\right) e^{a_l} 
        \leq e^{a_1 - \delta - a_l} e^{a_l} 
        = e^{a_1 - \delta}
        \quad (\forall l=2,\dots,n). \numberthis
    \end{align*}
    The above inequality can further expanded as follows:
    \begin{align*}
        \beta_k + e^{a_k} + a_k \beta_k - \alpha_k
        &< \left(1+a_k-a_1\right) e^{a_1} + (n-1)e^{a_1 - \delta} \\
        &< -e^{a_1} + ne^{a_1-\delta}
        = e^{a_1}\left(-1+ne^{-\delta}\right), \numberthis
    \end{align*}
    because $a_1 - a_k > \delta \geq 2$ by assumption. 
    
    Combining above calculations together, we have
    \begin{align}\label{eq:derivative_of_boltzmann_upper_bound}
        \frac{\partial}{\partial a_k} \va^\top \sigma_S[\va]
        < \frac{e^{a_1 + a_k}}{\left(\beta_k + e^{a_k}\right)^2} \left(-1+ne^{-\delta}\right).
    \end{align}
    Therefore, if $\delta$ is larger than $\log n$, the right-hand side of the above inequality is negative, and the lemma has been proven.
\end{proof}

\begin{lem}[Concavity]\label{lem:concavity_of_boltzmann}
    Let $k \in \mathbb{N}$ and $\va = (a_1,\dots,a_k) \in \R^k$ be a sequence such that for all $l = 2,\dots,k$
    \begin{equation*}
        a_1 - a_l > \delta = \max\left(2,\log n\right). \numberthis
    \end{equation*}
    Then, $\boltz(a_1,\dots,a_k,x)$ is concave with respect to $x < a_1 - \max(2+e,\log n)$.
\end{lem}
\begin{proof}
    Let $\gamma_k$ and $\xi_k$ be
    \begin{align*}
        \gamma_k = \sum_{l=1}^{k} a_l e^{a_l}
        \quad \text{and} \quad
        \xi_k = \sum_{l=1}^{k} e^{a_l}. \numberthis
    \end{align*}
    The derivative of $\boltz\left(a_1, \dots, a_k, x\right)$ with respect to $x < a_1-\delta$ is negative by \cref{lem:monotonicity_of_boltzmann}, and \eqref{eq:derivative_of_boltzmann} implies that its value is 
    \begin{align*}
        \frac{d}{dx} \boltz(a_1,\dots,a_k,x)
        &= \frac{e^x\left(\xi_k + e^x + x \xi_k - \gamma_k\right)}{\left(\xi_k + e^x\right)^2}. \numberthis
    \end{align*}
    Furthermore, its second derivative with respect to $x < a_k-\delta$ is
    \begin{align*}
        &\frac{d^2}{dx^2} \boltz(a_1,\dots,a_k,x) \\
        &= \frac{d}{dx} \frac{e^x\left(\xi_k + e^x + x \xi_k - \gamma_k\right)}{\left(\xi_k + e^x\right)^2} \\
        &= \frac{\left[e^x\left(\xi_k + e^x + x \xi_k - \gamma_k\right) + e^x\left(e^x + \xi_k\right)\right] \cdot \left(\xi_k + e^x\right)^2}{\left(\xi_k + e^x\right)^4} \\
        &\quad\quad
        - \frac{e^x\left(\xi_k + e^x + x \xi_k - \gamma_k\right) \cdot 2e^x\left(\xi_k + e^x\right)}{\left(\xi_k + e^x\right)^4} \\
        &= \frac{e^x}{\left(\xi_k + e^x\right)^3}
        \left[\left(\xi_k + e^x + x \xi_k - \gamma_k\right)\left(\xi_k - e^x\right) + \left(\xi_k + e^x\right)^2\right], \numberthis
    \end{align*}
    and by further expanding the expression within the right-side square brackets, we have
    \begin{align*}
        &\left(\xi_k + e^x + x \xi_k - \gamma_k\right)\left(\xi_k - e^x\right) + \left(\xi_k + e^x\right)^2 \\
        &= \left(\xi_k + e^x + x \xi_k - \gamma_k\right)\xi_k - \left(\xi_k + e^x + x \xi_k - \gamma_k\right)e^x + \left(\xi_k + e^x\right)^2 \\
        &= \left(\xi_k + e^x + x \xi_k - \gamma_k\right)\xi_k - \left(x \xi_k - \gamma_k\right)e^x + \left(\xi_k + e^x\right)\xi_k \\
        &= \left(2\xi_k + 2e^x + x \xi_k - \gamma_k\right)\xi_k + \left(\gamma_k - x \xi_k\right)e^x \\
        &\leq \left(2\xi_k + 2e^x + x \xi_k - \gamma_k\right)\xi_k + (a_1-x) \xi_k e^x
         \\
        &= \left(2\xi_k + (2+a_1-x)e^x + x \xi_k - \gamma_k\right)\xi_k, \numberthis
    \end{align*}
    where the inequality follows from
    \begin{align*}
        \gamma_k - x \xi_k
        = \sum_{l=1}^k a_le^{a_l} - x\xi_k
        \leq a_1\sum_{l=1}^k e^{a_l} - x\xi_k
        = (a_1-x)\xi_k. \numberthis
    \end{align*}
    Hereafter, we focus on the sign of $2\xi_k + (2+a_1-x)e^x + x \xi_k - \gamma_k$. By expanding $\gamma_k$ and $\xi_k$, for any $x < a_k - \delta$ it is upper-bounded as
    \begin{align*}
        2\xi_k + (2+a_1-x)e^x + x \xi_k - \gamma_k
        &= (2+a_1-x)e^x + \sum_{l=1}^k \left(2+x-a_l\right)e^{a_l} \\
        &< (2+a_1-x)e^x + \left(2+x-a_1\right)e^{a_1} \\
        &\leq e\cdot e^{a_1-x} \cdot e^x + \left(2+x-a_1\right)e^{a_1} \\
        &= \left(2+e+x-a_1\right)e^{a_1}, \numberthis
    \end{align*}
    where the first inequality follows from
    \begin{align}
        a_l - x \geq a_k - x > \delta \geq 2
        \quad (\forall l=1,\dots,k),
    \end{align}
    while the second inequality is implied by the fact that $2+a_1-x \leq e^{1+a_1-x}$.

    Combining the above calculations together, the second derivative of the Boltzmann operator can be evaluated as
    \begin{align}
        \frac{d^2}{dx^2} \boltz(a_1,x)
        &\leq \frac{\xi_k e^{a_1+x}}{\left(\xi_k + e^x\right)^3} \cdot (2 + e + x - a_1)
        < 0,
    \end{align}
    by assumption $a_1 - x \geq a_k - x > \delta > 2+e$, and this means that the Boltzmann operator is concave as long as the assumption holds.

\end{proof}

\subsection{\texorpdfstring{Proof of \cref{lem: boltzmann_separation}}{}}
Before moving on to the proof, we first illustrate the proof sketch of \cref{lem: boltzmann_separation} using a simple example.

Since the Boltzmann operator is permutation invariant, without loss of generality we assume the elements of $\va^{(i)}$ and $\va^{(j)}$ are sorted in descending order, e.g., $\va^{(i)} = (8,6,5)$ and $\va^{(j)}=(4,3,1)$.
In this case, since the Boltzmann operator $\boltz$ can be regarded as an approximation of $\max$, we have 
\begin{equation}
    \boltz(\va^{(i)}) \approx 8 > 4 \approx \boltz(\va^{(j)}),
\end{equation}
and so the Boltzmann operator can readily separate these two inputs.

The subtle case is where the initial tokens of $a^{(i)}$ and $a^{(j)}$ are identical, but the rest of each sequences differs, like $\va^{(i)} = (8,6,5)$ and $\va^{(j)}=(8,3,1)$.
However, a closer observation reveals that if the first coordinate and the second one of the input $\va \in \R^n$ are well-separated, then $\boltz(\va)$ is monotonically decreasing for each coordinate $k=2,\dots,n$. In the above example, this intuitively implies
\begin{equation}
    \boltz\left(\va^{(j)}\right)
    \geq
    \boltz\left(\overline{\va}^{(j)}\right)
    \quad \text{with $\overline{\va}^{(j)}=(8,3,3)$}
\end{equation}
and
\begin{equation}
    \boltz\left(\va^{(i)}\right)
    \leq
    \boltz\left(\underline{\va}^{(i)}\right)
    \quad \text{with $\underline{\va}^{(i)}=(8,6,-\infty)$},
\end{equation}
or by abuse of notation, $\boltz\left(\va^{(i)}\right)
    \leq \boltz\left(8,6\right)$.

Thus, if we can show $\boltz\left(\underline{\va}^{(i)}\right) < \boltz\left(\overline{\va}^{(j)}\right)$, then $\boltz\left(\va^{(i)}\right) < \boltz\left(\va^{(j)}\right)$ holds and we know that the Boltzmann operator can also separate these pattern of inputs.
Fortunately, this is indeed the case, when each element of the inputs is sufficiently separated, because
\begin{align*}
    \boltz\left(\overline{\va}^{(j)}\right)
    = \frac{8e^8 + 3e^3 + 3e^3}{e^8 + e^3 + e^3}
    &= \frac{8e^8 + 2\cdot 3e^3}{e^8 + 2\cdot e^3} \\
    &< \frac{8e^8 + (3+\log 2)e^{3+\log 2}}{e^8 + \cdot e^{3+\log 2}}
    = \boltz\left(8, 3+\log 2\right), \numberthis
\end{align*}
and we have $\boltz\left(8,6\right) < \boltz\left(8, 3+\log 2\right)$ by the monotonicity of the Boltzmann operator.

\begin{proof}
    We only have to show the case of $m = 2$, and for notational convenience, $\va^{(1)}$ (resp. $\va^{(2)}$) hereafter is denoted by $\va$ (resp. $\vb$). Also, since the Boltzmann operator is permutation invariant, we assume without loss of generality $a_1 > \cdots > a_n$ and $b_1 > \cdots > b_n$ (Since there is no duplicate token in $\va$, $a_k$ is strictly greater than $a_l$ for any $k < l$. The same holds for $\vb$).
    
    First, since the Boltzmann operator can be regarded as weighting averaging, we have
    \begin{align}
        |\boltz(\va)| \leq \max(|a_1|,|a_n|) \leq r.
    \end{align}
    For $\delta'$-separatedness, we show the lemma by dividing it into the following two cases:
    \begin{enumerate}
        \item $a_1 = b_1$

        If $\va = \vb$ we have nothing to prove. Thus, without loss of generality we assume that there exists $k \in [n-1]$ such that $a_1 = b_1, \dots, a_{k} = b_{k}$ and $a_{k+1} > b_{k+1}$ hold.
        
        Let $\underline{\va}$ and $\overline{\vb}$ be
        \begin{align}
            \underline{\va}
            &= \left(a_1, a_2, \dots, a_k, a_{k+1}\right) \in \R^{k+1} \\
            \overline{\vb}
            &= \left(a_1, a_2, \cdots, a_{k}, b_{k+1}, b_{k+1}, \dots, b_{k+1}\right) \in \R^n.
        \end{align}
        Then, by abuse of notation, \cref{lem:monotonicity_of_boltzmann} implies that
        \begin{align}
            \boltz\left(\va\right)
            < \underline{\va}^\top \sigma_S\left[\underline{\va}\right]
            = \boltz\left(\underline{\va}\right)
            \quad \text{and} \quad
            \boltz\left(\vb\right)
            > \boltz\left(\overline{\vb}\right),
        \end{align}
        and we only have to evaluate the magnitude of the difference $\boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)$.

        Let $\gamma_k$ and $\xi_k$ be
        \begin{align}
            \gamma_k = \sum_{l=1}^{k} a_l e^{a_l}
            \quad \text{and} \quad
            \xi_k = \sum_{l=1}^{k} e^{a_l}.
        \end{align}
        Then, $\boltz\left(\overline{\vb}\right)$ can be decomposed into
        \begin{align*}
            \boltz\left(\overline{\vb}\right)
            &= \frac{\gamma_k + (n-k)b_{k+1}e^{b_{k+1}}}{\xi_k + (n-k)e^{b_{k+1}}} \\
            &= \frac{\gamma_k + b_{k+1}e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1} + \log (n-k)}} \\
            &= \frac{\gamma_k + \left(b_{k+1} + \log (n-k)\right)e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1} + \log (n-k)}}
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}. \numberthis
        \end{align*}
        
        Therefore, the difference $\boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)$ can be written as
        \begin{align}\label{eq:difference_of_boltzmann}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \nonumber\\
            &\quad\quad\quad\quad\quad
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}.
        \end{align}

        
        Since $\boltz(a_1,\dots,a_k,x)$ is a monotone decreasing (\cref{lem:monotonicity_of_boltzmann}) and concave (\cref{lem:concavity_of_boltzmann}) with respect to $x < a_1 - \max(2+e,\log n)$, i.e.,
        \begin{align*}
            &\boltz\left(a_1,\dots,a_k, a^{\prime}\right)+\left(x-a^{\prime}\right) \cdot \left.\frac{d}{dx}\boltz(a_1,\dots,a_k,x)\right|_{a^{\prime}}\\
            &>\boltz\left(a_1,\dots,a_k, x\right), \numberthis
        \end{align*}
        
        the difference $\boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right) - \boltz\left(\underline{\va}\right)$ is lower-bounded as
        \begin{align*}
            &\boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \\
            &> \left(b_{k+1}+\log(n-k) - a_{k+1}\right) \cdot \left.\frac{d}{dx}\boltz(a_1,\dots,a_k,x)\right|_{b_{k+1}+\log(n-k)} \\
            &> \left(\log(n-k) - \delta\right) \cdot \left.\frac{d}{dx}\boltz(a_1,\dots,a_k,x)\right|_{b_{k+1}+\log(n-k)} \\
            &>- \left(\log(n-k) - \delta\right) \cdot \left.\left(-\frac{d}{dx}\boltz(a_1,\dots,a_k,x)\right|_{b_{k+1}+\log(n-k)}\right) \\
            &> \left(\delta-\log(n-k)\right) \cdot(-1)
            \frac{e^{a_1+b_{k+1} + \log (n-k)}}{\left(\xi_k + e^{b_{k+1} + \log (n-k)}\right)^2}\left(-1+ne^{-\delta}\right), \numberthis
        \end{align*}
        where the last inequality follows from \eqref{eq:derivative_of_boltzmann_upper_bound} and $\delta-\log(n-k)>0$. By flipping the signs on the right-hand side, the above inequality is further expanded as
        \begin{align*}
            &\boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \\
            &> \left(\log(n-k) - \delta\right) \cdot
            \frac{e^{a_1+b_{k+1} + \log (n-k)}}{\left(\xi_k + e^{b_{k+1} + \log (n-k)}\right)^2}\left(-1+ne^{-\delta}\right)\\
            &\geq 5 \log n \cdot
            \frac{e^{a_1+b_{k+1} + \log (n-k)}}{\left(\xi_k + e^{b_{k+1} + \log (n-k)}\right)^2} \cdot \left(1 - \frac{1}{n^5}\right)
            \quad \text{from}~\delta \geq 6 \log n \\
            &> 4 \log n \cdot
            \frac{e^{a_1+b_{k+1} + \log (n-k)}}{\left(\xi_k + e^{b_{k+1} + \log (n-k)}\right)^2} \quad \text{from}~n \geq 2. \numberthis
        \end{align*}
        Plugging this result into \eqref{eq:difference_of_boltzmann}, we see 
        \begin{align*}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \\
            &\quad\quad\quad\quad\quad
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &> 4 \log n \cdot
            \frac{e^{a_1+b_{k+1} + \log (n-k)}}{\left(\xi_k + e^{b_{k+1} + \log (n-k)}\right)^2} \\
            &\quad\quad\quad\quad\quad
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &= \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \left[\frac{4\log n \cdot e^{a_1}}{\xi_k + e^{b_{k+1}+\log (n-k)}} - \log (n-k) \right]. \numberthis \label{eq:difference_of_boltz_last}
        \end{align*}
        Lastly, notice that the following inequality follows from $\delta$-separatedness of $\va$ and $\vb$ and the assumption that $\va$ has no duplicate token:
        \begin{align*}
            \xi_k + e^{b_{k+1}+\log (n-k)}
            &< \sum_{l=1}^{k+1} e^{a_l} \quad (\because a_{k+1} > b_{k+1}+\log (n-k))\\
            &< e^{a_1} \sum_{l=1}^{k+1} e^{-(l-1)\delta} \\
            &< 2e^{a_1} \quad (\because \delta > \log 2). \numberthis
        \end{align*}
        By using this inequality twice, \eqref{eq:difference_of_boltz_last} is lower-bounded by
        \begin{align*}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &> \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \left[\frac{4\log n \cdot e^{a_1}}{\xi_k + e^{b_{k+1}+\log (n-k)}} - \log (n-k) \right] \\
            &> \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \left[\frac{4\log n \cdot e^{a_1}}{2e^{a_1}} - \log n\right] \\
            &= \log n \cdot \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &> \log n \cdot \frac{e^{b_{k+1}}}{2e^{a_1}} \\
            &> \frac{\log n}{2} \cdot e^{-2r}
            \quad \text{from}~-r \leq b_{k+1} < a_1 \leq r. \numberthis
        \end{align*}

        \item $a_1 \neq b_1$

        Without loss of generality, we assume $a_1 > b_1$.

        Since the Boltzmann operator can be regarded as weighted averaging, $\boltz(\vb) \leq b_1$ always holds. Thus, it is enough to evaluate how much greater $\boltz(\va)$ is than $b_1$.

        Let $\overline{\va} \in \R^n$ be
        \begin{align*}
            \overline{\va}
            =
            (a_1, a_1-\delta, \dots, a_1-\delta). \numberthis
        \end{align*}
        Then, $\boltz(\va) > \boltz(\overline{\va})$ follows from \cref{lem:monotonicity_of_boltzmann}, and its value is
        \begin{align*}
            \boltz\left(\overline{\va}\right)
            &= \frac{a_1e^{a_1} + (n-1)(a_1-\delta)e^{a_1-\delta}}{e^{a_1} + (n-1)e^{a_1-\delta}} \\
            &= \frac{a_1 + (n-1)(a_1-\delta)e^{-\delta}}{1 + (n-1)e^{-\delta}} \\
            &= a_1 - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}}. \numberthis
        \end{align*}
        Therefore, the difference $\boltz\left(\overline{\va}\right) - \boltz\left(\overline{\vb}\right)$ is
        \begin{align*}
            \boltz\left(\overline{\va}\right) - \boltz\left(\overline{\vb}\right)
            &> \boltz\left(\overline{\va}\right) - b_1 \\
            &> a_1 - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}}
            - (a_1 - \delta) \\
            &= \delta - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}} \\
            &= \frac{\delta}{1 + (n-1)e^{-\delta}} \\
            &\geq 3\log n \quad (\because \delta \geq 6\log n), \numberthis
        \end{align*}
        and the right-hand side is greater than $\frac{\log n}{2}e^{-2r}$.
    \end{enumerate}
\end{proof}


\end{document}
