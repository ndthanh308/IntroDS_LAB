
\documentclass{article} % For LaTeX2e

\usepackage{natbib}
\usepackage[margin=40truemm]{geometry}


% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{graphicx}
\usepackage{caption}
\usepackage[skip=0cm,list=true,labelfont=bf]{subcaption}

\usepackage{amssymb}

\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{cleveref}
\usepackage{url}
\usepackage[whole]{bxcjkjatype}

\usepackage{amsthm}

\usepackage{nomencl}
\makenomenclature

\usepackage{etoolbox}
\renewcommand\nomgroup[1]{%
  \item[\bfseries
  \ifstrequal{#1}{A}{Numbers and Arrays}{%
  \ifstrequal{#1}{B}{Sets}{%
  \ifstrequal{#1}{C}{Indexing}{
  \ifstrequal{#1}{F}{Functions}{}}}}%
]}


\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{ass}{Assumption}
\newtheorem{pro}{Proposition}


\theoremstyle{definition}
\newtheorem{dfn}{Definition}

\theoremstyle{remark}
\newtheorem{rem}{Remark}

\crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\crefname{cor}{Corollary}{Corollaries}
\crefname{pro}{Proposition}{Propositions}

\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}


\DeclareMathOperator{\boltz}{\mathbf{boltz}}
\DeclareMathOperator{\quant}{\mathbf{quant}}
\DeclareMathOperator{\penal}{\mathbf{penalty}}
\DeclareMathOperator{\bump}{\mathbf{bump}}
\DeclareMathOperator{\dist}{\mathbf{d}}
\date{}

\title{Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?}

\usepackage{authblk}
\author[1]{Tokio Kajitsuka\thanks{kajitsuka-tokio@g.ecc.u-tokyo.ac.jp}}
\author[1]{Issei Sato \thanks{sato@g.ecc.u-tokyo.ac.jp}}
\affil[1]{Department of Computer Science, The University of Tokyo}


\begin{document}

\maketitle

\begin{abstract}
Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. 
This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function.
By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.
As a consequence, we show that one-layer and single-head Transformers have a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous permutation equivariant functions on a compact domain.
\end{abstract}

\section{Introduction}
The Transformer model has been ubiquitously used in deep learning since its proposal by \citet{vaswani_attention_2017}.  
Its widespread application spans several domains, not only revolutionizing Natural Language Processing (NLP) through models like BERT \citep{devlin_bert_2019, liu_roberta_2019} and GPT \citep{brown_language_2020, radford_improving_nodate, radford_language_nodate} but also making significant advancements in image and graph processing as an alternative to conventional models like convolutional neural networks (CNNs) and graph neural networks (GNNs) \citep{dosovitskiy_image_2022, ying_Transformers_2022}.

One of the key reasons behind the success of the Transformer model is its ability to represent a wide range of functions. Various studies investigated this aspect, including the universal approximation theorem for Transformer models and its memorization capacity \citet{yun_are_2023, kim_provable_2023, mahdavi_memorization_2023, edelman_inductive_2022, 
gurevych_rate_2021,
takakura_approximation_2023,
likhosherstov_expressive_2023}.

The main challenge in proving universal approximation theorems for Transformer models lies in the fact that the Transformer needs to account for the context of the entire input sequence. Unlike feed-forward neural networks where each input is processed independently, the self-attention mechanism in Transformer models must take into account the dependencies between all elements in each input sequence. In constructive proofs \citep{edelman_inductive_2022, yun_are_2023,
kim_provable_2023,
mahdavi_memorization_2023,
gurevych_rate_2021,
takakura_approximation_2023}, these dependencies are often aggregated into a token-wise quantity, which we call a ``context id'' here, by a self-attention mechanism, and then feed-forward neural networks map each context id to the desired output.

The drawback of existing analyses is that they require excessively deep layers \citep{yun_are_2023, kim_provable_2023} or quite a lot of attention heads \citep{gurevych_rate_2021,
takakura_approximation_2023,
likhosherstov_expressive_2023} for data memorization, which leads to a discrepancy with Transformers being deployed in practice.
This discrepancy primarily arises from the interpretation of the softmax function as an approximation of the hardmax function. Consequently, to compute the ``context id'' within the self-attention mechanism, the number of required self-attention parameters scales linearly with the length of an input sequence.

In this work, we address this gap by closely examining the softmax function itself. First, we show that it is impossible to output the ``context id'' using just one layer of self-attention with the hardmax function. At the same time, we demonstrate that just one layer of single-head and softmax-based self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence.
This result implies that the Transformer with one self-attention layer is a universal approximator of continuous permutation equivariant functions by using two feed-forward neural networks connected before and after the single-head self-attention mechanism with an arbitrary head size.

Our contributions are summarized as follows.
\begin{enumerate}
    \item We show that one layer self-attention with the hardmax function is not a contextual mapping; that is, one layer hardmax-based Transformer has no memorization capacity.

    \item In contrast, 
    we provide a framework for constructing a context mapping with one-layer and single-head self-attention using the softmax function.

    \item We prove that one layer Transformer has a memorization capacity for finite samples, and Transformers with one-layer and single-head self-attention are universal approximators of continuous permutation equivariant functions.
\end{enumerate}

\subsection{Related Works}
\textbf{Universal approximation theorems.} 
The history of the universal approximation theorem begins around 1990 \citep{cybenko_approximation_1989, carroll_construction_1989, hornik_approximation_1991,funahashi_on_1989}.
Recent studies on this topic include analyses of how network width and depth affect the expressive power \citep{lu_expressive_2017}, and analyses for specific architectures \citep{lin_resnet_2018}.
There have also been analyses of the memorization capacity of models \citep{baum_capabilities_1988, huang_upper_1998}.
The main focus 
of the memorization capacity is mainly on the analysis of parameter efficiency for storing finite samples \citep{huang_learning_2003, vershynin_memory_2020, park_provable_2021, vardi_optimal_2022, yun_small_2019, bubeck_network_2020, hardt_identity_2016, rajput_exponential_2021, zhang_understanding_2016}.
Notably, \citet{zhang_understanding_2016} demonstrated that a neural network of the size used in practice can perfectly memorize a randomly labeled data set.
\citet{belkin_reconciling_2019, nakkiran_deep_2019} pointed out that the minimum number of parameters required to memorize a dataset is related to the double descent threshold.

\textbf{Expressive capacity of Transformer.} 
Ever since \citet{vaswani_attention_2017} first proposed the Transformer architecture, there have been various theoretical analyses on its expressive capacity.
\citet{yun_are_2023} proved for the first time the universal approximation theorem for Transformer models, showing that a continuous function on a compact domain can be approximated if the number of Transformer blocks is on the order of the power of $n$, where $n$ is the length of each input sequence.
Later, \citet{kim_provable_2023} showed that $2n$ self-attention blocks are sufficient for the memorization of finite samples.
Since the studies of \citet{yun_are_2023} and \citet{kim_provable_2023} are closely related to our paper, we discuss the details in more depth in \Secref{sec:background} later.
Their results were based on the assumption that the inputs are separated to some extent, which is an assumption we also make in this paper. Alternatively, under the assumption that input sequences are linearly independent, \citet{mahdavi_memorization_2023} showed that a one-layer $H$-head self-attention mechanism can memorize $O(Hn)$ samples.
Relatedly, \citet{edelman_inductive_2022} demonstrated that the bounded self-attention head is capable of expressing a sparse Boolean function while obtaining an upper bound on the covering number of self-attention.
\citet{gurevych_rate_2021} analyzed the theoretical performance of Transformers as a hierarchical composition model.
Later, \citet{takakura_approximation_2023} extended their result by utilizing a sinusoidal positional encoding and multiple heads, and showed that a one-layer Transformer with an embedding layer is a universal approximator for shift-equivariant $\gamma$-smooth functions. 
\citet{jiang_approximation_2023} recently used the Kolmogorov representation theorem to provide a non-constructive proof of the existence of a two-layer Transformer that approximates an arbitrary continuous function on a certain domain.
There are variants of universal approximation theorems for Transformers, such as analyses of sparse Transformers \citep{yun_on_2020} and constrained universal approximation theorems \citep{kratsios_universal_2021}.
\citet{likhosherstov_expressive_2023} showed that, given parameters, there exists an input such that self-attention approximates an arbitrary sparse pattern.
While \citet{bhojanapalli_low-rank_2020} proved that Transformers with a small head size, which is typical for multi-head self-attention, cannot express certain  positive column-stochastic matrices, \citet{aghajanyan_intrinsic_2021} demonstrated empirically that pre-trained Transformers have a very low intrinsic dimension, and \citet{reif_visualizing_2019} visualized context embeddings in BERT.
\citet{luo_your_2022} showed the existence of functions that cannot be approximated by Transformers with relative positional encoding.
There is also a series of papers analyzing Transformer's expressive capabilities from the perspective of formal languages \citep{hahn_theoretical_2020,
bhattamishra_ability_2020,
yao_self-attention_2021,
hao_formal_2022,
merrill_saturated_2022,
chiang_overcoming_2022,
chiang_tighter_2023}, where a softmax function in a self-attention mechanism is treated as an averaging or hardmax function.

\section{Preliminaries}
\subsection{Notation}
We use bold lowercase letters to represent vectors and bold uppercase letters to represent matrices.
For any vector $\vv \in \R^a$, we denote by $v_i$ the $i$-the element of $\vv$.
For any matrix $\mA \in \R^{a \times b}$, we denote its $i$-th row by $\mA_{i,:}$, its $k$-th column by $\mA_{:,k}$ and the element at its $i$-th row and $k$-th column by $A_{i,k}$.
For any positive integer $m \in \mathbb{N}_+$, $[m]$ represents the set $\{1,\dots,m\}$. 
For any real numbers $a < b$, $[a,b]$ represents the interval $\{x \in \R \mid a \leq x \leq b\}$, $(-\infty, a)$ represents $\{x \in \R \mid x < a\}$, and $(b,\infty)$ represents $\{x \in \R \mid x > b\}$.
Let $\sigma_S[\vv]$ and $\sigma_H[\vv]$ for any input vector $\vv$ be the softmax function and hardmax function, respectively. Note that when there are multiple indices with maximum values, the hardmax function is defined such that the sum of the values at these indices equals one.
By abuse of notation, for any input matrix $\mA$, $\sigma_S\left[\mA\right]$ and $\sigma_H\left[\mA\right]$ are defined as column-wise softmax and column-wise hardmax, respectively.
We denote the ReLU activation function by $\sigma_R$. Unlike $\sigma_S$ and $\sigma_H$, $\sigma_R$ is always an element-wise operator, regardless of whether the input is a vector or a matrix.
Let $\|\cdot\|$ be the $\ell^2$ norm and $\|\cdot\|_p~(1 \leq p<\infty)$ be the $\ell^p$ norm.
We define the distance between two functions $f_1, f_2: \mathbb{R}^{d \times n} \rightarrow \mathbb{R}^{d \times n}$ by
\begin{align}
\dist_p\left(f_1, f_2\right):=\left(\int\left\|f_1(\mathbf{X})-f_2(\mathbf{X})\right\|_p^p \mathrm{d} \mathbf{X}\right)^{1 / p}. \label{eq:dist_of_f}
\end{align}
In this paper, $n$ denotes the length of an input sequence, $N$ the number of input sequences, $C$ the number of output classes, and $d$ the embedding dimension.
In addition, $i,j$ are basically used for the indices of finite samples and $k,l$  for the indices in each input sequence.


\subsection{Transformer block}
Transformer was first introduced in \citet{vaswani_attention_2017}. Here we follow the definitions adopted in \citet{kim_provable_2023}:
the Transformer block is composed of the self-attention mechanism and the feed-forward neural network, each accompanied by a skip connection.
Given an input sequence $\mZ \in \R^{d \times n}$, composed of $n$ tokens each with an embedding dimension of size $d$, a dot-product self-attention mechanism with $h$ heads outputs the following values:
\begin{align}
    \mathcal{F}^{(SA)}_S(\mZ)
    = \mZ + \sum_{i=1}^h \mW_{i}^{(O)}
    \left(\mW_{i}^{(V)}\mZ \right)
    \sigma_S \left[
    \left(\mW_{i}^{(K)}\mZ \right)^\top
    \left(\mW_{i}^{(Q)}\mZ \right)
    \right]
    \in \R^{d \times n}, \label{eq:formulation_of_attention}
\end{align}
where $\mW_{i}^{(V)},\, \mW_{i}^{(K)},\, \mW_{i}^{(Q)} \in \R^{s \times d}$ and $\mW_{i}^{(O)} \in \R^{d \times s}$ are the weight matrices, and $s$ is the head size.
Note that here, as with \citet{yun_are_2023} and \citet{kim_provable_2023}, we adopt the definition of the self-attention mechanism, which excludes layer normalization from the original definition of \citet{vaswani_attention_2017} for the sake of simplicity.

In contrast, given an input $\mH \in \R^{d \times n}$, the output of feed-forward neural network with a skip connection at index $k \in [n]$ is
\begin{align}
    \mathcal{F}^{(FF)}\left(\mH\right)_{:,k}
    = \mH_{:,k} + \mW^{(2)}\sigma_R\left[
    \mW^{(1)}\mH_{:,k} + \vb^{(1)}
    \right] + \vb^{(2)} \in \R^d,
\end{align}
where $q$ is the hidden dimension, $\mW^{(1)} \in \R^{q \times d}$ and $\mW^{(2)} \in \R^{d \times q}$ are weight matrices, and $\vb^{(1)} \in \R^q$ and $\vb^{(2)}$ are bias terms.

On the basis of the above definition, the Transformer block is represented as a composition of a self-attention mechanism and a feed-forward neural network: for any input sequence $\mZ \in \R^{d \times n}$, composed of $n$ tokens each with an embedding dimension of size $d$, the Transformer block $\mathcal{F}:\R^{d \times n} \to \R^{d \times n}$ outputs
\begin{align}
    \mathcal{F}\left(\mZ\right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mZ\right)
    \right).
\end{align}
From the above definition, we see that the interaction of each token occurs only in the self-attention mechanism.

\section{Attention is a Contextual Mapping}
\subsection{Problem setting}
Let $(\mX^{(1)}, \mY^{(1)}),\dots,(\mX^{(1)}, \mY^{(1)}) \subset \R^{d \times n} \times [C]^{d \times n}$ be an $N$ input-output pairs of sequences, each of which consists of a sequence $\mX^{(i)}$ of $n$ tokens with embedding dimension $d$, and an output $\mY^{(i)}$, where $\mY^{(i)}_{:,k}$ corresponds to the label of the token $\mX^{(i)}_{:,k}$ at index $k$.
In addition, we define the $i$-th vocabulary set for $i \in [N]$ by $\mathcal{V}^{(i)} = \bigcup_{k \in [n]} \mX^{(i)}_{:,k} \subset \R^{d}$, and the whole vocabulary set $\mathcal{V}$ is defined by $\mathcal{V} = \bigcup_{i \in [N]} \mathcal{V}^{(i)} \subset \R^d$.


\subsection{Background}\label{sec:background}
\citet{yun_are_2023} proved affirmatively one of the most fundamental questions on the expressive capacity of Transformer models, namely, whether the universal approximation theorem for Transformer models holds.
Their proof approach is to quantize the input domain and reduce the universal approximation theorem to  the memorization analysis of finite samples, i.e., the construction of a model that achieves zero loss for a finite number of training data, which was also analyzed later by \citet{kim_provable_2023}.
In the analysis of memorization capacity, assumptions are usually made on the inputs in order to perform a meaningful analysis beyond the lower bound of \citet{sontag_shattering_1997}.
Here, as with the assumptions adopted by \citet{yun_are_2023, kim_provable_2023}, we assume that the input tokens are separated by a certain distance.

\begin{dfn}[Tokenwise Separatedness]
Let $m \in \mathbb{N}$ and $\mZ^{(1)},\dots,\mZ^{(N)} \in \R^{m \times n}$ be input sequences. Then, $\mZ^{(1)},\dots,\mZ^{(N)}$ are called tokenwise $(r_{\min}, r_{\max},\delta)$-separated if the following three conditions hold.
\begin{enumerate}
    \item For any $i \in [N]$ and $k \in [n]$, $\left\|\mZ^{(i)}_{:,k}\right\| > r_{\min}$ holds.
    
    \item For any $i \in [N]$ and $k \in [n]$, $\left\|\mZ^{(i)}_{:,k}\right\| < r_{\max}$ holds.
    
    \item For any $i,j \in [N]$ and $k,l \in [n]$ with $\mZ^{(i)}_{:,k} \neq \mZ^{(j)}_{:,l}$, $\left\|\mZ^{(i)}_{:,k} - \mZ^{(j)}_{:,l}\right\| > \delta$ holds.
\end{enumerate}
Note that we refer to $\mZ^{(1)},\dots,\mZ^{(N)}$ as tokenwise $(r_{\max}, \epsilon)$-separated instead if the sequences satisfy conditions $2$ and $3$.
\end{dfn}

The achievement of \citet{yun_are_2023} was not only to prove the universal approximation theorem for Transformers, but also to clarify the difficulties in the analysis of this kind of expressive capacity of Transformers and elucidated an approach to establishing the proof. 
Namely, what makes Transformers' memorization different from that of feed-forward neural networks is that Transformers need to capture the context of each input sequence as a whole, rather than simply associating each token with a label. 

Remarkably, \citet{yun_are_2023, kim_provable_2023} formulated this concept as a contextual mapping, which assigns a unique id to a pair of an input sequence and each of their tokens. We define it here using the notion of $(r,\delta)$-separatedness.
\begin{dfn}[Contextual Mapping]
    Let $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ be input sequences. Then, a map $q: \R^{d \times n} \to \R^{d \times n}$ is called an $(r,\delta)$-contextual mapping if the following two conditions hold:
    \begin{enumerate}
        \item For any $i \in [N]$ and $k \in [n]$, $\left\|q\left(\mX^{(i)}\right)_{:,k}\right\| < r$ holds.
        
        \item For any $i,j \in [N]$ and $k,l \in [n]$ such that $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$ or $\mX^{(i)}_{:,k} \neq \mX^{(j)}_{:,l}$, \\
        $\left\|q\left(\mX^{(i)}\right)_{:,k} - q\left(\mX^{(j)}\right)_{:,l}\right\| > \delta$ holds.
    \end{enumerate}
    In particular, $q(\mX^{(i)})$ for $i \in [N]$ is called a context id of $\mX^{(i)}$.
\end{dfn}

If we have such a contextual mapping, a label sequence can be associated with a unique id for each input sequence using the existing analysis of memorization in feed-forward neural networks.

Thus, the central question is: how to construct a contextual mapping in Transformer models?
The only place in Transformer models where interaction between tokens can be taken into account is in the self-attention mechanism; therefore, the self-attention mechanism must be used to construct a contextual mapping.
\citet{yun_are_2023} first constructed a contextual mapping by using $|\mathcal{V}|^d + 1$ self-attention layers\footnote{To be precise, when the continuous input range is quantized into $1/\delta$ pieces for some $0 < \delta < 1$, they demonstrated that there exists a contextual mapping composed of $\delta^{-d}$ self-attention layers.}, and later \citet{kim_provable_2023} improved it to $2n$ self-attention layers.
However, this is still far from the practical implementation of Transformers, and it remains unclear whether a reasonably-sized Transformer would possess such memorization capacity or if the universal approximation theorem would hold.
This leads to the following question.

\textbf{How many self-attention layers are both necessary and sufficient to construct a contextual mapping?}

We first point out the reason for requiring a significant number of self-attention layers in the construction of contextual mapping in the analyses of \citet{yun_are_2023, kim_provable_2023}.
Their approach entails interpreting the softmax function in the self-attention mechanism as an approximation of the hardmax function, which also hinders a detailed analysis of the specific properties of the softmax function.
As evidence of this, we illustrate in \Secref{sec:self_attention_with_hardmax} that using a single layer of self-attention with the hardmax function does not suffice to construct a contextual mapping.

Next, in \Secref{sec:self_attention_with_softmax}, we demonstrate that a contextual mapping can be constructed by using only one self-attention layer with the softmax function.
This is somewhat surprising because this implies the probability of fully capturing the context of each input sequence only through the attention coefficients computed by the pairwise dot-product of the softmax function and its weighted average.


\subsection{Self-attention with hardmax}\label{sec:self_attention_with_hardmax}
In previous studies analyzing the memorization capacity of Transformers \citep{yun_are_2023,kim_provable_2023}, the softmax function is taken to be an approximation of the hardmax function.
However, we show here that the attention block with the hardmax function is not a contextual mapping.

First we define the attention block with the hardmax function: for an input sequence $\mZ \in \R^{d \times n}$, the attention with the hardmax function is calculated as
\begin{align}
    \mathcal{F}^{(SA)}_H(\mZ)
    = \mZ + \sum_{i=1}^h \mW_{i}^{(O)}
    \left(\mW_{i}^{(V)}\mZ \right)
    \sigma_H \left[
    \left(\mW_{i}^{(K)}\mZ \right)^\top
    \left(\mW_{i}^{(Q)}\mZ \right)
    \right],
\end{align}
where $\mW_{i}^{(V)},\, \mW_{i}^{(K)},\, \mW_{i}^{(Q)} \in \R^{s \times d}$ and $\mW_{i}^{(O)} \in \R^{d \times s}$ are the weight matrices.

The following theorem holds for such a model. The proof is in Appendix \ref{sec: proof_of_hardmax}.
\begin{thm}\label{thm:hardmax_is_not_contextual_mapping}
$1$-layer multi-head self-attention $\mathcal{F}^{(SA)}_H$ with the hardmax function cannot be a contextual mapping.
\end{thm}
Since the self-attention mechanism is the only place in Transformer models where interaction between tokens happens, this theorem indicates that one-layer Transformers with hardmax attention do not have a memorization capacity.


\subsection{Self-attention with softmax}\label{sec:self_attention_with_softmax}
In this subsection, we show that a softmax-based $1$-layer attention block with low-rank weight matrices is a contextual mapping for almost all input sequences.
This result is consistent with recent empirical evidence that pre-trained Transformers are low-rank \citep{aghajanyan_intrinsic_2021, choromanski_rethinking_2020,wang_linformer_2020, lialin_stack_2023}, and theoretically supports that the low-rank self-attention mechanism is sufficient to fully comprehend the contextual information of an input sequence.
It is worth noting that our construction allows for an arbitrary head size.
By considering the case of a head size of $1$, this particularly indicates that the self-attention mechanism has the ability to compress the information of an input sequence through a scalar value.
\begin{thm}\label{thm:softmax_is_contextual_mapping}
    Let $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ be input sequences with no duplicate word token in each sequence, that is,
    \begin{align*}
        \mX^{(i)}_{:,k} \neq \mX^{(i)}_{:,l} \numberthis
    \end{align*}
    for any $i \in [N]$ and $k,l \in [n]$.
    Also assume that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated.
    Then, there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)}, \mW^{(K)}, \mW^{(Q)} \in \R^{s \times d}$ such that the ranks of $\mW^{(V)}, \mW^{(K)}$ and $\mW^{(Q)}$ are all $1$, and $1$-layer single head attention with softmax, i.e., $\mathcal{F}^{(SA)}_S$ with $h=1$ is an $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)},\dots,\mX^{(N)} \in \R^{d \times n}$ with $r$ and $\delta$ defined by
    \begin{align}
        r &= r_{\max} + \frac{\epsilon}{4}, \\
        \delta &= \frac{2(\log n)^2 \epsilon^2 r_{\min}}{r_{\max}^2 (|\mathcal{V}|+1)^4 (2\log n + 3) \pi d} \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{(2\log n + 3)\pi d r_{\max}^2}{4\epsilon r_{\min}}\right). \label{eq:delta_of_contextual_mapping}
    \end{align}
\end{thm}

Here we provide a simple proof sketch.
The full proof can be found in Appendix \ref{sec: proof_of_contextual_mapping}.
\begin{proof}[Proof Overview]
For simplicity, we here assume $s = 1$.
If we have a unique id, i.e., sequence id, corresponding to each input sequence $\mX^{(i)}$ for $i \in [N]$, a context id can be constructed from a suitable linear combination of the sequence id and the value of each token. Since this linear combination can be calculated by the output projection matrix $\mW^{(O)}$ and skip connection, the problem is how to configure weight parameters $\mW^{(V)}, \mW^{(K)}, \mW^{(Q)} \in \R^{1 \times d}$ so that each row of the values' softmax weighted average,
\begin{equation*}
    \left(\mW^{(V)}\mX^{(i)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(i)} \right)^\top
    \left(\mW^{(Q)}\mX^{(i)} \right)
    \right]
    \in \R^{1 \times n}, \numberthis
\end{equation*}
outputs the unique sequence id of $\mX^{(i)}$.

Actually, an even weaker condition is sufficient for an attention block to be a contextual mapping: there is no need to have just one unique sequence id for each input sequence. In fact, it is possible to construct a contextual mapping, provided that for each token $\vv \in \mathcal{V}$, input sequences in which the token appears can be identified by some $\vv$-dependent sequence ids. This condition can be expressed in a mathematical form as follows: what we have to show is to construct weight matrices $\mW^{(V)},\mW^{(K)},\mW^{(Q)} \in \R^{1 \times d}$ with some $\epsilon > 0$ such that
\begin{align*}
    &\left|\left(\mW^{(V)}\mX^{(i)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(i)} \right)^\top
    \left(\mW^{(Q)}\mX^{(i)}_{:,k} \right)
    \right] \right.\\
    &\quad\quad\quad\quad-
    \left.\left(\mW^{(V)}\mX^{(j)} \right)
    \sigma_S \left[
    \left(\mW^{(K)}\mX^{(j)} \right)^\top
    \left(\mW^{(Q)}\mX^{(j)}_{:,l} \right)
    \right] \right|
    > \epsilon \numberthis
\end{align*}
holds for any distinct $i,j \in [N]$ and any $k,l \in [n]$ such that $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ and $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.

For simplicity, we choose $\mW^{(V)}=\mW^{(K)}=\mW^{(Q)} = \vw^\top$ \footnote{In our actual proof, there exist unit vectors $\vv,\vv' \in \R^d$ such that $\mW^{(V)},\mW^{(K)}$ and $\mW^{(Q)}$ may be defined by $\mW^{(V)}=\vu'' \vv^\top, \mW^{(K)}=\vu' \vv^\top$ and $\mW^{(Q)}=\vu \vv'^\top$ for arbitrary vectors $\vu,\vu',\vu'' \in \R^s$ satisfying certain constraints.} such that the linear operator $\vw \in \R^d$ projects each token to a scalar value while approximately preserving the distance between each pair of tokens: for any pair of tokens $\vv_a,\vv_b \in \mathcal{V}$,
\begin{align*}
    c\|\vv_a - \vv_b\|
    \leq \left|\vw^\top \vv_a - \vw^\top \vv_b\right|
    \leq \|\vv_a - \vv_b\| \numberthis
\end{align*}
with some constant $0 < c < 1$. Then, by using the assumption $\vt = \mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ for some token $\vt \in \R^d$, we have
\begin{align*}
    &\left|
    \vw^\top \vt
    \right|
    \cdot 
    \left|
    \left(\vw^\top\mX^{(i)} \right)
    \sigma_S \left[
    \left(\vw^\top\mX^{(i)} \right)^\top
    \left(\vw^\top \vt \right)
    \right]
    -
    \left(\vw^\top\mX^{(j)} \right)
    \sigma_S \left[
    \left(\vw^\top\mX^{(j)} \right)^\top
    \left(\vw^\top \vt \right)
    \right]
    \right| \\
    &\geq
    \left|
    \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
    -
    \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
    \right|, \numberthis
\end{align*}
where we denote $\va^{(i)} = \left(\vw^\top\mX^{(i)} \right)^\top
    \left(\vw^\top \vt \right) \in \R^n$ and $\va^{(j)} = \left(\vw^\top\mX^{(j)} \right)^\top
    \left(\vw^\top \vt \right) \in \R^n$.
Therefore, in order to prove that a self-attention block serves as a contextual mapping, we only have to focus on the separability of the function 
\begin{align*}
    \boltz: \mathbb{R}^n \to \mathbb{R}, \va \mapsto \va^\top \sigma_S[\va], \numberthis
\end{align*}
which is known as the Boltzmann operator \citep{littman_algorithms_1996, asadi_alternative_2017}.

The following lemma shows that the Boltzmann operator is a mapping that projects input sequences to scalar values while preserving some distance, and is central to our proof that the self-attention function is a contextual mapping.
\begin{lem}\label{lem: boltzmann_separation}
Let $\va^{(1)},\dots,\va^{(m)} \in \R^n$ be tokenwise $(r,\delta)$-separated vectors with no duplicate element in each vector and 
\begin{equation*}
    \delta > 2\log n + 3. \numberthis
\end{equation*}
Then, the outputs of the Boltzmann operator are $(r,\delta')$-separated, that is,
\begin{align}
    \left|\boltz(\va^{(i)})\right| &\leq r, \\
    \left|\boltz(\va^{(i)}) - \boltz(\va^{(j)})\right|
    &> \delta'
    = (\log n)^2 e^{-2r}
\end{align}
hold for each $i,j \in [m]$ with $\va^{(i)} \neq \va^{(j)}$.
\end{lem}
Taking into account the above arguments, this separability of the Boltzmann operator allows us to construct one self-attention layer to be a contextual mapping.
\end{proof}

\begin{rem}[Masked self-attention]
    In practice, attention matrices are often masked to avoid directing attention to undesired tokens.
    This is performed, for example, for autoregressive text generation or padding of inputs with different lengths.
    It is relatively straightforward to extend \cref{thm:softmax_is_contextual_mapping} to masked self-attention mechanisms.
    See Appendix \ref{sec:masked_self_attention} for more details.
\end{rem}

\section{Applications of Contextual Mapping}
\subsection{Memorization capacity of one-layer Transformer}
As a first application of \cref{thm:softmax_is_contextual_mapping}, we prove that a 1-layer Transformer can completely memorize finite samples, each of which has no duplicate token.
This result emphasizes that in contrast to the proof of \citet{kim_provable_2023}, which requires $2n$ self-attention layers for Transformer memorization, one layer of self-attention is actually sufficient.
In addition, it is worth noting that the hardmax-based Transformers do not have a memorization capacity, which is implied straightforwardly from \cref{thm:hardmax_is_not_contextual_mapping}.

\begin{cor}[Memorization capacity of one-layer Transformer]\label{cor:memorization_of_one_layer_Transformer}
    Let $\epsilon > 0, r_{\max} > r_{\min} > 0$ and $(\mX^{(1)}, \mY^{(1)}),\dots, (\mX^{(N)}, \mY^{(N)}) \subset \R^{d \times n} \times [C]^{d \times n}$ be sequences of input-output-pairs such that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated input sequences with no duplicate token in each sentence and consistently labeled, that is, $
        \mY^{(i)}_{:,k} = \mY^{(j)}_{:,l}
    $
    holds for any $i,j \in [N]$ and $k,l \in [n]$ such that $\mathcal{V}^{(i)} = \mathcal{V}^{(j)}$ and $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$.
    
    Then, there exist $4(s + d) + d(2nN + d)$ weight parameters such that for any $i \in [N]$
    \begin{align*}
        \mathcal{F}\left(\mX^{(i)} \right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mX^{(i)}\right)
    \right)
        = \mY^{(i)} \numberthis
    \end{align*}
    holds.
\end{cor}

\begin{rem}[Parameter efficiency]
To achieve the memorization with a one-layer Transformer, the one-hidden-layer feed-forward block has to map each context id to the corresponding label.
Since the possible number of context ids is at most $nN$ in the worst case, the linear dependency on $nN$ of the number of parameters in \cref{cor:memorization_of_one_layer_Transformer} is optimal up to logarithmic factors \citep{bartlett_nearly_2019}.
It is worth mentioning that this linear dependency can be relaxed to the milder requirement $\tilde{O}\left(\sqrt{nN}\right)$ by allowing for deeper layers in the feed-forward block \citep{vardi_optimal_2022}, under the assumption that the size $|\mathcal{V}|$ of the vocabulary set is independent of $n$ and $N$.
\end{rem}


In addition, it is straightforward to show that a $1$-layer Transformer with trainable positional encodings has a memorization capacity for arbitrary input sequences possibly with duplicate tokens.

\begin{cor}[Memorization capacity of one-layer Transformer with positional encodings]\label{cor:memorization_of_one_layer_Transformer_with_positional_encodings}
    Let $\epsilon > 0, r_{\max} > r_{\min} > 0$ and $(\mX^{(1)}, \mY^{(1)}),\dots, (\mX^{(N)}, \mY^{(N)}) \subset \R^{d \times n} \times [C]^{d \times n}$ be sequences of input-output-pairs such that $\mX^{(1)},\dots,\mX^{(N)}$ are tokenwise $(r_{\min},r_{\max},\epsilon)$-separated input sequences and are consistently labeled, that is, $
        \mY^{(i)} = \mY^{(j)}
    $
    holds for any $i,j \in [N]$ such that $\mX^{(i)} = \mX^{(j)}$.
    
    Then, there exist $4(s + d) + d(2nN + d)$ weight parameters and positional encodings $\mE \in \R^{d \times n}$ such that for any $i \in [N]$,
    \begin{align*}
        \mathcal{F}\left(\mX^{(i)} + \mE\right)
    = \mathcal{F}^{(FF)}\left(
    \mathcal{F}^{(SA)}_S\left(\mX^{(i)} + \mE\right)
    \right)
        = \mY^{(i)} \numberthis
    \end{align*}
    holds.
\end{cor}

\subsection{Transformers with one self-attention layer are universal approximators}
As a further application of \cref{thm:softmax_is_contextual_mapping}
we here provide a proof that Transformer with one self-attention layer is a universal approximator.
More precisely, let $\mathcal{F}_{\mathrm{PE}}$ be the set of all permutation equivariant continuous functions that take values on a compact domain in $\R^{d \times n}$, and let $\mathcal{T}_2$ be the set of all two layer Transformers with one-layer and single-head self-attention, that is,
\begin{align*}
    \mathcal{T}_2
    = \left\{
    \mathcal{F}^{(FF)}_2
    \circ
    \mathcal{F}^{(SA)}_S
    \circ
    \mathcal{F}^{(FF)}_1
    : \R^{n \times d} \to \R^{n \times d}
    \right\}, \numberthis
\end{align*}
where $\mathcal{F}^{(FF)}_1,\mathcal{F}^{(FF)}_2$ and $\mathcal{F}^{(SA)}_S$ are feed-forward neural network layers and a single-head self-attention layer with the softmax function, respectively.
Then the following proposition holds (see the definition (\ref{eq:dist_of_f})).
\begin{pro}[Transformers with one layer self-attention are universal approximators]\label{pro:Transformers_are_universal_approximator}
Let $1 \leq p < \infty$. Then, for any $f \in \mathcal{F}_{\mathrm{PE}}$ and $\epsilon > 0$, there exists a Transformer $g \in \mathcal{T}_2$  with one-layer and single-head self-attention such that
$
    \dist_p(f,g)
    < \epsilon.
$
holds.
\end{pro}
To the best of our knowledge, this is the first universal approximation theorem for two-layer Transformers with a self-attention of realistic size. 
\citet{takakura_approximation_2023} showed that a one-layer Transformer with an embedding layer is capable of approximating shift-equivariant $\gamma$-smooth functions.
However, their construction requires a considerably high number of self-attention heads and a large head size to flatten an input sequence into outputs of self-attention.
\citet{jiang_approximation_2023} used the Kolmogorov representation theorem to give a non-constructive proof of the universal approximation theorem for two-layer Transformers.
They make a particular assumption on the domain of functions, which in turn implies the first universal approximation theorem of two-layer Transformer with positional encoding for continuous functions on a compact domain.
Nevertheless, they again require a very high hidden dimension $4n^2d + 2n$.
In contrast, thanks to \cref{thm:softmax_is_contextual_mapping}, we have shown that Transformers using a single-head self-attention are universal approximators for continuous permutation equivariant functions on an arbitrary compact domain.
Our result can be readily extended for continuous but not necessarily permutation equivariant functions on a compact domain by using positional encoding, and at the same time is significant from the perspective of geometric deep learning.


\section{Experiments}
As shown in \cref{thm:softmax_is_contextual_mapping}, a self-attention mechanism with rank $1$ weight matrices already has enough expressive capacity to become a contextual mapping.
In particular, its proof leads us to consider the following simplified form of a self-attention mechanism: for any input sequence $\mZ \in \R^{d \times n}$,
\begin{align}
    \mathcal{F}^{(R1)}_S(\mZ)
    = \mZ + \mW^{(O)}
    \left(\vv_{1}^\top\mZ \right)
    \sigma_S \left[
    \left(\vv_{1}^\top\mZ \right)^\top
    \left(\vv_{2}^\top\mZ \right)
    \right]
    \in \R^{d \times n}, \label{eq:formulation_of_rank1_attention}
\end{align}
where $\vv_1,\vv_2 \in \R^d$ and $\mW^{(O)} \in \R^{d \times 1}$ are weight matrices.
This architecture corresponds to a common self-attention with the head size $s = 1$, and value and query matrices having the same weight vector $\vv_1$.
In this section, we test whether Transformers with self-attention layers replaced by \eqref{eq:formulation_of_rank1_attention}, which we call rank-$1$ Transformers, actually have the theoretically predicted expressive capacity by using a real-world dataset.

We train rank-$1$ Transformers on a token classification task with the CoNLL-2003 \citep{tjong_kim_sang_introduction_2003} dataset.
The batch size is $32$ and the training are conducted over $400$ epochs.

We train three different depths of rank-1 transformers on the dataset and do not use layer normalization to match the situation with our theoretical analysis.

% Figure environment removed

\Figref{fig:accuracy} shows training accuracies of $1$-layer, $3$-layer and $6$-layer rank-$1$ Transformers on each task over $400$ epochs.
It can be seen that the $1$-layer rank-$1$ Transformer is already able to memorise the CoNLL-2003 dataset almost perfectly.
On the other hand, while the accuracy curve for the $1$-layer rank-$1$ Transformer shows that the accuracy is still increasing steadily at $400$ epochs, reaching $0.9872$ at $800$ epochs, its rate of increase is much slower than for the $3$-layer and $6$-layer Transformers.

From this observation, we conjecture that  while theoretically $1$-layer Transformers already have a memorisation capacity for finite samples, the advantage of deepening layers lies in speeding up the learning of such tasks.
Since our analysis is on the expressive capabilities of Transformers, we leave this hypothesis on the optimisation aspect of Transformers as a future work.


\section{Conclusions}
We demonstrated that a contextual mapping can be implemented in one-layer and single-head self-attention with low-rank matrices, by clarifying the connection between a self-attention mechanism and the Boltzmann operator. This particularly indicates that one-layer Transformers have a memorization capacity for finite samples, and that Transformers with one-layer and single-head self-attention are universal approximators for continuous permutation equivariant functions on a compact domain.
Our proof of the universal approximation theorem requires one feed-forward neural network layer before the self-attention layer to quantize continuous inputs.
We leave it as future work to clarify whether the one-layer Transformers without such a quantization layer are universal approximators or not.
We also expect that our analysis of the softmax function will have an impact on the evaluation of Transformer's expressive capability from the perspective of formal languages.

\clearpage

\bibliography{iclr2024_conference,zotero_2023_9_28}

\bibliographystyle{iclr2024_conference}

\appendix
\renewcommand{\nomname}{Notation Table}
\nomenclature[A, 03]{\(\mA\)}{A matrix}
\nomenclature[A, 02]{\(\va\)}{A vector}
\nomenclature[A, 01]{\(a\)}{A scalar}
\nomenclature[A, 08]{\(\mX^{(i)}\)}{$i$-th input sequence, consisting of $n$ tokens of embedding dimension $d$}
\nomenclature[A, 04]{$n$}{The length of an input sequence}
\nomenclature[A, 05]{$N$}{The number of input sequences}
\nomenclature[A, 06]{$C$}{The number of output classes}
\nomenclature[A, 07]{$d$}{Embedding dimension}

\nomenclature[B, 04]{\([a,b]\)}{Closed interval from $a$ to $b$}
\nomenclature[B, 03]{\([m]\)}{Set of all integers from $1$ to $m$}
\nomenclature[B, 01]{\(\R\)}{Set of real numbers}
\nomenclature[B, 02]{\(\mathbb{N}_+\)}{Set of positive integers}
\nomenclature[B, 05]{\(\mathcal{V}^{(i)}\)}{$i$-th vocabulary set}

\nomenclature[C, 02]{\(A_{i,j}\)}{Element $i, j$ of matrix $\mA$}
\nomenclature[C, 01]{\(a_i\)}{Element $i$ of vector $\va$, with indexing starting at $1$}
\nomenclature[C, 03]{\(\mA_{i, :}\)}{Row $i$ of matrix $\mA$}
\nomenclature[C, 03]{\(\mA_{:, i}\)}{Column $i$ of matrix $\mA$}

\nomenclature[F, 02]{$\lVert\vx\rVert$}{$\ell^2$ norm of $\vx$}
\nomenclature[F, 03]{$\lVert\vx\rVert_p$}{$\ell^p$ norm of $\vx$}
\nomenclature[F, 04]{\(\1_\mathrm{condition}\)}{is 1 if the condition is true, 0 otherwise}
\nomenclature[F, 05]{$\dist_p(f_1,f_2)$}{$\left(\int\left\lVert f_1(\mathbf{X})-f_2(\mathbf{X})\right\rVert_p^p \mathrm{d} \mathbf{X}\right)^{1 / p}$}
\nomenclature[F, 06]{$\sigma_S$}{Softmax function}
\nomenclature[F, 07]{$\sigma_H$}{Hardmax function}
\nomenclature[F, 08]{$\sigma_R$}{ReLU activation function}
\nomenclature[F, 09]{$\mathcal{F}^{(SA)}_H$}{Hardmax-based self-attention mechanism with a skip-connection}
\nomenclature[F, 10]{$\mathcal{F}^{(SA)}_S$}{Softmax-based self-attention mechanism with a skip-connection}
\nomenclature[F, 11]{$\mathcal{F}^{(FF)}$}{Feed-forward neural network with a skip-connection}
\nomenclature[F, 12]{$\boltz$}{Boltzmann opeartor}

\printnomenclature


\section{Proof of Main Results}

First, we introduce the Boltzmann operator, which frequently appears in our proofs.
\begin{dfn}(Boltzmann operator)
The Boltzmann operator is defined by
\begin{align*}
    \boltz: \R^m \to \R,\,
    \va \mapsto \va^\top \sigma_S[\va]. \numberthis
\end{align*}
By abuse of notation, we use the same notation $\boltz$ for various dimension $m \in \mathbb{N}_+$.
\end{dfn}

\subsection{\texorpdfstring{Proof of \cref{thm:hardmax_is_not_contextual_mapping}}{}}\label{sec: proof_of_hardmax}
\begin{proof}
Let $\vv \in \R^d$ be an arbitrary nonzero vector, and consider the situation that all input tokens can be written as
\begin{equation}
    \mathcal{V} = \{\alpha_1 \vv, \alpha_2 \vv, \alpha_3 \vv, \alpha_4 \vv\} \subset \R^d
\end{equation}
for some scalars $\alpha_1 < \alpha_2 < \alpha_3 < \alpha_4$. 
Then, the attention matrix inside the hardmax function at head $i$ can be expressed as
\begin{align}
    \left(\mW_{i}^{(K)}\vv \va^\top \right)^\top
    \left(\mW_{i}^{(Q)}\vv \va^\top \right)
    = \va \left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \va^\top 
\end{align}
with input coefficients $\va \in \{\alpha_1,\alpha_2,\alpha_3,\alpha_4\}^n \subset \R^n$.
In particular, when we focus on a certain index, at which the token is, e.g., $\alpha_2 \vv$, the above expression can further be written as
\begin{align*}
    \left(\mW_{i}^{(K)}\vv \va^\top \right)^\top
    \left(\mW_{i}^{(Q)}\alpha_2\vv \right)
    &= \va \left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \alpha_2 \\
    &= \underbrace{\left(\mW_{i}^{(K)}\vv \right)^\top
    \left(\mW_{i}^{(Q)}\vv \right)
    \alpha_2}_{\in \R} \cdot \va \numberthis
\end{align*}
The right-hand side is a vector $\va$ multiplied by some scalar. So it is evident that the maximum value of the vector on the right-hand side is achieved only at the indices where the values of the input sequence $\vv \va^\top$ are $\alpha_1$ or $\alpha_4$.
This implies that a self-attention with the hardmax function invariably gets distracted by the indices where $\alpha_1$ or $\alpha_4$ are present, thereby overlooking information from other tokens in the input sequence.
As a result, no matter how many heads there are, one-layer self-attention with the hardmax function cannot distinguish input sequences, e.g., $(\alpha_1 \vv, \alpha_2 \vv, \alpha_4 \vv)$ and $(\alpha_1 \vv, \alpha_3 \vv, \alpha_4 \vv)$.
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{thm:softmax_is_contextual_mapping}}{}} \label{sec: proof_of_contextual_mapping}
\begin{proof}[Proof of \cref{thm:softmax_is_contextual_mapping}]
    Recall that a softmax-based self-attention function $\mathcal{F}^{(SA)}_S:\R^{d \times n} \to \R^{d \times n}$ with $h = 1$ is defined as
    \begin{align}
        \mathcal{F}^{(SA)}_{S}\left(\mZ\right)
        = \mZ + \mW^{(O)}\left(\mW^{(V)}\mZ\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mZ\right)^\top
        \left(\mW^{(Q)}\mZ\right)
        \right],
    \end{align}
    where $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)},\mW^{(K)},\mW^{(Q)} \in R^{s \times d}$ are weight matrices.
    
    We construct a softmax-based self-attention function $\mathcal{F}^{(SA)}_S$ with the property that
    \begin{align}
        \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| < \frac{\epsilon}{4}
    \end{align}
    holds for any input sequence $\mX^{(i)}$ with $i \in [N]$ and index $k \in [n]$.
    When this property is fulfilled, it is easy to show that
    \begin{align}
        \left\|\mathcal{F}^{(SA)}_{S}\left(\mX^{(i)}\right)_{:,k}\right\|
        &\leq
        \left\|\mX^{(i)}_{:,k}\right\|
        + \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \nonumber \\
        &< r_{\max} + \frac{\epsilon}{4} \label{eq:contextual_mapping_condition1}
    \end{align}
    holds for any $i \in [N]$ and $k \in [n]$, and also
    \begin{align}
        &\left\|
        \mathcal{F}^{(SA)}_{S}\left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_{S}\left(\mX^{(j)}\right)_{:,l}
        \right\| \nonumber\\
        &\geq \left\|\mX^{(i)}_{:,k} - \mX^{(j)}_{:,l}\right\|
        - \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \nonumber\\
        &\quad\quad\quad\quad -
        \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(j)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(j)}\right)^\top
        \left(\mW^{(Q)}\mX^{(j)}_{:,l}\right)
        \right]
        \right\| \nonumber\\
        &> \epsilon - \frac{\epsilon}{4} - \frac{\epsilon}{4} 
        = \frac{\epsilon}{2} \label{eq:contextual_mapping_condition2}
    \end{align}
    for any $i,j \in [N]$ and $k,l \in [n]$ such that $\mX^{(i)}_{:,k} \neq \mX^{(j)}_{:,l}$. So all that remains to prove is to construct a self-attention function $\mathcal{F}^{(SA)}$ that has the properties described above and can also distinguish input tokens $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ such that $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.
    
    Let $\delta = 2\log n + 3$ and fix any vectors $\vu,\vu' \in \R^s$ with 
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}. \label{eq:setting_of_u}
    \end{align}
    Then, according to \cref{lem:attention_projection} with $\delta = 2\log n + 3$, we see that there exists a unit vector $\vv \in \R^d$ such that 
    \begin{gather}
        \left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right|
        > \delta, \label{eq:attention_projection_delta} \\
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\| \label{eq:attention_projection_min_max}
    \end{gather}
    for any $\vv_a,\vv_b,\vv_c \in \mathcal{V}$ with $\vv_a \neq \vv_b$, where $\mW^{(K)} = \vu \vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu' \vv^\top \in \R^{s \times d}$.

    Furthermore, we configure $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)} \in \R^{s \times d}$ to be $\mW^{(V)} = \vu'' \vv^\top$ for any nonzero vector $\vu'' \in \R^s$ such that 
    \begin{align}
        \left\|\mW^{(O)} \vu''\right\|
        = \frac{\epsilon}{4 r_{\max}} \label{eq:condition_on_w_o}
    \end{align}
    holds. This can be accomplished, e.g., $\mW^{(O)} = \vu''' \vu''^\top$ for any vector $\vu''' \in \R^d$ which satisfies $\left\|\vu'''\right\| = \epsilon/(4r_{\max}\left\|\vu''\right\|^2)$.
    In this case, the value of the self-attention without a skip-connection is upper-bounded by
    \begin{align*}
        &\left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]
        \right\| \\
        &= \left\|
        \sum_{k'=1}^n s_{k'}^k\mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\|
        \quad\text{with } s^k_{k'} = \sigma_S
        \left[
        \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)
        \right]_{k'} \\
        &\leq \sum_{k'=1}^n s_{k'}^k\left\|
         \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\| \\
        &\leq \max_{k' \in [n]} \left\|
         \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)_{:,k'}
        \right\| \quad (\text{from} \sum_{k'=1}^n s^k_{k'} = 1) \\
        &=  \max_{k' \in [n]} \left\|\mW^{(O)} \vu'' \vv^\top \mX^{(i)}_{:,k'}\right\|\\
        &=  \left\|\mW^{(O)} \vu''\right\| \cdot \max_{k' \in [n]} \left| \vv^\top \mX^{(i)}_{:,k'}\right|\\
        &\leq \frac{\epsilon}{4 r_{\max}} \cdot 
        \max_{k' \in [n]} \left\|\mX^{(i)}_{:,k'}\right\|
        \quad (\text{from \eqref{eq:attention_projection_min_max} and \eqref{eq:condition_on_w_o}}) \numberthis \label{eq:output_norm_of_self_attention}\\
        &< \frac{\epsilon}{4}, \numberthis 
    \end{align*}
    which means that \eqref{eq:contextual_mapping_condition1} and \eqref{eq:contextual_mapping_condition2} are satisfied with the weight matrices defined above.

    Now, we see that the weight matrices $\mW^{(O)},\mW^{(V)},\mW^{(K)},\mW^{(Q)}$ configured above can distinguish the most subtle pattern of input tokens, i.e. $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ with $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$.

    Pick up any $i,j \in [N]$ and $k,l \in [n]$ such that $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ and $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$. In addition, we define $\va^{(i)}, \va^{(j)}$ by
    \begin{align}
        \va^{(i)}
        &= \left(\mW^{(K)}\mX^{(i)}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right) \in \R^n, \\
        \va^{(j)}
        &= \left(\mW^{(K)}\mX^{(j)}\right)^\top
        \left(\mW^{(Q)}\mX^{(j)}_{:,l}\right) \in \R^n.
    \end{align}
    Then, \eqref{eq:attention_projection_delta} and \eqref{eq:attention_projection_min_max} imply that $\va^{(i)}$ and $\va^{(j)}$ are tokenwise $(r,\delta)$-separated, where $r$ is defined by
    \begin{align}
        r = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta r_{\max}^2}{\epsilon r_{\min}},
    \end{align}
    because for any $k' \in [n]$, we have
    \begin{align*}
        \left|a^{(i)}_{k'}\right|
        &= \left|\left(\mW^{(K)}\mX^{(i)}_{:,k'}\right)^\top
        \left(\mW^{(Q)}\mX^{(i)}_{:,k}\right)\right| \\
        &= \left|\left(\vv^\top \mX^{(i)}_{:,k'}\right)^\top \right|
        \cdot
        \left|\vu^\top \vu'^\top \right|
        \cdot
        \left| \left(\vv^\top \mX^{(i)}_{:,k}\right)\right| \\
        &\leq \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}r_{\max}^2 \quad (\text{from \eqref{eq:setting_of_u} and \eqref{eq:attention_projection_min_max}}), \numberthis
    \end{align*}
    and the same upper-bound also holds for $\va^{(j)}$.

    Since $\mathcal{V}^{(i)} \neq \mathcal{V}^{(j)}$ and there exists no duplicate token in $\mX^{(i)}$ and $\mX^{(j)}$ respectively, it follows from \cref{lem: boltzmann_separation} that
    \begin{align}
        \left|\boltz(\va^{(i)}) - \boltz(\va^{(j)})\right|
        &> \delta'
        = (\log n)^2 e^{-2r},
    \end{align}
    that is,
    \begin{align}
        \left|
        \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
        -
        \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
        \right| > \delta'. \label{eq:a_i_minus_a_j}
    \end{align}
    Since $\mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}$ by assumption, \eqref{eq:a_i_minus_a_j} are further expanded as
    \begin{align*}
        \delta'
        &< \left|
        \left(\va^{(i)}\right)^\top \sigma_S\left[\va^{(i)}\right]
        -
        \left(\va^{(j)}\right)^\top \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &= \left|
        \left(\mX^{(i)}_{:,k}\right)^\top \left(\mW^{(Q)}\right)^\top \mW^{(K)}
        \left(
        \mX^{(i)}
        \sigma_S\left[\va^{(i)}\right]
        -
        \mX^{(j)}
        \sigma_S\left[\va^{(j)}\right]
        \right)
        \right| \\
        &= \left|
        \left(\mX^{(i)}_{:,k}\right)^\top \vv \vu'^\top \vu \vv^\top
        \left(
        \mX^{(i)}
        \sigma_S\left[\va^{(i)}\right]
        -
        \mX^{(j)}
        \sigma_S\left[\va^{(j)}\right]
        \right)
        \right| \\
        &= \left|\vv^\top \mX^{(i)}_{:,k}\right|
        \cdot
        \left|\vu^\top \vu'\right|
        \cdot
        \left|
        \left(\vv^\top\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &\leq r_{\max}
        \cdot
        \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}
        \cdot
        \left|
        \left(\vv^\top\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right|, \numberthis
    \end{align*}
    where the last inequality follows from \eqref{eq:setting_of_u} and \eqref{eq:attention_projection_min_max}.

    Therefore, the gap between the outputs of th self-attention function for $\mX^{(i)}$ and $\mX^{(j)}$ are lower-bounded as follows:
    \begin{align*}
        &\left\|
        \mathcal{F}^{(SA)}_S \left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_S \left(\mX^{(j)}\right)_{:,l}
        \right\| \\
        &= \left\|
        \mW^{(O)}\left(\mW^{(V)}\mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \mW^{(O)}\left(\mW^{(V)}\mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right\|
        \quad (\because \mX^{(i)}_{:,k} = \mX^{(j)}_{:,l}) \\
        &= \left\|\mW^{(O)} \vu''\right\| \cdot 
        \left|
        \left(\vv^\top \mX^{(i)}\right)
        \sigma_S\left[\va^{(i)}\right]
        -
        \left(\vv^\top \mX^{(j)}\right)
        \sigma_S\left[\va^{(j)}\right]
        \right| \\
        &> \frac{\epsilon}{4r_{\max}} \cdot \frac{\delta'}{(|\mathcal{V}|+1)^4} \frac{8\epsilon r_{\min}}{\pi d \delta r_{\max}}, \numberthis 
    \end{align*}
    where $\delta$ and $\delta'$ are defined respectively as
    \begin{align}
        \delta
        &= 2\log n + 3, \\
        \delta'
        &= (\log n)^2 e^{-2r} \quad \text{with} \quad r = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta r_{\max}^2}{\epsilon r_{\min}}.
    \end{align}
    By plugging $\delta$ and $\delta'$, the above inequality is simplified as
    \begin{align*}
        &\left\|
        \mathcal{F}^{(SA)}_S \left(\mX^{(i)}\right)_{:,k}
        -
        \mathcal{F}^{(SA)}_S \left(\mX^{(j)}\right)_{:,l}
        \right\| \\
        &> \frac{2(\log n)^2 \epsilon^2 r_{\min}}{r_{\max}^2 (|\mathcal{V}|+1)^4 (2\log n + 3) \pi d} \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{(2\log n + 3)\pi d r_{\max}^2}{4\epsilon r_{\min}}\right). \numberthis
    \end{align*}
\end{proof} 

\subsection{\texorpdfstring{Proof of \cref{cor:memorization_of_one_layer_Transformer}}{}}
\begin{proof}
    According to \cref{thm:softmax_is_contextual_mapping}, we can construct such self-attention to be contextual mapping, that is,
    there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)}, \mW^{(K)}, \mW^{(Q)} \in \R^{s \times d}$ such that $\mathcal{F}^{(SA)}_S$ with $h=1$ is a $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)},\dots,\mX^{(N)}$ with $r$ and $\delta$ defined by
    \begin{align*}
        r &= r_{\max} + \frac{\epsilon}{4}, \numberthis\\
        \delta &= \frac{\epsilon r_{\min} \log n}{r_{\max}^2 (|\mathcal{V}|+1)^4 (2\log n + 3) \pi d} \\
        &\quad\quad\quad\quad
        \cdot \exp \left(-\left(|\mathcal{V}|+1\right)^4
        \frac{(2\log n + 3) \pi d r_{\max}^2 }{4\epsilon r_{\min}}\right). \numberthis
    \end{align*}
    So what remains to do is to associate each context id with the corresponding output label using a feed-forward neural network $\mathcal{F}^{(FF)}$.
    Construction of such a network is a typical memorization task of a one-hidden-layer feed-forward neural network.
    Here we adopt the implementation from \citet{zhang_understanding_2016}. 
    In this case, since the possible number of context ids is upper-bounded by $nN$, the required parameters for the FF layer with output dimension $d$ is at most $d \times (2nN + d)$ \citep{zhang_understanding_2016}.
    As for the self-attention layer, rank-$1$ weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{(V)}, \mW^{(K)}, \mW^{(Q)} \in \R^{s \times d}$ all require $s + d$ parameters each. Thus, the number of parameters of the self-attention layer is 4(s + d).
    In conclusion, the total number of parameters for one-layer Transformers to memorize the dataset is at most $4(s + d) + d(2nN + d)$.
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{cor:memorization_of_one_layer_Transformer_with_positional_encodings}}{}}
\begin{proof}
First, we define the positional encoding matrix $\mE \in \R^{d \times n}$ as follows:
\begin{align*}
    \mE
    =
    \left(
    \begin{array}{cccc}
    2r_{\max} & 4r_{\max} & \dots & 2nr_{\max} \\
    \vdots & \vdots & \ddots & \vdots \\
    2r_{\max} & 4r_{\max} & \dots & 2nr_{\max}
    \end{array}
    \right). \numberthis
\end{align*}
Then, $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ are tokenwise $(r_{\max}, (2n+1)r_{\max}, \epsilon)$-separated, and each sentence has no duplicate token.

From \cref{thm:softmax_is_contextual_mapping},
there exist weight matrices $\mW^{(O)} \in \R^{d \times s}$ and $\mW^{V}, \mW^{K}, \mW^{Q} \in \R^{s \times d}$ such that $\mathcal{F}^{(SA)}_S$ with $h=1$ is a $(r,\delta)$-contextual mapping for the input sequences $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ with $r$ and $\delta$ defined by
\begin{align*}
    r &= (2n+1)r_{\max} + \frac{\epsilon}{4}, \numberthis\\
    \delta
    &= \frac{2(\log n)^2 \epsilon^2}{(2n+1)^2r_{\max} (nN+1)^4 (2\log n + 3) \pi d} \\
    &\quad\quad\quad\quad
    \cdot \exp \left(-(2n+1)^2\left(nN+1\right)^4
        \frac{(2\log n + 3)\pi d r_{\max}}{4\epsilon}\right), \numberthis
\end{align*}
because the size of the vocabulary set of $\mX^{(1)}+\mE,\dots,\mX^{(N)}+\mE$ is at most $nN$.
Hence, hereafter we do the same thing as in the proof of \cref{cor:memorization_of_one_layer_Transformer}, that is, implementing a feed-forward neural network $\mathcal{F}^{(FF)}$ which associates each context id with the corresponding label. 
The total number of parameters required to implement this construction can be evaluated in the same manner as in \cref{cor:memorization_of_one_layer_Transformer}.
\end{proof}

\subsection{\texorpdfstring{Proof of \cref{pro:Transformers_are_universal_approximator}}{}}
\begin{proof}
We show the propositioin by the same steps as in \citet{yun_are_2023}. Namely,
\begin{enumerate}
    \item First, given a permutation equivariant continuous function $f \in \mathcal{F}_{\mathrm{PE}}$ defined on a compact set, it follows from typical analysis that $f$ can be approximated by a step function with arbitrary precision in terms of $p$-norm. Therefore, to show a universal approximation theorem, it is sufficient to show that such a step function can be approximated by a Transformer with one self-attention layer.

    \item Second, we use a first feed-forward neural network layer $\mathcal{F}^{(FF)}_1$ to quantize the input domain, reducing the problem to memorization of finite samples.

    \item Then, by a similar analysis as in \cref{cor:memorization_of_one_layer_Transformer}, it can be shown that a combination of the self-attention layer $\mathcal{F}^{(SA)}$ and $\mathcal{F}^{(FF)}_2$ can memorize the step function almost everywhere, in the sense that quantized input domains corresponding to sentences with duplicate tokens are negligibly small. 
\end{enumerate}
We hereafter provide rough proofs of the three steps outlined above, because there are multiple ways to construct a model that satisfies the above requirements, and we do not pursue the efficiency of feed-forward neural networks in this paper.

First, without loss of generality, we ignore skip-connections in $\mathcal{F}^{(FF)}_1$ and $\mathcal{F}^{(FF)}_2$.
\begin{enumerate}
    \item Since $f$ is a continuous function on a compact set, $f$ has maximum and minimum values on the domain.
    By scaling with $\mathcal{F}^{(FF)}_1$ and $\mathcal{F}^{(FF)}_2$, $f$ is assumed to be normalized without loss of generality: for any $\mZ \in \R^{d \times n} \setminus [0,1]^{d \times n}$
    \begin{align}
        f(\mZ) = 0,
    \end{align}
    and for any $\mZ \in [-1,1]^{d\ \times n}$
    \begin{align*}
        -1 \leq f(\mZ) \leq 1. \numberthis
    \end{align*}
    Let $D \in \mathbb{N}$ be the granularity of a grid
    \begin{align}
        \mathbb{G}_D
        = \{1/D, 2/D, \dots, 1\}^{d \times n} \subset \R^{d \times n}
    \end{align}
    such that a piece-wise constant approximation
    \begin{align*}
        \overline{f}(\mZ)
        = \sum_{\mL \in \mathbb{G}_D} f\left(\mL\right) \1_{\mZ \in \mL + [-1/D,0)^{d \times n}} \numberthis
    \end{align*}
    satisfies 
    \begin{align}
        \dist_p(f, \overline{f}) < \epsilon/3.
        \label{eq:estimate_of_step_function}
    \end{align}
    Such a $D$ always exists because of uniform continuity of $f$.

    \item We use $\mathcal{F}^{(FF)}_1$ to quantize the input domain into $\mathbb{G}_D$.

    For any small $\delta > 0$, the following $\delta$-approximated step function can be constructed with one-hidden-layer feed-forward neural network: for any $z \in \R$
    \begin{align}
        \frac{
        \sigma_R\left[z/\delta\right]
        -
        \sigma_R\left[z/\delta - 1\right]
        }{D}
        = \begin{cases}
            0 & z < 0 \\
            z/\delta D & 0 \leq z < \delta \\
            1/D & \delta \leq z
        \end{cases}.
    \end{align}
    By shifting and stacking this step function, we have an approximated multiple-step function
    \begin{align*}
        &\sum_{t = 0}^{D-1} \frac{\sigma_R\left[z/\delta-t/\delta D\right]
        -
        \sigma_R\left[z/\delta - 1-t/\delta D\right]
        }{D}\\
        &\approx \quant_D(z)
        =
        \begin{cases}
            0 & z < 0 \\
            1/D & 0\leq z < 1/D \\
            \vdots & \vdots \\
            1 & 1-1/D \leq z
        \end{cases}, \numberthis
    \end{align*}
    and subtracting the last step function from it,
    \begin{align}
        \sum_{t = 1}^{D} \frac{\sigma_R\left[z/\delta-t/\delta D\right]
        -
        \sigma_R\left[z/\delta - 1-t/\delta D\right]
        }{D}
        - \left(\sigma_R\left[z/\delta - 1/\delta\right]
        -
        \sigma_R\left[z/\delta - 1 - 1/\delta\right]\right)
    \end{align}
    approximately quantize $[0,1]$ into $\{1/D,\dots,1\}$, while it projects $\R \setminus [0,1]$ to $0$.

    These operations can be realized by one-hidden-layer neural network, and it is straightforward to approximate its extension $\quant_D$ to dimension $d \times n$, which we denote $\quant_D^{d \times n}: \R^{d \times n} \to \R^{d \times n}$.
    
    In addition to that, we also add a penalty term, with which we identify whether an input sequence is in $[0,1]^{d \times n}$ or not.
    This is defined by
    \begin{align*}
        &
        \sigma_R\left[(z-1)/\delta\right]
        -
        \sigma_R\left[(z-1)/\delta - 1\right]
        -
        \sigma_R\left[-z/\delta\right]
        -
        \sigma_R\left[-z/\delta - 1\right] \\
        &\approx\penal(z)
        =
        \begin{cases}
            -1 & z \leq 0 \\
            0 & 0 < z \leq 1 \\
            -1 & 1 < z
        \end{cases}, \numberthis
    \end{align*}
    which can also be implemented by one-hidden-layer feed-forward neural network.

    Combining these components together, the first feed-forward neural network layer $\mathcal{F}^{(FF)}_1$ approximates the following function:
    \begin{align*}
        \overline{\mathcal{F}}^{(FF)}_1(\mZ) 
        = \quant_D^{d \times n}(\mZ)
        + \sum_{t=1}^d \sum_{k=1}^n \penal(\mZ_{t,k}) \numberthis
    \end{align*} 
    Note that this function quantizes inputs in $[0,1]^{d \times n}$ with granularity $D$, while every element of the output is non-positive for inputs outside $[0,1]^{d \times n}$.
    In particular, the norm of the output is upper-bounded by
    \begin{align}
        \max_{\mZ \in \R^{d \times n}}\left\|\mathcal{F}^{(FF)}_1(\mZ)_{:,k}\right\|
        = dn \cdot \sqrt{d} \label{eq:maximum_norm_of_quantization}
    \end{align}
    for any $k \in [n]$.

    \item Let $\tilde{\mathbb{G}}_D \subset \mathbb{G}_D$ be a sub-grid
    \begin{align}
        \tilde{\mathbb{G}}_D
        = \left\{
        \mL \in \mathbb{G}_D
        \relmiddle|
        \forall k,l \in [n],\ \mL_{:,k} \neq \mL_{:,l}
        \right\},
    \end{align}
    and consider memorization of $\tilde{\mathbb{G}}_D$ with its labels given by $f(\mL)$ for each $\mL \in \tilde{\mathbb{G}}_D$.
    Note that the label sets are consistent because $f$ is a permutation equivariant function.
    Then, \cref{thm:softmax_is_contextual_mapping} allows us to construct a self-attention $\mathcal{F}^{(SA)}$ to be a contextual mapping for such input sequences, because $ \tilde{\mathbb{G}}_D$ can be regarded as tokenwise $(1/D, \sqrt{d}, 1/D)$-separated input sequences, each of which has no duplicate token by definition. The idea is that when the granularity $D$ of $\mathbb{G}_D$ is sufficiently large, the number of cells with duplicate tokens, that is, $|\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D|$ is negligible compared to the total number $|\mathbb{G}_D|$ of cells, and thus the memorization of $\tilde{\mathbb{G}}_D$ suffices for universal approximation theorem.

    From the way the self-attention $\mathcal{F}^{(SA)}$ is constructed, we have
    \begin{align}
        \left\|\mathcal{F}^{(SA)}_S(\mZ)_{:,k} - \mZ_{:,k}\right\|
        < \frac{1}{4\sqrt{d}D}\max_{k' \in [n]}\left\|\mZ_{:,k'}\right\|
    \end{align}
    for any $k \in [n]$ and $\mZ \in \R^{d \times n}$.
    This follows from the fact that  $\mX^{(i)}$ in \eqref{eq:output_norm_of_self_attention} may actually be replaced with any input sequence $\mZ$, because $\vv$ in \eqref{eq:output_norm_of_self_attention} is a unit vector.
    In particular, combining this upper-bound with \eqref{eq:maximum_norm_of_quantization}, we have
    \begin{align*}
        \left\|\mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ_{:,k}\right) - \mathcal{F}^{(FF)}\left(\mZ_{:,k}\right)\right\|
        < \frac{dn}{4D}. \numberthis
    \end{align*}
    Thus, if we take large enough $D$, every element of the output for $\mZ \in \R^{d \times n} \setminus [0,1]^{d \times n}$ is upper-bounded by
    \begin{align}
        \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ\right)_{t,k} < \frac{1}{4D}
        \quad (\forall t \in [d],\ k \in [n]),
    \end{align}
    while the output for $\mZ \in [0,1]^{d \times n}$ is lower-bounded by
    \begin{align}
        \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1\left(\mZ\right)_{t,k}
        > \frac{3}{4D}
        \quad (\forall t \in [d],\ k \in [n]).
    \end{align}
    
    Therefore, what remains to show is construct a feed-forward neural network $\mathcal{F}^{(FF)}_2$ which associates the context id of each $\mL \in \tilde{\mathbb{G}}_D \subset (3/4D, \infty)^{d \times n}$ to its corresponding label, while it outputs $0$ for any input matrix $\mZ \in (-\infty, 1/4D)^{d \times n}$.
    This can be accomplished by usual bump-functions.
    Precisely, construct a bump function of scale $R > 0$
    \begin{align}
        \bump_R(\mZ)
        &=\frac{f(\mL)}{dn}\sum_{t=1}^d\sum_{k=1}^n (\sigma_R\left[R(Z_{t,k}-L_{t,k})-1\right]
        - \sigma_R\left[R(Z_{t, k}-L_{t,k})\right] \\
        &\quad\quad\quad\quad\quad\quad\quad\quad
        + \sigma_R\left[R(Z_{t,k}-L_{t,k})+1\right])
    \end{align}
    for each input sequence $\mL \in \tilde{\mathbb{G}}_D$ and add up these functions to implement $\mathcal{F}^{(FF)}_2$.

    For large enough $R > 0$, $\mathcal{F}^{(FF)}_2$ maps each input sequence $\mL \in \tilde{\mathbb{G}}_D$ to its labels $f(\mL)$ and $\mZ \in (-\infty, 1/4D)^{d \times n}$ to 0.
    In addition, the value of $\mathcal{F}^{(FF)}_2$ is always bounded: $0 \leq \mathcal{F}^{(FF)}_2 \leq 1$. Thus, by taking sufficiently small $\delta > 0$, we have
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \mathcal{F}^{(FF)}_1, \mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1\right) < \frac{\epsilon}{3}.
        \label{eq:estimate_of_quantization}
    \end{align}
    Lastly, there are $|\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D| = O\left(D^{-d}\left|\mathbb{G}_D\right|\right)$ input sequences with duplicate tokens, each corresponding to a cell of area $D^{-d}$. Thus, by taking sufficiently large $D$, areas of $\mathbb{G}_D \setminus \tilde{\mathbb{G}}_D$ becomes negligible and
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1,
        \overline{f}\right) < \frac{\epsilon}{3}. \label{eq:estimate_of_duplicate_area}
    \end{align}
    Combining \eqref{eq:estimate_of_step_function}, \eqref{eq:estimate_of_quantization} and \eqref{eq:estimate_of_duplicate_area} together, we get the approximation error of the Transformer:
    \begin{align}
        \dist_p\left(\mathcal{F}^{(FF)}_2 \circ \mathcal{F}^{(SA)}_S \circ \overline{\mathcal{F}}^{(FF)}_1,
        f\right) < \epsilon. 
    \end{align}
\end{enumerate}
\end{proof}

\section{Technical Lemmas}
We cite Lemma 13 in \citet{park_provable_2021}, with which we configure weight matrices of a self-attention mechanism.
\begin{lem}[\citet{park_provable_2021}]\label{lem:projection_into_scalar}
Let $d \in \mathbb{N}$. Then, for any finite subset $\mathcal{X} \subset \R^d$, there exists a unit vector $\vv \in \R^d$ such that
\begin{align}\label{eq:projection_into_scalar}
    \frac{1}{\left|\mathcal{X}\right|^2}\sqrt{\frac{8}{\pi d}} \left\|\vx - \vx'\right\|
    \leq \left|\vv^\top \left(\vx - \vx'\right)\right|
    \leq \left\|\vx - \vx'\right\|
\end{align}
holds for any $\vx,\vx' \in \mathcal{X}$.
\end{lem}

The following lemma follows immediately from \cref{lem:projection_into_scalar}. We use $\mW^{(K)}$ and $\mW^{(Q)}$ in the lemma as low-rank weight matrices.\footnote{It is easy to show that different unit vectors $\vv, \vv' \in \R^d$ may be used for $\mW^{(K)}$ and $\mW^{(Q)}$, respectively, that is, $\mW^{(K)} = \vu \vv^\top$ and $\mW^{(Q)} = \vu' {\vv'}^\top$, as long as $\vv$ and $\vv'$ satisfy \eqref{eq:projection_into_scalar}.}
\begin{lem}\label{lem:attention_projection}
    Given $(r_{\min},r_{\max},\epsilon)$-separated finite vocabulary $\mathcal{V} \subset \R^{d}$ with $r_{\min} > 0$. Then, for any $\delta > 0$, there exists a unit vector $\vv \in \R^d$ such that for any vectors $\vu,\vu' \in \R^s$ with 
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}},
    \end{align}
    we have
    \begin{gather}
        \left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right|
        > \delta, \\
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\|
    \end{gather}
    for any $\vv_a, \vv_b, \vv_c \in \mathcal{V}$ with $\vv_a \neq \vv_b$, where $\mW^{(K)} = \vu\vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu'\vv^\top \in \R^{s \times d}$.
\end{lem}
\begin{proof}
    By applying \cref{lem:projection_into_scalar} to $\mathcal{V} \cup \{0\}$, we know that there exists a unit vector $\vv \in \R^d$ such that for any $\vv_a,\vv_b \in \mathcal{V} \cup \{0\}$ such that $\vv_a \neq \vv_b$, we have
    \begin{align}
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_a - \vv_b\right\|
        \leq
        \left|\vv^\top \left(\vv_a - \vv_b\right)\right|
        \leq 
        \left\|\vv_a - \vv_b\right\|.
    \end{align}
    In particular, this means that for any $\vv_c \in \mathcal{V}$
    \begin{align}
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\|
        \leq
        \left|\vv^\top \vv_c\right|
        \leq 
        \left\|\vv_c\right\|
    \end{align}
    holds. Thus, pick up arbitrary vectors $\vu, \vu' \in \R^s$ with
    \begin{align}
        \left|\vu^\top \vu' \right| = \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}},
    \end{align}
    and by setting $\mW^{(K)} = \vu\vv^\top \in \R^{s \times d}$ and $\mW^{(Q)} = \vu'\vv^\top \in \R^{s \times d}$, we have
    \begin{align*}
        &\left|
        \left(\mW^{(K)}\vv_a\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        -
        \left(\mW^{(K)}\vv_b\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right| \\
        &=\left|
        \left(\vv_a - \vv_b\right)^\top \left(\mW^{(K)}\right)^\top
        \left(\mW^{(Q)}\vv_c\right)
        \right| \\
        &= \left|\left(\vv_a - \vv_b\right)^\top \vv\right|
        \cdot
        \left|\vu^\top \vu' \right|
        \cdot
        \left|\vv^\top \vv_c\right| \\
        &\geq \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}} \left\|\vv_a - \vv_b\right\|
        \cdot
        \left(|\mathcal{V}|+1\right)^4
        \frac{\pi d}{8} \frac{\delta}{\epsilon r_{\min}}
        \cdot
        \frac{1}{\left(|\mathcal{V}|+1\right)^2}\sqrt{\frac{8}{\pi d}}
        \left\|\vv_c\right\| \\
        &> \delta \numberthis,
    \end{align*}
    where the last inequality follows from $(r_{\min}, r_{\max}, \epsilon)$-separatedness of $\mathcal{V}$.
\end{proof}

\subsection{Properties of Boltzmann Operator}
For any vector $\va = (a_1,\dots,a_n) \in \R^n$, let denote $\vp = (p_1,\dots,p_n) = \sigma_S[\va]$.
In addition, we define the Boltzmann operator, partition function, and entropy as follows.
\begin{align}
    &\boltz(\va)=\sum_{i=1}^ {n}a_ip_i,\\
    &\mathcal{L}(\va)=\log\left(\sum_{i=1}^{n}e^{a_i}\right),\\
    &\mathcal{S}(\boldsymbol{p})=-\sum_{i=1}^{n}p_i\log p_i.
\end{align}
The following lemma shows that the Boltzmann operator is monotonically decreasing when the maximum and the rest of the arguments are far enough apart.
\begin{lem}[Monotonicity]\label{lem:monotonicity_of_boltzmann}
Let $n \in \mathbb{N}_+$, $i \in [n]$ and $\va = (a_1,\dots,a_n) \in \R^n$.
Then, the derivative of the Boltzmann operator $\boltz(\va) = \va^\top \sigma_S[\va]$ with respect $a_i$ is
\begin{align}
        \frac{\partial}{\partial a_i}\boltz(\va)
        = p_i(1 + \log p_i + \mathcal{S}(\vp)). \label{eq:derivative_of_boltzmann}
\end{align}
In particular, the Boltzmann operator is monotonically decreasing with respect to $a_i$ whenever
\begin{align}
    a_i < \max_{j \in [n]} a_j - \log n - 1
\end{align}
holds.
\end{lem}
\begin{proof}
Since
\begin{align*}
    \mathcal{L}(\va) - \mathcal{S}(\vp)
    &= \log\left(\sum_{j=1}^{n}e^{a_{j}}\right)
    + \sum_{k=1}^{n}p_{k}\log p_{k} \\
    &= \sum_{k=1}^n p_{k} \log \left(p_{k} \sum_{j=1}^n e^{a_j}\right) \\
    &= \sum_{k=1}^n p_k \log e^{a_k}
    = \sum_{k=1}^n p_k a_k
    = \boltz(\va), \numberthis
\end{align*}
we have
\begin{align*}
    \frac{\partial}{\partial a_i}\boltz(\va) = \frac{\partial}{\partial a_i}\mathcal{L}(\va) - \frac{\partial}{\partial a_i}\mathcal{S}(\vp). \numberthis \label{eq:decoupled_derivative_of_boltz}
\end{align*}
Notice that
\begin{align*}
    \frac{\partial p_i}{\partial a_j}
    &= \frac{\partial}{\partial a_j} \frac{e^{a_i}}{\sum_{k=1}^n e^{a_k}}\\
    &= \frac{\frac{\partial}{\partial a_j} e^{a_i} \cdot \sum_{k=1}^n e^{a_k} - e^{a_i} \cdot \frac{\partial}{\partial a_j}\sum_{k=1}^n e^{a_{k}}}{\left(\sum_{k=1}^n e^{a_{k}}\right)^2} \\
    &= \frac{\delta_{ij} e^{a_j} \cdot \sum_{k=1}^n e^{a_k} - e^{a_i} \cdot e^{a_{j}}}{\left(\sum_{k=1}^n e^{a_{k}}\right)^2} 
    = p_j (\delta_{ij} - p_i) \numberthis \label{eq:derivative_of_p}
\end{align*}
holds for any $i,j \in [n]$. Here $\delta_{ij}$ is the Kronecker delta, that is,
\begin{align}
    \delta_{ij} = \begin{cases}
        0 & \text{if $i \neq j$}, \\
        1 & \text{if $i=j$}.
    \end{cases}
\end{align}
Thus,
\begin{align*}
    \frac{\partial \mathcal{L}(\va)}{\partial a_i}
    &= \frac{e^{a_i}}{\sum_{j=1}^n e^{a_j}}
    = p_i, \numberthis \label{eq:derivative_of_logsumexp}\\
    \frac{\partial \mathcal{S}(\vp)}{\partial a_i}
    &= \sum_{j=1}^n \frac{\partial \mathcal{S}(\vp)}{\partial p_j} \cdot \frac{\partial p_j}{\partial a_i} \\
    &= \sum_{j=1}^n \frac{\partial}{\partial p_j} \left(-\sum_{k=1}^n p_k \log p_k\right) \cdot p_i(\delta_{ji} - p_j) \\
    &= \sum_{j=1}^n \left(-\log p_j - 1\right) \cdot p_i(\delta_{ji} - p_j) \\
    &= -p_i \sum_{j=1}^n \left[\delta_{ji} (\log p_j + 1) - p_j\log p_j - p_j\right] \\
    &= -p_i \left(\log p_i + 1 + \mathcal{S}(\vp) - 1\right) \\
    &= -p_i(\log p_i + \mathcal{S}(\vp)). \numberthis \label{eq:derivative_of_entropy}
\end{align*}
Plugging \eqref{eq:derivative_of_logsumexp} and \eqref{eq:derivative_of_entropy} into \eqref{eq:decoupled_derivative_of_boltz}, we have
\begin{align*}
    \frac{\partial}{\partial a_i}\boltz(\va)
    &= \frac{\partial}{\partial a_i}\mathcal{L}(\va) - \frac{\partial}{\partial a_i}\mathcal{S}(\vp) \\
    &= p_i + p_i(\log p_i + \mathcal{S}(\vp)) \\
    &= p_i(1 + \log p_i + \mathcal{S}(\vp)). \numberthis
\end{align*}
In particular, the derivative of the Boltzmann operator at index $i$ is negative when
\begin{align*}
    1 + \mathcal{S}(\vp) + \log p_i < 0
    & \Leftrightarrow 1 + \mathcal{S}(\vp) + \left(a_i - \mathcal{L}(\va)\right) < 0 \\
    & \Leftrightarrow a_i < \mathcal{L}(\va) - \mathcal{S}(\vp) - 1. \numberthis
\end{align*}
Since $\max_{j \in [n]} a_j \leq \mathcal{L}(\va)$(p.72, \cite{BoydVandenbergheConvex2004}) and $\mathcal{S}(\vp) \leq \log n$, we have $\frac{\partial}{\partial a_i} \boltz(\va) < 0$ whenever
\begin{align}
    a_i < \max_{j \in [n]} a_j - \log n - 1
\end{align}
holds.
\end{proof}

\begin{lem}[Concavity]\label{lem:concavity_of_boltzmann}
Let $n \in \mathbb{N}_+$, $i \in [n]$ and $\va = (a_1,\dots,a_n) \in \R^n$.
Then, the Boltzmann operator $\boltz(\va)$ is concave with respect to $a_i$, that is,
\begin{align}
    \frac{\partial^2}{\partial a_i^2}\boltz(\va) < 0
\end{align}
holds in a domain where $\va$ satisfies
\begin{align}
    a_i < \max_{j \in [n]} a_j - \log n - 3. \label{eq:condition_of_concavity}
\end{align}
\end{lem}
\begin{proof}
According to \cref{lem:monotonicity_of_boltzmann}, we have
\begin{align}
    \frac{\partial}{\partial a_i}\boltz(\va)
    = p_i(1 + \log p_i + \mathcal{S}(\vp)). 
\end{align}
Thus,
\begin{align*}
    \frac{\partial^2}{\partial a_i^2}\boltz(\va)
    &= \frac{\partial}{\partial a_i}\left[p_i(1 + \log p_i + \mathcal{S}(\vp))\right] \\
    &= \frac{\partial p_i}{\partial a_i} \cdot (1 + \log p_i + \mathcal{S}(\vp))
    + p_i \cdot \frac{\partial}{\partial a_i}(1 + \log p_i + \mathcal{S}(\vp)) \\
    &= p_i(1-p_i) \cdot (1 + \log p_i + \mathcal{S}(\vp))
    + p_i \cdot \left[\frac{p_i(1-p_i)}{p_i} - p_i(\log p_i + \mathcal{S}(\vp))\right] \\
    &= p_i\left[(1 - 2p_i)(\log p_i + \mathcal{S}(\vp) + 1) + 1\right], \numberthis
\end{align*}
where we used \eqref{eq:derivative_of_p} and \eqref{eq:derivative_of_entropy}.

Hereafter, we show that the right-hand side of the above equality is negative under the assumption that \eqref{eq:condition_of_concavity} holds. First, the fact that $\max_{j \in [n]} a_j \leq \mathcal{L}(\va)$(p.72, \cite{BoydVandenbergheConvex2004}) and $\mathcal{S}(\vp) \leq \log n$ implies
\begin{align}
    a_i < \max_{j \in [n]} a_j - \log n - 3
    < \mathcal{L}(\va) - \mathcal{S}(\vp) - 3.
\end{align}
It follows from $a_i - \mathcal{L}(\va) = \log p_i$ that
\begin{align}
    a_i < \max_{j \in [n]} a_j - \log n - 3
    \Rightarrow
    \log p_i < - \mathcal{S}(\vp) - 3.
\end{align}
Next, since the entropy $\mathcal{S}(\vp)$ is always non-negative, we have $\log p_i < -3$ under \eqref{eq:condition_of_concavity}.
By using an inequality $e^{-x} \leq \frac{1}{1+x}$ for $x > -1$, this implies
\begin{align}
    p_i < e^{-3} \leq \frac{1}{1 + 3} = \frac{1}{4}.
\end{align}
Thus, as long as \eqref{eq:condition_of_concavity} holds, we have
\begin{align}
    (1 - 2p_i)(\log p_i + \mathcal{S}(\vp) + 1) + 1
    &< \left(1 - 2\cdot \frac{1}{4}\right) \cdot (-2) + 1
    = 0,
\end{align}
which in turn implies $\frac{\partial^2}{\partial a_i^2}\boltz(\va) < 0$.
\end{proof}

\begin{lem}\label{lem:difference_of_boltzmann}
    Let $n \geq 2$ and $\va = (a_1,\dots,a_{n-1},a_n), \vb = (b_1, \dots b_{n-1}, b_n) \in \R^n$ be sequences such that the first $n-1$ elements of $\va$ and $\vb$ match, that is, $a_i = b_i$ for all $i \in [n-1]$. In addition, if
    \begin{align}
        \max_{i \in [n-1]} a_i - \delta
        > a_n
        > b_n
    \end{align}
    with $\delta > \log n + 3$, the difference $\boltz(\va) - \boltz(\vb)$ is lower-bounded by
    \begin{align}
        \boltz(\vb) - \boltz(\va)
        > (a_n - b_n) (\delta + a_n - b_n - \log n - 1) \cdot \frac{e^{b_n}}{\sum_{i=1}^n e^{b_i}}.
    \end{align}
\end{lem}
\begin{proof}
    According to the monotonicity (\cref{lem:monotonicity_of_boltzmann}) and concavity (\cref{lem:concavity_of_boltzmann}) of the Boltzmann operator, we have
    \begin{align}
        \boltz(b_1, \dots, b_{n-1}, x)
        + (a_n - x) \cdot \frac{\partial}{\partial x} \boltz(b_1,\dots,b_{n-1},x)
        > \boltz(b_1,\dots,b_{n-1},a_n)
    \end{align}
    for any $x < a_n$. In particular, by setting $x = b_n$ and using \eqref{eq:derivative_of_boltzmann}, this implies
    \begin{align*}
        \boltz(\vb) - \boltz(\va)
        &> (a_n - b_n) \cdot \left(-\left.\frac{\partial}{\partial x} \boltz(b_1,\dots,b_{n-1},x)\right|_{x=b_n}\right) \\
        &= (a_n - b_n)  \cdot \left[-p_n(1 + \log p_n + \mathcal{S}(\vp))\right] \\
        &> (a_n - b_n)  \cdot \left[-p_n\left(1 + b_n - \max_{i \in [n]}b_i+\log n\right)\right] \\
        &> (a_n - b_n) \cdot p_n (\delta + a_n - b_n - \log n - 1), \numberthis
    \end{align*}
    where $\vp = (p_1,\dots,p_n) \in \R^n$ is the softmax function of $\vb$, i.e., $\vp = \sigma_S[\vb]$.
\end{proof}


\subsection{\texorpdfstring{Proof of \cref{lem: boltzmann_separation}}{}}
Before moving on to the proof, we first illustrate the proof sketch of \cref{lem: boltzmann_separation} using a simple example.

Since the Boltzmann operator is permutation invariant, without loss of generality we assume the elements of $\va^{(i)}$ and $\va^{(j)}$ are sorted in descending order, e.g., $\va^{(i)} = (8,6,5)$ and $\va^{(j)}=(4,3,1)$.
In this case, since the Boltzmann operator $\boltz$ can be regarded as an approximation of $\max$, we have 
\begin{equation}
    \boltz(\va^{(i)}) \approx 8 > 4 \approx \boltz(\va^{(j)}),
\end{equation}
and so the Boltzmann operator can readily separate these two inputs.

The subtle case is where the initial tokens of $a^{(i)}$ and $a^{(j)}$ are identical, but the rest of each sequences differs, like $\va^{(i)} = (8,6,5)$ and $\va^{(j)}=(8,3,1)$.
However, a closer observation reveals that if the first coordinate and the second one of the input $\va \in \R^n$ are well-separated, then $\boltz(\va)$ is monotonically decreasing for each coordinate $k=2,\dots,n$. In the above example, this intuitively implies
\begin{equation}
    \boltz\left(\va^{(j)}\right)
    \geq
    \boltz\left(\overline{\va}^{(j)}\right)
    \quad \text{with $\overline{\va}^{(j)}=(8,3,3)$}
\end{equation}
and
\begin{equation}
    \boltz\left(\va^{(i)}\right)
    \leq
    \boltz\left(\underline{\va}^{(i)}\right)
    \quad \text{with $\underline{\va}^{(i)}=(8,6,-\infty)$},
\end{equation}
or by abuse of notation, $\boltz\left(\va^{(i)}\right)
    \leq \boltz\left(8,6\right)$.

Thus, if we can show $\boltz\left(\underline{\va}^{(i)}\right) < \boltz\left(\overline{\va}^{(j)}\right)$, then $\boltz\left(\va^{(i)}\right) < \boltz\left(\va^{(j)}\right)$ holds and we know that the Boltzmann operator can also separate this pattern of inputs.
Fortunately, this is indeed the case if each element of the inputs is sufficiently separated, because
\begin{align*}
    \boltz\left(\overline{\va}^{(j)}\right)
    = \frac{8e^8 + 3e^3 + 3e^3}{e^8 + e^3 + e^3}
    &= \frac{8e^8 + 2\cdot 3e^3}{e^8 + 2\cdot e^3} \\
    &\approx \frac{8e^8 + (3+\log 2)e^{3+\log 2}}{e^8 + \cdot e^{3+\log 2}}
    = \boltz\left(8, 3+\log 2\right), \numberthis
\end{align*}
and we have $\boltz\left(8,6\right) < \boltz\left(8, 3+\log 2\right)$ by the monotonicity of the Boltzmann operator.

\begin{proof}[Proof of \cref{lem: boltzmann_separation}]
    We only have to show the case of $m = 2$, and for notational convenience, $\va^{(1)}$ (resp. $\va^{(2)}$) hereafter is denoted by $\va$ (resp. $\vb$). Also, since the Boltzmann operator is permutation invariant, we assume without loss of generality $a_1 > \cdots > a_n$ and $b_1 > \cdots > b_n$ (Since there is no duplicate element in $\va$, $a_k$ is strictly greater than $a_l$ for any $k < l$. The same holds for $\vb$).

    First, since the Boltzmann operator can be regarded as weighting averaging, we have
    \begin{align}
        |\boltz(\va)| \leq \max(|a_1|,|a_n|) < r.
    \end{align}
    For $\delta'$-separatedness, if $\va \neq \vb$, w.l.o.g. we assume that there exists $k \in \{0,\dots,n-1\}$ such that
    \begin{align}
        (a_1,\dots,a_k) = (b_1,\dots,b_k) \text{ and }a_{k+1} > b_{k+1}.
    \end{align}
    Then, \cref{lem:eval_of_boltzmann_separation} implies that we have
    \begin{align}
        \left|\boltz(\va) - \boltz(\vb)\right|
        &> (\log n)^2 e^{-(a_1 - b_{k+1})}.
    \end{align}
    Since $\va$ and $\vb$ are tokenwise $(r,\delta)$-separated, $a_1 - b_{k+1} < 2r$ holds. Thus, the right-hand side of the above inequality is further lower-bounded by
    \begin{align}
        \left|\boltz(\va) - \boltz(\vb)\right|
        &> (\log n)^2 e^{-(a_1 - b_{k+1})}
        > (\log n)^2 e^{-2r}.
    \end{align}
\end{proof}

\begin{lem}\label{lem:eval_of_boltzmann_separation}
Let $\va,\vb \in \R^n$ be tokenwise $\delta$-separated vectors in a decreasing order with no duplicate element in each vector and $\delta > 2\log n + 3$, that is,
\begin{align}
    i > j &\Rightarrow a_i - a_j, b_i - b_j > \delta, \\
    a_i \neq b_j &\Rightarrow |a_i - b_j| > \delta
\end{align}
for any $i,j \in [n]$.

In addition, suppose there exists $k \in \{0,\dots,n-1\}$ such that
\begin{align}
    (a_1,\dots,a_k) = (b_1,\dots,b_k) \text{ and }a_{k+1} > b_{k+1}.
\end{align}
Then, the outputs of the Boltzmann operator are $(\log n)^2 e^{-(a_1 - b_{k+1})}$-separated, that is,
\begin{align}
    \left|\boltz(\va) - \boltz(\vb)\right|
    &> (\log n)^2 e^{-(a_1 - b_{k+1})}
\end{align}
holds.
\end{lem}
\begin{proof}
    We show the lemma by dividing it into the following two cases:
    \begin{enumerate}
        \item $k\geq1$
        
        Let $\underline{\va}$ and $\overline{\vb}$ be
        \begin{align}
            \underline{\va}
            &= \left(a_1, a_2, \dots, a_k, a_{k+1}\right) \in \R^{k+1} \\
            \overline{\vb}
            &= \left(a_1, a_2, \cdots, a_{k}, b_{k+1}, b_{k+1}, \dots, b_{k+1}\right) \in \R^n.
        \end{align}
        Then, by abuse of notation, \cref{lem:monotonicity_of_boltzmann} implies that
        \begin{align}
            \boltz\left(\va\right)
            < \underline{\va}^\top \sigma_S\left[\underline{\va}\right]
            = \boltz\left(\underline{\va}\right)
            \quad \text{and} \quad
            \boltz\left(\vb\right)
            > \boltz\left(\overline{\vb}\right),
        \end{align}
        and we only have to evaluate the magnitude of the difference $\boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)$.

        Let $\gamma_k$ and $\xi_k$ be
        \begin{align}
            \gamma_k = \sum_{l=1}^{k} a_l e^{a_l}
            \quad \text{and} \quad
            \xi_k = \sum_{l=1}^{k} e^{a_l}.
        \end{align}
        Then, $\boltz\left(\overline{\vb}\right)$ can be decomposed into
        \begin{align*}
            \boltz\left(\overline{\vb}\right)
            &= \frac{\gamma_k + (n-k)b_{k+1}e^{b_{k+1}}}{\xi_k + (n-k)e^{b_{k+1}}} \\
            &= \frac{\gamma_k + b_{k+1}e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1} + \log (n-k)}} \\
            &= \frac{\gamma_k + \left(b_{k+1} + \log (n-k)\right)e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1} + \log (n-k)}}
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}. \numberthis
        \end{align*}
        
        Therefore, the difference $\boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)$ can be written as
        \begin{align}\label{eq:difference_of_boltzmann}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \nonumber\\
            &\quad\quad\quad\quad\quad
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}.
        \end{align}
        According to \cref{lem:difference_of_boltzmann}, the first two terms on the right-hand side are evaluated as
        \begin{align*}
            &\boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \\
            &> (a_{k+1} - b_{k+1}-\log(n-k))(\delta + a_{k+1} - b_{k+1}-\log(n-k) - \log (k+1) - 1) \\
            &\quad\quad\quad\quad\cdot \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &> (\delta - \log n)(2\delta - 2\log n - 1)
            \cdot \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}. \numberthis
        \end{align*}
        Since $\delta > 2\log n + 3$ by assumption, the above inequality is further lower-bounded by
        \begin{align*}
            &\boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \\
            &> (\delta - \log n)(2\delta - 2\log n - 1)
            \cdot \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &> (\log n + 3)(2\log n + 5)
            \cdot \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}. \numberthis
        \end{align*}
        Plugging the above inequality into \eqref{eq:difference_of_boltzmann}, we see 
        \begin{align*}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &= \boltz\left(a_1,\dots,a_k, b_{k+1}+\log(n-k)\right)
            - \boltz\left(\underline{\va}\right) \nonumber\\
            &\quad\quad\quad\quad\quad
            - \frac{\log(n-k) \cdot e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}} \\
            &> \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \left[(\log n + 3)(2\log n + 5) - \log (n-k)\right] \\
            &> \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \cdot 2(\log n)^2. \numberthis \label{eq:difference_of_boltz_last}
        \end{align*}
        Lastly, notice that the following inequality follows from $\delta$-separatedness of $\va$ and $\vb$ and the assumption that $\va$ has no duplicate token:
        \begin{align*}
            \xi_k + e^{b_{k+1}+\log (n-k)}
            &< \sum_{l=1}^{k+1} e^{a_l} \quad (\because a_{k+1} > b_{k+1}+\log (n-k))\\
            &< e^{a_1} \sum_{l=1}^{k+1} e^{-(l-1)\delta} \\
            &< 2e^{a_1} \quad (\because \delta > \log 2). \numberthis
        \end{align*}
        By using this inequality, \eqref{eq:difference_of_boltz_last} is lower-bounded by
        \begin{align*}
            \boltz\left(\overline{\vb}\right) - \boltz\left(\underline{\va}\right)
            &> 
            \frac{e^{b_{k+1} + \log (n-k)}}{\xi_k + e^{b_{k+1}+\log (n-k)}}
            \cdot 2(\log n)^2 \\
            &> \frac{e^{b_{k+1} + \log (n-k)}}{2e^{a_1}}
            \cdot 2(\log n)^2 \\
            &> (\log n)^2 e^{-(a_1-b_{k+1})}, \numberthis
        \end{align*}
        which implies $\boltz(\vb) - \boltz(\va) > (\log n)^2 e^{-(a_1-b_{k+1})}$.

        \item $k = 0$

        Since the Boltzmann operator can be regarded as weighted averaging, $\boltz(\vb) \leq b_1$ always holds. Thus, it is enough to evaluate how much greater $\boltz(\va)$ is than $b_1$.

        Let $\overline{\va} \in \R^n$ be
        \begin{align*}
            \overline{\va}
            =
            (a_1, a_1-\delta, \dots, a_1-\delta). \numberthis
        \end{align*}
        Then, $\boltz(\va) > \boltz(\overline{\va})$ follows from \cref{lem:monotonicity_of_boltzmann}, and its value is
        \begin{align*}
            \boltz\left(\overline{\va}\right)
            &= \frac{a_1e^{a_1} + (n-1)(a_1-\delta)e^{a_1-\delta}}{e^{a_1} + (n-1)e^{a_1-\delta}} \\
            &= \frac{a_1 + (n-1)(a_1-\delta)e^{-\delta}}{1 + (n-1)e^{-\delta}} \\
            &= a_1 - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}}. \numberthis
        \end{align*}
        Therefore, the difference $\boltz\left(\va\right) - \boltz\left(\vb\right)$ is
        \begin{align*}
            \boltz\left(\va\right) - \boltz\left(\vb\right)
            &\geq \boltz\left(\overline{\va}\right) - b_1 \\
            &> a_1 - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}}
            - (a_1 - \delta) \\
            &= \delta - \frac{(n-1)\delta e^{-\delta}}{1 + (n-1)e^{-\delta}} \\
            &= \frac{\delta}{1 + (n-1)e^{-\delta}} \\
            &\geq \log n, \numberthis
        \end{align*}
        where the last inequality follows from the assumption $\delta > 2\log n$.
        Note that the right-hand side is greater than $(\log n)^2 e^{-(a_1-b_1)}$, because $a_1 - b_1 > \log n$ implies $\log n \cdot e^{-(a_1-b_1)} < 1$.
    \end{enumerate}
\end{proof}

\section{Extension to Masked Self-Attention}\label{sec:masked_self_attention}
Masked self-attention mechanisms are formulated as follows: the Softmax part in \eqref{eq:formulation_of_attention} is replaced by 
\begin{align}
    \sigma_S \left[
    \left(\mW_{l,i}^{(K)}\mZ \right)^\top
    \left(\mW_{l,i}^{(Q)}\mZ \right)
    + \mC
    \right]
    \in \R^{d \times n},
\end{align}
with some masking matrix $\mC \in \R^{n \times n}$ whose elements are either $0$ or $-\infty$.

Our main result \cref{thm:softmax_is_contextual_mapping} can be readily extended to the case where attention masking are conducted. The idea is that
\begin{align*}
    \boltz(\va + \vc) 
    &= (\va + \vc)^\top \sigma_S\left[\va + \vc\right] \\
    &= \va^\top \sigma_S\left[\va + \vc\right]
    + \vc^\top \sigma_S\left[\va + \vc\right] \\
    &= \va^\top \sigma_S\left[\va + \vc\right] \numberthis
\end{align*}
holds for any masking vector $\vc \in \R^n$ whose elements are either $0$ or $-\infty$.
Thus, in order to ensure that the masked attention is a contextual mapping, it is sufficient to verify $\boltz(\va + \vc)$ are well-separated.
The caveat is that the Boltzmann operator now has to distinguish inputs consisting of the same attention $\va$, but different masks $\vc_1, \vc_2$.
However, the separability in this case can also be proved in the same way as in the proof of \cref{lem: boltzmann_separation}.

\end{document}
