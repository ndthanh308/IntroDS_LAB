In this Section we will conduct the NeurIPS Ethics Review, a critical and an important step in helping ensure beneficial research outcomes. 
The research process of this paper did not cause any direct harms to person since our work is theoretical and mathematical, so our ethical analysis focuses on \emph{Societal Impact and Potential Harmful Consequences}. 

The areas of concern on the NeurIPS guidelines for social impact are  safety, security, discrimination, surveillance, deception and harassment, environmental, humans rights, and bias and fairness.

Of this list, \emph{discrimination, and bias and fairness} are the most relevant to our work. 
We will consider these notions together.

This section of the Ethics Review directs us to consider `known or anticipated consequences of research'.
The high level idea in our paper is studying and improving the robustness of fairness constrained learning in the presence of malicious noise.
Our work is a step towards models that are robust to malicious noise which would ideally result in fair \emph{and} accurate learning systems. 
This paper also exposes when models are inevitably fragile and thus direct actors towards a different solution concept or system.
These two threads would ideally have a positive benefit in mitigating discrimination and bias.

However, there exists a certain constant risk that mathematical models of bias (and models learned on data) are mismatched with
the underlying empirical reality and that when deployed supposedly unambiguously beneficial interventions can be harmful.
The malicious noise model is intentionally a worst case model with quite capacious assumptions, so we believe our results are quite general and would not fall in this trap; but these concerns are worth keeping in mind when considering a new deployment.
In particular, the field of fairness constrained learning needs more experimental verification.

Despite these risks, we thoroughly believe that it is critical to study fairness in learning since different demographic groups exist in the real world  and machine learning can affect those groups differently.  
Theoretical work like ours is critical to exposing fundamental trade-offs and 
baselines that can help craft effective fairness interventions.

We are confident our work passes high ethical standards and is a net ethical benefit to society at large. 