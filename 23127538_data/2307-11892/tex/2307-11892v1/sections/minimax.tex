
\section{Minimax Fairness}
\label{subsec:minimaxfair}
In this Section, we will consider Minimax Fairness.
Introduced in \cite{minimaxfair} this notion  optimizes for a different objective. 


Using their notation ($\epsilon_k = \mathbb{E}_{(x,y) \sim \mathcal{D}_k}[h(x) \neq y]$ or group-wise error),
\[ h^{*} = \argmin_{h \in \Delta{H}} \quad \{ \max_{ \leq k \leq K}  \epsilon_{k} (h) \} \]
Letting $OPT$ refer to the value of solution of the optimization problem, the learning goal is to find an $h$ that is $\epsilon$-approximately optimal for the mini-max objective, meaning that $h$ satisfies: 
\[ max_{k} \epsilon_{k}(h) \leq OPT + \epsilon\]

Observe that if the goal of the learner is compete with the value of $OPT$ on the unmodified data, in our malicious noise model this objective is
ineffective since if one group is of size $O(\alpha)$, the adversary can always drive the error rate on that group $\Omega(1)$.

This model seems incompatible with malicious noise due to the sensitivity of minimax fairness to small groups. 

Observe that the minimax fairness framework includes Equalized Error
rates as a special case.


%\begin{align*}
%& \min_{h \in \simplex{H}} err(h) \\
%& \text{subject to} err_{k}(h) \leq \gamma, k=1, \dots K
%\end{align*}

%Observe that when one sub-group is $O(\alpha)$ size, the adversary can always drive t