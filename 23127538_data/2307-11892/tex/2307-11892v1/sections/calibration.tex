\label{sec:calib}
In this section, we will explore a mix of Calibration \cite{dawid} notions in in our model.
Roughly Calibration requires that predicted labels correspond with observed probabilities.
In the case of weather forecasting a calibrated predictor is one such that among days with a predicted $60\%$ chance of rain, in the long run, the actual outcomes (i.e. rain/no rain) should occur approximately $60\%$ of time. 
This requirement should hold for every predicted probability of the model.
This notion has important \kmsdelete{real world} fairness implications \cite{flores2016false, chouldechova2017fair, faircalib,multicalib} since if a predictor is mis-calibrated using that classifier could result in harmful actions in high stakes settings, like over-incarceration \cite{compassgender}.
\kmsdelete{Ideally, calibrated classifiers can be used as a form of triage, in order to allocate effort or interventions to higher risk 
individuals or data-points. }
We will show that variations in the requirements of the exact calibration notion result in a substantially different accuracy loss when malicious noise is in the training data. 
%These results may be of independent interest to the calibration literature. 

\begin{comment}
To recall, as before the learning problem is 

\begin{align}
\min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y,Z)\sim\mathcal{D}} \left[\mathbbm{1}(h(X) \neq Y)\right] ~~~~\ \\
\text{subject to} & |K(z)-K(z')| \leq \delta \qquad \forall z,z'\in \mathcal{Z}. \label{FairnessConstraint}
\end{align}
where $K: h \rightarrow \mathbbm{R}$ is some notion of calibration error for $h$ for group $z$ given the true labels $y$.

Typically calibration requirements are most natural for regression problems where predictor $h$ provides 
\emph{fine-grained} scores that corresponds to the underlying 
probability of some outcome. 
\end{comment}
In this section, we will align closely  with \cite{faircalib}, where the learner wants to maximize accuracy while having  a perfectly Calibrated classifier.
Throughout our paper we have focused on binary classifiers, so in Section \ref{subsec: predparity} we consider a notion related  called predictive parity \cite{chouldechova2017fair, flores2016false}, before considering calibration notions for hypothesis with output in $[0,1]$.
%Throughout these sections we consider a property titled \emph{shared range}.
%Namely that even though the hypotheses are fine tuned for each group, these calibrated classifiers
%share the same range. 

\subsection{ Predictive Parity Lower Bound}
\label{subsec: predparity}
\begin{definition}[Predictive Parity \cite{chouldechova2017fair}]
A binary classifier $h: \mathcal{X} \rightarrow \{0,1\}$ satisfies predictive parity if for groups A and B $P_{x \sim \DA}[h(x)=1]>0$, $P_{x \sim \DB}[h(x)=1]>0$\footnote{This mild technical remark is explained a in the Appendix} and 
\[P_{(x,y) \sim \DA} [y=1 | h(x)=1] = P_{(x,y) \sim \DB} [y=1 | h(x)=1] \]
\end{definition}
In later sections we will consider other calibration notions.
Now we will consider an adversary who is attacking a learner constrained to equal predictive parity when group sizes are \emph{imbalanced}.

\begin{comment}
\begin{theorem}
With probability $1-(1-n)^{\alpha}$, there exists a FAIR ERM learner constrained learner with $O(\alpha)$
excess error. 
\end{theorem}
\end{comment}


\begin{theorem}
\label{thm:predparity}
    For a malicious adversary with corruption fraction $\alpha$, for FAIR-ERM constrained to satisfy Predictive Parity, then there is no $h \in \PQ$ with less than $\Omega(1)$ error. 
\end{theorem}

The intuition for this statement is that imbalanced group size will allow the adversary to change the conditional mean substantially.
%In expectation, 
Below, we have an informal proof:
\begin{proof}[Proof Sketch:]
Suppose $P(x \in A)=1-\alpha$ and $P(x \in B)= \alpha$.
Observe that whatever the initial value of $P_{(x,y) \sim \DB} [y=1 | h(x)=1]$, the adversary can drive this value $P_{(x,y) \sim \mathcal{D}_{Bmal}} [y=1 | h(x)=1]$ to $50\%$ or below
by adding a duplicate copy of every natural example in group $B$ with the opposite label.

Since these points are information theoretically indistinguishable, any hypothesis on Group $B$ that makes any positive predictions can do at best 
$50\%$ error and $50\%$ calibration error.
%will have to do the same for 
Any classifier on Group $A$ satisfying Predictive Parity will have to do the same, yielding our $\Omega(1)$ error.
%The full proof in Section \ref{proof:predparity}.
%\begin{align*}
 %   blehp
%\end{align*}
%Observe that this attack 
\end{proof}

\subsection{Extension to Finer Grained Hypothesis Classes}
\label{subsec:calib}
A criticism of this lower bound might be that these calibration notions are very coarse and calibration is intended for fine-grained predictors.
%and inappropriate for a binary classifier that in effect has two bins. 
%While a diversion from the rest of the paper where we tend to focus on binary classifiers, 
We now provide extensions for these lower bounds to real valued $\mathcal{H}$. 
Interestingly, we show if the learner can modify their `binning strategy', the learner can `decouple' the classifiers for the groups in the population and 
thus only suffer $O(\alpha)$ accuracy loss.
Rather than being an algorithmic trick, this attack is fundamental as it seems to occur in 
the wild
%organically 
as a type of red-lining. This is because absent further constraints, calibration is a weak notion of mere self-consistency.
%Attacks of this type motivate more constrained notions of calibration like 
We adopt the version of calibration from \cite{faircalib}.
\begin{definition}[Calibration] \label{def:calib}
A classifier $h: \mathcal{X} \rightarrow [0,1]$ is Calibrated with respect to distribution $\mathcal{D}$ if 
\[\forall r \in [0,1], r= \mathbb{E}_{(x,y) \sim \mathcal{D} }[y=1| h(x)=r]\]
We will primarily focus on the discretized version of this definition where the classifier assigns every data point to one of $R$ bins, each with a corresponding label $r$, that partition $[0,1]$ dis-jointly. 
We will refer to this partition as $[R]$ with $r \in [R]$ corresponding to the predicted probability of some bin. 
\[ \forall r \in [R], r= \mathbb{E}_{(x,y) \sim \mathcal{D} }[y=1| h(x)=r] \]
\end{definition}
Observe that nothing in this initial definition references groups. 
The natural generalization to the above definition with demographic groups is that we require calibration with respect to $\DA$ and $\DB$ 
simultaneously. 
In the sections that follow when we say `calibrated' this always refers to calibration with respect to $\DA$ and $\DB$. 

\begin{theorem}
\label{thm:calib}
    The learner wants to maximize accuracy subject to using a calibrated classifier, $h: \mathcal{X} \rightarrow [R]$ where $[R]$ is a partition of $[0,1]$ into bins.%^labelled bins with each label.
    
    The learner may modify the binning strategy after the adversary commits to a corruption strategy.
    Then an adversary with corruption fraction $\alpha$ can force at most $O(\alpha)$ excess accuracy loss over the non-corrupted optimal
    classifier. 
\end{theorem}



\subsection{Parity Calibration}
Motivated by Theorem \ref{thm:calib}, we introduce a \emph{novel} fairness notion we call \emph{Parity Calibration}\footnote{This is quite a strong fairness constraint and should be thought of as a strong prior that while conditional label distribution $\mathcal{D}_{y|x}$ can be different among groups, how much of each group falls in each risk category is the same.}.
Informally, this notion is a generalization of Statistical/Demographic parity \cite{dwork2012fairness} for the case of classifier with 
$R$ bins partitioning $[0,1]$.
\begin{definition}[Parity Calibration]
\label{def:paritycalib}
Classifier $h: \mathcal{X} \rightarrow [R]$, where $[R]$ is a partition of $[0,1]$ into labelled bins, satisfies
\emph{Parity Calibration} if the classifier is Calibrated (Definition \ref{def:calib}) \emph{and}
\begin{align*}
\forall r \in [R], P_{(x,y) \sim \DA} [h(x)=r] =  P_{(x,y) \sim \DB} [h(x)=r]
\end{align*} 
\end{definition}


%These lower bounds still hold for stronger notions of calibration error, namely $K_1(h, \mathcal{D})$ and $K_2(h, \mathcal{D})$ 
%which are average calibration error for the $l_1$ and $l_2$ norms respectively.
\begin{theorem}
\label{thm:paritycalib} 
Consider a learner maximizing accuracy subject to satisfying Parity Calibration.
%$h: \mathcal{X} \rightarrow [R]$ where $[R]$ is a partition of $[0,1]$ into labelled bins with each label.
    The learner may modify the binning strategy after the adversary commits to a corruption strategy.
    Then an adversary with corruption fraction $\alpha$ can force $\Omega(1)$ excess accuracy loss over the non-corrupted optimal
    classifier. 
\end{theorem}
We defer the proof of this statement to the appendix, but the intuition is a follows.
If the size of Group $B$ is $O(\alpha)$, then following a similar duplication strategy for Predictive Parity Theorem \ref{thm:predparity},
then the adversary can force Group $B$ to have an expected label of $50\%$, i.e.
$\forall x \in B, \mathbb{E}_{x \sim \DB}[y|x]=50\%$.
Thus any classifier that is calibrated must assign all of Group $B$ to a $50\%$ bucket.
In order to satisfy \emph{Parity Calibration}, the classifier must do the same to Group $A$, yielding $50\%$ error on Group $A$.

\kmsdelete{\subsection{Discussion}
In general these results are consistent with the observed behavior of Calibration in other parts of theoretical computer science.
If the learner/society really only cares about accuracy, then the insensitivity in Section \ref{subsec:calib} is somewhat of a feature, not a bug, 
especially if the unreliability of data in Group $B$ optimistically could be transient?
%However, advocates for stronger notions calibration would instead note that in \ref{thm: calib} 
In general, when thinking about accuracy loss and malicious in the context of fair ERM; what is the appropriate amount of sensitivity in
the learning process? We shall discuss this somewhat more in Section \ref{sec: discussion}.}
%We would observe that are substantial 


\begin{comment}
\begin{definition}[Average Calibration Error]
The avergae calibration error of a predictor $h$ (with $h: \mathcal{X} \rightarrow [0,1]$) on distribution $\mathcal{D}$ is:
\[ K_1(f, \mathcal{D}) = \sum_{v \in R(h) } P_{(x,y) \sim D} [h(x)=v]|v-\mathbbm{E}_{(x,y) \sim \mathcal{D}}[y|h(x)=v] |\]
where $R(h)$ is the range of $h$. 

Similarly, the average squared calibration error is 
    \[ K_1(f, \mathcal{D}) = \sum_{v \in R(h) } P_{(x,y) \sim D} [h(x)=v]|(v-\mathbbm{E}_{(x,y) \sim \mathcal{D}}[y|h(x)=v])^2\]\end{definition}

\begin{theorem}
    
\end{theorem}
\end{comment}


