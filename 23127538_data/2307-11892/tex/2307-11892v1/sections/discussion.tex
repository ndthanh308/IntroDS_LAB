\section{Discussion}
\label{sec: discussion}
\kmsdelete{In this work} We study \kmsreplace{Fairness-Aware PAC learning}{Fair-ERM} in the malicious noise model, and  in some cases allow 
the learner to maintain optimal overall accuracy despite the signal in Group $B$ being almost entirely washed out.
%when we allow learners to use the
%$\PQ$ randomized expansion of the hypothesis class $\mathcal{H}$
In particular we show that different fairness constraints have fundamentally different behavior in the presence of Malicious Noise, in terms of the amount of accuracy loss that a given level of Malicious Noise could cause a fairness-constrained learner to incur. 
The key to achieving our results, which are more optimistic than those in \cite{lampert}, is allowing for improper learners using the (P,Q)-randomized expansions of the given class $\mathcal{H}$.
%We \kmsreplace{present a picture of the}{prove upper and lower bounds on}
%accuracy loss for a range of fairness notions, given \kmsreplace{this simple randomization step.}{learning over $\PQ$.
%In general our results indicate Fair-ERM (given learning over $\PQ$) is more robust than claimed in \cite{lampert}.
The type of smoothness we create by using $\PQ$ seems to be a natural property that is likely shared by many natural hypothesis classes.

Fairness notions are motivated as a response to learned disparities when there is \kmsdelete{data corruption or} systemic error affecting \kmsdelete{the data for}
one group. 
Fairness notions are supposed to mitigate this by ruling out classifiers that have worse performance on a sub-group. 
This can peg both classifiers at a lower level of performance \kmsdelete{(e.g that the lower subgroup)} in order to \emph{motivate} \cite{hardt16} improving the data collection or labelling process to obtain more reliable performance. 
%So in \kmsreplace{some}{a} sense, sensitivity of the fairness notion to poor sub-group performance caused by malicious noise is the \textit{point} of fairness constraints! 
However, it also desirable that fairness constraints perform gracefully when subject to Malicious Noise because fairness constraints will be used in contexts where the data is unreliable and noisy and this might not be known to the learner.
This tension, exposed by our work, motivates 
%a revisiting of fairness notions from first principles approach and trying to axiomatize the 
%desired properties of a fairness intervention a la cryptography and privacy. \footnote{Work in multi-calibration \cite{multicalib} is a viable direction for this problem but it is unclear how 
%that and related notions behave with unreliable data. }
on going work studying the sensitivity level of fairness constraints. 
%If we we are to take a view, if a classifier is deployed 
