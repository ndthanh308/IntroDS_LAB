%\kevin{testedit}
Machine learning continues to be deployed into a wide range of settings and environments including high-stakes decisions, where poor performance can be  harmful to the recipients of a decision. 
%Social contexts infuse many aspects of the data-generation and labeling process, from how the 
%features are distributed across groups, to levels of noise in measuring those features, and to how the labels are assigned to the data-points. 
%This latent social information affecting the data pipeline prompts concerns that learned models may exhibit  disparate performance on some sub-groups, and that by the time this disparity is detected, significant harm could have occurred. 

One way to mitigate or prevent these effects in models is to modify the learning algorithm to include fairness constraints that are evaluated on the training data to result in a `fair'\footnote{Scare quotes here are meant to indicate that many important aspects of fairness are only roughly captured by these group constraints and that group constraints are a relatively weak guarantee.} model.
%These constraints are often thought of a legal or ethical requirement. 
Foundational work showed how several of the constraints \cite{klein16,chouldechova2017fair} are incompatible unless a model has perfect accuracy.

%However,
%as these constraints are often introduced as moral or legal requirements and
%There remain substantial questions about the utility of fairness constrained learning in %deployment. 
In learning theory, fundamental theorems and basic algorithms are typically proven %
and evaluated when assuming an i.i.d training/test set split.
%This simplifies the generalization problem.
%However, 
In the wild, distributions can shift substantially between model development and deployment (e.g. drift over time or due to strategic manipulation) resulting in worse and different performance when a model goes from
training and development to deployment.
%Additionally, where does the disparity in learning come from? 

%Often the disparity can be seen to come from explicit human actions in the past or differing distributions of features. 
%In other ways, the disparity seems to be operationalized as increased uncertainty when making decisions about a subgroup, due to limited or poor quality (e.g. explicitly biased) training data. 

%Collecting better data seems like the gold standard solution, but there may be substantial barriers to that solution.
%In some high stakes contexts, like medical settings, accurate decisions must be made now. 

To capture this concern about learning with unreliable data, we posit that some natural distribution does exist and most of the data does come from this distribution but there is an Adversary who can arbitrarily manipulate some fraction of the available distribution, in order to `fool' the classifier. 

In this malicious noise model, the goal is to learn a classifier with good performance on natural examples and  the fear is that synthetic examples may mislead the learning process. 
Lower bounds in this model typically argue that the malicious noise makes two distributions indistinguishable, so the learner is forced to exhibit high error on one of the two distributions.

Recent work in this adversarial model \cite{lampert}\footnote{Our learning settings are similar, with a key difference using an improper classifier.} 
has studied the question of fairness constrained learning with empirical risk minimization.
At a high level, that work shows that Fairness Constrained Learning will in the worst case return classifiers with $50\%$ errror due to the dependence on the size of the smaller sub-group.
%has the same worst case performance as unconstrained learning when subject to malicious noise, but complicated by dependence on the size of the smaller sub-group. 


In this work, we probe the durability of these negative results by allowing the learner to exhibit a randomized improper classifier, created from hypotheses in a base class, using some kind of black box processing. 
We term this `closure' of a hypothesis class $\mathcal{H}$, $\closure$, in that we are including hypotheses near to $\mathcal{H}$.
In words, for each classifier $h \in \mathcal{H}$, there exists an $h^{'} \in \closure$, parameterized by two parameters $p,q \in [0,1]$.
\begin{align*}
    h^{'} = 
    \begin{cases}
    h(x) \quad 1-p \\
    \Bern(q) \quad \text{otherwise}
    \end{cases}
\end{align*}
In words, larger $p$ means we ignore more of the information in the base classifier and rely on the $\Bern(q)$.

Similarly, to \cite{lampert}, our work emphasizes the relationship between excess accuracy loss, the size of Group $B$, and the amount of corrupted data. More details of our model will come in Section \ref{subsec:closure}. The main technical question we ask here is:
%
\begin{center}
\emph{When a fairness aware ERM-learner is being targeted by an Adversary with malicious noise, by allowing the learner to exhibit a simple improper learning rule, can we exhibit hypotheses that are as robust to malicious noise as an ERM learner who is indifferent to fairness?}
\end{center}
In the rest of the paper, we shall answer this question in the affirmative for some fairness constraints and show constant 
%
%In other words, the classical result \cite{kearns1988learning}
%\subsection{Our Contributions}



We focus on proving the existence of $h^{'} \in \closure$ that satisfies a given fairness constraint and experiences minimal accuracy loss on the natural data distribution. 
%Our proofs are constructive. 
Now we enumerate a hierarchy of fairness constraints and informally conjecture about how much accuracy is lost for each, when subject to $\alpha$ corruption.
%Here are the notions within order of robustness to malicious noise. (Again, here more robustness mean more accuracy on the natural data (despite the malicious noise) while still satisfying a given fairness constraint on biased data).
%In other words, train and select a model using the 

\begin{align*}
\textit{Demographic Parity} \geq Equal Opportunity \geq  Equalized Odds 
\geq Equal Error Rates
\end{align*}

\begin{theorem} \label{thm: zoo}
Now we combine these fairness notions into one location and discuss their error in the presence of $O(\alpha)$ corruptions. 
Observe that from \cite{malnoise}, unconstrained ERM has an unconditional lower bound of $\Omega(\alpha)$.
This is our baseline and we want Fair ERM in the closure model to compete with this.
\begin{enumerate}
    \item Unconstrained ERM excess error $\Theta(\alpha)$ [exhibit $h^{*}$]
    \item Demographic Parity also has accuracy loss at most $\Theta(\alpha)$
    \item Eopp is at worst $\Omega\sqrt{\alpha})$
    \item Equal Error Rates : $\Omega(1)$
       \item Equalized Odds gets error $\min \{ r_A, r_B \}=\Omega(1)$

    \item Calibration has error in the worst case $\Omega(1)$
    \item Mini-Max Fairness has error in the worst case $\Omega(1)$
\end{enumerate}
\end{theorem}

\begin{tabular}{ |p{2.7cm}||p{2.7cm}|p{2.7cm}|p{2.7cm}|p{2.7cm}| }
 \hline
 \multicolumn{5}{|c|}{ Results Summary} \\
 \hline
 Size of Group $B$ & Demographic Parity &  Equal Opportunity & Equal Error Rates & Equalized Odds \\
 \hline
 $ |A|=|B|$  & AF    &AFG&   004  & 420 \\
$|A| > O(\alpha) = |B|$ &   AX  & ALA   &248 & 420\\
 \hline
\end{tabular}



 
 \subsection{Related Work}

 \cite{lampert} study explore the limits of fairness-aware PAC learning within the classic malicious adversary
model of Valiant \citep{valiant}, where the adversary can replace a fraction of the data points with arbitrary data, with full knowledge of the learning algorithm, the data distribution and
the remaining samples. They focus on binary classification with two popular group fairness constraints - demographic parity (Calders et al., 2009) and equal opportunity (Hardt et al., 2016).
First, let us focus on interpreting their results on parity for a bit.





\begin{enumerate}
    \item First they show that learning under this adversarial model is provably impossible in a PAC sense - there is no learning algorithm that can ensure convergence with high probability
to a point on the accuracy-fairness Pareto front on the set of all finite hypothesis spaces, even in the limit of infinite training data.
\item Furthermore, the irreducible excess gap in the fairness measures they study is inversely proportional to the frequency of the rarer of the two protected attributes groups. This makes the robust learning problem especially hard when one of the protected subgroups in the data is underrepresented. 
\item They also show that the adversary can ensure that any learning algorithm will output a classifier that is optimal in terms of accuracy, but exhibits a large amount of unfairness. 
They also show that their hardness results are tight up to constant factors, in terms of the corruption ratio and the protected group frequencies, by proving matching upper bounds.
\end{enumerate}

Note that this group fairness constraints \cite{chouldechova2017fair,klein16, hardt16} are relatively simple to evaluate and provide relatively weak guarantees in contrast to fairness notions in \cite{dwork2012fairness, dwork2021outcome, hebert2018multicalibration}, among others.  
However, despite this weakness,  these group notions are likely to be used as primitive/first cut notions \cite{metricsinpractice}, so continuing to investigate them in parallel to the stronger fairness notions is worthwhile.

\subsection{Black Box Fairness Processing \red{incomplete}}
%move this else where
Our paper can be thought of as a way to black box post-process an existing model, in order to be fair and accurate.
 There is extensive literatue on post processing methods since they 

 Additionally, post processing in the presence of malicious noise is 
%Fair projection \cite{fairprojection}.