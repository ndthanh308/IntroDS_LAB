\subsection{Equal Opportunity}

% \begin{theorem}\label{thm:regimesEopp}
% There are three regimes depending on the size of $|B|$ ( where group $B$ is the smaller group) and we typically think of it as disadvantaged.


% Consider an initial case where $TPR_{A}=TPR_{B}=100 \%$ (think this follow for the other directions. 
% In each of

% To wit, the regimes are 
% \begin{enumerate}
%     \item $|B| = O( \alpha)$. Excess accuracy loss is $O(\alpha)$. Observe that the learner can simply exhibit the all positive classifier on Group $B$ and thus still satisfy Equal Opportunity. 
%      \item $|A|=|B|$. Excess Accuracy loss is still $O(\alpha)$. The adversary can only push the TPR rate of Group $B$ down by at most $O(\alpha)$. Then we can do a randomized closure of $h_{A}^{*}$ (e.g. if $h_{A}^{*}=1$, report $-1$ with probability $\alpha$. 
%     \item if $|B| \sim \sqrt{\alpha}$ the accuracy loss is $O(\sqrt{\alpha})$ [Hardest case]
% \end{enumerate}
% \end{theorem}

\begin{comment}
\begin{align*}
    & Error(Closure) = \min \{ \frac{\alpha}{r_{B}} (1-r_B), r_B \} \\
    & LHS = \text{ERROR when rejecting some of the true positives in Group A}  \\
    & p_{A}= P(y=1 \cap x \in A), p_{B}= P(y=1 \cap x \in B)  \\
    & r_{B} = P(x \in B) \\
    & p_{B} \in [ r_{B}^{+},1], r_{B}^{+}>0, \text{ imagine like $25 \%$} \\
    & RHS =\text{error when accepting everyone in Group B} \\
    &\frac{\alpha}{r_{B}^{+}}  r_A p_{A}= r_B (1-p_{B}) \\
      &\frac{\alpha}{r_{B}} r_A p_{A}= r_B (1-p_{B}) \\
    &\frac{\alpha}{r_{B}} (1-r_B) p_{A}= r_B (1-p_{B}) \\
    & \frac{\alpha}{r_{B}} (1-r_B) p_{A}= r_B (1-p_{B})  \quad \quad \text{dropping base rates} \\
    & \text{ approx equality for } r_B = \sqrt{\alpha} \\
    & \frac{\alpha}{\sqrt{\alpha}} (1-\sqrt{\alpha}) \sim \sqrt{\alpha} = r_B
\end{align*}
\end{comment}

% \begin{proposition}[TPR after corruption]\label{prop:fixedhypo}
%     Let $\widetilde{\mathcal{D}}$ be the corrupted distribution, and $h$ be a fixed hypothesis in $\mathcal{H}$. For a fixed group $A$, the following inequality bounds the change in True Positive Rate of $h$:
%     \begin{equation}
%         \left| \text{TPR}_A(h, \widetilde{\mathcal{D}}) - \text{TPR}_A(h, \dist) \right| \leq \frac{\alpha}{(1-\alpha) r_A^+ + \alpha }
%     \end{equation}
%     where $\text{TPR}_A(h, \dist) = P_{(x,y) \sim \DA} [h(x)=1 | y = 1]$ and $r_A^+ = P_{(x,y) \sim \DA} [y = 1]$
% \end{proposition}


\corrupttpr*

\begin{proof}[Proof of Proposition~\ref{prop:corrupttpr}]\label{proof:corrupttpr}
For a fixed group $A$, the TPR of $h$ in $\widetilde{\mathcal{D}}$ can be expressed in terms of the TPR of $h$ in the original distribution $\mathcal{D}$ as follows:
\begin{equation}
        \text{TPR}_A(h, \widetilde{\mathcal{D}}) = \frac{(1-\alpha) \text{TPR}_A(h, \mathcal{D}) \cdot \RA + E_A^+}{(1-\alpha) \RA + \alpha_A^+}
\end{equation}
where $\alpha_A$ is the proportion of the data set that is corrupted and in group $A$ and $E_A^+$ is the proportion of the data set that is corrupted, in group $A$, positively labeled and . Thus,
\begin{equation}
    \left| \text{TPR}_A(h, \widetilde{\mathcal{D}}) - \text{TPR}_A(h, \mathcal{D}) \right| = \left| \frac{E_A - \alpha_A \text{TPR}_A(h, \mathcal{D}) }{(1-\alpha) \RA + \alpha_A} \right| \leq \frac{\alpha}{(1-\alpha) r_A^+ + \alpha }
\end{equation}
since $E_A \leq \alpha$ and $\alpha_A \leq \alpha$
\end{proof}

% \begin{theorem}
%     For any hypothesis class $\mathcal{H}$ and distribution $ \dist = (\DA, \DB)$, a robust fair-ERM learner for the equal opportunity constraint in the Malicious Adversarial Model returns a hypothesis $\hhat$ such that 
%     \begin{equation*}
%         \error{\hhat} \leq O(\sqrt{\alpha})
%     \end{equation*}
% \end{theorem}

\maineopp*

\begin{proof} [Proof of Theorem~\ref{thm:maineopp}]\label{proof:maineopp}
It suffices to show that there exists $h \in \PQ$ that satisfies the guarantees above. 
Consider $\hstar \in \hclass$. By the realizability assumption, $\hstar$ satisfies the equal opportunity constraint i.e $\text{TPR}_A(h^*, \mathcal{D}) = \text{TPR}_B(h^*, \mathcal{D})$. 
After the corruption, the equal opportunity violation of $h^*$, $|\text{TPR}_A(h^*, \widetilde{\mathcal{D}}) - \text{TPR}_B(h^*, \widetilde{\mathcal{D}})|$ may increase. Now we define the following parameters ($p_z^i$ and $q_z^i$) for $i, z \in \{A, B \}$. 
\begin{equation}\label{eq:tpr-prob}
    p_z^i = \begin{cases}
        \frac{\corruptF_i(h^*) - \corruptF_z(h^*)}{1 - \corruptF_z(h^*)} & \text{if} \ \corruptF_i(h^*) \geq \corruptF_z(h^*)\\
        \frac{\corruptF_z(h^*) - \corruptF_i(h^*)}{\corruptF_z(h^*)} & \text{otherwise}\\
    \end{cases} \quad
    q_z^i = \begin{cases}
        1 & \text{if} \ \corruptF_i(h^*) \geq \corruptF_z(h^*)\\
        0 & \text{otherwise}\\
    \end{cases}
\end{equation}
One can think of the parameter $p_z^i$ as the proportion of samples in group $z$ whose outcomes needs to be changed in order to match the true positivity rate of group $i$,
\pcocomment{The parameters should be clipped so that they are in the $[0,1]$ interval. will fix later} 
    Now consider two hypothesis $\hhat_i$ for $i \in \{ A, B\}$ that behaves as follows: Given a sample $x$:
    \begin{itemize}
        \item  If $x \in A$, with probability $p_A^i$, return label $q_A^i$. Otherwise return $h^* (x)$
        \item Similarly, if $x \in B$, with probability $p_B^i$, return label $q_B^i$. Otherwise return $h^* (x)$
    \end{itemize}
One can think of $\hhat_i$ as a hypothesis that deviates from $h^*$ on every other group to make their true positive rate on the corrupted distribution match that of group $i$.
$\hhat_i \in \PQ$ for $i \in \{A, B\}$ since it follows the definition of our closure model. We will now show that $\hhat_i$ for $i \in \{ A, B\}$ satisfies the true positivity rate constraint in the corrupted distribution (i.e $\corruptF_A(\hhat_i) = \corruptF_B(\hhat_i)$ for fixed $i \in \{ A, B\}$). First, observe that for $z \in \{A, B \} $, if $\corruptF_i(h^*) \geq \corruptF_z(h^*)$, then $\corruptF_z(\hhat_i) = \corruptF_i(h^*)$. This is because
    \begin{align*}
        \corruptF_z(\hhat_i) 
        &= (1 - p_z) \corruptF_z(h^*) + p_z q_z \\
        &= \corruptF_z(h^*) + p_z(1 - \corruptF_z(h^*)) \\
        &= \corruptF_z(h^*) + \corruptF_i(h^*) - \corruptF_z(h^*) \\
        &= \corruptF_i(h^*)
    \end{align*}
    Similarly, if $\corruptF_i(h^*) < \corruptF_z(h^*)$, then $\corruptF_z(\hhat) = \corruptF_i(h^*)$. This is because
    \begin{align*}
        \corruptF_z(\hhat) 
        &= (1 - p_z) \corruptF_z(h^*) + p_z q_z \\
        &= \corruptF_z(h^*) + p_z(0 - \corruptF_z(h^*)) \\
        &= \corruptF_z(h^*) + \corruptF_i(h^*) - \corruptF_z(h^*) \\
        &= \corruptF_i(h^*)
    \end{align*}
    Thus, $\corruptF_A(\hhat_i) = \corruptF_i(h^*) = \corruptF_B(\hhat_i)$. Therefore $\hhat_i$ for $i \in \{ A, B\}$ satisfies the equal opportunity constraint in the corrupted distribution.
    
    We will now show that for at least one $\hhat_i$ for $i \in \{A, B\}$ satisfies $\error{\hhat} \leq O(\sqrt{\alpha})$. Since $\hhat_i$ deviates from $\hstar$ with probability $p_A^i$ on samples from $A$, and with probability $p_B^i$ on samples from $B$, it suffices to show that $p_A^i \cdot r_A + p_B^i \cdot r_B$ is $O(\sqrt{\alpha})$ for one some $i \in \{A, B\}$. 
    We consider the following cases:
\begin{enumerate}
    \item Suppose wlog $r_B \leq \frac{\sqrt{\alpha}}{1 - \sqrt{\alpha}}$. Then $\hat{h}_{B}$ satisfies the guarantee. This is because $p_B^B = 0$ (by equation~\ref{eq:tpr-prob} ) and $p_A^B \leq 1$. Thus, $p_A^B \cdot r_A + p_B^B \cdot r_B$ is $O(\sqrt{\alpha})$

    \item If instead $\min (r_A, r_B) > \frac{\sqrt{\alpha}}{1 - \sqrt{\alpha}}$. wlog let $B$ be a group with the highest true positive rate greater than 0.5 or the smallest true positive rate less than 0.5. At least one group must satisfy this constraint. If $B$ has the highest true positive rate greater than 0.5, then 
    \begin{align*}
    p_B^A &= \frac{\corruptF_B (h^*) - \corruptF_A (h^*)}{\corruptF_B (h^*)} \\
    &\leq \frac{\corruptF_B (h^*) - \corruptF_B (h^*) + \corruptF_A (h^*) - \corruptF_A (h^*)}{0.5} \intertext{since $\corruptF_B (h^*) \geq 0.5$ and by realizability assumption $\normalF_B (h^*) = \normalF_A (h^*)$}
    &\leq 2 |\corruptF_B (h^*) - \corruptF_B (h^*)| + 2 |\normalF_A (h^*) - \corruptF_A (h^*)| \\ \intertext{by proposition~\ref{prop:corrupttpr}}
    &\leq O (\sqrt{\alpha})
    \end{align*} 
    Thus, $p_A^A \cdot r_A + p_B^A \cdot r_B$ is at most $O(\sqrt{\alpha})$
    The case where $B$ has the smallest true positive rate follows similarly.
\end{enumerate}
The argument above can be extended to derive a bound of $\sqrt{(n-1)\alpha}$ in the general case of $n$ groups. It is important to note that this bound has a different dependency on the number of groups compared to the parity case, where there is no dependence on $n$.
% To extend, consider all the groups with size greater than alpha, then transform every group's tpr to that of the smallest tpr greater than 0.5 or largest less than 0.5
    
% Fix a group $i \in \{A, B\}$ and let $j$ be the other group. If $\corruptF_i (h^*) \geq \corruptF_j (h^*)$, then with probability $p_j = \frac{\normalF_i (h^*) -\corruptF_j (h^*)}{1 - \corruptF_j (h^*)}$, $\hhat_i$ returns a positive label for samples in group $j$ and samples in group $i$ remain unchanged. Thus, 
% \begin{align*}
%     \EE [\mathbbm{1} (\hhat_i (x) \neq h^* (x))] &\leq p_A^i \cdot r_A + p_B^i \cdot r_B \\
%     &\leq p_j^i \cdot P_{(x,y) \sim \dist } [x \in j] \\
%     &= \frac{\corruptF_i (h^*) -\corruptF_j (h^*)}{1 - \corruptF_j (h^*)} \cdot P_{(x,y) \sim \dist } [x \in j]
% \end{align*}
% Similarly, if $\corruptF_j (h^*) > \corruptF_i (h^*)$, then with probability $p_j = \frac{\corruptF_j (h^*) -\corruptF_i (h^*)}{\corruptF_j (h^*)}$, $\hhat$ returns a negative label and samples in group $i$ remain unchanged.
% \begin{align*}
%     \EE_ [\mathbbm{1}(\hhat (x) \neq h^* (x))] 
%     &\leq p_j^i \cdot P_{(x,y) \sim \dist } [x \in j] \cdot \corruptF_z (h^*) \\
%     &= \frac{\corruptF_j (h^*) -\corruptF_i (h^*)}{\corruptF_j (h^*)} \cdot P_{(x,y) \sim \dist } [x \in j] 
% \end{align*}
% Observe that the term $|\corruptF_i (h^*) -\corruptF_j (h^*)|$ can be bounded as follows:
% \begin{align*}
%     |\corruptF_i (h^*) -\corruptF_j (h^*)|
%     &= |\corruptF_i (h^*) - \normalF_i (h^*) + \normalF_j (h^*) - \corruptF_j (h^*)| \\ \intertext{ since $\normalF_i (h^*) = \normalF_j (h^*)$}
%     &= |\corruptF_i (h^*) - \normalF_i (h^*)| + |\normalF_j (h^*) - \corruptF_j (h^*)| \\
%     &= \frac{\alpha}{(1- }
% \end{align*}
% Therefore, the expected total number of samples such that $\hhat (x) \neq h^* (x)$ across the entire distribution is bounded as follows:
% \begin{align*}
%     \mathbb{E}_{(x,y) \sim \mathcal{D}} \ [\mathbbm{1} (\hhat (x) \neq h^* (x))] 
%     &= \sum_{z \in \{ A, B \}} |\corruptF_z (h^*) -\normalF_z (h^*)| \cdot P_{(x,y) \sim \dist } [x \in z] \\ 
%     &\leq \sum_{z \in \{ A, B \}} \frac{\alpha}{(1- \alpha) P_{(x,y) \sim \dist } [x \in z] + \alpha} \cdot P_{(x,y) \sim \dist } [x \in z] \\ \intertext{by proposition \ref{prop:fixedh-parity}}
%     &\leq 
% \end{align*}
\end{proof}

%\lowereopp*
%\begin{proof} [Proof of Theorem~\ref{thm:lowereopp}]\label{proof:lowereopp}
%Suppose group A is of size $1 - \sqrt{a}$ and B is of size $\sqrt{a}$. Suppose the best classifier in the hypothesis class can only attain $(1 - \sqrt{\alpha})\%$ percent TPR on both groups. An adversary with $\alpha\%$ can corrupt the distribution so that this classifier has $100 \%$ percent on corrupted distribution for group $B$. Fix a classifier $h$ returned by a learner in this setting. In order to satisfy the perceived fairness constraint of the ERM solver i.e tpr of $h$ must be the same for both groups in the corrupted distribution, then  

%\end{proof}

\lowereopp*

\begin{proof} [Proof of Theorem~\ref{thm:lowereopp}]\label{proof:lowereopp}
We will show a distribution and a malicious adversary of power $\alpha$ such that any hypothesis returned by a learner incurs at least $\sqrt{\alpha}$ expected error.
The distribution $\mathcal{D}$ will be such that $P_{x \sim \mathcal{D}}[x \in B]= \Omega(\sqrt{\alpha})$. This distribution will be supported on exactly four points $x_1 \in A, x_2 \in A, x_3 \in B, x_4 \in B$ with labels $y_1 = +, y_2 = -, y_3 = +, y_4 = -$. We also have that 
$$P_{x,y \sim \mathcal{D}}[x = x_1, y = +] = P_{x, y \sim \mathcal{D}}[x = x_2, y = -] = \frac{1 - \sqrt{\alpha}}{2}$$ 
and 
$$P_{x, y \sim \mathcal{D}}[x = x_3, y = +] = P_{x,y \sim \mathcal{D}}[x = x_4, y = -] = \frac{\sqrt{\alpha}}{2}$$
That is, each group has equal proportion of positives and negatives. 

The adversary commits to a poisoning strategy that places positive examples from Group $B$ into the negative region of the optimal classifier. That is, the adversary changes the original distribution $\mathcal{D}$ so that 
$$P_{x, y \sim \mathcal{D}}[x = x_1, y = +] = P_{x, y \sim \mathcal{D}}[x = x_2, y = -] = \frac{(1-\alpha)(1 - \sqrt{\alpha})}{2}$$
$$P_{x, y \sim \mathcal{D}}[x = x_3, y = +] =
P_{x, y \sim \mathcal{D}}[x = x_4, y = -] = \frac{(1-\alpha)\sqrt{\alpha}}{2}$$ and $P_{x, y \sim \mathcal{D}}[x = x_4, y = +] = \alpha$

We assume the perfect classifier is in the hypothesis class.
Now fix a classifier $h$ returned by a learner. This classifier must satisfy equal opportunity. Let $p_1, p_2, p_3, p_4$ be the probability that 
$h$ classifies $x_1, x_2, x_3, x_4$  as positive, respectively. 
Observe that $\widetilde{\text{TPR}}(h_A) = p_1$ and $\widetilde{\text{TPR}}(h_B) = 1 - (1 - p_4) \alpha' - (1-p_3)(1 - \alpha')$ where $\alpha' = \frac{2\sqrt{\alpha}}{(1 - \alpha) + 2\sqrt{\alpha}}$. The latter is due to the samples $(x_4, +)$ which the adversary added to the distribution. 
The adversary added an $\alpha$ amount which turned out to be an $\alpha'$ proportion of the positives in $B$. 
Since this classifier satisfies equal opportunity on the corrupted distribution, it must be the case that $p_1 = 1 - (1 - p_4) \alpha' - (1-p_3)(1 - \alpha')$. Thus, $(1 - p_1) \geq (1 - p_4) \alpha'$.
The error of $h$ on the original distribution is therefore
\begin{align*}
& (1 - p_1 + p_2) \frac{(1 - \sqrt{\alpha})}{2} + (1 - p_3 + p_4) \frac{\sqrt{\alpha}}{2} \\
\geq & \ (1 - p_1) \frac{(1 - \sqrt{\alpha})}{2} + p_4 \frac{\sqrt{\alpha}}{2} \\ \intertext{by the equal opportunity constraint}
\geq & \ (1 - p_4) \alpha' \frac{(1 - \sqrt{\alpha})}{2} + p_4 \frac{\sqrt{\alpha}}{2} \\
= & \ (1 - p_4) \cdot \frac{2\sqrt{\alpha}}{(1 - \alpha) + 2\sqrt{\alpha}} \cdot \frac{(1 - \sqrt{\alpha})}{2} + p_4 \frac{\sqrt{\alpha}}{2} \\
\geq & \ (1 - p_4) \frac{\sqrt{\alpha}}{2} + p_4 \frac{\sqrt{\alpha}}{2}  \geq \Omega (\sqrt{\alpha}) 
\end{align*}
% This decreases $TPR(h_{B}^{*})$.
% Informally we can think of the original distribution as consisting of four points, positive points from Group $A$, negative points from Group $A$,
% positive points from Group $B$, and negative points from Group $B$.
% The adversary places $\alpha$ positive points on top of the negative points from Group $B$. 

% In order to balance the True Positive Rates on the biased data, without loss of generality, we will need to correctly classify at least some of those malicious positive points in Group $B$ and  intentionally mis-classify some natural positives from Group $A$.


% Because of the corruption, an $\frac{\alpha}{\sqrt{\alpha} + \alpha} = \Omega(\sqrt{\alpha})$ of the positive points in Group $B$ are misclassified by $h^{*}$ and 
% we will need to modify $h^{*}_{B}$ to classify those points as positive. 

% %we need to balance the True Positive Rate, and we do this by increasing the True Positive Rate by some amount. 
% %we will need to exhibit a classifier that classifies some of the mal
% %Given whatever parameters $p,q$ we choose, the True Positive Rate for Group $B$ on the biased data is
% Assume we pick any $h_B \in \PQ$ such that $TPR(h_B)=1-\sqrt{\alpha}(1-\hat{p}_A)$ for some $\hat{p}_A$.
% $\hat{p}_A$ is implicitly determined by $p_A,q_A$. 

% Then the True Positive Rate equality (Equal Opportunity) requires that;
% \begin{align*}
% & TPR(h_{A}) = 1-\hat{p}_B= 1-\sqrt{\alpha}(1-\hat{p}_A) = TPR(h_B) \\
% & \hat{p}_B =\sqrt{\alpha}(1-\hat{p}_A)  
% \end{align*}
% $\hat{p}_B$ is an amount we deviate from $h_{B}^{*}$ by intentionally mis-classifying positive points in Group $A$.

% The excess error compared to $h^{*}$ is
% \begin{align*}
% & =  \hat{p}_A \sqrt{\alpha} + \hat{p}_{B}(1-\sqrt{\alpha}) \\
% & = \hat{p}_A \sqrt{\alpha}+  \sqrt{\alpha}(1-\hat{p}_A ) (1-\sqrt{\alpha}) \\
% & = \sqrt{\alpha} - \alpha - p \sqrt{\alpha} + p \alpha + p \sqrt{\alpha} = \sqrt{\alpha} - \alpha -  + p \alpha \\
% &  \geq \sqrt{\alpha} - \alpha = \Omega(\sqrt{\alpha})
% \end{align*}
% [Recall we need to weight by group size and the only deviation from $h^{*}$ of corruption is given by $\hat{p}_B$ and $1-\hat{p}_B$.
\end{proof}


