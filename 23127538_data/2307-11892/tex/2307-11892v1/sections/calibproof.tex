
\begin{proof}[Proof of Theorem ~\ref{thm:predparity}, Predictive Parity Lower Bound]  \label{proof:predparity}


%The proof of this lower bound is similar to that of Proof \ref{proof:eodds} but with the flipped conditioning. 

To show that Predictive Parity requires $\Omega(1)$ error when the adversary has corruption budget $\alpha$, even with our hypothesis class $\PQ$, 
it suffices to exhibit a `bad' distribution and matching corruption strategy; which we exhibit below. 

Recall that we require that $P_{x \sim \DA}[h(x)=1]>0$ and $P_{x \sim \DB}[h(x)=1]>0$. 
This is to avoid the case where the learner rejects all points from Group $B$.

\begin{enumerate}
\item Say Group A has $1-\alpha$ of the probability mass i.e. $P_{x \sim \calD}[x \in A] \geq 1-\alpha$ and thus $P_{x \sim \calD}[x \in B] \leq \alpha$.
\item The positive fraction for each group under distribution $\mathcal{D}$ is $P_{x \sim \DA}[y=1]=P_{x \sim D_B}[y=1]=\frac{1}{2}$
\item Since $P_{x \sim \calD}[x \in B] \leq \alpha$, the adversary has sufficient corruption budget such that they can a duplicate copy of each example in B but with the opposite label.   
That is, for each example x in Group B in the training set, the adversary adds another identical example but with the opposite label.
\end{enumerate}

This adversarial data ensures that on Group $B$, any hypothesis $h$ (of any form) will now satisfy 
\[  P_{x \sim \hat{\mathcal{D}}_{B}}[y=1 | h(x)=1 ] = P_{x \sim   \hat{\mathcal{D}}_{B}}[y=0  | h(x)=0]=\frac{1}{2} \]
due to the indistinguishable duplicated examples.  
So, for Group A, to satisfy predictive parity, both these terms must also equal $\frac{1}{2}$ and induce $50 \%$ error on Group $A$.
\end{proof}




\begin{proof}[Proof of Theorem~\ref{thm:calib}, Calibration $O(\alpha)$.]\label{proof:calib}

In order to prove this statement, we consider $h^{*}$ which is the Bayes Predictor $h^{*} = \mathbb{E}[y|x]$, but using some finite binning scheme $[R]$.
Clearly $h^{*}$ is calibrated on natural data and $h^{*}: \mathcal{X} \rightarrow [R]$. 

We  will show how to modify $h^{*}$ to still satisfy the fairness constraint on the corrupted data without losing too much accuracy, regardless
of the adversary's strategy.

In the case of calibration,  we will do this by just separately re-calibrating each group. 

Let $[\hat{R}]:=[R]$.
We will now modify $[\hat{R}]$ from $[R]$ to be calibrated on the malicious data.

That is;
For each group $g$ (i.e $g=A$ or $g=B$), for each bin $r \in [R]$ (i.e., ${x: h^{*}(x)=r}$), we create a new bin if there is no bin in $[R]$ with value  $\hat{r} = E_{g}[y | h^{*}(x)=r]$.  

That is, we define $\hat{h}(x) = \hat{r}$ for all $x \in g$ such that $h^{*}(x)=r$.

Observe that by construction, $\hat{h}$ is calibrated separately for each group, so it is calibrated overall.  We just need to analyze the excess error of $\hat{h}$ compared to $h^{*}$. 
We will show this is only
$O(\alpha)$.

Recall that our notion of accuracy for this problem is given by thresholding the binning scheme at $\frac{1}{2}$ and giving bins above that threshold value the positive prediction and predicting negative otherwise. 
%Observe that we now are considering the overall error, and can imagine points from each group falling into some bin. 
%Once points are in the bin, all the points the receive the same label,so it suffices to focus on the range $[R]$.

Observe that increase in expected error is how much that bin is shifted from the true probability $h^{*}(x)$.

For each bin $r \in[R]$, the shift in 
$|r - \hat{r}|$ is at most the fraction of points in the bin that are malicious noise. 
Let $x \in MAL$ mean point $x$ is a  corrupted point.

Then 
\begin{align*}
& \mathbb{E}_{x \sim \mathcal{D}}[h^{*}(x)-\hat{h}(x)] \leq \sum_{r \in [R]} P[x \in r] |r-\hat{r}| \\
%\leq \sum_{r \in [R]} P[x \in r] |r-\hat{r}| \\
& = \sum_{r \in [R]} P[x \in r] \frac{P[x \in r \cap x \in MAL]}{P[x \in r]} \\
& \leq \sum_{r \in [R]} P[x \in r \cap x \in MAL] = O(\alpha) \quad \text{ Definition of our Malicious Noise Model}
\end{align*}



\begin{comment}
Recall that the adversary using $\alpha$ points can force at most $O(\alpha)$ excess accuracy loss overall and we can always post-process a model to be
calibrated by simply re-labeling the bins (aka patching).
The only concern is we have maintaining calibration simultaneously with respect to $\DA$ and $\hat{\mathcal{D}}_B$. 
The fear is that a bin could have three points in it and be calibrated with respect to all three points but not when we zoom into the distribution for one group.
To make this simply, we will try to decouple the groups so every bin is homogeneous and only has points from one Group.

Assume we learn $\hat{h}$ on corrupted data with excess accuracy loss of $\alpha$.
Now we will show to post-process that classifier to be calibrated by group. 
Initially assume that $\hat{h}$ only has two bins, with values $0$ and $1$. 

%Without loss of generality we can assume all $\alpha$ points affect Group $B$, the smaller group in the population.
%To see this, we can imagine that the adversary has $2 \alpha$ points but is restrained to use $\alpha$ points targeting each group. 
%These $2\alpha$ points could increase the error of $h^{*}$ overall by $O(\alpha)$ and the error on Group $A$ by at most $O(\alpha)$.
%For each corrupted point in Group $A$, we can re-calibrate whatever bin it lies on to maintain calibration. 

Now if  Group $A$ and Group $B$ do not land in any of the same bins our proof is done, since we can simply re-label the bins of
$\hat{h}$ that have corrupted points in them to maintain calibration. 

Otherwise, by using Lemma \ref{lem:decouple} if we satisfy Definition \ref{def:means} on the corrupted points, 
we can segregate bins by Group and then re-label each bin.  
Clearly this maintains calibration per each group distribution.

If the corrupted points no longer satisfy Definition \ref{def:means}, we can merge the groups with same conditional label distribution into the same bin.
Since these values are equal, those bins are calibrated and calibrated by Group. 

Nowhere in this patching process have we have changed the prediction of $\hat{h}$ so we are done. 


%Accuracy alone on Group $B$ could of-course be bad, but the adversary has only changed our overall accuracy by $O(\alpha)$.

%We make no assumptions on the size of Group $A$ and Group $B$ and show we always pay at most $O(\alpha)$ in accuracy regardless of the size of 
%Group $B$, other than that $P(A) \geq \frac{1}{2}$
%Without loss of generality we can assume all $\alpha$ points affect Group $B$, the smaller group in the population.

%To see this, we can imagine that the adversary has $2 \alpha$ points but is restrained to use $\alpha$ points targeting each group. 
%These $\alpha$ points could increase the error of $h_{A}^{*}$ on Group $A$ by at most $O(\alpha)$ and we can re-calibrate each bin in $[R]$ for Group $A$ by
%relabelling each bin to maintain calibration. 

%Then we have suffered an $O(\alpha)$ loss but that is our accuracy loss goal. 
%Without loss of generality we can assume all $\alpha$ points affect Group $B$, the smaller group in the population.

%Recall we have binning strategy $[R]$ with bin values $[r_0 , r_1 .., r_{\tau}, .. ,r_R]$. 
%Let $\tau$ be some threshold such that all bins greater than or equal to $\tau$ receive the positive classification. 
%$Assume we have a decoupling strategy such we can consider how to calibrate the classifiers for each group seper

%Observe that every $\alpha$ point must land in some bin. 
%If that bin is empty, we simply do nothing. 
%\end{proof}

\begin{definition}[Distinct Tail-Means] \label{def:means}
Group $1, \dots, k$ have distinct tail-means if for the Bayes Risk $f(x)=\mathbb{E}[y|x]$ then 
\begin{align*}
&\mathbb{E}_{(x,y) \sim \mathcal{D}_1}[y|x, f^{*}(x) < \frac{1}{2}] \neq  \mathbb{E}_{(x,y) \sim \mathcal{D}_2}[y|x, f^{*}(x) < \frac{1}{2}] \neq  \dots \neq \mathbb{E}_{(x,y) \sim \mathcal{D}_k}[y|x, f^{*}(x) < \frac{1}{2}]  \\
& \text{ and } \\
&  \mathbb{E}_{(x,y) \sim \mathcal{D}_1}[y|x, f^{*}(x) \geq \frac{1}{2}] \neq  \mathbb{E}_{(x,y) \sim \mathcal{D}_2}[y|x, f^{*}(x) \geq \frac{1}{2}] \neq  \dots \neq \mathbb{E}_{(x,y) \sim \mathcal{D}_k}[y|x, f^{*}(x) \geq \frac{1}{2}] 
\end{align*}
\end{definition}
Observe that distributions that do not pair-wise satisfy Definition \ref{def:means} can be treated identically when we are doing the coarse binning in our proofs, so this really only means that groups are different, an extremely weak assumption if we think that $y|x$ is a continuous probability distribution. 

\begin{lemma}[Decoupling] \label{lem:decouple}
Consider a calibrated classifier $f$ with $k$ demographic groups in distribution $\mathcal{D}$ and the Groups satisfy Definition \ref{def:means}.
Then we can create a calibrated classifier $\hat{f}$ with the same accuracy and such that no points from different groups land in the same bin.
\end{lemma}  

\begin{proof}
Consider points $x$ such that  $f(x)<\frac{1}{2}$.
Using our distinct tail-means property we can assign each group to its own bin and calibrate it with the appropriate tail mean.
Then we can repeat this for when $f(x)>\frac{1}{2}$.

Accuracy is preserved because we do not change the location of any points with respect to the decision boundary at $\frac{1}{2}$.
\end{proof}




\begin{proof}[Proof of Theorem ~\ref{thm:paritycalib} Parity Calibration]  \label{proof:paritycalib}
The proof here is analogous to the core of the paper but observe that if the size of Group $B$ is $O(\alpha)$, then following a similar duplication strategy for Predictive Parity Theorem \ref{proof:predparity},
then the adversary can force Group $B$ to have an expected label of $50\%$, i.e.
$\forall x \in B, \mathbb{E}_{x \sim \DB}[y|x]=50\%$.

Thus any classifier that is calibrated must assign all of Group $B$ to the $50\%$ bucket.
In order to satisfy \emph{Parity Calibration}, the classifier must do the same to Group $A$, yielding $50\%$ error on Group $A$.

\end{proof}

\end{comment}
\end{proof}




