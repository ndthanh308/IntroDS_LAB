\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
%\usepackage{neurips_2023}

\input{arxiv_style}
\include{notations}
\usepackage{mkolar_definitions}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{comment}

\newcommand{\dist}{\mathcal{D}}
\newcommand{\DA}{\mathcal{D}_A}
\newcommand{\DB}{\mathcal{D}_B}
\newcommand{\closure}{cl(\mathcal{H})}
\newcommand{\Bern}{\text{Bernoulli}}

%\usepackage{color-edits}%[suppress] % use suppress below instead to implement all the edits and remove comments
\usepackage[suppress]{color-edits} 
\addauthor{pco}{purple}
\addauthor{kms}{orange}
\addauthor{as}{cyan}
\usepackage{alg}
\usepackage{multirow}
\usepackage{graphicx}


%\usepackage[demo]{graphicx}
%\usepackage{subfig}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}


\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%%%%%%
%Best title


\title{On the Vulnerability of Fairness Constrained Learning to Malicious Noise}


%%%%%%%%%%%%%
%\title{Malicious Noise and an Improper Fair ERM: Not Impossible Fairness Aware Learning from Corrupted Data}

%\title{ Fairness Aware Learning from Corrupted Data is not Impossible}

%%%GOOD titles 
%\title{On Fairness Constrained Learning and Malicious Noise}
%\title{On Fairness and Malicious Noise }********
%\title{Fair Learning with Malicious Noise Is Possible, with Randomized Classifiers }

%\title{On Fairness, Malicious Noise, and Improper Learning} %%% SABA LIKES THIS
%\title{Fair Classification and Malicious Noise}

%\title{On Fair Learning, Malicious Noise, and Randomization}
%\title{On the vulnerability of Fair Learning to Malicious Noise}


%\title{Fairness Through Randomization despite Malicious Noise}
%\title{Fairness and Malicious Noise}


%\title{On The Vulnerability of Fairness-Constrained Learning to Data Poisoning and Malicious Noise} 

%------
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
 Avrim Blum \\
TTIC \footnote{Toyota Technological Institute at Chicago  } \\
avrim@ttic.edu
 \and
 Princewill  Okoroafor \\
 Cornell University \\
 pco9@cornell.edu
 \and
 Aadirupa Saha \\
 TTIC \\
 aadirupa@ttic.edu
 \and
 Kevin Stangl \\
TTIC  \\
 kevin@ttic.edu
}
% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

\begin{document}


\maketitle


\input{NeurIPS23/abstract.tex}

\input{NeurIPS23/intro.tex}

\input{sections/preliminaries}

\input{NeurIPS23/multigroup}

\input{sections/main_results}

%\input{sections/parity}

%\input{sections/eopp}

%\input{sections/eodds}

\section{Main Results: Calibration}
\input{sections/calibration}

%\input{sections/minimax}

\begin{comment}
\section{Sufficient Properties for Robust Fair-ERM, Discussion from 4/18/23} 
%This isn't ready for prime time unfortunately-follow up work
In the previous section we exhibited a randomized classifier derived from a base class $\mathcal{H}$ that satisfied 
the fairness constraints on the biased data \emph{and} suffered a minimum accuracy loss.

However, there are obstacles to using this type of randomized classifier in practice.
Primarily that in fairness contexts, test time randomness feels `unfair' or in some sense procedural inappropriate in high stakes settings.
This observation motives us to try to abstract away the joint properties 
of the classifier and the distribution that would allow us to implement a deterministic classifier with the same properties as the randomized closure. 

\begin{example}
blah blah I am an example
\end{example}
\end{comment}
\input{sections/discussion}

%\newpage

% We choose the "plain" reference style
%\bibliographystyle{plain}
\bibliography{ref}

\newpage

\input{sections/appendix}

\end{document}