\section{Related Work}
\cite{kearns1988learning} introduced the notion of malicious noise which is analyzed in \cite{bshouty2002pac, auer1998line, klivans2009learning, long2011learning, awasthi2014power}.
  \cite{balcan2022robustly} considers a related adversary as a way to formalize data poisoning attacks in adversarial robustness
 \cite{goodfellow2014explaining}.
%\subsection{Fairness and Unreliable Data}

The interaction of fairness constraints with explicitly unreliable data is a critical research direction
%, which is extremely salient since 
issues of bias and fairness are often closely connected with data reliability concerns \cite{gianfrancesco2018potential}.
\kmsedit{Malicious noise is both a way to model an explicit adversary and a way to consider unknown natural issues with the data distribution.}\kmsmargincomment{more citations!}


\kmsedit{\cite{celis2021fair}, which is quite related, primarily focuses on the stronger Nasty Noise Model \cite{bshouty2002pac}
%That work focuses on providing algorithms and analysis under that model
combined with an assumption on the minimum size of groups/events. 
They do not consider how randomized post-processing makes robustness easier.}

The closest related work to ours, \cite{lampert}, studies explore the limits of fairness-aware PAC learning within the classic malicious adversary
model of Valiant \citep{valiant}, where the adversary can replace a \kmsedit{random} fraction of the data points with arbitrary data, with full knowledge of the learning algorithm, the data distribution, and the remaining examples. 
%They \footnote{Their work also considers}

\cite{lampert} focus on binary classification with two popular group fairness constraints Demographic Parity  \cite{calders2009building} and Equal Opportunity \cite{hardt16}, while we consider more constraints.
Similarly to \cite{lampert}, a key aspect of our results is how the size of the smaller group makes it more vulnerable to data corruption.

\begin{comment}
\begin{enumerate}
    \item First they show that learning under this adversarial model is provably impossible in a PAC sense - there is no learning algorithm that can ensure convergence with high probability
to a point on the accuracy-fairness Pareto front on the set of all finite hypothesis spaces, even in the limit of infinite training data.
\item Furthermore, the irreducible excess gap in the fairness measures they study is inversely proportional to the frequency of the rarer of the two protected attributes groups. This makes the robust learning problem especially hard when one of the protected subgroups in the data is underrepresented. 
\item They also show that the adversary can ensure that any learning algorithm will output a classifier that is optimal in terms of accuracy, but exhibits a large amount of unfairness. 
They also show that their hardness results are tight up to constant factors, in terms of the corruption ratio and the protected group frequencies, by proving matching upper bounds.
\end{enumerate}
\end{comment}

Note that these group fairness constraints \cite{chouldechova2017fair,klein16, hardt16} are relatively simple to evaluate and provide relatively weak guarantees in contrast to fairness notions in \cite{dwork2012fairness, dwork2021outcome, hebert2018multicalibration}, among others.  
However, despite this weakness,  these group notions are used in practice\cite{metricsinpractice} to check model performance, so continuing to investigate them in parallel to the stronger fairness notions is worthwhile.


\cite{blum2019recovering} take a somewhat converse perspective to this paper.
Instead of considering worst case instances for how much fairness constraints force excess accuracy loss with an adversary, that paper
asks how fairness constraints can help us recover from the biased data when the bias is more benign.
Future work could fill in the gaps in our results by considering an intermediate adversary model, like \cite{massart2006risk}.


%\subsection{Black Box Fairness Processing \red{incomplete}}
%move this else where
%Our paper can be thought of as a way to black box post-process an existing model, in order to be fair and accurate.
% There is extensive literature on post processing methods since they 

 %xAdditionally, post processing in the presence of malicious noise is 
%Fair projection \cite{fairprojection}.

