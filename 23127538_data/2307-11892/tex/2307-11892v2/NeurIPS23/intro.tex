
\section{Introduction}

%\kmscomment{we should move proofs to appendix. Note that the}

%\kevin{testedit}
\kmsdelete{
Machine learning continues to be deployed in a wide range of settings and environments including high-stakes decisions, where poor performance can be  harmful to the recipients of a decision. 
%Social contexts infuse many aspects of the data-generation and labeling process, from how the 
%features are distributed across groups, to levels of noise in measuring those features, and to how the labels are assigned to the data-points. 
%This latent social information affecting the data pipeline prompts concerns that learned models may exhibit  disparate performance on some sub-groups, and that by the time this disparity is detected, significant harm could have occurred. 
One way to mitigate or prevent these effects in machine learning is to require that a classifier satisfy 
%some 
fairness
\footnote{Many important aspects of fairness are only roughly captured by these group constraints and that group constraints are a relatively weak guarantee. However they are an important baseline and require continued study.} constraints  on the training data.  
%These constraints are often thought of a legal or ethical requirement. 
%Foundational work showed how several of the constraintss,
\kmsdelete{which are typically \cite{klein16,chouldechova2017fair} are incompatible unless a model has perfect accuracy.\pcomargincomment{This doesn't feel like the right place to bring up incompatibility of fairness notions- agreed we can just cite that without comment in related work}}
%However,
%as these constraints are often introduced as moral or legal requirements and
%There remain substantial questions about the utility of fairness constrained learning in %deployment. 
%In learning theory, 
Fundamental theorems and 
\kmsdelete {basic} 
algorithms are typically proven %
and evaluated when assuming an i.i.d training/test set split \pcomargincomment{i.i.d training/test set split is also done in practice right?}.
%This simplifies the generalization problem.
%However, 
In the wild \pcomargincomment{In the wild feels quite informal}, distributions can shift substantially between model development and deployment\footnote{ e.g. Due to drift over time or due to strategic manipulation.}
resulting in worse performance when a model goes from
training and development to deployment.

%Additionally, where does the disparity in learning come from? 

%Often the disparity can be seen to come from explicit human actions in the past or differing distributions of features. 
%In other ways, the disparity seems to be operationalized as increased uncertainty when making decisions about a subgroup, due to limited or poor quality (e.g. explicitly biased) training data. 

%Collecting better data seems like the gold standard solution, but there may be substantial barriers to that solution.
%In some high stakes contexts, like medical settings, accurate decisions must be made now. 

To capture this concern about learning with unreliable data, we posit that some natural distribution does exist and most of the data does come from this distribution but there is an Adversary who can arbitrarily manipulate some fraction of the available distribution, in order to fool the classifier. 
The  exact corruption model we adopt is \emph{Malicious Noise}, introduced by \cite{malnoise}, which we will explain in more detail in Section \ref{subsec:adversary}.
%The learner's goal is to learn a classifier with good performance on natural examples and  the fear is that the malicious training examples may mislead the learning algorithm \footnote{Lower bounds in this model typically argue that the malicious noise makes two distributions indistinguishable, so the learner is forced to exhibit high error on one of the two distributions.}

Recent work in this adversarial model\cite{lampert}
%\footnote{Our learning settings are similar, with a key difference using an improper classifier.} 
%has studied the behavior of fairness constrained learning with empirical risk minimization.
%By \emph{fairness constrained learning} we 
%At a high level,  
has shown that Fairness Constrained Learning for the constraints known as Demographic
Parity \cite{dwork2012fairness} and Equal Opportunity \cite{hardt16} 
will in the worst case return classifiers with $50\%$ error on 
\kmsreplace{atural}{un-modified} examples \pcomargincomment{what do you mean by natural examples: unmodified-g00d idea} due to the dependence on the size of the smaller sub-group.
%has the same worst case performance as unconstrained learning when subject to malicious noise, but complicated by dependence on the size of the smaller sub-group. 

}
The widespread adoption of machine learning algorithms across various domains, including recidivism \citep{flores2016false,dieterich2016compas}, credit lending \cite{Kozodoi_2022}, 
and predictive policing \cite{lum2016predict}, 
has raised significant concerns regarding biases and unfairness in these models. %\cite{angwin}. 
Consequently, substantial efforts have been devoted to developing approaches for learning fair classification models that exhibit \kmsreplace{comparable}{effective} \pcomargincomment{the goal of comparable was to say something like similar performance across protected attributes. I don't think effective communicates this. Maybe similar is a better term? to me effective is meant to indicate accurate for all subgroups} performance across protected attributes such as race and gender.
%[citation needed we don't need a citation for this]. 

One critical 
%\pcodelete{\kmsedit{(and in our opinion understudied)}} \pcomargincomment{don't think "and in our opinion understudied" is necessary here} 
aspect of addressing fairness in machine learning is ensuring the robustness of models against small amounts of adversarial noise present in the training data. 
This data corruption may arise due to flawed data collection or cleaning processes [citation], strategic misreporting \cite{hardt2016strategic}, under-representation of certain subgroups \cite{blum2019recovering}, or distribution shift over time \cite{schrouff2022maintaining}.

Empirical studies have demonstrated that such noise \kmsreplace{ can further amplify the unfairness of trained models}{is often centered on sensitive groups} e.g. \cite{gianfrancesco2018potential}, thereby emphasizing the need to understand the vulnerability of fair learning to adversarial perturbations. \pcodelete{from a theoretical perspective} % you added this later on, so we should remove it here
A concerning possibility is \pcoreplace{if}{that} fairness constraints \pcoreplace{could}{might} allow the adversary to amplify the effect of their \pcodelete{data}corruptions by exploiting how these constraints require the classifier to have \pcoreplace{desirable}{comparable} % I actually think comparable is a better term here
performance on every relevant sub-group, even small ones.

Previous work by \cite{lampert} and \cite{celis2021fair} have explored this topic \kmsedit{from a theoretical perspective}, \kmsdelete{albeit} considering different adversarial noise models. 
\cite{celis2021fair} focused on the eta-Hamming model, where the adversary selectively perturbs a fraction of the dataset by modifying the protected attribute. 

\cite{lampert} on the other hand, investigated the malicious noise model, where an alpha fraction of the data-set \kmsedit{(or distribution)} 
is randomly chosen and arbitrarily perturbed by the adversary.
\kmsedit{We will focus on this malicious noise model.}
\kmsdelete{In \kmsreplace{this}{our} study,we \kmsreplace{specifically build upon the findings of \cite{lampert} by focusing on}{extend the learning framework of \cite{lampert} and  consider a broader range of fairness constraints in} the malicious noise model \kmsmargincomment{this is too shy. I'd say we "we extend their learning framework and consider a broader range of fairness constraints"}.}
\kmsreplace{Their research}{\cite{lampert}} presents a \kmsdelete{rather} pessimistic outlook, highlighting data distributions in which any proper learner, particularly in scenarios with imbalanced group sizes, exhibits high vulnerability to adversarial attacks when \pcodelete{the learner is} constrained by \kmsreplace{various  fairness constraints}{Demographic Parity \cite{calders2009building} or Equal Opportunity \cite{hardt16}}. 
These results underscore the inherent challenges involved in designing fair learning algorithms resilient to adversarial manipulation \kmsedit{in the form of malicious noise}.

In this paper, we  present a more optimistic perspective on the vulnerability of fairness-constrained learning to \kmsreplace{adversarial}{malicious} noise 
%\kmsmargincomment{adversarial noise=test time attacks, not our model} 
by introducing randomized classifiers\kmsdelete{into the framework}. 
By \kmsreplace{incorporating}{allowing} randomized classifiers, we can explore alternative strategies that effectively mitigate the impact of adversarial noise and enhance the robustness of fairness-constrained models.
\pcoreplace{We also consider fairness constraints not in \cite{lampert} and characterize the robustness of each one and observe a range of vulnerabilities to malicious noise. }{In addition, we extend the analysis beyond the fairness constraints examined in \cite{lampert}, providing a complete characterization of the robustness of each constraint and revealing a diverse range of vulnerabilities to malicious noise.}


\subsection{Our Contributions}
\kmsdelete{In this work,} We \pcoreplace{probe the durability of those negative results}{bypass the impossibility results in \cite{lampert}} by allowing the learner to exhibit a randomized improper classifier, created from hypotheses in a base class \pcoedit{$\mathcal{H}$} using our \pcodelete{randomized} post-processing \pcoedit{procedure}.
We \pcoreplace{name our method}{refer to this process as the} $(P,Q)$-Randomized Expansion of a hypothesis class $\mathcal{H}$, 
or $\PQ$\footnote{$\PQ$ is \pcoreplace{so named}{chosen} because the base hypotheses are parameterized by $p,q$. for short.}
%in that we are including hypotheses near to $\mathcal{H}$.
\begin{definition}[$\PQ$] \label{defn:pq}
For each classifier $h \in \mathcal{H}$, for $p,q \in[0,1]$ \pcomargincomment{this should actually be $p,q \in[0,1]^{|Z|}$, a vector of biases for each group}
\kmsmargincomment{I think it is ok to be a bit informal in this section}
%there exists an $h^{'} \in \PQ$, 
%parameterized by two parameters $p,q \in [0,1]$.
\begin{align*}
    h_{p,q}(x) : = 
    \begin{cases}
    h(x)\quad \text{ with probability }1-p \\
    y \sim \Bern(q) \quad \text{otherwise}
    \end{cases}
\end{align*}
Using the notion of $h_{p,q}$ we define $\PQ$ as the expanded hypothesis class created by the union of all possible
$h_{p,q}(x)$.
\[ \PQ := \cup_{h \in \calH} \cup_{p,q \in [0,1]} h_{p,q}(x) \]
\end{definition}
When clear from context we drop the dependence on $p,q$ and simply refer to $\hat{h} \in \PQ$.

Larger $p$ means we ignore more of the information in the base classifier and rely on the $\Bern(q)$.
The main technical questions in this paper are:
%
\begin{center}
 \emph{
How susceptible are fairness constrained learning algorithms
to malicious noise and to what extent does this vulnerability
depend on the specific fairness notion, especially if we allow improper learning?}

%  \emph{To What extent can improper learning bypass prior impossibility results for fairness constrained
% learning?}

%\emph{When a fairness aware ERM-learner is being targeted by an Adversary with malicious noise, by allowing the learner to exhibit a simple improper learning rule ($h^{'} \in \PQ$), can we exhibit hypotheses that are as robust to malicious noise as an ERM learner who is indifferent to fairness?}
\end{center}


%In the rest of the paper, we shall answer this question in the affirmative for some fairness constraints and show constant 
%
%In other words, the classical result \cite{kearns1988learning}
%\subsection{Our Contributions}


We focus on proving the existence of $h^{'} \in \PQ$ that satisfies a given fairness constraint and experiences minimal accuracy loss on the natural data distribution. 
Recall 
\pcomargincomment{not sure we mention this before--its in abstract}
that $\alpha$ is the fraction of the overall distribution that is corrupted by the adversary.
%Our proofs are constructive. 
%We enumerate a hierarchy of fairness constraints and describe our results. 
%Here are the notions within order of robustness to malicious noise. (Again, here more robustness mean more accuracy on the natural data (despite the malicious noise) while still satisfying a given fairness constraint on biased data).
%In other words, train and select a model using the 

\begin{comment}
\begin{align*}
\textit{Demographic Parity} \geq Equal Opportunity \geq  Equalized Odds 
\geq Equal Error Rates
\end{align*}
\end{comment}

\begin{comment}
\begin{theorem} \label{thm: zoo}
Now we combine these fairness notions into one location and discuss their error in the presence of $O(\alpha)$ corruptions. 
Observe that from \cite{malnoise}, unconstrained ERM has an unconditional lower bound of $\Omega(\alpha)$.
This is our baseline and we want Fair ERM in the closure model to compete with this.
\begin{enumerate}
    \item Unconstrained ERM excess error $\Theta(\alpha)$ [exhibit $h^{*}$]
    \item Demographic Parity also has accuracy loss at most $\Theta(\alpha)$
    \item Eopp is at worst $\Omega\sqrt{\alpha})$
    \item Equal Error Rates : $\Omega(1)$
       \item Equalized Odds gets error $\min \{ r_A, r_B \}=\Omega(1)$

    \item Calibration has error in the worst case $\Omega(1)$
    %\item Min-Max Fairness has error in the worst case $\Omega(1)$
\end{enumerate}
\end{theorem}
\end{comment}

Our list of contributions is: 
\begin{enumerate}
\item We propose a way to 
%improperly learn a fair classifier in the 
bypass 
lower bounds \cite{lampert} in Fair-ERM with malicious noise by extending/smoothing \pcomargincomment{putting this here as a reminder to pick one of the two} the hypothesis class using  the $\PQ$ notion.
%\item We show that this approach returns a classifier that 
%satisfies the fairness guarantee (parity and equal opportunity) with only a small loss in 
%accuracy when compared to the best hypothesis in the class. 
\item For the parity \cite{lampert} constraint, our approach guarantees no more than $O(\alpha)$ loss in accuracy (which is \emph{optimal}
%the best one can expect 
in the malicious adversary model without fairness constraints \cite{malnoise}). 
In other words, \pcomargincomment{not sure if this is in contrast to the perspective in lampert. I think they may have said something like that while setting up their results but it might be good to reference it directly} in contrast to the perspective in \cite{lampert}, we show parity constrained ERM is just as robust to malicious noise
as \pcoreplace{typical}{unconstrained} ERM.
\item For the Equal Opportunity \cite{hardt16} constraint, we guarantee no more than $O(\sqrt{\alpha})$ accuracy loss
and this is tight, i.e no classifier can do better. 
%We also show that no approach can do better than this.
\item For the fairness constraints Equalized Odds \cite{hardt16}, Minimax Error \cite{minimaxfair}, Predictive Parity, and our novel fairness constraint Parity Calibration, we show strong negative results.
%hat is, 
Namely, for each constraint there exist natural distributions such that an adversary that can force any algorithm to return a fair classifier with $\Omega(1)$ loss in accuracy.
\item For a Calibration notion, \cite{faircalib}, we observe that the excess accuracy loss is at most $O(\alpha)$.
%\item Lastly we present sufficient conditions for a hypothesis class to be robust in the malicious adversarial model.
\end{enumerate}
%These results contradict \pcomargincomment{I think contradict is a strong wording here. Also, I think we're hammering a bit much on this distinction} parts of and extend the landscape introduced in \cite{lampert}. 
%\kmsreplace{This}{Our} work prompts \kmsdelete{high level} questions \pcomargincomment{what high level questions? It might be good to state them directly. Might not be obvious to the reader. I'm not sure what they are.} 
These results prompts questions  about the right sensitivity level of fairness constraints.
%We do not focu

\begin{comment}
\begin{tabular}{ |p{2.7cm}||p{2.7cm}|p{2.7cm}|p{2.7cm}|p{2.7cm}| }
 \hline
 \multicolumn{5}{|c|}{ Results Summary} \\
 \hline
 Size of Group $B$ & Demographic Parity &  Equal Opportunity & Equal Error Rates & Equalized Odds \\
 \hline
 $ |A|=|B|$  & AF    &AFG&   004  & 420 \\
$|A| > O(\alpha) = |B|$ &   AX  & ALA   &248 & 420\\
 \hline
\end{tabular}
\end{comment}

\input{NeurIPS23/rel_works.tex}