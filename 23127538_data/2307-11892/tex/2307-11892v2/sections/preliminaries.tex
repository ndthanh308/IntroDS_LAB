\section{Preliminaries}

\kmsreplace{We consider the problem of}{In}
fairness-constrained learning \kmsdelete{, where} the goal is to \kmsdelete{train}{learn} a classifier that achieves good predictive performance while satisfying certain fairness constraints.
Specifically, we start with a dataset consisting of examples with feature vectors $(x \in \mathcal{X})$, labels $(y \in \mathcal{Y})$, and group attributes $(z \in \mathcal{Z})$. 
We assume that each example is drawn i.i.d from a joint distribution $\mathcal{D}$ of random variables $(X,Y,Z)$. 
\kmsreplace{We assume that there}{There} are multiple groups in the dataset, and we aim to ensure that the classifier's predictions do not unfairly favor or disfavor any particular group. 
We will denote $\mathcal{D}_z$ as the conditional distribution of random variables $X$ and $Y$ given $Z = z$. \pcoedit{For simplicity, we will assume there are two \kmsedit{disjoint} groups: $A$ and $B$ in the dataset with B being the smaller of the two. However, our results apply more broadly to any number of groups.}

\kmsreplace{In learning theory, we use a finite dataset to selet }{We aim to use the data set to learn} a classifier $f : \mathcal{X} \rightarrow \mathcal{Y}$ 
%\pcocomment{might update this to $f : \mathcal{X} \times \mathcal{Z} \rightarrow \mathcal{Y}$.
%It's all about whether we are trying to explicitly say the classifier is group-aware.} %\pcocomment{Alternatively, we could add a footnote explaining the choice}
  given a hypothesis class $\mathcal{H}$. 
\kmsedit{However, in this paper we suppress sample complexity\pcodelete{/} learning issues and focus on characterizing the
accuracy properties of the best hypothesis in the expanded hypothesis class with a corrupted data distribution $\widetilde{D}$.
The goal is to probe the fundamental sensitivity of Fair-ERM to unreliable data in the large sample limit.}
  
We will typically think of the classifier $f$ as being quite group 
specific; i.e. the groups are easily identifiable from the data and the distributions $\mathcal{D}_{A}$, $\mathcal{D}_B$
\pcoreplace{are}{may be} substantially different.\footnote{This is not  necessary for our technical results but is useful for reader clarity.} 
%\pcomargincomment{at this point, we have not yet introduced A and B yet as the groups. Ok, I just added something above}

Thus we will occasionally use the notation $f_{A}$ or $f_{B}$ to refer to the behavior of the classifier only on Group $A$ or $B$ respectively.\pcomargincomment{I actually already mentioned this at the end of the realizability paragraph. It seems we have a couple repetitions.}
Additionally, the reader should think of Group $B$ as the smaller and more vulnerable group.
To this end, we consider solving the standard risk minimization problem with fairness constraints\kmsreplace{.}{, known as Fair-ERM.}
%\kmscomment{I don't love this noation with little z being a group. typically I think most people think of groups as Sets and people are in a set and sets are typically capital elements not lower case. Could keep Z as a collection of groups and then since our proofs focus on the binary case we just A and B or whatever we want to call the two groups}
%\pcocomment{Understandable. Although I wouldn't think of little z as a set here but more like an identifier for the group. It's sort of like a classification label (red or blue). I can then reason about the set of $x$ that have that classification label.
\begin{align}
\min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y,Z)\sim\mathcal{D}} \left[\mathbbm{1}(h(X) \neq Y)\right] ~~~~\ \\
\text{subject to} & ~~~~ |F_z(h)-F_{z'}(h)|\leq \delta \qquad \forall z,z'\in \mathcal{Z}. \label{FairnessConstraint}
\end{align}
where $F_z(h)$ is some fairness statistic of $h$ for group $z$ given the true labels $y$, such as \emph{true positive rate} :
$ \text{(TPR)}: F_z(h) = \mathbb{P}(h(X)=+1|Y=+1,Z=z)$.

$\delta$ represents the maximum allowable difference between the fairness statistic of the classifier for any two groups in the data-set.
As we will show, our upper bounds will produce classifiers satisfying the fairness constraint\footnote{We might require multiple fairness constraints be satisfied simultaneously, as in Equalized Odds, which would
require a trivial modification of the above optimization problem.} with the same value of $\delta$ as \pcoreplace{$h^{*}$}{the Bayes-optimal fair classifier}, but our lower bounds will apply even if we allowed a constant-sized gap in the fairness constraint.
Equivalently, we will often think of $\delta$ as zero.
%, which is consistent with our worst case perspective.
%Finally,
% \pcomargincomment{not sure how this is consistent with a worst case perspective}
%; the adversary is obviously more potent on the margin.


We make a \emph{realizability assumption} that there exists a solution to this risk minimization problem. That is, there is at least one hypothesis in the class that satisfies the fairness constraint. 
We will refer to the solution as $h^*$.
%\pcocomment{Not sure if this last sentence is necessary yet. Just putting it here to remind myself.} 
\kmsreplace{Since we have defined our}{As noted above, we allow our} hypothesis class to be group-aware, we can reason about $h^*_z$ for all $z \in Z$, where $h^*_z$ is the restriction of the optimal classifier $h^*$ to members of group $z$. 
\kmsdelete{We will use the equivalent term Fair-ERM to refer the risk minimization problem above.}

\subsection{Fairness Notions}
\kmsdelete{Our fair-ERM learning problem requires $F_z(h)$, a fairness statistic of $h$ for group $z$ given the true labels $y$.}
Many different formal notions of group fairness have previously been proposed in the literature. 
\kmsreplace{The problem of selecting}{Selecting} the “right” fairness measure is in general application-dependent and beyond the scope of this work. 
%\pcomargincomment{need to add a bit more on fairness notions here}
When we will call a classifier `unfair' it will mean that one of these fairness constraints is violated.
For the convenience of the reader, we include a table summarizing the fairness notions we consider in this paper. 
Other than Calibration, these all are notions for binary classifiers. 

\begin{center}
\begin{tabular}{ |p{4cm}||p{9cm}| }
 \hline
 \multicolumn{2}{|c|}{ Fairness Constraints}\\
 \hline
 Demographic Parity \cite{dwork2012fairness} & $ P_{(x,y) \sim \DA} [h(x)=1]= P_{(x,y) \sim \DB} [h(x)=1] $  \\
 \hline 
 Equal Opportunity \cite{hardt16} & $ P_{(x,y) \sim \DA} [h(x)=1| y=1 ]= P_{(x,y) \sim \DB} [h(x)=1| y=1 ] $ \\
 \hline
 Equalized Odds \cite{hardt16} &   $P_{(x,y) \sim \DA} [h(x)=1| y=1 ]= P_{(x,y) \sim \DB} [h(x)=1| y=1 ]$ and \\
  & $P_{(x,y) \sim \DA} [h(x)=1| y=0 ]= P_{(x,y) \sim \DB} [h(x)=1| y=0 ]$ \\
  \hline
  Predictive Parity \cite{chouldechova2017fair} & $P_{(x,y) \sim \DA} [y=1| h(x)=1 ]= P_{(x,y) \sim \DB} [y=1| h(x)=1 ]$ \\
\hline
Calibration\footnote{$h:\mathcal{X} \rightarrow [0,1]$} \cite{klein16,dawid} & $ \forall r \in [0,1],\quad  r = \mathbb{E}_{x,y \sim \mathcal{D}} [y|h(x)=r] $\\
\hline
\end{tabular}
\end{center}

In Section \ref{subsec:minimaxfair}, we will consider Minimax Fairness \cite{minimaxfair} but 
%\pcoreplace{elide}{present} 
omit the definition here for clarity.
In Section \ref{subsec:calib} we will introduce a new variant of calibration and will defer discussion of that notion until then.


\subsection{Adversary Model}
\label{subsec:adversary}
Throughout this paper, we focus on the Malicious Noise Model, introduced by \cite{malnoise}. This model considers a worst-case scenario where an adversary has complete control over an $\alpha$ proportion of the training data and can manipulate it to move the learning algorithm towards their desired outcomes.  
However, that $\alpha$ fraction is chosen uniformly from natural examples, unlike in \cite{bshouty2002pac}.

\kmsreplace{We follow the notion of a Fairness-ware adversary in \cite{lampert}.}{The core fear in \cite{lampert} and our work is that an adversary could use the fairness constraints to amplify their power when they have a small corruption budget $\alpha$.}
\kmsreplace{In our setting,}{Accordingly,}
a fairness-aware adversary is any procedure for manipulating a data set that takes a clean data set and outputs a new, corrupted data-set of the same size in order to \kmsedit{maximize the test time  error of a learner who uses that data. }

\kmsreplace{following the steps below:}{This corruption follows this scheme:}
\begin{enumerate}
    \item A point $(x,y,z)$ is drawn i.i.d from $\mathcal{D}$.
    \item With probability $\alpha \in [0,0.5)$, the adversary can replace this point with another point $(\widetilde{x},\widetilde{y},\widetilde{z})$. 
    That is, the adversary can change the feature vector, label, and the group attribute of the sample and the adversary has 
    perfect knowledge of $\mathcal{D}$.
    \item The learner observes $(\widetilde{x},\widetilde{y},\widetilde{z})$ instead of $(x,y,z)$
\end{enumerate}
\kmsdelete{We assume the adversary has knowledge of the joint distribution $\mathcal{D}$ and the hypothesis class $\mathcal{H}$. }
Equivalently, a fairness-aware adversary with power $\alpha$ can replace the joint distribution $\mathcal{D}$ with a new distribution $\widetilde{\mathcal{D}}$ that is $\alpha$-close to the original in total variation distance. 



\subsection{Core Learning Problem}
\label{subsec: learningproblem}
In the fair-ERM problem with a malicious adversary, our goal is to find the optimal classifier $h^*$ subject to a fairness constraint. However, the presence of the malicious adversary makes this objective challenging. Instead of observing samples from the true distribution $\mathcal{D}$, we observe samples from a corrupted distribution $\widetilde{\mathcal{D}}$. In the standard ERM setting, \cite{kearns1988learning} the optimal that can be learned is a classifier that is $O(\alpha)$-close to $h^*$ in terms of accuracy.

\kmsmargincomment{We don't use this alpha/beta notation later or really use the delta notion. While maybe more general I feel like it is confusing to the reader}
The fair-ERM problem with a malicious adversary introduces an additional layer of complexity, as we must also ensure fairness while achieving high accuracy. To address this, we introduce the concept of $\beta(\alpha)$-robustness with respect to a fairness constraint $F$. A learning algorithm is considered $\beta(\alpha)$-robust if it returns a classifier $f$ that satisfies two conditions. 

First, the perceived fairness, i.e, the difference in the estimated fairness constraint $\widetilde{F}$ between any two groups $z$ and $z'$ in $\widetilde{\mathcal{D}}$ should be at most $\delta$. Second, the expected error of $f$ and $h^*$ with respect to $\mathcal{D}$ should differ by at most $\beta(\alpha)$, where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$. 
This definition captures the desired properties of a learning algorithm that can perform well under the malicious adversary model while achieving both accuracy and fairness.
\begin{definition}
\label{def:robust}
We say a learning algorithm for the fair-ERM problem is $\beta(\alpha)$-robust with respect to a fairness \kmsreplace{statistic}{constraint} \pcomargincomment{technically isn't TPR a statistic and the constraint, tpr(A) = tpr(B)?} \kmsmargincomment{yes? maybe the language is a bit loose}$F$ in the malicious adversary model with $\alpha$ corruption fraction if the learning algorithm returns a classifier $f$ such that $|\widetilde{F}_z(f)-\widetilde{F}_{z'}(f)|\le \delta$ and 
\begin{equation}
| \mathbb{E}_{\mathcal{D}} \ [\mathbbm{1} (f(X, Z) \neq Y)] - \mathbb{E}_{\mathcal{D}} \ [\mathbbm{1}  (h^{*} (X, Z) \neq Y)] | \leq \beta(\alpha)
\end{equation}
where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to a hypothesis 
class $\mathcal{H}$ and $\beta$ is a function of $\alpha$.
\end{definition}
Thus, this is an agnostic learning problem \cite{HAUSSLER199278} with an adversary and fairness constraints.
As referenced in the introduction, we will allow the learner to return $f \in \PQ$, where $\PQ$ is a way to post-process each
$h \in \mathcal{H}$ using randomness.
\kmsedit{In Sections \ref{sec:mainresults} and  \ref{subsec:calib} will characterize the optimal value of $\beta$ given the relevant fairness constraint $F$ and base hypothesis class
$\mathcal{H}$.}
% \begin{align}
% \min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y)\sim\mathcal{D}} \left[\mathbbm{1}(h(X) \ne Y)\right] ~~~~\ \\
% \text{subject to} & ~~~~ |F_z(h)-F_{z'}(h)|\le \delta \qquad \forall z,z'\in Z. \label{FairnessConstraint}
% \end{align}
%\end{definition}
% \subsection{Review of Classical Results in Malicious Noise Model}
% Observe that an adversary with power $\alpha$ can always induce at least $\Omega(\alpha)$ additive accuracy loss \red{cite + add more details}

\kmsdelete{Note that throughout this paper we will be thinking of Malicious Noise as fundamentally changing the source distribution rather
than attacking a specific finite data set. 
Thus we will suppress issues of sample complexity/learning from finite samples. 
Thinking about these issues with fairness \emph{and} corrupted data is an important research direction but this paper
focuses on the core phenomenon of the adversary attacking demographic groups with malicious data. 
}

