\section{Discussion}
\label{sec: discussion}
We study Fair-ERM in the Malicious Noise model, and in some cases allow 
the learner to maintain optimal overall accuracy despite the signal in Group $B$ being almost entirely washed out.
%when we allow learners to use the
%$\PQ$ randomized expansion of the hypothesis class $\mathcal{H}$
In particular, we show that different fairness constraints have fundamentally different behavior in the presence of Malicious Noise, in terms of the amount of accuracy loss that a given level of Malicious Noise could cause a fairness-constrained learner to incur. 
The key to achieving our results, which are more optimistic than those in \cite{lampert}, is allowing for improper learners using the (P,Q)-randomized expansions of the given class $\mathcal{H}$.
%We \kmsreplace{present a picture of the}{prove upper and lower bounds on}
%accuracy loss for a range of fairness notions, given \kmsreplace{this simple randomization step.}{learning over $\PQ$.
%In general our results indicate Fair-ERM (given learning over $\PQ$) is more robust than claimed in \cite{lampert}.
The type of smoothness we create by using $\PQ$ seems to be a natural property that is likely shared by many natural hypothesis classes.

Fairness notions are motivated as a response to learned disparities when there is systemic error affecting one group. 
Fairness notions are supposed to mitigate this by ruling out classifiers that have worse performance on a sub-group. 
This can peg both classifiers at a lower level of performance in order to \emph{motivate} \cite{hardt16} improving the data collection or labelling process to obtain more reliable performance. 
%So in \kmsreplace{some}{a} sense, sensitivity of the fairness notion to poor sub-group performance caused by malicious noise is the \textit{point} of fairness constraints! 
However, it is also desirable that fairness constraints perform gracefully when subject to Malicious Noise, because fairness constraints will be used in contexts where the data is unreliable and noisy. %without the learner's knowledge.
This tension, exposed by our work, motivates 
%a revisiting of fairness notions from first principles approach and trying to axiomatize the 
%desired properties of a fairness intervention a la cryptography and privacy. \footnote{Work in multi-calibration \cite{multicalib} is a viable direction for this problem but it is unclear how 
%that and related notions behave with unreliable data. }
ongoing work studying the sensitivity level of fairness constraints. 


This work was supported in part by the National Science Foundation under grant CCF-2212968, by the
Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, by the Defense
Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in
this work do not necessarily reflect the position or the policy of the Government and no official endorsement
should be inferred. Approved for public release; distribution is unlimited.
