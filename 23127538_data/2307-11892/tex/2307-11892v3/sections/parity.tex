\corruptparity*

\begin{proof}[Proof of Proposition~\ref{prop:corruptparity}]\label{proof:corruptparity}
We want to bound the change in the proportion of positive labels assigned by $h$ when we move from the original distribution $\mathcal{D}$ to the corrupted distribution $\widetilde{\mathcal{D}}$. For a fixed group $A$, we can express the proportion of positive labels assigned by $h$ in $\widetilde{\mathcal{D}}$ in terms of the proportion of positive labels assigned by $h$ in $\mathcal{D}$ as follows:

\begin{equation}
    P_{(x,y) \sim \DAC} [h(x)=1] = \frac{(1-\alpha) P_{(x,y) \sim \DA} [h(x)=1] \cdot \RA + E_A}{(1-\alpha) \RA + \alpha_A}
\end{equation}

where $\alpha_A$ is the proportion of the data set that is corrupted and in group $A$ and $E_A$ is the proportion of the data set that is corrupted, in group $A$ and positively labeled by $h$.

Our goal is to obtain an upper bound on the difference between $P_{(x,y) \sim \DAC} [h(x)=1]$ and $P_{(x,y) \sim \DA} [h(x)=1]$.
We use the fact that $E_A \leq \alpha$ and $\alpha_A \leq \alpha$ to obtain the following upper bound:

\begin{equation}
    \left| P_{(x,y) \sim \DAC} [h(x)=1] - P_{(x,y) \sim \DA} [h(x)=1] \right| = \left| \frac{E_A - \alpha_A P_{(x,y) \sim \DA} [h(x)=1] }{(1-\alpha) \RA + \alpha_A} \right| \leq \frac{\alpha}{(1-\alpha) r_A + \alpha  }
\end{equation}
\end{proof}

\mainparity*

\begin{proof}[Proof of Theorem~\ref{thm:mainparity}]\label{proof:mainparity}
For $z \in \{A, B \}$, let $\normalF_z(h)$ and $\corruptF_z(h)$ denote the proportions of positive labels assigned by $h$ in group $z$ in the original and corrupted distributions respectively. That is, for group $A$, $\normalF_A(h) = P_{(x,y) \sim \DA} [ h (x)=1]$ and $\corruptF_A(h) = P_{(x,y) \sim \DAC} [ h (x)=1]$.
    It suffices to show that there exists $h \in \closure$ that satisfies the guarantees above. 
    % \pcocomment{Might need to add a lemma before this where we show that this is sufficient.}
    Consider $\hstar \in \hclass$. By the realizability assumption 
    % \pcocomment{there are two realizability assumptions here, one where $h^*$ satisfies the violation up to $\delta$ and one where it's exact equality. We'll use the one assuming equality and we'll add a lemma showing that things work fine for the $\delta$ violation one.}
    , $\hstar$ satisfies the parity constraint i.e $\normalF_A(h^*) = \normalF_B(h^*)$. 
    After the corruption, the parity violation of $h^*$, $|\corruptF_A(h^*) - \corruptF_B(h^*)|$ may increase. Now we define the following parameters ($p_z$ and $q_z$) for $z \in \{A, B \}$.
    \begin{equation}
        p_z = \begin{cases}
            \frac{\normalF_z(h^*) - \corruptF_z(h^*)}{1 - \corruptF_z(h^*)} & \text{if} \ \normalF_z(h^*) \geq \corruptF_z(h^*)\\
            \frac{\corruptF_z(h^*) - \normalF_z(h^*)}{\corruptF_z(h^*)} & \text{otherwise}\\
        \end{cases} \quad
        q_z = \begin{cases}
            1 & \text{if} \ \normalF_z(h^*) \geq \corruptF_z(h^*)\\
            0 & \text{otherwise}\\
        \end{cases}
    \end{equation}
    Now consider a hypothesis $\hhat$ that behaves as follows: Given a sample $x$:
    \begin{itemize}
        \item  If $x \in A$, with probability $p_A$, return label $q_A$. Otherwise return $h^* (x)$
        \item Similarly, if $x \in B$, with probability $p_B$, return label $q_B$. Otherwise return $h^* (x)$
    \end{itemize}
    $\hhat \in \PQ$ since it follows the definition of our closure model. We will now show that $\hhat$ satisfies the parity constraint in the corrupted distribution (i.e $\corruptF_A(\hhat) = \corruptF_B(\hhat)$). First, observe that for $z \in \{A, B \} $, if $\normalF_z(h^*) \geq \corruptF_z(h^*)$, then $\corruptF_z(\hhat) = \normalF_z(h^*)$. This is because
    \begin{align*}
        \corruptF_z(\hhat) 
        &= (1 - p_z) \corruptF_z(h^*) + p_z q_z \\
        &= \corruptF_z(h^*) + p_z(1 - \corruptF_z(h^*)) \\
        &= \corruptF_z(h^*) + \normalF_z(h^*) - \corruptF_z(h^*) \\
        &= \normalF_z(h^*)
    \end{align*}
    Similarly, if $\normalF_z(h^*) < \corruptF_z(h^*)$, then $\corruptF_z(\hhat) = \normalF_z(h^*)$. This is because
    \begin{align*}
        \corruptF_z(\hhat) 
        &= (1 - p_z) \corruptF_z(h^*) + p_z q_z \\
        &= \corruptF_z(h^*) + p_z(0 - \corruptF_z(h^*)) \\
        &= \corruptF_z(h^*) + \normalF_z(h^*) - \corruptF_z(h^*) \\
        &= \normalF_z(h^*)
    \end{align*}
    Thus, $\corruptF_A(\hhat) = \normalF_A(h^*) = \normalF_B(h^*) = \corruptF_B(\hhat)$. Therefore $\hhat$ satisfies the parity constraint in the corrupted distribution.
    
    We will now show that $\error{\hhat} \leq O(\alpha) $. Since $\hhat$ deviates from $\hstar$ with probability $p_A$ on samples from $A$, and with probability $p_B$ on samples from $B$, we only need to show that the proportion of samples such that $\hhat (x) \neq h^* (x)$ is small. Fix a group $z \in \{A, B\}$. If $\normalF_z (h^*) \geq \corruptF_z (h^*)$, then with probability $p_z = \frac{\normalF_z (h^*) -\corruptF_z (h^*)}{1 - \corruptF_z (h^*)}$, $\hhat$ returns a positive label for samples in group $z$. Thus, the expected proportion of samples in group $z$ such that $\hhat (x) \neq h^* (x)$ is $p_z$ times the proportion of negative labelled samples (by $h^*$) in group $z$ (since those get flipped to positive).
    \begin{align*}
        \EE_{x \in z} [\one (\hhat (x) \neq h^* (x))] &= p_z \cdot P_{(x,y) \sim \dist } [x \in z] (1 - \corruptF_z (h^*) ) \\
        &= \frac{\normalF_z (h^*) -\corruptF_z (h^*)}{1 - \corruptF_z (h^*)} \cdot P_{(x,y) \sim \dist } [x \in z] (1 - \corruptF_z (h^*) ) \\
        &= (\normalF_z (h^*) -\corruptF_z (h^*)) \cdot P_{(x,y) \sim \dist } [x \in z] 
    \end{align*}
    Similarly, if $\corruptF_z (h^*) > \normalF_z (h^*)$, then with probability $p_z = \frac{\corruptF_z (h^*) -\normalF_z (h^*)}{\corruptF_z (h^*)}$, $\hhat$ returns a negative label. Thus, the expected proportion of samples in group $z$ such that $\hhat (x) \neq h^* (x)$ is $p_z$ times the proportion of positively labelled samples (by $h^*$) in group $z$ (since those get flipped to negative).
    \begin{align*}
        \EE_{x \in z} [\one(\hhat (x) \neq h^* (x))] &= p_z \cdot P_{(x,y) \sim \dist } [x \in z] \cdot \corruptF_z (h^*) \\
        &= \frac{\corruptF_z (h^*) -\normalF_z (h^*)}{\corruptF_z (h^*)} \cdot P_{(x,y) \sim \dist } [x \in z] \cdot \corruptF_z (h^*)  \\
        &= (\corruptF_z (h^*) -\normalF_z (h^*)) \cdot P_{(x,y) \sim \dist } [x \in z] 
    \end{align*}
    Therefore, the expected total number of samples such that $\hhat (x) \neq h^* (x)$ across the entire distribution is bounded as follows:
    \begin{align*}
        \mathbb{E}_{(x,y) \sim \mathcal{D}} \ [\one (\hhat (x) \neq h^* (x))] 
        &= \sum_{z \in \{ A, B \}} |\corruptF_z (h^*) -\normalF_z (h^*)| \cdot P_{(x,y) \sim \dist } [x \in z] \\ 
        &\leq \sum_{z \in \{ A, B \}} \frac{\alpha}{(1- \alpha) P_{(x,y) \sim \dist } [x \in z] + \alpha} \cdot P_{(x,y) \sim \dist } [x \in z] \\ \intertext{by proposition \ref{prop:corruptparity}} 
        &\leq \frac{2\alpha}{(1- \alpha)}
    \end{align*}
    Note that even though the adversary can choose a different distribution at each timestep, we can wlog assume the adversary chooses the same distribution $\widetilde{D}$ where the quantity $|\corruptF_z (h^*) -\normalF_z (h^*)|$ is maximized at every timestep, as in Proposition \ref{prop:corruptparity}.
    Although the model in \cite{kearns1988learning} is slightly weaker than \cite{lampert}, this theorem holds in full generality for both models where we replace the difference $|\corruptF_z (h^*) -\normalF_z (h^*)|$ with the bounds from Lemma 2 of \cite{lampert}. The dependence on $\alpha$ remains the same in both cases.
    % \pcocomment{Because of the way I define $z$, the entire proof/construction should work for any number of groups but we would have $n \times \alpha$ in the numerator for accuracy loss. I wonder if that's avoidable} \pcocomment{Update: I think it's avoidable. The bound in proposition 1 could be improved to make the adversary's changes across all groups sum up to $\alpha$}
\end{proof}