\label{sec:calib}
In this section, we explore various notions of calibration \cite{dawid} for our model.
Calibration is a desirable property typically considered for classifiers, where predicted label probabilities should correspond to observed frequencies in the long run. For example, in weather forecasting, a well-calibrated predictor should have approximately 60\% of days with rain when it forecasts a 60\% chance of rain. This calibration requirement should hold for every predicted probability value output by the model.

Calibration has important fairness implications \cite{flores2016false, chouldechova2017fair, faircalib,multicalib} because a mis-calibrated predictor can lead to harmful actions in high-stakes settings, such as over-incarceration \cite{compassgender}. 
We show that varying the exact calibration requirements can substantially impact the model's accuracy loss when malicious noise is present in the training data.
%These results may be of independent interest to the calibration literature.

In this section, we align closely  with \cite{faircalib}, where the learner seeks to maximize accuracy while ensuring the classifier is perfectly calibrated.
Throughout this paper, we have focused on binary classifiers, so in Section \ref{subsec: predparity} we consider a related notion called Predictive Parity \cite{chouldechova2017fair, flores2016false}, before considering calibration notions for hypotheses with output in $[0,1]$.


\begin{comment}
To recall, as before the learning problem is 

\begin{align}
\min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y,Z)\sim\mathcal{D}} \left[\mathbbm{1}(h(X) \neq Y)\right] ~~~~\ \\
\text{subject to} & |K(z)-K(z')| \leq \delta \qquad \forall z,z'\in \mathcal{Z}. \label{FairnessConstraint}
\end{align}
where $K: h \rightarrow \mathbbm{R}$ is some notion of calibration error for $h$ for group $z$ given the true labels $y$.

Typically calibration requirements are most natural for regression problems where predictor $h$ provides 
\emph{fine-grained} scores that corresponds to the underlying 
probability of some outcome. 
\end{comment}

%Throughout these sections we consider a property titled \emph{shared range}.
%Namely that even though the hypotheses are fine tuned for each group, these calibrated classifiers
%share the same range. 

\subsection{ Predictive Parity Lower Bound}
\label{subsec: predparity}
\begin{definition}[Predictive Parity \cite{chouldechova2017fair}]
A binary classifier $h: \mathcal{X} \rightarrow \{0,1\}$ satisfies predictive parity if for groups A and B, $P_{x \sim \DA}[h(x)=1]>0$, $P_{x \sim \DB}[h(x)=1]>0$ and
% \footnote{This mild technical remark is explained a in the Appendix} 
\[P_{(x,y) \sim \DA} [y=1 | h(x)=1] = P_{(x,y) \sim \DB} [y=1 | h(x)=1] \]
\end{definition}
In later sections we consider other calibration notions.
Here we consider an adversary who is attacking a learner constrained by equal predictive parity when group sizes are \emph{imbalanced}.

\begin{comment}
\begin{theorem}
With probability $1-(1-n)^{\alpha}$, there exists a FAIR ERM learner constrained learner with $O(\alpha)$
excess error. 
\end{theorem}
\end{comment}


\begin{theorem}
\label{thm:predparity}
    For a malicious adversary with corruption fraction $\alpha$, for Fair-ERM constrained to satisfy Predictive Parity, then there is no $h \in \PQ$ with less than $\Omega(1)$ error. 
\end{theorem}

The intuition for this statement is that imbalanced group size will allow the adversary to change the conditional mean substantially.
%In expectation, 
Below, we have an informal proof:
\begin{proof}[Proof Sketch:]
Suppose $P(x \in A)=1-\alpha$ and $P(x \in B)= \alpha$.
Observe that whatever the initial value of $P_{(x,y) \sim \DB} [y=1 | h(x)=1]$, the adversary can drive this value $P_{(x,y) \sim \mathcal\DBC} [y=1 | h(x)=1]$ to $50\%$ or below
by adding a duplicate copy of every natural example in group $B$ with the opposite label.

Since all of these points are information-theoretically indistinguishable, any hypothesis for group $B$ that makes any positive predictions incurs at least $50\%$ error and $1/2=P_{(x,y) \sim \mathcal\DBC} [y=1 | h(x)=1]$ calibration error.
%will have to do the same for
Any classifier for group $A$ satisfying Predictive Parity will have to do the same, yielding our $\Omega(1)$ error.
%The full proof in Section \ref{proof:predparity}.
%\begin{align*}
 %   blehp
%\end{align*}
%Observe that this attack 
\end{proof}

\subsection{Extension to Finer Grained Hypothesis Classes}
\label{subsec:calib}
A criticism of this lower bound might be that these calibration notions are very coarse and calibration is intended for fine-grained predictors, meaning those that have a finer grained discretization of the probabilities in $[0,1]$.
%and inappropriate for a binary classifier that in effect has two bins. 
%While a diversion from the rest of the paper where we tend to focus on binary classifiers, 
We now provide extensions for these lower bounds to real valued $\mathcal{H}$. 
Interestingly, we show if the learner can modify their `binning strategy', the learner can `decouple' the classifiers for the groups in the population and 
thus only suffer $O(\alpha)$ accuracy loss.
%Rather than being an algorithmic trick, this attack is fundamental as it seems to occur in the wild
%organically 
%as a type of red-lining. 
%This is because absent further constraints, calibration is a weak notion of mere self-consistency.
%Attacks of this type motivate more constrained notions of calibration like 
We adopt the version of calibration from \cite{faircalib}.
\begin{definition}[Calibration] \label{def:calib}
A classifier $h: \mathcal{X} \rightarrow [0,1]$ is Calibrated with respect to distribution $\mathcal{D}$ if 
\[\forall r \in [0,1], r= \mathbb{E}_{(x,y) \sim \mathcal{D} }[y=1| h(x)=r]\]
We will primarily focus on the discretized version of this definition where the classifier assigns every data point to one of $R$ bins, each with a corresponding label $r$, that partition $[0,1]$ dis-jointly. 
We will refer to this partition as $[R]$ with $r \in [R]$ corresponding to the prediction of a bin. 
\[ \forall r \in [R], r= \mathbb{E}_{(x,y) \sim \mathcal{D} }[y=1| h(x)=r] \]
\end{definition}
%Observe that nothing in this initial definition references groups. 
%The natural generalization to the above definition with 
Calibration as a fairness requirements with demographic groups requires that the classifier $h$ is calibrated with 
respect to the group distributions $\DA$ and $\DB$ simultaneously. 
In the sections that follow when we say `calibrated' this always refers to calibration with respect to $\DA$ and $\DB$. 

\begin{theorem}
\label{thm:calib}
    The learner wants to maximize accuracy subject to using a calibrated classifier, $h: \mathcal{X} \rightarrow [R]$ where $[R]$ is a partition of $[0,1]$ into bins.%^labelled bins with each label.
    
    The learner may modify the binning strategy after the adversary commits to a corruption strategy.
    Then an adversary with corruption fraction $\alpha$ can force at most $O(\alpha)$ excess accuracy loss over the non-corrupted optimal
    classifier. 
\end{theorem}

\newpage


\subsection{Parity Calibration}
Motivated by Theorem \ref{thm:calib}, we introduce a \emph{novel} fairness notion we call \emph{Parity Calibration}\footnote{We would note that this is initial discussion of a novel fairness constraint that arose naturally from considering Theorem \ref{thm:calib}. The idea is in some cases it might be more desirable to have a more sensitive calibration notion, hence we define Parity Calibration. This notion requires further study and analysis before deployment in sensitive contexts.}
% \footnote{This is a strong fairness constraint and should be thought of as a strong prior that while conditional label distribution $\mathcal{D}_{y|x}$ can be different among groups, how much of each group falls in each risk category is the same.}.
Informally, this notion is a generalization of Statistical/Demographic parity \cite{dwork2012fairness} for the case of classifier with 
$R$ bins partitioning $[0,1]$.
\begin{definition}[Parity Calibration]
\label{def:paritycalib}
Classifier $h: \mathcal{X} \rightarrow [R]$, where $[R]$ is a partition of $[0,1]$ into labelled bins, satisfies
\emph{Parity Calibration} if the classifier is Calibrated (Definition \ref{def:calib}) \emph{and}
\begin{align*}
\forall r \in [R], P_{(x,y) \sim \DA} [h(x)=r] =  P_{(x,y) \sim \DB} [h(x)=r]
\end{align*} 
\end{definition}


%These lower bounds still hold for stronger notions of calibration error, namely $K_1(h, \mathcal{D})$ and $K_2(h, \mathcal{D})$ 
%which are average calibration error for the $l_1$ and $l_2$ norms respectively.
\begin{theorem}
\label{thm:paritycalib} 
Consider a learner maximizing accuracy subject to satisfying Parity Calibration.
%$h: \mathcal{X} \rightarrow [R]$ where $[R]$ is a partition of $[0,1]$ into labelled bins with each label.
    The learner may modify the binning strategy after the adversary commits to a corruption strategy.
    Then an adversary with corruption fraction $\alpha$ can force $\Omega(1)$ excess accuracy loss over the non-corrupted optimal
    classifier. 
\end{theorem}

%We defer the proof of this statement to the appendix, but the intuition is a follows.

If the size of Group $B$ is $O(\alpha)$, then following a similar duplication strategy for Predictive Parity Theorem \ref{thm:predparity},
then the adversary can force Group $B$ to have an expected label of $50\%$, i.e.
$\forall x \in B, \mathbb{E}_{x \sim \DB}[y|x]=50\%$.
Thus, any classifier that is calibrated must assign all of Group $B$ to a $50\%$ bucket.
In order to satisfy \emph{Parity Calibration}, the classifier must do the same to Group $A$, yielding $50\%$ error on Group $A$.

% \kmsdelete{\subsection{Discussion}
% In general these results are consistent with the observed behavior of Calibration in other parts of theoretical computer science.
% If the learner/society really only cares about accuracy, then the insensitivity in Section \ref{subsec:calib} is somewhat of a feature, not a bug, 
% especially if the unreliability of data in Group $B$ optimistically could be transient?
% %However, advocates for stronger notions calibration would instead note that in \ref{thm: calib} 
% In general, when thinking about accuracy loss and malicious in the context of fair ERM; what is the appropriate amount of sensitivity in
% the learning process? We shall discuss this somewhat more in Section \ref{sec: discussion}.}
% %We would observe that are substantial 


\begin{comment}
\begin{definition}[Average Calibration Error]
The avergae calibration error of a predictor $h$ (with $h: \mathcal{X} \rightarrow [0,1]$) on distribution $\mathcal{D}$ is:
\[ K_1(f, \mathcal{D}) = \sum_{v \in R(h) } P_{(x,y) \sim D} [h(x)=v]|v-\mathbbm{E}_{(x,y) \sim \mathcal{D}}[y|h(x)=v] |\]
where $R(h)$ is the range of $h$. 

Similarly, the average squared calibration error is 
    \[ K_1(f, \mathcal{D}) = \sum_{v \in R(h) } P_{(x,y) \sim D} [h(x)=v]|(v-\mathbbm{E}_{(x,y) \sim \mathcal{D}}[y|h(x)=v])^2\]\end{definition}

\begin{theorem}
    
\end{theorem}
\end{comment}


