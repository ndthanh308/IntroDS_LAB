\section{Main Results: Demographic Parity, Equal Opportunity and Equalized Odds}
\label{sec:mainresults}
%\kmsmargincomment{I want this version in orange and then 4.1, I did kms delete on previous version. 
%I want to strike this paragaph since I think we are being unnecessarily shy about our results. I do think the Lampert results and ours meanignfully clash and I think this extensive dive into Lampert will simply confuse the %reader/make us vulnerable to skeptical reviewers. 
%I could be amenable to a long/detailed comparsion/constrast in appendix}
We now present our technical findings for Demographic Parity, Equal Opportunity, and Equalized Odds, and show how randomization enables better accuracy for Fair-ERM with Malicious Noise.
\cite{lampert} show impossibility results for Demographic Parity and Equal Opportunity  where a \emph{proper} learner is forced to return a classifier with $\Omega(1)$ excess unfairness and accuracy
 compared to $h^{*}$ for a synthetic and finite hypothesis class/distribution. 
 
To overcome this limitation, we propose a novel approach to make the hypothesis class $\mathcal{H}$ more robust, by injecting noise into each hypothesis $h \in \mathcal{H}$. In other words, we allow improper learning, and refer to the resulting expanded set of hypotheses as $\PQ$.
By injecting controlled noise into the hypotheses, we effectively ``smooth out" the hypothesis class $\mathcal{H}$, making it more resilient against adversarial manipulation. 

Since we allow group-aware classifiers, we learn two classifiers $h_{A}, h_{B} \in \PQ$, typically distinct from each other.
% Our approach limits the amount of fairness loss for any hypothesis class and true distribution
% $\mathcal{D}$ that satisfies a mild realizability assumption that at least one classifier in the hypothesis class must satisfy the fairness constraint.
Our method minimizes fairness loss for any hypothesis class and true distribution $\mathcal{D}$, under the assumption that at least one classifier in the original hypothesis class $\mathcal{H}$ satisfies the fairness constraints. 
We aim to find a fair classifier $\hat{h} \in \PQ$ that is as good as the best $h^{*} \in \mathcal{H}$.
% This is mild because the accuracy of such a classifier could be poor.
% We  consider the existence of a fair $\hat{h} \in \PQ$ that competes with that optimal $h^{*} \in \mathcal{H}$. 


%%%%%%%%%%%
% In this section, we present our technical findings for Parity, Equal Opportunity, and Equalized Odds. \kmsmargincomment{I want to strike this paragaph since I think we are being uneccearily shy about our results. I do think the Lampert results and ours meanignfully clash and I think this extensive dive into Lampert will simply confuse the reader/make us vulnerable to skeptical reviewers. 
% I could be amenable to a long/detailed discussion in appendix
% }
% We will show how randomization allows us to get better accuracy guarantees for the fair ERM problem in the presence of a malicious adversary. 
% Although \cite{lampert} show impossibility results for Demographic Parity and Equal Opportunity on the Pareto frontier (accuracy and fairness) when the learner must return a classifier in the hypothesis class, 
% their lower bound results are not directly \kmsreplace{applicable}{comparable} to our setting for two reasons. 

% The first is that our objectives are slightly different; they seek to optimize across two  (accuracy and fairness) while we seek to optimize accuracy subject to the fairness constraint. 
% Our result can be thought of as showing how to achieve one specific and \emph{important} point on that Pareto frontier, that  of optimal accuracy given zero fairness disparity.
% %\kmsedit{They then exhibit a lower bound that forces excess accuracy and fairness disparity compared to $h^{*}$. }
% The second reason is that we make a \kmsedit{mild} realizability assumption i.e at least one classifier in the hypothesis class must satisfy the fairness constraint. 
% %\kmsedit{This is mild because the accuracy of such a classifier could be very bad.} 
% \kmsedit{However, the accuracy of that $h^{*}$ could be poor.}
% \pcomargincomment{It's not clear to me what mild means here or how it relates to accuracy. Maybe we can just say "The accuracy of this classifier may be poor"}

% In contrast, the lower bound construction for Demographic Parity, in contrast, uses a hypothesis class that contains 
% two unfair classifiers. 
% %\kmsedit{To summarize, despite this technical differences our results are a substantial }
% \kmsdelete{Therefore it does not follow directly from their results that proper learning for our fair-ERM problem is not possible, and in our framework we show the optimal accuracy loss.}

% %}{I don't think this is true, see page 
% %11 of lampert? $h_0$ satifies parity equality on $D_1$ and $h_1$ does so on $D_1$. I want to delete this paragraph}

% \pcomargincomment{I intend to include a result that says proper learning in the fair-ERM with malicious adversary is not possible}

% \kmsdelete{As we can see from the result above, in some cases, a proper fair-ERM learner in the malicious adversary model may return an unhelpful and inaccurate classifier.}

% To overcome this limitation, we propose a novel approach to "robustify" the hypothesis class $\mathcal{H}$. Our method of "robustifying" the hypothesis class $\mathcal{H}$ involves injecting noise into each hypothesis $h \in \mathcal{H}$. Specifically, for each hypothesis $h \in \mathcal{H}$, we construct a family of hypotheses, each defined by parameters $p_A, q_A, p_B, q_B \in [0,1]$ that exhibit behavior identical to $h$ but with a probability of $p_A$, deviate from $h$ for samples in group $A$ and flip a biased coin with bias $q_A$. 

% Similarly, with a probability of $p_B$, they deviate from $h$ for samples in group $B$ and flip a biased coin with bias $q_B$. We denote this augmented hypothesis class as $cl(\mathcal{H})$. 
% By injecting controlled noise into the hypotheses, we effectively "smooth out" the hypothesis class $\mathcal{H}$, making it more resilient against adversarial manipulation.
% Therefore, our proposed approach represents a promising advancement in addressing the challenges posed by malicious adversaries within the context of fair ERM


\subsection{Demographic Parity}
Demographic Parity \cite{calders2009building} requires that the decisions of the classifier are independent of the group membership;
%o%the samples
 that is, $P_{(x,y) \sim \DA} [h(x)=1] = P_{(x,y) \sim \DB} [h(x)=1]$\footnote{Note there is no reference in the definition to the true labels, so a trivial hypothesis that flips a random coin for all examples would satisfy this notion, albeit at minimal accuracy. }. 

When the original distribution $\mathcal{D}$ is corrupted, a fair hypothesis on $\mathcal{D}$ may seem unfair to the learner. In order to analyze our approach it is important to understand how the fairness violation of a fixed hypothesis changes after the adversary corrupts an $\alpha$ proportion of the distribution.

\begin{restatable}[Parity after corruption]{proposition}{corruptparity}\label{prop:corruptparity}
Let $\widetilde{\mathcal{D}}$ be any corrupted distribution chosen by the adversary, and $h$ be a fixed hypothesis in $\mathcal{H}$. For a fixed group $A$, the following inequality bounds the change in the proportion of positive labels assigned by $h$:
$
\left| P_{(x,y) \sim \DAC} [h(x)=1] - P_{(x,y) \sim \DA} [h(x)=1] \right|  \leq \\
 \frac{\alpha}{(1-\alpha) r_A + \alpha }
$
where $r_A = \RA$, i.e how prevalent the group is in the original distribution. 
\end{restatable}

% \begin{linked}[Parity after corruption]{proposition}{corruptparity}\label{prop:fixedh-parity}
% Let $\mathcal{D}'$ be the corrupted distribution, and $h$ be a fixed hypothesis in $\mathcal{H}$. For a fixed group $A$, the following inequality bounds the change in the proportion of positive labels assigned by $h$:
% \begin{equation}
% \left| P_{(x,y) \sim \DAC} [h(x)=1] - P_{(x,y) \sim \DA} [h(x)=1] \right| \leq \frac{\alpha}{(1-\alpha) r_A + \alpha }
% \end{equation}
% where $r_A = \RA$.
% \end{linked}

This proposition provides an upper bound on the change in the proportion of positive labels assigned by a fixed hypothesis $h$ in $\mathcal{H}$ after the distribution has been corrupted according to the Malicious Noise Model. The full proof can be found in the Appendix \ref{proof:corruptparity}. 
The proof shows that this change is bounded by a function of the corruption rate $\alpha$ and the proportion of the dataset in the fixed group $A$, denoted by $r_A$. 

Intuitively, this means that the smaller a group is, the easier it is for the adversary to make a fair hypothesis seem unfair for members of that group.

% \begin{theorem}
%     For any hypothesis class $\mathcal{H}$ and distribution $ \dist = (\DA, \DB)$, a robust fair-ERM learner for the parity constraint in the Malicious Adversarial Model returns a hypothesis $\hhat$ such that 
%     \begin{equation*}
%         \error{\hhat} \leq O(\alpha)
%     \end{equation*}
% \end{theorem}

\begin{restatable}{theorem}{mainparity}\label{thm:mainparity}
For any hypothesis class $\mathcal{H}$ and distribution $ \dist = (\DA, \DB)$, a robust fair-ERM learner for the parity constraint in the Malicious Adversarial Model returns a hypothesis $\hhat \in \PQ$ such that 

$\error{\hhat} \leq O(\alpha)$ \\
where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to hypothesis class $\mathcal{H}$.
\end{restatable}

This theorem states that a fair-ERM learner searching over the smoothed hypothesis class 
$\PQ$
returns a classifier that is within $\alpha$ of the accuracy of the best fair classifier in the original class $\mathcal{H}$. The full constructive proof can be found in the appendix \ref{proof:mainparity}.

The proof exhibits classifier $h \in \PQ$ that satisfies the desired guarantee. 
This classifier mostly behaves identically to $h^*$ but deviates with probability $p_A$ on samples from group $A$ (and with probability $p_B$ on samples from group $B$). We give an explicit assignment of these probability values $p_A$, $q_A$, $p_B$, $q_B$ in $[0,1]$ so that $h$ is perceived as fair by the learner. Then, we show that these values are small enough that the proportion of samples where $h(x) \neq h^*(x)$ is small ($O(\alpha)$). \emph{This is the best possible outcome in the malicious adversary model without fairness constraints \cite{kearns1988learning}.}

\subsection{Equal Opportunity}
Equal Opportunity \cite{hardt16} requires that the True Positive Rates of the classifier are equal across all the groups, that is, $P_{(x,y) \sim \DA} [h(x)=1 \mid y = 1] = P_{(x,y) \sim \DB} [h(x)=1 \mid y = 1]$. 
Similarly to Demographic Parity, we first provide bounds on how the fairness violation of a fixed hypothesis changes after the adversary corrupts an $\alpha$ proportion of the dataset. 
This is important because it gives an estimate of how much violation must be offset.

\begin{restatable}[TPR after corruption]{proposition}{corrupttpr}\label{prop:corrupttpr}
    Let $\widetilde{\mathcal{D}}$ be any corrupted distribution chosen by the adversary, and $h$ be a fixed hypothesis in $\mathcal{H}$. For a fixed group $A$, the following inequality bounds the change in True Positive Rate of $h$:
    \begin{equation}
        \left| \text{TPR}_A(h, \widetilde{\mathcal{D}}) - \text{TPR}_A(h, \dist) \right| \leq \frac{\alpha}{(1-\alpha) r_A^+ + \alpha }
    \end{equation}
    where $\text{TPR}_A(h, \dist) = P_{(x,y) \sim \DA} [h(x)=1 | y = 1]$ and $r_A^+ = P_{(x,y) \sim \mathcal{D}} [y = 1 \cap x \in A]$
\end{restatable}

This proposition provides an upper bound on the change to the true positive rate in group $A$ assigned by a fixed hypothesis $h$ in $\mathcal{H}$ after the dataset has been corrupted according to the Malicious Noise Model. 
The full proof can be found in the appendix \ref{proof:corrupttpr}. 

\emph{Since $\alpha \in [0,1]$, $O(\sqrt{\alpha})$ means larger (meaning worse) accuracy loss, compared $O(\alpha)$.}

The function that bounds the change in True Positive rate is similar to that of Demographic Parity with the proportional size of group A $r_A$ replaced with the proportion of the dataset that is positively labeled and in group A, $r_A^+$.
We will see that this slight change in dependence makes the robust learning problem more difficult and leads to a worse dependence on $\alpha$.

\begin{restatable}[Upper Bound]{theorem}{maineopp}\label{thm:maineopp}
For any hypothesis class $\mathcal{H}$ and distribution $ \dist = (\DA, \DB)$, a robust fair-ERM learner for the equal opportunity constraint in the Malicious Adversarial Model returns a hypothesis $\hhat$ such that 
$\error{\hhat} \leq O(\sqrt{\alpha})$
where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to hypothesis class $\mathcal{H}$.
\end{restatable}

This theorem states that a fair-ERM learner, when applied with the smoothed hypothesis class $\PQ$, returns a classifier that is within $\sqrt{\alpha}$ of the accuracy of the best fair classifier in the original class $\mathcal{H}$. The full proof can be found in Appendix \ref{proof:mainparity}. 
%The proof follows a similar structure to that of demographic parity.

In constructing a classifier $h \in \PQ$, we aim for it to behave mostly identically to $h^*$ but introduce deviations with probability $p_A$ for samples from group $A$ and probability $p_B$ for samples from group $B$. 
However, in the case of the Equal Opportunity fairness constraint, this approach, as used for Demographic Parity, does not work effectively. 
We observe that the amount of correction required for each group depends inversely on the true positive rate, which presents challenges when the true positive rate (TPR) is close to 0 or 1.

For example, suppose the classifier achieves a 95\% TPR for a fixed group. The adversary can manipulate the TPR to reach 100\% by corrupting only a few samples. 
Correcting this change and bringing the TPR back down to 95\% is an incredibly difficult task, similar to finding a \textit{needle in a haystack}, since the learner essentially has to identify the corrupted samples to do so.
In such cases, it might be easier for the learning algorithm to increase the TPR of the other groups from 95\% to 100\% instead. 

The tradeoff lies in equalizing the corrections that only transform the TPR of a fixed group to its original value versus the corrections that transform the TPR of other groups to match the TPR of the group with the most corruptions.

\begin{restatable}[Lower Bound]{theorem}{lowereopp}\label{thm:lowereopp}
There exists a distribution $ \dist = (\DA, \DB)$ and a malicious adversary of power $\alpha$ that guarantees that any hypothesis, $\hhat$, returned by an improper learner for the fair-ERM problem with the equal opportunity constraint satisfies the following:
$\error{\hhat} \geq \Omega(\sqrt{\alpha})$
where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to a hypothesis class $\mathcal{H}$.
\end{restatable}

In this lower bound, under the given conditions, no proper or improper learner can achieve an error rate lower than a threshold that scales with the square root of the adversary's power. In other words, as the adversary becomes more powerful ($\alpha$ increases), the error rate of the hypothesis returned by an improper learner will unavoidably be at least on the order of $\sqrt{\alpha}$.

The proof of this lower bound result sets up a scenario reflecting the \textit{needle in the haystack} issue described earlier. We present a distribution with two groups, one of size $\sqrt{\alpha}$ and the other of size $1 - \sqrt{\alpha}$. We construct a hypothesis class where the optimal classifier has a high but not perfect true positive rate. 
Then we show that any improper learner must either suffer poor accuracy on the smaller group or lose $\Omega(\sqrt{\alpha})$ accuracy on the larger group. 
The full proof can be found in the Appendix \ref{proof:corrupttpr}.
% \kmsedit{While not the best achievable for no fairness constraints [which \cite{kearns1988learning}$O(\alpha)]$, this $\theta(\sqrt{\alpha})$ accuracy loss is a substantial improvement over the $\theta(1)$ loss in \cite{lampert}.}

\subsection{Equalized Odds}
Equalized Odds \cite{hardt16} is a fairness constraint that requires equalizing
True Positive Rates (TPRs) and False Positive Rates (FPRs) across different groups. 
This notion is very sensitive to the adversary's corrupted data and we exhibit a problematic lower bound, showing the adversary can force terrible performance.

% This is our first \kmsedit{strong} negative result. 
The intuition is as follows; for a small group, the Adversary can set the Bayes Optimal TPR/FPRs rates of that group towards arbitrary values and so the learner must do the same on the larger group, regardless of their hypothesis class, forcing large error.
The full proof is in Appendix \ref{sec:eoddsproof}.
\begin{theorem}[Lower Bound]
\label{thm: Equalized Odds} 
For a learner seeking to maximize accuracy subject to satisfying Equalized Odds, an adversary with corruption fraction $\alpha$ can force an additional $\Omega(1)$ accuracy loss when compared to the performance of the optimal fair classifier on the true distribution.
\end{theorem}


%It requires that the classifier's TPR and FPR are the same for each group.










%%%%ALT vsn for paragraph 4

\begin{comment}
Although \cite{lampert} shows `impossibility results'\footnote{Direct quote from the authors} for Demographic Parity and Equal Opportunity on the Pareto frontier (accuracy and fairness) with a proper learner; 
our settings are somewhat different. 

The first difference is objectives; they seek to optimize across two parameters (accuracy and fairness) while we seek to optimize accuracy subject to the fairness constraint. 
Their results are framed as excessive loss of accuracy and fairness  compared to the best Pareto dominant 
classifier in that restricted class. 

Our result show how achieve one specific and \emph{important} point on that Pareto frontier, that  of optimal accuracy given zero fairness disparity.
%\kmsedit{They then exhibit a lower bound that forces excess accuracy and fairness disparity compared to $h^{*}$. }
The second reason is that we make a \kmsedit{mild} realizability assumption i.e at least one classifier in the hypothesis class must satisfy the fairness constraint. 
%\kmsedit{This is mild because the accuracy of such a classifier could be very bad.} 
However, the accuracy of that $h^{*}$ could be poor.

In contrast, the lower bound construction for Demographic Parity, in contrast, 
uses a finite hypothesis class that contains  two unfair\footnote{unfair in our setting where we need equality in the fairness constraints. If we allow those classifiers are `fair'} classifiers. 
Therefore it does not follow directly from their results that proper learning for our fair-ERM problem is not possible, and within our framework we show the optimal accuracy loss.
\end{comment}
