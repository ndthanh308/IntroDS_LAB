\section{Preliminaries}
\label{sec:prelim}
In fairness-constrained learning, the goal is to learn a classifier that achieves good predictive performance while satisfying certain fairness constraints that
connect the performance of the classifier on multiple groups, to ensure effective performance on all groups.

Specifically, we start with a dataset consisting of examples with feature vectors $(x \in \mathcal{X})$, labels $(y \in \mathcal{Y})$, and group attributes $(z \in \mathcal{Z})$. 
We assume that each example is drawn i.i.d from a joint distribution $\mathcal{D}$ of random variables $(X,Y,Z)$. 
There are multiple groups in the dataset, and we aim to ensure that the classifier's predictions do not unfairly favor or disfavor any particular group. 
We will denote $\mathcal{D}_z$ as the conditional distribution of random variables $X$ and $Y$ given $Z = z$. For simplicity, we will assume there are two disjoint groups: $A$ and $B$ in the dataset with B being the smaller and more vulnerable of the two. However, our results apply more broadly to any number of groups.

We aim to use the dataset to learn a classifier $f : \mathcal{X} \rightarrow \mathcal{Y}$ given a hypothesis class $\mathcal{H}$. 
However, in this paper we suppress sample complexity learning issues and focus on characterizing the
accuracy properties of the best hypothesis in the expanded hypothesis class with a corrupted data distribution $\widetilde{D}$.
The goal is to probe the fundamental sensitivity of Fair-ERM to unreliable data in the large sample limit.
%\footnote{This focus is s}.
% We will typically think of the classifier $f$ as being quite group 
% specific; i.e. the groups are easily identifiable from the data and the distributions $\mathcal{D}_{A}$, $\mathcal{D}_B$
% may be substantially different.\footnote{This is not  necessary for our technical results but is useful for reader clarity.} 
% \pcomargincomment{at this point, we have not yet introduced A and B yet as the groups. Ok, I just added something above} Thus, we will occasionally use the notation $f_{A}$ or $f_{B}$ to refer to the behavior of the classifier only on Group $A$ or $B$ respectively.\pcomargincomment{I actually already mentioned this at the end of the realizability paragraph. It seems we have a couple repetitions.}
% % Additionally, the reader should think of Group $B$ as the smaller and more vulnerable group.

To this end, we consider solving the standard risk minimization problem with fairness constraints, known as Fair-ERM.
\begin{align}
\min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y,Z)\sim\mathcal{D}} \left[\one(h(X) \neq Y)\right] ~~~~\ \\
\text{subject to} & ~~~~ F_z(h) = F_{z'}(h) \qquad \forall z,z'\in \mathcal{Z}. \label{FairnessConstraint}
\end{align}
where $F_z(h)$ is some fairness statistic of $h$ for group $z$ given the true labels $y$, such as \emph{true positive rate} :
$ \text{(TPR)}: F_z(h) = \mathbb{P}(h(X)=+1|Y=+1,Z=z)$.

% $\delta$ represents the maximum allowable difference between the fairness statistic of the classifier for any two groups in the dataset. We will 
% As we will show, our upper bounds will produce classifiers satisfying the fairness 
% with the same value of $\delta$ as $h^{*}$, the Bayes-optimal fair classifier, but our lower bounds will apply even if we allowed a constant-sized gap in the fairness constraint.
% Equivalently, we will often think of $\delta$ as zero.


We make a mild \emph{realizability assumption} that there exists a solution to this risk minimization problem. That is, there is at least one hypothesis in the class that satisfies the fairness constraint. This optimal solution is denoted as $h^*$.

As noted above, since we allow our hypothesis class to be group-aware, we can reason about $h^*_z$ for all $z \in Z$, where $h^*_z$ is the restriction of the optimal classifier $h^*$ to members of group $z$. In other words, $h_{z}^{*}$ is the optimal group-specific classifier for Group $z$.

 We also make another mild assumption that each group $z$ has at least some constant fraction of positive examples, and at least some constant fraction of negative examples (e.g., the fraction of positive examples in each group is between 5\% and 95\%.   If positive or negative examples are too rare within any group, then some of the fairness notions do not make sense, and we find this assumption reasonable.

For the results in our paper, we only need the assumption that each group has non-trivial fraction of positives.
Formally, we assume that that for each fixed group $A$, where $r_A= P_{(x,y) \sim \mathcal{D} } [x \in A]$,
\begin{equation}
     P_{(x,y) \sim \mathcal{D}} [y = 1 \cap x \in A] := r_{A}^{+} \geq \frac{r_A}{c}
    \label{ass:raplus}
\end{equation}
for some  integer c.
Think $c \leq 20$.
This will allow the adversary to modify each group's true positive rate substantially, but not arbitrarily, because there is some non-trivial fraction of positives in each group. 



\subsection{Fairness Notions}
Different formal notions of group fairness have previously been proposed in literature. These notions include, but are not limited to, Demographic Parity, Equal Opportunity, Equalized Odds, Minimax Fairness, and Calibration\cite{dwork2012fairness, calders2009building, hardt16, klein16, chouldechova2017fair}.

Selecting the “right” fairness measure is, in general, application-dependent.\footnote{We would also note that these fairness constraints are imperfect measures of fairness that likely do not capture all of the normative properties relevant to a specific task or system.  }%\pcomargincomment{need to add a bit more on fairness notions here}
% When we will call a classifier `unfair', it will mean that one of these fairness constraints is violated.
One of our goals in this work is to provide understanding of their implications under adversarial attack, which could aid in the selection process.
For the convenience of the reader, we include a table in Appendix \ref{fairtable} summarizing the fairness notions we consider in this paper. 
Other than Calibration, these all are notions for binary classifiers. 
%In Appendix \ref{subsec:minimaxfair}, we will consider Minimax Fairness \cite{minimaxfair} but 
%\pcoreplace{elide}{present} 
%omit the definition here for clarity.
In Section \ref{subsec:calib} we will introduce a new variant of Calibration and will defer discussion of that notion until then.


\subsection{Adversary Model}
\label{subsec:adversary}
Throughout this paper, we focus on the Malicious Noise Model, introduced by \cite{malnoise}.
This model considers a worst-case scenario where an adversary has complete control over a uniformly chosen $\alpha$ proportion of the training data and can manipulate that fraction in order to move the learning algorithm towards their desired outcomes, i.e. increasing test time error [on un-corrupted data].
\kmsdelete{However, that $\alpha$ fraction is chosen uniformly from natural examples, unlike in \cite{bshouty2002pac}.}

% \kmsreplace{We follow the notion of a Fairness-ware adversary in \cite{lampert}.}{The core fear in \cite{lampert} and our work is that an adversary could use the fairness constraints to amplify their power when they have a small corruption budget $\alpha$.}
%\cite{lampert} introduces the fairness-aware adversary who corrupts a fixed fraction of a data set in order to maximize the test time error [on un-corrupted data] of a learner who uses the corrupted data to train a model.
%\cite{lampert}.
In \cite{malnoise}'s model, the samples are drawn sequentially from a fixed distribution. With probability $\alpha$ and full knowledge of the learning algorithm, data distribution and all the samples that have been drawn so far, the adversary can replace sample $(x,y)$ with an arbitrary sample $(\tilde{x}, \tilde{y})$.

%We reframe the data generating procedure as follows: Each timestep $t$ 
At each time-step $t$,
\begin{enumerate}
    \item The adversary chooses a distribution $\widetilde{\mathcal{D}}_t$ that is $\alpha-$close to the original distribution $\mathcal{D}$ in Total Variation distance.
    \item The algorithm draws a sample $(x_t,y_t)$ from $\widetilde{\mathcal{D}}_t$ instead of $\mathcal{D}$
\end{enumerate}
Note that the adversary's choice at time $t$, $\widetilde{\mathcal{D}}_t$ can depend on the samples $\{x_1,y_1, \ldots, x_{t-1}, y_{t-1}\}$ chosen so far.
%\footnote{}

Reframing the Malicious Noise Model in this manner simplifies analysis and allows us to focus on the fundamental aspect of this model which is how the accuracy guarantees of fairness constrained learning change as a function of $\alpha$.

% In the finite data setting, $n$ data points are sampled and $\alpha n$ of them are marked in expectation [i.e. marked by tossing independent coins with bias $\alpha$].
% \kmsedit{These marked points are those available for the adversary to manipulate arbitrarily.}
% %n order to induce large error when the corrupted dataset is used to select a model usiong Fair-ERM.}


% With knowledge of all the sampled points and the learner's algorithm, the adversary outputs a new corrupted data-set of $n$ points \kmsedit{in order to maximize error on the true distribution}.
% % we are interested in the power of the adversary and to avoid sampling

% %In order to suppress issues relating to sample complexity and focus on the 
% In the main body of this paper, we assume that we have sufficient data-points such that all relevant probabilistic events, i.e. 
% (number of data points observed from each group, empirical True Positive Rates, etc) are tightly concentrated around their means and we focus on the behavior of this adversary model \emph{in expectation}.

% This is for clarity of presentation so we focus on the fundamental aspect of this model
% %, namely how the adversary can corrupt and move fairness statistics and how this complicates learning. 
% which is how the accuracy guarantees of fairness constrained learning change as a function of $\alpha$, the fraction of corrupted data. 

% In the Appendix, we consider sample complexity issues fully, most of which follows immediately from results in \cite{lampert}.

% We will refer to the empirical distribution of the corrupted dataset as $\widetilde{\mathcal{D}}$. 

% An alternative view of the Malicious Noise Process and interpretation of $\widetilde{\mathcal{D}}$ is as follows.
% Consider an adversary who at each time step is choosing points to add to the corrupted dataset.
% We can view the adversary as at each time step choosing a distribution $\tilde{D}$ that is $\alpha$-close to D in variation distance (and it could choose different such distributions $\tilde{D}$ at each time step based on the samples drawn so far, though it will never be advantageous for the adversary to do so).  


%Then, instead of talking about "the" distribution $\tilde{D}$, to talk about "any" distribution $\tilde{D}$ in our theorems.

%We can view the adversary as at each time step choosing a distribution $\tilde{D}$ that is alpha-close to D in variation distance (and it could choose different such distributions $\tilde{D}$ at each time step based on the samples drawn so far, though it will never be advantageous for the adversary to do so).  
%Thus, instead of talking about "the" distribution $\tilde{D}$, to talk about "any" distribution \tilde{D} in our theorems.

%With sufficiently many data points, $\widetilde{\mathcal{D}}$ will be $\alpha$-close to the original distribution $\mathcal{D}$ in total variation distance.

%fundamental limits of fairness-constrained learning, we will assume the ERM oracle is able to draw an infinitely-sized dataset 

%However, we will focus on the infinite data setting where these $n$-sized data-sets are expanded to infinite-sized datasets. 
%\kmsedit{This focus is to ensure that we study the most challenging part of this model; which is how the adversary can change fairness statistics in expectation, rather than issues caused by finite sample error. }

%Now, the algorithm samples from a distribution over corrupted infinite-sized datasets. 

%Once an infinite-sized dataset is sampled, as far as ERM minimization algorithms are concerned, this is simply a corrupted %distribution $\widetilde{\mathcal{D}}$ over the $(x,y)$ domain. 
%We can therefore think of the algorithm as sampling points from this corrupted distribution $\widetilde{\mathcal{D}}$, \kmsedit{and the adversary chooses the hardest distribution $\widetilde{\mathcal{D}}$ for the fair-ERM algorithm.}

% \kmsedit{Observe that due to these adversarial corruptions, the data we observe coming from $\widetilde{\mathcal{D}}$ will not be i.i.d; there will be extensive correlations caused by the adversarial corruptions. 
% The adversary is able to exploit the full adaptivity of Malicious Noise.}

% \kmsedit{We shall observe and show that} with respect to all the statistical properties we will be examining, a fairness-aware adversary with power $\alpha$ can replace the joint distribution $\mathcal{D}$ with a new distribution $\widetilde{\mathcal{D}}$ that is $\alpha$-close to the original in total variation distance.

%\kmsedit{Lemma ?}

% \kmsreplace{following the steps below:}{This corruption follows this scheme:}
% \begin{enumerate}
%     \item A point $(x,y,z)$ is drawn i.i.d from $\mathcal{D}$.
%     \item With probability $\alpha \in [0,0.5)$, the adversary can replace this point with another point $(\widetilde{x},\widetilde{y},\widetilde{z})$. 
%     That is, the adversary can change the feature vector, label, and the group attribute of the sample and the adversary has 
%     perfect knowledge of $\mathcal{D}$.
%     \item The learner observes $(\widetilde{x},\widetilde{y},\widetilde{z})$ instead of $(x,y,z)$
% \end{enumerate}
% \kmsdelete{We assume the adversary has knowledge of the joint distribution $\mathcal{D}$ and the hypothesis class $\mathcal{H}$. }
% Equivalently, a fairness-aware adversary with power $\alpha$ can replace the joint distribution $\mathcal{D}$ with a new distribution $\widetilde{\mathcal{D}}$ that is $\alpha$-close to the original in total variation distance. 



\subsection{Core Learning Problem}
\label{subsec: learningproblem}
In the fair-ERM problem with Malicious Noise, our goal is to find the optimal classifier $h^*$ subject to a fairness constraint. However, the presence of the Malicious Noise makes this objective challenging. 
Instead of observing samples from the true distribution $\mathcal{D}$, we observe samples from a corrupted distribution $\widetilde{\mathcal{D}}$. 

In the standard ERM setting, \cite{kearns1988learning} show that the optimal classifier that can be learned using this corrupted data is one that is $O(\alpha)$-close to $h^*$ in terms of accuracy [on the original distribution]. 
The fair-ERM problem with a Malicious Noise adversary introduces an additional layer of complexity, as we must also ensure fairness while achieving high accuracy. 

%We say a learning algorithm is robust to  if it returns a classifier $h$ that satisfies two conditions. 
%First, the perceived unfairness, i.e, the difference in the estimated fairness constraint $\widetilde{F}$ between any two %groups $z$ and $z'$ in $\widetilde{\mathcal{D}}$ should be at most $\delta$. Second, the expected error of $h$ and $h^*$ %with respect to $\mathcal{D}$ should differ by at most a function of $\alpha$, where $h^*$ is the optimal classifier for %the fair-ERM problem on the true distribution $\mathcal{D}$. 
\begin{definition}
\label{def:robust}
We say a learning algorithm for the fair-ERM problem is $\beta$-robust with respect to a fairness constraint $F$ in the malicious adversary model with corruption fraction $\alpha$, if it returns a classifier $h$ such that $\widetilde{F}_z(h) = \widetilde{F}_{z'}(h)$ and 
\begin{align*}
| \mathbb{E}_{\mathcal{D}} \ [\one (h(X, Z) \neq Y)] - \mathbb{E}_{\mathcal{D}} \ [\one  (h^{*} (X, Z) \neq Y)] | \leq \beta(\alpha)
\end{align*}
where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to a hypothesis 
class $\mathcal{H}$ and $\beta$ is a function of $\alpha$.
\end{definition}
This definition captures the desired properties of a learning algorithm that can perform well under the malicious noise model while achieving both accuracy and fairness, as measured by the fairness constraint $F$.


Thus, this is an agnostic learning problem \cite{HAUSSLER199278} with an adversary and fairness constraints.
As referenced in the introduction, we will allow the learner to return $h^{'} \in \PQ$, where $\PQ$ is a way to post-process each
$h \in \mathcal{H}$ using randomness.
In Sections \ref{sec:mainresults} and  \ref{subsec:calib} will characterize the optimal value of $\beta$ given the relevant fairness constraint $F$ and base hypothesis class
$\mathcal{H}$.

% \kmsmargincomment{We don't use this alpha/beta notation later or really use the delta notion. While maybe more general I feel like it is confusing to the reader}
% The fair-ERM problem with a malicious adversary introduces an additional layer of complexity, as we must also ensure fairness while achieving high accuracy. To address this, we introduce the concept of $\beta(\alpha)$-robustness with respect to a fairness constraint $F$. A learning algorithm is considered $\beta(\alpha)$-robust if it returns a classifier $f$ that satisfies two conditions. 

% First, the perceived fairness, i.e, the difference in the estimated fairness constraint $\widetilde{F}$ between any two groups $z$ and $z'$ in $\widetilde{\mathcal{D}}$ should be at most $\delta$. Second, the expected error of $f$ and $h^*$ with respect to $\mathcal{D}$ should differ by at most $\beta(\alpha)$, where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$. 
% This definition captures the desired properties of a learning algorithm that can perform well under the malicious adversary model while achieving both accuracy and fairness.
% \begin{definition}
% \label{def:robust}
% We say a learning algorithm for the fair-ERM problem is $\beta(\alpha)$-robust with respect to a fairness \kmsreplace{statistic}{constraint} \pcomargincomment{technically isn't TPR a statistic and the constraint, tpr(A) = tpr(B)?} \kmsmargincomment{yes? maybe the language is a bit loose}$F$ in the malicious adversary model with $\alpha$ corruption fraction if the learning algorithm returns a classifier $f$ such that $|\widetilde{F}_z(f)-\widetilde{F}_{z'}(f)|\le \delta$ and 
% \begin{equation}
% | \mathbb{E}_{\mathcal{D}} \ [\one (f(X, Z) \neq Y)] - \mathbb{E}_{\mathcal{D}} \ [\one  (h^{*} (X, Z) \neq Y)] | \leq \beta(\alpha)
% \end{equation}
% where $h^*$ is the optimal classifier for the fair-ERM problem on the true distribution $\mathcal{D}$ with respect to a hypothesis 
% class $\mathcal{H}$ and $\beta$ is a function of $\alpha$.
% \end{definition}
% Thus, this is an agnostic learning problem \cite{HAUSSLER199278} with an adversary and fairness constraints.
% As referenced in the introduction, we will allow the learner to return $f \in \PQ$, where $\PQ$ is a way to post-process each
% $h \in \mathcal{H}$ using randomness.
% \kmsedit{In Sections \ref{sec:mainresults} and  \ref{subsec:calib} will characterize the optimal value of $\beta$ given the relevant fairness constraint $F$ and base hypothesis class
% $\mathcal{H}$.}
% \begin{align}
% \min_{h\in\mathcal{H}} & ~~\mathbb{E}_{(X,Y)\sim\mathcal{D}} \left[\one(h(X) \ne Y)\right] ~~~~\ \\
% \text{subject to} & ~~~~ |F_z(h)-F_{z'}(h)|\le \delta \qquad \forall z,z'\in Z. \label{FairnessConstraint}
% \end{align}
%\end{definition}
% \subsection{Review of Classical Results in Malicious Noise Model}
% Observe that an adversary with power $\alpha$ can always induce at least $\Omega(\alpha)$ additive accuracy loss \red{cite + add more details}


