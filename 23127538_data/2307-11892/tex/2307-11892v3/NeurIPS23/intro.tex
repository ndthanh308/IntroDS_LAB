
\section{Introduction}

%\kmscomment{we should move proofs to appendix. Note that the}

%\kevin{testedit}
% \kmsdelete{
% Machine learning continues to be deployed in a wide range of settings and environments including high-stakes decisions, where poor performance can be  harmful to the recipients of a decision. 
% %Social contexts infuse many aspects of the data-generation and labeling process, from how the 
% %features are distributed across groups, to levels of noise in measuring those features, and to how the labels are assigned to the data-points. 
% %This latent social information affecting the data pipeline prompts concerns that learned models may exhibit  disparate performance on some sub-groups, and that by the time this disparity is detected, significant harm could have occurred. 
% One way to mitigate or prevent these effects in machine learning is to require that a classifier satisfy 
% %some 
% fairness
% \footnote{Many important aspects of fairness are only roughly captured by these group constraints and that group constraints are a relatively weak guarantee. However they are an important baseline and require continued study.} constraints  on the training data.  
% %These constraints are often thought of a legal or ethical requirement. 
% %Foundational work showed how several of the constraintss,
% \kmsdelete{which are typically \cite{klein16,chouldechova2017fair} are incompatible unless a model has perfect accuracy.\pcomargincomment{This doesn't feel like the right place to bring up incompatibility of fairness notions- agreed we can just cite that without comment in related work}}
% %However,
% %as these constraints are often introduced as moral or legal requirements and
% %There remain substantial questions about the utility of fairness constrained learning in %deployment. 
% %In learning theory, 
% Fundamental theorems and 
% \kmsdelete {basic} 
% algorithms are typically proven %
% and evaluated when assuming an i.i.d training/test set split \pcomargincomment{i.i.d training/test set split is also done in practice right?}.
% %This simplifies the generalization problem.
% %However, 
% In the wild \pcomargincomment{In the wild feels quite informal}, distributions can shift substantially between model development and deployment\footnote{ e.g. Due to drift over time or due to strategic manipulation.}
% resulting in worse performance when a model goes from
% training and development to deployment.

% %Additionally, where does the disparity in learning come from? 

% %Often the disparity can be seen to come from explicit human actions in the past or differing distributions of features. 
% %In other ways, the disparity seems to be operationalized as increased uncertainty when making decisions about a subgroup, due to limited or poor quality (e.g. explicitly biased) training data. 

% %Collecting better data seems like the gold standard solution, but there may be substantial barriers to that solution.
% %In some high stakes contexts, like medical settings, accurate decisions must be made now. 

% To capture this concern about learning with unreliable data, we posit that some natural distribution does exist and most of the data does come from this distribution but there is an Adversary who can arbitrarily manipulate some fraction of the available distribution, in order to fool the classifier. 
% The  exact corruption model we adopt is \emph{Malicious Noise}, introduced by \cite{malnoise}, which we will explain in more detail in Section \ref{subsec:adversary}.
% %The learner's goal is to learn a classifier with good performance on natural examples and  the fear is that the malicious training examples may mislead the learning algorithm \footnote{Lower bounds in this model typically argue that the malicious noise makes two distributions indistinguishable, so the learner is forced to exhibit high error on one of the two distributions.}

% Recent work in this adversarial model\cite{lampert}
% %\footnote{Our learning settings are similar, with a key difference using an improper classifier.} 
% %has studied the behavior of fairness constrained learning with empirical risk minimization.
% %By \emph{fairness constrained learning} we 
% %At a high level,  
% has shown that Fairness Constrained Learning for the constraints known as Demographic
% Parity \cite{dwork2012fairness} and Equal Opportunity \cite{hardt16} 
% will in the worst case return classifiers with $50\%$ error on 
% un-modified examples due to the dependence on the size of the smaller sub-group.
% %has the same worst case performance as unconstrained learning when subject to malicious noise, but complicated by dependence on the size of the smaller sub-group. 

% }
The widespread adoption of machine learning algorithms across various domains, including recidivism prediction \cite{flores2016false,dieterich2016compas}, credit lending \cite{Kozodoi_2022}, 
and predictive policing \cite{lum2016predict}, 
has raised significant concerns regarding biases and unfairness in these models. Consequently, substantial efforts have been devoted to developing approaches for learning fair classification models that exhibit effective performance across protected attributes such as race and gender.

One critical aspect of addressing fairness in machine learning is ensuring the robustness of models against small amounts of adversarial corruption present in the training data. 
This data corruption may arise due to flawed data collection or cleaning processes \cite{saunders2013accuracy}, strategic misreporting \cite{hardt2016strategic}, under-representation of certain subgroups \cite{blum2019recovering}, or distribution shift over time \cite{schrouff2022maintaining}.

Empirical studies have demonstrated that such data unreliability is often centered on sensitive groups e.g. \cite{gianfrancesco2018potential}, emphasizing the need to understand the vulnerability of fair learning to adversarial perturbations. 
A concerning possibility is that fairness constraints might allow the adversary to amplify the effect of their corruptions by exploiting how these constraints require the classifier to have comparable performance on every relevant sub-group, even small ones.
%\footnote{Instead of thinking of our results in terms of an explicit adversary, they can also be interpreted as a worst case statement over corrupted data distributions, and that we want our fairness constrained learning pipelines to be `stable}'.

Previous work by \cite{lampert} and \cite{celis2021fair} have explored this topic from a theoretical perspective, considering different adversarial noise models. 
\cite{celis2021fair} focused on the $\eta$-Hamming model, where the adversary selectively perturbs a fraction of the dataset by modifying the protected attribute. 

\cite{lampert} on the other hand, investigated the Malicious Noise model, where an $\alpha$ fraction of the data-set (or distribution) 
is uniformly chosen and those data points are arbitrarily perturbed by the adversary.
We will focus on this Malicious Noise model.
In our study, we extend the framework of fair learning in the presence of Malicious Noise \cite{lampert} by  considering a broader range of fairness constraints and introducing a way to bypass some of their negative results by randomizing the hypothesis class.

\cite{lampert} present a pessimistic outlook, highlighting data distributions in which any proper learner, particularly in scenarios with imbalanced group sizes, exhibits high vulnerability to adversarial corruption when the learner is constrained by Demographic Parity \cite{calders2009building} or Equal Opportunity \cite{hardt16}. 
These results demonstrate novel and concerning challenges to designing fair learning algorithms resilient to adversarial manipulation in the form of 
Malicious Noise. 

The results of \cite{lampert} indicate that fairness constrained learning is much less robust than unconstrained learning.

In this paper, we  present a more optimistic perspective on the vulnerability of fairness-constrained learning to malicious noise by introducing randomized classifiers. 
By allowing randomized classifiers, we can explore alternative strategies that effectively mitigate the impact of malicious noise and enhance the robustness of fairness-constrained models.
In addition, we extend the analysis beyond the fairness constraints examined in \cite{lampert}, providing a complete characterization of the robustness of each constraint and revealing a diverse range of vulnerabilities to Malicious Noise.


\subsection{Our Contributions}
We bypass the impossibility results in \cite{lampert} by allowing the learner to produce a randomized improper classifier. This classifier is constructed from hypotheses in the base class $\mathcal{H}$ using our post-processing procedure, which we refer to as the $(P,Q)$-Randomized Expansion of a hypothesis class $\mathcal{H}$, or $\PQ$
%\footnote{$\PQ$ is chosen because the base hypotheses are parameterized by $p,q$.}
%in that we are including hypotheses near to $\mathcal{H}$.
\begin{definition}[$\PQ$] \label{defn:pq}
For each classifier $h \in \mathcal{H}$, for $p,q \in[0,1]$ 
% \pcomargincomment{this should actually be $p,q \in[0,1]^{|Z|}$, a vector of biases for each group}
% \kmsmargincomment{I think it is ok to be a bit informal in this section}
\begin{align*}
    h_{p,q}(x) : = 
    \begin{cases}
    h(x)\quad \text{ with probability }1-p \\
    y \sim \Bern(q) \quad \text{otherwise}
    \end{cases}
\end{align*}
We define $\PQ$ as the expanded hypothesis class created by the set of all possible
$h_{p,q}(x)$.
\[ 
%\PQ := \cup_{h \in \calH} \cup_{p,q \in [0,1]} \{ h_{p,q}(x) \} 
\PQ := \{ h_{p,q} \mid h \in \calH, p,q \in [0,1]\} 
\]
\end{definition}
When clear from context we drop the dependence on $p,q$ and simply refer to $\hat{h} \in \PQ$.

Larger $p$ means we ignore more of the information in the base classifier $h$ and rely on the $\Bern(q)$.
The main technical questions we address in this paper are:
%
\begin{center}
 \emph{
How susceptible and sensitive are fairness constrained learning algorithms
to Malicious Noise and to what extent does this vulnerability
depend on the specific fairness notion, especially if we allow improper learning?}

%  \emph{To What extent can improper learning bypass prior impossibility results for fairness constrained
% learning?}

%\emph{When a fairness aware ERM-learner is being targeted by an Adversary with malicious noise, by allowing the learner to exhibit a simple improper learning rule ($h^{'} \in \PQ$), can we exhibit hypotheses that are as robust to malicious noise as an ERM learner who is indifferent to fairness?}
\end{center}


%In the rest of the paper, we shall answer this question in the affirmative for some fairness constraints and show constant 
%
%In other words, the classical result \cite{kearns1988learning}
%\subsection{Our Contributions}


We focus on proving the existence of $h^{'} \in \PQ$ that satisfies a given fairness constraint and exhibits minimal accuracy loss on the original data distribution. 
Recall that $\alpha$ is the fraction of the overall distribution that is corrupted by the adversary.
%Our proofs are constructive. 
%We enumerate a hierarchy of fairness constraints and describe our results. 
%Here are the notions within order of robustness to malicious noise. (Again, here more robustness mean more accuracy on the natural data (despite the malicious noise) while still satisfying a given fairness constraint on biased data).
%In other words, train and select a model using the 

\begin{comment}
\begin{align*}
\textit{Demographic Parity} \geq Equal Opportunity \geq  Equalized Odds 
\geq Equal Error Rates
\end{align*}
\end{comment}

\begin{comment}
\begin{theorem} \label{thm: zoo}
Now we combine these fairness notions into one location and discuss their error in the presence of $O(\alpha)$ corruptions. 
Observe that from \cite{malnoise}, unconstrained ERM has an unconditional lower bound of $\Omega(\alpha)$.
This is our baseline and we want Fair ERM in the closure model to compete with this.
\begin{enumerate}
    \item Unconstrained ERM excess error $\Theta(\alpha)$ [exhibit $h^{*}$]
    \item Demographic Parity also has accuracy loss at most $\Theta(\alpha)$
    \item Eopp is at worst $\Omega\sqrt{\alpha})$
    \item Equal Error Rates : $\Omega(1)$
       \item Equalized Odds gets error $\min \{ r_A, r_B \}=\Omega(1)$

    \item Calibration has error in the worst case $\Omega(1)$
    %\item Min-Max Fairness has error in the worst case $\Omega(1)$
\end{enumerate}
\end{theorem}
\end{comment}

Our list of contributions is: 
\begin{enumerate}
\item We propose a way to 
%improperly learn a fair classifier in the 
bypass 
lower bounds \cite{lampert} in Fair-ERM with Malicious Noise by extending the hypothesis class using  the $\PQ$ notion.
%\item We show that this approach returns a classifier that 
%satisfies the fairness guarantee (parity and equal opportunity) with only a small loss in 
%accuracy when compared to the best hypothesis in the class. 
\item For the Demographic Parity \cite{calders2009building} constraint, our approach guarantees no more than $O(\alpha)$ loss in accuracy (which is \emph{optimal}
%the best one can expect 
in the Malicious Noise model \emph{without} fairness constraints \cite{malnoise}). 
In other words, in contrast to the perspective in \cite{lampert} which shows $\Omega(1)$ accuracy loss, we show that Demographic Parity constrained ERM can be made just as robust to Malicious Noise as unconstrained ERM.
\item For the Equal Opportunity \cite{hardt16} constraint, we guarantee no more than $O(\sqrt{\alpha})$ accuracy loss
and show that this is tight, i.e no classifier can do better. 
%We also show that no approach can do better than this.
\item For the fairness constraints Equalized Odds \cite{hardt16}, Minimax Error \cite{minimaxfair}, Predictive Parity, and our novel fairness constraint Parity Calibration, we show strong negative results.
%hat is, 
Namely, for each constraint there exist natural distributions such that an adversary that can force any algorithm to return a fair classifier that has $\Omega(1)$ loss in accuracy.
\item For Calibration \cite{faircalib}, we observe that the excess accuracy loss is at most $O(\alpha)$.
%\item Lastly we present sufficient conditions for a hypothesis class to be robust in the malicious adversarial model.
\end{enumerate}
%These results contradict \pcomargincomment{I think contradict is a strong wording here. Also, I think we're hammering a bit much on this distinction} parts of and extend the landscape introduced in \cite{lampert}. 
%\kmsreplace{This}{Our} work prompts \kmsdelete{high level} questions \pcomargincomment{what high level questions? It might be good to state them directly. Might not be obvious to the reader. I'm not sure what they are.} 
% These results prompts questions \pcomargincomment{"prompt". But also, what questions? This is very vague} about the right sensitivity level of fairness constraints.
%We do not focu

\begin{comment}
\begin{tabular}{ |p{2.7cm}||p{2.7cm}|p{2.7cm}|p{2.7cm}|p{2.7cm}| }
 \hline
 \multicolumn{5}{|c|}{ Results Summary} \\
 \hline
 Size of Group $B$ & Demographic Parity &  Equal Opportunity & Equal Error Rates & Equalized Odds \\
 \hline
 $ |A|=|B|$  & AF    &AFG&   004  & 420 \\
$|A| > O(\alpha) = |B|$ &   AX  & ALA   &248 & 420\\
 \hline
\end{tabular}
\end{comment}

\input{NeurIPS23/rel_works.tex}