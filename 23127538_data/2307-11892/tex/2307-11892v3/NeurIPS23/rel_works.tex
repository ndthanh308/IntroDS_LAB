\section{Related Work}
\cite{kearns1988learning} introduced the notion of malicious noise which is analyzed in \cite{bshouty2002pac, auer1998line, klivans2009learning, long2011learning, awasthi2014power}.
\cite{balcan2022robustly} considers a related adversary as a way to formalize data poisoning attacks in adversarial robustness
 \cite{goodfellow2014explaining}.
%\subsection{Fairness and Unreliable Data}

The interaction of fairness constraints with explicitly unreliable data is a critical research direction
%, which is extremely salient since 
since issues of bias and fairness are often closely connected with data reliability concerns \cite{gianfrancesco2018potential}.
Malicious noise is both a way to model an explicit adversary and a way to consider unknown natural issues with the data distribution.

\cite{celis2021fair}, which also studies fairness with data corruptions. 
%considers similar questions to the ones explored in this paper,
primarily focuses on the stronger Nasty Noise Model \cite{bshouty2002pac}
%That work focuses on providing algorithms and analysis under that model
combined with an assumption on the minimum size of groups/events. 
They do not consider how randomized post-processing improves robustness.

%\subsection{Comparision with \cite{lampert}}
The most closely related work to ours, \cite{lampert}, explores the limits of fairness-aware PAC learning within the classic malicious noise model of  \cite{valiant}, where the adversary can replace a uniformly random fraction of the data points with arbitrary data, with full knowledge of the learning algorithm, the data distribution, and the remaining examples. 
%They \footnote{Their work also considers}
% That paper, 
\cite{lampert} focuses on binary classification with just two popular group fairness constraints: Demographic Parity  \cite{calders2009building} and Equal Opportunity \cite{hardt16}. In addition to those constraints, we also consider Equalized Odds and multiple Calibration notions.
Similarly to \cite{lampert}, a key aspect of our results is how the size of the smaller group makes it more vulnerable to data corruption.

\begin{comment}
\begin{enumerate}
    \item First they show that learning under this adversarial model is provably impossible in a PAC sense - there is no learning algorithm that can ensure convergence with high probability
to a point on the accuracy-fairness Pareto front on the set of all finite hypothesis spaces, even in the limit of infinite training data.
\item Furthermore, the irreducible excess gap in the fairness measures they study is inversely proportional to the frequency of the rarer of the two protected attributes groups. This makes the robust learning problem especially hard when one of the protected subgroups in the data is underrepresented. 
\item They also show that the adversary can ensure that any learning algorithm will output a classifier that is optimal in terms of accuracy, but exhibits a large amount of unfairness. 
They also show that their hardness results are tight up to constant factors, in terms of the corruption ratio and the protected group frequencies, by proving matching upper bounds.
\end{enumerate}
\end{comment}

\subsection{Group Fairness Discussion}

Note that group fairness constraints \cite{chouldechova2017fair,klein16, hardt16} are relatively simple to evaluate and provide relatively weak guarantees in contrast to fairness notions in \cite{calders2009building, dwork2021outcome, hebert2018multicalibration}, among others.  
However, despite this weakness,  these group notions are used in practice \cite{metricsinpractice} to check model performance, so continuing to investigate them in parallel to the stronger fairness notions is worthwhile.

\cite{blum2019recovering} takes a somewhat converse perspective to this paper.
Instead of considering worst case instances for how much fairness constraints force excess accuracy loss with an adversary, that paper
asks how fairness constraints can help us recover from the biased data when the bias is more benign, but still complicates Empirical Risk Minimization.
Future work could connect our results with theirs by considering an intermediate adversary model, such as \cite{massart2006risk}.

Interestingly, there may be high level connections between our paper and work in federated learning with differing data quality levels, e.g \cite{chu2023focus}.
%\subsection{Black Box Fairness Processing \red{incomplete}}
%move this else where
%Our paper can be thought of as a way to black box post-process an existing model, in order to be fair and accurate.
% There is extensive literature on post processing methods since they 

 %xAdditionally, post processing in the presence of malicious noise is 
%Fair projection \cite{fairprojection}.

