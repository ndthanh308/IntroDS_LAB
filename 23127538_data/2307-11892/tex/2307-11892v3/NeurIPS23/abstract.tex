\begin{abstract}

We consider the vulnerability of fairness-constrained learning to small amounts of malicious noise in the training data. \cite{lampert} initiated the study of this question and presented negative results showing there exist data distributions where for several fairness constraints, any proper learner will exhibit high vulnerability when group sizes are imbalanced. Here, we present a more optimistic view, showing that if we allow randomized classifiers, then the landscape is much more nuanced. For example, for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible even without fairness constraints. For Equal Opportunity, we show we can incur an $O(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$ lower bound. In contrast, \cite{lampert} showed for proper learners the loss in accuracy for both notions is $\Omega(1)$. The key technical novelty of our work is how randomization can bypass simple "tricks" an adversary can use to amplify his power.  
We also consider additional fairness notions including Equalized Odds and Calibration. For these fairness notions, the excess accuracy clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$, and $O(1)$. These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.

%We consider fairness-constrained ERM in the presence of  malicious noise \cite{malnoise}, where
%the adversary can corrupt a randomly sampled $\alpha$ fraction of the dataset.
  %in the setting of \cite{lampert}. 
 %\cite{lampert}shows undesirable excessive accuracy loss in this setting.
 %\pcomargincomment{I wonder if the previous sentence can be paraphrased, particularly the part "interpreting this..."}
  %  We consider a broader range of fairness notions and characterize the maximum accuracy loss 
   % when we allow a simple randomized classifer.
    %pcomargincomment{what does min-max mean here exactly? 
    %Is it like accuracy loss against worst-case adversary?} 
    %accuracy loss when we allow a simple  improper randomized classifier.
  
  %In particular, our work shows a more optimistic view than the impossibility results described in \cite{lampert}.
  %For Demographic Parity we attain the optimal $\Theta(\alpha)$ loss in accuracy and for Equal Opportunity $\Omega(\sqrt{\alpha})$ regardless of group size, in sharp contrast to\cite{lampert} where the loss in accuracy for both notions was $\Omega(1)$.
%  The key technical novelty of our work is how randomization weakens the ability of the adversary to use the fairness constraints to force poor classifier performance.
 
  %The key difference is we allow a randomized classifier created from a base hypothesis class that allows for smoothing out the power of the adversary. \pcocomment{what does smoothing out the power of the adversary mean?}
 %Additionally, we  consider a wide range of fairness notions including Demographic Parity, Equal Opportunity, Equalized Odds, Minimax fairness.
  %\ascomment{need to capitalize too?}, and a mix of Calibration notions. 
  %When considering these regimes, the excess accuracy clusters into three natural regimes $O(\alpha), O(\sqrt{\alpha}$, and $O(1)$.
  %These results provide a more fine-grained view of the sensitivity of Fair-ERM. 
  %78\pcomargincomment{I like this concluding sentence}
  %  At a high level our work shows that Fair ERM is as robust to malicious noise as plain ERM, assuming we have access to such a randomized classifier. 
  %For the range of fairness not
\end{abstract}