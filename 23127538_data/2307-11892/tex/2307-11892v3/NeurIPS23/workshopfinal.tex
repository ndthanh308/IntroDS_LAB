\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[final]{neurips_2023}

%\input{arxiv_style}
\include{notations}
\usepackage{mkolar_definitions}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{comment}


\newcommand{\dist}{\mathcal{D}}
\newcommand{\DA}{\mathcal{D}_A}
\newcommand{\DB}{\mathcal{D}_B}
\newcommand{\closure}{cl(\mathcal{H})}
\newcommand{\Bern}{\text{Bernoulli}}

%\usepackage{color-edits}%[suppress] % use suppress below instead to implement all the edits and remove comments
\usepackage{color-edits}[suppress]
\addauthor[suppress]{kms}{red}
\addauthor[suprress]{pco}{purple}
\addauthor{suppress}{orange}
\addauthor{as}{cyan}
\usepackage{alg}
\usepackage{multirow}
\usepackage{graphicx}


%\usepackage[demo]{graphicx}
%\usepackage{subfig}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%\usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}


\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
%%%%%%
%Best title


\title{On the Vulnerability of Fairness Constrained Learning to Malicious Noise}


%%%%%%%%%%%%%
%\title{Malicious Noise and an Improper Fair ERM: Not Impossible Fairness Aware Learning from Corrupted Data}

%\title{ Fairness Aware Learning from Corrupted Data is not Impossible}

%%%GOOD titles 
%\title{On Fairness Constrained Learning and Malicious Noise}
%\title{On Fairness and Malicious Noise }********
%\title{Fair Learning with Malicious Noise Is Possible, with Randomized Classifiers }

%\title{On Fairness, Malicious Noise, and Improper Learning} %%% SABA LIKES THIS
%\title{Fair Classification and Malicious Noise}

%\title{On Fair Learning, Malicious Noise, and Randomization}
%\title{On the vulnerability of Fair Learning to Malicious Noise}


%\title{Fairness Through Randomization despite Malicious Noise}
%\title{Fairness and Malicious Noise}


%\title{On The Vulnerability of Fairness-Constrained Learning to Data Poisoning and Malicious Noise} 

%------
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
 Avrim Blum \\
TTIC  \\
avrim@ttic.edu
 \And
 Princewill  Okoroafor \\
 Cornell University \\
 pco9@cornell.edu
 \And
 Aadirupa Saha  \\
 TTIC \\
  aadirupa@ttic.edu
 \And
 Kevin Stangl \\
TTIC  \\
 kevin@ttic.edu
}
% \author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \AND
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
%   % \And
%   % Coauthor \\
%   % Affiliation \\
%   % Address \\
%   % \texttt{email} \\
% }

\begin{document}


\maketitle


%\input{NeurIPS23/abstract.tex}
\begin{abstract}
  We 
  %\kmsreplace{consider the vulnerability of}{study} 
  study
  fairness-constrained learning with
  %\kmsdelete{small amounts of} 
  malicious noise.
  %\kmsdelete{ in the training data.} 
\cite{lampert} introduced \kmsreplace{the study of this question}{this model} and \kmsdelete{presented negative results showing}{exhibited} hard data distributions where for \kmsedit{Demographic Parity and Equal Opportunity}
%\kmsdelete{natural} 
fairness constraints, any proper learner \kmsedit{satisfying the fairness constraints on corrupted data} will exhibit 
\kmsedit{$\Omega(1)$ accuracy loss} \kmsedit{on the original data distribution.}\kmsdelete{high vulnerability when group sizes are imbalanced}. 
\kmsdelete{Here,} We present a more optimistic view, showing that if we allow randomized classifiers, then the \kmsedit{robustness} landscape is \kmsdelete{much} more nuanced. 

For Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in accuracy, where $\alpha$ is the malicious noise rate, matching the best possible accuracy loss even without fairness constraints. 
For Equal Opportunity, we show we can incur an \kmsedit{$\Theta(\sqrt{\alpha})$} accuracy loss. \kmsdelete{$(\sqrt{\alpha})$ loss, and give a matching $\Omega(\sqrt{\alpha})$ lower bound.}
\kmsdelete{In contrast, \cite{lampert} showed for proper learners the loss in accuracy for both notions is $\Omega(1)$.} 
\kmsreplace{The key technical novelty of our work}{Our key insight} is how randomized classifiers bypass the way the 
\kmsdelete{can bypass simple "tricks" an }
adversary \kmsreplace{can use to amplify his power}{exploits the fairness constraints}.  
We also consider \kmsdelete{additional fairness notions including} Equalized Odds and Calibration \kmsreplace{For these fairness notions,}{and show that} the excess accuracy \kmsedit{loss} clusters into three natural regimes $O(\alpha)$,$O(\sqrt{\alpha})$, and $O(1)$.
\kmsdelete{These results provide a more fine-grained view of the sensitivity of fairness-constrained learning to adversarial noise in training data.}
\end{abstract}


\input{NeurIPS23/intro.tex}

\input{sections/preliminaries}

%\input{NeurIPS23/multigroup}

\input{sections/main_results}

%\input{sections/parity}

%\input{sections/eopp}

%\input{sections/eodds}

\section{Main Results: Calibration}
\input{sections/calibration}

%\input{sections/minimax}

\begin{comment}
\section{Sufficient Properties for Robust Fair-ERM, Discussion from 4/18/23} 
%This isn't ready for prime time unfortunately-follow up work
In the previous section we exhibited a randomized classifier derived from a base class $\mathcal{H}$ that satisfied 
the fairness constraints on the biased data \emph{and} suffered a minimum accuracy loss.

However, there are obstacles to using this type of randomized classifier in practice.
Primarily that in fairness contexts, test time randomness feels `unfair' or in some sense procedural inappropriate in high stakes settings.
This observation motives us to try to abstract away the joint properties 
of the classifier and the distribution that would allow us to implement a deterministic classifier with the same properties as the randomized closure. 

\begin{example}
blah blah I am an example
\end{example}
\end{comment}
\input{sections/discussion}

%\newpage

% We choose the "plain" reference style
\bibliographystyle{plain}
\bibliography{ref}

\newpage

\input{sections/appendix}

\end{document}