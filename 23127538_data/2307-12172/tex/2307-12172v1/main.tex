%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

%In case you encounter the following error:
%Error 1010 The PDF file may be corrupt (unable to open PDF file) OR
%Error 1000 An error occurred while parsing a contents stream. Unable to analyze the PDF file.
%This is a known problem with pdfLaTeX conversion filter. The file cannot be opened with acrobat reader
%Please use one of the alternatives below to circumvent this error by uncommenting one or the other
%\pdfobjcompresslevel=0
%\pdfminorversion=4

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{svg.path}

\definecolor{orcidlogocol}{HTML}{A6CE39}
\tikzset{
  orcidlogo/.pic={
    \fill[orcidlogocol] svg{M256,128c0,70.7-57.3,128-128,128C57.3,256,0,198.7,0,128C0,57.3,57.3,0,128,0C198.7,0,256,57.3,256,128z};
    \fill[white] svg{M86.3,186.2H70.9V79.1h15.4v48.4V186.2z}
                 svg{M108.9,79.1h41.6c39.6,0,57,28.3,57,53.6c0,27.5-21.5,53.6-56.8,53.6h-41.8V79.1z M124.3,172.4h24.5c34.9,0,42.9-26.5,42.9-39.7c0-21.5-13.7-39.7-43.7-39.7h-23.7V172.4z}
                 svg{M88.7,56.8c0,5.5-4.5,10.1-10.1,10.1c-5.6,0-10.1-4.6-10.1-10.1c0-5.6,4.5-10.1,10.1-10.1C84.2,46.7,88.7,51.3,88.7,56.8z};
  }
}

\newcommand\orcidicon[1]{\href{https://orcid.org/#1}{\mbox{\scalerel*{
\begin{tikzpicture}[yscale=-1,transform shape]
\pic{orcidlogo};
\end{tikzpicture}
}{|}}}}

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{comment}
\usepackage{tabularx}
\usepackage{tablefootnote}
\usetikzlibrary{tikzmark}
\usepackage{soul}

\usepackage{hyperref}

\title{Challenges for Monocular 6D Object Pose Estimation in Robotics}

\author{
Stefan Thalhammer\orcidicon{0000-0002-0008-430X}\,, %\IEEEmembership{Member, IEEE}, 
Dominik Bauer\orcidicon{0000-0002-1260-1319}\,, %\IEEEmembership{Member, IEEE}, 
Peter H{\"o}nig\orcidicon{0000-0002-8990-6110}\,,
%\IEEEmembership{Student Member, IEEE},\\
Jean-Baptiste Weibel\orcidicon{0000-0003-0201-4740}\,, \\%\IEEEmembership{Member, IEEE},
José García-Rodríguez\orcidicon{0000-0002-7798-3055}\,, %\IEEEmembership{Senior Member,
%IEEE}\\
and Markus Vincze\orcidicon{0000-0002-2799-491X}\,, %\IEEEmembership{Senior Member, IEEE}% <-this % stops a space
%\thanks{*This work was not supported by any organization}% <-this % stops a space
\thanks{Stefan Thalhammer, Peter H{\"o}nig, Jean-Baptiste Weibel and Markus Vincze are with the Automation and Control Institute, TU Wien, Austria
        {\tt\footnotesize \{thalhammer,hoenig,weibel,vincze\}@acin.tuwien.ac.at}}%
\thanks{Dominik Bauer is with the Columbia Artificial Intelligence and Robotics Lab, Columbia University, USA
        {\tt\footnotesize dominik.bauer@columbia.edu}}%
\thanks{José García-Rodríguez is with the Department of Computer Technology, University of Alicante, Spain
        {\tt\footnotesize jgarcia@dtic.ua.es}}%
% \thanks{Manuscript received April tba.}
}

\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}

\begin{abstract}
Object pose estimation is a core perception task that enables, for example, object grasping and scene understanding.
The widely available, inexpensive and high-resolution RGB sensors and CNNs that allow for fast inference based on this modality make monocular approaches especially well suited for robotics applications.
We observe that previous surveys on object pose estimation establish the state of the art for varying modalities, single- and multi-view settings, and datasets and metrics that consider a multitude of applications.
We argue, however, that those works' broad scope hinders the identification of open challenges that are specific to monocular approaches and the derivation of promising future challenges for their application in robotics.
By providing a unified view on recent publications from both robotics and computer vision, we find that occlusion handling, novel pose representations, and formalizing and improving category-level pose estimation are still fundamental challenges that are highly relevant for robotics.
Moreover, to further improve robotic performance, large object sets, novel objects, refractive materials, and uncertainty estimates are central, largely unsolved open challenges.
In order to address them, ontological reasoning, deformability handling, scene-level reasoning, realistic datasets, and the ecological footprint of algorithms need to be improved.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
Object pose estimation is an essential task for applications like robotic grasping~\cite{patten2020dgcm}, bin picking~\cite{kleeberger2019large} and semantic scene understanding~\cite{Nie_2020_CVPR}.
Though depth readings have proven to be reliable for deriving poses of objects with distinctive geometry, special hardware is required.
These sensors are comparably expensive, have varying noise patterns at different distances, image regions and between sensors and have lower resolutions than RGB sensors.
Due to the RGB sensors' low cost, expressiveness, low noise in comparison to depth sensors, and efficient processing with CNNs, it provides a broader range of application. 
It offers abundant information to disambiguate object symmetries through texture, provides noise-free captures of objects with challenging material properties like transparency, which are difficult to sense with depth sensors, and has larger working ranges ideally suited for the unstructured open-world.
As a consequence RGB gained significant relevance for object pose estimation in the recent years.
In the most recent edition of the Benchmark for $6D$ Object Pose estimation (BOP), the top two, and three out of the top five methods generate initial pose hypotheses in RGB, and only use depth data for pose refinement~\cite{hodan2022bop}.
This trend, that monocular approaches, taking RGB as input, outperform depth-based ones already started in the previous edition~\cite{hodan2020epos} and as such this type of approaches is the go-to choice for detecting objects and providing initial pose hypotheses.
This substantiates the importance of monocular single-shot $6D$ object pose estimation for robotics.

Recent summaries of the state-of-the-art for object pose estimation~\cite{he20206d,sahin2020review,fan2022deep,hoque2021comprehensive} report domain shift, end-to-end training, occlusion handling, symmetry handling, improving datasets, reasoning about geometric properties, clutter handling, and pose estimation of metallic, deformable, and textureless objects as future challenges.
Since the field is evolving rapidly, works published in the meantime largely solved the problems of domain shift, handling symmetries and clutter, textureless objects, and end-to-end training on standard datasets.
%In comparison to these works, we focus on that problem and how to further progress research to effectively improve robotics.
In comparison to these works, we focus on the specific context of monocular object pose estimation for robotics.
Thus, we are able to discuss relevant research problems more in detail and derive ongoing and future robotics-related challenges that received less attention in prior reviews.
% better?

% The scope of this work is to present important research problems for single-shot monocular $6D$ object pose estimation.
We base our discussion on a review of current key problems, identified by extracting works published in the years $2021$, $2022$, and $2023$ (up to paper submission) in robotics and computer vision venues with high scientific impact.
In addition, we include seminal work that addresses specific problems in pose estimation and identify the remaining gap to solve them.
Our review of these works suggests that two of the previous major challenges are largely solved; that is, overcoming the domain shift and occlusion handling, which we argue is not a specific challenge to solve, but rather a characteristic of high-performing approaches.
We instead highlight the importance of more fundamental problems such as finding an unambiguous yet compact pose representation, multi-object handling, category-level pose estimation, novel object pose estimation beyond such known categories, and handling challenging material properties and the resulting visual uncertainties.
Future challenges are derived by identifying the gaps in this problem landscape from a robotics perspective. Specifically, downstream robotic and vision tasks benefit from knowledge of the object interaction on scene-level, and learning object ontologies that describe non-distinct class memberships helps resolve edge cases of the real world. Novel datasets that reflect the complexity of unstructured, natural robotic environments will highlight the improvement required beyond current standard algorithms, and as such progress robotic research. Finally, measures need to be taken to reduce the environmental impact of pose estimation research and its application in robotics.
In summary, this review provides
\begin{itemize}
    %\item critical examination of the current research landscape of single-view monocular object pose estimation, identifying unsolved problems that are relevant for the advancement of robotics,
    %\item evaluation of the relevance of common dataset for open challenges in robotics, and
    %\item identification of superordinate problems, and derivation of meaningful future research problems to effectively progress robotics.
    \item arguments which indicate that many core challenges identified by previous surveys, such as domain adaptation, are thoroughly investigated in the meantime,
    \item an evaluation of the relevance of common dataset for open challenges in robotics, and
    \item a survey focused on robotics-specific challenges in ever-changing unstructured environments, allowing us to a) identify solutions to, e.g., handling many and novel objects, that still lack in performance for robotic application and b) identify problems that occur in typical robotics settings, such as non-distinct categories or natural scene structures, that are overlooked by previous surveys.
\end{itemize}

The remainder of the paper is organized as follows. %Section~\ref{sec:diff} presents the differentiation to contemporary summary articles. In 
Section~\ref{sec:problems}, presents our analyses of currently investigated research problems. Sections~\ref{sec:datasets} and~\ref{sec:trends} presents common datasets and the status of the currently progress towards solving these research problems. 
Subsequently, Section~\ref{sec:future} presents promising novel directions. 
Section~\ref{sec:conclusion} provides a summary of our findings and discusses future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Open Problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ongoing Research and Common Datasets}
\label{sec:problems}

%This section provides an analysis of currently important research problems for monocular single-shot $6D$ pose estimation.
In order to investigate problems, we gathered a representative sample of works published in the years $2021$, $2022$ and $2023$, up to the first draft submission of this script.
Publications are selected from the top six robotics and computer vision venues, as determined by the h5-index, impact factor and SJR metric. 
For robotics we consider works published in IEEE/RAS ICRA, IEEE RA-L, IEEE/RSJ IROS, IEEE/ASME T-MECH, IEEE T-RO and SAGE IJRR.
For computer vision we consider works published in IEEE/CVF CVPR, IEEE/CVF ICCV, Springer ECCV, IEEE T-PAMI, IEEE T-IP and Elsevier PR.

Table~\ref{tab:topics} presents the motivating problems for the respective scientific works, sorted by frequency. The total number of publications considered for identifying these active research problems is $50$, of which $22$ are published in the field of robotics and $28$ in computer vision. Single works count toward multiple problems. 
The following section presents datasets that are commonly used for benchmarking these single-shot approaches for $6D$ object pose estimation.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|}
 \hline
 Problem & References & freq. \\ \hline
 %\tikzmark{Smark}
 Domain shift & \cite{zhang2021keypoint}\cite{hu2022perspective} \cite{nguyen2022templates} \cite{wang2021self6d++} \cite{thalhammer2021pyrapose} \cite{shi2021fastUQ} & \\
 & \cite{lu2022slam}  \cite{ikeda2022sim2real} \cite{yang2022image} \cite{jawaid2023towards} \cite{Chen_2023_CVPR} & 11 \\
 Occlusion & \cite{ma2022robust} \cite{nguyen2022templates} \cite{peng2022pvnet} \cite{shugurov2022dpodv2} \cite{liu2021mfpn6d} \cite{liu2021kdfnet} & \\
 & \cite{mei2022spatial} \cite{hai2023rigidity} & 8 \\
  End-to-end & \cite{wang2021gdr} \cite{di2021so} \cite{chen2022epro} \cite{lipson2022coupled} \cite{cao2022dgecn} \cite{hai2023shape} & 6 \\
 Representation & \cite{huang2022neural} \cite{di2021so} \cite{park2022dprost} \cite{su2022zebrapose} \cite{yen2021inerf} & 5 \\
  Category & \cite{fan2022object} \cite{wen2022disp6d} \cite{ma2022robust} \cite{lin2022single}  \cite{Lee2021category}  & 5 \\ 
 %\tikzmark{Emark}  \\
 Refinement & \cite{iwase2021repose} \cite{haugaard2021surfemb} \cite{lipson2022coupled} \cite{shugurov2022dpodv2} \cite{araki2021iterative} & 5 \\
 Symmetry & \cite{wen2022disp6d} \cite{haugaard2021surfemb} \cite{richter2021handling} \cite{bengston2021pose} \cite{jeon2023ambiguity} & 5 \\
 Novel objects & \cite{liu2022gen6d} \cite{shugurov2022osop} \cite{nguyen2022templates} \cite{gu2022ossid} \cite{saxena2023generalizable} & 5 \\
 Multi-object & \cite{iwase2021repose} \cite{wen2022disp6d} \cite{thalhammer2021pyrapose} \cite{jeon2023ambiguity} & 4 \\
 Material properties & \cite{he2022generative} \cite{liu2021mfpn6d} \cite{yu2023TGFnet} & 3 \\
 Uncertainty estimation & \cite{Yang_2023_CVPR} \cite{jeon2023ambiguity} \cite{shi2021fastUQ} & 3 \\
 %Few real samples & \cite{hu2022perspective} \cite{Chen_2023_CVPR} & 2 \\
 Learning principles & \cite{guo2023knowledge} \cite{Chen_2023_CVPR} & 2 \\
\hline
%\end{tabularx}
\end{tabular}
\caption{\textbf{Ongoing monocular object pose estimation research} Frequencies of the motivating problems for papers published in the years 2021 and 2022 and 2023 (up to paper submission).}
%%  \draw[red] ([shift={(-3.6ex,1.5ex)}]pic cs:Smark) rectangle ([shift={(2.3ex,-0.5ex)}]pic cs:Emark);
%\end{tikzpicture}
\label{tab:topics}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Datasets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Datasets}
\label{sec:datasets}

Common datasets are crucial for comparing different methods in a reproducible manner.
As a consequence pose estimation methods are designed with the focus of maximizing accuracy on them. 
Obviously, if the datasets' complexity does not reflect the relevant real-world complexity of robotic scenarios, this will lead to a performance discrepancy at deployment.
The following discussion of these datasets thus helps identifying shortcomings in the problem landscape of mainstream methods, and allows deriving important challenges that are required to improve dataset design to the performance in the real world. 
Datasets that do not provide real test images are not included in the following list, since methods tested on them do not reflect the expected performance in the real world and thus in robotics applications.

We group existing datasets based on the respective object sets. 
We first present instance-level datasets, which are the most used and considered the de-facto standard for benchmarking object pose estimation approaches~\cite{hinterstoisser2012model,brachmann2014learning,hodan2018bop,doumanoglou2016recovering,homebrewedDB,hodan2017tless,drost2017introducing}.
%since the first eight of these constitute the core set for the BOP challenge.
%Only one of these datasets features objects with specular reflective surfaces, ITODD~\cite{drost2017introducing}.
Following that, datasets for category-level pose estimation are listed. These are of interest for robotics due to the possibility of testing the generalization ability of approaches~\cite{wang2019normalized,chen2022clearpose,dai2022dreds,wang2022phocal,liu2020keypose}.
Lastly, we discuss datasets that include annotations for grasping, used to benchmark the object pose estimation works presented in~\cite{fang2022transcg,Cao2021suction}.

% Figure environment removed


\begin{table*}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
 Dataset & Multi-object & Multi-instance & Occlusion & Category-level & Symmetries & Challenging Materials \\ \hline
 1) LM/LM-O\cite{hinterstoisser2012model,brachmann2014learning} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\
 2) YCB-V\cite{xiang2017posecnn} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\
 3) TUD-L\cite{hodan2018bop} & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 4) IC-BIN\cite{doumanoglou2016recovering} & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\
 5) HB\cite{homebrewedDB} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\
 6) HOPE\cite{tyree20226} & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark \\
 7) T-LESS\cite{hodan2017tless} & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\
 8) ITODD\cite{drost2017introducing} & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark \\
 9) MP6D\cite{chen2022MP6D} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark \\
 10) ClearPose\cite{chen2022clearpose} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark \\
 11) NOCS\cite{wang2019normalized} & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{red}\xmark \\
 12) PhoCal\cite{wang2022phocal} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark \\
 13) DREDS\cite{dai2022dreds} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark \\
 14) KeyPose\cite{liu2020keypose} & \color{red}\xmark & \color{red}\xmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark \\
 15) TransCG\cite{fang2022transcg} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark & \color{green}\cmark \\
 16) Grasp/-SuctionNet\cite{fang2020graspnet,Cao2021suction} & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark & \color{green}\cmark & \color{red}\xmark \\ \hline
\end{tabular}
\caption{\textbf{Dataset comparison} Challenges of datasets in comparison.} 
\label{tab:datasets}
\end{table*}


\subsubsection{Linemod/-Occlusion~\cite{hinterstoisser2012model,brachmann2014learning} (LM/LM-O)} 
$13$ objects with one test sequence of approximately $1,300$ images for each are provided. 
LM has been introduced as challenging due to clutter backgrounds, yet objects are fully visible in virtually all test images. 
To create a challenging occlusion patterns,~\cite{brachmann2014learning} re-annotated LM's second test sequence, introducing Linemod-Occlusion (LM-O).
LM-O provides annotations for eight of LM's objects under severe occlusion.
For both datasets no real-world training images are available, yet an established standard is that $15\%$ of the test images of LM are used for training~\cite{Park_2019_ICCV}. 

\subsubsection{YCB-video (YCB-V)~\cite{xiang2017posecnn}} $21$ objects from YCB~\cite{calli2015ycb} are annotated in $134k$ real images.
Out of the complete set, $113k$ images are used for training and the rest for testing.
Challenges are varying levels of occlusion and large variety in illumination.

\subsubsection{TUD-L~\cite{hodan2018bop}} This dataset was presented alongside the $2018$ edition of the BOP.
In the current edition of the challenge, three objects with $11,000$ real-world training and $200$ test images for each, are used.
The challenges are strong viewpoint and illumination changes.
Additionally, in contrast to other datasets, the objects are not supported by a table plane, only a small object-sized support.

\subsubsection{IC-BIN~\cite{doumanoglou2016recovering}} This dataset provides multi-object test images of eight objects in clutter and with occlusion.
For the BOP, three sequences featuring three of the eight objects are used, resulting in $150$ test images.
The test set features tightly packed objects in a pile, resulting in heavy occlusion.

\subsubsection{Homebrewed-Database (HB)~\cite{homebrewedDB}} This dataset consists of $33$ household and industrial objects.
No official training set is available. Of each of the available $13$ sequences, $340$ images are used for validation.
Three sets with $100$ images each are used as the BOP test set.
The challenges of the dataset are symmetries, occlusion and strong illumination variation.

\subsubsection{HOPE~\cite{tyree20226}} The dataset consists of $28$ toy grocery objects in household scenes.  
For testing, $238$ images in $50$ different scenes are provided.
The challenges are multiple object instances per image, occlusion, strong illumination changes, and clutter. 

\subsubsection{T-LESS~\cite{hodan2017tless}} Provided are $30$ texture-less objects which are highly symmetrical, similar in shape, and some objects are parts of others; objects one might find in an industrial setting.
For each of the dataset objects, CAD models, reconstructions, and $1,296$ real training images of uniformly distributed views are available.
The challenges for this dataset are to handle symmetries, occlusion, textureless objects, and inter-object similarities.

\subsubsection{ITODD~\cite{drost2017introducing}} consists of $28$ metallic objects and grayscale images. 
This dataset is also part of the BOP, yet it is the only dataset in the challenge with more challenging material properties than diffusely reflective surfaces.
The test set exhibits heavy occlusion and challenging material properties for both RGB and depth-based methods.

\subsubsection{MP6D~\cite{chen2022MP6D}} provides $20,100$ real-world images, where $20$ object instances are annotated.
Of these, $\approx6,000$ are used for testing.
These fully specular-reflective objects are placed in a multi-object settings.
Aside from their specular reflectivity, the challenges are handling clutter, symmetry and occlusion.

\subsubsection{ClearPose~\cite{chen2022clearpose}} The dataset features $63$ transparent objects, most of them exhibiting symmetries.
Provided are $354,481$ real images in $51$ scenes that provide diverse backgrounds, changing illumination, and objects in clutter with occlusion.


\subsubsection{NOCS~\cite{wang2019normalized}} presents two datasets for category-level object pose estimation.
Context-Aware MixEd ReAlity (CAMERA) consists of $300k$ images with real background and rendered object instances from $1,085$ objects, of which $25k$ images and $184$ object instances are for validation. 
REAL provides $4,300$ training, $950$ validation and $2,750$ test images, with six categories of three objects for training and three for testing. For the validation set, another instance per object category is provided.
The challenge for these datasets is category-level pose estimation in multi-object scenes, with symmetries and occlusion in clutter.

\subsubsection{PhoCal~\cite{wang2022phocal}} The scenes of the dataset show $60$ specular-reflective, opaque and transparent objects, across eight categories. 
The scenes are multi-object and introduce occlusion.
The test set also provides novel instances per category.

\subsubsection{DREDS~\cite{dai2022dreds}} provides richly randomized synthetic training data of $119,580$ images, of which $19,380$ are for testing. 
These are composed of $1,801$ objects of seven categories.
An additional synthetic test set with $11,520$ images of $60$ novel objects is provided.
In the synthetic sets, object materials are randomized, ranging from diffuse over specular reflectivity, to transparency.
The real sets consists of $27k$ images of $42$ object instances from seven categories.
Additionally, $11k$ images of eight novel objects of known categories are provided.


%\subsubsection{Objectron~\cite{ahmadyan2021objectron}} The dataset provides four million object-centric annotated images of $9$ categories.
%An interesting aspect is the geo-diversity, since videos from which frames are derived, are captured in $10$ countries across five continents.
%This dataset provides the challenge of category-level $3D$ pose estimation. While no $6D$ ground truth is provided, it is possible to retrieve from the annotated $3D$ bounding boxes.
%Although, this dataset is an outlier in comparison to the other presented ones, since being object-centric, it is worth to include it since object scales range from small objects like cups to big objects like motorbikes.


\subsubsection{KeyPose~\cite{liu2020keypose}} $15$ object instances of five categories are provided.
The test sets show isolated object instances on diverse backgrounds.% on scene level.
The challenge consist of detecting and estimating poses of fully transparent objects.

\subsubsection{TransCG~\cite{fang2022transcg}} This grasping dataset provides $57,715$ images of $51$ transparent objects, of which $12$ are used for testing.
The challenge is objects' transparency, interacting with diverse opaque distractors and with changing background.
Opaque tags are attached to each object and present in the images.
One aspect that thus needs to be evaluated when using the dataset for training networks, is the influence of these tags on the learned feature encoding.

\subsubsection{GraspNet/SuctionNet-1billion~\cite{fang2020graspnet,Cao2021suction}} Provided are $97k$ RGB-D images in $190$ cluttered scenes, with $10$ of $83$ objects per scene.
The dataset is split in $100$ scenes for training and $90$ for testing.
The test set is split in $30$ scenes of known instance, $30$ scenes of known categories, and $30$ scenes of completely novel objects.
These datasets provide $6D$ poses and grasp points for all objects.
Connected challenges are multi-object learning, due to the high number of object classes, and estimating poses of novel objects~\cite{gou2022unseen}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% current problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ongoing Research Problems}
\label{sec:trends}

This section presents the important current research problems listed in Table~\ref{tab:topics} in detail.
Recent works that fundamentally contribute to solving the respective problems are presented and evaluated for open research question.  
Note that the order of presentation does not follow the order of frequencies of the respective motivating topics, in order to provide a more natural reasoning through the grouping of semantically close open problems.

%%%%%%%%%%%%%%%%%%%%%% Domain shift
\subsubsection{Domain Shift}

\begin{comment}
definitely Cat A: works very well and is largely solved, overlooking overfitting to the dataset
\end{comment}

Overcoming the domain shift between training and test data is an ever-present topic in machine learning. This problem is even more relevant in applied robotics, where gathering real data is expensive (e.g., due to robot operation or human demonstration) and the generation of synthetic training data thus is a compelling alternative.
A lot of progress has been made with respect to monocular object pose estimation~\cite{li2020robust,thalhammer2021pyrapose,zhang2021keypoint,hu2022perspective,nguyen2022templates,wang2020self6d,shi2021fastUQ,lu2022slam,ikeda2022sim2real}.
%The most recent edition of the BOP indicates that performance under domain shift is strongly correlated with the choice of training data, data augmentation, network depth and multi-task training.

Table~\ref{tab:bop_rgb} summarizes the Average Recall (\textit{AR}) for pose estimation, and the mean Average Precision (\textit{mAP}) for object detection, comparing methods that only use data from Physically-based Rendering (PBR) for training and those that use a combination including real data.
The question arises if the difference in pose estimation performance when training on a combination of PBR and real data is insignificant and the observed performance improvements actually occur solely because more training data is used.
For TUD-L, the performance gains are higher since more than eight times as many real samples are available per object.
For YCB-V, even more improvement is shown, which correlates with the increased amount of training images of more than $18$ times as many per object class.
Interestingly, the increase in detection performance is similar to that of pose estimation. 
The improvement in pose estimation correlates with the increased amount of data and is also partially attributable to better detection rates. % 


\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
 Method & Data & T-LESS & TUD-L & YCB-V \\ \hline
 \multicolumn{5}{|c|}{Pose Estimation} \\ \hline
 GDRNet++ & PBR & 79.6 & 75.2 & 71.3 \\ 
 GDRNet++ & PBR+real & 78.6 & 83.1 & 82.5 \\ 
\hline
 \multicolumn{2}{|c|}{Performance difference} & -0.1\% & +9.5\% & +13.6\%  \\ \hline
 PFA & PBR & 71.9 & 73.2 & 64.8 \\
 PFA & PBR+real & 77.8 & 83.9 & 80.6 \\\hline
 \multicolumn{2}{|c|}{Relative performance change} & +7.6\% & +12.8\% & +19.6\%  \\ \hline
  \multicolumn{5}{|c|}{Detection} \\ \hline
 GDRNet++ & PBR & 86.5 & 72.8 & 78.6 \\
 GDRNet++ & PBR+real & 87.6 & 89.5 & 85.2 \\ \hline
  \multicolumn{2}{|c|}{Performance difference} & +1.3\% & +18.7\% & +7.7\%  \\ \hline
 PFA & PBR & 73.4 & 66.3 & 73.5 \\
 PFA & PBR+real & 79.8 & 86.6 & 85.0 \\\hline
   \multicolumn{2}{|c|}{Performance difference} & +8.0\% & +11.9\% & +13.5\%  \\ \hline
\end{tabular}
\caption{\textbf{BOP-Challenge results} Comparison of selected results with respect to used training data, taken from~\cite{hodan2022bop}. The Average Recall (\textit{AR}) is reported for pose estimation and the mean Average Precision (\textit{mAP}) is reported for object detection.} 
\label{tab:bop_rgb}
\end{table}

Table~\ref{tab:bop_paper} reports results of the BOP \cite{hodan2022bop} winning method GDRNet++ in comparison to the results reported in the supplementary material of GDRNet~\cite{wang2021gdr}. 
The performance difference is achieved by using stronger domain randomization, using ConvNeXT~\cite{liu2022convnet} as backbone instead of ResNet-34~\cite{he2016deep}, two mask heads for amodal and visible mask prediction, and by optimizing hyperparameters of the training process. 
This indicates that some performance improvement may already be gained through tuning existing methods, without providing sophisticated solutions for tackling the domain gap.


\begin{table}
\centering
\begin{tabular}{|c|c|c|}
\hline
 Method & LM-O & YCB-V \\ \hline
 GDRNet & 0.672 & 0.755 \\ 
 GDRNet++ & 0.713 & 0.825 \\\hline
\end{tabular}
\caption{\textbf{GDRnet~\cite{wang2021gdr}} Performance improvement achieved through hyperparameter tuning.}
\label{tab:bop_paper}
\end{table}

Similar behaviour is observed in the VisDA-2021 challenge~\cite{visda2021}, that provides a benchmark for domain adaptation.
Training is performed on ImageNet1k~\cite{russakovsky2015imagenet}.
Both a development set with and a test set without annotations are provided. 
The development set consists of images and annotations from ImageNet-C~\cite{hendrycks2019imagenet}, ImageNet-R~\cite{hendrycks2020imagenet} and ObjectNet~\cite{barbu2019objectnet}, and introduces novel classes. 
For the test set no annotations are available and the class distribution varies from the development set.
It is allowed to use the development set for hyperparameter tuning, yet for submitting the final model, it is only allowed to use the test set for domain adaptation.

Table~\ref{tab:visda} reports the results of the top three methods using the classification ACCuracy (ACC) and the Area Under the Receiver Operating characteristic Curve (AUROC)~\cite{hand2001auroc}. 
The ACC quantifies the true positive rate in class prediction.
The AUROC quantifies the separation between known and unknown classes.
The two best performing methods use no domain adaptation strategies. 
The winning method achieves its performance through pretraining a transformer with ImageNet1k.
The second best method uses EfficientNetB7~\cite{tan2019efficientnet} in conjunction with extensive data augmentation and regularization.
These results indicate that tuning hyperparameters, such as data augmentations plays a predominant role for overcoming the domain shift.

\begin{table}[t!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
 Ranking & Method & ACC & AUROC \\ \hline
 1 & Tayyab\tablefootnote{https://ai.bu.edu/visda-2021/assets/pdf/Burhan\_Report.pdf} & 56.29 & 69.79 \\ \hline
 2 & Rajagopalan\tablefootnote{https://ai.bu.edu/visda-2021/assets/pdf/Chandramouli\_Report.pdf} & 48.49 & 76.86 \\ \hline
 3 & Liao~\cite{liao20212nd} & 48.49 & 70.8 \\ \hline
\end{tabular}
\caption{\textbf{VisDA-2021 challenge~\cite{visda2021}.} Results of the top three methods.}
\label{tab:visda}
\end{table}

Considering the leaderboards of the BOP and VisDA-2021 challenges, it appears that domain shift is alleviated using deeper models, extensive data augmentation, and further regularization techniques during training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Occlusion
\subsubsection{Occlusion Handling}

\begin{comment}
I'd call that Cat B: Many works are motivated by occlusion handling. They are doing quite well in the meantime and it is imo pretty much correlated with general performance improvement... and I'll eat a broom if training data diversification is not one of the main contributing factors to the large recent performance improvements here.
\end{comment}

Occlusion handling is an important challenge for object pose estimation and present in many real-world robotics scenarios. Dense clutter, a hand or gripper manipulating an object, or even an ill-selected viewpoint may result in large parts of the object of interest being occluded.
Table~\ref{tab:topics} shows that occlusion handling is one of the major challenges based on the frequency at which it is a motivating current research. 
Figure~\ref{fig:occlusionProblem} presents the performance of general purpose methods~\cite{hu2019segpose,oberweger2018making,peng2019pvnet,Park_2019_ICCV,song2020hybridpose,su2022zebrapose,zhang2021keypoint} compared to those designed for occlusion handling~\cite{hu2022perspective,iwase2021repose,tekin2018real,thalhammer2022cope,wang2020self6d,wang2021gdr,xiang2017posecnn,zakharov2019dpod}.
Evaluations on the LM~\cite{hinterstoisser2012model} and the LMO~\cite{brachmann2014learning} dataset are presented.
LM features individual test sets for each of the $13$ dataset objects with no occlusion.
LMO provides annotations for all eight LM objects that appear in the second test set, under strong occlusion.
The left plot of Figure~\ref{fig:occlusionProblem} presents the performance of diverse methods on the LMO~\cite{brachmann2014learning} dataset, plotted against the year of publication.
The performance of general purpose methods and those motivated by occlusion improved similarly over the years.
The right plot shows the correlation between the performance on LMO and LM~\cite{hinterstoisser2012model}, for general purpose methods and those motivated by occlusion handling.
Both types of methods exhibit similar performance ratios.

The presented comparisons indicate that general pose estimation performance and occlusion handling correlate, since methods which handle occlusion well perform better overall.
Consequently, occlusion handling is not a specific trait to improve, but rather one that general pose estimation converges to.
A further indicator that reinforces this observation is that recently proposed datasets (later than $2017$) almost exclusively feature images with object occlusion (see Table~\ref{tab:datasets}).
The only exceptions are TUD-L, which focuses on challenging illumination changes, and KeyPose, where the primary challenge is transparency.

The substantial progress that has lately been made for handling occlusions is to be attributed to different influencing factors. 
Improvements with respect to data rendering not only reduced the domain shift between the rendered and the real domain, but also improved occlusion handling.
The major advantage of using rendered training data is that the distribution of aspects such as occluding patterns, illumination, and viewpoints alike, are arbitrarily diverse and thus reduce biases of trained estimators~\cite{homebrewedDB}.
Occlusion handling improvements are furthermore attributable to designing approaches that incorporating probabilities from local hypotheses~\cite{peng2019pvnet}, strategies for deriving poses from multiple pose representations~\cite{song2020hybridpose}, and to reasoning about self-occlusion.
However, while works exist that systematically analyse the influence of pose continuity~\cite{zhou2019continuity} and the strategies for symmetry handling~\cite{richter2021handling}, occlusion handling research lacks comparable thoroughness.
Detailed systematic investigations of the influence of specific occlusion patterns, and the visibility of specific object parts, on the pose estimation accuracy are missing.
Yet, such investigations are required to design approaches that exhibit robust occlusion handling, and to provide reliable uncertainty estimates.  

% Figure environment removed

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Representation
\subsubsection{Pose representations}
\label{sec:poserep}

\begin{comment}
Cat B: works quite well, but it is probably open ended. Many things to do here. Should prob. be located closely to end-to-end, multi-object and also deformable objects?
\end{comment}

Early deep learning works in pose estimation identified performance improvements when using keypoints as regression target instead of directly regressing the $6D$ pose~\cite{tekin2018real,crivellaro2015novel,rad2017bb8,xiang2017posecnn}.
Such geometric correspondences are nowadays the most commonly used surrogate training targets for representing $6D$ object poses~\cite{di2021so,haugaard2021surfemb,hodan2020epos,huang2022neural,hu2022perspective,su2022zebrapose,thalhammer2022cope,liu2021kdfnet}.
The $6D$ pose is derived by registering the estimated $2D$ correspondences to the corresponding ground-truth $3D$ ones.
Algorithms for such $2D$-$3D$ correspondence registration are either classical ones, like the Perspective-\textit{n}-Points (P\textit{n}P) algorithm~\cite{lepetit2009epnp}, or learned functions~\cite{chen2022epro,hu2020single,thalhammer2022cope,wang2021gdr}. 

The most commonly used surrogate training targets are either dense uv-coordinates~\cite{li2019cdpn,Park_2019_ICCV,zakharov2019dpod,huang2022neural,di2021so,liu2021kdfnet,hodan2020epos,haugaard2021surfemb,hu2022perspective} or sparse keypoints~\cite{crivellaro2015novel,rad2017bb8,peng2019pvnet,hu2019segpose,thalhammer2021pyrapose,liu2021kdfnet,zhang2021keypoint}.
%Deep learning-based pose estimation uses keypoints as sparse geometric correspondences to improve pose estimation accuracy for several years now~\cite{crivellaro2015novel}.
For the sparse keypoints, improvements are proposed by assigning keypoints to geometrically relevant positions on the object surface and through more sophisticated keypoint location voting schemes~\cite{crivellaro2015novel,peng2019pvnet,liu2021kdfnet}.
Keypoints are also advantageous for guiding domain adaptation through aligning keypoint distributions across different domains~\cite{zhang2021keypoint}. 
Uv-coordinates are used as dense geometric correspondences for pose estimation~\cite{Park_2019_ICCV,li2019cdpn}.
It has been demonstrated that they are more versatile, e.g. with respect to symmetry handling~\cite{hodan2020epos}. 
Due to the dense prediction space, occlusion handling is improved~\cite{di2021so} and object symmetries can be learned in self-supervised ways~\cite{hodan2020epos,haugaard2021surfemb}.
Recently,~\cite{su2022zebrapose} proposed pixel-wise regression of binary patterns assigned to multiple hierarchical vertex groups.
These hierarchical dense geometric correspondences lead to improved occlusion handling compared to their alternatives.
Figure~\ref{fig:gcs} presents visual examples of the aforementioned types of geometric correspondences.

% Figure environment removed

\footnotetext{Courtesy of~\cite{su2022zebrapose}}

Currently proposed pose representations allow to improve occlusion and symmetry handling, domain adaptation and pose estimation accuracy in general. We conjecture that investigating novel representations to be a viable direction to significantly improve performance in both existing and currently unsolved tasks.
Future directions (see Section~\ref{sec:future}) will be providing improved solutions for category-level pose estimation, challenging material properties, and deformable and articulated objects.
As an alternative, learning principles that do not explicitly use $6D$ poses or $2D$-$3D$ correspondences as regression targets is gaining momentum~\cite{nguyen2022templates,wang2020self6d,Chen_2023_CVPR,guo2023knowledge,thalhammer2023self}.
Self-supervision possesses the potential to improve over manually designed pose representations, since it allows learning semantic correspondences between different objects and domains~\cite{florence2018dense}.
Recently it has been shown that directly deriving $6D$ poses from geometric correspondences in an end-to-end trainable manner improves pose estimation accuracy over only regressing geometric correspondences and deriving the pose using P\textit{n}P~\cite{hu2020single}.

\subsubsection{Multi-object and End-to-end Training}
\label{sec:e2e}

Handling multiple different objects is important because robotic scenarios, like assistant tasks in households, feature a multitude of objects with diverse properties.
The concept of multi-object pose estimation is strongly interleaved with end-to-end trainability, yet a clear differentiation is missing.
For the sake of clarification, we hereby break down these concepts.
By relating them to the respective concepts for object detection highlights the differences, and thus, uncovers potential challenges and aspects for improving pose estimation.

The state of the art for object detection groups algorithms into single-staged~\cite{lin2017focal,bochkovskiy2020yolov4,ge2021yolox,tian_fcos} and multi-staged ones~\cite{he2017mask,zhou2021probabilistic}.
The difference between these two types is depending on the usage of intermediate object proposals.
Single-staged object detectors extract features and predict attributes such as object class and bounding boxes directly.
Multi-staged approaches extract features, create a set of object proposals, which are in turn used for predicting the desired attributes.
Object proposals often already contain class, location and scale information~\cite{zhou2021probabilistic}.
Both types of approaches are single networks that handle a) multiple object classes and b) multiple instances simultaneously, and c) provide the desired object attributes directly, without requiring additional separately trained or executed stages.
The predicted object attributes are $2D$ bounding box coordinates for object detection.

Object pose estimation research treats these concepts differently.
Single-stage refers to detecting and estimating poses in the same end-to-end trainable stage~\cite{hu2020single,thalhammer2022cope,lin2022single}. 
Yet, this does not necessarily mean that multiple instances of the same object are handled~\cite{hu2020single}.
End-to-end trainability in object pose estimation research refers to directly estimating the $6D$ pose and backpropagating the respective loss for the pose to layers that learn features, or regress geometric correspondences~\cite{di2021so,thalhammer2022cope,wang2021gdr}.
Yet, this end-to-end trainability is not interchangeable with the concept of single-stage approaches, as is the case for object detection.
Such approaches are multi-staged, requiring a detector for estimating sparse location priors~\cite{di2021so,wang2021gdr}.
The following paragraphs presents those two concepts, multi-object and end-to-end trainable pose estimation, in detail and presents the connected challenges.

\textbf{Multi-object Approaches:}
The main problems connected to multi-object learning is that imbalances in the training data need to be handled to reduce the network bias.
These biases reduce estimation accuracy of objects underrepresented in the training data and typically are training samples per object, different object scales and different convergence behaviour per object, caused by aspects such as texture, geometry and material properties. 
Yet, the advantages of multi-object training are abundant. 
Recent approaches present advantages with respect to runtime, scalability, memory footprint and general applicability~\cite{gard2022casapose,hodan2020epos,thalhammer2022cope,thalhammer2021pyrapose,zakharov2019dpod}.
Despite these obvious advantages, the performance trails behind single-object approaches, which train separate pose estimators per object.
%Bridging that performance gap of such single-model to their multi-model counterparts is a challenging task.

\textbf{End-to-end Trainable Approaches:}
Recently, advances have been made by additionally supervising learning with the direct $6D$ pose as downstream learning task.
As such, geometric correspondences are regressed as an intermediate representation and the $6D$ pose is inferred from them.
Training in such an end-to-end manner improves performance over regressing the 6D pose directly, or using P\textit{n}P and alike~\cite{hu2020single,wang2021gdr,song2020hybridpose,thalhammer2022cope,iwase2021repose,park2022dprost,cao2022dgecn}.

Direct pose regression also provides more diverse means of supervision~\cite{di2021so,thalhammer2022cope}.
By using the estimated $6D$ pose for re-projecting the geometric correspondences, keypoints~\cite{thalhammer2022cope} or uv-coordinates and self-occlusion maps~\cite{di2021so}, training is additionally guided in $3D$ space.
End-to-end training of pose estimators provides means to enforce consistency with other tasks that are learned in parallel.
As such encoded representations are more general and do improve through multi-task learning~\cite{lopes2023cross}.
%While directly deriving $6D$ poses from intermediate representations improves initial pose estimation accuracy, applying refinement steps is currently required to maximize it. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Refinement
\subsubsection{Refinement}

\begin{comment}
which Cat?: what are open challenges here? Dominik's got a plan?
I would put this in Cat B - a lot of work already, many variations proposed - but still some limitations (runtime, requiring a 3D model, etc) for practical use in robotics
\end{comment}

Classical multi-stage depth-based approaches~\cite{hinterstoisser2012model,drost2010model,vidal20186d,hodavn2015detection,aldoma2011cad} refine estimates using ICP~\cite{rusinkiewicz2001efficient}.
Also methods that estimate poses in a monocular fashion~\cite{kehl2017ssd,Park_2019_ICCV,labbe2020cosypose,xiang2017posecnn} exploit additional depth information for ICP-based refinement.
To be able to rely only on monocular images thus requires alternative solutions for pose refinement.
Some early learning-based works use contour-based~\cite{kehl2017ssd,manhardt2018deep} or keypoint-based refinement~\cite{rad2017bb8}.
A seminal work that has been adopted and modified often is presented in~\cite{li2018deepim}.
An input image pair, the observation and a rendered estimate, is processed by a network to predict their relative transformation.
Many recent works build on this iterative refinement approach~\cite{labbe2020cosypose,hu2022perspective,lipson2022coupled,zakharov2019dpod,iwase2021repose,haugaard2021surfemb,araki2021iterative,yen2021inerf}.
Recent works propose to replace iterative refinement by parallel hypotheses scoring and matching against templates~\cite{hu2022perspective,he2022generative}.
Yet, these methods still benefit from using an additional iterative refinement stage~\cite{hodan2022bop}.
A common aspect of these approach is their render-and-compare nature, creating a dependence on (textured) 3D object models which is not easily address for, e.g., novel objects. Furthermore, these approaches need to be able to render the object of interest with reasonable realism. However, for complex materials found in highly reflective or refractive objects, rendering may be prohibitively compute intensive. Also, this typically relies on material properties that are not readily available for existing models and are difficult to acquire for novel objects. 


This is an especially limiting requirement as, lately, a considerable shift is happening towards pose estimation for objects with more challenging material properties like metallic and transparent materials~\cite{liu2020keypose,wang2022phocal}.
Environmental properties, like illumination and light direction, and scene background have to be inferred in order to synthesize templates accurately enough to precisely match observations against them.

Generally, the question of the need for pose refinement arises since single-shot pose estimation improves rapidly, and use cases such as object grasping tolerate certain pose inaccuracies.
%Augmented reality scenarios, where the $2D$ projection of objects is the desired task, are often undemanding in the accuracy of the estimated camera distance. This can also be seen when comparing accuracies using the ADD/S~\cite{hinterstoisser2012model} and the $2D$ projection~\cite{rad2017bb8} metrics.
%The $2D$ Projection metric accuracy saturates with significantly less accurate poses than the $3D$ ADD/S metric~\cite{brachmann2014learning,rad2017bb8,peng2019pvnet}.
Similarly, robotic grippers are designed to tolerate grasp pose inaccuracies and as such often execute successfully, even when the estimated pose is visually significantly off.
Yet, since robotic scenarios like %hand-overs and 
affordance-based grasping requires higher accuracy, we conjecture that pose refinement will remain relevant in the near future.

%%%%%%%%%%%%%%%%%%%%%%%%%% Symmetry
\subsubsection{Symmetry Handling}

\begin{comment}
Cat A: there are some really nice solutions, GDRNet, Ambiguity-aware.... However, what is definitely missing is some well designed approach for objects that are beyond datasets.
\end{comment}

Object symmetries are comparably easy to handle for classical template-based approaches.
Templates are rendered in a viewing sphere around the object model. Views are encoded into descriptors to create a look-up table of the training samples~\cite{drost2010model,hodavn2015detection,aldoma2011cad,hinterstoisser2012model}.
Such template matching approaches do not require learning a representation or regressing a pose.
Matched templates with ambiguous views but incorrect poses are thus handled by common metrics by computing point-pair distances in $3D$ space~\cite{hinterstoisser2012model,rad2017bb8,hodan2020bop}.
Yet, incorporating information about symmetries still helps reducing the memory footprint and runtime~\cite{alexandrov2019leveraging}.

Deep learning approaches, in contrast, encode representations of the training data.
Approaches based on supervised learning, which are the most common ones, require a the $6D$ pose or other pose representations for backpropagation during training.
This representation is often not invariant with respect to visual object ambiguities.
Deep learning approaches thus suffer from pose estimation accuracy reduction when such visual ambiguities are not handled properly at training time~\cite{rad2017bb8,sundermeyer2018implicit,xiang2017posecnn}.

% Figure environment removed

\footnotetext{Courtesy of~\cite{hodan2020epos}}

Common strategies to tackle this problem are limiting the range of views seen during training~\cite{rad2017bb8,sundermeyer2018implicit},
using loss functions that account for ambiguous views~\cite{xiang2017posecnn,Park_2019_ICCV,thalhammer2022cope}, and designing network structures to provide multiple hypotheses for disambiguation~\cite{manhardt2019explaining,shi2021stablepose}. 
For these approaches to work, oftentimes symmetries have to be known beforehand which limits their applicability and generality.  
Furthermore, manually assigning symmetries is also a potential source of error and standard geometric correspondence formulations are often insufficient~\cite{richter2021handling}.
When using uv-coordinates as regression targets, sophisticated solutions for solving object symmetries implicitly during training exist.
Figure~\ref{fig:uv-sym} shows a visualization of the approaches in ~\cite{hodan2020epos}, which learns symmetries by predicting ambiguous surface regions, and~\cite{haugaard2021surfemb}, which encodes geometric representations invariant to symmetries. An open problem is finding such solutions for keypoint-based approaches, whose sparseness is beneficial in terms of runtime and scalability as compared to uv-coordinate approaches.
In general, diverse strategies exist for handling symmetrical objects and modern approaches experience minor drops in pose estimation accuracy when faced with them.

%%%%%%%%%%%%%%%%%%%%%%%% Category-level
\subsubsection{Category-level Training}

\begin{comment}
Cat C: RGB lacks way behind depth-based approaches
\end{comment}

Instance-level object pose estimation approaches aim at retrieving poses of known object instances. By comparison, category-level pose estimation aims at generalizing to unknown object instances of the same category~\cite{he2022fs6d,chen2020category,lin2022single,ma2022robust,fan2022object,wen2022disp6d,Lee2021category,remus2023i2c-net,deng2022icaps}.
Common principles are encoding canonical or category-specific features~\cite{lin2022single,chen2020category,fan2022object,wen2022disp6d} and render-and-compare~\cite{ma2022robust}% and fast adaptation~\cite{he2022fs6d}.
In~\cite{chen2020category}, category-specific features are used to reconstruct object views conditional on a pose% with a variational Auto-encoder. 
During inference, the network is iteratively optimized for pose and shape.
Alternatively, the authors of~\cite{Lee2021category} retrieve object poses by aligning predicted depth with object coordinates.
Similarly, in~\cite{lin2022single} the authors estimate the $6D$ pose by aligning predicted object depth and coordinate from an extracted shape prior.
In~\cite{wen2022disp6d}, latent representations are compared to codebook encodings for retrieving the object pose. 
The authors of~\cite{ma2022robust} employ a contrastive learning framework, from which object proposals are generated and compared against rendered templates, for retrieving the $6D$ pose.

Table~\ref{tab:category} compares approaches based on the input image modality on CAMERA and REAL~\cite{wang2019normalized}.
The pose and keypoint estimation accuracy for approaches with RGB-D input increases significantly as compared to RGB-only approaches. 
An open challenge for monocular approaches is thus to close this huge performance gap.

\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 Method & Year & Input & \multicolumn{2}{|c|}{CAMERA} & \multicolumn{2}{|c|}{REAL} \\ 
 \cline{4-7}
 & & & $3D_{50}$ & 10°/cm & $3D_{50}$ & 10°/cm \\\hline
 NOCS\cite{wang2019normalized} & 2019 & RGB\textbf{D} & 85.3 & 62.2.8 & 80.5 & 26.7 \\
Remus\cite{remus2023i2c-net} & 2023 & RGB\textbf{D} & - & - & 92.5 & 67.1 \\\hline
 Chen\cite{chen2020category} & 2020 & RGB & - & - & - & 4.8 \\
 Lee\cite{Lee2021category} & 2021 & RGB & 32.4 & 19.2 & 23.4 & 9.6 \\
 %Deng\cite{deng2022icaps} & 2022 & RGBD
 Fan\cite{fan2022object} & 2022 & RGB & 32.1 & 23.4 & 25.4 & 9.8 \\\hline
\end{tabular}
\caption{\textbf{Category-level Pose Estimation} Comparison of RGB and RGB-D input on the CAMERA and REAL datasets~\cite{wang2019normalized}.}
\label{tab:category}
\end{table}

Inspecting category-level pose estimation from a broader perspective, there are gaps that need to be filled in order to reap its full potential for unstructured open-world scenarios, where it is impractical to employ instance-level pose estimators for all objects.
The intra-category variation of the standard datasets used for designing and testing approaches is narrow.
Figure~\ref{fig:category_viz} shows the individual instances of the known categories of DREDS, and the category \textit{camera} of NOCS.
The categories of DREDS and NOCS are very similar and exhibit significant overlap.
Four of NOCS categories: \textit{bottle}, \textit{bowl}, \textit{camera}, \textit{can}, \textit{laptop}, and \textit{mug}, also appear in DREDS.
Both datasets have little intra-category variation, while having large inter-category variation.

Important questions to answer are: Which variation of instances is required to effectively extrapolate to unknown instances of a category? 
How to handle objects that lie at the intersections of two similar categories? 
Which pose to retrieve in such cases? 
And how to circumvent these cases in the real world?
In order to answer these questions, formal rules for grouping object instances into categories are required. 
Future work thus has to investigate taxonomies of category-level pose estimation.
Solving this problem potentially requires algorithms that establish this taxonomy based on what instance-related features can be learned and as such category clustering itself.
Ideally, these also define the object origins to circumvent algorithms to interpolate the estimated pose between two categories with obviously diverging origins.

% Figure environment removed

\footnotetext{Courtesy of~\cite{wang2019normalized} and~\cite{dai2022dreds}}

An interesting aspect of category-level approaches is that they generalize beyond known objects, within known categories, when compared to instance-level ones, yet still exhibit fast inference.
Alternatives, if fast inference is not a requirement, are novel object pose estimation approaches.
%This type of methods eliminates the problem of defining categories by relying on template matching and thus even generalize to unknown instances and categories~\cite{liu2022gen6d,nguyen2022templates}. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% material properties
\subsubsection{Novel Objects}

\begin{comment}
Cat C: there are some nice approaches, foundational learning is tackled. Yet, there are no strict guidelines on which training data to use and how to include novel objects (tempaltes, annotations of real-samples)
\end{comment}

Instance- and category-level object pose estimation do not generalize to unknown object categories.
Novel object pose estimation aims to solve this task and doing so from monocular images is an emerging topic that already gained some momentum
~\cite{gu2022ossid,li2018deepim,shugurov2022osop,nguyen2022templates,Sun_2022_onepose,goodwin2022zero,thalhammer2023self,okorn2021zephyr}.
Common strategies are using support views~\cite{liu2022gen6d,fan2023pope}, template-matching~\cite{nguyen2022templates,shugurov2022osop} from renderings, and foundational models~\cite{goodwin2022zero,thalhammer2023self}. 

Approaches retrieving the object pose using support views require labelled views around the object~\cite{liu2022gen6d,Sun_2022_onepose}.
These approaches assume the availability of real, but less, views as compared to approaches that use rendered templates.
Recently,~\cite{fan2023pope} relaxed these assumptions, only requiring a single image without annotation for retrieving a relative pose.
While the generality and the requirement of only one support view allows easy applicability, the pose estimation accuracy lags behind that of rendered template-based approaches, such as~\cite{nguyen2022templates,shugurov2022osop,thalhammer2023self}.
Such approaches match the query image against uniformly sampled views of the object to retrieve the query's pose.
This is done by computing the mutual similarities of the features extracted using fine-tuned CNNs.
An alternative is to use foundational models such as Vision Transformer (ViTs)~\cite{dosovitskiy2020image} pre-trained on ImageNet1k~\cite{goodwin2022zero} in a self-supervised manner~\cite{goodwin2022zero,thalhammer2023self,fan2023pope}.
Using pre-trained ViTs is promising to generalize to arbitrary objects, and circumvent the requirement for fine-tuning. 

Novel object pose estimation has great potential for example for assistance robots that encounter a vast number of objects over the course of their deployment. Using these approaches, estimating poses of such objects that are unknown during training does not require re-training the pose estimator~\cite{labbe2022megapose}.
The challenge of deep template matching for novel object pose estimation is also identified as an important task in the next edition of the BOP-challenge.
There is still much space for improvements with respect to accuracy when using RGB and efficiency, i.e. object properties, which are limited to opaque objects at the moment, and runtime, which is depending on the number of templates that are compared against the query.
As such it is expected that future research will solve these issues by exploring alternative ways to retrieve poses of novel objects without requiring rendered templates.
Furthermore, considering the current solutions and their limitations with respect to the object used, bridging novel object pose estimation to robotics, especially mobile robotics, will be challenging and important.
Considering unstructured open-world scenarios, novel object pose estimators need to deal with strong illumination changes, different cameras, differences between the available object models for creating templates and the query object, and different challenging material properties such as specularity and transparency.
Especially, the problem of handling challenging object material, is still largely unsolved even for monocular instance-level pose estimation.
Substantial progress has to be made to transfer the high pose estimation accuracy on opaque objects to novel objects with challenging material properties.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% material properties
\subsubsection{Challenging Material Properties}
\label{sec:obj_material}

\begin{comment}
Cat C: largely untouched, though there are some approaches for metallic objects, with little to no exhaustive evaluation against the sota for these objects. Some recent approaches for transparent objects, but practically none RGB and monocular, except TGFnet and that does not provide comparison to sota beyond their GDRnet baseline.
\end{comment}

Most of the state of the art research primarily focuses on a limited range of object surface material properties. 
Considering the BOP, six of the seven core datasets feature fully opaque and diffusely reflecting object surfaces~\cite{brachmann2014learning,homebrewedDB,hodan2017tless,doumanoglou2016recovering,xiang2017posecnn,hodan2018bop,drost2017introducing}. 
Four of those datasets contain primarily textureless objects~\cite{brachmann2014learning,homebrewedDB,hodan2017tless,drost2017introducing}.
Only one of these seven datasets includes specular, metallic object surfaces~\cite{drost2017introducing}.

% Figure environment removed

Metallic surfaces are challenging due to their specular reflectivity, which leads to higher variation in appearance depending on the incoming light and camera position, visualized by the left images of Figure~\ref{fig:met_trans}.
Especially for industrial application, handling metallic objects is of great importance.
Yet, solutions for handling them are comparably scarce as compared to diffusely reflective objects~\cite{he2022generative,drost2017introducing}.
The authors of~\cite{drost2017introducing} propose a metallic object dataset and solve pose estimation using classical methods on grayscale images.
In~\cite{he2022generative}, a non-publicly available dataset is used for pose estimation from RGB images of specular reflective objects in clutter.
Their solution is to match query images against templates, using geometric edge representations as input.
While this method provides a feasible solution for metallic object pose estimation and grasping, an empirical analysis of how much accuracy improvement is to be expected when using edge representation, and not RGB images, as input is currently missing. 
This may help to identify the specific scene propreties and situations where RGB input leads pose estimation accuracy to drop.
Having this information would on the one hand help to robustify algorithms against image artefacts occurring due to specular reflectivity of the object, and on the other hand, it would facilitate the design of benchmarking datasets that feature this specific challenges, deviating less from realistic scenarios.

Even more challenging are transparent object surfaces.
Instead of showing the texture of the object itself, RGB observations may only show a refracted view of the background that is depending on the transparent object, visualized by the right images of Figure~\ref{fig:met_trans}. Additionally, many transparent objects are also specularly reflective.
Especially for robotics, transparent objects are of great importance because they are frequent in unconstrained real-life environments, for example, glassware or food containers.
Handling such non-opaque objects gained considerable momentum in computer vision~\cite{zhang2022transnet,chen2022clearpose,liu2020keypose,ichnowski2021dex,chen2022stereopose}.
Presented approaches for retrieving object poses rely on reconstructing depth data or on multi-view registration~\cite{chen2022clearpose,liu2020keypose,chen2022stereopose}.
Similar to the approach of~\cite{he2022generative} that showed the usefulness of edge images as input for metallic object pose estimation, it has been shown that such edge representations also improve pose estimation of transparent objects~\cite{yu2023TGFnet}. 
Concurrent research showed that, alternatively, extensive randomized synthetic data helps algorithms to encode meaningful visual cues for transparent object pose estimation~\cite{byambaa20226d}.
For future work, thorough investigation of the strengths and weaknesses of RGB and depth data for transparency handling is advisable, and will be crucial when considering the requirements of vision systems for robotics.

Finding viable solutions for the problem of challenging material properties requires bridging computer vision, concerned with analysis, and computer graphics, concerned with synthesis.
Approaches at this intersection do not only aim at analyzing data, but also to encode physically-based functions for synthesizing such data~\cite{ichnowski2021dex}. 
This requires exploring diverse learning principles and algorithm designs at that intersection, such as Neural Radiance Fields~\cite{mildenhall2021nerf,li2022nerf,ho2020denoising}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% learning principle
%\subsection{Learning Principles}
\subsubsection{Beyond Supervised Learning}

\begin{comment}
Cat C: I'm not sure that it should be a separate section, yet it is very much needed for novel object pose estimation. Either combine these two, or cluster them.
\end{comment}

Due to the complexity of deriving $6D$ poses from $2D$ input, object pose estimation is traditionally approached as a supervised learning task.
Yet, alternative training principles expose a number of advantages which also have been applied to pose estimation.

Self-supervised learning improves pose estimation accuracy, when data without annotations are available in the target domain~\cite{wang2020self6d,Chen_2023_CVPR,wang2021self6d++,gu2022ossid}.
The authors of~\cite{wang2020self6d,wang2021self6d++} show performance improvements using a limited number of real images without annotations, and differentiable rendering.
The approach of~\cite{gu2022ossid} enables pose accuracy improvement for novel objects using an online self-supervised learning scheme.
Using images of the object in the target domain and a coarse pose,~\cite{Chen_2023_CVPR} 
improves pose estimation while simultaneously improving the texture quality of the object meshes.

Contrastive learning not only shows great potential for novel object pose estimation in RGB~\cite{shugurov2022osop,nguyen2022templates,labbe2022megapose}, but also for knowledge distillation from large to small networks~\cite{guo2023knowledge}.
Such learning schemes, in combination with ViTs, are particularly interesting since they are general feature extractors and as such provide robust image-to-image correspondences without fine-tuning~\cite{caron2021emerging,thalhammer2023self,goodwin2022zero}.

Reinforcement learning (RL) approaches in pose refinement allow to limit the data annotation effort by exploiting segmentation masks as proxy for pose annotations~\cite{shao2020pfrl}, or by defining an expert policy that annotates new samples as the agent explores the pose space during training~\cite{bauer2021reagent}. Also, in the framework of RL, utilizing a replay buffer of refinement sequences allows to train with a large number of refinement steps. While end-to-end supervised refinement \cite{wang2019densefusion} needs to retain information about all steps for backpropagation and is hence memory limited, in RL temporal consistency is enforced by return computation. This allows to train on smaller batches from the replay buffer. Another benefit of the RL formulation is that any additional learning objective that is not differentiable may be incorporated in the reward~\cite{bauer2022sporeagent}. To improve the convergence of RL-based methods, they are often combined with Imitation Learning in the form of Behavioral Cloning~\cite{shao2020pfrl,bauer2021reagent,bauer2022sporeagent}.

Researching non-supervised learning principles encourages alternative research avenues for tackling, for example, domain adaptation, few-shot learning, overcoming the requirement for annotations, and knowledge distillation.
The success of self-supervised learning with vision transformers~\cite{caron2021emerging}, and in particular their feature encoding, also suggests possible performance improvements.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% uncertainty estimation
\subsection{Uncertainty Estimation}

Scores for estimating uncertainty are essential for virtually all tasks in computer vision, from detection scores~\cite{tian_fcos}, over descriptor similarities~\cite{sundermeyer2018implicit}, to inlier ratings~\cite{fischler1981random,lepetit2009epnp}.
Pose estimation approaches generally benefit from providing many hypotheses that are ranked by a confidence score.
On the one hand, evaluation metrics are primarily sensitive to accuracy~\cite{hodan2020bop}, and on the other hand robotic scenarios benefit from pruning hypotheses in order to retrieve the one with the highest confidence.
Uncertainty estimation allows to select the best object pose for use in down-stream tasks. When all pose hypotheses are uncertain, for example, it may be better for the robot to change its viewpoint rather than attempting a potentially unsuccessful manipulation. The notion of uncertainty may also be used to explain the robot's behavior in HRI scenarios. Finally, we may opt to spend additional computation time on refining the most promising hypotheses when the task allows it.
Ideally, confidences are retrieved from hypotheses distributions that describe actual probabilities.

Uncertainties are used in different ways to improve pose estimation accuracy.
In~\cite{peng2019pvnet}, a modified P\textit{n}P is proposed to incorporate the $2D$ distribution of predicted keypoint locations.
The authors of~\cite{manhardt2019explaining} identify an object's axis of ambiguity by analyzing the pose hypotheses distribution. 
In~\cite{deng2021pose}, object ambiguities are identified through deriving rotation estimation uncertainties from consecutive frames. 
The authors of~\cite{hu2019segpose} and~\cite{thalhammer2022cope} aim at deriving confidences per hypotheses, in order to retrieve a pose averaged over hypotheses with high accuracy.
Uncertainty estimation is achieved by quantifying the disagreement over an ensemble of pose estimators in~\cite{shi2021fastUQ}.
The authors of~\cite{huang2022confidence} learn to predict keypoint confidences in an unsupervised fashion.
In~\cite{jeon2023ambiguity}, object ambiguities are leveraged to derive uncertainties for keypoint selection.
Recently,~\cite{Yang_2023_CVPR} proposed a framework for estimating uncertainties from keypoint heatmaps that correlate with the estimation deviation from the ground truth keypoint locations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% uncertainty estimation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Challenges}
\label{sec:future}

The previous section presented popular ongoing research topics.
In the following, we identify gaps in the existing problem landscape based on this overview.
By accumulating these missing problems, we are able to derive high-level challenges that need to be addressed to effectively advance the state of the art.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% object ontologies
\subsection{Object Ontology}

% Figure environment removed

A topic largely untouched and completely unsolved is how to estimate poses of objects which are excluded from the known categories and for which no prior information like CAD-models or training images are available.
Novel and category-level pose estimation assumes prior information about the object origin, which is used for prediction making.
Again, coming back to the open-world scenario, generalizing to out-of-distribution objects poses a number of problems.
How to incorporate the pose of truly unknown objects into vision pipelines, if no prior information is available?
Also, especially category-level approaches interpolate between training samples of known categories. What if unknown objects lie at the intersection of two categories. 
Consider the left image of Figure~\ref{fig:ontology}, The shape of the cup is similar to the instances of DREDS's category \textit{bowl}, yet has a handle like the instances of the category \textit{cup}, see Figure~\ref{fig:category_viz}.
While \textit{bowl} has to be treated as rotationally symmetric since missing cues for disambiguation, \textit{cup} is not symmetric.  
How to handle such scenarios and prevent interpolating between the two, or possibly more, object origins of categories that are matched?
Another related issue are objects that are parts of others.
An example is visualized in the right image of Figure~\ref{fig:ontology}. 
TLESS object $6$ is a part of object $7$. 
A case which is difficult to resolve if the ontology of objects is unknown.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% object ontologies
\subsection{Deformable and Articulated Objects}
\label{sec:obj_deform}

Most common methods and datasets for monocular pose estimation methods rely on the assumption of rigid objects, where the shape remains fixed and only the position and orientation changes.
Pose estimation of deformable and articulated objects, however, presents significant challenges due to their inherent variability in shape.
Articulated objects pose may be treated as individual object parts that are connected by joints, hence only introducing a limited number of additional degrees of freedom~\cite{EisnerZhang2022FLOW} as compared to rigid objects.
The method in~\cite{liu2022toward} assumes a finite set of joints and uses NOCS~\cite{wang2019normalized} for local geometric correspondence prediction on each part.
While this presents an intuitive solution for non-rigid objects with only a few degrees of freedom, transferring such a strategies to fully deformable objects poses seems infeasible due to their shape's infinite degrees of freedom. The partitioning of the object into sub-entities typically degenerates into dense prediction of its surface or volume.
As a consequence, approaches for handling deformable objects are commonly task specific and may involve robotic manipulation to incrementally learn the morphology of the object's shape~\cite{corl2020softgym,chi2021garmentnets}.
Yet, handling these types of objects is of particular interest, since they are common in everyday life, e.g. bags and textiles, and thus are important for robotics. % and augmented reality.


Objects that undergo smaller scale deformations may be handled by estimating local correspondences~\cite{florence2018dense,goodwin2022zero}.
Such approaches allow to treat such objects within the existing pose estimation framework by estimating dense geometric correspondences from the local neighborhood. 
Alternatively, with robotics-based approaches for textile handling in mind, reasoning about an object's deformability might require solving tasks like learning semantically meaningful parts~\cite{chen2023autobag}.
Apart from learning dense displacement from a rest shape, there is no clear common definition of a deformable object's ``pose'' and origin. Therefore different metrics are used for evaluation in related works, complicating comparison.
Evaluating on proxy tasks like grasping is valid to show the efficacy for that task, yet metrics and formal definitions are required for general quantitative evaluation and reproducibility.

Handling arbitrary objects requires algorithms to understand objects from a global, topological and a local, geometrical point of view.
Factors describing the change of physical configuration under deformation need to be encoded in order to effectively extrapolate to out-of-distribution objects.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% physically based
\subsection{Scene-level Consistency}

\begin{comment}
Cat C: some category-level works and also Neural Correspondence Fields work on that. In my head that section is for bridging the aspect of monocular pose estimation of caring little for geometry in 3D to simulation-like approaches (what Dominik did)
\end{comment}

Incorporating geometrical feedback loops for pose estimation leads to performance improvements through joint pose refinement of multiple objects~\cite{aldoma2012global,sock2018multi,labbe2020cosypose} and verification of physical plausibility~\cite{bauer2020verefine}. By considering multiple objects in the scene in parallel, their mutual occlusion and support relationships allow to restrict the space of admissible poses and, thereby, simplifies the pose estimation task.

Since the objects' distance from the camera is directly observable in depth images and hence more accurately predicted~\cite{wang2019densefusion}, recent approaches explore depth and occupancy estimation from monocular input~\cite{yen2021inerf,huang2022neural,cao2022dgecn}. While depth sensors produce noisy or incomplete observations of transparent and highly reflective objects, RGB-based methods are able to reconstruct complete depth observations~\cite{chen2022clearpose}. Depth data is found to generalize well and requiring less data than RGB-based methods~\cite{bauer2022sporeagent}. However, this is traded for inability to discern geometrically ambiguous but texturally distinct symmetries. Therefore, starting from RGB input (with optional depth estimation) is expected to cover more challenging situations than using depth sensors alone.
Alternative to depth estimation, end-to-end training detailed in Section~\ref{sec:e2e}, enables similar geometrical guidance of the learning process. 
Regressing multiple pose representations simultaneously enables enforcing consistency between them~\cite{thalhammer2022cope,di2021so}.

Another promising direction for future work is to consider the consistency between an estimated scene configuration and, for example, its simulation, reconstruction or rendering. The latter may furthermore incorporate factors such as surface materials which would be especially relevant for reflective and refractive objects in clutter, as their appearance depends on the broader scene. We conjecture that inverse rendering or NeRF-like \cite{mildenhall2021nerf} approaches will advance the state of the art in pose estimation in such situations.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Clutter (Markus)
\subsection{Benchmark Realism}
\begin{comment}
Cat B in my opinion, since domain shift is mentioned as mostly solved - then you can always just say "someone needs to generate better scenes but the problem itself is solved". So it's rather about extending existing work, not a new future challenge.
It can be cat B, but talking about the accompanied challenges does somehow make it a future direction, which people ignore due to the complexity of getting the data annotated.
What would you guys do here?
\end{comment}

% Figure environment removed

Existing datasets are still in an early stage with respect to their complexity and challenges in comparison to the real world.
There are a number of aspects which are simplified for the ease of dataset creation and annotation.
These aspects are scene setup, object variation, interactions between objects and varying material properties. 
The following are examples where existing datasets do not depict real-world complexities:
\begin{itemize}
 \item \textbf{Clutter:} Rich multi-object, multi-instance scenarios are required to exhaustively evaluate and improve occlusion handling, and as such general performance.
 Existing datasets, however, only feature manually crafted scenes. As a result, objects of interest are generally placed in the middle of the scene (and the camera view) and the diversity of occluding distractor objects as well as their placement is limited.
 This presents a large discrepancy to real scenarios. Considering for instance a kitchen scenario, occluding objects are unrestricted, and can be as indiscriminate as vegetable scraps, multiple instances of the same object are apparent in varying states and often interact with one another, their poses being only restricted by the scene geometry, and objects may even appear behind refractive occluders such as refrigerator shelves.
 \item \textbf{Backgrounds:} Relevant datasets mostly feature objects arranged on a single support plane, with a limited set of background textures and non-dataset distractors (see Figure~\ref{fig:instdata}). 
 Only few datasets assume more complex scene backgrounds~\cite{tyree20226,rennie2016rutgers}.
 Yet, generally, datasets that incorporate multi-faceted object placements with respect to lateral and vertical placement in the world and significantly changing scene backgrounds are missing.
 \item \textbf{Object variations:} As stated in Section~\ref{sec:obj_material}~\ref{sec:obj_deform}, mainstream approaches' ability to handle object property variation is very limited. 
 This is a consequence of the standard datasets that typically feature a single type of material (e.g., textured, metallic, refractive), and similar-sized and rigid objects.
 
 In contrast, a realistic dataset needs to feature a diverse object ontology. Objects need to be a) annotated on instance- and category level, and novel objects and categories, which are not available for training, need to be introduced for testing. b) Object shapes need to vary from small to large size, and from simple to complex geometries. c) Visual materials need to include opaque and transparent, diffuse and reflective, and textured and texture-less variations. These dimensions are ideally quantified with tractable metrics to allow creating exhaustive test scenarios and pose estimation accuracy with respect to them.
\end{itemize}

Figure~\ref{fig:real_data} shows a real household scene, with different objects of various sizes and materials, some also behind glass and as multiple instances. 
Objects are distributed over the full image space, many objects are further away from the camera as is assumed by most of the standard datasets, and the image is not centered with respect to a support plane.
In order to improve dataset creation, a paradigm shift needs to happen.
For one, the dataset creation and annotation process itself needs to improve.
The standard process is to choose an object set and subsequently deliberately place them in backgrounds that allows pose annotation, respectively simplifies annotation, e.g. marker boards as done in~\cite{chen2022MP6D,hinterstoisser2012model,doumanoglou2016recovering,homebrewedDB,hodan2017tless,wang2019normalized,liu2020keypose,fang2022transcg}.
Rather, test scenarios for household robotics should organically grow and objects of interest are ideally chosen from the set of objects that happened to be there.
This way, not only the object arrangements, but also the occlusion patterns are more natural, and ultimately, datasets created in that fashion depict the real-world complexity and object variations more accurately.
To this end, recent annotation tools allow to propagate poses over consecutive frame, given camera poses are accurate enough~\cite{suchi20233d}. 
The lower bound for pose annotation error is thus limited by camera pose accuracy. Still, object instances that are strongly occluded will be difficult to annotate.
Geometric reasoning for objects and the scene during annotation is required to alleviate such issues.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% environment
\subsection{Environmental Impact}

Object pose estimation research and application consumes large amounts of energy.
Especially under current concerns regarding sustainable research and environmental impact, addressing this issue is an obligation for future research.

Using instance-level pose estimators is still the standard case for robotic scenarios. 
This requires re-training them every time a new object or object set is used.
For example, the pose estimator of GDR-Net~\cite{wang2021gdr} trains on average approximately $6h$ on an NVIDIA RTX 3090 (with a TDP of 350W) for maximizing its performance for a single object of the BOP-challenge.
%This leads to the requirement of more than $8$ days of continuous GPU usage for the $33$ objects of HB, or about $69$kWh for the GPU alone (assuming full utilization), which equals roughly the power consumption of a Tesla Model Y driven for $440$ kilometers.
This requires more than $8$ days of continuous GPU usage for the $33$ objects of HB, or about $69$kWh for the GPU alone (assuming full utilization) -- the amount of energy the average U.S. consumer used over $2.1$ days%\footnote{https://www.eia.gov/energyexplained/use-of-energy/electricity-use-in-homes.php}
, or the worldwide-average consumer used over $7.9$ days in 2021\footnote{https://www.eia.gov/international/data/world/electricity/electricity-consumption. Most recent numbers (2021) accessed on July 20, 2023.
%https://www.statista.com/statistics/280704/world-power-consumption/
}.
Category-level object pose estimation may have a lower environmental impact, since trained models generalize to objects of the same category and as such may be used in more application scenarios, but still models are usually trained for each category separately.
Both these types of approaches exhibit fast inference, in the range of seconds, but are very limited with respect of the objects for which poses are inferred.
Novel object pose estimators have the potential to alleviate the need for long training times~\cite{goodwin2022zero,fan2023pope}, albeit, most approaches are still in the range of hours to days~\cite{thalhammer2023self,shugurov2022osop,labbe2022megapose}. Runtime is usually in the range of minutes since template matching requires calculating similarities to thousands of templates.
As such, novel object pose estimation still has not reached a state that largely improves power consumption.

Exploring alternatives to template matching might reduce the runtime for novel object pose estimation.
For instance, methods for inference from a few or single reference images are a potential solution, although currently still lack accuracy.
An alternative is offered by foundational models such as ViTs, pre-trained on large datasets, which already showed that they transfers well to pose estimation~\cite{goodwin2022zero,fan2023pope,thalhammer2023self}.
Exhaustively exploring such approaches that require no re-training for novel objects or changing test domains has the potential to reduce the power consumption of research and for robotic application, as the power expanded for the pre-training stage, while larger, is shared across all final applications.
Generally, methods that require no training and exhibit fast inference are important to allow easy and resource efficient utilization for diverse scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% conclusion
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
\label{sec:conclusion}

This work identifies and presents current research problems for monocular single-shot 6D object pose estimation and derives important future challenges. 
A discrepancy between object pose estimation research, that aims at maximizing performance on benchmark datasets, and robotic requirements is uncovered.
Consequently, we present important future research challenges towards bringing both fields closer together.
Benchmarking datasets need to exhibit more realism, in order to speed up computer vision research for robotics.
Having datasets with diverse known und unknown objects in known and unknown categories allows searching for ontolgies describing objects on a fundamental level.  
Rich multi-object scenarios with different challenging surface materials and deformations requires advancing the state of the art for handling these properties and enable effective implementation of robots in households.
Raising pose estimates to configurations that are consistent on scene-level will provides reliable priors for robotic downstream tasks. 
Especially deformable and articulated objects require novel metrics to measure benchmarking progress.
And, finally, climate change and sustainable research demands reducing the ecological footprint of algorithms, which has to be considered for future research.


\section*{Acknowledgments}
We gratefully acknowledge the support of the EU-program EC Horizon 2020 for Research and Innovation under grant agreement No. 101017089, project TraceBot, the Austrian Science Fund (FWF), under project No. I 6114, project iChores, under project No. J 4683, project Making Sense of Objects, and the NVIDIA Corporation.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


{\small
\bibliographystyle{IEEEtranS}
\bibliography{bib}
}

\end{document}