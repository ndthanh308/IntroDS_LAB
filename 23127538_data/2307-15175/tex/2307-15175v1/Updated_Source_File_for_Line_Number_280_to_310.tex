\subsubsection{Online Exploration of the Utility Function of DR Customers}
\label{sec:learn_utility_function_DR}
In the set up of Fig.~\ref{fig:attack}, the utility or  aggregator can  sequentially record broadcasted DR incentives and reductions in power consumption  of the DR customers in response to the incentives. Using this recorded  data, $\mathcal{H}_t =\{\lambda_{\tau},$ $x_{i,\tau}\},~\forall i \in \mathcal{N},~\forall \tau \in \mathcal{T}$, where $\mathcal{{N}} =\{1,\ldots, N\}$, $\mathcal{T} =\{1,\ldots, t-1\}$, $|\mathcal{T}|=T$, and $|\mathcal{N}|=N$, the utility or aggregator estimates $\boldsymbol{\beta}_{i,t}$ by minimizing the Mean Squared Error (MSE) of the expected response of DR customers given by Eq.~\eqref{eq:x_DR_opt}. This objective can be parameterized by the empirical loss function: 
\begin{subequations}
\label{eq:MSE}
\begin{align}
   \label{eq:loss}
    L_{i,t}((\lambda,x);\boldsymbol{\beta})= \frac{1}{2T}\sum_{\tau=1}^{T}(x_{i,\tau}-\beta_{1,i,\tau}\lambda_{\tau} -\beta_{0,i,\tau})^2 .
\end{align}
The first order optimality condition of $L((\lambda,x); \boldsymbol{\beta})$ over $\boldsymbol{\beta}_{i,t}$ is expressed as:
\begin{align}
\label{eq:partial_R1}
   \frac{\partial L_{i,t}}{\partial \beta_{1,i,t}} &=\frac{1}{T}\sum_{\tau=1}^{T} (x_{i,\tau}\lambda_{\tau}-\beta_{1,i,\tau}\lambda_{\tau}^2-\beta_{0,i,\tau}\lambda_{\tau}) =0, \\
   \label{eq:partial_R2}
   \frac{\partial L_{i,t}}{\partial \beta_{0, i, t}} &=\frac{1}{T}\sum_{\tau=1}^{T} (x_{i,\tau}-\beta_{1,i,\tau}\lambda_{\tau}-\beta_{0,i,\tau}) =0.
\end{align}
Solving Eqs.~\eqref{eq:partial_R1} and \eqref{eq:partial_R2} and setting $ \frac{1}{T}\sum_{\tau=1}^{T} x_{i,\tau} =\hat{x}_{i,t}$  and $ \frac{1}{T}\sum_{\tau=1}^{T} \lambda_{\tau} =\hat{\lambda}_{t}$, it follows:
\allowdisplaybreaks
\begin{align}
\label{eq:beta1}
    \beta_{1,i,t} &=\frac{\frac{1}{T}\sum_{\tau=1}^{T}x_{i,\tau}\lambda_{\tau} -\hat{x}_{i,t}\hat{\lambda}_{t}}{\frac{1}{T}\sum_{\tau=1}^{T}\lambda^2_{\tau}-\hat{\lambda}^2_{t}},\\
    \label{eq:beta0}
    \beta_{0,i,t} &=\hat {x}_{i,t}-\beta_{1,i,t}\hat{\lambda}_{t}.
\end{align}
Since set $\mathcal{H}_t$  is updated at every DR event at time $t$, Eqs.~\eqref{eq:beta1} and \eqref{eq:beta0}  use \textit{streaming} historical data to estimate $\boldsymbol{\beta}_{i,t}$. This is computationally unattractive for all DR customers over time due to the addition of new data points $\{x_{i,t}, \lambda_t\}$  at each DR event at time $t$. To overcome the computational complexity, the utility or aggregator can use the online gradient descent to update $\boldsymbol{\beta}$ for a DR event at time $t+1$ as:
\begin{align}
 \label{eq:ogd}
     \boldsymbol{\beta}_{i,t+1} = \boldsymbol{\beta}_{i,t}- {\eta_l}(\boldsymbol{\beta}^{\top}_{i,t}\boldsymbol{\lambda}_{t} -x_{i,t})\lambda_{t},
\end{align}
\end{subequations}
where ${\eta_l}$ is a user-defined model learning rate and vector $\boldsymbol{\lambda}_{t} = [1 ~ \lambda_{t}]^{\top}$. In the setting of Eq.~\eqref{eq:ogd}, $\boldsymbol{\beta}$ obtained in Eqs.~\eqref{eq:beta1} and \eqref{eq:beta0} can be realized as $\boldsymbol{\beta}_{i,t}$, which is updated at time $t+1$ to $\boldsymbol{\beta}_{i,t+1}$ based on the new sample set $\{x_{i,t}, \lambda_t\}$ of the DR event at time $t$.