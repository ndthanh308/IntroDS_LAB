\section{Experiments}

\begin{table*}[!t]
\centering
\begin{tabular}{|l|cc|cc|c|}
    \hline
    \multirow{2}{*}{\textbf{Planner with Heuristic}}  & \multicolumn{2}{c|}{\textbf{d1}} & \multicolumn{3}{c|}{\textbf{d2}} \\ \cline{2-6} 
     & \multicolumn{1}{c|}{\textbf{m1}} & \multicolumn{1}{c|}{\textbf{m2}} & \multicolumn{1}{c|}{\textbf{m1}} & \multicolumn{1}{c|}{\textbf{m2}} & \multicolumn{1}{c|}{\textbf{SAS+\textsuperscript{*}}} \\ \hline
    
    FastDownward with Blind & \multicolumn{1}{c|}{78 \textit{(100\%)}} & 67 \textit{(100\%)}& \multicolumn{1}{c|}{56 \textit{(100\%)}} & 65 \textit{(100\%)} & 66 \textit{(100\%)}\\ 
    
    % FastDownward with Max & \multicolumn{1}{c|}{107} & - & \multicolumn{1}{c|}{-} & - & - \\ 
    
    FastDownward with Causal Graph & \multicolumn{1}{c|}{96 \textit{(66.67\%)}} & 76 \textit{(81.58\%)} & \multicolumn{1}{c|}{72 \textit{(68\%)}} & 75 \textit{(80\%)} & 77 \textit{(75.32\%)} \\ 
    
    FastDownward with Context-enhanced Additive & \multicolumn{1}{c|}{99 \textit{(61.62\%)}} & 71 \textit{(85.92\%)} & \multicolumn{1}{c|}{68 \textit{(66.18\%)}} & 75 \textit{(78.67\%)} & 77 \textit{(75.32\%)} \\ 
    
    FastDownward with Goal count & \multicolumn{1}{c|}{103 \textit{(99.03\%)}} & 88 \textit{(97.73\%)} & \multicolumn{1}{c|}{75 \textit{(94.67\%)}} & 85 \textit{(89.41\%)} & 87 \textit{(89.66\%)} \\ 
    
    FastDownward with LM-Cost Partitioning & \multicolumn{1}{c|}{103 \textit{(100\%)}} & 97 \textit{(100\%)} & \multicolumn{1}{c|}{75 \textit{(100\%)}} & 86 \textit{(100\%)} & 87 \textit{(100\%)} \\ 
     
    \textbf{FastDownward with FF }& \multicolumn{1}{c|}{\textbf{137} \textit{(99.27\%)}} & \textbf{135} \textit{(88.89\%)} & \multicolumn{1}{c|}{\textbf{104} \textit{(98\%)}} & \textbf{113} \textit{(79.64\%)} & \textbf{123} \textit{(79.67\%)} \\ 
    
    \hline
    \hline
    Scorpion with Merge \& Shrink & \multicolumn{1}{c|}{114 \textit{(100\%)}} & 105 \textit{(100\%)} & \multicolumn{1}{c|}{82 \textit{(100\%)}} & 95 \textit{(100\%)} & 90 \textit{(100\%)}\\
    
    Scorpion with Max Manual PDB & \multicolumn{1}{c|}{95 \textit{(100\%)}} & 83 \textit{(100\%)} & \multicolumn{1}{c|}{64 \textit{(100\%)}} & 78 \textit{(100\%)} & 123 \textit{(100\%)} \\
    
    Scorpion with Max Systematic PDB & \multicolumn{1}{c|}{88 \textit{(100\%)}} & 78 \textit{(100\%)} & \multicolumn{1}{c|}{63 \textit{(100\%)}} & 73 \textit{(100\%)} & 120 \textit{(100\%)} \\ 
    
    \hline
    \hline
    DeepCubeA (12 action model) & \multicolumn{2}{c|}{200 \textit{(94.5\%)}} & \multicolumn{3}{c|}{200 \textit{(78.5\%)}}  \\
    \hline
\end{tabular}
\caption{Comparison of planner configurations based on the total number of solved problems and the percentage of optimal plans for different Rubik's Cube models. (\textsuperscript{*}SAS+ dataset presented by \citet{buchner2022comparison})}
\label{tab:exp-results}
\end{table*}

In the following section, we will discuss the heuristics considered in our evaluation and the experimental setup, which includes the datasets, problem representations, and details about the planner.

% In the following sections, we will discuss about the experimental setup and the evaluation of different heuristics performances on the RC PDDL model. We use the Fast Downward planning system to evaluate the performance of different heuristics including abstraction heuristics which were studied in the recent literature. Fast Downward is a domain-independent classical planning system based on a heuristic search. Further, we describe the problem tasks we use to evaluate these heuristics performance on our RC PDDL model.

\subsection{Heuristics Considered}
% In this section, we briefly describe the heuristic specifications considered for our experiment.

% \subsubsection{Landmark Count Heuristic} 
% We use the Landmark Count (LM Count) heuristic \citep{richter2010lama} with RHW Landmarks \cite{richter2008landmarks} as they support conditional effects modeled in the domain and CPLEX solver. 

\subsubsection{Blind Heuristic} refers to a decision-making strategy that does not incorporate any specific information regarding the problem domain. It relies solely on the present state of the problem and employs a trial-and-error method to find a solution.
        
\subsubsection{Causal Graph Heuristic} utilizes the causal relationships between state variables in a planning task. This graph captures dependencies among variables, with arcs indicating the influence between them. By leveraging the graph's structure, the heuristic offers insights into variable interactions, facilitating more efficient planning in domains where causal understanding is pivotal. \cite{helmert2004planning}

\subsubsection{Context-enhanced Additive Heuristic} refines the additive heuristic by using contextual information, offering a more precise estimate. While the additive heuristic determines costs based on individual goals, the context-enhanced version evaluates conditions within distinct contexts, differing from the seed state's heuristic value. Tied closely to the causal graph and additive heuristics, this method selects optimal contexts for condition evaluations. It formulates as a shortest-path problem on a graph where nodes, representing problem atoms, are contextually labeled. This approach adeptly captures side effects, leading to enhanced heuristic evaluations. \cite{helmert2008unifying}

\subsubsection{Max-cost Heuristic} evaluates the cost of achieving a set of goals by recursively determining the maximum cost for each individual goal. For each goal, the heuristic computes the minimum cost over all actions that can produce the goal, considering the action's cost and the max-cost of its preconditions. \cite{ghallab2016automated} 

\subsubsection{Goal Count Heuristic} estimates the number of unsatisfied goals in a state, prioritizing states with fewer unsatisfied goals. This method is useful in problems with multiple goals, such as game playing and planning, and can improve the efficiency of finding a solution.

\subsubsection{LM-cost partitioning Heuristic} is a technique that distributes the costs of operators among the landmarks they achieve. By doing so, it ensures that the heuristic value remains admissible. In essence, the cost of each operator is divided among the landmarks it helps achieve, ensuring that the sum of the heuristic values of these landmarks does not exceed the actual cost. This approach allows for a more informed estimate of the cost to reach the goal by leveraging the structure of the problem's landmarks and the operators' costs. \cite{karpas2009cost}

\subsubsection{FF Heuristic} derived from the FF planning system, estimates the distance to the goal using a relaxed plan. In this relaxed scenario, negative action effects are ignored, and actions can be parallelized. By counting actions in the relaxed plan, the heuristic provides a lower bound on steps needed for the actual solution, guiding the search efficiently. \cite{hoffmann2001ff}

\subsubsection{Merge and Shrink Heuristic} is a technique for generating lower bounds in factored state spaces. Originating from model checking of automata networks, it offers a more general class of abstractions than traditional pattern databases. By merging and shrinking states, it can represent a broader range of abstractions, potentially more compactly. In essence, merge-and-shrink provides a balance between abstraction size and heuristic accuracy, making it a valuable tool for heuristic search in complex domains \cite{helmert2014merge}. In the Merge and Shrink (M\&S) heuristic we use bisimulation as shrinking strategy \cite{nissim2011computing}, strongly connected components as merging strategy \cite{sievers2016analysis}, and exact label reduction \cite{sievers2014generalized}. We limit the abstractions to 50,000 states.
        
\subsubsection{Pattern Database Heuristics}
The key step in using Pattern Database (PDBs) heuristics is selecting appropriate patterns for the problem at hand. \citet{korf1997finding} specified two sets of patterns for solving the Rubik's Cube. We evaluate two settings of PDBs:

{\em \textbf{Max Manual PDB:}} Inspired by Korf's patterns, \citet{buchner2022comparison} have considered 2 patterns for the corner cube pieces and 3 patterns for the edge cube pieces resulting in 4 variables for each pattern. We have considered these patterns for the evaluation of PDDL and SAS+ models.

{\em \textbf{Max Systematic PDB:}} This configuration systematically generates all
interesting patterns up to a certain size \cite{pommerening2013getting}. A pattern size of 3 has been considered for this evaluation in the interest of memory constraints.


% \begin{table*}[t]
% \centering
% \begin{tabular}{|l|l|l|c|c|}
% \hline
% \textbf{Planner} & \textbf{Heuristic} & \textbf{Specifications} & \multicolumn{1}{l|}{\textbf{PDDL}} & \multicolumn{1}{l|}{\textbf{SAS+}*} \\ \hline
% Scorpion & Merge \& Shrink & \begin{tabular}[c]{@{}l@{}}Shrinking strategy: Bisimulation\\ Merging strategy: SCCS\\ Max abstraction states: 50,000\\ Exact label reduction\end{tabular} & 95 & 90 \\ \hline
% Fast Downward & LM Count & \begin{tabular}[c]{@{}l@{}}Solver: CPLEX\\ RHW Landmarks\end{tabular} & 84 & \textless{}NA\textgreater{} \\ \hline
% Scorpion & Max Manual PDB & \begin{tabular}[c]{@{}l@{}}Corner cubelets: 2 patterns\\ Edge cubelets: 3 patterns\\ Pattern: 4 variables\end{tabular} & 78 & 123 \\ \hline
% Scorpion & Max Systematic PDB & Max pattern size: 3 & 73 & 119 \\ \hline
% \end{tabular}
% \caption{Results (* \citet{buchner2022comparison})}
% \label{tab:exp-results}
% \end{table*}

% \subsection{Comparision of RC representations}
% ...
% \subsubsection{DeepCubeA}
% The DeepCubeA algorithm adopts a unidimensional array as a representation of the Rubik's Cube (RC) state. Specifically, this array encompasses 54 elements, each of which corresponds to a unique sticker color present on a cube piece of the RC. While this array-based modeling offers computational advantages, it is limited by its inability to fully encapsulate the spatial orientation of Rubik's Cube. Furthermore, the usage of a hard-coded representation and implicit assumptions concerning the position of cubelets poses a challenge to novice users seeking to comprehend the array-based representation.

% \subsubsection{PDDL}
% The PDDL representation of the Rubik's Cube consists of 8 corner cubies and 12 edge cubies. Each corner cubie is defined by its color on three sides, represented as a combination of Red (R), Blue (B), Green (G), Orange (O), White (W), and Yellow (Y). Each edge is defined by the two adjacent colors. This PDDL representation provides a more explicit and comprehensive way to represent the Rubik's Cube, including the spatial orientation of each cube piece.

% \subsubsection{SAS+}

\subsection{Experimental Setup}
To compare the performance of our RC PDDL model with the existing literature work, we have used the benchmark problem test set presented by \citet{buchner2022comparison}. In the benchmark test set, the problem tasks have been generated using 18 actions of RC - 12 actions correspond to 90-degree rotations of each face in clockwise and anti-clockwise directions, and the additional six actions are 180-degree rotation (suffix '2') on each face. The problem test set consists of 200 problems of varying difficulties. We have considered the scramble sequences provided to generate the respective PDDL versions of the problems. Additionally, we have generated our own test set of 200 RC problems considering only 12 actions. The problem generator starts from the goal state of RC and applies \textit{n} arbitrary actions from the list of 12 available actions. For every value of \textit{n}, ten unique random problem states are generated. The value of \textit{n} is between 1 and 20. The upper limit of 20 is chosen because the authors in \cite{rokicki2008twenty} state that all the RC problem instances can be solved with at-most 20 moves. It has been considered that every consecutive rotation corresponds to a different face of the RC, as such rotations can not be combined into a single rotation. 
% These 200 unique problem instances of RC have been used for the evaluation of our RC PDDL model.

% We now define the nomenclature of the problem test sets and encoding models considered for evaluation -
% \begin{itemize}
%     \item \textbf{\em d1} - 200 problem task data-set generated by considering 12 RC actions.
%     \item \textbf{\em d2} - 200 problem task data-set presented by \citet{buchner2022comparison}, generated using 18 RC actions.
%     \item \textbf{\em m1} - RC PDDL model with 12 RC actions.
%     \item \textbf{\em m2} - RC PDDL model with 18 RC actions.
% \end{itemize}

The main difference between the two datasets \textit{d1} and \textit{d2} is that a 180-degree turn (half-turn) is considered as two actions in generating dataset \textit{d1}, while it has been considered as a single action in generating dataset \textit{d2}. The reason for evaluating two different datasets is that we wanted to capture the performance difference between the two PDDL models \textit{m1} (12 actions) and \textit{m2} (18 actions) in accordance with the difference in the branching factor. The PDDL model \textit{m2} and SAS+ model have similar branching factors.

To evaluate the RC PDDL model, we have used Scorpion planner \cite{seipp2020saturated}, which is an extension of Fast-Downward planner \cite{helmert2006fast}. Scorpion planner contains the implementation for PDBs that support conditional effects modeled in the domain file. We perform A* searches with each heuristic mentioned above on the test sets and the two PDDL models. We bound the A* search with an overall time limit of 30 minutes and a memory limit of 3.5GB. This constraint is the same for the abstraction heuristics as well, despite the fact that these heuristics require significant time for preprocessing and generating abstractions prior to the start of the search.

% The generated test set of 200 problem instances have been tested with the above mentioned heuristics. We use A* search with all the 4 heuristics mentioned with a time limit of 30 minutes and memory limit of 3.5GB in our experiment setup. In terms of number of solved problems, Merge and Shrink heuristic performed the best by solving 113 problems while the LM Count heuristic was able to solve 103 problems followed by Max Systematic PDB with 84 problems solved and Max Manual PDB was able to solve 68 problems performing the worst among all. The results of the heuristics performance is shown in Figure \ref{FIG: exp_results} where the x-axis represents the problem instance number with increasing complexity. Figure \ref{FIG: exp_results}(a) and Figure \ref{FIG: exp_results}(b) shows the comparison of time and memory used by different heuristics to solve a given problem instance. In contrast to other situations, it has been found that all failed executions of the Merge and Shrink and Max Systematic PDB heuristics were caused by time constraints.

% Figure environment removed