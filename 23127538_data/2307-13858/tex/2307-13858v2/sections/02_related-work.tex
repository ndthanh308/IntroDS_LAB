\section{Related Work}

Our work is related to two main areas of prior work: 
(1) how people read charts and text and (2) supporting document authoring.
\subsection{How People Read Charts and Text}
Prior research has shown that visualizations and text complement each other when communicating information about the data.
Specifically, Elzer et al.~\cite{elzer2005exploring} and Carberry et al.~\cite{carberry2006information} found that visualizations often contribute information not available in the text alone. On the other hand, researchers have found that text guides readers' attention while viewing visualizations~\cite{gould1976looking,tufte2001visual,xiong2019curse} and improves memorability of the key messages in visualizations~\cite{borkin2015beyond,kim2021towards,cheng2022captions}.

The two representations facilitate different tasks for readers; Ottley et al.~\cite{ottley2019curious} found that readers easily \textit{identify} key information with visualizations and easily \textit{extract} key information from text.
Despite the complementary nature of visualizations and text, prior studies did not find particularly synergistic benefits in comprehending information present in both chart and caption text~\cite{khan2015benefits,micallef2012assessing,ottley2012visually,ottley2015improving}.

Researchers have noticed that one of the major problems limiting the synergistic benefits of using visualizations and text together is \textit{split attention} as readers have to look back and forth between the spatially separated representations~\cite{biderman1979graph,sweller2011split,tufte2001visual}.
Whitacre and Saul~\cite{whitacre2016high} and Ottley et al.~\cite{ottley2019curious} concluded that readers tend not to integrate the information between the visualizations and text. The research community has put forth a range of solutions for the split attention problem. Tufte~\cite{tufte2001visual} introduced \textit{sparklines}, which are word-sized line charts embedded into the text. Goffin et al.~\cite{goffin2015design} and Beck and Weiskopf~\cite{beck2017word} extended the idea by adding interactivity to the sparklines. In addition, multiple systems (e.g., table-text reference display systems~\cite{badam2018elastic,kim2018facilitating}, visualization-text reference display systems~\cite{kong2014extracting,lai2020automatic,latif2018exploring,pinheiro2022charttext})
display the references between visualizations and text to reduce the effort needed to locate relevant information.

The research community has recently begun diving deeper into the relationship between visualizations and text by studying how 
comprehension depends on the content of the visualizations and text.
Kong et al.~\cite{kong2018frames,kong2019trust} studied how slants, framing in chart titles, as well as misalignments between charts and text, affect what people read from charts and their titles. They found that while people identify bias, they still consider charts to be impartial. Whitacre and Saul~\cite{whitacre2016high} studied high school students and found that they had difficulty identifying the inconsistencies between graphs and their captions. Kim et al.~\cite{kim2021towards} found that when charts and captions emphasize the same feature in the data, people tend to take away the message related to the doubly emphasized feature. When the caption describes a feature that is not visually prominent in a chart, readers are more likely to consider the visually prominent chart features as carrying the key messages. Subsequent work found that semantic levels of text~\cite{cheng2022captions,lundgard2021accessible,stokes2022striking}, as well as its placement~\cite{stokes2022striking}, can influence how readers integrate charts with text.
Although our work on the \toolname{}~tool is targeted toward authors, the principles behind how 
the tool helps authors write effective chart-text pairs, rely on these theories about how readers read charts and text together. 

\subsection{Supporting Document Authoring}
Document authoring tools often support authors so that they can easily write high-quality text with a clear exposition.
Spell-checkers~\cite{earnest2011first} have been incorporated into many writing environments, including word processors and e-mail~\cite{google2022check,libreoffice2022checking,google2022fix,microsoft2022checking}.
Grammar checkers are also available in such environments through software extensions or plug-ins, such as Grammarly~\cite{grammarly2022grammarly} and LanguageTool~\cite{language2022language}.
More recently, writing environments have incorporated AI-based autocompletion~\cite{google2022using} that tries to  predict what the writer intends to write.
Similar to these tools, \toolname{} is designed to help people write documents, but instead of purely analyzing text, \toolname{} considers chart and text pairs and analyzes the visuals as well as the words together. 

The research community has developed methods for automatically generating caption text for visualizations. 
Many of these techniques  
are designed to generate basic captions that simply explain how the visualization encodes data (e.g., \cite{cui2019datasite,demiralp2017foresight,tableau2022tableau,wills2010autovis}).
A few techniques go beyond basic captions and generate descriptions and summaries of features (e.g., ranking, extrema, trends) in the visualizations, such as techniques based on Bayesian models~\cite{carberry2006information,elzer2011automated} and neural networks~\cite{chen2019neural,chen2019figure,obeid2020chart,qian2021generating}.
Rather than generating captions, Contextifier~\cite{hullman2013contextifier} adds annotations based on financial news headlines to noteworthy features of stock charts.
Unlike the fully-automated systems, \toolname~follows the interface paradigm of a spell- or grammar-checker by guiding the author while they have complete control over their writing.

More advanced tools provide assistance beyond adding text to visualizations. TimeLineCurator~\cite{Fulda2016TimeLineCuratorIA} is a web-based timeline authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing, along with controls for curating and editing the events. VizByWiki~\cite{lin2018vizbywiki} retrieves visualizations relevant to given news articles from Wikimedia Commons~\cite{wikimedia2022wikimedia} to enrich articles.
PostGraphe~\cite{fasciano1996postgraphe} creates text and graphics based on tabular input data and users' intents.
Kori~\cite{latif2021kori} takes a mixed-initiative approach in helping authors construct interactive references between visualizations and text into their documents with suggestions based on natural language processing techniques.
CrossData~\cite{chen2022crossdata} links text and the underlying data to reduce efforts in writing data documents and data exploration.
Although \toolname{} leverages references between visualizations and text similarly to these prior works, our tool also analyzes the \textit{chart data} while identifying references in order to establish comparisons between chart and text emphasis. Furthermore, the reference extraction is only a component of \toolname{} whose end goal is to provide an interface for identifying mismatches between visualizations and the text.

