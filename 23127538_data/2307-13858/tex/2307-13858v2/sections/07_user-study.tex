\section{User Study}
\label{sec:user-study}
To evaluate how the \toolname{} tool helps authors
write charts and captions, we conducted a within-subjects study comparing the complete \toolname{} tool (\toolname{} condition) against a baseline system that only includes the chart and text authoring interface in Figure~\ref{fig:base-interface} (baseline condition). 
We consider the following two hypotheses:

\vspace{0.05in}
\noindent\textbf{[H1]} Users will find the features in \toolname{} useful in authoring charts and captions.

\vspace{0.05in}
\noindent\textbf{[H2]} The features in \toolname{} are easy to use.

\vspace{0.05in}
We note that the accuracy of the text reference extractor was lower at 56\% at the time of the user study due to an issue with our implementation of the time reference extraction that was later revised.
\subsection{Participants}

We recruited 12 participants through online communities within KAIST and personal referrals.
We required participants to be capable of reading and writing in English and
we administered a test to ascertain skills such as understanding the chart encoding, reading off values, and recognizing trends and extrema~\cite{kim2021towards}.
In order to model the target population of the tool, we further required that the participants regularly author charts and captions.
All of the participants reported having regular exposure to chart-caption authoring for academic purposes (12 participants, P1-P12), presentations (11 participants), coursework (5 participants), industrial purposes (4 participants), articles (2 participants), social media (2 participants), and personal messages (2 participants).

\subsection{Procedure}
We conducted the study online through Google Meet~\cite{google2023meet} with screen sharing.
We began the study with a pre-survey asking about the participant's background in authoring charts and text as well as their domain of expertise.
We then showed the participant a chart and three types of captions: a basic caption, a caption about prominent chart features, and a caption about non-prominent chart features. 
After reading through the chart-caption pairs, they answered which caption is the most effective and their rationale.
We then provided instructions about the baseline and \toolname{} tools for authoring chart-caption pairs.

During the study, the participant chose two distinct time-series line charts either from the real-world corpus from Kim et al.~\cite{kim2021towards} or one of their own.
The participant authored one chart-caption pair with the baseline tool and another pair with the \toolname{} tool in a counterbalanced order.
After the use of each tool, the participant completed a post-task reflection survey that comprised three parts: (1) the participant's intended messages; (2) a usefulness assessment of the guidance provided by the tool and the expected capability of captions generated with the tool in communicating authors' main messages on 5-point Likert scales and free form comments about the benefits and disadvantages of using the tool; and (3) a usability assessment using the System Usability Scale (SUS)~\cite{brooke1996sus} and free-form comments about the usability of the tool.

After completing the two chart-caption pair authoring tasks, we asked the participant to \textit{compare} their experience with the two tools as a post-survey.
We specifically asked the participants to compare the usefulness and ease of use of the two tools in chart-caption authoring on 5-point scales (i.e., Baseline+2, Baseline+1, Neutral, \toolname{}+1, \toolname{}+2) and asked for rationales for their scores.
We finally asked for any free-form comments on the two tools.

The study lasted 60-75 minutes, and we compensated each participant with an equivalent of 15 USD as a direct deposit. 
We include the survey materials and responses in the supplemental material.

\subsection{Results and Discussion}
% Figure environment removed

\vspace{0.05in}
\noindent\textbf{\textit{Assessing H1.}}
All 12 participants stated that \toolname{} is more useful than the baseline tool in authoring (Figure~\ref{fig:user-comparisons}a), with 4 participants explicitly stating their excitement about the technology potentially being applied to their actual authoring routines in their free-form responses.
Comparisons of the ratings of each of the two tools are also in line with their comparisons.
The participants ranked the usefulness of the guides provided by \toolname{} as 4.33 ($\sigma=0.85$), which is significantly higher than the baseline tool's 2.75 ($\sigma=1.09$) (two-sided Wilcoxon signed-rank test; $W(11) = 5.5 < 10$, the critical $W$ value for $\alpha = 0.05$).
The participants also ranked the messaging capability of captions written with \toolname{} as 4.17 ($\sigma=0.69$), also significantly higher than the baseline tool's 3.18 ($\sigma = 0.55$) (two-sided Wilcoxon signed-rank test; $W(8) = 0.0 < 3$, the critical $W$ value for $\alpha = 0.05$).

When we asked the participants about the rationale for their assessment of the usefulness of \toolname{}, 6 of the 12 participants explicitly mentioned that the check for the alignment between the chart and text helped with their authoring process.
For example, P7 wrote, \textit{``(translated) [\toolname{}'s checking feature] gave me a chance to reflect on whether other people would agree on what I described as important in the text.''}

An additional six participants specifically pointed to the prominent feature detection as a component that contributes to the usefulness of \toolname{}.
P8 went further to suggest that the tool could be useful for exploratory data analysis while also precautioning that the features being shown up front in these cases could bias analysis.

All captions we collected through the study described prominent chart features.
We hypothesize that this is because all the participants we had in the pool were not only experienced with authoring charts and captions but also were reminded of the properties of effective chart-caption pairs through a pre-study exercise.

\vspace{0.05in}
\noindent\textbf{\textit{Assessing H2.}} 
While five participants stated that authoring charts and text with \toolname{} was easier,
the majority (7 of 12) of the participants were neutral on the comparison of the ease of use of the two tools (Figure~\ref{fig:user-comparisons}b).
This result is similar to our findings on the SUS scores of each of the two tools; the average SUS score of our tool was 88.33 ($\sigma = 8.06$) was on par with that of the baseline tool at 82.08 ($\sigma = 11.22$) (two-sided paired t-test: $t(11) = 1.96, p = 0.08 > 0.05$).

While it is counterintuitive that \toolname{} with more features was deemed easier to use than the simpler baseline tool by some participants, diving deeper into the participants' comments provides an insight into why this was the case.
Looking first at the comments on the ease of use from the seven participants who were neutral, six of them commented that while \toolname{} includes additional features, the two tools were both very intuitive and easy to use.
The participants who rated \toolname{} as easier to use pointed out that the synergistic effect between the basic authoring interface and the added features reduces the mental burden on the users.
For instance, P4 wrote, \textit{``While rescaling the graph and seeing prominent points, it is easier to focus on the main characteristics of the graph,''} 
pointing to the synergistic effect between the chart-editing interface and the prominent feature display.
P10, who was neutral about the comparison, stated, \textit{``(translated) Physically, Tool A [baseline] was easier because it was simpler but Tool B [\toolname{}] required less effort in authoring charts and was psychologically easier to use.''} 

\vspace{0.05in}
\noindent\textbf{\textit{Participants' reactions to errors in text references.}}
While writing captions with \toolname{}, 10 of the 12 participants experienced errors.
All of the participants immediately noticed errors. 
We note that this is different from how \textit{readers} often fail to see errors in reference extraction methods~\cite{kim2018facilitating}.
We hypothesize that this is because the authors of our tool have a clear intention and expectation of the correct results in mind when they run it, whereas readers do not know the expected results beforehand.
After noticing the error, three of the participants attempted a revision of the text so that their text would be detected correctly, whereas the others read over the text and continued.
In summary, the results suggest that while authors are unlikely to be misled by the system's errors, the errors can lead authors to spend unnecessary efforts in trying to satisfy the system.

However, the prevalence of the errors in the tool occasionally resulted in the failure of participants to realize their error even when \toolname{} behaved correctly.
For example, P12 typed in the wrong year (2018 instead of 2008), which was out of the time range shown in the chart (up to 2015), and \toolname{} correctly identified no match between the text and chart.
However, the participant thought that it was an error of the system for not detecting the time range in the chart and moved on.
P6, on the other hand, made a typo by typing in the year 1987 instead of 1997, causing \toolname{} to detect a time range different from what the participant intended (Figure~\ref{fig:results}b Sentence 4)
As with the other participant, the participant deemed it an error of our tool and continued on.
We believe that these errors are partially due to the authors trusting themselves more than \toolname{} and that such issues will diminish as \toolname{} becomes more accurate.