\section{Limitations and Future Work}

\noindent\textbf{\textit{Providing additional guidance in authoring charts and captions.}}

Our tool currently provides rudimentary guidance in authoring charts and captions with matched emphasis but can be improved to provide further guidance to support authors.
As P3 and P8 suggested, the tool can be extended to provide more information about the detected features (e.g., if the feature is rising/falling, the slope).
Future work could also apply natural language generation techniques and suggest automatically generated captions for selected prominent features to further help with the authoring process.
Alternatively, future work could provide further explanations and revision suggestions on detected emphasis mismatches or factual errors to help authors make more informed decisions.
Furthermore, during the pre-study activity of choosing the most effective caption and citing rationale, four of the participants mentioned that useful integration of external information not available in the chart is a property of effective captions, which is in line with the findings of Kim et al.~\cite{kim2021towards}.
Based on these observations, we believe that suggestion of suitable external information in the process could also provide useful additional guidance.
Another potential future direction would be to extend the tool to assist in writing chart descriptions for accessibility. As blind and sighted individuals perceive the usefulness of chart descriptions differently~\cite{lundgard2021accessible}, properties of effective text descriptions for blind individuals should be incorporated into the extended tool design.

\vspace{0.05in}
\noindent\textbf{\textit{Avoiding introducing bias to chart-caption pairs.}}
While all participants agreed on the usefulness of the tool, some participants suggested that guidance needs to be provided to the authors with careful consideration.
P8's concern about the potential biasing of the chart and caption was shared by two other participants.
They mentioned that being shown the prominent features too early while exploring the data could skew what messages they decide to include in the caption.
From these comments, we believe that the guidance provided by the tool could be personalized and adjusted based on the state of the writing process, avoiding too much guidance during the exploratory stages.
Moreover, focusing only on the match between chart and caption emphasis can lead to biased messages or violations of principles of good visualization design, such as the banking to 45 degrees principle~\cite{cleveland1988shape}.
Future developments of the tool can include detectors for biased representation of data and violations of good visualization design to help authors stay faithful to the data within the boundaries of good visualization design while matching chart and caption emphasis.


\vspace{0.05in}
\noindent\textbf{\textit{Generalizing tool to other chart features \& chart types.}}
Our tool is designed to work with time-series line charts, with a focus on universally present features: local extrema and trends. 
Yet, depending on the domain and context of the data, time-series charts can include other types of features, such as seasonal or cyclic patterns in monthly sales of AC units, L- or V-shapes in curves showing economic recovery, or double bottoms in stock charts (`W'-shaped features signaling potential future increases in stock price).
We believe that future work can expand \toolname{} by implementing such features for authors in specific domains and contexts.
Furthermore, future work can go beyond univariate time-series line charts and cover other chart types, starting with the more common charts~\cite{battle2018beagle} such as multi-line charts, bar charts, and scatter plots.
However, we expect that the generalization of \toolname{} to other chart features and chart types will require a deeper understanding of the features not currently covered in this work. \toolname{} is based on being able to compare the prominence of the chart features, and generalization of the tool will require similar levels of understanding of the chart features not covered in our work.
Moreover, the design of the tool is grounded on the assumption that there is a one-to-one mapping between the x-axis and the data points. While the current design would hold for chart types that follow this assumption (e.g., horizontal bar charts), the design of the tool may need modifications for other chart types to help users recognize features while avoiding visual clutter.

\vspace{0.05in}
\noindent\textbf{\textit{Incorporating information outside sentence-chart pairs.}}
Our text references extractor operates at the sentence level and utilizes information available strictly within the sentence and the chart.
The text references extractor currently does not utilize the information available in prior sentences. 
For instance, our tool fails to detect any references in Figure~\ref{fig:results} (d) Sentence 3 because the reference to time (\textit{`May 2012'}) appears in the previous sentence.
Anaphora resolution methods~\cite{mitkov2014anaphora} would be a starting point for future work. 
In addition, while our use of the Named Entity Module in the Stanford CoreNLP toolkit~\cite{chang2012sutime,finkel2005incorporating,manning2014stanford} allows our text references extractor to cover a variety of time expressions (e.g., year, date, time, seasons, quarters, duration, etc.), it is unable to comprehend time expressions requiring external knowledge.
For example, were it not for the explicit mention of the year \textit{`2020'} in Figure~\ref{fig:results} (c) Sentence 3, the tool would not have been able to comprehend the time range described as \textit{`the period of Covid-19.'}
Allowing the text references extractor to draw information from external knowledge bases (e.g., WolframAlpha~\cite{wolfram2022wolframalpha}) or LLM-based tools (e.g., GPT~\cite{brown2020language}) could allow it to detect more descriptions of time reliably.


\vspace{0.05in}
\noindent\textbf{\textit{Computing visual prominence in rendered charts.}}
Our tool, like that of Hullman et al.~\cite{hullman2013contextifier}, utilizes only the properties of the underlying time-series data to approximate visual prominence; the tool does not consider the rendering of the chart.
But, chart authors often use specific encodings (e.g., color, size) and annotations that guide the readers' attention to specific chart features. Such encodings and annotations can affect the visual prominence of features. Future work could analyze the visual properties of the rendered chart as well as the statistical properties of the underlying data to develop 
additional measures of visual prominence.

\vspace{0.05in}