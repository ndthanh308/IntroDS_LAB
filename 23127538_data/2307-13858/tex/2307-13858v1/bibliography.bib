
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
@misc{aidungeon2022aidungeon,
  author =       "AI Dungeon",
  year =         "2022",
  lastaccessed = "June 15, 2022",
  url =          "https://play.aidungeon.io/main/home",
}

@inproceedings{auer2007dbpedia,
  title={{DBpedia: A Nucleus for a Web of Open Data}},
  author={S. Auer and Christian Bizer and Georgi Kobilarov and Jens Lehmann and Richard Cyganiak and Zachary G. Ives},
  booktitle={International Semantic Web Conference / Asian Semantic Web Conference},
  year={2007},
  doi="10.1007/978-3-540-76298-0_52"
}

@incollection{auer2007dbpedia2,
  title={DBpedia: A Nucleus for a Web of Open Data},
  author={Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  booktitle={The semantic web},
  pages={722--735},
  year={2007},
  publisher={Springer},
  doi="10.1007/978-3-540-76298-0_52"
}

@ARTICLE{badam2018elastic,
  author={Badam, Sriram Karthik and Liu, Zhicheng and Elmqvist, Niklas},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading}}, 
  year={2019},
  volume={25},
  number={1},
  pages={661--671},
  abstract={Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.},
  keywords={},
  doi={10.1109/TVCG.2018.2865119},
  ISSN={1941-0506},
}

@inproceedings{battle2018beagle,
author = {Battle, Leilani and Duan, Peitong and Miranda, Zachery and Mukusheva, Dana and Chang, Remco and Stonebraker, Michael},
title = {{Beagle: Automated Extraction and Interpretation of Visualizations from the Web}},
year = {2018},
isbn = {9781450356206},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3173574.3174168},
doi = {10.1145/3173574.3174168},
abstract = {"How common is interactive visualization on the web?" "What is the most popular visualization design?" "How prevalent are pie charts really?" These questions intimate the role of interactive visualization in the real (online) world. In this paper, we present our approach (and findings) to answering these questions. First, we introduce Beagle, which mines the web for SVG-based visualizations and automatically classifies them by type (i.e., bar, pie, etc.). With Beagle, we extract over 41,000 visualizations across five different tools and repositories, and classify them with 85\% accuracy, across 24 visualization types. Given this visualization collection, we study usage across tools. We find that most visualizations fall under four types: bar charts, line charts, scatter charts, and geographic maps. Though controversial, pie charts are relatively rare for the visualization tools that were studied. Our findings also suggest that the total visualization types supported by a given tool could factor into its ease of use. However this effect appears to be mitigated by providing a variety of diverse expert visualization examples to users.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1--8},
numpages = {8},
keywords = {visualization classification, web mining, design mining},
location = {Montreal QC, Canada},
}

@misc{bbc2022bbc,
  author =       "{The British Broadcasting Corporation (BBC)}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.bbc.com/}",
}

@ARTICLE{beck2017word,
  author={Beck, Fabian and Weiskopf, Daniel},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Word-Sized Graphics for Scientific Texts}}, 
  year={2017},
  volume={23},
  number={6},
  pages={1576--1587},
  abstract={Generating visualizations at the size of a word creates dense information representations often called sparklines. The integration of word-sized graphics into text could avoid additional cognitive load caused by splitting the readers' attention between figures and text. In scientific publications, these graphics make statements easier to understand and verify because additional quantitative information is available where needed. In this work, we perform a literature review to find out how researchers have already applied such word-sized representations. Illustrating the versatility of the approach, we leverage these representations for reporting empirical and bibliographic data in three application examples. For interactive Web-based publications, we explore levels of interactivity and discuss interaction patterns to link visualization and text. We finally call the visualization community to be a pioneer in exploring new visualization-enriched and interactive publication formats.},
  keywords={},
  doi={10.1109/TVCG.2017.2674958},
  ISSN={1941-0506},
}



@article{biderman1979graph,
   author = "Biderman, Albert D.",
   title = "{The Graph as a Victim of Adverse Discrimination and Segregation}", 
   journal= "Information Design Journal",
   year = "1979",
   volume = "1",
   number = "4",
   pages = "232--241",
   doi = "https://doi.org/10.1075/idj.1.4.03bid",
   url = "https://www.jbe-platform.com/content/journals/10.1075/idj.1.4.03bid",
   publisher = "John Benjamins",
   issn = "0142-5471",
   type = "Journal Article",
   abstract = "Constraints of typography have led to the physical segregation of diagrammatic treatments of information from the linear alphanumerics that have dominated scientific communication. Information Design Journal reflects these constraints. The segregated and subordinated status of graphics is metaphorically compared here to the cultural roles of women and minorities. Aspects of graphics treated include the retardation of their intellectual development and influence; their relegation to relatively tangential, simplistic and ornamental functions; and prevalent suspicions regarding their deceptiveness. However, technology now makes it possible to reintegrate graphics into the mainstream of intellectual communication.",
  }

@inproceedings{bollacker2008freebase,
  title={Freebase: a collaboratively created graph database for structuring human knowledge},
  author={Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
  booktitle={Proceedings of the 2008 ACM SIGMOD international conference on Management of data},
  pages={1247--1250},
  year={2008}
}

@ARTICLE{borkin2015beyond,
  author={Borkin, Michelle A. and Bylinskii, Zoya and Kim, Nam Wook and Bainbridge, Constance May and Yeh, Chelsea S. and Borkin, Daniel and Pfister, Hanspeter and Oliva, Aude},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Beyond Memorability: Visualization Recognition and Recall}}, 
  year={2016},
  volume={22},
  number={1},
  pages={519--528},
  abstract={In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable “at-a-glance” are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.},
  keywords={},
  doi={10.1109/TVCG.2015.2467732},
  ISSN={1941-0506},
}

@article{borkin2013makes,
  title={What makes a visualization memorable?},
  author={Borkin, Michelle A and Vo, Azalea A and Bylinskii, Zoya and Isola, Phillip and Sunkavalli, Shashank and Oliva, Aude and Pfister, Hanspeter},
  journal={IEEE transactions on visualization and computer graphics},
  volume={19},
  number={12},
  pages={2306--2315},
  year={2013},
  publisher={IEEE}
}

@article{bouaricha1997algorithm,
  title={Algorithm 768: TENSOLVE: A software package for solving systems of nonlinear equations and nonlinear least-squares problems using tensor methods},
  author={Bouaricha, Ali and Schnabel, Robert B},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={23},
  number={2},
  pages={174--195},
  year={1997},
  publisher={ACM New York, NY, USA}
}

@article{brase2009pictorial,
  title={Pictorial representations in statistical reasoning},
  author={Brase, Gary L},
  journal={Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition},
  volume={23},
  number={3},
  pages={369--381},
  year={2009},
  publisher={Wiley Online Library}
}

@incollection{brooke1996sus,
  author      = "John Brooke",
  title       = "{SUS: A 'Quick and Dirty' Usability Scale}",
  booktitle   = "Usability Evaluation In Industry",
  publisher   = "Taylor \& Francis Group",
  address     = "London, UK",
  year        = 1996,
  pages       = "189--194",
  chapter     = 21,
  doi="10.1201/9781498710411-35"
}

@inproceedings{brown2020language,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {{Language Models Are Few-Shot Learners}},
year = {2020},
isbn = {9781713829546},
publisher = {159. Curran Associates Inc.},
address = {Red Hook, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
doi="10.48550/arXiv.2005.14165"
}

@inproceedings{carberry2006information,
author = {Carberry, Sandra and Elzer, Stephanie and Demir, Seniz},
title = {{Information Graphics: An Untapped Resource for Digital Libraries}},
year = {2006},
isbn = {1595933697},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/1148170.1148270},
doi = {10.1145/1148170.1148270},
abstract = {Information graphics are non-pictorial graphics such as bar charts and line graphs that depict attributes of entities and relations among entities. Most information graphics appearing in popular media have a communicative goal or intended message; consequently, information graphics constitute a form of language. This paper argues that information graphics are a valuable knowledge resource that should be retrievable from a digital library and that such graphics should be taken into account when summarizing a multimodal document for subsequent indexing and retrieval. But to accomplish this, the information graphic must be understood and its message recognized. The paper presents our Bayesian system for recognizing the primary message of one kind of information graphic (simple bar charts) and discusses the potential role of an information graphic's message in indexing graphics and summarizing multimodal documents.},
booktitle = {Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {581--588},
numpages = {8},
keywords = {graphics, multimedia, Bayesian reasoning, summarization},
location = {Seattle, Washington, USA},
}

@inproceedings{chang2012sutime,
    title = "{SUTime: A Library for Recognizing and Normalizing Time Expressions}",
    author = "Chang, Angel X.  and
      Manning, Christopher",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation",
    year = "2012",
    address="Paris, France",
    location = "Istanbul, Turkey",
    publisher = "ELRA",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf",
    pages = "3735--3740",
    abstract = "We describe SUTIME, a temporal tagger for recognizing and normalizing temporal expressions in English text. SUTIME is available as part of the Stanford CoreNLP pipeline and can be used to annotate documents with temporal information. It is a deterministic rule-based system designed for extensibility. Testing on the TempEval-2 evaluation corpus shows that this system outperforms state-of-the-art techniques.",
}

@article{chazal2011scalar,
  title={Scalar field analysis over point cloud data},
  author={Chazal, Fr{\'e}d{\'e}ric and Guibas, Leonidas J and Oudot, Steve Y and Skraba, Primoz},
  journal={Discrete \& Computational Geometry},
  volume={46},
  number={4},
  pages={743--775},
  year={2011},
  publisher={Springer}
}

@article{chazal2013persistence,
  title={Persistence-based clustering in Riemannian manifolds},
  author={Chazal, Fr{\'e}d{\'e}ric and Guibas, Leonidas J and Oudot, Steve Y and Skraba, Primoz},
  journal={Journal of the ACM (JACM)},
  volume={60},
  number={6},
  pages={1--38},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@misc{chen2021double,
  author =       "James Chen",
  year =         "2021",
  title =        "What are Double Bottom Patterns? What It Signals, Target, and Example",
  lastaccessed = "June 15, 2022",
  url =          "https://www.investopedia.com/terms/d/doublebottom.asp",
}

@inproceedings{chen2014fast,
    title = "{A Fast and Accurate Dependency Parser Using Neural Networks}",
    author = "Chen, Danqi  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
    year = "2014",
    address= "Stroudsburg, USA",
    location = "Doha, Qatar",
    publisher = "ACL",
    url = "https://aclanthology.org/D14-1082",
    doi = "10.3115/v1/D14-1082",
    pages = "740--750",
}

@inproceedings{chen2019neural,
author = {Chen, Charles and Zhang, Ruiyi and Kim, Sungchul and Cohen, Scott and Yu, Tong and Rossi, Ryan and Bunescu, Razvan},
title = {{Neural Caption Generation over Figures}},
year = {2019},
isbn = {9781450368698},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3341162.3345601},
doi = {10.1145/3341162.3345601},
abstract = {Figures are human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning. The goal is to automatically generate a natural language description of a given figure. We create a new dataset for figure captioning, FigCAP. To achieve accurate generation of labels in figures, we propose the Label Maps Attention Model. Extensive experiments show that our method outperforms the baselines. A successful solution to this task allows figure content to be accessible to those with visual impairment by providing input to a text-to-speech system; and enables automatic parsing of vast repositories of documents where figures are pervasive.},
booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
pages = {482--485},
numpages = {4},
keywords = {CNN, LSTM, figure captioning, neural networks},
location = {London, United Kingdom},
}

@article{chen2019figure,
      title={{Figure Captioning with Reasoning and Sequence-Level Training}}, 
      author={Charles Chen and Ruiyi Zhang and Eunyee Koh and Sungchul Kim and Scott Cohen and Tong Yu and Ryan Rossi and Razvan Bunescu},

  journal={ArXiv},
  year={2019},
  volume={abs/1906.02850},
  doi="10.48550/arXiv.1906.02850"
}

@article{cheng2022captions,
  title={{How Do Captions Affect Visualization Reading?}},
  author={Hanxiu 'Hazel' Zhu and Shelly Shiying Cheng and Eugene Wu},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.01263},
  doi="10.48550/arXiv.2205.01263"
}

@inproceedings{chen2022crossdata,
author = {Chen, Zhutian and Xia, Haijun},
title = {{CrossData: Leveraging Text-Data Connections for Authoring Data Documents}},
year = {2022},
isbn = {9781450391573},
publisher = {95. ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3491102.3517485},
doi = {10.1145/3491102.3517485},
abstract = {Data documents play a central role in recording, presenting, and disseminating data. Despite the proliferation of applications and systems designed to support the analysis, visualization, and communication of data, writing data documents remains a laborious process, requiring a constant back-and-forth between data processing and writing tools. Interviews with eight professionals revealed that their workflows contained numerous tedious, repetitive, and error-prone operations. The key issue that we identified is the lack of persistent connection between text and data. Thus, we developed CrossData, a prototype that treats text-data connections as persistent, interactive, first-class objects. By automatically identifying, establishing, and leveraging text-data connections, CrossData enables rich interactions to assist in the authoring of data documents. An expert evaluation with eight users demonstrated the usefulness of CrossData, showing that it not only reduced the manual effort in writing data documents but also opened new possibilities to bridge the gap between data exploration and writing.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {95},
numpages = {15},
keywords = {Interactive Article, Language-oriented Authoring, Text-based Editing, Data Document, Natural Language Processing},
location = {New Orleans, LA, USA},
}


@inproceedings{chung2022talebrush,
  title={TaleBrush: Sketching Stories with Generative Pretrained Language Models},
  author={Chung, John Joon Young and Kim, Wooseok and Yoo, Kang Min and Lee, Hwaran and Adar, Eytan and Chang, Minsuk},
  booktitle={CHI Conference on Human Factors in Computing Systems},
  pages={1--19},
  year={2022}
}

@book{cleveland1985elements,
  title={The elements of graphing data},
  author={Cleveland, William S},
  year={1985},
  publisher={Wadsworth Publ. Co.}
}

@article{cleveland1993model,
  title={A model for studying display methods of statistical graphics},
  author={Cleveland, William S},
  journal={Journal of Computational and Graphical Statistics},
  volume={2},
  number={4},
  pages={323--343},
  year={1993},
  publisher={Taylor \& Francis}
}

@article{cleveland1988shape,
author = { William S.   Cleveland  and  Marylyn E.   McGill  and  Robert   McGill },
title = {{The Shape Parameter of a Two-Variable Graph}},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {402},
pages = {289--300},
year  = {1988},
publisher = {Taylor \& Francis},
address={London, UK},
doi = {10.1080/01621459.1988.10478598},

URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478598
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478598
    

}

}


@book{cleveland1993visualizing,
  title={Visualizing data},
  author={Cleveland, William S},
  year={1993},
  publisher={Hobart press}
}

@misc{cnn2022cnn,
  author =       "CNN",
  year =         "2022",
  lastaccessed = "June 15, 2022",
  url =          "https://edition.cnn.com/",
}

@article{coenen2021wordcraft,
  title={Wordcraft: A human-AI collaborative editor for story writing},
  author={Coenen, Andy and Davis, Luke and Ippolito, Daphne and Reif, Emily and Yuan, Ann},
  journal={arXiv preprint arXiv:2107.07430},
  year={2021}
}


@article{cui2019datasite,
author = {Zhe Cui and Sriram Karthik Badam and M Adil Yalçin and Niklas Elmqvist},
title ={{DataSite: Proactive Visual Data Exploration with Computation of Insight-Based Recommendations}},
journal = {Information Visualization},
volume = {18},
number = {2},
pages = {251--267},
year = {2019},
doi = {10.1177/1473871618806555},

URL = { 
        https://doi.org/10.1177/1473871618806555
    
},
eprint = { 
        https://doi.org/10.1177/1473871618806555
    
}
,
    abstract = { Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well. }
}

@inproceedings{de2014universal,
  title={Universal Stanford dependencies: A cross-linguistic typology},
  author={De Marneffe, Marie-Catherine and Dozat, Timothy and Silveira, Natalia and Haverinen, Katri and Ginter, Filip and Nivre, Joakim and Manning, Christopher D},
  booktitle={Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)},
  pages={4585--4592},
  year={2014}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{{BERT}: Pre-Training of Deep Bidirectional Transformers for Language Understanding}",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    year = "2019",
    address="Stroudsburg, USA",
    location = "Minneapolis, Minnesota",
    publisher = "ACL",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{demiralp2017foresight,
  title={{Foresight: Rapid Data Exploration Through Guideposts}},
  author={Çagatay Demiralp and Peter J. Haas and Srinivasan Parthasarathy and Tejaswini Pedapati},
  journal={ArXiv},
  year={2017},
  volume={abs/1709.10513},
  doi="10.48550/arXiv.1709.10513"
}

@misc{distill2017distill,
  author =       "Distill",
  year =         "2017",
  lastaccessed = "June 15, 2022",
  url =          "https://distill.pub/",
}

@article{douglas1973algorithms,
author = {David H. Douglas and Thomas K. Peucker},
title = {{Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or Its Caricature}},
journal = {Cartographica: The International Journal for Geographic Information and Geovisualization},
volume = {10},
number = {2},
pages = {112--122},
year = {1973},
doi = {10.3138/FM57-6770-U75U-7727},

URL = { 
    
        https://doi.org/10.3138/FM57-6770-U75U-7727
    
    

},
eprint = { 
    
        https://doi.org/10.3138/FM57-6770-U75U-7727
    
    

}
,
    abstract = { All digitizing methods, as a general rule, record lines with far more data than is necessary for accurate graphic reproduction or for computer analysis. Two algorithms to reduce the number of points required to represent the line and, if desired, produce caricatures, are presented and compared with the most promising methods so far suggested. Line reduction will form a major part of automated generalization. }
}

@inproceedings{dragicevic2019increasing,
  title={Increasing the transparency of research papers with explorable multiverse analyses},
  author={Dragicevic, Pierre and Jansen, Yvonne and Sarma, Abhraneel and Kay, Matthew and Chevalier, Fanny},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--15},
  year={2019}
}

@article{elzer2011automated,
title = {{The Automated Understanding of Simple Bar Charts}},
journal = {Artificial Intelligence},
volume = {175},
number = {2},
pages = {526--555},
year = {2011},
issn = {0004-3702},
doi = {10.1016/j.artint.2010.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210001670},
author = {Stephanie Elzer and Sandra Carberry and Ingrid Zukerman},
keywords = {Natural language processing, Bayesian networks, Bar charts, Communicative signals},
abstract = {While identifying the intention of an utterance has played a major role in natural language understanding, this work is the first to extend intention recognition to the domain of information graphics. A tenet of this work is the belief that information graphics are a form of language. This is supported by the observation that the overwhelming majority of information graphics from popular media sources appear to have some underlying goal or intended message. As Clark noted, language is more than just words. It is any “signal” (or lack of signal when one is expected), where a signal is a deliberate action that is intended to convey a message (Clark, 1996 [15]). As a form of language, information graphics contain communicative signals that can be used in a computational system to identify the message that the graphic conveys. We identify the communicative signals that appear in simple bar charts, and present an implemented Bayesian network methodology for reasoning about these signals and hypothesizing a bar chart's intended message. Once the message conveyed by an information graphic has been inferred, it can then be used to facilitate access to this information resource for a variety of users, including 1) users of digital libraries, 2) visually impaired users, and 3) users of devices where graphics are impractical or inaccessible.}
}

@inproceedings{elzer2005exploring,
    title = "{Exploring and Exploiting the Limited Utility of Captions in Recognizing Intention in Information Graphics}",
    author = "Elzer, Stephanie  and
      Carberry, Sandra  and
      Chester, Daniel  and
      Demir, Seniz  and
      Green, Nancy  and
      Zukerman, Ingrid  and
      Trnka, Keith",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics",
    year = "2005",
    address="Stroudsburg, USA",
    location = "Ann Arbor, Michigan",
    publisher = "ACL",
    url = "https://aclanthology.org/P05-1028",
    doi = "10.3115/1219840.1219868",
    pages = "223--230",
}

@misc{earnest2011first,
  author =       "Earnest, Les",
  year =         "2011",
  title =        "{The First Three Spelling Checkers}",
  howpublished =          {Retrieved July 18, 2023 from \url{https://web.archive.org/web/20121022091418/http://www.stanford.edu/~learnest/spelling.pdf}}
}

@inproceedings{fasciano1996postgraphe,
    title = "{{P}ost{G}raphe: A System for the Generation of Statistical Graphics and Text}",
    author = "Fasciano, Massimo  and
      Lapalme, Guy",
    booktitle = "Eighth International Natural Language Generation Workshop",
    year = "1996",
    url = "https://aclanthology.org/W96-0406",
    address="Stroudsburg, USA",
    publisher = "ACL",
}

@misc{fed2022fed,
  author =       "Federal Reserve Board",
  year =         "2022",
  lastaccessed = "June 15, 2022",
  url =          "https://www.federalreserve.gov/",
}

@inproceedings{ferres2007improving,
  title={Improving accessibility to statistical graphs: the iGraph-Lite system},
  author={Ferres, Leo and Verkhogliad, Petro and Lindgaard, Gitte and Boucher, Louis and Chretien, Antoine and Lachance, Martin},
  booktitle={Proceedings of the 9th International ACM SIGACCESS Conference on Computers and Accessibility},
  pages={67--74},
  year={2007}
}

@inproceedings{finkel2005incorporating,
    title = "{Incorporating Non-Local Information into Information Extraction Systems by {G}ibbs Sampling}",
    author = "Finkel, Jenny Rose  and
      Grenager, Trond  and
      Manning, Christopher",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics",
    year = "2005",
    address="Stroudsburg, USA",
    location = "Ann Arbor, Michigan",
    publisher = "ACL",
    url = "https://aclanthology.org/P05-1045",
    doi = "10.3115/1219840.1219885",
    pages = "363--370",
}

@article{galletta2005does,
  title={Does spell-checking software need a warning label?},
  author={Galletta, Dennis F and Durcikova, Alexandra and Everard, Andrea and Jones, Brian M},
  journal={Communications of the ACM},
  volume={48},
  number={7},
  pages={82--86},
  year={2005},
  publisher={ACM New York, NY, USA}
}

@article{garcia2017designing,
  title={Designing visual aids that promote risk literacy: A systematic review of health research and evidence-based design heuristics},
  author={Garcia-Retamero, Rocio and Cokely, Edward T},
  journal={Human factors},
  volume={59},
  number={4},
  pages={582--627},
  year={2017},
  publisher={Sage Publications Sage CA: Los Angeles, CA}
}

@article{garcia2013visual,
  title={Visual representation of statistical information improves diagnostic inferences in doctors and their patients},
  author={Garcia-Retamero, Rocio and Hoffrage, Ulrich},
  journal={Social Science \& Medicine},
  volume={83},
  pages={27--33},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{goffin2015design,
  title={{Design Considerations for Enhancing Word-Scale Visualizations with Interaction}},
  author={Pascal Goffin and Wesley Willett and Jean-Daniel Fekete and Petra Isenberg},
  booktitle={Posters of the Conference on Information Visualization (InfoVis)},
  publisher={IEEE},
  address="New York, USA",
  year={2015}
}

@misc{google2022check,
  author =       "Google",
  year =         "2023",
  title =        "Check {Y}our {S}pelling \& {G}rammar in {G}oogle {D}ocs",
  howpublished =          {Retrieved July 18, 2023 from \url{https://support.google.com/docs/answer/57859?hl=en}}
}

@misc{google2022fix,
  author =       "Google",
  year =         "2023",
  title =        "Fix {S}pelling \& {G}rammar as {Y}ou {T}ype in {G}mail",
  howpublished = {Retrieved July 18, 2023 from \url{https://support.google.com/mail/answer/7987?hl=en}},
}

@misc{google2021word,
    title={Word {E}mbeddings {T}rained on {G}oogle {N}ews},
    note={Retrieved September 2, 2021 from https://code.google.com/archive/p/word2vec/},
    year={2021}
}

@misc{google2022docs,
  author =       "Google {D}ocs",
  year =         "2022",
  lastaccessed = "2023",
  url =          "https://www.google.com/docs/about/",
}

@misc{google2023meet,
  author =       "{Google {M}eet}",
  year =         "2023",
  howpublished = {Retrieved July 18, 2023 from \url{https://meet.google.com/}},
}

@misc{google2022use,
  author =       "Google",
  year =         "2023",
  title =        "Use {S}mart {C}ompose and {S}mart {R}eply",
  howpublished = {Retrieved July 18, 2023 from \url{https://support.google.com/docs/answer/9643962?hl=en}},
}

@misc{google2022using,
  author =       "Google",
  year =         "2023",
  title =        "Use {S}mart {C}ompose",
  howpublished = {Retrieved July 18, 2023 from \url{https://support.google.com/mail/answer/9116836?hl=en}},
}

@misc{grammarly2022grammarly,
  author =       "Grammarly",
  year =         "2023",
  howpublished = {Retrieved July 18, 2023 from \url{https://www.grammarly.com}}
}

@incollection{gould1976looking,
  author      = "Gould, John D",
  title       = "{Looking at Pictures}",
  editor      = "Richard A. Monty and John W. Senders",
  booktitle   = "Eye Movements and Psychological Processes",
  publisher   = "Lawrence Erlbaum Associates, Inc.",
  address     = "Hillsdale, USA",
  year        = 1976,
  pages       = "323--345",
  chapter     = "5.2"
}

@inproceedings{govindaraju2013understanding,
  title={Understanding tables in context using standard NLP toolkits},
  author={Govindaraju, Vidhya and Zhang, Ce and R{\'e}, Christopher},
  booktitle={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={658--664},
  year={2013}
}

@inproceedings{head2021augmenting,
  title={Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols},
  author={Head, Andrew and Lo, Kyle and Kang, Dongyeop and Fok, Raymond and Skjonsberg, Sam and Weld, Daniel S and Hearst, Marti A},
  booktitle={Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--18},
  year={2021}
}

@article{heer2006multi,
  title={Multi-scale banking to 45 degrees},
  author={Heer, Jeffrey and Agrawala, Maneesh},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={12},
  number={5},
  pages={701--708},
  year={2006},
  publisher={IEEE}
}

@article{hershberger1992speeding,
  title={Speeding up the Douglas-Peucker line-simplification algorithm},
  author={Hershberger, John Edward and Snoeyink, Jack},
  year={1992},
  publisher={Citeseer}
}

@article{hoffman1997salience,
  title={Salience of visual parts},
  author={Hoffman, Donald D and Singh, Manish},
  journal={Cognition},
  volume={63},
  number={1},
  pages={29--78},
  year={1997},
  publisher={Elsevier}
}

@article{hsu2021scicap,
  title={Scicap: Generating captions for scientific figures},
  author={Hsu, Ting-Yao and Giles, C Lee and Huang, Ting-Hao'Kenneth'},
  journal={arXiv preprint arXiv:2110.11624},
  year={2021}
}

@inproceedings{hu2018dive,
  title={DIVE: A mixed-initiative system supporting integrated data exploration workflows},
  author={Hu, Kevin and Orghian, Diana and Hidalgo, C{\'e}sar},
  booktitle={Proceedings of the workshop on human-in-the-loop data analytics},
  pages={1--7},
  year={2018}
}

@inproceedings{hullman2013contextifier,
    author = {Hullman, Jessica and Diakopoulos, Nicholas and Adar, Eytan},
    title = {{Contextifier: Automatic Generation of Annotated Stock Visualizations}},
    year = {2013},
    isbn = {9781450318990},
    publisher = {ACM},
    address = {New York, USA},
    url = {https://doi.org/10.1145/2470654.2481374},
    doi = {10.1145/2470654.2481374},
    abstract = {Online news tools - for aggregation, summarization and automatic generation - are an area of fruitful development as reading news online becomes increasingly commonplace. While textual tools have dominated these developments, annotated information visualizations are a promising way to complement articles based on their ability to add context. But the manual effort required for professional designers to create thoughtful annotations for contextualizing news visualizations is difficult to scale. We describe the design of Contextifier, a novel system that automatically produces custom, annotated visualizations of stock behavior given a news article about a company. Contextifier's algorithms for choosing annotations is informed by a study of professionally created visualizations and takes into account visual salience, contextual relevance, and a detection of key events in the company's history. In evaluating our system we find that Contextifier better balances graphical salience and relevance than the baseline.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    pages = {2707--2716},
    numpages = {10},
    keywords = {time series, annotation, information visualization},
    location = {Paris, France},
}

@misc{imf2023imf,
  author =       "{International Monetary Fund}",
  year =         "2023",
  howpublished = {Retrieved July 18, 2023 from \url{https://www.imf.org/en/Home}}
}

@article{itti1998model,
  title={A model of saliency-based visual attention for rapid scene analysis},
  author={Itti, Laurent and Koch, Christof and Niebur, Ernst},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={20},
  number={11},
  pages={1254--1259},
  year={1998},
  publisher={Ieee}
}

@article{jhamtani2021truth,
  title={Truth-Conditional Captioning of Time Series Data},
  author={Jhamtani, Harsh and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2110.01839},
  year={2021}
}

@article{jugel2014m4,
author = {Jugel, Uwe and Jerzak, Zbigniew and Hackenbroich, Gregor and Markl, Volker},
title = {{M4: A Visualization-Oriented Time Series Data Aggregation}},
year = {2014},
issue_date = {June 2014},
publisher = {VLDB Endowment},
volume = {7},
number = {10},
issn = {2150--8097},
url = {https://doi.org/10.14778/2732951.2732953},
doi = {10.14778/2732951.2732953},
abstract = {Visual analysis of high-volume time series data is ubiquitous in many industries, including finance, banking, and discrete manufacturing. Contemporary, RDBMS-based systems for visualization of high-volume time series data have difficulty to cope with the hard latency requirements and high ingestion rates of interactive visualizations. Existing solutions for lowering the volume of time series data disregard the semantics of visualizations and result in visualization errors.In this work, we introduce M4, an aggregation-based time series dimensionality reduction technique that provides error-free visualizations at high data reduction rates. Focusing on line charts, as the predominant form of time series visualization, we explain in detail the drawbacks of existing data reduction techniques and how our approach outperforms state of the art, by respecting the process of line rasterization.We describe how to incorporate aggregation-based dimensionality reduction at the query level in a visualization-driven query rewriting system. Our approach is generic and applicable to any visualization system that uses an RDBMS as data source. Using real world data sets from high tech manufacturing, stock markets, and sports analytics domains we demonstrate that our visualization-oriented data aggregation can reduce data volumes by up to two orders of magnitude, while preserving perfect visualizations.},
journal = {Proceedings of the VLDB Endowment},
pages = {797–808},
numpages = {12},
keywords = {dimensionality reduction, query rewriting, line rasterization, relational databases}
}


@article{keogh2001dimensionality,
	title = {{Dimensionality {Reduction} for {Fast} {Similarity} {Search} in {Large} {Time} {Series} {Databases}}},
	volume = {3},
	issn = {0219-1377},
	url = {https://doi.org/10.1007/PL00011669},
	doi = {10.1007/PL00011669},
	abstract = {The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The most promising solutions involve first performing dimensionality reduction on the data, and then indexing the reduced data with a spatial access method. Three major dimensionality reduction techniques have been proposed: Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Piecewise Aggregate Approximation (PAA). We theoretically and empirically compare it to the other techniques and demonstrate its superiority. In addition to being competitive with or faster than the other methods, our approach has numerous other advantages. It is simple to understand and to implement, it allows more flexible distance measures, including weighted Euclidean queries, and the index can be built in linear time.},
	number = {3},
	journal = {Knowledge and Information Systems},
	author = {Keogh, Eamonn and Chakrabarti, Kaushik and Pazzani, Michael and Mehrotra, Sharad},
	year = {2001},
	pages = {263--286},
}

@article{khan2015benefits,
title = {{Benefits of Visualization in the Mammography Problem}},
journal = {International Journal of Human-Computer Studies},
volume = {83},
pages = {94--113},
year = {2015},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2015.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1071581915001081},
author = {Azam Khan and Simon Breslav and Michael Glueck and Kasper Hornbæk},
keywords = {Bayesian reasoning, Decision making, Comparability criteria, Visualization, Crowdsourcing, Mammography Problem},
abstract = {Trying to make a decision between two outcomes, when there is some level of uncertainty, is inherently difficult because it involves probabilistic reasoning. Previous studies have shown that most people do not correctly apply Bayesian inference to solve probabilistic problems for decision making under uncertainty. In an effort to improve decision making with Bayesian problems, previous work has studied supplementing the textual description of problems with visualizations, such as graphs and charts. However, results have been varied and generally indicate that visualization is not an effective technique. As these studies were performed over many years with a variety of goals and experimental conditions, we sought to re-evaluate the use of visualization as an aid in solving Bayesian problems. Many of these studies used the classic Mammography Problem with visualizations portraying the problem structure, the quantities involved, or the nested-set relations of the populations involved. We selected three representative visualizations from this work and developed two hybrid visualizations, combining structure types and frequency with structure. We also included a text-only baseline condition and a text-legend condition where all nested-set problem values were given to eliminate the need for participants to estimate or calculate values. Seven hundred participants evaluated these seven conditions on the classic Mammography Problem in a crowdsourcing system, where micro-interaction data was collected from the participants. Our analysis of the user input and of the results indicates that participants made use of the visualizations but that the visualizations did not help participants to perform more accurately. Overall, static visualizations do not seem to aid a majority of people in solving the Mammography Problem.}
}

@inproceedings{kim2020answering,
  title={Answering questions about charts and generating visual explanations},
  author={Kim, Dae Hyun and Hoque, Enamul and Agrawala, Maneesh},
  booktitle={Proceedings of the 2020 CHI conference on human factors in computing systems},
  pages={1--13},
  year={2020}
}

@inproceedings{kim2018facilitating,
author = {Kim, Dae Hyun and Hoque, Enamul and Kim, Juho and Agrawala, Maneesh},
title = {{Facilitating Document Reading by Linking Text and Tables}},
year = {2018},
isbn = {9781450359481},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3242587.3242617},
doi = {10.1145/3242587.3242617},
abstract = {Document authors commonly use tables to support arguments presented in the text. But, because tables are usually separate from the main body text, readers must split their attention between different parts of the document. We present an interactive document reader that automatically links document text with corresponding table cells. Readers can select a sentence (or tables cells) and our reader highlights the relevant table cells (or sentences). We provide an automatic pipeline for extracting such references between sentence text and table cells for existing PDF documents that combines structural analysis of tables with natural language processing and rule-based matching. On a test corpus of 330 (sentence, table) pairs, our pipeline correctly extracts 48.8\% of the references. An additional 30.5\% contain only false negatives (FN) errors -- the reference is missing table cells. The remaining 20.7\% contain false positives (FP) errors -- the reference includes extraneous table cells and could therefore mislead readers. A user study finds that despite such errors, our interactive document reader helps readers match sentences with corresponding table cells more accurately and quickly than a baseline document reader.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology},
pages = {423--434},
numpages = {12},
keywords = {text analysis, interactive documents, visualization},
location = {Berlin, Germany},
}

@inproceedings{kim2021towards,
author = {Kim, Dae Hyun and Setlur, Vidya and Agrawala, Maneesh},
title = {{Towards Understanding How Readers Integrate Charts and Captions: A Case Study with Line Charts}},
year = {2021},
isbn = {9781450380966},
publisher = {610. ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3411764.3445443},
doi = {10.1145/3411764.3445443},
abstract = {Charts often contain visually prominent features that draw attention to aspects of the data and include text captions that emphasize aspects of the data. Through a crowdsourced study, we explore how readers gather takeaways when considering charts and captions together. We first ask participants to mark visually prominent regions in a set of line charts. We then generate text captions based on the prominent features and ask participants to report their takeaways after observing chart-caption pairs. We find that when both the chart and caption describe a high-prominence feature, readers treat the doubly emphasized high-prominence feature as the takeaway; when the caption describes a low-prominence chart feature, readers rely on the chart and report a higher-prominence feature as the takeaway. We also find that external information that provides context, helps further convey the caption’s message to the reader. We use these findings to provide guidelines for authoring effective chart-caption pairs.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
numpages = {11},
keywords = {takeaways., visually prominent features, Captions, line charts},
location = {Yokohama, Japan},
}

@inproceedings{kong2009perceptual,
  title={Perceptual interpretation of ink annotations on line charts},
  author={Kong, Nicholas and Agrawala, Maneesh},
  booktitle={Proceedings of the 22nd annual ACM symposium on User interface software and technology},
  pages={233--236},
  year={2009}
}

@article{kong2012graphical,
  title={Graphical overlays: Using layered elements to aid chart reading},
  author={Kong, Nicholas and Agrawala, Maneesh},
  journal={IEEE transactions on visualization and computer graphics},
  volume={18},
  number={12},
  pages={2631--2638},
  year={2012},
  publisher={IEEE}
}

@inproceedings{kong2014extracting,
author = {Kong, Nicholas and Hearst, Marti A. and Agrawala, Maneesh},
title = {{Extracting References between Text and Charts via Crowdsourcing}},
year = {2014},
isbn = {9781450324731},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/2556288.2557241},
doi = {10.1145/2556288.2557241},
abstract = {News articles, reports, blog posts and academic papers often include graphical charts that serve to visually reinforce arguments presented in the text. To help readers better understand the relation between the text and the chart, we present a crowdsourcing pipeline to extract the references between them. Specifically, we give crowd workers paragraph-chart pairs and ask them to select text phrases as well as the corresponding visual marks in the chart. We then apply automated clustering and merging techniques to unify the references generated by multiple workers into a single set. Comparing the crowdsourced references to a set of gold standard references using a distance measure based on the F1 score, we find that the average distance between the raw set of references produced by a single worker and the gold standard is 0.54 (out of a max of 1.0). When we apply clustering and merging techniques the average distance between the unified set of references and the gold standard reduces to 0.39; an improvement of 27\%. We conclude with an interactive document viewing application that uses the extracted references; readers can select phrases in the text and the system highlights the related marks in the chart.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {31--40},
numpages = {10},
keywords = {crowdsourcing, visualization, interactive documents},
location = {Toronto, Ontario, Canada},
}

@inproceedings{kong2018frames,
author = {Kong, Ha-Kyung and Liu, Zhicheng and Karahalios, Karrie},
title = {{Frames and Slants in Titles of Visualizations on Controversial Topics}},
year = {2018},
isbn = {9781450356206},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3173574.3174012},
doi = {10.1145/3173574.3174012},
abstract = {Slanted framing in news article titles induce bias and influence recall. While recent studies found that viewers focus extensively on titles when reading visualizations, the impact of titles in visualization remains underexplored. We study frames in visualization titles, and how the slanted framing of titles and the viewer's pre-existing attitude impact recall, perception of bias, and change of attitude. When asked to compose visualization titles, people used five existing news frames, an open-ended frame, and a statistics frame. We found that the slant of the title influenced the perceived main message of a visualization, with viewers deriving opposing messages from the same visualization. The results did not show any significant effect on attitude change. We highlight the danger of subtle statistics frames and viewers' unwarranted conviction of the neutrality of visualizations. Finally, we present a design implication for the generation of visualization titles and one for the viewing of titles.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1--12},
numpages = {12},
keywords = {attitude change, visualization title, data visualization, frames, bias},
location = {Montreal QC, Canada},
}

@inproceedings{kong2019trust,
author = {Kong, Ha-Kyung and Liu, Zhicheng and Karahalios, Karrie},
title = {{Trust and Recall of Information across Varying Degrees of Title-Visualization Misalignment}},
year = {2019},
isbn = {9781450359702},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3290605.3300576},
doi = {10.1145/3290605.3300576},
abstract = {Visualizations are emerging as a means of spreading digital misinformation. Prior work has shown that visualization interpretation can be manipulated through slanted titles that favor only one side of the visual story, yet people still think the visualization is impartial. In this work, we study whether such effects continue to exist when titles and visualizations exhibit greater degrees of misalignment: titles whose message differs from the visually cued message in the visualization, and titles whose message contradicts the visualization. We found that although titles with a contradictory slant triggered more people to identify bias compared to titles with a miscued slant, visualizations were persistently perceived as impartial by the majority. Further, people's recall of the visualization's message more frequently aligned with the titles than the visualization. Based on these results, we discuss the potential of leveraging textual components to detect and combat visual-based misinformation with text-based slants.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1--13},
numpages = {13},
keywords = {misinformation, visualization title, confirmation bias},
location = {Glasgow, Scotland Uk},
}

@inproceedings{lai2020automatic,
author = {Lai, Chufan and Lin, Zhixian and Jiang, Ruike and Han, Yun and Liu, Can and Yuan, Xiaoru},
title = {{Automatic Annotation Synchronizing with Textual Description for Visualization}},
year = {2020},
isbn = {9781450367080},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3313831.3376443},
doi = {10.1145/3313831.3376443},
abstract = {In this paper, we propose a technique for automatically annotating visualizations according to the textual description. In our approach, visual elements in the target visualization, along with their visual properties, are identified and extracted with a Mask R-CNN model. Meanwhile, the description is parsed to generate visual search requests. Based on the identification results and search requests, each descriptive sentence is displayed beside the described focal areas as annotations. Different sentences are presented in various scenes of the generated animation to promote a vivid step-by-step presentation. With a user-customized style, the animation can guide the audience's attention via proper highlighting such as emphasizing specific features or isolating part of the data. We demonstrate the utility and usability of our method through a user study with use cases.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1--13},
numpages = {13},
keywords = {annotation, machine learning, visualization, natural language interface},
location = {Honolulu, HI, USA},
}

@misc{language2022language,
  author =       "LanguageTool",
  year =         "2023",
  howpublished = {Retrieved July 18, 2023 from \url{https://languagetool.org/}}
}

@inproceedings{latif2018exploring,
author = {Latif, Shahid and Liu, Diao and Beck, Fabian},
title = {{Exploring Interactive Linking between Text and Visualization}},
year = {2018},
publisher = {The EG Association},
address = {Eindhoven, Netherlands},
abstract = {Visualizations are included in documents as augmentation to text and they become more intuitive if readers have the ability to interact with them. Modern web technologies facilitate the development of interactive documents including both text and visualizations. The aim of this research it to explore the design space of possible visualization-text linking and interactions based on various triggers such as mouse events. We describe a framework that takes text containing markup, a related dataset, and a configuration file as inputs and produces an interactive document. The resulting document provides interactions such as details on demand, visual highlighting and comparison, and bushing-and-linking. In addition to regular sized graphics, the use of word-sized graphics or sparklines presents related content in view-focus of the reader. Finally, an illustrative example is presented to showcase the approach.},
booktitle = {Proceedings of the 2018 Eurographics/IEEE VGTC Conference on Visualization: Short Papers},
pages = {91--94},
numpages = {4},
location = {Brno, Czech Republic},
doi="10.2312/eurovisshort.20181084"
}

@ARTICLE{latif2021kori,
  author={Latif, Shahid and Zhou, Zheng and Kim, Yoon and Beck, Fabian and Kim, Nam Wook},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Kori: Interactive Synthesis of Text and Charts in Data Documents}}, 
  year={2022},
  volume={28},
  number={1},
  pages={184--194},
  abstract={Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.},
  keywords={},
  doi={10.1109/TVCG.2021.3114802},
  ISSN={1941-0506},}

@inproceedings{li2018extracting,
  title={Extracting figures and captions from scientific publications},
  author={Li, Pengyuan and Jiang, Xiangying and Shatkay, Hagit},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={1595--1598},
  year={2018}
}

@misc{libreoffice2022checking,
  author =       "{LibreOffice}",
  year =         "2023",
  title =        "{Checking Spelling and Grammar}",
  howpublished = {Retrieved July 18, 2023 from \url{https://help.libreoffice.org/6.2/en-US/text/swriter/guide/spellcheck_dialog.html}}
}

@inproceedings{lin2018vizbywiki,
author = {Lin, Allen Yilun and Ford, Joshua and Adar, Eytan and Hecht, Brent},
title = {{VizByWiki: Mining Data Visualizations from the Web to Enrich News Articles}},
year = {2018},
isbn = {9781450356398},
publisher = {IW3C2},
address = {Geneva, Switzerland},
url = {https://doi.org/10.1145/3178876.3186135},
doi = {10.1145/3178876.3186135},
abstract = {Data visualizations in news articles (e.g., maps, line graphs, bar charts) greatly enrich the content of news articles and result in well-established improvements to reader comprehension. However, existing systems that generate news data visualiza-tions either require substantial manual effort or are limited to very specific types of data visualizations, thereby greatly re-stricting the number of news articles that can be enhanced. To address this issue, we define a new problem: given a news ar-ticle, retrieve relevant visualizations that already exist on the web. We show that this problem is tractable through a new system, VizByWiki, that mines contextually relevant data visualizations from Wikimedia Commons, the central file reposi-tory for Wikipedia. Using a novel ground truth dataset, we show that VizByWiki can successfully augment as many as 48\% of popular online news articles with news visualizations. We also demonstrate that VizByWiki can automatically rank visualizations according to their usefulness with reasonable accuracy (nDCG@5 of 0.82). To facilitate further advances on our "news visualization retrieval problem", we release our ground truth dataset and make our system and its source code publicly available.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {873--882},
numpages = {10},
keywords = {peer production, news articles, wikimedia commons, data visualizations, wikipedia, user-generated content},
location = {Lyon, France},
}

@ARTICLE{lundgard2021accessible,
  author={Lundgard, Alan and Satyanarayan, Arvind},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content}}, 
  year={2022},
  volume={28},
  number={1},
  pages={1073-1083},
  doi={10.1109/TVCG.2021.3114770}}

@article{macdonald1982writer,
  title={The writer's workbench: Computer aids for text analysis},
  author={Macdonald, Nina and Frase, Lawrence and Gingrich, P and Keenan, Stacey},
  journal={IEEE Transactions on Communications},
  volume={30},
  number={1},
  pages={105--110},
  year={1982},
  publisher={IEEE}
}

@inproceedings{manning2014stanford,
    title = "{The {S}tanford {C}ore{NLP} Natural Language Processing Toolkit}",
    author = "Manning, Christopher  and
      Surdeanu, Mihai  and
      Bauer, John  and
      Finkel, Jenny  and
      Bethard, Steven  and
      McClosky, David",
    booktitle = "Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    year = "2014",
    address= "Stroudsburg, USA",
    location = "Baltimore, Maryland",
    publisher = "ACL",
    url = "https://aclanthology.org/P14-5010",
    doi = "10.3115/v1/P14-5010",
    pages = "55--60",
}

@article{matzen2017data,
  title={Data visualization saliency model: A tool for evaluating abstract data visualizations},
  author={Matzen, Laura E and Haass, Michael J and Divis, Kristin M and Wang, Zhiyuan and Wilson, Andrew T},
  journal={IEEE transactions on visualization and computer graphics},
  volume={24},
  number={1},
  pages={563--573},
  year={2017},
  publisher={IEEE}
}

@article{mendhurwar2017discriminative,
  title={The discriminative power of shape an empirical study in time series matching},
  author={Mendhurwar, Kaustubha and Gu, Qing and Mudur, Sudhir and Popa, Tiberiu},
  journal={IEEE Transactions on Visualization and Computer Graphics},
  volume={24},
  number={5},
  pages={1799--1813},
  year={2017},
  publisher={IEEE}
}

@misc{merriam2022thesaurus,
  author =       "Merriam-Webster",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.merriam-webster.com}",
}

@inproceedings{meratnia2004spatiotemporal,
  title={Spatiotemporal compression techniques for moving point objects},
  author={Meratnia, Nirvana and others},
  booktitle={International Conference on Extending Database Technology},
  pages={765--782},
  year={2004},
  organization={Springer}
}

@ARTICLE{micallef2012assessing,
  author={Micallef, Luana and Dragicevic, Pierre and Fekete, Jean-Daniel},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Assessing the Effect of Visualizations on Bayesian Reasoning through Crowdsourcing}}, 
  year={2012},
  volume={18},
  number={12},
  pages={2536-2545},
  abstract={People have difficulty understanding statistical information and are unaware of their wrong judgments, particularly in Bayesian reasoning. Psychology studies suggest that the way Bayesian problems are represented can impact comprehension, but few visual designs have been evaluated and only populations with a specific background have been involved. In this study, a textual and six visual representations for three classic problems were compared using a diverse subject pool through crowdsourcing. Visualizations included area-proportional Euler diagrams, glyph representations, and hybrid diagrams combining both. Our study failed to replicate previous findings in that subjects' accuracy was remarkably lower and visualizations exhibited no measurable benefit. A second experiment confirmed that simply adding a visualization to a textual Bayesian problem is of little help, even when the text refers to the visualization, but suggests that visualizations are more effective when the text is given without numerical values. We discuss our findings and the need for more such experiments to be carried out on heterogeneous populations of non-experts.},
  keywords={},
  doi={10.1109/TVCG.2012.199},
  ISSN={1941-0506},
}


@misc{microsoft2022checking,
  author =       "Microsoft",
  year =         "2023",
  title =        "{Editor Settings in Outlook.com and Outlook on the Web}",
  howpublished = "Retrieved July 18, 2023 from \url{https://support.microsoft.com/en-us/office/editor-settings-in-outlook-com-and-outlook-on-the-web-c6b1283d-81a2-47f1-bc85-9e8dfc0cbf15}",
}

@misc{microsoft2022word,
  author =       "Microsoft {W}ord",
  year =         "2022",
  lastaccessed = "2023",
  url =          "http://products.office.com/word",
}

@misc{microsoft2022spelling,
  author =       "Microsoft",
  year =         "2023",
  title =        "{Check Grammar, Spelling, and More in Word}",
  howpublished = "Retrieved July 18, 2023 from \url{https://support.microsoft.com/en-gb/office/check-grammar-spelling-and-more-in-word-0f43bf32-ccde-40c5-b16a-c6a282c0d251}",
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@incollection{mitkov2014anaphora2,
  author      = "John Brooke",
  title       = "{SUS: A 'Quick and Dirty' Usability Scale}",
  editor      = "Patrick W. Jordan and Bruce Thomas and Bernard Weerdmeester and Ian Lyall McClelland",
  booktitle   = "Usability Evaluation In Industry",
  publisher   = "Taylor \& Francis Group",
  address     = "London, UK",
  year        = 1996,
  pages       = "189-194",
  chapter     = 21,
  doi="10.1201/9781498710411-35"
}
@book{mitkov2014anaphora,
  title={{Anaphora Resolution}},
  author={Ruslan Mitkov},
  publisher   = "Taylor \& Francis Group",
  address     = "London, UK",
  year={2014},
  isbn="9781315840086",
  doi="10.4324/9781315840086"
}

@inproceedings{mittal1995generating,
  title={Generating explanatory captions for information graphics},
  author={Mittal, Vibhu and Roth, Steven and Moore, Johanna D and Mattis, Joe and Carenini, Giuseppe},  
  booktitle={Proceeedings of the International Joint Conference on Artificial Intelligence},
  year={1995},
  pages={1276--1283}
}

@inproceedings{muckell2010algorithms,
  title={Algorithms for compressing GPS trajectory data: an empirical evaluation},
  author={Muckell, Jonathan and Hwang, Jeong-Hyon and Lawson, Catherine T and Ravi, SS},
  booktitle={Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems},
  pages={402--405},
  year={2010}
}

@article{muckell2014compression,
  title={Compression of trajectory data: a comprehensive evaluation and new approach},
  author={Muckell, Jonathan and Olsen, Paul W and Hwang, Jeong-Hyon and Lawson, Catherine T and Ravi, SS},
  journal={GeoInformatica},
  volume={18},
  number={3},
  pages={435--460},
  year={2014},
  publisher={Springer}
}

@article{mudelsee2009break,
  title={Break function regression},
  author={Mudelsee, Manfred},
  journal={The European Physical Journal Special Topics},
  volume={174},
  number={1},
  pages={49--63},
  year={2009},
  publisher={Springer}
}

@article{mudelsee2019trend,
  title={Trend analysis of climate time series: A review of methods},
  author={Mudelsee, Manfred},
  journal={Earth-science reviews},
  volume={190},
  pages={310--322},
  year={2019},
  publisher={Elsevier}
}

@article{muggeo2003estimating,
  title={Estimating regression models with unknown break-points},
  author={Muggeo, Vito MR},
  journal={Statistics in medicine},
  volume={22},
  number={19},
  pages={3055--3071},
  year={2003},
  publisher={Wiley Online Library}
}

@misc{springer2023nature,
  author =       "{Springer Nature}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.nature.com/}",
}

@misc{nyt2022nyt,
  author =       "{The New York Times}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.nytimes.com/}",
}

@inproceedings{obeid2020chart,
    title = "{Chart-to-Text: Generating Natural Language Descriptions for Charts by Adapting the Transformer Model}",
    author = "Obeid, Jason  and
      Hoque, Enamul",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    year = "2020",
    address="Stroudsburg, USA",
    location = "Dublin, Ireland",
    publisher = "ACL",
    url = "https://aclanthology.org/2020.inlg-1.20",
    pages = "138--147",
    abstract = "Information visualizations such as bar charts and line charts are very popular for exploring data and communicating insights. Interpreting and making sense of such visualizations can be challenging for some people, such as those who are visually impaired or have low visualization literacy. In this work, we introduce a new dataset and present a neural model for automatically generating natural language summaries for charts. The generated summaries provide an interpretation of the chart and convey the key insights found within that chart. Our neural model is developed by extending the state-of-the-art model for the data-to-text generation task, which utilizes a transformer-based encoder-decoder architecture. We found that our approach outperforms the base model on a content selection metric by a wide margin (55.42{\%} vs. 8.49{\%}) and generates more informative, concise, and coherent summaries.",
}

@inproceedings {ottley2019curious,
booktitle = {Proceedings of the 2019 Eurographics/IEEE VGTC Conference on Visualization: Short Papers},
title = {{The Curious Case of Combining Text and Visualization}},
author = {Ottley, Alvitta and Kaszowska, Aleksandra and Crouser, R. Jordan and Peck, Evan M.},
year = {2019},
publisher = {The EG Association},
address={Eindhoven, Netherlands},
ISBN = {978-3-03868-090-1},
DOI = {10.2312/evs.20191181}
}

@inproceedings {ottley2019curious-long,
booktitle = {EuroVis 2019 - Short Papers},
editor = {Johansson, Jimmy and Sadlo, Filip and Marai, G. Elisabeta},
title = {{The Curious Case of Combining Text and Visualization}},
author = {Ottley, Alvitta and Kaszowska, Aleksandra and Crouser, R. Jordan and Peck, Evan M.},
year = {2019},
publisher = {The Eurographics Association},
location={Eindhoven, Netherlands},
ISBN = {978-3-03868-090-1},
DOI = {10.2312/evs.20191181}
}

@incollection{ottley2012visually,
  title={{Visually Communicating Bayesian Statistics to Laypersons}},
  author={Ottley, Alvitta and Metevier, Blossom and Han, PK and Chang, Remco},
  year=2012,
  booktitle = {Technical Report 2012-02},
  publisher={Tufts University},
  address={Medford, USA},
  howpublished = "Retrieved Jul 18, 2023 from \url{https://www.cs.tufts.edu/t/tr/techreps/TR-2012-02}"
}

@ARTICLE{ottley2015improving,
  author={Ottley, Alvitta and Peck, Evan M. and Harrison, Lane T. and Afergan, Daniel and Ziemkiewicz, Caroline and Taylor, Holly A. and Han, Paul K. J. and Chang, Remco},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability}}, 
  year={2016},
  volume={22},
  number={1},
  pages={529--538},
  abstract={Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.},
  keywords={},
  doi={10.1109/TVCG.2015.2467758},
  ISSN={1941-0506},
}

@misc{pew2022pew,
  author =       "{Pew Research Center}",
  year =         "2023",
  howpublished = "Retrieved June 18, 2023 from \url{https://www.pewresearch.org/}",
}

@article{pinheiro2022charttext,
  title={{ChartText: Linking Text with Charts in Documents}},
  author={Pinheiro, Joao and Poco, Jorge},
  journal={ArXiv},
  year={2022},
  volume={abs/2201.05043},
  doi="10.48550/arXiv.2201.05043"
}

@article{poco2017reverse,
author = {Poco, Jorge and Heer, Jeffrey},
title = {{Reverse-Engineering Visualizations: Recovering Visual Encodings from Chart Images}},
year = {2017},
issue_date = {June 2017},
publisher = {The Eurographs Association \& John Wiley \& Sons, Ltd.},
address = {Chichester, UK},
volume = {36},
number = {3},
issn = {0167-7055},
url = {https://doi.org/10.1111/cgf.13193},
doi = {10.1111/cgf.13193},
abstract = {We investigate how to automatically recover visual encodings from a chart image, primarily using inferred text elements. We contribute an end-to-end pipeline which takes a bitmap image as input and returns a visual encoding specification as output. We present a text analysis pipeline which detects text elements in a chart, classifies their role e.g., chart title, x-axis label, y-axis title, etc., and recovers the text content using optical character recognition. We also train a Convolutional Neural Network for mark type classification. Using the identified text elements and graphical mark type, we can then infer the encoding specification of an input chart image. We evaluate our techniques on three chart corpora: a set of automatically labeled charts generated using Vega, charts from the Quartz news website, and charts extracted from academic papers. We demonstrate accurate automatic inference of text elements, mark types, and chart specifications across a variety of input chart types.},
journal = {Computer Graphics Forum},
pages = {353--363},
numpages = {11}
}

@inproceedings{qian2021generating,
author = {Qian, Xin and Koh, Eunyee and Du, Fan and Kim, Sungchul and Chan, Joel and Rossi, Ryan A. and Malik, Sana and Lee, Tak Yeon},
title = {{Generating Accurate Caption Units for Figure Captioning}},
year = {2021},
isbn = {9781450383127},
publisher = {ACM},
address = {New York, USA},
url = {https://doi.org/10.1145/3442381.3449923},
doi = {10.1145/3442381.3449923},
abstract = {Scientific-style figures are commonly used on the web to present numerical information. Captions that tell accurate figure information and sound natural would significantly improve figure accessibility. In this paper, we present promising results on machine figure captioning. A recent corpus analysis of real-world captions reveals that machine figure captioning systems should start by generating accurate caption units. We formulate the caption unit generation problem as a controlled captioning problem. Given a caption unit type as a control signal, a model generates an accurate caption unit of that type. As a proof-of-concept on single bar charts, we propose a model, FigJAM, that achieves this goal through utilizing metadata information and a joint static and dynamic dictionary. Quantitative evaluations with two datasets from the figure question answering task show that our model can generate more accurate caption units than competitive baseline models. A user study with ten human experts confirms the value of machine-generated caption units in their standalone accuracy and naturalness. Finally, a post-editing simulation study demonstrates the potential for models to paraphrase and stitch together single-type caption units into multi-type captions by learning from data.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {2792--2804},
numpages = {13},
keywords = {figure question answering, image captioning, text generation, web accessibility, Data visualization},
location = {Ljubljana, Slovenia},
}

@article{ramer1972iterative,
title = {{An Iterative Procedure for the Polygonal Approximation of Plane Curves}},
journal = {Computer Graphics and Image Processing},
volume = {1},
number = {3},
pages = {244--256},
year = {1972},
issn = {0146-664X},
doi = {10.1016/S0146-664X(72)80017-0},
url = {https://www.sciencedirect.com/science/article/pii/S0146664X72800170},
author = {Urs Ramer},
abstract = {The approximation of arbitrary two-dimensional curves by polygons is an importanttechnique in image processing. For many applications, the apparent ideal procedure is to represent lines and boundaries by means of polygons with minimum number of vertices and satisfying a given fit criterion. In this paper, an approximation algorithm is presented which uses an iterative method to produce polygons with a small—but not minimum—number of vertices that lie on the given curve. The maximum distance of the curve from the approximating polygon is chosen as the fit criterion. The results obtained justify the abandonment of the minimum-vertices criterion which is computationally much more expensive.}
}

@article{rong2017asap,
author = {Rong, Kexin and Bailis, Peter},
title = {{ASAP: Prioritizing Attention via Time Series Smoothing}},
year = {2017},
issue_date = {August 2017},
publisher = {VLDB Endowment},
volume = {10},
number = {11},
issn = {2150-8097},
url = {https://doi.org/10.14778/3137628.3137645},
doi = {10.14778/3137628.3137645},
abstract = {Time series visualization of streaming telemetry (i.e., charting of key metrics such as server load over time) is increasingly prevalent in modern data platforms and applications. However, many existing systems simply plot the raw data streams as they arrive, often obscuring large-scale trends due to small-scale noise. We propose an alternative: to better prioritize end users' attention, smooth time series visualizations as much as possible to remove noise, while retaining large-scale structure to highlight significant deviations. We develop a new analytics operator called ASAP that automatically smooths streaming time series by adaptively optimizing the trade-off between noise reduction (i.e., variance) and trend retention (i.e., kurtosis). We introduce metrics to quantitatively assess the quality of smoothed plots and provide an efficient search strategy for optimizing these metrics that combines techniques from stream processing, user interface design, and signal processing via autocorrelation-based pruning, pixel-aware preaggregation, and on-demand refresh. We demonstrate that ASAP can improve users' accuracy in identifying long-term deviations in time series by up to 38.4\% while reducing response times by up to 44.3\%. Moreover, ASAP delivers these results several orders of magnitude faster than alternative search strategies.},
journal = {Proceedings of the VLDB Endowment},
pages = {1358--1369},
numpages = {12}
}

@inproceedings{roth1994interactive,
  title={Interactive graphic design using automatic presentation knowledge},
  author={Roth, Steven F and Kolojejchick, John and Mattis, Joe and Goldstein, Jade},
  booktitle={Proceedings of the SIGCHI conference on Human factors in computing systems},
  pages={112--117},
  year={1994}
}

@inproceedings{setlur2021semantic,
  title={Semantic Resizing of Charts Through Generalization: A Case Study with Line Charts},
  author={Setlur, Vidya and Chung, Haeyong},
  booktitle={2021 IEEE Visualization Conference (VIS)},
  pages={1--5},
  year={2021},
  organization={IEEE}
}

@article{sharma2016trend,
  title={Trend analysis and change point techniques: a survey},
  author={Sharma, Shilpy and Swayne, David A and Obimbo, Charlie},
  journal={Energy, Ecology and Environment},
  volume={1},
  number={3},
  pages={123--130},
  year={2016},
  publisher={Springer}
}

@incollection{silva2016enhancing,
  title={Enhancing exploratory analysis by summarizing spatiotemporal events across multiple levels of detail},
  author={Silva, Ricardo Almeida and Pires, Jo{\~a}o Moura and Santos, Maribel Yasmina and Datia, Nuno},
  booktitle={Geospatial Data in a Changing World},
  pages={219--238},
  year={2016},
  publisher={Springer}
}

@ARTICLE{stokes2022striking,
  author={Stokes, Chase and Setlur, Vidya and Cogley, Bridget and Satyanarayan, Arvind and Hearst, Marti A.},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{Striking a Balance: Reader Takeaways and Preferences when Integrating Text and Charts}}, 
  year={2023},
  volume={29},
  number={1},
  pages={1233--1243},
  abstract={While visualizations are an effective way to represent insights about information, they rarely stand alone. When designing a visualization, text is often added to provide additional context and guidance for the reader. However, there is little experimental evidence to guide designers as to what is the right amount of text to show within a chart, what its qualitative properties should be, and where it should be placed. Prior work also shows variation in personal preferences for charts versus textual representations. In this paper, we explore several research questions about the relative value of textual components of visualizations. 302 participants ranked univariate line charts containing varying amounts of text, ranging from no text (except for the axes) to a written paragraph with no visuals. Participants also described what information they could take away from line charts containing text with varying semantic content. We find that heavily annotated charts were not penalized. In fact, participants preferred the charts with the largest number of textual annotations over charts with fewer annotations or text alone. We also find effects of semantic content. For instance, the text that describes statistical or relational components of a chart leads to more takeaways referring to statistics or relational comparisons than text describing elemental or encoded components. Finally, we find different effects for the semantic levels based on the placement of the text on the chart; some kinds of information are best placed in the title, while others should be placed closer to the data. We compile these results into four chart design guidelines and discuss future implications for the combination of text and charts.},
  keywords={},
  doi={10.1109/TVCG.2022.3209383},
  ISSN={1941-0506},
}

@incollection{sweller2011split,
author="Sweller, John
and Ayres, Paul
and Kalyuga, Slava",
title="{The Split-Attention Effect}",
bookTitle="Cognitive Load Theory",
year="2011",
publisher="Springer",
address="New York, USA",
chapter="9",
pages="111--128",
abstract="The split-attention effect arose from the worked example effect following the discovery that worked examples with a particular format were relatively ineffective (Tarmizi {\&} Sweller, 1988). Worked examples are valuable because they reduce extraneous cognitive load compared to solving the equivalent problems but, of course, it is unlikely that all worked examples, irrespective of their structure and function, will be equally effective. Indeed, some worked examples are likely to be ineffective because their format itself imposes a heavy extraneous cognitive load. Split-source worked examples fall into this category.",
isbn="978-1-4419-8126-4",
doi="10.1007/978-1-4419-8126-4_9",
url="https://doi.org/10.1007/978-1-4419-8126-4_9"
}

@misc{tableau2022public,
  author =       "{Tableau {P}ublic}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://public.tableau.com/app/discover}",
}

@misc{tableau2022tableau,
  author =       "{Tableau Software}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.tableau.com}",
}

@article{tome2004piecewise,
  title={Piecewise linear fitting and trend changing points of climate parameters},
  author={Tom{\'e}, AR and Miranda, PMA},
  journal={Geophysical Research Letters},
  volume={31},
  number={2},
  year={2004},
  publisher={Wiley Online Library}
}

@article{tome2005continuous,
  title={Continuous partial trends and low-frequency oscillations of time series},
  author={Tom{\'e}, Ant{\'o}nio Rodrigues and Miranda, PMA},
  journal={Nonlinear Processes in Geophysics},
  volume={12},
  number={4},
  pages={451--460},
  year={2005},
  publisher={Copernicus GmbH}
}

@book{tufte2001visual,
  title={{The Visual Display of Quantitative Information}},
  author={Tufte, Edward R.},
  year={2001},
  publisher={Graphics Press},
  address={Cheshire, USA},
  isbn={9780961392147}
}

@misc{un2023ilo,
  author =       "{United Nations}",
  year =         "2023",
  title =        "{International Labour Organization}",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.ilo.org/global/lang--en/}",
}

@misc{universal2022universal,
  author =       "Universal Dependencies",
  year =         "2022",
  lastaccessed = "June 15, 2022",
  url =          "https://universaldependencies.org/u/dep/index.html",
}

@misc{us2023treasury,
  author =       "{U.S. Department of the Treasury}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://home.treasury.gov/}",
}

@article{vartak2015seedb,
  title={SeeDB: supporting visual analytics with data-driven recommendations},
  author={Vartak, Manasi and Madden, Samuel and Parameswaran, Aditya and Polyzotis, Neoklis},
  journal={Proceedings of the VLDB Endowment},
  volume={8},
  number={13},
  pages={2015},
  year={2015}
}

@misc{victor2011explorable,
  author =       "Bret Victor",
  year =         "2011",
  title =        "Explorable Explanations",
  lastaccessed = "June 15, 2022",
  url =          "http://worrydream.com/ExplorableExplanations/",
}

@inproceedings{vig2014large,
  title={Large-scale optimization of hierarchical features for saliency prediction in natural images},
  author={Vig, Eleonora and Dorr, Michael and Cox, David},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2798--2805},
  year={2014}
}

@misc{vox2023vox,
  author =       "{Vox Media}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://www.vox.com/}",
}

@article{wakita2019guiding,
  title={Guiding Readers to the Focus and Context of Industrial Statistical Reports},
  author={Wakita, Ken and Arimoto, Kohei},
  year={2019},
  publisher={OSF Preprints}
}

@misc{walton2019ai,
  author =       "Nick Walton",
  year =         "2019",
  title =        "AI Dungeon 2",
  lastaccessed = "June 15, 2022",
  url =          "https://aidungeon.cc/",
}

@article{wattenberg2004analyzing,
  title={Analyzing perceptual organization in information graphics},
  author={Wattenberg, Martin and Fisher, Danyel},
  journal={Information Visualization},
  volume={3},
  number={2},
  pages={123--133},
  year={2004},
  publisher={SAGE Publications Sage UK: London, England}
}


@article{whitacre2016high,
	title = {High {School} {Girls}’ {Interpretations} of {Science} {Graphs}: {Exploring} {Complex} {Visual} and {Natural} {Language} {Hybrid} {Text}},
	volume = {14},
	issn = {1573--1774},
	url = {https://doi.org/10.1007/s10763-015-9677-7},
	doi = {10.1007/s10763-015-9677-7},
	abstract = {Science communication, as a hybrid, involves the concomitant reading of graphical representations and natural language. A scientifically literate individual should be able to make sense of both sites of information in relation to one another. In this paper, we explore the ability of 61 students from a highly rated, all-girls Catholic School to make sense of inconsistent graphical and textual data and examine their interpretations within 3 distinct settings: individually on a paper assessment, in a one-to-one interview with the researchers, and through classroom conversation. This study indicates that (1) sense-making was difficult for even advanced students and (2) different interpretations of text evolved within each of the 3 settings. Many students initially privileged natural language over the data represented on the graph and sought ways to explain how the incorrect description could somehow be construed as accurate. Although these students could successfully complete school assignments related to graphs, their skills in reading authentic, real-world science communication was limited. We recommend that teachers invite discussion of various semiotic forms rather than scaffold for correctness. Students have learned to do schoolwork, but they are much less able to engage critically in real-world science.},
	number = {8},
	journal = {International Journal of Science and Mathematics Education},
	author = {Whitacre, Michelle P. and Saul, E. Wendy},
	year = {2016},
	pages = {1387--1406},
}

@misc{wikimedia2022wikimedia,
  author =       "{Wikimedia Commons}",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://commons.wikimedia.org/wiki/Main_Page}",
}

@misc{wikipedia2021preposition,
  title={List of English prepositions},
  note={Retrieved September 2, 2021 from \url{https://en.wikipedia.org/wiki/List_of_English_prepositions}},
  year={2021}
}

@article{wills2010autovis,
author = {Graham Wills and Leland Wilkinson},
title ={{AutoVis: Automatic Visualization}},
journal = {Information Visualization},
volume = {9},
number = {1},
pages = {47-69},
year = {2010},
doi = {10.1057/ivs.2008.27},

URL = { 
        https://doi.org/10.1057/ivs.2008.27
    
},
eprint = { 
        https://doi.org/10.1057/ivs.2008.27
    
}
,
    abstract = { AutoVis is a data viewer that responds to content–text, relational tables, hierarchies, streams, images–and displays the information appropriately (that is, as an expert would). Its design rests on the grammar of graphics, scagnostics and a modeler based on the logic of statistical analysis. We distinguish an automatic visualization system (AVS) from an automated visualization system. The former automatically makes decisions about what is to be visualized. The latter is a programming system for automating the production of charts, graphs and visualizations. An AVS is designed to provide a first glance at data before modeling and analysis are done. AVS is designed to protect researchers from ignoring missing data, outliers, miscodes and other anomalies that can violate statistical assumptions or otherwise jeopardize the validity of models. The design of this system incorporates several unique features: (1) a spare interface–analysts simply drag a data source into an empty window, (2) a graphics generator that requires no user definitions to produce graphs, (3) a statistical analyzer that protects users from false conclusions, and (4) a pattern recognizer that responds to the aspects (density, shape, trend, and so on) that professional statisticians notice when investigating data sets. }
}




@misc{wolfram2022wolframalpha,
    author = "WolframAlpha",
    year =         "2023",
    howpublished = {Retrieved July 18, 2023 from \url{https://www.wolframalpha.com/}}
}

@misc{writer2022writer,
  author =       "Writer",
  year =         "2023",
  howpublished = "Retrieved July 18, 2023 from \url{https://writer.com/}",
}

@ARTICLE{xiong2019curse,
  author={Xiong, Cindy and Van Weelden, Lisanne and Franconeri, Steven},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{The Curse of Knowledge in Visual Data Communication}}, 
  year={2020},
  volume={26},
  number={10},
  pages={3051--3062},
  abstract={A viewer can extract many potential patterns from any set of visualized data values. But that means that two people can see different patterns in the same visualization, potentially leading to miscommunication. Here, we show that when people are primed to see one pattern in the data as visually salient, they believe that naïve viewers will experience the same visual salience. Participants were told one of multiple backstories about political events that affected public polling data, before viewing a graph that depicted those data. One pattern in the data was particularly visually salient to them given the backstory that they heard. They then predicted what naïve viewers would most visually salient on the visualization. They were strongly influenced by their own knowledge, despite explicit instructions to ignore it, predicting that others would find the same patterns to be most visually salient. This result reflects a psychological phenomenon known as the curse of knowledge, where an expert struggles to re-create the state of mind of a novice. The present findings show that the curse of knowledge also plagues the visual perception of data, explaining why people can fail to connect with audiences when they communicate patterns in data.},
  keywords={},
  doi={10.1109/TVCG.2019.2917689},
  ISSN={1941-0506},
}

@inproceedings{yu2003sumtime,
  title={Sumtime-turbine: a knowledge-based system to communicate gas turbine time-series data},
  author={Yu, Jin and Reiter, Ehud and Hunter, Jim and Sripada, Somayajulu},
  booktitle={International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems},
  pages={379--384},
  year={2003},
  organization={Springer}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{zhang2008sun,
  title={SUN: A Bayesian framework for saliency using natural statistics},
  author={Zhang, Lingyun and Tong, Matthew H and Marks, Tim K and Shan, Honghao and Cottrell, Garrison W},
  journal={Journal of vision},
  volume={8},
  number={7},
  pages={32--32},
  year={2008},
  publisher={The Association for Research in Vision and Ophthalmology}
}

@article{zhao2018method,
  title={A method for simplifying ship trajectory based on improved Douglas--Peucker algorithm},
  author={Zhao, Liangbin and Shi, Guoyou},
  journal={Ocean Engineering},
  volume={166},
  pages={37--46},
  year={2018},
  publisher={Elsevier}
}

@ARTICLE{Fulda2016TimeLineCuratorIA,
  author={Fulda, Johanna and Brehmer, Matthew and Munzner, Tamara},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text}}, 
  year={2016},
  volume={22},
  number={1},
  pages={300--309},
  abstract={We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.},
  keywords={},
  doi={10.1109/TVCG.2015.2467531},
  ISSN={1941-0506},
}


@article{ryan:2018,
author = {Ryan, Gabriel and Mosca, Abigail and Chang, Remco and Wu, Eugene},
year = {2018},
month = {08},
pages = {1-1},
title = {At a Glance: Pixel Approximate Entropy as a Measure of Line Chart Complexity},
volume = {PP},
journal = {IEEE Transactions on Visualization and Computer Graphics},
doi = {10.1109/TVCG.2018.2865264}
}

@ARTICLE{Rosen2020LineSmoothAA,
  author={Rosen, Paul and Quadri, Ghulam Jilani},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={{LineSmooth: An Analytical Framework for Evaluating the Effectiveness of Smoothing Techniques on Line Charts}}, 
  year={2021},
  volume={27},
  number={2},
  pages={1536--1546},
  abstract={We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing-rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as Gaussian filters and TOPOLOGY-based subsampling, perform well in general. Other methods, such as low-pass CUTOFF filters and Douglas-peucker subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used UNIFORM subsampling, produce low-quality results, and should, therefore, be avoided, if possible.},
  keywords={},
  doi={10.1109/TVCG.2020.3030421},
  ISSN={1941-0506},}
