   % This is samplepaper.tex, a sample chapter demonstrating the
  % LLNCS macro package for Springer Computer Science proceedings;
  % Version 2.21 of 2022/01/12
  %
  \documentclass[runningheads]{llncs}
  %
  \usepackage[T1]{fontenc}
  % T1 fonts will be used to generate the final print and online PDFs,
  % so please use T1 fonts in your manuscript whenever possible.
  % Other font encondings may result in incorrect characters.
  %
  \usepackage{graphicx}
  \usepackage{cite}
  \usepackage{amsmath,amssymb,amsfonts}
  \usepackage{algorithmic}
  \usepackage{textcomp}
  \usepackage{xcolor}
  \usepackage{dsfont}
  \usepackage[ruled,linesnumbered]{algorithm2e}
  \usepackage{float}
  \usepackage{enumerate}
  \usepackage{epstopdf}
  \usepackage{bbding}
  \usepackage{pifont}
  \usepackage{subfigure}
  \usepackage{epsfig}
  \newtheorem{assumption}{Assumption}
  \DeclareMathOperator\erf{erf}
  % Used for displaying a sample figure. If possible, figure files should
  % be included in EPS format.
  %
  % If you use the hyperref package, please uncomment the following two lines
  % to display URLs in blue roman font according to Springer's eBook style:
  %\usepackage{color}
  %\renewcommand\UrlFont{\color{blue}\rmfamily}
  %
  \setcounter{secnumdepth}{3}
  \begin{document}	
  	%
  	\title{Accelerating Recursive and Variance Reduced Stochastic Gradient Methods by Adaptive Barzilai-Borwein Step Sizes}
  	%
  	\titlerunning{AR-VR-SGD-RHBB}
  	% If the paper title is too long for the running head, you can set
  	% an abbreviated paper title here
  	%
  	\author{JiangShan Wang, YiMing Yang,  Zheng  Peng}
  	% \authorrunning{J. Long, C. Wang, C. Ou et al.}
  	% First names are abbreviated in the running head.
  	% If there are more than two authors, 'et al.' is used.
  	%
  	\institute{{School of Mathematics and Computational Science, Xiangtan University}\\ {Xiangtan, Hunan 411105, China}}%\\
  	%\email{longjiangshan@whu.edu.cn \\wchenxu@whu.edu.cn \\ouchanghai@whu.edu.cn \\tangming@whu.edu.cn}
  	\maketitle              % typeset the header of the contribution
  	%
  	\begin{abstract}
  		Mini-Batch version of StochAstic Recursive grAdient algoritHm and Stochastic Variance Reduced Gradient method, employed random Barzilai-Borwein step size (shorted as MB-SARAH-RBB and mS2GD-RBB), have surged into prominence through timely step size sequence. Inspired by modern adaptors and variance reduction techniques, we propose two new variant rules, referred to as RHBB and RHBB+, thereby leading to MB-SARAH-RHBB/RHBB+ and mS2GD-RHBB/RHBB+, respectively. They are aggressive in updates, robust in performance and self-adaptive along iterations. The enhanced version, RHBB+, additionally incorporates importance sampling. We analyze the flexible convergence structures and the complexity in strongly convex cases. Comprehensive tuning guidance is provided later for reference in practical implementations. Technical insight over their designs are supplied. Experiments show that, the proposed methods consistently outperform the original and various state-of-the-art methods on frequently tested data sets. Tests over RHBB+ confirm the efficacy of applying importance sampling at the step size level. Numerous explorations show the promising scalability of our iterative adaptors.
  		\keywords{Variance reduction \and stochastic optimization \and random hedge Barzilai-Borwein method \and importance sampling \and iterative adaptors}
  	\end{abstract}
  	
  	\section{Introduction}
  	We focus on the following stochastic optimization problem
  	$$
  	\min _{w \in \mathbb{R}^d} P(w)=\mathbb{E}_{\mathbb{P}}[f(w ; \xi)_{\xi\in \Omega}],
  	$$
  	where $\xi$ is a random instance of an input-output pair $(x_i, z_i)$, with input representation vector $x_i$, target output $z_i$, hence $f(\cdot)$ takes the form 
  	$$f(w ; \xi)=f\left(w ; (x_i, z_i)\right).$$
  	Define $P(\cdot)$ by empirical expectation on probability space $(\Omega, \mathbb{P})$, where  $\Omega=\left\{\xi_1, \cdots, \xi_{n}\right\}$ is a finite support set and $\mathbb{P}$ is the probability measure over $\Omega$. It hence transforms to an unconstrained finite-sum model as
  	\begin{equation}
  	\label{1}
  	\min _{w \in \mathbb{R}^d} P(w)=\int_{\Omega} f\left(w ; (x_i, z_i)\right) d \mathbb{P}(x_i,z_i)\approx  \frac{1}{n} \sum_{0\le i\le |\Omega|} f(w; (x_i, z_i)).
  	\end{equation}
  	
  	
  	Problem (\ref{1}) covers a broad range of applications. We assume a lower bound $P(w_*)$ of $P(\cdot)$ exists, hence, iteratively update $w$ to reduce $P(\cdot)$ steadily and swiftly. Given a sequence of $n$ labeled pairs, $\{\left(x_1, z_1\right), \ldots,\left(x_{n}, z_{n}\right)\}$, into finite dimensional spaces $\{(\mathbb{R}^d, \mathbb{R})\}$, the linear least squares regression is of $f(w) \stackrel{\text { def }}{=}$ $\left(x_i^T w-z_i\right)^2$. As for logistic regression, we exploit $f(w) \stackrel{\text { def }}{=} \log \left(1+\exp \left(-z_i x_i^T w\right)\right)$. Efficient regularization process may be taken into account for specific purposes, thus develops into a composite finite-sum model
  	$$\min _{w \in \mathbb{R}^d} \frac{1}{n} \sum_{0\le i\le |\Omega|} f(w; (x_i, z_i))+\psi(w),$$
  	where $\psi(\cdot)$ is a proper, closed, and convex penalty on model parameters. Its subdifferential at $w$ defines as $\partial \psi(w)=\left\{v \in \mathbb{R}^d \mid \psi(d) \geq \psi(w)+ v^T \left(d-w\right), \forall d \in \operatorname{dom}(\psi)\right\}$. Throughout the paper, we mainly utilize smoothing regularization, thus $\psi(\cdot)$ is differentiable, then $\partial \psi(w)=\{\nabla \psi(w)\}$. 
  	
  	
  	Stochastic optimization, due to productive, scalable frameworks, is prevalent among large-scale problems or complex relationships, it replaces costly deterministic schemes with a universal version as
  	$$w^{(t)}=w^{(t-1)}-\eta_t g_t\left(w^{(t-1)}, \nu_t\right),$$
  	where $g(\cdot)$ represents the gradient estimator, $\nu$ the randomness pointer. Vanilla stochastic gradient descent (SGD) \cite{Robbins}\cite{shalev} specifies $g(\cdot)$ into basic moment estimate. It enjoys cheap computational cost per update and independence with $n$ in term of complexity. However, its straightforward estimator inevitably introduces variance and noise to the steps. Diminishing step size $\eta_t=O(1 / t)$ is forced to employed in\cite{luo}\cite{solodov}\cite{nemirovski} for a sublinear convergence rate of $O(1 / t)$ (Moulines et al.\cite{moulines}), which should satisfy
  	$$
  	\sum_{t=1}^{\infty} \eta_t=\infty \text {  and  } \sum_{t=1}^{\infty} \eta_t^2<\infty.
  	$$
  	Such update rule comes with a side effect of halt in convergence near the eventual limit\cite{eon}. Batch methods of \cite{dekel}\cite{cotter}\cite{shalev} decrease the intrinsic variance through a bunch of samples, but at the cost of further computational workload. Parallel processing hence becomes indispensable for explosion-scale data.
  	
  	
  	Shifts on $g(\cdot)$ have been intensively studied with a vast number of research papers. Canonical examples include but not limit to SAG/SAGA \cite{Schmidt}\cite{defazio}, SVRG \cite{johnson}, ProxSVRG \cite{xiao}, MSVRG\cite{reddi}, S2GD \cite{konevcny2}, mS2GD \cite{konevcny}, MISO \cite{mairal}, SARAH\cite{nguyen2}, iSARAH \cite{nguyen3}, MB-SARAH \cite{nguyen2}, SPIDER \cite{fang}, etc. Konecny et al \cite{konevcny} proposed mS2GD and showed it reaches a pre-defined accuracy with less overall work than a method without
    batching. They established a threshold for the batch size, at which more than linear speedup can be achieved, it's worthy to further explore. MB-SARAH is presented by Nguyen et al. \cite{nguyen2} for solving non-convex problems. Recursive updates free from the storage of past gradients and avoid oscillation of the Euclidean norm of $g(\cdot)$ in inner stages, well-suited for modern complex scenarios. Researches have also extended to the dual space of (\ref{1}) by updating random dual variables or variable blocks, such as RCDM\cite{nesterov2}, AsySPDC\cite{liu2}, SDCA\cite{shalev2}, SPDC\cite{zhang}, mSDCA\cite{takac}, ASDCA\cite{shalev3} and QUARTZ\cite{zheng}, Prox-SDCA \cite{zhao}. The subsequent drawback lies in that these algorithms rely on a tuning step size by hand, which can be time consuming in practice.
  	
  	
  	Several ways of auto-tuning prevail among the stochastic algorithms. Barzilai-Borwein method of second order tuning is outstanding in this trend, due to its simplicity and numerical efficiency. SopyÅ‚a et al.  \cite{sopyla} employed BB in vanilla SGD to solve linear SVM in dual space. Tan et al. \cite{tan} incorporated BB into SVRG (SVRG-BB) and established linear convergence in strongly convex cases. To further accelerate the rate, Yang et al.  \cite{yang3} introduced BB to mS2GD (mS2GD-BB) for the nonsmooth strongly convex functions. On the basis of Hessian and its eigenvalues, Ma et al.  \cite{ma} proposed stabilized Barzilai-Borwein (SBB) and matched SVRG (SVRG-SBB) for the ordinal embedding problems, which avoids the denominator tending to zero. Yang et al.  \cite{yangy} considered inexact SARAH (iSARAH-BB) to reduce the cost in deterministic steps, and showed its robustness in implementations. Byrd et al. \cite{byrd} utilized batch methods to approximate quasi-Newton property. Recently, the timely random Barzilai-Borwein method (RBB) emerged and was primarily applied in MB-SARAH (MB-SARAH-RBB\cite{yang}) and mS2GD (mS2GD-RBB\cite{yang2}) settings. The promising performance outperformed and matched state-of-the-art algorithms. However, they still have flaws and aspects that can be explored and improved.
  	
  	In the context of the RBB rule, when the batch size is insufficient, it can result in a high level of noise, primarily caused by step sizes. This noise leads to an increasing or oscillating trend towards divergence. 
   %On the other hand, increasing the batch size reduces the variance of stochastic curvature but unfortunately also slows down convergence, which is discouraging.
   %In the RBB rule, an insufficient batch may lead to steps with a high level of noise (most noise from step sizes), which incurs an increasing trend or oscillating trend towards divergence. 
   As batch size increases, the variance of stochastic curvature decreases, but using a larger batch discouragingly slows down the convergence. In addition, it's insensitive to iteration periods, and the well-worn uniform sampling deserves to be generalized. Therefore, we introduce the random hedge Barzilai-Borwein method (RHBB) in pursuit of improvement. We can further incorporate our RHBB with importance sampling and develop another enhanced version, RHBB+.
  	
  	\vspace{3pt}
  	The key contributions are summarized as follows:
  	\begin{enumerate}
  		\item[1)] We propose random hedge Barzilai-Borwein step size rule for MB-SARAH and mS2GD settings, hence obtain MB-SARAH-RHBB, mS2GD-RHBB algorithms. Adaptive acceleration mechanism is analyzed, trade-off rules are studied and tuning guidance is provided.
  		\item[2)] We consider importance sampling at the step size level to filter inherent efficiency in data sets (e.g. elementwise sparsity), which leads to two enhanced versions as MB-SARAH-RHBB+ and mS2GD-RHBB+.
  		\item[3)] We establish global convergence under strongly convex cases, ill-conditioning is also evaluated for the efficacy of new rules. In MB-SARAH-RHBB/RHBB+, the square of the overall gradients converges linearly in expectation. And the expected distance of iterates to the global optimum has linear convergence in mS2GD-RHBB/RHBB+. 
  		\item[4)] We conduct extensive experiments and show the exceptional performance. We explore a tentative, incremental scheme over the iterative adaptor and view immense potential for scalability.
  	\end{enumerate}
  	
  	\section{Common Assumptions and Inequalities}
  	We add subscripts to distinguish element functions, e.g. $f_i$ for the $i$-th component. Unless otherwise specified, $\|\cdot\|$ denotes Euclidean norm in this paper. We provide following common assumptions.\\
  	\textbf{Assumption 1} ({\bf Smoothness}). Each convex $f_i: \mathbb{R}^d \rightarrow \mathbb{R}$ is $L$-smooth over any compact set of its domain, i.e., there exists an $L>0$, for all $w, w^{\prime} \in \text{dom}(f_i)$
  	$$
  	\left\|\nabla f_i(w)-\nabla f_i\left(w^{\prime}\right)\right\| \leq L\left\|w-w^{\prime}\right\|.
  	$$
  	
  	
  	Due to $\nabla f_i(w)-\nabla f_i\left(w^{\prime}\right)=H\left(w-w^{\prime}\right)$ where $H=\nabla^2 f_i\left(\hat{w}\right)$ is Hessian at midpoint $\hat{w}$, the largest eigenvalue of the Hessian $H$ is no more than $L$. In view of it, we obtain a boundary for the square distance between gradients
  	\begin{equation}
  	\label{L}
  	\begin{aligned}
  	\left\|\nabla f_i(w)-\nabla f_i\left(w^{\prime}\right)\right\|^2
  	& =\left(w-w^{\prime}\right)^{\top} H^2\left(w-w^{\prime}\right) \\
  	&\leq  L\left(w-w^{\prime}\right)^{\top} H\left(w-w^{\prime}\right) \\
  	&=  L\left(\nabla f_i(w)-\nabla f_i\left(w^{\prime}\right)\right)^{\top}\left(w-w^{\prime}\right).
  	\end{aligned}
  	\end{equation}
  	
  	The individual $L$-smoothness implies overall $P(w)=\frac{1}{n} \sum_{0\le i\le |\Omega|} f_i(w)$ is $L$-smooth as well. Hence, we derive the following bound 
  	\begin{equation}
  	\label{inequation1}
  	P(w) \leq P(w^{\prime})+\nabla P(w^{\prime})^T(w-w^{\prime})+\frac{L}{2}\|w-w^{\prime}\|^2.
  	\end{equation}
  	
  	For $L$-smoothness of overall $P(\cdot)$, we have another equivalent claim 
  	\begin{equation}
  	\label{L2}
  	P(w) \geq P(w^{\prime})+\nabla P(w)^T(w-w^{\prime})+\frac{1}{2 L}\|\nabla P(w)-\nabla P(w^{\prime})\|^2.
  	\end{equation}
  	\textbf{Assumption 2a} ({\bf Strong convexity I}). $P(w)$ is $\mu$-strongly convex, i.e., there exists $\mu>0$ such that, for all $w, w^{\prime} \in \text{dom}(P)$
  	\begin{equation}
  	\label{C}
  	(\nabla P(w)-\nabla P(w^{\prime}))^T(w-w^{\prime}) \geq \mu\|w-w^{\prime}\|^2.
  	\end{equation}
  	\textbf{Assumption 2b} ({\bf Strong convexity II}). Each component $f_i$ is $\mu$-strongly convex, i.e., there exists $\mu>0$ for each $f_i$ such that, all $w, w^{\prime} \in \text{dom}(f_i)$\\
  	\begin{equation}
  	\label{C2}
  	(\nabla f_i(w)-\nabla f_i(w^{\prime}))^T(w-w^{\prime}) \geq \mu\|w-w^{\prime}\|^2.
  	\end{equation}
  	
  	Assumption 2b implies Assumption 2a, but not vice versa. Assumption 2b is a stricter premise which requires the strong convexity on each $f_i$.
  	
  	For a $\mu$-strongly convex $P(\cdot)$, we have another equivalent claim as
  	$$
  	P(w) \geq P(w^{\prime})+\nabla P(w^{\prime})^T(w-w^{\prime})+\frac{\mu}{2}\|w-w^{\prime}\|^2 .
  	$$
  	
  	We define the global optimum $w_*=\operatorname{argmin}_w P(w)$, it further indicates
  	\begin{equation}
  	\label{convex1}
  	2 \mu\left[P(w)-P\left(w_*\right)\right] \leq\|\nabla P(w)\|^2, \forall w \in \mathbb{R}^d.
  	\end{equation}
  	To see this, we have by strong convexity
  	$$P\left(w_*\right) \geq P(w)+\nabla P(w)^T\left(w_*-w\right)+\frac{\mu}{2}\left\|w-w_*\right\|^2.$$
  	Then, by some basic derivation 
  	$$
  	\begin{aligned}
  	& 2 \mu\left[P(w_*)-P\left(w\right)\right]+\|\nabla P(w)\|^2 \geq 2 \mu \nabla P(w)^T\left(w_*-w\right)+\mu^2 \left\|w-w_*\right\|^2+\|\nabla P(w)\|^2,\\
  	& 2 \mu\left[P(w_*)-P\left(w\right)\right]+\|\nabla P(w)\|^2 \geq \left\|\nabla P(w)+\mu\left(w_*-w\right)\right\|^2 \geq 0,\\
  	& 2 \mu\left[P(w)-P\left(w_*\right)\right] \leq\|\nabla P(w)\|^2.
  	\end{aligned}
  	$$
  	
  	
  	\section{Motivation}
  	\subsection{Barzilai-Borwein Method and the Random Version}
  	Barzilai-Borwein method, originally developed in the pioneer work of Barzilai and Borwein [1], shows great preeminence in solving nonlinear optimization problems and has been widely improved up to now. 
  	
  	
  	We automatically hope $\theta_k I$ approximates $H_k$ in the $k$-th epoch, where $\theta_k$ denotes the $k$-th step size, $I$ the identity matrix and $H_k$ the inverse of Hessian. To minimize the residual of the secant equations, i.e., $\left\|\left(1 / \theta\right) s_k-y_k\right\|_2^2$ or $\left\|\theta y_k-s_k\right\|_2^2$, we have the following step size solutions respectively
  	$$\theta_k^{\mathrm{BB} 1}=\frac{s_{k}^T s_{k}}{s_{k}^T y_{k}}, \quad
  	\theta_k^{\mathrm{BB} 2}=\frac{s_{k}^T y_{k}}{y_{k}^T y_{k}},$$
  	where $s_{k}=w_{k+1}-w_{k}$, $y_{k}=\nabla f\left(w_{k+1}\right)-\nabla f\left(w_{k}\right)$.
  	
  	
  	Among popular Barzilai-Borwein methods, either BB1 or BB2 is computed at the beginning of each outer epoch and employed for the entire consecutive stochastic stages (see \cite{tan}\cite{yang3}\cite{ma}\cite{yangy} for a brief reference). Yang et al \cite{yang}\cite{yang2} advocated to calculate the Barzilai-Borwein step size timely by stochastic curvature (batch curvature, similar to Castera et al \cite{castera}), within each stochastic stage. In mS2GD setting, they proposed a random version of BB1, we instinctively deduce the random BB2 by analogy
  	$$
  	\tilde{\eta}_k^{\mathrm{RBB} 1}=\frac{1}{|S_1|}\cdot \frac{\left\|w_k-w_{k-1}\right\|^2}{\left(\left(w_k-w_{k-1}\right)^T\left(\nabla P_{S_1}\left(w_k\right)-\nabla P_{S_1}\left(w_{k-1}\right)\right)\right)},
  	$$
  	$$
  	\tilde{\eta}_k^{\mathrm{RBB} 2}=\frac{1}{|S_2|}\cdot \frac{\left(\left(w_k-w_{k-1}\right)^T\left(\nabla P_{S_2}\left(w_k\right)-\nabla P_{S_2}\left(w_{k-1}\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k\right)-\nabla P_{S_2}\left(w_{k-1}\right)\right\|^2},
  	$$
  	where $\nabla P_{S_1}\left(w_k\right)=\frac{1}{|S_1|} \sum_{i \in S_1} \nabla f_i\left(w_k\right), \nabla P_{S_2}\left(w_{k}\right)=\frac{1}{|S_2|} \sum_{j \in S_2} \nabla f_j\left(w_{k}\right)$. The $S_1, S_2\subset\{1, \ldots, n\}$ are randomly  selected  subsets with size $|S_1|$ and $ |S_2|$ respectively.    	  	
  	As in MB-SARAH setting, another parameter, $\gamma$, is multiplied to adjust the current RBB for a better Hessian approximation, we have
  	$$
  	\eta_k^{\mathrm{RBB} 1}=\frac{\gamma}{|S_1|}\cdot \frac{\left\|w_k-w_{k-1}\right\|^2}{\left(\left(w_k-w_{k-1}\right)^T\left(\nabla P_{S_1}\left(w_k\right)-\nabla P_{S_1}\left(w_{k-1}\right)\right)\right)},
  	$$
  	$$
  	\eta_k^{\mathrm{RBB} 2}=\frac{\gamma}{|S_2|}\cdot \frac{\left(\left(w_k-w_{k-1}\right)^T\left(\nabla P_{S_2}\left(w_k\right)-\nabla P_{S_2}\left(w_{k-1}\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k\right)-\nabla P_{S_2}\left(w_{k-1}\right)\right\|^2}.
  	$$
  	
  	% Figure environment removed
  	
  	
  	Let's observe a series of trajectories in Fig. \ref{fig1}, where the $x$-axis denotes the number of inner iterations within the $2$-nd epoch and the $y$-axis represents the corresponding step sizes. All relevant parameters are set as suggested in \cite{yang}\cite{yang2}. For illustration, we discard the first epoch, which is insufficient to compute the BB1 step sizes (instead a specified constant is applied during the first epoch). We can observe that BB1 remains unvaried with a single value throughout the entire epoch. RBB1 and RBB2 update in a timely manner, evolving with RBB1 taking precedence, or both intertwining mutually. 
  	
  	
  	Among the prevailing methods, the BB1-type is commonly preferred due to its aggressive finesse, but in a wide range of cases it may not reach the extreme of efficiency as well. Hedging is an innocuous way to mitigate risks in the financial sector. Inspired by the above, in our RHBB rule, we ensure or deliberately expand the effective magnitude of RBB1, while offsetting any over-utility from opposite direction via its `twin' RBB2. Additionally, we incorporate an adaptor to improve the adaptivity along updates. To be specific, RHBB is based on an affine combination of RBB1, RBB2 with an adaptive parameter $\alpha^{h(\sigma_1s+\sigma_2k)}$, where $\alpha^{h(\sigma_1s+\sigma_2k)}>1$. The adaptor $h$ are exponential decay rates of the affine magnitude. In the early epoch, we enlarge the step sizes at low cost to accelerate training. As approaching the global optimum, we then enforce them conservative to ensure final convergence. Therefore, the adaptor $h$ of the current update, in the $s$-th epoch and $k$-th inner stage, is monotone decreasing from the linear indicators $\left(\sigma_1s+\sigma_2k\right)$, and iteratively satisfies
  	$$|\alpha- h(\sigma_1s+\sigma_2k)|>\epsilon(s), \forall k\in m, \forall s.$$
  	In practice, we make $\sigma_1, \sigma_2 \in \{0, 1\}$ and $\epsilon(s)>\frac{s}{m}$, the gap should be expanded in latter periods. It's of great distinction in structural sense from the composite Barzilai-Borwein method (CBB in \cite{li}) and the composite adaptive Barzilai-Borwein method (CABB in \cite{li}), which utilize a convex combination with parameter within range $(0, 1)$. The CBB and CABB are composed of two components, each of which only extracts partial resources from BB1 or BB2. Essentially, all Barzilai-Borwein methods enjoy the `calculation' adaptivity, we attach another adaptor, $h(\cdot)$, to enhance the adaptivity along iterations.
  	
  	
  	The employment of RBB2 introduces another type of curvature as well. We capture the stochastic curvature from two probabilistic subsets, $S_1$ and $S_2$, thereby reducing the noise in second order level. It implies we use quite another subset to do hedging. For the sake of convergence, we pick the larger batch size as the batch correction, hence, produce RHBB step size sequence for MB-SARAH method (with total $\alpha^{h(\sigma_1s+\sigma_2k)}>1$) as
  	\begin{equation}
  	\label{SARAH-RHBB}
  	\begin{aligned}
  	(\eta_k^s)^{\mathrm{RHBB}}
  	& = \frac{\gamma}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}\left(w_k^s\right)-\nabla P_{S_1}\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right).
  	\end{aligned}
  	\end{equation}
  	
  	
  	We introduce an extra balance parameter, $\gamma_2$, to the RHBB step sizes in mS2GD setting. In latter section, we will elaborate its role in relaxing the stochastic hedge effect, related trade-off rule will as well be studied
  	\begin{equation}
  	\label{mS2GD-RHBB}
  	\begin{aligned}
  	(\tilde{\eta}_k^s)^{\mathrm{RHBB}}
  	& = \frac{\gamma_2}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}\left(w_k^s\right)-\nabla P_{S_1}\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right).
  	\end{aligned}
  	\end{equation}
  	
  	
  	We choose the batch settings for possible opportunities of parallel processing. Note from Tan et al. \cite{tan}, the absolute value has been taken upon the denominators in the BB step size calculations. They utilized a convex combination to approximate the full gradient actually. We employ batch estimate, hence bear no worry on negative step sizes. According to Castera et al.\cite{castera}, when the curvature condition $(g_k^s)^T\nabla^2P(w_k^s)g_k^s$ does not keep positive ($g_k^s$ is an update direction), it's advisable to set $\eta_k^s=c$ ($c>0$). Different from AS in Liu et al. \cite{liu3}, we conduct and use real-time estimation instead of moving average.
  	
  	
  	\subsection{Importance Sampling Strategy}
  	In terms of theory, this strategy leads to the improvement of the leading constant in the complexity estimate (Richtarik et al \cite{richtarik}, Needell et al \cite{needell}). The overhead associated with configuring distributions and withdrawing is negligible, and hence the net effect \cite{csiba} is speedup.
  	
  	
  	Uniform sampling enables unbiased estimators but sacrifices potential opportunities of variance reduction, algorithms nowadays have strived for the opposite. Most strategies have been applied in the gradient estimates, which include but not limit to Prox-SVRG \cite{xiao}, Prox-SDCA \cite{zhao} and SARAH-I \cite{liu}. Actually, in addition to the refined search directions, it's as well pivotal to focus on the step sizes during the line search. Hence, we inventively exploit sampling schemes in step size level, customizing probability distributions to filter stochastic variations. We configure $Q \sim$ $\left\{q_1, q_2, \ldots q_n\right\}$ according to Zhao et al \cite{zhao}.
  	
  	
  	Moment estimate of $\nabla P(\cdot)$ by uniform distribution on subset $S\subset \Omega$ is 
  	\begin{equation}
  	\label{stochastic estimate}
  	\nabla P_{S}\left(w_k\right)=\frac{1}{|S|} \sum_{0\le i \le |S|} \nabla f_i\left(w_k\right).
  	\end{equation}
  	In light of (\ref{stochastic estimate}), consider that from a general distribution we have
  	\begin{equation}
  	\label{P+}
  	\nabla P_{S}^+\left(w_k\right)=\frac{1}{|S|} \sum_{0\le i \le |S|} \frac{\nabla f_i\left(w_k\right)}{n\cdot q_i} = \frac{1}{|S|} \sum_{0\le i \le |S|} \nabla f^+_i\left(w_k\right).
  	\end{equation}
  	Uniformity delivers $q_1=...=q_{n}=\frac{1}{n}$, suggesting a special case (\ref{stochastic estimate}) of $(\ref{P+})$.
  	
  	
  	By substituting (\ref{stochastic estimate}) with (\ref{P+}), we equip the RHBB (\ref{SARAH-RHBB}) (\ref{mS2GD-RHBB}) with general distributions, which develops into enhanced version RHBB+ as follows, respectively 
  	\begin{equation}
  	\label{SARAH-RHBB+}
  	\begin{aligned}
  	(\eta_k^s)^{\mathrm{RHBB+}}
  	& = \frac{\gamma}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}^+\left(w_k^s\right)-\nabla P_{S_1}^+\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right\|^2}\right),
  	\end{aligned}
  	\end{equation}
  	\begin{equation}
  	\label{mS2GD-RHBB+}
  	\begin{aligned}
  	(\tilde{\eta}_k^s)^{\mathrm{RHBB+}}
  	& = \frac{\gamma_2}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}^+\left(w_k^s\right)-\nabla P_{S_1}^+\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right\|^2}\right).
  	\end{aligned}
  	\end{equation}
  	
  	
  	\section{Algorithms}
  	We shall first clarify the notations:  $\{v_k^s\}$ denotes the estimate sequence of $\nabla P(\cdot)$ in MB-SARAH-RHBB/RHBB+, each estimate has recursive formula as
  	\begin{equation}
  	\label{v}
  	v_k^s=\nabla P_S\left(w_k^s\right)-\nabla P_S\left(w_{k-1}^s\right)+v_{k-1}^s.
  	\end{equation}
  	In mS2GD-RHBB/RHBB+, we signify the estimate array as $\{\tilde{v}_k^s\}$, with 
  	\begin{equation}
  	\label{v2}
  	\tilde{v}_k^s=\nabla P_S\left(w_k^s\right)-\nabla P_S(\widetilde{w})+\nabla P(\widetilde{w}).
  	\end{equation}	
  	The $\{w_k^s\}$ represents an inner iterative sequence within the $s$-th outer epoch. Meanwhile, we use $\{\widetilde{w}_{s}\}$ for the outer series and $\widetilde{w}$ for the snapshots. 
  	
  	
  	\begin{algorithm}
  		\label{alg1}
  		\caption{MB-SARAH-RHBB/RHBB+}
  		\KwIn{$\widetilde{w}_0$, update frequency $m$, batch sizes $b, b_1, b_2$, constant sequence $\{\eta_0^s\}$, modification parameter $\gamma>0$, hedge base $\alpha$ and monotone decreasing function $h$, probability distribution $Q$.}
  		\vspace{2pt}
  		\KwOut{approximate solution $\widetilde{w}_s$.}
  		
  		\vspace{4pt}
  		\textbf{Outer Loop:} \For{$s = 1,2,...,$}
  		{$w_0^s=\widetilde{w}_{s-1}$\\
  			\vspace{4pt}
  			$v_0^s=\frac{1}{n} \sum_{i \in \Omega} \nabla f_i\left(w_0^s\right)=\nabla P\left(w_0^s\right)$\\
  			\vspace{4pt}
  			$w_1^s=w_0^s-\eta_0^s v_0^s$\\
  			\vspace{4pt}
  			\textbf{Inner Loop:} \For{$k = 1,2,...,m-1$}
  			{   \vspace{4pt}
  				Pick subset $S \subset\{1, \ldots, n\}$ of size $b$ uniformly at random\\
  				\vspace{4pt}
  				Update the stochastic recursive gradient $v_k^s$ by\
  				$$v_k^s=\nabla P_S\left(w_k^s\right)-\nabla P_S\left(w_{k-1}^s\right)+v_{k-1}^s$$
  				
  				Compute the step size $\eta_k^s$ by {\bf (1) Option I} or {\bf (2) Option II} \\
  				\vspace{8pt}
  				\textbf{(1) Option I : RHBB rule}\\
  				\vspace{4pt}
  				Configure $Q$ as uniform probability distribution \\
  				\vspace{4pt}
  				Pick subset $S_1 \subset\{1, \ldots, n\}$ of size $b_1$ randomly according to $Q$\\
  				\vspace{4pt}
  				Pick subset $S_2 \subset\{1, \ldots, n\}$ of size $b_2$ randomly according to $Q$\\
  				\vspace{4pt}
  				Calculate $\eta_k^s$ according to (\ref{SARAH-RHBB})\\ 
  				
  				\vspace{16pt}
  				\textbf{(2) Option II : RHBB+ rule}\\
  				\vspace{4pt}
  				Configure $Q$ to our needs \\
  				\vspace{4pt}
  				Pick subset $S_1 \subset\{1, \ldots, n\}$ of size $b_1$ randomly according to $Q$\\
  				\vspace{4pt}
  				Pick subset $S_2 \subset\{1, \ldots, n\}$ of size $b_2$ randomly according to $Q$\\
  				\vspace{4pt}
  				Compute $\nabla P_{S_1}^+, \nabla P_{S_2}^+$ according to (\ref{P+})\\
  				\vspace{4pt}
  				Calculate $\eta_k^s$ according to (\ref{SARAH-RHBB+})\\
  				\vspace{16pt}
  				
  				Update the iterate by		
  				$$w_{k+1}^s=w_k^s-\eta_k^s v_k^s$$
  				
  			}
  			$\widetilde{w}_s=w_m^s$
  		}
  	\end{algorithm}
  	
  	
  	\begin{algorithm}
  		\label{alg2}
  		\caption{mS2GD-RHBB/RHBB+}
  		\KwIn{$\widetilde{w}_0$, update frequency $m$, batch sizes $b, b_1, b_2$, constant sequence $\{\tilde{\eta}_0^s\}$, balance parameter $\gamma_2\geq1$, hedge base $\alpha$ and monotone decreasing function $h$, probability distribution $Q$.}
  		\vspace{2pt}
  		\KwOut{approximate solution $\widetilde{w}_s$.}
  		
  		\vspace{4pt}
  		\textbf{Outer Loop:} \For{$s = 1,2,...,$}
  		{$\widetilde{w}=\widetilde{w}_{s-1}$\\
  			\vspace{4pt}	
  			$w_0^s=\widetilde{w}$\\
  			\vspace{4pt}
  			$\varphi=\frac{1}{n} \sum_{i \in \Omega} \nabla f_i\left(\widetilde{w}\right)=\nabla P\left(\widetilde{w}\right)$\\
  			\vspace{4pt}
  			$\tilde{v}_0^s=\varphi$\\
  			\vspace{4pt}
  			$w_1^s=w_0^s-\tilde{\eta}_0^s \tilde{v}_0^s$\\
  			\vspace{4pt}
  			\textbf{Inner Loop:} \For{$k = 1,2,...,m-1$}
  			{   \vspace{4pt}
  				Pick subset $S \subset\{1, \ldots, n\}$ of size $b$ uniformly at random\\
  				\vspace{4pt}
  				Update the semi-stochastic gradient by\
  				$$\tilde{v}_k^s=\nabla P_S\left(w_k^s\right)-\nabla P_S(\widetilde{w})+\varphi$$
  				
  				Compute the step size $\tilde{\eta}_k^s$ by {\bf (1) Option I} or {\bf (2) Option II}: \\
  				\vspace{8pt}
  				\textbf{(1) Option I : RHBB rule}\\
  				\vspace{4pt}
  				Configure $Q$ as uniform probability distribution \\
  				\vspace{4pt}
  				Pick subset $S_1 \subset\{1, \ldots, n\}$ of size $b_1$ randomly according to $Q$\\
  				\vspace{4pt}
  				Pick subset $S_2 \subset\{1, \ldots, n\}$ of size $b_2$ randomly according to $Q$\\
  				\vspace{4pt}
  				Calculate $\tilde{\eta}_k^s$ according to (\ref{mS2GD-RHBB})\\
  				\vspace{16pt}
  				
  				\textbf{(2) Option II : RHBB+ rule}\\
  				\vspace{4pt}
  				Configure $Q$ to our needs \\
  				\vspace{4pt}
  				Pick subset $S_1 \subset\{1, \ldots, n\}$ of size $b_1$ randomly according to $Q$\\
  				\vspace{4pt}
  				Pick subset $S_2 \subset\{1, \ldots, n\}$ of size $b_2$ randomly according to $Q$\\
  				\vspace{4pt}
  				Compute $\nabla P_{S_1}^+, \nabla P_{S_2}^+$ according to (\ref{P+})\\
  				\vspace{4pt}
  				Calculate $\tilde{\eta}_k^s$ according to (\ref{mS2GD-RHBB+})\\
  				\vspace{16pt}
  				
  				Update the iterate:		
  				$$w_{k+1}^s=w_k^s-\tilde{\eta}_k^s \tilde{v_k}^s$$
  			}
  			$\widetilde{w}_s=w_m^s$
  		}
  	\end{algorithm}
  	
  	\newpage
  	
  	\begin{remark}
  		At beginning of each epoch $s$, constant step sizes $\eta_0^s$ and $\tilde{\eta}_0^s$ are used in the first deterministic step of full pass. Our RHBB or RHBB+ are placed in following stochastic stages to match recursive or variance reduced estimators and create smooth paths for convergence. Distribution $Q$ can be tailored to the needs of particular data sets.
  	\end{remark}
  	
  	
  	\section{Convergence Analysis}
  	%	\begin{definition}
  	Hereafter, we use following notations:
  	Set the batch correction $\overline{b}=\max\{|S_1|, |S_2|\}=\max\{b_1, b_2\}$. 
  	%	\end{definition}
  	%\begin{definition}
  	Under probability distribution $Q \sim\left\{q_1, q_2, \ldots q_{n}\right\}$, let%we define the $L_q$, $L_r$, $\mu_q$ and $\mu_{r}$ as
  	\begin{equation}
  	\label{L_u_Q}
  	L_q=\max _i \frac{L}{n\cdot q_i}, \quad L_r=\frac{L}{L_q}, \quad \mu_q=\min _i \frac{\mu}{n\cdot q_i}, \quad \mu_r = \frac{\mu_q}{\mu}.
  	\end{equation}
  	%	\end{definition}
  	
  	
  	We have straightforward results: $L_q\geq L$, $\mu_q\leq\mu$ and $L_r\leq1$, $\mu_r\leq1$. We obtain an approximate condition number of $\kappa^+ = \frac{L_q}{\mu_q}=\frac{\kappa}{L_r\mu_r}\geq\kappa$.
  	
  	\noindent
  	\textbf{Assumption 3} ({\bf Uniform boundness}). Iterative adaptor, $h(\cdot)$, is continuous on the bounded closed domain, monotone decreasing with respect to epoch count $s$, inner stage count $k$, there exist constants $\hat{\alpha}$, $\tilde{\alpha}$ such that
  	$$
  	\label{assumption3}
  	1<\tilde{\alpha}<\alpha^{h(\sigma_1s+\sigma_2k)}<\hat{\alpha}.
  	$$
  	
  	
  	\subsection{MB-SARAH-RHBB/RHBB+}
  	
  	\begin{lemma}
  		\label{lemma1}
  		Suppose Assumption 1, 2a and 3 hold. Subsets $S$, $S_1$, $S_2$ are selected uniformly at random of size $b, b_1, b_2$ respectively. Within the $s$-th epoch of MB-SARAH-RHBB, we have a summative boundary of $\mathbb{E}\left[\left\|\nabla P\left(w\right)\right\|^2\right]$ as follows
  		$$
  		\begin{aligned}
  		& \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right] \leq \frac{2 \mu \overline{b} L}{\hat{\alpha} \gamma L+(1-\tilde{\alpha})\gamma\mu} \mathbb{E}\left[P\left(w_0^s\right)-P\left(w_*\right)\right] \\
  		& +\sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right]-\left(1-\frac{\hat{\alpha} \gamma L^2+(1-\tilde{\alpha}) \gamma L \mu}{\mu \overline{b} L}\right) \sum_{k=0}^m \mathbb{E}\left[\left\|v_k^s\right\|^2\right].
  		\end{aligned}
  		$$
  		Furthermore, if Assumption 2b holds, and subsets $S_1$ and $S_2$ are sampled according to the probability distribution $Q$ of size $b_1$ and $b_2$. Then, we derive an internal and summative boundary for MB-SARAH-RHBB+ as
  		$$
  		\begin{aligned}
  		& \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right] \leq \frac{2 \mu_q \overline{b} L_q}{\hat{\alpha} \gamma L_q+(1-\tilde{\alpha})\gamma\mu_q} \mathbb{E}\left[P\left(w_0^s\right)-P\left(w_*\right)\right] \\
  		& +\sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] - \left(1-\frac{\hat{\alpha} \gamma L L_q+(1-\tilde{\alpha}) \gamma L \mu_q}{\mu_q \overline{b} L_q}\right) \sum_{k=0}^m \mathbb{E}\left[\left\|v_k^s\right\|^2\right].
  		\end{aligned}
  		$$
  	\end{lemma}
  	\proof 
  	According to the strong convexity (\ref{C}) and the smoothness (\ref{L}) of $P(\cdot)$, we obtain an upper boundary for RHBB step size 
  	$$\begin{aligned}
  	(\eta_k^s)^{\mathrm{RHBB}}
  	& = \frac{\gamma}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}\left(w_k^s\right)-\nabla P_{S_1}\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& \leq \frac{\gamma}{\overline{b}}\cdot \left( \frac{\hat{\alpha}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\mu\left\|w_k^s-w_{k-1}^s\right\|^2}+\frac{(1-\tilde{\alpha})\cdot\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}{L\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& = \frac{\gamma}{\overline{b}}\cdot\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L}.\\
  	\end{aligned}$$
  	By the definition of $P^+(\cdot)$ (\ref{P+}) and $L_q$, $\mu_q$ (\ref{L_u_Q}), the individual $L$-smoothness of $f_i(\cdot)$ implies the uniform $L_q$-smoothness of $P^+(\cdot)$, we achieve
  	$$\begin{aligned}
  	(\eta_k^s)^{\mathrm{RHBB+}}& = \frac{\gamma}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}^+\left(w_k^s\right)-\nabla P_{S_1}^+\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& \leq \frac{\gamma}{\overline{b}}\cdot\left(\frac{\hat{\alpha}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\frac{1}{|S_1|} \sum_{i \in S_1} \frac{\mu}{{n}q_i}}+\frac{\left(1-\tilde{\alpha}\right)\cdot\left\|\nabla P^+_{S_2}\left(w_k^s\right)-\nabla P^+_{S_2}\left(w_{k-1}^s\right)\right\|^2}{L_q\left\|\nabla P^+_{S_2}\left(w_k^s\right)-\nabla P^+_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& = \frac{\gamma}{\overline{b}}\cdot\frac{\hat{\alpha} L_q+(1-\tilde{\alpha})\mu_q}{\mu_q L_q}.\\
  	\end{aligned}$$
  	On the basis of (\ref{inequation1}), we have
  	$$\mathbb{E}\left[P\left(w_{k+1}^s\right)\right]\leq \mathbb{E}\left[P\left(w_k^s\right)\right]-\eta_k^s \mathbb{E}\left[\nabla P\left(w_k^s\right)^{\top} v_k^s\right]+\frac{L (\eta_k^s)^2}{2} \mathbb{E}\left[\left\|v_k^s\right\|^2\right].$$
  	Due to the fact $\theta_1^T\theta_2 = \frac{1}{2}\left[\|\theta_1\|^2+\|\theta_2\|^2-\|\theta_1-\theta_2\|^2\right]$, substituting the related boundary of RHBB step size, we obtain
  	$$
  	\begin{aligned}
  	& \mathbb{E}\left[P\left(w_{k+1}^s\right)\right]\\ 
  	& \leq \mathbb{E}\left[P\left(w_k^s\right)\right]-\frac{\gamma\hat{\alpha} L+ \gamma(1-\tilde{\alpha})\mu}{\overline{b}\mu L} \mathbb{E}\left[\nabla P\left(w_k^s\right)^{\top} v_k^s\right]+\frac{L \gamma^2}{2 \overline{b}^2} \cdot \left(\frac{\hat{\alpha} L+ (1-\tilde{\alpha})\mu}{\mu L}\right)^2\mathbb{E}\left[\left\|v_k^s\right\|^2\right] \\
  	& =\mathbb{E}\left[P\left(w_k^s\right)\right]-\frac{\gamma\hat{\alpha} L+ \gamma(1-\tilde{\alpha})\mu}{2 \overline{b}\mu L} \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right]+\frac{\gamma\hat{\alpha} L+ \gamma(1-\tilde{\alpha})\mu}{2 \overline{b}\mu L}\mathbb{E}\left[\| \nabla P\left(w_k^s\right)\right.\left.-v_k^s \|^2\right]\\
  	&-\frac{\hat{\alpha} L+ (1-\tilde{\alpha})\mu}{\mu L}\cdot\left(\frac{\gamma}{2 \overline{b}}-\frac{L \gamma^2}{2 \overline{b}^2}\cdot\frac{\hat{\alpha} L+ (1-\tilde{\alpha})\mu}{\mu L}\right) \mathbb{E}\left[\left\|v_k^s\right\|^2\right].
  	\end{aligned}
  	$$
  	Adding up $k$ from $0$ to $m$, we have
  	$$\begin{aligned} \mathbb{E}\left[P\left(w_{m+1}^s\right)\right]  & \leq  \mathbb{E}\left[P\left(w_0^s\right)\right]-\sum_{k=0}^m\frac{\gamma}{2\overline{b}}\cdot\frac{ \hat{\alpha} L +(1-\tilde{\alpha})\mu}{\mu L} \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right]\\ & +\sum_{k=0}^m\frac{\gamma}{2\overline{b}}\cdot\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L} \mathbb{E}\left[\| \nabla P\left(w_k^s\right)\right.  \left.-v_k^s \|^2\right]\\ &- \sum_{k=0}^m\frac{\gamma}{2\overline{b}}\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L}\left(1-\frac{L \gamma}{ \overline{b}}\cdot\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L}\right) \mathbb{E}\left[\left\|v_k^s\right\|^2\right].\end{aligned}$$
  	Since $w_*=\arg \min _w P(w)$, we ascertain that
  	$$\begin{aligned}\sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right] &\leq \frac{2 \overline{b}\mu L}{\gamma\hat{\alpha} L+\gamma(1-\tilde{\alpha})\mu} \mathbb{E}\left[P\left(w_0\right)-P\left(w_*\right)\right]+\sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] \\ & -\left(1-\frac{L \gamma}{\overline{b}}\cdot\frac{\hat{\alpha} L+\left(1-\tilde{\alpha}\right)\mu}{\mu L}\right) \sum_{k=0}^m \mathbb{E}\left[\left\|v_k^s\right\|^2\right].\end{aligned}$$
  	
  	By substituting RHBB+ step size and its corresponding boundary, the remaining parts of Lemma \ref{lemma1} can be proven in a parallel manner. We will no longer expand in detail.
  	
  	\begin{lemma}
  		\label{lemma2}
  		Suppose Assumption 1, 2a hold. Subsets $S$, $S_1$, $S_2$ are selected uniformly at random of size $b, b_1,  b_2$, respectively. Within the $s$-th epoch of MB-SARAH-RHBB, for any inner $1\leq k \leq m$, the expected distance of the full gradient to the recursive estimates is bounded by
  		\begin{equation}
  		\label{lemma21}
  		\mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] \leq \frac{n-b}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L^2+\left(1-\tilde{\alpha}\right)\gamma\mu L}{\overline{b} \mu L}\right)^2\sum_{j=1}^k \mathbb{E}\left[\left\|v_{j-1}^s\right\|^2\right].
  		\end{equation}
  		If Assumption 2b holds further and subsets $S_1$, $S_2$ are sampled according to the probability distribution $Q$. Within the $s$-th epoch of MB-SARAH-RHBB+, for any inner $1\leq k \leq m$, the expected deviation is limited by 
  		$$
  		\mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] \leq \frac{L_r^2\left(n-b\right)}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L_q + (1-\tilde{\alpha})\gamma  \mu_q }{\overline{b} \mu_q }\right)^2\sum_{j=1}^k \mathbb{E}\left[\left\|v_{j-1}^s\right\|^2\right].
  		$$
  	\end{lemma}
  	\proof  	
  	By Lemma 3 in \cite{nguyen2}, we readily obtain
  	$$ 
  	\mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] \leq \frac{1}{b}\left(\frac{n-b}{n-1}\right) L^2  \sum_{j=1}^k \left(\eta_{j-1}^s\right)^2 \mathbb{E}\left[\left\|v_{j-1}^s\right\|^2\right].
  	$$
  	By replacing the boundaries of step size, we obtain the conclusions in Lemma \ref{lemma2}.
  	
  	\vspace{8pt}
  	Analysis over the inner stages is supplied in subsequent Theorem \ref{theorem1}.
  	\begin{theorem}
  		\label{theorem1}
  		Suppose Assumption 1, 2a hold. Pick subsets $S, S_1, S_2 \subset
  		\{1, \ldots, n\}$ of size $b, b_1, b_2$ uniformly at random. In the $s$-th epoch of MB-SARAH-RHBB, for any finite $m>1$ with
  		\begin{equation}
  		\label{theorem1i}
  		\frac{m\left(n-b\right)}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L^2+\left(1-\tilde{\alpha}\right)\gamma\mu L}{\mu L\overline{b}}\right)^2+\frac{\hat{\alpha} \gamma L+(1-\tilde{\alpha}) \gamma \mu}{\mu \overline{b}} \leq 1,
  		\end{equation}
  		the inner iterates $\{w_k^s\}$ converges sublinearly in expectation:
  		\begin{equation}
  		\label{rate1}
  		\mathbb{E}\left[\left\|\nabla P\left(w_m^s\right)\right\|^2\right] \leq \frac{2 \overline{b} \mu L}{\gamma(m+1)(\hat{\alpha} L+(1-\tilde{\alpha})\mu)}\left[P\left(w_0^s\right)-P\left(w_*\right)\right].
  		\end{equation}
  		If Assumption 2b holds further and subsets $S_1$, $S_2$ are sampled according to the probability distribution $Q$. Within the $s$-th epoch of MB-SARAH-RHBB+, for any finite $m>1$ with
  		\begin{equation}
  		\label{theorem1ii}
  		\frac{mL_r^2\left(n-b\right)}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L_q+\left(1-\tilde{\alpha}\right)\gamma\mu_q}{\mu_q \overline{b}}\right)^2+L_r\frac{\hat{\alpha} \gamma  L_q+(1-\tilde{\alpha}) \gamma  \mu_q}{\mu_q \overline{b} } \leq 1,
  		\end{equation}
  		the inner $\left\|\nabla P\left(w_k^s\right)\right\|^2$ has sublinear convergence with increasing $m$:
  		$$
  		\mathbb{E}\left[\left\|\nabla P\left(w_m^s\right)\right\|^2\right] \leq \frac{2 \overline{b} \mu_q L_q}{\gamma(m+1)(\hat{\alpha} L_q+(1-\tilde{\alpha})\mu_q)}\left[P\left(w_0^s\right)-P\left(w_*\right)\right].
  		$$
  	\end{theorem}
  	\proof  
  	Since $\left\|\nabla P\left(w_0^s\right)-v_0^s\right\|^2=0$, we apply (\ref{lemma21}) in the Lemma \ref{lemma2} and sum over $k=0, ..., m$ and obtain
  	$$\begin{aligned}  \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right] & \leq \frac{\left(\hat{\alpha}\gamma L^2+\left(1-\tilde{\alpha}\right)\gamma\mu L\right)^2}{b \overline{b}^2 \mu^2 L^2}\cdot \left(\frac{n-b}{n-1}\right)\\ & \cdot\left(m \mathbb{E}\left[\left\|v_0^s\right\|^2\right]+(m-1) \mathbb{E}\left[\left\|v_1^s\right\|^2\right]+\ldots+\mathbb{E}\left[\left\|v_{m-1}^s\right\|^2\right]\right).\end{aligned}$$
  	Plugging (\ref{theorem1i}) in, we hence have
  	$$\begin{aligned} & \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right]-\left(1-\frac{L \gamma}{\overline{b}}\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L}\right) \sum_{k=0}^m \mathbb{E}\left[\left\|v_k\right\|^2\right] \\ & \leq	\left(\left(\frac{\hat{\alpha}L+\left(1-\tilde{\alpha}\right)\mu}{\mu L}\right)^2\cdot\frac{L^2 \gamma^2}{b \overline{b}^2}\left(\frac{n-b}{n-1}\right) m-\left(1-\frac{\hat{\alpha} \gamma L+(1-\tilde{\alpha}) \gamma \mu)}{\mu \overline{b}}\right)\right)  \\ &
  	\times \left(\sum_{k=1}^m\mathbb{E}\left[\left\|v_{k-1}^s\right\|^2\right]\right) \leq 0. \end{aligned}$$
  	Using the Lemma \ref{lemma1}, we further derive
  	$$
  	\begin{aligned}
  	& \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right] \leq \frac{2 \mu \overline{b} L}{\hat{\alpha} \gamma L+(1-\tilde{\alpha})\gamma\mu} \mathbb{E}\left[P\left(w_0^s\right)-P\left(w_*\right)\right] \\
  	& +\sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)-v_k^s\right\|^2\right]-\left(1-\frac{\hat{\alpha} \gamma L^2+(1-\tilde{\alpha}) \gamma L \mu}{\mu \overline{b} L}\right) \sum_{k=0}^m \mathbb{E}\left[\left\|v_k^s\right\|^2\right].
  	\end{aligned}
  	$$
  	By the definition of $\widetilde{w}_s$ and the outer update rule $\widetilde{w}_s=w_m^s$, we ascertain
  	$$\begin{aligned} \mathbb{E}\left[\left\|\nabla P\left(w_m^s\right)\right\|^2\right] & =\frac{1}{m+1} \sum_{k=0}^m \mathbb{E}\left[\left\|\nabla P\left(w_k^s\right)\right\|^2\right] \\ & \leq \frac{2 \overline{b} \mu L}{\gamma(m+1)(\hat{\alpha} L+(1-\tilde{\alpha})\mu)} \mathbb{E}\left[P\left(w_0^s\right)-P\left(w_*\right)\right].\end{aligned}$$
  	
  	
  	By substituting RHBB+ step size and the corresponding upper boundary, the remaining parts of Theorem \ref{theorem1} can be proven similarly in parallel. We will not spread them in detail.
  	
  	\vspace{8pt}
  	Theorem \ref{theorem1} demonstrates sublinear convergence in the inner stages. We're sufficient to fix $s=1$ or dispose of the outer epoch, Algorithm \ref{alg1} degenerates to MB-SARAH-IN-RHBB/RHBB+ (see \cite{nguyen2} for reference). To obtain an $\varepsilon$-accurate solution in MB-SARAH-IN-RHBB, the number of iterations, $m$, is put up so that $\mathbb{E}\left[\|\nabla P(w_m)\|^2\right] \leq \varepsilon$, which suggests 
  	\begin{equation}
  	\label{con1}
  	\frac{2 \overline{b} \mu L}{\gamma\left(m+1\right)\left(\hat{\alpha} L+\left(1-\tilde{\alpha}\right)\mu\right)}\left[P\left(w_0\right)-P\left(w_*\right)\right] \leq \varepsilon.
  	\end{equation}
  	Assume $P\left(w_0\right)-P\left(w_*\right)=\sigma$, (\ref{con1}) implies $m_{RH} = \lceil \frac{2\overline{b}\mu \sigma\kappa}{\varepsilon \gamma\left(\hat{\alpha} \kappa+1-\tilde{\alpha}\right)}-1\rceil$. Compare $m_{R} = \lceil \frac{2\overline{b}\mu \sigma}{\varepsilon \gamma}-1\rceil$ in \cite{yang}, we obtain $\left(m_{RH}<m_{R}\right)$ due to $L > \mu$. In MB-SARAH-RHBB+, that's $m_{RH+} = \lceil \frac{2\overline{b}\mu_q \sigma\kappa^+}{\epsilon \gamma(\hat{\alpha} \kappa^+ +1-\tilde{\alpha})}-1\rceil$ to achieve the same $\varepsilon$-accuracy. Regardless of rounding errors, it's very likely that $\left(m_{RH+}\leq m_{RH}\right)$.
  	
  	
  	Ineq. (\ref{con1}) indicates we can locally manipulate $\hat{\alpha}$, $\tilde{\alpha}$ in early epochs (use a different $h$ in early epochs temporarily), to address the issue of a poor initial $w_0^0$ (or $w_0^s$) with unexpectedly large $\sigma$, which can not be handled in lots of  existing methods such as \cite{yang} \cite{yang3} \cite{nguyen} \cite{nguyen2} \cite{yangy} \cite{liu3}, etc.  
  	
  	
  	For a class of ill-conditioned objective functions $P(\cdot)$ with $L \gg \mu$, (\ref{con1}) implies that $\left(m_{RH+}+1\leq m_{RH}+1\approx \frac{1}{\hat{\alpha}}\left(m_{R}+1\right)\right)$, the inner speedup is approximately proportional to $\mathcal{O}\left(\frac{1}{\hat{\alpha}}\right)$. Hence, we may tolerate towards the functional form and decay rate of the $h$, while remain mindful to the $\hat{\alpha}$ in ill-conditioning. 
  	
  	
  	We evaluate the workload in terms of incremental first order oracle (IFO) complexity model in Agarwal et al.\cite{agarwal}. In \cite{ghadimi}, it's SFO under stochastic frameworks. MB-SARAH-RHBB/RHBB+ are both IFO algorithms that are specified through calls to an IFO, regardless of $P(\cdot)$. Each epoch invokes SFO at most $2bm$ times in recursive gradient evaluations (\ref{v}), which corresponds to an overall cost of $\mathcal{O}\left(n+2bm\right)$ SFOs. It's common practice to bound the smallest number of oracles to obtain $\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right)\right\|^2\right] <\varepsilon$. Since the adaptor, $h$, is freely specified, we can force $\hat{\alpha}$ and $\tilde{\alpha}$ to be arbitrarily large and small. When setting $L+\frac{1-\tilde{\alpha}}{\hat{\alpha}}\mu=\mathcal{O}\left(L\right)$ and $L_q+\frac{1-\tilde{\alpha}}{\hat{\alpha}}\mu_q=\mathcal{O}\left(L_q\right)$, it's sufficient to choose $m =\mathcal{O}\left(\frac{\overline{b}\mu}{\varepsilon \gamma\hat{\alpha}}\right)$ and $m =\mathcal{O}\left(\frac{\overline{b}\mu_q}{\varepsilon \gamma\hat{\alpha}}\right)$, correspondingly. By bounding the number of iterations, we obtain the following conclusion for complexity.
  	
  	\begin{corollary}
  		Suppose Assumption 1 and 2a hold. MB-SARAH-IN-RHBB converges sublinearly in expectation with a rate of $\mathcal{O}\left(\mu\overline{b} / \gamma m \hat{\alpha}\right)$, the complexity to achieve an $\varepsilon$-accurate solution is $n+2bm=\mathcal{O}\left(n+\frac{b\overline{b}\mu}{\varepsilon \gamma\hat{\alpha}}\right)$. Suppose Assumption 2b holds further. MB-SARAH-IN-RHBB+ owns sublinear convergence rate of  $\mathcal{O}\left(\overline{b}\mu_q/\gamma m \hat{\alpha}\right)$, the complexity for the   $\varepsilon$-accuracy corresponds to $n+2bm=\mathcal{O}\left(n+\frac{b\overline{b}\mu_q}{\varepsilon \gamma\hat{\alpha}}\right)$ units of work.
  	\end{corollary}
  	
  	Let's proceed to theoretical analysis with multiple outer steps.
  	\begin{theorem}
  		\label{theorem2}
  		Suppose Assumption 1, 2a hold. Pick subsets $S, S_1, S_2 \subset\{1, \ldots, n\}$ of size $b, b_1, b_2$ uniformly at random. In MB-SARAH-RHBB, for any $s>1$ with
  		$$
  		\frac{m\left(n-b\right)}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L^2+\left(1-\tilde{\alpha}\right)\gamma\mu L}{\mu L\overline{b}}\right)^2+\frac{\hat{\alpha} \gamma L+(1-\tilde{\alpha}) \gamma \mu}{\mu \overline{b}} \leq 1,
  		$$
  		the outer iterates $\{\widetilde{w}_{s}\}$ converges linearly in expectation:
  		\begin{equation}
  		\label{rate2}
  		\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right)\right\|^2\right] \leq\left(\frac{\kappa\overline{b}}{\gamma\left(m+1\right)\left(\hat{\alpha}\kappa+1-\tilde{\alpha}\right)}\right)^s\left\|\nabla P\left(\widetilde{w}_0\right)\right\|^2.
  		\end{equation}
  		If Assumption 2b holds further and subsets $S_1$, $S_2$ are sampled according to the probability distribution $Q$. In MB-SARAH-RHBB+, for any $s>1$ with
  		$$
  		\frac{mL_r^2\left(n-b\right)}{b\left(n-1\right)}\left(\frac{\hat{\alpha}\gamma L_q+\left(1-\tilde{\alpha}\right)\gamma\mu_q}{\mu_q \overline{b}}\right)^2+L_r\frac{\hat{\alpha} \gamma  L_q+(1-\tilde{\alpha}) \gamma  \mu_q}{\mu_q \overline{b} } \leq 1,
  		$$
  		then the outer iterates $\{\widetilde{w}_{s}\}$ converges linearly in expectation:
  		$$
  		\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right)\right\|^2\right] \leq\left(\frac{\mu_r\kappa^+\overline{b}}{\gamma\left(m+1\right)\left(\hat{\alpha}\kappa^++1-\tilde{\alpha}\right)}\right)^s\left\|\nabla P\left(\widetilde{w}_0\right)\right\|^2
  		.$$
  	\end{theorem}
  	\proof
  	Since $w_0^s=\widetilde{w}_{s-1}$ and $\widetilde{w}_s=w_m^s$, we apply the Theorem \ref{theorem1} and have
  	$$
  	\begin{aligned}
  	\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right) \mid \widetilde{w}_{s-1}\right\|^2\right] & =\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right) \mid w_0^s\right\|^2\right] \\
  	& \leq \frac{2 \overline{b}}{\gamma(m+1)}\cdot\frac{\mu L}{\hat{\alpha} L+(1-\tilde{\alpha})\mu} \mathbb{E}\left[P\left(w_0^s\right)-P\left(w_*\right)\right]. \\
  	\end{aligned}
  	$$
  	By taking expectation and using the convexity (\ref{convex1}), we obtain
  	$$
  	\begin{aligned}
  	\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right)\right\|^2\right] & \leq \frac{\overline{b} L}{\hat{\alpha} \gamma (m+1)L+(1-\tilde{\alpha})\gamma (m+1)\mu} \mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_{s-1}\right)\right\|^2\right] \\
  	& \leq\left[\frac{\overline{b} L}{\hat{\alpha} \gamma (m+1)L+(1-\tilde{\alpha})\gamma (m+1)\mu}\right]^s\left\|\nabla P\left(\widetilde{w}_0\right)\right\|^2.
  	\end{aligned}
  	$$
  	
  	
  	By substituting RHBB+ step size and the corresponding upper boundary, the remaining parts in Theorem \ref{theorem2} follow a similar line of reasoning.
  	
  	\vspace{8pt}
  	Assume $\left\|\nabla P\left(\widetilde{w}_0\right)\right\|^2=\zeta$, to obtain $\mathbb{E}\left[\left\|\nabla P\left(\widetilde{w}_s\right)\right\|^2\right] <\varepsilon$ in MB-SARAH-RHBB, the number of the outer epoch, $s$, must satisfy
  	$$
  	\left(\frac{\kappa\overline{b}}{\gamma\left(m+1\right)\left(\hat{\alpha}\kappa+1-\tilde{\alpha}\right)}\right)^s\cdot\zeta\leq \varepsilon,
  	$$
  	which infers  $s_{RH}=\lceil \frac{log(\zeta)-log(\varepsilon)}{log(\hat{\alpha}\kappa+1-\tilde{\alpha})-log(\kappa)+log(\gamma(m+1))-log(\overline{b})}\rceil$. Compared to \cite{yang} of $s_{R}=\lceil\frac{log(\zeta)-log(\epsilon)}{log(\gamma(m+1))-log(\overline{b})}\rceil$, we notice that the overhead of the outer epoch decreases. In MB-SARAH-RHBB+, both $\mu_r\leq1$, $\kappa^+\geq\kappa$ implies $\left(s_{RH+}\leq s_{RH}\right)$, indicating the iterative cost can be further reduced via effective sampling. In ill-conditioning, we then have $\left(s_{RH^+}\leq s_{RH}\approx\lceil\frac{log(\zeta)-log(\varepsilon)}{log(\hat{\alpha})+log(\gamma(m+1))-log(\overline{b})}\rceil\right)$.
  	
  	
  	Furthermore, our analysis can be refined to obtain smaller rate constants in some gradient dominated scenarios (see in Polyak et al.\cite{polyak}, Reddi et al. \cite{reddi}). If $P(\cdot)$ is $\delta$-gradient dominated with $\delta<\frac{1}{2\mu}$, we have another rate constants
  	$$
  	\rho'_{RH}=\frac{2\overline{b}\mu L\delta}{\gamma(m+1)\left(\hat{\alpha}L+(1-\tilde{\alpha})\mu\right)}, \quad
  	\rho'_{RH+}=\frac{2 \overline{b} \mu_q L_q\delta}{\gamma(m+1)(\hat{\alpha} L_q+(1-\tilde{\alpha})\mu_q)},
  	$$
  	for MB-SARAH-RHBB and MB-SARAH-RHBB+ respectively. The theoretical convergence speed increases further in virtue of $2\mu\delta<1$.
  	
  	\begin{corollary}
  		\label{corollary2}
  		Suppose Assumption 1 and 2a hold. MB-SARAH-RHBB converges linearly and the total complexity to achieve an $\varepsilon$-accurate solution is $\mathcal{O}\left(\left(n+\frac{b\overline{b}\mu}{\varepsilon \gamma\hat{\alpha}}\right)\log(1/\varepsilon)\right)$. Suppose Assumption 2b holds further. MB-SARAH-RHBB+ has linear convergence rate, the overall complexity for the same $\varepsilon$-accuracy is of order $\mathcal{O}\left(\left(n+\frac{b\overline{b}\mu_q}{\varepsilon \gamma\hat{\alpha}}\right)\log\left(1/\varepsilon\right)\right)$.
  	\end{corollary}
  	
  	
  	Compared with MB-SARAH \cite{nguyen2}, MB-SARAH-RBB \cite{yang}, MB-SARAH-HD \cite{yang5}, iSARAH-BB \cite{yangy}, Corollary \ref{corollary2} indicates MB-SARAH-RHBB/RHBB+ have lower complexity when using an appropriate adaptor $h(\cdot)$ and a proper $\overline{b}$.
  	
  	
  	\subsection{mS2GD-RHBB/RHBB+}
  	We exhibit the following lemma, based on Lemma $2$ from \cite{yang2}, to start the convergence analysis for mS2GD-RHBB/RHBB+.  
  	\begin{lemma}
  		\label{lemma3}
  		Suppose Assumption 1, 2a hold. The subset $S$ is selected uniformly at random with size $b$. Then, we have an upper bound for the semi-stochastic estimate $\tilde{v}$ as follows
  		\begin{equation}
  		\mathbb{E}\left[\left\|\tilde{v}_k^s\right\|^2\right] \leq \frac{4 L}{b}\left[P\left(w_{k-1}^s\right)\!-P\left(w_*\right)\!+P(\widetilde{w}_{s-1})\!-P\left(w_*\right)\right] \\
  		+\frac{2}{b}\left\|\nabla P\left(w_{k-1}^s\right)\right\|^2 .
  		\end{equation}
  	\end{lemma}
  	\proof 
  	Before the formal proof, let's define the $j$-th estimate at $w_k^s$ as $\tilde{v}_j=\nabla f_j\left(w_k^s\right)-\nabla f_j(\widetilde{w}_{s-1})+\nabla P(\widetilde{w}_{s-1})$, where $\nabla f_j$ represents the gradient of the $j$-th component function. According to $\tilde{v}_k^s=\frac{1}{b}\sum_{j \in S} \tilde{v}_j$, we obtain
  	$$
  	\begin{aligned}
  	\mathbb{E}\left[\left\|\tilde{v}_k^s\right\|^2\right]& =\frac{1}{b^2} \mathbb{E}\left[\|\sum_{j \in S} \tilde{v}_j\|^2\right] \\
  	& =\frac{1}{b^2} \mathbb{E}\left[\|\sum_{j \in S^{\prime}} \tilde{v}_j\|^2+2(\sum_{j \in S^{\prime}} \tilde{v}_j)^T (\tilde{v}_{j \in S-S^{\prime}})+\|\tilde{v}_{j \in S-S^{\prime}}\|^2\right] \\
  	& =\frac{1}{b^2}\left[\mathbb{E}[\|\sum_{j \in S^{\prime}} \tilde{v}_j\|^2]+2\|\nabla P\left(w_{k-1}^s\right)\|^2 +\mathbb{E}[\|\tilde{v}_{j \in S-S^{\prime}}\|^2]\right] \\
  	& =\cdots\\
  	& =\frac{1}{b^2}\left[\sum_{j \in S} \mathbb{E}\left[\|\tilde{v}_j\|^2\right]+2\left(b-1\right)\|\nabla P\left(w_{k-1}^s\right)\|^2\right] \\
  	& \leq \frac{1}{b^2}\left[\sum_{j \in S} \mathbb{E}\left[\|\tilde{v}_j\|^2\right]+2 b\|\nabla P\left(w_{k-1}^s\right)\|^2\right] \\
  	& \leq \frac{4 L}{b}\left[P\left(w_{k-1}^s\right)-P\left(w_*\right)+P(\widetilde{w}_{s-1})-P\left(w_*\right)\right]+\frac{2}{b}\|\nabla P\left(w_{k-1}^s\right)\|^2, \\
  	\end{aligned}
  	$$
  	where the subset $S^{\prime} \subset S$ with the
  	number of members of $|S-S^{\prime}|=1$. The last equality follows   Lemma 3 in \cite{yang2}.
  	
  	\vspace{8pt}
  	Based on Lemma \ref{lemma3}, we present following Theorem 3 to demonstrate the linear convergence of mS2GD-RHBB/RHBB+.
  	\begin{theorem}
  		\label{theorem3}
  		Suppose Assumption 1, 2a hold. Let $\kappa_r=\hat{\alpha}\kappa+1-\tilde{\alpha}$, and pick the subsets $S, S_1, S_2 \subset\{1, \ldots, n\}$ of size $b, b_1, b_2$ uniformly at random. Assume that $b\overline{b}>4\kappa_r\gamma_2$, and $h(\cdot)$ is chosen such that
  		\begin{equation}
  		\label{rate3}
  		\tilde{\rho}_1=\frac{\kappa b \overline{b}^2}{m\gamma_2\kappa_r\left(b \overline{b}-4\gamma_2\kappa_r\right)}+\frac{2\gamma_2 \kappa_r}{b \overline{b}-4\gamma_2\kappa_r}<1.
  		\end{equation}
  		Then, the mS2GD-RHBB converges linearly in expectation with rate $\tilde{\rho}_1$, that's
  		$$
  		\mathbb{E}\left[P\left(\widetilde{w}_s\right)\right]-P\left(w_*\right) \leq (\tilde{\rho}_{1})^s\left[P\left(\widetilde{w}_0\right)-P\left(w_*\right)\right].
  		$$
  		If Assumption 2b holds further, let $\kappa_r^+=\hat{\alpha} \kappa^+ +1-\tilde{\alpha}$, and sample subsets $S_1, S_2 \subset\{1, \ldots, n\}$ according to the probability distribution $Q$. Assume that $b\overline{b}>4\kappa_r^+\gamma_2L_r$, and $h(\cdot)$ is chosen such that
  		\begin{equation}
  		\label{rate4}
  		\tilde{\rho}_{2}=\frac{\mu_r\kappa^+b\overline{b}^2}{m\gamma_2\kappa_r^+\left(b \overline{b}-4\gamma_2\kappa_r^+ L_r\right)}+\frac{2 \gamma_2\kappa_r^+L_r}{b \overline{b}-4\gamma_2\kappa_r^+L_r}<1.
  		\end{equation}
  		Then, the mS2GD-RHBB+ converges linearly in expectation with rate $\tilde{\rho}_2$, that's 
  		$$
  		\mathbb{E}\left[P\left(\widetilde{w}_s\right)\right]-P\left(w_*\right) \leq (\tilde{\rho}_{2})^s\left[P\left(\widetilde{w}_0\right)-P\left(w_*\right)\right].
  		$$
  	\end{theorem}
  	\proof 
  	By Lemma \ref{lemma3} and $\mathbb{E}\left[\tilde{v}_{k-1}^s\right]=\nabla P\left(w_{k-1}^s\right)$, we obtain 
  	$$\begin{aligned} &\mathbb{E}\left[\left\|w_k^s-w_*\right\|^2\right]\\
  	& = \mathbb{E}\left[\left\|w_{k-1}^s-\eta_{k-1}^s \tilde{v}_{k-1}^s-w_*\right\|_2^2\right]\\
  	& =  \left\|w_{k-1}^s-w_*\right\|_2^2-2 \tilde{\eta}_{k-1}^s \mathbb{E}\left[\left(w_{k-1}^s-w_*\right)^T \tilde{v}_{k-1}^s\right] +(\tilde{\eta}_{k-1}^s)^2 \mathbb{E}\left[\left\|\tilde{v}_{k-1}^s\right\|^2\right] \\ 
  	& \leq\left\|w_{k-1}^s-w_*\right\|^2-2 \tilde{\eta}_{k-1}^s\left(w_{k-1}^s-w_*\right)^T \nabla P\left(w_{k-1}^s\right) \\
  	&+\frac{4 L (\tilde{\eta}_{k-1}^s)^2}{b}\left[P\left(w_{k-1}^s\right)\right.   \left.-P\left(w_*\right)+P(\widetilde{w}_{s-1})-P\left(w_*\right)\right]+\frac{2 (\tilde{\eta}_{k-1}^s)^2}{b}\left\|\nabla P  \left(w_{k-1}^s\right)\right\|^2 \\  
  	& \leq \left\|w_{k-1}^s-w_*\right\|^2-2 \tilde{\eta}_{k-1}^s\left[P\left(w_{k-1}^s\right)-P\left(w_*\right)\right] \\ 
  	&+ \frac{4 L (\tilde{\eta}^s_{k-1})^2}{b}\left[P\left(w_{k-1}^s\right)-P\left(w_*\right)\right. \left.+P(\widetilde{w}_{s-1})-P\left(w_*\right)\right]+\frac{2 (\tilde{\eta}_{k-1}^s)^2}{b}\left\|\nabla P\left(w_{k-1}^s\right)\right\|^2 \\  
  	&\leq \left\|w_{k-1}^s-w_*\right\|^2-2 \tilde{\eta}_{k-1}^s\left(1-\frac{4 L \tilde{\eta}_{k-1}^s}{b}\right) \left[P\left(w_{k-1}^s\right)-P\left(w_*\right)\right]\\  &+\frac{4 L (\tilde{\eta}^s_{k-1})^2}{b} \cdot\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right],
  	\end{aligned}$$
  	where we use the convexity of $P(\cdot)$ in the second inequality and (\ref{L2}) in the last.
  	
  	
  	We derive the upper boundary for RHBB step size in mS2GD as follows
  	$$\begin{aligned}
  	(\tilde{\eta}_k^s)^{\mathrm{RHBB}}& = \frac{\gamma_2}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}\left(w_k^s\right)-\nabla P_{S_1}\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& \leq \frac{\gamma_2}{\overline{b}}\cdot \left( \frac{\hat{\alpha}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\mu\left\|w_k^s-w_{k-1}^s\right\|^2}+\frac{(1-\tilde{\alpha})\cdot\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}{L\left\|\nabla P_{S_2}\left(w_k^s\right)-\nabla P_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& = \frac{\gamma_2}{\overline{b}}\cdot\frac{\hat{\alpha} L+(1-\tilde{\alpha})\mu}{\mu L}.
  	\end{aligned}$$
  	
  	Plugging it in, we have
  	$$\begin{aligned} \mathbb{E}\left\|w_k^s-w_*\right\|^2 & \leq  \left\|w_{k-1}^s-w_*\right\|^2 \\&-\frac{2\hat{\alpha}\gamma_2 L+2(1-\tilde{\alpha})\gamma_2\mu}{\mu L\overline{b}}\left(1-\frac{4 L(\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu)}{b \overline{b}\mu L}\right)\left[P\left(w_{k-1}^s\right)-P\left(w_*\right)\right] \\ & +\frac{4 L}{b \overline{b}^2}\left(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L}\right)^2\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right].\end{aligned}$$
  	
  	
  	By the definition of $\widetilde{w}_{s-1}$ in mS2GD-RHBB, we have (see in \cite{yang}\cite{konevcny})
  	$$
  	\mathbb{E}\left[P\left(\widetilde{w}_{s}\right)\right]=\frac{1}{m} \sum_{k=1}^m \mathbb{E}\left[P\left(w_k^s\right)\right].
  	$$
  	
  	
  	By summing over the previous inequality over $k$, we take expectation conditioned on history randomness. Since $\widetilde{w}_s=w_m^s$, $w_0^s=\widetilde{w}_{s-1}$, we obtain
  	$$\begin{aligned} & \mathbb{E}\left\|w_m^s-w_*\right\|^2+\frac{2m\hat{\alpha} \gamma_2L+2m(1-\tilde{\alpha})\gamma_2\mu}{\mu L\overline{b}}\left(1-\frac{4 L(\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu)}{b \overline{b}\mu L}\right)\mathbb{E}\left[P\left(\widetilde{w}_s\right)-P\left(w_*\right)\right] \\ 
  	& \leq\mathbb{E}\left\|w_0^s-w_*\right\|^2+\frac{4m L}{b \overline{b}^2}\left(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L}\right)^2 \mathbb{E}\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right]\\
  	& =\mathbb{E}\left\|\widetilde{w}_{s-1}-w_*\right\|_2^2+\frac{4m L}{b \overline{b}^2}(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L})^2 \mathbb{E}\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right].\end{aligned}$$
  	
  	
  	Employing the strong convexity (\ref{convex1}), we further attain
  	$$ \begin{aligned} &\mathbb{E}\left\|\widetilde{w}_s-w_*\right\|_2^2+\frac{4m L}{b \overline{b}^2}\left(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L}\right)^2 \mathbb{E}\left[P(\widetilde{w}_{s})-P\left(w_*\right)\right] \\ & \leq \frac{2}{\mu} \mathbb{E}\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right]+\frac{4m L}{b \overline{b}^2}\left(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L}\right)^2 \mathbb{E}\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right] \\ & =
  	\left(\frac{2}{\mu}+\frac{4m L}{b \overline{b}^2}(\frac{\hat{\alpha}\gamma_2 L+(1-\tilde{\alpha})\gamma_2\mu}{\mu L})^2\right)\mathbb{E}\left[P(\widetilde{w}_{s-1})-P\left(w_*\right)\right].
  	\end{aligned}$$
  	By the definition of $\kappa_r$, we at last simplify it into
  	$$\mathbb{E}\left[P\left(\widetilde{w}_s\right)-P\left(w_*\right)\right] \leq \left(\frac{\kappa}{\gamma_2\kappa_r}\cdot\frac{b \overline{b}^2}{m(b \overline{b}-4\gamma_2\kappa_r)}+\frac{2\gamma_2 \kappa_r}{b \overline{b}-4\gamma_2\kappa_r}\right)\mathbb{E}\left[P\left(\widetilde{w}_{s-1}\right)-P\left(w_*\right)\right].$$
  	
  	
  	By recursively applying the previous procedures, we derive
  	$$\mathbb{E}\left[P\left(\widetilde{w}_s\right)-P\left(w_*\right)\right] \leq \left(\frac{\kappa}{\gamma_2\kappa_r}\cdot\frac{b \overline{b}^2}{m(b \overline{b}-4\gamma_2\kappa_r)}+\frac{2\gamma_2 \kappa_r}{b \overline{b}-4\gamma_2\kappa_r}\right)^s\mathbb{E}\left[P\left(\widetilde{w}_{0}\right)-P\left(w_*\right)\right].$$
  	
  	\vspace{8pt}
  	By substituting RHBB+ step size and its corresponding boundary, the remaining parts in Theorem \ref{theorem3} can be proven similarly in parallel.
  	
  	We supply the relational boundary of RHBB+ in mS2GD as follows
  	$$\begin{aligned}
  	(\tilde{\eta}_k^s)^{\mathrm{RHBB+}}& = \frac{\gamma_2}{\max\{|S_1|, |S_2|\}}\cdot\left( \frac{\alpha^{h(\sigma_1s+\sigma_2k)}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_1}^+\left(w_k^s\right)-\nabla P_{S_1}^+\left(w_{k-1}^s\right)\right)\right)}\right.\\
  	&+ \left.\frac{\left(1-\alpha^{h(\sigma_1s+\sigma_2k)}\right)\cdot\left(\left(w_k^s-w_{k-1}^s\right)^T\left(\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right)\right)}{\left\|\nabla P_{S_2}^+\left(w_k^s\right)-\nabla P_{S_2}^+\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& \leq \frac{\gamma_2}{\overline{b}}\cdot\left(\frac{\hat{\alpha}\cdot\left\|w_k^s-w_{k-1}^s\right\|^2}{\frac{1}{|S_1|} \sum_{i \in S_1} \frac{\mu}{{n}q_i}}+\frac{\left(1-\tilde{\alpha}\right)\cdot\left\|\nabla P^+_{S_2}\left(w_k^s\right)-\nabla P^+_{S_2}\left(w_{k-1}^s\right)\right\|^2}{L_q\left\|\nabla P^+_{S_2}\left(w_k^s\right)-\nabla P^+_{S_2}\left(w_{k-1}^s\right)\right\|^2}\right)\\
  	& = \frac{\gamma_2}{\overline{b}}\cdot\frac{\hat{\alpha} L_q+(1-\tilde{\alpha})\mu_q}{\mu_q L_q}.\\
  	\end{aligned}$$
  	
  	\vspace{8pt}
  	\textbf{Further Argument:}
  	
  	\vspace{4pt}
  	We show how to configurate an $h(\cdot)$ to ensure improvement theoretically. For clarity, we use $\tilde{\rho}_{R}$, $\tilde{\rho}_{RH}$, $\tilde{\rho}_{RH+}$ to denote the convergence rates of mS2GD-RBB, mS2GD-RHBB, mS2GD-RHBB+ respectively. In mS2GD-RBB, the update frequency $m$ and the batch sizes $b, b_1$ are chosen (here, $\overline{b}=b_1$) so that
  	\begin{equation}
  	\label{RBB}
  	\tilde{\rho}_{R}=\frac{\mu b\overline{b}^2+2mL}{\mu b \overline{b} m-4mL}<1.
  	\end{equation}
  	Under the identical parameter set $\{m, b, \overline{b}\}$, mS2GD-RHBB owns $$\tilde{\rho}_{RH}=\frac{\left(\frac{\kappa}{\kappa_r\gamma_2}\right)^2\cdot\mu b\overline{b}^2+2mL}{\left(\frac{\kappa}{\kappa_r\gamma_2}\right)\cdot\mu b \overline{b} m-4mL}.$$
  	Assume $\tilde{\rho}_{R}=\frac{\mu b\overline{b}^2+2mL}{\mu b \overline{b} m-4mL}=c<1$, we thus obtain
  	\begin{equation}
  	\label{RHBB good}
  	mL=\frac{c}{2+4c}m\mu b\overline{b} -  \frac{1}{2+4c}\mu b \overline{b}^2.
  	\end{equation}
  	According to (\ref{RHBB good}), the rate of $\tilde{\rho}_{RH}$ can be re-expressed as
  	$$
  	\tilde{\rho}_{RH}=\frac{1}{2}\cdot\frac{\left((1+2c)(\frac{\kappa}{\kappa_r\gamma_2})^2-1-c\right)\mu b\overline{b}^2+c\mu b\overline{b}^2 + c\mu b\overline{b}m}{\left(\frac{\left(1+2c\right)\kappa}{2\kappa_r\gamma_2}-1-c\right)\mu b\overline{b}m+\mu b\overline{b}^2 + \mu b\overline{b}m}.
  	$$
  	Mark the $\left((1+2c)(\frac{\kappa}{\kappa_r\gamma_2})^2-1-c\right)$ by $A$ and the $\left(\frac{\left(1+2c\right)\kappa}{2\kappa_r\gamma_2}-1-c\right)$ by 
  	$B$. If condition $A\overline{b}<c Bm$ satisfied, it follows $\left(\tilde{\rho}_{RH}<\frac{1}{2}\tilde{\rho}_{R}=\frac{1}{2}c\right)$. We infer that $A<0$ if $\frac{\kappa_r}{\kappa}>\frac{1}{\gamma_2}\sqrt{\frac{1+2c}{c+1}}$, and that $B<0$ if $\frac{\kappa_r}{\kappa}>\frac{2c+1}{\gamma_2\left(2c+2\right)}$. The balance parameter, $\gamma_2$ ($\gamma_2\geq1$), has relaxed the essential boundaries of terms A and B. To meet the above condition, we may choose a $\gamma_2$ to make $A<0$, and ensure
  	$$\overline{b}>\frac{\frac{\left(1+2c\right)c\kappa}{2\kappa_r\gamma_2}-c-c^2}{\frac{1+2c}{2}\cdot(\frac{\sqrt{2}\kappa}{\kappa_r\gamma_2})^2-1-c}\cdot m.$$
  	It suggests configuring an $h(\cdot)$ with $\frac{\kappa_r}{\kappa}<\frac{2}{\gamma_2}$ and making $\overline{b}>m$ will realize a speed of $\tilde{\rho}_{RH}<\frac{1}{2}\tilde{\rho}_{R}$ in theory. Our update allows to flexibly trade-off between $h(\cdot)$ and $\gamma_2$, but it's not necessary to strictly tune a rate constant that prompts more than twice improvement (shrinking to less than the half). In practice, we commonly set $\gamma_2$ and configure $\hat{\alpha}$, $\tilde{\alpha}$ slightly larger than $1$ for the robustness.
  	
  	
  	For a comprehensive analysis, we explicate it in terms of the effective range. According to (\ref{RBB}), we derive the effective range of $\tilde{\rho}_{RH}$ as follows
  	\begin{equation}
  	\label{RHBB good2}
  	\tilde{\rho}_{RH} < \frac{\left(\left(\frac{\kappa}{\gamma_2\kappa_r}\right)^2-\frac{1}{3}\right)\mu b \overline{b}^2+\frac{1}{3}m\mu b\overline{b}} {\frac{2}{3}\mu b\overline{b}^2+\left(\frac{\kappa}{\gamma_2\kappa_r}-\frac{2}{3}\right)m\mu b\overline{b}}< 1+ \frac{\left(\left(\frac{\kappa}{\gamma_2\kappa_r}\right)^2-1\right)\overline{b}-\left(\frac{\kappa}{\gamma_2\kappa_r}-1\right)m}{\frac{2}{3}\left(\overline{b}-m\right)+\frac{\kappa}{\gamma_2\kappa_r}m}.
  	\end{equation}
  	Therefore, restricting the batch correction, $\overline{b}$, to a broad interval of
  	$$
  	\left(-\infty, \left(1-\frac{3\kappa}{2\gamma_2\kappa_r}\right)m\right) \cup \left(\left(1-\frac{\kappa}{\kappa+\gamma_2\kappa_r}\right)m, +\infty\right)
  	$$
  	will make the second term in (\ref{RHBB good2}) negative, which urges $\tilde{\rho}_{RH}$ to fall into a narrower interval, compared to original $\tilde{\rho}_{R}$ in (\ref{RBB}). By selecting set $\{m, b, b_1\}$ identical to mS2GD-RBB, tuning $b_2$ can affect the slowest magnitude of convergence speed, guaranteeing from theory that the worst convergence result is as well a fast one. A straightforward trade-off of $\overline{b}>m$ will strongly enforce the second term in (\ref{RHBB good2}) negative, habitually applied in practical. In fact, numerous experiments show no strong or intense limits among the related parameters, which supplies a wide range of selections.
  	
  	
  	Furthermore, let's minimize the `ineffective' range
  	$$
  	\min _{h} \quad \left|\frac{3\kappa}{2\gamma_2\kappa_r}-\frac{\kappa}{\kappa+\gamma_2\kappa_r}\right|m.
  	$$
  	By virtue of the monotonicity, the `ineffective' interval shortens as $\hat{\alpha}$ increases or $\tilde{\alpha}$ decreases. We can enlarge the span of the adaptor $h(\cdot)$ or increase the decay rate, while satisfying $b\overline{b}>4\kappa_r\gamma_2$, to obtain sufficient speedup in convergence.
  	
  	
  	Analysis follows a similar line of reasoning in mS2GD-RHBB+, the rate $\tilde{\rho}_{RH+}$ can be built smaller even than the $\tilde{\rho}_{RH}$, due to the facts $\mu_r\leq1$ and $\kappa^+\geq\kappa$. To obtain $\left(\tilde{\rho}_{RH+}<\frac{1}{2}\tilde{\rho}_{R}=\frac{1}{2}c\right)$, we should make sure $\frac{\kappa_r^+}{\kappa^+}>\frac{\mu_{r}}{\gamma_2}\sqrt{\frac{1+2c}{c+1}}$ $(\mu_{r}\leq1)$, hence, it allows a freer selection of the exponential adaptor $h(\cdot)$.
  	
  	
  	For the class of ill-conditioned functions $P(\cdot)$ with $L \gg \mu$, we have $\frac{\kappa}{\kappa_r} \approx \frac{\kappa^+}{\kappa_r^+} \approx \frac{1}{\hat{\alpha}}$. Related limitations become dependent barely on the upper bound $\hat{\alpha}$ of $h(\cdot)$, which is almost equivalent to the original $\hat{\alpha}>1$. This can be solved with ease in the initial inputs, saving plenty of tuning effort.
  	
  	
  	\vspace{8pt}
  	Note from Theorem 3, it's feasible to forsake the outer epoch and set up the mS2GD-IN-RHBB/RHBB+ algorithms (similar to \cite{nguyen2}). Theorem 3 suggests whenever the set $\{m, b_1, b_2\}$ chosen, the second terms of (\ref{rate3}) (\ref{rate4}) can be regulated sufficiently small by adjusting the adaptor $h(\cdot)$ appropriately.
  	
  	
  	When setting $\frac{\hat{\alpha}}{\tilde{\alpha}}L+\frac{1-\tilde{\alpha}}{\tilde{\alpha}}\mu=\mathcal{O}\left(\mu\right)$ and $\frac{\hat{\alpha}}{\tilde{\alpha}}L_q+\frac{1-\tilde{\alpha}}{\tilde{\alpha}}\mu_q=\mathcal{O}\left(\mu_q\right)$, from (\ref{rate3}) (\ref{rate4}) we obtain $m=\mathcal{O}\left(\frac{\overline{b}\kappa}{\gamma_2\tilde{\alpha}}\right)$ and $m=\mathcal{O}\left(\frac{\mu_{r}\overline{b}\kappa^+}{\gamma_2\tilde{\alpha}}\right)=\mathcal{O}\left(\frac{\overline{b}\kappa}{L_{r}\gamma_2\tilde{\alpha}}\right)$, correspondingly. 
  	
  	
  	To satisfy $\mathbb{E}\left[P\left(\widetilde{w}_s\right)\right]-P\left(w_*\right) \leq \left(\tilde{\rho}_{RH}\right)^s\cdot\left[P\left(\widetilde{w}_0\right)-P\left(w_*\right)\right] \leq \varepsilon$ in mS2GD-RHBB, the number of outer epoch, $s$, must satisfy
  	$$s\geq \frac{log\left(P(\widetilde{w}_0)-P(w_*)\right)-log\left(\varepsilon\right)}{-log\left(\tilde{\rho}_{RH}\right)}.$$
  	By the same token, we demand the $s$ in MB-SARAH-RHBB+ such that
  	$$s\geq \frac{log\left(P(\widetilde{w}_0)-P(w_*)\right)-log\left(\varepsilon\right)}{-log\left(\tilde{\rho}_{RH+}\right)}.$$
  	Therefore, to bound the number of oracles of IFO model, the following result for the total complexity is obtained.
  	\begin{corollary}
  		\label{corollary3}
  		Suppose Assumption 1 and 2a hold. The complexity of mS2GD-RHBB to achieve an $\varepsilon$-accurate solution is $\mathcal{O}\left(\left(n+\frac{b\overline{b}}{\gamma_2\tilde{\alpha}}\kappa\right)log\left(\frac{1}{\varepsilon}\right)\right)$. Suppose Assumption 2b holds further. To obtain an $\varepsilon$-accurate solution, the overall complexity of mS2GD-RHBB+ is of order
  		$\mathcal{O}\left(\left(n+\frac{b\overline{b}}{L_{r}\gamma_2\tilde{\alpha}}\kappa\right)log\left(\frac{1}{\varepsilon}\right)\right)$.
  	\end{corollary}
  	
  	
  	Compared with mS2GD \cite{konevcny}, mS2GD-BB \cite{yang3}, mS2GD-RBB \cite{yang2}, Corollary \ref{corollary3} indicates that, to achieve an $\varepsilon$-accurate solution, mS2GD-RHBB/RHBB+ have lower total complexity when choosing $h(\cdot)$ and $\overline{b}$ properly.
  	
  	
  	\section{Experiments}
  	\subsection{Experiment Settings}
  	To be specific, experiments are performed on the well-worn problems of training ridge regression, i.e.,
  	$$
  	\min _{w \in \mathbb{R}^d} P(w) = \frac{1}{n} \sum_{i=1}^n \log \left(1+\exp \left(-z_i x_i^T w\right)\right) +\frac{\lambda}{2}\|w\|^2.
  	$$
  	
  	
  	For our RHBB+ rule, we specify the probability distribution $Q$ with two options. Since $f_i(w)=\log \left(1+\exp \left(-z_i x_i^T w\right)\right)$ with $z_i\in\{-1, 1\}$, then $\|\nabla f_i(w)\|\leq\|x_i\|\leq\sqrt{d}\|x_i\|_{\infty}$, for option I we set: $q_i=\frac{\left\|x_i\right\|_{\infty}^{\tau}}{\sum_{j=1}^{n}\left\|x_j\right\|_{\infty}^{\tau}}$. For option II, we consider sparsity and set: $q_i=\frac{\left\|x_i\right\|_{0}^{\tau}}{\sum_{j=1}^{n}\left\|x_j\right\|_{0}^{\tau}}$. Here, $\tau$ is equipped to mitigate the batch influence on importance sampling and improve filtering effectiveness. 
  	
  	We verify MB-SARAH-RHBB and mS2GD-RHBB on data sets $a8a$, $w8a$, $ijcnn1$, $covtype$, $phishing$ and $mushrooms$. Due to the statistical characteristics of distribution $Q$, we explore MB-SARAH-RHBB+ and mS2GD-RHBB+ on another three $australian$, $madelon$ and $german.numer$. All data sets are publicly available in LIBSVM \footnote{ \url{https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/}.}. More details are referred to \textbf{Table \ref{table1}}.
  	
  	\begin{table}
  		\setlength{\abovecaptionskip}{0pt}
  		\setlength{\belowcaptionskip}{1pt}
  		\caption{DATA INFORMATION OF EXPERIMENTS}
  		\centering
  		\vspace{4pt}
  		\begin{tabular}{lccc}
  			\hline
  			\multicolumn{1}{l}{Datasets}&\multicolumn{1}{l}{\qquad Instances ($n$)} &\multicolumn{1}{l}{\qquad Features ($d$)} &\multicolumn{1}{c}{\qquad $\lambda$}
  			\\ \hline\noalign{\smallskip}
  			a8a & \qquad 22,696 & \qquad 123 & \qquad$10^{-2}$\\
  			w8a & \qquad 49,749 & \qquad 300 & \qquad$10^{-2}$ \\
  			ijcnn1 & \qquad 49,990 & \qquad 22 & \qquad$10^{-2}$ \\
  			covtype & \qquad 581,012 & \qquad 54 & \qquad$10^{-2}$ \\
  			phishing & \qquad 11,055 & \qquad 68 & \qquad$10^{-2}$ \\
  			mushrooms & \qquad 8,124 & \qquad 112 & \qquad$10^{-2}$ \\
  			australian & \qquad 690 & \qquad 14 & \qquad$10^{-2}$\\
  			madelon & \qquad 2,000 & \qquad 600 & \qquad$10^{-2}$\\
  			german.numer & \qquad 1,000 & \qquad 24 & \qquad$10^{-2}$\\
  			\hline
  		\end{tabular}
  		\label{table1}
  	\end{table}
  	
  	\subsection{Experiments investigating for Unvaried Hedge Effect}      
  	Our first aim is to investigate whether the hedge ideology is helpful to improve numerical efficiency. By fixing $h(\sigma_1s+\sigma_2k)=1$, we separate adaptive technique from the hedge operation and verify the unvaried hedge effect. 
  	
  	We conduct experiments with $b_1=b_2=b_H$ first, where $b_H$ conveys the unified batch size and is used in the legends. For clarity, notations of this subsection are summarized in \textbf{Table 2}.
  	\begin{table}
  		
  		\setlength{\abovecaptionskip}{0pt}
  		\setlength{\belowcaptionskip}{1pt}
  		\caption{NOTATIONS DESCRIPTIONS}
  		\centering
  		\vspace{4pt}
  		\begin{tabular}{lclc}
  			\hline
  			\multicolumn{1}{l}{Notations}&\multicolumn{1}{l}{\qquad Hedge Bases} &\multicolumn{1}{l}{\qquad Step Sizes} &\multicolumn{1}{c}{\qquad Adaptivity}
  			\\ \hline\noalign{\smallskip}
  			MB-SARAH-RBB  & \qquad \ding{56} & \qquad RBB & \qquad \ding{56} \\
  			MB-SARAH-RBB+ & \qquad \ding{56} & \qquad RBB+ & \qquad \ding{56} \\
  			MB-SARAH-RHBB($\alpha$) & \qquad $\alpha$ & \qquad RHBB & \qquad \ding{56} \\
  			MB-SARAH-RHBB($\alpha$)+ & \qquad $\alpha$ & \qquad RHBB+ & \qquad \ding{56} \\
  			mS2GD-RBB  & \qquad \ding{56} & \qquad RBB & \qquad \ding{56} \\
  			mS2GD-RBB+ & \qquad \ding{56} & \qquad RBB+ & \qquad \ding{56} \\
  			mS2GD-RHBB($\alpha$) & \qquad $\alpha$ & \qquad RHBB & \qquad \ding{56} \\
  			mS2GD-RHBB($\alpha$)+ & \qquad $\alpha$ & \qquad RHBB+ & \qquad \ding{56} \\
  			\hline
  		\end{tabular}
  		\label{table2}
  	\end{table}
  	
  	
  	\subsubsection{MB-SARAH-RHBB and mS2GD-RHBB}
  	
  	\ 
  	
  	\vskip 10pt
  	
  	\noindent\textbf{Parameter Settings: } We set $b=4$ and sample subsets $S$, $S_1$, $S_2$ according to uniform distribution. In addition, under $b=4$, we follow guidelines in \cite{yang} and set $\gamma=1$ in MB-SARAH-RHBB. For mS2GD-RHBB, we conduct a conservative trade-off with a moderate $\gamma_2=1$. We set the unified $b_H=40$ in general experiments, while in the last experiment, we varied $b_H=20, 30, 40, 50, 60$. Eventually, we sequentially opt $\alpha$ from set $\{2, 3, 4, 5\}$ and set $\{10, 11, 12, 13\}$. 
  	
  	
  	Figs. \ref{fig2} - \ref{fig9} show the numerical results for the properties of MB-SARAH-RHBB and mS2GD-RHBB. In all sub-figures, the horizontal axis denotes the number of effective passes, and the vertical axis stands for the Euclidean norm of $\nabla P(\cdot)$.
  	
  	
  	In Figs. \ref{fig2} - \ref{fig5}, we analyze the unvaried hedge effect by increasing the value of $\alpha$ gradually or drastically. From Figs. \ref{fig2}, \ref{fig3}, we observe that, the practical speeds of MB-SARAH-RHBB and mS2GD-RHBB are continuously improving with increasing $\alpha$ from $\{2, 3, 4, 5\}$. The following Figs. \ref{fig4}, \ref{fig5} indicate that, the performance of algorithms reaches a plateau as $\alpha$ becomes more aggressive from $\{10, 11, 12, 13\}$. MB-SARAH-RHBB and mS2GD-RHBB outperform the original RBB-type algorithms consistently on all data sets.
  	
  	
  	In Fig. \ref{fig6}, we analyze the constant step size sequences $\{\eta_0^s\}$ $\{\tilde{\eta}_0^s\}$ ($s\geq1$) that are applied in deterministic steps. For reliability, we randomly tossed a value of $\alpha=3$ to run the algorithms. We pick four unvaried sequences $\{0.05\}$, $\{0.1\}$, $\{0.5\}$, $\{1\}$ and four mingle sequences with ascending $mix1 = \{0.05, 0.1, 0.5, 1, ...\}$, descending $mix2 = \{1, 0.5, 0.1, 0.05, ...\}$, disordered $mix3 = \{0.5, 1, 0.05, 0.1, ...\}$ and disordered $mix4 = \{1, 0.05, 0.1, 0.5, ...\}$, as participants. The performance of MB-SARAH-RHBB and mS2GD-RHBB is not influenced, implying $\{\eta_0^s\}$ $\{\tilde{\eta}_0^s\}$ ($s\geq1$) are immaterial but provide sufficient curvature for following RHBB/RHBB+ calculations.
  	
  	
  	Fig. \ref{fig7} exhibits comparisons between MB-SARAH-RHBB and mS2GD-RHBB, where we have set multiple comparison levels at $\alpha=2, 3, 4, 5$. MB-SARAH-RHBB outperforms mS2GD-RHBB on $ijcnn1$, mS2GD-RHBB performs better on $covtype$, $phishing$, $mushrooms$, $w8a$, and they performed equally well on $a8a$. In most cases, mS2GD-RHBB delivers stronger performance than MB-SARAH-RHBB.
  	
  	To analyze the properties in inner stages, we discard the outer epoch and explore the performance of MB-SARAH-IN-RHBB and mS2GD-IN-RHBB in Figs. \ref{fig8}, \ref{fig9}. We notice mS2GD is more sensitive to RHBB step sizes than MB-SARAH in inner stochastic stages.
  	
  	In Figs. \ref{fig8} - \ref{fig11}, we evaluate different unified batch sizes (namely, the batch correction) of $b_H=20, 30, 40, 50, 60$ on data set $a8a$. We note that MB-SARAH-RHBB and mS2GD-RHBB are both sensitive to the selection of $b_H$ (or the the batch correction). Overall, the algorithms possess resilient performance when the unified batch size $b_H$ (or the the batch correction) tends to be around $40$. 
  	
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed	
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	
  	\subsubsection{MB-SARAH-RHBB+ and mS2GD-RHBB+}
  	\ 
  	\vskip10pt
  	
  	\noindent\textbf{Parameter Settings:} We set $b=4$, the unified $b_H=40$ and sample subsets $S_1$ and $S_2$ according to distributions $Q$, where $Q$ are configured by option I and option II. To mitigate the impact of $b_H=40$, we set $\tau=2$ in both option I and option II. To avoid potential over-utility, we conduct $\gamma=0.8$, $\gamma_2=0.8$. At last, we opt $\alpha=3, 6, 8$ to represent various hedge scenarios.
  	
  	
  	Figs. \ref{fig12} - \ref{fig15} display numerical results of MB-SARAH-RHBB+ and mS2GD-RHBB+. We actively select $\alpha=3, 6, 8$ to provide various hedge scenarios (slight, moderate, intense), under which we analyze the effect of importance sampling.
  	
  	
  	In Figs. \ref{fig12}, \ref{fig13} of MB-SARAH-RHBB+, when a conservative value of $\alpha=3$ is adopted (under slight hedge scenario), both option I and option II seem to make limited improvement; however, with aggressive choices of $\alpha=6, 8$ (under moderate and intense hedge scenarios), importance sampling accelerates the convergence significantly.
  	
  	
  	Figs. \ref{fig14}, \ref{fig15} show that, the performance of mS2GD-RHBB+ has notable refinement when applying large $\alpha=8$ (under intense hedge scenario). Whereas, its numerical results on data set $madelon$ are not stable. Thereby, the performance of MB-SARAH-RHBB+ is more robust than mS2GD-RHBB+.
  	
  	
  	To supply comprehensive illustrations, we have further integrated importance sampling into the original RBB rule and obtained the corresponding by-products of MB-SARAH-RBB+ and mS2GD-RBB+ algorithms. In Figs. \ref{fig12} - \ref{fig15}, we have as well included comparisons for RBB and RBB+, showing importance sampling can not yield improvement in the original RBB rule. It can be stated that, the importance sampling in RHBB+ is more attuned to large values of $\alpha$ (under moderate and intense hedge scenarios). 
  	
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	\subsubsection{Comparison with other state-of-art methods}
  	\ 
  	\vskip10pt
  	\noindent  \textbf{Parameter Settings:} In MB-SARAH-RHBB and mS2GD-RHBB, we set $b=4$, the unified $b_H=40$ and sample subsets $S$, $S_1$, $S_2$ according to uniform distribution. We employ the same $\gamma$ and $\gamma_2$ as in previous experiments to provide consistent illustartions, which are $\gamma=1$, $\gamma_2=1$. In addition, we fine-tune each of the other algorithms as follow: \\
  	(1) \textbf{SVRG:} Accelerating stochastic gradient descent using predictive variance reduction \cite{johnson}. We employ best-tuned constant step size in SVRG setting.\\
  	(2) \textbf{SVRG-BB:} Stochastic variance reduced algorithm (SVRG) with Barzilai and Borwein step size \cite{tan}.\\
  	(3) \textbf{mS2GD-BB:} Semi-stochastic algorithm (mS2GD) with Barzilai and Borwein step size \cite{yang3}, a batch version of SVRG-BB.\\
  	(4) \textbf{Acc-Prox-SVRG:} Accelerating variance reduced algorithm with momentum Nesterov's structure \cite{nitanda}. We set $\eta=1$, $\delta = 1$, $b=100$, $m=\delta b$ and $\beta_k=\frac{b-2}{b+2}$ as suggested in \cite{nitanda}.\\
  	(5) \textbf{Acc-Prox-SVRG-BB:} Acc-Prox-SVRG stochastic algorithm with Barzilai and Borwein step size \cite{yang4}. We set the related parameters according to \cite{yang4}.\\
  	(6) \textbf{SARAH+:} An implementation version of SARAH \cite{nguyen}. Best hand-tuned constant step size was employed in the optimization process. \\
  	(7) \textbf{SVRG-ABB:} SVRG stochastic algorithm with adaptive Barzilai and Borwein step size. Adaptive parameter $k=0.5$ is set for robustness.
  	
  	
  	It's shown from Fig. \ref{fig16} that, MB-SARAH-RHBB and mS2GD-RHBB, with unvaried $\alpha=3$, outperform other state-of-art methods consistently on all the six data sets. A few algorithms may be competitive on $ijcnn1$, but soon expose their powerlessness on others. Therefore, MB-SARAH-RHBB and mS2GD-RHBB are both robust in performance and implementations.
  	
  	
  	Referring back to Figs. \ref{fig2} - \ref{fig5}, we observe that, there exists an optimal bound on the hedge magnitude. Therefore, we argue that, further improvement can be achieved by controlling the hedge magnitude dynamically, rather fixing $h(\cdot)$ as a constant.
  	
  	% Figure environment removed	
  	
  	\subsection{Experiment investigating for Adaptive Hedge Effect}
  	Now, we reveal the numerical efficiency of our iterative adaptor $h(\cdot)$.
  	
  	
  	The previous Figs. \ref{fig2}, \ref{fig3} display that a properly-tuned $\alpha$ greatly accelerates the convergence, but the subsequent Figs. \ref{fig4}, \ref{fig5} show that the effective magnitude can be corrupted due to over-hedging or excessive enlargement. In view of it, we propose decreasing the value of $h(\cdot)$ along the updates, on one side to positively accelerate the convergence in early periods, on the other for avoiding over-aggressive steps around the global optimum. For conciseness, we decide the increment to be inversely proportional to iterative indicators $\left(\sigma_1s+\sigma_2k\right)$, namely we set  $h(\sigma_1s+\sigma_2k)=\frac{1+\sigma_1s+\sigma_2k}{\sigma_1s+\sigma_2k}$. Equally, we conduct experiments with $b_1=b_2=b_H$ first.
  	
  	
  	In this subsection, we use notations with suffix `pure' to represent algorithms with $h(\sigma_1s+\sigma_2k)=1$. Notations in this subsection are summarized in \textbf{Table \ref{table3}}.
  	
  	\begin{table}
  		\setlength{\abovecaptionskip}{0pt}
  		\setlength{\belowcaptionskip}{1pt}
  		\caption{NOTATIONS DESCRIPTIONS}
  		\centering
  		\vspace{4pt}
  		\begin{tabular}{lclc}
  			\hline
  			\multicolumn{1}{l}{Notations}&\multicolumn{1}{l}{\qquad Hedge Bases} &\multicolumn{1}{l}{\qquad Step Sizes} &\multicolumn{1}{c}{\qquad Adaptivity}
  			\\ \hline\noalign{\smallskip}
  			MB-SARAH-RBB  & \qquad \ding{56} & \qquad RBB & \ding{52}\\
  			MB-SARAH-RBB+ & \qquad \ding{56} & \qquad RBB+ & \ding{52}\\
  			MB-SARAH-RHBB($\alpha$) & \qquad $\alpha$ & \qquad RHBB & \ding{52}\\
  			MB-SARAH-RHBB($\alpha$)+ & \qquad $\alpha$ & \qquad RHBB+ & \ding{52}\\
  			MB-SARAH-RHBB($\alpha$) - pure & \qquad $\alpha$ & \qquad RHBB & \ding{56}\\
  			MB-SARAH-RHBB($\alpha$)+ - pure & \qquad $\alpha$ & \qquad RHBB+ & \ding{56}\\
  			mS2GD-RBB  & \qquad \ding{56} & \qquad RBB & \ding{52}\\
  			mS2GD-RBB+ & \qquad \ding{56} & \qquad RBB+ & \ding{52}\\
  			mS2GD-RHBB($\alpha$) & \qquad $\alpha$ & \qquad RHBB & \ding{52}\\
  			mS2GD-RHBB($\alpha$)+ & \qquad $\alpha$ & \qquad RHBB+ & \ding{52}\\
  			mS2GD-RHBB($\alpha$) - pure & \qquad $\alpha$ & \qquad RHBB & \ding{56}\\
  			mS2GD-RHBB($\alpha$)+ - pure & \qquad $\alpha$ & \qquad RHBB+ & \ding{56}\\
  			\hline
  		\end{tabular}
  		\label{table3}
  	\end{table}
  	
  	
  	\subsubsection{MB-SARAH-RHBB and mS2GD-RHBB}
  	\ 
  	\vskip10pt
  	\noindent  \textbf{Parameter Settings:} We set $b=4$, the unified $b_H=40$ and sample subsets $S$, $S_1$, $S_2$ according to uniform distribution. We perform an extensive search for the adaptive pair $(\sigma_1, \sigma_2)$ with three different settings: $(0.6, 0.2), (0.7, 0.1), (0.4, 0.4)$ (we ensure $\sigma_1+\sigma_2=0.8<1$ to allow several quadratic accelerations in the early iterations.). To ensure the comparability across aspects, the hedge base $\alpha$ is opted within $\{2, 3, 4, 5\}$. Following guidelines from \cite{yang}, we choose $\gamma=1$. By considering a moderate trade-off in mS2GD-RHBB, we implement $\gamma_2=1$.
  	
  	
  	In Figs. \ref{fig17} - \ref{fig22}, we compare the RBB rule, the `non-adaptive' RHBB rule and the RHBB rule in terms of the evolution of $\|\nabla P(\cdot)\|^2$. The adaptive pair $(\sigma_1$, $\sigma_2)$ in iterative adaptor $h(\cdot)$ is explored triply, with $(0.6, 0.2)$ in Fig. \ref{fig17}, \ref{fig18}, $(0.7, 0.1)$ in Fig. \ref{fig19}, \ref{fig20} and $(0.4, 0.4)$ in Fig. \ref{fig21}, \ref{fig22}. 
  	
  	
  	From Figs. \ref{fig17}, \ref{fig19}, \ref{fig21}, we can observe that, the adaptive MB-SARAH-RHBB consistently outperforms the `non-adaptive' MB-SARAH-RHBB and surpass the original MB-SARAH-RBB by a large margin. Similar and consistent results can be seen in the adaptive mS2GD-RHBB as in Figs. \ref{fig18}, \ref{fig20}, \ref{fig22}.
  	
  	
  	Clearly, in most cases, the iterator adaptor, $h(\cdot)$, provides significant refinements for algorithms with the hedge base $\alpha=4, 5$. However, under $\alpha=4, 5$, associated algorithms perform on par with each other on $w8a$. Combining previous results from Figs \ref{fig4},  \ref{fig5}, it is evident that, the optimal hedge magnitude of RHBB is slightly lower on $w8a$ than on other data sets. 
  	
 
   The performance of adaptive algorithms varies significantly depending on related hyper-parameters, nevertheless, we do not require strict guidelines for parameter selection, our three casual and moderate choices have proven to be sufficiently effective.
  	
  	
  	We fix an exponential adaptor at the beginning for the convenience in paper. A simple incremental function has already led to noticeable improvements. We believe that, if more research is conducted (e.g. using sigmoid increments of $h(\cdot)$ or choosing non-exponential adaptors), our algorithms can be accelerated further.
  	
  	% Figure environment removed	
  	
  	% Figure environment removed		
  	
  % Figure environment removed	
  
  % Figure environment removed		
  	
  	% Figure environment removed	
  	
  	% Figure environment removed		
  	
  	
  	\subsubsection{MB-SARAH-RHBB+ and mS2GD-RHBB+}
  	\ 
  	\vskip10pt
  	\noindent  \textbf{Parameter Settings:} We set $b=4$, the unified $b_H=40$ and sample subsets $S_1$ and $S_2$ according to distributions $Q$, where $Q$ are configured by option I and option II. Analogously, we set $\tau=2$ in both option I and option II. To avoid possible over-utility, we implement $\gamma=0.8$ and $\gamma_2=0.8$. We employ the first values in the adaptive pair as $(\sigma_1, \sigma_2)=(0.6, 0.2)$. Eventually, the hedge base $\alpha$ is selected within the parallel set $\{2, 3, 4, 5\}$.
  	
  	
  	Fig. \ref{fig23}, \ref{fig24} exhibit that the practical speeds of the adaptive MB-SARAH-RHBB+ are faster than the `non-adaptive' MB-SARAH-RHBB+, under different hedge bases and different distribution options. Especially, when $\alpha=5$, we notice that the total improvements are particularly remarkable which begins from the very early stages of iterations. We have corresponding results for the adaptive mS2GD-RHBB+ in Fig. \ref{fig25}, \ref{fig26}, which show the adaptive mS2GD-RHBB+ outperforms the `non-adaptive' mS2GD-RHBB+ uniformly. Therefore, it's reasonable also advisable to equip an iterative adaptor to achieve additional acceleration in the early periods. 
  	
  	
    Fig. \ref{fig23} - \ref{fig26} as well corroborate our previous conclusions that importance sampling is more sensitive in intense hedge scenarios (with relatively large $\alpha$).
    
  	
  	Massive results in this subsetion have suggested that, the iterative adaptor $h(\cdot)$ is instrumental in completing an efficient step size rule, as well addresses the defect of inflexibility in stochastic algorithms. The consistent performance implies the accordance between importance sampling and iterative scaling, dispelling concerns about potential discrepancies along the iterations.
  	
  	
  	Moreover, the current alternatives (option I and option II) are especially productive on $phishing$, $mushrooms$ and $german.numer$. Still, practitioners can configure particular distributions to match targeted sets to their needs.
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	% Figure environment removed	
  	
  	
  	\subsubsection{Comparison with other state-of-art methods}
  	\ 
  	\vskip10pt
  	\noindent  \textbf{Parameter Settings:} In MB-SARAH-RHBB and mS2GD-RHBB, we set $b=4$, the unified $b_H=40$ and sample subsets $S$, $S_1$, $S_2$ according to uniform distribution. We set $\gamma=1$, $\gamma_2=1$ and decide the adaptive pair $(\sigma_1,\sigma_2)=(0.8, 0.8)$ for a fresh try. Eventually, we opt $\alpha=3$ as a gentle hedge base.  

  	
  	As can be seen from Fig \ref{fig27}, all of our adaptive and `non-adaptive' methods outperform various state-of-the-art algorithms.
  	
  
  	% Figure environment removed	
  	
  	
  	
  	\subsection{Investigation on Batch Sizes}
  	We technically supply this subsection to demonstrate that, the performance of our algorithms is not sensitive to the batch sizes. Here, we arrange $b_1=40$ and $b_2=20, 25, 30, 35, 40$ alternately for verification (all settings must ensure the batch correction $\overline{b}=\max\{b_1, b_2\}=40$ unvaried). We have displayed the results from Fig. \ref{fig28} to Fig. \ref{fig33}. Hence, in all the previous experiments, priorly setting the unified batch size of $b_1=b_2=b_H$ is a reasonable also economical choice.
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed
  	
  	% Figure environment removed 
  	
  	% Figure environment removed   
  	
  	% Figure environment removed
  	
  	
  	
  	\section{Conclusion}
  	In this paper, we propose two novel and efficient rules for stochastic optimization, which are motivated by the random Barzilai-Borwein method, the important sampling strategy and modern iterative adaptors. The idea of design is aggressive yet robust: by leveraging untapped curvature, we enlarge the random Barzilai-Borwein step sizes effectively, thereby accelerating stochastic algorithms with ease. The additional iterative adaptor enhances adaptivity at full scale.
  	
  	
  	We take two prevalent stochastic frameworks, MB-SARAH and mS2GD, to verify their numerical efficiency. For our algorithms MB-SARAH-RHBB/RHBB+ and mS2GD-RHBB/RHBB+, we rigorously analyze the acceleration mechanism and evaluate the corresponding complexity. Comprehensive tuning guidlines are later provided for reference in practical implementations. We prove that they're both effective in ill-conditioned scenarios. Due to the flexibility, we can obtain different performance by trading-off related parameters.
  	
  	
  	Numerical experiments have been conducted to present the properties of our four algorithms. Massive comparisons have been made in all-round aspects and demonstrate their superiority in modern stochastic optimization. Extensive explorations for the iterative adaptor show its promising scalability.  
  	
  	
  	\bibliographystyle{abbrv}
  	\bibliography{ref}
  \end{document}
   
  
 