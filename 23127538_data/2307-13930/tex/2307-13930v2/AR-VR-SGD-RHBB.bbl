\begin{thebibliography}{10}

\bibitem{agarwal}
A.~Agarwal and L.~Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock In {\em International conference on machine learning}, pages 78--86.
  PMLR, 2015.

\bibitem{byrd}
R.~H. Byrd, S.~L. Hansen, J.~Nocedal, and Y.~Singer.
\newblock A stochastic quasi-newton method for large-scale optimization.
\newblock {\em SIAM Journal on Optimization}, 26(2):1008--1031, 2016.

\bibitem{castera}
C.~Castera, J.~Bolte, C.~F{\'e}votte, and E.~Pauwels.
\newblock Second-order step-size tuning of sgd for non-convex optimization.
\newblock {\em Neural Processing Letters}, 54(3):1727--1752, 2022.

\bibitem{cotter}
A.~Cotter, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{csiba}
D.~Csiba and P.~Richt{\'a}rik.
\newblock Importance sampling for minibatches.
\newblock {\em The Journal of Machine Learning Research}, 19(1):962--982, 2018.

\bibitem{defazio}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{dekel}
O.~Dekel, R.~Gilad-Bachrach, O.~Shamir, and L.~Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock {\em Journal of Machine Learning Research}, 13(1), 2012.

\bibitem{eon}
L.~eon Bottou.
\newblock Online learning and stochastic approximations.
\newblock {\em On-linelearning in neural networks}, 17(9):142, 1998.

\bibitem{fang}
C.~Fang, C.~J. Li, Z.~Lin, and T.~Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{ghadimi}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{johnson}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock {\em Advances in neural information processing systems}, 26, 2013.

\bibitem{konevcny}
J.~Kone{\v{c}}n{\`y}, J.~Liu, P.~Richt{\'a}rik, and M.~Tak{\'a}{\v{c}}.
\newblock Mini-batch semi-stochastic gradient descent in the proximal setting.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing},
  10(2):242--255, 2015.

\bibitem{konevcny2}
J.~Kone{\v{c}}n{\`y} and P.~Richt{\'a}rik.
\newblock S2gd: Semi-stochastic gradient descent methods.
\newblock {\em arXiv preprint arXiv:1312.1666}, 2013.

\bibitem{li}
T.~Li and Z.~Wan.
\newblock New adaptive barzilai--borwein step size and its application in
  solving large-scale optimization problems.
\newblock {\em The ANZIAM Journal}, 61(1):76--98, 2019.

\bibitem{liu2}
J.~Liu and S.~J. Wright.
\newblock Asynchronous stochastic coordinate descent: Parallelism and
  convergence properties.
\newblock {\em SIAM Journal on Optimization}, 25(1):351--376, 2015.

\bibitem{liu3}
Y.~Liu, C.~Han, and T.~Guo.
\newblock A class of stochastic variance reduced methods with an adaptive
  stepsize.
\newblock {\em URL http://www. optimization-online. org/DB\_FILE/2019/04/7170.
  pdf}, 2019.

\bibitem{liu}
Y.~Liu, X.~Wang, and T.~Guo.
\newblock A linearly convergent stochastic recursive gradient method for convex
  optimization.
\newblock {\em Optimization Letters}, 14:2265--2283, 2020.

\bibitem{luo}
Z.-Q. Luo.
\newblock On the convergence of the lms algorithm with adaptive learning rate
  for linear feedforward networks.
\newblock {\em Neural Computation}, 3(2):226--245, 1991.

\bibitem{ma}
K.~Ma, J.~Zeng, J.~Xiong, Q.~Xu, X.~Cao, W.~Liu, and Y.~Yao.
\newblock Stochastic non-convex ordinal embedding with stabilized
  barzilai-borwein step size.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem{mairal}
J.~Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock {\em SIAM Journal on Optimization}, 25(2):829--855, 2015.

\bibitem{moulines}
E.~Moulines and F.~Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{needell}
D.~Needell, R.~Ward, and N.~Srebro.
\newblock Stochastic gradient descent, weighted sampling, and the randomized
  kaczmarz algorithm.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{nemirovski}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{nesterov2}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nguyen}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  2613--2621. PMLR, 2017.

\bibitem{nguyen2}
L.~M. Nguyen, J.~Liu, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock Stochastic recursive gradient algorithm for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1705.07261}, 2017.

\bibitem{nguyen3}
L.~M. Nguyen, K.~Scheinberg, and M.~Tak{\'a}{\v{c}}.
\newblock Inexact sarah algorithm for stochastic optimization.
\newblock {\em Optimization Methods and Software}, 36(1):237--258, 2021.

\bibitem{nitanda}
A.~Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock {\em Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem{polyak}
B.~T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  3(4):864--878, 1963.

\bibitem{reddi}
S.~J. Reddi, A.~Hefny, S.~Sra, B.~Poczos, and A.~Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages
  314--323. PMLR, 2016.

\bibitem{richtarik}
P.~Richt{\'a}rik and M.~Tak{\'a}{\v{c}}.
\newblock On optimal probabilities in stochastic coordinate descent methods.
\newblock {\em Optimization Letters}, 10:1233--1243, 2016.

\bibitem{Robbins}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem{Schmidt}
M.~Schmidt, N.~Le~Roux, and F.~Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock {\em Mathematical Programming}, 162:83--112, 2017.

\bibitem{shalev}
S.~Shalev-Shwartz, Y.~Singer, and N.~Srebro.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning}, pages 807--814, 2007.

\bibitem{shalev3}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock {\em Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem{shalev2}
S.~Shalev-Shwartz and T.~Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(1), 2013.

\bibitem{solodov}
M.~V. Solodov.
\newblock Incremental gradient algorithms with stepsizes bounded away from
  zero.
\newblock {\em Computational Optimization and Applications}, 11:23--35, 1998.

\bibitem{sopyla}
K.~Sopy{\l}a and P.~Drozda.
\newblock Stochastic gradient descent with barzilai--borwein update step for
  svm.
\newblock {\em Information Sciences}, 316:218--233, 2015.

\bibitem{takac}
M.~Tak{\'a}c, A.~Bijral, P.~Richt{\'a}rik, and N.~Srebro.
\newblock Mini-batch primal and dual methods for svms.
\newblock In {\em International Conference on Machine Learning}, pages
  1022--1030. PMLR, 2013.

\bibitem{tan}
C.~Tan, S.~Ma, Y.-H. Dai, and Y.~Qian.
\newblock Barzilai-borwein step size for stochastic gradient descent.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{xiao}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock {\em SIAM Journal on Optimization}, 24(4):2057--2075, 2014.

\bibitem{yangy}
Y.-m. Yang, F.-s. Wang, J.-x. Li, and Y.-y. Qin.
\newblock A new inexact stochastic recursive gradient descent algorithm with
  barzilai--borwein step size in machine learning.
\newblock {\em Nonlinear Dynamics}, 111(4):3575--3586, 2023.

\bibitem{yang5}
Z.~Yang, Z.~Chen, and C.~Wang.
\newblock An accelerated stochastic variance-reduced method for machine
  learning problems.
\newblock {\em Knowledge-Based Systems}, 198:105941, 2020.

\bibitem{yang}
Z.~Yang, Z.~Chen, and C.~Wang.
\newblock Accelerating mini-batch sarah by step size rules.
\newblock {\em Information Sciences}, 558:157--173, 2021.

\bibitem{yang3}
Z.~Yang, C.~Wang, Y.~Zang, and J.~Li.
\newblock Mini-batch algorithms with barzilai--borwein update step.
\newblock {\em Neurocomputing}, 314:177--185, 2018.

\bibitem{yang2}
Z.~Yang, C.~Wang, Z.~Zhang, and J.~Li.
\newblock Random barzilai--borwein step size for mini-batch algorithms.
\newblock {\em Engineering Applications of Artificial Intelligence},
  72:124--135, 2018.

\bibitem{yang4}
Z.~Yang, C.~Wang, Z.~Zhang, and J.~Li.
\newblock Accelerated stochastic gradient descent with step size selection
  rules.
\newblock {\em Signal Processing}, 159:171--186, 2019.

\bibitem{zhang}
Y.~Zhang and X.~Lin.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock In {\em International Conference on Machine Learning}, pages
  353--361. PMLR, 2015.

\bibitem{zhao}
P.~Zhao and T.~Zhang.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In {\em international conference on machine learning}, pages 1--9.
  PMLR, 2015.

\bibitem{zheng}
Q.~Zheng, P.~Richt{\'a}rik, and T.~Zhang.
\newblock Randomized dual coordinate ascent with arbitrary sampling, 2014.

\end{thebibliography}
