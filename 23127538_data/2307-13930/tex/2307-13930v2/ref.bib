@article{Robbins,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{shalev,
  title={Pegasos: Primal estimated sub-gradient solver for svm},
  author={Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={807--814},
  year={2007}
}

@article{Schmidt,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  pages={83--112},
  year={2017},
  publisher={Springer}
}



@article{defazio,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{johnson,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{konevcny,
  title={Mini-batch semi-stochastic gradient descent in the proximal setting},
  author={Kone{\v{c}}n{\`y}, Jakub and Liu, Jie and Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={10},
  number={2},
  pages={242--255},
  year={2015},
  publisher={IEEE}
}

@inproceedings{nguyen,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@article{nitanda,
  title={Stochastic proximal gradient descent with acceleration techniques},
  author={Nitanda, Atsushi},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}

@article{shang,
  title={Fast stochastic variance reduced gradient method with momentum acceleration for machine learning},
  author={Shang, Fanhua and Liu, Yuanyuan and Cheng, James and Zhuo, Jiacheng},
  journal={arXiv preprint arXiv:1703.07948},
  year={2017}
}

@article{pham,
  title={ProxSARAH: An efficient algorithmic framework for stochastic composite nonconvex optimization},
  author={Pham, Nhan H and Nguyen, Lam M and Phan, Dzung T and Tran-Dinh, Quoc},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={4455--4502},
  year={2020},
  publisher={JMLRORG}
}

@article{nguyen2,
  title={Stochastic recursive gradient algorithm for nonconvex optimization},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={arXiv preprint arXiv:1705.07261},
  year={2017}
}

@article{yang,
  title={Accelerating mini-batch sarah by step size rules},
  author={Yang, Zhuang and Chen, Zengping and Wang, Cheng},
  journal={Information Sciences},
  volume={558},
  pages={157--173},
  year={2021},
  publisher={Elsevier}
}

@article{yang2,
  title={Random Barzilai--Borwein step size for mini-batch algorithms},
  author={Yang, Zhuang and Wang, Cheng and Zhang, Zhemin and Li, Jonathan},
  journal={Engineering Applications of Artificial Intelligence},
  volume={72},
  pages={124--135},
  year={2018},
  publisher={Elsevier}
}

@article{barzilai,
  title={Two-point step size gradient methods},
  author={Barzilai, Jonathan and Borwein, Jonathan M},
  journal={IMA journal of numerical analysis},
  volume={8},
  number={1},
  pages={141--148},
  year={1988},
  publisher={Oxford University Press}
}

@article{zhou,
  title={Gradient methods with adaptive step-sizes},
  author={Zhou, Bin and Gao, Li and Dai, Yu-Hong},
  journal={Computational Optimization and Applications},
  volume={35},
  pages={69--86},
  year={2006},
  publisher={Springer}
}

@article{huang,
  title={A new nonmonotone spectral residual method for nonsmooth nonlinear equations},
  author={Huang, Shuai and Wan, Zhong},
  journal={Journal of Computational and Applied Mathematics},
  volume={313},
  pages={82--101},
  year={2017},
  publisher={Elsevier}
}

@article{biglari,
  title={New quasi-Newton methods via higher order tensor models},
  author={Biglari, Fahimeh and Hassan, Malik Abu and Leong, Wah June},
  journal={Journal of computational and applied mathematics},
  volume={235},
  number={8},
  pages={2412--2422},
  year={2011},
  publisher={Elsevier}
}

@book{nesterov,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{tan,
  title={Barzilai-borwein step size for stochastic gradient descent},
  author={Tan, Conghui and Ma, Shiqian and Dai, Yu-Hong and Qian, Yuqiu},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{yang3,
  title={Mini-batch algorithms with Barzilai--Borwein update step},
  author={Yang, Zhuang and Wang, Cheng and Zang, Yu and Li, Jonathan},
  journal={Neurocomputing},
  volume={314},
  pages={177--185},
  year={2018},
  publisher={Elsevier}
}

@article{yang4,
  title={Accelerated stochastic gradient descent with step size selection rules},
  author={Yang, Zhuang and Wang, Cheng and Zhang, Zhemin and Li, Jonathan},
  journal={Signal Processing},
  volume={159},
  pages={171--186},
  year={2019},
  publisher={Elsevier}
}

@article{xiao,
  title={A proximal stochastic gradient method with progressive variance reduction},
  author={Xiao, Lin and Zhang, Tong},
  journal={SIAM Journal on Optimization},
  volume={24},
  number={4},
  pages={2057--2075},
  year={2014},
  publisher={SIAM}
}

@inproceedings{zhao,
  title={Stochastic optimization with importance sampling for regularized loss minimization},
  author={Zhao, Peilin and Zhang, Tong},
  booktitle={international conference on machine learning},
  pages={1--9},
  year={2015},
  organization={PMLR}
}

@article{liu,
  title={A linearly convergent stochastic recursive gradient method for convex optimization},
  author={Liu, Yan and Wang, Xiao and Guo, Tiande},
  journal={Optimization Letters},
  volume={14},
  pages={2265--2283},
  year={2020},
  publisher={Springer}
}

@article{li,
  title={New adaptive barzilai--borwein step size and its application in solving large-scale optimization problems},
  author={Li, Ting and Wan, Zhong},
  journal={The ANZIAM Journal},
  volume={61},
  number={1},
  pages={76--98},
  year={2019},
  publisher={Cambridge University Press}
}


@article{shalev2,
  title={Stochastic dual coordinate ascent methods for regularized loss minimization.},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={14},
  number={1},
  year={2013}
}


@article{mairal,
  title={Incremental majorization-minimization optimization with application to large-scale machine learning},
  author={Mairal, Julien},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={2},
  pages={829--855},
  year={2015},
  publisher={SIAM}
}


@inproceedings{ma,
  title={Stochastic non-convex ordinal embedding with stabilized barzilai-borwein step size},
  author={Ma, Ke and Zeng, Jinshan and Xiong, Jiechao and Xu, Qianqian and Cao, Xiaochun and Liu, Wei and Yao, Yuan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}


@article{yangy,
  title={A new inexact stochastic recursive gradient descent algorithm with Barzilai--Borwein step size in machine learning},
  author={Yang, Yi-ming and Wang, Fu-sheng and Li, Jin-xiang and Qin, Yuan-yuan},
  journal={Nonlinear Dynamics},
  volume={111},
  number={4},
  pages={3575--3586},
  year={2023},
  publisher={Springer}
}


@article{fang,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{konevcny2,
  title={S2GD: Semi-stochastic gradient descent methods},
  author={Kone{\v{c}}n{\`y}, J and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1312.1666},
  year={2013}
}

@article{nesterov2,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}

@inproceedings{takac,
  title={Mini-batch primal and dual methods for SVMs},
  author={Tak{\'a}c, Martin and Bijral, Avleen and Richt{\'a}rik, Peter and Srebro, Nati},
  booktitle={International Conference on Machine Learning},
  pages={1022--1030},
  year={2013},
  organization={PMLR}
}

@article{shalev3,
  title={Accelerated mini-batch stochastic dual coordinate ascent},
  author={Shalev-Shwartz, Shai and Zhang, Tong},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@misc{zheng,
  title={Randomized dual coordinate ascent with arbitrary sampling},
  author={Zheng, Q and Richt{\'a}rik, P and Zhang, T},
  journal={arXiv preprint arXiv:1411.5873},
  year={2014},
  publisher={e-print}
}

@article{nguyen3,
  title={Inexact SARAH algorithm for stochastic optimization},
  author={Nguyen, Lam M and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Methods and Software},
  volume={36},
  number={1},
  pages={237--258},
  year={2021},
  publisher={Taylor \& Francis}
}

@inproceedings{zhang,
  title={Stochastic primal-dual coordinate method for regularized empirical risk minimization},
  author={Zhang, Yuchen and Lin, Xiao},
  booktitle={International Conference on Machine Learning},
  pages={353--361},
  year={2015},
  organization={PMLR}
}

@article{liu2,
  title={Asynchronous stochastic coordinate descent: Parallelism and convergence properties},
  author={Liu, Ji and Wright, Stephen J},
  journal={SIAM Journal on Optimization},
  volume={25},
  number={1},
  pages={351--376},
  year={2015},
  publisher={SIAM}
}

@article{eon,
  title={Online learning and stochastic approximations},
  author={eon Bottou, L},
  journal={On-linelearning in neural networks},
  volume={17},
  number={9},
  pages={142},
  year={1998}
}

@article{moulines,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}


@article{luo,
  title={On the convergence of the LMS algorithm with adaptive learning rate for linear feedforward networks},
  author={Luo, Zhi-Quan},
  journal={Neural Computation},
  volume={3},
  number={2},
  pages={226--245},
  year={1991},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{solodov,
  title={Incremental gradient algorithms with stepsizes bounded away from zero},
  author={Solodov, Mikhail V},
  journal={Computational Optimization and Applications},
  volume={11},
  pages={23--35},
  year={1998},
  publisher={Springer}
}

@article{nemirovski,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{dekel,
  title={Optimal Distributed Online Prediction Using Mini-Batches.},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={1},
  year={2012}
}

@article{cotter,
  title={Better mini-batch algorithms via accelerated gradient methods},
  author={Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@inproceedings{reddi,
  title={Stochastic variance reduction for nonconvex optimization},
  author={Reddi, Sashank J and Hefny, Ahmed and Sra, Suvrit and Poczos, Barnabas and Smola, Alex},
  booktitle={International conference on machine learning},
  pages={314--323},
  year={2016},
  organization={PMLR}
}

@article{sopyla,
  title={Stochastic gradient descent with Barzilai--Borwein update step for SVM},
  author={Sopy{\l}a, Krzysztof and Drozda, Pawe{\l}},
  journal={Information Sciences},
  volume={316},
  pages={218--233},
  year={2015},
  publisher={Elsevier}
}

@article{csiba,
  title={Importance sampling for minibatches},
  author={Csiba, Dominik and Richt{\'a}rik, Peter},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={962--982},
  year={2018},
  publisher={JMLR. org}
}

@article{castera,
  title={Second-order step-size tuning of SGD for non-convex optimization},
  author={Castera, Camille and Bolte, J{\'e}r{\^o}me and F{\'e}votte, C{\'e}dric and Pauwels, Edouard},
  journal={Neural Processing Letters},
  volume={54},
  number={3},
  pages={1727--1752},
  year={2022},
  publisher={Springer}
}


@article{richtarik,
  title={On optimal probabilities in stochastic coordinate descent methods},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Optimization Letters},
  volume={10},
  pages={1233--1243},
  year={2016},
  publisher={Springer}
}

@article{needell,
  title={Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm},
  author={Needell, Deanna and Ward, Rachel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{agarwal,
  title={A lower bound for the optimization of finite sums},
  author={Agarwal, Alekh and Bottou, Leon},
  booktitle={International conference on machine learning},
  pages={78--86},
  year={2015},
  organization={PMLR}
}

@article{polyak,
  title={Gradient methods for the minimisation of functionals},
  author={Polyak, Boris Teodorovich},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={3},
  number={4},
  pages={864--878},
  year={1963},
  publisher={Elsevier}
}

@article{byrd,
  title={A stochastic quasi-Newton method for large-scale optimization},
  author={Byrd, Richard H and Hansen, Samantha L and Nocedal, Jorge and Singer, Yoram},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={2},
  pages={1008--1031},
  year={2016},
  publisher={SIAM}
}

@article{liu3,
  title={A class of stochastic variance reduced methods with an adaptive stepsize},
  author={Liu, Yan and Han, Congying and Guo, Tiande},
  journal={URL http://www. optimization-online. org/DB\_FILE/2019/04/7170. pdf},
  year={2019}
}

@article{yang5,
  title={An accelerated stochastic variance-reduced method for machine learning problems},
  author={Yang, Zhuang and Chen, Zengping and Wang, Cheng},
  journal={Knowledge-Based Systems},
  volume={198},
  pages={105941},
  year={2020},
  publisher={Elsevier}
}

@article{ghadimi,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}