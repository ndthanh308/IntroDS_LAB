
\section{Prompting Model in Image-Text Matching}
\label{sec:4-clip}

% Figure environment removed

\subsection{Preliminary of Image-Text Matching Models}

Matching-based VLMs have introduced a novel training paradigm that facilitates the acquisition of joint multi-modal representations. Prominent models in this field, such as CLIP~\cite{radford2021learning}, ALIGN~\cite{li2021alignb}, ALBEF~\cite{li2021alignb} and Multi-Event CLIP~\cite{geng2022meclip}, leverage contrastive learning techniques to achieve joint representations for images and texts with a learning objective that aims to bring the representation of an image-text pair closer together while pushing non-pairs further apart.


By expanding training datasets~\cite{radford2021learning} and scaling up the model parameter, matching-based models exhibit adaptability across a broad spectrum of downstream tasks, including zero-shot benchmarks and fine-tuning scenarios.


Depending on the target of prompting, existing methods can be classified into three categories: \textit{prompting the text encoder}, \textit{prompting the visual encoder}, or \textit{jointly prompting both branches} as shown in Fig.~\ref{fig:branch}. These approaches aim to enhance the flexibility and task-specific performance of VLMs in recent studies.

A classic matching loss is formulated as below to align the image and text embeddings with an Image-to-Text loss $\mathcal{L}_{i2t}$ and a Text-to-Image loss $\mathcal{L}_{t2i}$.

% some equations
\begin{equation}
    \mathcal{L}_{i2t} = - \frac{1}{N}\sum^{N}_{i}\log(\frac{\exp{\mathrm{sim}(f_v^l(v_i), f_t^l(t_i))}}{\sum^{N}_{j}\exp{\mathrm{sim}(f_v^{l}(v_i), f^{l}_t(t_j)}})
\end{equation}

\begin{equation}
      \mathcal{L}_{t2i} = - \frac{1}{N}\sum^{N}_{i}\log(\frac{\exp{\mathrm{sim}(f_t^l(t_i), f_v^l(v_i))}}{\sum^{N}_{j}\exp{\mathrm{sim}(f_t^{l}(t_i), f^{l}_v(v_j)}})
\end{equation}


By prompting, we substitute model input by following learnable prompts:
for textual prompts:
$
    f^{l}_{v}([\{v_i\}^{M}_{i=1},\{z_i\}^{M}_{i=1}])
$
and for the general visual prompt:
$    f^{l}_{t}([\{c_i\}^{M}_{i=1},\{t_i\}^{M}_{i=1}])$
where $M$ is the number of prompts we use.



\subsection{Prompting Text Encoder of VLM}
Prompting language models has been long studied. 
As discussed in Sec.~\ref{sec:m2t}, we categorize prompts into hard prompts and soft prompts in this section.
As shown in Fig.~\ref{fig:branch}(a), learnable textual prompts are optimized on image-text pairs in a supervised manner. Recent works~\cite{huang2022unsupervised} also investigate a different scenario with unlabelled data.
In this section, we will delve into the details of soft prompts, exploring different types, such as global prompts, task-specific prompts, and instance-specific prompts.

\subsubsection{Hard prompt}
Prompting language models within the context of VLMs has been extensively investigated. The introduction of prompts has played a pivotal role in discovering and utilizing large-scale pre-trained Language Models.
Textual prompts mitigate handcrafted text templates (\eg,``\textit{a photo of a [CAT]}"), 
which enables the model to understand and respond to specific tasks without requiring explicit task-specific training, showcasing the flexibility and versatility of the model.
\cite{radford2021learning} utilize hard prompts to test its zero-shot performance on several tasks.
Hard prompts demand significant expertise in the domain and often involve high costs. This has given rise to a new learning paradigm as carefully refining prompts to optimize performance.

\subsubsection{Soft prompt}
The task of selecting an appropriate prompt is a complex endeavor that demands experience and domain expertise, and significantly impacts model performance. This raises an important question: can we dynamically 'search' for optimal prompts using gradient-descent-based learning methods?
Soft prompts refer to prompts that incorporate learnable parameters within their design. We categorize soft prompts into three main types: global soft prompts, group-specific prompts, and instance-specific prompts.

\noindent
\textbf{Global Soft Prompt.}
A straightforward yet powerful approach to adapting language models for downstream tasks involves modifying template tokens specifically for those tasks. Studies such as~\cite{gao2021making, shu2022test,zhou2022learning} have employed learnable prompts as input token embeddings when dealing with new tasks. Compared to fine-tuning the entire model, learning a small set of prompt embedding parameters proves to be more parameter-efficient and data-efficient.
These prompts, referred to as ``global soft prompts," are utilized consistently across all instances within a given task. The term ``global" signifies their universal usage throughout the task, enabling the model to generalize and perform well across inputs. 

\noindent
\textbf{Group-specific Prompt.}
Several recent studies~\cite{zhou2022learning, ju2022prompting, shen2022multitask} have employed a group of soft prompts specifically tailored to adapt to different tasks or types of inputs. These models enable the models to query and select appropriate prompts dynamically.
\cite{zhou2022learning,ju2022prompting,shen2022multitask} use a group of soft prompts targeted to adapt different tasks/types. Different prompts are queried based on the input data.
CoOp~\cite{zhou2022learning} finds that using different context prompts for classes (class-specific context) can enhance performance in fine-grained classification.  
\cite{ju2022prompting} uses task-specific prompts to adapt CLIP on a wide range of video understanding tasks.
\cite{shen2022multitask} proposes to use MVLPT with different task prompts for source and target tasks to share knowledge across task-specific prompts.



\noindent
\textbf{Instance-specific Prompt.}
While effective in some cases, task-grouped prompts can suffer from overfitting issues and may struggle to adapt to unseen classes or novel samples. In contrast, instance-specific prompts aim to customize prompts for individual samples, allowing for a more personalized and adaptive approach.
CoCoOp~\cite{zhou2022conditional} is a model that adopts instance-adaptive prompts, specifically instance-specific prompts, instead of relying solely on global prompts. This approach has been shown to enhance the generability of the model.

\subsection{Prompting Image Encoder of VLM}
In line with the achievements of prompt tuning in Natural Language Processing, there have been endeavors to extend the concept to visual inputs. According to the way of designing visual prompts, we categorize them into two classes: patch-wise prompts, where prompts are added as visual patches that are prepended to the original images, and annotation prompts which involve annotating prompts directly on the raw images

\noindent
\textbf{Patch-wise Prompts.}
Adding learnable patches as visual prompts is an intuitive method to incorporate visual cues into VLMs. Just as textual soft prompts serve as input tokens,~\cite{jia2022visual} introduces Visual Prompt Tuning (VPT), which learns a small set of visual prompts as visual patches. These patches are concatenated with input images to adapt pre-trained models to new tasks.
VPT investigates visual prompts in the input and latent layers and outperforms most other adaptation methods like full fine-tuning. 
In a similar vein,~\cite{bahng2022exploring} explores the use of visual perturbation as a visual prompting technique. Through adversarial reprogramming, the model learns to add visual prompts to input images.
Additionally,~\cite{shen2022multitask} adopts patchified visual tokens as learnable input embeddings.
\cite{wu2022unleashing} proposes applying normalized visual prompts to augmented images, unleashing the potential of visual prompting in diverse data settings.
In terms of promoting diversity in prompts,~\cite{huang2023diversity} employs different visual prompts for distinct subsets of data.

\noindent
\textbf{Annotation Prompts.}
Visual prompting can also be performed explicitly by directly manipulating images, similar to the process of annotation, which we term annotation prompts.
Colorful Prompt Tuning (CPT) introduced in~\cite{yao2022cpt} focuses on colorizing specific regions of images as visual prompts. By incorporating color cues, the model is guided to ground objects and better understand the visual context.
\cite{shtedritski2023what} explores the use of annotations, such as red circles, as an innovative visual prompting design. These annotations serve as cues to guide the model's attention toward specific areas of interest, thereby enhancing its understanding of images. The study delves into CLIP's emergent ability to comprehend images through the clever use of visual prompts.
Furthermore,~\cite{bar2022visual} proposes the use of example input and output images as visual prompts. By providing a pair of images demonstrating a desired task, such as image inpainting, edge detection, or image colorization, the model is guided to complete similar tasks based on the provided examples.
These studies demonstrate the creative and effective use of explicit visual prompting methods and offer a practical and interpretable approach to improving the model's performance.


\subsection{Unified Prompting on VLM}
As prompt engineering continues to advance in both the vision and language branches, there has been a recent development in joint prompting. This approach aims to enhance matching-based VLMs by leveraging prompts from both the visual and language domains. As in Fig.~\ref{fig:branch}, learnable prompts in both branches are optimized.
According to whether the visual prompt and textual prompt are independent of each other, they can be categorized into coupled and decoupled unified prompting, respectively, in out follow-up discussion.

\noindent
\textbf{Coupled Unified Prompting.}
UPT~\cite{zang2022unified} issues that prompting single modality does not fit all cases:  textual prompts may struggle to handle data with high intra-class visual variance, while visual prompts may struggle with data exhibiting high inter-class visual variance. Thus it employs a tiny neural network to optimize learnable textual and visual prompts jointly and finds unified prompting outperforms any unimodal prompting.



\noindent
\textbf{Decoupled Unified Prompting.}
\cite{shen2022multitask} employs soft textual prompts and VPT-like visual prompts on both the language and vision branches. This approach leverages the benefits of prompt engineering in both modalities
~\cite{yao2022cpt} introduces an innovative approach that combines visual and textual sub-prompts for visual grounding tasks. By utilizing regions in the image as visual prompts and phrases in a sentence as textual sub-prompts, the model can establish co-reference across different modalities. 
\cite{khattak2023maple} introduces MaPLe hierarchical prompts on both branches and synergizes prompt training in both modalities via a Vision-Language coupling function.
By leveraging the strengths of both visual and textual prompts, joint prompting contributes to more effective and versatile VLMs that excel in multimodal understanding.



\subsection{Application of Prompting}
Prompting matching-based VLMs offers the promise of transferring representations learned by pre-trained models to downstream domains and niche tasks like pure-vision tasks including image/video classification, semantic segmentation, relation detection, and multimodal tasks.

\noindent
\textbf{Image Classification.}
Image classification is extensively researched in computer vision for many years. A new approach to object classification has been suggested by prompting the text encoders in VLMs.
A Na\"ive solution to image classification is using a fixed prompt like ``\texttt{An image of [CLASS]}" as in CLIP~\cite{radford2021learning} and TPT~\cite{shu2022test},  which uncovers pre-trained capacities of zero-shot classification performance. 
Accordingly, learnable prompts are adapted to image classification in works like~\cite{shu2022test}
Prompt engineering also shows its efficacy in more challenging classification tasks like long-tailed classifcation~\cite{dong2023lpt}, multi-label classification~\cite{guo2023texts,sun2022dualcoop}.

\noindent
\textbf{Text Classification.}
Text classification appears to present a dual challenge akin to that of image classification. Due to the scope of this survey, we only cover works that focus on text classification of VLMs.
\cite{wen2022visual} uses visual prompts concerning different classes to better leverage visual information for text classification.

\noindent
\textbf{Object Detection.}
Object detection is aimed at predicting class labels of object bounding boxes in an image.
With abundant information on classes in texts, prompt engineering is also used for multi-label recognition.
\cite{sun2022dualcoop} proposes Dual Context Optimization (DualCoOp) using labels as a part of prompts and learning positive and negative prompt pairs to align images and prompts to solve multi-label recognition tasks.
\cite{guo2023texts} proposed Texts-as-Images (TaI) prompting for multi-label detection.
Open-vocabulary object classification is a promising application of prompt engineering in object classification, where the detectors can predict new classes that are not in training. ViLD~\cite{gu2022openvocabulary} generate a fixed prompt template, \eg ``\textit{a photo of [CATEGORY] in the scene}".~\cite{du2022learning} introduces detection prompt (DetPro) to learn continuous prompt representations. 
PromptDet\cite{feng2022promptdet} uses regional prompt learning to align region features and text features.


\noindent
\textbf{Visual Relation Detection.}
Visual relation detection is a computer vision task that targets extracting relations between objects in an image.
Prompt tuning boosts visual relation detection with its powerful commonsense knowledge contained in LLMs.
\cite{xiao2022optimizing} optimizes a small continuous task-specific vector for visual relation detection.
\cite{he2022openvocabularya} pre-trains VLMs with a matching-based strategy to align image regions and dense captions and fine-tune a decoder with soft prompts to generate relation predictions.
\cite{gao2023compositionala} presents a Relation Prompt for video open-vocabulary relation detection by generating subject-object sensitive prompts based on object motion cues. 

\noindent
\textbf{Semantic Segmentation.}
Semantic segmentation is a classic computer vision task with the goal of assigning each pixel to a class label.
DenseCLIP~\cite{rao2022denseclip} converts image-text matching to pixel-text matching to enable a pixel-wise dense prediction including semantic segmentation task; class-conditioned text prompts are used to contextualize visual cues in texts.
Segment anything~\cite{kirillov2023segment} presents a large-scale foundation model for segmentation which takes images and promptable segmentation queries as inputs.

Not only on a single task but prompting has also been proven to be beneficial for domain adaptation and generalization of pre-trained models. Further studies investigate prompt tuning on pre-trained model transferability under distribution shift.

\noindent
\textbf{Domain Adaptation.}
Prompt learning also enables continual learning of pre-trained models in tasks like test-time domain adaptation, which aims to adapt models to unlabeled test data under a distribution shift.
\cite{ge2022domain} attempts to embed domain information discrepancy in domain-specif textual prompts and can preserve semantic features of pre-trained VLMs. 
\cite{gao2022visual} adds prompts to different stages of ViT and fine-tunes prompts in the unlabelled target domain.

\noindent
\textbf{Continual Learning.}
Continual learning is aimed at tackling catastrophic forgetting in non-stationary data distribution. Prompt tuning becomes a new methodology for continual learning.
Learning to Prompt (L2P)~\cite{wang2022learning} shows a brand new prompt-based approach for continual learning by querying trainable task-specific prompts from a prompt pool for each input instance and prepends it to input before pre-trained models to instruct the model.
\cite{wang2022dualprompt} presents DualPrompt, to learn task-invariant and task-specific instructions across tasks, unlike that L2P uses only one prompt tool and calls them General and Expert prompt space.

\noindent
\textbf{Domain Generalization}
Domain generalization targets adapting models to unseen domains in the training stage. 
\cite{zheng2022prompt} encapsulates domain-specific knowledge in domain prompts generated by a prompt adapter and prepends it with input data; at test-time, prompts are generated based on the similarities between domains.

\subsection{Responsible AI Considerations of Prompting}
The importance of AI Integrity and ethics has been attached to prompting matching-based VLMs to construct trustworthy multimodal models. The discussion covers model robustness, safety, fairness, bias, privacy, and the like.

\noindent
\textbf{Adversarial Robustness of Prompt.}
Robustness analysis evaluates the performance of the model under different conditions and perturbations.
\cite{mao2023understanding} studies how VPT and fine-tuning improve zero-shot robustness under adversarial attack on CLIP and finds that VPT is more effective in the absence of texts. 
\cite{chen2023visual} also attempt to leverage universal visual prompting to improve the adversarial robustness at test time. Visual prompting is more flexible compared to conventional adversarial defenses, as it allows universal (\ie, data-agnostic) input prompting templates, which are capable of plug-and-play during testing. 
\cite{fang2022data, shi2023effective} investigate the reasons behind VLMs' robustness to natural distribution shifts systematically and reveals that diverse training data is the primary reason for robustness gain.
\cite{xu2022exploring} explores the model vulnerability that injecting triggers brings to pre-trained models in prompt tuning.  


\noindent
\textbf{Backdoor Attack of Prompt Learning.}
\cite{carlini2022poisoning} studies on the backdoor and poisoning attacks on CLIP and find CLIP trained on manually labeled data suffer badly from such attacks. It shows that the training on noise and uncurated datasets makes backdoor and poisoning attacks a significant threat. 
\cite{jia2022badencoder} proposes a new backdoor attack method named BadEncoder on CLIP and exposes this threat to VLMs. Once a pre-trained image encoder has been injected backdoors, the downstream classifiers built on it for different downstream tasks simultaneously inherit the backdoor behavior. 
Given such vulnerability to backdoor attacks, CleanCLIP~\cite{bansal2023cleanclip} is proposed as a finetuning framework that weakens the learned spurious associations introduced by backdoor attacks. 

\noindent
\textbf{Fairness and Bias.}
Social bias is an important topic in a fair AI system. A wide range of works have studied different aspects of biases.
\cite{agarwal2021evaluating} showcases an analysis of bias regarding race and gender misclassification in the CLIP model. 
In the meantime, many existing works focus on de-biasing the model.
In particular,~\cite{chuang2023debiasing} attempts to alleviate bias by calibrating the biased prompted texts to debiased content while~\cite{kong2023mitigating} proposes to mitigate biased results in image retrieval tasks by post-processing of the VLMs output.
In addition,~\cite{smith2023balancing} introduces a new dataset debiasing pipeline to augment the dataset with healthy data.


