

\section{Taxonomy}
\label{sec:2-taxonomy}
In this section, terms and notations related to Prompting Engineering on VLMs used throughout the paper are introduced.

\subsection{Terminology}
\label{sec:terminology}
This is a list of terms along with their descriptions. Note that instead of formally defining the following concepts, we provide a general description for readers. 

\vspace{0.1cm}
\begin{itemize}[noitemsep,nolistsep]
\setlength\itemsep{0.5em}
    \item \textit{Prompt:} Additional information or hints provided to a model to guide its behavior or help it perform a specific task;
    \item \textit{Prompting Method:} An approach used to incorporate prompts into the input to guide model behavior or enhance model performance;
    \item \textit{Multimodal-to-Text Generation:} Generating textual descriptions or narratives from multimodal input data, \eg a combination of vision and language data; 
    \item \textit{Image-Text Matching:} Establishing a semantic relationship or alignment between images and textual descriptions;
    \item \textit{Text-to-Image Generation:} Generating visual images from textual descriptions.
    \item \textit{In-context Learning:} A prompting method by providing models with instructions or demonstrations within relevant contexts to solve new tasks without requiring additional training. 
    
    \item \textit{Chain-of-thought:} A prompting method that enhances reasoning skills by instructing a model to generate a sequence of intermediary actions that guide towards solving a multi-step problem and reaching the ultimate solution.


\end{itemize}

\subsection{Notations}
These are the mathematical notations that are followed throughout the paper (Tab.~\ref{tab:notation}). All the formulations of this work will stick to these notations unless otherwise specified. 

\begin{table}[!ht]
    \centering
    \caption{The used mathematical notations are listed. They are followed throughout the paper.}
    \begin{tabular}{c|c}
    \toprule
       $x$  & A clean input image \\
    \midrule
       $t$  & A sentence paired with an image \\
    \midrule
       $y$  & A ground-truth class label of an image \\
    \midrule
       $\chi$ & Input distribution \\
    \midrule
       $f(\cdot)$   & A vision-language model \\
    \midrule
       $f_v(\cdot)$   & A visual encoder \\
    \midrule
       $f_e(\cdot)$   & A textual encoder \\
    \midrule
       $\{v_i\}_{i=1}^M$   &  visual tokens \\
    \midrule
       $\{c_i\}_{i=1}^M$  &  textual tokens \\
    \midrule
        $\{z_i\}_{i=1}^M$   &  visual prompt tokens \\
    \midrule
        $\{t_i\}_{i=1}^M$   &  textual prompt tokens \\
    \midrule
        $H^l$  & $l^{th}$ A layer of the target network \\
    \midrule
        L & Label word token \\
    \midrule
        $H^i_k$ & the $k^{th}$ activation in $l^{th}$ layer of the target model \\
    \midrule
        $z^i$  & Model output logits \\
    \bottomrule
    \end{tabular}
    \label{tab:notation}
\end{table}

