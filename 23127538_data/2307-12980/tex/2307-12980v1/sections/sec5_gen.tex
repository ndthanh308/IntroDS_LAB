\section{Prompting Model in Text-Image Generation}
\label{sec:5-text-img}
\subsection{Preliminary of Text-Image Generation Models}
% Figure environment removed

This section provides an overview of the preliminaries required to understand the prompting model in text-image generation, with a specific focus on diffusion models. 

Text-image generation automatically synthesis vivid and realistic images from natural language descriptions and has attracted much more attention. From the pioneering work DRAW~\cite{gregor2015draw}, text-image generation models have seen numerous breakthroughs. Generative adversarial network (GAN)~\cite{goodfellow2020generative} then used to design end-to-end differentiable image generation structure~\cite{reed2016generative} which is followed by many works~\cite{pan2023drag, karras2019style, isola2017image}. Besides, variational auto-encoder (VAE)~\cite{kingma2019introduction} is also adapted to generate images~\cite{pu2016variational, vahdat2020nvae}. However, these models are trained on small-scale data and lack generalization~\cite{ramesh2021zero}. Autoregressive methods driven by large-scale datasets, such as DALL-E~\cite{ramesh2021zero}, and Parti~\cite{yu2022scaling}, are proposed and demonstrate surprising zero-shot generation ability. Recently, the diffusion model (DM) has spurred another line of state-of-the-art models for text-image generation~\cite{dhariwal2021diffusion}. 
Diffusion models, also known as diffusion probabilistic models~\cite{sohl2015deep},  originate from non-equilibrium statistical physics~\cite{jeulin1997dead} and sequential Monte Carlo~\cite{neal2001annealed} and are designed to fit any data distribution while keeping tractable. The denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising} first adopt DMs in the image generation domain and inspire the whole community of generative models. In inference, DDPMs build a Markov chain that generates images from noisy data within finite transitions which is called \textit{reverse process}. In training, DDPMs learn from the \textit{forward process} where noise is added to the natural images and estimated by the model. Given a clean image $x_0$ from a distribution $q$, diffusion step $T$ and hyperparameters $\beta_t$, the forward process generates $x_T$ following
\begin{equation}
    q(x_{1:T}|x_0) \coloneqq \prod_{t=1}^T q\left(x_t \mid x_{t-1}\right),
\end{equation}

\begin{equation}
    q\left(x_t \mid x_{t-1}\right)\coloneqq\mathcal{N}\left(x_t ; \sqrt{1-\beta_t} x_{t-1}, \beta_t I\right).
\end{equation}

The noised image from any arbitrary step $t$ then can be reformulated as 
\begin{equation}
    q\left(x_t \mid x_0\right)\coloneqq\mathcal{N}\left(x_t ; \sqrt{\bar{\alpha}_t} x_0,\left(1-\bar{\alpha}_t\right) I\right),
\end{equation}
where $\alpha_t\coloneqq 1-\beta_t, \bar{\alpha_t}\coloneqq \prod^t_{s=0}\alpha_s$.

Given the defined forward process, the DDPM is trained in the reverse process which starts from $p_{\theta}(x_T)$ by the loss defined as 

\begin{equation}
    L(\theta)\coloneqq\mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t\right)\right\|^2\right],
\end{equation}
where $t$ is uniform between $1$ and $T$, $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ is random noise, and $\boldsymbol{\epsilon}_{\theta}$ is known as noise predictor parametrized by $\theta$. 

By incorporating additional control information, typically in the form of textual prompts, the efficacy of the reverse process in diffusion models has been significantly enhanced to control the synthesis results rather than random sampling. This textual-based generation has solidified its position as the pioneering foundation in the field of text-to-image generation. Consequently, let $\Gamma$ be an encoder that maps a conditioning input prompt $P$ into a conditioning vector $\boldsymbol{c}\coloneqq \Gamma(P)$, the conditioned learning objective has been expanded with $\boldsymbol{c}$ that represents textual prompt. 
\begin{equation}
    L(\theta)\coloneqq\mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}, \boldsymbol{c}}\left[\left\|\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\sqrt{\bar{\alpha}_t} \mathbf{x}_0+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}, t, \boldsymbol{c}\right)\right\|^2\right],
\end{equation}

Figure 2 illustrates a typical text-image generation framework, highlighting its key components and functionalities, including (1) fixed or learnable conditional information, such as hard textual prompts or learnable soft prompts. The conditional information can be in textual form or in other modalities; (2) An encoder $\mathcal{E}$ of the input image; (3) A generative model, such as diffusion model, autoregressive model, or GAN; (4) Noise injection or interference; (5) A representation of features in the latent space or low-resolution images; (6) A decoder $\mathcal{D}$ for image decoding or super-resolution for a high-fidelity generation. The training process involves dataset utilization, loss functions, and optimization techniques to train the model for generating coherent and visually appealing images based on text prompts. During the inference stage, the trained model is utilized to generate images based on user-specified prompts. The formulation of prompts plays a crucial role as it governs communication with the model and influences the desired outcomes of image generation. This section focuses on prompt engineering in text-image generation and its applications.

\subsection{Understanding Prompting} \label{sec 5.2}
To gain a deeper understanding of the factors that influence the generated images, we will introduce prompt design in text-to-image from the view of semantics, prompt diversity, and controllable prompts.

\noindent\textbf{Semantic Prompt Design. }
The art of prompt semantics has significant impacts on image generation in diffusion models~\cite{witteveen2022investigating}. 
The linguistic components such as adjectives, nouns, and proper nouns in prompt influence image generation in different ways but consistently. While descriptors (simple adjectives) subtly affect the output, nouns introduce new content more effectively. Interestingly, using an artist's name tends to generate images deviating significantly from the original, and incorporating lighting phrases can dramatically modify image content and mood. Therefore, the quality of image generation can be enhanced through clear, noun-based statements, effective seeds, and the emulation of artist styles.

\noindent\textbf{Diversify Generation with Prompt.}
Apart from direct handcrafting individual prompts in a semantic way, recent works experiment with various prompt modifiers $\mathcal M$ focusing on enhancing the diversity of initial prompts $P$ by $\Tilde{P} = \mathcal M(P)$ with $\Tilde{P}$ be the diversified prompts. 
DiffuMask~\cite{wu2023diffumask} explores two strategies in prompt modifiers $\mathcal M$, \ie, retrieval-based prompt and prompt with Sub-class, with $P$ set to \textit{"Photo of a [sub-class] car in the street"}. Specifically, they retrieve real images and captions sets~\cite{beaumont2022clip, radford2021learning}, with captions as the prompt sets for generating synthetic images. Besides, they select sub-classes from Wiki based on the main class. 
ImaginaryNet~\cite{ni2022imaginarynet} uses GPT2~\cite{radford2019language} as $\mathcal M$ with a given class name $y$ of the target object to generate a complete description of an imaginary scene $\Tilde{P}_y$ under the guidance of prefix phrases of \textit{"A photo of a"}. The prompt serves as generating diversified photo-realistic imaginary images for the imaginary supervised object detection task. 
Similarly,~\cite{he2023synthetic} uses a word-to-sentence T5 model~\cite{raffel2020exploring} as $\mathcal M$ to generate detailed prompts $\Tilde{P}_y$ targeted for a specific label space $y$, thereby maximizing the potential of synthesized data in data-scarce settings by enriching the diversity of prompts.
These approaches further obtain diversified images $I$ by $I = \mathcal{G}(\boldsymbol{\epsilon}|\Tilde{P})$ where $\mathcal{G}$ represents the generative model.

\noindent\textbf{Complex Control of Synthesis Results.}
As the synthesized image generation is usually inconsistent due to noise injection and randomness lying in the stochastic nature of diffusion models, recent work has been emerging in the area of complexly controllable generation. To avoid controllability limitations with user-provided masks that restrict the modified area~\cite{avrahami2022blended, nichol2021glide}, prompt-based control is gaining attention. OneWord~\cite{gal2022image} aims to solve the problem of generating personalized images with specific subjects that are hard to describe with pure texts. Therefore they proposed a prompt method that designates a placeholder string $S_{\ast}$ to represent the new concept such as \textit{"a photograph of $S_{\ast}$ on the beach"} with its associated learned embedding $v_\ast$.
A similar design is done by DreamBooth~\cite{ruiz2023dreambooth}. Instead of creating new words, 
they design prompts with \textit{(unique identifier, subject)} pairs that bind rare tokens from T5-XXL tokenizer~\cite{raffel2020exploring} as unique identifiers for the specific subjects and the coarse class name of the subjects, such as \textit{"A [V] dog"}, with \textit{[V]} as the rare-token identifiers. They further retain the representation of class names in prompts by introducing extended class-prior preservation loss to the training objective. 
Custom Diffusion~\cite{kumari2022multi} extended the customization into a multi-concept scenario where multiple personalized concepts are composed in the same generated image, such as family members in the same family photo. They design prompts at this aim by including the use of a unique modifier token \textit{$S^*_i$} for each concept \textit{$i$}, initialized with different rare tokens and positioned ahead of category namex. 


Over the days, textual prompts only cannot meet the specific needs of image-processing tasks, and controllable text-to-image generation is gaining attention~\cite{feng2022training, epstein2023diffusion, kawar2023imagic}. A wide range of task-specific input conditions, such as canning edge encoded by image encoder~\cite{canny1986computational}, are added with trainable network architecture to the diffusion model in the work of ControlNet~\cite{zhang2023adding}. The additional task-specific conditions $\boldsymbol{c}_{\mathrm{f}}$ is added to the overall training objective as 
$\left.L(\theta)\coloneqq\mathbb{E}_{t, \boldsymbol{x}_0, \boldsymbol{\epsilon}, \boldsymbol{c}, \boldsymbol{c}_{\mathrm{f}}}\left[\| \boldsymbol{\epsilon}-\boldsymbol{\epsilon}_\theta\left(\boldsymbol{x}_t, t, \boldsymbol{c}, \boldsymbol{c}_\mathrm{f}\right)\right) \|_2^2\right]$.
Notably, in order to improve the semantic recognition ability of the encoder from control maps and optimize ControlNet's performance even when explicit prompts are absent, ControlNet's training utilizes a method where half of the text prompts are randomly replaced with empty strings.

Controlling the synthesis results can also be done after the generation process with prompt editing methods. To bypass the common demand of user-defined spatially fixed masks~\cite{avrahami2022blended, nichol2021glide}, Prompt-to-Prompt~\cite{hertz2022prompt} can edit images by only editing prompts by replacing a word, specifying a style, changing adjectives, \etc. The manipulations are infiltrated by injecting the cross-attention maps controlling which pixels attend to which tokens of the prompt text during which diffusion steps. Prompt-based image editing methods that merely modify the text prompt provide more intuitive editing experiences. 



\subsection{Application of Prompting}
Text-to-image diffusion models, aided by prompting techniques, have excelled in data generation applications. This section investigates their efficacy in generating training data that boost the scope and flexibility of learning procedures. Additionally, we explore the versatility of these models in crafting diverse data in target domains, spanning diverse output formats like images, videos, 3D models, and motion. Also, we unveil its potential in complex task-solving and adversarial attacks. 


\subsubsection{Generating Synthetic Training Data} Recent advancements have sparked a growing interest in prompting text-to-image models as innovative synthesized training data generators for various tasks downstream tasks such as segmentation, object detection, and image recognition. Challenges such as data scarcity and the need for high-resolution synthetic images can be mitigated through intricate prompt engineering. DiffuMask~\cite{wu2023diffumask} automatically generates high-resolution synthetic training images with the aforementioned prompt engineering strategies in Sec.~\ref{sec 5.2}. Its created pixel-level semantic masks between prompts and generated images can be seamlessly applied for segmentation tasks, including semantic segmentation, open-vocabulary segmentation, and domain generalization on real images.
ImaginaryNet~\cite{ni2022imaginarynet} generates synthesis data to tackle the challenge of insufficient real images and annotations for training object detection. It generates scene descriptions with LLM from class labels and prompts the text-to-image model for creating imaginary training data.
Under different training settings of pure or mixed imaginary and real data, object detectors are enhanced for the Imaginary Supervised Object Detection task (ISOD), especially under settings where real images and annotations are unavailable.
Synthetic data is also proven feasible for image recognition tasks, specifically in zero-shot and few-shot settings. ~\cite{he2023synthetic} creates the synthetic data for image recognition in a two-phase manner. Firstly, novel samples are synthesized using target category names. Secondly, a fine-tuned language model is used to convert category names into richly contextual, diversified language prompts for diversifying the training data. 


\subsubsection{Generating Data in Target Domain} In addition to the role of training data generators, diffusion models also play a pivotal role as target data generators. Importantly, their capabilities extend beyond the generation of images. They can efficiently generate video data, three-dimensional data, and motion data, further broadening their application range and utility.

\noindent\textbf{Text-to-Video Generation.}
Make-A-Video from~\cite{singer2022make} is the first approach for directly translating the tremendous recent progress in text-to-image (T2I) generation to text-to-video (T2V) without paired text-video data. It infers actions and events in the prompt and generates video by leveraging joint text-image priors to bypass the need for paired text-video data. 
Imagen Video~\cite{ho2022imagen} propels T2V generation towards a more efficient stage, delivering higher video resolution outputs by combining a frozen T5 text encoder~\cite{raffel2020exploring}, a base video diffusion model, and interleaved spatial and temporal super-resolution diffusion models, \ie, cascaded diffusion models~\cite{ho2022cascaded}.
However, works on T2V commonly face challenges in editing capabilities and effective training on specific domains. FateZero~\cite{qi2023fatezero} overcomes these limitations with a zero-shot text-based editing method capable of editing attributes, style, and shape on real-world videos without per-prompt training or use-specific mask. Specifically, FateZero utilizes a pair of user-provided source prompt $P_{src}$ and the editing prompt $P_{edit}$. The source prompt is for obtaining a noisy latent representation $\boldsymbol{x}_t$ of the source video frame, then $\boldsymbol{x}_t$ is denoised conditioned on the editing prompt $P_{edit}$.
Tune-A-Video~\cite{wu2022tune} tackles the challenge of computational expensiveness with the one-shot tuning strategy on one text-video pair and only on the first and former video frames. This study is in the inspiration that T2I models attend well to verbs in the prompt in generating still images and exhibit surprisingly good motion consistency alignment with prompts when extended to T2V. Tune-A-Video is also equipped with editing capability by capturing essential motion information from the input video and synthesizing novel videos with edited prompts preserving the motion words. Moreover, textual prompt-based generation has advanced to multi-modal generation, \eg generating simultaneously aligned audio-video pairs~\cite{ruan2023mm, zhu2023moviefactory}.

\noindent\textbf{Text-to-3D Generation.}
Previous works face challenges of insufficient large-scale labeled 3D datasets and inefficient architectures for denoising 3D data. As a consequence, prompt-based generation has advanced from T2I to T2V models and also in text-to-3D scenarios where high-quality 3D objects and scenes are generated from text prompts~\cite{muller2023diffrf}. 
DreamFusion~\cite{poole2022dreamfusion} firstly randomly initializes the 3D object with NeRF~\cite{mildenhall2021nerf}  for each text prompt and produces 2D image renderings $x=g(\eta)$ with differentiable image generator $g(\eta)$. These renderings are generated from various angles and paired with view-dependent prompts prefixes such as \textit{"overhead view"} and \textit{"front view"} and then diffused and reconstructed by Imagen~\cite{ho2022imagen} with $q\left(x_t \mid x_0\right)\coloneqq q\left(g(\eta)_t \mid g(\eta)_0\right)$. The sampled noise $\boldsymbol{\epsilon}$ guides a gradient direction to be backpropagated to the NeRF parameters $\eta$.
To tackle the issue in the growing popular DreamFusion regarding the optimization efficiency of NeRF which leads to low-quality 3D models with a long processing time, Magic3D~\cite{lin2022magic3d} proposed a two-phase coarse-to-fine optimization framework, \ie, firstly obtaining coarse diffusion prior from text prompts with Imagen~\cite{ho2022imagen} and then rendering efficiently with high-resolution latent diffusion models (LDM)~\cite{rombach2022high}. Borrowing the idea from~\cite{ruiz2023dreambooth}, Magic3D is capable of personalized prompt-based editing of 3D models by binding the \textit{[V]} identifier in the prompt with the 3D object.
Besides, prompt-based editing can be done through finetuning with LDM in the coarse-to-fine stage with the modified prompt. 
Inaccurate and unfaithful structures in text-to-3D generation due to random shape initialization without prior knowledge lead Dream3D~\cite{xu2022dream3d} to explicit 3D shape priors into the CLIP-guided 3D optimization process~\cite{poole2022dreamfusion, lee2022understanding, khalid2022clip}. Specifically, it connects the T2I model and a shape generator as the text-to-shape stage to produce a 3D shape prior with shape components in the prompts. Then it harnesses the 3D shape prior to the initialization of NeRF and optimizes it with the full prompt. To close the gap between the synthesis image and shape, and also inspired by ~\cite{gal2022image, ruiz2023dreambooth}, Dream3D links renderings with stylized text prompt suffixes in the format of \textit{"a CLS in the style of $\ast$"} where \textit{CLS} represents the shape category and \textit{$\ast$} is a placeholder token that requires optimization of its text embedding jointly with the weights of Stable Diffusion for capturing the style of the rendered images.

\noindent\textbf{Text-to-Motion Generation.}
Another area where the power of prompt-based generation is exemplified is in the realm of text-to-motion (T2M).
MotionDiffuse~\cite{zhang2022motiondiffuse} is a diffusion model-based text-driven motion generation framework with motion sequence as the input $\textbf{x}_0$. It has a body part-independent controlling scheme that generates separate sequences for each body part under $m$ fine-grained prompts $P_i$ with $i\in [1,m]$ for each body part $i$ and predicts each $\epsilon_i^{part} =\epsilon_\theta\left(\mathbf{x}_t, t, \Gamma(P_i)\right)$. Besides, it generates arbitrary-length continuous motion synthesis using time-varied text prompts with $m$ intervals, denoted as array $\left\{P_{i, j},\left[l_{i, j}, r_{i, j}\right]\right\}$ and predicts the $\epsilon_i^{time}$. All noises are interpolated mutually with other parts for the continuous motion sequence generation.
Similarly, as in T2I, T2V, and text-to-3D, it is also required for T2M synthesis with flexible editing capability.  
Thus, FLAME~\cite{kim2022flame} enables editing with free-form language description with novel transformer-based diffusion architecture. It takes diffusion time-step tokens, motion length tokens, language tokens, and motion tokens as input tokens to the transformer and can therefore handle motion sequences of variable length. MDM~\cite{tevet2022human} also introduces editability and controllability with a similar idea borrowed from image inpainting by adding suffixes and prefixes to the motion in the temporal domain. And the textual condition guides MDM to fill the missing body part with a specific motion while keeping the rest intact in the spatial domain.

\noindent\textbf{Complex Conditional Scene Generation.} The use of diffusion models has expanded beyond single target data generation, finding applications in various scenarios that involve generating more complex scenes tailored to specific use cases with more complex conditional inputs. In robotics, text guidance is used to perform aggressive data augmentation on top of our existing robotic manipulation datasets to generate robotic scenes via inpainting various unseen objects for manipulation, backgrounds, and distractors~\cite{yu2023scaling}. In autonomous driving, diffusion models are leveraged to generate controllable pedestrian trajectories that align with the surrounding environment's context that enables the simulation of realistic pedestrian behavior~\cite{rempeluo2023tracepace}. Additionally, diffusion models can incorporate conditional information in the form of graphs that represent individual rooms to generate house floorplans, facilitating the design and planning of residential spaces~\cite{shabani2023housediffusion}.

\subsubsection{Prompt-centered Complex Task} 
Beyond the former direct applications of text-to-other generation, prompt-centered complex application in various scenarios reveals the field's true versatility and potential. In the context of storytelling, StoryBook~\cite{jeong2023zero} retains a visual narrative storybook with consistent character faces through a series of prompt-centered steps. It first generates prompts of scene descriptions with LLM, which are prompted to the latent diffusion model with designated special token placeholder $S_\ast$ like~\cite{gal2022image}, to ground consistent character faces during generation.
Similarly,~\cite{lu2023multimodal} proposed multimodal procedure planning (MPP) task, where the initial stepwise textual plan is generated with LLM and then serves as prompts to diffusion model for synthesizing text-grounded image plan. What's different is that the image plans are verbalized through image captioning backward to the LLM for revising the initial plan showing the potential for multimodal prompting.

\subsection{Responsible AI Considerations of Prompting}
\label{sec:t2i-ethical}
Artificial Intelligence is revolutionizing our world through its formidable learning ability, transformative force, and profound influence across diverse areas of society. It also spurred intense debate about ethical issues, principles, and integrity in AI development and applications. There is a global convergence around five ethical principles~\cite{jobin2019global}: transparency, justice and fairness, non-maleficence, responsibility, and privacy. In this subsection, we discuss ethical issues when prompting text-to-image generative models.  

\noindent
\textbf{Adversarial Robustness of Prompt.}
The adversarial attacks have been introduced to text-to-image diffusion models for mainly 2 aims. Some work takes diffusion models as a tool to facilitate or defend against adversarial attacks~\cite{chen2023diffusion, nie2022diffusion}. Some work directly attacks diffusion models~\cite{zhuang2023pilot} and aims to erase image content given character perturbations. As the pioneer to introduce diffusion models in the adversarial attack field, DiffAttack~\cite{chen2023diffusion} unveils the potential of diffusion models for crafting adversarial examples with satisfactory imperceptibility and transferability by manipulating the latent space rather than pixel space. This approach maintains visual quality with embedding perturbations undetectable to humans and transferable across diverse model architectures. Diffusion models can be utilized for adversarial purification - a defense strategy that removes adversarial perturbations. DiffPure~\cite{nie2022diffusion} implements this approach, adding a minimal amount of noise to an adversarial example before reversing the generative process to restore the original image, thus exhibiting robust defense capabilities against powerful adaptive attacks. Zhuang \etal~\cite{zhuang2023pilot} study the query-free attack generation on Stable Diffusions where an adversarial text prompt is obtained in the absence of end-to-end model queries. They show the vulnerability of Stable Diffusions rooted in the text encoders. A five-character text perturbation is able to shift the output content. 

\noindent
\textbf{Backdoor Attack of Prompt Learning.}
Backdoor attacks on text-to-image generative models aim to control the content of generated images during inference by embedding inputs with predefined backdoor triggers. The attacker secretly injects backdoors, such as specific text characters, into the model during training to trigger the model to either generate images with pre-defined attributes or images following a hidden or even malicious description. The backdoor attack may lead to inappropriate outputs such as offensive content. On the other hand, it can also be used in copyright protection by watermarking the models. Struppek \etal~\cite{struppek2022rickrolling} demonstrate that the text encoders pose a major tampering risk. The attack is a teacher-student approach and only involves fine-tuning a text encoder by generating backdoor targets and triggers on the fly. Zhai \etal~\cite{zhai2023text} design three types of backdoor attacks, namely pixel-backdoor, object-backdoor, and style-backdoor, and demonstrate the text-to-image diffusion models' vulnerability to backdoor attacks. 
Huang \etal~\cite{huang2023zero} explore the vulnerability to backdoor attacks via personalization for a more efficient attack. Text-to-image personalization guides the diffusion-based text-to-image model to generate user-provided novel concepts through natural language. Huang \etal~\cite{huang2023zero} devised backdoor attacks on two families of personalization methods, Textual Inversion~\cite{gal2022image} and DreamBooth~\cite{ruiz2023dreambooth}. 

\noindent
\textbf{Fairness and Bias.}
Generative AI models are typically trained on web-scale datasets scraped from the internet and are inevitable to biased human behavior as shown in~\cite{friedrich2023fair, naik2023social, wang2023T2IAT, luccioni2023stable}. For example, Stable Diffusion only generates images with white male-appearing persons as firefighters~\cite{friedrich2023fair}. Some studies start to pay more attention to the fairness issues related to text-to-image generations and can be grouped into three paradigms: 1) training data pre-processing to remove bias before learning~\cite{smith2023balancing, seth2023dear}, 2) enforcing fairness during training by introducing constraints on the learning objective~\cite{seth2023dear}, 3) post-processing approaches to modify the model outcome at the deployment stage~\cite{friedrich2023fair, chuang2023debiasing, kim2023explaining}. 

\noindent
\textbf{Privacy.}
There might be privacy-sensitive information, \eg face identity, in the huge amount of training data for training text-to-image models. Such information may arise privacy risks in real-world applications such as information leaks. Membership inference attacks are an approach to investigating privacy leakage by inferring whether a specific data sample was used in the training phase (called member or non-member respectively)~\cite{wu2022membership}.  Some work~\cite{wu2022membership, duan2023diffusion, webster2023reproducible} studies the privacy risks of text-to-image generation models from the perspective of membership attacks. From the perspective of prompting, Shen \etal~\cite{shen2023prompt} propose \textit{prompt stealing attack}, which steals prompts from images generated by text-to-image generation models. The creation of high-quality prompts can be challenging, time-consuming, and costly. Hence successful prompt stealing attacks direct violate intellectual property and even jeopardize the business model of prompt trading markets. 
