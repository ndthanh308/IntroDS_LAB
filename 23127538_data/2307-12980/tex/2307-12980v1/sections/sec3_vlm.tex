\section{Prompting Model in Multimodal-to-Text Generation}
\label{sec:3-mm-text}
\subsection{Preliminaries of Multimodal-to-Text Generation}

Large language models (LLMs) have demonstrated impressive capabilities in the field of NLP, prompting researchers to explore ways of integrating visual modality into these models' training framework. This integration aims to enhance their linguistic prowess and expand their applicability to multimodal tasks.
% 

To maintain consistency with the training methodologies employed by LLMs, generation-based vision-language models (VLMs) typically comprise three essential components: \textit{text feature}, \textit{visual feature}, and \textit{fusion module}. These components synergistically collaborate, enabling the models to effectively leverage textual and visual information to generate coherent and contextually relevant outputs.


Incorporating the visual modality into LLMs has opened up exciting opportunities for various applications, such as visual commonsense reasoning~\cite{huang2023language}, visual question answering~\cite{alayrac2022flamingo, yang2022empirical, tsimpoukelli2021multimodal, huang2023language}, multimodal dialogue systems~\cite{openaigpt4, alayrac2022flamingo, wu2023visual}, \etc. By combining textual and visual cues, VLMs have the potential to provide a more comprehensive understanding of multimodal data and produce outputs that align with human-like reasoning and perception~\cite{vlmsurvey}. Furthermore, the fusion of text and visual features within VLMs plays a crucial role in seamlessly integrating information from both modalities. This fusion process enables the model to capture interdependencies and interactions between textual and visual elements, resulting in more accurate and contextually grounded generations~\cite{vlmsurvey}.


\textbf{Text Feature.} Early studies on VLMs commonly employed the preprocessing technique introduced by BERT~\cite{devlin2018bert}. The raw text undergoes tokenization and is concatenated with special tokens, \texttt{[CLS]} and \texttt{[SEP]}, represented as \texttt{<[CLS],$c_1$,...,$c_m$,[SEP]>}, where token $c_i$ is associated with a word embedding. However, with the progression of language model research, more advanced models have emerged, showcasing emergent abilities such as in-context learning~\cite{pritchett2015learning} and chain-of-thought reasoning~\cite{wei2022chain}. Building upon these advancements, the latest generation of VLMs has embraced powerful language models like T5~\cite{raffel2020exploring} and GPTs~\cite{brown2020language}, which further enhances their linguistic capabilities.


To accommodate different modalities in the input, recent works have introduced new special tokens. For example,~\cite{singh2022flava} incorporate an additional image classification token \texttt{[CLS\_I]}, while~\cite{huang2023language} use \texttt{<image>} and \texttt{</image>} to indicate the beginning and end of the encoded image embedding and \texttt{<s>} and \texttt{</s>} to mark the beginning and end of a sequence. In another approach,~\cite{alayrac2022flamingo} employs \texttt{<BOS>} to represent the ``beginning of sequence" and \texttt{<EOC>} to denote ``end of chunk". These special tokens serve to differentiate and identify the boundaries between different modalities, allowing the model to effectively process and leverage multimodal information.

\noindent
\textbf{Visual Feature.} To obtain a consistent representation of input as a sequence of embeddings for both modalities, the image $x$ is transformed into a sequence of embedding vectors: $x = <v_1, v_2, ..., v_M>$. Accurately representing the information conveyed by images is crucial for downstream tasks but can be challenging. CNN structures have been commonly used in prior research for extracting image features. For instance, models like ViLBERT~\cite{lu2019vilbert} and VL-T5~\cite{cho2021unifying} employ faster R-CNN~\cite{ren2015faster} to detect object regions in images and encode them as a sequence of Region-Of-Interest (ROI) features. However, this approach may overlook important regions in an image. To address this limitation, approaches like OFA~\cite{wang2022ofa} and Flamingo~\cite{alayrac2022flamingo} utilize ResNet to encode information from the entire image, considering a broader context. Additionally, leveraging the powerful feature extraction capabilities of the transformer architecture, models such as SimVLM~\cite{wang2021simvlm}, PaLI~\cite{chen2022pali}, MAGMA~\cite{eichenberg2021magma}, and BLIP2~\cite{li2023blip2} adopt the Vision Transformer (ViT)~\cite{dosovitskiy2020image} architecture for image representation. This allows them to effectively capture visual information and incorporate it into the multimodal framework.

\noindent
\textbf{Fusion Module.} The fusion module plays a crucial role in integrating text and image embeddings to create a joint representation. A well-designed fusion module can capture interactions and relationships between modalities, prevent information loss, avoid semantic mismatch, mitigate biases, and enables comprehensive understanding. For example, in Visual Question Answering (VQA), the fusion module enables the model to leverage both textual and visual information to understand the question and the corresponding image, leading to accurate answers. To improve the ability of answer generation, prompts can be manually designed for different tasks and included as part of the input to the fusion module. These prompts serve as additional information or cues that guide the model's understanding of the question and the image.
As for generation-based VLMs, there are two main types of fusion module approaches based on the integration of visual and textual modalities: \textit{encoder-decoder as a multi-modal fusion module} and \textit{decoder-only as a multi-modal fusion module}.


In the encoder-decoder as a multi-modal fusion module approach, models like VL-T5~\cite{cho2021unifying}, SimVLM~\cite{wang2021simvlm}, OFA~\cite{wang2022ofa}, and PaLI~\cite{chen2022pali} focus on creating a joint representation that combines both modalities at an early stage. The overall formulation can be represented as:

\begin{equation}
\begin{aligned}
y = \mathcal{G}(\mathcal{E}(x_{input}))
\end{aligned}
\end{equation}

\noindent
where the $x_{input}$ represents the given input and $y$ denotes the corresponding ground-truth, respectively. The fusion encoder function $\mathcal{E}$ integrates the visual and textual information to create a joint representation that captures their interactions and dependencies. This fused representation is then fed into the generating module $\mathcal{G}$, which performs further processing and generates the desired outputs for the downstream tasks.

In the decoder-only as a multi-modal fusion module approach, models like Frozen~\cite{tsimpoukelli2021multimodal}, Flamingo~\cite{alayrac2022flamingo}, and MAGMA~\cite{eichenberg2021magma} directly combines the visual and textual information in the decoding stage, without explicitly creating a joint representation at an earlier stage. This approach allows the model to effectively incorporate both modalities during the generation process and produce contextually relevant outputs. The formulation can be represented as:

\begin{equation}
y = \mathcal{G}(x_{input})
\end{equation}


A special case is PICa~\cite{yang2022empirical}, which represents images as textual descriptions and utilizes GPT-3 as the fusion module. This approach treats images as text and leverages a pre-trained language model like GPT-3 to generate outputs based on the pure text input.


In addition, BLIP-2~\cite{li2023blip2} examines the fusion of two distinct modules for integration: the decoder-based OPT~\cite{zhang2022opt} and the encoder-decoder based FlanT5~\cite{chung2022scaling}. The study further offers an analysis of the respective strengths and benefits offered by these fusion modules.


\subsection{Multimodal-Text Prompting Methods}\label{sec:m2t}

Fig.~\ref{fig:chapt3_prompting_method} illustrates the classification of prompting methods. Prompting methods fall into two categories: hard prompts, which are labor-intensive, manually crafted text prompts with discrete tokens, and soft prompts, which are optimizable, learnable tensors concatenated with input embeddings, but lack human readability due to their non-alignment with real word embeddings.

\subsubsection{Hard prompt}
Hard prompts involve manually crafted, interpretable text tokens, \eg adding ``\textit{A photo of }" before the input for captioning tasks. Hard prompts can be further divided into four subcategories: \textit{task instruction}, \textit{in-context learning}, \textit{retrieval-based prompting}, and \textit{chain-of-thought prompting}. It is important to note that retrieval-based prompting is often used to select samples for in-context learning. 

% Figure environment removed

\noindent
\textbf{Task Instruction Prompting.} This method involves the use of carefully designed prompts that provide explicit task-related instructions to guide the model's behavior~\cite{radford2019language,efrat2020turking}. The formulation for this method can be represented as $x_{\text{input}} = \mathcal{H}(x, t)$. Here, $\mathcal{H}$ serves as the task instruction function, taking the image $x$ and text $t$ as inputs and producing the modified input representation $x_{\text{input}}$. 

\noindent
\textbf{In-context Learning.} In-context Learning~\cite{dong2022survey, brown2020language} is a method where the model is exposed to a sequence of related examples or prompts, enabling it to learn and generalize from the provided context. The in-context learning method can be represented using the equation $x_{\text{input}} = \mathcal{H}(\mathcal{C}, x, t)$.  Here, $\mathcal{H}$ denotes the task instruction function which integrates the given context $\mathcal{C}$ with the image $x$ and text $t$ inputs. The resulting modified input representation $x_{\text{input}}$ captures the model's understanding of the context and is used to generate coherent and contextually relevant responses. By exposing the model to a sequence of related examples or prompts, the in-context learning method promotes improved performance in understanding and generating responses~\cite{wei2022chain}.

\noindent
\textbf{Retrieval-based Prompting.} Retrieval-based Prompting~\cite{yang2022empirical, rubin-etal-2022-learning, li-etal-2023-unified, ye2023compositional} is a method that involves selecting prompts or context using retrieval techniques. In this approach, the model retrieves relevant prompts or context from a prompt pool or external knowledge base to guide its generation or decision-making process. The retrieval-based prompting method can be denoted by the formulation: $\mathcal{C} = \mathcal{R}(x,t)$. In this equation, $\mathcal{R}$ signifies the retrieval method that garners pertinent prompts or context based on the image $x$ and text $t$ inputs. The retrieved context $\mathcal{C}$ is then used to guide the model's generation or decision-making process. It is worth noting that the retrieval method $\mathcal{R}$ can vary depending on the specific approach and the available prompt pool or knowledge base. This method allows the model to benefit from existing information and improve its performance by leveraging relevant prompts or context during the generation process~\cite{rubin-etal-2022-learning, li-etal-2023-unified, ye2023compositional}.

\noindent
\textbf{Chain-of-Thought Prompting.} Chain-of-Thought Prompting~\cite{wei2022chain, qiao2022reasoning, zhang2022automatic} is a method where the model is prompted with a series of instructions or questions that progressively build upon each other. Each prompt in the chain adds context or narrows down the focus, enabling the model to generate more coherent and contextually appropriate responses. This method helps the model maintain a logical ``chain" throughout the conversation. The formulation for the chain-of-thought prompting method does not involve a specific equation but rather the iterative process of applying prompts~\cite{wei2022chain}. At each step $l$ in the chain, the model's response from the previous prompt is used as input for the next prompt. This can be represented as $\mathcal{T}^{l+1} = \mathcal{T}^l(x, t)$. Here $\mathcal{T}$ represents the prompt function that takes the image $x$ and text $t$ inputs and generates a response. The output of the $l$-th prompt, denoted as $\mathcal{T}^l(x, t)$, serves as the input for the $(l+1)$-th prompt $\mathcal{T}^{l+1}$. By progressively building upon the previous prompts, the iterative nature of the chain-of-thought prompting method helps the model maintain coherence and generate responses that align with the evolving context of the conversation~\cite{wei2022chain}. 


\subsubsection{Soft prompt}
Unlike hard prompts, soft prompts are characterized as continuous vectors that can be fine-tuned using gradient-based methods~\cite{lester2021power, qin-eisner-2021-learning}. For example, this process might involve concatenating a learnable vector with the input embeddings and subsequently optimizing these to align with a particular dataset. Soft prompts can be classified according to whether new tokens are internally incorporated within the model's architecture or simply attached to the input. This distinction generally relates to two specific strategies: \textit{prompt tuning} and \textit{prefix token tuning}. However, this survey focuses exclusively on prompt methods that do not involve modifying the underlying model itself, and thus techniques like P-tuning~\cite{liu2021gpt} and LoRa~\cite{hu2021lora}, which alter the fundamental structure of the model, are not within the primary scope of this study.

\noindent
\textbf{Prompt Tuning.} Prompt tuning~\cite{lester2021power} creates continuous vector representations as input hints. During the training process, the model learns to refine the prompts, aiming to improve its performance on specific tasks. This method enables the model to dynamically generate effective prompts based on its understanding of the task. The objective of prompt tuning, with the prompting parameter $x_p$, can be demonstrated as follows:
\begin{equation}  
    \operatorname*{argmin}_{x_p} \mathcal{L}(\mathcal{F}(y_i,x_p)|y_{\textless i}, x_{input})
\end{equation}
where $\mathcal{F}(y_i,x_p)$ represents the model's output given the prompting parameter $x_p$. Here $y_{\textless i}$ denotes the previously generated outputs, and $x_{\text{input}}$ refers to the modified input based on the prompt. The objective of prompt tuning is to minimize the loss $\mathcal{L}$ between the model's output and the desired output, given the previously generated outputs and the modified input. By continuously refining the prompts through prompt tuning, the model adapts its behavior and improves its performance on specific tasks. The dynamic generation of effective prompts based on the model's understanding enhances its capability to generate accurate and contextually relevant responses. 

\noindent
\textbf{Prefix Token Tuning.} Similar to prompt tuning, prefix token tuning~\cite{li2021prefix} involves adding task-specific vectors to the input. However, in this case, these vectors are inserted in all model layers and can be trained and updated independently while keeping the rest of the pre-trained model's parameters frozen.

It's worth noting that these prompting methods are not mutually exclusive. They can be combined and used together to achieve desired results in various settings and tasks. The choice of prompting method depends on the specific task, dataset availability, and the desired level of control and customization required for the model's behavior.

\subsection{Advances in Prompting Techniques for VLM}
This section will overview the use of prompting techniques in various VLMs to boost performance. For a clear and structured presentation, models will be divided into two main types based on their fusion modules: 1) models utilizing an encoder-decoder as the fusion module, and 2) models employing a decoder-only as the fusion module.

\noindent
\textbf{Prompting Models with Encoder-decoder as the Fusion Module.} Early studies in VLMs often involved designing task-specific architectures on top of transformer encoders. However, recent advancements have introduced a unified vision-language framework that incorporates an encoder as the fusion module. Notable examples of such models include VL-T5~\cite{cho2021unifying}, SimVLM~\cite{wang2021simvlm}, and OFA~\cite{wang2022ofa}. They employ two main prompting methods: hand-crafted instructions and prompt tuning.


Both VL-T5~\cite{cho2021unifying} and OFA~\cite{wang2022ofa} utilize text prefixes as prompts. For example, ``\textit{vqa:}" is used for vision question answering, and ``\textit{caption:}" is employed for image captioning tasks. SimVLM~\cite{wang2021simvlm} introduces the prefix ``\textit{a photo of:}" to enhance the quality of decoded captions. In addition, VL-T5~\cite{cho2021unifying} introduces shared visual sentinel tokens (\texttt{<$vis\_i$>}) to specify corresponding image regions of Region of Interest (RoI) features. Text sentinel tokens (\texttt{<$text\_i$>}) are used to replace contiguous text. Similarly, OFA~\cite{wang2022ofa} generates location tokens that specify the position of the region (\texttt{<$x_1$,$y_1$,$x_2$,$y_2$>}). These special tokens facilitate the structured incorporation of visual and textual information.


Building upon these special tokens, VL-T5~\cite{cho2021unifying} utilizes the prompt ``\textit{caption region: \texttt{<$vis\_i$>}}" for the grounded captioning task, indicating that the model should generate a caption based on the specified visual region. OFA~\cite{wang2022ofa} prompts the proposed grounded question answering task using the template ``\textit{Q: what color is the car in the region? region: \texttt{<$x_1$,$y_1$,$x_2$,$y_2$>} A:}", providing instructions for the model to answer the question by referring to the specified visual region. Prompt tuning on OFA is explored by~\cite{yang2022prompt}, who introduce tunable prompt embeddings at each layer. Experimental results demonstrate that this lightweight prompt-tuning approach is not only efficient but also resilient against adversarial attacks.

\noindent
\textbf{Prompting Models with Decoder-based Fusion Module.} Another line of research focuses on utilizing the decoder as a fusion module in VLMs. Frozen~\cite{noever2023multimodal} and BLIP-2~\cite{li2023blip} exemplify models that employ image conditional prefix tuning. Frozen~\cite{tsimpoukelli2021multimodal} introduces the concept of preserving the language capabilities of a LLM while incorporating visual information as a prefix. It achieves this by freezing the model and training a separate vision encoder to represent images. In Frozen, visual information is represented as a sequence of two embeddings, serving as a visual prefix. The authors also propose task induction techniques, such as instructing the model to ``\textit{Answer with dax or blicket,}" and evaluate the model's performance with various forms and amounts of in-context learning for downstream tasks. To effectively facilitate cross-modal alignment, BLIP-2~\cite{li2023blip} does not fine-tune the vision encoder. Instead, it introduces a Querying Transformer (Q-Former) to extract visual features from the frozen image encoder, using the extracted query embeddings as soft visual prompts. MAGMA~\cite{eichenberg2021magma} follows a similar approach to Frozen, incorporating a new image prefix encoder while keeping the language model frozen. Task instructions, such as ``\textit{A picture of }" are used for image captioning. Flamingo~\cite{alayrac2022flamingo} explores the capabilities of few-shot learning and employs various prompt techniques. The authors introduce special tokens, \texttt{<BOS>} (beginning of sequence) and \texttt{<EOC>} (end of chunk), to differentiate sample pairs. In the zero-shot scenario, text prompts that do not contain corresponding vision information are used. In the few-shot setting, different formatting is employed for various tasks (\eg ``\textit{Question: \{question\} Answer: \{answer\}}" for visual question-answering tasks), and the retrieval-based in-context example selection (RICES)~\cite{yang2022empirical} approach is utilized to select suitable sample pairs as prompts. Prompt ensembling techniques are also employed to calculate the final scores. For specific tasks such as HatefulMemes, prompts are designed to incorporate provided OCR information. Additionally, hand-crafted dialogue prompts are specifically designed for presented dialogues.
\cite{chen2022pali} extends the multilingual capabilities of LLMs to VLMs without freezing any parameters. They achieved this by explicitly specifying the intended language in the prompt instruction. For example, a prompt may be formulated as ``\textit{Generate the alt\_text in \texttt{<lang>}}", where \texttt{<lang>} represents the language code associated with the desired text string. Furthermore, Microsoft proposes a series of Multimodal Large Language Models (MLLM), namely Kosmos-1~\cite{huang2023language} and Kosmos-2~\cite{peng2023kosmos}. These models possess the ability to perceive diverse modalities and evaluate a wide range of tasks, including zero-shot, few-shot, and multimodal chain-of-thought prompting scenarios. Textual instructions are used to enable the model to better understand downstream tasks. For example, in Kosmos-1~\cite{huang2023language} phrases like ``\textit{Here are three/four/eight images:}" and ``\textit{The following image is:}" are employed for the Raven IQ test. In chain-of-thought prompting, Kosmos-1 first uses a prompt (\eg ``\textit{Introduce this picture in detail:}") to guide the model to generate a rationale. Then, a task-aware prompt incorporating the generated rationale is utilized to produce the final results. Based on Kosmos-1~\cite{huang2023language}, Kosmos-2~\cite{peng2023kosmos} incorporating grounding and referring capabilities by using text span with bounding box as prompt, \ie, ``\textit{\texttt{<p>} text span \texttt{</p><box><loc1><loc2></box>}}, where \texttt{<loc1>} and \texttt{<loc2>} are location tokens  \texttt{<p>}, \texttt{</p>}, \texttt{<box>} and \texttt{</box>} are special boundary and text span tokens, respectively. 



PICa~\cite{yang2022empirical} takes a different approach by not learning visual embeddings. Instead, it converts images into textual descriptions and queries GPT-3 directly to predict the answer. Leveraging the few-shot learning ability of GPT-3, PICa adapts to the visual question-answering (VQA) task with only a few in-context examples during inference time. GPT-4~\cite{openaigpt4}, the latest version of ChatGPT, has been introduced as an advanced VLM. In addition to employing task-specific hard prompts, GPT-4 also incorporates the in-context learning approach to tackle complex tasks such as AP Art History~\cite{Nici_2020}.

\subsection{Understanding Prompting}
To deeply understand the factors impacting prompting in multimodal-to-text generation models, the following aspects will be introduced:

\noindent
\textbf{Dataset-specific Prefixes.} The choice of text prompts can have a significant impact on the performance of models. VL-T5~\cite{cho2021unifying} experimented with a single prefix ``\textit{vqa}" for both Visual Question Answering (VQA) and GQA~\cite{hudson2019gqa} tasks. The results demonstrated that a single model can effectively handle multiple VQA tasks without the need for dataset-specific prefixes.

\noindent
\textbf{Freezing the Language Model.} Many explorations of prompting in multimodal-to-text generation models rely on the powerful generative capabilities of language models. To preserve the extensive capabilities of LLMs, approaches like Frozen~\cite{tsimpoukelli2021multimodal}, MAGMA~\cite{eichenberg2021magma}, Flamingo~\cite{alayrac2022flamingo}, and BLiP2~\cite{li2023blip2} freeze the language model during training. This prevents knowledge loss and enables the retention of prompt capabilities. On the other hand, approaches like OFA~\cite{zang2022unified} and KOSMOS-1~\cite{huang2023language} directly adopt the encoder-decoder structure without additional model components to pursue unified models. However, fine-tuning the language model alone can lead to a decrease in language ability. To address this, both OFA~\cite{zang2022unified} and KOSMOS-1~\cite{huang2023language} add language-only tasks during training to prevent the loss of language ability.

\noindent
\textbf{In-context Learning.} Recent studies have demonstrated that the in-context learning capabilities of language models can be successfully transferred to vision-language-generating models. Frozen~\cite{tsimpoukelli2021multimodal} exhibits the capability of fast concept binding, enabling the model to associate a new word with a visual category using only a few examples and immediately utilize that word appropriately. While the model performs well in the two-way setting (two new words), this ability fails to transfer to the five-way setting (five new words). Experimental results also indicate that increasing the number of in-context learning samples enhances model performance, but there is a saturation point, and additional repeated content can even lead to a decline in performance. Similar conclusions have been drawn in the case of Flamingo~\cite{alayrac2022flamingo}. Both Flamingo~\cite{alayrac2022flamingo} and Kosmos-1~\cite{huang2023language} demonstrate that employing individual text prompts instead of image-text pairs can improve model performance. However, it is important to note that using individual text prompts can introduce bias to the model~\cite{alayrac2022flamingo}.

\noindent
\textbf{Prompt Tuning.}~\cite{yang2022prompt} conducted a study on Prompt tuning in generative multimodal models. Their findings indicate that prompt tuning consistently exhibits greater robustness than finetuning across various tasks. The study also highlights the impact of different setups on prompting performance, revealing that longer prompts with more parameters can facilitate improvements. However, there is a diminishing marginal utility, and excessively long prompts may even have a detrimental effect on performance. Furthermore, the results suggest that inserting prompts at the bottom layers might lead to better performance.

\subsection{Application of Prompting}
Prompting has been widely adopted in many vision-language tasks evolving text generation, demonstrated promising results, and inspired a new learning paradigm, \ie, in-context learning. 

\noindent
\textbf{Visual Question Answering.} The goal of visual question answering (VQA) is to train models to understand the information in an image and answer questions about it in natural language. In-context prompts show surprising results in few-shot~\cite{alayrac2022flamingo, yang2022empirical, tsimpoukelli2021multimodal, huang2023language} and zero-shot scenarios~\cite{wang2021simvlm, chen2022pali, alayrac2022flamingo, huang2023language}.  Some work also applies prompts to web page question answering~\cite{huang2023language} and grounded question answering~\cite{wang2022ofa}. Web page question answering aims to find answers to questions from web pages which requires comprehension of both the semantics and structures of texts. Huang \etal~\cite{huang2023language} uses the template prompt \textit{``Given the context below from the web page, extract the answer from the given text like this: Question: Who is the publisher of this book? Answer: Penguin Books Ltd. Context: \{WebText\} Q: \{question\} A: \{answer\}"} where \textit{\{WebText\}} stands for the text extracted from web pages. Grounded question answering is firstly designed to reflect the strong transferability of the One For All (OFA) model~\cite{wang2022ofa}. In this task, the model should answer a question about a certain region, and special region tokens for hard prompts are designed. 

\noindent
\textbf{Visual Commonsense Reasoning.} This task requires an understanding of the properties of everyday objects in the real world, such as object size reasoning and object color reasoning\cite{huang2023language}. The model is required to predict the size or color relation between  The Kosmos model~\cite{huang2023language} uses example prompts like \textit{Is \{Item1\} larger than \{Item2\}? \{Answer\}} and \textit{The color of \{Object\} is? \{Answer\}} in the zero-shot scenarios and achieves promising results. 


\noindent
\textbf{Zero-shot Image Classification.}
Prompting combined with large-pre-trained multimodal models has shown great transferability on out-of-domain test data such as zero-shot image classification. Kosmos~\cite{huang2023language} concatenates the input image with a prompt like \textit{The photo of the} and lets the model complete the prompt sentence with the predicted class. Besides, to incorporate additional rules in the classification, Kosmos also sends class descriptions along with prompts to prompt the model for a specific category. 

\noindent
\textbf{Image Captioning.}
Generating descriptions given an image is a typical multimodal-to-text generation task that requires the comprehension of both vision and language information. Prompts are used mostly in few-shot and zero-shot scenarios and demonstrate powerful capacity. Flamingo~\cite{alayrac2022flamingo} and PaLI~\cite{chen2022pali} adopt prompts to generate image captions in few-shot settings. For example, PaLI~\cite{chen2022pali} uses the prompt \textit{Generate the alt\_text in EN} to generate image captions. Prompts are also studied in zero-shot settings, such as BLIP-2\cite{li2023blip2}, MAGMA~\cite{eichenberg2021magma}, SimVLM~\cite{wang2021simvlm}, and OFA~\cite{wang2022ofa}. 


\noindent
\textbf{Chatbot.} The advent of chatbots such as ChatGPT~\cite{chatgpt} is one of the most remarkable breakthroughs in AI research. Following work such as Visual ChatGPT~\cite{wu2023visual} and GPT4~\cite{openaigpt4} extend chatbots to multimodal applications which support both images and text prompts. Visual ChatGPT~\cite{wu2023visual} is built based on ChatGPT and visual foundation models. It uses a Prompt Manager which specifies input-output formats, converts visual information to language format, and handles histories of different visual foundation models. GPT4~\cite{openaigpt4} is able to accept prompts consisting of both images and texts, which lets users specify any vision and language task by generating text outputs given arbitrarily interlaced text and image prompts. Besides, some work migrates GPT to a specific domain such as BiomedGPT on biomedical research~\cite{zhang2023biomedgpt}.


\subsection{Responsible AI Considerations of Prompting}
\label{sec:m2t-ethical}
Language-based VLMs inherit the risks of the underlying LLMs and vision models, such as gender and racial biases when prompted with images~\cite{weidinger2021ethical}. Several surveys on the ethics of LLMs are available~\cite{weidinger2021ethical, guo2022threats}. Some work studies the robustness of VLMs against both natural distribution shifts~\cite{qiu2022benchmarking} and adversarial robustness~\cite{zhao2023evaluating}. A recent study~\cite{chen2023benchmarking} investigates the robustness of prompt tuning on VLMs against natural distribution shifts. Moreover,~\cite{gu2023towards} proposes robust prompt tuning on VLMs by integrating multiple-scale image features into the prompt. 



