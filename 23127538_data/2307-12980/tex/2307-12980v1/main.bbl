\begin{thebibliography}{100}

\bibitem{alayrac2022flamingo}
J.-B. Alayrac et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:23716--23736, 2022.

\bibitem{radford2021learning}
A.~Radford et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pp.
  8748--8763. PMLR, 2021.

\bibitem{rombach2022high}
R.~Rombach et~al.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 10684--10695, 2022.

\bibitem{wei2022chain}
J.~Wei et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24824--24837, 2022.

\bibitem{zhang2022automatic}
Z.~Zhang et~al.
\newblock Automatic chain of thought prompting in large language models.
\newblock In {\em The Eleventh International Conference on Learning
  Representations (ICLR 2023)}, 2023.

\bibitem{lester2021power}
B.~Lester et~al.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock {\em arXiv preprint arXiv:2104.08691}, 2021.

\bibitem{dong2022survey}
Q.~Dong et~al.
\newblock A survey for in-context learning.
\newblock {\em arXiv preprint arXiv:2301.00234}, 2022.

\bibitem{liu2023pre}
P.~Liu et~al.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em ACM Computing Surveys}, 55(9):1--35, 2023.

\bibitem{qiao2022reasoning}
S.~Qiao et~al.
\newblock Reasoning with language model prompting: A survey.
\newblock {\em arXiv preprint arXiv:2212.09597}, 2022.

\bibitem{bahng2022exploring}
H.~Bahng et~al.
\newblock Exploring visual prompts for adapting large-scale models.
\newblock {\em arXiv preprint arXiv:2203.17274}, 1(3):4, 2022.

\bibitem{wang2023seggpt}
X.~Wang et~al.
\newblock {SegGPT}: Segmenting everything in context.
\newblock {\em arXiv preprint arXiv:2304.03284}, 2023.

\bibitem{wu2023visual}
C.~Wu et~al.
\newblock Visual chatgpt: Talking, drawing and editing with visual foundation
  models.
\newblock {\em arXiv preprint arXiv:2303.04671}, 2023.

\bibitem{liu2021gpt}
X.~Liu et~al.
\newblock Gpt understands, too.
\newblock {\em arXiv preprint arXiv:2103.10385}, 2021.

\bibitem{hu2021lora}
E.~J. Hu et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{huang2023language}
S.~Huang et~al.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock {\em arXiv preprint arXiv:2302.14045}, 2023.

\bibitem{yang2022empirical}
Z.~Yang et~al.
\newblock An empirical study of gpt-3 for few-shot knowledge-based vqa.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem{tsimpoukelli2021multimodal}
M.~Tsimpoukelli et~al.
\newblock Multimodal few-shot learning with frozen language models.
\newblock {\em Advances in Neural Information Processing Systems}, 34:200--212,
  2021.

\bibitem{openaigpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{vlmsurvey}
S.~Long et~al.
\newblock Vision-and-language pretrained models: A survey.
\newblock In L.~D. Raedt, editor, {\em Proceedings of the Thirty-First
  International Joint Conference on Artificial Intelligence, {IJCAI-22}}, pp.
  5530--5537. International Joint Conferences on Artificial Intelligence
  Organization, 7 2022.
\newblock Survey Track.

\bibitem{devlin2018bert}
J.~Devlin et~al.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{pritchett2015learning}
L.~Pritchett and J.~Sandefur.
\newblock Learning from experiments when context matters.
\newblock {\em American Economic Review}, 105(5):471--475, 2015.

\bibitem{raffel2020exploring}
C.~Raffel et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock In {\em JMLR}, 2020.

\bibitem{brown2020language}
T.~Brown et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{singh2022flava}
A.~Singh et~al.
\newblock Flava: A foundational language and vision alignment model.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 15638--15650, 2022.

\bibitem{lu2019vilbert}
J.~Lu et~al.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{cho2021unifying}
J.~Cho et~al.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In {\em International Conference on Machine Learning}, pp.
  1931--1942. PMLR, 2021.

\bibitem{ren2015faster}
S.~Ren et~al.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem{wang2022ofa}
P.~Wang et~al.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In {\em International Conference on Machine Learning}, pp.
  23318--23340. PMLR, 2022.

\bibitem{wang2021simvlm}
Z.~Wang et~al.
\newblock Sim{VLM}: Simple visual language model pretraining with weak
  supervision.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{chen2022pali}
X.~Chen et~al.
\newblock Pa{LI}: A jointly-scaled multilingual language-image model.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{eichenberg2021magma}
C.~Eichenberg et~al.
\newblock {MAGMA} {--} multimodal augmentation of generative models through
  adapter-based finetuning.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pp. 2416--2428, Abu Dhabi, United Arab Emirates, December 2022.
  Association for Computational Linguistics.

\bibitem{li2023blip2}
J.~Li et~al.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{dosovitskiy2020image}
A.~Dosovitskiy et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{zhang2022opt}
S.~Zhang et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{chung2022scaling}
H.~W. Chung et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{radford2019language}
A.~Radford et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{efrat2020turking}
A.~Efrat and O.~Levy.
\newblock The turking test: Can language models understand instructions?
\newblock {\em arXiv preprint arXiv:2010.11982}, 2020.

\bibitem{rubin-etal-2022-learning}
O.~Rubin et~al.
\newblock Learning to retrieve prompts for in-context learning.
\newblock In {\em Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp. 2655--2671, Seattle, United States, July 2022. Association
  for Computational Linguistics.

\bibitem{li-etal-2023-unified}
X.~Li et~al.
\newblock Unified demonstration retriever for in-context learning.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pp. 4644--4668, Toronto,
  Canada, July 2023. Association for Computational Linguistics.

\bibitem{ye2023compositional}
J.~Ye et~al.
\newblock Compositional exemplars for in-context learning.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2023.

\bibitem{qin-eisner-2021-learning}
G.~Qin and J.~Eisner.
\newblock Learning how to ask: Querying {LM}s with mixtures of soft prompts.
\newblock In {\em Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp. 5203--5212, Online, June 2021. Association for
  Computational Linguistics.

\bibitem{li2021prefix}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{yang2022prompt}
H.~Yang et~al.
\newblock Prompt tuning for generative multimodal pretrained models.
\newblock {\em arXiv preprint arXiv:2208.02532}, 2022.

\bibitem{noever2023multimodal}
D.~Noever and S.~E.~M. Noever.
\newblock The multimodal and modular ai chef: Complex recipe generation from
  imagery.
\newblock {\em arXiv preprint arXiv:2304.02016}, 2023.

\bibitem{li2023blip}
J.~Li et~al.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{peng2023kosmos}
Z.~Peng et~al.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock {\em arXiv preprint arXiv:2306.14824}, 2023.

\bibitem{Nici_2020}
J.~B. Nici.
\newblock {\em AP Art History: 5 Practice Tests + Comprehensive Review + Online
  Practice.}
\newblock Barron’s Educational Series, 2020.

\bibitem{hudson2019gqa}
D.~A. Hudson and C.~D. Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional
  question answering.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pp. 6700--6709, 2019.

\bibitem{zang2022unified}
Y.~Zang et~al.
\newblock Unified {{Vision}} and {{Language Prompt Learning}}.
\newblock {\em arXiv:2210.07225}, 2022.

\bibitem{chatgpt}
Chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}.
\newblock Accessed: 2023-07-22.

\bibitem{zhang2023biomedgpt}
K.~Zhang et~al.
\newblock Biomedgpt: A unified and generalist biomedical generative pre-trained
  transformer for vision, language, and multimodal tasks.
\newblock {\em arXiv preprint arXiv:2305.17100}, 2023.

\bibitem{weidinger2021ethical}
L.~Weidinger et~al.
\newblock Ethical and social risks of harm from language models.
\newblock {\em arXiv preprint arXiv:2112.04359}, 2021.

\bibitem{guo2022threats}
S.~Guo et~al.
\newblock Threats to pre-trained language models: Survey and taxonomy.
\newblock {\em arXiv preprint arXiv:2202.06862}, 2022.

\bibitem{qiu2022benchmarking}
J.~Qiu et~al.
\newblock Benchmarking robustness under distribution shift of multimodal
  image-text models.
\newblock In {\em NeurIPS 2022 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2022.

\bibitem{zhao2023evaluating}
Y.~Zhao et~al.
\newblock On evaluating adversarial robustness of large vision-language models.
\newblock {\em arXiv preprint arXiv:2305.16934}, 2023.

\bibitem{chen2023benchmarking}
S.~Chen et~al.
\newblock Benchmarking robustness of adaptation methods on pre-trained
  vision-language models.
\newblock {\em arXiv preprint arXiv:2306.02080}, 2023.

\bibitem{gu2023towards}
J.~Gu et~al.
\newblock Towards robust prompts on vision-language models.
\newblock {\em arXiv preprint arXiv:2304.08479}, 2023.

\bibitem{li2021alignb}
J.~Li et~al.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock {\em Advances in neural information processing systems},
  34:9694--9705, 2021.

\bibitem{geng2022meclip}
G.~Zhang et~al.
\newblock Multi-event video-text retrieval.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV) (to appear)}, 2023.

\bibitem{huang2022unsupervised}
T.~Huang et~al.
\newblock Unsupervised prompt learning for vision-language models.
\newblock {\em arXiv preprint arXiv:2204.03649}, 2022.

\bibitem{gao2021making}
T.~Gao et~al.
\newblock Making {{Pre-trained Language Models Better Few-shot Learners}}.
\newblock In {\em Proceedings of the 59th {{Annual Meeting}} of the
  {{Association}} for {{Computational Linguistics}} and the 11th
  {{International Joint Conference}} on {{Natural Language Processing}}
  ({{Volume}} 1: {{Long Papers}})}, pp. 3816--3830, {Online}, August 2021.
  {Association for Computational Linguistics}.

\bibitem{shu2022test}
M.~Shu et~al.
\newblock Test-time prompt tuning for zero-shot generalization in
  vision-language models.
\newblock {\em Advances in Neural Information Processing Systems},
  35:14274--14289, 2022.

\bibitem{zhou2022learning}
K.~Zhou et~al.
\newblock Learning to prompt for vision-language models.
\newblock {\em International Journal of Computer Vision}, 130(9):2337--2348,
  2022.

\bibitem{ju2022prompting}
C.~Ju et~al.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV}, pp. 105--124.
  Springer, 2022.

\bibitem{shen2022multitask}
S.~Shen et~al.
\newblock Multitask {{Vision-Language Prompt Tuning}}.
\newblock {\em arXiv:2211.11720}, 2022.

\bibitem{zhou2022conditional}
K.~Zhou et~al.
\newblock Conditional prompt learning for vision-language models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 16816--16825, 2022.

\bibitem{jia2022visual}
M.~Jia et~al.
\newblock Visual prompt tuning.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII}, pp. 709--727.
  Springer, 2022.

\bibitem{wu2022unleashing}
J.~Wu et~al.
\newblock Unleashing the power of visual prompting at the pixel level.
\newblock {\em arXiv preprint arXiv:2212.10556}, 2022.

\bibitem{huang2023diversity}
Q.~Huang et~al.
\newblock Diversity-aware meta visual prompting.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 10878--10887, 2023.

\bibitem{yao2022cpt}
Y.~Yao et~al.
\newblock {{CPT}}: {{Colorful Prompt Tuning}} for {{Pre-trained Vision-Language
  Models}}.
\newblock {\em arXiv:2109.11797}, 2022.

\bibitem{shtedritski2023what}
A.~Shtedritski et~al.
\newblock What does clip know about a red circle? visual prompt engineering for
  vlms.
\newblock {\em arXiv preprint arXiv:2304.06712}, 2023.

\bibitem{bar2022visual}
A.~Bar et~al.
\newblock Visual prompting via image inpainting.
\newblock {\em Advances in Neural Information Processing Systems},
  35:25005--25017, 2022.

\bibitem{khattak2023maple}
M.~U. Khattak et~al.
\newblock Maple: Multi-modal prompt learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 19113--19122, 2023.

\bibitem{dong2023lpt}
B.~Dong et~al.
\newblock {{LPT}}: {{Long-tailed Prompt Tuning}} for {{Image Classification}}.
\newblock {\em arXiv preprint arXiv:2210.01033}, 2023.

\bibitem{guo2023texts}
Z.~Guo et~al.
\newblock Texts as images in prompt tuning for multi-label image recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 2808--2817, 2023.

\bibitem{sun2022dualcoop}
X.~Sun et~al.
\newblock Dualcoop: Fast adaptation to multi-label recognition with limited
  annotations.
\newblock {\em Advances in Neural Information Processing Systems},
  35:30569--30582, 2022.

\bibitem{wen2022visual}
J.~Wen et~al.
\newblock Visual {{Prompt Tuning}} for {{Few-Shot Text Classification}}.
\newblock In {\em Proceedings of the 29th {{International Conference}} on
  {{Computational Linguistics}}}, pp. 5560--5570, {Gyeongju, Republic of
  Korea}, October 2022. {International Committee on Computational Linguistics}.

\bibitem{gu2022openvocabulary}
X.~Gu et~al.
\newblock Open-vocabulary {{Object Detection}} via {{Vision}} and {{Language
  Knowledge Distillation}}.
\newblock {\em arXiv preprint arXiv:2104.13921}, 2022.

\bibitem{du2022learning}
Y.~Du et~al.
\newblock Learning to prompt for open-vocabulary object detection with
  vision-language model.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 14084--14093, 2022.

\bibitem{feng2022promptdet}
C.~Feng et~al.
\newblock Promptdet: Towards open-vocabulary detection using uncurated images.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part IX}, pp. 701--717.
  Springer, 2022.

\bibitem{xiao2022optimizing}
S.~Xiao and W.~Fu.
\newblock Optimizing {{Continuous Prompts}} for {{Visual Relationship
  Detection}} by {{Affix-Tuning}}.
\newblock {\em IEEE Access}, 10:70104--70112, 2022.

\bibitem{he2022openvocabularya}
T.~He et~al.
\newblock Towards open-vocabulary scene graph generation with prompt-based
  finetuning.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII}, pp. 56--73.
  Springer, 2022.

\bibitem{gao2023compositionala}
K.~Gao et~al.
\newblock Compositional {{Prompt Tuning}} with {{Motion Cues}} for
  {{Open-vocabulary Video Relation Detection}}.
\newblock {\em arXiv preprint arXiv:2302.00268}, 2023.

\bibitem{rao2022denseclip}
Y.~Rao et~al.
\newblock Denseclip: Language-guided dense prediction with context-aware
  prompting.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 18082--18091, 2022.

\bibitem{kirillov2023segment}
A.~Kirillov et~al.
\newblock Segment anything.
\newblock {\em arXiv preprint arXiv:2304.02643}, 2023.

\bibitem{ge2022domain}
C.~Ge et~al.
\newblock Domain {{Adaptation}} via {{Prompt Learning}}.
\newblock {\em arXiv:2202.06687}, 2022.

\bibitem{gao2022visual}
Y.~Gao et~al.
\newblock Visual {{Prompt Tuning}} for {{Test-time Domain Adaptation}}.
\newblock {\em arXiv:2210.04831}, 2022.

\bibitem{wang2022learning}
Z.~Wang et~al.
\newblock Learning to prompt for continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 139--149, 2022.

\bibitem{wang2022dualprompt}
Z.~Wang et~al.
\newblock Dualprompt: Complementary prompting for rehearsal-free continual
  learning.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI}, pp. 631--648.
  Springer, 2022.

\bibitem{zheng2022prompt}
Z.~Zheng et~al.
\newblock Prompt {{Vision Transformer}} for {{Domain Generalization}}.
\newblock {\em arXiv preprint arXiv:2208.08914}, 2022.

\bibitem{mao2023understanding}
C.~Mao et~al.
\newblock Understanding {{Zero-Shot Adversarial Robustness}} for {{Large-Scale
  Models}}, April 2023.

\bibitem{chen2023visual}
A.~Chen et~al.
\newblock Visual prompting for adversarial robustness.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp. 1--5. IEEE, 2023.

\bibitem{fang2022data}
A.~Fang et~al.
\newblock Data determines distributional robustness in contrastive language
  image pre-training (clip).
\newblock In {\em International Conference on Machine Learning}, pp.
  6216--6234. PMLR, 2022.

\bibitem{shi2023effective}
Z.~Shi et~al.
\newblock Effective robustness against natural distribution shifts for models
  with different training data.
\newblock {\em arXiv preprint arXiv:2302.01381}, 2023.

\bibitem{xu2022exploring}
L.~Xu et~al.
\newblock Exploring the universal vulnerability of prompt-based learning
  paradigm.
\newblock {\em arXiv preprint arXiv:2204.05239}, 2022.

\bibitem{carlini2022poisoning}
N.~Carlini and A.~Terzis.
\newblock Poisoning and {{Backdooring Contrastive Learning}}, March 2022.

\bibitem{jia2022badencoder}
J.~Jia et~al.
\newblock Badencoder: Backdoor attacks to pre-trained encoders in
  self-supervised learning.
\newblock In {\em 2022 IEEE Symposium on Security and Privacy (SP)}, pp.
  2043--2059. IEEE, 2022.

\bibitem{bansal2023cleanclip}
H.~Bansal et~al.
\newblock Cleanclip: Mitigating data poisoning attacks in multimodal
  contrastive learning.
\newblock {\em arXiv preprint arXiv:2303.03323}, 2023.

\bibitem{agarwal2021evaluating}
S.~Agarwal et~al.
\newblock Evaluating clip: towards characterization of broader capabilities and
  downstream implications.
\newblock {\em arXiv preprint arXiv:2108.02818}, 2021.

\bibitem{chuang2023debiasing}
C.-Y. Chuang et~al.
\newblock Debiasing vision-language models via biased prompts.
\newblock {\em arXiv preprint arXiv:2302.00070}, 2023.

\bibitem{kong2023mitigating}
F.~Kong et~al.
\newblock Mitigating test-time bias for fair image retrieval.
\newblock {\em arXiv preprint arXiv:2305.19329}, 2023.

\bibitem{smith2023balancing}
B.~Smith et~al.
\newblock Balancing the picture: Debiasing vision-language datasets with
  synthetic contrast sets.
\newblock {\em arXiv preprint arXiv:2305.15407}, 2023.

\bibitem{gregor2015draw}
K.~Gregor et~al.
\newblock Draw: A recurrent neural network for image generation.
\newblock In {\em International conference on machine learning}, pp.
  1462--1471. PMLR, 2015.

\bibitem{goodfellow2020generative}
I.~Goodfellow et~al.
\newblock Generative adversarial networks.
\newblock {\em Communications of the ACM}, 63(11):139--144, 2020.

\bibitem{reed2016generative}
S.~Reed et~al.
\newblock Generative adversarial text to image synthesis.
\newblock In {\em International conference on machine learning}, pp.
  1060--1069. PMLR, 2016.

\bibitem{pan2023drag}
X.~Pan et~al.
\newblock Drag your gan: Interactive point-based manipulation on the generative
  image manifold.
\newblock {\em arXiv preprint arXiv:2305.10973}, 2023.

\bibitem{karras2019style}
T.~Karras et~al.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pp. 4401--4410, 2019.

\bibitem{isola2017image}
P.~Isola et~al.
\newblock Image-to-image translation with conditional adversarial networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp. 1125--1134, 2017.

\bibitem{kingma2019introduction}
D.~P. Kingma et~al.
\newblock An introduction to variational autoencoders.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  12(4):307--392, 2019.

\bibitem{pu2016variational}
Y.~Pu et~al.
\newblock Variational autoencoder for deep learning of images, labels and
  captions.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{vahdat2020nvae}
A.~Vahdat and J.~Kautz.
\newblock Nvae: A deep hierarchical variational autoencoder.
\newblock {\em Advances in neural information processing systems},
  33:19667--19679, 2020.

\bibitem{ramesh2021zero}
A.~Ramesh et~al.
\newblock Zero-shot text-to-image generation.
\newblock In {\em International Conference on Machine Learning}, pp.
  8821--8831. PMLR, 2021.

\bibitem{yu2022scaling}
J.~Yu et~al.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2022.

\bibitem{dhariwal2021diffusion}
P.~Dhariwal and A.~Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in Neural Information Processing Systems},
  34:8780--8794, 2021.

\bibitem{sohl2015deep}
J.~Sohl-Dickstein et~al.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International Conference on Machine Learning}, pp.
  2256--2265. PMLR, 2015.

\bibitem{jeulin1997dead}
D.~Jeulin.
\newblock Dead leaves models: from space tesselation to random functions proc.
  of the symposium on the advances in the theory and applications of random
  sets (fontainebleau, 9-11 october 1996) ed d jeulin, 1997.

\bibitem{neal2001annealed}
R.~M. Neal.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11:125--139, 2001.

\bibitem{ho2020denoising}
J.~Ho et~al.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6840--6851, 2020.

\bibitem{witteveen2022investigating}
S.~Witteveen and M.~Andrews.
\newblock Investigating prompt engineering in diffusion models.
\newblock {\em arXiv preprint arXiv:2211.15462}, 2022.

\bibitem{wu2023diffumask}
W.~Wu et~al.
\newblock Diffumask: Synthesizing images with pixel-level annotations for
  semantic segmentation using diffusion models.
\newblock {\em arXiv preprint arXiv:2303.11681}, 2023.

\bibitem{beaumont2022clip}
R.~Beaumont.
\newblock Clip retrieval: Easily compute clip embeddings and build a clip
  retrieval system with them, 2022.

\bibitem{ni2022imaginarynet}
M.~Ni et~al.
\newblock Imaginarynet: Learning object detectors without real images and
  annotations.
\newblock In {\em ICLR}, 2023.

\bibitem{he2023synthetic}
R.~He et~al.
\newblock Is synthetic data from generative models ready for image recognition?
\newblock In {\em ICLR}, 2023.

\bibitem{avrahami2022blended}
O.~Avrahami et~al.
\newblock Blended latent diffusion.
\newblock {\em arXiv preprint arXiv:2206.02779}, 2022.

\bibitem{nichol2021glide}
A.~Nichol et~al.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{gal2022image}
R.~Gal et~al.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{ruiz2023dreambooth}
N.~Ruiz et~al.
\newblock Dreambooth: Fine tuning text-to-image diffusion models for
  subject-driven generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 22500--22510, 2023.

\bibitem{kumari2022multi}
N.~Kumari et~al.
\newblock Multi-concept customization of text-to-image diffusion.
\newblock {\em arXiv preprint arXiv:2212.04488}, 2022.

\bibitem{feng2022training}
W.~Feng et~al.
\newblock Training-free structured diffusion guidance for compositional
  text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2212.05032}, 2022.

\bibitem{epstein2023diffusion}
D.~Epstein et~al.
\newblock Diffusion self-guidance for controllable image generation.
\newblock {\em arXiv preprint arXiv:2306.00986}, 2023.

\bibitem{kawar2023imagic}
B.~Kawar et~al.
\newblock Imagic: Text-based real image editing with diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 6007--6017, 2023.

\bibitem{canny1986computational}
J.~Canny.
\newblock A computational approach to edge detection.
\newblock {\em IEEE Transactions on pattern analysis and machine intelligence},
  pp. 679--698, 1986.

\bibitem{zhang2023adding}
L.~Zhang and M.~Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2302.05543}, 2023.

\bibitem{hertz2022prompt}
A.~Hertz et~al.
\newblock Prompt-to-prompt image editing with cross attention control.
\newblock {\em arXiv preprint arXiv:2208.01626}, 2022.

\bibitem{singer2022make}
U.~Singer et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock {\em arXiv preprint arXiv:2209.14792}, 2022.

\bibitem{ho2022imagen}
J.~Ho et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock {\em arXiv preprint arXiv:2210.02303}, 2022.

\bibitem{ho2022cascaded}
J.~Ho et~al.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock {\em J. Mach. Learn. Res.}, 23(47):1--33, 2022.

\bibitem{qi2023fatezero}
C.~Qi et~al.
\newblock Fatezero: Fusing attentions for zero-shot text-based video editing.
\newblock {\em arXiv preprint arXiv:2303.09535}, 2023.

\bibitem{wu2022tune}
J.~Z. Wu et~al.
\newblock Tune-a-video: One-shot tuning of image diffusion models for
  text-to-video generation.
\newblock {\em arXiv preprint arXiv:2212.11565}, 2022.

\bibitem{ruan2023mm}
L.~Ruan et~al.
\newblock Mm-diffusion: Learning multi-modal diffusion models for joint audio
  and video generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 10219--10228, 2023.

\bibitem{zhu2023moviefactory}
J.~Zhu et~al.
\newblock Moviefactory: Automatic movie creation from text using large
  generative models for language and images.
\newblock {\em arXiv preprint arXiv:2306.07257}, 2023.

\bibitem{muller2023diffrf}
N.~M{\"u}ller et~al.
\newblock Diffrf: Rendering-guided 3d radiance field diffusion.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 4328--4338, 2023.

\bibitem{poole2022dreamfusion}
B.~Poole et~al.
\newblock Dreamfusion: Text-to-3d using 2d diffusion.
\newblock {\em arXiv preprint arXiv:2209.14988}, 2022.

\bibitem{mildenhall2021nerf}
B.~Mildenhall et~al.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock {\em Communications of the ACM}, 65(1):99--106, 2021.

\bibitem{lin2022magic3d}
C.-H. Lin et~al.
\newblock Magic3d: High-resolution text-to-3d content creation.
\newblock {\em arXiv preprint arXiv:2211.10440}, 2022.

\bibitem{xu2022dream3d}
J.~Xu et~al.
\newblock Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and
  text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2212.14704}, 2022.

\bibitem{lee2022understanding}
H.-H. Lee and A.~X. Chang.
\newblock Understanding pure clip guidance for voxel grid nerf models.
\newblock {\em arXiv preprint arXiv:2209.15172}, 2022.

\bibitem{khalid2022clip}
N.~M. Khalid et~al.
\newblock Clip-mesh: Generating textured meshes from text using pretrained
  image-text models.
\newblock {\em arXiv preprint arXiv:2203.13333}, 2022.

\bibitem{zhang2022motiondiffuse}
M.~Zhang et~al.
\newblock Motiondiffuse: Text-driven human motion generation with diffusion
  model.
\newblock {\em arXiv preprint arXiv:2208.15001}, 2022.

\bibitem{kim2022flame}
J.~Kim et~al.
\newblock Flame: Free-form language-based motion synthesis \& editing.
\newblock {\em arXiv preprint arXiv:2209.00349}, 2022.

\bibitem{tevet2022human}
G.~Tevet et~al.
\newblock Human motion diffusion model.
\newblock {\em arXiv preprint arXiv:2209.14916}, 2022.

\bibitem{yu2023scaling}
T.~Yu et~al.
\newblock Scaling robot learning with semantically imagined experience.
\newblock {\em arXiv preprint arXiv:2302.11550}, 2023.

\bibitem{rempeluo2023tracepace}
D.~Rempe et~al.
\newblock Trace and pace: Controllable pedestrian animation via guided
  trajectory diffusion.
\newblock In {\em Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2023.

\bibitem{shabani2023housediffusion}
M.~A. Shabani et~al.
\newblock Housediffusion: Vector floorplan generation via a diffusion model
  with discrete and continuous denoising.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 5466--5475, 2023.

\bibitem{jeong2023zero}
H.~Jeong et~al.
\newblock Zero-shot generation of coherent storybook from plain text story
  using diffusion models.
\newblock {\em arXiv preprint arXiv:2302.03900}, 2023.

\bibitem{lu2023multimodal}
Y.~Lu et~al.
\newblock Multimodal procedural planning via dual text-image prompting.
\newblock {\em arXiv preprint arXiv:2305.01795}, 2023.

\bibitem{jobin2019global}
A.~Jobin et~al.
\newblock The global landscape of ai ethics guidelines.
\newblock {\em Nature Machine Intelligence}, 1(9):389--399, 2019.

\bibitem{chen2023diffusion}
J.~Chen et~al.
\newblock Diffusion models for imperceptible and transferable adversarial
  attack.
\newblock {\em arXiv preprint arXiv:2305.08192}, 2023.

\bibitem{nie2022diffusion}
W.~Nie et~al.
\newblock Diffusion models for adversarial purification.
\newblock {\em arXiv preprint arXiv:2205.07460}, 2022.

\bibitem{zhuang2023pilot}
H.~Zhuang et~al.
\newblock A pilot study of query-free adversarial attack against stable
  diffusion.
\newblock {\em arXiv preprint arXiv:2303.16378}, 2023.

\bibitem{struppek2022rickrolling}
L.~Struppek et~al.
\newblock Rickrolling the artist: Injecting invisible backdoors into
  text-guided image generation models.
\newblock {\em arXiv preprint arXiv:2211.02408}, 2022.

\bibitem{zhai2023text}
S.~Zhai et~al.
\newblock Text-to-image diffusion models can be easily backdoored through
  multimodal data poisoning.
\newblock {\em arXiv preprint arXiv:2305.04175}, 2023.

\bibitem{huang2023zero}
Y.~Huang et~al.
\newblock Zero-day backdoor attack against text-to-image diffusion models via
  personalization.
\newblock {\em arXiv preprint arXiv:2305.10701}, 2023.

\bibitem{friedrich2023fair}
F.~Friedrich et~al.
\newblock Fair diffusion: Instructing text-to-image generation models on
  fairness.
\newblock {\em arXiv preprint arXiv:2302.10893}, 2023.

\bibitem{naik2023social}
R.~Naik and B.~Nushi.
\newblock Social biases through the text-to-image generation lens.
\newblock {\em arXiv preprint arXiv:2304.06034}, 2023.

\bibitem{wang2023T2IAT}
J.~Wang et~al.
\newblock T2iat: Measuring valence and stereotypical biases in text-to-image
  generation.
\newblock {\em arXiv preprint arXiv:2306.00905}, 2023.

\bibitem{luccioni2023stable}
A.~S. Luccioni et~al.
\newblock Stable bias: Analyzing societal representations in diffusion models.
\newblock {\em arXiv preprint arXiv:2303.11408}, 2023.

\bibitem{seth2023dear}
A.~Seth et~al.
\newblock Dear: Debiasing vision-language models with additive residuals.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 6820--6829, 2023.

\bibitem{kim2023explaining}
Y.~Kim et~al.
\newblock Explaining visual biases as words by generating captions.
\newblock {\em arXiv preprint arXiv:2301.11104}, 2023.

\bibitem{wu2022membership}
Y.~Wu et~al.
\newblock Membership inference attacks against text-to-image generation models.
\newblock {\em arXiv preprint arXiv:2210.00968}, 2022.

\bibitem{duan2023diffusion}
J.~Duan et~al.
\newblock Are diffusion models vulnerable to membership inference attacks?
\newblock {\em arXiv preprint arXiv:2302.01316}, 2023.

\bibitem{webster2023reproducible}
R.~Webster.
\newblock A reproducible extraction of training images from diffusion models.
\newblock {\em arXiv preprint arXiv:2305.08694}, 2023.

\bibitem{shen2023prompt}
X.~Shen et~al.
\newblock Prompt stealing attacks against text-to-image generation models.
\newblock {\em arXiv preprint arXiv:2302.09923}, 2023.

\bibitem{khashabi-etal-2020-unifiedqa}
D.~Khashabi et~al.
\newblock {UNIFIEDQA}: Crossing format boundaries with a single {QA} system.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp. 1896--1907, Online, November 2020. Association for
  Computational Linguistics.

\bibitem{jiang2021can}
Z.~Jiang et~al.
\newblock How can we know when language models know? on the calibration of
  language models for question answering.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:962--977, 2021.

\bibitem{lester-etal-2021-power}
B.~Lester et~al.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp. 3045--3059, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{schick-schutze-2021-shot}
T.~Schick and H.~Sch{\"u}tze.
\newblock Few-shot text generation with natural language instructions.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp. 390--402, Online and Punta Cana, Dominican
  Republic, November 2021. Association for Computational Linguistics.

\bibitem{KnowPrompt}
X.~Chen et~al.
\newblock Knowprompt: Knowledge-aware prompt-tuning with synergistic
  optimization for relation extraction.
\newblock In {\em Proceedings of the ACM Web Conference 2022}, WWW '22, pp.
  2778–2788, New York, NY, USA, 2022. Association for Computing Machinery.

\bibitem{cui-etal-2021-template}
L.~Cui et~al.
\newblock Template-based named entity recognition using {BART}.
\newblock In {\em Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pp. 1835--1845, Online, August 2021. Association for
  Computational Linguistics.

\bibitem{ouyang2022training}
L.~Ouyang et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{anil2023palm}
R.~Anil et~al.
\newblock Palm 2 technical report.
\newblock {\em arXiv preprint arXiv:2305.10403}, 2023.

\bibitem{paranjape2021prompting}
B.~Paranjape et~al.
\newblock Prompting contrastive explanations for commonsense reasoning tasks.
\newblock {\em arXiv preprint arXiv:2106.06823}, 2021.

\bibitem{liu2021makes}
J.~Liu et~al.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock {\em arXiv preprint arXiv:2101.06804}, 2021.

\bibitem{fu2022complexity}
Y.~Fu et~al.
\newblock Complexity-based prompting for multi-step reasoning.
\newblock {\em arXiv preprint arXiv:2210.00720}, 2022.

\bibitem{lewis2020retrieval}
P.~Lewis et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9459--9474, 2020.

\bibitem{borgeaud2022improving}
S.~Borgeaud et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In {\em International conference on machine learning}, pp.
  2206--2240. PMLR, 2022.

\bibitem{izacard2022few}
G.~Izacard et~al.
\newblock Few-shot learning with retrieval augmented language models.
\newblock {\em arXiv preprint arXiv:2208.03299}, 2022.

\bibitem{press2022measuring}
O.~Press et~al.
\newblock Measuring and narrowing the compositionality gap in language models.
\newblock {\em arXiv preprint arXiv:2210.03350}, 2022.

\bibitem{kazemi2022lambada}
S.~M. Kazemi et~al.
\newblock Lambada: Backward chaining for automated reasoning in natural
  language.
\newblock {\em arXiv preprint arXiv:2212.13894}, 2022.

\bibitem{khot2022decomposed}
T.~Khot et~al.
\newblock Decomposed prompting: A modular approach for solving complex tasks.
\newblock {\em arXiv preprint arXiv:2210.02406}, 2022.

\bibitem{yang2022prompting}
Y.~Yang et~al.
\newblock A prompting-based approach for adversarial example generation and
  robustness enhancement.
\newblock {\em arXiv preprint arXiv:2203.10714}, 2022.

\bibitem{dong2023promptattack}
X.~Dong et~al.
\newblock Promptattack: Probing dialogue state trackers with adversarial
  prompts.
\newblock {\em arXiv preprint arXiv:2306.04535}, 2023.

\bibitem{delobelle-etal-2022-measuring}
P.~Delobelle et~al.
\newblock Measuring fairness with biased rulers: A comparative study on bias
  metrics for pre-trained language models.
\newblock In {\em Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp. 1693--1706, Seattle, United States, July 2022. Association
  for Computational Linguistics.

\bibitem{Lou2023Is}
R.~Lou et~al.
\newblock Is prompt all you need? no. {A} comprehensive and broader view of
  instruction learning.
\newblock {\em CoRR}, abs/2303.10475, 2023.

\bibitem{Ding2022OpenPrompt}
N.~Ding et~al.
\newblock Openprompt: An open-source framework for prompt-learning.
\newblock In V.~Basile et~al., editors, {\em Proceedings of the 60th Annual
  Meeting of the Association for Computational Linguistics, {ACL} 2022 - System
  Demonstrations, Dublin, Ireland, May 22-27, 2022}, pp. 105--113. Association
  for Computational Linguistics, 2022.

\bibitem{wang2022images}
X.~Wang et~al.
\newblock Images speak in images: A generalist painter for in-context visual
  learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 6830--6839, 2023.

\bibitem{loedeman2023prompt}
J.~Loedeman et~al.
\newblock Prompt generation networks for input-based adaptation of frozen
  vision transformers.
\newblock {\em arXiv preprint arXiv:2210.06466}, 2023.

\bibitem{tu2022visual}
C.-H. Tu et~al.
\newblock Visual query tuning: Towards effective usage of intermediate
  representations for parameter and memory efficient transfer learning.
\newblock {\em arXiv preprint arXiv:2212.03220}, 2022.

\bibitem{zhang2022promptcal}
S.~Zhang et~al.
\newblock Promptcal: Contrastive affinity learning via auxiliary prompts for
  generalized novel category discovery.
\newblock {\em arXiv preprint arXiv:2212.05590}, 2022.

\bibitem{salman2021unadversarial}
H.~Salman et~al.
\newblock Unadversarial examples: Designing objects for robust vision.
\newblock {\em Advances in Neural Information Processing Systems},
  34:15270--15284, 2021.

\bibitem{li2023exploring}
Y.~Li et~al.
\newblock Exploring the benefits of visual prompting in differential privacy.
\newblock {\em arXiv preprint arXiv:2303.12247}, 2023.

\bibitem{Girdhar_2023_CVPR}
R.~Girdhar et~al.
\newblock Imagebind: One embedding space to bind them all.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pp. 15180--15190, June 2023.

\bibitem{li2023otter}
B.~Li et~al.
\newblock Otter: A multi-modal model with in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.03726}, 2023.

\bibitem{bai2022constitutional}
Y.~Bai et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock {\em arXiv preprint arXiv:2212.08073}, 2022.

\bibitem{gao2023llamaadapterv2}
P.~Gao et~al.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock {\em arXiv preprint arXiv:2304.15010}, 2023.

\bibitem{gao2023backdoor}
K.~Gao et~al.
\newblock Backdoor defense via adaptively splitting poisoned dataset.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 4005--4014, 2023.

\bibitem{huang2022backdoor}
K.~Huang et~al.
\newblock Backdoor defense via decoupling the training process.
\newblock {\em arXiv preprint arXiv:2202.03423}, 2022.

\bibitem{yang2023backdoor}
S.~Yang et~al.
\newblock Backdoor defense via suppressing model shortcuts.
\newblock In {\em ICASSP 2023-2023 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp. 1--5. IEEE, 2023.

\bibitem{madry2017towards}
A.~Madry et~al.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{jia2022adversarial}
X.~Jia et~al.
\newblock Las-at: adversarial training with learnable attack strategy.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pp. 13398--13408, 2022.

\bibitem{wu2022towards}
B.~Wu et~al.
\newblock Towards efficient adversarial training on vision transformers.
\newblock In {\em European Conference on Computer Vision}, pp. 307--325.
  Springer, 2022.

\bibitem{gu2022vision}
J.~Gu et~al.
\newblock Are vision transformers robust to patch perturbations?
\newblock In {\em European Conference on Computer Vision}, pp. 404--421.
  Springer, 2022.

\bibitem{gu2021effective}
J.~Gu et~al.
\newblock Effective and efficient vote attack on capsule networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{anderljung2023frontier}
M.~Anderljung et~al.
\newblock Frontier ai regulation: Managing emerging risks to public safety.
\newblock {\em arXiv preprint arXiv:2307.03718}, 2023.

\bibitem{hacker2023regulating}
P.~Hacker et~al.
\newblock Regulating chatgpt and other large generative ai models.
\newblock In {\em Proceedings of the 2023 ACM Conference on Fairness,
  Accountability, and Transparency}, pp. 1112--1123, 2023.

\bibitem{li2022dall}
H.~Li et~al.
\newblock Do dall-e and flamingo understand each other?
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV) (to appear)}, 2023.

\end{thebibliography}
