


%{sections/sec1_intro}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{huang2022unsupervised,
  title={Unsupervised prompt learning for vision-language models},
  author={Huang, Tony and Chu, Jack and Wei, Fangyun},
  journal={arXiv preprint arXiv:2204.03649},
  year={2022}
}


@article{dong2022survey,
  title={A Survey for In-context Learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{shi2023effective,
  title={Effective Robustness against Natural Distribution Shifts for Models with Different Training Data},
  author={Shi, Zhouxing and Carlini, Nicholas and Balashankar, Ananth and Schmidt, Ludwig and Hsieh, Cho-Jui and Beutel, Alex and Qin, Yao},
  journal={arXiv preprint arXiv:2302.01381},
  year={2023}
}
@article{qiao2022reasoning,
  title={Reasoning with Language Model Prompting: A Survey},
  author={Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  journal={arXiv preprint arXiv:2212.09597},
  year={2022}
}

@article{bahng2022exploring,
  title={Exploring visual prompts for adapting large-scale models},
  author={Bahng, Hyojin and Jahanian, Ali and Sankaranarayanan, Swami and Isola, Phillip},
  journal={arXiv preprint arXiv:2203.17274},
  volume={1},
  number={3},
  pages={4},
  year={2022}}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}}

@article{wang2023seggpt,
  title={{SegGPT}: Segmenting everything in context},
  author={Wang, Xinlong and Zhang, Xiaosong and Cao, Yue and Wang, Wen and Shen, Chunhua and Huang, Tiejun},
  journal={arXiv preprint arXiv:2304.03284},
  year={2023} }

@article{wu2023visual,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}



@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{zhang2022automatic,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  booktitle={The Eleventh International Conference on Learning Representations (ICLR 2023)},
  year={2023}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

%{sections/sec2_preli}




%{sections/sec3_vlm}
@inproceedings{vlmsurvey,
  title     = {Vision-and-Language Pretrained Models: A Survey},
  author    = {Long, Siqu and Cao, Feiqi and Han, Soyeon Caren and Yang, Haiqin},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {5530--5537},
  year      = {2022},
  month     = {7},
  note      = {Survey Track},
  doi       = {10.24963/ijcai.2022/773},
  url       = {https://doi.org/10.24963/ijcai.2022/773},
}
@article{efrat2020turking,
  title={The turking test: Can language models understand instructions?},
  author={Efrat, Avia and Levy, Omer},
  journal={arXiv preprint arXiv:2010.11982},
  year={2020}
}
@inproceedings{rubin-etal-2022-learning,
    title = "Learning To Retrieve Prompts for In-Context Learning",
    author = "Rubin, Ohad  and
      Herzig, Jonathan  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.191",
    doi = "10.18653/v1/2022.naacl-main.191",
    pages = "2655--2671",
    abstract = "In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.",
}
@inproceedings{li-etal-2023-unified,
    title = "Unified Demonstration Retriever for In-Context Learning",
    author = "Li, Xiaonan  and
      Lv, Kai  and
      Yan, Hang  and
      Lin, Tianyang  and
      Zhu, Wei  and
      Ni, Yuan  and
      Xie, Guotong  and
      Wang, Xiaoling  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.256",
    pages = "4644--4668",
    abstract = "In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown sensitive to the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works train task-specific retrievers for several tasks separately, these methods are hard to transfer and scale on various tasks, and separately trained retrievers will cause a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks{'} training signals into a unified list-wise ranking formulation by language model{'}s feedback. Then we propose a multi-task list-wise ranking training framework with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks{'} signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR{'}s strong ability in various scenarios including different LMs (1.3B 175B), unseen datasets, varying demonstration quantities, etc. We will release the code and model checkpoint after review.",
}
@inproceedings{ye2023compositional,
  title={Compositional exemplars for in-context learning},
  author={Ye, Jiacheng and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng},
  booktitle={International Conference on Machine Learning},
  year={2023},
  organization={PMLR}
}
@inproceedings{qin-eisner-2021-learning,
    title = "Learning How to Ask: Querying {LM}s with Mixtures of Soft Prompts",
    author = "Qin, Guanghui  and
      Eisner, Jason",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.410",
    doi = "10.18653/v1/2021.naacl-main.410",
    pages = "5203--5212",
    abstract = "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to {``}fill in the blank{''} in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent{---}either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of {``}soft words,{''} i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.",
}
%bailan
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}
@article{ren2015faster,
  title={Faster r-cnn: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}
@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}
@article{pritchett2015learning,
  title={Learning from experiments when context matters},
  author={Pritchett, Lant and Sandefur, Justin},
  journal={American Economic Review},
  volume={105},
  number={5},
  pages={471--475},
  year={2015},
  publisher={American Economic Association 2014 Broadway, Suite 305, Nashville, TN 37203}
}
@article{tan2019lxmert,
  title={Lxmert: Learning cross-modality encoder representations from transformers},
  author={Tan, Hao and Bansal, Mohit},
  journal={arXiv preprint arXiv:1908.07490},
  year={2019}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{huang2020pixel,
  title={Pixel-bert: Aligning image pixels with text by deep multi-modal transformers},
  author={Huang, Zhicheng and Zeng, Zhaoyang and Liu, Bei and Fu, Dongmei and Fu, Jianlong},
  journal={arXiv preprint arXiv:2004.00849},
  year={2020}
}

@inproceedings{wang2021simvlm,
title={Sim{VLM}: Simple Visual Language Model Pretraining with Weak Supervision},
author={Zirui Wang and Jiahui Yu and Adams Wei Yu and Zihang Dai and Yulia Tsvetkov and Yuan Cao},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=GUrhfTuf_3}
}


@inproceedings{singh2022flava,
  title={Flava: A foundational language and vision alignment model},
  author={Singh, Amanpreet and Hu, Ronghang and Goswami, Vedanuj and Couairon, Guillaume and Galuba, Wojciech and Rohrbach, Marcus and Kiela, Douwe},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15638--15650},
  year={2022}
}

@article{mao2022understanding,
  title={Understanding Zero-Shot Adversarial Robustness for Large-Scale Models},
  author={Mao, Chengzhi and Geng, Scott and Yang, Junfeng and Wang, Xin and Vondrick, Carl},
  journal={arXiv preprint arXiv:2212.07016},
  year={2022}
}

@article{kong2023mitigating,
  title={Mitigating Test-Time Bias for Fair Image Retrieval},
  author={Kong, Fanjie and Yuan, Shuai and Hao, Weituo and Henao, Ricardo},
  journal={arXiv preprint arXiv:2305.19329},
  year={2023}
}

@article{xu2022exploring,
  title={Exploring the universal vulnerability of prompt-based learning paradigm},
  author={Xu, Lei and Chen, Yangyi and Cui, Ganqu and Gao, Hongcheng and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2204.05239},
  year={2022}
}

@article{carlini2021poisoning,
  title={Poisoning and backdooring contrastive learning},
  author={Carlini, Nicholas and Terzis, Andreas},
  journal={arXiv preprint arXiv:2106.09667},
  year={2021}
}

@inproceedings{xia2021xgpt,
  title={Xgpt: Cross-modal generative pre-training for image captioning},
  author={Xia, Qiaolin and Huang, Haoyang and Duan, Nan and Zhang, Dongdong and Ji, Lei and Sui, Zhifang and Cui, Edward and Bharti, Taroon and Zhou, Ming},
  booktitle={Natural Language Processing and Chinese Computing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13--17, 2021, Proceedings, Part I 10},
  pages={786--797},
  year={2021},
  organization={Springer}
}

@inproceedings{cho2021unifying,
  title={Unifying vision-and-language tasks via text generation},
  author={Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
  booktitle={International Conference on Machine Learning},
  pages={1931--1942},
  year={2021},
  organization={PMLR}
}
@article{yang2022prompt,
  title={Prompt Tuning for Generative Multimodal Pretrained Models},
  author={Yang, Hao and Lin, Junyang and Yang, An and Wang, Peng and Zhou, Chang and Yang, Hongxia},
  journal={arXiv preprint arXiv:2208.02532},
  year={2022}
}
@article{tsimpoukelli2021multimodal,
  title={Multimodal few-shot learning with frozen language models},
  author={Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={200--212},
  year={2021}
}

@inproceedings{yang2022empirical,
  title={An empirical study of gpt-3 for few-shot knowledge-based vqa},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022}
}
@article{yao2021filip,
  title={FILIP: fine-grained interactive language-image pre-training},
  author={Yao, Lewei and Huang, Runhui and Hou, Lu and Lu, Guansong and Niu, Minzhe and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Jiang, Xin and Xu, Chunjing},
  journal={arXiv preprint arXiv:2111.07783},
  year={2021}
}
@inproceedings{wang2022ofa,
  title={Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
  author={Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  booktitle={International Conference on Machine Learning},
  pages={23318--23340},
  year={2022},
  organization={PMLR}
}
@article{yu2022coca,
  title={Coca: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{wang2022image,
  title={Image as a foreign language: Beit pretraining for all vision and vision-language tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  journal={arXiv preprint arXiv:2208.10442},
  year={2022}
}

@inproceedings{
chen2022pali,
title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=mWVoBz4W0u}
}


@inproceedings{eichenberg2021magma,
    title = "{MAGMA} {--} Multimodal Augmentation of Generative Models through Adapter-based Finetuning",
    author = "Eichenberg, Constantin  and
      Black, Sidney  and
      Weinbach, Samuel  and
      Parcalabescu, Letitia  and
      Frank, Anette",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.179",
    pages = "2416--2428",
}

@article{huang2023language,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}
@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}
@article{guo2022threats,
  title={Threats to pre-trained language models: Survey and taxonomy},
  author={Guo, Shangwei and Xie, Chunlong and Li, Jiwei and Lyu, Lingjuan and Zhang, Tianwei},
  journal={arXiv preprint arXiv:2202.06862},
  year={2022}
}
@inproceedings{qiu2022benchmarking,
  title={Benchmarking Robustness under Distribution Shift of Multimodal Image-Text Models},
  author={Qiu, Jielin and Zhu, Yi and Shi, Xingjian and Tang, Zhiqiang and Zhao, Ding and Li, Bo and Li, Mu},
  booktitle={NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications},
  year={2022}
}

@article{zhao2023evaluating,
  title={On Evaluating Adversarial Robustness of Large Vision-Language Models},
  author={Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
  journal={arXiv preprint arXiv:2305.16934},
  year={2023}
}
@article{chen2023benchmarking,
  title={Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models},
  author={Chen, Shuo and Gu, Jindong and Han, Zhen and Ma, Yunpu and Torr, Philip and Tresp, Volker},
  journal={arXiv preprint arXiv:2306.02080},
  year={2023}
}

@article{gu2023towards,
  title={Towards Robust Prompts on Vision-Language Models},
  author={Gu, Jindong and Beirami, Ahmad and Wang, Xuezhi and Beutel, Alex and Torr, Philip and Qin, Yao},
  journal={arXiv preprint arXiv:2304.08479},
  year={2023}
}

@article{openaigpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@book{Nici_2020, place={Hauppauge}, title={AP Art History: 5 Practice Tests + Comprehensive Review + Online Practice.}, publisher={Barronâ€™s Educational Series}, author={Nici, John B.}, year={2020}} 

@article{zhang2023biomedgpt,
  title={BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks},
  author={Zhang, Kai and Yu, Jun and Yan, Zhiling and Liu, Yixin and Adhikarla, Eashan and Fu, Sunyang and Chen, Xun and Chen, Chen and Zhou, Yuyin and Li, Xiang and others},
  journal={arXiv preprint arXiv:2305.17100},
  year={2023}
}
@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}

@inproceedings{li2022dall,
  title={Do DALL-E and Flamingo Understand Each Other?},
  author={Li, Hang and Gu, Jindong and Koner, Rajat and Sharifzadeh, Sahand and Tresp, Volker},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (to appear)},
  year={2023}
}

@misc{chatgpt,
  title = {ChatGPT},
  howpublished = {\url{https://openai.com/blog/chatgpt}},
  note = {Accessed: 2023-07-22}
}

@inproceedings{geng2022meclip,
  title={Multi-event Video-Text Retrieval},
  author={Zhang, Gengyuan and Ren, Jisen and Gu, Jindong and Tresp, Volker},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) (to appear)},
  year={2023}
}

%{sections/sec4_clip}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}


@article{zheng2022prompt,
  title = {Prompt {{Vision Transformer}} for {{Domain Generalization}}},
  author = {Zheng, Zangwei and Yue, Xiangyu and Wang, Kai and You, Yang},
  journal={arXiv preprint arXiv:2208.08914},
  year={2022}
}


@inproceedings{wang2022dualprompt,
author = {Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and
Perot, Vincent and Dy, Jennifer and others},
booktitle = {Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI},
organization = {Springer},
pages = {631--648},
year = {2022},
title = {Dualprompt: Complementary prompting for rehearsal-free continual learning},
venue = {Computer Vision-ECCV }
}

@inproceedings{wang2022learning,
  title={Learning to prompt for continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={139--149},
  year={2022}
}

@incollection{feng2022promptdeta,
  title = {{{PromptDet}}: {{Towards Open-Vocabulary Detection Using Uncurated Images}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2022},
  author = {Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Chu, Xiangxiang and Ren, Haibing and Wei, Xiaolin and Xie, Weidi and Ma, Lin},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  volume = {13669},
  pages = {701--717},
  publisher = {{Springer Nature Switzerland}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20077-9_41},
  urldate = {2023-05-22},
  isbn = {978-3-031-20076-2 978-3-031-20077-9},
  langid = {english}
}

@inproceedings{guo2023texts,
  title={Texts as images in prompt tuning for multi-label image recognition},
  author={Guo, Zixian and Dong, Bowen and Ji, Zhilong and Bai, Jinfeng and Guo, Yiwen and Zuo, Wangmeng},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2808--2817},
  year={2023}
}


@article{shu2022test,
  title={Test-time prompt tuning for zero-shot generalization in vision-language models},
  author={Shu, Manli and Nie, Weili and Huang, De-An and Yu, Zhiding and Goldstein, Tom and Anandkumar, Anima and Xiao, Chaowei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={14274--14289},
  year={2022}
}


@inproceedings{wen2022visual,
  title = {Visual {{Prompt Tuning}} for {{Few-Shot Text Classification}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Wen, Jingyuan and Luo, Yutian and Fei, Nanyi and Yang, Guoxing and Lu, Zhiwu and Jiang, Hao and Jiang, Jie and Cao, Zhao},
  year = {2022},
  month = oct,
  pages = {5560--5570},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  urldate = {2023-05-22}
}

@article{peng2023kosmos,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
  journal={arXiv preprint arXiv:2306.14824},
  year={2023}
}

@article{gao2023compositionala,
  title = {Compositional {{Prompt Tuning}} with {{Motion Cues}} for {{Open-vocabulary Video Relation Detection}}},
  author = {Gao, Kaifeng and Chen, Long and Zhang, Hanwang and Xiao, Jun and Sun, Qianru},
  journal={arXiv preprint arXiv:2302.00268},
  year={2023}
}


@article{dong2023lpt,
  title = {{{LPT}}: {{Long-tailed Prompt Tuning}} for {{Image Classification}}},
  author = {Dong, Bowen and Zhou, Pan and Yan, Shuicheng and Zuo, Wangmeng},
  journal={arXiv preprint arXiv:2210.01033},
  year={2023}
}

@article{gu2022openvocabulary,
  title = {Open-Vocabulary {{Object Detection}} via {{Vision}} and {{Language Knowledge Distillation}}},
  author = {Gu, Xiuye and Lin, Tsung-Yi and Kuo, Weicheng and Cui, Yin},
  journal={arXiv preprint arXiv:2104.13921},
  year={2022}
}

@inproceedings{du2022learning,
  title={Learning to prompt for open-vocabulary object detection with vision-language model},
  author={Du, Yu and Wei, Fangyun and Zhang, Zihe and Shi, Miaojing and Gao, Yue and Li, Guoqi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14084--14093},
  year={2022}
}

@inproceedings{chen2023visual,
  title={Visual prompting for adversarial robustness},
  author={Chen, Aochuan and Lorenz, Peter and Yao, Yuguang and Chen, Pin-Yu and Liu, Sijia},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}
@InProceedings{Girdhar_2023_CVPR,
    author    = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
    title     = {ImageBind: One Embedding Space To Bind Them All},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {15180-15190}
}

@inproceedings{fang2022data,
  title={Data determines distributional robustness in contrastive language image pre-training (clip)},
  author={Fang, Alex and Ilharco, Gabriel and Wortsman, Mitchell and Wan, Yuhao and Shankar, Vaishaal and Dave, Achal and Schmidt, Ludwig},
  booktitle={International Conference on Machine Learning},
  pages={6216--6234},
  year={2022},
  organization={PMLR}
}


@misc{mao2023understanding,
  title = {Understanding {{Zero-Shot Adversarial Robustness}} for {{Large-Scale Models}}},
  author = {Mao, Chengzhi and Geng, Scott and Yang, Junfeng and Wang, Xin and Vondrick, Carl},
  year = {2023},
  month = apr,
  number = {arXiv:2212.07016},
  eprint = {2212.07016},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-04},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{jia2022badencoder,
  title={Badencoder: Backdoor attacks to pre-trained encoders in self-supervised learning},
  author={Jia, Jinyuan and Liu, Yupei and Gong, Neil Zhenqiang},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={2043--2059},
  year={2022},
  organization={IEEE}
}



@inproceedings{zhou2022conditional,
  title={Conditional prompt learning for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16816--16825},
  year={2022}
}

@misc{carlini2022poisoning,
  title = {Poisoning and {{Backdooring Contrastive Learning}}},
  author = {Carlini, Nicholas and Terzis, Andreas},
  year = {2022},
  month = mar,
  number = {arXiv:2106.09667},
  eprint = {2106.09667},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-04},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}



@misc{zang2023contextual,
  title = {Contextual {{Object Detection}} with {{Multimodal Large Language Models}}},
  author = {Zang, Yuhang and Li, Wei and Han, Jun and Zhou, Kaiyang and Loy, Chen Change},
  year = {2023},
  month = may,
  number = {arXiv:2305.18279},
  eprint = {2305.18279},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-06-01},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}


@inproceedings{gao2021making,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  year = {2021},
  month = aug,
  pages = {3816--3830},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.295},
  urldate = {2023-06-12}
}

@inproceedings{feng2022promptdet,
  title={Promptdet: Towards open-vocabulary detection using uncurated images},
  author={Feng, Chengjian and Zhong, Yujie and Jie, Zequn and Chu, Xiangxiang and Ren, Haibing and Wei, Xiaolin and Xie, Weidi and Ma, Lin},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part IX},
  pages={701--717},
  year={2022},
  organization={Springer}
}

@inproceedings{he2022openvocabularya,
  title={Towards open-vocabulary scene graph generation with prompt-based finetuning},
  author={He, Tao and Gao, Lianli and Song, Jingkuan and Li, Yuan-Fang},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVIII},
  pages={56--73},
  year={2022},
  organization={Springer}
}

@article{ge2022domain,
  title = {Domain {{Adaptation}} via {{Prompt Learning}}},
  author = {Ge, Chunjiang and Huang, Rui and Xie, Mixue and Lai, Zihang and Song, Shiji and Li, Shuang and Huang, Gao},
  journal={arXiv:2202.06687},
  year={2022}
}

@article{bar2022visual,
  title={Visual prompting via image inpainting},
  author={Bar, Amir and Gandelsman, Yossi and Darrell, Trevor and Globerson, Amir and Efros, Alexei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25005--25017},
  year={2022}
}

@inproceedings{huang2023diversity,
  title={Diversity-Aware Meta Visual Prompting},
  author={Huang, Qidong and Dong, Xiaoyi and Chen, Dongdong and Zhang, Weiming and Wang, Feifei and Hua, Gang and Yu, Nenghai},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10878--10887},
  year={2023}
}




@inproceedings{ju2022prompting,
  title={Prompting visual-language models for efficient video understanding},
  author={Ju, Chen and Han, Tengda and Zheng, Kunhao and Zhang, Ya and Xie, Weidi},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV},
  pages={105--124},
  year={2022},
  organization={Springer}
}

@article{li2023blip2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}


@article{shtedritski2023what,
  title={What does clip know about a red circle? visual prompt engineering for vlms},
  author={Shtedritski, Aleksandar and Rupprecht, Christian and Vedaldi, Andrea},
  journal={arXiv preprint arXiv:2304.06712},
  year={2023}
}


@article{wu2022unleashing,
  title={Unleashing the power of visual prompting at the pixel level},
  author={Wu, Junyang and Li, Xianhang and Wei, Chen and Wang, Huiyu and Yuille, Alan and Zhou, Yuyin and Xie, Cihang},
  journal={arXiv preprint arXiv:2212.10556},
  year={2022}
}
@inproceedings{rao2022denseclip,
  title={Denseclip: Language-guided dense prediction with context-aware prompting},
  author={Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18082--18091},
  year={2022}
}


@article{xiao2022optimizing,
  title = {Optimizing {{Continuous Prompts}} for {{Visual Relationship Detection}} by {{Affix-Tuning}}},
  author = {Xiao, Shouguan and Fu, Weiping},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {70104--70112},
  doi = {10.1109/ACCESS.2022.3187263},
  keywords = {affix-tuning transformers,prompt template,Semantics,Task analysis,Training data,Transformers,Visual relationship detection,Visualization}
}




@article{shen2022multitask,
  title = {Multitask {{Vision-Language Prompt Tuning}}},
  author = {Shen, Sheng and Yang, Shijia and Zhang, Tianjun and Zhai, Bohan and Gonzalez, Joseph E. and Keutzer, Kurt and Darrell, Trevor},
  journal = {arXiv:2211.11720},
  year = {2022},
}


@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{li2021alignb,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@article{zang2022unified,
  title = {Unified {{Vision}} and {{Language Prompt Learning}}},
  author = {Zang, Yuhang and Li, Wei and Zhou, Kaiyang and Huang, Chen and Loy, Chen Change},
  year = {2022},
  journal = {arXiv:2210.07225}
}

@inproceedings{jia2022visual,
  title={Visual prompt tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII},
  pages={709--727},
  year={2022},
  organization={Springer}
}


@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}


@article{yao2022cpt,
  title = {{{CPT}}: {{Colorful Prompt Tuning}} for {{Pre-trained Vision-Language Models}}},
  author = {Yao, Yuan and Zhang, Ao and Zhang, Zhengyan and Liu, Zhiyuan and Chua, Tat-Seng and Sun, Maosong},
  journal={arXiv:2109.11797},
  year={2022}
}


@inproceedings{khattak2023maple,
  title={Maple: Multi-modal prompt learning},
  author={Khattak, Muhammad Uzair and Rasheed, Hanoona and Maaz, Muhammad and Khan, Salman and Khan, Fahad Shahbaz},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19113--19122},
  year={2023}
}


@article{gao2022visual,
  title = {Visual {{Prompt Tuning}} for {{Test-time Domain Adaptation}}},
  author = {Gao, Yunhe and Shi, Xingjian and Zhu, Yi and Wang, Hao and Tang, Zhiqiang and Zhou, Xiong and Li, Mu and Metaxas, Dimitris N.},
  journal={arXiv:2210.04831},
  year={2022}
}


@article{sun2022dualcoop,
  title={Dualcoop: Fast adaptation to multi-label recognition with limited annotations},
  author={Sun, Ximeng and Hu, Ping and Saenko, Kate},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30569--30582},
  year={2022}
}

@article{agarwal2021evaluating,
  title={Evaluating clip: towards characterization of broader capabilities and downstream implications},
  author={Agarwal, Sandhini and Krueger, Gretchen and Clark, Jack and Radford, Alec and Kim, Jong Wook and Brundage, Miles},
  journal={arXiv preprint arXiv:2108.02818},
  year={2021}
}

@article{bansal2023cleanclip,
  title={CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning},
  author={Bansal, Hritik and Singhi, Nishad and Yang, Yu and Yin, Fan and Grover, Aditya and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2303.03323},
  year={2023}
}


%{sections/sec5_gen}
@inproceedings{reed2016generative,
  title={Generative adversarial text to image synthesis},
  author={Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  booktitle={International conference on machine learning},
  pages={1060--1069},
  year={2016},
  organization={PMLR}
}
@inproceedings{gregor2015draw,
  title={Draw: A recurrent neural network for image generation},
  author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1462--1471},
  year={2015},
  organization={PMLR}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{pan2023drag,
  title={Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold},
  author={Pan, Xingang and Tewari, Ayush and Leimk{\"u}hler, Thomas and Liu, Lingjie and Meka, Abhimitra and Theobalt, Christian},
  journal={arXiv preprint arXiv:2305.10973},
  year={2023}
}

@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@inproceedings{isola2017image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1125--1134},
  year={2017}
}

@article{kingma2019introduction,
  title={An introduction to variational autoencoders},
  author={Kingma, Diederik P and Welling, Max and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={12},
  number={4},
  pages={307--392},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{pu2016variational,
  title={Variational autoencoder for deep learning of images, labels and captions},
  author={Pu, Yunchen and Gan, Zhe and Henao, Ricardo and Yuan, Xin and Li, Chunyuan and Stevens, Andrew and Carin, Lawrence},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{vahdat2020nvae,
  title={NVAE: A deep hierarchical variational autoencoder},
  author={Vahdat, Arash and Kautz, Jan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19667--19679},
  year={2020}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@article{yu2022scaling,
  title={Scaling autoregressive models for content-rich text-to-image generation},
  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and others},
  journal={arXiv preprint arXiv:2206.10789},
  year={2022}
}

@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8780--8794},
  year={2021}
}

@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{beaumont2022clip,
  title={Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them},
  author={Beaumont, Romain},
  year={2022},
  publisher={GitHub}
}

@article{avrahami2022blended,
  title={Blended latent diffusion},
  author={Avrahami, Omri and Fried, Ohad and Lischinski, Dani},
  journal={arXiv preprint arXiv:2206.02779},
  year={2022}
}

@article{nichol2021glide,
  title={Glide: Towards photorealistic image generation and editing with text-guided diffusion models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}

@article{witteveen2022investigating,
   title={Investigating Prompt Engineering in Diffusion Models}, 
  author={Sam Witteveen and Martin Andrews},
  journal={arXiv preprint arXiv:2211.15462},
  year={2022}
}

@article{wu2023diffumask,
   title={DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models}, 
   author={Weijia Wu and Yuzhong Zhao and Mike Zheng Shou and Hong Zhou and Chunhua Shen},
   journal={arXiv preprint arXiv:2303.11681},
  year={2023}
}

@inproceedings{ni2022imaginarynet,
  title={ImaginaryNet: Learning Object Detectors without Real Images and Annotations}, 
  author={Minheng Ni and Zitong Huang and Kailai Feng and Wangmeng Zuo},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{he2023synthetic,
  title={Is synthetic data from generative models ready for image recognition?}, 
  author={Ruifei He and Shuyang Sun and Xin Yu and Chuhui Xue and Wenqing Zhang and Philip Torr and Song Bai and Xiaojuan Qi},
  booktitle={ICLR},
  year={2023}
}

@inproceedings{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
  author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  booktitle={JMLR},
  year={2020}
}


@inproceedings{
gal2022image,
title={An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
author={Rinon Gal and Yuval Alaluf and Yuval Atzmon and Or Patashnik and Amit Haim Bermano and Gal Chechik and Daniel Cohen-or},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=NAQvF08TcyG}
}

@article{canny1986computational,
  title={A computational approach to edge detection},
  author={Canny, John},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  pages={679--698},
  year={1986},
  publisher={Ieee}
}

@article{zhang2023adding,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Agrawala, Maneesh},
  journal={arXiv preprint arXiv:2302.05543},
  year={2023}
}


@article{hertz2022prompt,
  title={Prompt-to-prompt image editing with cross attention control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-Or, Daniel},
  journal={arXiv preprint arXiv:2208.01626},
  year={2022}
}

@article{kumari2022multi,
  title={Multi-Concept Customization of Text-to-Image Diffusion},
  author={Kumari, Nupur and Zhang, Bingliang and Zhang, Richard and Shechtman, Eli and Zhu, Jun-Yan},
  journal={arXiv preprint arXiv:2212.04488},
  year={2022}
}

@article{ho2022imagen,
  title={Imagen video: High definition video generation with diffusion models},
  author={Ho, Jonathan and Chan, William and Saharia, Chitwan and Whang, Jay and Gao, Ruiqi and Gritsenko, Alexey and Kingma, Diederik P and Poole, Ben and Norouzi, Mohammad and Fleet, David J and others},
  journal={arXiv preprint arXiv:2210.02303},
  year={2022}
}

@article{ho2022cascaded,
  title={Cascaded Diffusion Models for High Fidelity Image Generation.},
  author={Ho, Jonathan and Saharia, Chitwan and Chan, William and Fleet, David J and Norouzi, Mohammad and Salimans, Tim},
  journal={J. Mach. Learn. Res.},
  volume={23},
  number={47},
  pages={1--33},
  year={2022}
}

@article{qi2023fatezero,
  title={Fatezero: Fusing attentions for zero-shot text-based video editing},
  author={Qi, Chenyang and Cun, Xiaodong and Zhang, Yong and Lei, Chenyang and Wang, Xintao and Shan, Ying and Chen, Qifeng},
  journal={arXiv preprint arXiv:2303.09535},
  year={2023}
}

@article{singer2022make,
  title={Make-a-video: Text-to-video generation without text-video data},
  author={Singer, Uriel and Polyak, Adam and Hayes, Thomas and Yin, Xi and An, Jie and Zhang, Songyang and Hu, Qiyuan and Yang, Harry and Ashual, Oron and Gafni, Oran and others},
  journal={arXiv preprint arXiv:2209.14792},
  year={2022}
}

@article{wu2022tune,
  title={Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation},
  author={Wu, Jay Zhangjie and Ge, Yixiao and Wang, Xintao and Lei, Weixian and Gu, Yuchao and Hsu, Wynne and Shan, Ying and Qie, Xiaohu and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2212.11565},
  year={2022}
}

@article{poole2022dreamfusion,
  title={Dreamfusion: Text-to-3d using 2d diffusion},
  author={Poole, Ben and Jain, Ajay and Barron, Jonathan T and Mildenhall, Ben},
  journal={arXiv preprint arXiv:2209.14988},
  year={2022}
}

@article{mildenhall2021nerf,
  title={Nerf: Representing scenes as neural radiance fields for view synthesis},
  author={Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and Barron, Jonathan T and Ramamoorthi, Ravi and Ng, Ren},
  journal={Communications of the ACM},
  volume={65},
  number={1},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10684--10695},
  year={2022}
}


@article{lin2022magic3d,
  title={Magic3D: High-Resolution Text-to-3D Content Creation},
  author={Lin, Chen-Hsuan and Gao, Jun and Tang, Luming and Takikawa, Towaki and Zeng, Xiaohui and Huang, Xun and Kreis, Karsten and Fidler, Sanja and Liu, Ming-Yu and Lin, Tsung-Yi},
  journal={arXiv preprint arXiv:2211.10440},
  year={2022}
}

@article{xu2022dream3d,
  title={Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models},
  author={Xu, Jiale and Wang, Xintao and Cheng, Weihao and Cao, Yan-Pei and Shan, Ying and Qie, Xiaohu and Gao, Shenghua},
  journal={arXiv preprint arXiv:2212.14704},
  year={2022}
}

@article{lee2022understanding,
  title={Understanding pure clip guidance for voxel grid nerf models},
  author={Lee, Han-Hung and Chang, Angel X},
  journal={arXiv preprint arXiv:2209.15172},
  year={2022}
}

@article{khalid2022clip,
  title={Clip-mesh: Generating textured meshes from text using pretrained image-text models},
  author={Khalid, Nasir Mohammad and Xie, Tianhao and Belilovsky, Eugene and Popa, Tiberiu},
  journal={arXiv preprint arXiv:2203.13333},
  year={2022}
}

@article{zhang2022motiondiffuse,
  title={Motiondiffuse: Text-driven human motion generation with diffusion model},
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2208.15001},
  year={2022}
}


@article{kim2022flame,
  title={Flame: Free-form language-based motion synthesis \& editing},
  author={Kim, Jihoon and Kim, Jiseob and Choi, Sungjoon},
  journal={arXiv preprint arXiv:2209.00349},
  year={2022}
}

@article{tevet2022human,
  title={Human motion diffusion model},
  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Cohen-Or, Daniel and Bermano, Amit H},
  journal={arXiv preprint arXiv:2209.14916},
  year={2022}
}

@article{jeong2023zero,
  title={Zero-shot Generation of Coherent Storybook from Plain Text Story using Diffusion Models},
  author={Jeong, Hyeonho and Kwon, Gihyun and Ye, Jong Chul},
  journal={arXiv preprint arXiv:2302.03900},
  year={2023}
}

@article{noever2023multimodal,
  title={The Multimodal And Modular Ai Chef: Complex Recipe Generation From Imagery},
  author={Noever, David and Noever, Samantha Elizabeth Miller},
  journal={arXiv preprint arXiv:2304.02016},
  year={2023}
}

@article{lu2023multimodal,
  title={Multimodal Procedural Planning via Dual Text-Image Prompting},
  author={Lu, Yujie and Lu, Pan and Chen, Zhiyu and Zhu, Wanrong and Wang, Xin Eric and Wang, William Yang},
  journal={arXiv preprint arXiv:2305.01795},
  year={2023}
}

@article{chen2023diffusion,
  title={Diffusion Models for Imperceptible and Transferable Adversarial Attack},
  author={Chen, Jianqi and Chen, Hao and Chen, Keyan and Zhang, Yilan and Zou, Zhengxia and Shi, Zhenwei},
  journal={arXiv preprint arXiv:2305.08192},
  year={2023}
}

@article{nie2022diffusion,
  title={Diffusion models for adversarial purification},
  author={Nie, Weili and Guo, Brandon and Huang, Yujia and Xiao, Chaowei and Vahdat, Arash and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2205.07460},
  year={2022}
}

@article{zhai2023text,
  title={Text-to-image diffusion models can be easily backdoored through multimodal data poisoning},
  author={Zhai, Shengfang and Dong, Yinpeng and Shen, Qingni and Pu, Shi and Fang, Yuejian and Su, Hang},
  journal={arXiv preprint arXiv:2305.04175},
  year={2023}
}

@article{huang2023zero,
  title={Zero-Day Backdoor Attack against Text-to-Image Diffusion Models via Personalization},
  author={Huang, Yihao and Guo, Qing and Juefei-Xu, Felix},
  journal={arXiv preprint arXiv:2305.10701},
  year={2023}
}
@article{struppek2022rickrolling,
  title={Rickrolling the Artist: Injecting Invisible Backdoors into Text-Guided Image Generation Models},
  author={Struppek, Lukas and Hintersdorf, Dominik and Kersting, Kristian},
  journal={arXiv preprint arXiv:2211.02408},
  year={2022}
}


@inproceedings{ruiz2023dreambooth,
  title={Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation},
  author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22500--22510},
  year={2023}
}

@article{jobin2019global,
  title={The global landscape of AI ethics guidelines},
  author={Jobin, Anna and Ienca, Marcello and Vayena, Effy},
  journal={Nature Machine Intelligence},
  volume={1},
  number={9},
  pages={389--399},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{zhuang2023pilot,
  title={A Pilot Study of Query-Free Adversarial Attack against Stable Diffusion},
  author={Zhuang, Haomin and Zhang, Yihua and Liu, Sijia},
  journal={arXiv preprint arXiv:2303.16378},
  year={2023}
}

@article{friedrich2023fair,
  title={Fair diffusion: Instructing text-to-image generation models on fairness},
  author={Friedrich, Felix and Schramowski, Patrick and Brack, Manuel and Struppek, Lukas and Hintersdorf, Dominik and Luccioni, Sasha and Kersting, Kristian},
  journal={arXiv preprint arXiv:2302.10893},
  year={2023}
}

@article{chuang2023debiasing,
  title={Debiasing vision-language models via biased prompts},
  author={Chuang, Ching-Yao and Jampani, Varun and Li, Yuanzhen and Torralba, Antonio and Jegelka, Stefanie},
  journal={arXiv preprint arXiv:2302.00070},
  year={2023}
}

@article{naik2023social,
  title={Social Biases through the Text-to-Image Generation Lens},
  author={Naik, Ranjita and Nushi, Besmira},
  journal={arXiv preprint arXiv:2304.06034},
  year={2023}
}

@article{wang2023t2iat,
  title={T2IAT: Measuring Valence and Stereotypical Biases in Text-to-Image Generation},
  author={Wang, Jialu and Liu, Xinyue Gabby and Di, Zonglin and Liu, Yang and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2306.00905},
  year={2023}
}

@article{luccioni2023stable,
  title={Stable Bias: Analyzing Societal Representations in Diffusion Models},
  author={Luccioni, Alexandra Sasha and Akiki, Christopher and Mitchell, Margaret and Jernite, Yacine},
  journal={arXiv preprint arXiv:2303.11408},
  year={2023}
}
@article{smith2023balancing,
  title={Balancing the Picture: Debiasing Vision-Language Datasets with Synthetic Contrast Sets},
  author={Smith, Brandon and Farinha, Miguel and Hall, Siobhan Mackenzie and Kirk, Hannah Rose and Shtedritski, Aleksandar and Bain, Max},
  journal={arXiv preprint arXiv:2305.15407},
  year={2023}
}

@article{kim2023explaining,
  title={Explaining visual biases as words by generating captions},
  author={Kim, Younghyun and Mo, Sangwoo and Kim, Minkyu and Lee, Kyungmin and Lee, Jaeho and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2301.11104},
  year={2023}
}
@inproceedings{seth2023dear,
  title={DeAR: Debiasing Vision-Language Models with Additive Residuals},
  author={Seth, Ashish and Hemani, Mayur and Agarwal, Chirag},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6820--6829},
  year={2023}
}

@article{wu2022membership,
  title={Membership Inference Attacks Against Text-to-image Generation Models},
  author={Wu, Yixin and Yu, Ning and Li, Zheng and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2210.00968},
  year={2022}
}

@misc{jeulin1997dead,
  title={Dead Leaves Models: from space tesselation to random functions Proc. of the Symposium on the Advances in the Theory and Applications of Random Sets (Fontainebleau, 9-11 October 1996) ed D Jeulin},
  author={Jeulin, D},
  year={1997},
  publisher={Singapore: World Scientific}
}
@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}
@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@article{feng2022training,
  title={Training-free structured diffusion guidance for compositional text-to-image synthesis},
  author={Feng, Weixi and He, Xuehai and Fu, Tsu-Jui and Jampani, Varun and Akula, Arjun and Narayana, Pradyumna and Basu, Sugato and Wang, Xin Eric and Wang, William Yang},
  journal={arXiv preprint arXiv:2212.05032},
  year={2022}
}

@article{epstein2023diffusion,
  title={Diffusion self-guidance for controllable image generation},
  author={Epstein, Dave and Jabri, Allan and Poole, Ben and Efros, Alexei A and Holynski, Aleksander},
  journal={arXiv preprint arXiv:2306.00986},
  year={2023}
}

@inproceedings{kawar2023imagic,
  title={Imagic: Text-based real image editing with diffusion models},
  author={Kawar, Bahjat and Zada, Shiran and Lang, Oran and Tov, Omer and Chang, Huiwen and Dekel, Tali and Mosseri, Inbar and Irani, Michal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6007--6017},
  year={2023}
}

@inproceedings{muller2023diffrf,
  title={Diffrf: Rendering-guided 3d radiance field diffusion},
  author={M{\"u}ller, Norman and Siddiqui, Yawar and Porzi, Lorenzo and Bulo, Samuel Rota and Kontschieder, Peter and Nie{\ss}ner, Matthias},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4328--4338},
  year={2023}
}

@inproceedings{ruan2023mm,
  title={Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation},
  author={Ruan, Ludan and Ma, Yiyang and Yang, Huan and He, Huiguo and Liu, Bei and Fu, Jianlong and Yuan, Nicholas Jing and Jin, Qin and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10219--10228},
  year={2023}
}

@article{zhu2023moviefactory,
  title={MovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images},
  author={Zhu, Junchen and Yang, Huan and He, Huiguo and Wang, Wenjing and Tuo, Zixi and Cheng, Wen-Huang and Gao, Lianli and Song, Jingkuan and Fu, Jianlong},
  journal={arXiv preprint arXiv:2306.07257},
  year={2023}
}

@article{yu2023scaling,
  title={Scaling robot learning with semantically imagined experience},
  author={Yu, Tianhe and Xiao, Ted and Stone, Austin and Tompson, Jonathan and Brohan, Anthony and Wang, Su and Singh, Jaspiar and Tan, Clayton and Peralta, Jodilyn and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2302.11550},
  year={2023}
}

@inproceedings{rempeluo2023tracepace,
    author={Rempe, Davis and Luo, Zhengyi and Peng, Xue Bin and Yuan, Ye and Kitani, Kris and Kreis, Karsten and Fidler, Sanja and Litany, Or},
    title={Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion},
    booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023}
}       

@inproceedings{shabani2023housediffusion,
  title={Housediffusion: Vector floorplan generation via a diffusion model with discrete and continuous denoising},
  author={Shabani, Mohammad Amin and Hosseini, Sepidehsadat and Furukawa, Yasutaka},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5466--5475},
  year={2023}
}

%{sections/sec6_}
%%%%%%% sec6-1
@article{paranjape2021prompting,
  title={Prompting contrastive explanations for commonsense reasoning tasks},
  author={Paranjape, Bhargavi and Michael, Julian and Ghazvininejad, Marjan and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2106.06823},
  year={2021}
}
@article{fu2022complexity,
  title={Complexity-based prompting for multi-step reasoning},
  author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  journal={arXiv preprint arXiv:2210.00720},
  year={2022}
}

@article{press2022measuring,
  title={Measuring and Narrowing the Compositionality Gap in Language Models},
  author={Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2210.03350},
  year={2022}
}

@article{kazemi2022lambada,
  title={LAMBADA: Backward Chaining for Automated Reasoning in Natural Language},
  author={Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
  journal={arXiv preprint arXiv:2212.13894},
  year={2022}
}


@article{khot2022decomposed,
  title={Decomposed prompting: A modular approach for solving complex tasks},
  author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2210.02406},
  year={2022}
}


@inproceedings{khashabi-etal-2020-unifiedqa,
    title = "{UNIFIEDQA}: Crossing Format Boundaries with a Single {QA} System",
    author = "Khashabi, Daniel  and
      Min, Sewon  and
      Khot, Tushar  and
      Sabharwal, Ashish  and
      Tafjord, Oyvind  and
      Clark, Peter  and
      Hajishirzi, Hannaneh",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.171",
    doi = "10.18653/v1/2020.findings-emnlp.171",
    pages = "1896--1907",
}
@article{jiang2021can,
  title={How can we know when language models know? on the calibration of language models for question answering},
  author={Jiang, Zhengbao and Araki, Jun and Ding, Haibo and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={962--977},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@inproceedings{schick-schutze-2021-shot,
    title = "Few-Shot Text Generation with Natural Language Instructions",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.32",
    doi = "10.18653/v1/2021.emnlp-main.32",
    pages = "390--402",

}
@inproceedings{KnowPrompt,
author = {Chen, Xiang and Zhang, Ningyu and Xie, Xin and Deng, Shumin and Yao, Yunzhi and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
title = {KnowPrompt: Knowledge-Aware Prompt-Tuning with Synergistic Optimization for Relation Extraction},
year = {2022},
isbn = {9781450390965},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485447.3511998},
doi = {10.1145/3485447.3511998},
booktitle = {Proceedings of the ACM Web Conference 2022},
pages = {2778â€“2788},
numpages = {11},
keywords = {Prompt-tuning, Relation Extraction, Knowledge-aware},
location = {Virtual Event, Lyon, France},
series = {WWW '22}
}

@inproceedings{cui-etal-2021-template,
    title = "Template-Based Named Entity Recognition Using {BART}",
    author = "Cui, Leyang  and
      Wu, Yu  and
      Liu, Jian  and
      Yang, Sen  and
      Zhang, Yue",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.161",
    doi = "10.18653/v1/2021.findings-acl.161",
    pages = "1835--1845",
}

@article{yang2022prompting,
  title={A prompting-based approach for adversarial example generation and robustness enhancement},
  author={Yang, Yuting and Huang, Pei and Cao, Juan and Li, Jintao and Lin, Yun and Dong, Jin Song and Ma, Feifei and Zhang, Jian},
  journal={arXiv preprint arXiv:2203.10714},
  year={2022}
}

@article{dong2023promptattack,
  title={PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts},
  author={Dong, Xiangjue and He, Yun and Zhu, Ziwei and Caverlee, James},
  journal={arXiv preprint arXiv:2306.04535},
  year={2023}
}

@inproceedings{delobelle-etal-2022-measuring,
    title = "Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models",
    author = "Delobelle, Pieter  and
      Tokpo, Ewoenam  and
      Calders, Toon  and
      Berendt, Bettina",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.122",
    doi = "10.18653/v1/2022.naacl-main.122",
    pages = "1693--1706",
}

@article{Lou2023Is,
  author       = {Renze Lou and
                  Kai Zhang and
                  Wenpeng Yin},
  title        = {Is Prompt All You Need? No. {A} Comprehensive and Broader View of
                  Instruction Learning},
  journal      = {CoRR},
  volume       = {abs/2303.10475},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2303.10475},
  doi          = {10.48550/arXiv.2303.10475},
  eprinttype    = {arXiv},
  eprint       = {2303.10475},
  timestamp    = {Wed, 22 Mar 2023 14:41:36 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2303-10475.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Ding2022OpenPrompt,
  author    = {Ning Ding and
               Shengding Hu and
               Weilin Zhao and
               Yulin Chen and
               Zhiyuan Liu and
               Haitao Zheng and
               Maosong Sun},
  editor    = {Valerio Basile and
               Zornitsa Kozareva and
               Sanja Stajner},
  title     = {OpenPrompt: An Open-source Framework for Prompt-learning},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2022 - System Demonstrations, Dublin, Ireland,
               May 22-27, 2022},
  pages     = {105--113},
  publisher = {Association for Computational Linguistics},
  year      = {2022},
  url       = {https://doi.org/10.18653/v1/2022.acl-demo.10},
  doi       = {10.18653/v1/2022.acl-demo.10},
  timestamp = {Tue, 24 Jan 2023 15:06:31 +0100},
  biburl    = {https://dblp.org/rec/conf/acl/DingHZCLZS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%%%%%%% sec6-2 

@article{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}
@inproceedings{wang2022images,
  title={Images speak in images: A generalist painter for in-context visual learning},
  author={Wang, Xinlong and Wang, Wen and Cao, Yue and Shen, Chunhua and Huang, Tiejun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6830--6839},
  year={2023}
}

@article{yu2022towards,
  title={Towards a Unified View on Visual Parameter-Efficient Transfer Learning},
  author={Yu, Bruce XB and Chang, Jianlong and Liu, Lingbo and Tian, Qi and Chen, Chang Wen},
  journal={arXiv preprint arXiv:2210.00788},
  year={2022}
}
@article{tu2022visual,
  title={Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning},
  author={Tu, Cheng-Hao and Mai, Zheda and Chao, Wei-Lun},
  journal={arXiv preprint arXiv:2212.03220},
  year={2022}
}
@article{zhang2022promptcal,
  title={PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery},
  author={Zhang, Sheng and Khan, Salman and Shen, Zhiqiang and Naseer, Muzammal and Chen, Guangyi and Khan, Fahad},
  journal={arXiv preprint arXiv:2212.05590},
  year={2022}
}

@article{salman2021unadversarial,
  title={Unadversarial examples: Designing objects for robust vision},
  author={Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Vemprala, Sai and Madry, Aleksander and Kapoor, Ashish},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15270--15284},
  year={2021}
}

@article{loedeman2023prompt,
      title={Prompt Generation Networks for Input-based Adaptation of Frozen Vision Transformers}, 
      author={Jochem Loedeman and Maarten C. Stol and Tengda Han and Yuki M. Asano},
      journal={arXiv preprint arXiv:2210.06466},
    year={2023}
    }

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={International conference on machine learning},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@article{izacard2022few,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv preprint arXiv:2208.03299},
  year={2022}
}

@article{duan2023diffusion,
  title={Are diffusion models vulnerable to membership inference attacks?},
  author={Duan, Jinhao and Kong, Fei and Wang, Shiqi and Shi, Xiaoshuang and Xu, Kaidi},
  journal={arXiv preprint arXiv:2302.01316},
  year={2023}
}

@article{shen2023prompt,
  title={Prompt Stealing Attacks Against Text-to-Image Generation Models},
  author={Shen, Xinyue and Qu, Yiting and Backes, Michael and Zhang, Yang},
  journal={arXiv preprint arXiv:2302.09923},
  year={2023}
}
@article{yang2023sneakyprompt,
  title={SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters},
  author={Yang, Yuchen and Hui, Bo and Yuan, Haolin and Gong, Neil and Cao, Yinzhi},
  journal={arXiv preprint arXiv:2305.12082},
  year={2023}
}
@article{webster2023reproducible,
  title={A Reproducible Extraction of Training Images from Diffusion Models},
  author={Webster, Ryan},
  journal={arXiv preprint arXiv:2305.08694},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{li2023exploring,
  title={Exploring the Benefits of Visual Prompting in Differential Privacy},
  author={Li, Yizhe and Tsai, Yu-Lin and Ren, Xuebin and Yu, Chia-Mu and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2303.12247},
  year={2023}
}

%{sections/sec7_discuss}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@article{gao2023llamaadapterv2,
  title = {LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{anderljung2023frontier,
  title={Frontier AI Regulation: Managing Emerging Risks to Public Safety},
  author={Anderljung, Markus and Barnhart, Joslyn and Leung, Jade and Korinek, Anton and O'Keefe, Cullen and Whittlestone, Jess and Avin, Shahar and Brundage, Miles and Bullock, Justin and Cass-Beggs, Duncan and others},
  journal={arXiv preprint arXiv:2307.03718},
  year={2023}
}

@inproceedings{gu2021effective,
  title={Effective and efficient vote attack on capsule networks},
  author={Gu, Jindong and Wu, Baoyuan and Tresp, Volker},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{yang2023backdoor,
  title={Backdoor Defense via Suppressing Model Shortcuts},
  author={Yang, Sheng and Li, Yiming and Jiang, Yong and Xia, Shu-Tao},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{wu2022towards,
  title={Towards efficient adversarial training on vision transformers},
  author={Wu, Boxi and Gu, Jindong and Li, Zhifeng and Cai, Deng and He, Xiaofei and Liu, Wei},
  booktitle={European Conference on Computer Vision},
  pages={307--325},
  year={2022},
  organization={Springer}
}

@article{huang2022backdoor,
  title={Backdoor defense via decoupling the training process},
  author={Huang, Kunzhe and Li, Yiming and Wu, Baoyuan and Qin, Zhan and Ren, Kui},
  journal={arXiv preprint arXiv:2202.03423},
  year={2022}
}

@inproceedings{gu2022vision,
  title={Are vision transformers robust to patch perturbations?},
  author={Gu, Jindong and Tresp, Volker and Qin, Yao},
  booktitle={European Conference on Computer Vision},
  pages={404--421},
  year={2022},
  organization={Springer}
}

@inproceedings{gao2023backdoor,
  title={Backdoor Defense via Adaptively Splitting Poisoned Dataset},
  author={Gao, Kuofeng and Bai, Yang and Gu, Jindong and Yang, Yong and Xia, Shu-Tao},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4005--4014},
  year={2023}
}

@inproceedings{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@inproceedings{jia2022adversarial,
  title={LAS-AT: adversarial training with learnable attack strategy},
  author={Jia, Xiaojun and Zhang, Yong and Wu, Baoyuan and Ma, Ke and Wang, Jue and Cao, Xiaochun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13398--13408},
  year={2022}
}

@inproceedings{hacker2023regulating,
  title={Regulating ChatGPT and other large generative AI models},
  author={Hacker, Philipp and Engel, Andreas and Mauer, Marco},
  booktitle={Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
  pages={1112--1123},
  year={2023}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}
