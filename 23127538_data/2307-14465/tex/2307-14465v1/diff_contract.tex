\subsection{Difficulty in Contract Comprehension}
\label{sec:dcc}

In this section, we analyzed when the challenges are observed behind understanding \ML contracts. Discovering categories of \ML contracts is challenging because a significant number of \SO~posts do not contain contracts with accepted answers. Furthermore, the \SO~reputation of the user giving an answer does not necessarily determine whether an answer reveals a challenging \ML contract. To discover the correlation between \ML~contracts and user expertise in terms of the contract violation issue and to answer \RQ{3}, we have conducted an experiment that discerns respondent reliability and resolving time.  
We gather evidence from various perspectives described below to develop an educated guess. Prior work ~\cite{zhang2019empirical} analyzes the scores of \SO~post to comprehend the types of deep learning questions are more difficult. 

Inspired by that study, we have leveraged the \SO~reputation score to determine the overall expertise of a user. In our study, we slightly adapted the \SO reputation score, and named it the \emph{reliability score}. The reputation metric is often used to measure a user's expertise level on \SO, because it summarizes the overall impression of that userâ€™s \SO activity. We observe that a user can earn a reputation for various topics unrelated to \ML-related skill sets. As a result, this metric poses a significant threat when we want to assess the expertise of a user in resolving ML contracts specifically. The \emph{Reliability score} tries to mitigate this threat to an extent.  Moreover, our adapted metric incorporates the number of accepted answers into account for higher confidence in the metric.

As an example, let us say, we are interested to know if a particular user from an \SO~post\footnote{https://stackoverflow.com/users/5098368/} is an expert in~\keras. While the user has a high reputation score \emph{(26,164)}, they only have a score of \emph{6} when filtered through the \keras~tag from answering two questions. This indicates that this user has accumulated most of his reputation from other areas. So, to measure the expertise level of a user, we consider their score only on relevant tags. Since we have only included posts having \textit{accepted} answers for this study, we refined this score to prioritize users having more accepted answers, which we call their \emph{reliability score}, measured as follows:

\begin{equation*}
\resizebox{0.9\columnwidth}{!}{
	$reliabilityScore$ $=$ $totalScore\times \frac{(totalAcceptedAnswer+\mathbb{C})}{(totalAnswer+\mathbb{C})}$.
}
\end{equation*}


Since a user may have no accepted answers, which would reduce their
reliability score to 0; we add an equal constant value, $\mathbb{C} > 0$
to both numerator and denominator of accepted answer percentage of the reliability score to prioritize among authors who do not have accepted answers. Here, we have used 1 as the value of $\mathbb{C}$ in the study. For example, suppose two users $A$ and $B$ have obtained a total score of 1200 and 80 respectively by answering an equal number of questions without any accepted answers. In this case, the reliability score would be zero, had it not been for the normalization constant, and both the authors would be rated equally. As author $A$ has achieved a significantly higher score compared to author $B$ for the same number of questions answered, adding $\mathbb{C}$ to the accepted answer fraction adds priority to the author with a higher answer score.

\input{rq3tablenew}

The dataset includes \emph{average resolve time} for each type of contract, considering the time required to get accepted answers from the study and \emph{reliability score} for these respondents.

We fitted the dataset in a linear regression model first. However, it violated multiple assumptions such as linearity and normality assumptions of residuals of linear regression. Therefore, we choose the kernel ridge regression technique, a non-parametric (without any underlying assumptions about distributions of the dataset), and non-linear technique \cite{murphy2012machine}. We used radial basis function (RBF) as a kernel for fitting nonlinearity of the dataset and a gamma value of 0.1 chosen through trial-and-error analysis.
Since, in our dataset, we only included the accepted answers, the regression model predicts a minimum expected level of a user to solve the problem successfully. To that end, we use leaf contracts as features and the \emph{reliability scores} as a target variable. Considering that features are categorical data, we converted them into a one-hot encoded vector to feed into the model. Table \ref{tab:reliable} shows the expected \emph{reliability scores} and \emph{average resolve time} for a \SO~post respondent to comprehend different contracts for all \ML~libraries. For example, to respond to an intra-argument contract (IC-1) from the \tf~library, a respondent's expected  \emph{reliability score} is 5.26, and the \emph{average resolve time} is 265.33 hours. Additionally, \emph{reliability scores} are comparable for respondents within a library. Contract components with a support of less than 10\% are excluded from consideration. 

\finding{For \ML~libraries, F contracts require a higher level of expertise and a longer average time to resolve.}

A general observation is that \emph{F} contracts have respondents with comparatively higher reliability scores, ranging from 6.38 to 8.77, compared to other types of contracts. Consequently, the average resolve time for these ranges from 47 to 1080 hours (approximately). 
We reason that this difficulty is because F contracts are not as evident as G contracts, since F contracts must only eventually hold before the program terminates. 
From our dataset, it is possible to provide a benchmark for experts who can resolve F contract violations. This benchmark could be used by the \SO~forum, for example, to recommend new \ML-related posts to specific experts.
Furthermore, F contracts often rely on an implicit assumption; a significant research direction could be automating \ML~program repair tools such as DLFix \cite{dlfix} to resolve this contract violation. 

\finding{For Scikit-learn, \ML API users mostly struggle with comprehending \ML type checking.}

Surprisingly, we found that for \scikit, \ML API users mostly struggle with type checking contracts. The reliability score for this case is 10.88, and the average resolve time is 788.50 hours. We realize that \scikit provides off-the-shelf \ML~algorithms for supervised and unsupervised learning, whereas, the other DNN libraries we have chosen allow API users to implement these deep learning algorithms and neural networks. Therefore, deep neural network (DNN) \ML API users have some level of expertise towards \ML type checking compared to the API users who use higher-level \ML~libraries such as \scikit. Additionally, the DNN libraries in our study have typing rules to address type checking issues as discussed in \S{\ref{subsec:ctvs}}. There are contract-checking tools (e.g., PyContracts ~\cite{pycontracturl}) that can check simple non-\ML contracts. So, we recommend writing a similar extension tool that supports \texttt{scipy}, \texttt{CSR matrix} type checking, etc. \scikit~users can avoid type errors using such an extension tool. 
Additionally, such extensions can enforce these contracts through static or dynamic analysis.
To further verify our findings, we obtain two more measures: the average elapsed time between the post time of first response and the response that is accepted, and the percentage of time the first answer is accepted. We annotate this as \emph{average elapsed time}, and the \emph{first answer accepted} in Table \ref{tab:reliable}. A low rate of the first answer marked as accepted and higher elapsed time would generally indicate a difficulty in contract comprehension. We found that this additional evidence also confirms our finding that F contracts are usually harder to comprehend. We notice a relatively lower rate for accepting the first answer and higher elapsed time between a successful resolution and an initial attempt for the findings presented.