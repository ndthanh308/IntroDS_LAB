\subsection{Labeling}
\label{subsec:label}

The classification schemes described in \S{\ref{subsec:con}}, \S{\ref{subsec:vl}, and \S{\ref{subsec:class}} 
were used to label all 1565 collected \SO~posts. 
First, the second and the third authors with strong \ML~background, have familiarized themselves with contract literature. The authors have all studied key papers in the area of software specification and design-by-contract methods. Then we trained these two authors to understand the classification schema with the help of some example posts. In this training process, the two authors were shown multiple examples for each category in the classification schema. The examples were demonstrative of where the contract is broken for an \ML~API and how the accepted answer describes the correct usage for that precise API.
Then, each rater performed independent labeling of these posts in two iterative rounds. The 10\% sampled data analyzed for the classification coding scheme and the first iterative round of labeling served as part of the training process for the labelers.
To measure the inter-rater agreement, we have used Cohen's Kappa 
coefficient~\cite{viera2005understanding} as labeling progressed at 
1\%, 2\%, 5\%, 10\%, and continued in this fashion. 
We have followed the methodology used in prior 
works~\cite{10.1145/1062455.1062539, CHATTERJEE2020110454, islam19, islam20repairing}
to reconcile inter-rater 
disagreements at fixed intervals. 
During first iterative round, at 5\% and 10\%, we report the Kappa coefficient to be 40\% 
and 51\%, respectively. 
The low value
of the agreement directed the raters to meet more frequently (at each 2\%) for a second iterative round during the first few intervals to clarify the labels that raters were using for each post. During these meetings,
raters discussed the reasoning behind cases where a strong disagreement occurred in a moderator's presence. We continuously checked the Kappa coefficient at these intervals, 
and 
even if the Kappa value fluctuated we 
reached values over 80\% after
completing labeling 22\% of the posts for the entire dataset. 
According to ~\cite{10.1093/ptj/85.3.257}, a Kappa coefficient 
value higher than 0.80 is considered as almost perfect agreement. 

