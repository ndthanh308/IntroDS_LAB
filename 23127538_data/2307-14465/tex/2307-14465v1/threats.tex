\subsection{Threats to Validity}
\label{ssec:threat}

{\textbf{Internal threats}}. The first internal threat to the validity of our results is the classification scheme we used to identify \ML~contracts. To alleviate this threat, we have prepared the classification on top of well-established contract categories \cite{10.1109/ASE.2009.60, khairunnesa2017exploiting, 10.1145/1127878.1127884}.
We have followed an open coding scheme only to add categories novel and \ML-specific. 
The group effort to create the categories helped to make consistent choices. 

To avoid the internal threat of bias in labeling attempts after training, the labelers performed an independent study, and the Kappa coefficient was used to measure inter-rater agreement. A moderator was present during the reconciliation of disagreements between raters. \newline

{\textbf{External threats}}. 
The first external threat to validity is the reliability of the dataset we have used to conduct the empirical study. There are two sources of this threat: data source and data quality.
For the data source, we have collected our data from a popular Q\&A forum for software developers, \textit{StackOverflow} (\SO). \SO, as a forum, maintains a strict moderation policy, promotes a peer-reviewing mechanism, and incorporates a reward system for encouraging quality answers from the developers\footnote{https://stackoverflow.com/help/reputation}. Moreover, the latest software developer survey on the usage of \SO forum reveals that its users come from all walks of managerial hierarchy, countries, experiences, age groups, expertise, races, etc\footnote{https://survey.stackoverflow.co/2022/}. As such, a huge user base, an abundance of topics, and a way to benchmark the quality of the contents make SO a frequent source in many SE studies~\cite{10.1145/3180155.3180260, 10.1145/3338906.3341186, Aghajani2019SoftwareDI, Beyer2014AMC, Barua2012WhatAD, Rosen2015WhatAM, Cummaudo2020InterpretingCC}. Therefore, SO represents real-world \ML developers (\ML API users) and their concerns well and makes an ideal candidate for our study. 

Next, to ensure good quality posts, we have gathered \SO~posts
that have a high enough score \cite{islam19} in terms of questions and includes an accepted answer.

We have collected \SO~posts from four top \ML~libraries; however, the number of posts that we collected varies by the library. To measure the impact of this imbalance in the dataset, we have performed a two-tail test (inequality test) on the contract types for each library.  Here, based on the \emph{t\--Stat}, and \emph{t\--Critical\--two\--tail} values, $0.178 < 3.182$, the observed difference between the sample means is small enough 
to say that the average number of contracts obtained from the four different \ML~libraries do not differ significantly. This result indicates that even though the dataset seems unbalanced in terms of the posts' frequency, the contract distribution is not unbalanced to a statistically significant extent. Additionally, prior works have~\cite{10.1145/2884781.2884800, 10.1145/3106237.3119875} recognized \SO~as an important source to extract documentation for other domains. Multiple factors have enhanced the collected \SO post used in this study compared to these prior related works. For example,  the SO posts collected were from the years 2008 and 2021 and thus contained more recent posts than those used for the paperâ€™s submission. We also added some additional filtering criteria that suited the paper's main goal, e.g., the accepted answer post has a score higher than five and was required to contain code snippets or description that potentially describes a contract. We furthermore note that we closely followed the guidelines from prior works to conduct our study; however, there must have been some common SO posts that these previous works have studied.

Next, the nature of the methodology requires extensive manual work; thus, the number of libraries we could study is another closely related external threat. To lessen this threat, we have studied the highly-discussed four \ML~libraries based on \SO~trends since 2008. We have also observed that the number of curated posts for other \ML~libraries are less significant compared to the libraries that we have studied. For example, the libraries \emph{apache-spark-mllib} and \emph{weka} have only fourteen and two posts, respectively, for which contracts are relevant.

Furthermore, the \SO~posts are mostly about contract violations, and the answer posts talk about the needed contracts posing another external threat. If certain contracts (or violations) are not present in the dataset, our study will not find them. In essence, this is an out-of-the-vocabulary problem that is common in data mining techniques. Another possible external threat source is the need for validating findings with surveys and software developer interviews. While additional validations could raise confidence in the results, it is mitigated by the strict filtering criteria we use. We only look at the answers where at least five more users agree with the answers than those that disagree (which have been used in the past as a measure of the reliability of the answer~\cite{islam19}). Moreover, we ensure that the only accepted answers by the questioner are studied. Thus, our filtering ensures that the derived contracts reflect a consensus among the questioner, the responder (via \emph{acceptance tag}), and at least five users (minimum answer score pf 5).


Finally, we must also consider \ML API users expertise in our dataset as a threat to external validity. We have used a reliability score to mitigate this threat. Instead of using the general expertise of a software developer, the reliability score measures expertise on \ML~libraries. In short, the expertise of an user counts if they have earned the reputation from answering \ML library related questions.
