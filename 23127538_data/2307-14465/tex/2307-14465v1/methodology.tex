\section{Methodology}
\subsection{Overview}
\label{sec:data}
\input{overviewfigure}

\boxed{A} In this study, we used
Stack Overflow (\SO) 
to investigate API contracts' requirements 
for the most-asked about and widely-used \ML~libraries and frameworks.
\SO is a forum for software development professionals and enthusiasts.
In recent years \SO~has served as an open repository for conducting studies on 
software engineering topics \cite{10.1145/3180155.3180260, 10.1145/3338906.3341186, Aghajani2019SoftwareDI, Beyer2014AMC, Barua2012WhatAD, Rosen2015WhatAM, Cummaudo2020InterpretingCC}. \SO, as a forum, maintains a strict moderation policy, promotes a peer-reviewing mechanism, and incorporates a reward system for encouraging quality answers from the software developers~\cite{reputationSO}.
Moreover, it has a vibrant user community and includes software developers from all walks of life, experiences, etc.~\cite{SOsurvey}.
As a result, it offers a wealth of well-vetted information on numerous software development topics. As such, \SO makes an excellent source for our study, as the primary goal of this study is to derive ML contracts from peer-reviewed and well-vetted content for the reliability of the findings.
To capture the contracts, analyzing the large code corpus of API usages~\cite{10.1145/1287624.1287632, 10.1145/1595696.1595767, khairunnesa2017exploiting} or the implementation of the software itself~\cite{cousot2013automatic} are both well-known techniques. Our chosen methodology is closer to the former.

\boxed{B} We used the SO forum's tags to identify the relevancy of a post to an ML library; if the question's tag contained an ML library name, it was considered a post related to that library and was thus a candidate to be studied in this work.

We ranked the top \ML~libraries using the \emph{frequency of these tags},
resulting in these four as the object of our study:
\tf, \scikit, \keras, and \torch.

\boxed{C} Next we filtered these posts based on a set of defined criteria that are described in detail in \S{\ref{ssec:filterdataset}}. 

\boxed{D} The second and third authors (labelers), both with a strong background in~\ML, were given background information on contract literature. Then they were given hands-on training with sample SO posts as described in \S{\ref{subsec:label}}. 

\boxed{E} After the training process, 10\% of the filtered dataset is used by the first three authors to develop the taxonomies used to label the filtered posts. Two iterations were needed to propose the final taxonomy presented here. The process is described in detail in \S{\ref{subsec:class}}. 

\boxed{F} Next, these labelers identified contracts implicitly present in \SO~posts. We obtained 162, 122, 103, and 26 contracts, respectively, from the previously curated posts. Table \ref{tab:datasummary} shows a summary of the dataset for each library in our study. For each \SO~question, we used the taxonomy of contracts (including proposed categories)  
from \S{\ref{subsec:con}} to investigate the available information from the question 
and accepted answer to decide the type of contract obligation missing in the question 
and marked in the response. Hence, if the~\SO response describes the correct way of using an API of interest violated in the question, we identify that as a contract for the API that was implicitly present in~\SO posts. We have also used the taxonomies presented in \S{\ref{subsec:vl}} and \S{\ref{subsec:class}} to complete the labeling in this stage. In \S{\ref{subsec:con}}, we describe the process of identifying the contract violation and potential contract for \ML APIs with example \SO posts from our study. Then, the first author, with expertise (of approx. $6$ years) in contracts, reviewed the identified contracts and the~\SO post the contracts were extracted from. This served two purposes: it ensured that the identified contracts were correct and helped to reduce the threat of missing contracts that was implicitly present in the dataset from the second and third authors. The first author found only a handful of contracts missed by these two labelers. However, these missing contracts were found by at least one of the two labelers. Therefore, we did not note any new contracts this way. If one labeler identified a contract and the other did not, as they performed their labeling using the proposed taxonomy, this was identified as one of the reasons behind creating a conflict between the two labelers. We discussed in \S{\ref{subsec:label}} how we resolved the conflicts in our study.

\boxed{G} As the labeling process is completed, we analyze our labeled dataset. Additionally, we have created a separately filtered dataset (a subset of the original) based on the question scores and analyzed questions with a relatively high score (in the range of 30-339). The intuition behind further separating these posts is that an author may ask one question, and only a handful of \ML API users might run into it. Then another \SO~question may be inquired by someone but up-voted by hundreds of others who have the same problem. Thus, the intuition behind further separating these filtered posts was to understand how many \ML API users are struggling with each problem. This separate dataset was compared against the entire dataset to be vigilant about the representative issues and respective conclusions we draw from the posts in \S{\ref{sec:result}.} This subset is selected with the following criteria: select high-quality posts and keep manual efforts manageable. To that end, to ensure high-quality posts, we select posts having better than average scores (avg. 18.9). To keep the manual effort manageable, we find a trade-off between sample size and its statistical power. We specifically choose 30 as a cut-off to have reasonable confidence in this additional study while keeping manual efforts manageable (about 90\% confidence level with a 5\% margin of error), resulting in 222 posts. 
We discarded posts if they did not capture any information regarding correct usage of \ML~APIs. 
Additionally, we grouped the discarded \SO~posts that we could  not label as containing contracts 
during the manual curation step. 
We were unable to label some posts due to the following reasons: posts asking general 
clarification questions, unresolved issues with specific APIs of interest, an API unidentified 
in a post, a solution involving tuning, or a dependency between an unrelated API and 
a related API. For instance, in some of these posts, the \ML API users is usually curious about performing a task at hand or inter-library code transformation and refactoring. To illustrate, in one \SO~post\footnote{https://stackoverflow.com/questions/42480111/}, the author is knowledgeable that they can use the \texttt{summary()} API from Keras to print a model summary. Yet, they want to know how to do the same using Pytorch. Even if these posts are of interest to \ML API users, these do not fall into the category of contract requirements. In our study, the number of total unlabelled posts is 1159, and the number of total unlabelled posts with a score of or higher 30  is 161.

\boxed{H} Finally we present our result in detail in \S{\ref{sec:result}}. However, we did not add the statistics for unlabeled posts in RQs, as these posts did not unveil any \ML contracts. 

\begin{table}[htbp!]
\centering
	\caption{Dataset for Empirical Study on ML Contracts}
\begin{tabular}{|c|rrrr|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\cellcolor[HTML]{C0C0C0} & \multicolumn{4}{c|}{\cellcolor[HTML]{C0C0C0}SO \# Posts}                                  & \multicolumn{1}{c|}{\cellcolor[HTML]{C0C0C0}} \\ \cline{2-5}
\rowcolor[HTML]{EFEFEF} 
\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}Library} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Criteria \boxed{1.}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Criteria \boxed{2.}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Criteria \boxed{3.}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}Criteria \boxed{4.}} &
  \multicolumn{1}{c|}{\multirow{-2}{*}{\cellcolor[HTML]{C0C0C0}\# Contracts}} \\ \hline
\tf       & \multicolumn{1}{r|}{24368} & \multicolumn{1}{r|}{2205} & \multicolumn{1}{r|}{1400} & 605  & 162                                           \\ \hline
\scikit   & \multicolumn{1}{r|}{12506} & \multicolumn{1}{r|}{1641} & \multicolumn{1}{r|}{1127} & 551  & 122                                           \\ \hline
\keras    & \multicolumn{1}{r|}{12300} & \multicolumn{1}{r|}{1285} & \multicolumn{1}{r|}{821}  & 333  & 103                                           \\ \hline
\torch    & \multicolumn{1}{r|}{2500}  & \multicolumn{1}{r|}{439}  & \multicolumn{1}{r|}{313}  & 76   & 26                                            \\ \hline
Total                    & \multicolumn{1}{r|}{51674} & \multicolumn{1}{r|}{5570} & \multicolumn{1}{r|}{3661} & {\cellcolor[HTML]{EFEFEF}1565} & {\cellcolor[HTML]{EFEFEF}413}                                           \\ \hline
\end{tabular}%
\label{tab:datasummary}
%}
\end{table}
\subsection{Filter Dataset} 
\label{ssec:filterdataset}

We processed the collected posts further to enable a classification scheme for contracts. We followed two main steps to filter these posts. The \emph{initial step} is an \emph{automatic} pre-processing of the collected posts 
based on the following criteria: 
\boxed{1.} Within these posts, authors asking the question must include some code snippet(s). 
We reason that a question post discussing these libraries and including snippets 
of code is more likely to have difficulty with API contracts, thus
may show challenges related to contract violation for relevant APIs. %\textcolor{blue}{
Furthermore, posts with code enable identifying the \ML~APIs being used. We have collected a total of 51674 posts with this filtering criteria. \boxed{2.} We further filter the posts having a score higher than five based on 
the guidelines from prior works~\cite{6405249, 10.1145/3213846.3213866, islam19, islam20repairing} to 
ensure that posts are of high quality. This additional criterion produced a total of 5570 posts.
\boxed{3.} We considered posts with accepted answers (having a score higher than five) only, as those answer 
posts have successfully identified and resolved the problem faced 
by the author of the question post. The criterion for including accepted answers to the dataset follows prior studies~\cite{islam19, 8987482, islam20repairing} that have argued that if the answer of a post is not accepted, then that answer might not have addressed the issue. This step enabled us to collect 3661 posts in total. The steps up to this point are automatic. All queries to filter datasets according to this set of criteria are provided in the \emph{figshare} repository\footnote{\url{https://figshare.com/s/c288c02598a417a434df}}.

\boxed{4.} Furthermore, the accepted answers frequently contain code, and 
we expect that these code snippets focus on the required contracts for \ML~APIs. 

Additionally, we manually checked if the accepted answer to a post clearly describes the API contracts using text (without code). If this is true, we also consider such a post. The question posts in our study provided us with the contract violations \ML~software is susceptible to and the accepted answer posts directed us towards the contracts. Thus, the considered \SO~posts capture both contract violations and potential contracts. After these stages, we curated a total of 1565 posts; the posts
specific to each of \tf,~\scikit,~\keras,~\torch~contained 605, 551, 333, and 76 posts (Table \ref{tab:datasummary}), respectively. 

The posts obtained after manual filtering each contain at least one \ML API-related contract but may contain more. Our study observed a blend of behavioral and temporal contracts for \ML APIs. We called this a hybrid category in our classification (in \S{\ref{subsec:con}}). The posts from where we have extracted such contracts are the source behind multiple contracts from a single post. If multiple contracts were present in a single post, we extracted all contracts and labeled these using our taxonomy. For instance, in one SO post (\SO post~\ref{fig:post1}), it contained two contracts. The API in question is \texttt{random\_shuffle()} from the TensorFlow library. The first extracted contract is to specify the argument seed with the desired value. The second extracted contract is to call \texttt{tf.random\_shuffle()} and then call \texttt{tf.reset\_default\_graph()}. And the \texttt{random\_shuffle()} API will ensure a shuffled \texttt{Tensor} if the contract is maintained in either of the ways mentioned.

Besides, the \SO~forum has a general strategy to tackle duplicate questions with the same (potential) answers. Users can flag a question on~\SO if it is analogous to a previously posted question concerning a concept. According to~\SO, the reasoning behind marking duplicate questions is that users should not discuss duplicate questions, but anyone with the same query can refer to the previously posted discussions. In our study, we have found 359 unique contracts from a total of 413 contracts reported. SOâ€™s strategy for flagging duplicating questions has enabled us to collect many unique ML contracts.

We note that the forum may contain other relevant \ML API posts but not included in our dataset 
if the posts do not contain any contract or match the filtering criteria mentioned above. 
We have inspected the impact of imbalance in our dataset across libraries and address this in \S{\ref{ssec:threat}}. 

Next, we present a classification for \ML~API contracts and associated root causes 
in \S{\ref{subsec:con}}. 
This classification is used to identify and label posts with \ML~contracts. \S{\ref{subsec:class}} demonstrates the taxonomy of the effects of these root causes of contract violations. Finally, we present a classification to identify locations of \ML~API contract violation (\S{\ref{subsec:vl}}) based on \ML~pipeline stages.

\input{method-classification.tex}
\input{method-label.tex}

