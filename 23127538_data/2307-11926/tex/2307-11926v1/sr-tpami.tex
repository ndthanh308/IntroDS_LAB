\documentclass[10pt,journal,compsoc]{IEEEtran}

\usepackage[export]{adjustbox}

\ifCLASSOPTIONcompsoc
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi

\usepackage{lipsum}
\usepackage{comment}

\usepackage{amssymb,amsmath}
\usepackage{stix}
\newcommand{\farless}{\mathbin{\rotatebox[origin=c]{90}{$\Wedge$}}}


\usepackage{overpic}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.17}
\usetikzlibrary{arrows, arrows.meta}

\usepackage{multirow,makecell,colortbl,arydshln}

\definecolor{c1}{HTML}{c1272d}
\definecolor{c2}{HTML}{0000a7}
\definecolor{c3}{HTML}{eecc16}
\definecolor{c4}{HTML}{27E1C1}

\definecolor{americanrose}{rgb}{1.0, 0.01, 0.24}
\definecolor{mathblue}{HTML}{DC0000}
\definecolor{myred}{rgb}{0.753, 0.314, 0.275}
\definecolor{myblue}{rgb}{0.0, 0.24, 0.95}
\definecolor{tbl_gray}{gray}{0.85}


\newcommand\MYhyperrefoptions{bookmarks=true,bookmarksnumbered=true,
pdfpagemode={UseOutlines},plainpages=false,pdfpagelabels=true,
colorlinks=true,linkcolor={americanrose},citecolor={myblue},urlcolor={myblue},
pdftitle={Deep Hough Transform for Semantic Line Detection},%<!CHANGE!
pdfsubject={Typesetting},%<!CHANGE!
pdfauthor={Kai Zhao et al.}}%<^!CHANGE!
\usepackage[\MYhyperrefoptions,pdftex]{hyperref}

% cref must be loaded after hyperref
\usepackage{cleveref}
\crefname{equation}{Eq.}{Eq.}
\crefname{figure}{Fig.}{Fig.}
\crefname{table}{Tab.}{Tab.}
\crefname{section}{Sec.}{Sec.}

\hyphenation{op-tical net-works semi-conduc-tor}


\newcommand{\lr}{low-resolution}
\newcommand{\hr}{high-resolution}
\newcommand{\sr}{super-resolution}
\newcommand{\imsr}{image~\sr}
\newcommand{\lrim}{\lr{} image}
\newcommand{\hrim}{\hr{} image}
\newcommand{\lrims}{\lr{} images}
\newcommand{\hrims}{\hr{} images}
\newcommand{\lrhrim}{low- and high-resolution images}
\newcommand{\sota}{state-of-the-art}

% shortcuts
\newcommand{\mypar}[1]{\vspace{0.5em}\noindent\textbf{#1}.\hspace{0.2em}}
\newcommand{\kai}[1]{{\color{americanrose}{#1}}}

% math symbols
\newcommand{\stdgs}{\mathcal{N}(0, \mathbf{I})}


% \newcommand{\withauthorbio}{}
% \newcommand{\withappendix}{}


\begin{document}
\title{PartDiff: Image Super-resolution with Partial Diffusion Models}

\author{
    Kai Zhao,
    \and Alex Ling Yu Hung,
    \and Kaifeng Pang,
    \and Haoxin Zheng,
    \and Kyunghyun Sung
    \thanks{
        Submitted on \today{} for review.
        This work was supported in part
        by the National Institutes of Health R01-CA248506 and funds
        from the Integrated Diagnostics Program, Departments of Radiological
        Sciences and Pathology, David Geffen School of Medicine, UCLA.
    }\thanks{
        Kai Zhao and Kyunghyun Sung are with the
        Department of Radiological Sciences, University of California, Los Angeles
        (UCLA), CA 90095 USA.
        e-mail: \url{kz@kaizhao.net}, \url{ksung@mednet.ucla.edu},
        .
    }
    \thanks{
        Alex Ling Yu Hung, Kaifeng Pang, and Haoxin Zheng  are with the Departments
        of Radiological Sciences and Computer Science,
        University of California, Los Angeles (UCLA), CA 90095, USA.
        email: \{alexhung96@, calvinpang777@g., hzheng@mednet.\}ucla.edu
    }
}

\markboth{}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Computer Society Journals}

\IEEEtitleabstractindextext{%
\begin{abstract}
    Denoising diffusion probabilistic models (DDPMs) have achieved impressive performance on various image generation tasks,
    including \imsr.
    By learning to reverse the process of gradually diffusing the data distribution into Gaussian noise,
    DDPMs generate new data by iteratively denoising from random noise.
    %
    Despite their impressive performance, diffusion-based generative models suffer from high computational costs due to the large number of denoising steps.
    %
    In this paper, we first observed that the intermediate latent states gradually converge and become indistinguishable when diffusing a pair of
    low- and high-resolution images.
    %
    This observation inspired us to propose the Partial Diffusion Model (PartDiff),
    which diffuses the image to an intermediate latent state instead of pure random noise,
    where the intermediate latent state is approximated by the latent of diffusing the low-resolution image.
    %
    During generation, Partial Diffusion Models start denoising from the intermediate distribution and perform only a part of the denoising steps.
    %
    Additionally, to mitigate the error caused by the approximation, we introduce ‘latent alignment,’ which aligns the latent between
    low- and high-resolution images during training.
    %
    Experiments on both magnetic resonance imaging (MRI) and natural images show that,
    compared to plain diffusion-based super-resolution methods, Partial Diffusion Models significantly reduce the number of denoising steps
    without sacrificing the quality of generation.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Image \sr, Deep learning, Generative models, Diffusion models,
Magnetic Resonance Imaging.
\end{IEEEkeywords}}
% make the title area
\maketitle

\IEEEpeerreviewmaketitle


% ------------------------------------------------------------------------------------------------------------ %

\IEEEraisesectionheading{\section{Introduction}\label{sec:intro}}
Single \imsr~\cite{irani1991improving}, which aims to generate
a high-resolution (HR) raster image consistent with a low-resolution (LR) input,
is an important problem in computer vision and image processing.
%
However, it is an ill-posed challenging task
because a specific low-resolution input may correspond to multiple high-resolution outputs.
%
Many recent studies employ powerful deep neural networks (DNNs)
for \imsr{} and promising performance has been achieved.

Early DNN-based \sr{} methods train feed-forward DNNs to learn the mapping between low-
and high-resolution images~\cite{dong2015image,johnson2016perceptual} from substantial training pairs.
%
Although feed-forward networks can achieve impressive results at low upsampling factors, they cannot reproduce high-fidelity
details at high upsampling factors because of the highly complex distribution of output images
condition on the input images.
%
% Figure environment removed
%
Deep generative models, including
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative}
and Variational Auto-encoders (VAEs)~\cite{kingma2013auto}
have shown impressive results in data generation of various modalities
including image~\cite{goodfellow2014generative,karras2018progressive,brock2018large}
video~\cite{kalchbrenner2017video,brooks2022generating},
and audio~\cite{vandenoord16_ssw,prenger2019waveglow}.
%
Generative Adversarial Networks (GANs)~\cite{goodfellow2014generative} have shown impressive results in image generation
and have been applied to various conditional image generation tasks including \imsr~\cite{chen2018fsrnet,dahl2017pixel}.
%
Though generating striking images, GANs often suffer from instability in model optimization and model collapse.

% Figure environment removed

Recently, Denoising Diffusion Probabilistic Models
(DDPMs)~\cite{ho2020denoising} have shown striking
performance on image generation tasks
and have been applied to \imsr~\cite{sr3,li2022srdiff}.
%
By modeling the reverse process of gradually diffusing the data distribution
into Gaussian noise, diffusion models generate new data by iteratively
denoising from a random Gaussian noise.
%
SR3~\cite{sr3} and SRDiff~\cite{li2022srdiff} adopt diffusion models for 
\imsr{} and have shown their outstanding performance in generating
realistic high-resolution images.
%
One notorious deficiency of diffusion models is the tedious generation process which includes
thousands of denoising steps.
%
SRDiff~\cite{li2022srdiff} proposes to denoise in a low-dimensional
embedding space, but it still requires a large number of denoising steps.
%
Being slow in a generation has become the main obstacle to the practical application
of diffusion models to \sr.

Unlike unconditional image generation, where models are expected to generate
samples from pure noise,
in \imsr, 
a high-resolution output is generated from a low-resolution image input.
%
The low-resolution input exhibits similar structure, content,
and appearance as a high-resolution image,
except for certain high-frequency details that are missing.
%
This unique feature raises an interesting question:
is it necessary to denoise from a pure noise when applying diffusion models to \imsr?
%
To answer this question, we first investigate the diffusion processes
of low- and high-resolution image pairs.
%
As shown in~\cref{fig:diff-lr-hr}, we found that the intermediate latent states
of low- and high-resolution images gradually converge and become indistinguishable in the midway.
%
Based on the observation, we conjecture that the low-resolution latent states could be used as the proxy for
a high-resolution latent, and this motivates us to propose a novel sampling algorithm that only executes part of
the denoising steps.
%
As illustrated in~\cref{fig:diff-lr-hr},
our method use the intermediate latent of a \lrim{} ($x_K^{LR}$)
to substitute a latent of the high-resolution image ($x_K^{HR}$).
%
This allows us to bypass all denoising steps before $x_K^{HR}$
and accelerate the denoising process.

Although the two latent states are visually similar, there is still a subtle disparity in between
and this will inevitably have a detrimental effect on the quality of generation.
%
Therefore, we further propose the `latent alignment' that aligns the low- and high-resolution latent states to mitigate the disparity between the two.
%
In particular, we align the latent states by progressively
interpolating between low- and high-resolution latent states during training.
%
This allows a gradual shift from low-resolution to high-resolution in the latent space
and avoids the sudden disparity with the proxy.
%
In summary, the contributions of this paper are in three folds:
\vspace{-0.5em}
\begin{itemize}
    \item We qualitatively and quantitatively evaluate that the diffusion
                processes of low- and high-resolution images gradually converge in the midway
                and the intermediate latent states become indistinguishable.
    \item We are motivated to use the intermediate latent states
                of low-resolution images as the proxy for that of the high-resolution image.
                This allows us to accelerate diffusion models in training and testing by bypassing many denoising steps.
    \item We propose the `latent alignment' to mitigate the distributional disparity between
                low- and high-resolution latent states to further improve the quality of generation.
\end{itemize}
\vspace{-0.5em}
Experiments were performed on both magnetic resonance imaging (MRI) images and natural images to demonstrate that our method is able
to accelerate state-of-the-art diffusion-based \sr{} methods without incurring a significant loss in image quality.
%
~\cref{fig:sr-t2-im-face} shows some exemplar \sr{} results on different image modalities.

The rest of this paper is organized as follows:
~\cref{sec:related} summarizes the related works in \imsr{} and diffusion probabilistic models.
~\cref{sec:bg-diff} introduces the background about the denoising diffusion probabilistic models (DDPMs)~\cite{ho2020denoising}.
~\cref{sec:partdiff} elaborates on the proposed partial diffusion models (PartDiff)
and discusses the key components.
~\cref{sec:exp} presents experimental details and reports comparison results.
~\cref{sec:conclusion} makes a conclusion remark.

% ------------------------------------------------------------------------------------------------------------ %
% https://faculty.ucmerced.edu/mhyang/papers/eccv14_super.pdf
\section{Related Work}\label{sec:related}
Our method is inspired by recent works in generative models, especially diffusion-based models,
for \imsr.
%
In this section, we first briefly  introduce related works in \imsr{} with a focus on deep learning-based methods
and then cover some recent advances in diffusion models for \imsr.

\subsection{Image Super-resolution}
Image \sr{} is a one-to-many problem in which low-resolution input
may correspond to many high-resolution outputs.
%
Conventional methods for \imsr{} mainly seek to limit the solution space by leveraging prior information.
%
According to the priors, those \sr{} methods can be categorized into several types.
%
Edge-based methods generate edge-preserving \hrim s by learning image edge priors.
%
Various
edge features have been proposed, such as the depth and width of an edge~\cite{fattal2007image}
or gradient profile~\cite{sun2008image}.
%
Those methods generate \hrim s with high-fidelity edges but don't perform well on other high-frequency
structures such as textures.
%
Statistical-based methods exploit the statistical image information,
e.g. the heavy-tailed gradient distribution~\cite{shan2008fast},
the sparsity property of large gradients in generic images~\cite{kim2010single},
as prior for \imsr.
Example-based methods learn prior from exemplar images.
%
Those examples can be from external images
~\cite{freeman2002example,chang2004super},
the input image
~\cite{glasner2009super,freedman2011image}
or combined sources~\cite{yang2013fast}.


With the rapid development of deep learning techniques
in recent years, a line of deep learning-based \sr{}  approaches have
been invented with state-of-the-art performance on various benchmarks.
%
Many of the early explorations directly use convolutional neural networks (CNNs)
to regress a \hrim~\cite{dong2015image,wang2015deep,shi2016real,sajjadi2017enhancenet}.
%
Many new architectures~\cite{wang2015deep,shi2016real,li2019feedback}
and loss functions~\cite{sajjadi2017enhancenet,johnson2016perceptual} have been proposed to
improve the quality of \sr{}.
%
A recent work~\cite{chen2021learning} uses implicit representations to upsample images at an arbitrary scale.
%
Although being able to generate images close to the ground-truth,
regression-based methods tend to produce blurry images
that correlate poorly to human perception.
%
Deep generative models,
e.g., generative adversarial networks (GANs),
have shown impressive performance in generating high-fidelity realistic images
and benefited conditional tasks such as 
% \imsr~\cite{ledig2017photo,brock2018large,parmar2018image}.
\imsr~\cite{ledig2017photo,brock2018large}.
%
A number of methods have been proposed to improve GAN-based \imsr{}
in network architecture
%~\cite{ledig2017photo,brock2018large,parmar2018image},
~\cite{ledig2017photo,brock2018large},
training strategies~\cite{ledig2017photo},
and domain-specific priors~\cite{chen2018fsrnet}.
%
Although GANs provide a promising direction, they generally suffer
from common failure cases of mode collapse~\cite{arjovsky2017wasserstein} and unstable training~\cite{metz2017unrolled,heusel2017gans}.


\subsection{Diffusion Probabilistic Models}
Diffusion probabilistic models~\cite{sohl2015deep} is a class of generative models
that match a data distribution by learning to reverse a gradual noising process.
%
Diffusion models have received growing attention in recent years due to its
promising results in generating high perceptual quality samples in various data modalities
including
% image~\cite{ho2020denoising,nichol2021improved,rombach2022high},
% video~\cite{ho2022video_a},
% audio~\cite{chen2021wavegrad,kong2021diffwave},
% and text~\cite{li2022diffusionlm}.
image~\cite{ho2020denoising,nichol2021improved},
video~\cite{ho2022video_a},
audio~\cite{chen2021wavegrad},
and text~\cite{li2022diffusionlm}.
%
% Recently, Ho et al.~\cite{ho2020denoising} draw a connection
% between diffusion models and score-based generative models~\cite{song2019generative,song2020improved,song2021scorebased}
% which learns the Stein score function of the data distribution using denoising score matching.
%
%
Diffusion models have also shown impressive results in condition image generation tasks
such \imsr~\cite{sr3,li2022srdiff}.
%
Concretely, SR3~\cite{sr3} takes the low-resolution image as an additional input
to the denoising network and sets up a conditional denoising framework.
%
SRDiff~\cite{li2022srdiff} also uses the \lrim{} as the condition but executes the diffusion
process in a lower-dimensional hidden space.
%
Both SR3 and SRDiff have shown impressive \sr{} results in restoring high-fidelity
details, especially under large upsampling factors.


While showing great potential in various generation tasks, diffusion models are notoriously
slow in inference because generating high-quality samples generally need hundreds or thousands of sequential
denoising steps~\cite{ho2020denoising,nichol2021improved}.
%
Many efforts have been made to accelerate diffusion models~\cite{chen2021wavegrad,nichol2021improved}.
%
Lu et al.~\cite{lu2022dpm} propose an exact formulation that analytically computes the linear part of
the solution to diffusion ordinary differentiable equations (ODEs).
%
Chen et al.~\cite{chen2021wavegrad} condition denoising models on continuous
noise scales rather than discrete denoising steps in DDPM~\cite{ho2020denoising},
such that separate noise schedules can be used in training and testing,
allowing flexible adjustment of denoising steps in inference.
%
This method requires carefully tuned noise schedules in testing and the resulting noise schedule
is unstable in different datasets.
%
Model distillation was also introduced to reduce the denoising steps of diffusion models~\cite{salimans2022progressive}.
%
More recently, Zheng et al.~\cite{zheng2023truncated} propose to add noise not until the data become pure random noise,
but until they reach a hidden noisy data distribution that can be confidently learned.
%
Consequently, few denoising steps are required to generate data from the hidden noise distribution.
%
However, learning this hidden noise distribution is more difficult than sampling from pure noise.

Our method is very different from all those general-purpose diffusion model acceleration methods.
%
It is specifically designed for the \imsr{} task where \lrim s are given as the condition.
%
We use the intermediate latent states of \lrim{} as a proxy to that of the
\hrim{} so that fewer denoising
steps are required to recover \hrim{} from \lrim.


% ------------------------------------------------------------------------------------------------------------ %
\section{Background on Diffusion Models}\label{sec:bg-diff}
We introduce some background of diffusion models
and its application to \sr.
%
We adopt the notation in DDPMs~\cite{ho2020denoising} for both unconditional image generation
and conditional image generation for \imsr.


\subsection{Denoising Diffusion Probabilistic Models}
Diffusion models~\cite{song2019generative,ho2020denoising} transform data samples $x_0$
into Gaussian noise $x_T$ through a gradual noising process
and generate new data by learning to reverse this process.
%
The transition from data to noise is referred to as the forward process (or diffusion process),
and the opposite is called the reverse process (or denoising process).

\subsubsection{Forward process}
The forward process transforms data into Gaussian noise
by iteratively adding Gaussian noise to a clean sample $x_0$.
%
This can be formulated as a Markov process with pre-defined Gaussian transitions:
\begin{equation}
    p(x_{1:T} | x_0) = \prod_{t=1}^T q(x_t | x_{t-1}),
    \label{eq:diffusion}
\end{equation}
where
\begin{equation}
    q(x_t|x_{t-1}) := \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t\mathbf{I})
\end{equation}
is the forward Gaussian transition with pre-defined variances $\beta_t$.
%
Ideally, with large iterative steps $T$ and carefully designed $\beta_t$,
the resulting $x_T$ loses all its information and becomes pure Gaussian noise.
%
We can marginalize out the intermediate steps and sample at arbitrary timestep $t$
in a closed form:
\begin{equation}
    \begin{aligned}
        q(x_t|x_0) &= \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}}x_0, (1-\bar{\alpha}_t)\mathbf{I}) \\
        \alpha_t &:= 1 - \beta_t, \ \ \bar{\alpha}_t = \prod_{\tau=1}^t \alpha_{\tau}.
    \end{aligned}\label{eq:qxtx0}
\end{equation}
Using the reparameterization trick~\cite{kingma2013auto},
~\cref{eq:qxtx0} can be rewritten as:
\begin{equation}
        x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \ \ \
        x_0 = \frac{x_t - \sqrt{1-\bar{\alpha}_t\epsilon}}{\sqrt{\bar{\alpha}_t}},
        \label{eq:reparam_xt}
\end{equation}
where  $\epsilon\sim\mathcal{N}(0, \mathbf{I})$ is Gaussian noise.
%
$\sqrt{\bar{\alpha}_t}$ is also referred to as the `noise scale' of $x_t$.
%
Furthermore, following DDPM ~\cite{ho2020denoising},
we can derive the posterior distribution of $x_{t-1}$ given $x_T$ and $x_0$:
\begin{align}
    q(x_{t-1}|x_t, x_0) &= \mathcal{N}(x_{t-1}; \tilde{\mu}_t(x_t, x_0), \tilde{\beta}_t\mathbf{I}), \label{eq:q_xt1_xt_x0}\\
    \tilde{\mu}_t(x_t, x_0) &:=  \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha_t}}x_0 + \frac{\sqrt{\alpha}_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \label{eq:q_xt1_xt_x0_mu} \\
    \tilde{\beta}_t &:= \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t}\beta_t. \label{eq:q_xt1_xt_x0_beta}
\end{align}
The forward posterior in~\cref{eq:q_xt1_xt_x0} will be compared with the learnt
reverse posterior during training.

\subsubsection{Reverse process}
The reverse process (denoising process) learns to recover the original data $x_0$ from Gaussian noise
$x_T\sim\stdgs$ via iterative denoising.
%
It is formulated as a Markov process with learned transitions:
\begin{equation}
    p_{\theta}(x_{0:T}) = p(x_T)\prod_{t=1}^T p_{\theta}(x_{t-1}|x_t),
\end{equation}
where
\begin{equation}
    p_{\theta}(x_{t-1}|x_t) = \mathcal{N}\big(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)\big)
    \label{eq:reverse-p}
\end{equation}
is the reverse Gaussian transition with learnt mean $\mu_{\theta}(x_t, t)$ and variance $\Sigma_{\theta}(x_t, t)$.
%
Note that the variance $\Sigma_{\theta}(x_t, t)$ can be either a time-dependent constant~\cite{ho2020denoising}
or learned by a neural network~\cite{nichol2021improved},
%
and the mean $\mu_{\theta}(x_t, t)$ is parameterized by a neural network.
%
The reverse process transforms the standard Gaussian distribution $x^T\sim\stdgs$
into data distribution $p(x_0)$.

With learnt transition distribution $p_{\theta}$, to generate new image from the reverse process,
we first sample $x_T$ from standard Gaussian distribution, and then sample $x_{t-1}$ from $p_{\theta}(x_{t-1}|x_t)$
for $t = T, T-1, ..., 1$.
%
$x_0$ is the data generated from DDPMs.
%
Data generation of DDPMs is extremely time-consuming because it
involves hundreds or even thousands of evaluations of
the neural network parameterizing transition $p_{\theta}$.
%
Therefore, It is necessary to accelerate the sampling process of the DDPM for practical utilization.

\subsubsection{Optimization}
Like other latent variable generative models, such as VAE~\cite{kingma2013auto},
training DDPMs is performed by optimizing the evidence lower bound (ELBO) on
negative log-likelihood~\cite{ho2020denoising}:
\begin{equation}
    \begin{aligned}
        \mathcal{L} &= \mathbb{E}_q \log{\frac{p_{\theta}(x_{0:T})}{q(x_{1:T}|x_0)}} \\
                               &= \mathbb{E}_q \Big[
                                \underbrace{D_{KL}\big(q(x_T|x_0)  \ || \ P(x_T)\big)}_{L_T} \\
                                                &\hspace{1em} + \sum_{t>1} \underbrace{D_{KL}\big(q(x_{t-1}|x_t, x_0) \ || \ p_{\theta}(x_{t-1} | x_t) \big)}_{L_{t-1}} \\
                                                & \hspace{1em} \underbrace{-\log\big(p_{\theta}(x_0|x_1)\big)}_{L_0}
                                                                  \Big] \\
                            &= L_0 + \sum_{t>1} L_t + L_T,
    \end{aligned}\label{eq:ddpm-loss}
\end{equation}
where
\begin{equation*}
    \begin{aligned}
        L_0 &= -\log\big(p_{\theta}(x_0|x_1)\big) \\
        L_t &= D_{KL}\big(q(x_{t-1}|x_t, x_0) \ || \ p_{\theta}(x_{t-1} | x_t) \big) \\
        L_T &= D_{KL}\big(q(x_T|x_0)  \ || \ P(x_T)\big).
    \end{aligned}
\end{equation*}
We refer the readers to Appendix of~\cite{ho2020denoising} for more detailed derivations.
%
$L_0$ can be evaluated with the histogram of pixel values of images.
%
$L_T$ is independent of $\theta$ and will ideally be zero with adequate diffusion steps $N$
and proper noise schedule $\{\beta_1, \beta_2, ..., \beta_t\}$.

$L_{t-1}$ in~\cref{eq:ddpm-loss} compares the KL-divergence between estimated posterior
$p_{\theta}(x_{t-1} | x_t)$ and forward posterior in~\cref{eq:q_xt1_xt_x0}.
%
The KL-divergence between two Gaussians can be analytically expressed:
\begin{equation}
    \begin{aligned}
        L_{t-1} &= D_{KL}\big(q(x_{t-1}|x_t, x_0) \ || \ p_{\theta}(x_{t-1} | x_t) \big) \\
                      &= \frac{1}{2}\log{\frac{\Sigma_{\theta}}{\tilde{\beta}_t}} + \frac{\tilde{\beta}_t^2 + (\tilde{\mu}_t - \mu_{\theta})^2}{2\Sigma_{\theta}^2} - \frac{1}{2}. \\
    \end{aligned}\label{eq:lt1-kld}
\end{equation}

Ho et al. ~\cite{ho2020denoising} suggest to fix the variance $\Sigma_{\theta}$
to a constant value, e.g. $\Sigma_{\theta}=\sigma\mathbf{I}$.
%
After ignoring all constant variables independent of $\theta$,
~\cref{eq:lt1-kld} can be simplified as:
\begin{equation}
    \begin{aligned}
        L_{t-1} = \mathbb{E}_q \Big[\frac{(\tilde{\mu}_t - \mu_{\theta})^2}{2\sigma^2} \Big].
    \end{aligned}\label{eq:lt1-kld-simp}
\end{equation}
%
There are different ways to parameterize $\mu_{\theta}$.
%
The most straightforward option is to approximate $\tilde{\mu}_t$ with a neural network parameterized by $\theta$.
%
An alternative way is to predict $x_0$ which can be used to derive $\tilde{\mu}_t$ using~\cref{eq:q_xt1_xt_x0_mu}.

The network could also predict the noise $\epsilon$.
%
Bring ~\cref{eq:reparam_xt} into ~\cref{eq:q_xt1_xt_x0_mu}:
\begin{equation}
    \begin{aligned}
        \tilde{\mu}_t(x_t, x_0) &= \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha_t}} {\color{blue}{x_0}} + \frac{\sqrt{\alpha}_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \\
        &= \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha_t}} \cdot {\color{blue}{\frac{x_t - \sqrt{1-\bar{\alpha}_t\epsilon}}{\sqrt{\bar{\alpha}_t}}}} + \frac{\sqrt{\alpha}_t(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t \\
        &= \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon).
    \end{aligned}\label{eq:mu_xt_x0_reparam}
\end{equation}
We can use a neural network $\epsilon_{\theta}(x_t, t)$ to approximate $\epsilon$ in~\cref{eq:mu_xt_x0_reparam}
and then $\mu_{\theta}$ can be derived by:
\begin{equation}
    \mu_{\theta}(x_t, t) = \frac{1}{\sqrt{\alpha}_t}\big(x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t, t)\big).
\end{equation}
%
Under this parameterization, the objective $L_{t-1}$ becomes
\begin{equation}
    L_{t-1} = \mathbb{E}_{\epsilon, t}\big[\lVert\epsilon_{\theta}(x_t, x_0, t) - \epsilon\rVert\big]
    \label{eq:loss-discrete}
\end{equation}
where the network predicts the noise $\epsilon$.
%
Ho et al.~\cite{ho2020denoising} found predicting $\epsilon$ works the best and
the estimated noise $\epsilon_{\theta}$ is equivalent to  Stein score function~\cite{hyvarinen2005estimation,vincent2011connection}
of the log data density.
%
This connects DDPM with score-based generative models and Langevin dynamics~\cite{song2019generative,song2020improved}.
%
% drawing a connection to score-matching and Langevin dynamics~\cite{song2019generative,song2020improved}.
\begin{comment}
\mypar{Reparameterization trick} The last question is how to parameterize
$\mu_{\theta}$.
%
A straightforward way is to directly predict $\tilde{\mu}_t$ with a neural network.
%
However, we can expand~\cref{eq:lt1-kld-simp} by reparameterizing~\cref{eq:qxtx0}
as 
\begin{equation}\begin{cases}
    x_t &= \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon \\
    x_0 &= \frac{x_t}{\sqrt{\bar{\alpha}_t}} - \sqrt{1-\bar{\alpha}_t}\epsilon 
\end{cases}
\end{equation}
where $\epsilon\sim\stdgs$ is a noise sampled from standard Gaussian distribution.
%
Then, we substitute $x_0$ into~\cref{eq:q_xt1_xt_x0,eq:lt1-kld-simp}:
\begin{equation}
    \begin{aligned}
        L_{t-1} &= \mathbb{E}_{x_0, \epsilon} \Big[\frac{1}{2\sigma^2} \big\lvert\big\lvert \tilde{\mu}_t\big(x_t, x_0\big) - \mu_{\theta}\big(x_t, t\big) \big\lvert\big\lvert^2  \Big] \\
                   &= \mathbb{E}_{x_0, \epsilon} \Big[\frac{1}{2\sigma^2} \big\lvert\big\lvert {\color{mathblue}{\frac{1}{\sqrt{\alpha}_t} \big( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\big)}}- \mu_{\theta}\big(x_t, t\big) \big)  \Big\lvert\Big\lvert^2  \Big]
    \end{aligned}\label{eq:lt1-kld-mu}
\end{equation}
where $x_t = x_t(x_0, \epsilon)$ is a function of $x_0$ and $\epsilon$.
%
\cref{eq:lt1-kld-mu} reveals that $\mu_{\theta}$ must match $\frac{1}{\sqrt{\alpha}_t} \big( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon\big)$.
%
Since $x_t$ is the input to the model,
and $\beta_t, \bar{\alpha}_t$ are constant hyper-parameters, we may parameterize with:
\begin{equation}
    \mu_{\theta}\big(x_t, t\big) = \frac{1}{\sqrt{\alpha}_t} \big( x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t, t)\big).
\end{equation}
where $\epsilon_{\theta}$ is a function predicts $\epsilon$ from $(x_t, t)$.
%
With this reparameterization, ~\cref{eq:lt1-kld-mu} can be further simplified as:
\begin{equation}
    L_{t-1} = \mathbb{E}_{x_0, \epsilon} \Big[ \frac{\beta_t^2}{2\sigma_t^2\alpha_t(1-\bar{\alpha}_t)} \big\lvert\big\lvert \epsilon - \epsilon_{\theta}\big(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t\big) \big\lvert\big\lvert^2\Big]    
    \label{eq:lt1-kld-epsilon}
\end{equation}

Optimization is performed by first sampling a timestep $t$, an image $x_0$ from the dataset, and Gaussian noise $\epsilon$,
and then computing the loss defined in~\cref{eq:lt1-kld-epsilon} and updating $\theta$ through back-propagation.
% To sample $x_{t-1}\sim p_{\theta}(x_{t-1}|x_t)$ is to compute
% $$
% x_{t-1} = \frac{1}{\sqrt{\alpha_t}}\big(x_t - \frac{\beta_t}{1 - \sqrt{1-\bar{\alpha}_t}}\epsilon_{\theta}(x_t, t)\big) + \sigma_t z
% $$

% During training, we first sample $\epsilon$ from standard Gaussian and then get $\epsilon_{\theta}(x_0, t)$

% \mypar{Parameterization of $p_{\theta}$}
% Next, we discuss how to parameterize the reverse posterior $p_{\theta}$ in~\cref{eq:reverse-p}.
% %
% A straightforward way is to use two separate neural networks to estimate
% $\Sigma_{\theta}$ and $\mu_{\theta}$ given $x_t$ and $t$ as input.
% %
% However, Ho et al. ~\cite{ho2020denoising} suggest to fix the variance
% $\Sigma_{\theta} = \sigma_t^2\mathbf{I}$ works well and they empirically verified that
% both $\sigma^2_t = \beta_t$ and $\sigma^2_t = \tilde{\beta}_t$ have
% similar results.
% %
% Intuitively, we can use a fixed $\Sigma_{\theta}$ and use a neural network  to predict
% $\mu_{\theta}$ given $x_t$ and $t$ as input.
\end{comment}


\subsection{Diffusion Models Conditioned on Noise Level}
In the original DDPM~\cite{ho2020denoising},
noise schedule $\{\beta_1, ..., \beta_T\}$ and number of diffusion (or denoising) steps $N$
have to be carefully tuned to ensure high-quality data generation.
%
The noise schedule is typically determined by hyper-parameter heuristic, e.g., linear~\cite{ho2020denoising}
or cosine~\cite{nichol2021improved} decay.
%
To generate high-quality images at high resolution, $N$ has to be large enough as well.
%
For example, Ho et al. ~\cite{ho2020denoising} use $N=1,000$ to sample $256\times 256$ images.

Instead of conditioning on discrete step index $t$,
Chen et al.~\cite{chen2021wavegrad} reparameterize the model to condition on continuous noise level $\bar{\alpha}_t$.
%
This allows separate noise schedules $\{\beta_t\}_{t=1}^T$
and number of iterative steps $N$ in training and testing.
%
Hereby, the network $\epsilon$ is conditioned on noise scale and the objective in~\cref{eq:loss-discrete}
becomes:
\begin{equation}
    L_{t-1} = \mathbb{E}_{\epsilon, t}\big[\lVert\epsilon_{\theta}(x_t, x_0, t) - \epsilon\rVert\big].
    \label{eq:loss-continuous}
\end{equation}


\subsection{Conditional DDPMs for Image SR}

In \imsr{}, we are given a dataset of pairwise low- and \hrim s  $\mathcal{D} = \{(x^{LR}, x^{HR})_1, ..., (x^{LR}, x^{HR})_N\}$
and expected to learn a distribution of high-resolution images conditioned on a low-resolution input image: $p({x^{HR} | x^{LR}})$.

% Before going to the details, we first introduce some of the variables and symbols used in \imsr.
% %
% In the training phase, we use $x^{LR}$ and $x^{HR}$ to represent the low- and high-resolution
% image pair.
% %
% $x^{LR}_t$ and $x^{HR}_t$ are the latent states at timestep $t$ for \lrhrim, respectively.
% %
% Specifically, $x^{LR}_0 = x^{LR}$ and $x^{HR}_0 = x^{HR}$ are clean samples at $t=0$.
% %
% In the test phase, we use $\hat{x}^{HR}$ to denote the
% predicted high-resolution image given input low-resolution image $x^{LR}$.

Two concurrent works SR3~\cite{sr3} and SRDiff~\cite{li2022srdiff} approach this problem by adapting the 
denoising diffusion probabilistic models (DDPMs) to conditional image generation.
%
The basic idea is to use the \lrim{} as the condition for the DDPM image generation framework.
%
Under this setting, the reverse process of conditional DDPMs is:
\begin{equation}
    p_{\theta}(x_{0:T} | x^{LR}) = p(x_T)\prod_{t=1}^{T} p_{\theta}(x_{t-1}|x_t, x^{LR}).
    \label{eq:reverse-cddpm}
\end{equation}
where the reverse transition $p_{\theta}(x_{t-1}|x_t, x^{LR})$ conditions not only on denoising step $t$, but also
on \lrim{} $x^{LR}$.
%
Similarly, to generate $x^{HR}$ from $x^{LR}$, we first sample $x_T$ from Gaussian distribution and
then iteratively sample from  $p_{\theta}(x_{t-1}|x_t, x^{LR})$ for $t=T, T-1, ..., 1$.
% Let $x_0^{LR}$ and $x_0^{HR}$ be the low- and \hrim s, respectively.
% %
% The reverse process of conditional DDPMs can be reformulated as:
% \begin{equation}
%     p_{\theta}(x_{0:T}) = p(x_T)\prod_{t=1}^T p_{\theta}\big(x_{t-1}| \delta(x_t, x_0^{LR})\big),
% \end{equation}
% where $\delta$ is the `fusion' operation that fuses the input $x_t$ and condition $x^{LR}$,
% and $p_{\theta}(x_{t-1}|\delta(x_t, x_0^{LR}))$ is parameterized by a neural network,
% which is similarly to ~\cref{eq:reverse-p}.


% ------------------------------------------------------------------------------------------------------------ %
\section{Partial Diffusion Models for Image SR}\label{sec:partdiff}
In this section, we introduce our proposed DDPM-based method
for \imsr{}.
%
We first compare the diffusion process of \lrhrim{} in~\cref{sec:diff-lr-hr},
and then introduce the two key components of our proposed method,
partial diffusion and latent alignment,
in ~\cref{sec:part-diff} and~\cref{sec:aug-align}, respectively.

% % Figure environment removed

\subsection{Diffusing LR and HR Images}\label{sec:diff-lr-hr}
We first compare the diffusion process of \lrhrim.
%
Let $p(x_{1:T}^{LR} | x_0^{LR})$ and $p(x_{1:T}^{HR} | x_0^{HR})$ be the forward processes
of \lrhrim,
and $x_t^{LR}$ and $x_t^{HR}$ are the latent states.
%
The two processes start different distributions, i.e., low- and high-resolution images,
but end up with the same Gaussian distribution $\mathcal{N}(0, \mathbf{1})$.
%
We hypothesize that the two processes converge at the midway
and $x_t^{HR}$ and $x_t^{LR}$ become indistinguishable.
%
We verify our hypothesis in two aspects, qualitatively and quantitatively.

First, we visualize the diffusion processes of low- and high-resolution images.
%
As shown in~\cref{fig:diff-lr-hr}, the diffused images become visually indistinguishable
after several diffusion steps.
%
Second, we quantitatively analyze the
peak signal noise ratio (PSNR)
between high-resolution image $x_0^{HR}$ and diffused images ($x_t^{LR}$ and $x_t^{HR}$) at different
time steps.
%
We gather the results of 5,000 low- and high-resolution image pairs
and calculate the average results:
$\text{PSNR}(x_t^{LR}, x_0^{HR})$, $\text{PSNR}(x_t^{HR}, x_0^{HR})$.
%
Since the diffusion process gradually removes the information from the original image,
the PSNR can reflect how much information is remained during the diffusion process.
% Figure environment removed

We can observe from~\cref{fig:psnr} that the latent states of high-resolution images
are more informative than low-resolution counterparts at the beginning, and gradually converge to
the same level after the diffusion process.
%
The PSNR curves in~\cref{fig:psnr} become very close after about a third of the diffusion steps.
%
The observation in~\cref{fig:diff-lr-hr} and ~\cref{fig:psnr} reveal that the intermediate latent states
become barely distinguishable in the middle of the diffusion process,
which motivates us to use $p(x_t^{LR}|x_0^{LR})$ as the proxy for $q_{\theta}(x_t^{HR} | x_0^{HR})$
to accelerate data generation.

% % Figure environment removed


\subsection{Partial Diffusion Models}\label{sec:part-diff}
Based on the analysis in~\cref{sec:diff-lr-hr}, we propose the partial diffusion Models (PartDiff)
which executes only part of the denoising steps by using
$x_K^{LR}$ as a proxy of $x_K^{HR}$ in the reverse process,
where $K < T$ is an intermediate step after which
$x_K{LR}$ and $x_K{HR}$ become indistinguishable.
%
PartDiff accelerates the diffusion models by bypassing all steps with $t\ge K$.
%
Specifically, given the \lrim{} $x_0^{LR}$ as input,
we first diffuse it by $K$ steps to derive $x_K^{LR}$.
%
This can be analytically evaluated using~\cref{eq:qxtx0}:
$$
q(x_K^{LR}|x_0^{LR}) = \mathcal{N}(x_K; \sqrt{\bar{\alpha_K}}x_0, (1-\bar{\alpha}_K)\mathbf{I}).
$$

Then we use $x_K^{LR}$ as the proxy for $x_K^{HR}$
and start denoising from $x_K^{HR}$ until reach
$x_0^{HR}$, which is the generated \hrim.


\subsection{Latent Alignment}\label{sec:aug-align}
The partial diffusion introduced in~\cref{sec:part-diff} can be seamlessly integrated
into any pretrained diffusion models.
%
However, the subtle disparity between $x_K^{LR}$ and $x_K^{HR}$
may cause a slight degradation in generation quality due to approximation errors.
%
The disparity is even more noticeable with larger upsampling factors.
%
To mitigate the approximation error,
we further propose the `latent alignment'
that gradually aligns the disparity between $x_t^{LR}$ and $x_t^{HR}$.
% %
% This avoids a sudden 
% % and conpensate for the performance drop.
% %
% `Latent alignment' linearly interpolates 
% between $q(x_t^{LR}|x_0^{LR})$ and $q(x_t^{HR}|x_0^{HR})$
% during training.

For each training iteration, we first randomly sample a step-index $t\in(0, K]$
and diffuse both \lrhrim{} to derive
$q(x_t^{LR}|x_0^{LR})$ and $q(x_t^{HR}|x_0^{HR})$ according to~\cref{eq:qxtx0}.
%
Then, we linearly interpolate between them:
\begin{equation*}
    q(\hat{x}_t | x_0^{LR}, x_0^{HR}) = 
    \mathcal{N}\Big(
        \hat{x}_t; \sqrt{\bar{\alpha_t}}\big(\lambda x_0^{HR} + (1-\lambda)x_0^{LR}\big), (1-\bar{\alpha}_t)\mathbf{I}
    \Big),
\end{equation*}
where $\hat{x}_t$ is the interpolated latent and
$\lambda=\frac{K-t}{K} \in [0, 1]$
is a weight linearly increasing from 0 to 1.
%
~\cref{fig:aug-align} illustrates the latent interpolation in latent alignment.
%
Intuitively, latent alignment gradually distributes the disparity
to each step and avoids a sudden large gap at $t=K$.
% Though our model is actually conditioned on the continuous noise level
% instead of step indices, we can still compute the corresponding $t$ given an arbitrary
% noise scale $\sqrt{\bar{\alpha_t}}$.


Similar to~\cref{eq:q_xt1_xt_x0}, the forward diffusion posterior for $t<K$ is:
\begin{equation}
    \begin{aligned}
    q(\hat{x}_{t-1} | x_t, x_0^{LR}, x_0^{HR}) &= \mathcal{N}\big(x_{t-1}; \hat{\mu}_t(x_t, x_0), \hat{\beta}_t\mathbf{I}\big), \\
\end{aligned}\label{eq:qxt1_xtx0lrx0hr}
\end{equation}
where
\begin{equation*}
\begin{aligned}
    \hat{\mu}_t(x_t, x_0) &= \lambda\tilde{\mu}_t(x^{HR}_t, x^{HR}_0) + (1-\lambda)\tilde{\mu}_t(x^{LR}_t, x^{LR}_0) \\
    \hat{\beta}_t &= \tilde{\beta}_t.
\end{aligned}
\end{equation*}
and $\tilde{\mu}(\cdot, \cdot)$ and $\hat{\beta}_t$ are the mean and variance of forward posterior
defined in~\cref{eq:q_xt1_xt_x0_mu} and ~\cref{eq:q_xt1_xt_x0_beta}, respectively.
%
The interpolated posterior is used as the target distribution in training the denoising model,
and the loss term $L_{t-1}$ in~\cref{eq:ddpm-loss} becomes
$$
L_{t-1} = D_{KL}\Big(q(\hat{x}_{t-1} | x_t, x_0^{LR}, x_0^{HR}) \ || \ p_{\theta}(x_{t-1} | x_t)\Big)
$$
and the denoising model learns to gradually approach $X_0^{HR}$ from $X_K^{LR}$,
as illustrated in~\cref{fig:aug-align}.

% Figure environment removed



\begin{comment}
% Figure environment removed
\end{comment}


% ------------------------------------------------------------------------------------------------------------ %
\section{Experiments}\label{sec:exp}
In this section, we introduce our detailed implementation
and report the experimental results.
%
In~\cref{sec:impl-details}, we introduce the implementation details
in our experiments, including datasets, model architecture, and model training.
%
In~\cref{sec:exp-mri,sec:exp-natural-img}, we report experimental results on
MRI and natural images
and make qualitative and quantitative comparisons with other \sr{} methods.
%
In addition to \sr{}, in~\cref{sec:exp-downstream}, we apply
the \sr{} methods to several downstream tasks.
%
Finally, we compare our method with SR3~\cite{sr3} in~\cref{sec:comp-sr3}.


\subsection{Implementation details}\label{sec:impl-details}
All the models have been implemented with the PyTorch~\cite{paszke2019pytorch} framework.
%
We use $T=2,000$ in training all the models, and the inference denoising steps are
detailed in~\cref{sec:exp-noise-schedule}.
%
All hyper-parameters, unless otherwise stated, are kept identical with that of SR3~\cite{sr3} for fair comparisons.


\subsubsection{Network Architecture and Model Training}
We use a similar UNet architecture to SR3~\cite{sr3} which is adopted from DDPM~\cite{ho2020denoising}
with several minor modifications.
%
We change the number of residual blocks in each down-sample stage
from 3 to 2 for $64\times64\rightarrow 512\times512$ \sr{} to reduce the model size.
%
Instead of using a fixed number of groups, which is 32 in SR3,
we fix the number of channels in each group to 16.
%
Detailed network configurations for each task is summarized in~\cref{tab:netarch},
where base channels are the feature dimension in the first downsampling stage,
channels multiplier is the multiplicative factor of the feature dimension of subsequential stages,
and residual blocks are the number of residual blocks in each stage.
%

\begin{table}[!htb]
    \centering
    \caption{
        Detailed network configurations for each task.
        The number inside $(\cdot)$ denotes the target resolution.}
    \vspace{-1em}
    \resizebox{1\linewidth}{!}{
    \begin{tabular}{cccc}
        \Xhline{2\arrayrulewidth}
        Modality & \makecell{Base \\ Channels} & \makecell{Channels \\ Multiplier} & \makecell{Residual \\ Blocks} \\
        \hline
        MRI (*)   & 128 & \{1, 1, 2, 2, 4, 4\} & 3 \\
        RGB (128, 256)   & 128 & \{1, 1, 2, 2, 4, 4\} & 3 \\
        RGB (512)   & 64 & \{1, 1, 2, 2, 4, 4, 8, 8\} & 2 \\
        \hline
    \end{tabular}
    }
    \label{tab:netarch}
\end{table}

We upsample the \lrim{} to the target resolution using bicubic interpolation with anti-aliasing enabled,
and the upsampled image is concatenated with Gaussian noise $x^{HR}_T$
in the channel dimension as the input to the model.


All models are trained on a server with 4 NVIDIA RTX 3090 GPUs.
%
We use the AdamW optimizer~\cite{loshchilov2018decoupled} with a fixed learning rate of $10^{-4}$ and zero weight decay.
%
RGB and MRI image models are trained for 0.5M and 0.2M iterations, respectively.
%
The batch size on each GPU is set to 4 for all experiments except
$64\times64\rightarrow512\times512$ facial \imsr{}  where the batch size is 2 due to limited GPU memory.
%
The training recipe is adopted from previous studies~\cite{sr3,ho2020denoising}
with minor modifications to model size, batch size, and training iterations because
of our computational budget.
%
Detailed configurations are summarized in~\cref{tab:batch-iters}.
\begin{table}[!htb]
    \centering
    \vspace{-1.5em}
    \caption{
        Batch size (BS) and total training iterations for each setting.
    }
    \vspace{-1em}
    \resizebox{0.75\linewidth}{!}{
    \begin{tabular}{llcc}
        \Xhline{2\arrayrulewidth}
        \makecell{Target \\ resolution} & Dataset & \makecell{Batch \\ size} & Iters (M) \\
        \hline
        $128\times 128$   &  \multirow{3}{*}{FFHQ~\cite{karras2019style}} & 32 & 0.5 \\
        $256\times 256$ &   & 32 & 0.5 \\
        $512\times 512$  &   & 8 & 0.75 \\
        $256\times 256$  &  ImageNet~\cite{deng2009imagenet} & 32 & 1 \\
        $320\times 320$  &  Prostate MRI & 64 & 0.25 \\
        \hline
    \end{tabular}
    }
    \label{tab:batch-iters}
\end{table}



\subsubsection{Noise Schedule and Denoising Steps}\label{sec:exp-noise-schedule}
% Figure environment removed

% Figure environment removed
Follow DDPM~\cite{ho2020denoising} and SR3~\cite{sr3},
we set the forward process variances $\{\beta_t\in(0, 1)\}_{t=1}^{T}$ to constants increasing linearly from
$\beta_1$ to $\beta_T$, defined as Linear$(\beta_1, \beta_T, T)$.
%
We set $T=2,000$ and $\beta_1=5\times 10^{-5}, \beta_T=0.01$ for all experiments.

During training, we adapt the technique from~\cite{chen2021wavegrad}
and condition the model on the continuous noise scale
$\sqrt{\bar{\alpha}_t}$ rather than discrete iteration indice $t$.
%
Specifically, at each training step, we randomly sample a discrete iteration index $t$ from uniform
distribution $\mathbf{U}(1, T)$, and then
randomly sample the noise scale $\sqrt{\bar{\alpha}}$ uniformly from $\mathbf{U}(\sqrt{\bar{\alpha}_t}, \sqrt{\bar{\alpha}_{t-1}})$.
%
By conditioning on continuous noise scales, the mode is more flexible in
a number of diffusion steps and the noise schedule during inference.



During inference, we set $T=100$
and manually search a linear noise schedule.
%
Concretely, with a pretrained model, we do a grid search on $\beta_1$ and $\beta_T$ on the test set
to find a proper noise scale.
%
Consequently, the inference noise schedule used in our experiment is Linear$(10^{-5}, 0.213, 100)$.
%
The noise schedules in training and inference are plotted in~\cref{fig:noise-scale}.



Given a specific inference noise schedule Linear$(\beta_0, \beta_T, T)$,
PartDiff can easily bypass some of the denoising steps by setting up a noise scale
threshold.
%
For example, with inference noise schedule Linear$(10^{-5}, 0.213, 100)$
({\color{blue}{blue}} curve in~\cref{fig:noise-scale}),
we set the threshold to $\sqrt{\bar{\alpha}_{K=50}}=0.25$ and this reduces the denoising
steps from 100 to 50.
%
Note that $K$ is closely related to image modality and upsampling factors.
%
In general, high-upsampling factors and images with high-frequency details
require a larger $K$ (and more denoising steps).

% For example, we can use the threshold $\sqrt{\bar{\alpha}_K}=0.25$
% and bypass all denoising steps $t$ with $\sqrt{\bar{\alpha}_t}<0.25$ (dotted lines in~\cref{fig:noise-scale}).
% %
% Compared to SR3~\cite{sr3}, PartDiff reduces the denoising steps in half.

% Let $\mathbb{T}$ be the set of denoising steps being executed during inference.
% %
% When conditioned on discrete iteration indices, our method bypass denoising steps
% larger than a specific step: $\mathbb{T} = \{t, t\le K\}$.
% %
% When conditioned on continuous noise scale $\sqrt{\bar{\alpha}}$,
% our method bypasses denoising
% steps bolow a specific noise scale: $\mathbb{T} = \{t, \sqrt{\bar{\alpha}_t} \ge \sqrt{\bar{\alpha}_K}\}$.






\subsubsection{Data and Data Preprocessing}
We test our method on clinical MRI images and natural images.
%
We crop the images into squares and then resize them into proper sizes before feeding them into the model.
%
The training images are randomly cropped along the longer edge and test images are center cropped.
%
A random left-right flip is performed for data augmentation.
%
SR3~\cite{sr3} suggests applying varying amounts of Gaussian filters to the low-resolution images
during training and proves it improves the quality of generation significantly.
%
We adopt this trick and the sigma of the Gaussian kernel is randomly chosen from
$(0\sim 1.5)$.
% ~\footnote{we use the \href{https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.ndimage.gaussian_filter.html\#scipy.ndimage.gaussian_filter}{scipy.ndimage.gaussian\_filter}}.

\mypar{Prostate MRI}
For MRI, we experiment on two datasets:
1) our in-house prostate MRI dataset, and
2) the publically available prostateX dataset from the cancer imaging archive (TCIA)~\cite{clark2013cancer}.
%
Our in-house dataset consists of T1- and T2-weighted images from 871 patients.
%
We randomly select 200 patients for testing and use other patients for training.
%
We collect all slices of different scan planes (axial, sagittal, and coronal)
and totally we have 18,813 T1-weighted slices and  17,445 T2-weighted slices.
%
The prostateX dataset consists of 347 patients in total. 
203 patients are randomly selected for the test, and the rest of the patients are used as a training set.
%

% Figure environment removed

\mypar{Natural images}
In addition to MRI, we test our method on two kinds of natural images:
facial images and the ImageNet~\cite{deng2009imagenet}.
%
Following SR3~\cite{sr3},
we train facial \imsr{} models on Flickr-faces-HQ (FFHQ)~\cite{karras2019style}
dataset and evaluate on the CelebA-HQ~\cite{karras2018progressive} dataset.
%
The upsampling scale is set to $8\times$ to test the performance under radical situations.
%
We test two different resolutions: $16\times16\rightarrow128\times128$ and $64\times64\rightarrow512\times512$.
%
The ImageNet dataset has more than one million images of 1,000 categories.
%
We use its training set model training and test on its validation set.
%
We perform $\times4$ ($64\times64\rightarrow256\times256$) \sr{} on this dataset.

\subsection{Experiments on Clinical MRI}\label{sec:exp-mri}
We first test our method on clinical prostate MRI scans.
%
%We use the turbo spin echo (TSE) imaging, a standard imaging sequence for a wide range of clinical applications due to high in-plane resolution (e.g., 0.3-1 mm) with sufficient image contrast.   
%A stack of multiple slices of 2D images is commonly used for TSE imaging to achieve volumetric coverage, which is limited by low through-plane (e.g., 3-6 mm) resolutions.
%%
Let $(h\times w\times d)$ be the shape of a multiple slices of 2D MRI images where  $(h\times w)$ is the in-plane
resolution and $d$(epth) is the number of slices.
%
We perform \sr{} on two different settings:
1) in-plane \sr{} that improves the resolution in the $(h\times w)$ plane
and 2) through-plane \sr{} that improves the resolution in the $d$-axis.

\subsubsection{In-plane MRI Super-resolution}
We use each 2D MRI image as a training sample and downsample
the images to get high- and low-resolution pairs for in-plane  MRI \sr.
%
We conduct MRI \sr{} on both T1- and T2-weighted images
as they are often used to identify prostate cancers
following the clinical guideline ~\cite{Turkbey2019PIRADS}.
An example of the prostate cancer lesion is shown in ~\cref{fig:t2w-idx}
as asymmetrical hypointensity areas in the T2-weighted images.



\mypar{Qualitative comparisons}
Some visual results are shown in~\cref{fig:t1w-idx,fig:t2w-idx}.
%
~\cref{fig:t1w-idx} shows the \sr{} results on $80\times80\rightarrow320\times320$ \sr
of T1-weighted MRI images with zoomed-in patches.
%
SRGAN~\cite{ledig2017photo} generates decent outputs, but the contrast is obviously
different from the high-resolution images (see zoomed-in patches).
%
Both LIIF~\cite{chen2021learning} and SRGAN~\cite{ledig2017photo} generate blurry results without high-frequency textures.
%
Our method generates realistic textures and the overall contrast is the closest
to high-resolution references compared to other approaches.

~\cref{fig:t2w-idx} presents some \sr{} results on $160\times160\rightarrow320\times320$ \sr{} of T2-weighted MRI images.
%
The prostate area is marked by a blue square and enlarged to visualize the detailed prostate cancer lesion inside the prostate.
%
Prostate tumors are marked with red arrows.
%
Both the GAN-based method (SRGAN) and implicit representation-based method (LIIF)
deteriorate the contrast between the lesion and normal tissue, making the lesion less
clear in the image.
%
Our method preserves the contrast the most, and the lesion is more evident in the image than the other methods.


\mypar{Quantitative performance}
~\cref{tab:prostatex} and ~\cref{tab:idx} summarize the quantitative performance on ProstateX and
our in-house prostate MRI datasets, respectively.
%
We use the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)
to measure the similarity between upsampled images and high-resolution references.
%
As shown in ~\cref{tab:prostatex} diffusion-based methods outperform other methods
and the performance gap is even more clear with larger upsampling factors.
%
Partial diffusion models achieve very close performance to SR3 with significantly fewer denoising steps.%
%
From ~\cref{tab:idx} we can see that the PSNR and SSIM are generally higher in T1-weighted images
than T2-weighted images.
%
This is probably because T1-weighted images are less noisy than T2-weighted images,
as shown in~\cref{fig:t1w-idx,fig:t2w-idx}.
%
Diffusion-based methods achieve state-of-the-art performance on both T1-weighted
and T2-weighted images and our method performs on par with SR3 with less
computational footprint.

% \mypar{Consistency to low-resolution images}


% Figure environment removed

% Figure environment removed

% We can see in ~\cref{fig:t1w-idx} that results of GAN-based method, e.g. SRGAN~\cite{ledig2017photo},
% are blurry and miss the texture details.
% %
% Implicit representation-based method LIIF~\cite{chen2021learning} generates smooth
% textures with blurry boundaries.
% %
% Our generates images with more visual pleasing tissue contrast and realistic textures.
% %
% In ~\cref{fig:t2w-idx} we show some \sr{}  results on T2-weighted
% prostate MRI images.
% %
% The lesions are marked with red arrows.
% %
% As shown in~\cref{fig:t2w-idx},
% compared to SRGAN~\cite{ledig2017photo} and LIIF~\cite{chen2021learning},
% the contrast is better preserved
% and the lesion is more clear in our results.
%




% \begin{table}[!htb]
%     \centering
%     \caption{
%         In-plane MRI \sr{} results on ProstateX dataset.
%     }
%     \vspace{-1em}
%     \newcommand{\CC}{\cellcolor{gray!15}}
%     \setlength{\tabcolsep}{1.6pt}
%     \resizebox{0.55\linewidth}{!}{
%     \begin{tabular}{lccc}
%         \Xhline{2\arrayrulewidth}
%         Methods & & PSNR ($\uparrow$) & SSIM ($\downarrow$) \\
%         \hline
%         Bicubic & \multirow{3}{*}{$\times2$} & 28.9638 & 0.8424 \\
%         Regression \\
%         SRGAN~\cite{ledig2017photo} & & 33.3979 & 0.9135 \\
%         SMORE~\cite{zhao2020smore}  \\
%         LIIF~\cite{chen2021learning}  \\
%         \CC SR3 ($T=100$) \\
%         \CC PartDiff ($K=25$) \\
%         \CC PartDiff ($K=50$) \\
%         \hline
%         Bicubic & \multirow{3}{*}{$\times4$} & 24.6771 & 0.6808 \\
%         Regression \\
%         SRGAN~\cite{ledig2017photo} & & 26.9268 & 0.7503 \\
%         SMORE~\cite{zhao2020smore}  \\
%         LIIF~\cite{chen2021learning}  \\
%         \CC SR3 ($T=100$) \\
%         \CC PartDiff ($K=25$) \\
%         \CC PartDiff ($K=50$) \\
%     \end{tabular}
%     }
%     \label{tab:prostatex}
% \end{table}

\begin{table}[!htb]
    \centering
    \vspace{-1em}
    \caption{
        In-plane MRI \sr{} results on ProstateX dataset.
    }
    \vspace{-1em}
    \newcommand{\CC}{\cellcolor{gray!15}}
    \setlength{\tabcolsep}{1.6pt}
    \resizebox{0.8\linewidth}{!}{
        \begin{tabular}{lcc|cc}
            \Xhline{2\arrayrulewidth}
             & \multicolumn{2}{c|}{$\times2$} & \multicolumn{2}{c}{$\times4$} \\
            Method & PSNR ($\uparrow$) & SSIM ($\downarrow$) & PSNR ($\uparrow$) & SSIM ($\downarrow$) \\
            \hline
            Bicubic  & 32.0775 & 0.9226 & 24.6771 & 0.6808 \\
            Regression & 32.7084 & 0.9109 & 25.3750 & 0.7112 \\
            SRGAN~\cite{ledig2017photo} & 33.3979 & 0.9135 & 26.9268 & 0.7503 \\
            % SMORE~\cite{zhao2020smore}  \\
            LIIF~\cite{chen2021learning} & 33.4791 & 0.9270 & 27.9112 & 0.7929 \\
            \CC SR3 ($T=100$) & \CC\textbf{33.5382} & \CC0.9301 & \CC 28.2462 & \CC\textbf{0.8101} \\
            \CC PartDiff ($K=25$) & \CC 33.5194 & \CC 0.9287 & \CC 28.2329 & \CC 0.8002\\
            \CC PartDiff ($K=50$) & \CC 33.5318 & \CC 0.9295 & \CC \textbf{28.2463} & \CC 0.8093\\
            \Xhline{2\arrayrulewidth}
        \end{tabular}
    }\label{tab:prostatex}
\end{table}

\begin{table}[!htb]
    \centering
    \vspace{-1em}
    \caption{
        Quantitative performance of in-plane MRI
        \sr{} results on our in-house prostate MRI dataset.
    }
    \vspace{-1.2em}
    \newcommand{\CC}{\cellcolor{gray!15}}
    \setlength{\tabcolsep}{1.2pt}
    \resizebox{0.85\linewidth}{!}{
    \begin{tabular}{lc|cc|cc}
        \Xhline{2\arrayrulewidth}
        \multirow{2}{*}{Method} & & \multicolumn{2}{c|}{T1W} & \multicolumn{2}{c}{T2W}\\
        & & PSNR ($\uparrow$) & SSIM ($\uparrow$) & PSNR($\uparrow$) & SSIM ($\uparrow$) \\
        \hline
        Bicubic & \multirow{6}{*}{$\times2$} & 33.3657 & 0.7509 & 29.5263 & 0.9091 \\
        Regression & & 34.4634 & 0.9265 & 32.7347 & 0.9035 \\
        SRGAN~\cite{ledig2017photo} & & 36.0610 & 0.9434 & 36.2626 & 0.9365 \\
        % SMORE~\cite{zhao2020smore} & & 36.7349 &  0.9571 & 36.9507 & 0.9407 \\
        LIIF~\cite{chen2021learning}  & & 37.3901 & 0.9524 & 35.8739 & 0.9433\\
        SR3~\cite{sr3} ($T=100$) & & \CC \textbf{37.5087} &  \CC 0.9670 & \CC \textbf{36.3017} & \CC \textbf{0.9413} \\
        PartDiff ($K=25$) & & \CC 37.5109 &  \CC 0.9672 & \CC 36.2859 & \CC 0.9411 \\
        PartDiff ($K=50$) & &  \CC  37.5071 &  \CC \textbf{0.9672} & \CC 36.2770 & \CC 0.9411 \\
        \hline
        Bicubic & \multirow{6}{*}{$\times4$} & 26.5617 & 0.8278 & 24.9121 & 0.6999 \\
        Regression & & 29.0838 & 0.8095 & 27.3084 & 0.7199\\
        SRGAN~\cite{ledig2017photo} & & 30.2013 &  0.8444 & 29.6069 & 0.7443 \\
        LIIF~\cite{chen2021learning}  & & 31.3574 & 0.8647 & 30.2751 & 0.8261 \\
        % SMORE~\cite{zhao2020smore} & & 30.2113 &  0.8576 & 30.0111 & 0.8021 \\
        SR3~\cite{sr3} ($T=100$) & & \CC 31.5104 &  \CC \textbf{0.8656} & \CC \textbf{30.4349} & \CC 0.8348 \\
        PartDiff ($K=25$) & & \CC 31.3581 & \CC 0.8607 & \CC 30.3109 & \CC 0.8301 \\
        PartDiff ($K=50$) & & \CC \textbf{31.5149} &  \CC 0.8617 & \CC  30.1342 & \CC \textbf{0.8156} \\
        \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \label{tab:idx}
    \vspace{-1.2em}
\end{table}


\subsubsection{Through-plane MRI Super-resolution}\label{sec:exp-through}
Multi-slice 2D MRI images have an anisotropic resolution due to the
lower through-plane resolution.
%
For example, in our prostate MRI dataset, the in-plane pixel spacing
(the physical distance between two pixels) is 0.625 mm, while the slice spacing
(the physical distance between two slices) is 3.6 mm.
%
In this experiment, we test our method in improving the through-plane resolution of MRI,
where the upsampling scale is set to $6=0.36/0.625$.

To construct low- and high-resolution training pairs,
we use two distinct scans acquired in orthogonal planes, e.g., axial scan and coronal scan,
during training and testing.
%
The two orthogonal scans are illustrated in~\cref{fig:cor-ax}.
%
Let anterior$\leftrightarrow$posterior (AP) % qian hou
left$\leftrightarrow$right (LR) and % zuoyou
superior$\leftrightarrow$inferior (SI) % shang xia
be the anatomical directions in 3D space.
%
Axial scan captures 2D slices in the AP and LR plane, and coronal scan captures slices in
SI and LR plane.
%
Let $h\times w\times d$ be the shape of MRI data where $h\times w$ is the in-plane image size
and $d$ is the number of slices.
%
In our data, $h=w=320$ and $d=20$.
%
During training, we collect $h\times w$ in-plane slices from the 
coronal scan 
as the high-resolution image,
and downsample those slices along the $h$ dimension (SI) to simulate
the low through-plane (SI direction) in the axial scan.
%
The models learn to \sr{} along the SI direction
by training with the data pairs.
%
During test, the model takes as input the $w\times d$ through-plane slices of the
axial scan and performs \sr{} in the $d$ dimension (SI).

% Figure environment removed

% Figure environment removed

Example through-plane MRI \sr{} results of the axial scan are shown in~\cref{fig:through-plane}.
%
There is no such `ground-truth' in through-plane \sr,
we use the in-plane slice from a separate acquisition of
the coronal scan as the visual reference.
%
The visual references are not necessarily perfectly aligned with the \sr{} results due to
potential patient motion between the two scans.
%
Note that we use an average of 6 ($\approx 3.6/0.625$) consecutive through-plane
images as the input to \sr{} models to compensate for the differences
in pixel spacing (0.625 mm) and the slice thickness (3.6 mm).
%
As shown in~\cref{fig:through-plane}, SRGAN generates blurry results which do not contain much
texture details.
%
LIIF and our method generate much better high-frequency details and are generally aligned better
with the visual reference.


% Figure environment removed


\subsection{Experiments on Natural Images}\label{sec:exp-natural-img}

\subsubsection{ImageNet}\label{sec:exp-imagenet}
We apply our method to $64\times64\rightarrow256\times256$ \sr{} on
the challenging ImageNet~\cite{deng2009imagenet} dataset with
over one million images of one thousand categories.
%
All models are trained with the training set and evaluated on the test set.
%
During training, we randomly crop a square image along the longer edge,
and test images are center cropped.
%
~\cref{fig:imagenet} compares PartDiff with other \sr{} methods including
GAN-based, Flow-based, and diffusion-based methods.
%
SRGAN~\cite{ledig2017photo} tends to generate blurry patterns with less detailed textures.
%
ESRGAN~\cite{wang2018esrgan}
improve the details to some extent but there is still an obvious gap with
real high-resolution images.
%
SR3 and PartDiff generate highly realistic textures that are close to real images.



We quantify the performance on ImageNet in two aspects:
1) consistency with low-resolution inputs and
2) reality of generated images.
%
To quantify the consistency, we compute the MSE between the downsampled outputs and low-resolution inputs.
%
The reality is measured by FID and inception score (IS).
%
~\cref{tab:imagenet-fid-is} summarize the quantitative comparisons on the ImageNet dataset.
\begin{table}[!htb]
    \centering
    \vspace{-1.2em}
    \caption{
        Quantitative results on ImageNet dataset~\cite{deng2009imagenet}.
    }\vspace{-1em}
    \newcommand{\CCa}{\cellcolor{gray!10}}
    \newcommand{\CCb}{\cellcolor{c2!10}}
    \resizebox{0.8\linewidth}{!}{
    \begin{tabular}[!htb]{lccc}
        \Xhline{2\arrayrulewidth}
        Model & Consist $\downarrow$ &  FID $\downarrow$ & IS $\uparrow$ \\
        \hline
        Reference & - & 1.9 & 240.8 \\
        Regression & 3.11 & 16.1 & 119.7 \\
        SRGAN & 27.7 & 13.9 & 162.5 \\
        \hline
        SR3 & 3.12 & 6.1 & 174.7 \\
        PartDiff ($K=25$) & 3.19 & 7.2 & 167.4 \\
        PartDiff ($K=50$) & 3.13 & 6.3 & 172.9 \\
        PartDiff ($K=75$) & 3.12 & 6.1 & 174.6 \\
        \hline
    \end{tabular}
    }
    \label{tab:imagenet-fid-is}
    \vspace{-1.5em}
\end{table}
As shown in~\cref{tab:imagenet-fid-is},
diffusion-based methods significantly outperform others in terms
of FID and IS scores, revealing that diffusion models generate highly realistic images.
%
All the methods achieve similar consistency with the low-resolution input,
but SR3 and PartDiff outperform others by a slight margin.
%
Our reimplemented SR3 performs slightly worse but still outperforms other competitors.
%
PartDiff achieves 157.4 in IS score and 11.2 in FID using only 25 denoising steps.
%
Using 50 denoising steps, PartDiff surpasses GAN-based methods by a significant margin.
%
To achieve similar performance, SR3 requires 100 denoising steps.


\subsubsection{Facial Images}
~\cref{fig:face128} and ~\cref{fig:face512} show some results
on $16\times16\rightarrow128\times128$ and $64\times64\rightarrow512\times512$ face \sr, respectively.
%
The upsampling scale is set to $8$ to show the generation ability of diffusion models.
%
As shown in~\cref{fig:face128,fig:face512}, regression-based approach produce
blurry output images without fine details.
%
We found that both implicit representation-based methods, i.e. LIIF~\cite{chen2021learning},
and GAN-based method, i.e. SRGAN~\cite{ledig2017photo}, generate visually unpleasing
results under this setting.
%
In contrast, diffusion-based methods generate visually pleasing images with highly-realistic details  such as hair
and eyes (see the zoomed-in patches).
%
PartDiff generates almost the same image quality as SR3 does but with fewer denoising steps.
%
\ifdefined\withappendix
More visualization results can be found in the Appendix.
\fi


Many previous works~\cite{chen2018fsrnet,dahl2017pixel,sr3} have pointed out that
PSNR and SSIM do not correlate well with human perception when the generated images
contain a large volume of high-frequency details.
%
We quantify the performance of facial \imsr{} in two aspects:
1) perceptual reality and
2) consistency to low-resolution input.
%
The Learned Perceptual Image Patch Similarity (LPIPS)~\cite{zhang2018perceptual} LPIPS can well judge the perceptual similarity between two images
and is used to measure perceptual reality.
%
We compute the MSE between the downsampled outputs and the low-resolution inputs
as the consistency metric.
\begin{table}[!htb]
    \centering
    \vspace{-1em}
    \caption{
        Quantitative results on facial image \sr.
    }
    \vspace{-1.2em}
    \newcommand{\CCa}{\cellcolor{gray!10}}
    \newcommand{\CCb}{\cellcolor{c2!5}}
    \resizebox{0.98\linewidth}{!}{
    \begin{tabular}[!htb]{lcc|cc}
        \Xhline{2\arrayrulewidth}
        & \multicolumn{2}{c|}{$16\times16\rightarrow128\times128$} & \multicolumn{2}{c}{$64\times64\rightarrow512\times512$} \\
        Model & Consist $\downarrow$ & LPIPS$\downarrow$ & Consist $\downarrow$ & LPIPS$\downarrow$  \\
        \hline
        % Reference & 0 & 1.9 & 240.8 \\
        Regression & 2.71 & 0.635 & 2.63 & 0.4981 \\
        SRGAN~\cite{ledig2017photo} & 127.1 & 0.210 & 131.8 & 0.2821 \\
        % PULSE & 161.1 & - \\
        ESRGAN & 158.2 & 0.174 & 164.2 & 0.2795 \\
        % FSRGAN & 33.8 & 11.2 & 159.4 \\
        \hline
        SR3 & 2.68 & 0.0799 & 2.61 &  0.2175 \\
        \CCb PartDiff ($K=25$) & \CCb 2.71 & \CCb 0.1491 & \CCb 2.69 & \CCb 0.2217 \\
        \CCb PartDiff ($K=50$) & \CCb 2.70 & \CCb 0.1299 & \CCb 2.64 & \CCb 0.2173 \\
        \hline
    \end{tabular}
    }
    \label{tab:face}
    \vspace{-0.2em}
\end{table}
As shown in~\cref{tab:face}, diffusion-based methods reduce the consistency error from hundreds
to less  than 3 on both $16\times16\rightarrow128\times128$ and $64\times64\rightarrow512\times512$ \sr,
and improve the LPIPS score as well.
%
With 25 denoising steps, partial diffusion achieves comparable performance with GANs.
%
Using 50 denoising steps, partial diffusion almost matches the performance of SR3
with 100 denoising steps.


\subsection{Comparison to SR3}\label{sec:comp-sr3}
In this experiment, we compare our method with SR3~\cite{sr3}.
%
As pointed out by many previous works~\cite{chen2018fsrnet,dahl2017pixel,sr3},
automated evaluation metrics, e.g., PSNR and SSIM, do not correlate
well with human perception when the generated images contain a large volume of
high-frequency details.
%
To capture the subtle differences between PartDiff and SR3,
we use human evaluation to compare the image quality.
%
For each dataset, we randomly select 500 test samples for human evaluation
because full-dataset evaluation on these datasets is time-consuming.
%
We adopt the two alternative force choice (2AFC) paradigm
where  human subjects are shown a low-resolution image and two shuffled \sr{}
results, and forced to select a result of `high-quality'.
%
Then we calculate the fool rate which is the percentage of PartDiff results being
selected.
%
The fool rate reflects how realistic PartDIff generated images are.
%
Theoretically, if PartDiff perfectly approximates SR3, 
the fool rate will be infinitely close to 50\%.
%
% Figure environment removed
The fool rates in ~\cref{fig:fool-rate} consistently improve with the raising of denoising steps.
%
And the fool rate and the upsampling scale are inversely proportional:
under the same denoising steps, large upsampling factors lead to low fool rates.
%
This is not surprising because, as analyzed in~\cref{sec:diff-lr-hr},
the gap between the latent states of low and high-resolution images
become more significant with larger upsampling factors,
and such a gap harms the accuracy of using the low-resolution latent states
as the proxy for high-resolution latent states.
%
In general, the results in~\cref{fig:fool-rate} prove that PartDiff is able
to generate samples of almost  the same quality as SR3 with less
denoising steps.


\subsection{Application to Downstream Tasks}\label{sec:exp-downstream}
In this experiment, we apply \sr{} outputs to downstream tasks.
%
We test 3 tasks:
1) zonal segmentation on T2-weighted prostate MR images,
2) face recognition on LFW~\cite{huang2008labeled} dataset, and
3) image classification on ImageNet~\cite{deng2009imagenet} dataset.
%
Models for each task are pretrained with high-resolution images.
%
During testing, we first downsample the test images,
and then upsample them using different
\sr{} approaches.
%
The upsampled images are finally used as test images as the input to downstream
models.
%
Under this setting, models might achieve higher performance if it generates
samples that match the distribution of real images.

\subsubsection{Prostate Zonal Segmentation}\label{sec:zonal}
Prostate zonal segmentation is an important step in automatic prostate cancer detection,
and a suspicious lesion should be analyzed differently in different prostate zones,
%
due to variations in image appearance and cancer prevalence~\cite{israel2020multiparametric}.
%
In this experiment, we test the performance of zonal segmentation using images upsampled
with different methods.
%
We use a pretrained model~\cite{hung2022cat} which segments prostate into
peripheral zone (PZ) and transition zone (TZ).
%
% Figure environment removed
%
~\cref{fig:zonal} compares the dice coefficients of segmentation results
under various upsampling factors
and ~\cref{fig:zonal-seg} shows some zonal segmentation results
of images upsampled by $\times4$.
% Figure environment removed

Our method consistently achieves higher segmentation performance
under various upsampling factors, and the performance gap is becoming
wider at large upsampling factors.
%
Interestingly, although our upsampled images are visually much more realistic
than SRGAN, the improvements in segmentation are not
as noticeable as the visual differences.
%
This reveals that visual realism is not well correlated with segmentation
quality.

\subsubsection{Face Recognition}
We use ArcFace~\cite{deng2019arcface} face recognition model as the baseline to evaluate
the performance of various \sr{} methods on face recognition.
%
The baseline model is pretrained on MS1MV2~\cite{guo2016ms} dataset and the performance
is evaluated on the LFW~\cite{huang2008labeled} dataset.
%
We use the same preprocessing protocol as ~\cite{deng2019arcface} to align and crop the test images.
%
The test images are cropped into $112\times 112$ patches.
%
These patches are first downsampled by a factor of 4 with bicubic interpolation,
and then upsample to the original resolution using different \sr{} approaches.
%
All the \sr{} models are trained on FFHQ dataset~\cite{karras2019style}.
\begin{table}[!htb]
    \centering
    \vspace{-0.5em}
    \caption{
        Super-resolution applied to face recognition on the LFW dataset.
        The test images are first bicubically downsampled ($\times4$) 
        and then upsampled by different \sr{} approaches.
    }
    \vspace{-1em}
    \resizebox{0.58\linewidth}{!}{
    \begin{tabular}{c|c}
        \Xhline{2\arrayrulewidth}
        Method & LFW Acc\\
        \Xhline{0.6\arrayrulewidth}
        ArcFace~\cite{deng2019arcface} (baseline) & 99.43 \\
        Bicubic & 88.72 \\
        Regression & 89.75 \\
        SRGAN~\cite{ledig2017photo}  & 89.71 \\
        \hline
        SR3~\cite{sr3} ($T=100$) & \textbf{91.01} \\
        PartDiff ($K=25$) & 89.88 \\
        PartDiff ($K=50$) & 90.85 \\
        PartDiff ($K=75$) & \textbf{91.01} \\
        \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \label{tab:face-reco}
\end{table}
As shown in ~\cref{tab:face-reco},
the regression-based model performs slightly better than bicubic interpolation,
and SRGAN performs even worse than bicubic interpolation.
%
Diffusion-based methods surpass all other methods and reach the highest accuracy
of 91.01\%.
%
The proposed method achieves the accuracy of 89.97\% and 90.00\% with 25 and 50 denoising steps,
which is very close to the accuracy of SR3 with 100 steps.

\subsubsection{Image Classification}
We test the classification accuracy of \sr{} outputs
on the ImageNet dataset.
%
We use a pretrained ResNet-50 model as the baseline.
%
We perform $4\times$ \sr{} on the center-cropped $56\times56$ validation
images.
%
Classification accuracy on the ImageNet validation set is
summarized in~\cref{tab:img-cls}.
\begin{table}[!htb]
    \centering
    \vspace{-1em}
    \caption{
        Imagenet classification accuracy of a pretrained ResNet-50 network
        on the validation set.
        %
        All test images are bicubically downsampled ($\times4$) and then upsampled
        by different models.
    }
    \vspace{-1em}
    \resizebox{0.70\linewidth}{!}{
    \begin{tabular}{lcc}
        \Xhline{2\arrayrulewidth}
        Method & Top-1 Acc & Top-5 Acc \\
        \hline
        Baseline & 0.748 & 0.920 \\
        Bicubic & 0.608  & 0.811 \\
        Regrerssion & 0.617  & 0.827 \\
        % DRLN~\cite{anwar2020densely} & 0.655 & 0.879 \\
        \hline
        SR3 ($T=100$) & \textbf{0.683} & \textbf{0.880} \\
        PartDiff ($K=25$) & 0.632 & 0.851\\
        PartDiff ($K=50$) & 0.679  &  \textbf{0.880} \\
        PartDiff ($K=75$) & \textbf{0.682} & \textbf{0.880} \\
        \Xhline{2\arrayrulewidth}
    \end{tabular}
    }
    \label{tab:img-cls}
\end{table}
Diffusion-based methods achieve the best top-1 and top-5 accuracies, suggesting
the outputs align with real images the best.
%
Our method, with three-quarters of the denoising steps, achieves identical accuracies.
%
Using only half denoising steps, our method achieves the same top-5 accuracy with and
very close top-1 accuracy to SR3.


% \subsection{Ablation Study and Discussion}\label{sec:exp-ablation}



\section{Conclusions}\label{sec:conclusion}
We introduce the Partial Diffusion Model, a novel diffusion-based model for accelerated image super-resolution.
%
Our method accelerates plain denoising diffusion models by skipping some of the diffusion steps.
%
We first observed that the latent states of a pair of low- and high-resolution images gradually converge and become indistinguishable.
%
Based on this observation, we propose approximating an intermediate state in denoising a high-resolution image
with a low-resolution image’s latent, which allows us to skip many denoising steps.
%
We further propose a latent alignment mechanism that gradually interpolates between the latent states of
diffusing low- and high-resolution images to compensate for the error caused by the approximation.
%
Experiments on both MRI and natural images demonstrate that the proposed method significantly reduces
the number of denoising steps without sacrificing the quality of generation.
%
One possible limitation is that it is only applicable to conditional generation tasks where a conditional input
is used for the approximation.
%
Possible future work includes exploring its application in other conditional generation tasks such as image
denoising and image inpainting.



\bibliographystyle{IEEEtran}
\bibliography{mr-sr.bib}

\newcommand{\AddPhoto}[1]{\includegraphics%
[width=1in,height=1.25in,clip,keepaspectratio]{figures/photos/#1}}



\ifdefined\withauthorbio
\begin{IEEEbiography}[\AddPhoto{kai.jpg}]{Kai Zhao}
    received his Ph.D. degree in Computer Science from Nankai University in 2020,
    and his M.S. and B.S. in electrical engineering from Shanghai University in 2014 and 2017, respectively.
    %
    He is currently a postdoctoral researcher at the University of California,
    Los Angeles.
    %
    Prior to joining UCLA, he was a research scientist at Tencent.
    %
    His research interests include medical image analysis,
    computer vision, and statistical learning,
    with a specific focus on low-data deep learning,
    where the amount of annotated data is limited.
    %
    %
    More information can be found on his homepage \url{https://kaizhao.net}.
\end{IEEEbiography}
\vspace{-.5in}

\begin{IEEEbiography}[\AddPhoto{alex.pdf}]{Alex Ling Yu Hung}
    is currently a Ph.D. student in Computer Science at the University of California, Los Angeles (UCLA), advised by Prof. Kyunghyun Sung and Prof. Demetri Terzopoulos. 
    %
    He received his Master's degree in Biomedical Engineering from Carnegie Mellon University in 2021, and Bachelor's degree from Peking University in 2019 while majoring Biomedical Engineering and minoring in Computer Science. 
    %
    His main research focus is on medical image analysis, generative models, and general computer vision. 
    %
    %
    More information can be found on his homepage \url{https://web.cs.ucla.edu/~alexhung/}.
\end{IEEEbiography}
\vspace{-.5in}

\begin{IEEEbiography}[\AddPhoto{kaifeng.jpg}]{Kaifeng Pang}
    is currently a Master student in the 
    Electrical and Computer Engineering Department of the 
    University of California, Los Angeles (UCLA), 
    working with Dr Kai Zhao and Prof. Kyunghyun Sung. 
    He received his Bachelor's degree from Nanjing University in 2022. 
    His research interests include medical image analysis and deep learning.
\end{IEEEbiography}
\vspace{-.5in}


\begin{IEEEbiography}[\AddPhoto{haoxin}]{Haoxin Zheng}
    is currently a Ph.D. student in the
    Computer Science Department of the University of California, Los Angeles (UCLA),
    advised by Prof. Kyunghyun Sung and Prof. Fabien Scalzo.
    He received his Master's degree from UCLA in 2019,
    and Bachelor's degree from Nankai University in 2017.
    His research interests include medical image analysis, 
    multi-modality learning, and computer vision.
\end{IEEEbiography}
\vspace{-.5in}

\begin{comment}
\begin{IEEEbiography}[\AddPhoto{steven}]{Steven S Raman}
    is the Director of Prostate MR Imaging and Interventions and
    UCLA Prostate MR Imaging Research Group.
    %
    He received his MD from Keck School of Medicine of USC.
    %
    He is an expert in Abdominal and Pelvic imaging
    (CT, MRI, Ultrasound and X-Ray)
    and interventional radiology (image guided procedures),
    especially in the area of tumor ablation and Fibroid Treatment.
    He is also director of the Abdominal Imaging Fellowship at UCLA,
    and co-director of the Fibroid Treatment Program at UCLA.
\end{IEEEbiography}
\end{comment}


\begin{IEEEbiography}[\AddPhoto{kyung}]{Kyunghyun Sung}
    received the Ph.D. degree
    in electrical engineering from the University of
    Southern California, Los Angeles, in 2008. From
    2008 to 2012, he finished his postdoctoral training
    with the Departments of Radiology, Stanford. In
    2012, he joined the Department of Radiological
    Sciences, University of California at Los Angeles,
    Los Angeles (UCLA). He is currently an Associate
    Professor of Radiology, where his research primarily 
    focuses on the development of novel medical
    imaging methods and artificial intelligence using magnetic resonance
    imaging (MRI). In particular, his research group  \url{https://mrrl.ucla.edu/sunglab/}
    is currently focused on developing advanced deep learning algorithms and
    quantitative MRI techniques for early diagnosis, treatment guidance, and
    therapeutic response assessment for oncologic applications. Such developments
    can offer more robust and reproducible measures of biologic markers
    associated with human cancers.
\end{IEEEbiography}
\fi

\clearpage
\pagebreak

\ifdefined\withappendix
% Figure environment removed


% Figure environment removed

% Figure environment removed



% Figure environment removed


% Figure environment removed

\fi

\end{document}


