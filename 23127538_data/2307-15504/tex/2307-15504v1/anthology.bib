% Please download the latest anthology.bib from
%
% http://aclweb.org/anthology/anthology.bib.gz
@article{dong2022survey,
  title={A Survey for In-context Learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}


@article{deb2022boosting,
  title={Boosting Natural Language Generation from Instructions with Meta-Learning},
  author={Deb, Budhaditya and Zheng, Guoqing and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2210.11617},
  year={2022}
}

@inproceedings{ye2021crossfit,
  title={CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7163--7189},
  year={2021}
}

@inproceedings{le2020adversarial,
  title={Adversarial filters of dataset biases},
  author={Le Bras, Ronan and Swayamdipta, Swabha and Bhagavatula, Chandra and Zellers, Rowan and Peters, Matthew and Sabharwal, Ashish and Choi, Yejin},
  booktitle={International Conference on Machine Learning},
  pages={1078--1088},
  year={2020},
  organization={PMLR}
}

@article{su2021roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
  journal={arXiv preprint arXiv:2104.09864},
  year={2021}
}

@inproceedings{koh2017understanding,
  title={Understanding black-box predictions via influence functions},
  author={Koh, Pang Wei and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={1885--1894},
  year={2017},
  organization={PMLR}
}

@inproceedings{sennrich2012perplexity,
  title={Perplexity minimization for translation model domain adaptation in statistical machine translation},
  author={Sennrich, Rico},
  year={2012},
  organization={Association For Computational Linguistics}
}

@article{chen1999empirical,
  title={An empirical study of smoothing techniques for language modeling},
  author={Chen, Stanley F and Goodman, Joshua},
  journal={Computer Speech \& Language},
  volume={13},
  number={4},
  pages={359--394},
  year={1999},
  publisher={Elsevier}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{anaby2020not,
  title={Do not have enough data? Deep learning to the rescue!},
  author={Anaby-Tavor, Ateret and Carmeli, Boaz and Goldbraich, Esther and Kantor, Amir and Kour, George and Shlomov, Segev and Tepper, Naama and Zwerdling, Naama},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={7383--7390},
  year={2020}
}

@inproceedings{yang-etal-2020-generative,
    title = "Generative Data Augmentation for Commonsense Reasoning",
    author = "Yang, Yiben  and
      Malaviya, Chaitanya  and
      Fernandez, Jared  and
      Swayamdipta, Swabha  and
      Le Bras, Ronan  and
      Wang, Ji-Ping  and
      Bhagavatula, Chandra  and
      Choi, Yejin  and
      Downey, Doug",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.90",
    doi = "10.18653/v1/2020.findings-emnlp.90",
    pages = "1008--1025",
    abstract = "Recent advances in commonsense reasoning depend on large-scale human-annotated training sets to achieve peak performance. However, manual curation of training sets is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit to. We propose a novel generative data augmentation technique, G-DAUG{\^{}}C, that aims to achieve more accurate and robust learning in a low-resource setting. Our approach generates synthetic examples using pretrained language models and selects the most informative and diverse set of examples for data augmentation. On experiments with multiple commonsense reasoning benchmarks, G-DAUG{\^{}}C consistently outperforms existing data augmentation methods based on back-translation, establishing a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA, as well as enhances out-of-distribution generalization, proving to be robust against adversaries or perturbations. Our analysis demonstrates that G-DAUG{\^{}}C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance.",
}

@inproceedings{wang-etal-2022-promda,
    title = "{P}rom{DA}: Prompt-based Data Augmentation for Low-Resource {NLU} Tasks",
    author = "Wang, Yufei  and
      Xu, Can  and
      Sun, Qingfeng  and
      Hu, Huang  and
      Tao, Chongyang  and
      Geng, Xiubo  and
      Jiang, Daxin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.292",
    doi = "10.18653/v1/2022.acl-long.292",
    pages = "4242--4255",
    abstract = "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
}

@article{fang2022pseudoreasoner,
  title={PseudoReasoner: Leveraging Pseudo Labels for Commonsense Knowledge Base Population},
  author={Fang, Tianqing and Do, Quyet V and Zhang, Hongming and Song, Yangqiu and Wong, Ginny Y and See, Simon},
  journal={arXiv preprint arXiv:2210.07988},
  year={2022}
}

@inproceedings{mishra2022cross,
  title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={3470--3487},
  year={2022}
}

@inproceedings{DBLP:conf/iclr/AribandiTSRZMZ022,
  author    = {Vamsi Aribandi and
               Yi Tay and
               Tal Schuster and
               Jinfeng Rao and
               Huaixiu Steven Zheng and
               Sanket Vaibhav Mehta and
               Honglei Zhuang and
               Vinh Q. Tran and
               Dara Bahri and
               Jianmo Ni and
               Jai Prakash Gupta and
               Kai Hui and
               Sebastian Ruder and
               Donald Metzler},
  title     = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2022}
}

@inproceedings{DBLP:conf/iclr/WeiBZGYLDDL22,
  author    = {Jason Wei and
               Maarten Bosma and
               Vincent Y. Zhao and
               Kelvin Guu and
               Adams Wei Yu and
               Brian Lester and
               Nan Du and
               Andrew M. Dai and
               Quoc V. Le},
  title     = {Finetuned Language Models are Zero-Shot Learners},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2022}
}

@article{ivison2022hint,
  title={HINT: Hypernetwork Instruction Tuning for Efficient Zero-Shot Generalisation},
  author={Ivison, Hamish and Bhagia, Akshita and Wang, Yizhong and Hajishirzi, Hannaneh and Peters, Matthew},
  journal={arXiv preprint arXiv:2212.10315},
  year={2022}
}

@article{honovich2022instruction,
  title={Instruction induction: From few examples to natural language task descriptions},
  author={Honovich, Or and Shaham, Uri and Bowman, Samuel R and Levy, Omer},
  journal={arXiv preprint arXiv:2205.10782},
  year={2022}
}

@inproceedings{DBLP:conf/iclr/HendrycksBBZMSS21,
  author    = {Dan Hendrycks and
               Collin Burns and
               Steven Basart and
               Andy Zou and
               Mantas Mazeika and
               Dawn Song and
               Jacob Steinhardt},
  title     = {Measuring Massive Multitask Language Understanding},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2021}
}

@inproceedings{DBLP:conf/iclr/SanhWRBSACSRDBX22,
  author    = {Victor Sanh and
               Albert Webson and
               Colin Raffel and
               Stephen H. Bach and
               Lintang Sutawika and
               Zaid Alyafeai and
               Antoine Chaffin and
               Arnaud Stiegler and
               Arun Raja and
               Manan Dey and
               M Saiful Bari and
               Canwen Xu and
               Urmish Thakker and
               Shanya Sharma Sharma and
               Eliza Szczechla and
               Taewoon Kim and
               Gunjan Chhablani and
               Nihal V. Nayak and
               Debajyoti Datta and
               Jonathan Chang and
               Mike Tian{-}Jian Jiang and
               Han Wang and
               Matteo Manica and
               Sheng Shen and
               Zheng Xin Yong and
               Harshit Pandey and
               Rachel Bawden and
               Thomas Wang and
               Trishala Neeraj and
               Jos Rozen and
               Abheesht Sharma and
               Andrea Santilli and
               Thibault F{\'{e}}vry and
               Jason Alan Fries and
               Ryan Teehan and
               Teven Le Scao and
               Stella Biderman and
               Leo Gao and
               Thomas Wolf and
               Alexander M. Rush},
  title     = {Multitask Prompted Training Enables Zero-Shot Task Generalization},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2022}
}

@article{su2022one,
  title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings},
  author={Su, Hongjin and Kasai, Jungo and Wang, Yizhong and Hu, Yushi and Ostendorf, Mari and Yih, Wen-tau and Smith, Noah A and Zettlemoyer, Luke and Yu, Tao and others},
  journal={arXiv preprint arXiv:2212.09741},
  year={2022}
}

@article{iyer2022opt,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
  journal={arXiv preprint arXiv:2212.12017},
  year={2022}
}

@article{gu2022robustness,
  title={Robustness of Learning from Task Instructions},
  author={Gu, Jiasheng and Xu, Hanzi and Nie, Liangyu and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2212.03813},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@inproceedings{wang2022super,
  title={Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks},
  author={Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Naik, Atharva and Ashok, Arjun and Dhanasekaran, Arut Selvan and Arunkumar, Anjana and Stap, David and others},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={5085--5109},
  year={2022}
}

@article{longpre2023flan,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@article{phang2022hypertuning,
  title={HyperTuning: Toward Adapting Large Language Models without Back-propagation},
  author={Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2211.12485},
  year={2022}
}
@inproceedings{DBLP:conf/emnlp/XieW0ZSYWZYWZWL22,
  author    = {Tianbao Xie and
               Chen Henry Wu and
               Peng Shi and
               Ruiqi Zhong and
               Torsten Scholak and
               Michihiro Yasunaga and
               Chien{-}Sheng Wu and
               Ming Zhong and
               Pengcheng Yin and
               Sida I. Wang and
               Victor Zhong and
               Bailin Wang and
               Chengzu Li and
               Connor Boyle and
               Ansong Ni and
               Ziyu Yao and
               Dragomir Radev and
               Caiming Xiong and
               Lingpeng Kong and
               Rui Zhang and
               Noah A. Smith and
               Luke Zettlemoyer and
               Tao Yu},
  title     = {UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding
               with Text-to-Text Language Models},
  booktitle = {{EMNLP}},
  pages     = {602--631},
  publisher = {Association for Computational Linguistics},
  year      = {2022}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}

@inproceedings{ye-etal-2021-crossfit,
    title = "{C}ross{F}it: A Few-shot Learning Challenge for Cross-task Generalization in {NLP}",
    author = "Ye, Qinyuan  and
      Lin, Bill Yuchen  and
      Ren, Xiang",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.572",
    doi = "10.18653/v1/2021.emnlp-main.572",
    pages = "7163--7189",
    abstract = "Humans can learn a new language task efficiently with only few examples, by leveraging their knowledge obtained when learning prior tasks. In this paper, we explore whether and how such cross-task generalization ability can be acquired, and further applied to build better few-shot learners across diverse NLP tasks. We introduce CrossFit, a problem setup for studying cross-task generalization ability, which standardizes seen/unseen task partitions, data access during different learning stages, and the evaluation protocols. To instantiate different seen/unseen task partitions in CrossFit and facilitate in-depth analysis, we present the NLP Few-shot Gym, a repository of 160 diverse few-shot NLP tasks created from open-access NLP datasets and converted to a unified text-to-text format. Our analysis reveals that the few-shot learning ability on unseen tasks can be improved via an upstream learning stage using a set of seen tasks. We also observe that the selection of upstream learning tasks can significantly influence few-shot performance on unseen tasks, asking further analysis on task similarity and transferability.",
}

@inproceedings{bach2022promptsource,
  title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
  author={Bach, Stephen and Sanh, Victor and Yong, Zheng Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V and Sharma, Abheesht and Kim, Taewoon and Bari, M Saiful and F{\'e}vry, Thibault and others},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
  pages={93--104},
  year={2022}
}

@misc{honovich2022unnatural,
      title = {Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor},
      author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
      url = {https://arxiv.org/abs/2212.09689},
      publisher = {arXiv},
      year={2022}
}


@article{DBLP:journals/jmlr/RaffelSRLNMZLL20,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {J. Mach. Learn. Res.},
  volume    = {21},
  pages     = {140:1--140:67},
  year      = {2020}
}

@article{efrat2022lmentry,
  title={Lmentry: A language model benchmark of elementary language tasks},
  author={Efrat, Avia and Honovich, Or and Levy, Omer},
  journal={arXiv preprint arXiv:2211.02069},
  year={2022}
}

@article{honovich2022instruction,
  title={Instruction induction: From few examples to natural language task descriptions},
  author={Honovich, Or and Shaham, Uri and Bowman, Samuel R and Levy, Omer},
  journal={arXiv preprint arXiv:2205.10782},
  year={2022}
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@inproceedings{kendall2018multi,
  title={Multi-task learning using uncertainty to weigh losses for scene geometry and semantics},
  author={Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7482--7491},
  year={2018}
}

@inproceedings{navon2022multi,
  title={Multi-Task Learning as a Bargaining Game},
  author={Navon, Aviv and Shamsian, Aviv and Achituve, Idan and Maron, Haggai and Kawaguchi, Kenji and Chechik, Gal and Fetaya, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={16428--16446},
  year={2022},
  organization={PMLR}
}

@inproceedings{lopes2023cross,
  title={Cross-task Attention Mechanism for Dense Multi-task Learning},
  author={Lopes, Ivan and Vu, Tuan-Hung and de Charette, Raoul},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2329--2338},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}

@misc{peng2023instruction,
      title={Instruction Tuning with GPT-4}, 
      author={Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
      year={2023},
      eprint={2304.03277},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2022few,
  title={Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning},
  author={Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1950--1965},
  year={2022}
}

@inproceedings{vu2020exploring,
  title={Exploring and Predicting Transferability across NLP Tasks},
  author={Vu, Tu and Wang, Tong and Munkhdalai, Tsendsuren and Sordoni, Alessandro and Trischler, Adam and Mattarella-Micke, Andrew and Maji, Subhransu and Iyyer, Mohit},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7882--7926},
  year={2020}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={arXiv preprint arXiv:2305.11206},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{DBLP:journals/corr/abs-2205-10782,
  author       = {Or Honovich and
                  Uri Shaham and
                  Samuel R. Bowman and
                  Omer Levy},
  title        = {Instruction Induction: From Few Examples to Natural Language Task
                  Descriptions},
  journal      = {CoRR},
  volume       = {abs/2205.10782},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2205.10782},
  doi          = {10.48550/arXiv.2205.10782},
}