\begin{thebibliography}{27}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aribandi et~al.(2022)Aribandi, Tay, Schuster, Rao, Zheng, Mehta,
  Zhuang, Tran, Bahri, Ni, Gupta, Hui, Ruder, and
  Metzler}]{DBLP:conf/iclr/AribandiTSRZMZ022}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng,
  Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q. Tran, Dara Bahri, Jianmo Ni,
  Jai~Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler. 2022.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock In \emph{{ICLR}}. OpenReview.net.

\bibitem[{Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma,
  Kim, Bari, F{\'e}vry et~al.}]{bach2022promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel, Nihal~V
  Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault F{\'e}vry,
  et~al. 2022.
\newblock Promptsource: An integrated development environment and repository
  for natural language prompts.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pages 93--104.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing. 2023.
\newblock \href {https://lmsys.org/blog/2023-03-30-vicuna/} {Vicuna: An
  open-source chatbot impressing gpt-4 with 90\%* chatgpt quality}.

\bibitem[{Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma et~al.}]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al. 2022.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}.

\bibitem[{Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and
  Zhou}]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou. 2023.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}.

\bibitem[{Fang et~al.(2022)Fang, Do, Zhang, Song, Wong, and
  See}]{fang2022pseudoreasoner}
Tianqing Fang, Quyet~V Do, Hongming Zhang, Yangqiu Song, Ginny~Y Wong, and
  Simon See. 2022.
\newblock Pseudoreasoner: Leveraging pseudo labels for commonsense knowledge
  base population.
\newblock \emph{arXiv preprint arXiv:2210.07988}.

\bibitem[{Hinton et~al.(2015)Hinton, Vinyals, and Dean}]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem[{Honovich et~al.(2022{\natexlab{a}})Honovich, Scialom, Levy, and
  Schick}]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2212.09689} {Unnatural instructions:
  Tuning language models with (almost) no human labor}.

\bibitem[{Honovich et~al.(2022{\natexlab{b}})Honovich, Shaham, Bowman, and
  Levy}]{DBLP:journals/corr/abs-2205-10782}
Or~Honovich, Uri Shaham, Samuel~R. Bowman, and Omer Levy. 2022{\natexlab{b}}.
\newblock \href {https://doi.org/10.48550/arXiv.2205.10782} {Instruction
  induction: From few examples to natural language task descriptions}.
\newblock \emph{CoRR}, abs/2205.10782.

\bibitem[{Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster,
  Wang, Liu, Koura et~al.}]{iyer2022opt}
Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov,
  D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh
  Koura, et~al. 2022.
\newblock Opt-iml: Scaling language model instruction meta learning through the
  lens of generalization.
\newblock \emph{arXiv preprint arXiv:2212.12017}.

\bibitem[{Koh and Liang(2017)}]{koh2017understanding}
Pang~Wei Koh and Percy Liang. 2017.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pages
  1885--1894. PMLR.

\bibitem[{Le~Bras et~al.(2020)Le~Bras, Swayamdipta, Bhagavatula, Zellers,
  Peters, Sabharwal, and Choi}]{le2020adversarial}
Ronan Le~Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew
  Peters, Ashish Sabharwal, and Yejin Choi. 2020.
\newblock Adversarial filters of dataset biases.
\newblock In \emph{International Conference on Machine Learning}, pages
  1078--1088. PMLR.

\bibitem[{Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei et~al.}]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al. 2023.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}.

\bibitem[{Mishra et~al.(2022)Mishra, Khashabi, Baral, and
  Hajishirzi}]{mishra2022cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 3470--3487.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
  2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}.

\bibitem[{Peng et~al.(2023)Peng, Li, He, Galley, and Gao}]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.
\newblock \href {http://arxiv.org/abs/2304.03277} {Instruction tuning with
  gpt-4}.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{DBLP:journals/jmlr/RaffelSRLNMZLL20}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:140:1--140:67.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Wang and Komatsuzaki(2021)}]{gpt-j}
Ben Wang and Aran Komatsuzaki. 2021.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}.

\bibitem[{Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith,
  Khashabi, and Hajishirzi}]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi. 2022{\natexlab{a}}.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}.

\bibitem[{Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap et~al.}]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, et~al. 2022{\natexlab{b}}.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109.

\bibitem[{Wang et~al.(2022{\natexlab{c}})Wang, Xu, Sun, Hu, Tao, Geng, and
  Jiang}]{wang-etal-2022-promda}
Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and
  Daxin Jiang. 2022{\natexlab{c}}.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.292} {{P}rom{DA}:
  Prompt-based data augmentation for low-resource {NLU} tasks}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 4242--4255,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}.

\bibitem[{Yang et~al.(2020)Yang, Malaviya, Fernandez, Swayamdipta, Le~Bras,
  Wang, Bhagavatula, Choi, and Downey}]{yang-etal-2020-generative}
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan
  Le~Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.
  2020.
\newblock \href {https://doi.org/10.18653/v1/2020.findings-emnlp.90}
  {Generative data augmentation for commonsense reasoning}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1008--1025, Online. Association for Computational
  Linguistics.

\bibitem[{Ye et~al.(2021{\natexlab{a}})Ye, Lin, and Ren}]{ye2021crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren. 2021{\natexlab{a}}.
\newblock Crossfit: A few-shot learning challenge for cross-task generalization
  in nlp.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7163--7189.

\bibitem[{Ye et~al.(2021{\natexlab{b}})Ye, Lin, and
  Ren}]{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren. 2021{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.572}
  {{C}ross{F}it: A few-shot learning challenge for cross-task generalization in
  {NLP}}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 7163--7189, Online and Punta Cana,
  Dominican Republic. Association for Computational Linguistics.

\bibitem[{Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu
  et~al.}]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al. 2023.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}.

\end{thebibliography}
