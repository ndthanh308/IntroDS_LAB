\section{Conclusion}
In this paper, our unified instruction-tuning framework (UIT) provides a standardized approach to enhancing the generalization ability for instruction tuning by unifying the format of existing instruction tuning datasets and enabling format transfer between them. Our approach has been tested on various datasets and has shown improvements in model performance. We also propose a denoising method and an offline model training method to make our UIT more feasible in practice. In general, we study an under-explored facet, i.e., the format consistency, for instruction tuning, and we hope our work could facilitate more attempts in relevant areas.

\section*{Limitation}
While our proposed UIT framework and format transferer offer a promising approach to enhancing the generalization performance of instruction-tuned LLMs, there are several limitations that should be acknowledged. Firstly, our method relies on the assumption that the user knows the target instruction format in advance, which may not always be the case. Secondly, we focus on instruction tuning for NLP tasks, instead of broader settings (e.g., real-world instructions~\citep{alpaca,vicuna2023}) where formats are hard to define. We expect future works to explore whether our UIT framework can be applied to broader scenarios.