\section{Introduction}

% Figure environment removed
% \vspace{-1in}
Recently, instruction tuning has gained considerable attention as a potent strategy for enhancing large language models (LLMs) in following human instructions and generating appropriate responses. For instance, by reformulating various NLP tasks with an instruction template, models trained on the converted dataset exhibit powerful capabilities of zero-shot generalization on unseen tasks~\citep{wei2021finetuned}. Later studies have demonstrated that instruction tuning is critical to facilitating LLMs in grounding their inner knowledge to diverse real-world scenarios~\citep{ouyang2022training,iyer2022opt,chung2022scaling,ding2023enhancing}. Up to now, considerable efforts have been dedicated to creating datasets for instruction tuning~\citep{honovich2022unnatural,bach2022promptsource,wei2021finetuned,wang2022super,wang2022self,DBLP:conf/iclr/AribandiTSRZMZ022} and researchers find that increasing the task diversity (i.e., the number of unique tasks) of the training data can consistently enhance generalization performance~\citep{wang2022super,iyer2022opt,longpre2023flan}. Therefore, the community has witnessed a growing endeavor to collect various instructions and integrate existing instruction tuning datasets into larger collections~\citep{iyer2022opt, longpre2023flan, chung2022scaling,zhou2023lima}.

While previous works strive to increase \textbf{task diversity} and merge existing instruction tuning datasets, they typically ignore the \textbf{format consistency} among these datasets. More specifically, different users have their unique ways of expressing instructions, even if these instructions correspond to the same intent. Hence, there often exist variations across different datasets in the instruction styles and formats, which is dubbed as the format inconsistency issue. Take the case of a summarization task, the instruction can be as detailed as ``\textit{In this task, you are given a conversation, and your task is to generate a summary... Input: ... Output: ...}'' in Ni-v2~\citep{wang2022super} or simply composed of a few keywords, e.g., ``\textit{Summarize: ...}'' in CrossFit~\citep{ye2021crossfit}. Due to the format inconsistency issue, fine-tuned LLMs may have difficulty in handling unseen instructions in a different format at the test time, exhibiting poor out-of-distribution (OOD) generalization. 
Hence, before directly merging diverse datasets from various sources and performing multi-task training (i.e., the common practice), it is essential to conduct a comprehensive study of how format inconsistency may impact the performance of instruction tuning and whether mitigating such inconsistency could enhance the generalization.

However, unifying the format across different datasets is not easy. First, instructions are inherently diverse and nuanced, and the vast range of possible expressions makes it challenging to devise a fixed rule for format transfer. Second, standardizing formats can sometimes inadvertently change the meaning of the original instructions. This is particularly problematic for complex tasks where the instructionâ€™s wording and style are crucial to correctly guiding the model behavior. In this paper, we introduce a format transfer framework, \textbf{\underline{U}nified \underline{I}nstruction \underline{T}uning~(UIT)} (Figure~\ref{training-testing-time}) to explore the effects of format consistency. Specifically, we use OpenAI GPT3.5\footnote{\url{https://platform.openai.com/docs/models/gpt-3-5}} for automatic instruction format transfer. Leveraging its powerful in-context learning capabilities, GPT3.5 can successfully transfer the instruction from a source format to a target format based on only a few handcrafted examples.
Then we analyze how format inconsistency could affect generalization under two settings: (1) testing-time setting, which simulates the format inconsistency between the training data and the testing data, and (2) training-time setting, which simulates the format inconsistency among different sources of instructions in the training data. We perform analysis across five benchmarks and show that our method successfully mitigates the format inconsistency issue and improves the generalization performance on unseen instructions in both settings.

Despite its simplicity and performance, the above framework encounters two practical challenges. To begin with, the converted instructions are not perfect as human-written ones and sometimes involve noises. For instance, an auto-converted instruction may express a slightly different meaning than the original one. To address this issue, we propose a novel perplexity-based denoising strategy which samples multiple possible conversions of a source instruction and then filters those low-quality ones based on perplexity. Experimental results reveal that this strategy effectively reduces the noises of format transfer and improves robustness and performance.
Second, converting large-scale instructions via OpenAI API  can result in substantial costs for API calls, which is infeasible in practice. To this end, we propose to learn an offline model for format transfer by distilling from GPT3.5. We demonstrate that with a few examples generated by GPT3.5, a much smaller model can be trained to achieve almost equivalent performance in format transfer, which saves the costs for API calls in practice.

In general, our findings sheds light on an essential but previously overlooked aspect, i.e., format consistency, for instruction tuning. We envision our research could inspire more efforts in advancing the instruction tuning methodologies for LLMs.

\begin{table*}[htbp]
    \centering
% \renewcommand{\arraystretch}{1.1} 
    \small
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Resource}  & 
         \textbf{Task Number} & \textbf{Category Number}  & \textbf{Total Examples} & \textbf{Instruction format} % & \textbf{Prompt structure}
         % & \textbf{Public?}
        \\
        \midrule
         % \cmidrule(lr){1-1}  \cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5} \cmidrule(lr){6-6}  \cmidrule(lr){7-7}  
        %   Usage of supporting fact & $47.7\%$ & $56.0\%$ & $56.0\%$ & $4.7\%$ &  & -- & -- \\ 
          
           \makecell{ \textbf{Ni-v2} ~\citep{wang2022super} }  & $1616$ & $76$ & 5M & task-level  \\ 
          \makecell{ \textbf{Flan 2021}~\citep{wei2021finetuned} }   & $62$ & $12$ & 4.4M & instance-level  \\ 
          \makecell{ $\textbf{CrossFit}$~\citep{ye-etal-2021-crossfit} }  & $159$ & $13$ & 7.1M & keywords-level  \\
          \makecell{ \textbf{P3}~\citep{bach2022promptsource} } & $62$ & $13$ & $12M$ & instance-level   \\
          \makecell{\textbf{Unnatural-Inst}~\citep{honovich2022unnatural}}  & $117$ & $\mathbf{-}$ & 64k & task-level  \\
          % \hdashline
          \makecell{\textbf{OPT-IML}~\citep{iyer2022opt}}   & 1545 & 93 & 17.9M & mixed  \\ 
          \makecell{\textbf{Flan 2022}~\citep{longpre2023flan}}   & 1836 & 162 & 15M & mixed \\ 
          % \hdashline
          % \makecell{\textbf{Uni-I}\\(this work)}   & $3071$ & $\mathbf{-}$ & 21.2M & task level  & \cmark & \cmark \\ 
         \bottomrule
    \end{tabular}
    \caption{A comparison of representative instruction tuning datasets of different instruction formats.
    % ``--'' means the information is unknown or not applicable. ``Cate.Num'', ``Cate.'', ``Exp.'', ``Inst.'',  refer to Category Number, Category, Example, Instruction, respectively.
    }
    \label{tab:dataset_comparison}
\end{table*}

% \begin{table*}[htbp]
%     \centering
%     \small
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{cccccccccc}
%         \toprule
%         \textbf{Resource} & \textbf{Base Model} & \textbf{Model-size} & 
%          \textbf{Task Num} & \textbf{Cate. Num}  & \textbf{Total Exp.} & \textbf{Inst. format} & \textbf{prompt struct.} & \textbf{F.Consist} & \textbf{Public?}
%         \\
%          \cmidrule(lr){1-1}  \cmidrule(lr){2-2}  \cmidrule(lr){3-3}  \cmidrule(lr){4-4}  \cmidrule(lr){5-5} \cmidrule(lr){6-6}  \cmidrule(lr){7-7} \cmidrule(lr){8-8} \cmidrule(lr){9-9} \cmidrule(lr){10-10}
%         %   Usage of supporting fact & $47.7\%$ & $56.0\%$ & $56.0\%$ & $4.7\%$ &  & -- & -- \\ 
          
%            \makecell{ \textbf{Ni-v2} \\~\citep{wang2022super} } & T5-LM & 13B & $1616$ & $76$ & 5M & task level & DPNE & $\mathbf{-}$ & \cmark\\ 
%           \makecell{ \textbf{Flan 2021}\\~\citep{wei2021finetuned} }   & LaMDA & 136B & $62$ & $12$ & 4.4M & instance level & template & $\mathbf{-}$ & \cmark \\ 
%           \makecell{ $\textbf{CrossFit}$\\~\citep{ye-etal-2021-crossfit} }  & BART & 140M & $159$ & $13$ & 7.1M & keywords & template & $\mathbf{-}$ & \cmark \\
%           \makecell{ \textbf{P3}\\~\citep{bach2022promptsource} } & T5-LM & 11B & $62$ & $13$ & $12M$ & instance level & template & $\mathbf{-}$ & \cmark \\
%           \makecell{\textbf{Unnatural-Inst}\\~\citep{honovich2022unnatural}} & T5-LM & 11B & $117$ & $\mathbf{-}$ & 64k & task level & DC & $\mathbf{-}$ & \cmark\\
%           % \hdashline
%           \makecell{\textbf{OPT-IML}\\~\citep{iyer2022opt}}   & OPT & 175B & 1545 & 93 & 17.9M & mixed & mixed & \xmark & \xmark \\ 
%           \makecell{\textbf{Flan 2022}\\~\citep{longpre2023flan}}   & T5,Palm & 540B & 1836 & 162 & 15M & mixed & mixed & \xmark & \xmark\\ 
%           % \hdashline
%           \makecell{\textbf{Alpaca-Inst}\\~\citep{alpaca}}  & LLaMA & 13B &  $\mathbf{-}$ & $\mathbf{-}$ & 52k & task level & D & \xmark & \cmark \\ 
%           \hdashline
%           \makecell{\textbf{Uni-I}\\(this work)}   & T5-LM & 11B & $3071$ & $\mathbf{-}$ & 21.2M & task level & UIT & \cmark & \cmark \\ 
%          \bottomrule
%     \end{tabular}
%     }
%     \caption{A comparison of our Uni-I datasets to relevant instruction tuning datasets. ``--'' means the information is unknown. ``Cate.Num'', ``Cate.'', ``Exp.'', ``Inst.'', ``struct.'', ``F.'' refer to Category Number, Category, Example, Instruction, structure, and Format, respectively.
%     }
%     \label{tab:dataset_comparison}
% \end{table*}