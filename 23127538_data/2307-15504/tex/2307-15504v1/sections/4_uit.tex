% \section{Format Transfer Framework}
\section{Framework and Experiments}
\label{sec:framework}
To mitigate format inconsistency, we propose a format transfer framework, \textit{Unified Instruction Tuning}~(UIT), to convert the instruction formats of existing datasets into a unified format. 

\subsection{Unified Instruction Format Transfer}
Denote the target unified instruction format as $\mathcal{F}_t$ and the original instruction format of a source dataset as $\mathcal{F}_s$, we aim to convert $\mathcal{F}_s$ into $\mathcal{F}_t$ to alleviate the OOD generalization in the instruction format.
Taking inspiration from \citet{honovich2022unnatural}, we rely on the LLM's in-context learning ability to conduct format transfer in an automatic manner.
%
Specifically, we manually select $k$ seed parallel data $\{[s_{1}, t_{1}], \cdots, [s_{k}, t_{k}]\}$, where $s_{i}$ and $t_{i}$ are the same instance (task) expressed in format $\mathcal{F}_s$ and $\mathcal{F}_t$ respectively.
% $[s_\text{i}, t_\text{i}] (i \in \{1,\cdots, N\})$ denotes a pair of instructions describing the same instance with different formats. 
%
% Typically, $t_\text{i}$ means the instance is in the format $\mathcal{F}_t$ while $s_\text{i}$ is in the format $\mathcal{F}_s$.
%
% Specifically, we manually select $N$ seed parallel data $\{(src_\text{1}, tgt_\text{1}), \cdots, (src_\text{N}, tgt_\text{N})\}$, where $(src_\text{1}, tgt_\text{1})$ denotes a pair of instructions describing an instance with different formats. 
% %
% Typically, $tgt_\text{1}$ means the instance is in the format $\mathcal{F}_t$ while $src_\text{1}$ is in the format $\mathcal{F}_s$.
% %
% We manually select three pairs of $(src_\text{N}, tgt_\text{N})$ from source dataset and target dataset as seed parallel data, with each pair describing different task such as summarization, classification and paraphrase.
% By leveraging the strong ICL capability, we are able to generate an instance denoted as $I_\text{t}$, which possesses the same content as $I_\text{s}$ but in the target format. 
%
Given a new instance $s_{new}$ with format $\mathcal{F}_s$, we transfer its instruction format into the unified instruction format $\mathcal{F}_t$ via in-context learning as follows:
\begin{equation}
% \small
\begin{aligned}
    % \mathcal{T}(S,I_\text{s}) &= I_\text{t},
    t_{new}=\text{LLM}\left(s_{new} , [s_{1},t_{1}], 
    \cdots, [s_{k},t_{k}] \right), 
\end{aligned}
\label{eq:ICL_seed}
\end{equation}
%
where $t_{new}$ refers to the transferred instance with $\mathcal{F}_t$.
%
We choose \texttt{text-davinci-003} (GPT3.5) as the LLM for format transfer.
%
Details of the prompt for format transfer are shown in Figure~\ref{ICL-construction}.

% Figure environment removed

\subsection{Experiments}
\label{sec:main_exp}
\paragraph{Settings}
To simulate the format inconsistency problem, we design two experimental settings:
%
\begin{itemize}
    \item \textbf{Testing-time Format Transfer}: the training data is formatted in $\mathcal{F}_t$, while the test data is formatted in $\mathcal{F}_s$. To mitigate the format inconsistency, we convert the instruction format of the test data into $\mathcal{F}_t$, without modifying the training data. 
    %
    This setting is designed to explore the format inconsistency impact between training data and the test data in the inference phase.
    
    \item \textbf{Training-time Format Transfer}: the training data is mixed with different formats (e.g., both $\mathcal{F}_s$ and $\mathcal{F}_t$), and the testing data is in the format of $\mathcal{F}_t$. Instead of modifying the testing data, here we convert the training data from format $\mathcal{F}_s$ to $\mathcal{F}_t$. 
    %
    This setting is designed to simulate the format inconsistency of different sources of the training data. % thereby allowing the model to concentrate on discerning genuine task differences instead of the instruction formats.
\end{itemize}

\paragraph{Datasets}
% Without loss of generality, we select Ni-v2 dataset as the training dataset for both training-time and testing-time setting. For target format selection, in testing stage experiments we choose four primary formats from Ni-v2, namely \textbf{DP}, \textbf{DPN}, \textbf{DPE}, and \textbf{DPNE}. In training stage experiments, we specifically select the \textbf{DP} format as the target format.

For the testing-time setting, we select Ni-v2~\citep{wang2022super} as the training dataset and use DiversePrompt~\citep{DBLP:journals/corr/abs-2205-10782}, Flan~\citep{wei2021finetuned}, CrossFit~\citep{ye-etal-2021-crossfit}, and PromptSource~\citep{bach2022promptsource} as the test dataset.
%
We evaluate the tasks that do not appear in the training stage. These tasks are the same as or similar to those in Ni-v2 test set.
%
In Ni-v2, the instruction format incorporates four components: (1) task definition (\textbf{D}), (2) positive example (\textbf{P}) for demonstration instances with the ground-truth label, (3) negative examples (\textbf{N}) for demonstration instances with a false label, and (4) explanations (\textbf{E}) that provide detailed explanations for the examples.
%
Different formats refer to distinct combinations of the above components.
%
For example, the \textbf{DP} format includes the task definition and positive examples information. 
%
In our experiments, we consider four primary formats, namely \textbf{DP}, \textbf{DPN}, \textbf{DPE}, and \textbf{DPNE} as the unified instruction format, respectively.

For the training-time setting, we use the training set of Ni-v2 together with Flan, CrossFit, and P3 respectively for training and use the test set of Ni-v2 for evaluation.
%
As Flan, CrossFit, and P3 may contain instances that exist in the test set of Ni-v2, to prevent data leakage, we filter the overlapped data in Flan, CrossFit, and P3 and use the remaining data for training. 
%
In this setting, we choose \textbf{DP} as the unified instruction format.

\paragraph{Settings and Baselines}
For both settings, we choose T5-LM-xl~\footnote{\url{https://huggingface.co/google/t5-xl-lm-adapt}} as our model and use Exact Match (EM) and Rouge-L as evaluation metrics. 
%
We construct two baselines:
%
(1) \textbf{Raw} does not involve any modifications on the instruction format for both training and testing. For instance, we directly test an instance from Flan in its original format using a model trained with Ni-v2 in \textbf{DPN} format. 
%
(2) \textbf{Heuristic} applies manually-designed rules to transfer different instruction formats into the unified one.
%
If the information from the original format matches the corresponding field in the unified format, we fill the unified format with that information. 
%
Otherwise, we leave the respective field in the unified format blank.
%
For instance, an instance from Flan can be transferred to the \textbf{DPE} format by leaving the \textit{Definition} and \textit{Explanations} fields blank and filling the \textit{Positive Examples} field with randomly selected instances from the Flan training set. 

% testing stage exp.
\begin{table*}[!htb]
  \centering
  \resizebox{15.5cm}{3.4cm}{
    \begin{tabular}{l l cc cc cc cc cc}
    \toprule
    % \multicolumn{12}{c}{\textbf{Testing stage transfer (Ni-v2 as target dataset)}} \\
    % \midrule
    \multirow{2}*{\textbf{Format}} & \multirow{2}*{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DiversePrompt}} & \multicolumn{2}{c}{\textbf{FLAN}} &\multicolumn{2}{c}{\textbf{CrossFit}} &\multicolumn{2}{c}{\textbf{PromptSource}}&\multicolumn{2}{c}{\textbf{Average}}\\
    & & EM & Rouge-L & EM & Rouge-L& EM & Rouge-L & EM & Rouge-L & EM & Rouge-L\\ 
    % \midrule
    % Avg on 4 benchmarks & heuristic & 34.3 & 44.6 & 33.1 & 43.8 & 33.9 & 44.8 & 33.4 & 44.1\\
    % Avg on 4 benchmarks & unified & \textbf{36.3} & \textbf{46.6} & \textbf{35.5} & \textbf{46.0} & \textbf{37.1} & \textbf{47.6} & \textbf{36.6} & \textbf{46.7}\\
    \midrule
     DP & raw & $0.1$ & $4.7$ & $11.6$ & $20.8$ & $0.2$ & $3.9$ & $6.6$ & $13.6$ & $4.6$ & $10.8$\\
     DPE & raw & $0.1$ & $5.0$ & $18.1$ & $27.8$ & $0.3$ & $4.4$ & $14.4$ & $19.2$ & $8.2$ & $14.1$\\
    % \midrule
    DPN & raw & $0.2$ & $5.5$ & $12.0$ & $22.6$ & $0.2$ & $4.5$ & $5.3$ & $11.6$ & $4.4$ & $11.1$\\
     DPNE & raw & $0.1$ & $5.2$ & $15.2$ & $25.3$ & $0.2$ & $3.8$ & $19.2$ & $23.7$ & $8.7$ & $14.5$\\
    \midrule
     DP & heuristic & $\textbf{34.7}$ & $45.1$ & $31.4$ & $44.8$ & $43.7$ & $56.0$ & $27.3$ & $32.7$ & $34.3$ & $44.6$\\
     DPE & heuristic & $32.5$ & $43.4$ & $32.0$ & $45.3$ & $41.3$ & $54.2$ & $26.6$ & $31.1$ & $33.1$ & $43.8$\\
    % \midrule
    DPN & heuristic & $30.6$ & $43.5$ & $31.9$ & $45.3$ & $43.5$ & $55.5$ & $29.0$ & $33.9$ & $33.9$ & $44.8$\\
     DPNE & heuristic & $30.6$ & $43.4$ & $30.7$ & $43.6$ & $42.8$ & $54.6$ & $29.1$ & $33.7$ & $33.4$ & $44.1$\\
    \midrule
    DP & unified & $34.2$ & $\textbf{45.4}$ & $32.6$ & $46.3$ & $49.1$ & $60.1$ & $29.2$ & $34.7$ & $36.3$ & $46.6$\\
    DPE & unified & $32.9$ & $44.8$ & $33.5$ & $46.9$ & $46.9$ & $58.4$ & $27.8$ & $32.8$ & $35.5$ & $46.0$\\
    % \midrule
    DPN & unified & $31.5$ & $44.3$ & $34.8$ & $\textbf{48.3}$ & $\textbf{50.3}$ & $\textbf{60.4}$ & $\textbf{32.4}$ & $\textbf{38.3}$ & $\textbf{37.5}$ & $\textbf{48.2}$\\
    DPNE & unified & $32.2$ & $43.4$ & $\textbf{35.0}$ & $48.0$ & $48.6$ & $59.3$ & $29.8$ & $34.9$ & $36.6$ & $46.7$\\
    \bottomrule
    \end{tabular}%
    }
    \caption{\label{testing-time-main}
Testing-time format transfer experiment with four target unified instruction formats (\textbf{DP}, \textbf{DPE}, \textbf{DPN}, \textbf{DPNE}), respectively. We evaluate three methods: (1) raw instructions, transferred instructions based on (2) heuristic rules and (3) our proposed UIT. The training is conducted on Ni-v2 while the testing is conducted on DiversePrompt, FLAN, CrossFit, and PromptSource, respectively.}
\end{table*}

\paragraph{Results and Analyses}
Testing-time format transfer results are shown in Table~\ref{testing-time-main}, and we find that: (1) transferring the instruction format either through the heuristic rule or our UIT significantly improves the performance than the vanilla baseline (i.e., raw), demonstrating the necessity of maintaining format consistency in instruction tuning; (2)  our UIT consistently outperforms the heuristic method across all benchmarks and almost all formats in Ni-v2. Compared with the heuristic method, UIT fully utilizes the semantic understanding and generation abilities of GPT3.5 to derive better transferred instructions; (3) the \textbf{DPN} format demonstrates the highest average performance and exhibits the largest improvements with UIT. 

Training-time format transfer results are shown in Table~\ref{training-time-main}, which shows that format transfer also brings performance improvements compared to raw baseline and performs slightly better than the heuristic method. This again demonstrates that UIT can improve the generalization performance by unifying the instruction format.
%
However, the improvements in the training-time setting are not as significant as that in the testing-time setting. We conjecture this may be because the format inconsistency issue is more evident in our testing-time setting than the training-time setting. Overall, the results under both settings validate our hypothesis that mitigating the instruction format inconsistency issue conduces to improved generalization.

\begin{table}[!t]
  \centering
  \resizebox{1.0\linewidth}{!}{
    \begin{tabular}{l cc cc cc cc}
    \toprule
    % \multicolumn{7}{c}{\textbf{Training stage Transfer (Ni-v2 as target dataset)}} \\
    % \midrule
    \multirow{2}*{\textbf{Method}} & \multicolumn{2}{c}{\textbf{+CrossFit}} & \multicolumn{2}{c}{\textbf{+FLAN}} & \multicolumn{2}{c}{\textbf{+P3}}\\
               & EM & Rouge-L &EM & Rouge-L &EM & Rouge-L\\ 
    % \midrule
    %  \multicolumn{7}{l}{w.o. unknown} \\
    %  \midrule
    %  raw & $\textbf{38.0}$ & $\textbf{56.1}$ & $37.6$ & $55.8$ & $37.3$ & $54.9$\\
    %  heuristic & $37.3$ & $55.7$ & $38.2$ & $56.5$ & $38.7$ & $56.9$\\
    %  unified & $37.6$ & $55.9$ & $\textbf{38.4}$ & $\textbf{56.6}$ & $\textbf{38.9}$ & $\textbf{56.9}$\\
    % \midrule
     % \multicolumn{7}{l}{w. unknown} \\
     \midrule
     raw & $37.5$ & $\textbf{56.0}$ & $38.4$ & $56.9$ & $38.8$ & $56.7$\\
     heuristic & $37.7$ & $55.9$ & $\textbf{38.9}$ & $57.4$ & $\textbf{39.9}$ & $\textbf{58.1}$\\
     unified & $\textbf{37.9}$ & $\textbf{56.0}$ & $\textbf{38.9}$ & $\textbf{57.5}$ & $39.4$ & $57.3$\\
    \bottomrule
    \end{tabular}%
    }
    \caption{\label{training-time-main}
Training-time format transfer experiment with \textbf{DP} format. We compare our UIT with two baselines: raw instructions and instructions transferred by the heuristic rule. The training dataset is Ni-v2 combined with CrossFit, Flan, or P3.
}
\end{table}

\paragraph{Limitations in Practice}
Despite the favorable performance, the proposed framework still has some limitations: first, automatic format transfer sometimes involves noises or even errors in the generated data, which may produce adverse effects; second, the proposed method heavily relies on OpenAI API calls, which entail substantial costs especially for large-scale instruction datasets. Both issues would limit UIT's real-world deployment. In the following, we discuss potential solutions for the above limitations by proposing a denoising strategy (\cref{denoising_method}) and training an offline transfer model (\cref{sec:offline_transfer}), respectively.