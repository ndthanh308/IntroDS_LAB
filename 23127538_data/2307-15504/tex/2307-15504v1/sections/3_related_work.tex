\section{Related Work}
\paragraph{Instruction Tuning}

Instruction tuning regulates LLMs to accurately comprehend and interpret natural language instructions. Prior works in this field focus on reformulating NLP tasks using the templates of instructions. \citet{wei2021finetuned} pioneered to show that fine-tuning LLMs on large collections of tasks formatted in instructions enables the model to generalize to unseen tasks in a zero-shot manner. Since then, there is a surge of interest in manually constructing high-quality instruction datasets by first reformulating the formats of existing NLP datasets and then merging them~\citep{mishra2022cross, bach2022promptsource, ye2021crossfit, ouyang2022training}. Another line of study~\citep{longpre2023flan,iyer2022opt} demonstrates that scaling the number of training tasks and task diversity can further enhance the model's generalization performance. However, all these works directly mix all the existing instruction datasets while ignoring the potential issue of format inconsistency. Instead of investigating the number and diversity of training instructions, we instead explore an under-explored facet, i.e., the instruction format of instruction tuning, and investigate its impact on generalization.

\paragraph{Data Augmentation}
Besides manually curating instruction tuning datasets, \citet{honovich2022unnatural} show that fine-tuning LLMs with machine-generated instruction tuning data achieves excellent performance compared with human-written data, indicating that data augmentation is an effective method to enhance the data quantity and task diversity, which overcomes the time-consuming issues of human annotation. Recently, \citet{alpaca, peng2023instruction, ding2023enhancing} adopt machine-annotation method~\citep{wang2022self} to generate real-world human instructions (rather than instructions that describe NLP tasks) and model responses based on powerful LLMs such as ChatGPT. Similarly, in this paper, we also leverage  LLMs for automatic format transfer and data augmentation. Since real-world instructions are quite diverse and hard to annotate their formats, we instead focus on instructions that describe NLP tasks to rigorously study the effects of instruction format. We believe the derived findings can potentially be applied to real-world instructions in the future.

\paragraph{Synthetic Data Denoising}
Generative models are commonly utilized for data augmentation~\citep{alpaca}. However, these synthetic datasets are not always as reliable as those human-annotated ones, and filtering out noisy examples can boost the model performance~\citep{le2020adversarial}. Recent studies have suggested different approaches for denoising. For instance, \citet{yang-etal-2020-generative,fang2022pseudoreasoner} adopted influence functions~\citep{koh2017understanding} to evaluate the quality of the synthetic data; \citet{wang-etal-2022-promda} employ the NLU Consistency Filtering~\citep{koh2017understanding} to filter out low-quality samples.
In our research, we utilized LLMs for instruction format transfer, which may introduce noise throughout the process. To overcome this challenge, we adopted a simple and effective perplexity scoring strategy to denoise our auto-constructed dataset (\cref{denoising_method}).
