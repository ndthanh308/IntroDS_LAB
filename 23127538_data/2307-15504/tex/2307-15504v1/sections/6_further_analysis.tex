\section{Further Analysis}

% Model size scaling test
% Figure environment removed


\begin{table}[!t]
  \centering
  \resizebox{6.2cm}{0.9cm}{
    \begin{tabular}{ccccc}
    \toprule
    % \multicolumn{5}{c}{\textbf{Changing Target in Testing time}} \\
    % \midrule
    \textbf{Source} & \textbf{Target} & \textbf{Method} &EM & Rouge-L\\ 
    \midrule
     Ni-v2 & Flan & heuristic & $18.4$ & $28.3$\\
     Ni-v2 & Flan & unified & $\textbf{29.3}$ & $\textbf{43.0}$ \\
    \bottomrule
    \end{tabular}%
    }
    \caption{\label{flan-test-exp}
Testing time transfer experiment results when Flan is selected as the target dataset and Ni-v2 as the source dataset. Transferring Ni-v2 to the target format brings significant performance improvements during inference when training is conducted with target format.
}
\end{table}

\paragraph{Effects of the Target Unified Format}
In previous experiments, we mainly use Ni-v2 as the target instruction format. To verify the versatility of UIT for various target instructions formats, we select Flan, an instance-level dataset as the target dataset and conduct testing-time transfer experiments. Results are shown in Table~\ref{flan-test-exp}, from which we find that testing-time format transfer brings even more significant performance improvements than the scenario when Ni-v2 is selected as target dataset.
This again validates our hypothesis that format consistency is essential to OOD generalization for instruction tuning, no matter which target format is.

\paragraph{Effects of Model Scaling} 
As observed in previous works~\citep{iyer2022opt}, larger models tend to perform better in following human instructions.
We also conduct model scaling experiments in the testing-time setting with T5~\citep{DBLP:journals/jmlr/RaffelSRLNMZLL20}, with the model size ranging from $5$ million (T5-small) to $10$ billion (T5-XXL).
Results presented in Figure~\ref{fig:testing_scaling} demonstrate that in general, the performance tends to improve as the model size increases. 
% While this general trend is evident across the board, some variation is observed between the performance of large and X-large models. Specifically, the gap in performance between XX-large(11B) and  X-large models is noticeably larger than the gap observed among small, base, and large models. 
% These findings suggest that UIT may be better suited for training on larger models, as it has the potential to more effectively leverage the increased computational power afforded by these larger architectures. Indeed, the observed performance gains for T5-LM models as they increase in size meet the expectation of scalability results in the Large language models.
These findings suggest that instruction format consistency is consistently essential to language models of various sizes.


\begin{table}[!t]
  \centering
  \resizebox{7.65cm}{0.95cm}{
    \begin{tabular}{l cc cc cc cc}
    \toprule
    % \multicolumn{7}{c}{\textbf{Training time transfer (DP as target format)}} \\
    % \midrule
    \textbf{Method} & \multicolumn{2}{c}{\texttt{src}+\texttt{same}} & \multicolumn{2}{c}{\texttt{src}+\texttt{diff}} & \multicolumn{2}{c}{\texttt{src}+\texttt{same}+\texttt{diff}}\\
               & EM & Rouge-L &EM & Rouge-L & EM & Rouge-L\\ 
    \midrule
     raw & $29.3$	& $46.3$ & $28.3$	& $45.3$ & $29.1$ & $45.8$ \\
     % heuristic &
     unified & $30.8$	& $47.6$ & $30.7$	& $47.7$ & $\textbf{31.0}$ & $\textbf{47.8}$\\
    \bottomrule
    \end{tabular}%
    }
    \caption{\label{format-task-ablation}
Experiments for task diversity and format consistency. For task diversity, we set the training dataset to \texttt{src}+\texttt{same}, \texttt{src}+\texttt{diff} or \texttt{src}+\texttt{same}+\texttt{diff}. For format consistency, we either use the raw format or use the unified format.
}
\end{table}


%  In Table \ref{testing-time-main}, we can see that our unified policy functioning well. Our unified policy results outperform all baseline models among all instruction settings (DP, DEP, etc.). In average, we see that DPN outperforms other instruction formats among different datasets testing. And thus, we chose ..., as our final unified instuction format for uni-I. 
% The experimental results presented in Table \ref{unified-exp} provide compelling evidence for the effectiveness of our proposed unified policy. Across all instruction settings (DP, DPE, etc.), our unified policy consistently outperforms all baseline models, proving the importance of our approach in enabling improved performance across a broad range of tasks. Notably, our analysis reveals that the DPN instruction format consistently outperforms other instruction formats across different datasets, suggesting that this instruction format is particularly well-suited for use with our Uni-I dataset. Based on these results, we have selected the DPN instruction format as our final choice for Uni-I, as we believe it represents the best balance between performance and versatility for our purposes. 

\paragraph{Task Diversity v.s. Format Consistency}
We show that both task diversity and format consistency have impacts on the generalization performance for instruction tuning.
As task diversity can only be a variable during the training stage, we only conduct training-time transfer experiments.
Specifically, we choose Ni-v2 as the target dataset with \textbf{DP} as target format and P3 as the source dataset.
We first randomly select $20$ tasks from Ni-v2 (denoted as \texttt{src}). Then we choose the same $20$ training tasks from P3, denoted as \texttt{same}, and $20$ different tasks from P3, which is denoted as \texttt{diff}.
We treat whether to integrate \texttt{same} or \texttt{diff} to the training set (\texttt{src}) as a variable and evaluate on the original Ni-v2 test set.

As shown in Table~\ref{format-task-ablation}, no matter which tasks are chosen as the training data, our UIT always performs better than the vanilla baseline (raw), which again demonstrates the importance of format consistency.
We can also observe that without format unification, \texttt{src}+\texttt{same} performs better than \texttt{src}+\texttt{diff}, which indicates that increasing task diversity may be inferior without format consistency.
Besides, \texttt{source}+\texttt{same}+\texttt{diff} with UIT performs the best among all combinations, suggesting that increasing task diversity and maintaining format consistency at the same time is the best practice for merging datasets in instruction tuning. We believe this finding can guide practitioners to better prepare the datasets for insturction tuning in the future.