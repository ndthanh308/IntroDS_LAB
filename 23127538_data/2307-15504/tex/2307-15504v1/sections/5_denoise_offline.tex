\section{Denoising for Format Transfer}
\label{denoising_method}


% During format transfer, we sample from the output distributions of LLMs. We find empirically that, with varying random seeds, the sampled generations could contain noises, e.g., some may contain errors like critical changes to task definition or hallucinatory restrictions. Intuitively, utilizing erroneous instructions would impair the model's generalization performance.

Empirically, transferring format via LLMs will introduce noises unavoidably. The transferred instances may contain errors like critical changes to task definition or hallucinatory restrictions. Intuitively, utilizing erroneous instructions would impair the model's generalization performance. To the end, we propose a perplexity-based denoising strategy to filter low-quality instances.

\paragraph{Perplexity-based Denoising Strategy}

Perplexity (PPL) is a widely used metric for evaluating the semantic coherence and certainty of language models. We assume that noisy instructions can reduce the certainty of LLMs in accurately predicting the correct output token sequence\footnote{We merely use the positive example (\textbf{P}) as mentioned in \cref{sec:main_exp} for noise assessment.}, leading to higher perplexity. As a result, perplexity serves as a useful metric for assessing the quality of transferred instructions. Hence, we propose to sample multiple times from LLM to obtain multiple transferred instructions. Then we calculate the perplexity for each instruction. Specifically, we concatenate the transferred instruction and the input query, then predict the annotated label and calculate its perplexity, and filter those with high perplexity.

We employ GPT3.5 with temperature $1.0$ to perform sampling for $N$ times with different random seeds, where $N$ is chosen from $\{1, 2, 4, 8, 16, 32\}$. Then, we sort the generated instructions based on perplexity using GPT-J~\citep{gpt-j} and select the sample with the lowest perplexity. We compare our method with the baseline that only samples once from GPT3.5. Without loss of generality, we conduct experiments on Ni-v2 and PromptSource under both the testing-time and training-time settings. For the former, we select the transferred instruction sample with the lowest perplexity; while for the latter, we incorporate multiple transferred instructions with lower perplexity as the training data.

\paragraph{Results}
As shown in Table~\ref{tab:denoising}, our proposed denoising strategy significantly improves the performance at the testing time, and this improvement continues to increase when more instructions are sampled, which shows our method can successfully filter out those low-quality instructions to reduce noises during format transfer. In addition, the method can also improve performance in the training-time setting but the improvement is not more evident when more transferred instructions are included in the training data. It reveals that the model is less sensitive to noises during the training phase.
\begin{table}[!t]
  \centering
  \small
    \begin{tabular}{c cc cc}
    \toprule
    \multirow{2}*{\textbf{Sample Num}} &\multicolumn{2}{c}{\textbf{Testing time}} 
    &\multicolumn{2}{c}{\textbf{Training time}}\\
     & EM & Rouge-L & EM & Rouge-L \\ 
    \midrule
    1 & $32.4$ & $38.3$ & $37.4$ & $56.1$\\
    2 & $33.0$ & $38.9$ & $\textbf{39.4}$ & $\textbf{57.6}$\\
    4 & $33.5$ & $39.4$ & $38.2$ & $56.5$\\
    8 & $33.7$ & $39.7$ & $38.8$ & $57.0$\\
    16 & $\textbf{33.8}$ & $\textbf{39.8}$ & $38.5$ & $56.3$\\

    \bottomrule
    \end{tabular}
    \caption{\label{tab:denoising}
The performance of the denoising strategy at the testing and training time with different number of samples.}
    
\end{table}

\section{Training Offline Model for Format Transfer}
\label{sec:offline_transfer}

Converting large-scale instructions via OpenAI API can cause substantial costs for API calls. To alleviate the reliance on OpenAI API, it is necessary to derive an offline model that has comparable format transfer performance than GPT3.5 but involves fewer costs. Hence we propose to distill the format transfer ability of GPT3.5 into small-scale models.

\paragraph{Fine-tuned Offline Model with Knowledge Distillation}
Compared with larger models, small offline models are less capable to complete format transfer directly through in-context learning without training. Therefore, we strive to enhance small-scale models via knowledge distillation~\citep{hinton2015distilling}. In pursuit of higher quality, we always make GPT3.5 convert the relatively complex and informative instruction format (e.g., Ni-v2) into a simpler and less informative one (e.g., PromptSource). In this way, we obtain parallel data and use it to fine-tune GPT-J for format transfer. Without loss of generality, we use the generated PromptSource-style instructions as the source and the original Ni-v2 instructions as the target to construct a dataset of approximately 3,000 instances. To assess the quality of GPT-J's transfer results, we compare them with the heuristic baseline and GPT3.5's conversion results in the testing-time setting with two formats (\textbf{DP} and \textbf{DPN}).

\begin{table*}[!htb]
  \centering
  \small
    \begin{tabular}{l@{~~~}l@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c@{~~~}c}
    \toprule
    % \multicolumn{12}{c}{\textbf{Offline Transfer at Testing time (Ni-v2 as target dataset)}} \\
    % \midrule
    \multirow{2}*{\textbf{Format}} & \multirow{2}*{\textbf{Method}} & \multicolumn{2}{c}{\textbf{DiversePrompt}} & \multicolumn{2}{c}{\textbf{Flan}} &\multicolumn{2}{c}{\textbf{CrossFit}} &\multicolumn{2}{c}{\textbf{PromptSource}} &\multicolumn{2}{c}{\textbf{Average}}\\
    & & EM & Rouge-L & EM & Rouge-L& EM & Rouge-L & EM & Rouge-L & EM & Rouge-L\\ 
    % \midrule
    % Avg on 4 benchmarks & heuristic & 34.3 & 44.6 & 33.1 & 43.8 & 33.9 & 44.8 & 33.4 & 44.1\\
    % Avg on 4 benchmarks & unified & \textbf{36.3} & \textbf{46.6} & \textbf{35.5} & \textbf{46.0} & \textbf{37.1} & \textbf{47.6} & \textbf{36.6} & \textbf{46.7}\\
    \midrule
     DP & heuristic & $34.7$ & $45.1$ & $31.4$ & $44.8$ & $43.7$ & $56.0$ & $27.3$ & $32.7$ & $34.3$ & $44.6$\\

    DPN & heuristic & $30.6$ & $43.5$ & $31.9$ & $45.3$ & $43.5$ & $55.5$ & $29.0$ & $33.9$ & $33.9$ & $44.8$\\
    
    \midrule
    DP & GPT3.5 & $34.2$ & $45.4$ & $32.6$ & $46.3$ & $49.1$ & $60.1$ & $29.2$ & $34.7$ & $36.3$ & $46.6$\\

    DPN & GPT3.5 & $31.5$ & $44.3$ & $\textbf{34.8}$ & $48.3$ & $\textbf{50.3}$ & $\textbf{60.4}$ & $30.8$ & $36.1$ & $\textbf{37.1}$ & $\textbf{47.6}$\\

    \midrule
    DP & GPT-J & $\textbf{35.2}$ & $45.6$ & $33.5$ & $46.6$ & $43.6$ & $54.5$ & $\textbf{31.6}$ & $36.4$ & $36.0$ & $45.8$\\

    DPN & GPT-J & $34.7$ & $\textbf{45.7}$ & $\textbf{34.8}$ & $\textbf{48.4}$ & $46.0$ & $55.5$ & $31.4$ & $\textbf{36.5}$ & $36.7$ & $46.5$\\

    \bottomrule
    \end{tabular}%
    \caption{\label{tab: local_conversion}
Results of training an offline model (GPT-J) for format transfer at testing time. We compare the transferred instructions using heuristic rules, GPT3.5, or our fine-tuned GPT-J. Other settings are similar to those in Table~\ref{testing-time-main}.}
\end{table*}

\paragraph{Results}
As exhibited in Table \ref{tab: local_conversion}, 
the fine-tuned GPT-J performs much better than the heuristic baseline but slightly worse than GPT3.5. This shows that our method can well distill the format transfer ability into small-scale models, which saves the costs in practice. Additionally, the performance is highly correlated with the similarity of the source and target formats. For instance, for DiversePrompt whose instruction format is similar to the target format, the transfer process is less challenging. As a result, the fine-tuned model demonstrates comparable or even superior performance than GPT3.5. Conversely, for CrossFit which only describes keywords and lacks natural language instructions, it is more difficult for small models to produce high-quality instructions, resulting in inferior performance.