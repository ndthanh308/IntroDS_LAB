\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anaby-Tavor et~al.(2020)Anaby-Tavor, Carmeli, Goldbraich, Kantor,
  Kour, Shlomov, Tepper, and Zwerdling]{anaby2020not}
Ateret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour,
  Segev Shlomov, Naama Tepper, and Naama Zwerdling.
\newblock Do not have enough data? deep learning to the rescue!
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  7383--7390, 2020.

\bibitem[Aribandi et~al.(2022)Aribandi, Tay, Schuster, Rao, Zheng, Mehta,
  Zhuang, Tran, Bahri, Ni, Gupta, Hui, Ruder, and
  Metzler]{DBLP:conf/iclr/AribandiTSRZMZ022}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng,
  Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q. Tran, Dara Bahri, Jianmo Ni,
  Jai~Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock In \emph{{ICLR}}. OpenReview.net, 2022.

\bibitem[Bach et~al.(2022)Bach, Sanh, Yong, Webson, Raffel, Nayak, Sharma, Kim,
  Bari, F{\'e}vry, et~al.]{bach2022promptsource}
Stephen Bach, Victor Sanh, Zheng~Xin Yong, Albert Webson, Colin Raffel, Nihal~V
  Nayak, Abheesht Sharma, Taewoon Kim, M~Saiful Bari, Thibault F{\'e}vry,
  et~al.
\newblock Promptsource: An integrated development environment and repository
  for natural language prompts.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics: System Demonstrations}, pp.\  93--104, 2022.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Ding et~al.(2023)Ding, Chen, Xu, Qin, Zheng, Hu, Liu, Sun, and
  Zhou]{ding2023enhancing}
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan
  Liu, Maosong Sun, and Bowen Zhou.
\newblock Enhancing chat language models by scaling high-quality instructional
  conversations.
\newblock \emph{arXiv preprint arXiv:2305.14233}, 2023.

\bibitem[Fang et~al.(2022)Fang, Do, Zhang, Song, Wong, and
  See]{fang2022pseudoreasoner}
Tianqing Fang, Quyet~V Do, Hongming Zhang, Yangqiu Song, Ginny~Y Wong, and
  Simon See.
\newblock Pseudoreasoner: Leveraging pseudo labels for commonsense knowledge
  base population.
\newblock \emph{arXiv preprint arXiv:2210.07988}, 2022.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Honovich et~al.(2022{\natexlab{a}})Honovich, Scialom, Levy, and
  Schick]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human
  labor, 2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2212.09689}.

\bibitem[Honovich et~al.(2022{\natexlab{b}})Honovich, Shaham, Bowman, and
  Levy]{DBLP:journals/corr/abs-2205-10782}
Or~Honovich, Uri Shaham, Samuel~R. Bowman, and Omer Levy.
\newblock Instruction induction: From few examples to natural language task
  descriptions.
\newblock \emph{CoRR}, abs/2205.10782, 2022{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2205.10782}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2205.10782}.

\bibitem[Iyer et~al.(2022)Iyer, Lin, Pasunuru, Mihaylov, Simig, Yu, Shuster,
  Wang, Liu, Koura, et~al.]{iyer2022opt}
Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov,
  D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh
  Koura, et~al.
\newblock Opt-iml: Scaling language model instruction meta learning through the
  lens of generalization.
\newblock \emph{arXiv preprint arXiv:2212.12017}, 2022.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pp.\
  1885--1894. PMLR, 2017.

\bibitem[Le~Bras et~al.(2020)Le~Bras, Swayamdipta, Bhagavatula, Zellers,
  Peters, Sabharwal, and Choi]{le2020adversarial}
Ronan Le~Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew
  Peters, Ashish Sabharwal, and Yejin Choi.
\newblock Adversarial filters of dataset biases.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1078--1088. PMLR, 2020.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le,
  Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock \emph{arXiv preprint arXiv:2301.13688}, 2023.

\bibitem[Mishra et~al.(2022)Mishra, Khashabi, Baral, and
  Hajishirzi]{mishra2022cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3470--3487,
  2022.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{DBLP:journals/jmlr/RaffelSRLNMZLL20}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Wang \& Komatsuzaki(2021)Wang and Komatsuzaki]{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith,
  Khashabi, and Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap, et~al.]{wang2022super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  5085--5109, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Xu, Sun, Hu, Tao, Geng, and
  Jiang]{wang-etal-2022-promda}
Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and
  Daxin Jiang.
\newblock {P}rom{DA}: Prompt-based data augmentation for low-resource {NLU}
  tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  4242--4255,
  Dublin, Ireland, May 2022{\natexlab{c}}. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.292}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.292}.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Yang et~al.(2020)Yang, Malaviya, Fernandez, Swayamdipta, Le~Bras,
  Wang, Bhagavatula, Choi, and Downey]{yang-etal-2020-generative}
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan
  Le~Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.
\newblock Generative data augmentation for commonsense reasoning.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp.\  1008--1025, Online, November 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.90}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.90}.

\bibitem[Ye et~al.(2021{\natexlab{a}})Ye, Lin, and Ren]{ye-etal-2021-crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock {C}ross{F}it: A few-shot learning challenge for cross-task
  generalization in {NLP}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  7163--7189, Online and Punta Cana,
  Dominican Republic, November 2021{\natexlab{a}}. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.572}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.572}.

\bibitem[Ye et~al.(2021{\natexlab{b}})Ye, Lin, and Ren]{ye2021crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock Crossfit: A few-shot learning challenge for cross-task generalization
  in nlp.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  7163--7189, 2021{\natexlab{b}}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, Yu,
  et~al.]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, Lili Yu, et~al.
\newblock Lima: Less is more for alignment.
\newblock \emph{arXiv preprint arXiv:2305.11206}, 2023.

\end{thebibliography}
