\clearpage
\onecolumn

\renewcommand{\thesection}{A.\arabic{section}}
\renewcommand{\thefigure}{A.\arabic{figure}}
\renewcommand{\thetable}{A.\arabic{table}} 
\renewcommand{\theequation}{A.\arabic{equation}} 
\renewcommand{\theHsection}{A\arabic{section}}

\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{equation}{0}


\noindent \textbf{\LARGE{Appendix}}\\
\normalfont

In the following sections, we report additional experiments and detailed analysis to further illustrate the performance of our proposed generalist model, Med-PaLM M. 


We provide details on:

\begin{itemize}[leftmargin=1.5em,rightmargin=0em]

\item  Datasets and tasks in MultiMedBench

\item  Med-PaLM M training procedure 
\item  Interpretations of Med-PaLM M performance by task type:
\begin{itemize}
    \item Performance analysis on language-only medical question answering
    \item Performance analysis on radiology report summarization
    \item Performance analysis on medical image classification tasks
    \item Performance analysis on medical visual question answering
    \item Performance analysis on chest X-ray report generation
\end{itemize}
\item  Human evaluation of model-generated chest X-ray reports
\item  Examples from MultiMedBench tasks


\end{itemize}



\section{MultiMedBench}
\label{appendix:multimedbench-details}

In this section, we offer a comprehensive overview of \emph{MultiMedBench}, including a detailed description of the datasets, data preprocessing, and task setups. Figure~\ref{fig:datasets-overiew} summarizes MultiMedBench over its various biomedical tasks. 


\subsection{Language-only datasets}
\paragraph{MultiMedQA}
We used three of the multiple-choice medical question-answering datasets from
MultiMedQA~\cite{singhal2022large}: the MedQA~\cite{jin2021disease}, MedMCQA~\cite{pal2022medmcqa}, and PubMedQA~\cite{jin2019pubmedqa} datasets for training and evaluation of Med-PaLM M. These question answering tasks are language-only and do not require the interpretation of additional modalities. The training set consists of 10,178 questions from MedQA and 182,822 questions from MedMCQA. The test set comprises 1,273 questions from MedQA, 4,183 questions from MedMCQA, and 500 questions from PubMedQA. Note that PubMedQA was not included in the training data mixture and only used for evaluation.

\paragraph{MIMIC-III} is a large publicly-available medical database that contains medical records of patients admitted to intensive care units~\cite{johnson2016mimic}. It contains 79,790 radiology reports across two imaging modalities (CT and MRI) and seven anatomic regions (head, abdomen, chest, head, neck, sinus, spine, pelvis). A total of 78,875 reports were chosen based on criteria such as the length of the report. We used the radiology report summarization dataset from~\cite{van2023radadapt}, which comprises six most common modality/anatomy pairs for training and evaluation: CT head, CT abdomen, CT chest, MRI head, CT spine, and CT neck. To evaluate out-of-distribution (OOD) performance we used five less common modality/anatomy pairs: MRI spine, CT sinus, MRI abdomen, MRI pelvis, and MRI neck. This resulted in a total of 58,405 reports for training, 7,413 reports for validation, and 13,057 reports for testing. Note that chest X-ray reports are excluded from this dataset to avoid data contamination with the MIMIC-CXR dataset for the report generation task. For each report, we used the same preprocessing functions as in~\cite{delbrouck2022vilmedic, delbrouck2022toward} to extract the findings and impression sections. Specifically, we filtered out the reports whose findings section are longer than 600 tokens. We performed a report summarization task by predicting the impression section given the findings section as input, which is another language-only task that does not require multi-modal input.


% Figure environment removed

\subsection{Multimodal datasets}

\paragraph{PAD-UFES-20} consists of 2,298 clinical images of skin lesions collected from different smartphone devices with varying resolutions, sizes, and lighting conditions~\cite{pacheco2020pad}. The data was collected through the Dermatological and Surgical Assistance Program at the Federal University of Espírito Santo (UFES-Brazil), a nonprofit program that provides free skin lesion treatment. The dataset contains six different types of skin lesions including: Basal Cell Carcinoma (BCC), Malignant Melanoma (MEL), Squamous Cell Carcinoma (SCC), Actinic Keratosis (ACK), Melanocytic Nevus (NEV), and Seborrheic Keratosis (SEK). Each image is associated with up to 21 patient clinical features such as patient demographics, family cancer history lesion location, lesion size. We set up a 6-class classification task in a generative framework through a language decoder using skin lesion images and the associated clinical textual features as the multimodal input. Specifically, we selected 14 clinical attributes in the metadata for each lesion including: \textit{age}, \textit{gender}, \textit{smoke}, \textit{drink}, \textit{skin cancer history}, \textit{cancer history}, \textit{region}, \textit{fitspatrick}, \textit{horizontal and vertical diameters}, \textit{itch}, \textit{grew}, \textit{bleed}, and \textit{elevation}. The class ratio is approximately 16:1:4:14:5:4 over three skin cancers (BCC, MEL, and SCC) and three skin disease (ACK, NEV, and SEK). Since there are no published official train/test splits, we randomly split the dataset into a training set (80\%) and a test test (20\%) using a stratified sampling to the preserve original class ratio. We applied a series of image augmentation operations using RandAugment~\cite{cubuk2020randaugment} to the training set including: \textit{autoContrast}, \textit{equalize}, \textit{invert}, \textit{rotate}, \textit{posterize}, \textit{solarize}, \textit{color}, and \textit{contrast}.



\paragraph{VinDr-Mammo} is a full-field digital mammography dataset which consists of 5000 breast X-ray imaging studies and a total of 20,000 gray-scale images with extensive breast-level assessment and lesion-level annotations, collected from two hospitals in in Hanoi, Vietnam~\cite{nguyen2023vindr}. Each study contains four images where the left and right breasts are imaged with mediolateral-oblique (MLO) and cranio-caudal (CC) views. Each image has breast-level assessment following the Breast Imaging Reporting and Data System (BI-RADS). BI-RADS assessment ranges from 1 (negative) to 5 (highly suggestive of malignancy). In addition to the BI-RADS score, the breast density level is also provided as well as regional abnormality finding annotations. We performed a breast-level 5-class BI-RADS classification task similar to the setup in~\cite{wantlin2023benchmd}, except that the laterality and view position of the image was provided as additional contextual features. We used the official train/test splits where the train split contains 16,000 samples with a class ratio of 60:21:4:3:1 across BI-RADS 1-5, respectively and the test split contains 4,000 samples with the same class ratio. We applied the following transformations to the images in the training set: \textit{contrast}, \textit{equalize}, \textit{rotate}, \textit{shearX}, \textit{shearY}, \textit{translateX}, and \textit{translateY}. To mitigate the class imbalance in the training data, we upsampled for each minority class (BI-RADS 2-5) by a factor of 3. 

\paragraph{CBIS-DDSM} is the Curated Breast Imaging Subset of Digital Database for Screening Mammography~\cite{lee2017curated}. This dataset contains 2,620 scanned film mammography studies. Unlike VinDr-Mammo, CBIS-DDSM does not have breast-level BI-RADS assessment. Annotations are provided at the lesion level including BI-RADS, subtlety level, and pathology type. There are two types of lesions: mass and calcification. Both of them are annotated with three possible pathology labels: benign, benign without callback, and malignant. We performed a 3-class abnormality (patch-level) pathology classification task on this dataset for mass and calcification abnormalities separately. Abnormality image patch is cropped by the bounding box of the region-of-interest (ROI) from the full mammogram and used as the model input along with its view position (CC or MLO) information. We used the official train/test splits for both abnormality types. For mass cases, the training and test sets contain 1,318 and 378 images (class ratio: 6:1:6), respectively. For calcification cases, the total number of images in the training and test sets are 1,544 and 326 (class ratio: 1:1:1), respectively. For both cases, we applied the same image augmentation as in VinDr-Mammo to the training set.


\paragraph{PrecisionFDA Truth Challenge V2} was developed for benchmarking the state-of-the-art of variant calling in challenging genomics regions~\cite{olson2022pfda}. Genomic variant calling is a task aiming at identifying genetic variants from sequencing data~\cite{depristo2011gatk}, which can identify disease-causing mutations~\cite{aldubayan2020clincohort}. For variant calling, sequencing data is mapped to the coordinates of a reference genome~\cite{liao2023completeref}. The mappings can be represented as an image-like format that computational methods such as DeepVariant~\cite{poplin2018deepvariant} use to call variants, or in a human-friendly image format which experts use to inspect and quality control variants of interest~\cite{thorvaldsdottir2012igv}. For this task, we used an extensively characterized groundtruth set from the National Institute of Standards and Technology (NIST)~\cite{zook2016giab} for the HG002 sample. We generated examples from sequencing from the PrecisionFDA Truth Challenge V2. For training, we use 4\% of the examples from the whole genome (except for chromosome 20, 21, and 22). For evaluation, we used chromosome20, bases 3000001-9444417. This generated 197,038 candidate variants for training and 13,030 candidate variants for evaluation. For each example, the model predicts three possible genotypes, corresponding to how many copies (0, 1, or 2) of the given alternate allele are present. The training set consists of  45,011, 93,246, and 58,781 samples for classes 0, 1, 2, respectively. The evaluation set contains 3,016, 6,169, and 3,845 for classes 0, 1, 2, respectively.


We used DeepVariant v1.3.0's~\cite{poplin2018deepvariant} example generation method to create image-like examples suitable for machine classification. Specifically, input examples to DeepVariant v1.3.0 have a shape of (100, 221, 6) corresponding to (height, width, channels). Channels are shown in grey-scale below in the following order:
\begin{enumerate}
  \item Read base: different intensities represent A, C, G, and T.
  \item Base quality: set by the sequencing machine. White is higher quality.
  \item Mapping quality: set by the aligner. White is higher quality.
  \item Strand of alignment: Black is forward; white is reverse.
  \item Read supports variant: White means the read supports the given alternate allele, grey means it does not.
  \item Base differs from ref: White means the base is different from the reference, dark grey means the base matches the reference.
\end{enumerate}
To reshape the input example to be compatible with the Med-PaLM M input shape of (224, 224, 3), we stacked up channels 1, 2, 3 with channels 4, 5, 6 such that the original tensor of shape (100, 221, 6) became an RGB image of shape (200, 221, 3). We then padded the image on the width and height dimensions to give it a final shape of (224, 224, 3).




\paragraph{VQA-RAD} is a radiology visual question answering (VQA) dataset which consists of 315 radiology images and 3,515 question–answer pairs created and validated by clinicians~\cite{lau2018dataset}. The radiology images are selected from three imaging modalities (CT, MRI, and X-rays) and three anatomical regions (head, abdominal, chest). The types of question fall into 11 categories including modality, plane, organ system, abnormality, size, plane, positional reasoning, color, counting, attribute and other. 58\% of  the question–answer (QA) pairs are closed-ended (yes/no or limited choices) and the rest 42\% are open-ended (short answer). We adopted the official train/test splits, where the training set contains 1,797 QA pairs (only free-form and paraphrased questions were included) and the test set contains 451 QA pairs (not filtered).

\paragraph{Path-VQA} is a pathology VQA dataset, containing a total of 4,998 pathology images with 32,799 question-answer pairs~\cite{he2020pathvqa}. Pathology images are extracted from medical textbooks and online digital libraries. Each image is associated with multiple QA pairs pertaining to different aspects of the pathology including color, location, appearance, shape, etc. Open-ended questions account for 50.2\% of all questions, which are categorized into 7 categories: what, where, when, whose, how, and how much/how many, accounting for 50.2\% of all questions. The rest are close-ended questions with simple "yes/no" answer. We adopted the official data partitioning where the training, validation, and test sets contain 19,755, 6,279, and 6,761 QA pairs, respectively.

\paragraph{Slake-VQA} is a semantically annotated and knowledge-enhanced bilingual (English and Chinese) VQA dataset on radiology images~\cite{liu2021slake}. It contains 642 annotated images with 14,028 question-answer pairs covering 12 diseases, 39 organ systems and 3 imaging modalities (CT, MRI, and chest X-rays). Questions are either open-ended (free-form) or closed-ended (balanced yes/no) related to various aspects of the image content including plane, quality, position, organ, abnormality, size, color, shape, knowledge graph, etc. The training, validation, and test sets contain 9,849, 2,109, and 2,070 samples, respectively.



\paragraph{MIMIC-CXR} is a large dataset of chest radiographs with free-text radiology reports~\cite{johnson2019mimic}. A total of 377,110 images are available in the dataset from 227,835 image studies collected for 65,379 patients. Each patient may have multiple studies and each study may contain one or more images associated with the same free-text report. Images in MIMIC-CXR are collected from multiple view positions: e.g., anterior-posterior (AP), posterior-anterior, and lateral (LA). Protected health information (PHI) in radiology reports and images is removed, which results in missing information in some sentences of the reports. Since this dataset contains sequential imaging studies of an individual patient, a large number of reports refer to information in prior studies of the same patient. Each report is annotated with structured labels of 14 common radiological observations using CheXpert labeler~\cite{irvin2019chexpert}. We performed two tasks using this dataset: chest X-ray report generation and binary classification of clinically-relevant pathology observations. We preprocessed the radiology reports by extracting the indication, findings, and impression sections, removing redundant white-spaces in the reports, following previous work~\cite{chen2020generating}. We used the official train/validation/test splits. We discarded images without reports and reports where the findings section can not be extracted across train and test. We also filtered out the reports where the length of findings section exceeds 800 characters. However, unlike most previous work using focusing only on the frontal view, we treated images of different orientation that are associated with the same report as independent samples (retaining the patient-level train/test splits to avoid contamination of the test data). The goal is to improve the image understanding capability of the model to process images of different view positions. In a separate evaluation, we also studied a subset of samples where reports are accompanied by both a front and lateral view (two-view report generation).

For the report generation task, we combined the chest X-ray image with the contextual information from the indication section (reason for the study) to predict the findings section of the target report. The total number of samples in the training, validation, and test sets are: 353,542, 2,866, and 4,834, respectively.

For the binary classification task, we grouped negative and uncertain labels as the negative class for 11 pathological conditions: no finding, atelectasis, cardiomegaly, consolidation, edema, pleural effusion, lung opacity, enlarged cardiomediastinum, fracture, pneumonia, and support devices. Atelectasis, cardiomegaly, consolidation, edema, and pleural effusion are 5 major conditions given their clinical relevance and prevalence. The "No finding" label captures the cases without any pathology and therefore this classification task simply helps the model to distinguish normal cases from cases with any type of abnormality. Due to class imbalance, during training we upsampled the positive class by a factor of 2 for the following conditions: consolidation, enlarged cardiomediastinum, fracture, and pneumonia. These binary classification tasks are auxiliary to the report generation task when they are trained simultaneously since they help the model to distinguish among different types of clinical observations in the chest X-ray images.

% Figure environment removed


\section{Med-PaLM M Training Details}
\subsection{Training data mixture}
\cref{fig:datasets-mixture-overiew,tab-appendix:data mixture} show the mixture ratio and few-shot task setup of the training data mixture. The majority of the
data distribution is medical vision-language tasks, with less than 15\% consisting of language-only tasks. While the majority of vision-language tasks were trained with a text-only 1-shot setup (without the corresponding image), the CBIS-DDSM classification and genomic variant calling tasks were trained with a 0-shot setup.
\begin{table}[ht]
\small
\centering
\caption{\textbf{Med-PaLM M data mixture.} Summary of the task types, modalities, mixture ratios, and few-shot setups in Med-PaLM M training data mixture.}
\label{tab-appendix:data mixture}
\begin{tabular}{ccccc}
\toprule
Task & Modality & Dataset          & Mixture ratio  & Few-shot setup  \\ \midrule
\multirow{2}{*}{Question Answering} & \multirow{2}{*}{Text}
& MedQA       & 3.13\%       & 2-shot  \\ 
& & MedMCQA     & 6.25\%        & 2-shot \\ 
\midrule
Report Summarization & Radiology & MIMIC-III       & 3.13\% & 0-shot   \\ 
\midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Visual\\Question Answering\end{tabular}} & \multirow{2}{*}{Radiology}
& VQA-RAD & 0.15\%  & text-only 1-shot  \\
&& Slake-VQA & 2.64\% & text-only 1-shot \\
& Pathology & Path-VQA  & 1.90\%  & text-only 1-shot  \\
\midrule
Report Generation &Chest X-ray &MIMIC-CXR         & 59.90\% & text-only 1-shot  \\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Medical\\Image Classification\end{tabular}}
& Dermatology &PAD-UFES-20  & 6.25\%   & text-only 1-shot  \\
& \multirow{2}{*}{Mammography}
& VinDr-Mammo         & 1.56\% & text-only 1-shot  \\
&& CBIS-DDSM          & 1.56\% & 0-shot  \\
& Chest X-ray & MIMIC-CXR        & 11.98\%  & text-only 1-shot  \\ 
& {\begin{tabular}[c]{@{}c@{}}Genomics\end{tabular}} & {\begin{tabular}[c]{@{}c@{}}PrecisionFDA\\Truth Challenge V2~\cite{olson2022pfda}\end{tabular}}     & 1.56\% & 0-shot  \\

\bottomrule 
\end{tabular}
\end{table}

\subsection{Training hyperparameters}
PaLM-E projects the multimodal inputs into the same language embedding space as latent vectors such that continuous observations (e.g., images, time series) can be processed the same way by a pre-trained LLM as the language tokens, and thereby generates textual completions autoregressively given a multimodal prompt. In our experiments, the ViT maps the visual input to a fixed number of 256 tokens which are further processed by the LLM along with the additional text/multimodal tokens~\cite{driess2023palme}. 
Med-PaLM M was finetuned on the pretrained PaLM-E checkpoints. \cref{tab-appendix:finetuning hyperparameters} shows the training hyperparameters for Med-PaLM M 12B, 84B, and 562B, respectively. 

\begin{table}[]
\small
\centering
\caption{\textbf{Med-PaLM M finetuning hyperparameters.} Summary of the finetuning hyperparameters for Med-PaLM M 12B, 84B, and 562B.}
\label{tab-appendix:finetuning hyperparameters}
\begin{tabular}{cccc}
\toprule
Hyperparameter            & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B)  \\ \midrule
Learning rate & $5\times 10^{-5}$ & $5\times 10^{-5}$ & $2.5\times 10^{-5}$   \\
Batch size & 128 & 128 & 256   \\
Max token input length & 710 & 710 & 710\\
Max token output length & 256 & 256 & 256\\

\bottomrule 
\end{tabular}
\end{table}
\section{Detailed Med-PaLM M Performance}
\label{appendix:multimedbench_detailed_performance}

\paragraph{Performance on text-only medical question answering}
We report the few-shot performance of Med-PaLM M on MedQA, MedMCQA, and PubMedQA in Table~\ref{tab:results-med-qa}. SOTA results were chosen from Med-PaLM 2 with ensemble refinement prompting and PaLM 540B few-shot results reported in~\cite{singhal2022large, singhal2023towards}. Med-PaLM M outperformed the baseline PaLM model (from which it inherits) by a large margin on all three datasets, despite falling behind the Med-PaLM 2 best results obtained with ensemble refinement. Scaling up the language model from 8B to 540B significantly improves the accuracy on the multiple-choice medical question answering tasks, where strong capabilities to comprehend, recall, and reason about medical knowledge are important. These results can be partly explained by the improved base language model used for Med-PaLM 2.

\begin{table}[ht]
\small
\centering
\caption{\textbf{Language-only medical question answering accuracy on MultiMedQA.} Med-PaLM 2 results with ensemble refinement \cite{singhal2023towards} and PaLM few-shot results \cite{singhal2022large} are presented for comparison. Few-shot Med-PaLM~M outperforms the corresponding PaLM baseline by a large margin, despite falling short of the state-of-the-art Med-PaLM 2.}
\label{tab:results-med-qa}
\begin{tabular}{@{\hspace{.01cm}}ccccccc@{\hspace{.01cm}}}
\toprule
Dataset       & Med-PaLM 2   & PaLM   & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \midrule
MedQA (USMLE) & \textbf{86.50\%} & 58.90\% & 29.22\%        & 46.11\%         & 69.68\% \\
\midrule
MedMCQA       & \textbf{72.30\%} & 54.50\% & 32.20\%         & 47.60\%          & 62.59\% \\
\midrule
PubMedQA      & \textbf{81.80\%} & 55.00\%  & 48.60\%         & 71.40\%          & 80.00\%    \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Performance on radiology report summarization}
We report commonly used metrics such as ROUGE-L~\cite{lin2004rouge}, BLEU~\cite{papineni2002bleu}, and F1-RadGraph~\cite{jain2021radgraph} scores on the radiology report summarization task as in~\citet{van2023radadapt} in Table~\ref{tab:results-report-summarization}. Med-PaLM M (562B) yielded the best overall performance compared to the smaller model variants, consistent with our observations on medical question answering tasks. Med-PaLM M performed worse than the SOTA results which were obtained with a parameter-efficient finetuning method (low-rank adaptation, LoRA~\cite{hu2021lora}) on a 738M-parameter clinical-T5 model~\cite{lehman2023clinical}. However, as noted in~\cite{van2023radadapt}, one caveat of clinical-T5 is that it is unclear if~\citet{lehman2023clinical} pretrained the model on the test set of MIMIC-III which led to potential data leakage. Notably, Med-PaLM M compared favorably to the results in~\citet{van2023radadapt} based on the T5 model which was not pretrained on clinical text, similar to the PaLM model.

\begin{table}[ht]
\small
\centering
\caption{\textbf{Med-PaLM M performance on MIMIC-III radiology report summarization.}} 
\label{tab:results-report-summarization}
\begin{tabular}{cccccc}
\toprule
Dataset                    & Metric      & SOTA     & Med-PaLM M (12B) & Med-PaLM M (84B) & Med-PaLM M (562B) \\ \midrule
\multirow{3}{*}{MIMIC-III} & ROUGE-L     & \textbf{38.70}\%  & 29.45\%        & 31.47\%         & 32.03\% \\
& BLEU        & \textbf{16.20}\%   & 12.14\%        & 15.36\%         & 15.21\% \\
                           & F1-RadGraph & \textbf{40.80}\%   & 31.43\%        & 33.96\%         & 34.71\% \\
 \bottomrule
\end{tabular}
\end{table}


\paragraph{Performance on medical image classification tasks}
Table~\ref{tab:results-classifcation} shows the performance of Med-PaLM M on a set of classification tasks across multiple modalities including dermatology, radiology, and genomics. Since these tasks all have imbalanced class distributions, we reported macro-AUC (unweighted mean of all the per-class AUC scores) and macro-F1 scores (unweighted mean of all the per-class F1 scores) as the classification metrics except for the genomic variant calling task where the F1 scores for single nucleotide polymorphisms (SNPs) and short insertions and deletions (indels) in the context of variant discovery were used instead. 

On VinDr-Mammo, all size variants of Med-PaLM M exceeded the prior SOTA using a smaller ViT (9.7M) on macro-AUC~\cite{wantlin2023benchmd}. On CBIS-DDSM, our model achieved the best macro-F1 of 51.12\% and 67.86\% on the mass and calcification classification, respectively, behind the SOTA F1 of 70.71\% reported on the calcification classification~\cite{panambur2022effect}. Note that most previous works on CBIS-DDSM focused on a two-class patch-level classification (benign versus malignant) problem in contrast to our 3-class setup as discussed in~\cite{petrini2022breast}. On Pad-UFES-20, since no official train/test splits are available, our results are not directly comparable with prior studies. Med-PaLM M 84B achieved a macro-AUC of 97.27\%, on par with previous reported results (94\% - 98\%) obtained using CNN and ViT variants~\cite{dai2022deeply, de2022exploring}. On MIMIC-CXR, we reported the macro-average of F1 scores across the binary classification of 5 major conditions: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion. Med-PaLM M (562B) achieved a macro-AUC of 79.09\%, slightly lower than the SOTA result of 81.27\% obtained from ParallelXNet~\cite{rammuni2022effective}, which used a parallelization of various CNN Architectures. On the variant calling task, DeepVariant model~\cite{poplin2018deepvariant} outperformed Med-PaLM M on both Indel-F1 and SNP-F1 scores. The SOTA DeepVariant model was trained with 2,633-fold more training examples. Training with the same examples resulted in a narrower advantage for DeepVariant for SNP (Med-PaLM M 99.35\% versus DeepVariant 99.63\%) and Indel (Med-PaLM M 97.04\% versus DeepVariant 98.55\%. Notably, Med-PaLM M outperformed the accuracy of the widely used GATK4 method~\cite{depristo2011gatk} for SNP calling (Med-PaLM M 99.35\% versus GATK4 99.29\%) but not Indel calling (Med-PaLM M  97.04\% versus GATK4 99.32\%).

Taken together, Med-PaLM M achieved competitive results on a variety of classification tasks using a single model compared to highly specialized SOTA models. It is worth noting that we did not perform any fine-grained task-specific customization and hyperparameter tuning beyond data augmentation and class balancing. It is expected that scaling up the language model does not significantly benefit the classification tasks where the vision encoder is likely the bottleneck for the model performance. There is no overall evidence to suggest that larger vision model outperforms the small one across all our experiments, suggesting that more domain-specific pretraining may be more important for improving vision encoder performance. It is also likely that relatively small-scale datasets we explored here are not sufficient to establish such a robust scaling relationship between the model size and task performance, as results were generally close to each other across model scales.

\begin{table}[ht]
\small
\centering
\caption{\textbf{Med-PaLM M performance on medical image classification.} We report macro-averaged AUC and F1 for all tasks. For MIMIC-CXR, metrics are averaged over 5 major pathological conditions.}
\label{tab:results-classifcation}
\begin{tabular}{ccccccc} 
\toprule
Dataset &
  \# Classes &
  Metric &
  SOTA &
  \begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (12B)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (84B)\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (562B)\end{tabular} \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MIMIC-CXR\\ (5 conditions)\end{tabular}}     & \multirow{2}{*}{2-class} & Macro-AUC & \textbf{81.27\%}    & 76.67\%          & 78.35\%          & 79.09\% \\
                                 &                           & Macro-F1  & N/A       & 38.33\%          & 36.83\%          & \textbf{41.57\%} \\ \midrule
\multirow{2}{*}{PAD-UFES-20}   & \multirow{2}{*}{6-class}  & Macro-AUC & N/A & 95.57\%          & \textbf{97.27\%} & 96.08\%          \\
                                 &                           & Macro-F1  & N/A       & 78.42\%          & \textbf{84.32\%} & 77.03\%          \\ \midrule
\multirow{2}{*}{Variant Calling} & \multirow{2}{*}{3-class}  & Indel-F1  & \textbf{99.40}\%   & 96.42\%          & 97.04\% & 95.46\%          \\
                                 &                           & SNP-F1    & \textbf{99.70}\%   & 99.35\% & 99.32\%          & 99.16\%          \\ \midrule
\multirow{2}{*}{VinDr-Mammo}    & \multirow{2}{*}{5-class}  & Macro-AUC & 64.50\%    & 66.29\%          & \textbf{71.76\%} & 71.42\%          \\
                                 &                           & Macro-F1  & N/A       & 29.81\%          & \textbf{35.7\%}           & 33.90\%           \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (mass)\end{tabular}} &
  \multirow{2}{*}{3-class} &
  Macro-AUC &
  N/A &
  70.11\% &
  73.09\% &
  \textbf{73.31\%} \\
                                                   
                                 &                           & Macro-F1  & N/A       & 47.23\%          & 49.98\%          & \textbf{51.12\%} \\ \midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (calcification)\end{tabular}} &
  \multirow{2}{*}{3-class} &
  Macro-AUC &
  N/A &
  81.40\% &
  \textbf{82.22\%} &
  80.90\% \\
                                 &                           & Macro-F1  & \textbf{70.71\%}       & 67.86\%          & 63.81\% & 63.03\%          \\ \bottomrule
\end{tabular}
\end{table}

\paragraph{Performance on medical visual question answering} 
Since we formulated both close-end and open-end QA pairs in three VQA datasets as an open-ended language decoding task conditioned on visual input, we used BLEU-1 and token-level F1 scores to assess the performance of Med-PaLM M. This is in contrast with many prior works which used a string-level accuracy evaluation metric as they often considered VQA as a classification task on a set of pre-defined fixed-number answer candidates~\cite{liu2023q2atransformer, eslami2021does}. This accuracy metric has the weakness of failing to capture "near misses" of groundtruth answers, particularly in our open-ended generative setup. We also noted that only human validation by experts can provide additional insights on the quality of model answers beyond token-level or string-level matching metrics. As shown in Table~\ref{tab:results-vqa}, Med-PaLM M surpassed previous SOTA using a similar generative approach across all three datasets and metrics~\cite{van2023open, bazi2023vision}. In particular, model performance increased with scaling up the language model on VQA-RAD and Path-VQA. On Slake-VQA, the best performance was achieved with the medium size model variant. These results suggest that scaling up language models is beneficial for visual-language tasks where language reasoning is conditioned on visual understanding.




\begin{table}[ht]
\small
\centering
\caption{\textbf{Med-PaLM M performance on medical visual question answering.} Med-PaLM exceeds prior SOTA on all three VQA tasks.}
\label{tab:results-vqa}
\begin{tabular}{cccccc}
\toprule
Dataset                    & Metric      & SOTA    & Med-PaLM M (12B)    & Med-PaLM M (84B)  & Med-PaLM M (562B) \\ \midrule
\multirow{2}{*}{VQA-RAD}   & BLEU-1    & 71.03\%  & 64.02\%        & 69.38\%          & \textbf{71.27\%} \\
                          
                           & F1          & N/A     & 50.66\%        & 59.90\%           & \textbf{62.06\%} \\ \midrule
\multirow{2}{*}{Path-VQA}  & BLEU-1      & 70.30\%  & 68.97\%        & 70.16\%          & \textbf{72.27\%} \\
                           
                           & F1          & 58.40\%  & 57.24\%        & 59.51\%          & \textbf{62.69\%} \\ \midrule
\multirow{2}{*}{Slake-VQA} & BLEU-1      & 78.60\%  & 90.77\%        & \textbf{92.7\%}  & 91.64\%          \\
                           & F1          & 78.10\%  & 86.22\%        & \textbf{89.28\%} & 87.50\%           \\ \bottomrule
\end{tabular}
\end{table}



\paragraph{Performance on chest X-ray report generation} 
To measure the quality of generated chest X-ray reports using automatic metrics, we computed common natural language generation metrics such as BLEU-1, BLEU-4, ROUGE-L, CIDEr-D~\cite{vedantam2015cider}, in addition to the clinical efficacy (CE) metrics and F1-RadGraph which were designed to capture the factuality and diagnostic accuracy in the generated reports. Specifically, we used CheXbert~\cite{smit2020chexbert}, an automatic radiology report labeller based on a BERT model improved with expert annotation, to extract the 14 CheXpert pathological observations from a given report. For each observation, the predicted label was compared against the groundtruth label to compute CE metrics. F1-RadGraph generalizes over CheXbert labeller to more observation categories by measuring the overlapping clinical entities and relations between a generated report and the reference report~\cite{yu2022evaluating}. In line with previous studies~\cite{liu2019clinically, chen2020generating, miura2020improving, nicolson2022improving, bannur2023learning, tanida2023interactive, jeong2023multimodal}, we reported the macro-F1 and micro-F1 scores averaged over 5 major observations and all 14 observations for CE metrics, respectively. As shown in Table~\ref{tab:results-cxr-report-gen}, Med-PaLM M achieved a new SOTA on all CE metrics and F1-RadGraph, with a substantial increase of about 9 points on macro-F1-14 and micro-F1-14 averaged across all clinical relevant observations over previous best SOTA results in~\cite{nicolson2022improving, jeong2023multimodal}. The macro-average F1 resulted in a lower score than the micro-average F1 over 14 observation categories because of the worse model performance on some categories with very low representation in the training data. Notably, improvements on F1 scores were more prominent across all 14 categories than over the 5 major categories for Med-PaLM M. This is likely due to the benefit of jointly training with the classification tasks on those minority conditions. We consider such positive task transfer as one of the main advantages of a generalist multi-task model over a specialized single-task model. On text overlap based natural language generation metrics, Med-PaLM M did not outperform existing SOTA results. However, the pitfalls of these automatic metrics have been raised by many studies, particularly in that they fail to capture the factual correctness and do not align well with radiologist judgements~\cite{liu2019clinically, miura2020improving, yu2022evaluating, tanida2023interactive}. 

Interestingly, our largest model Med-PaLM M (562B) did not achieve the best performance, falling slightly short of Med-PaLM M (84B). Furthermore, the gap in performance across three model sizes is relatively small across all types of metrics. The diminishing return of increasing the size of the language model is likely because the output space for chest X-ray report generation is fairly confined to a set of template sentences and limited number of conditions. It is also possible that the task performance is primarily limited by the vision encoder, particularly in how well it is adapted for this domain. As noted by~\citet{xu2021vitae}, ViT lacks inductive bias for modeling local visual features which are often crucial for interpreting medical images. To overcome this limitation, large-scale medical training data may be required to enable benefit from size scaling. Additionally, the input image size $224 \times 224 \times 3$ we used cause loss in resolution, which is a tradeoff we made to shorten the length of embedded image tokens to fit within the context length limit of the language model.

\begin{table}[ht]
\small
\centering
\caption{\textbf{Med-PaLM M performance on chest X-ray report generation.} We use text-overlap based and clinical factuality based automatic metrics to evaluate the quality of model-generated reports. Med-PaLM M sets new SOTA on all metrics designed to capture clinical efficacy and correctness. Across three Med-PaLM M variants, the medium-sized model achieves the best performance.}
\label{tab:results-cxr-report-gen}
\begin{tabular}{ccccc}
\toprule
Metric              & SOTA    & Med-PaLM M (12B)    & Med-PaLM M (84B)  & Med-PaLM M (562B) \\ \midrule
Micro-F1-14 & 44.20\%  & 51.41\%        & \textbf{53.56\%} & 51.60\% \\
Macro-F1-14 & 30.70\%  & 37.31\%        & \textbf{39.83\%} & 37.81\% \\
Micro-F1-5  & 56.70\%  & 56.54\%        & \textbf{57.88\%} & 56.28\% \\
Macro-F1-5  & N/A     & 50.57\%        & \textbf{51.60\%} & 49.86\% \\
F1-RadGraph         & 24.40\%  & 25.20\%        & \textbf{26.71\%} & 26.06\%          \\
BLEU-1           & \textbf{39.48\%} & 30.90\%        & 32.31\% & 31.73\% \\
BLEU-4           & \textbf{13.30\%} & 10.43\%        & 11.31\%          & 11.50\%  \\
ROUGE-L             & \textbf{29.60\%} & 26.16\%        & 27.29\% & 27.49\% \\
CIDEr-D               & \textbf{49.50\%} & 23.43\%        & 26.17\% & 25.27\%          \\
 \bottomrule 
\end{tabular}
\end{table}


\section{Details on Human Evaluation}

Figures \ref{fig-appendix:human-evaluation-ui-side-by-side} and \ref{fig-appendix:human-evaluation-ui-independent} depict the task interfaces used for side-by-side and independent radiologist evaluations, including the task input and annotation prompts presented to radiologist raters.
For ease of a detailed inspection (e.g., identification of subtle structures), the built-in medical image viewer provided tools for raters to adjust the chest X-ray image, including zoom, gamma, and blend controls.

It is worth noting that a non-trivial number of reports in the MIMIC-CXR dataset contain references to prior studies (e.g., ``Compared to the prior radiograph [...]'') or references to multiple views (e.g., ``Ap and lateral views of the chest are compared.''). By contrast, the input to our model is a single image and indication from a single study. As a result, these artifacts in the training corpus render the model prone to hallucination of references to non-existent prior imaging studies or non-existent X-ray views.
Our human evaluation task interface accounted for the presence of these artifacts by providing the option to categorize erroneous passages as ``Refers to view that is not present'' or ``Refers to study that is not present''.
Future work may leverage the CXR-PRO dataset~\cite{ramesh2022improving}, a cleaned version of MIMIC-CXR with all prior references removed, to mitigate this issue during model development. 

% Figure environment removed

% Figure environment removed

For the purpose of analysis, we distinguished between clinical errors (i.e., ``Finding I do not agree is present'', ``Incorrect location of finding'') and non-clinical errors (i.e., ``Refers to view that is not present'' or ``Refers to study that is not present'').
Table \ref{tab-appendix:human-evaluation-results} summarizes the rates of omissions and errors identified by clinician raters in radiology reports generated by different Med-PaLM M models. Here, we report the rate of total errors, including all clinical and non-clinical error types. On average, the best performing Med-PaLM M model produces 0.58 total errors per report.

One important limitation of our human evaluation approach is the inter-rater variability.
Similar to~\cite{yu2022evaluating}, which used a comparable evaluation scheme, we also observed that the same radiology report often was annotated with varying error and omission passages by different radiologist raters.
While this is a common phenomenon in studies that use subjective ratings from clinicians, future work may aim to further refine rater instructions and improve rater calibration to reduce variability.

\begin{table}[ht]
\small
\centering
\caption{\textbf{Independent human evaluation details.} Rates of omissions and errors identified by clinician raters in radiology reports generated by different Med-PaLM M models. Clinical errors are those related to the presence, location or severity of a clinical finding. Total errors include both clinical errors and non-clinical errors (i.e., passages referring to views or prior studies not present).}
\label{tab-appendix:human-evaluation-results}
\begin{tabular}{cccc}
\toprule
 Model Size                              & Med-PaLM M (562B)  & Med-PaLM M (84B)   & Med-PaLM M (12B)                 \\
\midrule
Significant Omissions                    & 0.10 (95\% CI, 0.08 - 0.12)  & 0.09 (95\% CI, 0.07 - 0.10)  & 0.08 (95\% CI, 0.06 - 0.10)  \\
Total Omissions                          & 0.13 (95\% CI, 0.11 - 0.16)  & 0.12 (95\% CI, 0.10 - 0.15)  & 0.12 (95\% CI, 0.10 - 0.15)  \\  \midrule
Significant Clinical Errors              & 0.26 (95\% CI, 0.23 - 0.29)  & 0.23 (95\% CI, 0.20 - 0.27)  & 0.26 (95\% CI, 0.22 - 0.29) \\
Total Clinical Errors                    & 0.29 (95\% CI, 0.25 - 0.32)  & 0.25 (95\% CI, 0.22 - 0.28)  & 0.28 (95\% CI, 0.24 - 0.31) \\
Total Errors                             & 0.63 (95\% CI, 0.58 - 0.68)  & 0.59 (95\% CI, 0.54 - 0.64)  & 0.58 (95\% CI, 0.53 - 0.63) \\
\bottomrule
\end{tabular}
\end{table}

\section{MultiMedBench Examples}

In \cref{tab:examples--multimedbench-classification,tab:examples--multimedbench-vqa-report-gen,tab:examples--report-summarization,tab:examples--medmcqa,tab:examples--medqa} we provide examples of various MultiMedBench tasks.

\begin{table}[ht]
\footnotesize
\caption{\textbf{Examples of the classification tasks in MultiMedBench.}}
\centering
\begin{tabular}{p{0.15\linewidth} p{0.6\linewidth}  p{0.1\linewidth}}
\toprule
Image & Task and input prompt & Target\\ \midrule


% dermatology
\raisebox{-1.8\height}{% Figure removed} 
& \textbf{Classification (PAD-UFES-20)}

\textbf{Instructions:} You are a helpful dermatology assistant. The following are questions about skin lesions. Categorize the skin lesions into the most likely class given the patient history.

Given <img>. Patient History: Age: 51, Gender: female, Smoke: false, Drink: false, Family skin cancer history: true, Family any cancer history: false, Lesion region: back, Lesion itch: false, Lesion grew: false, Lesion bled: false, Lesion elevation: false, Fitzpatrick scale: 1.0, Diameters (mm): [12.0, 8.0]. 
Q: Which of the following is the most likely diagnosis of the patient's skin lesion? 
(A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma 
A: Basal Cell Carcinoma.

Given \textbf{<img>}. Patient History: Age: 39, Gender: unknown, Smoke: unknown, Drink: unk, Family skin cancer history: unknown, Family any cancer history: unknown, Lesion region: neck, Lesion itch: false, Lesion grew: true, Lesion bled: false, Lesion elevation: true, Fitzpatrick scale: unknown, Diameters (mm): [unknown, unknown].
Q: Which of the following is the most likely diagnosis of the patient's skin lesion? 
(A) Nevus (B) Basal Cell Carcinoma (C) Squamous Cell Carcinoma (D) Actinic Keratosis (E) Seborrheic Keratosis (F) Melanoma
A:
& Nevus. \\

\midrule
% radiology
\raisebox{-1.1\height}{% Figure removed} 
& \textbf{Classification (MIMIC-CXR)}

\textbf{Instructions:} You are a helpful radiology assistant. The following are questions about findings in chest X-ray in different views. Identify if a specific type of abnormality is shown in the X-ray.

Given the AP view X-ray image <img>. Q: Is cardiomegaly indicated by the image?
(A) No (B) Yes

A: Yes.

Given the AP view X-ray image \textbf{<img>}. Q: Is cardiomegaly indicated by the image?
(A) No (B) Yes

A:
& Yes. \\
\midrule
%Vindr

\raisebox{-1.2\height}{% Figure removed} 
& \textbf{Classification (VinDr-Mammo)} 

\textbf{Instructions:} You are a helpful medical assistant. The following are questions about mammography reading. Provide a breast-level assessment based on the BI-RADS categories.

Given mammogram image <img>. Image view: bilateral craniocaudal
Q: What is the most likely breast BI-RADS score?
(A) 1 (B) 2 (C) 3 (D) 4 (E) 5
A: 4.

Given mammogram image \textbf{<img>}. Image view: bilateral craniocaudal
Q: What is the most likely breast BI-RADS score?
(A) 1 (B) 2 (C) 3 (D) 4 (E) 5
A:
& 
3.\\
\midrule
% cbis-calcification
\raisebox{-0.7\height}{% Figure removed} 
& \textbf{Classification (CBIS-DDSM Calcification)}

Given mammogram image \textbf{<img>}. Image view: CC
Q: Which of the following is the most likely type of the patient's breast calcification?
(A) BENIGN (B) BENIGN\_WITHOUT\_CALLBACK (C) MALIGNANT

A:
& MALIGNANT. \\

\midrule
% cbis-mass
\raisebox{-0.6\height}{% Figure removed} 
& \textbf{Classification (CBIS-DDSM Mass)}

Given mammogram image \textbf{<img>}. Image view: CC
Q: Which of the following is the most likely type of the patient's breast mass? (A) BENIGN (B) BENIGN\_WITHOUT\_CALLBACK (C) MALIGNANT
A:
& BENIGN. \\
\midrule
%variant calling

\raisebox{-0.9\height}{% Figure removed} 
& \textbf{Genomic variant calling} 

\textbf{Instructions:} You are a helpful genetic assistant. The following are questions about variant calling. Identify the number of copies of the putative variant in pileup images.

Given \textbf{<img>}. Q: How many copies of this putative variant are shown in the middle of the image? (A) 0 (B) 1 (C) 2 A:
& 
1.\\
\bottomrule
\end{tabular}
\label{tab:examples--multimedbench-classification}
\end{table}


\begin{table}[ht]
\footnotesize
\caption{\textbf{Examples of VQA and chest X-ray report generation tasks in MultiMedBench.}}
\centering
\begin{tabular}{p{0.15\linewidth} p{0.55\linewidth}  p{0.25\linewidth}}
\toprule
Image & Task and input prompt & Target\\ \midrule


% vqa-rad
\raisebox{-1\height}{% Figure removed} 
& \textbf{VQA-RAD}

\textbf{Instructions:} You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed.

Given <img>. Q: Can you diagnose a pericardial effusion from this image? (closed domain) 

A: No.

Given \textbf{<img>}. Q: What cut of the body is this image? (open domain) A:
& Axial. \\

\midrule
% slake-vqa
\raisebox{-1\height}{% Figure removed} 
& \textbf{Slake-VQA}

\textbf{Instructions:} You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed.

Given <img>. Q: Is the lung healthy? 

A: No.

Given \textbf{<img>}. Q: Which part of the body does this image belong to? A:
& Brain. \\
\midrule
%Path-VAQ

\raisebox{-1.2\height}{% Figure removed} 
& \textbf{Path-VQA} 

\textbf{Instructions:} You are a helpful medical assistant. The following are questions about medical knowledge. Solve them in a step-by-step fashion, referring to authoritative sources as needed.

Given <img>. Q: What is present ? (other) 

A: Abdomen.

Given \textbf{<img>}. Q: What is there of large numbers of macrophages within the alveolar spaces with only slight fibrous thickening of the alveolar walls? (other) 

A:
& 
accumulation of large numbers of macrophage.\\
\midrule

\raisebox{-2.3\height}{% Figure removed} 
& \textbf{Chest X-ray report generation} 

\textbf{Instructions:} You are a helpful radiology assistant. Describe what lines, tubes, and devices are present and each of their locations. Describe if pneumothorax is present; if present, describe size on each side. Describe if pleural effusion is present; if present, describe amount on each side. Describe if lung opacity (atelectasis, fibrosis, consolidation, infiltrate, lung mass, pneumonia, pulmonary edema) is present; if present, describe kinds and locations. Describe the cardiac silhouette size. Describe the width and contours of the mediastinum. Describe if hilar enlargement is present; if enlarged, describe side. Describe what fractures or other skeletal abnormalities are present. 

Given the LATERAL view X-ray image <img>. Reason for the study: Amiodarone routine surveillance. Q: Describe the findings in the image following the instructions.

A: As compared to the previous radiograph there is no relevant change. Normal lung volumes. Mild bilateral apical scarring. Normal size of the cardiac silhouette and tortuosity of the thoracic aorta. No pathologic findings in the lung parenchyma notably no evidence of fibrotic lung parenchymal changes. A faint 2 mm rounded opacity projecting over the lower aspect of the fourth right rib and internally to the upper border of the second right rib is seen on the frontal radiograph only and likely reflects structure on the skin. 

Given the PA view X-ray image \textbf{<img>}. Reason for the study: History m with malaise pneumonia. 
Q: Describe the findings in the image following the instructions. 

A:
& 
Again demonstrated is subtly increased opacity at the base of the right lung similar in appearance to multiple prior radiographs. There is no pneumothorax or pleural effusion. The cardiomediastinal and hilar contours are stable.\\
\bottomrule
\end{tabular}
\label{tab:examples--multimedbench-vqa-report-gen}
\end{table}


\begin{table}[!]
\footnotesize
\centering
\caption{\textbf{Example of the radiology report summarization task in MultiMedBench.}}
\vspace{3pt}
\label{tab:examples--report-summarization}
\begin{tabular}{l@{\hspace{.1em}}l@{\hspace{0.1em}}}
\toprule
{{\begin{tabular}[l]{@{}p{0.98\textwidth}}
\textbf{\textit{Input}}

\textbf{Instructions:} You are a helpful radiology assistant. The following are questions about radiology reports. Summarize the findings in the report into diagnostic statements. 

Given the findings: there is an intraparenchymal hemorrhage in the right cerebellar hemisphere measuring 1.7 cm with vasogenic edema and mass effect to the fourth ventricle. there is high density within the fissure of the right cerebellum suggestive of subarachnoid component. there is high density along the right tentorium, possibly representing subarachnoid hematoma, however, the finding is equivocal. there is no hydrocephalus, but there is mass effect and distortion of the fourth ventricle. there is no shift of normally midline supratentorial structures, and \underline{\hspace{0.6cm}}- white differentiations are preserved in the cerebral hemisphere. the surrounding osseous and soft tissue structures are unremarkable. mastoid air cells are not well pneumatized. there is mild mucosal thickening in the ethmoid sinuses. 
Q: Summarize the findings. 

A:
\\\\
\textbf{\textit{Target}}

1. 1.7-cm right cerebellar parenchymal hemorrhage with surrounding vasogenic edema and mass effect to the fourth ventricle, with adjacent subarachnoid hemorrhage. possible right subdural hemorrhage along the right tentorium, however, the evaluation is limited. differential diagnosis of the etiology of the bleeding included tumor, avm, and hypertension. the finding was discussed with dr. \underline{\hspace{0.6cm}} by telephone immediately after interpretation.
\end{tabular}}} 
& \\
\bottomrule 
 
\end{tabular}
\end{table}



\begin{table}[!]
\footnotesize
\centering
\caption{\textbf{Example from MedMCQA in MultiMedBench.}}
\vspace{3pt}
\label{tab:examples--medmcqa}
\begin{tabular}{l@{\hspace{.1em}}l@{\hspace{0.1em}}}
\toprule
{{\begin{tabular}[l]{@{}p{0.98\textwidth}}
\textbf{\textit{Input}}

\textbf{Instructions:} The following are multiple choice questions about medical knowledge. Solve them in a step-by-step fashion, starting by summarizing the available information. Output a single option from the four options as the final answer.
\\\\
Question: Which of the following is an intermediate-acting local anaesthetic which is an amino amide causing methemoglobinemia?\\
(A) Procaine (B) Prilocaine (C) Etidocaine (D) Ropivacaine\\
Answer: Prilocaine.
\\\\
Question: A 5-day-old male infant is diagnosed with Hirschsprung disease. CT scan examination reveals an abnormally dilated colon. Which of the following is the most likely embryologic mechanism responsible for Hirschsprung disease?\\
(A) Failure of neural crest cells to migrate into the walls of the colon (B) Incomplete separation of the cloaca (C) Failure of recanalization of the colon (D) Defective rotation of the hindgut\\
Answer: Failure of neural crest cells to migrate into the walls of the colon.
\\\\
Question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma
(A) Hyperplasia (B) Hyperophy (C) Atrophy (D) Dyplasia\\
Answer:
\\\\
\textbf{\textit{Target}}


Atrophy.
\end{tabular}}} 
& \\

\bottomrule 
\end{tabular}
\end{table}




\begin{table}[!]
\footnotesize
\centering
\caption{\textbf{Example from MedQA in MultiMedBench.}}
\vspace{3pt}
\label{tab:examples--medqa}
\begin{tabular}{l@{\hspace{.1em}}l@{\hspace{0.1em}}}
\toprule
{{\begin{tabular}[l]{@{}p{0.98\textwidth}}
\textbf{\textit{Input}}

\textbf{Instructions:} The following are multiple choice questions about medical knowledge. Solve them in a step-by-step fashion, starting by summarizing the available information. Output a single option from the four options as the final answer.\\
Question: A 57-year-old man presents to his family physician for a checkup. He has had type 2 diabetes mellitus for 13 years, for which he has been taking metformin and vildagliptin. He has smoked 10–15 cigarettes daily for 29 years. Family history is irrelevant. Vital signs include: temperature 36.6°C (97.8°F), blood pressure 152/87 mm Hg and pulse 88/min. Examination reveals moderate abdominal obesity with a body mass index of 32 kg/m². The remainder of the examination is unremarkable. His fasting lipid profile is shown: Total cholesterol (TC) 280 mg/dL Low-density lipoprotein (LDL)-cholesterol 210 mg/dL High-density lipoprotein (HDL)-cholesterol 40 mg/dL Triglycerides (TGs) 230 mg/dL Which of the following is the mechanism of action of the best initial therapy for this patient?\\
(A) Inhibition of cholesterol absorption (B) Bile acid sequestration (C) Inhibition of cholesterol synthesis (D) Activation of PPAR-alpha\\
Answer: Inhibition of cholesterol synthesis.\\ \\
Question: A 3-year-old girl presents with her mother for a well-child checkup. Recent laboratory data has demonstrated a persistent normocytic anemia. Her mother denies any previous history of blood clots in her past, but she says that her mother has also had to be treated for pulmonary embolism in the recent past, and her brother has had to deal with anemia his entire life. The patient's past medical history is noncontributory other than frequent middle ear infections. The vital signs upon arrival include: temperature, 36.7°C (98.0°F); blood pressure, 106/74 mm Hg; heart rate, 111/min and regular; and respiratory rate, 17/min. On physical examination, her pulses are bounding and fingernails are pale, but breath sounds remain clear. Oxygen saturation was initially 91\% on room air and electrocardiogram (ECG) shows sinus tachycardia. The patient's primary care physician orders a peripheral blood smear to further evaluate this finding, and preliminary results show a hemolytic anemia. Which of the following pathophysiologic mechanisms best describes sickle cell disease?\\
(A) Increased red blood cell sensitivity to complement activation, making patients prone to thrombotic events (B) A recessive beta-globin mutation causing morphological changes to the RBC (C) An X-linked recessive disease in which red blood cells are increasingly sensitive to oxidative stress (D) Secondarily caused by EBV, mycoplasma, CLL, or rheumatoid disease\\
Answer: A recessive beta-globin mutation causing morphological changes to the RBC.\\ \\
Question: A pulmonary autopsy specimen from a 58-year-old woman who died of acute hypoxic respiratory failure was examined. She had recently undergone surgery for a fractured femur 3 months ago. Initial hospital course was uncomplicated, and she was discharged to a rehab facility in good health. Shortly after discharge home from rehab, she developed sudden shortness of breath and had cardiac arrest. Resuscitation was unsuccessful. On histological examination of lung tissue, fibrous connective tissue around the lumen of the pulmonary artery is observed. Which of the following is the most likely pathogenesis for the present findings?\\
(A) Thromboembolism (B) Pulmonary ischemia (C) Pulmonary hypertension (D) Pulmonary passive congestion\\
Answer:
\\\\
\textbf{\textit{Target}}


Thromboembolism.
\end{tabular}}} 
& \\

\bottomrule 
 
\end{tabular}
\end{table}




