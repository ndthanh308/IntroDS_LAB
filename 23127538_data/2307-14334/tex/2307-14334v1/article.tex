\section{Introduction}
\label{sec:introduction}



Medicine is a multimodal discipline. Clinicians routinely interpret data from a wide range of modalities including clinical notes, laboratory tests, vital signs and observations, medical images, genomics, and more when providing care.


Despite significant progress in biomedical AI, most models today are unimodal single task systems~\cite{esteva2017dermatologist, gulshan2016development, tomavsev2019clinically}. Consider an existing AI system for interpreting mammograms~\cite{mckinney2020international}. Although the system obtains state-of-the-art (SOTA) performance on breast cancer screening, it cannot incorporate relevant information such as patient health records (e.g., breast cancer gene screening status), other modalities such as MRI, or published medical literature that might help contextualize, refine, and improve performance. Further, the system's output is constrained to a pre-specified set of possible classifications. It cannot verbally explain its prediction or engage in a collaborative dialogue to learn from a physician's feedback. This bounds performance and utility of these narrow, single-task, unimodal, specialist AI systems in real-world applications.



% Figure environment removed




The emergence of foundation models~\cite{bommasani2021opportunities} offers an opportunity to rethink the development of medical AI systems. These models are often trained on large-scale data with self-supervised or unsupervised objectives and can be rapidly and effectively
adapted to many downstream tasks and settings using in-context learning or few-shot finetuning~\cite{brown2020language,azizi2023robust}.
Further, they often have impressive generative capabilities that can enable effective human-AI interaction and collaboration. These advances enable the possibility of building a unified biomedical AI system that can interpret multimodal data with complex structures 
to tackle many challenging tasks. As the pace of biomedical data generation and innovation increases, so will the potential impact of such models, with a breadth of possible downstream applications spanning fundamental biomedical discovery to care delivery.

In this work, we detail our progress towards such a \textit{generalist} biomedical AI system - a unified model that can interpret multiple biomedical data modalities and handle many downstream tasks with the \textit{same set of model weights}. 
One of the key challenges of this goal has been the absence of comprehensive multimodal medical benchmarks.
To address this unmet need, we curate MultiMedBench, an open source multimodal medical benchmark spanning language, medical imaging, and genomics modalities with 14 diverse biomedical tasks including question answering, visual question answering, medical image classification, radiology report generation and summarization, and genomic variant calling.

We leverage MultiMedBench to design and develop Med-PaLM Multimodal (Med-PaLM M), a large-scale generalist biomedical AI system building on the recent advances in language~\cite{chowdhery2022palm, singhal2022large} and multimodal foundation models~\cite{driess2023palme, chen2022pali}. In particular, Med-PaLM M is a flexible multimodal sequence-to-sequence architecture that can easily incorporate and interleave various types of multimodal biomedical information. Further, the expressiveness of the modality-agnostic language decoder enables the handling of various biomedical tasks in a simple generative framework with a unified training strategy.


To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system that can interpret multimodal biomedical data and handle a diverse range of tasks with a single model. Med-PaLM M reaches performance competitive with or exceeding the state-of-the-art (SOTA) on all tasks in MultiMedBench, often surpassing specialized domain and task-specific models by a large margin. In particular, Med-PaLM M exceeds prior state-of-the-art on chest X-ray (CXR) report generation (MIMIC-CXR dataset) by over 8\% on the common success metric (micro-F1) for clinical efficacy. On one of the medical visual question answering tasks (Slake-VQA~\cite{liu2021slake}) in MultiMedBench, Med-PaLM M outperforms the prior SOTA results by over 10\% on the BLEU-1 and F1 metrics.

We perform ablation studies to understand the importance of scale in our generalist multimodal biomedical models and observe significant benefits for tasks that require higher-level language capabilities, such as medical (visual) question answering. Preliminary experiments also suggest evidence of zero-shot generalization to novel medical concepts and tasks across model scales, and emergent capabilities~\cite{wei2022emergent} such as zero-shot multimodal medical reasoning. We further perform radiologist evaluation of AI-generated chest X-ray reports and observe encouraging results across model scales.

Overall, these results demonstrate the potential of generalist biomedical AI systems for medicine. However, significant work remains in terms of large-scale biomedical data access for training such models, validating performance in real world applications, and understanding the safety implications. We outline these key limitations and directions of future research in our study. To summarize, our key contributions are as follows:

\begin{itemize}
    \item \textbf{Curation of MultiMedBench} We introduce MultiMedBench, a new multimodal biomedical benchmark spanning multiple modalities including medical imaging, clinical text and genomics with 14 diverse tasks for training and evaluating generalist biomedical AI systems.
    \item \textbf{Med-PaLM M, the first demonstration of a generalist biomedical AI system} We introduce Med-PaLM M, a single multitask, multimodal biomedical AI system that can perform medical image classification, medical question answering, visual question answering, radiology report generation and summarization, genomic variant calling, and more with the same set of model weights. Med-PaLM M reaches performance competitive with or exceeding state-of-the-art (SOTA) specialist models on multiple tasks in MultiMedBench without any task-specific customization.
    \item \textbf{Evidence of novel emergent capabilities in Med-PaLM M} Beyond quantitative evaluations of task performance, we observe evidence of zero-shot medical reasoning, generalization to novel medical concepts and tasks, and positive transfer across tasks. These experiments suggest promising potential of such systems in downstream data-scarce biomedical applications.
   \item \textbf{Human evaluation of Med-PaLM M outputs} Beyond automated metrics, we perform radiologist evaluation of chest X-ray reports generated by Med-PaLM M across different model scales.
   In a blinded side-by-side ranking on 246 retrospective chest X-rays, clinicians expressed a pairwise preference for Med-PaLM M reports over those produced by radiologists in up to 40.50\% of cases. Furthermore, the best Med-PaLM M model has on average 0.25 clinically significant errors per report.  These results are on par with human baselines from prior work~\cite{jeong2023multimodal}, suggesting potential clinical utility.
\end{itemize}

\section{Related Work}
\label{sec:related-work}



\subsection{Foundation models, multimodality, and generalists}
The emergence of the \textbf{foundation model} paradigm~\cite{bommasani2021opportunities} has had widespread impact across a variety of applications in language~\cite{chowdhery2022palm}, vision~\cite{dehghani2023scaling}, and other modalities~\cite{borsos2023audiolm}.  While the idea of transfer learning ~\cite{caruana1997multitask,thrun1998lifelong} using the weights of pretrained models has existed for decades~\cite{hinton2006fast,bengio2006greedy,vincent2008extracting,bengio2012deep}, a shift has come about due to the scale of data and compute used for pretraining such models~\cite{kaplan2020scaling}.  The notion of a foundation model further indicates that the model can be adapted to a wide range of downstream tasks~\cite{bommasani2021opportunities}.

Within the foundation model paradigm, \textbf{multimodality} \cite{ngiam2011multimodal} has also had a variety of important impacts -- in the datasets~\cite{schuhmann2022laion}, in the inter-modality supervision~\cite{jaegle2021perceiver}, and in the generality and unification of task specification~\cite{tsimpoukelli2021multimodal, alayrac2022flamingo}.  For example, language has specifically been an important enabler of foundation models in other modalities~\cite{chen2022pali, agostinelli2023musiclm}.  Visual foundation models such as CLIP~\cite{radford2021learning} are made possible by training on language-labeled visual datasets~\cite{thomee2016yfcc100m,schuhmann2022laion}, which are easier to collect from large-scale internet data than classification datasets with pre-determined class labels (i.e., ImageNet~\cite{deng2009imagenet}).  The benefits of joint language-and-vision supervision has also been noteworthy in generative modeling of images~\cite{rombach2021highresolution}, where text-to-image generative modeling has been notably more successful at producing high-fidelity image generation~\cite{saharia2022photorealistic} than purely unconditioned generative image modeling~\cite{dhariwal2021diffusion}.  Further, the flexibility of language also enables a wide range of task specifications all via one unified output space~\cite{radford2019language} -- it is possible to phrase tasks traditionally addressed by different output spaces, such as object detection and object classification, all jointly via the output space of language~\cite{chen2021pix2seq}.  Med-PaLM M additionally benefits from the generality of multimodality, both via a model~\cite{driess2023palme} pretrained on large vision-language datasets~\cite{chen2022pali}, and also by further biomedical domain finetuning through a unified generative language output space.

A related notion to that of a foundation model is that of a \textbf{generalist model} -- the same model with the same set of weights, without finetuning, can excel at a wide variety of tasks.  A single multitask \cite{caruana1997multitask} model which can address many tasks has been of long standing interest~\cite{collobert2008unified, ruder2017overview}, including for example in the reinforcement learning community \cite{lee2022multi}. Language-only models such as GPT-3~\cite{brown2020language} and PaLM~\cite{chowdhery2022palm} simultaneously excel at many tasks using only prompting and in-context learning. Recent work has also explored generalist models capable not only of performing many tasks, but also of processing many modalities~\cite{lu2022unified}. For example, the capabilities of Gato~\cite{reed2022generalist} span language, vision, and agent policy learning. PaLM-E~\cite{driess2023palme} further shows that it is possible to obtain a single generalist model which excels at  language-only tasks, vision-language tasks, and embodied vision-language tasks.
Med-PaLM M is specifically a generalist model designed for the biomedical domain, built by finetuning and aligning the PaLM-E generalist model.

\subsection{Multimodal foundation models in biomedicine}

Given the potential, there has been significant interest in multimodal foundation models for different biomedical applications.~\citet{moor2023foundation} discuss the notion of generalist medical AI, albeit without implementation or empirical results.~\citet{theodoris2023transfer} introduce Geneformer, a transformer~\cite{vaswani2017attention} based model pretrained on a corpus of about 30 million single-cell transcriptomes to enable context-specific predictions in low data network biology applications. BiomedGPT~\cite{zhang2023biomedgpt} is a multi-task biomedical foundation model pretrained on a diverse source of medical images, medical literature, and clinical notes using a combination of language model (LM) and masked image infilling objectives.  However, all these efforts are pretrained models and as such they require further task-specific data and finetuning to enable downstream applications. In contrast, Med-PaLM M is directly trained to jointly solve many biomedical tasks at the same time without requiring any further finetuning or model parameter updates. LLaVA-Med~\cite{li2023llava} is perhaps most similar to our effort. The authors use PubMed and GPT-4~\cite{bubeck2023sparks} to curate a multimodal instruction following dataset and finetune a LLaVA model with it. However, the experiments are limited to three medical visual question answering datasets and qualitative examples of conversations conditioned on a medical image. In contrast, our work is more comprehensive, spanning multiple modalities including medical imaging, clinical text, and genomics with 14 diverse tasks and expert evaluation of model outputs.

\subsection{Multimodal medical AI benchmarks}
To the best of our knowledge, there have been limited attempts to curate benchmarks for training and evaluating generalist biomedical AI models. Perhaps the work closest in spirit is BenchMD~\cite{wantlin2023benchmd}. The benchmark spans 19 publicly available datasets and 7 medical modalities, including 1D sensor data, 2D images, and 3D volumetric scans. However, their tasks are primarily focused on classification whereas our benchmark also includes generative tasks such as medical (visual) question answering, radiology report generation and summarization. Furthermore, there is currently no implementation of a generalist biomedical AI system that can competently handle all these tasks simultaneously.

\section{MultiMedBench: A Benchmark for Generalist Biomedical AI}
\label{sec:MultiMedBench}
We next describe MultiMedBench, a benchmark we curated to enable the development and evaluation of generalist biomedical AI. 
MultiMedBench is a multi-task, multimodal benchmark comprising 12 de-identified open source datasets and 14 individual tasks. It measures the capability of a general-purpose biomedical AI to perform a variety of clinically-relevant tasks. The benchmark covers a wide range of data sources including medical questions, radiology reports, pathology, dermatology, chest X-ray, mammography, and genomics. Tasks in MultiMedBench vary across the following axes:

\begin{itemize}
    \item \textbf{Task type:} question answering, report generation and summarization, visual question answering, medical image classification, and genomic variant calling.
    \item \textbf{Modality:} text, radiology (CT, MRI, and X-ray), pathology, dermatology, mammography, and genomics.
    \item \textbf{Output format:} open-ended generation for all tasks including classification.
\end{itemize}

\begin{table}[ht]
\small
\centering
\caption{\textbf{MultiMedBench overview.} Summary of MultiMedBench, the benchmark we introduce for the development and evaluation of Med-PaLM M. MultiMedBench consists of 14 individual tasks across 5 task types and 12 datasets spanning 7 biomedical data modalities. In total, the benchmark contains over 1 million samples.}
\label{tab:multimedbench-overview}
\begin{tabular}{@{}c@{\hspace{.01cm}}@{\hspace{.01cm}}c@{\hspace{.01cm}}ccc}
\toprule
\textbf{Task Type} & \textbf{Modality} & \textbf{Dataset}          & \textbf{Description}  \\ \midrule
\multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text}
& MedQA       & {\begin{tabular}[c]{@{}c@{}}US medical licensing exam-style, multiple-choice\end{tabular}} 
 \\ 
& & MedMCQA     &{\begin{tabular}[c]{@{}c@{}}Indian medical entrance exams, multiple-choice\end{tabular}} \\ 

& & PubMedQA     &{\begin{tabular}[c]{@{}c@{}}Biomedical literature questions, multiple-choice\end{tabular}} \\ 
\midrule
Report Summarization & Radiology & MIMIC-III       &{\begin{tabular}[c]{@{}c@{}}Summarizing findings in radiology reports\end{tabular}}  \\ 
\midrule
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Visual\\Question Answering\end{tabular}} & \multirow{2}{*}{Radiology}
& VQA-RAD & {\begin{tabular}[c]{@{}c@{}}Close/open-ended VQA on radiology images\end{tabular}}  \\
&& Slake-VQA & {\begin{tabular}[c]{@{}c@{}}English-Chinese bilingual VQA on radiology images\end{tabular}} \\
& Pathology & Path-VQA  & {\begin{tabular}[c]{@{}c@{}}Close/open-ended VQA on pathology images\end{tabular}}  \\
\midrule
Report Generation &Chest X-ray &MIMIC-CXR         & {\begin{tabular}[c]{@{}c@{}} Chest X-ray report generation\end{tabular}}  \\ 
\midrule
\multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Medical\\Image Classification\end{tabular}}
& Chest X-ray & MIMIC-CXR        &  {\begin{tabular}[c]{@{}c@{}} Binary classification of chest X-ray abnormalities\end{tabular}}  \\ 
& Dermatology &PAD-UFES-20  & {\begin{tabular}[c]{@{}c@{}} 6-class skin lesion image classification\end{tabular}} \\
& \multirow{3}{*}{Mammography}
& VinDr-Mammo         & {\begin{tabular}[c]{@{}c@{}} 5-class breast-level BI-RADS classification\end{tabular}} \\
&& CBIS-DDSM         &  {\begin{tabular}[c]{@{}c@{}} 3-class lesion-level classification (mass)\end{tabular}}  
\\
&& CBIS-DDSM          &  {\begin{tabular}[c]{@{}c@{}} 3-class lesion-level classification (calcification)\end{tabular}}
\\
& {\begin{tabular}[c]{@{}c@{}}Genomics\end{tabular}} & {\begin{tabular}[c]{@{}c@{}}PrecisionFDA\\Truth Challenge V2\end{tabular}}     & {\begin{tabular}[c]{@{}c@{}} Genomic variant calling as 3-class image classification\end{tabular}}  \\

\bottomrule 
\end{tabular}
\end{table}

Language-only tasks consist of medical question answering, including three of the MultiMedQA tasks used in \citet{singhal2022large}, and radiology report summarization. They were selected to assess a model's ability to comprehend, recall, and manipulate medical knowledge. Multimodal tasks include medical visual question answering (VQA), medical image classification, chest X-ray report generation, and genomic variant calling, which are well-suited to evaluate both the visual understanding and multimodal reasoning capabilities of these models.~\cref{tab:multimedbench-overview} includes an overview of the datasets and tasks in MultiMedBench - in total, the benchmark contains over 1 million samples. For detailed descriptions of individual datasets and tasks, see~\cref{appendix:multimedbench-details}.





\section{Med-PaLM M: A Proof of Concept for Generalist Biomedical AI}
\label{sec:med-palm-m}
In this section, we detail the methods underpinning the development of the Med-PaLM M model.
We first review preliminaries of the pretrained models in \cref{subsec:model-prelims} from which Med-PaLM M inherits, then discuss the datasets and training details involved in the finetuning and specialization of the model to the biomedical domain \cref{subsec:med-palm-m}.

\subsection{Model preliminaries}\label{subsec:model-prelims}

Note that Med-PaLM M inherits not only the architectures of these pretrained models, but also the general domain knowledge encoded in their model parameters.

\paragraph{Pathways Language Model (PaLM)} introduced by~\citet{chowdhery2022palm} is a densely-connected decoder-only Transformer \cite{vaswani2017attention} based large language model (LLM) trained using Pathways~\cite{barham2022pathways}, a large-scale ML accelerator orchestration system that enables highly efficient training across TPU pods. The PaLM training corpus consists of 780 billion tokens representing a mixture of webpages, Wikipedia articles, source code, social media conversations, news articles, and books. PaLM models were trained at sizes of 8, 62, and 540 billion parameters, and all three PaLM model variants are trained for one epoch of the training data. At the time of its announcement, PaLM 540B achieved breakthrough performance, outperforming finetuned state-of-the-art models on a suite of multi-step reasoning tasks and exceeding average human performance on BIG-bench~\cite{srivastava2022beyond}. 

\paragraph{Vision Transformer (ViT)} introduced by~\citet{dosovitskiy2020image} extends the Transformer~\cite{vaswani2017attention} architecture to visual data such as images and videos. In this work, we consider two ViT pre-trained models as vision encoders, the 4 billion (4B) parameters model from~\citet{chen2022pali} and the 22 billion (22B) parameters model from~\citet{dehghani2023scaling}. Both of these models were pretrained via supervised learning on a large classification dataset \cite{sun2017revisiting,zhai2022scaling} of approximately 4 billion images. 

\paragraph{PaLM-E} introduced by~\citet{driess2023palme} is a multimodal language model that can process sequences of multimodal inputs including text, vision, and sensor signals. The primary PaLM-E model uses pretrained PaLM and ViT, and was initially developed for embodied robotics applications but demonstrated strong performance on multiple vision language benchmarks such as OK-VQA~\cite{marino2019ok} and VQA v2~\cite{goyal2017making}. Furthermore, PaLM-E offers the flexibility to interleave images, text and sensor signals in a single prompt, enabling the model to make predictions with a fully multimodal context. PaLM-E also exhibits a wide array of capabilities including zero-shot multimodal chain-of-thought (CoT) reasoning, and few-shot in-context learning.
We therefore leverage the PaLM-E model as the base architecture for Med-PaLM M.

We consider three different combinations of LLM and vision encoders in our study - PaLM 8B with ViT 4B (PaLM-E 12B), PaLM 62B with ViT 22B (PaLM-E 84B) and PaLM 540B with ViT 22B (PaLM-E 562B). All models were pretrained on diverse vision-language datasets in addition to tasks across multiple robot embodiments as described in~\citet{driess2023palme}.  

\subsection{Putting it all together: Med-PaLM~M}
\label{subsec:med-palm-m}

Med-PaLM M is developed by finetuning and aligning the PaLM-E model to the biomedical domain using MultiMedBench. The following summarizes important methodological details underlying the development of the model.

\paragraph{Dataset and preprocessing}
We resized all the images in MultiMedBench to $224 \times 224\times3$, while preserving the original aspect ratio with padding if needed. The gray-scale images were converted to 3-channel images by stacking up the same image along the channel dimension. Task-specific prepossessing methods such as class balancing and image data augmentation are described in detail for each task in~\cref{appendix:multimedbench-details}.

\paragraph{Instruction task prompting and one-shot exemplar} Our goal is to train a generalist biomedical AI model to perform multiple tasks with multimodal inputs using a unified model architecture and a single set of model parameters. To this end, we trained the model with a mixture of distinct tasks simultaneously via instruction tuning \cite{wei2021finetuned}. Specifically, we provided the model with task-specific instructions to prompt the model to perform different types of tasks in a unified generative framework. The task prompt consists of an instruction, relevant context information, and a question. For example, as shown in~\cref{fig:task_prompts}, in the chest X-ray report generation task, we included the reason for the study and the image orientation information as additional context information for the model to condition its prediction on. Similarly, for the dermatology classification task, we provided the patient clinical history associated with the skin lesion image. We formulated all classification tasks as multiple choice questions where all possible class labels are provided as individual answer options and the model was prompted to generate the most likely answer as the target output. For other generative tasks such as visual question answering and report generation and summarization, the model was finetuned on the target response.


In order to enable the model to better follow instructions, for the majority of tasks (see~\cref{tab-appendix:data mixture}), we added a text-only \textit{``one-shot exemplar''} to the task prompt to condition the language model's prediction. The one-shot exemplar helps prompt the model with a partial input-output pair.
Importantly, for multimodal tasks, we replaced the actual image in the exemplar with a dummy text placeholder (with the text string ``<img>''): this (i) preserves training compute efficiency for single-image training, and also (ii) bypasses potential interference from cross-attention between a given text token and image tokens from multiple images~\cite{alayrac2022flamingo}. Our results show that this scheme is effective in prompting the model to generate the desired format of responses as detailed in~\cref{sec:results}.

\paragraph{Model training} We finetuned the pretrained 12B, 84B, and 562B parameter variants of PaLM-E on MultiMedBench tasks with mixture ratios denoted in~\cref{tab-appendix:data mixture}. These mixture ratios were empirically determined such that they are approximately proportional to the number of training samples in each dataset and ensuring at least one sample from each task is present in one batch. We performed an end-to-end finetuning of the PaLM-E model with the entire set of model parameters updated during training. For multimodal tasks, image tokens were interleaved with text tokens to form multimodal context input to the PaLM-E model. 
The multimodal context input contains at most 1 image for all finetuning tasks. However, we note that Med-PaLM M is able to process inputs with multiple images during inference.

We used the Adafactor optimizer~\cite{shazeer2018adafactor} with momentum of $\beta_1=0.9$, dropout rate of 0.1, and a constant learning rate schedule.
We used different sets of hyperparameters in our finetuning experiments for different model sizes, which are further detailed in~\cref{tab-appendix:finetuning hyperparameters}. 

The resulting model, Med-PaLM M (12B, 84B, and 562B), is adapted to the biomedical domain with the capability to encode and interpret multimodal inputs and perform tasks including medical (visual) question answering, radiology report generation and summarization, medical image classification, and genomic variant calling.

% Figure environment removed

\section{Evaluation}
\label{sec:evaluation}

In this section, we describe the purpose, scope, and methods of experimental evaluations. Results are presented in Section~\ref{sec:results}.
Evaluation experiments of Med-PaLM M were designed for the following purposes: 
\begin{itemize}
    \item \textbf{Evaluate generalist capabilities} We evaluated Med-PaLM M on all tasks in MultiMedBench across model scales. We provide initial insights on the effect of scaling ViT and LLM components across different tasks. We compared performance to previous SOTA (including specialist single-task or single-modality methods) and a state-of-art generalist model (PaLM-E) without biomedical finetuning.
    \item \textbf{Explore novel emergent capabilities} One hypothesized benefit of training a single flexible multimodal generalist AI system across diverse tasks is the emergence of novel capabilities arising from language enabled combinatorial generalization, such as to novel medical concepts and tasks. We explored this via qualitative and qualitative experiments.
    \item \textbf{Measure radiology report generation quality} Automatic natural language generation (NLG) metrics do not provide sufficient evaluation of the clinical applicability of AI-generated radiology reports. We therefore performed expert radiologist evaluation of AI-generated reports on the MIMIC-CXR dataset, including comparison to the radiologist-provided reference reports.
\end{itemize}



\subsection{Evaluation on MultiMedBench}
Med-PaLM M was simultaneously finetuned on a mixture of language-only and multimodal biomedical tasks in MultiMedBench. We assessed the model's in-distribution performance on these tasks by comparing to the corresponding SOTA results obtained from separate specialist models. Specifically, we used the same few-shot setup as in training for each task during evaluation. Task-specific metrics were computed on the test split of each task and compared to prior SOTA specialist AI systems. Note that for a small number of tasks described in~\cref{tab:multimedbench-overview}, we were not able to find a sufficiently similar prior attempt for comparison.

\subsection{Evaluation of language enabled zero-shot generalization}

To probe Med-PaLM M's ability to generalize to previously unseen medical concepts, we evaluate the model's ability to predict the presence or absence of tuberculosis (TB) from chest X-ray images.
We used the Montgomery County chest X-ray set (MC) for this purpose.
The dataset contains 138 frontal chest X-rays, of which 80 are normal cases and 58 cases have manifestations of TB~\cite{jaeger2014two}. Each case also contains annotations on the abnormality seen in the lung. We note that Med-PaLM M has been trained on MIMIC-CXR dataset; however, it is not trained to explicitly predict the TB disease label.

We evaluated the accuracy across model scales by formulating this problem as a two-choice question answering task where the model was prompted (with a text-only one-shot exemplar) to generate a yes/no answer about the presence of TB in the input image.

We further explored zero-shot chain-of-thought (CoT) multimodal medical reasoning ability of the model by prompting with a text-only exemplar (without the corresponding image) and prompting the model to generate the class prediction and an accompanying report describing the image findings. We note that while we did prompt the model with a single text-only input-output pair, we omitted the image (used a dummy text placeholder instead) and the text exemplar was hand-crafted rather than drawn from the training set. Hence, this approach can be considered zero-shot rather than one-shot.

In order to assess Med-PaLM M's ability to generalize to novel task scenarios, we evaluated the model performance on two-view chest X-ray report generation - this is a novel task given the model was trained to generate reports only from a single-view chest X-ray.

Finally, we also probed for evidence of positive task transfer as a result of jointly training a single generalist model to solve many different biomedical tasks. To this end, we performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification tasks from the task mixture. We compared this model variant to the Med-PaLM M 84B variant trained on the complete MultiMedBench mixture on the chest X-ray report generation task with the expectation of improved performance in the latter. 

\subsection{Clinician evaluation of radiology report generation}
To further assess the quality and clinical applicability of chest X-ray reports generated by Med-PaLM M and understand the effect of model scaling, we conducted a human evaluation using the MIMIC-CXR dataset. The evaluation was performed by four qualified thoracic radiologists based in India.

\paragraph{Dataset} The evaluation set consisted of 246 cases selected from the MIMIC-CXR test split. To match the expected input format of Med-PaLM M, we selected a single image from each study. We excluded studies that had ground truth reports mentioning multiple X-ray views or past examinations of the same patient.

\paragraph{Procedure} We conducted two complementary human evaluations: (1) \textit{side-by-side evaluation} where raters compared multiple alternative report findings and ranked them based on their overall quality, and (2) \textit{independent evaluation} where raters assessed the quality of individual report findings.
Prior to performing the final evaluation, we iterated upon the instructions for the raters and calibrated their grades using a pilot set of 25 cases that were distinct from the evaluation set. Side-by-side evaluation was performed for all 246 cases, where each case was rated by a single radiologist randomly selected from a pool of four. For independent evaluation, each of the four radiologists independently annotated findings generated by three Med-PaLM M model variants (12B, 84B, and 562B) for every case in the evaluation set. Radiologists were blind to the source of the report findings for all evaluation tasks, and the reports were presented in a randomized order.

\paragraph{Side-by-side evaluation} The input to each side-by-side evaluation was a single chest X-ray, along with the ``indication'' section from the MIMIC-CXR study. Four alternative options for the ``findings'' section of the report were shown to raters as depicted in~\cref{fig-appendix:human-evaluation-ui-side-by-side}. The four alternative ``findings'' sections corresponded to the dataset reference report's findings, and findings generated by three Med-PaLM M model variants (12B, 84B, 562B).
Raters were asked to rank the four alternative findings based on their overall quality using their best clinical judgement.

\paragraph{Independent evaluation} For independent evaluation, raters were also presented with a single chest X-ray, along with the indication and reference report's findings from the MIMIC-CXR study (marked explicitly as such), but this time only a single findings paragraph generated by Med-PaLM M as shown in~\cref{fig-appendix:human-evaluation-ui-independent}. Raters were asked to assess the quality of the Med-PaLM M generated findings in the presence of the reference inputs provided and their own judgement of the chest X-ray image.
The rating schema proposed in~\citet{yu2022evaluating} served as inspiration for our evaluation task design.

First, raters assessed whether the quality and view of the provided image were sufficient to perform the evaluation task fully.
Next, they annotated all passages in the model-generated findings that they disagreed with (errors), and all missing parts (omissions).
Raters categorized each error passage by its type (no finding, incorrect finding location, incorrect severity, reference to non-existent view or prior study), assessed its clinical significance, and suggested alternative text to replace the selected passage. Likewise, for each omission, raters specified a passage that should have been included and determined if the omission had any clinical significance.


\begin{table}[ht]
\footnotesize
\centering

\caption{\textbf{Performance comparison on MultiMedBench.} We compare Med-PaLM M with specialist SOTA models and a generalist model (PaLM-E 84B) without biomedical domain finetuning. Across all tasks, datasets and metrics combination in MultiMedBench, we observe Med-PaLM M performance near or exceeding SOTA. Note that these results are achieved by Med-PaLM M with the same set of model weights without any task-specific customization.}
\label{tab:results-med-palm-m-best}
\begin{tabular}{@{}c@{\hspace{.05cm}}@{\hspace{.07cm}}c@{\hspace{.07cm}}ccccc}
\toprule
Task Type & Modality  &   Dataset   & Metric   & SOTA &\begin{tabular}[c]{@{}c@{}}PaLM-E \\ (84B)\end{tabular} &\begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (Best)\end{tabular} \\ \midrule

\multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text}
& MedQA & Accuracy & \textbf{86.50\%}~\cite{singhal2023towards}       & 28.83\%         & 69.68\% \\
&& MedMCQA       & Accuracy & \textbf{72.30\%}~\cite{singhal2023towards}        & 33.35\%          & 62.59\% \\
&& PubMedQA      & Accuracy & \textbf{81.80\%}~\cite{singhal2023towards}        & 64.00\%          & 80.00\%    \\ 
\midrule
\multirow{3}{*}{Report Summarization} & \multirow{3}{*}{Radiology}
&\multirow{3}{*}{MIMIC-III} & ROUGE-L     & \textbf{38.70\%}~\cite{van2023radadapt}    & 3.30\%         & 32.03\% \\
&&& BLEU        & \textbf{16.20\%}~\cite{van2023radadapt}     & 0.34\%         & 15.36\% \\
&&& F1-RadGraph & \textbf{40.80\%}~\cite{van2023radadapt}           & 8.00\%         & 34.71\%\\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Visual \\Question Answering\end{tabular}} & \multirow{4}{*}{Radiology}
&\multirow{2}{*}{VQA-RAD}   & BLEU-1    & 71.03\%~\cite{bazi2023vision}   & 59.19\%          & \textbf{71.27\%} \\
                          
                           &&& F1          & N/A     & 38.67\%           & \textbf{62.06\%} \\ 
 &&\multirow{2}{*}{Slake-VQA} & BLEU-1      & 78.60\%~\cite{van2023open}   & 52.65\%        & \textbf{92.7\%}          \\
                          & && F1          & 78.10\%~\cite{van2023open}   & 24.53\%        & \textbf{89.28\%}         \\ 

& \multirow{2}{*}{Pathology} &\multirow{2}{*}{Path-VQA}  & BLEU-1   & 70.30\%~\cite{van2023open}  & 54.92\%          & \textbf{72.27\%} \\
                           
                           &&& F1          & 58.40\%~\cite{van2023open}    & 29.68\%          & \textbf{62.69\%} \\ 

\midrule
\multirow{9}{*}{Report Generation} &\multirow{9}{*}{Chest X-ray}
&\multirow{9}{*}{MIMIC-CXR} &Micro-F1-14 & 44.20\%~\cite{nicolson2022improving}  & 15.40\%        & \textbf{53.56\%} \\
&&&Macro-F1-14 & 30.70\%~\cite{nicolson2022improving}  & 10.11\%        & \textbf{39.83\%}  \\
&&&Micro-F1-5  & 56.70\%~\cite{miura2020improving}  & 5.51\%        & \textbf{57.88\%}  \\
&&&Macro-F1-5  & N/A     & 4.85\%        & \textbf{51.60\%}  \\
&&&F1-RadGraph         & 24.40\%~\cite{jeong2023multimodal}  & 11.66\%        & \textbf{26.71\%}          \\
&&&BLEU-1           & \textbf{39.48\%}~\cite{nicolson2022improving} & 19.86\%        & 32.31\% \\
&&&BLEU-4           & \textbf{13.30\%}~\cite{miura2020improving} & 4.60\%                & 11.50\%  \\
&&&ROUGE-L             & \textbf{29.60\%}~\cite{bannur2023learning} & 16.53\%         & 27.49\% \\
&&&CIDEr-D               & \textbf{49.50\%}~\cite{tanida2023interactive} & 3.50\%        & 26.17\%   \\

\midrule
\multirow{12}{*}{Image Classification} &\multirow{2}{*}{Chest X-ray}
&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MIMIC-CXR\\ (5 conditions)\end{tabular}} & Macro-AUC & \textbf{81.27\%}~\cite{rammuni2022effective}    & 51.48\%         & 79.09\% \\
&&& Macro-F1  & N/A       & 7.83\%        & \textbf{41.57\%} \\ 
&\multirow{2}{*}{Dermatology} 
&\multirow{2}{*}{PAD-UFES-20}  
& Macro-AUC & N/A & 63.37\%    & \textbf{97.27\%}      \\
&&& Macro-F1  & N/A       & 1.38\%          & \textbf{84.32\%}         \\ 

&\multirow{6}{*}{Mammography} 
&\multirow{2}{*}{VinDr-Mammo} & Macro-AUC & 64.50\%~\cite{wantlin2023benchmd}    & 51.49\%     & \textbf{71.76\%} \\
&&& Macro-F1  & N/A       & 16.06\%     & \textbf{35.70}\%             \\ 

&&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (mass)\end{tabular}}  &
  Macro-AUC & N/A & 47.75\%  & \textbf{73.31\%} \\
                                                   
&&& Macro-F1  & N/A       & 7.77\%        & \textbf{51.12\%} \\ 
&&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (calcification)\end{tabular}} &
  Macro-AUC & N/A & 40.67\% & \textbf{82.22\%}  \\
&&& Macro-F1  & \textbf{70.71\%}~\cite{panambur2022effect}     & 11.37\% & 67.86\%   \\ 
&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Genomics\\(Variant Calling)\end{tabular}}
&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}PrecisionFDA\\(Truth Challenge V2)\end{tabular}}  & Indel-F1  & \textbf{99.40\%}~\cite{poplin2018deepvariant}   & 53.01\%          & 97.04\%        \\
                                 &  && SNP-F1    & \textbf{99.70\%}~\cite{poplin2018deepvariant}  & 52.84\%   & 99.35\%                \\                                  
\bottomrule

\end{tabular}
\end{table}



\begin{table}[ht]
\footnotesize
\centering
\caption{\textbf{Performance of Med-PaLM M on MultiMedBench across model scales.} We summarize the performance of Med-PaLM M across three model scale variants 12B, 84B, 562B. All models were finetuned and evaluated on the same set of tasks in MultiMedBench. We observe that scaling plays a key role in language-only tasks and multimodal tasks that require reasoning such as visual question answering. However, scaling has diminishing benefit for image classification and chest X-ray report generation task.}
\label{tab:results-med-palm-m-scaling}
\begin{tabular}{@{}c@{\hspace{.05cm}}@{\hspace{.07cm}}c@{\hspace{.07cm}}ccccc}
\toprule
Task Type & Modality  &   Dataset   & Metric     &\begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (12B)\end{tabular}  & \begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (84B)\end{tabular}  &  \begin{tabular}[c]{@{}c@{}}Med-PaLM M \\ (562B)\end{tabular}  \\ \midrule
\multirow{3}{*}{Question Answering} & \multirow{3}{*}{Text}
& MedQA  & Accuracy  & 29.22\%        & 46.11\%         & \textbf{69.68\%} \\
&& MedMCQA       & Accuracy  & 32.20\%         & 47.60\%          & \textbf{62.59\%} \\
&& PubMedQA      & Accuracy  & 48.60\%         & 71.40\%          & \textbf{80.00\%}    \\ 
\midrule
\multirow{3}{*}{Report Summarization} & \multirow{3}{*}{Radiology}
&\multirow{3}{*}{MIMIC-III} & ROUGE-L      & 29.45\%        & 31.47\%         & \textbf{32.03\%} \\
&&& BLEU          & 12.14\%        & \textbf{15.36}\%         & {15.21\%} \\
                          & && F1-RadGraph    & 31.43\%        & 33.96\%         & \textbf{34.71\%}\\
\midrule
\multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Visual \\Question Answering\end{tabular}} & \multirow{4}{*}{Radiology}
&\multirow{2}{*}{VQA-RAD}   & BLEU-1  & 64.02\%        & 69.38\%          & \textbf{71.27\%} \\
                          
                           &&& F1    & 50.66\%        & 59.90\%           & \textbf{62.06\%} \\ 
 &&\multirow{2}{*}{Slake-VQA} & BLEU-1    & 90.77\%       & \textbf{92.70\%}  & 91.64\%          \\
                          & && F1    & 86.22\%       & \textbf{89.28\%} & 87.50\%           \\ 

& \multirow{2}{*}{Pathology} &\multirow{2}{*}{Path-VQA}  & BLEU-1 & 68.97\%        & 70.16\%          & \textbf{72.27\%} \\
                           
                           &&& F1     & 57.24\%        & 59.51\%          & \textbf{62.69\%} \\ 

\midrule
\multirow{9}{*}{Report Generation} &\multirow{9}{*}{Chest X-ray}
&\multirow{9}{*}{MIMIC-CXR} &Micro-F1-14  & 51.41\%        & \textbf{53.56\%} & 51.60\% \\
&&&Macro-F1-14  & 37.31\%        & \textbf{39.83\%} & 37.81\% \\
&&&Micro-F1-5   & 56.54\%        & \textbf{57.88\%} & 56.28\% \\
&&&Macro-F1-5     & 50.57\%        & \textbf{51.60\%} & 49.86\% \\
&&&F1-RadGraph      & 25.20\%        & \textbf{26.71\%} & 26.06\%          \\
&&&BLEU-1            & 30.90\%        & \textbf{32.31\%} & 31.73\% \\
&&&BLEU-4           & 10.43\%        & 11.31\%          & \textbf{11.50\%}  \\
&&&ROUGE-L           & 26.16\%        & 27.29\% & \textbf{27.49\%} \\
&&&CIDEr-D           & 23.43\%        & \textbf{26.17\%} & 25.27\%          \\

\midrule
\multirow{12}{*}{Image Classification} &\multirow{2}{*}{Chest X-ray}
&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MIMIC-CXR\\ (5 conditions)\end{tabular}} & Macro-AUC    & 76.67\%          & 78.35\%          & \textbf{79.09\%} \\
                                & & & Macro-F1      & 38.33\%          & 36.83\%          & \textbf{41.57\%} \\ 
&\multirow{2}{*}{Dermatology} 
&\multirow{2}{*}{PAD-UFES-20}  
& Macro-AUC & 95.57\%          & \textbf{97.27\%} & 96.08\%          \\
                        & && Macro-F1        & 78.42\%          & \textbf{84.32\%} & 77.03\%          \\ 

&\multirow{6}{*}{Mammography} 
&\multirow{2}{*}{VinDr-Mammo} & Macro-AUC     & 66.29\%          & \textbf{71.76\%} & 71.42\%          \\
&&& Macro-F1        & 29.81\%    & \textbf{35.70}\%           & 33.90\%           \\ 

&&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (mass)\end{tabular}}  &
  Macro-AUC  &
  70.11\% &
  73.09\% &
  \textbf{73.31\%} \\
                                                   
                                 &                 &          & Macro-F1        & 47.23\%          & 49.98\%          & \textbf{51.12\%} \\ 
&&\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}CBIS-DDSM\\ (calcification)\end{tabular}} &
  Macro-AUC  & 81.40\% &
  \textbf{82.22\%} &
  80.90\% \\
                                 &                  &         & Macro-F1        & \textbf{67.86}\%          & 63.81\% & 63.03\%          \\ 
&\multirow{2}{*}{Genomics} 
&\multirow{2}{*}{Variant Calling}  & Indel-F1    & 96.42\%          & \textbf{97.04\%} & 95.46\%          \\
                                 &  && SNP-F1      & \textbf{99.35\%} & 99.32\%          & 99.16\%          \\                                  
\bottomrule

\end{tabular}
\end{table}







\section{Results}
\label{sec:results}

Here we present results across the three different evaluation setups introduced in~\cref{sec:evaluation}.

\subsection{Med-PaLM M performs near or exceeding SOTA on all MultiMedBench tasks}

\paragraph{Med-PaLM M performance versus baselines} We compared Med-PaLM M with two baselines:
\begin{itemize}
    \item prior SOTA specialist models for each of the MultiMedBench tasks
    \item a baseline generalist model (PaLM-E 84B) without any biomedical domain finetuning. We used this model size variant (and not PaLM-E 562B) due to compute constraints.
\end{itemize}

Results are summarized in~\cref{tab:results-med-palm-m-best}. Across MultiMedBench tasks, Med-PaLM M's best result (across three model sizes) exceeded prior SOTA results on 5 out of 12 tasks (for two tasks, we were unable to find a prior SOTA comparable to our setup) while being competitive on the rest. Notably, these results were achieved with a generalist model using the same set of model weights without any task-specific architecture customization or optimization. 

On medical question answering tasks, we compared against the SOTA Med-PaLM 2 results~\cite{singhal2023towards} and observed higher performance of Med-PaLM 2. However, when compared to the baseline PaLM model on which Med-PaLM M was built, Med-PaLM M outperformed the previous best PaLM results~\cite{singhal2022large} by a large margin in the same few-shot setting on all three question answering datasets.

Further, when compared to PaLM-E 84B as a generalist baseline without biomedical domain finetuning, Med-PaLM M exhibited performance improvements on all 14 tasks often by a significant margin, demonstrating the importance of domain adaptation. Taken together, these results illustrate the strong capabilities of Med-PaLM M as a generalist biomedical AI model. We further describe the results in detail for each of the individual tasks in~\cref{appendix:multimedbench_detailed_performance}.


\paragraph{Med-PaLM M performance across model scales} We summarize Med-PaLM M performance across model scales (12B, 84B, and 562B) in~\cref{tab:results-med-palm-m-scaling}. The key observations are:
\begin{itemize}
    \item \textbf{Language reasoning tasks benefit from scale} For tasks that require language understanding and reasoning such as medical question answering, medical visual question answering and radiology report summarization, we see significant improvements as we scale up the model from 12B to 562B.
    \item \textbf{Multimodal tasks bottlenecked by vision encoder performance} For tasks such as mammography or dermatology image classification, where nuanced visual understanding is required but minimal language reasoning is needed (outputs are classification label tokens only), the performance improved from Med-PaLM M 12B to Med-PaLM 84B but plateaued for the 562B model, possibly because the vision encoder is not further scaled in that step (both the Med-PaLM M 84B and 562B models use the same 22B ViT as the vision encoder), thereby acting as a bottleneck to observing a scaling benefit. We note the possibility of additional confounders here such as the input image resolution.  
\end{itemize}

The scaling results on the chest X-ray report generation task are interesting (\cref{tab:results-med-palm-m-scaling}). While on the surface, the task seems to require complex language understanding and reasoning capabilities and would thus benefit from scaling the language model, we find the Med-PaLM M 84B model to be roughly on-par or slightly exceeding the 562B model on a majority of metrics, which may simply be due to fewer training steps used for the larger model.
Another possibility for the diminishing return of increasing the size of language model is likely that the output space for chest X-ray report generation in the MIMIC-CXR dataset is fairly confined to a set of template sentences and limited number of conditions. This insight has motivated the use of retrieval based approaches as opposed to a fully generative approach for the chest X-ray report generation task on this dataset ~\cite{ye2022retrieval, endo2021retrieval}. Additionally, the larger 562B model has a tendency towards verbosity rather than the comparative brevity of the 84B model, and without further preference alignment in training, this may impact its metrics.



\subsection{Med-PaLM M demonstrates zero-shot generalization to novel medical tasks and concepts}

Training a generalist biomedical AI system with language as a common grounding across different tasks allows the system to tackle new tasks by combining the knowledge it has learned for other tasks (i.e. combinatorial generalization).
We highlight preliminary evidence which suggests Med-PaLM M can generalize to novel medical concepts and unseen tasks in a zero-shot fashion. We further observe zero-shot multimodal reasoning as an emergent capability~\cite{wei2022emergent} of Med-PaLM M. Finally, we demonstrate benefits from positive task transfer as a result of the model's multi-task, multimodal training.

\subsubsection{Evidence of generalization to novel medical concepts}
We probed the zero-shot generalization capability of Med-PaLM M for an unseen medical concept by evaluating its ability to detect tuberculosis (TB) abnormality from chest X-ray images in the Montgomery County (MC) dataset. As shown in~\cref{tab:results-tb-classification}, Med-PaLM M performed competitively compared to SOTA results obtained by a specialized ensemble model optimized for this dataset~\cite{oloko2021ensemble}. We observed similar performance across three model variants, consistent with findings on other medical image classification tasks in MultiMedBench. Given the classification task was set up as an open-ended question answering task, we did not report the AUC metric which requires the normalized predicted probability of each possible class. 

\begin{table}[ht]
\small
\centering
\caption{\textbf{Zero-shot classification performance of Med-PaLM M on the tuberculosis (TB) detection task.} Med-PaLM M performs competitively to the SOTA model~\cite{oloko2021ensemble} finetuned on the Montgomery County TB dataset using model ensemble. Notably, Med-PaLM M achieves this result with a simple task prompt consisting of a single text-only exemplar (without task-specific image and hence zero-shot), in contrast to the specialist model that requires training on all the samples in the dataset.}
\label{tab:results-tb-classification}
\begin{tabular}{ccc}
\toprule

  Model & \# Training samples & Accuracy\\ \midrule
  SOTA~\cite{oloko2021ensemble}  & \textbf{138} &   \textbf{92.60\%} \\
  Med-PaLM M (12B) & 0 & 86.96\% \\
  Med-PaLM M (84B) & 0 & 82.60\% \\  
  Med-PaLM M (562B) & 0 & 87.68\% \\
  \bottomrule
\end{tabular}
\end{table}



\subsubsection {Evidence of emergent zero-shot multimodal medical reasoning}
We also qualitatively explored the zero-shot chain-of-thought (CoT) capability of Med-PaLM M on the MC TB dataset. In contrast to the classification setup, we prompted the model with a text-only exemplar to generate a report describing the findings in a given image in addition to a yes/no classification prediction.  In~\cref{fig:tb-zero-shot-cot}, we present qualitative examples of zero-shot CoT reasoning from the Med-PaLM M 84B and 562B variants. In particular, both Med-PaLM M variants were able to identify the major TB related lesion in the correct location. However, according to expert radiologist review, there are still some omissions of findings and errors in the model generated report, suggesting room for improvement. It is noteworthy that Med-PaLM M 12B failed to generate a coherent visually conditioned response, which indicates that scaling of the language model plays a key role in the zero-shot CoT multimodal reasoning capability (i.e. this might be an emergent capability~\cite{wei2022emergent}).
% Figure environment removed 

\subsubsection{Evidence of generalization to novel tasks}
Although Med-PaLM M was only trained with single-view chest X-ray image inputs, we observed the capability of the model to generalize to a novel task setup with multi-view visual inputs. Specifically, on a subset of studies from MIMIC-CXR where each report is accompanied with both a frontal and a lateral view X-ray image. we observe that Med-PaLM M is able to attain zero-shot performance comparable to the single-view report generation task as detailed in~\cref{tab:results-report-gen-ood}.  This ability is promising given medical imaging studies often benefit from the interpretation of prior historical studies in addition to the current instance for optimal performance.


\begin{table}[ht]
\small
\centering
\caption{\textbf{Zero-shot generalization to two-view chest X-ray report generation.} Med-PaLM M performance remains competitive on a novel two-view report generation task setup despite having not been trained with two visual inputs before. Med-PaLM M achieves SOTA results on clinical efficacy metrics for the two view report generation task.}
\label{tab:results-report-gen-ood}
\begin{tabular}{ccccc}
\toprule
Metric    & SOTA    & Med-PaLM M (12B)   & Med-PaLM M (84B)  & Med-PaLM M (562B) \\ \midrule
Micro-F1-14  & 44.20\% & 49.80\% & \textbf{50.54\%} & 48.85\%          \\
Macro-F1-14  &30.70\% & 37.69\% & \textbf{37.78\%} & 37.29\%          \\
Micro-F1-5   & 56.70\% & 54.49\% & \textbf{56.37\%} & 54.36\%          \\
Macro-F1-5   & N/A  & 48.33\% & \textbf{51.23\%} & 48.49\%          \\
F1-RadGraph  & 24.40\% & 26.73\% & \textbf{28.30\%} & 27.28\%  \\
BLEU-1       & \textbf{39.48\%} & 33.31\% & 34.58\% & 33.83\%          \\
BLEU-4       & \textbf{13.30\%} & 11.51\%          & 12.44\% & 12.47\%          \\
ROUGE-L      & \textbf{29.60\%} & 27.84\% & 28.71\% & 28.49\%          \\
CIDEr-D       & \textbf{49.50\%} & 27.58\% & 29.80\%  & 29.80\%           \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Evidence of positive task transfer}
To demonstrate the positive task transfer arising from joint training across modalities and tasks, we performed an ablation study where we trained a Med-PaLM M 84B variant by excluding the MIMIC-CXR classification task from the task mixture and compared this model variant against Med-PaLM M 84B trained on the full MultiMedBench mixture.
As seen in~\cref{tab:results-cxr-classification-positive-transfer}, we observed that the model trained jointly on both report generation and classification has higher performance across the board on all report generation metrics.
We also observe that the model trained only on chest X-ray report generation can generalize to abnormality detection in a zero-shot fashion with compelling performance, as evidenced by a higher macro-F1 score. This is another example of generalization to a novel task setting where the model learns to differentiate between types of abnormalities from training on the more complex report generation task. 


\begin{table}[ht]
\small
\centering
\caption{\textbf{Positive task transfer between CXR report generation and abnormality classification.} We observe positive transfer as a result of multi-task training with Med-PaLM M model trained jointly on both chest X-ray report generation and classification tasks. It exhibits higher performance on report generation metrics compared to a Med-PaLM M model trained without chest X-ray report classification. We also observe that training on the chest X-ray report generation task alone enables Med-PaLM M to generalize to abnormality detection in a zero-shot fashion.}
\label{tab:results-cxr-classification-positive-transfer}
\begin{tabular}{cccc}
\toprule
Dataset &
  Metric &
  Med-PaLM M (84B) &
  \begin{tabular}[c]{@{}c@{}}Med-PaLM M (84B)\\ No CXR classification\end{tabular} \\ \midrule
\multirow{9}{*}{MIMIC-CXR} &
  Micro-F1-14 & \textbf{53.56\%} & 52.94\% \\
 & Macro-F1-14 & \textbf{39.83\%} & 38.92\% \\
 & Micro-F1-5  & \textbf{57.88\%} & 57.58\% \\
 & Macro-F1-5  & \textbf{51.60\%} & 51.32\% \\
 & F1-RadGraph         & \textbf{26.71\%} & 26.08\% \\
 & BLEU-1            & \textbf{32.31\%} & 31.72\% \\
 & BLEU-4            & \textbf{11.31\%} & 10.87\%   \\
 & ROUGE-L             & \textbf{27.29\%} & 26.67\% \\
 & CIDEr-D               & \textbf{26.17\%} & 25.17\% \\
\midrule
\multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}MIMIC-CXR\\ (5 conditions)\end{tabular}}  &
  Macro-AUC &\textbf{78.35\%} & 73.88\% \\
 & Macro-F1            & 36.83\% & \textbf{43.97}\%  \\ \bottomrule
\end{tabular}
\end{table}



\subsection{Med-PaLM M performs encouragingly on radiology report generation across model scales}

To further understand the clinical applicability of Med-PaLM M, we conducted radiologist evaluations of model-generated chest X-ray reports (and reference human baselines). Under this evaluation framework, we observe encouraging quality of Med-PaLM M generated reports across model scales as detailed below.

\subsubsection{Side-by-side evaluation}

In a side-by-side evaluation, four clinician raters ranked the quality of four radiology reports, comparing the radiologist-provided reference report from the MIMIC-CXR dataset with reports generated by different Med-PaLM M model scales (12B, 84B, and 562B).

\cref{fig:ranking-best-of-four} summarizes how often each rater ranked a report generated by one of the three Med-PaLM M variants or the reference report as the best among four candidate reports.
Averaged over all four raters, the radiologist-provided reference report was ranked best in 37.14\% of cases, followed by Med-PaLM M (84B) which was ranked best in 25.78\% of cases, and the other two model scales, 12B and 562B, which were ranked best in 19.49\% and 17.59\% of cases respectively.

To enable a direct comparison of reports generated by each Med-PaLM M model scale to the radiologist-provided reference report, we derived pairwise preferences from the four-way ranking and provided a breakdown for each rater and model scale in~\cref{fig:ranking-pairwise-with-reference}.
Averaged over all four raters, Med-PaLM M 84B was preferred over the reference report in 40.50\% of cases, followed by the other two model scales, 12B and 562B, which were preferred over the reference report in 34.05\% and 32.00\% of cases, respectively.

% Figure environment removed

\subsubsection{Independent evaluation}

We report the rates of omissions and errors radiologists identified in findings paragraphs generated by Med-PaLM M.~\cref{fig:expert-error} provides breakdowns by model scales (12B, 84B, 562B). We observed different trends for omissions and errors. For omissions, we observed the lowest rate of 0.12 (95\% CI, 0.10 - 0.15) omissions per report on average for both the Med-PaLM M 12B and 84B models, followed by 0.13 (95\% CI, 0.11 - 0.16) for the 562B model.

In contrast, we measured the lowest mean error rate of 0.25 (95\% CI, 0.22 - 0.28) for Med-PaLM M 84B, followed by 0.28 (95\% CI, 0.24 - 0.31) for Med-PaLM M 12B and 0.29 (95\% CI, 0.25 - 0.32) for the 562B model. Notably, this error rate is comparable to those reported for human radiologists baselines on the MIMIC-CXR dataset in a prior study~\cite{jeong2023multimodal}. 

It is important to mention that our analysis is limited to errors of clinical relevance, ensuring a specific focus on clinical interpretation. This includes those errors related to the presence, location or severity of a clinical finding. Example of non-clinical errors are passages referring to views or prior studies not present, which stem from training artifacts.

These trends across model scales were identical for the subset of omissions and errors that were marked as significant by radiologist raters. We refer the reader to~\cref{tab-appendix:human-evaluation-results} for an overview of error and omission rates, including non-clinical errors.

% Figure environment removed

% Figure environment removed

In~\cref{fig:cxr-report-example}, we illustrate a qualitative example of chest X-ray reports generated by Med-PaLM M across three model sizes along with the target reference report. For this example, our panel of radiologists judged the Med-PaLM M 12B report to have two clinically significant errors and one omission, the Med-PaLM M 84B report to have zero errors and zero omissions, and the Med-PaLM M 562B report to have one clinically insignificant errors and no omissions.
 


\section{Discussion}

To the best of our knowledge, Med-PaLM M is the first demonstration of a generalist biomedical AI system that can interpret a wide range of medical modalities, perform competently (including near or exceeding prior SOTA) on a diverse array of tasks, and generalize to unseen biomedical concepts and tasks. This potentially opens up new possibilities in applications spanning scientific discovery to care delivery. We elaborate on the implications of this development as well as the challenges and limitations below.


\paragraph{Lack of benchmarks a key bottleneck for the development of generalist biomedical AI} AI progress to date has largely been catalyzed by the development of high quality benchmarks. While there exists several single-task biomedical AI datasets, there have been limited attempts to unify them and create benchmarks for the development of generalist biomedical AI systems. Our curation of MultiMedBench is a step towards addressing this unmet need. However, the benchmark has several important limitations including limited size of the individual datasets (a cumulative size of \~1 million samples) and limited modality and task diversity (e.g., lacking life sciences such as transcriptomics and proteomics). Another key barrier to developing models for use across an even wider variety of biomedical data types is the lack of large scale multimodal datasets, which would permit joint learning and alignment of the modality-specific encoders with the decoder.

\paragraph{Importance of medical finetuning and specialization} PaLM-E is a highly capable generalist AI model as evidenced by its SOTA performance on a wide range of vision-language and embodied robotics tasks. Yet, its out-of-the-box performance on MultiMedBench was poor and Med-PaLM M outperforms it by a wide margin across model scales.  This result suggests that finetuning with domain-specific biomedical data is critical to achieving good performance on biomedical tasks, perhaps due to the distribution shift presented by the domain overall compared to the plethora of non-medical tasks and modalities.

\paragraph{Scaling multimodal AI models is challenging} In the language domain, scaling the model has led to leapfrog improvements in performance and emergent capabilities. However, our preliminary experiments suggest this is likely more challenging for multimodal generalist models in the biomedical task domain due to the medical data scarcity. Given the wide array of modalities and tasks such generalist models are expected to understand and tackle, it is crucial that the encoders for such diverse modalities are scaled jointly with the language model. Otherwise, for tasks that require interpretation of data from a combination of modalities, the performance will end up being bottlenecked by the weakest encoder. We see evidence of this in medical image classification tasks such as mammography and dermatology where scaling the language model component has little effect on the task performance as the potential key bottleneck is the vision encoder. It is possible that the small volume of medical data in MultiMedBench is not be sufficient to effectively adapt a ViT pretrained on natural images to the medical domain, thereby limiting the benefits of model scaling. As such, our study only provides some initial insights on the effect of model scaling on biomedical task performance. Future research is needed to fully understand the effect of model scaling by teasing apart the scaling effect of the language model from that of modality-specific encoders, with sufficient amounts of biomedical data.

\paragraph{Technical considerations for generalist biomedical AI} Med-PaLM M builds on state-of-the-art vision and language components such as ViT and PaLM. Yet, putting them together requires careful considerations around token lengths allocated to visual encoder outputs, total context length of the model, sampling strategies, training data mixtures and so forth. Further, simple, yet important techniques such as the use of one-shot training with dummy image tokens make an important difference in the quality and compute efficiency of the final model. With increasing generality of the AI system, the number of details requiring careful consideration tends to increase as well. We also note that Med-PaLM M architecture as setup currently is not optimal for few-shot in-context learning.

\paragraph{Progress in AI for radiology report generation} Our evaluation by radiologists of Med-PaLM M generated radiology reports suggests encouraging performance of the model on a challenging multimodal task. In up to 40.50\% of the cases, a Med-PaLM M generated report was preferred over the human-generated reference report. Further, the average number of clinically significant errors within the model responses is comparable to those reported for human-generated reports in prior studies~\cite{jeong2023multimodal} on the same dataset. These promising results underpin rapid development in the task of automatic radiology report generation and suggest the potential for clinical utility in the future.

\paragraph{Generalist agents are not the only approach to multimodal biomedical AI} While generalist biomedical AI systems offer exciting possibilities~\cite{moor2023foundation}, there are other approaches to developing multimodal biomedical AI systems that might be more applicable depending on data availability, pretrained models, compute and application scenarios. These include leveraging frozen encoders with adapter layers~\cite{zhang2023llama} to glue together a multimodal biomedical AI system or developing LLMs that can interface with specialist biomedical encoders or task-specific agents through tool use~\cite{schick2023toolformer}. 

\paragraph{Considerations for real-world applications of generalist biomedical AI} While the development of generally capable biomedical AI systems is exciting, for such systems to be useful in practice or opening the door to new applications, they need to match or exceed specialized, single-task models or otherwise reach clinically applicable levels of performance. While beyond the scope of this work, the progress here necessitates careful considerations of safety and equity in the development and validation of such systems.


\section{Perspective on Generalist Biomedical AI}
\label{sec:perspective}
Reaching near or above SOTA on a diverse range of biomedical tasks with a single set of model weights is a noteworthy milestone for the development of generalist biomedical AI systems. While human clinicians can train for ``general practice''~\cite{marshall2022future}, helpful subspecialty-specific expertise is often found in different experts~\cite{blank2014referral}, to whom non-specialist clinicians may refer for specialist opinions in the course of care. It is also commonplace for multiple physician specialities to work together in care delivery. We envisage a similar future for biomedical AI where generalist and specialist AI systems interact and collaborate together with expert clinicians and researchers in a tight feedback loop to tackle grand challenges in biomedicine. 

Our finding, of a single generalist biomedical AI that reaches compelling performance across disparate tasks and contexts, hints at new frontiers for impact in applications. This includes the potential for near zero-shot insight in new domains, as a tool for discovery integrating insights from distinct areas of biomedicine, and as a common point of assistance providing access to expertise from many different fields.

\section{Conclusion}
\label{sec:conclusion}
Medicine is a multidisciplinary endeavour. Generalist biomedical AI systems that effectively assimilate and encode multimodal medical data at scale and rapidly adapt to new clinical contexts are likely to be the foundation of next generation learning health systems and make healthcare more accessible, efficient, equitable and humane. While further development and rigorous validation is needed, we believe Med-PaLM M represents an important step towards the development of such generalist biomedical AI.

\vspace{12pt}
\subsubsection*{Acknowledgments}
This project was an extensive collaboration between many teams at Google Research and Google DeepMind. We thank Andrew Sellergren, Yuan Liu, Michael Howell, Julie Wang, Sho Kannan, Christine Kingsley, Roy Lee, Naama Hammel, Jay Hartford, Preeti Singh, Kavita Kulkarni, Gavriel Goidel, Anil Palepu, Si Wai Man, Amy Wang, Sami Lachgar, Lauren Winer, Maggie Shiels, Annisah Um'rani, John Guilyard, Shravya Shetty and Evan Rapoport for their valuable insights and feedback during our research. We are also grateful to Karen DeSalvo, Zoubin Ghahramani, James Manyika, and Jeff Dean for their support during the course of this project.

\subsubsection*{Data Availability}
The benchmark used for training and evaluation in this study, MultiMedBench, comprises de-identified datasets that are all open source. We present an overview of datasets in \cref{tab:multimedbench-overview}.

\subsubsection*{Code Availability} We will not be able to open source the large language models (LLMs) used in this study. We have provided comprehensive details regarding our underlying methodology and build on previously detailed models \cite{chowdhery2022palm,driess2023palme}, so that similar approaches can be tried with other classes of LLMs.

\newpage
\setlength\bibitemsep{3pt}
\printbibliography
\balance
\clearpage