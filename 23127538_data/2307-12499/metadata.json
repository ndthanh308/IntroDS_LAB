{
  "title": "AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models",
  "authors": [
    "Xuelong Dai",
    "Kaisheng Liang",
    "Bin Xiao"
  ],
  "submission_date": "2023-07-24T03:10:02+00:00",
  "revised_dates": [
    "2023-09-29T00:19:59+00:00",
    "2024-02-28T02:07:55+00:00",
    "2024-07-16T00:40:44+00:00"
  ],
  "abstract": "Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often directly inject Projected Gradient Descent (PGD) gradients into the sampling of generative models, which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for GAN-based methods on large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable in generating high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective in generating unrestricted adversarial examples, which outperforms state-of-the-art unrestricted adversarial attack methods in terms of attack performance and generation quality.",
  "categories": [
    "cs.LG",
    "cs.CV"
  ],
  "primary_category": "cs.LG",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12499",
  "pdf_url": null,
  "comment": "ECCV 2024",
  "num_versions": null,
  "size_before_bytes": 130740288,
  "size_after_bytes": 15732302
}