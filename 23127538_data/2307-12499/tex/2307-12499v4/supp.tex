\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[review,year=2024,ID=*****]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{multirow}
\usepackage{makecell}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}

\appendix
\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
\title{Supplementary Materials for \\
AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models} 

% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{AdvDiff}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{Xuelong Dai\inst{1}\orcidlink{0000-0001-6646-6514} \and
Kaisheng Liang\inst{1}\orcidlink{0000-0002-8297-6378} \and
Bin Xiao\inst{1}\orcidlink{0000-0003-4223-8220}}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{X.~Dai et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{
The Hong Kong Polytechnic University\\
\email{\{xuelong.dai, kaisheng.liang\}@connect.polyu.hk, b.xiao@polyu.edu.hk }}



\maketitle

%%%%%%%%% BODY TEXT

\setcounter{equation}{10}
\setcounter{algorithm}{1}

\section{Detailed Proof of Equation 8}

 We can obtain the sample $x_{t-1}$ with condition label $y$, according to the sampling with the classifier-free guidance. To get the unrestricted adversarial example $x^*_{t-1}$, we add adversarial guidance to the conditional sampling process with Equation 8. With Bayes' theorem, we want to deduce the adversarial sampling with adversarial guidance at timestep $t$ by: 
\begin{equation}
    p({x}^*_{t-1}|y_{a}) = \frac{p(y_{a}|{x}^*_{t-1})p({x}^*_{t-1})}{p(y_{a})}
    \label{eq:s1}
\end{equation}
with Equation \ref{eq:s1}, we want to sample the adversarial examples with the target label $y_a$. Starting from $x_t$, the sampling of the reverse generation process with AdvDiff is:
\begin{align}
    p({x}^*_{t-1}|x_{t},y_{a}) =& \frac{p(y_{a}|{x}^*_{t-1},x_{t})p({x}^*_{t-1}|x_{t})}{p(y_{a}|x_{t})} 
    \label{eq:s2}
\end{align}
%\notag \\=&p({x}^*_{t-1}|x_{t})e^{\log p(y_{a}|{x}^*_{t-1})- \log p(y_{a}|x_{t}) }
Noted that Equation \ref{eq:s2} is the same as the deviation of classifier-guidance in \cite{dhariwal2021diffusion}'s Section 4.1, where they treated $p(y_{a}|x_{t})$ as a constant. Because $p({x}^*_{t-1}|x_{t})$ is the known sampling process by our conditional diffusion sampling, we evaluate $\frac{p(y_{a}|{x}^*_{t-1},x_{t})}{p(y_{a}|x_{t})}$ by:
\begin{equation}
    \log p_f(y_{a}|{x}^*_{t-1}) - \log p_f(y_{a}|x_{t})
    \label{eq:s3}
\end{equation}
We can approximate Equation \ref{eq:s3} using a Taylor expansion around ${x}^*_{t-1} = \mu(x_t)$ as:
\begin{align}
    \log p_f(y_{a}|{x}^*_{t-1}) - \log p_f(y_{a}|x_{t}) &\approx \log p_f(y_{a}|\mu(x_{t})) \notag\\ &+  ({x}^*_{t-1}-\mu(x_{t}))\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})) \notag\\ &- \log p_f(y_{a}|x_{t}) + C  \notag \\ &= ({x}^*_{t-1}-\mu(x_{t}))\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t}))+C
\end{align}
Assume $p({x}^*_{t-1}|x_{t}) = \mathcal{N}(x^*_{t-1};\mu(x_t),  \sigma_t^2\textbf{I}) \propto e^{{-(x^*_{t-1}-\mu(x_t))^2}/{2\sigma_t^2}}$, we have:
\begin{align}
   p({x}^*_{t-1}|x_{t},y_{a}) &\propto e^{{-(x^*_{t-1}-\mu(x_t))^2}/{2\sigma_t^2} +({x}^*_{t-1}-\mu(x_{t}))\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t}))} \notag  \\ &\propto e^{{-(x^*_{t-1}-\mu(x_t)-\sigma_t^2\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})))^2}/{2\sigma_t^2}+(\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})))^2/{2\sigma_t^2}} \notag \\&\propto  e^{{-(x^*_{t-1}-\mu(x_t)-\sigma_t^2\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})))^2}/{2\sigma_t^2}+C} \notag \\ &\approx \mathcal{N}(x^*_{t-1};\mu(x_t)+\sigma_t^2\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})),  \sigma_t^2\textbf{I} )
   \label{eq:s4}
\end{align}
Sampling with Equation \ref{eq:s4} should be:
\begin{equation}
    {x}^*_{t-1} = \mu({x}_t,y) + \sigma_t{\varepsilon} +\sigma_t^2 s\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t}))
\end{equation}
where $\mu({x}_t,y)$ is the conditional mean value and $\varepsilon$ is sampled from ${\varepsilon}\sim \mathcal{N}(0,\textbf{I})$. Note that $ \mu({x}_t,y) + \sigma_t{\varepsilon}$ is the normal sampling process that we will get $x_{t-1}$. In practice, in each diffusion step, the difference between $x_{t-1}$ and $\mu(x_t)$ should be small enough \cite{dhariwal2021diffusion,ho2020denoising} for a reasonable and stable diffusion sampling. Therefore, we adopt $x_{t-1}$ to calculate the adversarial gradient after the sampling with the conditional diffusion model, and we have:
\begin{equation}
    {x}^*_{t-1} = \mu({x}_t,y) + \sigma_t{\varepsilon} +\sigma_t^2 s\nabla_{\mu(x_{t})} \log p_{f}({y_{a}}|\mu(x_{t})) \approx x_{t-1}+\sigma_t^2 s\nabla_{{x}_{t-1}} \log p_{f}({y_{a}}|{x}_{t-1})
\end{equation}
where $s$ is the adversarial guidance scale. \hfill $\square$

\section{Detailed Proof of Equation 10}
The deviation of Equation 10 is similar to Equation 8, where the noise sampling guidance is added with the forward diffusion process. Similarly, we have Equation 9:
\begin{align}
    p({x}_{T}|y_{a}) =& \frac{p(y_{a}|{x}_{T})p({x}_{T})}{p(y_{a})} = \frac{p(y_{a}|x_T,x_0)p(x_T|x_0)}{p(y_{a}|x_0)} 
    \label{eq:noise}
\end{align}
And Taylor expansion around $x_{T}=x_0$ to evaluate $\frac{p(y_{a}|x_T,x_0)}{p(y_{a}|x_0)}$.
\begin{align}
    \log p_f(y_{a}|x_T) - \log p_f(y_{a}|x_0) = (x_T-x_0)\nabla_{x_0} \log p_{f}({y_{a}}|x_0)+C
\end{align}
From $x_0$ to $x_T$, we gradually add the Gaussian noise with the predefined schedule \cite{ho2020denoising}:
\begin{equation}
    p(x_T|x_0)=\mathcal{N}(x_T;\sqrt{\bar{\alpha}_T}x_0,(1-\bar{\alpha}_T)\textbf{I})
\end{equation}
The noise sampling guidance is as follows:
\begin{align}
    {x}_{T} &\approx (\bar\mu({x}_0,y) + \bar{\sigma}_T{\varepsilon}) +\bar{\sigma}_T^2 a\nabla_{x_0} \log p_{f}({y_{a}}|x_0) \notag \\&= x_T +\bar{\sigma}_T^2 a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)
\end{align}
where $\bar\mu({x}_0,y) + \bar{\sigma}_T{\varepsilon}$ is the forward diffusion process to get $x_T$ with $x_0$ and $a$ is the  noise sampling guidance scale.  \hfill $\square$

\section{AdvDiff for DDIM}
We give the derivation for AdvDiff for DDIM followed with \cite{dhariwal2021diffusion}. The score function for the DDIM diffusion model is:
\begin{equation}
    \nabla_{ {x}}\log p_f( {x}| {y}) = \nabla_{ {x}}\log p_f( {x}) + \nabla_{ {x}}\log p_f( {y}| {x})
\end{equation}
We set $y$ as our adversarial guidance $y_a$:
\begin{align}
    \nabla_{ {x}}\log p_f( {x}| {y}_a) &= \nabla_{ {x}}\log p_f( {x}) + \nabla_{ {x}}\log p_f( {y}_a| {x}) \notag \\ &= -\frac{1}{\sqrt{1-\bar{\alpha}}}\epsilon_\theta(x) + \nabla_{ {x}}\log p_f( {y}_a| {x})
\end{align}
Finally, the new epsilon prediction $\hat{\epsilon}_\theta(x_t)$ is defined as follows:
\begin{equation}
    \hat{\epsilon}_\theta(x_t) = \epsilon_\theta(x_t) - \sqrt{1-\bar{\alpha}_t}\nabla_{ {x}_t}\log p_f( {y}_a| {x}_t)
\end{equation}
Then the DDIM with AdvDiff is Algorithm \ref{alg:s1} over the trained classifier-free diffusion model $\epsilon_{\theta}(\cdot)$.
\begin{algorithm}[tb]
  \caption{DDIM Adversarial Diffusion Sampling} 
  \label{alg:s1}
  \begin{algorithmic}[1]
    \Require $y_a$: target label for adversarial attack
    \Require $y$: ground truth class label
    \Require $s,a$: adversarial guidance scale
    \Require $w$: classification guidance scale
    \Require $N$: noise sampling guidance steps
    \Require $T$: reverse generation process timestep

    \State $x_{T} \sim \mathcal{N}(0, \textbf{I})$
    \State $x_{adv} = \varnothing $
    \For{$i = 1\ldots N$}
    \For{$t=T, \dotsc, 1$}

      \State $\tilde{\epsilon}_t = (1+w)\epsilon_\theta(x_{t}, y) - w\epsilon_{\theta}(x_{t})$ 
      %\Statex  $\triangleright$ DDPM Sampling (can be replaced with DDIM)
      \State     $\hat{\epsilon}_t = \tilde{\epsilon}_t - \sqrt{1-\bar{\alpha}_t}\nabla_{ {x}_t}\log p_f( {y}_a| {x_t})$
      \State Classifier-free DDIM sampling $x_{t-1}$ with $\hat{\epsilon}_t$
      %Calculate $\mu({x}_t,y) + \sigma_t{\varepsilon}$ referring to [23]
    \EndFor
      \State Obtain classification result from $f(x_0)$
      \State Compute the gradient with $\log p_{f}({y_{a}}|{x}_0)$
      \State Update $x_{T}$ by $x_T = x_T + a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)$
      \State  $x_{adv} \gets x_{0}$ if $f(x_0)=y_a$
    
    
    \EndFor
    \State \textbf{return} $x_{adv}$
  \end{algorithmic}
\end{algorithm}

We can further deduce the DDIM with $\hat{\epsilon}_\theta(x_t)$ by:
\begin{align}
  x_{t-1}&= \sqrt{\bar{\alpha}_{t-1}} \left( \frac{x_t - \sqrt{1-\bar{\alpha}_t} \hat{\epsilon}_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1}} \hat{\epsilon}_\theta \notag \\&=\sqrt{\bar{\alpha}_{t-1}} \left( \frac{x_t - \sqrt{1-\bar{\alpha}_t}{\epsilon}_\theta}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1-\bar{\alpha}_{t-1}} {\epsilon}_\theta+C\cdot\nabla_{ {x}_t}\log p_f( {y}_a| {x}_t)
  \label{eq:s5}
\end{align}
where we can replace $C$ with our adversarial guidance scale.


\section{Related Work}

Since Szegedy et al. \cite{szegedy2013intriguing} had proved that DL models are extremely vulnerable to adversarial attacks, researchers have been digging into improving the model's adversarial robustness by proposing stronger adversarial attack methods and their counter-measurements. 

\textbf{Perturbation-based adversarial examples with generative models}: Most related works performed adversarial attacks by perturbing a subset of clean data to fool the target classifier. These attacks \cite{carlini2017towards,madry2017towards,kurakin2018adversarial} attempted to generate better perturbations with higher attack success rates and smaller perturbations. With the emergence of generative models, end-to-end adversarial attacks \cite{baluja2018learning,xiao2018generating,poursaeed2018generative} have greatly improved the generation efficiency by pre-training the generative module. These methods integrate the advertorial loss into the training of generative models and generate adversarial examples by trained generators with clean data.

\textbf{Unrestricted adversarial examples with generative models}: Perturbation-based adversarial examples require insignificant norm distance to the given clean data in order to guarantee the indistinguishability, which only covers a small fraction of all possible adversarial examples \cite{song2018constructing}. To remove such restrictions, Song et al. \cite{song2018constructing} proposed an unrestricted adversarial attack method that searches over the latent space of the input noise vector with an adversarial loss function and a well-trained AC-GAN \cite{odena2017conditional}. Inspired by Song's work \cite{song2018constructing}, recent works \cite{poursaeed2019fine,xiang2022egm} made improvements in the generation quality and generation efficiency of UAEs. Diffusion model based adversarial attacks \cite{chen2023advdiffuser,chen2023diffusion,chen2023content} also achieve satisfying attack performance against deep learning models. However, the performance of existing approaches suffers from the unstable training of GAN models as well as the lack of theoretical support for injecting PGD-based gradients. Therefore, we provide an effective and theoretically analyzed solution with the diffusion model in this paper. 

\subsection{Conditional Diffusion Model for Image Generation}

Diffusion models have shown great generation quality and diversity in the image synthesis task since Ho et al.\cite{ho2020denoising} proposed a probabilistic diffusion model for image generation that greatly improved the performance of diffusion models.  Diffusion models for conditional image generation are extensively developed for more usable and flexible image synthesis. Dhariwal \& Nichol \cite{dhariwal2021diffusion} proposed a conditional diffusion model that adopted classifier-guidance for incorporating label information into the diffusion model. They separately trained an additional classifier and utilized the gradient of the classifier for conditional image generation. Jonathan Ho \& Tim Salimans \cite{ho2021classifier} performed the conditional guidance without an extra classifier to a diffusion model. They trained a conditional diffusion model together with a standard diffusion model. During sampling, they adopted the combination of these two models for image generation. Their idea is motivated by an implicit classifier with the Bayes rule. Followed by \cite{dhariwal2021diffusion,ho2021classifier}'s works, many research \cite{rombach2022high,nichol2021glide,lugmayr2022repaint,gafni2022make} have been proposed to achieve state-of-the-art performance on image generation, image inpainting, and text-to-image generation tasks. Despite utilizing diffusion models for image generation has been widely discussed, none of these works have discovered the adversarial examples generation method with the diffusion model. Also, it is a new challenge to defend against the adversarial examples generated by the diffusion model.


\section{Implementation Details}
As AdvDiff supports both DDPM and DDIM sampling, we adopt LDM
\footnote{\url{https://github.com/CompVis/latent-diffusion}} with DDIM sampler for the experiment on ImageNet for reproducibility and poor performance of simple DDPM on ImageNet. We adopt 500 sampling steps for DDPM on MNIST and 200 sampling steps for LDM on ImageNet. For conditional sampling, we use one-hot label information for both DDPM and DDIM sampling for a fair comparison with GAN. The noise sampling step is set as $(0,0.5]$ for the MNIST dataset and $(0,0.2]$ for the ImageNet dataset. We follow the default settings in DiffAttack and AdvDiffuser in the experiments.


\section{More Experiment Results}

We give more experiment results in Figure \ref{fig:s1} to demonstrate the generation quality on the ImageNet dataset. We also provide some failure cases of our AdvDiff, which happens when we set the adversarial guidance scale $s$ and $a$ extremely large. Figure \ref{fig:s2} shows that a large $s$ (10.0) tends to generate images with noisy textures while a large $a$ (10.0) can generate noisy images. Figure \ref{fig:s3} shows that modifying the initial noise with $a$ can disturb the noise distribution if we add the gradient in an irrational manner.

   % Figure environment removed
   
% Figure environment removed

% Figure environment removed

   % Figure environment removed

\section{AdvDiff against Adversarial Training with Diffusion Models}


\begin{table}[ht]

\begin{center}

\caption{\textbf{Performance under AdvDiff attack against adversarial training on the ResNet18 model.}}
\label{tab:at}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{c|ccc|ccc|ccc}
\hline
Method   & Clean & PGD & AdvDiff Attack & AT-UAE  & PGD  & AdvDiff Attack & AT-PGD  & PGD  & AdvDiff Attack \\ \hline
Accuracy (\%) & 99.0 & 0.7  & 7.9            & 99.2 & 16.8 & 32.6  & 95.2 & 79.2 & 13.5     \\
\hline   
\end{tabular}
}
\end{center}
\end{table}


Adversarial training is an effective way to improve classification accuracy against adversarial attacks. Thus, it should be an effective way to defend against UAEs. However, UAEs are generated from random noise latents rather than fixed gradient perturbations by given input images. Therefore, adversarial training with UAEs is not as effective as it is with perturbation-based attacks. We test the AT-UAE with UAEs generated by AdvDiff on the MNIST dataset with 1000 images per class. The results are given in Table \ref{tab:at}. The result shows that AT with UAEs improves the robust accuracy against AdvDiff, but the performance is limited as there is an infinite number of random latents to generate UAEs. 


\section{Improving Attack Transferability}

AdvDiff achieves overwhelmingly better generation quality and attack success rate against white-box target models by adversarial diffusion sampling with a given target label $y$. However, the attack transferability is limited due to different decision boundaries from black-box models. Normally, black-box attackers use the gradient of the original label to generate perturbations. Therefore, we adopt the same settings to improve the attack transferability of AdvDiff (denoted as AdvDiff-Untargeted), i.e., $- \nabla_{{x}_{t-1}} \log p_{f}(y|{x}_{t-1})$, where $y$ is the ground truth label to generate samples. However, such sampling will decrease the generation quality as sampling from the negative distribution does not follow the benign diffusion process. Table \ref{tab:dbs} shows that the attack transferability significantly improved with a decrease in generation quality.  We leave a better design of attack transferability for future work. Additional experiments against transformers are also given in Table \ref{tab:dbs}. 



   
\begin{table}[t]

   
\caption{\textbf{The attack success rates (\%) of ResNet50 examples for transfer attack and attack against defenses on the ImagetNet dataset.}}

\vspace{-0.2in}
\begin{center}
\resizebox{\columnwidth}{!}{  
\begin{tabular}{c|ccc}

\Xhline{3\arrayrulewidth}
Method        & ResNet-152 \cite{he2016deep} & Inception v3 \cite{szegedy2016rethinking} &  DenseNet-121 \cite{huang2017densely}  \\ \hline
AutoAttack        & 32.5              & 38.6                 & 43.8               \\ \hline
U-BigGAN          & 30.8              & 35.3                & 16.8             \\
AdvDiffuser          & 18.3             & 20.0               & 24.8           \\
DiffAttack          & 21.1              & 43.9              & 23.8           \\
AdvDiff          & 20.5              & 14.9                &  35.8          \\ 
AdvDiff-Untargeted          & \textbf{52.0}              & \textbf{42.7}                &  \textbf{60.9}        \\ \Xhline{3\arrayrulewidth}
Method        & MobileNet v2 \cite{sandler2018mobilenetv2} & PNASNet \cite{liu2018progressive} &  MNASNet \cite{tan2019mnasnet}\\ \hline
AutoAttack       & 41.6              & 38.5  & 42.5             \\ \hline
U-BigGAN          & 18.4              & 22.1                & 16.8             \\
AdvDiffuser          & 30.3              & 15.2               & 26.7           \\
DiffAttack          & 22.3              & 26.9               & 30.4           \\
AdvDiff          & 15.4             & 23.2               & 38.9       \\
AdvDiff-Untargeted          & \textbf{49.5}             & \textbf{53.0}               & \textbf{47.6}        \\ \Xhline{3\arrayrulewidth}
Method        & VGG-19 \cite{simonyan2014very} & SENet \cite{hu2018squeeze} &  WRN \cite{zagoruyko2016paying}\\ \hline
AutoAttack       & 48.3               & 23.7               & 29.5            \\ \hline
U-BigGAN          & 18.4              & 22.1                & 16.8             \\
AdvDiffuser          & 28.7              & 18.8               & 22.0           \\
DiffAttack          & 30.0              & 22.1               & 23.6           \\
AdvDiff          & 16.8             & 10.0               & 11.8        \\
AdvDiff$_\text{transfer}$         & \textbf{58.5}             & \textbf{51.2}               & \textbf{57.4}        \\ \Xhline{3\arrayrulewidth}
Method        & ViT-B \cite{dosovitskiy2020image} & DeiT-B \cite{xie2020adversarial} & BEiT \cite{bao2021beit} \\ \hline
AutoAttack       & 9.3              & 8.9  & 45.3            \\ \hline
U-BigGAN          & 30.1              & 27.7                & 69.4             \\
AdvDiffuser          & 18.5              & 12.5               & 79.4           \\
DiffAttack          & 17.4              & 17.5               & 38.6           \\
AdvDiff          & 17.8              & 17.6              & 78.8        \\
AdvDiff-Untargeted          & \textbf{36.0}             & \textbf{58.5}               & \textbf{81.5}        \\ 
\Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}
\label{tab:dbs}

\vspace{-0.2in}
\end{table}


\section{User Study}

\begin{table}[t]

\begin{center}

\caption{\textbf{User Study about flipped label problem on MNIST.}}
\label{tab:us}
\resizebox{0.5\columnwidth}{!}{  
\begin{tabular}{c|cc}
\hline
 Method & U-GAN & AdvDiff \\ \hline
User Study & 425/1000         & \textbf{102/1000}         \\ \hline
\end{tabular}
}
\end{center}
   
\end{table}

We further perform a user study to justify the performance of AdvDiff, where we ask 20 participants to identify flipped label images on the MNIST dataset with 5 images on each class by U-GAN and AdvDiff. The results are given in Table \ref{tab:us}. We also give the tagged examples on UAEs generated by U-GAN and AdvDiff in Figure \ref{fig:us}, where AdvDiff's UAEs are remarkably better in generation quality and harder to identify flipped label images than U-GAN. 



\section{Improving the Generation Quality}

% Figure environment removed

   
AdvDiff crafts adversarial examples with imperceptible perturbations, making the generation quality of our methods largely reliant on the benign diffusion model's performance. Figure \ref{fig:imq} shows that AdvDiff produces higher-quality images when using StableDiffusion as the benign diffusion model.  Moreover, we can set the adversarial guidance to a smaller value for better quality with a decrease in the generation speed.  The guidance in the paper on the MNIST dataset aims at high ASR per batch for a fair comparison with previous attacks, while the visual quality can be affected by its limited $28\times28$ grey pixel space. We can also achieve stable AE generation by using latents obtained by conducting the forward diffusion process from the training dataset's clean images. 

\section{Comparing with Existing Diffusion Model Attacks}

% Figure environment removed

There are several diffusion model adversarial attacks \cite{chen2023advdiffuser,chen2023diffusion,chen2023content} achieve state-of-the-art performance. However, most of them did not release the official code which makes it difficult to compare with these methods. All these works adopt the optimization over given loss functions (i.e., PGD-like gradient) to generate UAEs with the diffusion models. Figure \ref{fig:diffcomp} provides a direct comparison of adversarial examples from different methods. Our findings indicate that PGD-like adversarial guidance perturbations significantly alter the texture of benign images from AdvDiffuser. Similarly, the perturbations from DiffAttack are also very similar to standard PGD perturbations, where the perturbations are uniformly applied across the entire image. In contrast, our perturbations mainly target the mushroom's contour and are substantially less noticeable than those from existing attacks.

Our work can easily be combined with some exciting works by replacing the gradient with AdvDiff's adversarial guidance (especially for AdvDiffuser \cite{chen2023advdiffuser} which directly adopts the PGD gradient to conduct the adversarial attack). We hope our work can gain new insight for designing adversarial attacks using diffusion models.




\section{Comparing with 2021 CVPR Competition Winner}

We compare with the 1st winner \cite{liu2023towards} of 2021 CVPR unrestricted adversarial attack competition \cite{chen2021unrestricted} follows their official implementation on ImageNet. Two variants of \cite{liu2023towards}'s attacks are compared, which are GA-IFGSM and GA-FSA. The results are given in Table \ref{tab:cvpr}. The proposed AdvDiff outperforms \cite{liu2023towards}'s attack in terms of generation quality and attack performance. It may not be a fair comparison as \cite{liu2023towards}'s attack is not a synthetic attack.  

   
\begin{table}[ht]
\caption{\textbf{The attack performance on the ImagetNet dataset.}}

\vspace{-0.2in}
\begin{center}
\resizebox{\columnwidth}{!}{  
\begin{tabular}{c|cc|ccccc}
\Xhline{3\arrayrulewidth} 
            & ASR           & PGD-AT        & FID           & LPIPS         & SSIM          & BRISQUE       & TRES          \\ \hline
AdvDiff-Untargeted & 99.5          & \textbf{94.5}          & 22.8          & 0.14          & 0.85          & 16.2          & 76.8          \\
AdvDiff     & 99.8 & 92.4 & \textbf{16.2} & \textbf{0.03} & \textbf{0.96} & \textbf{18.1} & \textbf{82.1} \\
GA-IFGSM    & 99.8          & 82.6          & 50.4          & 0.24          & 0.78          & 40.4          & 62.0          \\
GA-FSA      & \textbf{99.9} & 91.4          & 70.6          & 0.32          & 0.56          & 50.8          & 58.4     \\
\Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}
   

\vspace{-0.2in}
\label{tab:cvpr}
\end{table}

\section{Discussion about perturbations, flipped-label, and diffusion adversarial examples}

Perturbation-based adversarial attacks typically generate adversarial examples by iteratively adding adversarial gradients to clean images, which inevitably introduces noisy patterns. These patterns create visible defects that can be detected by humans. However, these perturbations are applied at the pixel level, leaving the content of the clean image unchanged. In contrast, GAN-based unrestricted adversarial attacks create Unrestricted Adversarial Examples (UAEs) by perturbing the latents. The generator then produces images based on these GAN latents. This method introduces perturbations at the content level, as GAN-based techniques do not directly add noise to the final images. Given the generator's sensitivity to changes in low-dimensional latents, adversarial latents can result in images with entirely different content. This can even lead to a change in the label of the adversarial images, creating what we refer to as flipped-label images.

Adversarial examples generated by diffusion models follow a diffusion generation process, which can be seen as a denoising process. As a result, the noisy gradients injected are removed during the generation process. This necessitates a larger Projected Gradient Descent (PGD) gradient in previous works to successfully generate a UAE, often resulting in a decrease in image quality. In our work, we inject the adversarial objective in an interpretable manner by increasing the conditional likelihood on the target attack label, following the diffusion process. We provide detailed proof of the effectiveness of our adversarial guidance in Appendix A and B. Consequently, our proposed AdvDiff method is more reliable in generating high-quality adversarial examples than simply conducting the PGD attack on the sampled images of the diffusion model.

\section{Ethics Concerns}

AdvDiff can bring security problems to existing DL-based applications, and it generates visually indistinguishable adversarial examples to humans while deceiving the target DL model. This characteristic makes the AdvDiff's images hard to detect by current defense mechanisms, even with human experts. However, our unrestricted adversarial examples can be adopted for adversarial training because our adversarial examples are generated close to the decision boundary of the target classifier. Another critical reason for achieving adversarial training is that the generated adversarial examples have high fidelity and high diversity on the large-scale dataset. Therefore, AdvDiff can have positive social impacts on improving the AI model robustness. 

\section{Limitations}

Although AdvDiff shows superior performance on the unrestricted adversarial attack with large-scale datasets, the generation speed of adversarial examples with diffusion models is relatively slower than GAN-based models. This limitation makes AdvDiff hard to perform a real-time attack. However, the unrestricted adversarial attack does not have a real-time attack scenario. And we can also adopt a fast-sampling method to improve the sampling speed of the AdvDiff, which we aim to improve in future work. Another limitation is that AdvDiff is sensitive to the parameter settings of two adversarial guidance scales $a$ and $s$. The reason is that AdvDiff can deploy in any conditional diffusion model, which has different sampling mechanisms in other datasets. Therefore, we should set the adversarial guidance scales accordingly, but the attack performances are not vastly changed if the scales are in an appropriate range.



\bibliographystyle{splncs04}
\bibliography{main}

\end{document}