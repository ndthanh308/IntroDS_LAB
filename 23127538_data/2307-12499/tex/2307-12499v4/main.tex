\documentclass[runningheads]{llncs}

% ---------------------------------------------------------------
% Include basic ECCV package
 
% TODO REVIEW: Insert your submission number below by replacing '*****'
% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[review,year=2024,ID=*****]{eccv}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{eccv}

% OPTIONAL: Un-comment the following line for a version which is easier to read
% on small portrait-orientation screens (e.g., mobile phones, or beside other windows)
%\usepackage[mobile]{eccv}


% ---------------------------------------------------------------
% Other packages

% Commonly used abbreviations (\eg, \ie, \etc, \cf, \etal, etc.)
\usepackage{eccvabbrv}

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{multirow}
\usepackage{makecell}

% The "axessiblity" package can be found at: https://ctan.org/pkg/axessibility?lang=en
\usepackage[accsupp]{axessibility}  % Improves PDF readability for those with disabilities.


% ---------------------------------------------------------------
% Hyperref package

% It is strongly recommended to use hyperref, especially for the review version.
% Please disable hyperref *only* if you encounter grave issues.
% hyperref with option pagebackref eases the reviewers' job, but should be disabled for the final version.
%
% If you comment hyperref and then uncomment it, you should delete
% main.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).

% TODO FINAL: Comment out the following line for the camera-ready version
%\usepackage[pagebackref,breaklinks,colorlinks,citecolor=eccvblue]{hyperref}
% TODO FINAL: Un-comment the following line for the camera-ready version
\usepackage{hyperref}

% Support for ORCID icon
\usepackage{orcidlink}


\begin{document}

% ---------------------------------------------------------------
% TODO REVIEW: Replace with your title
\title{AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models} 

% TODO REVIEW: If the paper title is too long for the running head, you can set
% an abbreviated paper title here. If not, comment out.
\titlerunning{AdvDiff}

% TODO FINAL: Replace with your author list. 
% Include the authors' OCRID for the camera-ready version, if at all possible.
\author{Xuelong Dai\inst{1}\orcidlink{0000-0001-6646-6514} \and
Kaisheng Liang\inst{1}\orcidlink{0000-0002-8297-6378} \and
Bin Xiao\inst{1}\orcidlink{0000-0003-4223-8220}}

% TODO FINAL: Replace with an abbreviated list of authors.
\authorrunning{X.~Dai et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.

% TODO FINAL: Replace with your institution list.
\institute{
The Hong Kong Polytechnic University\\
\email{xuelong.dai@connect.polyu.hk, cskliang@comp.polyu.edu.hk, b.xiao@polyu.edu.hk }}

\maketitle

\begin{abstract}
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often directly inject Projected Gradient Descent (PGD) gradients into the sampling of generative models, which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for GAN-based methods on large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable in generating high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective in generating unrestricted adversarial examples, which outperforms state-of-the-art unrestricted adversarial attack methods in terms of attack performance and generation quality.

  \keywords{Unrestricted Adversarial Attacks  \and Diffusion Models \and Interpretable Adversarial Diffusion Sampling}
\end{abstract}

\section{Introduction}

While the deep learning (DL) community continues to explore the wide range of applications of DL models, researchers \cite{szegedy2013intriguing} have demonstrated that these models are highly susceptible to deception by adversarial examples. Adversarial examples are generated by adding perturbations to clean data. The perturbed examples can deceive DL classifiers with high confidence while remaining imperceptible to humans. Many strong attack methods \cite{madry2017towards,dong2018boosting,carlini2017towards,croce2020reliable,liang2023styless,li2023physical} are proposed and investigated to improve the robustness of DL models.

In contrast to existing perturbation-based adversarial attacks, Song et al. \cite{song2018constructing}
found that using a well-trained generative adversarial network with an auxiliary classifier (AC-GAN) \cite{odena2017conditional} can directly generate new adversarial examples without perturbing the clean data. These newly generated examples are considered \textbf{unrestricted} as they are obtained by optimizing input noise vectors without any norm restrictions. Compared to traditional adversarial examples, unrestricted adversarial examples \cite{deb2020advfaces,qiu2020semanticadv} are more aggressive against current adversarial defenses. A malicious adversary can also generate an unlimited number of unrestricted adversarial examples using a trained GAN.

Diffusion models \cite{ho2020denoising} are likelihood-based generative models proposed recently, which emerged as a strong competitor to GANs. Diffusion models have outperformed GANs for image synthesis tasks \cite{dhariwal2021diffusion,rombach2022high,kim2022diffusionclip}. Compared with GAN models, diffusion models are more stable during training and provide better distribution coverage. Diffusion models contain two processes: a forward diffusion process and a reverse generation process. The forward diffusion process gradually adds Gaussian noise to the data and eventually transforms it into noise. The reverse generation process aims to recover the data from the noise by a denoising-like technique. A well-trained diffusion model is capable of generating images with random noise input. Similar to GAN models, diffusion models can achieve adversarial attacks by incorporating adversarial objectives \cite{chen2023advdiffuser,chen2023content,chen2023diffusion}. 


 GAN-based unrestricted adversarial attacks often exhibit poor 
 performance on high-quality datasets, particularly in terms of visual quality, because they directly add the PGD perturbations to the GAN latents without theoretic supports. These attacks tend to generate low-quality adversarial examples compared to benign GAN examples \cite{song2018constructing}. Therefore, these attacks are not imperceptible among GAN synthetic data. 
 Diffusion models, however, offer state-of-the-art generation performance \cite{dhariwal2021diffusion} on challenging datasets like LSUN \cite{yu2015lsun} and ImageNet \cite{deng2009imagenet}. The conditional diffusion models can generate images based on specific conditions by sampling from a perturbed conditional Gaussian noise, which can be carefully modified with adversarial objectives. These properties make diffusion models more suitable for conducting unrestricted adversarial attacks. Nevertheless, existing adversarial attack methods using diffusion models \cite{chen2023advdiffuser,chen2023content,chen2023diffusion} adopt similar PGD perturbations to the sample in each reverse generation process, making them generate relatively low-quality adversarial examples.  

In this paper, we propose a novel and interpretable unrestricted adversarial attack method called AdvDiff that utilizes diffusion models for adversarial examples generation, as shown in Figure \ref{fig:pip}. Specifically, AdvDiff uses a trained conditional diffusion model to conduct adversarial attacks with two new adversarial guidance techniques. 1) During the reverse generation process, we gradually add \emph{adversarial guidance} by increasing the likelihood of the target attack label. 2) We perform the reverse generation process multiple times, adding adversarial prior knowledge to the initial noise with the \emph{noise sampling guidance}.

Our theoretical analysis indicates that these adversarial guidance techniques can effectively craft adversarial examples by the reverse generation process with adversarial conditional sampling. Furthermore, the sampling of AdvDiff benefits from stable and high sample quality of the diffusion models sampling, which leads to the generation of realistic unrestricted adversarial examples.
Through extensive experiments conducted on two datasets, i.e., the high-quality dataset ImageNet, and the small, robust dataset MNIST, we have observed a significant improvement in the attack performance using  AdvDiff with diffusion models. 
These results prove that our proposed AdvDiff is more effective than previous unrestricted adversarial attack methods in conducting unrestricted adversarial attacks to generate high-fidelity and diverse examples without decreasing the generation quality.




% Figure environment removed
   
Our contributions can be summarized as follows:

\begin{itemize}
\setlength{\itemsep}{0pt}

\setlength{\parsep}{0pt}

\setlength{\parskip}{0pt}
   \item We propose AdvDiff, the new form unrestricted adversarial attack method that utilizes the reverse generation process of diffusion models to generate realistic adversarial examples.
   \item We design two new effective adversarial guidance techniques to the sampling process that incorporate adversarial objectives to the diffusion model without re-training the model. Theoretical analysis reveals that AdvDiff can generate unrestricted adversarial examples while preserving the high-quality and stable sampling of the conditional diffusion models.
   \item  We perform extensive experiments to demonstrate that AdvDiff achieves an overwhelmingly better performance than GAN models on unrestricted adversarial example generation.

\end{itemize}

\section{Preliminaries}
In this section, we introduce the diffusion model and the classifier guidance for constructing our adversarial diffusion model.

\subsection{Diffusion Model}
The Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising} is utilized to learn the Gaussian
transitions from $p(x_t)=\mathcal{N}(x_t;0,\mathbf{I})$ to recover the data $x_0 \sim q(x_0)$ with a Markov chain. We call this denoising-like process the \textit{reverse generation process}. Following the pre-defined $T$ time steps, DDPM obtains a sequence of noisy data $\{x_{T-1}, \dots, x_{1}\}$ and finally recovers the data $x_0$. It is defined as:
\begin{equation}
    p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1}:\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) 
\end{equation}

Conversely, the \textit{forward diffusion process} leverages a fixed Markov chain to iteratively add Gaussian noise to the sampled data $x_0 \sim q(x_0)$ according to the scheduling function $\beta_1, \dots, \beta_N$. Specifically, it is defined as:
\begin{equation}
    q(x_t|x_{t-1}):=\mathcal{N}(x_t:\sqrt[]{1-\beta_t}x_{t-1},\beta_t\textbf{I} ) 
\end{equation}

Training of the DDPM requires accurate inference of the mean value $\mu_{\theta}$ with a deep learning model in the reverse generation process. The objective is to learn the variational lower-bound (VLB) on $\log p_\theta(x_0)$. In order to complete the training objective, we train the model $\epsilon_\theta$ to predict the added Gaussian noise in the forward diffusion process. The standard mean-squared error loss is adopted:
\begin{equation}
    \mathcal{L}_{DDPM}:= E_{t\sim [1,T],\epsilon \sim\mathcal{N}(0,\textbf{I})}\left \| \epsilon-\epsilon_\theta (x_t,t) \right \|^2 
\end{equation}

Song et al. \cite{song2020denoising} proposed DDIM, which provides an alternative noising process without restricting to a Markov chain. DDIM can achieve a much faster sampling than DDPM with the same training as DDPM. We perform experiments on both DDPM and DDIM to demonstrate the usability of our AdvDiff. DDPM is adopted to introduce our method for simplicity.

\subsection{Classifier-Guided Guidance}
Dhariwal et al. \cite{dhariwal2021diffusion} achieved conditional diffusion sampling by adopting a trained classifier. The conditional information is injected into the diffusion model by modifying the mean value $\mu_{\theta}(x_t,t)$ of the samples according to the gradient of the prediction of the target class $y$ by the trained classifier. They adopted log probability to calculate the gradient, and the mean value is given by:
\begin{equation}
    \hat{\mu}_{\theta}(x_t,t) = \mu_{\theta}(x_t,t) + s\cdot \nabla_{x_t}\log p_{\phi}(y|x_t) 
\end{equation}
where $s$ is the guidance scale.

\subsection{Classifier-Free Guidance}
Ho et al. \cite{ho2021classifier} recently proposed a new conditional diffusion model using classifier-free guidance that injects class information without adopting an additional classifier. The classifier-free guidance utilizes a conditional diffusion model $p_{\theta}(x|y)$ for image synthesis with given labels. For effective training, they jointly train the unconditional diffusion model $p_{\theta}(x|\emptyset)$ and the conditional diffusion model $p_{\theta}(x|y)$, where the unconditional diffusion model is simply replacing the label information with $\emptyset$. Sampling is performed by pushing the model towards the latent space of  $p_{\theta}(x|y)$ and away from $p_{\theta}(x|\emptyset)$:
\begin{equation}
    \hat{\epsilon}_{\theta }(x_t|y)= {\epsilon}_{\theta }(x_t|\emptyset) + w\cdot({\epsilon}_{\theta }(x_t|y)-{\epsilon}_{\theta }(x_t|\emptyset))
\end{equation}
where $w$ is the weight parameter for class guidance and $\emptyset$ is the empty set. 

The idea of classifier-free guidance is inspired by the gradient of an implicit classifier $p^i(y|x)\propto p(x|y)/p(x)$, the gradient of the classifier would be:
\begin{align}
\nabla_x logp^i(y|x)&\propto  \nabla_x logp(x|y)-\nabla_x logp(x) \notag \\
&\propto {\epsilon}_{\theta }(x_t|y)-{\epsilon}_{\theta }(x_t|\emptyset)
\end{align}

The classifier-free guidance has a good capability of generating high-quality conditional images, which is critical for performing adversarial attacks. The generation of these images does not rely on a classification model and thus can better fit the conditional distribution of the data.

\section{Adversarial Diffusion Sampling}

\subsection{Rethinking Unrestricted Adversarial Examples}

Song et al. \cite{song2018constructing} presented a new form of adversarial examples called unrestricted adversarial examples (UAEs). These adversarial examples are not generated by adding perturbations over the clean data but are directly generated by any generative model. UAEs can be viewed as false negative errors in the classification tasks, and they can also bring server security problems to deep learning models. These generative-based UAEs can be formulated as:

\begin{equation}
   A_{\text{UAE}} \triangleq \{x \in \mathcal{G}(z_{\text{adv}},y)|y \neq f(x)\}
\end{equation}
where $f(\cdot)$ is the target model for unrestricted adversarial attacks. The unrestricted adversarial attacks aim to generate UAEs that fool the target model while still can be visually perceived as the image from ground truth label $y$.

Previous UAE works adopt GAN models for the generation of UAEs, and these works perturb the GAN latents by maximizing the cross-entropy loss of the target model, i.e., $\max_{z_{\text{adv}}}\mathcal{L}(f(\mathcal{G}(z_{\text{adv}},y)),y)$. Ideally, the generated UAEs should guarantee similar generation quality to the samples crafted by standard $z$ because successful adversarial examples should be imperceptible to humans. In other words, UAEs should not be identified among the samples with adversarial latents and standard latents. 

However, due to GAN's poor interpretability, there's no theoretical support on $z_{adv}$ that can craft UAEs with normally trained GANs. The generator of GAN is not trained with $z_{adv} = z + \nabla\mathcal{L}$ but only $z \sim \mathcal{N}(0, \textbf{I})$. Therefore, GAN-based UAEs encounter a significant decrease in generation quality because samples with $z_{adv}$ are not well-trained compared with samples with $z \sim \mathcal{N}(0, \textbf{I})$. Moreover, the GAN latents are sampled from low dimensional latent spaces. Therefore, GANs are extremely sensitive to the latent $z$ \cite{shen2020interpreting,liang2022exploring}. If we inject gradients of the classification results into GAN latents, GAN-based methods are more likely to generate flipped-label UAEs (images corresponding to the targeted attack label $y_a$ instead of the conditional generation label $y$) and distorted UAEs. However, these generation issues are hard to address only by attack success rate (ASR). In other words, even with a high ASR, some of the successful UAEs with GAN-based methods should be identified as failure cases for poor visual quality. However, such cases can not be reflected by ASR but can be evaluated by generation quality. All these problems may indicate that GAN models are not suitable for generative-based adversarial attacks.


Diffusion models have shown better performance on image generation than GAN models \cite{dhariwal2021diffusion}. They are log-likelihood models with interpretable generation processes. In this paper, we aim to generate UAEs by injecting the adversarial loss with theoretical proof and without sabotaging the benign generation process, where we increase the conditional likelihood on the target attack label by following the diffusion process. The perturbations are gradually injected with the backward generation process of the diffusion model by the same sample procedure. As shown in Figure \ref{fig:fn}, the diffusion model can sample images from the conditional distribution $p(x|y)$. The samples from $p(x|y,f(x)\ne y)$ are the adversarial examples that are misclassified by $f(\cdot)$. These examples also follow the data distribution $p(x|y)$ but on the other side of the label $y$ 's decision boundary of $f(\cdot)$. Moreover, the diffusion model's generation process takes multiple sampling steps. Thus, we don't need one strong perturbation to the latent like GAN-based methods. The AdvDiff perturbations at each step are unnoticeable, and perturbations are added to the high dimensional sampled data rather than low dimensional latents. Therefore, AdvDiff with diffusion models can preserve the generation quality and barely generates flipped-label or distorted UAEs. 




% Figure environment removed


\subsection{Adversarial Diffusion Sampling with Theoretical Support}

There are several existing adversarial attack methods \cite{chen2023advdiffuser,chen2023diffusion,chen2023content} that adopt diffusion models to generate adversarial examples. However, these methods still adopt PGD or I-FGSM gradients to perturb the diffusion process for constructing adversarial examples. As discussed earlier, the generation process of diffusion models is a specially designed sampling process from given distributions. Such adversarial gradients change the original generation process and can harm the generation quality of the diffusion model. Additionally, these methods fail to give a comprehensive discussion of the adversarial guidance with theoretical analysis. Therefore, we aim to design a \textbf{general} and \textbf{interpretable} method to generate adversarial examples using diffusion models \textbf{without} affecting the benign diffusion process.


\subsection{Adversarial Guidance}

Inspired by Dhariwal's work \cite{dhariwal2021diffusion} that achieves the conditional image generation by classifier gradient guidance $\nabla_{x_t}\log p_{\phi}(y|x_t)$, we generate our UAEs with adversarial gradient guidance over the reverse generation process. Our attack aims at utilizing a conditional diffusion model $\epsilon_{\theta}(x_t,y)$ to generate $x_0$ fits the ground truth label $y$ while deceiving the target classifier with $p_{f}(x_0) \ne y$. These generated samples are the false negative results in $p_{f}$'s classification results. 

Normally, we will obtain the images with label $y$ by following the standard reverse generation process with classifier-free guidance:
\begin{equation}
{x}_{t-1} = \mu({x}_t,y) + \sigma_t{\varepsilon}
\label{eq:cfg}
\end{equation}
where $\mu({x}_t,y)$ is the conditional mean value and $\varepsilon$ is sampled from ${\varepsilon}\sim \mathcal{N}(0,\textbf{I})$.

Sampling by Equation \ref{eq:cfg}, we obtain the samples with the generation process $p(x_{t-1}|x_t,y)$. Following the above-mentioned definition of UAEs, we can get our adversarial examples by adding adversarial guidance to the standard reverse process, which is performing another sampling with the adversarial generation process $p(x^*_{t-1}|x_{t-1},f(x)\ne y)$. We find that specifying a target label for the adversarial generation process is more effective during experiments. Suggest the target label $y_{a}$ is the target for the adversarial attacks, the adversarial example is sampled by the following steps:
\begin{equation}
{x}^*_{t-1} = x_{t-1} +\sigma_t^2 s\nabla_{{x}_{t-1}} \log p_{f}({y_{a}}|{x}_{t-1})
\label{eq:sample}
\end{equation}
where $s$ is the adversarial guidance scale. The derivation of Equation \ref{eq:sample} is given in Appendix A. Intuitively, the adversarial guidance encourages the generation of samples with a higher likelihood of the target label.

In practice, we utilize the classifier-free guidance to train a conditional diffusion model $\epsilon_{\theta}(\cdot)$ as our basic generation model.

\subsection{Noise Sampling Guidance}

We can improve the reverse process by adding an adversarial label prior to the noise data $x_T$. The UAEs are a subset of the dataset labeled with $y$. They can be viewed as the conditional probability distribution with $p(x|y, f(x)=y_{a})$ during sampling, and $y_a$ is the target label for the adversarial attack. Therefore, we can add the adversarial label prior to $x_t$ with Bayes' theorem:
\begin{align}
    p({x}_{T}|y_{a}) =& \frac{p(y_{a}|{x}_{T})p({x}_{T})}{p(y_{a})} = \frac{p(y_{a}|x_T,x_0)p(x_T|x_0)}{p(y_{a}|x_0)} \notag \\=&p(x_T|x_0)e^{\log p(y_{a}|x_T)- \log p(y_{a}|x_0) }
    \label{eq:noise}
\end{align}

We can infer the $x_t$ with the adversarial prior by Equation \ref{eq:noise}, i.e.,
\begin{equation}
    {x}_{T} = (\mu({x}_0,y) + \sigma_t{\varepsilon}) +\bar{\sigma}_T^2 a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)
    \label{eq:sample_noise}
\end{equation}
where $a$ is the noise sampling guidance scale. See Appendix B for detailed proof.

Equation \ref{eq:sample_noise} is similar to Equation \ref{eq:sample} as they both add adversarial guidance to the reverse generation process. However, the noise sampling guidance is added to $x_T$ according to the final classification gradient $\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)$, which provides a strong adversarial guidance signal directly to the initial input of the generative model. The gradient of Equation \ref{eq:sample_noise} is effective as it reflects the eventual classification result of the target classifier.

\subsection{Training-Free Adversarial Attack}

The proposed adversarial attack does not require additional modification on the training of the diffusion model. The adversarial examples are sampled by using Algorithm \ref{alg:sampling} over the trained classifier-free diffusion model $\epsilon_{\theta}(\cdot)$.  We give the AdvDiff algorithm on DDIM in the Appendix.

\begin{algorithm}[tb]
  \caption{DDPM Adversarial Diffusion Sampling} 
  \label{alg:sampling}
  \begin{algorithmic}[1]
    \Require $y_a$: target label for adversarial attack
    \Require $y$: ground truth class label
    \Require $s,a$: adversarial guidance scale
    \Require $w$: classification guidance scale
    \Require $N$: noise sampling guidance steps
    \Require $T$: reverse generation process timestep

    \State $x_{T} \sim \mathcal{N}(0, \textbf{I})$
    \State $x_{adv} = \varnothing $
    \For{$i = 1\ldots N$}
    \For{$t=T, \dotsc, 1$}

      \State $\tilde{\epsilon}_t = (1+w)\epsilon_\theta(x_{t}, y) - w\epsilon_{\theta}(x_{t})$ 
      %\Statex  $\triangleright$ DDPM Sampling (can be replaced with DDIM)
      \State Classifier-free sampling $x_{t-1}$ with $\tilde{\epsilon}_t$.
      %Calculate $\mu({x}_t,y) + \sigma_t{\varepsilon}$ referring to [23]
      \State Input $x_{t-1}$ to target model and get the gradient $\log p_{f}({y_{a}}|{x}_{t-1}))$
      \State ${x}^*_{t-1} = x_{t-1} +\sigma_t^2 s\nabla_{{x}_{t-1}} \log p_{f}({y_{a}}|{x}_{t-1})$
    \EndFor
      \State Obtain classification result from $f(x_0)$
      \State Compute the gradient with $\log p_{f}({y_{a}}|{x}_0)$
      \State Update $x_{T}$ by $x_T = x_T + \bar{\sigma}_T^2 a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)$
      \State  $x_{adv} \gets x_{0}$ if $f(x_0)=y_a$
    
    
    \EndFor
    \State \textbf{return} $x_{adv}$
  \end{algorithmic}
\end{algorithm}

\section{Experiments}

% Figure environment removed


\textbf{Datasets and target models.} We use two datasets for major evaluation: MNIST \cite{deng2012mnist} and ImageNet \cite{deng2009imagenet}. MNIST is a 10-classes dataset consisting of handwritten numbers from 0 to 9. We adopt the MNIST dataset to evaluate our method for low-quality robust image generation. ImageNet is a large visual database with 1000 object classes and is used for the high-quality generation task. For target classifiers, we adopt simple LeNet5 \cite{lecun1998gradient}, and ResNet18 \cite{he2016deep} for the MNIST dataset, and the widely-used ResNet50 \cite{he2016deep} and WideResNet50-2 \cite{zagoruyko2016wide} for the ImageNet dataset.

\textbf{Comparisons.} It is not applicable to give a clear comparison between perturbation attacks and unrestricted attacks because perturbation attacks have the corresponding ground truth while unrestricted attacks do not. We mainly compare our method with the unrestricted adversarial attack U-GAN \cite{song2018constructing} and give the discussion with the AutoAttack \cite{croce2020reliable}, PGD \cite{madry2017towards}, BIM \cite{dong2018boosting}, and C\&W \cite{carlini2017towards} perturbation-based attacks under norm $\ell_\text{inf}=8/255$. For U-GAN, We adopt AC-GAN \cite{odena2017conditional} for the MNIST dataset, and SAGAN \cite{zhang2019self} and BigGAN \cite{brock2018large} for the ImageNet dataset, as AC-GAN has shown poor performance on ImageNet. We use the official code from DiffAttack \cite{chen2023diffusion} and implement AdvDiffuser by ourselves \cite{chen2023advdiffuser} for comparisons. We do not compare with Chen et al. \cite{chen2023content}, because they use a similar method as DiffAttack and without official code. Because existing diffusion model attacks are all untargeted attacks, we include the untargeted version of AdvDiff for a clear comparison, which is represented by ``AdvDiff-Untargeted''. 

\textbf{Implementation details.} Because our adversarial diffusion sampling does not require additional training to the original diffusion model, we use the pre-trained diffusion model in our experiment. We adopt DDPM \cite{ho2020denoising} with classifier-free guidance for the MNIST dataset and Latent Diffusion Model (LDM) \cite{rombach2022high} with DDIM sampler for the ImageNet dataset.  For MNIST dataset, we use $N=10$, $s=0.5$, and $a=1.0$, And $N=5$, $s=0.7$, and $a=0.5$ for ImageNet dataset. More details and experiments are given in the Appendix.

\textbf{Evaluation metrics.} We utilize the top-1 classification result to evaluate the Attack Success Rate (ASR) on different attack methods under untargeted attack settings. As discussed earlier, GAN-based UAEs often encounter severe generation quality drops compared to benign GAN samples. Therefore, we give comparisons of generation performance on ImageNet to evaluate the attack performance of different UAEs in imperceptibly. The results are averaged with five runs. We use ResNet50 as the target model for default settings.


   
   
% Figure environment removed
   
\subsection{Attack Performance}
\noindent \textbf{MNIST} We show the attack success rate against the normally trained model and adversarially trained model \cite{madry2017towards} in the MNIST dataset. All the selected adversarial attacks achieve over 90\% attack success rate against the normally trained model. The adversarially trained model can effectively defend against perturbation-based adversarial attacks for their noise-like perturbation generation patterns, as reported in Table \ref{tab:mnis}. However, the UAEs obviously perform better with their non-perturbed image generation. Despite the fact that the unrestricted attack can break through the adversarial defenses, the crafted adversarial examples should also be imperceptible to humans for a reasonably successful attack. The visualized adversarial examples in Figure \ref{fig:mnist} show that the perturbation-based adversarial attacks tend to blur the original images while U-GAN can generate mislabeled adversarial examples.


\begin{table}[t]
\parbox{.38\textwidth}{
\centering

\caption{\textbf{The attack success rate on MNIST dataset.}  }
   
\label{tab:mnis}
\resizebox{0.38\textwidth}{!}{  
\begin{tabular}{l|cccc}


\Xhline{3\arrayrulewidth}
\multirow{3}{*}{Method} & \multicolumn{4}{c}{ASR(\%)}                                                                        \\ \cline{2-5} 
                        & \multicolumn{2}{c|}{LeNet5}                                & \multicolumn{2}{c}{ResNet18}          \\
                        & Clean        & \multicolumn{1}{c|}{PGD-AT}                 & Clean        & PGD-AT                 \\ \hline
PGD                     & 99.8         & \multicolumn{1}{c|}{25.6}                   & 99.3         & 20.8                   \\
BIM                     & 99.6         & \multicolumn{1}{c|}{34.6}                   & \textbf{100} & 31.5                   \\
C\&W                    & \textbf{100} & \multicolumn{1}{c|}{68.6}                   & \textbf{100} & 64.5                   \\ \hline
U-GAN                   & 88.5         & \multicolumn{1}{c|}{\textit{79.4}}          & 85.6         & \textit{75.1}          \\
AdvDiff              & 94.2         & \multicolumn{1}{c|}{\textit{\textbf{88.6}}} & 92.1         & \textit{\textbf{86.5}}\\


\Xhline{3\arrayrulewidth}
\end{tabular}
}
}
\hfill
\parbox{.6\textwidth}{
\centering
\caption{\textbf{The attack success rate on ImageNet dataset.} U-SAGAN and U-BigGAN represent the base GAN models for U-GAN are SAGAN and BigGAN, respectively.}
\resizebox{0.6\textwidth}{!}{

\label{tab:imagenet}
\begin{tabular}{l|cccccc|c}
\Xhline{3\arrayrulewidth}  
\multirow{3}{*}{Method} & \multicolumn{6}{c|}{ASR(\%)}                                                         & \multirow{3}{*}{Time (s)} \\ \cline{2-7}
                        & \multicolumn{3}{c|}{ResNet50}                  & \multicolumn{3}{c|}{WideResNet50-2} &                           \\
                        & Clean & DiffPure & \multicolumn{1}{c|}{PGD-AT} & Clean     & DiffPure    & PGD-AT    &                           \\ \hline
AutoAttack              & 95.1  & 22.2     & \multicolumn{1}{c|}{56.2}   & 94.9      & 20.6        & 55.4      &   0.5                        \\ \hline
U-SAGAN                 & 99.3  & 30.5     & \multicolumn{1}{c|}{80.6}   & 98.9      & 28.6        & 70.1     &    10.4                       \\
U-BigGAN                & 96.8  & 40.1     & \multicolumn{1}{c|}{81.5}   & 96.5      & 35.5       & 78.4      &    11.2                       \\
AdvDiffuser             & 95.4  & 28.9     & \multicolumn{1}{c|}{90.6}   & 94.6      & 26.5           & 88.9      &        38.6                   \\
DiffAttack              & 92.8  & 30.6     & \multicolumn{1}{c|}{88.4}   & 90.6      & 27.6           & 85.3      &         28.2                  \\

AdvDiff                 & \textbf{99.8}  & 41.6     & \multicolumn{1}{c|}{92.4}   & \textbf{99.9}      & 38.5       & 90.6      &    \textbf{9.2}        \\

AdvDiff-Untargeted      & 99.5  & \textbf{75.2}     & \multicolumn{1}{c|}{\textbf{94.5}}   & 99.4      & \textbf{70.5}        & \textbf{92.6}      &    9.6                       \\

\Xhline{3\arrayrulewidth}                 
\end{tabular}
}
}
\end{table}




\noindent \textbf{ImageNet} It is reported that deep learning models on ImageNet are extremely vulnerable to adversarial attacks. However, the state-of-the-art adversarial defense DiffPure \cite{nie2022DiffPure} and adversarial training \cite{madry2017towards} can still defend against the perturbation-based attacks, as reported in Table \ref{tab:imagenet}. More UAEs evade the current defenses, but the generation quality of U-GAN is relatively poor compared to our adversarial examples. This phenomenon also shows that the performance of UAEs is heavily affected by the generation quality of the generation model. The adversarial examples generated by AdvDiff are more aggressive and stealthy than U-GAN's. Meanwhile, the generation speed of AdvDiff is the best among all the unrestricted adversarial attack methods. Note that we adopt the clean images generated by LDM to achieve DiffAttack and AutoAttack for a fair comparison.



\subsection{Generation Quality: True ASR for UAEs}


We witness similar ASR with U-GAN and AdvDiff. However, imperceptibility is also critical for a successful unrestricted adversarial attack, so we adopt the evaluation metrics in \cite{dhariwal2021diffusion} to compare the generation quality with and without performing unrestricted attacks. Table \ref{tab:gvd} shows that the AdvDiff achieves an overwhelming better IS score and similar FID score on the large-scale ImageNet dataset, where FID \cite{heusel2017gans} and IS \cite{salimans2016improved} scores are commonly adopted for evaluating the quality of a generative model. Because the generation of UAEs does not modify the data distribution of the generated images, the Precision score can be inferred as generation quality, while the Recall score indicates the flipped-label problems. We witness the frequent generation of flipped-label UAEs and low-quality UAEs from GAN-based methods, which is reflected by the decrease in the Precision score and the increase in the Recall score. Figure \ref{fig:gvd} illustrates this problem with some examples. It can be further proved that U-BigGAN achieves much higher image quality on non-reference metrics than reference metrics, as shown in Table \ref{tab:more}.


   
\begin{table}[t]
\caption{\textbf{The generation performance on the ImagetNet dataset.}}


\begin{center}
\resizebox{0.8\columnwidth}{!}{  
\begin{tabular}{l|ccc|cc}

\Xhline{3\arrayrulewidth}

Method             & FID ($\downarrow$)  & sFID ($\downarrow$) & IS ($\uparrow$)   & Precision ($\uparrow$)  & Recall ($\uparrow$)  \\ \hline
SAGAN    & 41.9      & 50.2    & 26.7      & 0.50      & 0.51   \\
BigGAN   & 19.3      & 45.7    & 250.3     & 0.95      & 0.21   \\
LDM      & 12.3      & 25.4    & 385.5     & 0.94      & 0.73   \\ \hline
U-SAGAN  & 52.8/+26\% & 52.2/+4\%  & 12.5/-53\%  & 0.58      & 0.57   \\
U-BigGAN & 25.4/+31\%   & 52.1/+14\% & 129.4/-48\% & 0.81      & 0.35   \\
AdvDiffuser & 26.8/+117\% & 38.6/+51\%  & 206.8/-46\%  & 0.70      & 0.75   \\
DiffAttack & 20.5/+66\%   & 40.2/+58\% & 264.3/-31\% & 0.83      & 0.73   \\
AdvDiff  & \textbf{16.2}/+31\%    & \textbf{30.4}/+20\% & \textbf{343.8}/-10\%  & \textbf{0.90}      & 0.75   \\
AdvDiff-Untargeted & 22.8/+85\%    & 33.4/+28\% & 220.8/-45\%  & 0.85      & \textbf{0.76}   \\
\Xhline{3\arrayrulewidth}  
\end{tabular}
}
\end{center}
   


\label{tab:gvd}
\end{table}

\begin{table}[t]
\caption{\textbf{The image quality on the ImagetNet dataset.}}


\begin{center}
\resizebox{0.8\columnwidth}{!}{  
\begin{tabular}{l|ccc|cc}

\Xhline{3\arrayrulewidth}

Method             & FID ($\downarrow$)  & LPIPS ($\downarrow$) & SSIM ($\uparrow$)   & BRISQUE\cite{mittal2011blind} ($\downarrow$)  & TRES ($\uparrow$)  \\ \hline
AutoAttack      & 26.5      & 0.72    &  0.21     & 34.4      & 69.8   \\ \hline
U-BigGAN & 25.4   & 0.50  & 0.32 & 19.4      & 80.3   \\
AdvDiffuser & 26.8   & 0.21  & 0.84 & 18.9      & 75.6   \\
DiffAttack & 20.5   & 0.15  & 0.75 & 22.6      & 67.8   \\
AdvDiff  & \textbf{16.2}    & \textbf{0.03} & \textbf{0.96}  & \textbf{18.1}      & \textbf{82.1}   \\
AdvDiff-Untargeted  & 22.8    & 0.14 & 0.85 & 16.2      & 76.8   \\
\Xhline{3\arrayrulewidth}  
\end{tabular}
}
\end{center}
   

\label{tab:more}
\end{table}



We find the IS score is heavily affected by the transferability of adversarial examples due to the calculation method. Therefore, we further compare the image quality of adversarial examples by commonly used metrics in Table \ref{tab:more}. The results show that AdvDiff (average 5 out of 5) and AdvDiff-Untargeted (average 4 out of 5) outperform existing adversarial attack methods using diffusion models. The perturbation-based adversarial attacks, i.e., AutoAttack, achieve much worse image quality compared with UAEs.


\subsection{UAEs against Defenses and Black-box Models}

Current defenses assume the adversarial examples are based on perturbations over data from the training dataset, i.e., $x_{adv} = x + \nabla\mathcal{L}, x \in D$. However, UAEs are synthetic data generated by the generative model. Because of different data sources, current defenses are hard to defend UAEs, which brings severe security concerns to deep learning applications. The proposed AdvDiff achieves an average of 36.8\% ASR against various defenses, while AutoAttack only achieves 30.7\% ASR with significantly lower image quality. We also test the attack transferability of AdvDiff and the results show that the untargeted version of AdvDiff achieves the best performance against black-box models. Experiment results are given in Table \ref{tab:db}.


   
\begin{table}[t]

   
\caption{\textbf{The attack success rates (\%) of ResNet50 examples for transfer attack and attack against defenses on the ImagetNet dataset.}}


\begin{center}
\resizebox{0.85\columnwidth}{!}{  
\begin{tabular}{l|cccc}

\Xhline{3\arrayrulewidth}
Method        & ResNet-152 \cite{he2016deep} & Inception v3 \cite{szegedy2016rethinking} &  ViT-B \cite{dosovitskiy2020image} &  BEiT \cite{bao2021beit}  \\ \hline
AutoAttack        & 32.5             & 38.6 & 9.3      & 45.3          \\ \hline
U-BigGAN          & 30.8             & 35.3                & 30.1      & 69.4        \\
AdvDiffuser         & 18.3             & 20.0  & 18.5      & 79.4     \\ 
DiffAttack         & 21.1             & \textbf{43.9}  & 17.4      & 78.0    \\ 
AdvDiff          & 20.5              & 14.9                &  17.8       & 78.8    \\ 
AdvDiff-Untargeted          & \textbf{52.0}              &  42.7                & \textbf{36.0}     & \textbf{81.5}       \\ \Xhline{3\arrayrulewidth}
Method        & Adv-Inception \cite{madry2017towards} & AdvProp \cite{xie2020adversarial} & DiffPure \cite{nie2022DiffPure} & HGD \cite{liao2018defense} \\ \hline
AutoAttack       & 14.6               & 69.6  & 22.2      & 20.5       \\ \hline
U-BigGAN          & 40.6              & 75.2                & 40.1 &  22.6         \\
AdvDiffuser          & 24.4              & 84.0                & 30.5 & 10.8             \\
DiffAttack          & 30.9              & 85.1               & 30.6 & 20.5             \\
AdvDiff          & 19.4            & 89.7              & 41.6 & 17.8        \\ 
AdvDiff-Untargeted          & \textbf{60.1}              & \textbf{95.3}              & \textbf{75.2} & \textbf{53.8}        \\ \Xhline{3\arrayrulewidth}
Method        & R\&P \cite{xie2017mitigating} & RS \cite{cohen2019certified} & NRP \cite{naseer2020self} & Bit-Red \cite{xu2017feature} \\ \hline
AutoAttack       & 20.6               & 38.9  & 39.4     &   19.8     \\ \hline
U-BigGAN          & 14.2              & 34.5                & 30.9     & 13.1     \\
AdvDiffuser          & 15.4              & 38.4                & 40.5  & 11.4             \\
DiffAttack          & 23.7              & 40.8                & 38.5  &  20.1           \\
AdvDiff          & 17.4             & 47.6              & 45.2 & 15.8        \\
AdvDiff-Untargeted          & \textbf{56.8}             & \textbf{82.8}              & \textbf{74.2} & \textbf{52.6}        \\ \Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}
\label{tab:db}



\end{table}

\subsection{Better Adversarial Diffusion Sampling}

   
We present detailed comparisons with DiffAttack and AdvDiffuser. The results show that the proposed adversarial guidance achieves significantly higher generation quality than PGD-based adversarial guidance. With PGD gradient guidance, the diffusion model generates images with a similar Recall score but a much lower Precision score, which indicates that the PGD gradient influences the benign generation process and causes the generation of low-quality images.
The result proves that the adversarial guidance of diffusion models should be carefully designed without affecting the benign sampling process. Meanwhile, the generation speed of AdvDiff is the best among the existing diffusion attack methods.  Note that AdvDiff (36.8\%) sightly outperforms AdvDiffuser (32.0\%) and DiffAttack (36.2\%) against defenses. 
 However, previous attacks achieve slightly better transfer attack performance than the original AdvDiff. The reason could be the gradient of the cross-entropy loss is shared among nearly all the deep learning models and is better at attack transferability against these models. Nevertheless, the untargeted version of AdvDiff achieves overwhelmingly better performance, which further demonstrates the effectiveness of the proposed adversarial sampling. But the generation quality is affected, we leave a better design in the future work.


% Figure environment removed
   
\subsection{Ablation Study}



We discuss the impact of the parameters of AdvDiff in the subsection. Note that our proposed method does not require re-training the conditional diffusion models. The ablation study is performed only on the sampling process.

\noindent \textbf{Adversarial guidance scale $s$ and $a$}. The magnitudes of $s$ and $a$ greatly affect the ASR of AdvDiff, as shown in Figure \ref{fig:abl}. Noted that we witness the generation of unrealistic images when setting the adversarial guidance extremely large. See the Appendix for detailed discussions.

\noindent \textbf{Noise sampling guidance steps $N$}.  Like the iteration times of GAN-based unrestricted adversarial attacks, larger steps $N$ can effectively increase the attack performance against an accurate classifier, as shown in Figure \ref{fig:abl}. However, it can affect the initial noise distribution and hence decreases the generation quality. During experiments, we observe that adversarial guidance is already capable of generating adversarial examples with high ASR. Thus, we can set a small noise sampling guidance step $N$ for better sample quality.

\noindent \textbf{Adversarial guidance timestep $t^*$}. The reverse diffusion process gradually denoises the input noise. Therefore we generally get noisy images at most timesteps. Because the target classifier is not able to classify the noisy input, the adversarial guidance is not effective in the early reverse diffusion process. Figure \ref{fig:abl} shows our results, and we can improve the performance of adversarial guidance by training a separate classifier, which we leave for future work.


\section{Conclusion}

In this work, we propose a new method called AdvDiff, which can conduct unrestricted adversarial attacks using any pre-trained conditional diffusion model. We propose two novel adversarial guidance techniques in AdvDiff that lead diffusion models to obtain high-quality, realistic adversarial examples without disrupting the diffusion process. Experiments show that our AdvDiff vastly outperforms GAN-based and diffusion-based attacks in terms of attack success rate and image generation quality, especially in the ImageNet dataset. AdvDiff indicates that diffusion models have demonstrated effectiveness in adversarial attacks, and highlights the need for further research to enhance AI model robustness against unrestricted attacks.

\section*{Acknowledgements}
This work was supported in part by HK RGC GRF under Grant PolyU 15201323.

\bibliographystyle{splncs04}
\bibliography{main}
\end{document}
