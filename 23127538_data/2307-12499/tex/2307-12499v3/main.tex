% CVPR 2024 Paper Template; see https://github.com/cvpr-org/author-kit

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
 \usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[review]{cvpr}      % To produce the REVIEW version
% \usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, 
% e.g. with the file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete *.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}

%\PassOptionsToPackage{numbers, compress}{natbib}
%\usepackage{neurips_2023}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{9347} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2024}

\usepackage[pagebackref,breaklinks,colorlinks,citecolor=cvprblue]{hyperref}
\usepackage{algorithm,algorithmicx,algpseudocode}

\usepackage{multirow}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{makecell}

\begin{document}

%%%%%%%%% TITLE
\title{AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models}

\author{Xuelong Dai$^{1}$\thanks{$\dagger$ Corresponding author}, Kaisheng Liang$^{1}$,  Bin Xiao$^{1}$\\
$^{1}$The Hong Kong Polytechnic University \\
{\tt\small\{xuelong.dai,kaisheng.liang\}@connect.polyu.hk};
{\tt\small csbxiao@comp.polyu.edu.hk}
}

\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvDiff is effective to generate unrestricted adversarial examples, which outperforms GAN-based methods in terms of attack performance and generation quality.
    %Our work shows that diffusion models also beat GAN in conducting unrestricted adversarial attacks.

\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

While the deep learning (DL) community continues to explore the wide range of applications of DL models, researchers \cite{szegedy2013intriguing} have demonstrated that these models are highly susceptible to deception by adversarial examples. Adversarial examples are generated by adding perturbations to clean data. The perturbed examples can deceive DL classifiers with high confidence while remaining imperceptible to humans. Many strong attack methods \cite{madry2017towards,dong2018boosting,carlini2017towards,croce2020reliable} are proposed and investigated to improve the robustness of DL models.

In contrast to existing perturbation-based adversarial attacks, Song et al. \cite{song2018constructing}
found that using a well-trained generative adversarial network with an auxiliary classifier (AC-GAN) \cite{odena2017conditional} can directly generate new adversarial examples without perturbing the clean data. These newly generated examples are considered \textbf{unrestricted} as they are obtained by optimizing input noise vectors without any norm restrictions. Compared to traditional adversarial examples, unrestricted adversarial examples \cite{deb2020advfaces,qiu2020semanticadv} are more aggressive against current adversarial defenses. A malicious adversary can also generate an unlimited number of unrestricted adversarial examples using a trained GAN.

 Diffusion models \cite{ho2020denoising} are likelihood-based generative models proposed recently, which emerged as a strong competitor to GANs. 
 %In image synthesis task, diffusion models already beats GAN in related researches \cite{dhariwal2021diffusion,rombach2022high,kim2022diffusionclip}. 
 Diffusion models have outperformed GANs  for image synthesis tasks \cite{dhariwal2021diffusion,rombach2022high,kim2022diffusionclip}.
 Compared with GAN models, diffusion models are more stable during training and provide better distribution coverage. Diffusion models contain two processes: a forward diffusion process and a reverse generation process. The forward diffusion process gradually adds Gaussian noise to the data and eventually transforms it into noise. The reverse generation process aims to recover the data from the noise by a denoising-like technique. A well-trained diffusion model is capable of generating images with random noise input. Similar to GAN models, diffusion models can achieve conditional image generation with given label information. 

%Due to issues with interpretability and unstable training, GAN-based unrestricted adversarial attacks often exhibit poor  performance on high-quality datasets, particularly in terms of visual quality such as backgrounds and textures. 
 GAN-based unrestricted adversarial attacks often exhibit poor 
 performance on high-quality datasets, particularly in terms of visual quality, because of GAN's weak 
 interpretability and unstable training. These attacks tend to generate low-quality adversarial examples compared to benign GAN examples. Therefore, these attacks are not imperceptible among GAN synthetic data. 
 Diffusion models, however, offer state-of-the-art generation performance \cite{dhariwal2021diffusion} on challenging datasets like LSUN \cite{yu2015lsun} and ImageNet \cite{deng2009imagenet}. The conditional diffusion models can generate images based on specific conditions by sampling from a perturbed conditional Gaussian noise, which can be carefully modified with adversarial objectives. These properties make diffusion models more suitable for conducting unrestricted adversarial attacks.

In this paper, we propose a novel unrestricted adversarial attack method called AdvDiff that utilizes diffusion models for adversarial examples generation, as shown in Figure \ref{fig:pip}. Specifically, AdvDiff uses a trained conditional diffusion model to conduct adversarial attacks with two new adversarial guidance techniques. 1) During the reverse generation process, we gradually add \emph{adversarial guidance} by increasing the likelihood of the target attack label. 2) We perform the reverse generation process multiple times, adding adversarial prior knowledge to the initial noise with the \emph{noise sampling guidance}.
%We repeat the reverse generation process by adding an adversarial prior to the initial noise with the noise sampling guidance.
%These approaches  balance the trade-off between attack performance and generation quality. 
Our theoretical analysis indicates that these adversarial guidance techniques can effectively craft adversarial examples by the reverse generation process with adversarial conditional sampling. Furthermore, the sampling of AdvDiff benefits from stable and high sample quality of the diffusion models sampling, which leads to the generation of realistic unrestricted adversarial examples.
Through extensive experiments conducted on two datasets, i.e., the high-quality dataset ImageNet, and the small, robust dataset MNIST, we have observed a significant improvement in the attack performance using  AdvDiff with diffusion models. 
These results prove that our proposed AdvDiff is more effective than previous GAN-based methods in conducting unrestricted adversarial attacks to generate high-fidelity and diverse examples without decreasing the generation quality.
%Extensive experiments on two datasets (i.e., a large high-quality dataset ImageNet and a small robust dataset MNIST) demonstrate the remarkable increment in the attack performance, which proves that diffusion models are more effective at conducting unrestricted adversarial attacks with high fidelity and diversity samples compared with GAN-based methods.



% Figure environment removed
   
Our contributions can be summarized as follows:

\begin{itemize}
\setlength{\itemsep}{0pt}

\setlength{\parsep}{0pt}

\setlength{\parskip}{0pt}
   \item We propose AdvDiff, the new form unrestricted adversarial attack method that utilizes the reverse generation process of diffusion models to generate realistic adversarial examples.
   \item We design two new effective adversarial guidance techniques to the sampling process that incorporate adversarial objectives to the diffusion model without re-training the model. Theoretical analysis reveals that AdvDiff can generate unrestricted adversarial examples while preserving the high-quality and stable sampling of the conditional diffusion models.
   \item  We perform extensive experiments to demonstrate that AdvDiff achieves an overwhelmingly better performance than GAN models on unrestricted adversarial example generation.

\end{itemize}

\section{Preliminaries}
In this section, we introduce the diffusion model and the classifier guidance for constructing our adversarial diffusion model.

\subsection{Diffusion Model}
The Denoising Diffusion Probabilistic Model (DDPM) \cite{ho2020denoising} is utilized to learn the Gaussian
transitions from $p(x_T)=\mathcal{N}(x_T;0,\mathbf{I})$ to recover the data $x_0 \sim q(x_0)$ with a Markov chain. We call this denoising-like process the \textit{reverse generation process}. Following the pre-defined $T$ time steps, DDPM obtains a sequence of noisy data $\{x_1, \dots, x_{N-1}\}$ and finally recovers the data $x_0$. It is defined as:
\begin{equation}
    p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1}:\mu_{\theta}(x_t,t),\Sigma_{\theta}(x_t,t)) 
\end{equation}

Conversely, the \textit{forward diffusion process} leverages a fixed Markov chain to iteratively add Gaussian noise to the sampled data $x_0 \sim q(x_0)$ according to the scheduling function $\beta_1, \dots, \beta_N$. Specifically, it is defined as:
\begin{equation}
    q(x_t|x_{t-1}):=\mathcal{N}(x_t:\sqrt[]{1-\beta_t}x_{t-1},\beta_t\textbf{I} ) 
\end{equation}

Training of the DDPM requires accurate inference of the mean value $\mu_{\theta}$ with a deep learning model in the reverse generation process. The objective is to learn the variational lower-bound (VLB) on $\log p_\theta(x_0)$. In order to complete the training objective, we train the model $\varepsilon_\theta$ to predict the added Gaussian noise in the forward diffusion process. The standard mean-squared error loss is adopted:
\begin{equation}
    \mathcal{L}_{DDPM}:= E_{t\sim [1,T],\epsilon \sim\mathcal{N}(0,\textbf{I})}\left \| \epsilon-\epsilon_\theta (x_t,t) \right \|^2 
\end{equation}

Song et al. \cite{song2020denoising} proposed DDIM, which provides an alternative noising process without restricting to a Markov chain. DDIM can achieve a much faster sampling than DDPM with the same training as DDPM. We perform experiments on both DDPM and DDIM to demonstrate the usability of our AdvDiff. DDPM is adopted to introduce our method for simplicity.

\subsection{Classifier-Guided Guidance}
Dhariwal et al. \cite{dhariwal2021diffusion} achieved conditional diffusion sampling by adopting a trained classifier. The conditional information is injected into the diffusion model by modifying the mean value $\mu_{\theta}(x_t,t)$ of the samples according to the gradient of the prediction of the target class $y$ by the trained classifier. They adopted log probability to calculate the gradient, and the mean value is given by:
\begin{equation}
    \hat{\mu}_{\theta}(x_t,t) = \mu_{\theta}(x_t,t) + s\cdot \Sigma_\theta (x_t|y)\nabla_{x_t}\log p_{\phi}(y|x_t) 
\end{equation}
where $s$ is the guidance scale.

\subsection{Classifier-Free Guidance}
Ho et al. \cite{ho2022classifier} recently proposed a new conditional diffusion model using classifier-free guidance that injects class information without adopting an additional classifier. The classifier-free guidance utilizes a conditional diffusion model $p_{\theta}(x|y)$ for image synthesis with given labels. For effective training, they jointly train the unconditional diffusion model $p_{\theta}(x|\emptyset)$ and the conditional diffusion model $p_{\theta}(x|y)$, where the unconditional diffusion model is simply replacing the label information with $\emptyset$. Sampling is performed by pushing the model towards the latent space of  $p_{\theta}(x|y)$ and away from $p_{\theta}(x|\emptyset)$:
\begin{equation}
    \hat{\epsilon}_{\theta }(x_t|y)= {\epsilon}_{\theta }(x_t|\emptyset) + w\cdot({\epsilon}_{\theta }(x_t|y)-{\epsilon}_{\theta }(x_t|\emptyset))
\end{equation}
where $w$ is the weight parameter for class guidance and $\emptyset$ is the empty set. 

The idea of classifier-free guidance is inspired by the gradient of an implicit classifier $p^i(y|x)\propto p(x|y)/p(x)$, the gradient of the classifier would be:
\begin{align}
\nabla_x logp^i(y|x)&\propto  \nabla_x logp(x|y)-\nabla_x logp(x) \notag \\
&\propto {\epsilon}_{\theta }(x_t|y)-{\epsilon}_{\theta }(x_t|\emptyset)
\end{align}

The classifier-free guidance has a good capability of generating high-quality conditional images, which is critical for performing adversarial attacks. The generation of these images does not rely on a classification model and thus can better fit the conditional distribution of the data.

\section{Adversarial Diffusion Sampling}

\subsection{Rethinking Unrestricted Adversarial Examples}

Song et al. \cite{song2018constructing} presented a new form of adversarial examples called unrestricted adversarial examples (UAEs). These adversarial examples are not generated by adding perturbations over the clean data but are directly generated by any generative model. UAEs can be viewed as false negative errors in the classification tasks, and they can also bring server security problems to deep learning models. These generative-based UAEs can be formulated as:

\begin{equation}
   A_{\text{UAE}} \triangleq \{x \in \mathcal{G}(z_{\text{adv}},y)|y \neq f(x)\}
\end{equation}
where $f(\cdot)$ is the target model for unrestricted adversarial attacks. The unrestricted adversarial attacks aim to generate UAEs that fool the target model while still can be visually perceived as the image from ground truth label $y$.

Previous UAE works adopt GAN models for the generation of UAEs, and these works perturb the GAN latents by maximizing the cross-entropy loss of the target model, i.e., $\max_{z_{\text{adv}}}\mathcal{L}(f(\mathcal{G}(z_{\text{adv}},y)),y)$. Ideally, the generated UAEs should guarantee similar generation quality to the samples crafted by standard $z$ because successful adversarial examples should be imperceptible to humans. In other words, UAEs should not be identified among the samples with adversarial latents and standard latents. 

However, due to GAN's poor interpretability, there's no theoretical support on $z_{adv}$ that can craft UAEs with normally trained GANs. The generator of GAN is not trained with $z + \epsilon\nabla\mathcal{L}$ but only $z \sim \mathcal{N}(0, \textbf{I})$. Therefore, GAN-based UAEs encounter a significant decrease in generation quality because samples with $z_{adv}$ are not well-trained compared with samples with $z \sim \mathcal{N}(0, \textbf{I})$. Moreover, the GAN latents are sampled from low dimensional latent spaces. Therefore, GANs are extremely sensitive to the latent $z$ \cite{shen2020interpreting,liang2022exploring}. If we inject gradients of the classification results into GAN latents, GAN-based methods are more likely to generate flipped-label UAEs (images corresponding to the targeted attack label $y_a$ instead of the conditional generation label $y$) and distorted UAEs. However, these generation issues are hard to address only by attack success rate (ASR). In other words, even with a high ASR, some of the successful UAEs with GAN-based methods should be identified as failure cases for poor visual quality. However, such cases can not be reflected by ASR but can be evaluated by generation quality. All these problems may indicate that GAN models are not suitable for generative-based adversarial attacks.


Diffusion models have shown better performance on image generation than GAN models \cite{dhariwal2021diffusion}. They are log-likelihood models with interpretable generation processes. In this paper, we aim to generate UAEs by injecting the adversarial loss with theoretical proof and without sabotaging the benign generation process, where we increase the conditional likelihood on the target attack label by following the diffusion process. The perturbations are gradually injected with the backward generation process of the diffusion model by the same sample procedure. As shown in Figure \ref{fig:fn}, the diffusion model can sample images from the conditional distribution $p(x|y)$. The samples from $p(x|y,f(x)\ne y)$ are the adversarial examples that are misclassified by $f(\cdot)$. These examples also follow the data distribution $p(x|y)$ but on the other side of the label $y$ 's decision boundary of $f(\cdot)$. Moreover, the diffusion model's generation process takes multiple sampling steps. Thus, we don't need one strong perturbation to the latent like GAN-based methods. The AdvDiff perturbations at each step are unnoticeable, and perturbations are added to the high dimensional sampled data rather than low dimensional latents. Therefore, AdvDiff with diffusion models can preserve the generation quality and barely generates flipped-label or distorted UAEs. 




% Figure environment removed


%The demonising-like generation process of the diffusion model is potentially more resistant to current defense methods. 


\subsection{General Adversarial Diffusion Sampling}

There are several existing adversarial attack methods \cite{chen2023advdiffuser,chen2023diffusion,chen2023content} that adopt diffusion models to generate adversarial examples. However, these methods still adopt PGD or I-FGSM gradients to perturb the diffusion process for constructing adversarial examples. As discussed earlier, the generation process of diffusion models is a specially designed sampling process from given distributions. Such adversarial gradients change the original generation process and can harm the generation quality of the diffusion model. Additionally, these methods fail to give a comprehensive discussion of the adversarial guidance with theoretical analysis. Therefore, we aim to design a \textbf{general} and \textbf{interpretable} method to generate adversarial examples using diffusion models \textbf{without} affecting the benign diffusion process.


\subsection{Adversarial Guidance}

Inspired by Dhariwal's work \cite{dhariwal2021diffusion} that achieves the conditional image generation by classifier gradient guidance $\nabla_{x_t}\log p_{\phi}(y|x_t)$, we generate our UAEs with adversarial gradient guidance over the reverse generation process. Our attack aims at utilizing a conditional diffusion model $\epsilon_{\theta}(x_t,y)$ to generate $x_0$ fits the ground truth label $y$ while deceiving the target classifier with $p_{f}(x_0) \ne y$. These generated samples are the false negative results in $p_{f}$'s classification results. 

Normally, we will obtain the images with label $y$ by following the standard reverse generation process with classifier-free guidance:
\begin{equation}
{x}_{t-1} = \mu({x}_t,y) + \sigma_t{\varepsilon}
\label{eq:cfg}
\end{equation}
where $\mu({x}_t,y)$ is the conditional mean value and $\varepsilon$ is sampled from ${\varepsilon}\sim \mathcal{N}(0,\textbf{I})$.

Sampling by Equation \ref{eq:cfg}, we obtain the samples with the generation process $p(x_{t-1}|x_t,y)$. Following the above-mentioned definition of UAEs, we can get our adversarial examples by adding adversarial guidance to the standard reverse process, which is performing another sampling with the adversarial generation process $p(x^*_{t-1}|x_{t-1},f(x)\ne y)$. We find that specifying a target label for the adversarial generation process is more effective during experiments. Suggest the target label $y_{a}$ is the target for the adversarial attacks, the adversarial example is sampled by the following steps:
\begin{equation}
{x}^*_{t-1} = x_{t-1} +\sigma_t^2 s\nabla_{{x}_{t-1}} \log p_{f}({y_{a}}|{x}_{t-1})
\label{eq:sample}
\end{equation}
where $s$ is the adversarial guidance scale. The derivation of Equation \ref{eq:sample} is given in Appendix A. Intuitively, the adversarial guidance encourages the generation of samples with a higher likelihood of the target label.

In practice, we utilize the classifier-free guidance to train a conditional diffusion model $\epsilon_{\theta}(\cdot)$ as our basic generation model.

\subsection{Noise Sampling Guidance}

We can improve the reverse process by adding an adversarial label prior to the noise data $x_T$. The UAEs are a subset of the dataset labeled with $y$. They can be viewed as the conditional probability distribution with $p(x|y, f(x)=y_{a})$ during sampling, and $y_a$ is the target label for the adversarial attack. Therefore, we can add the adversarial label prior to $x_t$ with Bayes' theorem:
\begin{align}
    p({x}_{T}|y_{a}) =& \frac{p(y_{a}|{x}_{T})p({x}_{T})}{p(y_{a})} = \frac{p(y_{a}|x_T,x_0)p(x_T|x_0)}{p(y_{a}|x_0)} \notag \\=&p(x_T|x_0)e^{\log p(y_{a}|x_T)- \log p(y_{a}|x_0) }
    \label{eq:noise}
\end{align}

We can infer the $x_t$ with the adversarial prior by Equation \ref{eq:noise}, i.e.,
\begin{equation}
    {x}_{T} = (\mu({x}_0,y) + \sigma_t{\varepsilon}) +\bar{\sigma}_T^2 a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)
    \label{eq:sample_noise}
\end{equation}
where $a$ is the noise sampling guidance scale. See Appendix B for detailed proof.

Equation \ref{eq:sample_noise} is similar to Equation \ref{eq:sample} as they both add adversarial guidance to the reverse generation process. However, the noise sampling guidance is added to $x_T$ according to the final classification gradient $\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)$, which provides a strong adversarial guidance signal directly to the initial input of the generative model. The gradient of Equation \ref{eq:sample_noise} is effective as it reflects the eventual classification result of the target classifier.

\subsection{Training-Free Adversarial Attack}

The proposed adversarial attack does not require additional modification on the training of the diffusion model. The adversarial examples are sampled by using Algorithm \ref{alg:sampling} over the trained classifier-free diffusion model $\epsilon_{\theta}(\cdot)$.

\begin{algorithm}[tb]
  \caption{DDPM Adversarial Diffusion Sampling} 
  \label{alg:sampling}
  \begin{algorithmic}[1]
    \Require $y_a$: target label for adversarial attack
    \Require $y$: ground truth class label
    \Require $s,a$: adversarial guidance scale
    \Require $w$: classification guidance scale
    \Require $N$: noise sampling guidance steps
    \Require $T$: reverse generation process timestep

    \State $x_{T} \sim \mathcal{N}(0, \textbf{I})$
    \State $x_{adv} = \varnothing $
    \For{$i = 1\ldots N$}
    \For{$t=T, \dotsc, 1$}

      \State $\tilde{\epsilon}_t = (1+w)\epsilon_\theta(x_{t}, y) - w\epsilon_{\theta}(x_{t})$ 
      %\Statex  $\triangleright$ DDPM Sampling (can be replaced with DDIM)
      \State Classifier-free sampling $x_{t-1}$ with $\tilde{\epsilon}_t$.
      %Calculate $\mu({x}_t,y) + \sigma_t{\varepsilon}$ referring to [23]
      \State Input $x_{t-1}$ to target model and get the gradient $\log p_{f}({y_{a}}|{x}_{t-1}))$
      \State ${x}^*_{t-1} = x_{t-1} +\sigma_t^2 s\nabla_{{x}_{t-1}} \log p_{f}({y_{a}}|{x}_{t-1})$
    \EndFor
      \State Obtain classification result from $f(x_0)$
      \State Compute the gradient with $\log p_{f}({y_{a}}|{x}_0)$
      \State Update $x_{T}$ by $x_T = x_T + \bar{\sigma}_T^2 a\nabla_{{x}_0} \log p_{f}({y_{a}}|{x}_0)$
      \State  $x_{adv} \gets x_{0}$ if $f(x_0)=y_a$
    
    
    \EndFor
    \State \textbf{return} $x_{adv}$
  \end{algorithmic}
\end{algorithm}

The adversarial diffusion sampling starts with a random noisy input $x_T$. After the classifier-free guidance sampling \cite{ho2022classifier} at step 6, we utilize the adversarial guidance to incorporate the adversarial gradient into the sampling process. After the sampling, we can obtain the $x_0$ with the adversarial guidance. To achieve more success and stable adversarial sampling, we can integrate the gradient of the target model's classification of $x_0$ to the beginning noisy input $x_T$ for $N$ iterations. Finally, we get the adversarial examples that fool the target model with the label $y_a$. We give the AdvDiff algorithm on DDIM in the Appendix.


 \section{Related Work}

Since Szegedy et al. \cite{szegedy2013intriguing} had proved that DL models are extremely vulnerable to adversarial attacks, researchers have been digging into improving the model's adversarial robustness by proposing stronger adversarial attack methods and their counter-measurements. 

\textbf{Perturbation-based adversarial examples with generative models}: Most related works performed adversarial attacks by perturbing a subset of clean data to fool the target classifier. These attacks \cite{carlini2017towards,madry2017towards,kurakin2018adversarial} attempted to generate better perturbations with higher attack success rates and smaller perturbations. With the emergence of generative models, end-to-end adversarial attacks \cite{baluja2018learning,xiao2018generating,poursaeed2018generative} have greatly improved the generation efficiency by pre-training the generative module. These methods integrate the advertorial loss into the training of generative models and generate adversarial examples by trained generators with clean data.

\textbf{Unrestricted adversarial examples with generative models}: Perturbation-based adversarial examples require insignificant norm distance to the given clean data in order to guarantee the indistinguishability, which only covers a small fraction of all possible adversarial examples \cite{song2018constructing}. To remove such restrictions, Song et al. \cite{song2018constructing} proposed an unrestricted adversarial attack method that searches over the latent space of the input noise vector with an adversarial loss function and a well-trained AC-GAN \cite{odena2017conditional}. Inspired by Song's work \cite{song2018constructing}, recent works \cite{poursaeed2019fine,xiang2022egm} made improvements over the generation quality and generation efficiency of UAEs. However, the performance of existing approaches suffers from the unstable training of GAN models as well as the lack of theoretical support of GAN-based UAEs. Therefore, we provide an effective and theoretically analyzed solution with the diffusion model in this paper.

%\subsection{Conditional Diffusion Model for Image Generation}

%Diffusion models have shown great generation quality and diversity in the image synthesis task since Ho et al.\cite{ho2020denoising} proposed a probabilistic diffusion model for image generation that greatly improved the performance of diffusion models.  Diffusion models for conditional image generation are extensively developed for more usable and flexible image synthesis. Dhariwal \& Nichol \cite{dhariwal2021diffusion} proposed a conditional diffusion model that adopted classifier-guidance for incorporating label information into the diffusion model. They separately trained an additional classifier and utilized the gradient of the classifier for conditional image generation. Jonathan Ho \& Tim Salimans \cite{ho2022classifier} performed the conditional guidance without an extra classifier to a diffusion model. They trained a conditional diffusion model together with a standard diffusion model. During sampling, they adopted the combination of these two models for image generation. Their idea is motivated by an implicit classifier with the Bayes rule. Followed by \cite{dhariwal2021diffusion,ho2022classifier}'s works, many research \cite{rombach2022high,nichol2021glide,lugmayr2022repaint,gafni2022make} have been proposed to achieve state-of-the-art performance on image generation, image inpainting, and text-to-image generation tasks. Despite utilizing diffusion models for image generation has been widely discussed, none of these works have discovered the adversarial examples generation method with the diffusion model. Also, it is a new challenge to defend against the adversarial examples generated by the diffusion model.

\section{Experiments}

\textbf{Datasets and target models.} We use two datasets for major evaluation: MNIST \cite{deng2012mnist} and ImageNet \cite{deng2009imagenet}. MNIST is a 10-classes dataset consisting of handwritten numbers from 0 to 9. We adopt the MNIST dataset to evaluate our method for low-quality robust image generation. ImageNet is a large visual database with 1000 object classes and is used for the high-quality generation task. For target classifiers, we adopt simple LeNet5 \cite{lecun1998gradient}, and ResNet18 \cite{he2016deep} for the MNIST dataset, and the widely-used ResNet50 \cite{he2016deep} and WideResNet50-2 \cite{zagoruyko2016wide} for the ImageNet dataset.

\textbf{Comparisons.} It is not applicable to give a clear comparison between perturbation-based attacks and unrestricted attacks because perturbation-based attacks have the corresponding ground truth while unrestricted attacks do not. We mainly compare our method with the GAN-based unrestricted adversarial attack U-GAN \cite{song2018constructing} and give the discussion with the PGD \cite{madry2017towards}, BIM \cite{dong2018boosting}, and C\&W \cite{carlini2017towards} perturbation-based attacks. For U-GAN, We adopt AC-GAN \cite{odena2017conditional} for the MNIST dataset, and SAGAN \cite{zhang2019self} and BigGAN \cite{brock2018large} for the ImageNet dataset, as AC-GAN has shown poor performance on ImageNet. Additionally, we use PGD gradient to replace the adversarial guidance in our method for a clear comparison, which is represented by ``AdvDiff-PGD''. 

\textbf{Implementation details.} Because our adversarial diffusion sampling does not require additional training to the original diffusion model, we use the pre-trained diffusion model in our experiment. We adopt DDPM \cite{ho2020denoising} with classifier-free guidance for the MNIST dataset and Latent Diffusion Model (LDM) \cite{rombach2022high} with DDIM sampler for the ImageNet dataset.  For MNIST dataset, we use $N=10$, $s=0.5$, and $a=1.0$, And $N=5$, $s=0.25$, and $a=0.5$ for ImageNet dataset. More details and experiments are given in the Appendix.

\textbf{Evaluation metrics.} We utilize the top-1 classification result to evaluate the Attack Success Rate (ASR) on different attack methods. As discussed earlier, GAN-based UAEs often encounter severe generation quality drops compared to benign GAN samples. Therefore, we give comparisons of generation performance on ImageNet to evaluate the attack performance of different UAEs in imperceptibly. The results are averaged with five runs. We use ResNet50 as the target model for default settings.

% Figure environment removed


   
   
% Figure environment removed
   
\subsection{Attack Performance}
\noindent \textbf{MNIST} We show the attack success rate against the normally trained model and adversarially trained model \cite{madry2017towards} in the MNIST dataset. All the selected adversarial attacks achieve over 90\% attack success rate against the normally trained model. The adversarially trained model can effectively defend against perturbation-based adversarial attacks for their noise-like perturbation generation patterns, as reported in Table \ref{tab:mnis}. However, the UAEs obviously perform better with their non-perturbed image generation. Despite the fact that the unrestricted attack can break through the adversarial defenses, the crafted adversarial examples should also be imperceptible to humans for a reasonably successful attack. The visualized adversarial examples in Figure \ref{fig:mnist} show that the perturbation-based adversarial attacks tend to blur the original images while U-GAN can generate mislabeled adversarial examples.
%Note that adversarial examples crafted by AdvDiff effortlessly balance the magnitude between the visual quality and the attack success rate with the adversarial guidance.

   
\begin{table}[ht]

\begin{center}

\caption{\textbf{The attack success rate on MNIST dataset.} The unrestricted adversarial examples effortlessly break through current state-of-the-art defense methods, the reason is that unrestricted adversarial attacks adopt a different threat model. }
   
\label{tab:mnis}
\resizebox{0.7\columnwidth}{!}{  
\begin{tabular}{c|cccc}


\Xhline{3\arrayrulewidth}
\multirow{3}{*}{Method} & \multicolumn{4}{c}{ASR(\%)}                                                                        \\ \cline{2-5} 
                        & \multicolumn{2}{c|}{LeNet5}                                & \multicolumn{2}{c}{ResNet18}          \\
                        & Clean        & \multicolumn{1}{c|}{PGD-AT}                 & Clean        & PGD-AT                 \\ \hline
PGD                     & 99.8         & \multicolumn{1}{c|}{25.6}                   & 99.3         & 20.8                   \\
BIM                     & 99.6         & \multicolumn{1}{c|}{34.6}                   & \textbf{100} & 31.5                   \\
C\&W                    & \textbf{100} & \multicolumn{1}{c|}{68.6}                   & \textbf{100} & 64.5                   \\ \hline
U-GAN                   & 88.5         & \multicolumn{1}{c|}{\textit{79.4}}          & 85.6         & \textit{75.1}          \\
AdvDiff              & 94.2         & \multicolumn{1}{c|}{\textit{\textbf{88.6}}} & 92.1         & \textit{\textbf{86.5}}\\


\Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}
\vspace{-0.2in}
\end{table}



\noindent \textbf{ImageNet} It is reported that deep learning models on ImageNet are extremely vulnerable to adversarial attacks. However, the state-of-the-art adversarial defense DiffPure \cite{nie2022DiffPure} and adversarial training \cite{madry2017towards} can still defend against around 50\% of the perturbation-based attacks, as reported in Table \ref{tab:imagenet}. Most of UAEs evade the current defenses, but the generation quality of U-GAN is relatively poor compared to our adversarial examples. This phenomenon also shows that the performance of UAEs is heavily affected by the generation quality of the generation model. The adversarial examples generated by AdvDiff are more aggressive and stealthy than U-GAN's.
   
\begin{table}[ht]
\caption{\textbf{The attack success rate on ImageNet dataset.} U-SAGAN and U-BigGAN represent the base GAN models for U-GAN are SAGAN and BigGAN, respectively.}

\vspace{-0.2in}
\begin{center}
\resizebox{1.0\columnwidth}{!}{ 

\begin{tabular}{c|cccccc}
\Xhline{3\arrayrulewidth}
\multirow{3}{*}{Method} & \multicolumn{6}{c}{ASR(\%)}                                                                                                                \\ \cline{2-7} 
                        & \multicolumn{3}{c|}{ResNet50}                                                & \multicolumn{3}{c}{WideResNet50-2}                          \\
                        & Clean & DiffPure               & \multicolumn{1}{l|}{PGD-AT}                 & Clean & DiffPure               & \multicolumn{1}{l}{PGD-AT} \\ \hline
PGD                     & 99.8  & 51.2                   & \multicolumn{1}{c|}{56.9}                   & 99.5  & 45.3                   & 50.6                       \\ \hline
U-SAGAN                 & 99.3  & \textit{95.9}          & \multicolumn{1}{c|}{\textit{98.5}}          & 98.9  & \textit{90.4}          & \textit{97.6}              \\
U-BigGAN                & 96.8  & 89.2                   & \multicolumn{1}{c|}{\textit{94.3}}          & 96.5  & 84.5                   & \textit{91.2}              \\
AdvDiff-PGD                 & \textbf{99.8} & 98.0 & \multicolumn{1}{c|}{99.2}  & \textbf{99.9}   & \textit{\textbf{98.5}} & 99.0      \\
AdvDiff                 & \textbf{99.8} & \textit{\textbf{98.1}} & \multicolumn{1}{c|}{\textit{\textbf{99.3}}} & \textbf{99.9}   & \textit{\textbf{98.5}} & \textit{\textbf{99.1}}      \\
\Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}


\label{tab:imagenet}

\vspace{-0.2in}
\end{table}




\subsection{Generation Quality: True ASR for UAEs}


We witness similar ASR with U-GAN and AdvDiff. However, imperceptibility is also critical for a successful unrestricted adversarial attack, so we adopt the evaluation metrics in \cite{dhariwal2021diffusion} to compare the generation quality with and without performing unrestricted attacks. Table \ref{tab:gvd} shows that the AdvDiff achieves an overwhelming better IS score and similar FID score on the large-scale ImageNet dataset, where FID \cite{heusel2017gans} and IS \cite{salimans2016improved} scores are commonly adopted for evaluating the quality of a generative model.


   
\begin{table}[ht]
\caption{\textbf{The generation performance on the ImagetNet dataset.}}

\vspace{-0.2in}
\begin{center}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{c|ccc|cc}

\Xhline{3\arrayrulewidth}

Method             & FID ($\downarrow$)  & sFID ($\downarrow$) & IS ($\uparrow$)   & Precision ($\uparrow$)  & Recall ($\uparrow$)  \\ \hline
SAGAN    & 41.9      & 50.2    & 26.7      & 0.50      & 0.51   \\
BigGAN   & 19.3      & 45.7    & 250.3     & 0.95      & 0.21   \\
LDM      & 12.3      & 25.4    & 385.5     & 0.94      & 0.73   \\ \hline
U-SAGAN  & 52.8/+26\% & 52.2/+4\%  & 12.5/-53\%  & 0.58      & 0.57   \\
U-BigGAN & 25.4/+31\%   & 52.1/+14\% & 129.4/-48\% & 0.81      & 0.35   \\
AdvDiff-PGD  & 19.8/+61\%    & 32.6/+28\% & 212.6/-45\%  & 0.73      & \textbf{0.78}   \\
AdvDiff  & \textbf{16.2}/+31\%    & \textbf{30.4}/+20\% & \textbf{343.8}/-10\%  & \textbf{0.90}      & 0.75   \\
\Xhline{3\arrayrulewidth}  
\end{tabular}
}
\end{center}
   

\vspace{-0.2in}
\label{tab:gvd}
\end{table}

\begin{table}[ht]
\caption{\textbf{The image quality on the ImagetNet dataset.}}

\vspace{-0.2in}
\begin{center}
\resizebox{1.0\columnwidth}{!}{  
\begin{tabular}{c|ccc|cc}

\Xhline{3\arrayrulewidth}

Method             & FID ($\downarrow$)  & LPIPS ($\downarrow$) & SSIM ($\uparrow$)   & BRISQUE\cite{mittal2011blind} ($\downarrow$)  & TRES ($\uparrow$)  \\ \hline
PGD      & 53.5      & 0.32    &  0.82     & 46.5      & 58.3   \\ \hline
U-BigGAN & 25.4   & 0.50  & 0.32 & 19.4      & 80.3   \\
AdvDiff-PGD  & 19.8    & 0.14 & 0.86 & 21.4      & 76.3   \\
AdvDiff  & \textbf{16.2}    & \textbf{0.03} & \textbf{0.96}  & \textbf{18.1}      & \textbf{82.1}   \\
\Xhline{3\arrayrulewidth}  
\end{tabular}
}
\end{center}
   
\vspace{-0.2in}
\label{tab:more}
\end{table}

% Figure environment removed

FID score is calculated by $d^2((m,C),(m_w,C_w))=||m-m_w||^2_2+Tr(C+C_w-2(CC_w)^{1/2})$, where $(m,C)$ measures the Gaussian distribution of the data in feature space. Because both U-GAN and AdvDiff do not modify the parameter of the generative model, we are expected to witness similar FID scores. However, IS score is $\text{exp}(\mathbb{E}_x\textbf{KL}(p(y|x)||p(y)))$. The conditional label distribution $p(y|x)$ should have low entropy as the adversarial objective aims to fool the classifier with high confidence on the target attack label, while the $p(y)$ should be a uniform distribution. As we receive both high ASR on the ImageNet dataset with U-GAN and AdvDIff, the reason for the significant drop in U-GAN is the change of distribution $p(y)$. In other words, the U-GAN's UAEs are not equally distributed among 1000 classes, and the most possible reason is the frequent generation of flipped-label UAEs and low-quality UAEs, which can also be inferred by the drop in Precision. Figure \ref{fig:gvd} illustrates this problem with some examples.

We further observed an obvious increase in Recall by U-GAN with BigGAN, which indicates that U-GAN's $z_{adv}$ fails to follow the normal distribution and often has a high variance, which covers a larger latent space and increases the Recall. However, most state-of-the-art GANs \cite{brock2018large,karras2020training} adopt truncation tricks to GAN latents to improve the generation quality, which reduces the diversity (Recall). Therefore, GAN-based UAEs perform even worse with better GAN networks. In conclusion, evaluating generation quality is critical to measure the attack performance of unrestricted adversarial attacks, especially the IS score. UAEs with much lower generation performance can be easily detected by humans.

   
%We give a discussion about the reasons why AdvDiff can outperform U-GAN for unrestricted adversarial attacks. The main optimization objective of U-GAN can be formulated as $\mathcal{L} \triangleq  -\log f(y_{\text{target}}\mid g(z, y_{\text{source}}))$, where $g$ is the generator of U-GAN. This objective guide the generator to find the adversarial examples that can fool the target classifier while correctly classified by the generator's classifier. There are three possible problems with the U-GAN objective. The first problem is that the classifier of $g$ is imperfect and underfit to guarantee high ASR, which is unable to guarantee a realistic image generation with the given source label. Secondly, there is no direct derivation on generating adversarial examples by minimizing U-GAN objective with input noise $z$ due to the bad interpretability of GAN models. This is also reflected by U-GAN demands an extremely large number of iterations over $z$ to obtain a successful adversarial example. The third possible problem is the unstable training of GAN leads to bad performance of high-quality image generations, which limits its performance on the large-scale real-world dataset.

   
%The AdvDiff adopts 1) conditional image generation by distributions with classifier-free guidance training; 2) theoretically interpretable adversarial sampling by increasing the conditional likelihood on the target attack label; 3) more stable and high-quality image generation than GAN-based methods thanks to the advantages of the diffusion models. Based on these properties, AdvDiff achieves a significantly better attack performance than U-GAN, especially on the ImageNet dataset.  Interestingly, U-GAN tends to blur the generated images, while AdvDiff adds more detailed texture to the generated samples. 

\subsection{UAEs against Defenses and Black-box Models}

Experiments on MNIST and ImageNet datasets show that adversarial defense methods like adversarial denoising  \cite{nie2022DiffPure} and adversarial training \cite{madry2017towards} fail to defend against UAEs. Current defenses assume the adversarial examples are based on perturbations over data from the training dataset, i.e., $x_{adv} = x + \nabla\mathcal{L}, x \in D$. However, UAEs are synthetic data generated by the generative model. Because of different data sources, current defenses are hard to defend UAEs, which brings severe security concerns to deep learning applications. We also test the attack transferability of AdvDiff and the results show that it achieves a moderate performance against black-box models. Experiment results are given in Table \ref{tab:db}.


   
\begin{table}[ht]

   
\caption{\textbf{The attack success rates (\%) of ResNet50 examples for transfer attack and attack against defenses on the ImagetNet dataset.}}

\vspace{-0.2in}
\begin{center}
\resizebox{0.95\columnwidth}{!}{  
\begin{tabular}{c|ccc}

\Xhline{3\arrayrulewidth}
Method        & ResNet-152 \citep{he2016deep} & Inception v3 \citep{szegedy2016rethinking} &  DenseNet-121 \citep{huang2017densely}  \\ \hline
PGD        & \textbf{31.2}              & 14.7  & 36.0                \\ \hline
U-BigGAN          & 18.4              & 22.1                & 16.8             \\
AdvDiff-PGD          & 26.3              &  \textbf{24.9}                & \textbf{38.6}           \\
AdvDiff          & 20.5              & 10.9                &  35.8          \\ \Xhline{3\arrayrulewidth}
Method        & MobileNet v2 \citep{sandler2018mobilenetv2} & PNASNet \citep{liu2018progressive} &  MNASNet \citep{tan2019mnasnet}\\ \hline
PGD       & \textbf{38.4}               & 11.4  & 35.6             \\ \hline
U-BigGAN          & 14.2              & 8.5                & 16.1          \\
AdvDiff-PGD          & 22.4              & 22.1                & 37.1             \\
AdvDiff          & 15.4             & \textbf{23.2}               & \textbf{38.9}        \\ \Xhline{3\arrayrulewidth}
Method        & Adv-Inception \cite{madry2017towards} & AdvProp \cite{xie2020adversarial} & DiffPure \cite{nie2022DiffPure} \\ \hline
PGD       & \textbf{38.4}               & 11.4  & 35.6             \\ \hline
U-BigGAN          & 14.2              & 8.5                & 16.1          \\
AdvDiff-PGD          & 25.1              & 89.0                & 98.0             \\
AdvDiff          & 16.1              & \textbf{89.3}              & \textbf{98.1}        \\

\Xhline{3\arrayrulewidth}
\end{tabular}
}
\end{center}
\label{tab:db}

\vspace{-0.2in}
\end{table}


\subsection{Better Adversarial Diffusion Sampling}


We present a detailed comparison with PGD-based adversarial guidance \cite{chen2023advdiffuser,chen2023diffusion,chen2023content} in Table \ref{tab:gvd} and \ref{tab:more}. The results show that the proposed adversarial guidance achieves significantly higher generation quality than PGD-based adversarial guidance. With PGD gradient guidance, the diffusion model generates images with a similar Recall score but a much lower Precision score, which indicates that the PGD gradient influences the benign reverse generation process and causes the generation of low-quality images.
The result proves that the adversarial guidance of diffusion models should be carefully designed without affecting the benign sampling process. However, PGD-based AdvDiff achieves slightly better transfer attack performance than the original AdvDiff. The reason could be the gradient of the cross-entropy loss is shared among nearly all the deep learning models and is better at attack transferability against these models. But the transferability of AdvDiff can be further improved by selecting different target labels, which is discussed in the Appendix.


\subsection{Ablation Study}

We discuss the impact of the parameters of AdvDiff in the subsection. Note that our proposed method does not require re-training the conditional diffusion models. The ablation study is performed only on the sampling process.

\noindent \textbf{Adversarial guidance scale $s$ and $a$}. The magnitudes of $s$ and $a$ greatly affect the ASR of AdvDiff, as shown in Figure \ref{fig:abl}. Noted that we witness the generation of unrealistic images when setting the adversarial guidance extremely large. See the Appendix for detailed discussions.

\noindent \textbf{Noise sampling guidance steps $N$}.  Like the iteration times of GAN-based unrestricted adversarial attacks, larger steps $N$ can effectively increase the attack performance against an accurate classifier, as shown in Figure \ref{fig:abl}. But it can affect the initial noise distribution and hence decreases the generation quality. During experiments, we observe that adversarial guidance is already capable of generating adversarial examples with high ASR. Thus, we can set a small noise sampling guidance step $N$ for better sample quality.

\noindent \textbf{Adversarial guidance timestep $t^*$}. The reverse diffusion process gradually denoises the input noise. Therefore we generally get noisy images at most timesteps. Because the target classifier is not able to classify the noisy input, the adversarial guidance is not effective in the early reverse diffusion process. Figure \ref{fig:abl} shows our results, and we can improve the performance of adversarial guidance by training a separate classifier, which we leave for future work.

\section{Discussions}

AdvDiff proves that diffusion models can serve as more effective generation models than GANs for conducting unrestricted adversarial attacks. GAN-based unrestricted adversarial attack methods regularly generate adversarial examples from the target class by optimizing the input noise with the adversarial objective, which is visually perceptible by humans. Our AdvDiff, however, is more feasible to generate unrecognizable adversarial examples from the ground truth label with carefully designed conditional sampling.
A potential advantage is that the adversarial examples generated by AdvDiff can be adopted to improve the robustness of deep learning classifiers for achieving both successful attacks and high-quality image synthesis.

\section{Conclusion}

In this work, we propose a new method called AdvDiff, which can conduct unrestricted adversarial attacks using any pre-trained conditional diffusion model. We propose two adversarial guidance techniques in AdvDiff that lead diffusion models to obtain high-quality, realistic adversarial examples without disrupting the diffusion process. Experiments show that our AdvDiff vastly outperforms GAN-based attacks in terms of attack success rate and image generation quality, especially in the ImageNet dataset. AdvDiff indicates that diffusion models have demonstrated effectiveness in adversarial attacks, and highlights the need for further research to enhance AI model robustness against unrestricted attacks.

{    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{main}
}

\end{document}