 \documentclass[11pt]{article}
\pagestyle{plain}

\usepackage[margin=0.97in]{geometry}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{bbm}
\usepackage{verbatim}
\usepackage{hyperref,color}
\usepackage[capitalize,nameinlink]{cleveref}
\usepackage[dvipsnames]{xcolor}
\hypersetup{
	colorlinks=true,
	pdfpagemode=UseNone,
    citecolor=OliveGreen,
    linkcolor=NavyBlue,
    urlcolor=Magenta,
	pdfstartview=FitW
}
\usepackage{appendix}
\crefname{appsec}{Appendix}{Appendices}
\usepackage{tikz}
\usepackage{array}
\usepackage[ruled,linesnumbered]{algorithm2e}
\SetKwComment{Comment}{/* }{ */}
\usepackage{multicol}

%\usepackage{times}
\usepackage{newtxtext}

\theoremstyle{plain}%https://www.overleaf.com/project/62449cc3c5280521f8c7ca5a
\newtheorem{thm}{Theorem}[section]
\newtheorem{theorem}{Theorem}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{proposition}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{coro}[thm]{Corollary}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{conjecture}[thm]{Conjecture}
\newtheorem{prob}[thm]{Problem}
\newtheorem{problem}[thm]{Problem}
\newtheorem{clm}[thm]{Claim}
\newtheorem{claim}[thm]{Claim}
\newtheorem{fact}[thm]{Fact}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{definition}[thm]{Definition}
\newtheorem{eg}[thm]{Example}
\newtheorem{example}[thm]{Example}
\newtheorem{obs}[thm]{Observation}
\newtheorem{observation}[thm]{Observation}
\newtheorem{ass}[thm]{Assumption}
\newtheorem{assumption}[thm]{Assumption}
\newtheorem*{ass*}{Assumption}
\newtheorem*{assumption*}{Assumption}
\newtheorem{exercise}[thm]{Exercise}

\theoremstyle{remark}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{remark}[thm]{Remark}

\crefname{lem}{Lemma}{Lemmas}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}
\crefname{theorem}{Theorem}{Theorems}
\crefname{defn}{Definition}{Definitions}
\crefname{definition}{Definition}{Definitions}
\crefname{fact}{Fact}{Facts}
\crefname{fact}{Fact}{Facts}
\crefname{clm}{Claim}{Claims}
\crefname{claim}{Claim}{Claims}
\crefname{prop}{Proposition}{Propositions}
\crefname{proposition}{Proposition}{Propositions}
\crefname{algocf}{Algorithm}{Algorithms}

\newcommand{\M}{\mathcal{M}}
\newcommand{\Pinning}{\mathcal{P}}
\newcommand{\up}[1]{\mathrm{P}_{#1}^{\uparrow}}
\newcommand{\down}[1]{\mathrm{P}_{#1}^{\downarrow}}
\newcommand{\expan}{\lambda}
\newcommand{\updown}[1]{\mathrm{P}_{#1}^{\wedge}}
\newcommand{\updownnon}[1]{\widetilde{\mathrm{P}}_{#1}^{\wedge}}
\newcommand{\updownnonlink}[2]{\widetilde{\mathrm{P}}_{#1,#2}^{\wedge}}
\newcommand{\downup}[1]{\mathrm{P}_{#1}^{\vee}}
\newcommand{\Glauber}{\mathrm{P_{Gl}}}
\newcommand{\Glaubert}{\mathrm{P^\tau_{Gl}}}
\newcommand{\HB}{\mathrm{P_{HB}}}
\newcommand{\HBt}{\mathrm{P_{HB}^{\tau}}}

\newcommand{\ind}{\mathbbm{1}}
\newcommand{\dif}{\,\mathrm{d}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Exp}{\E}
\newcommand{\ExpCond}[2]{\E\left( #1 \mid #2 \right)}
\newcommand{\ExpCondSub}[3]{\E_{#3}\left( #1 \mid #2 \right)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Prob}[1]{\mathrm{Prob}\left( #1 \right)}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\MI}{\mathrm{I}}
\newcommand{\Ent}{\mathrm{Ent}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\allone}{\mathbf{1}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\wt}{\mathrm{weight}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ip}[2]{\left\langle #1 , #2 \right\rangle}
\newcommand{\grad}{\nabla}
\newcommand{\hessian}{\nabla^2}
\newcommand{\trans}{\intercal}
\newcommand{\diag}{\mathrm{diag}}
\newcommand{\poly}{\mathrm{poly}}
\newcommand{\dist}{\mathrm{dist}}
\newcommand{\sgn}{\mathrm{sgn}}
\newcommand{\fpras}{\mathsf{FPRAS}}
\newcommand{\fpaus}{\mathsf{FPAUS}}
\newcommand{\fptas}{\mathsf{FPTAS}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bs}{\backslash}

\def\cI{\mathcal{I}}
\def\bF{\mathbb{F}}
\def\bP{\mathbb{P}}
\def\cF{\mathcal{F}}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}%https://www.overleaf.com/project/62449cc3c5280521f8c7ca5a

\renewcommand{\AA}{\mathcal{A}}
\newcommand{\BB}{\mathcal{B}}
\newcommand{\CC}{\mathcal{C}}
\newcommand{\DD}{\mathcal{D}}
\newcommand{\EE}{\mathcal{E}}
\newcommand{\FF}{\mathcal{F}}
\newcommand{\GG}{\mathcal{G}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\II}{\mathcal{I}}
\newcommand{\JJ}{\mathcal{J}}
\newcommand{\KK}{\mathcal{K}}
\newcommand{\LL}{\mathcal{L}}
\newcommand{\MM}{\mathcal{M}}
\newcommand{\NN}{\mathcal{N}}
\newcommand{\OO}{\mathcal{O}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\QQ}{\mathcal{Q}}
\newcommand{\RR}{\mathcal{R}}
\renewcommand{\SS}{\mathcal{S}}
\newcommand{\TT}{\mathcal{T}}
\newcommand{\UU}{\mathcal{U}}
\newcommand{\VV}{\mathcal{V}}
\newcommand{\WW}{\mathcal{W}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\YY}{\mathcal{Y}}
\newcommand{\ZZ}{\mathcal{Z}}

\newcommand{\Dirichlet}{\mathcal{E}}

\renewcommand{\QQ}{\mathcal{K}}
\newcommand{\uu}{u}
\renewcommand{\complement}{\mathsf{c}}
\newcommand{\bb}{b}

\newcommand{\kl}[2]{D_{\mathrm{KL}}\left(#1 \,\Vert\, #2\right)}
\newcommand{\tv}[2]{d_{\mathrm{TV}}\left(#1 , #2\right)}
\newcommand{\chitwo}[2]{\chi^2\left(#1 \,\Vert\, #2\right)}

\newcommand{\bkl}{\varphi_{\mathrm{kl}}}
\newcommand{\Ber}{\mathrm{Ber}}
\newcommand{\maxcut}{\mathsf{max\text{-}cut}}

\newcommand{\coorora}{\mathsf{Coordinate\ Oracle}}
\newcommand{\subcora}{\mathsf{Subcube\ Oracle}}
\newcommand{\stanora}{\mathsf{General\ Oracle}}
\newcommand{\pairora}{\mathsf{Pairwise\ Oracle}}

\newcommand{\idtest}{\textsc{ID-TEST}}

\newcommand{\alg}{\mathsf{Alg}}
\newcommand{\ora}[1]{\mathsf{Ora}[#1]}

\newcommand{\RED}[1]{\textcolor{red}{#1}}
\newcommand{\ra}{\rightarrow}

\newcommand{\Trelax}{T_{\text{relax}}}
\newcommand{\Tmix}{T_{\text{mix}}}

 \def\eric#1{\marginpar{$\leftarrow$\fbox{E}}\footnote{$\Rightarrow$~{\sf #1 --E}}}
 \def\daniel#1{\marginpar{$\leftarrow$\fbox{D}}\footnote{$\Rightarrow$~{\sf #1 --D}}}




\title{Lecture Notes on Spectral Independence and Bases of a Matroid: Local-to-Global and Trickle-Down from a Markov Chain Perspective}

\author{
 	Daniel \v{S}tefankovi\v{c}\thanks{Department of Computer Science, University of Rochester. Email: stefanko@cs.rochester.edu. Research supported in part by NSF grant CCF-1563757.}
  	\and Eric Vigoda\thanks{Department of Computer Science, University of California, Santa Barbara. Email: vigoda@ucsb.edu. 
	Research supported in part by NSF grant CCF-2147094.}
}


\date{\today}

\begin{document}
\maketitle

\begin{abstract}
These are self-contained lecture notes for spectral independence.
  For an $n$-vertex graph, the spectral independence condition is a bound on the maximum eigenvalue of the $n\times n$ influence matrix whose entries capture the influence between pairs of vertices, it is closely related to the covariance matrix.  We will present recent results showing that spectral independence implies the mixing time of the Glauber dynamics is  polynomial (where the degree of the polynomial depends on certain parameters). The proof utilizes local-to-global theorems which we will detail in these notes. Finally, we will present more recent results showing that spectral independence implies an optimal bound on the relaxation time (inverse spectral gap) and with some additional conditions implies an optimal mixing time bound of $O(n\log{n})$ for the Glauber dynamics.  

  We also present the results of Anari, Liu, Oveis Gharan, and Vinzant (2019) for generating a random basis of a matroid.  The analysis of the associated bases-exchange walk utilizes the local-to-global theorems used for spectral independence with the Trickle-Down Theorem of Oppenheim (2018) to analyze the local walks.
  Our focus in these notes is on the analysis of the spectral gap of the associated Markov chains from a functional analysis perspective, and we present proofs of the associated local-to-global theorems from this same Markov chain perspective.

\end{abstract}

\thispagestyle{empty}

\newpage

\setcounter{page}{1}


\section{Introduction}

These are a comprehensive set of lecture notes on spectral independence and its implications on the mixing time of the single-site update Markov chain known as the Glauber dynamics.

Spectral independence is a simple, yet seemingly quite powerful technique to obtain optimal upper bounds on the convergence rate of Markov chains to its stationary distribution.  In these notes we will introduce the spectral independence technique, explain the basic properties of the associated influence matrix, and discuss connections to simplicial complexes.  We then prove or at least provide the main details for the main results that spectral independence implies fast mixing of the Glauber dynamics.  


We also present the prominent result of Anari, Liu, Oveis Gharan, and Vinzant~\cite{ALOV19} which shows fast mixing of a Markov chain, known as the bases-exchange walk, for randomly sampling bases of a matroid.   This utilizes the local-to-global theorems presented for spectral independence and the Trickle-Down Theorem of Oppenheim~\cite{Opp18} to bound the spectral gap of the local walks. 

Our aim in these notes is to present the analyses from a
Markov chain perspective, and to be as self-contained as possible.
These notes are based on lectures given at the UCSB Summer School on ``New tools for optimal mixing of Markov chains: Spectral independence and entropy decay'' in August, 2022. 
Our starting point for these notes were the summer school lectures, presented by Nima Anari, Pietro Caputo, Zongchen Chen, Heng Guo, Tali Kaufman, and Kuikui Liu, and the associated lecture notes which were initially prepared by the following students: Yuzhou Gu, Tushant Mittal, Amanda Priestley, and Juspreet Singh Sandhu.  
 For more information on the summer school, including lecture videos and lecture notes, see:
\href{https://sites.cs.ucsb.edu/~vigoda/School/}{https://sites.cs.ucsb.edu/$\sim$vigoda/School/}



\subsection{Setting}

These notes study high-dimensional distributions defined by a graph, this corresponds to spin systems in statistical physics and undirected graphical models in machine learning.  Let $G=(V,E)$ be a graph on $n=|V|$ vertices.  

Consider a distribution $\mu$ on a subset of $\{0,1\}^V$.  Let
\[
\Omega=\{\sigma\in\{0,1\}^V:\mu(\sigma)>0\}.
\]
All of the results in these notes generalize to distributions defined on $\{1,\dots,q\}^V$ for integer $q\geq 2$; we will comment on these generalizations later in these notes.

An example to keep in mind is the so-called {\em hard-core model} which we define next.

\subsection{Running example: Hard-core Model}

The input to the hard-core model is a graph $G=(V,E)$ and an {\em activity} $\lambda>0$.  The hard-core model is an idealized model of a gas where $\lambda$ corresponds to the fugacity of the gas.
Configurations of the model are defined on independent sets of $G$; recall, an independent set is a subset $S$ of vertices which does not contain an edge, i.e., for all $\{x,y\}\in E$ either $x\notin S$ and/or $y\notin S$.  
Let $\Omega$ denote the collection of independent sets of $G$ (regardless of their sizes).

For an independent set $\sigma\in\Omega$, we can view
$\sigma$ as an $n$-dimensional vector in $\{0,1\}^n$ where the $i$-th coordinate is assigned 1 if the $i$-th vertex is in $\sigma$ and is assigned 0 otherwise.  Each independent set $\sigma\in\Omega$ is assigned a weight
\[ w(\sigma) = \lambda^{|\sigma|},
\]
where $|\sigma|$ is the number of vertices in the independent set $\sigma$.
The Gibbs distribution $\mu=\mu_{G,\lambda}$ is the probability distribution proportional to the weights.  For $\sigma\in\Omega$,
\[ \mu(\sigma) = \frac{\lambda^{|\sigma|}}{Z},
\]
where the partition function  $Z=Z_{G,\lambda}$ is the normalizing factor $Z=\sum_{\eta\in\Omega} \lambda^{|\eta|}$.

\subsection{Glauber dynamics}
The Glauber dynamics, also known as the Gibbs sampler, is a simple Markov chain designed to sample from the Gibbs distribution.  We begin with the definition of the Glauber dynamics for the special case of the hard-core model (moreover one can further consider the case $\lambda=1$ for simplicity).

From a state $X_t\in\Omega$, the transitions $X_t\rightarrow X_{t+1}$ of the Glauber dynamics for the hard-core model are defined as follows:
\begin{enumerate}
    \item Choose a vertex $v$ uniformly at random from $V$.
    \item Let \[ X' = \begin{cases}
    X_t\cup\{v\} & \mbox{with probability }\lambda/(1+\lambda) \\
      X_t\setminus\{v\} & \mbox{with probability }1/(1+\lambda) 
      \end{cases}
      \]
     \item If $X'\in\Omega$ (i.e., it is a valid independent set) then set $X_{t+1}=X_t$ 
     and otherwise set $X_{t+1}=X_t$.
\end{enumerate}

Notice that the Glauber dynamics is irreducible (since all states can reach the empty set) and aperiodic (since there is a self-loop of not changing the state at $v$) and hence the chain is ergodic where the stationary distribution is the Gibbs distribution.

We can now present the general definition of the Glauber dynamics for more general distributions.  For a distribution $\mu$ on $\Omega\subset \{0,1\}^V$, the Glauber dynamics is defined as follows.  
From a state $X_t\in\{0,1\}^V$,
\begin{enumerate}
    \item Choose a vertex $v$ uniformly at random from $V$.
    \item For all $w\neq v$, let $X_{t+1}(w)=X_t(v)$.
    \item Choose $X_{t+1}(v)$ from the conditional Gibbs distribution $\mu(\sigma(v)|\sigma(w)=X_{t+1}(w) \mbox{ for all }w\neq v\})$, i.e., fix the spin at all vertices except $v$ and resample the spin/label at $v$ conditional on the fixed configuration on the rest of the vertices.
    \end{enumerate}
    
\subsection{Spectral Independence}
\label{sec:SI-definition}

Spectral independence was introduced by Anari, Liu, and Oveis Gharan~\cite{ALO20}.  It is defined by a $n\times n$ influence matrix which captures the pairwise influence or correlations between pairs of vertices.

\begin{definition}[Influence Matrix]
\label{defn:inf-matrix}
Let $G=(V,E)$ be a graph where $V=\{1,\dots,n\}$, and
$\mu$ be a distribution on a subset of $\{0,1\}^V$.
Let $\Psi$ be the following real-valued $n\times n$ matrix.  For $1\le i,j\le n$, 
\[
\Psi(i\rightarrow j) = \Psi(i,j) := \begin{cases}
    \mu\left[\sigma(j)=1 \mid \sigma(i)=1\right]
- 
\mu\left[\sigma(j)=1 \mid \sigma(i)=0\right]
& \mbox{if } i\neq j
\\
1 &\mbox{if } i=j.
\end{cases}
\]
\end{definition}

We will need to extend the above definition to arbitrary ``pinnings'' which are a fixed assignment $\tau$ to a subset $S$ of vertices.
For $S\subset [n]=\{1,\dots,n\}$, a pinning on $S$ is an assignment $\tau:S\rightarrow\{0,1\}$.
Let $\Pinning$ denote the collection of valid pinnings where we assume $\tau\in\Pinning$ is valid in the sense that there exists $\sigma\in\Omega$ where $\sigma(S)=\tau$; see \cref{sec:poly-mixing-proof} for a more detailed definition.  

For a pinning $\tau\in\Pinning$, let $\mu_{\tau}$ denote the conditional Gibbs distribution, i.e., the distribution $\mu$ conditional on the fixed assignment $\tau$ on $S$. For $\sigma\in\Omega$, we have 
\[ \mu_{\tau}(\sigma)=\mu(\sigma\mid\sigma(S)=\tau)) 
= \frac{\mu(\sigma(\overline{S})\cup\tau(S))}{\mu(\tau)}.
\]


For a pinning $\tau\in\Pinning$ 
where $\tau:S\rightarrow\{0,1\}$ for $S\subset V$, 
let $T\subset V\setminus S$ denote the set of vertices which are ``free'' in the following sense:
\[
i\in T \iff \mu_\tau(\sigma(i)=1)>0 \mbox{ and } \mu_\tau(\sigma(i)=0)>0.
\]
Note, a vertex $i\in V\setminus (S\cup T)$ is ``frozen'', i.e., it can only attain one spin and hence we can fix the configuration on that vertex and remove the corresponding two rows/columns from the matrix $\Psi_\tau$.

For $i,j\in T$, let 
\begin{align}
\label{eq:inf-pin}
\Psi_\tau(i\rightarrow j) &:= 
 \mu_\tau\left[\sigma(j)=1 \mid \sigma(i)=1\right] - 
\mu_\tau\left[\sigma(j)=1 \mid \sigma(i)=0\right]
\\
& = \mu\left[\sigma(j)=1 \mid \sigma(i)=1,\sigma(S)=\tau\right] - 
\mu\left[\sigma(j)=1 \mid \sigma(i)=0,\sigma(S)=\tau\right].
\nonumber
\end{align}
Note the empty pinning $\emptyset$ is always a valid pinning and hence $\Psi=\Psi_\emptyset$.



The matrix $\Psi$ can be asymmetric and the entries of $\Psi$ can be positive or negative.  Nevertheless all of its eigenvalues are real, see \cref{DL1}.  Intuitively the fact that $\Psi$ is positive semidefinite is because it is closely related to the covariance matrix $\Cov$.  In particular, the influence matrix $\Psi$ is equal to a positive diagonal matrix times the covariance matrix $\Cov$ and then the fact follows from the fact that the covariance matrix is positive semidefinite.

Since all eigenvalues of $\Psi$ are real numbers we can denote the maximum eigenvalue by $\lambda_{\max}(\Psi)$.  Now we can define spectral independence.

\begin{definition}[Spectral Independence]
\label{defn:SI}
For $\eta>0$, we say that $\mu$ is $\eta$-spectrally independent if for all pinnings $\tau\in\Pinning$, $\lambda_{\max}(\Psi_\tau)\leq 1+\eta$.
\end{definition}

Note that the spectral independence condition only depends on the distribution $\mu$, it has no explicit dependence on the Glauber dynamics.  
The definition was extended to non-binary spin spaces, such as the Potts model and colorings, in~\cite{FGYZ21,CGSV21} (see also~\cite{CLV21,BCCPSV22} for a general formulation).

When $\mu$ is a product distribution then 
$\eta=0$.  Our goal is to show that $\eta$ is constant.

\begin{remark}
Note the diagonals of the influence matrix $\Psi$ are 1 since if $i=j$ then conditioning on $i$ prescribes $j$.  We could have defined the influence matrix so that the off-diagonal entries remain the same and the diagonals are 0; this would decrease all of the eigenvalues by 1, and hence with this alternative definition we would change the spectral independence requirement from $1+\eta$ to $\eta$.
\end{remark}

\subsection{Main Results}

Spectral independence was introduced by Anari, Liu, and Oveis Gharan~\cite{ALO20}.  They proved the following result that spectral independence implies polynomial mixing time of the Glauber dynamics.
\begin{theorem}[\cite{ALO20}]\label{thm:SI-mixing}
Suppose there exists a constant $\eta>0$ such that
the system is $\eta$-spectrally independent, then
for any $n$-vertex graph $G=(V,E)$, the relaxation time of the Glauber dynamics satisfies:
\[ \Trelax \leq O(n^{1+\eta}).
\]
Therefore, the mixing time satisfies $T_{\mathrm{mix}}=O(n^{2+\eta}\log{n})$.
\end{theorem}

The above result was improved by Chen, Liu, and Vigoda~\cite{CLV21} to show optimal bounds on the relaxation time when the maximum degree is constant.

\begin{theorem}[\cite{CLV21}]\label{thm:SI-constant-relax}
Suppose there exists a constant $\eta>0$ such that
the system is $\eta$-spectrally independent. 
Then for all constant $\Delta\geq 2$,
for any $n$-vertex graph $G=(V,E)$ of maximum degree $\Delta$,
the relaxation time of the Glauber dynamics satisfies:
\[ \Trelax \leq O(n).
\]
In particular, there exists a constant $C(\eta,\Delta)$ where the relaxation time satisfies:
\[ \Trelax \leq C(\eta,\Delta)n.
\]
Therefore, the mixing time satisfies $T_{\mathrm{mix}}=O(n^{2}\log{n})$.
\end{theorem}

If one further assumes a lower bound on the marginal probability for every vertex and every valid spin assignment then Chen, Liu and Vigoda~\cite{CLV21} proved an optimal upper bound on the mixing time.

\begin{definition}[Marginally bounded]
\label{defn:marg-bound}
For $b>0$, we say that $\mu$ is $b$-marginally bounded if for all pinnings $\tau\in\Pinning$, for all $v\in V$, all $s\in\{0,1\}$, then the following holds:
\[
\mu_\tau(\sigma(v)=s)>b \mbox{ or } \mu_\tau(\sigma(v)=s)=0.
\]
\end{definition}

\begin{theorem}[\cite{CLV21}]\label{thm:SI-constant-mix}
Suppose there exist constants $\eta>0$ and $b>0$ such that
the system is $\eta$-spectrally independent and $b$-marginally bounded. 
Then for all constant $\Delta\geq 2$,
for any $n$-vertex graph $G=(V,E)$ of maximum degree $\Delta$,
the mixing time of the Glauber dynamics satisfies:
\[ \Tmix \leq O(n\log{n}).
\]
In particular, there exists a constant $C(\eta,\Delta,b)$ where the mixing time satisfies 
\[ \Tmix \leq C(\eta,\Delta,b)n\log{n}.
\]
\end{theorem}
This is an optimal bound on the mixing time as Hayes and Sinclair~\cite{HayesSinclair} proved that for any graph with constant maximum degree $\Delta$, the mixing time is $\geq C(\Delta)n\log{n}$.

\subsection{Outline of Paper}

In the following section (\cref{sec:properties}) we will prove the basic properties of the influence matrix~$\Psi$, namely that $\Psi$ is positive semidefinite.  In \cref{sec:MC} we will present the basic facts and definitions regarding Markov chains that are central to the
upcoming proofs of the main theorems.
The local-to-global theorem of Alev and Lau~\cite{AL20} known as the Random Walk Theorem is presented in \cref{sec:rapid}, and the local walk, which is closely connected to the influence matrix, is introduced in this section.  We then prove \cref{thm:SI-mixing} of Anari, Liu, and Oveis Gharan~\cite{ALO20}, using the Random Walk Theorem, in \cref{sec:poly-mixing-proof}.  

The Random Walk Theorem is proved in \cref{sec:RW}; here we introduce the up and down processes and explore their connection to the local walk.  Then in \cref{section-proof-relax} we present the proof of \cref{thm:SI-constant-relax} of Chen, Liu, and Vigoda~\cite{CLV21}, which shows that spectral independence implies an optimal bound on the spectral gap of the Glauber dynamics.  Finally, in \cref{sec:optimal} we present the high-level ideas used to further extend the proof from variance contraction to entropy contraction and obtain optimal mixing time.

The results for generating a random basis of a matroid are presented beginning in~\cref{sec:matroids} where we review the relevant definitions for matroids and present the main result for the mixing time of the bases exchange walk.  The analysis of the bases exchange walk utilizes the Random Walk Theorem, presented earlier in \cref{sec:RW}, and the Trickle Down Theorem, which is presented in~\cref{sec:trickledown}.  We then prove fast mixing of the bases exchange walk in~\cref{sec:basesexchange}, and finally we prove the Trickle Down Theorem in~\cref{sec:proof-trickledown}.

\section{Properties of the Influence Matrix}
\label{sec:properties}

In this section we will prove that the influence matrix is positive semidefinite.  

\begin{lemma}\label{DL1}
All eigenvalues of $\Psi$ are non-negative real numbers.
\end{lemma}

\begin{proof}
    For every $i,j \in [n]$, the covariance of $\ind_{i},\ind_{j}$ is given by
    \begin{align*}
        \Cov_{\mu}(i,j) 
        &= 
        \E_{\mu}[\ind_i\ind_j]
        - \E_{\mu}[\ind_i]\cdot\E_\mu[\ind_j]
        \\
        &=
        \Pr_{\sigma\sim\mu}[\sigma(i)=\sigma(j)=1] 
        - \Pr_{\sigma\sim\mu}[\sigma(i)=1]\cdot\Pr_{\sigma\sim\mu}[\sigma(j)=1]
        \\&=
        \Pr[\sigma(i)=1]\times\Big(\Pr[\sigma(j)=1 \mid \sigma(i)=1] - \Pr[\sigma(j)=1]\Big)
    \end{align*}
    Plugging in
    \begin{align*}
        \Pr[\sigma(j) = 1] = \Pr[\sigma(j) = 1 \mid \sigma(i) = 1] \cdot \Pr[\sigma(i) = 1] + \Pr[\sigma(j) = 1 \mid \sigma(i) = 0] \cdot \Pr[\sigma(i) = 0],
    \end{align*}
    we obtain that
    \begin{align*}
        \Cov_{\mu}(i,j) 
        &=  
        \Pr[\sigma(i)=1]\times\Big(\Pr[\sigma(j)=1 \mid \sigma(i)=1] - 
        \Pr[\sigma(j) = 1 \mid \sigma(i) = 1] \cdot \Pr[\sigma(i) = 1] 
         \\ & \hspace*{3in} - \Pr[\sigma(j) = 1 \mid \sigma(i) = 0] \cdot \Pr[\sigma(i) = 0]
        \Big)
        \\
             &=  
        \Pr[\sigma(i)=1]\times\Big(\Pr[\sigma(j)=1 \mid \sigma(i)=1](1-\Pr[\sigma(i) = 1]) 
        \\ & \hspace*{3in}
 - \Pr[\sigma(j) = 1 \mid \sigma(i) = 0] \cdot \Pr[\sigma(i) = 0]
        \Big)
             \\
             &=  
        \Pr[\sigma(i)=1]\cdot\Pr[\sigma(i) = 0]) \times\Big(\Pr[\sigma(j)=1 \mid \sigma(i)=1]
 - \Pr[\sigma(j) = 1 \mid \sigma(i) = 0]
        \Big).
    \end{align*}
From the definition of the influence matrix $\Psi$ we have:
\[
        \Cov_{\mu}(i,j) = \Pr[\sigma(i) = 1] \cdot \Pr[\sigma(i) = 0] \cdot \Psi_{\mu}(i,j).
  \]
    Since this holds for all $i,j \in [n]$, if we let $D$ denote the diagonal matrix with $D(i,i) = \Pr[\sigma(i) = 1] \cdot \Pr[\sigma(i) = 0]$ for all $i\in[n]$, then we have the matrix identity
    \begin{align*}
        \Psi_{\mu} = D^{-1}\Cov_{\mu}.
    \end{align*}
    Since $\Cov_{\mu}$ is symmetric positive semidefinite and $D$ is a diagonal matrix with positive diagonal entries (this follows from the definition of the set $T$ of free vertices in \cref{sec:SI-definition}), it follows that $\Psi_{\mu}$ has non-negative real eigenvalues.
\end{proof}






\section{Markov Chain Fundamentals}
\label{sec:MC}

To prove \cref{thm:SI-constant-relax} we will bound the spectral gap of the chain.  We will take a functional analysis formulation of the spectral gap.  In particular, we will bound the decay rate of the variance of the chain with respect to the stationary distribution for an arbitrary real-valued function on the state space.  This yields tight upper bounds on the relaxation time of the chain.  In \cref{sec:optimal}, we will extend the notions presented in this section from variance to entropy, and then we will bound the (modified) log-Sobolev constant which captures the decay rate of relative entropy; this yields optimal upper bounds on the mixing time, rather than the relaxation time.

Consider a function $f:\Omega\rightarrow\R$.  The expectation of $f$ with respect to the stationary distribution $\mu$ is the following:
\[
\mu(f) := \Exp_\mu[f] := \sum_{\eta\in\Omega}\mu(\eta)f(\eta).
\]
Moreover, the variance of $f$ with respect to $\mu$ has the following equivalent formulations that can be easily verified by simple algebra:
\[ \Var_\mu(f) := \Exp_\mu[(f - \mu(f))^2] = \sum_{\sigma\in\Omega}\mu(\sigma)\left(f(\sigma)-\mu(f)\right)^2
  = \frac12\sum_{\sigma,\eta\in\Omega}\mu(\sigma)\mu(\eta)(f(\sigma)-f(\eta))^2,
\]
where the last sum is over all ordered pairs (see \cite[(5.4-5.5)]{Jerrum:notes} for a proof of the last identity).

Notice that the later formulation of variance is a sum over all pairs of states of the difference in their functional value (squared to measure the absolute value).


In contrast to the variance which measures the ``global variation'' of the function $f$, the {\em Dirichlet form} is a measure of the ``local variation'' for a specified Markov chain.  The terms global vs. local are with respect to the graph $(\Omega,P)$.

The Dirichlet form measures the difference in the functional value over pairs of adjacent states with respect to the graph $(\Omega,P)$ as formulated in the following definition.
\begin{definition}[Dirichlet Form]
    \[
        \Dirichlet_P(f) := \frac{1}{2}
        \sum_{\sigma,\eta\in\Omega}\mu(\sigma)P(\sigma,\eta)(f(\sigma)-f(\eta))^2.
    \]
\end{definition}
 
For an ergodic Markov chain $(\Omega,P,\pi)$, recall that the stationary distribution is an eigenvector with eigenvalue $1$.  Hence, we denote the eigenvalues in the following manner:
 \[
 1> \lambda_2\geq\lambda_3\geq \cdots \geq \lambda_N>-1.
 \] 
 The fact that $\lambda_N>-1$ follows from the aperiodicity of an ergodic Markov chain.  
The spectral gap is defined as $\gamma=1-\lambda_2$.  Moreover, the absolute spectral gap is $1-\max\{\lambda_2,|\lambda_N|\}$.  When the chain has self-loop probabilities $\geq 1/2$ (i.e., $P(\sigma,\sigma)\geq 1/2$ for all $\sigma\in\Omega$), then all eigenvalues are non-negative and hence the spectral gap equals the absolute spectral gap.  The absolute spectral gap measures the decay rate of variance whereas the spectral gap is potentially off by a factor of $1/2$.  However the spectral gap is more convenient to work with due to the following formulation known as a Poincare inequality (see \cite[Lemma 13.12]{LevinPeresWilmer} for a proof of the equivalence).

\begin{lemma}[Spectral gap]
    The spectral gap $\gamma$ is the largest constant, so that the following inequality holds for any function $f:\Omega\rightarrow\R$,
    \begin{equation}
    \label{defn:poincare}
        \gamma\cdot \Var(f) \leq \Dirichlet_P(f)\, .
    \end{equation}
\end{lemma}

The spectral gap measures the decay rate of the variance of the lazy version of $P$ which is defined as:
\[ P_{zz} = \frac{1}{2}(P+I),
\]
where $I$ is the identity matrix which has 1 on the main diagonal and 0 on the off-diagonal entries.  This corresponds to adding a self-loop of $1/2$ to every state $i\in\Omega$, i.e., $P_{zz}(i,i)\geq 1/2$.  Notice that the eigenvalues of $P_{zz}$ lie between $0$ and $1$
and the absolute spectral gap is the same as the spectral gap in this case.
Hence, we have the following (see Mihail~\cite{Mihail,Jerrum:notes}):
\[ \Var(P_{zz}f)\leq (1-\gamma/2)\Var(f).
\]
This yields the following bound on the relaxation time and the mixing time~\cite{Jerrum,Caputo-notes}:  
\begin{equation}\label{eq:relax-gap-lazy}
    \Trelax(P_{zz}) \leq  {2n}{\gamma} \ \ \ \mbox{ and }
    \Tmix(P_{zz}) \leq \frac{2n}{\gamma}\log(1/\mu^*),
\end{equation}
where $\mu^*=\min_{x\in\Omega}\mu(x)$.

If the absolute spectral gap is the same as the spectral gap (which is true, for example, when $P$ is positive semidefinite) then:
\begin{equation}\label{eq:relax-gap}
    \Trelax(P) \leq  {n}{\gamma} \ \ \ \mbox{ and }
    \Tmix(P) \leq \frac{n}{\gamma}\log(1/\mu^*).
\end{equation}
  Moreover, Dyer, Greenhill, and Ullrich~\cite{DGU} proved that the heat-bath block dynamics is positive semidefinite and the Glauber dynamics is a special case of the heat-bath block dynamics where the blocks are single vertices.
Hence, we can apply the bounds in \cref{eq:relax-gap} to the Glauber dynamics, in particular to obtain~\cref{thm:SI-constant-relax}.




\section{Rapid Mixing from Spectral Independence}
\label{sec:rapid}

The main result of this section is \cref{thm:SI-mixing}, which states that if $\mu$ is $\eta$-spectrally independent for a constant $\eta$, then the Glauber dynamics has polynomial mixing time.

\subsection{Pinnings}
Recall that a pinning is a fixed assignment of spins to a subset of vertices.  Hence, for $S\subset [n]=\{1,\dots,n\}$ and for a pinning $\tau:S\rightarrow\{0,1\}$ then let $\mu_{\tau}$ denote the conditional Gibbs distribution, i.e., the distribution $\mu$ conditional on the fixed assignment $\tau$ on $S$.  

We will partition the pinnings based on the number of  pinned vertices.  Recall, 
the set of valid configurations is denoted by
\[
\Omega=\{\sigma\in\{0,1\}^V:\mu(\sigma)>0\}.
\]
For $S\subset V$ and a pinning $\tau:S\rightarrow\{0,1\}$, denote the assignments consistent with $\tau$ as $\Omega_\tau:=\{\sigma\in\Omega: \sigma(S)=\tau(S)\}$.
For $0\leq k\leq n$, denote the set of valid pinnings of size $k$ by 
\[ \Pinning_k:=\{\tau\in\{0,1\}^S: \mbox{ for some }S\subseteq V, \mbox{ such that } |S|=k, \Omega_\tau\neq\emptyset\}.
\]
Observe that $\Omega=\Pinning_n$ (notice that for $\tau\in\Omega$, $\Omega_\tau=\{\tau\}\neq\emptyset$ and
hence $\tau\in\Pinning_n$).
Finally, denote the collection of all (valid) pinnings by:
\[ \Pinning = \bigcup_{k=0}^n \Pinning_k.\]

\subsection{Rapid Mixing Theorem}
We will bound the mixing time of the Glauber dynamics by considering the spectral gap.
We will prove the following result of Anari, Liu, and Oveis Gharan~\cite{ALO20}. Namely, that if the Gibbs distribution $\mu$ is $\eta$-spectrally independent (note, \cref{defn:SI} requires it for all pinnings $\tau\in\Pinning$) then the spectral gap satisfies:
\begin{equation}
\label{eqn:gap-SI}
\gamma(\Glauber) \geq \frac{C(\eta)}{n^{1+\eta}},
\end{equation}
for a constant $C(\eta)>0$.
By \cref{eq:relax-gap} this implies relaxation time $O(n^{1+\eta})$
and mixing time $O(n^{2+\eta}\log{n})$, and hence 
this proves \cref{thm:SI-mixing}. 

\subsection{Local to Global: Random Walk Theorem of Alev-Lau}
\label{sec:local-walks}

The key technical tool in the proof of \cref{eqn:gap-SI} is the following local-to-global theorem of Alev and Lau~\cite{AL20}, which improved upon Kaufman and Oppenheim~\cite{KO20a}.


Let's begin by defining the ``local walks'' $Q$.  Our input graph $G=(V,E)$ has $n=|V|$ vertices and each vertex has two possible spin assignments $0$ or $1$.  The state space of the local walk are the $2n$ (vertex,spin) pairs $(v_i,s_i)\in V\times\{0,1\}$.  The stationary distribution of the walk corresponds to the marginal probability $\mu(\sigma(v_i)=s_i)$ of that particular vertex $v_i$ having the specified spin $s_i$.

The matrix $Q$ is a real-valued $2n\times 2n$ matrix.  For $1\le i\neq j\le n$, $s_i,s_j\in\{0,1\}$, let
\[ Q((i,s_i),(j,s_j)) = \frac{1}{n-1}\Pr_{\sigma\sim\mu}[\sigma(j)=s_j\mid \sigma(i)=s_i],
\]
and when $i=j$, then for $s,s'\in\{0,1\}$, let $Q((i,s),(i,s'))=0$.
Moreover, 
for a subset $S\subset V$ where $|S|=k-1$, for a pinning $\tau:S\rightarrow\{0,1\}$ where $\tau\in\Pinning$, let
\begin{equation}
\label{def:Q}
Q_{\tau}((i,s_i),(j,s_j)) = \frac{1}{n-k}\mu(\sigma(j)=s_j\mid \sigma(i)=s_i,\sigma(S)=\tau),
\end{equation}
and when $i=j$ we set the entries to $0$ as before.

The Markov chain $Q$ is called the ``local walk'' because it captures local information of the Gibbs distribution, namely the marginal probabilities of (vertex,spin) pairs.  In contrast, the Glauber dynamics is referred to as a ``global walk'' because it captures the probability of a configuration on the entire graph. 

The upcoming Random Walk Theorem of Alev and Lau~\cite{AL20} is referred to as a local-to-global theorem because it relates the behavior of local chains $Q$ to the Glauber dynamics which is a global chain.
To do this we need to consider the local walk on each level.  Namely, for $k$ where $0\leq k\leq n-2$, for $\tau\in\Pinning_k$, consider the local walk $Q_\tau$.  Let $\gamma_k$ be the minimum spectral gap for a local walk $Q_\tau$ where $\tau\in\Pinning_k$.

We can now formally state the Random Walk Theorem of Alev and Lau~\cite{AL20} (which improves upon earlier work of~\cite{KO20a}).
\begin{theorem}[Random Walk Theorem \cite{AL20}]
\label{thm:RW}
\begin{equation}
    \label{eqn:RW-thm-simplified}
 \gamma(P_{\mathrm{Glauber}}) \geq \frac{1}{n}\prod_{k=0}^{n-2}\gamma_k,
\end{equation}
where $$\gamma_k=\min_{\tau\in\Pinning_k}\gamma(Q_{\tau})$$ is the spectral gap
for the local walk with a worst-case pinning of $k$ vertices (this corresponds to a link on level $k$ when viewed as a simplicial complex).
\end{theorem}

We will present the proof of the Random Walk Theorem in \cref{sec:RW}.

\subsection{Local Walk Connection to Influence Matrix}
\label{sec:local-influence}

We begin by bounding $\lambda_2$ of the local walk $Q_{\tau}$ in terms of the influence matrix $\Psi_{\tau}$ for the spectral independence technique.  In fact, we can relate the entire spectrum of $Q_{\tau}$ with the spectrum of $\Psi_{\tau}$ in the following manner.

Let us first extend the definition of the influence matrix in \cref{defn:inf-matrix} to a fixed pinning.  For a subset $S\subset V$ where $|S|=k$, for a pinning $\tau:S\rightarrow\{0,1\}$ where $\tau\in\Pinning$, recall (see equation~\eqref{eq:inf-pin}) the influence matrix $\Psi_\tau$ is defined as:
  \[
\Psi_\tau(i\rightarrow j)  := \mu\left[\sigma(j)=1 \mid \sigma(i)=1,\sigma(S)=\tau\right]
- 
\mu\left[\sigma(j)=1 \mid \sigma(i)=0,\sigma(S)=\tau\right].
\]


\begin{lemma}
For a pinning $\tau\in\Pinning_k$,
\label{lem:QandPsi}
\begin{equation}
\label{eq:Inf-to-local}
\lambda_2(Q_{\tau}) = \frac{1}{n-k-1}\left(\lambda_{\mathrm{max}}(\Psi_{\tau}) - 1\right).
\end{equation}
Moreover, \begin{equation}\label{er3}
\mbox{spectrum}(Q_{\tau}) = \mbox{spectrum}\left(\frac{1}{n-k-1}\left(\Psi_{\tau} - I\right)\right)\cup\{1\}\cup\left\{n-k-1\mbox{ copies of } \frac{-1}{n-k-1}\right\}.
\end{equation}
\end{lemma}

As a consequence of the above lemma we have that 
$\gamma_k\geq 1-\eta/(n-k-1)$, see \cref{sec:poly-mixing-proof} for more details.

\begin{proof}[Proof of \cref{lem:QandPsi}]
We will prove the lemma for the case without a pinning.  Recall, $\mu$ denotes the Gibbs distribution.

Consider the local random walk $Q$. Notice that this walk has an $n$-partite structure, namely, for every $v_i\in V$, the states $(v_i,0)$ and $(v_i,1)$ are an independent set, and each of these $n$ independent sets are fully connected by a (weighted) bipartite clique.  This $n$-partite structure yields eigenvalues $-1/(n-1)$ for $Q$.   The stationary distribution for $Q$ is an eigenvector with eigenvalue $1$ (note that when $n\geq 3$ then the chain is clearly aperiodic and hence ergodic).

Let $\pi$ denote the stationary distribution of the local walk $Q$. For a vertex $j\in V$ and a spin/label $s_j\in\{0,1\}$,
we have $\pi((j,s_j))=(1/n) \Pr_{\sigma\sim\mu}[\sigma(j)=s_j]$, and hence the distribution $\pi$ corresponds to the normalized marginal distribution of the Gibbs distribution $\mu$. 
Moreover, let $\pi_j$ denote the marginal distribution at vertex~$j$; that is, $\pi_j((j,s_j))=\Pr_{\sigma\sim\mu}[\sigma(j)=s_j]$ for $s_j\in\{0,1\}$, and the remaining entries set to~$0$.

We will ``zero-out'' these trivial eigenvalues $1$ and $-1/(n-1)$ in the following manner:
\[
M_\pi = Q - \frac{n}{n-1}\mathbf{1}\pi^T + \frac{1}{n-1}\sum_{i=1}^n\mathbf{1}_i\pi_i^T.
\]

Then we notice that $M_{\pi}$ has the following block structure:
\begin{equation}\label{er1}
M_{\pi} = \begin{pmatrix} 
A_{\pi}  & -A_{\pi} \\ B_{\pi} & -B_{\pi}\end{pmatrix},
\end{equation}
where 
\begin{equation}\label{er2}
A_{\pi}-B_{\pi} = \frac{1}{n-1}(\Psi-I).
\end{equation}
To see~\eqref{er1} and~\eqref{er2} note that for $i\neq j$ we have
$$
M_\pi((i,s_i),(j,s_j)) = \frac{1}{n-1}\Big(
\Pr_{\sigma\sim\mu}[\sigma(j)=s_j\mid \sigma(i)=s_i]- \Pr_{\sigma\sim\mu}[\sigma(j)=s_j]
\Big),
$$
and 
$$
M_\pi((i,s_i),(i,s_i)) = M_\pi((i,s_i),(i,1-s_i)) = 0.
$$

Note that if $w$ is a left-eigenvector of $A_\pi-B_\pi$ then  $\bigl( \begin{smallmatrix}w \\ -w\end{smallmatrix}\bigr)$ is a left-eigenvector of $M_\pi$ with the same eigenvalue. Moreover, vectors of the form $(v^T v^T)$ (a space of dimension $n$) are both: right-eigenvectors of $M_\pi$ with eigenvalue $0$, and are perpendicular to the left-eigenvectors of the form $\bigl( \begin{smallmatrix}w \\ -w\end{smallmatrix}\bigr)$. These vectors 
of the form $(v^T v^T)$ yield right-eigenvectors of $\frac{n}{n-1}\mathbf{1}\pi^T - \frac{1}{n-1}\sum_{i=1}^n\mathbf{1}_i\pi_i^T$, where $\mathbf{1}$ has eigenvalue $1$ and the subspace perpendicular to $\pi$ (a space of dimension $n-1$)
has eigenvectors with eigenvalue $-\frac{1}{n-1}$. This implies~\eqref{er3}.

This completes the proof of the lemma for the case without pinning.  The proof easily generalizes to an arbitrary pinning $\tau\in\Pinning_k$ with $(n-1)$ replaced by $(n-k-1)$.  
\end{proof}


\subsection{Proof of Rapid Mixing (Theorem \ref{thm:SI-mixing})}
\label{sec:poly-mixing-proof}
We can now utilize~\cref{lem:QandPsi} and \cref{thm:RW} to conclude $\poly(n)$ relaxation time for the Glauber dynamics and thereby prove \cref{thm:SI-mixing}.

\begin{proof}[Proof of \cref{thm:SI-mixing}]
Recall, the definition of spectral independence in~\cref{defn:SI}; it states that the maximum eigenvalue of the influence matrix $\Psi$  is at most $1+\eta$.  Hence, from~\cref{lem:QandPsi}, we get that 
$\lambda_2(Q)\leq\eta/(n-1)$.  
Moreover, \cref{defn:SI} is for the worst-case pinning and hence we can apply \cref{lem:QandPsi} to any pinning.  Therefore, we have that 
\begin{equation}
\label{eqn:gammak}
    \gamma_k\geq 1-\eta/(n-k-1),
\end{equation} where $\gamma_k$ is defined below~\eqref{eqn:RW-thm-simplified}.
However, the above bound is not useful when $k\geq n-\eta-1$, but in this case we have the following bound.  

When $k\geq n-\eta-1$ then the local walk $Q_\tau$ for $\tau\in\Pinning_k$ is a walk on $2(n-k)\leq 2(\eta+1)$ states.  Since $\eta$ is a constant, then this is a walk on a constant number of states and hence the spectral gap is lower bounded by a constant $C=C(\eta)$.  Therefore, the following holds:
\begin{equation}
    \label{eqn:gap-bigk}
\mbox{ if } k\geq n - \eta-1, \mbox{ then } \gamma_k\geq C \mbox{ for some constant $C>0$.}
\end{equation}

Now we can apply \eqref{eqn:RW-thm-simplified} in the Random Walk Theorem (\cref{thm:RW}) and we obtain the following:
\begin{align*}
 \gamma(P_{\mathrm{Glauber}}) &\geq \frac{1}{n}\prod_{k=0}^{n-2}\gamma_k
 \\
 &\geq C^{\eta}\times\frac{1}{n}\prod_{k=0}^{n-\eta-2}\gamma_k
 & \mbox{by \cref{eqn:gap-bigk} }
 \\
 &\geq C^{\eta}\times\frac{1}{n}\prod_{k=0}^{n-\eta-2} \left(1-\frac{\eta}{n-k-1}\right)
 & \mbox{by \cref{eqn:gammak} }
 \\
&\geq \frac{C'}{n}\prod_{k=0}^{n-\eta-2} \left(1-\frac{\eta}{n-k-1}\right) 
%&\mbox{by breaking into $k\leq n-\eta-1$ and $k>n-\eta-1$}
\\
&\geq
\frac{C'}{n}\exp\left(-\sum_{k=0}^{n-\eta-2}\frac{\eta}{n-k-1-\eta}\right)
& \mbox{since $1-x\geq\exp\left(-\frac{x}{1-x}\right)$} \\
&\geq
\frac{C'}{n}\exp\left(-\eta\sum_{i=1}^{n-\eta-1}\frac{1}{i}\right)
\\
&\geq \frac{C''}{n}\exp(-\eta(1+\ln(n-\eta))) 
\\
&\geq \frac{C'''}{n}(n-\eta)^{-\eta}
\\
&\geq  \frac{C'''}{n^{1+\eta}},
\end{align*}
where $C,C',C'',C'''$ are positive constants (which may depend on $\eta$).
\end{proof}

\section{Random Walk Theorem: Proof of Theorem \ref{thm:RW}}
\label{sec:RW}

In this section we will prove the Random Walk Theorem (\cref{thm:RW}).  We will use random walks between adjacent levels of the associated simplicial complex which corresponds to walks between sets $\Pinning_k$ and $\Pinning_{k+1}$.  These will be denoted as up and down walks, and their composition as up-down and down-up walks.


\subsection{Up and Down Walks}\label{sec:chains}
We begin with the definitions of the down-up and up-down chains which are at the heart of the proof.  We do not explicitly utilize simplicial complexes, though readers familiar with simplicial complexes will understand the natural connection.


We start by defining a probability distribution on the collection of partial assignments to a set of $k$ vertices where $0\leq k\leq n$.
Recall, $\tau\in\Pinning_k$ is an assignment of spins $\{0,1\}$ to a subset $S\subseteq V$ where $|S|=k$.  We define the distribution $\pi_k$ on $\Pinning_k$ as follows:
\[ \mbox{for }\tau\in\Pinning_k, \ \ \ \ \pi_k(\tau) = \frac{1}{ {n \choose k}}\mu(\tau),
\]
where 
\begin{equation}\label{mudef}
\mu(\tau) := \sum_{\eta\in\{0,1\}^n; \tau\subseteq\eta} \mu(\eta).
\end{equation}

For any $S\subset V$, observe that $\sum_{\tau:S\rightarrow\{0,1\}} \mu(\tau) = 1$, and hence $\sum_{\tau\in\Pinning_k}\pi_k(\tau)=1$.  Moreover, for $k=n$, $\Pinning_n=\Omega$ and  $\pi_n = \mu$.   (Note that $\pi_1$ is the same as the distribution $\pi$ defined in the proof of \cref{lem:QandPsi} in~\cref{sec:local-influence}.)

It is useful to observe the following basic fact. 
Consider $0\leq k\leq r\leq n$ and $\sigma\in\Pinning_k$ where $\sigma:S\rightarrow \{0,1\}$ for $S\subset V$ where $|S|=k$.
First, observe that
\[
\sum_{\eta\in\Pinning_r: \sigma\subset\eta} \mu(\eta)
 = {n-k \choose r-k}\mu(\sigma).
\]
Hence,
we have the following:
\begin{equation}
\label{matroid:first-step}
\pi_k(\sigma) = 
\frac{\mu(\sigma)}{{n \choose k}} = \frac{1}{{n-k \choose r-k}} \sum_{\eta\in\Pinning_r: \sigma\subset\eta} \frac{\mu(\eta)}{{n \choose k}} 
= \frac{1}{{r \choose k}} \sum_{\eta\in\Pinning_r: \sigma\subset\eta} \frac{\mu(\eta)}{{n \choose r}} = 
{r \choose k}^{-1}\sum_{\eta\in\Pinning_r: \sigma\subset\eta}
\pi_r(\eta).
\end{equation}


We generalize the above definition to a fixed pinning $\eta\in\Pinning$. For $0\leq\ell\leq n$, consider $\eta\in\Pinning_\ell$ where $\eta$ is on $S$ (i.e., for $S\subset V$, $\eta:S\rightarrow\{0,1\}$). 
For $1\leq j\leq n-\ell$, let $\Pinning_{\eta,j}$ be the set of assignments $\tau$ of spins $\{0,1\}$ to a subset $S'\subseteq V\setminus S$ where $|S'|=j$ and $\tau\cup\eta\in\Pinning_{j+\ell}$.

We define the probability distribution for $j$ levels above $\eta$ with respect to the conditional Gibbs distribution $\mu_\eta$.  
Hence, for $j$ where $1\leq j\leq n-\ell$, define $\pi_{\eta,j}$ as follows:
\[ \mbox{for }\tau\in\Pinning_{\eta,j} \mbox{ where } \tau:S'\rightarrow\{0,1\} \mbox{ with } S'\subset V\setminus S, \ \ \ \ \pi_{\eta,j}(\tau) = \frac{1}{ {n-\ell \choose j}}\mu(\sigma(S')=\tau\mid\sigma(S)=\eta).
\]
We will only use the above definitions of $\Pinning_{\eta,j}$ and $\pi_{\eta,j}$ for the case $j=1$, in which case it is closely related to the local walk $Q_\eta$, see \cref{rem:local-downup,rem:second}.


For every $1\leq k\leq n$, we define the following random walks which are known as {\em Up} and {\em Down Operators}.
\begin{description}
\item[Down Walk:] The down-walk is denoted by $\down{k}$.  From $\tau\in \Pinning_k$ we remove an element of $\tau$ chosen uniformly at random.  Note the elements of $\tau$ are (vertex,spin) pair.
Hence, for $(j,s_j)\in\tau$, 
\[ \down{k}(\tau,\tau\setminus(j,s_j))=1/k.
\]
\item[Up Walk:] The up-walk is denoted by $\up{k}$ and corresponds to the following stochastic process from 
$\Pinning_k$ to $\Pinning_{k+1}$. Starting from $\tau \in \Pinning_k$ where $\tau$ is an assignment on $S\subset V, |S|=k$, then choose a random $j\not\in S$ and spin $s_j\in\{0,1\}$ where the probability of picking $(j,s_j)$ is proportional to $\pi_{k+1}(\tau\cup(j,s_j))$. Hence, 
\[ \up{k}(\tau,\tau\cup\{(j,s_j\}) =  
\frac{\pi_{k+1}(\tau\cup(j,s_j))}{(k+1)\pi_k(\tau)} = \frac{\mu(\tau\cup(j,s_j))}{(n-k)\mu(\tau)} = \frac{1}{(n-k)}\mu(\sigma(j)=s_j\mid \sigma(S)=\tau),
\]
where the denominator in the first identity comes from the observation that for $S\subset V, |S|=k$:
\begin{multline*}
    \sum_{\substack{(j',s_{j'}): \\ j'\notin S,s'_j\in\{0,1\}}} \pi_{k+1}(\tau\cup(j',s_{j'})) = \frac{1}{{n \choose k+1}} \sum_{j'\notin S}\sum_{s_{j'}\in\{0,1\}} \mu(\tau\cup(j',s_{j'}))   \\
= \frac{1}{{n \choose k+1}} \sum_{j'\notin S} \mu(\tau) = \frac{n-k}{{n\choose k+1}} \mu(\tau) = (k+1)\pi_k(\tau). 
\end{multline*}
From the definition of the up and down chains, we have that the reversibility condition is satisfied:
\[ \pi_k(\tau)\up{k}(\tau,\tau\cup\{(j,s_j\}) =  
\pi_{k+1}(\tau\cup(j,s_j))\down{k+1}.
\]
	\item[Up-Down Walk:] The up-down walk is denoted by $\updown{k}$ and corresponds to the following Markov chain from $\Pinning_k$ to $\Pinning_k$:
 	\[ \updown{k} = \up{k}\down{k+1}.\]
Consider $\sigma\in\Pinning_{k-1}$ where $\sigma:S\rightarrow\{0,1\}$ for $S\subset V$.  For $i,j\notin S$ and $s_i,s_j\in\{0,1\}$, the transition 
$\updown{k}(\sigma\cup(i,s_i),\sigma\cup(j,s_j))$ operates as follows.
Starting from $\tau=\sigma\cup (i,s_i) \in \Pinning_k$, in the up-step, we choose a random $(j,s_j)$ where the probability of picking $(j,s_j)$ is proportional to $\pi_{k+1}(\tau\cup(j,s_j))$. Then in the down step, we remove an element $(i,s_i)$ chosen uniformly at random from $\tau\cup(j,s_j)$.  More formally, we have the following:
\begin{equation}
\label{eqn:up-down}
    \updown{k}(\sigma\cup(i,s_i),\sigma\cup(j,s_j)) = \frac{\pi_{k+1}(\sigma\cup(i,s_i)\cup(j,s_j))}{(k+1)^2\pi_k(\sigma)}.
\end{equation}
	\item[Down-Up Walk:] The down-up walk is denoted by $\downup{k}$.  From $\tau\in\Pinning_k$, we first remove a uniformly random element $(i,s_i)$ from $\tau$ and then add an element $(j,s_j)$ with probability proportional to the weight of the resulting set $\pi_k(\tau\cup(j,s_j)\setminus(i,s_i))$,
	\[ \downup{k} = \down{k}\up{k-1}.\]
\end{description}

Observe that the stationary distribution of $\updown{k}$ and $\downup{k}$ is $\pi_k$.

\begin{remark}[{\bf Glauber dynamics}]\label{rem:glauber} For a spin system, the down-up chain $\downup{n}$  is equivalent to the Glauber dynamics.  
\end{remark}

The above definitions can be considered as walks on levels of the simplicial complex defined by the Gibbs distribution $\mu$ on $\Omega\subset \{0,1\}^V$ where the ground set of the simplicial complex is the $2n$ pairs of  $(vertex,spin)$ assignments $(v,\sigma(v))\in V\times\{0,1\}$.

Let us illustrate the above definitions for the special case of independent sets (this is the hard-core model with $\lambda=1$); here the Gibbs distribution is uniformly distributed over all independent sets (of any size) of~$G$. The corresponding simplicial complex has dimension $n=|V|$ since for every independent set $I$ we have 
$\{(v,1)\,|\, v\in I\}\cup \{(v,0)\,|\, v\not\in I\}$
in $\Pinning_{n}$.  The Glauber dynamics which updates the spin at a randomly chosen vertex is equivalent to $\downup{n}$.


\begin{remark}[{\bf Local Walk is Non-Backtracking $\updown{1}$}]\label{rem:local-downup} 
Notice that the local walk $Q$ defined in \cref{sec:local-walks}
is similar to the up-down chain.  For simplicity let us first consider the case without a pinning.  Observe that for the up-down chain starting from $(i,s_i)\in\Pinning_1$ the following holds when $i\neq j$:
\begin{align*}
\updown{1}((i,s_i),(j,s_j)) & = 
\up{1}((i,s_i),(i,s_i)\cup(j,s_j))\down{2}
((i,s_i)\cup(j,s_j),(j,s_j))
\\ & =
\frac{\pi_{2}((i,s_i)\cup(j,s_j))}{2\pi_1((i,s_i))}\times\frac{1}{2} 
\\ & = \frac{1}{2}\frac{n}{{n\choose 2}}\times\frac{\mu((i,s_i)\cup(j,s_j))}{2\mu(i,s_i))}
\\ & = \frac{1}{2}\frac{1}{n-1}\times\frac{\mu((i,s_i)\cup(j,s_j))}{\mu(i,s_i))} 
\\ & = \frac{1}{2}Q((i,s_i),(j,s_j)).
\end{align*}
\end{remark}
Observe that $\pi_1$ is the stationary distribution for both $\updown{1}$ and $Q$, and that \cref{rem:local-downup} implies that \[
\updown{1} = (Q + I)/2.
\]
The extra factor $1/2$ comes from the step of the down-walk which drops an element chosen uniformly at random from $(i,s_i),(j,s_j)$, whereas the local walk $Q$ corresponds to the {\em non-backtracking walk} which chooses $(j,s_j)$ to avoid the self-loop.

\begin{remark}
\label{rem:second}
More generally, for any $1\leq k<n$, consider $\eta\in\Pinning_{k-1}$ where $\eta$ is an assignment on $S\subset V$.
Observe that $\pi_{\eta,1}$ is the stationary distribution for the local walk $Q_\eta$.
For $(i,s_i),(j,s_j)\in V\setminus S\times\{0,1\}$, we have the following for $i\neq j$:
\begin{equation}
\label{step:rem:local-downup}
\updown{k}(\eta\cup(i,s_i),\eta\cup(j,s_j)) = \frac{1}{k+1 }Q_\eta((i,s_i),(j,s_j)).
\end{equation}
\end{remark}

Before proceeding, let us verify the above identity \cref{step:rem:local-downup}.  For $i\neq j$,
\begin{align*}
\updown{k}(\eta\cup(i,s_i),\eta\cup(j,s_j)) & = 
\up{k}(\eta\cup(i,s_i),\eta\cup(i,s_i)\cup(j,s_j))
\down{k+1}(\eta\cup(i,s_i)\cup(j,s_j),\eta\cup(j,s_j))
\\ & =
\frac{\pi_{k+1}(\eta\cup(i,s_i)\cup(j,s_j))}{(k+1)\pi_k(\eta\cup(i,s_i))}\times\frac{1}{(k+1)} 
\\ & = \frac{\mu(\eta\cup(i,s_i)\cup(j,s_j))}{(k+1)(n-k)\mu(\eta\cup(i,s_i))} 
\\ & = \frac{1}{(k+1)}Q_\eta((i,s_i),(j,s_j)).
\end{align*}



\subsection{Spectrum of Up-Down Chains}
We begin by pointing out that the spectral gaps for the up-down chain and down-up chain are the same.
\begin{lemma}
    \label{lem:updown-downup}
    \[
    \gamma(\updown{k-1}) = \gamma(\downup{k}).
    \]
    Moreover, they have the same non-zero spectrum of eigenvalues:
    \[
    spectrum_{\neq 0}(\updown{k-1}) = spectrum_{\neq 0}(\downup{k}).
    \]
\end{lemma}

\begin{proof}
Note that $\updown{k-1} = P^\uparrow_{k-1}P^\downarrow_{k}$ and $\downup{k} = P^\downarrow_{k}P^\uparrow_{k+1}$.
    Hence, the lemma follows from the general linear algebra fact that for an $n\times m$ matrix $A$ and $m\times n$ matrix $B$ then the non-zero spectrum of $AB$ is the same as the non-zero spectrum of $BA$.
    For a more detailed proof see \cite[Corollary 3.3.2]{Mousa-thesis}.

\end{proof}

\subsection{Proof Setup}
We can now restate the Random Walk Theorem in terms of the up-down chains. We will prove, for all $2\leq k\leq n$:
\begin{equation}
    \label{eqn:RW-thm}
 \gamma(\downup{k}) \geq \frac{1}{k}\prod_{i=0}^{k-2}\gamma_i.
\end{equation}
Note, the claim trivially holds for $k=1$ as $\gamma(\downup{1})=1$.
Since the Glauber dynamics is equivalent to $\downup{n}$, then the special case of \cref{eqn:RW-thm-simplified} where $k=n$ is \cref{thm:RW}.

\subsection{Key Technical Lemma}
The following lemma will be the key tool in the inductive proof of~\cref{thm:RW}.  
It relates the Dirichlet form for the up-down walk $\updown{k}$ at level $k$ with the down-up walk $\downup{k}$ at level $k$.  Recall that the spectral gap of the $\updown{k}$ is equal to the spectral gap of $\updown{k+1}$ (see~\cref{lem:updown-downup}).  Hence, the lemma is in essence an inductive statement as it relates the chain $\updown{k}$ to $\downup{k}$, rather than $\downup{k+1}$.  In this manner the lemma will immediately yield an inductive proof of the Random Walk Theorem.

\begin{lemma}
\label{lem:technical-RW}
For all $0\leq k< n$, and any $f: \Pinning_k \to\mathbb{R}$, the following holds:
\[
\Dirichlet_{\updown{k}}(f) \geq \frac{k}{k+1}\gamma_{k-1}\Dirichlet_{\downup{k}}(f).
\]
    \end{lemma}

    At an intuitive level, the reader can understand the factor $k/(k+1)$ because in the LHS the down operator is from level $k+1$ to $k$ and hence a transition occurs with probability $1/(k+1)$, whereas in the RHS the down operator is from level $k$ to $k-1$ and hence occurs with probability $1/k$.

We will utilize the following two claims to prove \cref{lem:technical-RW}.  The first claim captures the local-to-global nature of the process.  In particular, for the chain $\updown{k}$ we can express its Dirichlet form in terms of local walks for pinnings at level $k-1$.  Recall that the Dirichlet form for a chain $P$ is the local variation of a functional $f$ over pairs of states connected by a transition of $P$.  

For the chain $\updown{k}$, consider a pair of distinct states $\sigma,\tau$ 
where $\updown{k}(\sigma,\tau)>0$.  Notice that it must be the case that $|\sigma\oplus\tau|=2$ where $\oplus$ is the symmetric difference, i.e., $\sigma\oplus\tau=(\sigma\setminus\tau)\cup(\tau\setminus\sigma)$. This is because in the up-step we can add one element $(v_i,s_i)$ to $\sigma$ and then in the down-step we can remove one element $(v_j,s_j)$ from $\sigma\cup(v_i,s_i)$.  Hence, $\sigma\cap\tau=\eta$ where $\eta\in\Pinning_{k-1}$.  Then the transition 
$\updown{k}(\sigma,\tau)$ is closely related to the transition $Q_{\eta}((v_i,s_i),(v_j,s_j))$, which  yields the following identity.

\begin{claim}For all $0< k\leq n$, and all $f: \Pinning_k \to\mathbb{R}$,
    \label{claim:AAA}
   \[    \Dirichlet_{\updown{k}}(f)  =
    \frac{k}{k+1}\sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \Dirichlet_{Q_{\eta}}(f_\eta), \]
where for $\eta:S\rightarrow\{0,1\}$ and $(j,s_j)\in (V\setminus S)\times\{0,1\}$, then $f_\eta((j,s_j)) = f(\eta\cup(j,s_j))$.
\end{claim}

The second claim works in the reverse manner and relates the variance on level $1$ conditional on $\eta\in\Pinning_{k-1}$ to the global chain $\downup{k}$.

\begin{claim}For all $0< k\leq n$, and all $f: \Pinning_k\to\mathbb{R}$,
\label{claim:DDD}
\[
   \Dirichlet_{\downup{k}}(f) = \sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \Var_{\pi_{\eta,1}}(f_\eta).
   \]
\end{claim}


Using these two claims it is straightforward to prove the key technical lemma.

\begin{proof}[Proof of \cref{lem:technical-RW}]
For $1\leq k\leq n$, and $\eta\in\Pinning_{k-1}$ we know that     $\gamma(Q_\eta) \geq \gamma_{k-1}$,
    and hence since $\pi_{\eta,1}$ is the stationary distribution for $Q_\eta$ then for any 
    $g:(V\setminus S)\times\{0,1\}\rightarrow\mathbb{R}$
    we have that 
    \begin{equation}
        \label{eq:DDD}
    \Dirichlet_{Q_{\eta}}(g)\geq \gamma_{k-1}\Var_{\pi_{\eta,1}}(g).
    \end{equation}

We can now prove the lemma:
\begin{align*}
\nonumber
    \Dirichlet_{\updown{k}}(f)
&=
    \frac{k}{k+1}\sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \Dirichlet_{Q_{\eta}}(f_\eta)
   & \mbox{by \cref{claim:AAA}} \\
    &\geq 
    \frac{k}{k+1}\gamma_{k-1} \sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \Var_{\pi_{\eta,1}}(f_\eta)
       & \mbox{by \cref{eq:DDD} with $g=f_\eta$}
       \\
      &=
   \frac{k}{k+1}\gamma_{k-1} \Dirichlet_{\downup{k}}(f). 
      & \mbox{by \cref{claim:DDD}} \\
    \end{align*}
 
\end{proof}


\begin{proof}[Proof of \cref{claim:AAA}] 
As a warm-up let us consider the simpler case where $k=1$.  Recall that $\pi_1$ is the stationary distribution for $\updown{1}$ and $Q$.  In \cref{rem:local-downup} we showed that for $(i,s_i),(j,s_j)\in V\times\{0,1\}$ where $i\neq j$, \[ \updown{1}((i,s_i),(j,s_j))=\frac{1}{2}Q((i,s_i),(j,s_j)).
    \]
    For any $f:\Pinning_1\to\mathbb{R}$ (which is $f:V\times\{0,1\}\rightarrow\mathbb{R}$), 
    we have the following:
       \begin{align*}
           \Dirichlet_{\updown{1}}(f)  & =
           \frac{1}{2}\sum_{(i,s_i),(j,s_j)} \pi_1((i,s_i))\updown{1}((i,s_i),(j,s_j))(f((i,s_i))- f((j,s_j)))^2 \\
           & = \frac{1}{4}\sum_{(i,s_i),(j,s_j)} \pi_1((i,s_i))Q((i,s_i),(j,s_j))(f((i,s_i))- f((j,s_j)))^2 \\
           & = \frac{1}{2}\Dirichlet_{Q}(f),
           \end{align*}
           where the summation is over $(i,s_i),(j,s_j)\in V\times\{0,1\}$. 

    Since $\Pinning_{0}=\{\emptyset\}$, this proves the claim for the case $k=1$.

Before proving the general claim, consider the following observation.
For $\eta\cup a\in\Pinning_k$, note that:
\begin{equation}
    \label{step:claim1}
\pi_k(\eta\cup a) = \frac{1}{{n\choose k}}\mu(\eta\cup a) = \frac{k}{(n-k+1)}\pi_{k-1}(\eta)\mu_\eta(a) = k\pi_{k-1}(\eta)\pi_{\eta,1}(a).
\end{equation}

In general, for $k\geq 1$ we have the following for any $f:\Pinning_k\to\mathbb{R}$:
      \begin{align*}
           \Dirichlet_{\updown{k}}(f)  & =
           \frac{1}{2}\sum_{\eta\in\Pinning_{k-1}}\sum_{\substack{a,b\in V\setminus S\times\{0,1\}: \\ \eta:S\rightarrow \{0,1\}}} \pi_k(\eta\cup a)\updown{k}(\eta\cup a,\eta\cup b)(f(\eta\cup a)- f(\eta\cup b))^2 \\
        & =
           \frac{1}{2(k+1)}\sum_{\eta\in\Pinning_{k-1}}\sum_{a,b} \pi_k(\eta\cup a)Q_\eta(a,b)(f_\eta(a)- f_\eta(b))^2 & \mbox{by \cref{step:rem:local-downup}}
           \\
                        & =
           \frac{k}{2(k+1)}
           \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{a,b} \pi_{\eta,1}(a)Q_\eta(a,b)(f_\eta(a)- f_\eta(b))^2 & \mbox{by \cref{step:claim1}}
           \\
           & = \frac{k}{k+1}\sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\Dirichlet_{Q_\eta}(f_\eta).
           \end{align*}
\end{proof}



   \begin{proof}[Proof of \cref{claim:DDD}]
\begin{align*}
  \sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \Var_{\pi_{\eta,1}}(f_\eta)
               &= 
    \frac{1}{2}\sum_{\eta\in\Pinning_{k-1}} \pi_{k-1}(\eta) \sum_{\substack{a,b\in V\setminus S\times\{0,1\}:\\ \eta:S\rightarrow\{0,1\}}} \pi_{\eta,1}(\eta\cup a)\pi_{\eta,1}(\eta\cup b)(f_\eta(a)-f_\eta(b))^2
    \\
                       &= 
   \frac{1}{2}\sum_{\eta,a,b} \pi_{k-1}(\eta)\frac{\mu_\eta(a)}{(n-k+1)}\times\frac{\mu_\eta(b)}{(n-k+1)}(f(\eta\cup\{a\})-f(\eta\cup\{b\}))^2
   \\
   &= 
   \frac{1}{2}\sum_{\eta,a,b} \pi_{k}(\eta\cup\{a\})\times\frac{1}{k}\times\frac{\mu_\eta(b)}{(n-k+1)}(f(\eta\cup\{a\})-f(\eta\cup\{b\}))^2
      \\
      &= 
   \frac{1}{2}\sum_{\eta,a,b} \pi_{k}(\eta\cup\{a\})P^{\downarrow}_{k}(\eta\cup\{a\},\eta)P^{\uparrow}_{k-1}(\eta,\eta\cup\{b\})(f(\eta\cup\{a\})-f(\eta\cup\{b\}))^2
      \\
      &=
   \Dirichlet_{\downup{k}}(f).
    \end{align*}
   \end{proof}



\subsection{Inductive Proof of Random Walk Theorem: Proof of \cref{thm:RW}}
We can now prove \cref{eqn:RW-thm}, which implies the Random Walk Theorem (\cref{thm:RW}).

\begin{proof}[Proof of \cref{thm:RW}]

Our goal is to prove by induction that for all $2\leq\ell\leq n$:
\begin{equation}
    \label{eqn:ind-hyp-RW}
\gamma(\downup{\ell}) \geq \frac{1}{\ell}\prod_{i=0}^{\ell-2}\gamma_i.
\end{equation}
This is equivalent to the following statement in terms of the Dirichlet form and variance:
\begin{equation}
\label{eqn:ind-hyp-Dir-downup}
\forall f:\Omega\rightarrow\R, \ \  \Dirichlet_{\downup{\ell}}(f)  \geq \frac{1}{\ell}\prod_{i=0}^{\ell-2}\gamma_i\Var_{\pi_{\ell}}(f).
\end{equation}
Recall $\gamma(\updown{\ell-1})=\gamma(\downup{\ell})$ was established in \cref{lem:updown-downup}.  Hence, \cref{eqn:ind-hyp-RW} 
is equivalent to the following statement:
\begin{equation}
\label{eqn:ind-hyp-Dir-updown}
\forall f:\Omega\rightarrow\R, \ \ \Dirichlet_{\updown{\ell-1}}(f)  \geq \frac{1}{\ell}\prod_{i=0}^{\ell-2}\gamma_i\Var_{\pi_{\ell-1}}(f) 
\end{equation}
We will prove \cref{eqn:ind-hyp-Dir-updown} by induction and use \cref{eqn:ind-hyp-Dir-downup} in the inductive step.

Now let us assume the inductive hypothesis \cref{eqn:ind-hyp-RW} for all $\ell<k$ and we will establish it for $\ell=k$.
\begin{align*}
\Dirichlet_{\updown{k-1}}(f) 
& \geq 
\frac{(k-1)\gamma_{k-2}}{k}\Dirichlet_{\downup{k-1}}(f)
&\mbox{by \cref{lem:technical-RW}}
\\
&\geq 
\frac{(k-1)\gamma_{k-2}}{k}\times\frac{1}{k-1}\prod_{i=0}^{k-3}\gamma_i\Var_{\pi_{k-1}}(f)
&\mbox{by \cref{eqn:ind-hyp-Dir-downup} for $\ell=k-1$}
\\
& = \frac{1}{k}\prod_{i=0}^{k-2}\gamma_i\Var_{\pi_{k-1}}(f),
\end{align*}
which establishes \cref{eqn:ind-hyp-Dir-updown} (and hence \cref{eqn:ind-hyp-RW}) by induction.
\end{proof}

\section{Optimal Spectral Gap for Linear-Blocks}
\label{section-proof-relax}

The goal of this section is to prove that when the system is $\eta$-spectrally independent, then the relaxation time of the Glauber dynamics is $O(n)$, thereby proving~\cref{thm:SI-constant-relax} of~\cite{CLV21}.
Our proof follows the same approach as in~\cite{CLV21}.

\subsection{Uniform Block Dynamics}
Consider the following dynamics, parameterized by $0<\alpha<1$, which we will refer to as the {\em $\alpha n$-uniform block dynamics}.  The dynamics updates $\alpha n$ random vertices $S$ in each step.     Assume $\alpha n$ is an integer.  From $Y_t\in\Omega$, the transitions $Y_t\rightarrow Y_{t+1}$ of the $\alpha n$-uniform block dynamics are as follows:
\begin{enumerate}
    \item 
    Select $\alpha n$ random vertices, chosen uniformly at random from all ${n \choose \alpha n}$ vertices.   Let $S$ denote the chosen set.
    \item For all $w\notin S$, let $Y_{t+1}(w)=Y_t(w)$.
    \item Sample $Y_{t+1}(S)$ from the conditional Gibbs distribution $\mu(\sigma(S)\mid \sigma(w)=Y_{t+1}(w) \mbox{ for all } w\notin S).$
\end{enumerate}

Note the $\alpha n$-uniform block dynamics is identical to $\downup{n,(1-\alpha)n}$.  Using the Random Walk Theorem framework we will first prove fast mixing of the $\alpha$-block dynamics.  In particular we will show that the relaxation time is $O(1)$ where the constant in the big-$O()$ notation is a function of $1/\alpha$.  
We will then show that this implies fast mixing of the Glauber dynamics when $\alpha=O(1/\Delta)$ where $\Delta$ is the maximum degree of the graph.  

\subsection{Improved Random Walk Theorem}
The Random Walk Theorem is not strong enough to establish fast mixing of the $\alpha n$-uniform block dynamics, we will need the following improved local-to-global theorem which was presented by \cite[Theorem 5.4]{CLV21} and independently \cite[Theorem 4]{GM20}.  For simplicity we only present the spectral gap version, the stronger version for entropy contraction is presented in~\cite{CLV21,GM20}.

For $0<i\leq n$, let \[ \Gamma_i := \prod_{j=0}^{i-1}(2\gamma_j-1),
\] and let $\Gamma_0=1$.
Note that since, for all $j$, $\gamma_j\leq 1$ as the spectral gap is at most $1$, then $(2\gamma_j-1)\leq 1$ and hence $\Gamma_i\leq 1$ for all $i$.  

We will prove, for $0<k\leq n$, \begin{equation}
    \label{eqn:RW-one-improved}
 \gamma(\downup{k}) \geq \frac{\Gamma_{k-1}} {\sum_{i=0}^{k-1} \Gamma_i} 
\end{equation}

The above inequality will be a special case of the following improvement of the Random Walk Theorem. 
\begin{theorem}[Improved RW Theorem {\cite[Theorem A.9]{CLV21},\cite[Theorem 4]{GM20}}]
\label{lem:impr-RW-thm} 
    For $0\leq\ell<n$, 
    \begin{equation}
    \label{eqn:RW-improved-general}
 \gamma(\downup{n,\ell}) \geq \frac{\sum_{i=\ell}^{n-1} \Gamma_i}{\sum_{i=0}^{n-1} \Gamma_i} 
\end{equation}
\end{theorem}

Moreover, one could further improve the above result by replacing $(2\gamma_j-1)$ by $\gamma_j/(2-\gamma_j)$ in the definition of $\Gamma_i$; see the proof of~\cref{lem:improved-technical} for details. 

Before proving the above theorem let us first compare it to the Random Walk Theorem (\cref{thm:RW}) stated earlier.
Looking at \cref{eqn:RW-one-improved} for $k=n$ (or \eqref{eqn:RW-improved-general} for $\ell=n-1$) we have:
\[
 \gamma(\downup{n}) \geq \frac{\Gamma_{n-1}}{\sum_{i=0}^{n-1}\Gamma_i} = \frac{\prod_{j=0}^{n-1}(2\gamma_j-1)}{\sum_{i=0}^{n-1}\Gamma_i} \geq 
\frac{1}{n} \prod_{j=0}^{n-1}(2\gamma_j-1)  \ \ \ \mbox{ since $\Gamma_i\leq 1$.}
\]
In contrast, \cref{thm:RW} has $2\gamma_j-1$ replaced by $\gamma_j$ in the lower bound on the spectral gap of $\downup{n}$.



\subsection{Fast Mixing of Uniform Block Dynamics}
Applying \cref{lem:impr-RW-thm} we establish fast mixing of the $\alpha n$-uniform block dynamics.  

\begin{lemma}\label{lem:gap-global-block}
For all $\eta>0$ and $0<\alpha<1$ there exists a constant $C=C(\eta,\alpha)$ such that if the system is $\eta$-spectral independence then the spectral gap of the $\alpha n$-uniform block dynamics is $\gamma(\downup{n,(1-\alpha)n}) \geq C$.
\end{lemma}

\begin{proof}
We begin with a useful lower bound on $\Gamma_{(1-\alpha/2)n}$:
\begin{multline}   
\Gamma_{(1-\alpha/2)n}
= \prod_{i=0}^{(1-\alpha/2)n-2}(2\gamma_i-1)
\geq \prod_{i=0}^{(1-\alpha/2)n-1}
\left(1-\frac{2\eta}{n-i-1}\right)
\geq \left(1 - \frac{4\eta}{\alpha n}\right)^n
\geq \exp(-8\eta/\alpha),
\label{eqn:alpha-bound} 
\end{multline}
for $n\geq 8\eta/\alpha$, where the last inequality uses that $1-x\geq \exp(-2x)$ for $x\leq 1/2$.

We can now proceed to lower bound the spectral gap of the $\alpha n$-uniform block dynamics:
    \begin{align*}
    \label{gap:global-block}
 \gamma(\downup{n,(1-\alpha)n}) 
&\geq
\frac{\sum_{i=(1-\alpha)n}^{n-1} \Gamma_i}{\sum_{i=0}^{n-1} \Gamma_i} 
 & \mbox{by \cref{eqn:RW-improved-general}}
 \\
 &\geq
\frac{\sum_{i=(1-\alpha)n}^{(1-\alpha/2)n} \Gamma_i}{n} 
 & \mbox{since $\Gamma_i\leq 1$}
 \\
   &\geq
 \frac{(\alpha/2)n\Gamma_{(1-\alpha/2)n}}{n} 
 & \mbox{since $\Gamma_i\geq\Gamma_{i+1}$}
 \\
    &\geq
 (\alpha/2)\exp(-8\eta/\alpha) 
 & \mbox{by \cref{eqn:alpha-bound}}
 \\
 & = C(\alpha,\eta).
\end{align*}
\end{proof}


\subsection{Shattering}
\label{sec:shattering}

We established fast mixing of the $\alpha n$-uniform block dynamics, see \cref{lem:gap-global-block}, which updates $\alpha n$ random vertices in each step.  We will use that to establish fast mixing of the Glauber dynamics.  

The first step is to look at the properties of the updated vertices in a step of the block dynamics.  In particular, \cite{CLV21} showed that a random subset of $\alpha n$ vertices is ``shattered'' in the sense that the expected size of each component is $O(1)$ when $\alpha<1/(6\Delta)$ where $\Delta$ is the maximum degree of the graph $G$.
The shattering occurs because $\alpha\Delta<1$ and hence this corresponds to a sub-critical branching process.

For a subset $S\subset V$, let $\CC_S$ denote the collection of connected components in the induced subgraph on $S$, and for $v\in V$, let $C_v\in \CC_S$ denote the component containing $v$.

\begin{lemma}{\cite[Lemma 4.3]{CLV21}} 
\label{lem:shattering}
For a graph $G=(V,E)$ of maximum degree $\Delta$, for $\alpha>0$, choose a random subset $S\subset V$ where $|S|=\alpha n$.  Then for every integer $k\geq 1$, for every $v\in V$,
\[ \Prob{|T_v|=k} \leq \alpha(6\Delta\alpha)^{k-1},\]
where $T_v$ is the component containing $v$ in $\CC_v$, which is the induced subgraph on $S$.
\end{lemma}

\begin{proof}

See the proof of Lemma 4.3 on page 22 of \cite{CLV21}.
\end{proof}

\subsection{Optimal Relaxation Time of Glauber: Proof of \cref{thm:SI-constant-relax}}
\label{sec:proof-relax}

Using the above shattering result (\cref{lem:shattering}) with the fast mixing result for the $\alpha n$-uniform block dynamics (\cref{lem:gap-global-block}), we can prove optimal upper bound on the relaxation time of the Glauber dynamics and hence prove~\cref{thm:SI-constant-relax}.

\begin{proof}[Proof of \cref{thm:SI-constant-relax}]
In the following, for a subset $S\subset V$, we use $\HBt(S)$ to denote the heat-bath block dynamics on $S$;
this dynamics updates the configuration on all of $S$ in one step from the Gibbs distribution conditional on the fixed configuration $\tau$ on $\overline{S}$.
Similarly, we use $\Glaubert$ to denote the Glauber dynamics on a set $S$ with a fixed configuration $\tau$ on $\overline{S}$; in particular, for $v\in S$, with probability $1/|S|$ we update the configuration at $v$ from the Gibbs distribution conditional on the fixed configuration for the other vertices. 

For a function $f:\Omega\rightarrow\R$, we will consider the conditional variance.  For $\sigma\sim\mu$, let \[ F = f(\sigma). \]
Note that when we write $\Var(f)$ we are implicitly writing $\Var[F]$ since we first draw a sample $\sigma$ from the Gibbs distribution $\mu$ and then look at the variance of $F=f(\sigma)$.  

Consider a configuration $\tau\in\Omega$, and a subset $S\subset V$.  In the upcoming analysis we will consider the variance of $f$ within $S$ where we fix $\tau$ on $\overline{S}=V\setminus S$. We denote this as:
\[ \Var_S[F\mid\tau] := \Var[F\mid \sigma(\overline{S})=\tau(\overline{S})].
\]
Once again, in words $\Var_S[F\mid \tau]$ is the variance of the function $f$ over the choice of $\sigma$ from the Gibbs distribution $\mu$ conditional on $\sigma(\overline{S})=\tau(\overline{S})$.

Again fix $\tau\in\Omega$ and $S\subset V$.  Consider the heat-bath dynamics on $S$ with a fixed $\tau$ on $\overline{S}$.  The dynamics chooses the configuration on $S$ from $\mu_{\tau(\overline{S})}$, which is the Gibbs distribution $\mu$ conditional on the configuration on $\overline{S}$ being $\tau(\overline{S})$.  Hence,
\begin{equation}
\label{eqn:Cond-Var-Dirichlet}
  \Dirichlet_{\HBt(S)}(f) 
 = \Var[F\mid \sigma(\overline{S})=\tau(\overline{S})]  
\end{equation}
Moreover, if $S=\{v\}$ for $v\in V$ then this is heat-bath on the single vertex $v$.  The Glauber dynamics operates by choosing a random vertex $v$ and then applying the heat-bath dynamics on $v$.  Hence, we have the following:
\begin{align}
\nonumber
  \Dirichlet_{\Glauber}(f) 
 & = \frac{1}{2}\sum_{\tau\in\Omega}\sum_{\sigma\in\Omega}\mu(\tau)\Glauber(\tau,\sigma)(f(\tau)-f(\sigma))^2
 \\
 \nonumber
  & = \sum_{\tau\in\Omega}\mu(\tau)\frac{1}{n}\sum_{v\in V}\left[\frac12\sum_{\substack{\sigma\in\Omega:\\ \sigma(V\setminus\{v\})=\tau(V\setminus\{v\})}}\mu(\sigma)(f(\tau)-f(\sigma))^2\right]
 \\
 & = \frac{1}{n}\sum_{\tau\in\Omega}\mu(\tau)\Var[F\mid \sigma(V\setminus\{v\})=\tau(V\setminus\{v\})]  
 \label{eqn:Glauber-Cond-Var}
\end{align}

Recall, for a subset $S\subset V$, $\CC_S$ denotes the collection of connected components in the induced subgraph on $S$.

For any function $f:\Omega\rightarrow\R$,
\begin{align*}
\Var(f) 
& \leq
\frac{1}{C}
\Dirichlet_{\downup{n,(1-\alpha)n}}(f)
& \mbox{by \cref{lem:gap-global-block}}
\\
& =
\frac{1}{C}\frac{1}{{n \choose \alpha n}}\sum_{\substack{S\subset V: \\ |S|=\alpha n}} 
\sum_{\tau\in\Omega} \mu(\tau)\Dirichlet_{\HBt(S)}(f)
& \mbox{(see below)}
\\
& =
\frac{1}{C}\frac{1}{{n \choose \alpha n}}\sum_{\substack{S\subset V: \\ |S|=\alpha n}} 
\sum_{\tau\in\Omega} \mu(\tau)\Var_S[F\mid \tau]
& \mbox{by \cref{eqn:Cond-Var-Dirichlet}}
\\
& =
\frac{1}{C}\frac{1}{{n \choose \alpha n}}\sum_{\substack{S\subset V: \\ |S|=\alpha n}} 
\sum_{\tau\in\Omega} \mu(\tau) \sum_{T\in\CC_S}\Var_T[F\mid \tau]
& \mbox{by independence of $T,T'\in\CC_S$}
\\
& \leq 
 \frac{1}{C}\frac{1}{{n \choose \alpha n}}\sum_{\substack{S\subset V: \\ |S|=\alpha n}} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{T\in\CC_S}
  C'|T|^{\eta+1}\sum_{v\in T}\Dirichlet_{\Glaubert(T)}(f)
& \mbox{by \cref{thm:SI-mixing}, (see below)}
\\
& =
 \frac{1}{C}\frac{1}{{n \choose \alpha n}}\sum_{\substack{S\subset V: \\ |S|=\alpha n}} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{T\in\CC_S}
  C'|T|^{\eta+1}\frac{1}{|T|}\sum_{v\in T}\Var_{v}[F\mid \tau]
& \mbox{by \cref{eqn:Glauber-Cond-Var}}
\\
& =
 \frac{1}{C} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{v\in V}\Var_{v}[F\mid \tau]
 \sum_{k=1}^{\alpha n}\Prob{|T_v|=k}
\times C'k^{\eta-1}
& \mbox{rearranging}
\\
& \leq 
 \frac{1}{C} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{v\in V}\Var_{v}[F\mid \tau]
 \sum_{k=1}^{\alpha n}
 C'(6\Delta\alpha)^{k-1}k^{\eta-1}
  & \mbox{by \cref{lem:shattering}}
\\
& \leq 
 \frac{1}{C} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{v\in V}\Var_{v}[F\mid \tau]
 \sum_{k=1}^{\alpha n}
 C' 2^{-k}
 & \mbox{for $\alpha<\exp(-\eta)/(100\Delta)$}
  \\
& \leq 
 \frac{1}{C''} \sum_{\tau\in\Omega} \mu(\tau) 
 \sum_{v\in C}\Var_{v}[F\mid \tau]
\\
& =
\frac{n}{C''}\Dirichlet_{\Glauber}(f)
 & \mbox{by \cref{eqn:Glauber-Cond-Var}}
\end{align*}
In the second line we are implementing the $(n,(1-\alpha)n)$ down-up dynamics by first choosing the subset $S$ of size $\alpha n$ for update and then applying the heat-bath dynamics on this set $S$ with those vertices outside $S$ fixed to $\tau$ chosen from the Gibbs distribution $\mu$.
And in the fifth line where we are applying \cref{thm:SI-mixing}, note we are considering the Glauber dynamics which only operates on the vertices in this component $T$, where the vertices outside $T$ have the fixed configuration $\tau$.

This proves that the spectral gap of the Glauber dynamics is $\geq C''/n$ for a constant $C''=C''(\alpha,\Delta)$.  Hence, the relaxation time of the Glauber dynamics is $O(n)$ as claimed.
\end{proof}


\subsection{Improved Random Walk Theorem: Proof of \cref{lem:impr-RW-thm}}

In this section we will prove the improved Random Walk Theorem presented in~\cref{lem:impr-RW-thm}.
The proof will involve statements comparing Dirichlet forms and variance on different levels and hence we will need to define the projection of a function $f$ on lower levels.  

Consider an arbitrary function $f:\Omega\rightarrow\R$.  Recall $\Pinning_n=\Omega$, and let $f^{(n)}=f$.  We will define $f^{(k)}$ for $0\leq k<n$ in the following inductive manner.

For $f^{(k+1)}:\Pinning_{k+1}\rightarrow\R$, let 
\[ f^{(k)}=\up{k}f^{(k+1)}.
\]
Hence, for $\sigma\in\Pinning_{k}$ we have that $f^{(k)}(\sigma) = \sum_{\tau\in\Pinning_{k+1}}\up{k}(\sigma,\tau)f^{(k+1)}(\tau)$.

We will use the two identities involving the (global) variance and the local variance, measured by the Dirichlet form.  The following is an interesting and useful identity for relating the Dirichlet form for the down-up walk from level $i$ to $j$ in terms of the difference of the variance at levels $i$ and $j$.

\begin{lemma}
    \label{lem:diff-var}
For all $n\geq i>j\geq 0$,  for all $f^{(i)}:\Pinning_i\rightarrow \R$, the following holds:
\begin{equation}
   \label{eqn:general-basic-fact}
    \Dirichlet_{\downup{i,j}}(f^{(i)}) = \Var_{\pi_i}(f^{(i)}) - \Var_{\pi_{j}}(f^{(j)}).
\end{equation}
\end{lemma}

In the proof of the Random Walk Theorem (\cref{eqn:RW-thm}), the key technical inequality stated in \cref{lem:technical-RW} was the following:
\[
\Dirichlet_{\updown{k}}(f) \geq \frac{k}{k+1}\gamma_{k-1}\Dirichlet_{\downup{k}}(f).
\]

For the improved result we will use the following variant.  Recall, $f^{(k)}=\up{k}f^{(k+1)}$.

\begin{lemma}
    \label{lem:improved-technical}
For any $f^{(k+1)}:\Pinning_{k+1}\rightarrow\R$, the following holds:
\begin{equation}\label{eqn:NEW-D}
\Dirichlet_{\downup{k+1}}(f^{(k+1)}) \geq (2\gamma_{k-1}-1)\Dirichlet_{\downup{k}}(f^{(k)}).
\end{equation}
\end{lemma}

We defer the proofs of \cref{lem:diff-var,lem:improved-technical} to~\cref{sec:missing}, and we proceed with the proof of~\cref{lem:impr-RW-thm}.

\begin{proof}[Proof of \cref{lem:impr-RW-thm}]
We begin by proving \cref{eqn:RW-one-improved} which states that the spectral gap of $\downup{k}$ is at least $\Gamma_{k-1}/(\sum_{i=0}^{k-1}\Gamma_i$ where $\Gamma_i=\prod_{j=0}^{i-1}(2\gamma_{j}-1)$.  We will then use this result for the one-level down-up walk to derive the more general statement in \cref{lem:impr-RW-thm} for the down-up walk $\downup{n,\ell}$.

Our inductive hypothesis is the following statement about the spectral gap of $\downup{k}$:
\begin{equation}
\label{induct:simpler}
\left(\sum_{i=0}^{k-1}\Gamma_i\right)\Dirichlet_{\downup{k}}(f^{(k)}) 
 \geq 
\Gamma_{k-1}\Var_{\pi_{k}}(f^{(k)}).
\end{equation}


We can now complete the proof by induction as follows:
\begin{align*}
\lefteqn{
\left(\sum_{i=0}^{k}\Gamma_i\right)\Dirichlet_{\downup{k+1}}(f^{(k+1)}) 
} \hspace{.3in}
\\& = 
\Gamma_{k}\Dirichlet_{\downup{k+1}}(f^{(k+1)}) + \left(\sum_{i=0}^{k-1}\Gamma_i\right)\Dirichlet_{\downup{k+1}}(f^{(k+1)})
\\
& \geq 
\Gamma_{k}\Dirichlet_{\downup{k+1}}(f^{(k+1)}) + \left(\sum_{i=0}^{k-1}\Gamma_i\right)(2\gamma_{k-1}-1)\Dirichlet_{\downup{k}}(f^{(k)})
&\mbox{by \cref{eqn:NEW-D}}
\\
& \geq 
\Gamma_{k}\Dirichlet_{\downup{k+1}}(f^{(k+1)}) + \Gamma_{k-1}(2\gamma_{k-1}-1)\Var_{\pi_{k}}(f^{(k)})
&\mbox{by inductive hypothesis}
\\
& =
\Gamma_{k}\Dirichlet_{\downup{k+1}}(f^{(k+1)}) + \Gamma_{k}\Var_{\pi_{k}}(f^{(k)})
&\mbox{since } \Gamma_{k}=\gamma_{k-1}\Gamma_{k-1}
\\
& =
\Gamma_{k}\Var_{\pi_{k+1}}(f^{(k+1)})
&\mbox{by \cref{lem:diff-var} with $i=k+1,j=k$.}
\end{align*}
This completes the proof of \cref{eqn:RW-one-improved}.

Let us now prove the more general statement in~\cref{eqn:RW-improved-general}.
Establishing the bound stated in \cref{eqn:RW-improved-general} on the spectral gap of $\downup{n,\ell}$ is equivalent to proving the following for all $0<\ell<n$ and all $f:\Omega\rightarrow\R$:
\begin{equation}
    \label{induct:RW-improved-general}
\left(\sum_{i=0}^{n-1} \Gamma_i\right) \Dirichlet_{\downup{n,\ell}}(f) \geq 
\left( \sum_{i=\ell}^{n-1} \Gamma_i \right)\Var_{\pi_{n}}(f).
\end{equation}

Now we can prove \cref{induct:RW-improved-general} using \cref{induct:simpler}. 
From \cref{lem:diff-var} we have that $\Dirichlet_{\downup{k}}(f^{(k)}) = \Var_{\pi_k}(f^{(k)}) - \Var_{\pi_{k-1}}(f^{(k-1)})$, and hence \cref{induct:simpler} is equivalent to the following:
\begin{equation}
\label{induct:AAA-simpler}
\frac{\Var_{\pi_{k}}(f^{(k)}) }
{\left(\sum_{i=0}^{k-1}\Gamma_{i}\right)}
 \geq 
\frac{\Var_{\pi_{k-1}}(f^{(k-1)})}{\left(\sum_{i=0}^{k-2}\Gamma_i\right)}.
\end{equation}
Then by chaining these inequalities (with $f=f^{(n)}$ we obtain:
\begin{equation}
\label{induct:GGG-simpler}
\frac{\Var_{\pi_{n}}(f) }
{\left(\sum_{i=0}^{n-1}\Gamma_{i}\right)}
 \geq 
\frac{\Var_{\pi_{\ell}}(f^{(\ell)})}{\left(\sum_{i=0}^{\ell-1}\Gamma_i\right)}.
\end{equation}
Finally, applying \cref{lem:diff-var} with $i=n,j=\ell$, we have that \cref{induct:GGG-simpler} is equivalent to \cref{induct:RW-improved-general}, which completes the proof of~\cref{lem:impr-RW-thm}.
\end{proof}



\subsection{Proofs of Basic Facts for Dirichlet Form and Variance}
\label{sec:missing}

\subsubsection{Proof of \cref{lem:diff-var}}

\begin{proof}[Proof of \cref{lem:diff-var}]
 Without loss of generality we can assume that $\Exp_{\pi_i}[f^{(i)}] = \Exp_{\sigma\sim\pi_i}[f^{(i)}(\sigma)]=0$ and hence:
\begin{equation}
    \label{eqn:Var-A}
    \Var_{\pi_i}(f^{(i)}) = \sum_{\sigma\in\Pinning_i}\pi_i(\sigma)f^{(k)}(\sigma)^2.
\end{equation}

Since we have $\Exp_{\pi_i}[f^{(i)}]=0$, it follows that $\Exp_{\eta\sim\pi_{i-1}}[f^{(i-1)}(\eta)]=
\Exp_{\sigma\sim\pi_i}[f^{(i)}(\sigma)]=0$ and hence by induction we have that 
$\Exp_{\eta\sim\pi_{j}}[f^{(j)}(\eta)]=0$.

Now we can consider the variance at level $j$:
\begin{align}
\nonumber
    \Var_{\pi_j}(f^{(j)}) &= \sum_{\eta\in\Pinning_j}\pi_j(\eta)f^{(j)}(\eta)^2 
    \\
    \nonumber
    &= \sum_{\eta\in\Pinning_j}\pi_j(\eta)
    \left[\sum_{\sigma\in\Pinning_i}\up{j,i}(\eta,\sigma)f^{(i)}(\sigma)\right]^2 
    \\
    \nonumber
    & =
\sum_{\eta\in\Pinning_j}\pi_j(\eta)\sum_{\tau_1,\tau_2}
\up{j,i}(\eta,\eta\cup\tau_1)f^{(i)}(\eta\cup\tau_1)\up{j,i}(\eta,\eta\cup\tau_2)f^{(i)}(\eta\cup\tau_2)
\\
    \nonumber
    & =
\sum_{\eta\in\Pinning_j}\pi_j(\eta)\sum_{\tau_1,\tau_2}
\frac{\pi_i(\eta\cup\tau_1)\pi_i(\eta\cup\tau_2)}{(\pi_j(\eta)\times (j+1)\times\dots\times n)^2 }f^{(i)}(\eta\cup\tau_1)f^{(i)}(\eta\cup\tau_2)
    \nonumber
    \\
    & =
\sum_{\substack{\sigma_1,\sigma_2\in\Pinning_i: \\ \sigma_1\cap\sigma_2=\eta\in\Pinning_j}}
\frac{\pi_i(\sigma_1)\pi_i(\sigma_2)}{\pi_j(\eta)\times (j+1)\times\dots\times n)^2 }f^{(i)}(\sigma_1)f^{(i)}(\sigma_2)
   \nonumber
   \\
    & =
\sum_{\sigma_1,\sigma_2\in\Pinning_i}
\pi_i(\sigma_1)\downup{i,j}(\sigma_1,\sigma_2)f^{(i)}(\sigma_1)f^{(i)}(\sigma_2)
       \label{eqn:Var-B}
\end{align}
in the third line, let $S$ denote the set of vertices where $\eta$ is an assignment $\eta:S\rightarrow\{0,1\}$, then $\tau_1,\tau_2$ are assignments $\tau_1:T_1\rightarrow\{0,1\}$ and $\tau_2:T_2\rightarrow\{0,1\}$ where $T_1,T_2\subset V\setminus S, |T_1|=|T_2|=i-j$ and $T_1\cap T_2=\emptyset$. 

We can complete the proof by  decomposing the Dirichlet form of the down-up walk into two terms as follows:
\begin{align*}
 \Dirichlet_{\downup{i,j}}(f^{(i)}) 
  &= \frac12\sum_{\sigma_1,\sigma_2\in\Pinning_i}\pi_i(\sigma_1)\downup{i,j}(\sigma_1,\sigma_2)(f^{(i)}(\sigma_1)-f^{(i)}(\sigma_2))^2.
\\
 & = \sum_{\sigma\in\Pinning_i}\pi_i(\sigma)f^{(i)}(\sigma)^2
- \sum_{\sigma_1,\sigma_2\in\Pinning_i}\pi_i(\sigma_1)\downup{i,j}(\sigma_1,\sigma_2)f^{(i)}(\sigma_1)f^{(i)}(\sigma_2)
\\
& =: \Var_{\pi_i}f^{(k)} - \Var_{\pi_{j}}f^{(j)} & \mbox{by \cref{eqn:Var-A,eqn:Var-B}}
\end{align*}
where the second line follows from the reversibility condition $\pi_i(\sigma)\downup{i}(\sigma,\tau)=\pi_i(\tau)\downup{i}(\tau,\sigma)$ and the fact that  $\sum_{\tau\in\Pinning_i}\downup{i}(\sigma,\tau)=1$.  This completes the proof of the lemma.

\end{proof}

\subsubsection{Proof of \cref{lem:improved-technical}}
\label{subsec:missing-claim2}

We will use the following identity in the proof of~\cref{lem:improved-technical}. 
\begin{claim}
\label{claim:first-step}
\begin{equation}
\label{eqn:first-step}
\Var_{\pi_{k+1}}(f^{(k+1)}) - \Var_{\pi_{k-1}}(f^{(k-1)}) = \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Var_{\pi_{\tau,2}}(f_\tau^{(2)}).
\end{equation}    
\end{claim}

\begin{proof}
Let us begin by examining each of the terms in the LHS of \cref{eqn:first-step}.  Once again we can assume $\Exp_{\pi_{k+1}}[f^{(k+1)}]=\Exp_{\pi_{k-1}}[f^{(k-1)}]=0$.
\begin{equation}
\label{eqn:Var-first-k+1}
\Var_{\pi_{k+1}}(f^{(k+1)}) 
 = 
\sum_{\sigma\in\Pinning_{k+1}}\pi_{k+1}(\sigma)f^{(k+1)}(\sigma)^2
%= \sum_{\sigma\in\Pinning_{k+1}}\frac{\mu(\sigma)}{{n \choose k+1}}f^{(k+1)}(\sigma)^2
\end{equation}
And for the variance at level $k-1$ we have:
%\begin{align}
\begin{equation}
%\lefteqn{
\Var_{\pi_{k-1}}(f^{(k-1)}) 
%}
%\\
%& 
= 
\sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
% \\
% & = 
% \sum_{\sigma\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{\substack{\tau_1,\tau_2: \\ |\tau_1|=|\tau_2|=2}}\frac{\pi_{k+1}(\eta\cup\tau_1)\pi_{k+1}(\eta\cup\tau_2)}{(k(k+1)\pi_{k-1}(\eta))^2}f^{(k+1)}(\eta\cup\tau_1)f^{(k+1)}(\eta\cup\tau_2)
% \nonumber
% \\
% & = 
% \sum_{\sigma\in\Pinning_{k-1}}\frac{\mu(\eta)}{{n \choose k-1}}\sum_{\tau_1,\tau_2}
% \frac{\mu(\eta\cup\tau_1)\mu(\eta\cup\tau_2)(n-k+1)^2(n-k)^2}{k^2(k+1)^2\mu(\eta)^2}f^{(k+1)}(\eta\cup\tau_1)f^{(k+1)}(\eta\cup\tau_2)
\label{eqn:Var-second-k-1}
\end{equation}
%\end{align}

Now let us analyze the variance of the local walk on the RHS of \cref{eqn:first-step}.
 First note that for $\eta\in\Pinning_{k-1}$ where $\eta:S\rightarrow\{0,1\}$ for $S\subset V$, for $\tau:T\rightarrow\{0,1\}$ where $T\subset V\setminus S, |T|=2$, then $f_\eta^{(2)}(\tau)=f^{(k+1)}(\eta\cup\tau)$.
 In the following equation array, the sum over $\tau$ is over all $T\subset V\setminus S, |T|=S$, where $\eta$ is on $S$, and all $\tau:T\rightarrow \{0,1\}$. 

 \begin{align*}
\lefteqn{
\sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\Var_{\pi_{\eta,2}}(f_\eta^{(2)})
} 
\hspace{.5in}
\\
 & =
 \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)(f^{(k+1)}(\eta\cup\tau) - 
 f^{(k-1)}(\eta))^2
 \\
  & =
 \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)^2 - 2 \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)
 \\ & \hspace{.5in}
 +  \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2\sum_{\tau}\pi_{\eta,2}(\tau)
 %  \\
 %  & =
 % \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)^2 
 % - 2 \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)
 % \\ & \hspace{.5in}
 % +  \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
% \\
%  & = \sum_{\sigma\in\Pinning_{k+1}}\frac{2\mu(\sigma)}{{n \choose k+1}(k+1)k}f^{(k+1)}(\sigma)^2 
%  - 2\sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
%   +  \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
%   \\
%    & = \sum_{\sigma\in\Pinning_{k+1}}\frac{2\mu(\sigma)}{{n \choose k+1}(k+1)k}f^{(k+1)}(\sigma)^2 
%  - \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
   \\
   & = \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)\sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)^2 
 - \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
 \\
   & = \sum_{\sigma\in\Pinning_{k+1}}\pi_{k+1}(\sigma)f^{(k+1)}(\sigma)^2 
 - \sum_{\eta\in\Pinning_{k-1}}\pi_{k-1}(\eta)f^{(k-1)}(\eta)^2
\\
& = \Var_{\pi_{k+1}}(f^{(k+1)}) 
-  \Var_{\pi_{k-1}}(f^{(k-1)}),
\end{align*}
where we used the following two identities:
\begin{equation}
    \label{eqn:step111}
\mbox{for every $\eta\in\Pinning_{k-1}$, } 
f^{(k-1)}(\eta) = \sum_{\tau}\pi_{\eta,2}(\tau)f^{(k+1)}(\eta\cup\tau)
\end{equation}
 \begin{equation}
 \label{eqn:step222}
 \mbox{for every $\sigma\in\Pinning_{k+1}$, } \sum_{\substack{\eta\in\Pinning_{k-1}: \\ \eta\subset\sigma}}\pi_{k-1}(\eta)\pi_{\eta,2}(\sigma\setminus\eta)
 = \pi_{k+1}(\sigma)
 \end{equation}
This completes the proof of the claim.
\end{proof}

Now we can complete the proof of \cref{lem:improved-technical}.

\begin{proof}[Proof of \cref{lem:improved-technical}]
Recall, our aim is to prove, that for any $f^{(k+1)}:\Pinning_{k+1}\rightarrow\R$, the following holds:
\begin{equation}
\tag{\ref{eqn:NEW-D}}
\Dirichlet_{\downup{k+1}}(f^{k+1}) \geq (2\gamma_{k-1}-1)\Dirichlet_{\downup{k}}(f^{(k)}).
\end{equation}

Fix $\tau\in\Pinning_{k-1}$.  Note that by definition of $\gamma_j$ we have that the spectral gap of the local walk $Q$ satisfies $\gamma(Q_\tau)\geq\gamma_{k-1}$ and hence $\gamma(\updown{\tau,1})\geq\gamma_{k-1}/2$; therefore, by~\cref{lem:updown-downup}, we also have that
$\gamma(\downup{\tau,2})\geq\gamma_{k-1}/2$ which means that for any $f$:
\begin{equation}
\label{miss-222}
\Dirichlet_{\downup{\tau,2}}(f_\tau^{(2)})\geq \frac12\gamma_{k-1}\Var_{\pi_{\tau,2}}(f_\tau^{(2)}).
\end{equation}
\cref{lem:diff-var} yields that
    $\Dirichlet_{\downup{\tau,2}}(f^{(2)}_\tau) = \Var_{\tau,2}(f^{(2)}_\tau) - \Var_{\tau,1}(f^{(j)}_\tau)$, and hence combining with~\cref{miss-222}
we have the following:
    \begin{align}
\nonumber
\Var_{\pi_{\tau,2}}(f_\tau^{(2)})
&\geq 
\frac{1}{1-\gamma_{k-1}/2}\Var_{\pi_{\tau,1}}(f_\tau^{(1)})
\\
&\geq 
2\gamma_{k-1}\Var_{\pi_{\tau,1}}(f_\tau^{(1)}).
\label{missing-step}
\end{align}


We can now complete the proof of \cref{eqn:NEW-D}:
\begin{align*}
    \Dirichlet_{\downup{k+1}}(f^{(k+1)}) + 
      \Dirichlet_{\downup{k}}(f^{(k)})
       & = \Var_{\pi_{k+1}}(f^{(k+1)}) - \Var_{\pi_{k-1}}(f^{(k-1)})
       & \mbox{by \cref{lem:diff-var}}
       \\
       & = 
 \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Var_{\pi_{\tau,2}}(f_\tau^{(2)})
       &\mbox{by \cref{claim:first-step}}
\\
%        & = 
% \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Var_{\pi_{\tau,1}}(f_\tau^{(1)})
% +
%  \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Dirichlet_{\downup{\tau,2}}(f_\tau^{(2)})
%     & \mbox{by \cref{lem:diff-var}}
%     \\
%        & = 
% \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Var_{\pi_{\tau,1}}(f_\tau^{(1)})
% +
%  \sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Dirichlet_{Q_{\tau}}(f_\tau^{(1)})
%     & \mbox{by Eric's conjecture \cref{conj:Eric}?}
% \\
&\geq  2\gamma_{k-1}\sum_{\tau\in\Pinning_{k-1}}\pi_{k-1}(\tau)\Var_{\pi_{\tau,1}}(f_\tau^{(1)})
& \mbox{ by \cref{missing-step}}
\\
& = 2\gamma_{k-1}\Dirichlet_{\downup{k}}(f^{(k)}),
& \mbox{ by \cref{claim:DDD}}
\end{align*}
which proves that $  \Dirichlet_{\downup{k+1}}(f^{(k+1)})\geq  (2\gamma_{k-1} -1 )\Dirichlet_{\downup{k}}(f^{(k)})$, and hence proves \cref{eqn:NEW-D} and \cref{lem:improved-technical}.

If we used the bound $1/(1-\gamma_{k-1}/2)$ in \eqref{missing-step} instead of $2\gamma_{k-1}$ then the statement of \cref{lem:improved-technical} has $\gamma_{k-1}/(2-\gamma_{k-1})$ in place of $(2\gamma_{k-1}-1)$; this matches the statement of \cite[Fact A.8 and Theorem~A.9]{CLV21}.
\end{proof}

% Missing step in above is that for any $\tau\in\Pinning_{k-1}$:
% \begin{equation}
%  \Dirichlet_{\downup{\tau,2}}(f_\tau^{(2)})
%  \geq   \gamma_{k-1}\Var_{\pi_{\tau,1}}(f_\tau^{(1)})
% \end{equation}
% One way to prove this is to show that:
% \begin{equation}
% \label{conj:Eric}
%  \Dirichlet_{\downup{\tau,2}}(f^{(2)}) =      \Dirichlet_{Q_\tau}(f^{(1)}_\tau),
% \end{equation}
% and then apply \cref{eq:DDD} which says that
% $\Dirichlet_{Q_{\tau}}(f_\tau)\geq \gamma_{k-1}\Var_{\pi_{\tau,1}}(f_\tau)$ and then we're done.
% If we go back a few lines we need that:
% \begin{equation}
% \Var_{\pi_{\tau,2}}(f_\tau^{(2)})
% \geq 
% (1+\gamma_{k-1})\Var_{\pi_{\tau,1}}(f_\tau^{(1)}).
% \end{equation}


% This is the analog of the definition
% of local entropy contraction in \cite[Definition 5.3, page 26]{CLV21}.  And \cite{CLV21} claims it for variance in Appendix A, see Definition A.7 on page 51.
% It turns out that at least for entropy it's a nasty proof in Section 5.3.1 (see Theorem 5.6).  Is there a simpler proof for variance since don't need to use marginal boundedness in this case?



\section{Optimal Mixing Time via Entropy Decay}
\label{sec:optimal}

In the previous section we established \cref{thm:SI-constant-relax} which says that spectral independence implies optimal $O(n)$ relaxation time of the Glauber dynamics.  
\cref{thm:SI-constant-mix} is the stronger result, which says that spectral independence and marginal boundedness implies optimal $O(n\log{n})$ mixing time.  Both \cref{thm:SI-constant-relax,thm:SI-constant-mix} were proved by Chen, Liu, and Vigoda~\cite{CLV21}, and our proofs of these theorems follow their proof approach.  To obtain \cref{thm:SI-constant-mix}, we follow the same proof approach as in the proof of \cref{thm:SI-constant-relax} with the general objective of replacing $\Var$ by $\Ent$.

There are two main sticking points with replacing $\Var$ by $\Ent$.  The first is that the proof of \cref{thm:SI-constant-relax} uses the spectral gap of the Glauber dynamics.  Recall the formulation of the spectral gap in the Poincare inequality, see \cref{defn:poincare}, where the spectral gap $\gamma = \min_f \Dirichlet(f)/\Var(f)$.  Moreover, for the down-up chain $\downup{i,j}$ one can replace the $\Dirichlet_{\downup{i,j}}(f)$ by $\Var_{\pi_i}(f)-\Var_{\pi_j}(f)$, see \cref{lem:diff-var}. 
Notice that in \cref{section-proof-relax} for the proof of \cref{thm:SI-constant-relax} we only considered the down-up chain and hence we can rephrase the proof only in terms of $\Var(f)$, without use of $\Dirichlet(f)$.  Therefore we can aim to replace all occurrences of $\Var(f)$ with $\Ent(f)$ in order to establish entropy decay instead of variance decay.

The only technical obstacle in replacing $\Var$ by $\Ent$ is in the application of \cref{thm:SI-mixing} in the proof of \cref{thm:SI-constant-relax} in \cref{sec:proof-relax}.  \cref{thm:SI-mixing} established a lower bound on the spectral gap of the Glauber dynamics but it does not establish a bound on its entropy decay.  To get a corresponding bound on entropy decay we need to use an additional property known as {\em marginal boundedness}, see \cref{defn:marg-bound}.  Using $b$-marginal boundedness, \cite[Lemma 4.2]{CLV21} proved a corresponding bound on $\Ent_C(f)$ in terms of $\sum_{v\in C}\mu(\Ent_v(f))$ which depends on $b$ and $|C|$, and this bound can be used in the analogous step for establishing entropy decay, see the proof of Lemma 2.3 in~\cite{CLV21}.  

The analog of the proof of \cref{thm:SI-constant-relax} in~\cref{sec:proof-relax}, as outlined above with $\Ent$ in place of $\Var$, yields
entropy tensorization of the Gibbs distribution as formulated in the following definitions.

We first recall the definition of entropy of a functional.

\begin{definition}[Entropy]The entropy of a function $f:\Omega\rightarrow\R$ with respect to $\mu$ is the following:
\[
    \Ent[f] := \mu[f \log f] - \mu[f] \log \left(\mu[f]\right)
     = \sum_{\sigma\in\Omega} \mu(\sigma)f(\sigma)\log(f(\sigma)) - \sum_{\sigma\in\Omega}\mu(\sigma)f(\sigma)\log[\sum_{\eta\in\Omega}\mu(\eta)f(\eta)].
\]
\end{definition}

For a vertex $x$, denote the expected entropy at $x$ by:
\[
\mathrm{Ent}_{x}[f]=  \sum_{\tau}\mu(\tau)\mathrm{Ent}^{\tau}_{x}[f]  = \sum_{\tau}\mu(\tau)\Ent[F\mid \sigma(V\setminus\{x\})=\tau(V\setminus\{x\}].\]

Finally, we can define entropy tensorization.

\begin{definition}[Entropy Tensorization]\label{def:ent-tensorization}
A distribution $\mu$ with support $\Omega \subseteq \{0, 1\}^V$ satisfies approximate tensorisation of 
entropy with constant $C>0$, if for 
all $f:\Omega\to\mathbb{R}_{\geq 0}$ we have that
\begin{align}\nonumber
\mathrm{Ent}(f)\leq C\sum_{v\in V}\mu\left( \mathrm{Ent}_v(f)\right).
\end{align}
\end{definition}
Note, the corresponding notion of variance tensorization is equivalent to the spectral gap of the (heat-bath) Glauber dynamics as the RHS in the above is equivalent to the Dirichlet form, see~\cref{eqn:Cond-Var-Dirichlet}.

% is the average (over $\mu$ on all possible pinnings on $V\setminus\{x\}$) conditional entropy of $f$. 
%
Entropy tensorization with constant $C$ yields an optimal mixing time bound for the Glauber dynamics by bounding the decay rate of entropy of the Glauber dynamics with respect to the Gibbs distribution, see~\cite{CMT,Caputo-notes}:
\begin{equation}\label{eq:mix-ET}
   \Tmix \leq  C n\log(\log(1/\mu^*)).  
\end{equation}       
This outlines the approach of \cite{CLV21} to obtain optimal mixing time bounds.

Further improvements are obtained in \cite{CFYZ22,CE22} which bound the mixing time as $\leq C(\eta)n\log{n}$ instead of $C(\eta,\Delta)$ based on a stronger form of spectral independence, and hence their results potentially extend to unbounded maximum degree $\Delta$.


\section{Matroids}
\label{sec:matroids}

We now utilize the spectral independence approach to establish fast mixing of a Markov chain for generating a random basis of a matroid.  The main result for matroids is work of Anari, Liu, Oveis Gharan, and Vinzant~\cite{ALOV19}.  In fact, the work of~\cite{ALOV19} for matroids inspired the idea of spectral independence as we presented earlier and introduced in~\cite{ALO20}, however we present these results in the opposite order.  

The general approach to prove fast mixing of the chain on bases of a matroid is to apply the Random Walk Theorem (\cref{thm:RW}).  To bound the spectral gap of the local walks we will use a result of Oppenheim~\cite{Opp18} known as the Trickle-Down Theorem.  The Trickle-Down Theorem bounds the spectral gap of the local walk $Q_S$ for a pinning $S$ by induction on $|S|$.  The base case will be a bound on the gap of the local walk $Q_S$ when $k:=|S|=n-2$ which will correspond to rank-2 matroids; this is a significantly easier case to analyze as it corresponds to an unweighted walk.  We will then use the Trickle-Down Theorem to deduce a bound on the spectral gap of $Q_S$ for pinnings of size $k-1$ and hence any pinning $S$ by induction.  

\subsection{Matroids: Definitions}

A matroid $\M=(E,\cI)$ consists of the following:
\begin{itemize}
    \item {\em Ground Set} denoted by $E$;
    \item Collection $\cI$ of subsets of the ground set.  We refer to each $S\in \cI$ as an {\em independent set}.
\end{itemize}
To be a matroid we require the following properties:
\begin{description}
    \item[P1: Downward closure:]
    \label{matroid:P1}
    Every subset of an independent set is also an independent set.  This is means that $\cI$ is downward closed, or equivalently that $\cI$ forms a simplicial complex.  To be clear, this property says that for every $S \in \cI$, if $T\subset S$ then  $T\in \cI$.
    \item[P2: Exchange property:] If $S\in \cI$ and $T\in \cI$, and if $|S|<|T|$ then there exists $e\in S\setminus T$ where $S\cup e\in I$.
\end{description}

Note, the empty set $\emptyset$ is always an independent set by the downward closure property.  A set $R\subset E$ which is {\em not} an independent set is called  dependent.

One important consequence of the exchange axiom is that all maximal  independent sets are of the same size (maximal independent set means it is not contained in another independent set).


\begin{definition}
Consider a matroid $\M=(E,\cI)$. 
A {\em basis} is a maximal independent set.  In words, a basis is an independent set $S\in\cI$ where for all $e\not\in S$, we have that $S\cup \{e\}\not\in \cI$.  Note that the set of bases uniquely determines a matroid. We will use $\cF$ to denote the collection of bases of a matroid.

All bases of a matroid have the same size which is called the {\em rank} of the matroid $\M$.
\end{definition}



\begin{definition}[Truncation]
Let $M$ be a matroid. Let $k$ be a positive integer. The set $\{S\in \cI: |S| \le k\}$
is called the truncation of $M$. Note that the truncation of a matroid is also a matroid.
\end{definition}

We state a few exercises that detail important, fundamental properties of matroids.

\begin{exercise}\label{exc1}
Let $\cF$ be a set of bases of a matroid $\M$. Let $\cF^*$ be the set of complements, that is, $\cF^*=\{[n]-B : B\in \cF\}$. Show that $\cF^*$ are the bases of a matroid $\M^*$ (called the dual of the matroid $\M$). 
\end{exercise}

\begin{exercise}\label{exc2}
Let $\cI$ be a set of independent sets of a matroid $M=(E,\cI)$. Let $S\subseteq [n]$. Let $\cI''$ be the set of independent 
sets contained in $S$, that is, $\{T : T\in\cI\ \mbox{and}\ T\subseteq S\}$. Show that $\M''=(E,\cI'')$ is a matroid  (called the restriction of the matroid $\M$; denoted by $\M|S$).
\end{exercise}

\begin{exercise}\label{exc3}
Let $\cI$ be a set of independent sets of a matroid $\M=(E,\cI)$. Let $S\subseteq [n]$. Let $\cI'$ be the set of independent  sets containing $S$ with $S$ removed, that is, $\{T\setminus S : T\in\cI\ \mbox{and}\ S\subseteq T\}$. Show that $\M'=(E,\cI')$ is a matroid  (called the contraction of the matroid $\M$; denoted by $\M/S$).
\end{exercise}

\begin{remark}
A matroid $\M'$ that can be obtained from $\M$ by contraction and restriction is called a minor of~$\M$.
\end{remark}



\subsection{Matroids: Examples}

Here are some natural examples of matroids.

\begin{description}
    \item[{\bf Graphic matroid:}]
Let $G=(V,E)$ be a connected graph.  Let $\cI=\{S\subset E: (V,S) \mbox{ is acyclic}\}$ be the collection of all forests of $G$.  
    The collection $\M=(E,\cI)$ is a matroid. 
    The bases of $\M$ are maximal acyclic subgraphs.  Since $G$ is assumed to be connected then the bases of $\M$ are the spanning trees of $G$ since $G$, and hence this is also called the spanning tree matroid.
\item[{\bf Transversal Matroid:}]
Let $G=(V,E)$ be a bipartite graph where  $V=L\cup R$ is the bipartition.  Let $\cI$ denote the subsets of $L$ that are the endpoints of some matching of $G$.  Note, $S\in \cI$ is a set of vertices and they can correspond to multiple matchings.  

The collection $\M=(L,\cI)$ is a matroid.  The bases are the subsets of $L$ covered by a maximum matching.  
\item[{\bf Linear matroid:}]
Let $V = \{v_1,\ldots, v_n\}$ be vectors in some vector space.
Let 
\[
\cI = \{S \subseteq V : S \text{ is linear independent} \}.
\]
Then $\M=(V,\cI)$ is a matroid.
\end{description}


\subsection{Bases Exchange Walk}

Our central question is, for a given matroid $\M=(E,\cI)$, can we generate a basis, uniformly at random from the set $\cF$ of all bases, in time polynomial in $n=|E|$.  We are given the matroid in implicit form, so we can efficiently check whether a given subset $S\subset E$ is independent or dependent.
We will use the following natural Markov chain known as the bases exchange walk to generate a random bases.

The bases exchange walk is the following Markov chain $(B_t)$ on the set of bases of a given matroid $\M$.
Let $\cF$ denote the collection of bases of a matroid $M$.  From $B_t\in\cF$, the transitions $B_t\rightarrow B_{t+1}$ are defined as follows:
\begin{itemize}
    \item Choose an element $e\in B_t$ uniformly at random.
    \item Let $F=\{f\in E: B_t\cup \{f\}\setminus \{e\}\in\cI\}$ denote the set of edges that we can add to $B_t\setminus e$ while remaining independent.
    \item Choose an element $f\in F$ uniformly at random.  Let $B_{t+1} = B_t\cup \{f\}\setminus \{e\}$. 
\end{itemize}

Note the edge $e$ is in $F$ and hence $P(B_t,B_t)>0$; thus, the chain is aperiodic. 

\begin{exercise}
Using the exchange-axiom, prove by induction that the chain is irreducible.
\end{exercise}

  Therefore, the bases-exchange walk is an ergodic Markov chain.  Since the chain is symmetric then the unique stationary distribution is uniform over the set $\cF$ of bases of the matroid $M$.
In the terminology from the spectral independence section, the bases-exchange walk is equivalent to the down-up walk on the bases of a matroid. 

\subsection{Main Theorem: Fast mixing of Bases-Exchange Walk}

The main result is a bound on the spectral gap of the bases-exchange walk.

\begin{theorem}[\cite{ALOV19}]\label{thm:matroid-main}
For any matroid $M$, the spectral gap of the bases-exchange walk satisfies $\ge \frac 1{r(M)}$, where $r(M)$ is the rank of the matroid.
\end{theorem}


Using the upper bound on the mixing time in terms of the spectral gap (see \cref{eq:relax-gap}) we have the following.

\begin{corollary}
The mixing time of the bases-exchange walk is $O(r(M) \log(|\cF|)) = O(r(M)^2\log n)$.
\end{corollary}
By proving entropy contraction, \cite{CGM19} improves the mixing time bound to $O(r(M)\log{\log(|\cF|)}) = O(r(M) \log r(M) + r(M) \log{\log{n}})$; this is presented in Heng Guo's first lecture.
Furthermore, the $\log\log n$ term was subsequently removed in \cite{ALOVV21}, achieving $O(r(M) \log r(M))$ mixing time.
This is tight because we need $\Omega(r(M) \log r(M))$ time to replace every element due to coupon collector.


\subsection{Application: Reliability}

 Before proving fast convergence of the bases-exchange walk we discuss an important application for sampling/approximate counting bases of a matroid.
 
An interesting application is to compute reliability.
Let $X\subseteq [n]$ be a random set where every element is included independently with probability $p$.
The {\em reliability} is defined as the $\bP[X\supseteq \text{basis}]$,
and the {\em unreliability} is defined as $\bP[X\not\supseteq \text{basis}]$.


For a graphical matroid these problems are known as network reliability/unreliability as the problem corresponds to whether $X$ is connected/disconnected.  Karger~\cite{Karger} presented an $\fpras$ for network unreliability,
and Guo and Jerrum~\cite{GJ19} presented an $\fpras$ for network reliability.

\cref{thm:matroid-main} yields an $\fpras$ for the reliability of a matroid.
In particular, rapid mixing of the bases-exchange walk yields an efficient sampling algorithm for generating a basis of a matroid (almost) uniformly at random;
the classical result of \cite{JVV86} then yields 
an $\fpras$ for computing the number of bases.

Consider a matroid $\M$ for which we would like to compute the reliability, which is the probability that the random subset $X$ contains a basis of $\M$.
Let $\M^*$ denote the dual of $\M$, see \cref{exc1}; note $\M^*$ is a matroid.
Note,
\begin{align*}
    \Prob{X\supseteq \text{basis of $\M$}} = 
    \Prob{\overline{X}\subseteq \text{basis of $\M^*$}}=
     \sum_k (1-p)^k p^{n-k} \text{\#\{indep set of $\M^*$ of size $k$\}},
\end{align*}
and we can obtain an $\fpras$ for the number of independent sets (of a matroid $\M^*$) of size $k$ since the truncation of a matroid is a matroid.  This yields an $\fpras$ for the reliability of the matroid $\M$.



\section{Trickle-Down Theorem}
\label{sec:trickledown}

Here we present Oppenheim's Trickle-Down Theorem~\cite{Opp18} which is the new tool needed to prove \cref{thm:matroid-main}.  We present the theorem in the general context of simplicial complexes; this enables one to apply it both in the context of spin systems and for bases of a matroid.



\subsection{Simplical Complexes}

Let $\Lambda$ be a finite set.  Let $\Omega$ denote a collection of subsets of $\Lambda$ that is 1) closed 
under taking subsets (that is, if $S\in\Omega$
and $Z\subseteq S$ then $Z\in\Omega$), and 2) all 
maximal (under inclusion) sets in $\Lambda$
have the same size $r$. Such a pair $(\Lambda,\Omega)$
is called a pure abstract simplicial complex. 

For the example of spectral independence on a (binary) spin system, the set $\Lambda=V\times\{0,1\}$ for an input graph $G=(V,E)$.  Here $r=|V|$ while $|\Lambda|=2r$.  The distribution $\mu$ is the Gibbs distribution on assignments $\sigma\in \{0,1\}^V$. Now we construct $\Omega$. The maximal sets in $\Omega$ are the support of $\mu$. Defining the distributions $\pi_k$
as in \cref{sec:chains}, then the union of the supports of $\pi_0,\dots,\pi_r$ yields $\Omega$.
 
%For example, in the hard-core model the set $\Omega$ are the collection of independent sets of the input graph $G$ and the distribution $\mu$ has $\mu(S)\propto\lambda^{|S|}$ for an independent set $S\in\Omega$.

For the example of bases of a matroid, let $\Lambda=E$, the ground set of the matroid $\M=(E,\cI)$.  Then let $\Omega=\cI$ be the independent sets of $\M$.  The maximal sets in $\Omega$ are the bases of $\M$ and hence $r$ is the rank of the matroid.  All bases in $\Omega$ have the same weight, thus we set $\mu$ to be the uniform distribution over the set of bases $\cF$. Defining the distributions $\pi_k$
as in \cref{sec:chains}, then the union of the supports of $\pi_0,\dots,\pi_r$ is $\Omega$.

More generally, let $\mu$ be a distribution on sets of size $r$. Define the distributions $\pi_k$ and the up/down chains exactly as in \cref{sec:chains}.  
%We assume that $\Omega$ ar
The union of the supports of distributions $\pi_0,\pi_1,\dots,\pi_r$ is a pure abstract simplicial complex. 

%We will take the maximal sets of $\Omega$ to be the support of $\mu$, that is we take every $S\subseteq\Lambda$ with $|S|=r$ and $\mu(S)>0$.
%(Note that the maximal sets in $\Omega$ are the support of $\mu=\pi_r$.)





\subsection{Trickle-Down Statement}

Define the distributions $\pi_k$ and the up/down chains exactly as in \cref{sec:chains}, and the local walks $Q_S$ the same as in \cref{sec:local-walks}. Let $(\Lambda,\Omega)$ be the simplicial complex that is 
the support of $\pi_k$'s. We can now state the Trickle-Down theorem.

\begin{theorem}[\cite{Opp18}]
\label{thm:trickle-down}
    Consider $S\in\Omega$.  Let $i=|S|$ and assume $0\leq i<r-2$.  Let $\gamma_i=\gamma(Q_S)$.  Assume that $Q_S$ is irreducible and hence $\gamma_i>0$.  Suppose there exists $\gamma_{i+1}>0$ where for all $Z\in\Omega$ such that  $S\subset Z$ and $|Z|=|S|+1$, 
    \begin{equation}
        \label{TD:assume}
        \gamma(Q_{Z})\geq \gamma_{i+1}.
    \end{equation} Then the following holds:
    \[  \gamma_i \geq 2 - \frac{1}{\gamma_{i+1}}.\]
    \end{theorem}

As a consequence, if there is optimal spectral gap, namely, $\gamma(Q_{S'})\geq 1$, for all $S'\in\Omega$ where $|S'|=r-2$, then by induction we obtain, via the Trickle-Down Theorem, that there is optimal spectral gap of $\gamma(Q_S)\geq 1$ for all $S\in\Omega$.

    \begin{corollary}[Trickle-Down Without Loss]
    \label{cor:trickle-without-loss}
        If for all $S'\in\Omega$ with $|S'|=r-2$ we have $\gamma(Q_{S'})\geq 1$. Further assume that for all $S\in\Omega$ we have that $Q_S$ is irreducible. Then for all $S\in\Omega$ it holds that 
        \[ \gamma(Q_S)\geq 1.
        \]
    \end{corollary}
 


\section{Rapid Mixing of Bases-Exchange Walk: Proof of \cref{thm:matroid-main}}
\label{sec:basesexchange}

The proof of \cref{thm:matroid-main} uses the Trickle Down Theorem (\cref{thm:trickle-down}), which bounds the spectral gap of the local walks, and the Random Walk Theorem (\cref{thm:RW}), which is the local to global theorem.  

Our goal is to bound the spectral gap of the bases-exchange walk using the Random Walk Theorem.  To that end we need to bound the spectral gap of the local walk (this is the up-down walk $\updown{1}$) for every pinning (or link in the terminology of simplicial complexes). The pinnings are defined for every independent set $S$.  In particular, for an independent set $S$ the pinning is defined as $\{B\setminus S : S\subseteq B, B\in \cI\}$, which is a contraction of the matroid. \cref{exc3} establishes that the contraction of a matroid is also a matroid.


These local walks for a particular pinning are weighted as these are the projections in the original simplicial complex.  To handle these weighted local walks we utilize the Trickle Down Theorem.  The Trickle Down Theorem says that if we have an ``optimal'' bound on the spectral gaps for all local walks on $(r-2)$-dimensional pinning/link then we obtain an optimal bound on the spectral gap for the local walks for all pinnings/links, and then we can apply the Random Walk Theorem.  This greatly simplifies matters because the local walks on $(r-2)$-dimensional pinnings/links are unweighted,
and hence they simply correspond to (unweighted) rank~2 matroids.




\begin{lemma}\label{lem:spectralgap-rank2}
For a rank 2 matroid $\M$, the spectral gap of the local walk satisfies
\[ 
\gamma(Q_{\M}) \geq 1.
\]
\end{lemma}

\begin{proof}
Consider a rank 2 matroid $\M$.  Consider a graph $G_\M$ whose vertex set are the elements 
of the matroid $\M$ and whose edges are the bases of the matroid $\M$. We will argue that this graph consists of a 
complete multipartite graph plus some isolated vertices.
The isolated vertices $v$ are those for which $\{v\}$ is not independent. Remove the isolated vertices.  
%The graph has an edge $\
%{i,j\}\in E$ if and only if $\{i,j\}$ is independent. 
Now, by the Exchange Axiom, for any three vertices $i,j,k$ with $\{j,k\}\in E$, either $\{i,j\}\in E$ or $\{i,k\}\in E$. Equivalently, if $\{i,j\}\not\in E$ and $\{i,k\}\not\in E$ then 
$\{j,k\}\not\in E$. This means that we have a multipartite graph (where the partitions are maximal sets of pairwise dependent elements). 

The adjacency matrix $A$ of a complete multipartite graph is the all ones matrix (that has only one non-zero eigenvalue) minus a block diagonal matrix, so it has the second eigenvalue $\le 0$. (Adding the isolated vertices
doesn't change the non-zero part of the spectrum.)

The local walk $Q_\M$ is equivalent to the random walk on the graph $G_\M$.     The transition matrix of the random walk on $G_\M$ is the matrix $D^{-1} A$ where $D$ is the diagonal matrix with the degrees.
Note, $D^{-1} A$ is similar to $D^{-1/2} A D^{-1/2}$.
Sylvester's law of inertia says that for matrices $A$ and $B$,
the signature of $A$ and $BAB^T$ are the same, where the signature of a matrix refers to the number of positive and negative eigenvalues.
Hence, from Sylvester's law of inertia (which states that a change of basis does not change the number of eigenvalues of a given sign) we have that $\lambda_2(A) \le 0 \iff \lambda_2(D^{-1/2} A D^{-1/2}) \le 0$.  
This completes the proof.
\end{proof}

\begin{lemma}\label{lem:matroid-local}
Let $\M=(E,\cI)$ be a matroid of rank $r$. For any $S\in\cI$, $|S|\leq r-2$, we have that the spectral gap of the local walk $Q_S$ satisfies $\gamma(Q_S)\geq 1$.
\end{lemma}


\begin{proof}[Proof of~\cref{lem:matroid-local}]
This lemma follows from \cref{lem:spectralgap-rank2} and \cref{cor:trickle-without-loss} (since the $(r-2)$-links are rank $2$ matroids).
\end{proof}

We can now prove the main theorem establishing rapid mixing of the bases-exchange walk.

\begin{proof}[Proof of Theorem~\ref{thm:matroid-main}]
 The random walk theorem \cref{thm:RW} implies that the spectral gap $\gamma$ for the transition matrix $P$ of the bases exchange walk satisfies:
\begin{equation}
    \label{eqn:RW-thm222}
 \gamma(P) \geq \frac{1}{r}\prod_{k=0}^{r-2}\gamma_k,
\end{equation}
where $$\gamma_k=\min_{S\in\cI:|S|=k}\gamma(Q_{S})$$ and $\gamma(Q_S)$ is the spectral gap for the local walk $Q_S$, which by \cref{lem:matroid-local} we have that $\gamma_k\geq 1$ for every~$k$.
\end{proof}



\section{Proof of the Trickle-Down Theorem: Proof of \cref{thm:trickle-down}}
\label{sec:proof-trickledown}

In this section, we complete the proof of the Trickle-Down Theorem \cref{thm:trickle-down}; this is the only remaining task to complete the proof of fast mixing of the bases-exchange walk (\cref{thm:matroid-main}).

Fix $S\subset E$. Let $\pi_S = \pi_{S,1}$ and hence $\pi_S$ is a distribution over $a\in E\setminus S$.
Recall that the local walk $Q_S$ has stationary distribution $\pi_S$.

We will use the following technical lemma.

\begin{lemma}
\label{lem:TD111}
Let $S\subset E$ where $i=|S|$.  For any function $f:E\rightarrow\R$,
\begin{align}
\label{lem:TD1-Dir}
\Dirichlet_{Q_S}(f) & = \sum_{a\in E\setminus S}\pi_S(a)\Dirichlet_{Q_{S\cup a}}(f)
 \\
 \label{lem:TD1-Var}
 \Exp_{\pi_S}(f) 
& = \sum_{a\in E\setminus S}\pi_S(a)
\Exp_{\pi_{S\cup a}}(f).
\end{align}
\end{lemma}



We can now prove the Trickle-Down Theorem.

\begin{proof}[Proof of \cref{thm:trickle-down}]
Our goal is to bound $\gamma_i=\gamma(Q_S)$ which is the spectral gap for the 
local walk $Q_S$ with pinning $S\subset E$.  Recall, 
for all $f:E\rightarrow\R$,
\begin{equation}
    \label{TD:dirichlet}
\gamma_i = \min_{f} \frac{\Dirichlet_{Q_S}(f)}{\Var_{\pi_S}(f)},
\end{equation}
where the minimization is over all $f:E\rightarrow\R$ where $f$ is not constant on $E\setminus S$ and hence $\Var_{\pi_S}\neq 0$.
Let $f^*$ be the function $f$ which achieves the minimum in \cref{TD:dirichlet}, and hence
\[ \gamma_i\Var_{\pi_S}(f) = \Dirichlet_{Q_S}(f).
\]

The minimizer $f^*$ of~\eqref{TD:dirichlet} has to be an ``eigenvector'' in the following sense (this follows more directly by 
viewing Dirichlet forms through eigenvalues; we include
a self-contained proof).
\begin{lemma}
\label{TD:blue}
For any $a\in E\setminus S$,
\[
    \Exp_{\pi_{S\cup a}}(f^*) = (1-\gamma_i)f^*(a).
\]    
\end{lemma}


Without loss of generality, in \cref{TD:dirichlet} we can restrict attention to functions $f$ where $\Exp_{\pi_S}[f] = 0$ (since translating $f$ by a constant function does not change the numerator and the denominator), and hence for $f^*$ we can also assume that $\Exp_{\pi_S}[f^*]=0$.

\begin{align*}
    \gamma_i \Var_{\pi_S}(f^*) & = \Dirichlet_{Q_S}(f^*) 
\\
& = \sum_{a\in E\setminus S}\pi_S(a)\Dirichlet_{Q_{S\cup a}}(f^*) & \mbox{by \cref{lem:TD1-Dir}}
\\
& \geq \gamma_{i+1}\sum_{a\in E\setminus S}\pi_{S}(a)\Var_{\pi_{S\cup a}}(f^*)
& \mbox{by \cref{TD:assume}}
\\
& = \gamma_{i+1}\sum_a\pi_{S}(a)\Exp_{\pi_{S\cup a}}[(f^*)^2]
- \gamma_{i+1}\sum_a\pi_{S}(a)[\Exp_{\pi_{S\cup a}}(f^*)]^2
\\
& = \gamma_{i+1}\sum_a\pi_{S}(a)\Exp_{\pi_{S\cup a}}[(f^*)^2]
- \gamma_{i+1}(1-\gamma_i)^2\sum_a\pi_{S}(a)(f^*(a))^2
&\mbox{by \cref{TD:blue}}
\\
& = \gamma_{i+1}\sum_a\pi_{S}(a)\Exp_{\pi_{S\cup a}}[(f^*)^2]
- \gamma_{i+1}(1-\gamma_i)^2\Exp_{\pi_S}[(f^*)^2]
\\
& = \gamma_{i+1}\Exp_{\pi_S}[(f^*)^2]
- \gamma_{i+1}(1-\gamma_i)^2\Exp_{\pi_S}[(f^*)^2]&\mbox{by \cref{lem:TD1-Var}}
\\
& = \gamma_{i+1}\Var_{\pi_{S}}(f^*)
- \gamma_{i+1}(1-\gamma_i)^2\Var_{\pi_S}(f^*).
\end{align*}

Now, since $\Var_{\pi_S}(f^*)>0$ we have
$$
\gamma_i\geq \gamma_{i+1}(1-(1-\gamma_i)^2) = \gamma_{i+1}\gamma_i (2- \gamma_i).
$$
Since $\gamma_i>0$ (we assumed that $Q_S$ is irreducible) we have
$$
1 \geq \gamma_{i+1} (2- \gamma_i),
$$
which is equivalent to:
$$
\gamma_i \geq 2 - \frac{1}{\gamma_{i+1}},$$
which completes the proof.
\end{proof}

\subsection{Proofs of Technical Lemmas}

It remains to prove \cref{lem:TD111} and \cref{TD:blue}.

\begin{proof}[Proof of \cref{lem:TD111}]
Recall $i=|S|$, and $\mu$ is the uniform distribution over subsets $S\subset E$ which are bases of the matroid.
Let us begin with two basic facts.  
\begin{equation}
    \label{matroid:basic-fact}
\pi_{S}(a) = \pi_{S,1}(a) = \frac{\mu(S\cup a)}{(n-i)\mu(S)} = \frac{\pi_{i+1}(S\cup a){n \choose i+1}}{(n-i)\pi_i(S){n \choose i}}
= \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_i(S)}
\end{equation}

% For $T\subset E$ where $|T|=k$,
% first notice that
% \begin{equation}
% \label{matroid:first-step222}
% \sum_{a\in E\setminus T} \mu(T\cup a)
% = (n-k)\mu(T).
% \end{equation}

Applying \cref{matroid:first-step} we have the following:
\begin{equation}
\label{matroid:sum-a}
\sum_{a\in E\setminus T} \pi_{k+1}(T\cup a)
 = (k+1)\pi_{k}(T).
\end{equation}

We can now proceed with the proof of \cref{lem:TD1-Var}.
\begin{align*}
\Exp_{\pi_S}(f)  & = \sum_{b\in E\setminus S}\pi_S(b)f(b)
 \\
& = \sum_{b\in E\setminus S}\frac{\pi_{i+1}(S\cup b)}{(i+1)\pi_i(S)} f(b)
&\mbox{by \cref{matroid:basic-fact}}
 \\
& =  \sum_{b\in E\setminus S} \sum_{a\in E\setminus (S\cup b)} \frac{\pi_{i+2}(S\cup b\cup a)}{(i+1)(i+2)\pi_i(S)} f(b)
&\mbox{by \cref{matroid:sum-a} with $T=S\cup b$}
 \\
& =  \sum_{a\in E\setminus S} \sum_{b\in E\setminus (S\cup a)}  \frac{\pi_{i+2}(S\cup b\cup a)}{(i+2)\pi_{i+1}(S\cup a)} \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_i(S)} f(b)
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b\in E\setminus (S\cup a)}  \frac{\pi_{i+2}(S\cup b\cup a)}{(i+2)\pi_{i+1}(S\cup a)}  f(b)
&\mbox{by \cref{matroid:sum-a}}
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b\in E\setminus (S\cup a)}  \pi_{S\cup a}(b) f(b)
&\mbox{by \cref{matroid:basic-fact}}
\\
& = \sum_{a\in E\setminus S}\pi_S(a)
\Exp_{\pi_{S\cup a}}(f).
\end{align*}


We can establish \cref{lem:TD1-Dir} in a similar manner.
\begin{align*}
\Dirichlet_{Q_S}(f)  & = \sum_{b,c\in E\setminus S}\pi_S(b) Q_S(b,c) (f(b)-f(c))^2
 \\
 & = \sum_{b,c\in E\setminus S}\pi_S(b)
 (i+2)\updown{i+1}(S\cup b,S\cup c)(f(b)-f(c))^2
 &\mbox{by \cref{step:rem:local-downup}}
 \\
 & = \sum_{b,c\in E\setminus S}\pi_S(b)
 \frac{\pi_{i+2}(S\cup b\cup c)}{(i+2)\pi_{i+1}(S\cup b)}  (f(b)-f(c))^2
 &\mbox{by \cref{eqn:up-down}}
 \\
 & = \sum_{b,c\in E\setminus S}\frac{\pi_{i+1}(S\cup b)}{(i+1)\pi_i(S)}\frac{\pi_{i+2}(S\cup b\cup c)}{(i+2)\pi_{i+1}(S\cup b)}  (f(b)-f(c))^2
 &
 \mbox{ by \cref{matroid:basic-fact}}
 \\
& = \sum_{b,c\in E\setminus S}\frac{\pi_{i+2}(S\cup b\cup c)}{(i+1)(i+2)\pi_i(S)}  (f(b)-f(c))^2
 \\
& =  \sum_{b,c\in E\setminus S} \sum_{a\in E\setminus (S\cup b\cup c)} \frac{\pi_{i+3}(S\cup a\cup b\cup c)}{(i+3)(i+2)(i+1)\pi_i(S)} (f(b)-f(c))^2
& \mbox{by \cref{matroid:sum-a}}
 \\
& =  \sum_{a\in E\setminus S} \sum_{b,c\in E\setminus (S\cup a)}  \frac{\pi_{i+3}(S\cup a\cup b\cup c)}{(i+3)(i+2)\pi_{i+1}(S\cup a)} \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_i(S)} (f(b)-f(c))^2
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b,c\in E\setminus (S\cup a)}  \frac{\pi_{i+3}(S\cup a\cup b\cup c)}{(i+3)(i+2)\pi_{i+1}(S\cup a)}  (f(b)-f(c))^2
 &
 \mbox{ by \cref{matroid:basic-fact}}
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b,c\in E\setminus (S\cup a)}  \pi_{S\cup a}(b)\frac{\pi_{i+3}(S\cup a\cup b\cup c)}{(i+3)\pi_{i+2}(S\cup a\cup b)}(f(b)-f(c))^2
 & \mbox{by \cref{matroid:basic-fact}}
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b,c\in E\setminus (S\cup a)} \pi_{S\cup a}(b)(i+3)\updown{i+2}(S\cup a\cup b,S\cup a\cup c)(f(b)-f(c))^2
&\mbox{by \cref{eqn:up-down}}
\\
& =  \sum_{a\in E\setminus S} \pi_S(a) \sum_{b,c\in E\setminus (S\cup a)}  \pi_{S\cup a}(b)Q_{S\cup a}(b,c) (f(b)-f(c))^2
 &
 \mbox{by  \cref{step:rem:local-downup}}
\\
& = \sum_{a\in E\setminus S}\pi_S(a)
\Dirichlet_{Q_{S\cup a}}(f).
\end{align*}


\end{proof}

We now prove \cref{TD:blue}, which will complete the proof of the Trickle-Down Theorem (\cref{thm:trickle-down}).

\begin{proof}[Proof of \cref{TD:blue}]
It will be more convenient to
work with the following 
minimization problem equivalent to~\eqref{TD:dirichlet}:
\begin{equation}\label{minim}
\gamma_i = \min_{f; \Var_{\pi_S}(f^*)=1} \Dirichlet_{Q_S}(f).
\end{equation}
By Lagrange multipliers a critical point of~\eqref{minim} satisfies for every $a\in E\setminus S$
\begin{equation}\label{lagra}
\sum_{b\in E\setminus S} \frac{\pi_{i+2}(S\cup a\cup b)}{(i+2)(i+1)\pi_{i}(S)} (f(a)-f(b)) = \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)}\lambda f(a),
\end{equation}
for some multiplier $\lambda$. Plugging~\eqref{lagra} and 
$\Var_{\pi_S}(f^*)=1$ into $\Dirichlet_{Q_S}(f)$ we obtain
$\lambda=\gamma_i$.

Note that we can rewrite~\eqref{lagra} as follows.
\begin{align*}
\sum_{b\in E\setminus S} \frac{\pi_{i+2}(S\cup a\cup b)}{(i+2)(i+1)\pi_{i}(S)} f(b)
& =
f(a) \sum_{b\in E\setminus S} \frac{\pi_{i+2}(S\cup a\cup b)}{(i+2)(i+1)\pi_{i}(S)} - \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)}\lambda f(a)
\\ &
= 
f(a) \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)} - \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)}\lambda f(a) 
& \mbox{by \cref{matroid:first-step}}
\\
& = (1 - \lambda) f(a) \frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)}.
\end{align*}
Dividing both sides by $\frac{\pi_{i+1}(S\cup a)}{(i+1)\pi_{i}(S)}$ we obtain
$$
\sum_{b\in E\setminus S} \frac{\pi_{i+2}(S\cup a\cup b)}{(i+2)\pi_{i+1}(S\cup a)} f(b) = (1 - \lambda) f(a).
$$
Since $$\Exp_{\pi_{S\cup a}}(f^*) =\sum_{b\in E\setminus S} \frac{\pi_{i+2}(S\cup a\cup b)}{(i+2)\pi_{i+1}(S\cup a)} f(b),$$
this completes the proof of the lemma.
\end{proof}

\section{Acknowledgements}

The author are thankful to Zongchen Chen for many helpful conversations and to 
Catherine Greenhill for the many corrections after her careful reading.  




\bibliographystyle{alpha}
\bibliography{biblio}

\end{document}