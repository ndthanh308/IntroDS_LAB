@ARTICLE{Fortunato2010,
  author  = {Fortunato, S.},
  title   = {Community detection in graphs},
  journal = {Phys. Rep.-Rev. Sec. Phys. Lett.}, 
  volume  = {486},
  year    = {2010},
  pages   = {75-174}
}

@ARTICLE{NewmanGirvan2004,
  author  = {Newman, M. E. J. and Girvan, M.},
  title   = {Finding and evaluating community structure in networks},
  journal = {Phys. Rev. E.}, 
  volume  = {69},
  year    = {2004},
  pages   = {026113}
}
@article{kuchemann2020students,
  title={Studentsâ€™ understanding of non-inertial frames of reference},
  author={K{\"u}chemann, Stefan and Klein, Pascal and Fouckhardt, Henning and Gr{\"o}ber, Sebastian and Kuhn, Jochen},
  journal={Physical Review Physics Education Research},
  volume={16},
  number={1},
  pages={010112},
  year={2020},
  publisher={APS}
}
@ARTICLE{Vehlowetal2013,
  author  = {Vehlow, C. and Reinhardt, T. and Weiskopf, D.},
  title   = {Visualizing Fuzzy Overlapping Communities in Networks},
  journal = {IEEE Trans. Vis. Comput. Graph.}, 
  volume  = {19},
  year    = {2013},
  pages   = {2486-2495}
}

@ARTICLE{Raghavanetal2007,
  author  = {Raghavan, U. and Albert, R. and Kumara, S.},
  title   = {Near linear time algorithm to detect community structures in large-scale networks},
  journal = {Phys. Rev E.}, 
  volume  = {76},
  year    = {2007},
  pages   = {036106}
}

@ARTICLE{SubeljBajec2011a,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Robust network community detection using balanced propagation},
  journal = {Eur. Phys. J. B.}, 
  volume  = {81},
  year    = {2011},
  pages   = {353-362}
}

@article{hestenes1992force,
  title={Force concept inventory},
  author={Hestenes, David and Wells, Malcolm and Swackhamer, Gregg},
  journal={The physics teacher},
  volume={30},
  number={3},
  pages={141--158},
  year={1992},
  publisher={American Association of Physics Teachers}
}

@ARTICLE{Louetal2013,
  author  = {Lou, H. and Li, S. and Zhao, Y.},
  title   = {Detecting community structure using label propagation with weighted coherent neighborhood propinquity},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {3095-3105}
}

@ARTICLE{Clausetetal2004,
  author  = {Clauset, A. and Newman, M. E. J. and Moore, C.},
  title   = {Finding community structure in very large networks},
  journal = {Phys. Rev. E.}, 
  volume  = {70},
  year    = {2004},
  pages   = {066111}
}

@ARTICLE{Blondeletal2008,
  author  = {Blondel, V. D. and Guillaume, J. L. and Lambiotte, R. and Lefebvre, E.},
  title   = {Fast unfolding of communities in large networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {2008},
  year    = {2008},
  pages   = {P10008}
}

@ARTICLE{SobolevskyCampari2014,
  author  = {Sobolevsky, S. and Campari, R.},
  title   = {General optimization technique for high-quality community detection in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {90},
  year    = {2014},
  pages   = {012811}
}

@ARTICLE{FortunatoBarthelemy2007,
  author  = {Fortunato, S. and Barthelemy, M.},
  title   = {Resolution limit in community detection},
  journal = {Proc. Natl. Acad. Sci. U. S. A.}, 
  volume  = {104},
  year    = {2007},
  pages   = {36-41}
}

@ARTICLE{SubeljBajec2011b,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Unfolding communities in large complex networks: Combining defensive and offensive label propagation for core extraction},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {036103}
}

@ARTICLE{WangLi2013,
  author  = {Wang, X. and Li, J.},
  title   = {Detecting communities by the core-vertex and intimate degree in complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {2555-2563}
}

@ARTICLE{Lietal2013,
  author  = {Li, J. and Wang, X. and Eustace, J.},
  title   = {Detecting overlapping communities by seed community in weighted complex networks},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6125-6134}
}

@ARTICLE{Fabioetal2013,
  author  = {Fabio, D. R. and Fabio, D. and Carlo, P.},
  title   = {Profiling core-periphery network structure by random walkers},
  journal = {Sci. Rep.}, 
  volume  = {3},
  year    = {2013},
  pages   = {1467}
}

@ARTICLE{Chenetal2013,
  author  = {Chen, Q. and Wu, T. T. and Fang, M.},
  title   = {Detecting local community structure in complex networks based on local degree central nodes},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {529-537}
}

@ARTICLE{Zhangetal2007,
  author  = {Zhang, S. and Wang, R. and Zhang, X.},
  title   = {Identification of overlapping community structure in complex networks using fuzzy c-means clustering},
  journal = {Physica A.}, 
  volume  = {374},
  year    = {2007},
  pages   = {483-490}
}

@ARTICLE{Nepuszetal2008,
  author  = {Nepusz, T. and Petr\'oczi, A. and N\'egyessy, L. and Bazs\'o, F.},
  title   = {Fuzzy communities and the concept of bridgeness in complex networks},
  journal = {Phys. Rev. E.}, 
  volume  = {77},
  year    = {2008},
  pages   = {016107}
}

@ARTICLE{FabricioLiang2013,
  author  = {Fabricio, B. and Liang, Z.},
  title   = {Fuzzy community structure detection by particle competition and cooperation},
  journal = {Soft Comput.}, 
  volume  = {17},
  year    = {2013},
  pages   = {659-673}
}

@ARTICLE{Sunetal2011,
  author  = {Sun, P. and Gao, L. and Han, S.},
  title   = {Identification of overlapping and non-overlapping community structure by fuzzy clustering in complex networks},
  journal = {Inf. Sci.}, 
  volume  = {181},
  year    = {2011},
  pages   = {1060-1071}
}

@ARTICLE{Wangetal2013,
  author  = {Wang, W. and Liu, D. and Liu, X. and Pan, L.},
  title   = {Fuzzy overlapping community detection based on local random walk and multidimensional scaling},
  journal = {Physica A.}, 
  volume  = {392},
  year    = {2013},
  pages   = {6578-6586}
}

@ARTICLE{Psorakisetal2011,
  author  = {Psorakis, I. and Roberts, S. and Ebden, M. and Sheldon, B.},
  title   = {Overlapping community detection using Bayesian non-negative matrix factorization},
  journal = {Phys. Rev. E.}, 
  volume  = {83},
  year    = {2011},
  pages   = {066114}
}

@CONFERENCE{ZhangYeung2012,
  author  = {Zhang, Y. and Yeung, D.},
  title   = {Overlapping Community Detection via Bounded Nonnegative Matrix Tri-Factorization},
  booktitle = {In Proc. ACM SIGKDD Conf.}, 
  year    = {2012},
  pages   = {606-614}
}

@ARTICLE{Liu2010,
  author  = {Liu, J.},
  title   = {Fuzzy modularity and fuzzy community structure in networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {77},
  year    = {2010},
  pages   = {547-557}
}

@ARTICLE{Havensetal2013,
  author  = {Havens, T. C. and Bezdek, J. C. and Leckie, C., Ramamohanarao, K. and Palaniswami, M.},
  title   = {A Soft Modularity Function For Detecting Fuzzy Communities in Social Networks},
  journal = {IEEE Trans. Fuzzy Syst.}, 
  volume  = {21},
  year    = {2013},
  pages   = {1170-1175}
}

@misc{Newman2013,
  author = {Newman, M. E. J.},
  title  = {Network data},
  howpublished = "\url{http://www-personal.umich.edu/~mejn/netdata/}",
  year = {2013}
}

@ARTICLE{SubeljBajec2012,
  author  = {\v{S}ubelj, L. and Bajec, M.},
  title   = {Ubiquitousness of link-density and link-pattern communities in real-world networks},
  journal = {Eur. Phys. J. B.}, 
  volume  = {85},
  year    = {2012},
  pages   = {1-11}
}

@ARTICLE{Lancichinettietal2008,
  author  = {Lancichinetti, A. and Fortunato, S. and Radicchi, F.},
  title   = {Benchmark graphs for testing community detection algorithms},
  journal = {Phys. Rev. E.}, 
  volume  = {78},
  year    = {2008},
  pages   = {046110}
}

@ARTICLE{Liuetal2014,
  author  = {Liu, W. and Pellegrini, M. and Wang, X.},
  title   = {Detecting Communities Based on Network Topology},
  journal = {Sci. Rep.}, 
  volume  = {4},
  year    = {2014},
  pages   = {5739}
}

@ARTICLE{Danonetal2005,
  author  = {Danon, L. and Diaz-Guilera, A. and Duch, J. and Arenas, A.},
  title   = {Comparing community structure identification},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2005},
  pages   = {P09008}
}

@ARTICLE{Gregory2011,
  author  = {Gregory, S.},
  title   = {Fuzzy overlapping communities in networks},
  journal = {J. Stat. Mech.-Theory Exp.}, 
  volume  = {},
  year    = {2011},
  pages   = {P02017}
}

@ARTICLE{LancichinettiFortunato2009,
  author  = {Lancichinetti, A. and Fortunato, S.},
  title   = {Benchmarks for testing community detection algorithms on directed and weighted graphs with overlapping communities},
  journal = {Phys. Rev. E.}, 
  volume  = {80},
  year    = {2009},
  pages   = {016118}
}

@CONFERENCE{HullermeierRifqi2009,
  author  = {Hullermeier, E. and Rifqi, M.},
  title   = {A Fuzzy Variant of the Rand Index for Comparing Clustering Structures},
  booktitle = {in Proc. IFSA/EUSFLAT Conf.}, 
  year    = {2009},
  pages   = {1294-1298}
}

@article{van2021open,
  title={An open source machine learning framework for efficient and transparent systematic reviews},
  author={Van De Schoot, Rens and De Bruin, Jonathan and Schram, Raoul and Zahedi, Parisa and De Boer, Jan and Weijdema, Felix and Kramer, Bianca and Huijts, Martijn and Hoogerwerf, Maarten and Ferdinands, Gerbrich and others},
  journal={Nature machine intelligence},
  volume={3},
  number={2},
  pages={125--133},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and Individual Differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@article{kuzman2023chatgpt,
  title={Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification},
  author={Kuzman, Taja and Mozetic, Igor and Ljube{\v{s}}ic, Nikola},
  journal={ArXiv, abs/2303.03953},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{iwana2021empirical,
  title={An empirical survey of data augmentation for time series classification with neural networks},
  author={Iwana, Brian Kenji and Uchida, Seiichi},
  journal={Plos one},
  volume={16},
  number={7},
  pages={e0254841},
  year={2021},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of big data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={SpringerOpen}
}

@article{van2001art,
  title={The art of data augmentation},
  author={Van Dyk, David A and Meng, Xiao-Li},
  journal={Journal of Computational and Graphical Statistics},
  volume={10},
  number={1},
  pages={1--50},
  year={2001},
  publisher={Taylor \& Francis}
}
@article{west2023ai,
  title={AI and the FCI: Can ChatGPT project an understanding of introductory physics?},
  author={West, Colin G},
  journal={arXiv preprint arXiv:2303.01067}          ,
  year={2023}
}

@article{west2023advances,
  title={Advances in apparent conceptual physics reasoning in ChatGPT-4},
  author={West, Colin G},
  journal={arXiv preprint arXiv:2303.17012}          ,
  year={2023}
}


@article{surameery2023use,
  title={Use chat gpt to solve programming bugs},
  author={Surameery, Nigar M Shafiq and Shakor, Mohammed Y},
  journal={International Journal of Information Technology \& Computer Engineering (IJITC) ISSN: 2455-5290},
  volume={3},
  number={01},
  pages={17--22},
  year={2023}
}

@article{Bender.2021,
 abstract = {-  Computing methodologies  -{\textgreater}  Natural language processing.},
 author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
 year = {2021},
 title = {On the Dangers of Stochastic Parrots},
 pages = {610--623},
 journal = {FAccT},
 doi = {10.1145/3442188.3445922},
 file = {3442188.3445922:Attachments/3442188.3445922.pdf:application/pdf}
}


@article{Vaswani.2017,
 abstract = {Neural Information Processing Systems http://nips.cc/},
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
 year = {2017},
 title = {Attention is All you Need: Conference on Neural Information Processing Systems},
 pages = {6000--6010},
 journal = {Advances in Neural Information Processing Systems},
 file = {7181-attention-is-all-you-need:Attachments/7181-attention-is-all-you-need.pdf:application/pdf}
}


@article{Schubatzky.2023,
 author = {Schubatzky, Thomas and Wackermann, Rainer and W{\"o}hlke, Carina and Haagen-Sch{\"u}tzenh{\"o}fer, Claudia and Jedamski, Marko and Lindemann, Hannes and Cardinal, Kai},
 year = {2023},
 title = {Entwicklung des Concept-Inventory CCCI-422 zu den naturwissenschaftlichen Grundlagen des Klimawandels},
 volume = {29},
 number = {1},
 issn = {0949-1147},
 journal = {Zeitschrift f{\"u}r Didaktik der Naturwissenschaften},
 doi = {10.1007/s40573-023-00159-8}
}


@article{Rosenbaum.2022,
 abstract = {Artificial neural networks are often interpreted as abstract models of biological neuronal networks, but they are typically trained using the biologically unrealistic backpropagation algorithm and its variants. Predictive coding has been proposed as a potentially more biologically realistic alternative to backpropagation for training neural networks. This manuscript reviews and extends recent work on the mathematical relationship between predictive coding and backpropagation for training feedforward artificial neural networks on supervised learning tasks. Implications of these results for the interpretation of predictive coding and deep neural networks as models of biological learning are discussed along with a repository of functions, Torch2PC, for performing predictive coding with PyTorch neural network models.},
 author = {Rosenbaum, Robert},
 year = {2022},
 title = {On the relationship between predictive coding and backpropagation},
 keywords = {Algorithms;Neural Networks, Computer;Neurons/physiology},
 pages = {e0266102},
 volume = {17},
 number = {3},
 issn = {1932-6203},
 journal = {PloS one},
 doi = {10.1371/journal.pone.0266102},
 file = {journal.pone.0266102:Attachments/journal.pone.0266102.pdf:application/pdf}
}


@inproceedings{Porter.2014,
 author = {Porter, Leo and Taylor, Cynthia and Webb, Kevin C.},
 title = {Leveraging open source principles for flexible concept inventory development},
 pages = {243--248},
 publisher = {ACM},
 isbn = {9781450328333},
 series = {ACM Digital Library},
 editor = {Cajander, {\AA}sa and Daniels, Mats and Clear, Tony and Pears, Arnold},
 booktitle = {Proceedings of the 2014 conference on Innovation {\&} technology in computer science education},
 year = {2014},
 address = {New York, NY},
 doi = {10.1145/2591708.2591722},
 file = {Porter, Taylor et al. 2014 - Leveraging open source principles:Attachments/Porter, Taylor et al. 2014 - Leveraging open source principles.pdf:application/pdf}
}


@article{Nehm.2012,
 author = {Nehm, Ross H. and H{\"a}rtig, Hendrik},
 year = {2012},
 title = {Human vs. Computer Diagnosis of Students' Natural Selection Knowledge: Testing the Efficacy of Text Analytic Software},
 pages = {56--73},
 volume = {21},
 number = {1},
 issn = {1059-0145},
 journal = {Journal of Science Education and Technology},
 doi = {10.1007/s10956-011-9282-7},
 file = {Nehm-Haertig2012{\_}Article{\_}HumanVsComputerDiagnosisOfStud:Attachments/Nehm-Haertig2012{\_}Article{\_}HumanVsComputerDiagnosisOfStud.pdf:application/pdf}
}


@article{Manning.2022,
 author = {Manning, Christopher D.},
 year = {2022},
 title = {Human Language Understanding {\&} Reasoning},
 pages = {127--138},
 volume = {151},
 number = {2},
 issn = {0011-5266},
 journal = {Daedalus},
 doi = {10.1162/daed{\textunderscore }a{\textunderscore }01905},
 file = {Daedalus{\_}Sp22{\_}09{\_}Manning:Attachments/Daedalus{\_}Sp22{\_}09{\_}Manning.pdf:application/pdf}
}

@article{Wang.2023,
 author = {Wang, J.},
 year = {2023},
 title = {ChatGPT: A test drive},
 pages = {255--256},
 volume = {91},
 number = {4},
 journal = {American Journal of Physics},
 doi = {10.1119/5.0145897},
 file = {255{\_}1{\_}online:Attachments/255{\_}1{\_}online.pdf:application/pdf}
}




@article{Bommasani.2022,
 abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
 author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
 year = {2022},
 title = {On the Opportunities and Risks of Foundation Models},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computers and Society;Computer Science - Learning},
 journal = {arXiv},
 file = {2108.07258:Attachments/2108.07258.pdf:application/pdf}
}


@article{Wulff.2023,
 abstract = {Educ Inf Technol, doi:10.1007/s10639-022-11531-5}  ,
 author = {Wulff, Peter},
 year = {2023},
 title = {Network analysis of terms in the natural sciences insights from Wikipedia through natural language processing and network analysis},
 keywords = {Natural Language Processing;Network analysis;Science education and language;Wikipedia analyses},
 issn = {1360-2357},
 journal = {Education and Information Technologies},
 doi = {10.1007/s10639-022-11531-5},
 file = {s10639-022-11531-5-1:Attachments/s10639-022-11531-5-1.pdf:application/pdf}
}

@book{Liu.2010,
 abstract = {Front Cover -- Using and Developing Measurement Instruments in Science Education -- A Rasch Modeling Approach -- A volume in -- Science and Engineering Education Sources -- Series Editor: Calvin S. Kalman, Concordia University -- CONTENTS -- 1. Essential Concepts and Skills for Using and Developing Measurement Instruments 1 -- Evolution of Measurement Instruments in Science Education 2 -- Fundamental Theories for Using and Developing Measurement Instruments 5 -- Measurement Standards 11 -- Essential Skills for Developing Measurement Instruments 18 -- Locating measurement Instruments 28 -- Chapter Summary 29 -- Exercises 31 -- References 32 -- 2. Approaches to Developing Measurement Instruments 35 -- Classical Test Theory and Generalizability Theory 35 -- Rasch Models and Item Response Theory 40 -- Using Rasch Modeling to Develop Measurement Instruments 47 -- Chapter Summary 63 -- Exercises 64 -- References 65 -- 3. Using and Developing Instruments for Measuring Conceptual Understanding 67 -- What Is Conceptual Understanding? 67 -- Instruments for Measuring Conceptual Understanding 70 -- Developing Instruments for Measuring Conceptual Understanding 80 -- Chapter Summary 104 -- Exercises 106 -- References 106 -- 4. Using and Developing Instruments for Measuring Affective Variables 109 -- What are Affective Variables? 109 -- Instruments for Measuring Student Affective Variables 116 -- Instruments for Measuring Students' Motivation, Interest, and Self-Efficacy 127 -- Instruments for Measuring Teacher Affective Variables 133 -- Developing Instruments for Measuring Affective Variables 143 -- Chapter Summary 156 -- Exercises 157 -- References 158 -- 5. Using and Developing Instruments for Measuring Science Inquiry 165 -- What is Science Inquiry? 165 -- Instruments for Measuring Science Inquiry 171.},
 author = {Liu, Xiufeng},
 year = {2010},
 title = {Using and Developing Measurement Instruments in Science Education: A Rasch Modeling Approach},
 url = {https://ebookcentral.proquest.com/lib/kxp/detail.action?docID=4844589},
 address = {Charlotte, NC},
 publisher = {{Information Age Publishing Incorporated}},
 isbn = {9781617350054},
 series = {Science {\&} Engineering Education Sources}
}


@article{Liu.2021,
 abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub {\textquotedbl}prompt-based learning{\textquotedbl}. Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
 author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
 year = {2021},
 title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods  in Natural Language Processing},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computation and Language;Computer Science - Learning},
 journal = {arXiv},
 file = {2107.13586:Attachments/2107.13586.pdf:application/pdf}
}


@article{Lewkowycz.2022,
 abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
 author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and Wu, Yuhuai and Neyshabur, Behnam and Gur-Ari, Guy and Misra, Vedant},
 year = {2022},
 title = {Solving Quantitative Reasoning Problems with Language Models},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computation and Language;Computer Science - Learning},
 journal = {arXiv},
 file = {2206.14858:Attachments/2206.14858.pdf:application/pdf}
}

@article{Kuchemann.2023,
  title = {Can ChatGPT support prospective teachers in physics task development?},
  author = {K\"uchemann, Stefan and Steinert, Steffen and Revenga, Natalia and Schweinberger, Matthias and Dinc, Yavuz and Avila, Karina E. and Kuhn, Jochen},
  journal = {Phys. Rev. Phys. Educ. Res.},
  volume = {19},
  issue = {2},
  pages = {020128},
  numpages = {14},
  year = {2023},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevPhysEducRes.19.020128},
  url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.19.020128}
}

@article{Adams.2011,
 author = {Adams, Wendy K. and Wieman, Carl E.},
 year = {2011},
 title = {Development and Validation of Instruments to Measure Learning of Expert--Like Thinking},
 pages = {1289--1312},
 volume = {33},
 number = {9},
 journal = {International Journal of Science Education},
 file = {Development and Validation of Instruments to Measure Learning of Expert Like Thinking:Attachments/Development and Validation of Instruments to Measure Learning of Expert Like Thinking.pdf:application/pdf}
}

@article{Sinha.2023,
 abstract = {Background Artificial intelligence (AI) is evolving for healthcare services. Higher cognitive thinking in AI refers to the ability of the system to perform advanced cognitive processes, such as problem-solving, decision-making, reasoning, and perception. This type of thinking goes beyond simple data processing and involves the ability to understand and manipulate abstract concepts, interpret, and use information in a contextually relevant way, and generate new insights based on past experiences and accumulated knowledge. Natural language processing models like ChatGPT is a conversational program that can interact with humans to provide answers to queries. Objective We aimed to ascertain the capability of ChatGPT in solving higher-order reasoning in the subject of pathology. Methods This cross-sectional study was conducted on the internet using an AI-based chat program that provides free service for research purposes. The current version of ChatGPT (January 30 version) was used to converse with a total of 100 higher-order reasoning queries. These questions were randomly selected from the question bank of the institution and categorized according to different systems. The responses to each question were collected and stored for further analysis. The responses were evaluated by three expert pathologists on a zero to five scale and categorized into the structure of the observed learning outcome (SOLO) taxonomy categories. The score was compared by a one-sample median test with hypothetical values to find its accuracy. Result A total of 100 higher-order reasoning questions were solved by the program in an average of 45.31$\pm$7.14 seconds for an answer. The overall median score was 4.08 (Q1-Q3: 4-4.33) which was below the hypothetical maximum value of five (one-test median test p {\textless}0.0001) and similar to four (one-test median test p = 0.14). The majority (86{\%}) of the responses were in the {\textquotedbl}relational{\textquotedbl}~category in the SOLO taxonomy. There was no difference in the scores of the responses for questions asked from various organ systems in the subject of Pathology (Kruskal Wallis p = 0.55). The scores rated by three pathologists had an excellent level of inter-rater reliability (ICC = 0.975 [95{\%} CI: 0.965-0.983]; F = 40.26; p {\textless} 0.0001). Conclusion The capability of ChatGPT to solve higher-order reasoning questions in pathology had a relational level of accuracy. Hence, the text output had connections among its parts to provide a meaningful response. The answers from the program can score approximately 80{\%}. Hence, academicians or students can get help from the program for solving reasoning-type questions also. As the program is evolving, further studies are needed to find its accuracy level in any further versions.},
 author = {Sinha, Ranwir K. and {Deb Roy}, Asitava and Kumar, Nikhil and Mondal, Himel},
 year = {2023},
 title = {Applicability of ChatGPT in Assisting to Solve Higher Order Problems in Pathology},
 pages = {e35237},
 volume = {15},
 number = {2},
 issn = {2168-8184},
 journal = {Cureus},
 doi = {10.7759/cureus.35237},
 file = {Sinha, Deb Roy et al. 2023 - Applicability of ChatGPT in Assisting:Attachments/Sinha, Deb Roy et al. 2023 - Applicability of ChatGPT in Assisting.pdf:application/pdf}
}

@article{Dodge.2022,
 abstract = {By providing unprecedented access to computational resources, cloud computing has enabled rapid growth in technologies such as machine learning, the computational demands of which incur a high energy cost and a commensurate carbon footprint. As a result, recent scholarship has called for better estimates of the greenhouse gas impact of AI: data scientists today do not have easy or reliable access to measurements of this information, precluding development of actionable tactics. Cloud providers presenting information about software carbon intensity to users is a fundamental stepping stone towards minimizing emissions. In this paper, we provide a framework for measuring software carbon intensity, and propose to measure operational carbon emissions by using location-based and time-specific marginal emissions data per energy unit. We provide measurements of operational software carbon intensity for a set of modern models for natural language processing and computer vision, and a wide range of model sizes, including pretraining of a 6.1 billion parameter language model. We then evaluate a suite of approaches for reducing emissions on the Microsoft Azure cloud compute platform: using cloud instances in different geographic regions, using cloud instances at different times of day, and dynamically pausing cloud instances when the marginal carbon intensity is above a certain threshold. We confirm previous results that the geographic region of the data center plays a significant role in the carbon intensity for a given cloud instance, and find that choosing an appropriate region can have the largest operational emissions reduction impact. We also show that the time of day has notable impact on operational software carbon intensity. Finally, we conclude with recommendations for how machine learning practitioners can use software carbon intensity information to reduce environmental impact.},
 author = {Dodge, Jesse and Prewitt, Taylor and {Des Combes}, Remi Tachet and Odmark, Erika and Schwartz, Roy and Strubell, Emma and Luccioni, Alexandra Sasha and Smith, Noah A. and DeCario, Nicole and Buchanan, Will},
 year = {2022},
 title = {Measuring the Carbon Intensity of AI in Cloud Instances: FAccT '22},
 keywords = {carbon awareness;carbon intensity;cloud;CO2;Computer Science - Learning;emissions;grid},
 journal = {arXiv},
 file = {2206.05229:Attachments/2206.05229.pdf:application/pdf}
}


@article{Kruskal.1952,
 author = {Kruskal, William H. and Wallis, W. Allen},
 year = {1952},
 title = {Use of Ranks in One-Criterion Variance Analysis},
 pages = {583--621},
 volume = {47},
 number = {260},
 journal = {Journal of the American Statistical Association},
 doi = {10.1080/01621459.1952.10483441}
}


@article{Kortemeyer.2023,
 abstract = {Massive pre-trained language models have garnered attention and controversy due to their ability to generate human-like responses: attention due to their frequent indistinguishability from human-generated phraseology and narratives, and controversy due to the fact that their convincingly presented arguments and facts are frequently simply false. Just how human-like are these responses when it comes to dialogues about physics, in particular about the standard content of introductory physics courses? This study explores that question by having ChatGTP, the pre-eminent language model in 2023, work through representative assessment content of an actual calculus-based physics course and grading the responses in the same way human responses would be graded. As it turns out, ChatGPT would narrowly pass this course while exhibiting many of the preconceptions and errors of a beginning learner.},
 author = {Kortemeyer, Gerd},
 year = {2023},
 title = {Could an Artificial-Intelligence agent pass an introductory physics  course?},
 keywords = {Physics - Physics Education},
 pages = {15},
 volume = {19},
 number = {1},
 issn = {2469-9896},
 journal = {Physical Review Physics Education Research},
 doi = {10.1103/PhysRevPhysEducRes.19.010132},
 file = {2301.12127:Attachments/2301.12127.pdf:application/pdf}
}


@article{Koponen.2010,
 author = {Koponen, Ismo T. and Pehkonen, Maija},
 year = {2010},
 title = {Coherent Knowledge Structures of Physics Represented as Concept Networks in Teacher Education},
 pages = {259--282},
 volume = {19},
 number = {3},
 issn = {0926-7220},
 journal = {Science {\&} Education},
 doi = {10.1007/s11191-009-9200-z},
 file = {Coherent{\_}Knowledge{\_}Structures{\_}of{\_}Physics{\_}Represent:Attachments/Coherent{\_}Knowledge{\_}Structures{\_}of{\_}Physics{\_}Represent.pdf:application/pdf}
}


@incollection{Kieser.inpress,
 author = {Kieser, Fabian and Wulff, Peter},
 title = {Using large language models to probe cognitive constructs, augment data, and design instructional materials},
 publisher = {{Springer Nature}},
 editor = {Khine, M. S.},
 booktitle = {Machine Learning in Educational Sciences: Approaches, Applications and Advances},
 year = {in press}
}


@article{Katz.2023,
 author = {Katz, Daniel Martin and Bommarito, Michael James and Gao, Shang and Arredondo, Pable David},
 year = {2023},
 title = {GPT-4 Passes the Bar Exam},
 journal = {SSRN},
 doi = {10.2139/ssrn.4389233},
 file = {SSRN-id4389233:Attachments/SSRN-id4389233.pdf:application/pdf}
}


@article{Kaplan.2020,
 abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
 author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
 year = {2020},
 title = {Scaling Laws for Neural Language Models},
 keywords = {Computer Science - Learning;Statistics - Machine Learning},
 journal = {arXiv},
 file = {2001.08361:Attachments/2001.08361.pdf:application/pdf}
}


@article{Huang.2023,
 abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
 author = {Huang, Jie and Chang, Kevin Chen-Chuan},
 year = {2023},
 title = {Towards Reasoning in Large Language Models: A Survey},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computation and Language},
 journal = {arXiv},
 file = {2212.10403:Attachments/2212.10403.pdf:application/pdf}
}


@article{Hake.1998,
 author = {Hake, Richard R.},
 year = {1998},
 title = {Interactive-engagement versus traditional methods: A six-thousand-student survey of mechanics test data for introductory physics courses},
 pages = {64--74},
 volume = {66},
 number = {1},
 journal = {American Journal of Physics},
 doi = {10.1119/1.18809},
 file = {mz5agv0p:Attachments/mz5agv0p.pdf:application/pdf}
}


@article{Gregorcic.2023,
 abstract = {Physics Education, 58 (2023) 035021 doi: 10.1088/1361-6552/acc299}   ,
 author = {Gregorcic, Bor and Pendrill, Ann-Marie},
 year = {2023},
 title = {ChatGPT and the frustrated Socrates},
 keywords = {acceleration;Artificial intelligence;Chatbot;ChatGPT;discourse imitation;kinematics;Socratic dialogue},
 pages = {035021},
 volume = {58},
 number = {3},
 issn = {0031-9120},
 journal = {Physics Education},
 doi = {10.1088/1361-6552/acc299},
 file = {Gregorcic{\_}2023{\_}Phys.{\_}Educ.{\_}58{\_}035021:Attachments/Gregorcic{\_}2023{\_}Phys.{\_}Educ.{\_}58{\_}035021.pdf:application/pdf}
}


@article{Eaton.2021,
 abstract = {doi:10.1103/PhysRevPhysEducRes.17.010130      url:https://doi.org/10.1103/PhysRevPhysEducRes.17.010130}     ,
 author = {Eaton, Philip},
 year = {2021},
 title = {Evidence of measurement invariance across gender for the Force Concept Inventory},
 keywords = {doi:10.1103/PhysRevPhysEducRes.17.010130    url:https://doi.org/10.1103/PhysRevPhysEducRes.17.010130}   ,
 journal = {Phys Rev Spec Top Phys Edu Res},
 file = {PhysRevPhysEducRes.17.010130:Attachments/PhysRevPhysEducRes.17.010130.pdf:application/pdf}
}


@article{diSessa.1993,
 author = {diSessa, A.},
 year = {1993},
 title = {Toward an Epistemology of Physics},
 pages = {105--225},
 volume = {10},
 number = {2},
 journal = {Cognition and Instruction},
 file = {4emt15y5:Attachments/4emt15y5.pdf:application/pdf;eau5gjws:Attachments/eau5gjws.pdf:application/pdf}
}


@article{Devlin.2018,
 abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.  BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
 author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
 year = {2018},
 title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
 keywords = {Computer Science - Computation and Language},
 volume = {1810.04805},
 journal = {arXiv},
 file = {1810.04805:Attachments/1810.04805.pdf:application/pdf}
}


@book{Cohen.1988,
 author = {Cohen, Jacob},
 year = {1988},
 title = {Statistical power analysis for the behavioral sciences},
 url = {http://swb.eblib.com/patron/FullRecord.aspx?p=1192162},
 keywords = {Social sciences;Sozialwissenschaft;Statistical hypothesis testing;Statistical methods;Statistische Methodenlehre;Theorie},
 address = {Hillsdale, NJ},
 publisher = {Erlbaum},
 isbn = {0805802835}
}


@article{Caliskan.2017,
 abstract = {Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.},
 author = {Caliskan, Aylin and Bryson, Joanna J. and Narayanan, Arvind},
 year = {2017},
 title = {Semantics derived automatically from language corpora contain human-like biases},
 keywords = {Association;Female;Humans;Internet;Language;Machine learning;Male;Names;Semantics;Sex Factors},
 pages = {183--186},
 volume = {356},
 number = {6334},
 issn = {1095-9203},
 journal = {Science (New York, N.Y.)},
 doi = {10.1126/science.aal4230},
 file = {183.full:Attachments/183.full.pdf:application/pdf}
}


@article{Brown.2020,
 abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 year = {2020},
 title = {Language Models are Few-Shot Learners},
 keywords = {Computer Science - Computation and Language},
 journal = {arXiv},
 file = {2005.14165 (1):Attachments/2005.14165 (1).pdf:application/pdf}
}


@article{Bowman.2023,
 abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points:  1. LLMs predictably get more capable with increasing investment, even without targeted innovation.  2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.  3. LLMs often appear to learn and use representations of the outside world.  4. There are no reliable techniques for steering the behavior of LLMs.  5. Experts are not yet able to interpret the inner workings of LLMs.  6. Human performance on a task isn't an upper bound on LLM performance.  7. LLMs need not express the values of their creators nor the values encoded in web text.  8. Brief interactions with LLMs are often misleading.},
 author = {Bowman, Samuel R.},
 year = {2023},
 title = {Eight Things to Know about Large Language Models},
 keywords = {Computer Science - Artificial Intelligence;Computer Science - Computation and Language;ICML;Machine learning},
 journal = {arXiv},
 file = {2304.00612:Attachments/2304.00612.pdf:application/pdf}
}


@article{Bengio.2003,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
 year = {2003},
 title = {A Neural Probabilistic Language Model},
 pages = {1137--1155},
 volume = {3},
 journal = {Journal of Machine Learning Research},
 file = {bengio03a:Attachments/bengio03a.pdf:application/pdf}
}


@book{Wolfram.2023,
 author = {Wolfram, Stephen},
 year = {2023},
 title = {What is ChatGPT doing and why does it work?},
 publisher = {{Wolfram Media}},
 file = {Wolfram{\_}What is Chat GPT:Attachments/Wolfram{\_}What is Chat GPT.pdf:application/pdf}
}


@article{Yeadon.2023,
 abstract = {Physics Education, 58 (2023) 035027 doi: 10.1088/1361-6552/acc5cf}  ,
 author = {Yeadon, Will and Inyang, Oto-Obong and Mizouri, Arin and Peach, Alex and Testrow, Craig P.},
 year = {2023},
 title = {The death of the short-form physics essay in the coming AI revolution},
 keywords = {authentic assessment;ChatGPT;ethics;language models;NLP;Pedagogy},
 pages = {035027},
 volume = {58},
 number = {3},
 issn = {0031-9120},
 journal = {Physics Education},
 doi = {10.1088/1361-6552/acc5cf},
 file = {Yeadon{\_}2023{\_}Phys.{\_}Educ.{\_}58{\_}035027:Attachments/Yeadon{\_}2023{\_}Phys.{\_}Educ.{\_}58{\_}035027.pdf:application/pdf}
}



@article{Hammer.2000,
 author = {Hammer, David},
 year = {2000},
 title = {Student resources for learning introductory physics},
 pages = {S52-S59},
 volume = {68},
 number = {S1},
 issn = {0002-9505},
 journal = {American Journal of Physics},
 doi = {10.1119/1.19520}
}

@article{PhysRevPhysEducRes.16.020123,
  title = {Students' use and perception of textbooks and online resources in introductory physics},
  author = {Ruggieri, Charles},
  journal = {Phys. Rev. Phys. Educ. Res.},
  volume = {16},
  issue = {2},
  pages = {020123},
  numpages = {25},
  year = {2020},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevPhysEducRes.16.020123},
  url = {https://link.aps.org/doi/10.1103/PhysRevPhysEducRes.16.020123}
}

@unknown{unknown,
author = {Krupp, Lars and Steinert, Steffen and Kiefer-Emmanouilidis, Maximilian and Avila, Karina and Lukowicz, Paul and Kuhn, Jochen and Kuechemann, Stefan and Karolus, Jakob},
year = {2023},
month = {08},
pages = {},
doi = {10.48550/arXiv.2309.03087}
title = {Unreflected Acceptance -- Investigating the Negative Consequences of ChatGPT-Assisted Problem Solving in Physics Education}
}