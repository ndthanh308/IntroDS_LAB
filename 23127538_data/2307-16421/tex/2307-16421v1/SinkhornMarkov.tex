\documentclass[preprint]{amsart}
\usepackage{amsmath, amsfonts, amssymb, amsbsy, bigstrut, graphicx, enumerate,  upref, longtable, comment, bbm, physics, bm}
\usepackage{scalerel}
\usepackage{pstricks,csquotes}
\usepackage[breaklinks]{hyperref}

\hypersetup{
colorlinks=true,%
citecolor=blue,%
filecolor=blue,%
linkcolor=red,%
urlcolor=blue
}

\usepackage[numbers, sort&compress]{natbib} 
\usepackage{tikz}
\usetikzlibrary{decorations.markings}
%\usepackage[notcite,notref]{showkeys}
\usepackage[noabbrev,capitalize]{cleveref}
\usepackage{calc}
\usepackage{longtable}
\usepackage{accents}
\newcommand{\dbtilde}[1]{\accentset{\approx}{#1}}

% Macros

\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\argmax}{\operatorname{argmax}}
\newcommand{\argmin}{\operatorname{argmin}}
\newcommand{\xsph}{x^{\phi}}
\newcommand{\xsut}{x^{u_t}}
\newcommand{\emn}{\mathrm{EOT}(\mu,\nu;\vep)}
\newcommand{\xszt}{x^{\bu_0}}
\newcommand{\px}{p_{\scaleto{X}{3.5pt}}}
\newcommand{\py}{p_{\scaleto{Y}{3.5pt}}}
\newcommand{\ysdt}{y^{\bst}}
\newcommand{\bxsut}{(X_t)^{\bu_t}}
\newcommand{\bxszt}{(X_t)^{\bu_0}}
\newcommand{\bu}{\bar{u}}
\newcommand{\et}{\rho_t}
\newcommand{\mcx}{\mathcal{X}}
\newcommand{\mcxp}{\mathcal{X}'}
\newcommand{\ctx}{C_{T,\mcxp}}
\newcommand{\mcy}{\mathcal{Y}}
\newcommand{\secbx}{\frac{\partial}{\partial x} x^{\bu_t}}
\newcommand{\eh}{h_t}
\newcommand{\bmd}{\Delta}
\newcommand{\trc}{\mathrm{Trace}}
\newcommand{\mcI}{\mathcal{I}}
\newcommand{\mcJ}{\mathcal{J}}
\newcommand{\mcD}{\mathcal{D}}
\newcommand{\mc}{\mathcal{C}^{\vep}}
\newcommand{\secox}{\frac{\partial x^{u_t}}{\partial x \hfill}}
\newcommand{\secphx}{\frac{\partial\xsph}{\partial x\hfill}}
\newcommand{\secphxil}{\frac{\partial\xsph_i}{\partial x_{\ell}}}
\newcommand{\secphxim}{\frac{\partial \xsph_i}{\partial x_{m}}}
\newcommand{\bst}{\bu^*_t}
\newcommand{\bysut}{(Y_t)^{\bst}}
\newcommand{\ldet}{\log{\mathrm{det}}}
\newcommand{\secpx}{\frac{\partial X_t \hfill}{\partial\bxsut}}
\newcommand{\seczx}{\frac{\partial X_t\hfill}{\partial\bxszt}}
\newcommand{\secpy}{\frac{\partial Y_t\hfill}{\partial\bysut }}
\newcommand{\secpxs}{\frac{\partial x\hfill}{\partial\xsut }}
\newcommand{\secph}{\frac{\partial x\hfill}{\partial\xsph}}
\newcommand{\secphlj}{\frac{\partial x_{\ell}}{\partial\xsph_j}}
\newcommand{\secphjl}{\frac{\partial x_{j}}{\partial\xsph_{\ell}}}
\newcommand{\secphmk}{\frac{\partial x_{m}}{\partial\xsph_k}}
\newcommand{\secphmi}{\frac{\partial x_{m}}{\partial\xsph_i}}
\newcommand{\secpys}{\frac{\partial \bysut}{\partial Y_t} }
\newcommand{\lmn}{\lambda_{\mathrm{min}}}
\newcommand{\lmx}{\lambda_{\mathrm{max}}}
\newcommand{\mI}{KL}
\newcommand{\lsi}[1]{C_{#1}}
\newcommand{\ptac}{\mathcal{P}_2^{\mathrm{ac}}(\R^d)}
\newcommand{\sfe}{\sigma_F^2(t)}
\newcommand{\sse}{\sigma_S^2(t)}

\newcommand{\diffcont}{\mathcal{C}}
\newcommand{\iprod}[1]{\left\langle #1 \right\rangle}
\newcommand{\probspace}{\mathcal{P}_2\left( \R^d\right)}
\newcommand{\tanspace}{\mathrm{Tan}}
\newcommand{\ltwo}{\mathbf{L}^2}
\newcommand{\wass}{\mathbb{W}}
%\newcommand{\wass}{W}
\newcommand{\diffgen}{\mathcal{L}}
\newcommand{\vep}{\varepsilon}
\newcommand{\gpp}[1]{\tilde{\gamma}_{#1}^{\vep}}
\newcommand{\gnp}[1]{\gamma_{#1}^{\vep}}
\newcommand{\mk}[1]{\rho_{#1}^{\vep}}
\newcommand{\nk}[1]{\eta_{#1}^{\vep}}
\newcommand{\gvp}{\gamma^{\vep}}
\newcommand{\SP}[1]{\textcolor{purple}{SP:#1}}
\newcommand{\ND}[1]{\textcolor{blue}{#1}}
\newcommand{\wt}[3]{\wass_2^{#1}({#2},{#3})}
\newcommand{\dlt}[4]{d^{#1}_{\mathrm{LOT},{#2}}({#3},{#4})}
\newcommand{\lot}[1]{\mathrm{LOT}^2_{#1}}
\newcommand{\opV}{\mathcal{V}^{\vep}}
\newcommand{\opU}{\mathcal{U}^{\vep}}
\newcommand{\opS}{\mathcal{S}^{\vep}}
\newcommand{\rv}{\rho^{\vep}}
\newcommand{\opD}{\mathcal{D}}
\newcommand{\ophS}{\hat{\mathcal{S}}^{\vep}}
\newcommand{\hmn}{\hat{\mu}_n}
\newcommand{\hnn}{\hat{\nu}_n}
\newcommand{\mtd}{\mathbb{T}^d}
\newcommand{\cdt}{c_{\mtd}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\rr}{\mathbb{R}}
\newcommand{\ent}{\mathrm{Ent}}
\newcommand{\omap}{\mathbf{t}}
\newcommand{\id}{\mathbf{id}}
\newcommand{\txi}{\widetilde{\xi}}
\newcommand{\KL}[2]{\mathrm{KL}(#1 \parallel  #2)}
\newcommand{\mgf}[3]{\mathcal{M}_{#1}[#2:#3]}
\newcommand{\mfR}[3]{\mathrm{Rem}\big[#1\big](#2;#3)}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\fil}{\mathcal{F}}
\newcommand{\opP}{\bm{P}^{\vep}}
\newcommand{\opQ}{\bm{Q}^{\vep}}
\newcommand{\tpP}[1]{\bm{\tilde{P}}_{#1\vep}}
\newcommand{\tpQ}[1]{\bm{\tilde{Q}}_{#1\vep}}
\newcommand{\opR}{\bm{R}^{\vep}}
\newcommand{\fv}{\mathrm{FV}}
\newcommand{\hs}{\mathrm{HS}}



\renewcommand{\subjclassname}{\textup{2010} Mathematics Subject Classification} 
\allowdisplaybreaks
% Auxiliary 

\newtheorem{thm}{Theorem}
\newtheorem{lmm}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{assm}{Assumption}
\newtheorem{problem}[thm]{Problem}
\theoremstyle{definition}
\newtheorem{remark}[thm]{Remark}
\newtheorem{ex}[thm]{Example}

\numberwithin{thm}{section}
%\numberwithin{remark}{section}
%\numberwithin{ex}{section}
\numberwithin{assm}{section}
%\numberwithin{defn}{section}
\numberwithin{equation}{section}



\begin{document}

\title[Wasserstein Mirror Gradient Flows]{Wasserstein Mirror Gradient Flow as the limit of the Sinkhorn algorithm}% and Scaling Limit of the Sinkhorn Algorithm}



\author{Nabarun Deb}
\address{Nabarun Deb\\ Department of Mathematics \\ University of British Columbia\\ Vancouver, Canada\\ {Email: ndeb@math.ubc.ca}}
\author{Young-Heon Kim}
\address{Young-Heon Kim\\ Department of Mathematics \\ University of British Columbia\\ Vancouver, Canada\\ {Email: yhkim@math.ubc.ca}}
\author{Soumik Pal}
\address{Soumik Pal\\ Department of Mathematics \\ University of Washington\\ Seattle WA 98195, USA\\ {Email: soumik@uw.edu}}
\author{Geoffrey Schiebinger}
\address{Geoffrey Schiebinger\\ Department of Mathematics \\ University of British Columbia\\ Vancouver, Canada\\ {Email: geoff@math.ubc.ca}}


\begin{abstract}
 	We prove that the sequence of marginals obtained from the iterations of the Sinkhorn algorithm or the iterative proportional fitting procedure (IPFP) on joint densities, converges to an absolutely continuous curve on the $2$-Wasserstein space, as the regularization parameter $\vep$ goes to zero and the number of iterations is scaled as $1/\vep$ (and other technical assumptions). This limit, which we call the Sinkhorn flow, is an example of a Wasserstein mirror gradient flow, a concept we introduce here inspired by the well-known Euclidean mirror gradient flows. In the case of Sinkhorn, the gradient is that of the relative entropy functional with respect to one of the marginals and the mirror is half of the squared Wasserstein distance functional from the other marginal. Interestingly, the norm of the velocity field of this flow can be interpreted as the metric derivative with respect to the linearized optimal transport (LOT) distance. An equivalent description of this flow is provided by the parabolic Monge-Amp\`{e}re PDE whose connection to the Sinkhorn algorithm was noticed by Berman (2020). We derive conditions for exponential convergence for this limiting flow. We also construct a Mckean-Vlasov diffusion whose  marginal distributions follow the Sinkhorn flow.
\end{abstract}

\keywords{Entropy regularized optimal transport, Mckean-Vlasov diffusion, mirror descent, parabolic Monge-Amp\`ere, Sinkhorn Algorithm, Wasserstein gradient flows}
	
\subjclass[2000]{49N99, 49Q22,  60J60}


\thanks{Thanks to PIMS Kantorovich Initiative for facilitating this collaboration supported through a PIMS PRN and the NSF Infrastructure grant DMS 2133244.  Pal is supported by NSF grants DMS-2052239 and DMS-2134012.  Kim and Schiebinger are supported in part by New Frontier Research Funds (NFRF) of Canada as well as NSERC Discovery grant. Schiebinger is also supported by a MSHR Scholar Award, a CASI from the Burroughs Wellcome Fund and a CIHR Project Grant. Nabarun Deb is supported by the PIMS PRN postdoc fellowship.}


\maketitle

%\tableofcontents
%\marginpar{We can remove the table of contents after finishing the draft}
\input{Introduction}


\input{Mirror_flows}

\input{Sinkhorn_Diffusion}

\input{Sinkhorn_MC}

\input{error_analysis}


\input{Proofs}

\input{appendix}

%\input{SinkhornSections/Bermantype}
\bibliographystyle{amsalpha}
\bibliography{references}
\end{document}



%Leftover stuff

{\color{blue}
\begin{proof}
    Given a measure $\mu$ with positive density and functions $\psi_1$, $\psi_2$, we define:
    $$\mgf{\mu}{\psi_1}{\psi_2}:=\int (\psi_2-\psi_1)(x)\,d\mu(x)+\int (\psi_2^*-\psi_1^*)(y) e^{-g(y)}\,dy.$$
    Given $\xi,\rho$ from the question, set $\mu=\xi$, $\psi_1$ to be the Brenier potential from $\xi$ to $e^{-g}$ and $\psi_2$ to be the Brenier potential from $\rho$ to $e^{-g}$. Note that 
    $$\frac{1}{2}\wass_2^2(\xi,e^{-g})=\int \left(\frac{1}{2}\lVert x\rVert^2-\psi_1(x)\right)\,d\xi(x)+\int \left(\frac{1}{2}\lVert y\rVert^2-\psi_1^*(y)\right)e^{-g(y)}\,dy.$$
    We can write a similar relation with $\rho$ and $\psi_2$. Some simple algebra then yields:
    $$D_U(\xi|\rho)=\mgf{\xi}{\psi_1}{\psi_2}\ge 0.$$
    Given any $x,y\in\R^d$, note that by assumption
    $$\psi_2^*(y)\le \psi_2^*(x)+\iprod{y-x,\nabla\psi_2^*(x)}+L\lVert y-x\rVert^2.$$
    Therefore, 
    \begin{align*}
        \psi_2(z)&\ge \sup_{y\in\R^d} \left(\iprod{y,z}-\psi_2^*(x)-\iprod{y-x,\nabla\psi_2^*(x)}-L\lVert y-x\rVert^2\right)\\ &=-\psi_2^*(x)+\iprod{x,z}+\frac{1}{4L}\lVert z-\nabla\psi_2^*(x)\rVert^2.
    \end{align*}
    By putting $z=\nabla\psi_1^*(x)$ and integrating over the measure $e^{-g}$, we get:
    \begin{align*}
        &\;\;\;\;\;\int \left(\psi_2(\nabla\psi_1^*(y))+\psi_2^*(y)\right)e^{-g(y)}\,dy-\int \iprod{y,\nabla\psi_1^*(y)}e^{-g(y)}\,dy\\ &\ge \frac{1}{4L}\int \lVert \nabla\psi_1^*(y)-\nabla\psi_2^*(y)\rVert^2 e^{-g(y)}\,dy=\lot{e^{-g}}(\xi,\rho).
    \end{align*}
    By using a change of variable formula, the first term in the above display simplifies to
    $$\int \left(\psi_2(\nabla\psi_1^*(y))+\psi_2^*(y)\right)e^{-g(y)}\,dy=\int \psi_2(x)\,d\xi(x)+\int \psi_2^*(y)e^{-g(y)}\,dy.$$ 
    For the next term, we note that $\iprod{y,\psi_1^*(y)}=\psi_1^*(y)+\psi_1(\nabla\psi_1^*(y))$, to see that it equals 
    $$\int \iprod{y,\nabla\psi_1^*(y)}e^{-g(y)}\,dy=\int \psi_1(x)\,d\xi(x)+\int \psi_1^*(y)e^{-g(y)}\,dy.$$
    This completes the proof.
\end{proof}}

\SP{What follows is a non-rigorous argument that shows \eqref{eq:newgradflow} satisfies \eqref{eq:mirrorgradflow}. Assuming enough smoothness it should be possible to be rigorously argued. If \eqref{eq:mirrorgradflow} has a unique solution, it also proves the converse.} 
\bigskip

\paragraph{\bf Interpretation using the Wasserstein distance.}

Let $(\rho_t, v_t\; t\ge 0)$ be the solution of the continuity equation $\dot{\rho_t}+ \div(v_t \rho_t)=0$. Assume that $v_t \in \tanspace_{\rho_t}$. The following formula is a special case of \cite[Theorem 5.24]{santambrogio2015optimal}:
\begin{equation}\label{eq:derivwass}
    \frac{d}{dt} \wass_2^2\left(\rho_t, e^{-g} \right) = 2\int \rho^U_t(x) \cdot v_t(x) \rho_t(x) dx, 
\end{equation}
where $\rho^U_t=\mathbf{id} - u_t$ is the gradient of the Kantorovich potential transporting $\rho_t$ to $e^{-g}$.

Consider the pair of infinitesimally close time points $t$ and $t+dt$. In the nonrigorous argument below we will drop every term $o(dt)$. This should be true by boundedness assumption on terms and the absolute continuity of all curves. 

By \eqref{eq:derivwass},
\[
\begin{split}
\wass_2^2\left(\rho_{t+dt}, e^{-g} \right)&= \wass_2^2\left(\rho_t, e^{-g} \right) + \left(2\int \rho_t^U(x) \cdot v_t(x) \rho_t(x) dx \right)dt\\
&=\int \norm{\rho_t^U(x)}^2\rho_t(x) dx +  \left(2\int \rho_t^U(x)  \cdot v_t(x) \rho_t(x) dx \right)dt\\
&=\int \norm{\rho_t^U(x)  + v_t(x) dt}^2 \rho_t(x)dx. 
\end{split}
\]
The second equality above follows from definition of $\rho^U_t$ and the third follows by dropping the $(dt)^2$ term.

Now, by the continuity equation, $\rho_{t+dt}(x)= \rho_t(x) - \div(v_t(x)\rho_t(x)) dt$. Substituting in the last integral and one integration by parts, one gets
\[
\begin{split}
&\wass_2^2\left(\rho_{t+dt}, e^{-g} \right)=\int \norm{\rho_t^U(x) + v_t(x) dt}^2 \rho_{t+dt}(x)dx\\
&+ \left(\int \nabla \norm{\rho_t^U(x) + v_t(x) dt}^2 v_t(x)\rho_t(x)dx\right) dt\\
&=\int\left( \norm{\rho_t^U(x) + v_t(x) dt}^2 + \nabla \norm{\rho_t^U(x) + v_t(x) dt}^2 v_t(x) dt\right)\rho_{t+dt}(x)dx.
\end{split}
\]
The final equality is allowed by ignoring $O(dt)$ terms that arise by substituting $\rho_{t+dt}$ for $\rho_t$ in the second line which then gets multiplied by $dt$ to give us a $(dt)^2$ term.

Now, let us look at the integrand. Ignoring repeatedly terms that are $(dt)^2$,
\[
\begin{split}
   & \norm{\rho_t^U(x) + v_t(x) dt}^2 + \nabla \norm{\rho_t^U(x) + v_t(x) dt}^2 v_t(x) dt\\
   &= \norm{\rho_t^U(x) + v_t(x) dt}^2 +  2\left(\rho_t^U(x) + v_t(x) dt\right)\left( \nabla \rho_t^U(x) \right) v_t(x) dt\\
   &=\norm{\rho_t^U(x)  + v_t(x) dt}^2 + 2\left(\rho_t^U(x)  \right)\left(  \frac{\partial x^{u_t}}{\partial x\hfill} - I \right) v_t(x) dt\\
   &= \norm{\rho_t^U(x)  + \frac{\partial x^{u_t}}{\partial x\hfill} v_t(x) dt }^2.
\end{split}
\]
Thus, 
\[
\wass_2^2\left(\rho_{t+dt}, e^{-g} \right)= \int \norm{\rho_t^U(x) + \frac{\partial x^{u_t}}{\partial x\hfill} v_t(x) dt }^2\rho_{t+dt}(x)dx. 
\]
But this tells us that 
\[
\rho_{t+dt}^U = \rho_t^U(x) + \frac{\partial x^{u_t}}{\partial x\hfill} v_t(x) dt.
\]
In other words, 
\[
\frac{d}{dt} \rho_t^U = \frac{\partial x^{u_t}}{\partial x\hfill} v_t(x).
\]

Now, consider \eqref{eq:newgradflow} for which $v_t=-\nabla_{x^{u_t}}\left(\frac{\delta F}{\delta \rho_t} \right)$ by \eqref{eq:expgrad}. Substituting this expression in the above display and using chain rule for derivatives give us \eqref{eq:mirrorgradflow}. By a change fo sign it gives \eqref{eq:mirrorgradflow2} which gives the PMA \eqref{eq:pma}.

%The italicized phrase above is implied by showing that the solution to \eqref{eq:expgrad}, once started from a convex gradient, remains a convex gradient. 

\bigskip


{\color{blue}
\paragraph{\bf Interpretation using the graph of the mappings.}
As another view point, 
we will derive \eqref{eq:newgradflow} from \eqref{eq:mirrorgradflow} using the geometry of the graph of the mappings. 

The graph of the mapping $T_t : \Omega \to \Lambda$ is a set in the product space $\Omega \times \Lambda$ of the source and the target domain. At each point $(x, T_t (x))$ in the graph, we can consider the tangent vector  $(v, w)$ consisting of the horizontal vector $v$ and the vertical vector $w$; here the horizontal vector $v$ is nothing but a tangent vector of the source domain at $x$ and the vertical vector $w$ is a tangent vector of the target domain at $T_t (x)$.

The graph of $T_t$ as a set $\Sigma_t$  in the product space of the source and the target, can also be seen as the graph of the inverse mapping $(T_t)^{-1}$. Then, the derivative $\frac{\partial}{\partial t} T_t (x)$ corresponds to the change of the graph $\Sigma_t$  following the vertical vector field  $\frac{\partial}{\partial t} T_t (x)$ at $(x, T_t(x))$,  while 
the derivative $\left[\frac{\partial}{\partial t} (T_t)^{-1}\right] (T_t(x))$ corresponds to the change of the graph $\Sigma_t$ following the horizontal vector field  $\left[\frac{\partial}{\partial t} (T_t)^{-1}\right] ( T_t(x))$ at $(x, T_t(x))$. 
These horizontal and vertical vectors at $(x, T_t(x))$ are related in the following way (from chain rule)



 Moreover, push-forward of $\rho_t$ by $T_t$ is $e^{-g}$, in other words, $\rho_t$ is the result of the push-forward of $e^{-g}$ by $(T_t)^{-1}=(\nabla u_t)^{-1}$. One can view the change of $\rho_t$ in the time $t$  as the result of the moving mass on source domain by the horizontal vector field $\left[\frac{\partial}{\partial t} (T_t)^{-1}\right] ( T_t(x))$.  Therefore, $\rho_t$ follows the continuity equation:
\begin{align*}
    \frac{\partial}{\partial t} \rho_t(x)  + \div \left(\rho_t (x) \left[\frac{\partial}{\partial t} (T_t)^{-1} \right] (T_t(x))\right) =0.
\end{align*}
%To see this last equation, notice that $\rho_t (x) = e^{-g (T_t(x)) } \det [\nabla T_t (x)]$. If we differentiate this in $t$ we get
%\begin{align*}
%  \frac{\partial}{\partial t}  \rho_t (x) & = 
 %   \left( \frac{\partial}{\partial t}  e^{-g (T_t(x)) }\right) \det [\nabla T_t (x)] + 

%    e^{-g (T_t(x)) } \frac{\partial}{\partial t}  \det [\nabla T_t (x)].
%\end{align*}
%We use the formula $\frac{\partial}{\partial t} \det A(t) = (\det A(t)) \tr A(t)^{-1}\frac{\partial}{\partial t} A(t)  $.


Now since 
we have that  $$\frac{\partial}{\partial t}\rho_t^U  = - \frac{\partial}{\partial t} \nabla u_t =  - \frac{\partial}{\partial t} T_t.$$  Therefore, when 
$$\frac{\partial}{\partial t}\rho_t^U =-\nabla_{\mathbb{W}} F (\rho_t) $$ as in   \eqref{eq:mirrorgradflow} we see from \eqref{eq:inverse_derivative} that 
\begin{align*}
    \frac{\partial}{\partial t} \rho_t(x)  - \div \left(\rho_t (x) [\nabla^2 u_t ]^{-1}(x) \left[\nabla_{\mathbb{W}} F (\rho_t)\right] (x)  \right) =0
\end{align*}
as in \eqref{eq:newgradflow}.



}
------------------------------------------


%On the JKO scheme

One may also ask about the corresponding implicit mirror scheme, which is the JKO scheme for gradient flow. For the choice of $U$ given above and a $\rho \in \probspace$ one can define the following Bregman divergence at $\rho$:
\[
D_U( \xi \mid \rho )= \frac{1}{2}\wass_2^2(\xi, e^{-g}) - \frac{1}{2}\wass_2^2(\rho, e^{-g}) - \iprod{ \nabla_{\wass}U(\rho), \nabla \gamma_{\rho}^{\rho'}}_{\rho},
\]
where $\xi\in \probspace$ is absolutely continuous with density also denoted by $\xi$, and $\gamma_{\rho}^{\xi}$ is  the direction of the generalized geodesic joining $\rho$ to $\xi$ with base at $e^{-g}$ (\cite[Definition 9.2.2]{ambrosio2005gradient}). That is 
\[
\gamma_{\rho}^{\xi}= \mathbf{t}^{\xi} - \mathbf{\rho},
\]
where $\mathbf{t}^\xi$ is the gradient of the Kantorovich potential transporting $e^{-g}$ to $\xi$ and $\mathbf{t}^\rho$ is defined similarly. 

Since $U$ is generalized geodesically convex $D_U(\cdot \mid \cdot)\ge 0$. Although we do not prove this in this article, we conjecture that a JKO scheme with this divergence will convergence to mirror gradient flow \eqref{eq:velmirror}. 

{\color{blue} ND (based on discussion with YH). An alternate way of approaching the JKO scheme would be to consider the following Bregman divergence in continuum:
\[
D_U( \xi \mid \rho )= \frac{1}{2}\wass_2^2(\xi, e^{-g}) - \frac{1}{2}\wass_2^2(\rho, e^{-g}) - \iprod{ \nabla_{\fv}U(\rho), \xi-\rho}.
\]
Here $\nabla_{\fv}$ denotes the first variation. We can argue $D_U(\cdot|\cdot)\ge 0$ as follows: note that $\nabla_{\fv}U(\rho)=\phi$, the $c$-concave (with $c(x,y)=\lVert x-y\rVert^2/2$) Kantorovich potential from $\rho$ to $e^{-g}$. Therefore,
$$\frac{1}{2}\wass_2^2(\rho,e^{-g})=\int \phi(x)\,d\rho(x)+\int \phi^c(y)e^{-g(y)}\,dy.$$
By duality and the $c$-concavity of $\phi$, we also get:
\begin{equation}\label{eq:ineqdual}
\frac{1}{2}\wass_2^2(\xi,e^{-g})\ge \int \phi(x)\,d\xi(x)+\int \phi^c(y)e^{-g(y)}\,dy.
\end{equation}
By combining the two displays above, we get:
$$D_U(\xi|\rho)\ge \int \phi(x)\,d(\xi-\rho)(x)-\iprod{ \nabla_{\fv}U(\rho), \xi-\rho}=0.$$
Further, equality in the above holds if and only if equality holds in \eqref{eq:ineqdual}. If, for instance, $\xi$ has a positive density, then by uniqueness of Kantorovich potentials, equality holds if and only if $\xi=\rho$. This I believe is a reformulation of the fact that $\wass_2^2(\cdot,e^{-g})$ is strictly convex on the line (see \cite[Proposition 7.19]{santambrogio2015optimal}). With this in view, one potential way to construct an implicit JKO type scheme is to interatively minimize the following:
$$\argmin_{\rho} \left[\KL{\rho^{(k)}}{e^{-f}}+\iprod{\log{\rho^{(k)}}+f,\rho-\rho^{(k)}}+\tau^{-1}D_U(\rho|\rho^{(k)})\right],$$
for some small $\tau>0$. This is the implicit mirror descent scheme with an appropriate choice of the mirror function (see \cite{leger2021gradient,nemirovskii83,beck2003mirror}). Algebraically this seems to be harder to handle even with the observation $D_U(\xi|\rho)\ge 0$ (I believe we would need $D_{\KL{\cdot}{e^{-f}}}(\xi|\rho)\le D_U(\xi|\rho)$ to make it work rigorously; this is achievable in the fixed $\vep$ case; see \cite[Lemma 4]{leger2021gradient}).\par

Instead we can look at the explicit mirror descent scheme which is more stable, and given by:
$$\rho^{(k+1)}:=\argmin_{\rho}\left[\KL{\rho}{e^{-f}}+\tau^{-1}D_U(\rho|\rho^{(k)})\right].$$
The objective function is lower bounded, lower semi-continuous and convex on lines (see \cite[Proposition 7.19]{santambrogio2015optimal}). To guarantee existence of a minimizer, we can proceed as in the proof of \cite[Proposition 8.5 and Lemma 8.6]{santambrogio2015optimal} which does not seem to require any other property. The stationary conditions would then be given by
$$\phi^{(k+1)}-\phi^{(k)}=-\tau\left(\log{\rho^{(k+1)}}+f\right),$$
where $\phi^{(k+1)}$ and $\phi^{(k)}$ are the Kantorovich potentials from $\rho^{(k+1)}$ and $\rho^{(k)}$ to $e^{-g}$. The Brenier Potentials are then given by $u^{(k)}:=\lVert \cdot\rVert^2/2-\phi^{(k)}$ and consequently,
$$u^{(k+1)}-u^{(k)}=\tau\log{\rho^{(k+1)}}=\tau\log{\rho^{(k)}}+o(\tau)=\tau\left(f(x)-g(x^{\phi^{(k)}})+\ldet\frac{\partial x^{\phi^{(k)}}}{\partial x\hfill}\right)+o(\tau).$$
Therefore, in the scaling limit, we should get back the PMA \eqref{eq:pma}. Next note that by optimality,
$$\KL{\rho^{(k+1)}}{e^{-f}}+\tau^{-1}D_U(\rho^{(k+1)}|\rho^{(k)})\le \KL{\rho^{(k)}}{e^{-f}}.$$
By summing over $k$, we get:
$$\sum_{k=0}^T D_U(\rho^{(k+1)}|\rho^{(k)})\le \tau \KL{\rho^{(T+1)}}{e^{-f}}.$$
Under some assumptions $D_U(\cdot|\cdot)$ seems to have a nice connection with the linearized OT distance; see  \cite{Wang2013,moosmuller2020linear,merigot2020quantitative} for the relevant definition an applications.

---------------------------------------

%\subsection{Connections to Linearized optimal transport}

\begin{lmm}
Suppose $\xi$, $\rho$ have positive densities on $\R^d$. Also suppose that the Brenier potential from $e^{-g}$ to $\rho$ has a Hessian with operator norm upper bounded uniformly by $2L>0$. Then we have
$$D_U(\xi|\rho)\ge \frac{1}{4L}\lot{e^{-g}}(\xi,\rho).$$
\end{lmm}

\begin{remark}
    In fact, if we put both upper and lower bounds (say $2L$ and $2/L$) on the operator norm of the Hessian of the Brenier potential from $e^{-g}$ to $\rho$, then we get the following bound:
    $$4L\lot{e^{-g}}(\xi,\rho)\ge D_U(\xi|\rho)\ge \frac{1}{4L}\lot{e^{-g}}(\xi,\rho).$$
\end{remark}
{\color{red} Does this help understand the Hessian corresponding to $U$ in some way? If we take $\xi=\rho+d\rho$ with $d\rho$ an appropriately chosen small perturbation? Can't formalize it yet.}
}


%%% Earlier attempts to show process convergence

\section{Convergence to Markov process}\label{sec:convproc}


Suppose $(X_k^{(\vep)},\; k =0,1,2,\ldots)$ refer to the Sinkhorn Markov chain with an initial condition $X_0^\vep=x_0$ that does not depend on $\vep$. Turn this to a continuous time process by a piecewise constant interpolation:
\[
X_t^{\vep} := X_k^{\vep}, \quad \text{if}\; k\vep \le t < (k+1)\vep. 
\]
Let $(u_t)$ be the corresponding solution of the PMA. Our objective will be to show the following. Let $P^\vep$ denote the law of the process $(X_t^{\vep},\; t\ge 0)$ on the Skorokhod space $D^d[0, \infty)$ of RCLL paths from $[0,\infty)$ to $\R^d$, then 
\begin{enumerate}
    \item $\left( P^\vep,\; \vep >0 \right)$ is a tight family of probability distributions and any limit point has full measure over the set of continuous paths $C^d[0, \infty)$.
    \item Any limit point is a solution of the \textit{martingale problem} (MP) \cite[Chapter 5, Definition 4.5]{karatzas1991brownian} associated with the SDE \eqref{eq:diffSDE}. 
\end{enumerate}
%An explanation of what MP means follows. Given that we have shown SDE \eqref{eq:diffSDE} admits a unique weak solution, the two steps outlined above shows weak convergence of $(X_t^\vep,\; t\ge 0)$ to a solution of \eqref{eq:diffSDE}.



%Reversing the diffeomorphism, one can now show that $(X_t^\vep,\; t\ge 0)$, as $\vep \rightarrow 0+$, converges weakly to a solution of \eqref{eq:diffSDE}. 

%A definition of the martingale problem associated with an SDE can be found in \cite[Chapter 5, Definition 4.5]{karatzas1991brownian}. However, we will utilize \cite[Chapter 5, Proposition 4.6]{karatzas1991brownian} to simplify our requirement. Let $P$ denote any limit point of $P^\vep$, as $\vep \rightarrow 0+$. By the first step, it is a probability measure on $C^d[0, \infty)$ with the usual filtration generated by the coordinate projections. 

%Let $Y\sim Q$. To show that $Q$ satisfies the MP, we need to show that the following processes are (local) martingales,
%\begin{equation}\label{eq:MPreq}
%\begin{split}
%M_t(i)&=X_t(i) - \int_0^t (\text{drift at time $u$}) du, \quad i \in [d], \quad \text{and},\\
%M_t(i)&M_t(j) - 2 \int_0^t \frac{\partial X_u(i)\hfill}{\partial X_u^{w_u}(j)}du, \quad i,j\in [d]^2.
%\end{split}
%\end{equation}
%\SP{What is the proper placement of coordinates of vectors?}

%In other words, we need to verify It\^o's rule for the functions $f(y)=y(i)$ and $f(y)=y(i)y(j)$. The fact that the processes \eqref{eq:MPreq} are indeed martingales follow from our calculations on conditional expectations and considitonal variances of the Sinkhorn Markov chain. 

%To prove tightness and limiting continuity of paths as required by step (1) above we need to verify some moment estimates. See \cite[Chapter 5, problem 3.15]{karatzas1991brownian}.

%\SP{What do we need from the Markov chain?} 

Let $\left(\fil_t,\; t\ge 0\right)$ denote the usual right continuous filtration generated by the coordinate projections in $D^d[0, \infty)$. For $k\vep \le t< (k+1)\vep$, and $x\in \R^d$, define
\begin{enumerate}[(i)]
    \item $b_t^\vep(x):=\E(X_{k+1}^\vep - X^\vep_k \mid X^\vep_k=x)$. We know
    \[
    b_{k\vep}^\vep(x) = \vep \left(-\frac{\partial f\hfill}{\partial x^{u_{k\vep}}}(x)-\frac{\partial g\hfill}{\partial x^{u_{k\vep}}}\left(x^{u_{k\vep}}\right)+\frac{\partial h_{k\vep}\hfill}{\partial x^{u_{k\vep}}}(x)\right)  + o(\vep),
    \]
    where $h_{k\vep}$ is defined by the relation $e^{-h_{k\vep}}=(\nabla u_{k\vep})_{\# e^{-g}}$. 
    \item Also define the conditional covariance matrix $\Sigma^\vep_t(x)$ where, for $(i,j)\in [d]^2$,
    \[
    \Sigma^\vep_t(x)(i,j):=\mathrm{Cov}\left(X_{k+1}^\vep(i), X_{k+1}^\vep(j)  \mid X_k=x \right)\approx 2\vep\frac{\partial x(i)\hfill}{\partial x^{u_{k\vep}}(j)} + o(\vep).
    \]
\end{enumerate}

Consider the discrete time vector-valued martingale $(M_{k\vep}^\vep,\; k=0,1,2,\ldots)$ given by $M_0^\vep=0$ and, inductively,
\begin{equation}\label{eq:definemgle}
M_{(k+1)\vep}^\vep - M_{k\vep}^\vep = \sqrt{\vep}\left(\Sigma^\vep_{k\vep}(X_k^\vep) \right)^{-1/2} \left( X_{k+1}^\vep - X^\vep_k - b_k^\vep(X^\vep_k) \right).
\end{equation}
Extend the process to $\left(M_t^\vep,\; t\ge 0 \right)$ for piecewise interpolation as above. By construction, this discrete time martingale $\left( M_{k\vep}^\vep, \; k=0,1,\ldots \right)$, with respect to the filtration $\left(\fil_{k\vep},\; k=0,1,2,\ldots \right)$, satisfies
\begin{equation}\label{eq:condcovmglediff}
\mathrm{Cov}\left( M_{(k+1)\vep}^\vep - M_{k\vep}^\vep \mid \fil_{k\vep}\right)= \vep I_{d\times d},
\end{equation}
where $I_{d\times d}$ is the $d$-dimensional identity matrix.

The original process $\left( X_{k}^\vep\right)$ can be recovered as a discrete time stochastic integral with respect to this martingale. To wit, 
\begin{equation}\label{eq:discstocinteg}
X_{k+1}^\vep = x_0 + \sum_{i=0}^k b_{i\vep}^\vep(X^\vep_i) + \vep^{-1/2}\sum_{i=0}^k  \left(\Sigma^\vep_{i\vep}(X_i^\vep) \right)^{1/2} \left( M_{(i+1)\vep}^\vep - M_{i\vep}^\vep\right),
\end{equation}
where the integrand is predictable with respect to the filtration $\left(\fil_{k\vep},\; k=0,1,\ldots \right)$. 

Our proof proceeds by showing two lemmas. 

\begin{lmm}\label{lem:mglecnv}
    As $\vep\downarrow 0$, $(M_t^\vep, \; t\ge 0)$ converges weakly to a standard $d$-dimensional Brownian motion in $D^d[0, \infty)$ equipped with the locally uniform metric.
\end{lmm}

\begin{proof}
    This is a consequence of martingale CLT \cite[Theorem 8.2.4]{durrettprob}. To apply this theorem, consider the sequence of discrete martingale $(M_{k\vep}^\vep,\; k=0,1,\ldots)$ with their corresponding filtration $(\fil_{k\vep},\; k=0,1,\ldots)$. Take any arbitrary sequence $\vep_n\downarrow 0$. For $i\in [d]$, consider the real-valued martingale sequence $(M_{k\vep_n}^{\vep_n}(i),\; k=0,1,\ldots)$
    By definition, using the notation in \cite[Theorem 8.2.4]{durrettprob}, for any $t >0$,
    \[
    V_{n, [t/\vep_n]}(i):= \sum_{k=1}^{[t/\vep_n]} \E\left( \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)^2 \mid \fil_{k\vep}\right)= \vep_n [t/\vep_n] \rightarrow t, 
    \]
    as $n\rightarrow \infty$. 

    Condition (ii) in \cite[Theorem 8.2.4]{durrettprob} follows from the bound
    \begin{equation}\label{eq:4momentbnd}
    \E\left( \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)^4 \mid \fil_{k\vep}\right)= o\left( \vep_n\right).
    \end{equation}
    \SP{This is the moment bound we need to argue.}
    Since, then, for any $\delta >0$, by Markov's inequality
    \[
    \begin{split}
    \E&\left( \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)^2 1\left\{ \abs{M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i)} > \delta \right\} \mid \fil_{k\vep}\right)\\
    &\le \delta^{-2} \E\left( \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)^4 \mid \fil_{k\vep}\right).
    \end{split}
    \]
    Hence, 
    \[
    \begin{split}
    \sum_{k=0}^{1/\vep_n}& \E\left( \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)^2 1\left\{ \abs{M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i)} > \delta \right\} \mid \fil_{k\vep}\right)\\
    &\le \frac{1}{\delta\vep_n}o\left( \vep_n\right),\quad \text{ which goes to zero as $n\rightarrow \infty$}.
    \end{split}
    \]
   Thus, by the cited Theorem, every $(M_t^{\vep_n},\; t\ge 0)$ converges weakly, in the locally uniform metric, to a standard linear Brownian motion. Since this is true for every $i$ and every sequence of $\vep_n\downarrow 0$, it follows that the vector-valued process $\left(M^{\vep}_{t},\; t\ge 0\right)$ converges jointly, weakly in the locally uniform metric on $D^d[0, \infty)$, to a process $(B_t,\; t\ge 0)$, whose every coordinate process is a standard Brownian motion. 

    The fact $(B_t,\; t\ge 0)$ is a $d$-dimensional standard Brownian motion follows from Knight's Theorem \cite[Chapter 3, Theorem 4.13]{karatzas1991brownian}. Essentially, it suffices to show that the mutual variation between the coordinate processes of $B$ is zero throughout, which, in turn, follows from \eqref{eq:condcovmglediff} that the off-diagonal elements in the conditional covariance matrix of the martingale increments is zero by construction. By our established weak convergence and Skorokhod's Theorem, on a certain probability space, for every fix $t$, and every $i\neq j$, the following convergence holds in probability, 
    \[
    \lim_{n\rightarrow \infty} \sum_{k=1}^{[t/\vep_n]} \left(M_{(k+1)\vep_n}^{\vep_n}(i) - M_{k\vep_n}^{\vep_n}(i) \right)\left(M_{(k+1)\vep_n}^{\vep_n}(j) - M_{k\vep_n}^{\vep_n}(j) \right) = \iprod{B(i), B(j)}_t
    \]
    The fact that the limit on the left side of above is zero follows by taking expectation and use \eqref{eq:4momentbnd} to show convergence in probability. We skip the standard argument. 
\end{proof}


\begin{lmm}  
    The pair of processes $(X_t^\vep,\; M_t^\vep\; t\ge 0)$ converges weakly, in the locally uniform metric on $D^d[0, \infty) \times D^d[0, \infty)$ to a solution $(X,B)$ of the SDE \eqref{eq:diffSDE}. 
\end{lmm}


\begin{proof}
    As in the proof of the last lemma, it suffices to take a sequence $\vep_n\rightarrow 0$. For simplicity, we will take $\vep_n=1/n$, but the argument below does not depend on this particular choice. 

    By Lemma \ref{lem:mglecnv} and Skorokhod's theorem, on some filtered probability space, we may assume that, for each $T>0$,  
    \[
    \lim_{n\rightarrow \infty}\sup_{0\le t \le T} \norm{M_t^{\vep_n} - B_t}_2=0
    \]
    On this filtered probability space, by \eqref{eq:discstocinteg}, one can construct a copy of each $(X_k^{\vep_n},\; k =1,2,\ldots)$. 

    Our first claim is that the pair $(X_t^{\vep_n}, M_t^{\vep_n},\; t\ge 0)$ is jointly tight. Since the second coordinate has an almost sure limit, it suffices to argue the tightness of the first coordinate process itself.  

    In order to do this, we will first assume that the drift and the diffusion coefficients of \eqref{eq:diffSDE} are bounded and uniformly continuous. 
    
    First we compare $X_t^{\vep_n}$ with $\tilde{X}_t^{\vep_n}$, the piecewise constant interpolation of the discrete process 
    \begin{equation}\label{eq:discstocinteg2}
        \tilde{X}_{(k+1)\vep_n}^{\vep_n} = x_0 + \sum_{i=0}^k \tilde{b}_{i\vep_n}^{\vep_n}(\tilde{X}^{\vep_n}_{i\vep_n}) + \vep^{-1/2}\sum_{i=0}^k  \left(\tilde{\Sigma}^{\vep_n}_{i\vep_n}(X_{i\vep_n}^{\vep_n}) \right)^{1/2} \left( B_{(i+1)\vep_n}^{\vep_n} - B_{i\vep_n}^{\vep_n}\right),
    \end{equation}
    where $\tilde{b}_{i{\vep_n}}^{\vep_n}, \tilde{\Sigma}_{i\vep_n}^{\vep_n}$ are ${b}_{i\vep_n}^{\vep_n}, {\Sigma}_{i\vep_n}^{\vep_n}$, respectively, without the $o(\vep_n)$ error terms.  

    It can be shown that, for any $T>0$, 
    \[
    \lim_{n\rightarrow \infty}\sup_{0\le t \le T} \norm{X_t^{\vep_n} - \tilde{X}_t^{\vep_n}}=0.
    \]

    One the other hand, pathwise,  
    \[
    \tilde{X}_t^{\vep_n} = x_0 + \int_0^t \tilde{b}_{s}^{\vep_n}(\tilde{X}_u^{\vep_n})du + \int_0^t {\Sigma}_{u}^{\vep_n}(\tilde{X}_u^{\vep_n})dB_u. 
    \]
    Now, as $n\rightarrow \infty$, the convergence of $\tilde{X}^{\vep_n}$ to $X$, as in the solution of \eqref{eq:diffSDE} follows from the Dominated Convergence Theorem for stochastic integrals. 

    One can now relax the assumption of bounded, uniformly continuous coefficients via localization by stopping the process $X$ as it exits a sequence of balls around the origin of larger and larger radii. This completes the proof.  
\end{proof}


-----------------------------------------

% Some formal calculations

Some likely examples we can write  can be found in \cite[Section 8.4.2]{santambrogio2015optimal}.


A consequence of this Riemannian formalism is the following chain rule identity 
\begin{equation}\label{eq:rateofdecay}
\frac{d}{dt} F(\rho_t)= -\norm{\nabla_{\wass}^U F(\rho_t)}^2_U
\end{equation}
which shows that $t\mapsto F(\rho_t)$ decreases strictly along the mirror gradient flow until the first order condition is satisfied. For the explicit formula \eqref{eq:expgrad}, by chain rule we obtain
\begin{equation}\label{eq:rateofdecayexp}
\begin{split}
    \frac{d}{dt} F(\rho_t)&=- \int \iprod{\nabla_{x^u}\left(\frac{\delta F}{\delta \rho}\right), \nabla_{x}\left(\frac{\delta F}{\delta \rho}\right)} \rho(dx)\\
    &=- \int \iprod{\nabla_{x}\left(\frac{\delta F}{\delta \rho}\right), \nabla_x^2 u_t(x) \nabla_{x}\left(\frac{\delta F}{\delta \rho}\right)} \rho(dx).
 \end{split}   
\end{equation}
Thus one can compare the rate of decay of $F$ along this gradient flow to the usual gradient flow by checking how aligned the two vector fields $\nabla_{x^u}\left(\frac{\delta F}{\delta \rho}\right)$ and $\nabla_{x}\left(\frac{\delta F}{\delta \rho}\right)$ are in $\ltwo(\rho)$. 
\begin{comment}
\item (Integral bounds) Let $Z_{\vep,C}\sim N(0,\sqrt{\vep C} I_d)$ for $C>0$. Define the functions $\Theta_{C,\vep}:\R^d\to\R$ for $k\ge 1$ and $\vep>0$ as follows:
\begin{equation}\label{eq:tdef}
\Theta_{C,\vep}(y):=\E_{Z_{\vep,C}}\left[\left(\E_{|Z_{\vep,C}}\exp\left(-f((y+Z_{\vep,C})^{u_{k\vep}^*}+Z_{\vep,\frac{1}{C}})\right)\right)^{-1}\exp(-g(y+Z_{\vep,C}))\right].
\end{equation}
Assume that 
$$\tilde{\theta}_C:=\sup_{k\vep\le T}\E\left(\log{\Theta_{C,\vep}(Y)}\right)^2<\infty,$$
for some $\vep$ small enough.
\item (Moment generating function type integral bounds) For $\bm{j}:=(j_1,j_2,j_3,j_4,j_5)$ a $5$-tuple of reals, define the functions $\omega_{\bm{j},\vep}$ for $k\ge 1$ and $\vep>0$ as follows:
\begin{align}\label{eq:omdef}
\omega_{\bm{j},\vep}(x)&:=\E_{Z_{\vep,j_5}}\bigg[\bigg(\E_{|Z_{\vep,j_5}}\exp\bigg(-(j_1+j_2\lVert x\rVert)\lVert Z_{\vep,(j_5)^{-1}}\rVert-j_3 \lVert Z_{\vep,(j_5)^{-1}}\rVert^2\nonumber \\&-f((x^{u_{k\vep}}+Z_{\vep,j_5})^{u_{k\vep}^*}+Z_{\vep,(j_5)^{-1}})\bigg)\bigg)^{-1}\exp\big(-g(x^{u_{k\vep}}+Z_{\vep,j_5})\nonumber \\ &+(j_4)^{-1}(j_1+j_2\lVert x\rVert)\lVert Z_{\vep,j_5}\rVert+j_3(j_4)^{-2}\lVert Z_{\vep,j_5}\rVert^2\big)\bigg]
\end{align}
Assume that 
$$\tilde{\omega}_{\bm{j}}:=\sup_{k\vep\le T}\E\left(\log{\omega_{\bm{j},\vep}(Y^{u_{k\vep}^*})}\right)^2<\infty,$$
for some $\vep$ small enough.
\end{comment}

-------------------------

As a second application of our gradient flow formulation, we have the following perturbation result for Sinkhorn PDE.

\begin{prop}\label{prop:perturb}
    Let $(\rho_t,\; t\ge0)$ and $(\rho_t',\; t\ge 0)$ denote two solutions of the Sinkhorn PDE. Suppose $e^{-f}$ is log-concave with $\nabla^2f \ge \lambda I$, uniformly for some $\lambda >0$ and 
    \[
    \inf_{x\in\R^d}\lmn\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right), \inf_{x\in\R^d}\lmn\left(\frac{\partial x\hfill}{\partial x^{u'_t}}\right)\ge h>0.
    \]
    Then 
    \[
    \wass_2(\rho_t, \rho_t') \le \dlt{}{e^{-g}}{\rho_t}{\rho_t'} \le \dlt{}{e^{-g}}{\rho_0}{\rho'_0}e^{-\lambda h t}, \quad \forall \; t\ge 0.
    \]
\end{prop}

\begin{proof}[Proof of Proposition \ref{prop:perturb}]
Recall the concept of generalized geodesic semiconvexity \cite[Definition 9.2.4]{ambrosio2005gradient}. To put everything in our context, let $\sigma, \sigma'$ be any two probability measures in the Wasserstein space. A generalized geodesic from $\sigma$ to $\sigma'$ is given by the curve 
\begin{equation}\label{eq:gengeosigma}
\sigma_t^{\sigma \rightarrow \sigma'}:=\left( (1-t)y^w + t y^{w'}\right)_{\#e^{-g}},
\end{equation}
where $w$ and $w'$ are the convex Brenier potentials transporting $e^{-g}$ to $\sigma$ and $\sigma'$, respectively. 


For $\lambda \in \rr$, a function $\phi$ is said to be $\lambda$-semiconvex on the generalized geodesics if, for all $\sigma, \sigma'$, 
\begin{equation}\label{eq:gengeocon}
\phi(\sigma_t^{\sigma \rightarrow \sigma'})\le (1-t) \phi(\sigma) + t \phi(\sigma') - \lambda t(1-t) \dlt{2}{e^{-g}}{\sigma}{\sigma'}.
\end{equation}
It is known, see \cite[Section 9.3]{ambrosio2005gradient} that if $\nabla^2 f\ge \lambda I$, uniformly, for some $\lambda >0$, then $\phi(\sigma):=\KL{\sigma}{e^{-f}}$ is $\lambda$-semiconvex on generalized geodesics. Since $\lambda>0$, this implies uniform convexity over generalized geodesics.

The proof follows closely the argument in \cite[Theorem 11.1.4]{ambrosio2005gradient} with the role of $\wass_2$ replaced by $\dlt{}{e^{-g}}{}{}$. 

For any $\sigma$ in the Wasserstein space, consider the function 
\[
\gamma_t(\rho, \sigma)= \frac{d}{dt}\dlt{2}{e^{-g}}{\rho_t}{\sigma},
\]
assuming it exists $t$ a.e.. Similarly, consider $\gamma_t(\rho', \sigma)$. 

Let $\omega(t):=\dlt{2}{e^{-g}}{\rho_t}{\rho_t'}$. Then, following the above cited proof, by \cite[Lemma 4.3.4]{ambrosio2005gradient}, a.e.,  
\begin{equation}\label{eq:dlotderiv}
\dot{\omega}(t)= \gamma_t(\rho, \rho'_t)+ \gamma_t(\rho', \rho_t). 
\end{equation}
In order to evaluate the above, note
\[
\gamma_t(\rho, \sigma)= \frac{d}{dt} \int \norm{y^{w_t} - y^v}^2 e^{-g(y)}dy,
\]
where $w_t$ and $v$ are the convex Brenier potentials transporting $e^{-g}$ to $\rho_t$ and $\sigma$, respectively. By our assumption $T_t:=\nabla u_t=(\nabla w_t)^{-1}$ satisfies the PMA $\dot{T}_t= - \nabla_{\wass}\phi(\rho_t)$.

Thus
\[
\begin{split}
\gamma_t(\rho, \sigma) &= 2\int (y^{w_t}- y^v) \cdot \frac{d}{dt} (T_t)^{-1}(y) e^{-g(y)}dy\\
&= 2\int \left(x - \nabla v \circ T_t(x)\right) \cdot \frac{d}{dt} (T_t)^{-1}(T_t(x)) \rho_t(x)dx, 
\end{split}
\]
by a change of variable $y=T_t(x)$. By the chain rule for inverse functions \eqref{eq:inverse_derivative}, $\frac{d}{dt} (T_t)^{-1}(T_t)=-\nabla^U_{\wass}\phi(\rho_t)$. Hence, 
\[
\gamma_t(\rho, \sigma)=-2\int \left(x - \nabla v \circ T_t(x)\right) \cdot \nabla^U_{\wass}\phi(\rho_t)(x) \rho_t(x)dx.
\]

Thus, if $u_t', T_t'$ denote the corresponding quantities for $(\rho_t',\; t\ge 0)$, then, continuing from \eqref{eq:dlotderiv}, 
\begin{equation}\label{eq:omegaprime}
\begin{split}    
\dot{\omega}(t)&= -2\int \left(x - \left( T'_t\right)^{-1} \circ T_t(x)\right) \cdot \nabla^U_{\wass}\phi(\rho_t)(x) \rho_t(x)dx\\
&-2\int \left(x - \left( T_t\right)^{-1} \circ T'_t(x)\right) \cdot \nabla^U_{\wass}\phi(\rho'_t)(x) \rho'_t(x)dx.
\end{split}
\end{equation}

On the other hand, consider the AC curve $(\sigma^{\sigma \rightarrow \sigma'}_t,\; 0\le t \le 1)$ from \eqref{eq:gengeosigma}. For a compactly supported smooth test function $\xi$, 
\[
\begin{split}
\frac{d}{dt}& \int \xi(x) \sigma_t(dx)= \frac{d}{dt}\int \xi\left( (1-t)y^w + t y^{w'}\right) e^{-g(y)}dy\\
&=\int \nabla \xi((1-t)y^w + t y^{w'}) \cdot (y^{w'} - y^w) e^{-g(y)}dy.
\end{split}
\]
At $t=0$, the above expression becomes
\[
\int \nabla \xi(y^w) \cdot (y^{w'} - y^w) e^{-g(y)}dy= \int \nabla \xi(x) \cdot ( \nabla w' \circ (\nabla w)^{-1}(x) -x) \sigma(dx).
\]
Hence the velocity of this AC curve at $t=0$ is precisely $\nabla w' \circ (\nabla w)^{-1}(x) -x$.

Apply the subdifferential calculus for the function $\phi$ \cite[Sections 10.3, 10.4]{ambrosio2005gradient} to \eqref{eq:gengeocon}. By taking $t\rightarrow 0+$, 
\[
\begin{split}
\int \left( (\nabla w') \circ (\nabla w)^{-1}(x) - x\right) \cdot \nabla_{\wass}\phi(\sigma)(x) d\sigma &=\lim_{t \rightarrow 0+}\frac{\phi(\sigma_t^{\sigma \rightarrow \sigma'})- \phi(\sigma)}{t}\\
&\le \phi(\sigma') - \phi(\sigma) - \lambda \dlt{2}{e^{-g}}{\sigma}{\sigma'}.
\end{split}
\]
 

Taking $(\sigma, \sigma')$ to be $(\rho_t, \rho_t')$ and again $(\rho_t', \rho_t)$, we get the pair of inequalities
\[
\begin{split}
\phi(\rho'_t) - \phi(\rho_t) - \lambda \dlt{2}{e^{-g}}{\rho_t}{\rho_t'} &\ge -\int \left( x - (T_t')^{-1}\circ T_t(x)\right)\cdot \nabla_{\wass} \phi(\rho_t) \rho_t(x)dx\\
\phi(\rho_t) - \phi(\rho'_t) - \lambda \dlt{2}{e^{-g}}{\rho_t}{\rho_t'} &\ge -\int \left( x - (T_t)^{-1}\circ T'_t(x)\right)\cdot \nabla_{\wass} \phi(\rho_t) \rho'_t(x)dx.
\end{split}
\]
Adding the two,
\[
\begin{split}
-2\lambda \dlt{2}{e^{-g}}{\rho_t}{\rho_t'} &\ge -\int \left( x - (T_t')^{-1}\circ T_t(x)\right)\cdot \nabla_{\wass} \phi(\rho_t) \rho_t(x)dx\\
&- \int \left( x - (T_t)^{-1}\circ T'_t(x)\right)\cdot \nabla_{\wass} \phi(\rho'_t) \rho'_t(x)dx.
\end{split}
\]
To compare the above with \eqref{eq:omegaprime} we need to use the lower bound on the eigenvalues. 

\end{proof}

Also,
    \begin{align*}
        \mathcal{R}_{k\vep}(y)&:=w_{k\vep}(y)+\frac{\vep d}{2}\log{(2\pi\vep)}-\vep f(y^{w_{k\vep}})+\frac{\vep}{2}\ldet(\nabla^2 w_{k\vep}(y))\\ &\qquad -\vep^2 M[u_{k\vep},f](y)-\vep^2 \sum_{j=0}^{k-1} \mathcal{M}[u_{j\vep}](y^{w_{k\vep}}),
    \end{align*}
    
    \begin{align*}
        \mathcal{R}_{k\vep}(y;x)&:=w_{k\vep}(y)+\frac{\vep d}{2}\log{(2\pi\vep)}-\vep f(y^{w_{k\vep}})+\frac{\vep}{2}\ldet(\nabla^2 w_{k\vep}(y))\\ &\qquad -\vep^2 M[u_{k\vep},f](x^{u_{k\vep}})-\vep^2 \sum_{j=0}^{k-1} \mathcal{M}[u_{j\vep}](x).
    \end{align*}
    We define the remainder terms:
    \begin{align*}
        a_{k\vep}:=\sup_{y}\frac{1}{\vep}\bigg|\opV[u_k^{\vep}](y)-\mathcal{R}_{k\vep}(y)\bigg|,
    \end{align*}
    and 
    \begin{align*}
        \tilde{a}_{k\vep}:=\sup_{(y,x):\ \lVert y-x^{u_{k\vep}}\rVert \le r_{\vep}}\frac{1}{\vep}\bigg|\opV[u_k^{\vep}](y)-\mathcal{R}_{k\vep}(y;x)\bigg|.
    \end{align*}
    The corresponding terms for approximating the $\opU$ operator are as follows:
    \begin{align*}
        \tilde{\mathcal{R}}_{k\vep}(x)&:=u_{k\vep}(x)+\vep^2 \sum_{j=0}^{k-1} \mathcal{M}[u_{j\vep}](x),
    \end{align*}

    %In the final display, we have used the change of measure formula \cref{lem:jacobian} with $\phi=w_t$, $b=h_t$ , and $a=g$, to note that 
%    $$h_t(y^{w_t})=g(y)+\ldet\left(\frac{\partial y\hfill}{\partial y^{w_t}}\right).$$
%    By carrying out a further change of measure according to \cref{lem:jacobian} with $y=x^{u_t}$ in the final integral above, we further observe:
   

   % \begin{align*}
   % \int \bigg\lVert \frac{\partial\hfill}{\partial y} (f(y^{w_t})-h_t(y^{w_t}))\bigg\rVert^2 e^{-g(y)}\,dy&=\int \bigg\lVert \frac{\partial\hfill}{\partial x^{u_t}}(f(x)-h_t(x))\bigg\rVert^2 e^{-h_t(x)}\,dx\\ &=\int \lVert v_t(x)\rVert^2 \rho_t(x)\,dx.
   % \end{align*}
    %The last equality uses \eqref{eq:velocity}. This completes the proof of \eqref{eq:linot}.
    
    \begin{align*}
        \tilde{\mathcal{R}}_{k\vep}(x;y)&:=u_{k\vep}(x)+\vep^2 \sum_{j=0}^{k-1} \mathcal{M}[u_{j\vep}](y^{w_{k\vep}}).
    \end{align*}
    We define the remainder terms:
    \begin{align*}
        b_{k\vep}:=\sup_{x}\frac{1}{\vep}\bigg|u_k^{\vep}(x)-\tilde{\mathcal{R}}_{k\vep}(x)\bigg|,
    \end{align*}
    and 
    \begin{align*}
        \tilde{b}_{k\vep}:=\sup_{(x,y):\ \lVert x-y^{w_{k\vep}}\rVert \le r_{\vep}}\frac{1}{\vep}\bigg|u_k^{\vep}(x)-\tilde{\mathcal{R}}_{k\vep}(x;y)\bigg|.
    \end{align*}

 Note that for $k\ge 0$, 

\begin{align*}
    \Lambda & :=\exp\left(\frac{1}{\vep}\opV[u_k^{\vep}](y)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)\right)\\ &=\int \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}u_k^{\vep}(x)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)-f(x)\right)\,dx \\& = \int_{B_{r_{\vep}}(y^{w_{k\vep}})} \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x;y)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)-f(x)-\frac{1}{\vep}u_k^{\vep}(x)+\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x;y)\right)\,dx\\ &+\int_{B^c_{r_{\vep}}(y^{w_{k\vep}})} \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)-f(x)-\frac{1}{\vep}u_k^{\vep}(x)+\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x)\right)\\ &=: \Lambda_1+\Lambda_2.
\end{align*}

We begin with $\Lambda_1$. Observe that 
\begin{align*}
&\;\;\;\;\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)-f(x)-\frac{1}{\vep}u_k^{\vep}(x)+\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x;y)\\ &=\frac{1}{\vep}\mathcal{D}[u_{k\vep}](x|y)-f(x)+f(y^{w_{k\vep}})-\frac{1}{2}\ldet(\nabla^2 w_{k\vep}(y))+\vep M[u_{k\vep},f](y)-\frac{d}{2}\log{(2\pi\vep)}\\ &\qquad -\frac{1}{\vep}u_k^{\vep}(x)+\frac{1}{\vep}\tilde{\mathcal{R}}_{k\vep}(x;y).
\end{align*}
Therefore, with 
$$\Lambda_1^{(1)}:=\exp\left(f(y^{w_{k\vep}})+\vep M[u_{k\vep},f](y)\right)\frac{\sqrt{\mathrm{det}(\nabla^2 u_{k\vep}(y^{w_{k\vep}}))}}{(2\pi\vep)^{d/2}} \int\limits_{B_{r_{\vep}}(y^{w_{k\vep}})} \exp\left(\frac{1}{\vep}\mathcal{D}[u_{k\vep}](x|y)-f(x)\right)\,dx,$$
we have:
$$\exp(-\tilde{b}_{k\vep})\le \frac{\Lambda_1}{\Lambda_1^{(1)}}\le \exp(\tilde{b}_{k\vep}).$$
By \cref{lem:prelimestim}, we have:
\begin{align*}
    \big|\Lambda_1^{(1)}-1\big|\le \eta_t \left(\vep \xi_t(r_{\vep})+\vep^{3/2}(\log{(1/\vep)})^{9/2}\right).
\end{align*}
We now move to $\Lambda_2$. By invoking \cref{lem:prelimestim}, we get:
$$\Lambda_2\le \exp(b_{k\vep})\eta_t \vep^{10}.$$
Combining the above observations, we get:
\begin{align*}
    \exp\left(\frac{1}{\vep}\opV[u_k^{\vep}](y)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)\right)\ge \exp(-\tilde{b}_{k\vep})\left(1-\eta_t \left(\vep \xi_t(r_{\vep})+\vep^{3/2}(\log{(1/\vep)})^{9/2}\right)\right).
\end{align*}
Similarly, we have:
\begin{align*}
    &\;\;\;\;\exp\left(\frac{1}{\vep}\opV[u_k^{\vep}](y)-\frac{1}{\vep}\mathcal{R}_{k\vep}(y)\right)\\ &\le \exp(\tilde{b}_{k\vep})\left(1+\eta_t \left(\vep \xi_t(r_{\vep})+\vep^{3/2}(\log{(1/\vep)})^{9/2}\right)\right)+\eta_t\vep^{10}\exp(b_{k\vep}).
\end{align*}
By combining the two observations above, for $\vep>0$ small enough (depending only on $\eta_t$, we get: 
$$a_{k\vep}\le \tilde{b}_{k\vep}+\eta_t \left(\vep \xi_t(r_{\vep})+\vep^{3/2}(\log{(1/\vep)})^{9/2}\right)+\eta_t \vep^{10}\exp(b_{k\vep}).$$

We now fix $y,z$ such that $\lVert y-z^{u_{k\vep}}\rVert\le r_{\vep}$ and note that:

\vspace{0.1in}

    \emph{$k=1$ case}: 
     and $C_{D,1}$ depends on the first $4$ derivatives of $u_0$, the first $2$ derivatives of $f$ and their  modulus of continuities. Also define

    $$\omega[u_0,f](\delta):=\sup_{\lVert z_1-z_2\rVert \le \delta} \big|M[u_0,f](z_1)-M[u_0,f](z_2)\big|.$$
    Using this observation, we will invoke \cref{lem:prelimestim} for each iteration of the Sinkhorn algorithm. By applying \cref{lem:prelimestim} with $u\equiv u_0$, $G\equiv f$, and $\tilde{G}\equiv 0$, we get:
    \begin{align*}
        \lVert\mathcal{R}^{\vep}_0-\opV[u_0]\rVert_{\infty}\le C_{0} \vep^2 \left(\omega[u_0,f](\sqrt{\vep \log{(1/\vep)}})+\sqrt{\vep}(\log{(1/\vep)})^{9/2}\right)=:E_{\vep,0},
    \end{align*}
    
    Note that 
    \begin{align*}
        &\;\;\;\;\exp\left(\frac{1}{\vep}u_1^{\vep}(x)-\frac{1}{\vep}\tilde{u}_{\vep}(x)-\vep M[u_0,f](x^{u_0})\right)\\ &=\frac{\sqrt{\mathrm{det}(\nabla^2 w_0(x^{u_0}))}}{(2\pi\vep)^{d/2}}\int_{B_{r_{\vep}}^c(x^{u_0})}\exp\bigg(\frac{1}{\vep}\mcD[w_0](y|x)+G_0(y)-G_0(x^{u_0})\nonumber \\ &\qquad \qquad +\vep(M[u_0,f](y)-M[u_0,f](x^{u_0}))-\frac{1}{\vep}(\opV[u_0](y)-\mathcal{R}^{\vep}_0(y))\bigg)\,dy\\ &+\frac{\sqrt{\mathrm{det}(\nabla^2 w_0(x^{u_0}))}}{(2\pi\vep)^{d/2}}\int_{B_{r_{\vep}}(x^{u_0})}\exp\bigg(\frac{1}{\vep}\mcD[w_0](y|x)+G_0(y)-G_0(x^{u_0})\nonumber \\ &\qquad \qquad -\frac{1}{\vep}(\opV[u_0](y)-\mathcal{R}^{\vep}_0(y;x))\bigg)\,dy
    \end{align*}
    By using \eqref{eq:gradbound4}, the first term above is bounded by 
    $$C_1 \exp\left(\vep C_1+\vep E_{0,\vep}+2\vep \lVert M[u_0,f]\rVert_{\infty}\right)\vep^{10}.$$
    For the second term, note that 
    \begin{align*}
        &\;\;\;\;\log\frac{\int_{B_{r_{\vep}}(x^{u_0})}\exp\bigg(\frac{1}{\vep}\mcD[w_0](y|x)+G_0(y)-G_0(x^{u_0})-\frac{1}{\vep}(\opV[u_0](y)-\mathcal{R}^{\vep}_0(y;x))\bigg)\,dy}{\int_{B_{r_{\vep}}(x^{u_0})}\exp\bigg(\frac{1}{\vep}\mcD[w_0](y|x)+G_0(y)-G_0(x^{u_0})\bigg)\,dy}\\ &\in \left(-\sup_{y\in B_{r_{\vep}}(x^{u_0})} \frac{1}{\vep}\big|\opV[u_0](y)-\mathcal{R}^{\vep}_0(y;x)\big|,\ \sup_{y\in B_{r_{\vep}}(x^{u_0})} \frac{1}{\vep}\big|\opV[u_0](y)-\mathcal{R}^{\vep}_0(y;x)\big|\right).
    \end{align*}
    By \cref{lem:prelimestim}, we then have:
    \begin{align*}
        &\;\;\;\;\bigg|\int_{B_{r_{\vep}}(x^{u_0})}\exp\bigg(\frac{1}{\vep}\mcD[w_0](y|x)+G_0(y)-G_0(x^{u_0})\bigg)\,dy-1+\vep M[w_0,G_0](x)\bigg|\\ &\le C_2\left(\vep^{3/2}(\log{(1/\vep)})^{9/2}+\vep \omega_2(r_{\vep})\right).
    \end{align*}
    Consequently, we get:
    $$\big|u_1^{\vep}(x)-\tilde{u}_{\vep}(x)-\vep^2 M[u_0,f](x^{u_0})+\vep^2 M[w_0,G_0](x)\big|\le .$$

Next recall the notation $A_T$ and $B_T$ from \cref{asn:solcon}. By \cref{lem:prelimestim}, we further note that there exists $c_{T,2}>0$ such that 
\begin{align*}
&\;\;\;\xi_{k+1}^{\vep}(x,y)\\ &\le c_{T,2}\le c_{T,2}\frac{B_T}{(2\pi\vep)^{d/2}}\exp\left(-\frac{1}{2\vep A_T}\lVert x-y^{w_{k\vep}}\rVert^2+\lVert x-y^{w_{k\vep}}\rVert_1 \lVert \nabla f\rVert_{\infty}-g(y)\right).
\end{align*}
Let us define $\lambda_T:=\exp(2c_{T,1})c_{T,2}B_T$. By combining the above observation with \eqref{eq:jointcon2}, we get:
\begin{align}\label{eq:jointcon3}
    &\;\;\;\wass_2^2(\gamma_{k+1}^{\vep},\tilde{\xi}_{k\vep})\nonumber \\ &\le \frac{\lambda_T}{(2\pi\vep)^{d/2}} \int \exp(-g(y))\left(\int \exp\left(-\frac{1}{2\vep A_T}\lVert x-y^{w_{k\vep}}\rVert^2+\lVert x-y^{w_{k\vep}}\rVert_1 \lVert \nabla f\rVert_{\infty}\right)\,dx\right)\,dy\nonumber \\ &\le \lambda_T A_T^{d/2}\int \exp(-g(y)) \E[\lVert Z_{\vep}\rVert^2\exp(\lVert Z_{\vep}\rVert_1 \lVert \nabla f\rVert_{\infty})]\,dy\nonumber \\ &=\lambda_T A_T^{d/2}\E[\lVert Z_{\vep}\rVert^2\exp(\lVert Z_{\vep}\rVert_1 \lVert \nabla f\rVert_{\infty})],
\end{align}
We write $Z_{\vep}=(Z_{\vep,1},\ldots , Z_{\vep,d})$. Observe that 

\begin{align}\label{eq:secbdcall}
    \E[\lVert Z_{\vep}\rVert^2\exp(\lVert Z_{\vep}\rVert_1 \lVert \nabla f\rVert_{\infty})]&=d \E[Z_{\vep,1}^2 \exp(|Z_{\vep,1}|\lVert \nabla f\rVert_{\infty})]\prod_{j\ge 2}\E[\exp(|Z_{\vep,j}|\lVert \nabla f\rVert_{\infty})]\nonumber\\ &\le 2^{d-1}d\E[Z_{\vep,1}^2 \exp(|Z_{\vep,1}|\lVert \nabla f\rVert_{\infty})]\exp((d-1)\lVert \nabla f\rVert_{\infty}^2 \vep A_T/2)\nonumber \\ &\le 2^{d+1}d \vep A_T \exp(d\lVert \nabla f\rVert_{\infty}^2 \vep A_T).
\end{align}
 
 %\begin{align*}
%&=\bigg\langle \frac{\partial}{\partial x^{u_t}}\frac{\partial u_t}{\partial t\hfill}(x),\frac{\partial h_t}{\partial x\hfill}(x)\bigg\rangle-\sum_{i,j=1}^d \frac{\partial}{\partial x_i}\left(\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{i,j}\left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_j\right)\nonumber \\ &=\bigg\langle \frac{\partial}{\partial x^{u_t}}\frac{\partial u_t}{\partial t\hfill}(x),\frac{\partial h_t}{\partial x\hfill}(x)\bigg\rangle-\sum_{i,j} \left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{i,j}\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)_{i,j}-\sum_{j=1}^d \left(\sum_{i=1}^d\frac{\partial^2 x_j}{\partial x_i\partial x^{u_t}_i}\right)\left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_j\nonumber \\
%&=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill}(x), \frac{\partial h_t}{\partial x^{u_t}\hfill}(x)+\frac{\partial}{\partial x^{u_t}}\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle-\sum_{i,j} \left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{i,j}\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)_{i,j}\nonumber \\
%&=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill}(x), \frac{\partial g}{\partial x}(x^{u_t})\bigg\rangle-\sum_{i,j} \left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{i,j}\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)_{i,j}.
%\end{align*}
%Here, we have used \cref{lem:tensorel} in the penultimate display. The final display follows by invoking \cref{lem:jacobian} with $b=g$, $a=h_t$ and $\phi=u_t$. 

 Let $Y\sim\exp(-g)$ and set $X_t=Y^{w_t}$ (recall $w_t=u_t^*$). Observe that $\frac{\partial u_t}{\partial t\hfill}=f-h_t$. Then note that 
\begin{align*}
    \int_{\R^d} \lVert v_t(x)\rVert^2\exp(-h_t(x))\,dx&=\E\left[\bigg\lVert \left(\frac{\partial X_t\hfill}{\partial X_t^{u_t}}\right)\nabla \frac{\partial u_t}{\partial t\hfill}(X_t)\bigg\rVert^2\right]\\ &\le \E\left[\bigg\lVert \nabla\frac{\partial u_t}{\partial t\hfill}(Y^{u_t^*})\bigg\rVert^2\lmx^2\left(\frac{\partial Y^{u_t^*}}{\partial Y\hfill}\right)\right]<\infty.
\end{align*}
By combining the above observation with \eqref{eq:jointcon3}, we get: 
$$\wass_2^2(\gamma_{k+1}^{\vep},\tilde{\xi}_{k\vep})\le \left[\lambda_T A_T^{d/2} 2^{d+1}d \exp(d\lVert \nabla f\rVert_{\infty}^2 \vep A_T)\right]\vep A_T .$$
This completes the proof.
