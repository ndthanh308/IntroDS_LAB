\section{Introduction}\label{sec:intro}

We study the scaling limit of the sequence of iterates coming from the celebrated Sinkhorn (or IPFP) algorithm \cite{sinkhorn1967diagonal} as the regularization parameter $\vep\to 0+$ and the number of iterations scales like $\lceil{1/\vep}\rceil$. Given two probability measures $\mu$ and $\nu$ on $\R^d$ with finite second moments, the Sinkhorn algorithm aims to solve the entropy regularized  optimal transport problem:
\begin{equation}\label{eq:eot}
\emn:=\min_{\pi\in \Pi(\mu,\nu)} \left[\frac{1}{2}\int \lVert x-y\rVert^2\,\,d\pi(x,y)+\vep\KL{\pi}{\mu\otimes \nu}\right],
\end{equation}
where $\Pi(\mu,\nu)$ denotes the space of probability measures on $\R^d\times\R^d$ with marginals $\mu$ and $\nu$. Here
\begin{equation}\label{eq:KLdef}
\KL{\pi}{\mu\otimes\nu}=\int \log\left(\frac{d\pi}{d(\mu\otimes\nu)}(x,y)\right)\,d\pi(x,y)
\end{equation}
denotes the standard Kullback-Leibler (KL) divergence and $\vep>0$ is called the regularization parameter. The $\emn$ problem above has a wide array of applications and hence, has attracted significant attention in probability, statistics, and machine learning; see, for example, \cite{Cominetti1994,Chen2016,peyre2019computational,Christian2012,Guillaume2020,schiebinger2019optimal} and the references therein. The unregularized version of \eqref{eq:eot}, which theoretically corresponds to setting $\vep=0$ in $\emn$, is called the optimal transport problem~ \cite{monge1781memoire,Kantorovitch1942,Villani2003}, and it has thriving applications in large-scale problems~\cite{arjovsky2017wasserstein,rubner2000earth}. A computationally convenient way of addressing the optimal transport problem is to solve $\emn$ for small values of the regularization parameter $\vep>0$ \cite{cuturi2013sinkhorn}. 
%\marginpar{YH. I think we want to give more credits to Cuturi. His paper was not just one of the ways, but, in my opinion, it was a breakthrough {\color{red} ``one of the major breakthroughs ..."} that made the area of  computational OT. }
Consequently, the recent years have witnessed a rich and growing body of literature aimed at understanding the convergence and stability properties of $\emn$ as $\vep\to 0$. See~\cite{Nutz2022,Bernton2022,Conforti2021,pal2019difference,carlier2022convergence,Mikami2004,Christian2012,eckstein2022convergence,Gigli2018,chiarini2022gradient} and the references therein.  

% Figure environment removed

One of the major breakthroughs that led to the popularity of the $\emn$ problem is that it can be computed efficiently via the Sinkhorn algorithm, also called IPFP or the Iterative Proportional Fitting Procedure, (see \cite{csiszar1975divergence,ruschendorf1995convergence,franklin1989scaling,Chen2016}), which can in turn be parallelized easily~\cite{cuturi2013sinkhorn,knight2008sinkhorn}. The algorithm proceeds as follows: start with a probability measure $\gnp{0}$ on $\R^d\times\R^d$, and a function $u_0:\R^d\to\R$, given by:
\[
\frac{d\gnp{0}}{d(\mu\otimes\nu)}(x,y)\propto \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}u_0(x)\right),\quad x,y\in\R^d.
\]
For a joint probability distribution $\gamma$, let $\px \gamma$ and $\py\gamma$ denote the $X$ and $Y$ marginal distributions under $\gamma$. Then, for $k\ge 1$, the Sinkhorn algorithm  proceeds iteratively by defining a sequence of pairs of joint distributions $\left(\gpp{k}, \gnp{k} \right)$ given by 
\begin{equation}\label{eq:sinkupdt}
    \frac{d\gpp{k}\hfill}{d\gnp{k-1}}(x,y)=\frac{d\mu}{d(\px\gnp{k-1})}(x), \quad \text{and} \;\frac{d\gnp{k}}{d\gpp{k}}(x,y)=\frac{d\nu}{d(\py\gpp{k})}(y).
\end{equation}

It is known and easily verifiable that each $\gnp{k}$ an $\gpp{k}$ solves the entropic problem in \eqref{eq:eot} between its own marginals. Further, it is easy to check that $\px\gpp{k}=\mu$ and $\py\gnp{k}=\nu$. Therefore, the Sinkhorn algorithm alternately attains the target marginals on the two coordinates, and is hence completely characterized by the respective sequence of opposite marginals $\mk{k}:=\px\gnp{k}$ and $\nk{k}:=\py\gpp{k}$. Our aim here is to study the limit of $\mk{k}$ as $k\to\infty$ and $\vep\to 0$ in an appropriate sense (noting that the limit of $\nk{k}$ can be studied similarly).


While it is known \cite{ruschendorf1995convergence} that the sequence $\left(\mk{k},\; k \in \NN \right)$ converges to the target marginal $\mu$ as $k\rightarrow \infty$ for fixed $\vep>0$, the corresponding limiting objects when $\vep\to 0$ and $k\to\infty$ simultaneously are not well understood. %Hence the algorithm works. But what about the entire sequence itself? 
As $\vep \rightarrow 0+$, it is intuitive that $\mk{k}$ and $\mk{k+1}$ gets increasingly closer to each other. Assuming that all these measures have finite second moments, it makes sense to consider $\left( \mk{k},\; k \in \NN\right)$ as a sequence in the $2$-Wasserstein space, $\wass_2$, (defined below in \eqref{eq:2wass}) and ask if, as $\vep \rightarrow 0+$ and the number of iterations are properly scaled, this sequence of points in $\wass_2$ converges to an absolutely continuous (AC) curve (illustrated in Figure~\ref{fig:curve}). Every AC curve in the Wasserstein space is the solution of a continuity equation with a velocity field \cite[Chapter 8]{ambrosio2005gradient}. Therefore, characterizing the velocity field of this limiting AC curve gives approximate geometric properties of $\left(\mk{k},\; k \in \NN \right)$ as $\vep \rightarrow 0+$. This is the goal of this paper.
%\textcolor{red}{[GS: This paragraph could have a stronger topic sentence (first sentence of the paragraph). Several of the paragraphs here build up to a very strong last sentence, but these can be missed by a reader who is skimming (where one typically reads the first sentence of each paragraph to get the main idea).]}

% Figure environment removed

Under appropriate assumptions, in \cref{thm:inftheo}, we show that, indeed, as $\vep \rightarrow 0+$ with $k$ scaling as $O(1/\vep)$, the sequence $\left(\mk{k},\; k \in \NN \right)$ converges to an absolutely continuous curve in the Wasserstein space called the \emph{Sinkhorn flow}, characterized by a continuity equation that we call the \textit{Sinkhorn PDE}. We explicitly identify the velocity vector field for this evolution PDE.%, which leads to new convergence rates, perturbation bounds, limit distributions, etc. which are all of independent interest.

We further observe in \cref{sec:wassmirrorflowformal} that the Sinkhorn flow is one member of a particular class of absolutely continuous curves in the Wasserstein space which we introduce and refer to as  \emph{Wasserstein mirror gradient flows}, or \emph{Wasserstein mirror flows} for short (see \cref{sec:othexamp} for examples). The concept of mirror gradient flows, although popular in the Euclidean setting \cite{nemirovskii83}, appears to be unexplored in the Wasserstein space. In fact, the Sinkhorn flow can be viewed as the Wasserstein mirror flow of the KL divergence functional. This is different from the solution to the Fokker-Planck PDE \cite[Section 8.3]{santambrogio2015optimal}, which corresponds to the usual Wasserstein gradient flow of the KL divergence functional. Now, it is widely known that the Langevin diffusion characterizes the flow of the Fokker-Planck PDE. In the same vein, we also obtain a new Mckean-Vlasov \cite{mckean75} diffusion process (see \cref{sec:diffmirr} below), which we call the \emph{Sinkhorn diffusion}, whose marginal distribution exactly tracks the evolution of the Sinkhorn PDE mentioned above. 

A lot of the existing literature on the evolution of the Sinkhorn algorithm \eqref{eq:sinkupdt} focuses on obtaining rates of convergence for a fixed $\vep$ as a function of $k$. We refer the reader to \cite{Ghosal2022,ruschendorf1995convergence,Nutz2023} for some qualitative results and \cite{EcksteinNutz2022,deligiannidis2021quantitative,ghosal2022convergence} for quantitative ones. However, the constants appearing in these bounds blow up as $\vep \downarrow 0$. In contrast, we avoid this pitfall by scaling our iteration as $O(1/\vep)$, then taking a scaling limit and then analyzing the rate of convergence of the scaling limit using geometric ideas.
See \cref{lem:expcon} for conditions under which there is exponentially fast convergence. This knowledge can now be transferred to small but finite $\vep$ using our \emph{quantitative rates}, in the squared Wasserstein distance, for the convergence of $\mk{k}$ to the marginals of the limiting flow (see \cref{thm:convergence}).


%{\color{red}Such bounds however do not recover the aforementioned limiting objects (Sinkhorn PDE and diffusion) when $\vep\to 0$ and $k\vep\to t$ --- Geoff: Should we say that those bounds do not leverage structure from the Sinkhorn diffusion? And therefore they are weaker or lack something?}. 


%The near-linear time complexity of the Sinkhorn algorithm for approximating the $2$-Wasserstein distance was first proved in \cite{altschuler2017near}. An extension to the multimarginal case was demonstrated in \cite{carlier2022convergence}. It is well known that the convergence of the Sinkhorn algorithm and its stability with respect to perturbations of $\mu,\nu$ are closely linked; see \cite[Section 6.4]{nutz2021introduction}. Consequently, a number of papers over the past few years have studied both these related problems, beyond the squared error loss considered in \eqref{eq:eot} and under relatively mild assumptions on $\mu,\nu$; \textcolor{red}{[GS: Can we make a stronger conclusion here? What can Sinkhorn PDE and diffusion give, related to these bounds and perturbations. The current structure of this paragraph lines up a slam dunk with "Such bounds do not recover the aforementioned limiting objects" but the rest of the paragraph seems to fail to deliver...?]}


Two other lines of work on the Sinkhorn algorithm are more closely related to ours. In \cite{leger2021gradient}, the author shows that the Sinkhorn algorithm \eqref{eq:sinkupdt} can be viewed as a mirror descent algorithm on the space of probability measures for fixed $k$ and $\vep$. In particular, \cite[Corollary 1]{leger2021gradient} leverages this connection to show that $\mk{k}$ converges in KL to $\mu$ at a $O(k\vep)$ rate. However, this result cannot identify our limiting dynamics as $k\vep\to t$ and $t\ge 0$. In a different direction,  \cite{berman2020} studies the Sinkhorn algorithm \eqref{eq:sinkupdt} in the same scaling limit as we do, and proves that the evolution of the underlying Sinkhorn potentials (defined below in \eqref{eq:twostepit}) converge to the solutions of a parabolic Monge-Amp\`ere (PMA) PDE (see \eqref{eq:pma} below). However, while Berman's picture focuses on potentials, our focus is on  the convergence of the marginal measures. The proof of convergence of the evolution of the measures $\mk{k}$s to the Sinkhorn flow requires a higher order asymptotic analysis than Berman's proof in \cite{berman2020}. The main difficulty is that the convergence of potentials, as shown by Berman, does not imply the convergence of the flow. Rather, one needs to argue that discrete time difference of the potentials converge to the time derivative in the continuum. 
Although this required us to develop  different techniques than both Berman and L\'{e}ger, our ideas are inspired from their work.

As we argue in Section \ref{sec:wassmirrorflowformal}, every Wasserstein mirror gradient flow can be described by a pair of equivalent PDEs, one describing the time evolution of the corresponding potentials potentials and the other describing the time evolution of the marginal measures as a continuity equation. In our context, Berman's PMA tracks the evolution of the Sinkhorn potentials and equivalently, the Sinkhorn flow which we introduce in this paper, tracks the evolution of the marginal distributions. Thus the Sinkhorn flow (see \cref{thm:inftheo} below) also yields a geometric description of the PMA which may be of independent interest.   

\subsection{Notation}\label{sec:nota}

For convenience of the reader, we list some generic notation in the form of a table at the end of the paper; see \cref{tab:table3}. We also refer the reader to \cite[Chapter 10]{ambrosio2005gradient} for a background on Otto calculus which will be used throughout the paper. In this section let us focus on the more specialized notation.

For a function $u:\R^d\to\R \cup \{\infty\}$, its Legendre transform is given by
\[
u^*(y):=\sup_{x\in\R^d}(\iprod{x,y} -u(x)),\quad\quad y\in\R^d.
\]
Now let $u$ be a differentiable convex function such that $\nabla u$ is a diffeomorphism on $\R^d$. That is $\nabla u: \R^d \rightarrow \R^d$ is a differentiable map such that its inverse $\left( \nabla u\right)^{-1}= \nabla u^*$ is also differentiable. This allows us to think of $\R^d$ as a manifold with two global coordinate charts $x\mapsto x$ and $x\mapsto \nabla u(x)$. 



\begin{defn}[Mirror coordinates]\label{def:pdcor}
Given $x\in \mcx$, define $x^u:=\nabla u(x)$. We will call $x^u$ the mirror coordinate for $x$ with respect to $u$.  
\end{defn}
%\marginpar{[Isn't `mirror coordinates' a better name than `dual coordinates'?]}

We will refer to the Hessian matrix $\nabla^2 u(x)$ by the notation $\frac{\partial x^u}{\partial x}$. Note that $\left( x^u \right)^{u^*}=x$ and $\left(y^{u^*} \right)^u=y$. Thus (see, for example, \cite[Lemma 2.3]{berman2020}), if $y=x^u$,
\begin{equation}\label{eq:conjrel}
\frac{\partial x\hfill}{\partial x^u}:=\frac{\partial y^{u^*}}{\partial y\hfill}=\nabla^2u^*\left( x^u\right)= \left(\frac{\partial x^u}{\partial x\hfill} \right)^{-1}. 
\end{equation}
We will utilize this convenient notation throughout. 

\begin{remark}
    As a matter of convention, $\frac{\partial f\hfill}{\partial x^{u}} (x^{u})$ will denote the derivative of $f$ evaluated at $x^{u}$. This is different from taking the derivative of the composition function $f(x^{u})$ which we denote by $\frac{\partial}{\partial x}(f(x^{u}))$. In particular, 
    \begin{align}\label{eq:notcal}
    \frac{\partial}{\partial x}(f(x^{u}))=\left\langle  \frac{\partial x^{u}}{\partial x\hfill},\frac{\partial f\hfill}{\partial x^{u}}(x^{u})\right\rangle.
    \end{align}
\end{remark}

%\SP{The notation above is bad. the first derivative should be called $\frac{\partial f}{\partial x^{u_t}}(x^{u_t})$. This will lead to a notationally consistent chain rule:
%\[
%\frac{\partial}{\partial x}%(f(x^{u}))=\frac{\partial f\hfill}{\partial x^{u}}(x^{u}) \frac{\partial x^{u}}{\partial x\hfill}.
%\]
%Note how $\partial x^{u_t}$ cancels from numerator and denominator to give the LHS. Change this notation everywhere. In fact, I am quite sure you have not used it.
%}\marginpar{YH. I agree.}

Let $\ptac$ denote the space of probability measures on $\R^d$ with positive densities and finite second moments. We will need three notions of distances between two probability measures, say $\mu_1$ and $\mu_2$, which we write as $\KL{\mu_1}{\mu_2}$, $\wt{}{\mu_1}{\mu_2}$, and $\dlt{}{\sigma}{\mu_1}{\mu_2}$ that denote the KL divergence defined in \eqref{eq:KLdef}, the $2$-Wasserstein distance (see \cite[Equation 4]{Villani2003}, \cite[Equation 7.1.1]{ambrosio2005gradient}), and the linearized optimal transport distance (see \cite[Equations 2 and 3]{Wang2013}, \cite{cai2020linearized,moosmuller2023linear}) with respect to a reference probability distribution $\sigma\in \ptac$. The latter two are defined below.

 The squared $2$-Wasserstein distance is defined as
 \begin{equation}\label{eq:2wass}
 \wass_2^2(\mu_1,\mu_2):=\min_{\gamma\in \Pi(\mu_1,\mu_2)} \int \lVert x-y\rVert^2\,d\gamma(x,y),
 \end{equation}
 where, as mentioned before, $\Pi(\mu_1,\mu_2)$ is the space of probability measures on $\R^d\times\R^d$ which preserves the marginals at $\mu_1$ and $\mu_2$. The metric space $\left( \ptac, \wass_2\right)$ will be often denoted (by an abuse of notation) also as $\wass_2$ space. The squared linear optimal transport distance is given by:

 \begin{equation}\label{eq:2linot}
 \dlt{2}{\sigma}{\mu_1}{\mu_2}:=\int \lVert T_{\mu_1}(y)-T_{\mu_2}(y)\rVert^2 \,d\sigma(y),
 \end{equation}
 where $(T_{\mu_1})_{\#}\sigma=\mu_1$ and $(T_{\mu_2})_{\#}\sigma=\mu_2$ are the two optimal transport maps.

% \SP{You have not defined the density $e^{-g}$ yet. That comes in the following subsection. Also, $\mu, \nu$ are the target marginals. Don't use them for a generic measure here.}

%\marginpar{This section needs polishing. Also, we may want to include a backbone of Otto calculus here, especially regarding the notation $\frac{\delta F}{\delta \rho}$ while $F$ is a function on the Wasserstein spce.}
\subsection{Informal description of main results}\label{sec:formalres} 
Let us outline our main results which make the connection between the Sinkhorn algorithm and the notion of mirror descent more explicit. 
Recall that our state space is $\R^d$ for some $d\ge 1$.
Let $\mu$ and $\nu$ be two Borel probability measures in $\ptac$ (absolutely continuous with finite second moments). Let their densities be given by $e^{-f(x)}$ and $e^{-g(y)}$ respectively. For a full list of assumptions, see \cref{asn:solcon}. Recall the definition of absolutely continuous curves on the $2$-Wasserstein space and its correspondence with solutions of the continuity equations (see \cite[Chapter 8]{ambrosio2005gradient}).
%\SP{Note that the notation $\wass_2$ was undefined before. I have defined it right above (1.7)}

%\begin{assm}\label{asn:smoothfg} Assume that 
%\[
%\mu(x)=\exp(-f(x)),\quad \quad \nu(y)=\exp(-g(y)),
%\]
%for continuously differentiable functions $f,g: \R^d \to\R$. 
%\end{assm}
\begin{thm}[Existence of Sinkhorn flow (Informal)]\label{thm:infexist}
  There exists an absolutely continuous curve on the $\wass_2$ space which satisfies the continuity equation $\frac{\partial}{\partial t}\rho_t+\div(\rho_t v_t)=0$ with 
  \begin{equation}\label{eq:velocity}
 v_t(x)= -\frac{\partial\hfill}{\partial x^{u_t}}((f+\log{\rho_t})(x)) =
 -\left( \nabla^2 u_t(x)\right)^{-1}\nabla_{\mathbb{W}}\KL{\rho_t}{e^{-f}}(x).
 %-\left( \nabla^2 u_t(x)\right)^{-1}\frac{\partial\hfill}{\partial x}((f+\log{\rho_t})(x)).
\end{equation}
Here,  $\nabla u_t$ is the Brenier map transporting $\rho_t$ to $\nu$, i.e., $u_t$ is convex and $(\nabla u_t)_{\#}\rho_t=e^{-g}$ and $\left( \nabla^2 u_t(x)\right)^{-1}$ is the inverse of the Hessian matrix of $u_t$ at $x$. Here $\nabla_{\mathbb{W}}\KL{\rho_t}{e^{-f}}(x)$ refers to the usual Wasserstein gradient of the functional $\rho \mapsto \KL{\rho}{e^{-f}}$ evaluated at $x$ (see \cite[Chapter 10]{ambrosio2005gradient} for details on differential calculus on the $\wass_2$ space). We will call this curve the Sinkhorn flow. 

The Sinkhorn flow can be viewed as the Wasserstein mirror gradient flow of the KL functional $\KL{\cdot}{e^{-f}}$ with the mirror map given by half of the squared Wasserstein distance $\wass_2^2(\cdot, e^{-g})$. In particular, the family of Brenier maps satisfies the PDE 
\[
\frac{\partial}{\partial t} \nabla u_t(x)= \nabla_{\mathbb{W}}\KL{\rho_t}{e^{-f}}(x). 
\]
This is nothing but the parabolic Monge-Amp\`{e}re PDE, see  \eqref{eq:pma}, written slightly differently.
\end{thm}
%{\red
%[Can we put some of the concerte expressions in Section \ref{sec:wassmirrorflowformal} up front in here? ]
%}
%\SP{I don't think the Intro has any space left. Expressions are best left for the next section.}
\noindent For a pictorial description of the Sinkhorn flow, we refer the reader to Figure \ref{fig:evolution}. A precise version of the above statement can be found in \cref{thm:existlin}. Also, the notion of Wasserstein mirror gradient flows is introduced in \cref{sec:wassmirrorflowformal}. Very roughly, a ``mirror'' associates a dual pair of ``coordinate systems'' to each point on a manifold. Then, given a function $F$, the mirror gradient flow of $F$ is a curve on the manifold that admits two equivalent descriptions. They both describe the time derivative of one coordinate to be equal to the negative gradient of $F$ with respect to its mirror. When the mirror is the identity, we recover our usual gradient flow. We extend this Euclidean notion to the Wasserstein space by an analogous formalism that views the $2$-Waserstein space as an infinite-dimensional Riemannian manifold as in Otto \cite{Otto_2001}.

We note that $v_t$ as in \eqref{eq:velocity} is not a gradient with respect to the canonical coordinate system (as is the case with the usual Wasserstein gradient flows), but instead, it is a gradient with respect to the mirror coordinate system (see \cref{def:pdcor}). For the usual  Wasserstein gradient flows, 
it is well-known (see \cite[Theorem 2.15]{ambrosio2007gradient}) that there exists a velocity field $(v_t)_{t\ge 0}$ (gradient of a function with respect to the canonical coordinate system) of an absolutely continuous curve $(\rho_t)_{t\ge 0}$ which satisfies:
$$\limsup_{\delta\to 0} \delta^{-1}\wass_2\big(\rho_{t+\delta},\rho_t\big)= \lVert v_t\rVert_{L^2(\rho_t)}.$$
In other words, for Wasserstein gradient flows, $\lVert v_t\rVert_{L^2(\rho_t)}$ can be interpreted as the metric derivative of the curve $(\rho_t)_{t\ge 0}$ at time $t$, with respect to the $\wass_2$ distance. This interpretation may not hold for the Sinkhorn flow as in \cref{thm:inftheo} because $v_t$ is not a usual gradient. In \cref{prop:metderlin}, we therefore provide an alternate interpretation of $v_t$ as in \eqref{eq:velocity}. We show that $\lVert v_t\rVert_{L^2(\rho_t)}$ is the metric derivative of the curve $\rho_t$ but with respect to the linearized optimal transport distance (see~\eqref{eq:2linot}) instead of the usual $\wass_2$ distance (see \eqref{eq:2wass}), i.e.,  
$$\lim_{\delta\to 0} \delta^{-1}\dlt{}{e^{-g}}{\rho_{t+\delta}}{\rho_t}=\lVert v_t\rVert_{L^2(\rho_t)},$$
where $v_t$ and $\rho_t$ are defined as in \cref{thm:inftheo}. This provides an interesting connection between the Sinkhorn flow and the LOT literature \cite{Wang2013}. We refer the reader to Figure \ref{fig:evolution} for an illustration of this connection.

Given a flow in the space of probability measures, it is natural to ask if there is a stochastic process, preferably Markov, that generates the same family of marginals. In \cref{sec:diffmirr}, we answer this question in the affirmative.
\begin{thm}[Sinkhorn diffusion (Informal)]\label{thm:diffexist}
    Let $h_t:=-\log\rho_t$ where $\rho_t$ is a solution of the Sinkhorn PDE/flow in \cref{thm:infexist}. Let $B$ denote a standard $d$-dimensional Brownian motion and consider the following SDE:
    $$ dX_t=\left(-\frac{\partial f\hfill}{\partial \xsut}(X_t)-\frac{\partial g\hfill}{\partial\xsut}\left(X_t^{u_t}\right)+\frac{\partial h_t\hfill}{\partial \xsut}(X_t)\right)\,dt+\sqrt{2\frac{\partial X_t\hfill}{\partial X_t^{u_t}}}dB_t.$$
    Then, the marginals of the process $(X_t)_{t\ge 0}$ evolve according to the Sinkhorn flow in \cref{thm:infexist}. We will call this SDE the \emph{Sinkhorn diffusion}.
\end{thm}

A formal version of this result is provided in \cref{thm:existpropX}. The Sinkhorn diffusion is motivated from a natural Markov chain embedded in the Sinkhorn algorithm \eqref{eq:sinkupdt} defined in \cref{prop:mchn}. Finally, we connect the Sinkhorn flow with the Sinkhorn algorithm by proving that the $X$-marginals $\mk{k}$s from the algorithm converge to the marginals of the Sinkhorn flow, under an appropriate scaling.

\begin{thm}[Convergence as $\vep\downarrow 0$ (Informal)]\label{thm:inftheo}
Under appropriate assumptions on $\mu,\nu$ and $u_0$, given any $t>0$, we have $\wass_2^2(\rho_{\lfloor t/\vep\rfloor}^{\vep},\rho_t)=O(\vep)$.
\end{thm}

A formal version of this result is presented in \cref{thm:convergence}. 

\begin{remark}[Comparison with Fokker-Planck PDE]\label{rem:fplank}
 Recall from \cref{thm:diffexist} that $h_t=-\log{\rho_t}$. The Sinkhorn PDE then simplifies to 
 \begin{equation*}
 \frac{d}{dt}h_t(x)+\left\langle \frac{\partial g\hfill}{\partial x^{u_t}}(x^{u_t}),\frac{\partial}{\partial x}(h_t-f)(x)\right\rangle+\trc\left(\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)\nabla^2 (f-h_t)(x)\right)=0,
 \end{equation*}
 which has a similar form to the usual Fokker-Planck PDE \cite[Chapter 8.3]{santambrogio2015optimal}, given by:
 $$\frac{\partial}{\partial t} h_t(x)+\left\langle \frac{\partial}{\partial x} h_t(x),\frac{\partial}{\partial x} (h_t-f)(x) \right\rangle + \bmd{(f-h_t)(x)}=0.$$
 In particular, the velocity field for usual Fokker-Planck PDE is given by 
 \begin{equation}\label{eq:velfp}
 v_t(x)=-\frac{\partial}{\partial x}(f+\log{\rho_t})(x),
 \end{equation}
 which is the same as in~\eqref{eq:velocity} with $\frac{\partial\hfill}{\partial \xsut}$ replaced by $\frac{\partial}{\partial x}$.  
 \end{remark}

 %\begin{remark}\label{rem:equiv}
 Based on the above remark, it is natural to ask: (a) Is the Sinkhorn PDE always different from the Fokker-Planck PDE? (b) When they are different, can the Sinkhorn PDE converge faster as $t\to\infty$?
 
 We consider two simple examples below aimed at answering the above questions.

%\vspace{0.05in}


  \begin{ex}[Gaussian location]
     Suppose $\mu(dx)\sim N(0,1)$ and $\nu(dy)\sim N(\theta,1)$ for $\theta\in\R\setminus\{0\}$. Also let $\rho_0\sim N(\theta,1)$. Then elementary computations show that both the usual Fokker-Planck PDE and the Sinkhorn PDE (see \cref{thm:inftheo}) lead to the same flow given by 
     $$\rho_t\sim N(\theta \exp(-t),1).$$
  \end{ex}

  \medskip

 \begin{ex}[Gaussian scale]
   Suppose $\mu(dx)\sim N(0,1)$ and $\nu(dy)\sim N(0,\eta^2)$ for $\eta\in (0,1)$. Also let $\rho_0\sim N(0,\eta^2)$. Then both the usual Fokker-Planck PDE and the Sinkhorn PDE (see \cref{def:isid}) are Gaussians with mean $0$ and variances $\sfe$ and $\sse$ respectively, given by:
     $$\sfe=1-(1-\eta^2)\exp(-2t),\quad \sse=\left(1-2\frac{1-\eta}{\exp\left(\frac{2t}{\eta}\right)(\eta+1)+(1-\eta)}\right)^2.$$
     While both $\sfe$ and $\sse$ converge to $1$ exponentially, it is easy to check that 
     $$\frac{1-\sfe}{1-\sse}\ge \frac{(1+\eta)^2}{4}\exp\left(2t\left(\frac{1}{\eta}-1\right)\right)\to\infty,\quad \mbox{as}\ t\to\infty,$$
     as $\eta<1$. This provides an example where the convergence of the Sinkhorn PDE is much faster than the usual Fokker-Planck.
\end{ex}

\medskip 

As these examples demonstrate, the rate of convergence, as $t\rightarrow \infty$, of the Sinkhorn flow is subtle. We show in Section \ref{sec:mirror} that the convergence is determined by a Hessian geometry (see \eqref{eq:hessian-metric}) that is distinct from the usual Wasserstein geometry. This Hessian geometry can slow down or speed up the mirror gradient flow at any $\rho \in \ptac$ depending on the Hessian of the Brenier map transporting $\rho$ to $\nu$. This phenomenon is already well understood in Euclidean mirror flows \cite[Section B.2.4]{Wilson18}. However, as shown in Theorem \ref{lem:expcon}, under appropriately nice assumptions, one can have exponential convergence of the Sinkhorn flow.  

\begin{thm}[Convergence as $t\rightarrow \infty$ (Informal)]\label{thm:infrate}
Suppose that the marginal density $e^{-f}$ satisfies the logarithmic Sobolev inequality (see \cref{def:isid} below). Also, if $(u_t)$ is the solution of the corresponding PMA, assume that the eigenvalues of the inverse Hessian matrices $(\nabla^2 u_t)^{-1}(x)$ are uniformly bounded away from zero in $(t,x)$. Then, for some $c_0 >0$,  
\[
\KL{\rho_t}{e^{-f}}\le \KL{\rho_0}{e^{-f}}\exp(-c_0 t).
\]
\end{thm}

Sufficient conditions under which our assumptions are valid have already been explored in Berman \cite{berman2020}. We note that the rate of convergence results in Theorems~\ref{thm:inftheo} and \ref{thm:infrate} recover the correct scaling in terms of $\vep$ which allows us to work under the scaling limit regime where the number of iterations scales like $1/\vep$ with $\vep\to 0$.  On the other hand, in existing rate of convergence results in  \cite{leger2021gradient,ghosal2022convergence,Conforti2021} cited above, the rates involve terms depending on $\vep$ which blow up in the aforementioned scaling limit regime as $\vep\to 0$. Having said that, our results do require stronger regularity conditions on $f$, $g$ and solutions of the parabolic Monge-Amp\'{e}re equation described in \cref{thm:infexist} (also see \eqref{eq:pma}).
 
\subsection{Organization} The rest of the paper is organized as follows. In \cref{sec:mirror}, we introduce Wasserstein mirror gradient flows and argue that the Sinkhorn flow is a particular example of this larger class. We also zoom in on the Sinkhorn PDE and prove its existence, rate of convergence, and connections to the linear optimal transport distance. In \cref{sec:diffmirr}, we obtain a Mckean-Vlasov SDE whose marginal distributions follow the Sinkhorn PDE. In \cref{sec:mcconst}, we move our attention to the Sinkhorn algorithm and prove that its $X$ marginals converge to the marginals of the Sunkhorn PDE, with quantitative error bounds in terms of the regularization parameter $\vep$. Finally, in Sections \ref{sec:pfres} and \ref{sec:mainresultlems}, we prove all the technical lemmas.  


\begin{comment}
Given any natural number $n$, we write $[n]:=\{1,2,\ldots ,n\}$.

Given a square matrix $A$, let $\lmn(A)$, $\lmx(A)$, $\trc(A)$, $\lVert A\rVert_{\hs}$, and $\lVert A\rVert_{\mathrm{op}}$ be the minimum eigenvalue, the maximum eigenvalue, the trace, the Frobenius norm, and the $L^2$ operator norm of the matrix $A$. Also if $A$ is symmetric and positive definite, let $\sqrt{A}$ denote the Cholesky square root of $A$, i.e., $\sqrt{A}=S$ if $A=S^2$. $I_d$ will denote the $d\times d$ identity matrix. We will drop the $d$ from the subscript when the dimension is obvious. Further given any two matrices $A$ and $B$, the notation $A\preceq B$ will imply $B-A$ is non-negative definite. {\color{red}The generic notation $|\cdot|$ will be used for the standard Euclidean norm of vectors and the Frobenius norm of matrices.}  \par


Given a probability measures $\mu$ on some subset $\mcx$ of $\R^d$ and a function $T:\mcx \rightarrow \R^d$, the push-forward will be denoted by $T_{\#}\mu$. That is, $T_{\#}\mu$ is the probability distribution of $T(X)$ where $X\sim\mu$. Next, given a density function $\gamma(x,y)$ on $\R^d\times \R^d$
, we will write $\px \gamma$ and $\py \gamma$ to denote the $X$ and $Y$ marginal densities respectively. Similarly let $p_{X|Y}\gamma(\cdot|\cdot)$ and $p_{Y|X}\gamma(\cdot|\cdot)$ denote the conditional densities of $X|Y$ and $Y|X$ respectively. \par

Let $\diffcont^k(\R^d)$ be the space of $k$ times continuously differentiable functions on $\R^d$. We will write $\diffcont(\R^d)$ for the set of continuous functions on $\R^d$. Let $\div$ and $\bmd$ denote the divergence and Laplacian operators respectively.

Since there will be multiple coordinate charts even for $\R^d$, we will use $\nabla_x$ or $\frac{\partial}{\partial x}$ to denote gradient or partial derivative with respect to a coordinate chart $x$. Similarly, we write $\nabla^2$ and $\nabla^{-2}$ to denote the Hessian and the inverse Hessian operators. 
\end{comment}