

\section{Mirror gradient flows: Euclidean and Wasserstein}\label{sec:mirror}

\subsection{Mirror descent and mirror gradient flow in Euclidean spaces}

Mirror descent was originally proposed by Nemirovsky and Yudin in \cite{nemirovskii83} as a generalization of gradient descent. For a modern account and its connections to other gradient based optimization schemes, see \cite[Sections 2.1.3 and B.2]{Wilson18}. 




%it is an iterative procedure to do gradient descent on a function $F:\R^d\rightarrow \R$ on the dual coordinates with respect to $u$. There are several closely related and essentially equivalent versions of this algorithm. We choose the one that resembles most the Euler scheme for Euclidean gradient descent.

%Fix a positive step size $\tau \approx 0$. Start at $x_0$. The explicit mirror descent algorithm determines $x_1$ by
%\[
%x^u_1 = x^u_0 - \tau \nabla_x F(x_0). 
%\]
%Thereafter, iteratively, 
%\[
%x^u_{k+1} = x^u_k - \tau \nabla_x F(x_k). 
%\]
%The implicit scheme utilizes the Bregman divergence associated with $u$ given by 
%\[
%D_u(x \mid y)= u(x) - u(y) - \iprod{\nabla u(y), x-y}. 
%\]
%Iteratively, given $x_0, \ldots, x_k$, the next step is given by  \[
%x_{k+1}= \mathrm{argmin}\left[ F(x) + \frac{1}{\tau}D_u(x \mid x_k) \right].
%\]

%To see its connection with the regular gradient descent, note that when $u(z)=\frac{1}{2}\norm{z}^2$, the explicit and the implicit mirror descent schemes are the usual Euler explicit and implicit schemes for gradient descent of $F$.

Assume that $u:\R^d\rightarrow \R$ is a differentiable convex function such that $\nabla u: \R^d \rightarrow \R^d$ is a diffeomorphism. This diffeomorphism induces two global coordinate charts on $\R^d$, namely $x\leftrightarrow x^u:= \nabla u(x)$ (also see \cref{def:pdcor}). This map is called the \textit{mirror map} in the literature. Given a differentiable function $F: \rr^d \rightarrow \rr$, its Euclidean mirror gradient flow can be described by the ODE (with a given initial condition)
\[
\frac{d}{dt} x^u_t = - \frac{\partial}{\partial x} F(x_t),\; t\ge 0.
\]
Applying chain rule to the LHS, we get the equivalent alternative ODE
\begin{equation}\label{eq:mirrorflowEuclid}
\frac{d}{dt} x_t = -\frac{\partial x\hfill}{\partial x^u}(x^u_t) \frac{\partial F}{\partial x}(x_t)= - \frac{\partial F\hfill}{\partial x^u}(x_t). 
\end{equation}
In the last equality we are utilizing the idea that $x \mapsto x^u$ is a diffeomorphism on $\R^d$ and gradients of functions can be taken with respect to either coordinate system. When $u(x)=\frac{1}{2} \norm{x}^2$, both ODEs coincide and we recover the usual Euclidean gradient flow. 

The Euclidean mirror gradient flow can be interpreted as a ``true'' gradient flow if we change the manifold structure of $\R^d$ to make it a so-called \textit{Hessian Riemannian manifold}. See \cite{Hessian97} and \cite[Section B.2.4]{Wilson18}. Roughly, at a point $x\in \R^d$, consider the positive definite Hessian matrix $\nabla^{2} u(x)$. For any tangent vectors $y$ at $x$, change the Riemannian metric to $y^T \nabla^{2} u(x) y$. This induces a new Riemannian manifold on $\R^d$. The mirror gradient flow \eqref{eq:mirrorflowEuclid} is the gradient flow of $F$ on this Hessian Riemannian manifold. 

\subsubsection{Examples.} Consider the following examples where we take $d=1$, $F(x)=\frac{1}{2}x^2$, and different choices of the mirror function $u$. Notice that the function $F$ admits a unique minimizer at zero. For each of the following flows, the initial condition is $x_0=1$.

\begin{enumerate}[(i)]
    \item $u(x)=\frac{1}{2}x^2$. In this case $x^u=x$. Thus, we get back our usual gradient flow equation $\dot{x}_t=-\nabla_x F(x_t)=-x_t$. For our given initial condition, the solution is $x_t=\exp(-t)$, $t\ge 0$. It converges to zero exponentially fast. 
    \item $u(x)=x^4$. Thus $x^u=4x^3$ and $\frac{\partial x^u}{\partial x}=12x^2$. Thus, 
    \[
    \dot{x}_t= -\nabla_{x^u} F(x_t)= -\frac{1}{12 x_t^2} x_t= -\frac{1}{12 x_t}.
    \]
    The solution of this ODE, with $x_0=1$, is $x_t=\sqrt{1-t/6}$, $0\le t \le 6$. The solution does not extend beyond $[0,6]$ due to a singularity at $t=6$ when $x_6=0$, the minimizer of $F$. The flow converges to the minimizer in finite time, unlike the previous example. 
    \item $u(x)=1/x$ for $x>0$. Thus $x^u=-\frac{1}{x^2}$ and $\frac{\partial x^u}{\partial x\hfill}=\frac{2}{x^3}$. The mirror flow equation is given by $\dot{x}_t= -\frac{1}{2} x_t^4$. The unique solution with $x_0=1$ is $x_t=\left(1 + 3t/2 \right)^{-1/3}$. The solution is well defined for all $t\ge 0$ and converges to the minimizer of $F$ just polynomially. 
\end{enumerate}

%For a different set of example, consider again $d=1$, the same mirror function $u(x)=e^x$ and the family of convex functions $F_\lambda(x)=e^{\lambda x}$, for $\lambda \neq 0$. These functions do not admit any minimizer on the real line. However, on the extended real line, the minimizer is $-\infty$ for $\lambda >0$ and $+\infty$ for $\lambda <0$.  

%Take the initial condition $x_0=0$ throughout. 
%The usual gradient flow equation is $\dot{x}_t=-\lambda e^{\lambda x_t}$, $x_0=1$, which admits a unique solution
%\[
%x_t= -\frac{1}{\lambda}\log\left( \lambda^2 t + 1\right).
%\]
%The mirror gradient ODE is given by 
%\[
%\dot{x}_t = -\frac{1}{u''(x_t)} F'_\lambda(x_t)=-\lambda e^{(\lambda-1)x_t}, \quad x_0=0.
%\]
%The unique solution for $\lambda \neq 0$ is
%\[
%x_t=\begin{cases}
%-\frac{1}{\lambda-1} \log\left(\lambda(\lambda-1) t +1 \right), & %\text{when $\lambda \in (1, \infty) \cup (-\infty, 0)$},\\
%-\lambda t, & \text{when $\lambda =1$},\\
%\frac{1}{1-\lambda} \log\left(\lambda(1-\lambda) t +1 \right), & %\text{when $\lambda \in (0,1)$}.
%\end{cases}
%\]

%As one can see the impact of the mirror becomes pronounced in the interval $\lambda \in (0,1]$, with $\lambda=1$ giving a drastically different behavior and faster convergence.

In all these examples it is clear that the behavior of the Hessian of the mirror function $u$ in a neighborhood of the minimizer of $F$ plays a very important role. If this Hessian is very close to zero, the mirror flow speeds up significantly more than the gradient flow of $F$. This is an intuition that we expect to carry over to the Wasserstein set-up as well. 


\subsection{An informal description of Wasserstein mirror gradient flows}\label{sec:wassmirrorflowformal} 


%\marginpar{ In this Section 2.2, we need some citations; we can put [Villani, Topics on OT]. e.g. After (25), cite [Villaniâ€™s book] for the expression of Wasserstein Gradient. 
%}
Recall that $\ptac$ denote the set of Lebesgue absolutely continuous Borel probability measures on $\R^d$ with finite second moments. Equip this space with the Wasserstein $2$ metric. 
We take the point of view (originally due to Otto \cite{Otto_2001}) that this Wasserstein space can be thought of as an ``infinite dimensional Riemannian manifold'' in the following sense \cite[Chapter 8]{ambrosio2005gradient}. At any $\rho \in \ptac$ define the tangent space by $\tanspace_\rho = \overline{\left\{ \nabla g,\; g \in \diffcont_c^\infty \right\}}$,
where the closure is taken in $L^2(\rho)$ \cite[Section 8.4]{ambrosio2005gradient}. The metric tensor is given by the $L^2$  norm. 

%\marginpar{Otto calculus is usually for absolutely continuous (ac) measures, so on $P_{2,ac}$. }

For suitable functions $F:\ptac\rightarrow \R \cup \{\infty\}$, the Wasserstein gradient at an absolutely continuous probability measure with a $\mathcal{C}^1$ density is the element $\nabla_{\wass}F(\rho)\in \tanspace_\rho$, characterized by the following identity \cite[Lemma 10.4.1]{ambrosio2005gradient} that holds for all $v\in \tanspace_\rho$:
\[
\iprod{\nabla_{\wass}F(\rho), v }_{\ltwo(\rho)}= \int_{\R^d} \frac{\delta F}{\delta \rho}(x) \dot{\rho}(x) dx,
\]
where $\dot{\rho}= - \div(v\rho)$. The Wasserstein gradient flow is given by the continuity equation \cite[Section 11.1]{ambrosio2005gradient} $\frac{\partial}{\partial t} \rho_t = \div{\left(\nabla_{\wass}F(\rho_t) \rho_t\right)}$.


%Recall the notions of a generalized geodesic \cite[Definition 9.2.2]{ambrosio2005gradient} and generalized geodesically convex functions \cite[Definition 9.2.4]{ambrosio2005gradient}. Let $U:\probspace\rightarrow \R \cup \{\infty\}$ denote a generalized geodesically convex function. 
%\marginpar{ Before (2.2), generalized geodesics should be defined, etc. } 

Let 
\begin{equation}\label{eq:mirror}
U(\rho)=\frac{1}{2}\wass_2^2\left(\rho, e^{-g}\right),
\end{equation}
which is known to be convex over generalized geodesics with base $e^{-g}$ (\cite[Lemma 9.2.1 and Definition 9.2.2]{ambrosio2005gradient}). 
We will use $U$ as a ``mirror'' to generate a ``mirror potential'' given by 
\[
\rho \mapsto \rho^U:=\nabla_{\wass}U(\rho).
\]
Notice that while $\rho$ is a measure, its mirror potential $\rho^U$ is a function in $\ltwo(\rho)$. For the special case of $U(\rho)=\frac{1}{2}\wass_2^2\left(\rho, e^{-g}\right)$, $\rho^U$ is the  Kantorovich map (gradient of the Kantorovich potential) transporting $\rho$ to $e^{-g}$ \cite[Theorem 10.4.12]{ambrosio2005gradient}. 

By analogy with the Euclidean space, for a suitable function $F: \probspace\rightarrow \R\cup \{\infty\}$, one may define the Wasserstein mirror gradient flow by two equivalent PDEs. The first one takes time derivative in the mirror potential.
\begin{equation}\label{eq:mirrorgradflow}
\frac{\partial}{\partial t} \rho^U_t = - \nabla_{\wass} F(\rho_t),
\end{equation}
which represents an equality of two elements in $\tanspace_{\rho_t}$ viewed as a subspace of $\ltwo(\rho_t)$. 


Let $\nabla u$ denote the Brenier map transporting $\rho$ to $e^{-g}$ for a convex function $u$. Then 
\begin{equation}\label{eq:KPtoBP}
\nabla u(x) =  x - \rho^U(x).
\end{equation}
In terms of the convex Brenier potentials, \eqref{eq:mirrorgradflow} can be equivalently written as 
\begin{equation}\label{eq:mirrorgradflow2}
\frac{\partial}{\partial t} \nabla u_t = \nabla_{\wass} F(\rho_t).
\end{equation}
When $\nabla_{\wass} F(\rho_t)= \nabla \frac{\delta F}{\delta \rho_t}$, the gradient of the first variation of $F$ at $\rho_t$ \cite[Lemma 10.4.1]{ambrosio2005gradient}, by removing the gradients on both sides of the above, we get the PDE.
\begin{equation}\label{eq:mirrorgradflow3}
\frac{\partial u_t}{\partial t}(x)= \frac{\delta F}{\delta \rho_t}(x), \quad \text{where $u_t$ is convex and} \; (\nabla u_t)_{\#} \rho_t=e^{-g}.
\end{equation}
As shown in the following Section \ref{sec:pmasec}, this family of PDEs includes the parabolic Monge-Amp\`ere \eqref{eq:pma2}.

The second equivalent way of describing the Wasserstein mirror gradient flow is to take the time derivative of the curve in the space of measures, i.e., specify a continuity equation for the curve on Wasserstein space.  

Fix an absolutely continuous measure $\rho$. The Hessian matrix $\nabla^2 u$ is defined $\rho$-a.s. in the Alexandrov sense and is positive semidefinite. Define a new metric tensor on $\tanspace_{\rho}$ by 
\begin{equation}\label{eq:hessian-metric}
\iprod{v_1, v_2}_U= \iprod{v_1, \left(\nabla^2 u\right) v_2}_{\ltwo(\rho)}, \quad v_1, v_2 \in \tanspace_\rho. 
\end{equation}
This defines a new Riemannian gradient on the Wasserstein space $\nabla_{\wass}^U$ and a new gradient flow equation
\begin{equation}\label{eq:newgradflow}
\frac{\partial}{\partial t} \rho_t = \div\left( \nabla_{\wass}^U F(\rho_t) \rho_t \right).
\end{equation}
When it exists, we call this flow the Wasserstein mirror gradient flow of $F$ with respect to $U(\rho)=\frac{1}{2}\wass_2^2\left(\rho, e^{-g}\right)$. While $\nabla_{\wass} F(\rho)$ is given by the gradient of the first variation of $F$, $\nabla_x \left(\frac{\delta F}{\delta \rho}\right)$, for $\nabla_{\wass}^U F(\rho_t)$ we get
\begin{equation}\label{eq:expgrad}
\nabla_{\wass}^U F(\rho_t)= \nabla_{x^{u_t}}\left(\frac{\delta F}{\delta \rho_t}\right), \quad \text{where $u_t$ is convex and}\; (\nabla u_t)_{\#} \rho_t= e^{-g}.
\end{equation}
Since $\frac{\partial x\hfill}{\partial x^u} = \left(\nabla^2 u \right)^{-1}$, \eqref{eq:expgrad} can also be written as \[
\nabla_{\wass}^U F(\rho_t)=  \left(\nabla^2 u \right)^{-1} \nabla_{\wass} F(\rho)\]
which is consistent with the metric \eqref{eq:hessian-metric}.
Thus \eqref{eq:newgradflow} and \eqref{eq:expgrad} describe the Wasserstein mirror gradient flow as a continuity equation. 


The connection between \eqref{eq:newgradflow} and \eqref{eq:mirrorgradflow3} is that if $(u_t,\; t\ge 0)$ is a solution of the latter, then $\rho_t:=\left( \nabla u_t\right)^{-1}_{\#} e^{-g}$ 
 satisfies the former. We show this below assuming that the solution of \eqref{eq:mirrorgradflow3} satisfies, for each $t$, $\nabla^2 u_t(x)$ is invertible and the inverse is continuous in $x$, $\rho_t$ a.s.. 

 Let $T_t=\nabla u_t$ denote the transport map. Then, by the chain rule,  
\begin{align}\label{eq:inverse_derivative}
    \left[\frac{\partial}{\partial t} (T_t)^{-1} \right]( T_t(x)) = - [\nabla T_t(x)]^{-1}   \left[\frac{\partial}{\partial t} T_t \right](x).
\end{align}
Notice that $[\nabla T_t(x)]^{-1} =\left(\nabla^2u_t(x)\right)^{-1}$.
Thus, from \eqref{eq:mirrorgradflow2}, 
\[
\left[\frac{\partial}{\partial t} (T_t)^{-1} \right]( T_t(x)) = - \left(\nabla^2u_t(x)\right)^{-1}\nabla \frac{\delta F}{\delta \rho_t}=-\nabla_{x^{u_t}}\frac{\delta F}{\delta \rho_t}=- \nabla_{\wass}^UF(\rho_t). 
\]

Let $\xi$ be a smooth, compactly supported test function. Then 
\[
\begin{split}
\frac{d}{dt} &\int \xi(x) \rho_t(x)dx = \frac{d}{dt} \int \xi\left(T_t^{-1}(y)\right)e^{-g(y)}dy\\
&=\int \nabla \xi\left(T_t^{-1}(y)\right)\cdot  \left[\frac{\partial}{\partial t} (T_t)^{-1} \right](y) e^{-g(y)}dy  \\
&=\int \nabla \xi(x) \left[\frac{\partial}{\partial t} (T_t)^{-1} \right](T_t(x)) \rho_t(x)dx=- \int \iprod{\nabla \xi(x), \nabla_{\wass}^UF(\rho_t)} \rho_t(x)dx.
\end{split}
\]
This proves that $(\rho_t,\; t\ge 0)$ is a weak solution of the continuity equation \eqref{eq:newgradflow}. 

Although, to the best of our knowledge, this particular mirror gradient flow on the Wasserstein space is new in the literature, some related ideas that involve modifications to the usual Wasserstein geometry and/or considering gradient flows on them can be found in other recent papers such as \cite{SVGD17,RankinWong23, WassersteinNewton}.  

\begin{remark}\label{rmk:hori-ver-corr}
    The paragraph around  \eqref{eq:inverse_derivative} gives a  geometric way to understand the connection between \eqref{eq:mirrorgradflow3} and 
    \eqref{eq:newgradflow}. Namely, if we consider the graph of $\nabla u_t$ (the mirror map) in the product space $\mathbb{R}^d \times \mathbb{R}^d$, the vector field $\nabla_{\wass} F(\rho_t)$ gives the vertical variation of the graph, while the vector field $-\nabla_{\wass}^U F(\rho_t)=-  \left(\nabla^2 u \right)^{-1} \nabla_{\wass} F(\rho)$ gives its horizontal counterpart, which gives the same variation of the graph. The equation \eqref{eq:inverse_derivative} gives the precise relation between the horizontal and vertical variation.
    Note that the distribution $\rho_t$ is the result of the drift $\nabla_{\wass}^U F(\rho_t)$ via the continuity equation. The difference between the mirror flow and the usual Wasserstein gradient flow, is that the vector field $\nabla_{\wass} F(\rho_t)$ provides variation on $\rho_t$ not directly in the continuity equation but indirectly through the aforementioned vertical-horizontal correspondence, which in turn is provided by the mirror map. 
\end{remark}


\subsection{Parabolic Monge-Amp\`{e}re and the mirror gradient flow of Kullback-Leibler divergence}\label{sec:pmasec}



 %, especially for the general cost functions. } % and further studied by \cite{kitagawa2012parabolic} for domains with boundary.
The parabolic Monge-Amp\`{e}re (PMA) is the following partial differential equation (PDE): 
\begin{equation}\label{eq:pma}
    \frac{\partial u_t}{\partial t\hfill} (x)=f(x)-g(x^{u_t})+\ldet\left( \frac{\partial x^{u_t}}{\partial x\hfill} \right),
\end{equation}
for some initial function $u_0:\R^d\to\R$. The idea is that, as $t\rightarrow \infty$, the LHS of \eqref{eq:pma} should converge to zero, whereby $u_t$ should converge to the solution of the Monge-Amp\`{e}re equation for transporting $e^{-f}$ to $e^{-g}$. Although the PMA has been around in the literature, there has been some recent appearances in the context of optimal transport. For example,  \cite{kim2012parabolic} studied it for general cost functions on closed manifolds, while 
\cite{kitagawa2012parabolic} studies it for bounded domains. On the other hand, \cite{deb2025no} recently used it for generative modeling and variational inference. 



There is a more convenient form for us that we will utilize frequently. This needs the well-known change of measure lemma (see \cite[Theorem 1.6.9]{durrettprob}) which we note here for easy reference.
%\marginpar{ Probably this lemma should be stated before \eqref{eq:pma2}?}
\begin{lmm}\label{lem:jacobian}(Change of measure)
    Let $e^{-a}$ be a probability density function on $\R^d$ for some function $a:\R^d\rightarrow \R$. For a $\diffcont^2$ strictly convex function $\phi:\R^d\rightarrow \R$, let $e^{-b}$ be the pushforward $(\nabla \phi)_{\#} e^{-a}$ Then,
    \begin{equation}\label{eq:jacobian}
    b(x^\phi) = a(x) + \ldet \frac{\partial x^\phi}{\partial x\hfill}.
    \end{equation}
\end{lmm}


Now, let $h_t:\R^d\rightarrow \R$ be such that $e^{-h_t}$ is the pushforward of the measure $e^{-g}$ by the inverse of the map $x\mapsto x^{u_t}$. Then, by Lemma \ref{lem:jacobian}, it follows that \eqref{eq:pma} can be alternatively written as 
\begin{equation}\label{eq:pma2}
\frac{\partial u_t}{\partial t}(x)= f(x) - h_t(x) = \log \left( \frac{e^{-h_t(x)}}{e^{-f}}. \right).
\end{equation}

Consider the function on the Wasserstein space 
\begin{equation}\label{eq:choiceofF}
F(\rho):=\begin{cases}
\KL{\rho}{e^{-f}},& \text{when $\rho$ is dominated by {\color{red}$e^{-f}$}}, \\
+\infty,& \text{otherwise}.  
\end{cases}
\end{equation}
When $f$ is convex, it is well-known that $F$ is geodesically (and generalized geodesically) convex and lower semicontinuous. 

The first variation of $F$ (see \cite[Lemma 10.4.1]{ambrosio2005gradient}) at $\rho_t=e^{-h_t}$ is given by $\frac{\delta F}{\delta \rho_t}= f(x) - h_t(x)$, 
(ignoring an additive constant which does not affect its gradient $\nabla_{\wass} F = \nabla_x  \frac{\delta F}{\delta \rho}$).
Thus, \eqref{eq:pma2} may be written as $\dot{u}_t=\frac{\delta F}{\delta \rho_t}$ which is exactly \eqref{eq:mirrorgradflow3} and, by taking a gradient on both sides, we recover  \eqref{eq:mirrorgradflow2}. In our language, \textit{the PMA is the evolution of the mirror potential for the mirror gradient flow of relative entropy with respect to $e^{-f}$, where the mirror is generated by the function in \eqref{eq:mirror}}. 


By \eqref{eq:newgradflow} and \eqref{eq:expgrad}, the mirror gradient flow itself satisfies the continuity equation 
 \begin{equation*}
 \partial_t \et+\div{(\et v_t)}=0, \quad v_t(x)=-\frac{\partial\hfill}{\partial x^{u_t}} (f-h_t)(x),
 \end{equation*}
 where $(u_t,\; t\ge 0)$ solves the PMA \eqref{eq:pma}. This is the same equation we introduced in \eqref{eq:velocity}. We call $(\rho_t,\; t\ge 0)$ the Sinkhorn flow, and we will show that this is the limit of iterates of the Sinkhorn algorithm. Note that, alternatively, from \eqref{eq:pma2}, $u_t$ may also be expressed as
 \[
 u_t(x)= u_0(x) + \int_0^t (f(x) + \log \rho_s(x))ds, \quad t\ge 0.
 \]

\noindent Let us now formalize the existence of a solution to \eqref{eq:velocity}. To state this, we need some assumptions on the solution of the PMA \eqref{eq:pma}. 
\begin{assm}\label{asn:solcon}
Assume that $f,g,u_0$ are such that the PMA \eqref{eq:pma} admits a solution $\left( u_t,\; t\ge 0 \right)$. Additionally,
\begin{enumerate}[(i)]
\item Given any $T>0$, there exists constants $A_T>0$ and $B_T>0$ such that 
\begin{align}\label{eq:curvbd}
\inf_x \inf_{t\in [0,T]}\lmn\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\geq A_T,\quad \sup_x \sup_{t\in [0,T]} \lmx\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\le B_T.
\end{align}
\item The map $(t,x)\mapsto u_t(x)$  lies in $\diffcont^{1,2}([0,\infty)\times \R^d)$\footnote{The reader is warned that the  $\diffcont^{k,\ell}$ notation is not to be confused with the H\"{o}lder class of functions}. Further, for every $T>0$, all the corresponding mixed partial derivatives (in space and time) are bounded on $[0,T]\times \R^d$.
%\item The map $(t,y)\mapsto y^{u_t^*}$ (where $u_t^*$ is the convex conjugate of $u_t$) lies in $\diffcont^{1,2}([0,\infty)\times \R^d)$ (where the function $y^{u_t^*}$ is viewed componentwise). 
%\item (Moment assumptions) Let $Y\sim \exp(-g)$ and assume that for all $t,T>0$,
%$$\E\left[\bigg\lVert \nabla\frac{\partial u_t}{\partial t\hfill}(Y^{u_t^*})\bigg\rVert^2\lmx^2\left(\frac{\partial Y^{u_t^*}}{\partial Y\hfill}\right)\right]<\infty,$$
%$$\sup_{t\in [0,T]} \E\left(\frac{\partial u_t}{\partial t\hfill}(Y^{u_t^*})\right)^2<\infty.$$
%{\color{red} Circle back to check if (iii) is now needed.}
\item The first two derivatives of $f(\cdot)$ and $g(\cdot)$ are bounded and uniformly continuous. Further, the first four spatial derivatives of $\{u_t\}_{t\in [0,T]}$ and $\{u_t^*\}_{t\in [0,T]}$ are bounded and uniformly continuous, for all $T>0$.
 \end{enumerate}
\end{assm}

Given the solution of the PMA $(u_t,\; t\ge 0)$ (as in \eqref{eq:pma}), note that $\left(w_t=u_t^*,\; t\ge 0 \right)$ denotes the corresponding process of convex conjugates. By \cref{asn:solcon}, part (i), both $u_t$ and $w_t$'s are all strictly convex $\diffcont^2$ functions. Therefore, $\nabla u_t(\cdot)$ and $\nabla w_t(\cdot)$ are both diffeomorphisms on $\R^d$. In view of~\cref{def:pdcor}, we then have a system of dual coordinates on $\R^d$ given by $x\mapsto \xsut$ (or equivalently $y\mapsto y^{w_t}$), one for each $t\geq 0$. We show in Lemma \ref{lem:dualPMA} that the family $(w_t)_{t\ge 0}$ is also a solution of a PMA that we call the \text{dual PMA}. The following observation is an immediate consequence of \cref{asn:solcon} which we note as a remark below. 

\begin{remark}\label{rem:dualasn}
 If \cref{asn:solcon} holds for the solution of the PMA $(u_t)_{t\ge 0}$, then  the assumptions (i)--(iii)  also hold for $(w_t=u_t^*)_{t\ge 0}$, as $\nabla u_t = (\nabla w_t)^{-1}$, and vice versa. This is relevant for \cref{lem:dualPMA} below.
\end{remark}
  %\marginpar{I got stuck at this remark, what PME do we have for $w_t$? I understand $w_t$ corresponds to some flow, but, not clearly seeing its PME.. {\color{red} Wouldn't \eqref{eq:dualPMA} work?}} 


\begin{remark}\label{rem:bermanver}
Sufficient conditions for \cref{asn:solcon} have been studied in the literature under different assumptions on the supports of the probability measures $e^{-f}$ and $e^{-g}$, and the initializer $u_0$ for \eqref{eq:pma}. For example, in \cite[Proposition 4.5]{berman2020}, that if $u_0$ is four times continuously  differentiable, $f$ and $g$ are twice continuously differentiable, and $e^{-f}$, $e^{-g}$ are supported on the torus, then \cref{asn:solcon} is satisfied.  Similar results have also been established for compact Riemannian manifolds (see \cite[Theorem 1.1]{kim2012parabolic}), and general bounded convex domains (see \cite[Theorem 3.1]{kitagawa2012parabolic}, \cite[Theorem A]{Tang2013}, and \cite[Theorem 1.1]{Abedin2020}. A simple case when \cref{asn:solcon} holds for probability measures supported on $\R^d$ is where both $\mu$ and $\nu$ are Gaussian probability densities (see Examples \ref{ex:loc} and \ref{ex:scale}).

The most critical part of \cref{asn:solcon} is the curvature bound in \eqref{eq:curvbd}. When the probability measures are supported on $\R^d$, in a recent paper \cite[Lemma 2.2]{chiarini2024semiconcavity}, the authors show that the Sinkhorn potentials (see \eqref{eq:twostepit}) satisfy the curvature condition \eqref{eq:curvbd}, uniformly for all small enough $\vep$, provided that the Hessians of $f$, $g$ are bounded above and below by positive constants times identity, and both the initial potentials are strongly convex. Given the connections between the Sinkhorn potentials and the PMA \eqref{eq:pma} established in \cite[Lemma 4.4]{berman2020}, this suggests that the PMA \eqref{eq:pma} should also satisfy \eqref{eq:curvbd} under the additional log-concavity assumptions. A rigorous proof of this is left for future research.

\end{remark}


We are now in position to state our first main result which features the existence of a strong solution to \eqref{eq:velocity}. \begin{thm}\label{thm:existlin}
Fix $T>0$ and suppose that \cref{asn:solcon} holds. Recall $w_t=u_t^*$ from Remark \ref{rem:dualasn}. Consider the push-forward $\rho_t=(\nabla w_t)_{\#}e^{-g}$. Then $(\rho_t)_{t\in [0,T]}$ is an absolutely continuous curve in the $2$-Wasserstein space and it is a strong solution to \eqref{eq:velocity}. 
\end{thm}

The proof of \cref{thm:existlin} depends on the following change of coordinate lemma that will be used multiple times in this paper.

\begin{lmm}\label{lem:tensorel}
    Suppose that $\phi:\R^d \to\R$ is strictly convex such that the function $\nabla \phi: \R^d \rightarrow \R^d$ is a $\diffcont^2$ diffeomorphism. That is both  $\nabla \phi$ and its inverse $\nabla \phi^*$ are twice continuously differentiable. Then, 
    \begin{equation}\label{eq:tensorelpf}
    \frac{\partial}{\partial \xsph_j}\left(\log\det\left(\frac{\partial x^\phi}{\partial x\hfill}\right)\right)=-\sum_{\ell=1}^d \frac{\partial^2 x_j}{\partial x_{\ell}\partial \xsph_{\ell}},
    \end{equation}
    for all $j\in [d]$ and $x\in\R^d$.
\end{lmm}

The proof of \cref{lem:tensorel} is deferred to~\cref{sec:pfres}. 

\begin{proof}[Proof of \cref{thm:existlin}]
By invoking \cref{lem:jacobian} with $\phi=w_t$, $a=g$, we observe that $\rho_t$ is strictly positive and 
 $h_t:=-\log \rho_t$
 is well-defined. 
 Observe that 
 \begin{align}\label{eq:com1}
 h_t(x)=g(x^{u_t})-\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)=f-\frac{\partial u_t}{\partial t\hfill}
 \end{align}
 where the first equality is by \eqref{eq:jacobian} and the second equality is by \eqref{eq:pma}. 
 
Now starting with the first equality above along with the chain rule, we get:
\begin{align*}
\frac{\partial h_t}{\partial t\hfill}(x)\nonumber &=\frac{\partial}{\partial t}\left(g(x^{u_t})-\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)\nonumber \\ &=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill},\frac{\partial g\hfill}{\partial x^{u_t}} (x^{u_t})\bigg\rangle - \frac{\partial}{\partial t}\left(\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)
\end{align*}
For the second term, we will use \cite[Section A.4.1]{boyd2004convex} to note the fact that the derivative of $\ldet(A)$ with respect to the entries of $A$ is given by $A^{-1}$. By taking $A=\frac{\partial x^{u_t}}{\partial x\hfill}$ (therefore $A^{-1}=\frac{\partial x\hfill}{\partial x^{u_t}}$) and applying the chain rule again, we get
\begin{align}\label{eq:rhot1}
    -\frac{1}{\rho_t(x)}\frac{\partial \rho_t}{\partial t}(x)=\frac{\partial h_t}{\partial t\hfill}(x)=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill},\frac{\partial g\hfill}{\partial x^{u_t}} (x^{u_t})\bigg\rangle-\bigg\langle \frac{\partial x\hfill}{\partial x^{u_t}}, \frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle.\end{align}
%\nonumber \\ &=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill},-\frac{\partial g}{\partial x} (x^{u_t})\bigg\rangle + \sum_{i,j} \left(\frac{\partial x}{\partial x^{u_t}}\right)_{i,j} \frac{\partial}{\partial x}\left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_{i,j}.

Take 
\begin{align}\label{eq:vtalt}
v_t=\frac{\partial\hfill}{\partial x^{u_t}}((h_t-f)(x))=-\frac{\partial\hfill}{\partial x^{u_t}}\left(\frac{\partial u_t}{\partial t\hfill}(x)\right).
\end{align}
The first equality is the definition of $v_t$ as in  \eqref{eq:velocity}, and the second equality is the PMA \eqref{eq:pma}. Then by the product rule we have:
\begin{align}\label{eq:rhot2}
&\frac{\div(v_t\rho_t)}{\rho_t}(x)\nonumber =\langle v_t(x),\nabla \log{\rho_t(x)}\rangle + \div v_t(x)\nonumber \\ 
&=\bigg\langle -\frac{\partial}{\partial x^{u_t}}\left(\frac{\partial u_t}{\partial t\hfill}(x)\right),-\frac{\partial h_t}{\partial x\hfill}(x)\bigg\rangle - \div{\left(\frac{\partial}{\partial x^{u_t}}\left(\frac{\partial u_t}{\partial t\hfill}(x)\right)\right)},
\end{align}
where the last line uses \eqref{eq:vtalt}. We will now simplify each of the terms above. For the first term, observe that:

\begin{align*}
    \bigg\langle -\frac{\partial}{\partial x^{u_t}}\left(\frac{\partial u_t}{\partial t\hfill}(x)\right),-\frac{\partial h_t}{\partial x\hfill}(x)\bigg\rangle &=\bigg\langle \frac{\partial}{\partial x}\frac{\partial u_t}{\partial t\hfill}(x)\left(\frac{\partial x}{\partial x^{u_t}\hfill}\right),\frac{\partial h_t}{\partial x}(x)\bigg\rangle\\ &=\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill}, \left(\frac{\partial x}{\partial x^{u_t}\hfill}\right)\frac{\partial h_t}{\partial x}(x)\bigg\rangle =\bigg\langle \frac{\partial x^{u_t}}{\partial t\hfill},\frac{\partial h_t\hfill}{\partial x^{u_t}}(x)\bigg\rangle.
\end{align*}
The first and third equalities above use the chain rule (see \eqref{eq:notcal} for clarity of notation). For the second equality, we have interchanged the time and the space derivatives in the term $\frac{\partial}{\partial x}\frac{\partial u_t}{\partial t\hfill}(x)$ using \cref{asn:solcon}, part (ii). We now move on to the second term of \eqref{eq:rhot2}. Note that
\begin{align*}
    \div{\left(\frac{\partial}{\partial x^{u_t}}\left(\frac{\partial u_t}{\partial t\hfill}(x)\right)\right)}&=\div{\left(\frac{\partial x^{u_t}}{\partial t}\left(\frac{\partial x}{\partial x^{u_t}}\right)\right)}\\ &=\sum_{i,j=1}^d \frac{\partial}{\partial x_i}\left(\left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_j\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{j,i}\right).
\end{align*}
In the first equality, we have used the chain rule as before and the second display uses the definition of divergence. We will now  use the product rule to obtain 
\begin{align*}
    &\;\;\;\;\sum_{i,j=1}^d \frac{\partial}{\partial x_i}\left(\left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_j\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)_{j,i}\right)\\ &=\bigg\langle \frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right),\frac{\partial x\hfill}{\partial x^{u_t}}\bigg\rangle+\sum_{j=1}^d \left(\frac{\partial x^{u_t}}{\partial t\hfill}\right)_j\left(\sum_{i=1}^d \frac{\partial^2 x_i}{\partial x_i\partial x_i^{u_t}}\right)\\ &=\bigg\langle \frac{\partial x\hfill}{\partial x^{u_t}},\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle-\bigg\langle \left(\frac{\partial x^{u_t}}{\partial t\hfill}\right),\frac{\partial}{\partial x^{u_t}}\left(\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)\bigg\rangle.
\end{align*}
In the second inequality above, we have used \eqref{eq:tensorelpf} on the second term. We now combine our observations on the two terms of \eqref{eq:rhot2} to get: 
\begin{small}
\begin{align*}
    \frac{\div(v_t\rho_t)}{\rho_t}(x)&=\bigg\langle \left(\frac{\partial x^{u_t}}{\partial t\hfill}\right),\frac{\partial}{\partial x^{u_t}}\left(h_t(x)+\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)\bigg\rangle-\bigg\langle \frac{\partial x\hfill}{\partial x^{u_t}},\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle.
\end{align*}
\end{small}
Observe that, by applying $\frac{\partial}{\partial x_j^{u_t}}$ on both sides of \eqref{eq:com1} we get 
\begin{align*}
    \frac{\partial}{\partial x^{u_t}}\left(h_t(x)+\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)=\frac{\partial g\hfill}{\partial x^{u_t}}(x^{u_t}).
\end{align*}
Therefore, 
\begin{align}\label{eq:rhot3}
\frac{\div(v_t\rho_t)}{\rho_t}(x)=\bigg\langle \left(\frac{\partial x^{u_t}}{\partial t\hfill}\right),\frac{\partial g\hfill}{\partial x^{u_t}}(x^{u_t})\bigg\rangle-\bigg\langle \frac{\partial x\hfill}{\partial x^{u_t}},\frac{\partial}{\partial t}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle.
\end{align}

By adding \eqref{eq:rhot1} and \eqref{eq:rhot3}, it follows that $\frac{\partial \rho_t}{\partial t\hfill}+\div{(\rho_t v_t)}=0$.

\noindent Next, we establish absolute continuity of $(\rho_t)_{t\in [0,T]}$. By invoking \cite[Theorem 8.3.1]{ambrosio2005gradient}, it suffices to show that $\sup_{x,t\in [0,T]} \lVert v_t(x)\rVert<\infty$. By the representation of $v_t$ in \eqref{eq:vtalt} and the PMA \eqref{eq:pma}, we observe that for $t\in [0,T]$,
\begin{align*}
\lVert v_t(x)\rVert&=\left\lVert \left(\frac{\partial x}{\partial x^{u_t}}\right)\frac{\partial\hfill}{\partial x}\left(f(x)-g(x^{u_t})+\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)\right\rVert\\ &\le B_T \left\lVert \frac{\partial\hfill}{\partial x}\left(f(x)-g(x^{u_t})+\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right) \right\rVert.
\end{align*}
Here in the first equality, we have again used the chain rule and in the following inequality, we have used the upper bound in \cref{asn:solcon}, part (i). Next, note that by \cref{asn:solcon}, parts (i) and (iii), we have:
$$\sup_x \ \left\lVert \frac{\partial f}{\partial x}(x)\right\rVert<\infty, \quad \sup_x \left\lVert \frac{\partial\hfill}{\partial x}(g(x^{u_t}))\right\rVert\le \sup_x \left\lVert \frac{\partial g\hfill}{\partial x^{u_t}}(x^{u_t})\right\rVert \ \sup_x \left\lVert \frac{\partial x^{u_t}}{\partial x\hfill}\right\rVert<\infty.$$
Therefore, to prove $\sup_{x,t\in [0,T]} \lVert v_t(x)\rVert$ is bounded, it suffices to control the norm of $\frac{\partial}{\partial x}\left(\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)$. To wit, we will once again use \cite[Section A.4.1]{boyd2004convex} (as we did earlier while obtaining \eqref{eq:rhot1} above). For $i\in [d]$, we then get
\begin{align*}
    \frac{\partial}{\partial x}\left(\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)=\bigg\langle \frac{\partial x\hfill}{\partial x^{u_t}},\frac{\partial\hfill}{\partial x_i}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\bigg\rangle.
\end{align*}
Now observe that the entries of $\frac{\partial x\hfill}{\partial x^{u_t}}$ are uniformly bounded (in $x$ and $t\in [0,T]$) in terms of $A_T$ by using \cref{asn:solcon}, part (i). Also the entries of $\frac{\partial}{\partial x_i}\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)$ are uniformly bounded by \cref{asn:solcon}, part (iii), where we have assumed boundedness of the third derivative tensor of $u_t$'s. Therefore, 
$$\sup_{x,t\in [0,T]} \left\lVert \frac{\partial}{\partial x}\left(\ldet\left(\frac{\partial x^{u_t}}{\partial x\hfill}\right)\right)\right\rVert<\infty.$$
This readily implies $\sup_{x,t\in [0,T]} \lVert v_t(x)\rVert<\infty$ and completes the proof.
\end{proof}

The velocity field $(v_t)$ appearing in the Sinkhorn flow \eqref{eq:velocity} is not a gradient in $x$. Thus, it may not lie in the $2$-Wasserstein tangent space at $\rho_t$, and the metric derivative (as defined in \cite[Theorem 1.1.2]{ambrosio2005gradient}) of the curve at time $t$ may not be $\norm{v_t}_{L^2(\rho_t)}$, which is the case for usual gradient flows in an appropriate sense (see \cite[Theorem 8.3.1]{ambrosio2005gradient}). 

However, as we show below, if we replace the $2$-Wasserstein distance (see \eqref{eq:2wass}) with the $\dlt{}{e^{-g}}{\cdot}{\cdot}$ metric (see \eqref{eq:2linot}), the metric derivative of the curve is indeed given by $\norm{v_t}_{L^2(\rho_t)}$. 

\begin{thm}\label{prop:metderlin}
Suppose \cref{asn:solcon} holds. Then with $\rho_t$ as in \cref{thm:existlin}, for any $t\ge 0$, we have: 
\begin{align}\label{eq:linot}
\lim_{\delta\to 0} \frac{1}{\delta}\dlt{}{e^{-g}}{\rho_{t+\delta}}{\rho_t}=\lVert v_t\rVert_{L^2(\rho_t)},
\end{align}
and 
\begin{align}\label{eq:linotsec}
    \dlt{}{e^{-g}}{\rho_{t+\delta}}{(\nabla w_t+\delta v_t(\nabla w_t))_{\#} e^{-g}}=o(\delta).
\end{align}
\end{thm}

The conclusion in \eqref{eq:linotsec} shows that if we appropriately perturb $\nabla w_t$ in the direction of the velocity $v_t$, then the corresponding push-forward measure $(\nabla w_t+\delta v_t(\nabla w_t))_{\#} e^{-g}$ yields a first order approximation to $\rho_{t+\delta}$. Such results are very popular in the literature on continuity equations and usual gradient flows.  Similar results for Euclidean gradient flows imply that, following the tangent vector of a smooth curve $\{x_t\}$ at a point $t$ over a small timestep $\delta$ is ``close" up to the first order (in the Euclidean metric) to $x_{t+\delta}$. Alternatively, it means that the trajectory of a particle moving along a smooth curve can be approximated by piecewise constant velocity curves. This is also true for usual Wasserstein gradient flows (see \cite[Proposition 8.4.6]{ambrosio2005gradient}). There the authors show that if $(\rho_t)_{t\ge 0}$ satisfies a continuity equation with velocity $v_t$ which is a gradient in $x$ (not the case in our setting), then $\wass_2(\rho_{t+\delta},(\mathrm{Id}+\delta v_t)_{\#}\rho_t)=o(\delta)$. \cref{prop:metderlin} establishes a similar result for Wasserstein mirror flows with the Hilbertian linear optimal transport distance (see \eqref{eq:2linot}). Note that the Wasserstein distance is \emph{smaller than} the linear optimal transport distance. In this sense, the approximation of $\rho_{t+\delta}$ in \eqref{eq:linotsec} is stronger than that in \cite[Proposition 8.4.6]{ambrosio2005gradient} (albeit under stronger regularity conditions).


In order to prove \cref{prop:metderlin} we need some additional results. First is an important conjugacy relationship for the PMA. 
Remark \ref{rmk:hori-ver-corr} suggests that there is a dual flow because one can flip the choice of the horizontal and vertical directions. Indeed, recall that the potential $u_t$ in \eqref{eq:KPtoBP} gives the flow $\rho_t= (\nabla u_t)^{-1}_\# e^{-g}$. We can consider the family of convex conjugates $w_t=u_t^*$, whereby $\nabla w_t  = (\nabla u_t)^{-1}$. Viewed as vector fields the time derivative $\partial_t \nabla u_t$
    is equivalent to the vertical variation $\nabla_{\wass} F(\rho_t)$ while 
    $\partial_t \nabla w_t$  is equivalent to   the horizontal variation
   $- \nabla_{\wass}^U F(\rho_t).$
   By \eqref{eq:pma}, $(u_t)$ solves a PMA. We show in Lemma \ref{lem:dualPMA} that $(w_t)$ also solves a (different) PMA. This conjugacy is also evident from the Sinkhorn algorithm which comes with a pair of potentials at each step. The PMA \eqref{eq:pma} is the limit (in $\vep$) of one of these sequence of potentials. So, by symmetry, it is only natural that the other sequence of potentials has a PMA limit as well and the corresponding potentials in the two limiting PMAs are related by convex duality. 
  
   
   %One may understand the flow of $w_t$ as a mirror flow where the mirror potential is given by the (time-dependent) function 
   %$W_t(\cdot) =\frac{1}{2} \wass_2^2 ( \cdot, \rho_t)$.  
    
   %{\color{blue}[YH. I am not completely satisfied the latter half of this paragraph. ]
%\end{remark}

%\marginpar{\red
%[YH. As in Lemma~\ref{lem:dualPMA}, the mirror flow has its conjugate. Probably we want to say something about it in this subsection too? I tried it here. 
%Simply, for the conjugate $w$, \[\left(\nabla^2 w \right)^{-1} \nabla_{\wass}^U F(\rho_t)=   \nabla_{\wass} F(\rho)\]
%]


\begin{lmm}\label{lem:dualPMA}
Suppose \cref{asn:solcon} holds. Then the process $\left( w_t=u_t^*,\; t\ge 0 \right)$  is also the solution of a PMA:
\begin{equation}\label{eq:dualPMA}
\frac{\partial w_t}{\partial t}(y)= g(y) - f(y^{w_t}) + \ldet \left( \frac{\partial y^{w_t}}{\partial y\hfill}\right),
\end{equation}
with the initial condition $w_0=u_0^*$. 
\end{lmm}        

The next result is a property of the velocity field $v_t$ in \eqref{eq:velocity}.

\begin{lmm}\label{lem:convexcall}
Suppose \cref{asn:solcon} holds. Then, for $\delta>0$ small enough, the function $y\mapsto y^{w_t}+\delta v_t(y^{w_t})$ is the gradient in $y$ of the convex function $y\mapsto w_t(y)-\delta (f(y^{w_t})-h_t(y^{w_t}))$.
\end{lmm}

\begin{proof}[Proof of \cref{prop:metderlin}]

We first prove \eqref{eq:linot}. As $w_t=u_t^*$ satisfies $(\nabla w_t)_{\#} e^{-g}=\rho_t=e^{-h_t}$, we have:
    \begin{align}\label{eq:simlin}
    \frac{1}{\delta^2}\dlt{2}{e^{-g}}{\rho_{t+\delta}}{\rho_t}&=\int \lVert \delta^{-1}(\nabla w_{t+\delta}(y)-\nabla w_t(y))\rVert^2 e^{-g(y)}\,dy.
    \end{align}
    By \cref{asn:solcon} (also see \cref{rem:dualasn}), we note that
    $$\sup_y \bigg|\delta^{-1}(\nabla w_{t+\delta}(y)-\nabla w_t(y))-\nabla \frac{\partial w_t}{\partial t\hfill}(y)\bigg|\le \sup_{y,s\in [t,t+\delta]}\bigg|\nabla \frac{\partial w_s}{\partial s\hfill}(y)-\nabla \frac{\partial w_t}{\partial t\hfill}(y)\bigg|=o(1).$$
    By combining the above display with \eqref{eq:dualPMA}, we get:
    \begin{align*}
        \sup_y \bigg|\delta^{-1}(\nabla w_{t+\delta}(y)-\nabla w_t(y))- \nabla\left(g(y)-f(y^{w_t})+\ldet\left(\frac{\partial y^{w_t}}{\partial y\hfill}\right)\right)\bigg|=o(1).
    \end{align*}
    Here $o(1)$ is with respect to $\delta\to 0$. Next we use the first equality in \eqref{eq:vtalt} to observe that 
    \begin{align}\label{eq:labelgrad}
        v_t(y^{w_t})=\frac{\partial}{\partial y}((h_t-f)(y^{w_t}))=\frac{\partial}{\partial y}\left(g(y)-f(y^{w_t})+\ldet\left(\frac{\partial y^{w_t}}{\partial y\hfill}\right)\right).
    \end{align}
    In the second equality, we have used \eqref{eq:com1}. By combining the two displays above, we get:
    \begin{align}\label{eq:vtcall2}
        \sup_y \bigg|\delta^{-1}(\nabla w_{t+\delta}(y)-\nabla w_t(y))- v_t(y^{w_t})\bigg|=o(1).
    \end{align}
    By combining the above display with \eqref{eq:simlin}, we get
    \begin{align*}
    \frac{1}{\delta^2}\dlt{2}{e^{-g}}{\rho_{t+\delta}}{\rho_t}&=\int \lVert v_t(y^{w_t})\rVert^2 e^{-g(y)}\,dy+o(1).
    \end{align*}
    A final change of variable with $x=y^{w_t}$ establishes \eqref{eq:linot}. 

\vspace{0.05in}

We next prove \eqref{eq:linotsec}. By \cref{lem:convexcall}, there exists $\delta>0$ small enough such that the function $y\mapsto y^{w_t}+\delta v_t(y^{w_t})$ is the gradient of a convex function. With such $\delta>0$, by McCann's Theorem (see \cite{McCann1995}), $y^{w_t}+\delta v_t(y^{w_t})$ is the optimal transport map from $e^{-g}$ to $(y^{w_t}+\delta v_t(y^{w_t}))_{\#} e^{-g}$. As $y^{w_{t+\delta}}$ is the optimal transport map from $e^{-g}$ to $\rho_{t+\delta}$. With these observations, we get:
\begin{align*}
    &\;\;\;\;\dlt{}{e^{-g}}{\rho_{t+\delta}}{(\nabla w_t+\delta v_t(\nabla w_t))_{\#} e^{-g}}\\ &=\left(\int \lVert \nabla w_{t+\delta}(y)-\nabla w_t(y)-\delta v_t(y^{w_t})\rVert^2 e^{-g(y)}\,dy\right)^{\frac{1}{2}}=o(\delta), 
\end{align*}
where the last equality follows from \eqref{eq:vtcall2}. This proves \eqref{eq:linotsec}.
\end{proof}

\begin{remark}[Mirror descent for fixed $\vep>0$]\label{rem:Flavian}
In \cite[Theorem 2]{leger2021gradient}, the author showed that for fixed $\vep>0$, the marginals of the Sinkhorn algorithm \eqref{eq:sinkupdt} can be viewed as iterations of an explicit mirror descent algorithm. In particular, one can (informally) write 
\begin{align*}
\rho_{k+1}^{\vep}&=\argmin_{\mu\in\mathcal{P}_2(\R^d)}\bigg[\KL{\rho_{k}^{\vep}}{e^{-f}}+\bigg\langle \mu-\rho_{k}^{\vep}, \frac{\partial \hfill}{\partial \mu}\KL{\mu}{e^{-f}}\big|_{\mu=\rho_{k}^{\vep}}\bigg\rangle\\ &\quad\quad + F^*(\mu)-F^*(\rho_{k,\vep})-\bigg\langle \mu-\rho_{k}^{\vep},\frac{\partial \hfill}{\partial \mu}F^*(\mu)\bigg|_{\mu=\rho_{k}^{\vep}}\bigg\rangle,  
\end{align*}
where $F^*$ is the convex conjugate of the function $F$ defined by 
$$F(\phi)=\langle \opV[\phi],e^{-g}\rangle,$$
for ``smooth" functions $\phi:\R^d\to\R^d$ and $\opV$ defined in \eqref{eq:basedef} later. 

While \cite{leger2021gradient} focuses on the $\vep>0$ case, we take the limiting perspective ($\vep\to 0$) which allows us to explicitly identify the mirror function (see \eqref{eq:mirror}) and velocity vector field of the limiting (Sinkhorn) PDE \eqref{eq:velocity}. To our understanding, these explicit quantities cannot be derived from the proof techniques used in \cite{leger2021gradient}.
\end{remark}


\subsection{Exponential convergence of the 
Sinkhorn flow}\label{sec:sinkflow}

As \eqref{eq:velocity} 
arises out of the mirror flow of the entropy functional $\KL{\rho}{e^{-f}}$, it is natural to ask the following: does $\rho_t\to e^{-f}$ as $t\to\infty$? If so, then in what sense and what is the speed of convergence? 

To address this, we will now establish the exponential convergence of the Sinkhorn flow to $e^{-f}$. To begin, let us define the log-Sobolev inequality.

\begin{defn}\label{def:isid}
    We say that a probability measure $\xi\in \ptac$ satisfies a logarithmic Sobolev inequality (LSI) with constant $\lsi{\xi}>0$ if for all $\rho\in\ptac$, 
    \[
    \KL{\rho}{\xi}\le \frac{1}{2\lsi{\xi}}I(\rho|\xi),
    \]
    where $I(\rho|\xi)$ is the \emph{relative Fisher information} defined by
    \[
    I(\rho|\xi):=\int \left\lVert \nabla\log\frac{d\rho}{d\xi}\right\rVert^2\,d\rho.
    \]
\end{defn}

\begin{thm}\label{lem:expcon}
    Suppose there exists $\lsi{f}>0$ such that $e^{-f}$ satisfies LSI with constant $\lsi{f}$. Also assume that there exists a positive continuous function $h(\cdot)$ on $[0,\infty)$ such that
    \begin{equation}\label{eq:uniflb}
    \inf_{x\in\R^d}\lmn\left(\frac{\partial x\hfill}{\partial x^{u_t}}\right)\ge h(t).
    \end{equation}
    Then we have
    $$\KL{\rho_t}{e^{-f}}\le \KL{\rho_0}{e^{-f}}\exp(-2\lsi{f}H(t)),$$
    and 
    $$\wass_2(\rho_t,e^{-f})\le \sqrt{\frac{2\KL{\rho_0}{e^{-f}}}{\lsi{f}}}\exp(-\lsi{f} H(t)),$$
    where $H(t):=\int_0^t h(s)\,ds$.
\end{thm}

%{\red YH. The condition \eqref{eq:uniflb}  depends on the mirror-gradient flow itself. So, without providing a condition where it holds the whole theorem may not look attractive. 
%We can remark the equivalence between the mirror-flow and the PME, and use Assumption \ref{asn:solcon}. On the other hand, under the same assumption the PME solution converges exponentially, so this theorem is nothing but a translation of it. }

%\SP{ Part of this is, of course, unavoidable. Any condition for the convergence of the flow will be, vice versa, a condition on the convergence of the PMA.}

%{\color{blue} ND: Also \cite{berman2020}, only mentions exponential convergence of $u_t$ to $u_{\infty}$ in uniform norm. It is not clear to me if exponential rates in terms of KL divergence for $\rho_t$ to $e^{-f}$ follow from that. Given that Sinkhorm is the mirror flow of KL, I think exponential rates for KL as provided by \cref{lem:expcon} is what we want.}

\begin{proof}[Proof of \cref{lem:expcon}]
    For convenience, we define $m:=\KL{\rho_0}{e^{-f}}$. By using the standard terminology of analysis in the $2$-Wasserstein space (see \cite[Chapters 9 and 10]{ambrosio2005gradient}), we note that the function $\rho\mapsto \KL{\rho}{e^{-f}}$ admits a subdifferential $\nabla \log\rho_t+\nabla f$.  
    %\marginpar{ [YH. Don't we need convexity of $f$ for such displacement convexity? For the proof we only need LSI.] } 
 This implies
    \begin{align*}
        \frac{d}{dt}\KL{\rho_t}{e^{-f}}&=\int \iprod{\nabla \log{\rho_t(x)}+\nabla f(x),v_t(x)}\,d\rho_t(x)\\ &=-\int \left(\nabla \log{\rho_t(x)}+\nabla f(x)\right)^{\top}\frac{\partial x\hfill}{\partial x^{u_t}\hfill}\left(\nabla \log{\rho_t(x)}+\nabla f(x)\right)\,d\rho_t(x).
    \end{align*}
    In the last display, we have used the representation of $v_t$ from \eqref{eq:velocity} coupled with the chain rule as illustrated in \eqref{eq:notcal}. Next, by using \eqref{eq:uniflb}, we get:
    \begin{align*}
        &\;\;\;\;-\int \left(\nabla \log{\rho_t(x)}+\nabla f(x)\right)^{\top}\frac{\partial x\hfill}{\partial x^{u_t}\hfill}\left(\nabla \log{\rho_t(x)}+\nabla f(x)\right)\,d\rho_t(x)\\ &\le -h(t)\int \lVert \nabla \log{\rho_t(x)}+\nabla f(x)\rVert^2\,d\rho_t(x)\\ &=-h(t) I(\rho_t|e^{-f})\le -2\lsi{f}h(t)\KL{\rho_t}{e^{-f}}
    \end{align*}
    In the final display here we have used the logarithmic Sobolev inequality (LSI) assumption, see \cref{def:isid}. 
    Next by invoking Gronwall's inequality (see \cite[Theorem II]{walter2012differential}), we get:
    $$\KL{\rho_t}{e^{-f}}\le \KL{\rho_0}{e^{-f}}\exp(-2\lsi{f}\int_0^t h(s)\,ds).$$
    Next we use the HWI inequality (see \cite[Theorem 1]{Otto2000}) to get:
    $$\wass_2^2(\rho_t,e^{-f})\le \frac{2}{\lsi{f}}\KL{\rho_t}{e^{-f}}\le \frac{2}{\lsi{f}}\KL{\rho_0}{e^{-f}}\exp(-2\lsi{f}\int_0^t h(s)\,ds).$$
    This completes the proof.
\end{proof}

The LSI assumption (see~\cref{def:isid}) is standard in the literature on rates of convergence of flows and plays a pivotal role in establishing information geometric inequalities; see \cite{Anton2001,Otto2000} and the references therein. In particular, the LSI condition can be verified in many popular examples. We cite two of them below.

\begin{enumerate}
    \item If $\inf_{x\in\R^d}\lmn(\nabla^2 f(x))\ge c$ for some constant $c>0$, then $e^{-f}$ satisfies LSI with constant $c>0$ (see \cite{Bakry1985}).
    \item Suppose $e^{-\tilde{f}}$ satisfies LSI with constant $\tilde{c}$. Let $\bar{f}:=f-\tilde{f}$ and assume $\bar{f}\in L^{\infty}(\R^d)$. Then $e^{-f}$ satisfies LSI with constant $c:=\tilde{c}\exp(\inf \bar{f}-\sup \bar{f})$ (see \cite{Holley1987}).
\end{enumerate}
We refer the reader to \cite{Cattiaux2010,Chen2021,Wang2001} for other conditions under which  the LSI condition can be established.

%\SP{The following set of assumptions should be removed from here and stated before the main theorems.} \ND{In \cref{sec:mcconst}. We might need to use some assumption here that guarantees $\nabla u_t$'s are diffeomorphisms.}

\subsection{Other examples of Wasserstein mirror gradient flows}\label{sec:othexamp}

It is natural to be curious about Wasserstein mirror gradient flows with other choices of function $F$. We give a few examples below. The reader should be careful that the PDEs and flows that we calculate have not been shown to exist or be well-behaved. 
\medskip

%\noindent\textbf{Example 1: Potential energy.} 
\begin{ex}[Potential energy]
Consider the function $F(\rho)=\frac{1}{2}\int \norm{x}^2 \rho(dx)$. It is strictly geodesically convex and has a unique minimizer at $\delta_0$. 

Given $e^{-g}$ and the mirror function \eqref{eq:mirror}, let us compute the time evolution of the mirror gradient flow. The evolution of the mirror potential is given by 
\[
\frac{\partial u_t}{\partial t}(x)= \frac{\delta F}{\delta \rho_t}(x)= \frac{1}{2}\norm{x}^2.  
\]
Thus $u_t= u_0 + \frac{t}{2} \norm{x}^2$, $t\ge 0$. A simple but instructive special case is when $u_0(x)=\frac{1}{2} \norm{x}^2$, i.e. $\rho_0=e^{-g}$, itself. Then $\nabla u_t= (1+t)\mathbf{id}$. Thus $\rho_t$ is the law of $Y/(1+t)$, where $Y \sim e^{-g}$. Clearly $\lim_{t\rightarrow \infty}\rho_t=\delta_0$ in $\wass_2$.
\end{ex}

\medskip

%\noindent\textbf{Example 2: Entropy.} 
\begin{ex}[Entropy]
Let $F(\rho)= \int \rho(x) \log \rho(x) dx$ denote the entropy function. The function is defined to be $+\infty$ when the measure is not absolutely continuous. From \cite[Lemma 10.4.1]{ambrosio2005gradient}, $\frac{\delta F}{\delta \rho}=\log \rho + 1$. Thus, given $e^{-g}$, the time evolution of the mirror potential is given by the PDE
\begin{equation}\label{eq:entropymirror}
\frac{\partial u_t}{\partial t}(x)=  \log \rho_t(x) +1, \quad \text{i.e.},\quad \nabla u_t(x)= \nabla u_0(x) + \int_0^t \nabla \log \rho_s ds.
\end{equation}
The flow on the other hand is given by the continuity equation $\dot{\rho}_t + \nabla \cdot (v_t \rho_t)=0$ where
\[
v_t(x) = - \frac{\partial\hfill}{\partial x^{u_t}} \log \rho_t(x).  
\]
A closed form solution is available for the Gaussian family. Let $\rho_0, e^{-g}$ be both standard normal. Thus $\nabla u_0(x)= x$. Then $\rho_t=N(0, (1+t)^2I)$ is a solution of the mirror gradient flow. This can be easily verified from the fact that 
\[
\nabla \log \rho_t(x)= -\frac{x}{(1+t)^2}, \quad \nabla u_t(x)= \frac{x}{(1+t)},\quad \nabla \frac{\partial u_t}{\partial t}(x)= -\frac{x}{(1+t)^2}.
\]
This system is a solution to \eqref{eq:entropymirror}. 

It is, of course, well-known that the Wasserstein gradient flow of entropy is the heat flow which admits the solution $N(0, (1+t)I)$ at time $t$ when started with standard normal. Thus, in this case, the mirror gradient flow ``converges'' faster than the usual gradient flow, although in both cases the solution diffuses as $t\rightarrow \infty$.  
\end{ex}

\medskip

%\noindent\textbf{R\'enyi entropy.} 
\begin{ex}[R\'{e}nyi entropy]
In \cite{Otto_2001}, Otto identified the solution of the porous medium equation as the Wasserstein gradient flow of the following functional that is related to the R\'{e}nyi entropy:
\[
F(\rho)= \frac{1}{m-1}\int \rho^m(x)dx,  
\]
where $m \ge 1 - \frac{1}{d}$, $m> \frac{d}{d+2}$ and $m\neq 1$. The function is defined to be $+\infty$ if $\rho$ is not absolutely continuous.

To find its mirror gradient flow, fix $e^{-g}$ to generate the mirror potential. From \cite[Lemma 10.4.1]{ambrosio2005gradient}, $\frac{\delta F}{\delta \rho}=\frac{m}{m-1} \rho^{m-1}$. Then, the evolution of the mirror potential is given by the PDE 
\[
\frac{\partial u_t}{\partial t}(x)= \frac{m}{m-1} \rho_t^{m-1}, \quad \text{where}\quad \rho_t= (\nabla u^*_t)_{\#} e^{-g},
\]
and the continuity equation of the mirror gradient flow is given by the velocity
\[
v_t(x)=-m\rho_t^{m-2}\nabla_{x^{u_t}} \rho_t(x).
\]
It is not immediate how these new flows compare with the traditional ones. 
\end{ex}