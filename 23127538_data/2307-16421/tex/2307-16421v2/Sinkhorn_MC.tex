\section{Sinkhorn Markov chain and Limiting Dynamics}\label{sec:mcconst}

\noindent Fix $\epsilon>0$. Following \cite{berman2020}, we will track the evolution of the Sinkhorn algorithm  using the following  maps $\opV:\diffcont(\R^d)\to \diffcont(\R^d)$ and $\opU:\diffcont(\R^d)\to \diffcont(\R^d)$, where 
\begin{equation}\label{eq:basedef}
    \begin{split}
        \opV[u](y)&:= \vep \log\int \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}u(x)-f(x)\right)\,dx,\quad u \in \diffcont(\R^d)\\
        \opU[v](x)&:= \vep \log\int  \exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}v(y)-g(y)\right)\,dy, \quad v \in \diffcont(\R^d).
    \end{split}
\end{equation}



Next, consider the new operator $\opS:\diffcont(\R^d)\to \diffcont(\R^d)$ defined by:
\begin{equation}\label{eq:Sep}
\opS[u]:=\opU\circ \opV[u].    
\end{equation}
In terms of the Sinkhorn algorithm $\opS$ tracks the potential after two successive steps.

The following proposition from \cite[eqn. (2.1.4)]{berman2020} shows an useful property of the increment $\opS[u]-u$ which will be useful in the sequel. 

\begin{prop}\label{cl:normalize}
For any $u\in \diffcont(\R^d)$, consider the nonnegative function \begin{equation}\label{eq:claim12}
\rv[u](x):=\exp\left(\frac{1}{\vep}\left(\opS[u](x)- u(x)\right)\right).
\end{equation}
Then, 
\[
\int \rv[u](x)\exp(-f(x))\,dx=1.
\]
That is, $\rv[u]\exp(-f)$ is a probability measure. 
\end{prop}

The operator $\opS$ yields a natural iteration on $\diffcont(\R^d)$. Starting with some $u_0\in \diffcont(\R^d)$, consider
\begin{equation}\label{eq:twostepit}
u^{\vep}_{k+1}:=\opS[u_k],\quad \mk{k+1}:=\rv[u_k^{\vep}]\exp(-f),\quad k\ge 0.
\end{equation}
The $u_k^{\vep}$s are usually called \emph{Sinkhorn potentials}. By~\cref{cl:normalize}, we have
\begin{equation}\label{eq:discdiff}
\frac{u^{\vep}_{k+1}-u^{\vep}_k}{\vep}-f=\log{\mk{k+1}},\quad \mbox{where}\, \int_x \mk{k+1}(x)\,dx=1.
\end{equation}
Consequently, the average increment satisfies
\[
\int_x (u^{\vep}_{k+1}(x)-u^{\vep}_k(x))\,e^{-f(x)}\,dx=-\vep \KL{e^{-f}}{{\color{purple}\mk{k+1}}}\leq 0,
\]
where $\mI$ denotes the appropriate Kullback-Leibler divergence.

Note how the LHS of \eqref{eq:discdiff} looks like a discrete time derivative if iterations are indexed in units of $\epsilon$. That is, we replace $k$ by $k \vep$ and $(k+1)$ by $(k+1)\vep$. This observation will be useful later when we take scaling limit by sending $k\vep \rightarrow t>0$. 
\par 

%\subsection{Sinkhorn Markov chain} 

Define the following sequence of probability densities on $\R^d\times\R^d$ for $k\ge 0$:
\begin{align}\label{eq:coupling}
\gvp_{k+1}(x,y):=\exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}u^{\vep}_k(x)-\frac{1}{\vep}\opV[u^{\vep}_{k}](y)-f(x)-g(y)\right).
\end{align}
From the definitions of $\opV$, $\opU$ and $\opS$ in \eqref{eq:basedef} and \eqref{eq:claim12}, it is easy to check that
$$\int_{\R^d\times\R^d}\gvp_{k+1}(x,y)\,dx\,dy=1,\quad \forall \ k\ge 0,$$
and further
\begin{equation}\label{eq:curtaincall}
p_X \gvp_{k+1}=\mk{k},\quad p_Y \gvp_{k+1}=\exp(-g),\quad \quad \forall\ k\ge 0.
\end{equation}
Therefore $\gamma_{k}^{\vep}$ is the Schr\"{o}dinger Bridge (see \cite{Schrodinger1932}) coupling between $\mk{k}$ and $e^{-g}$.  
{\color{black}As the $Y$-marginals of all $\gvp_{k+1}$s remain stationary at $\exp(-g)$, one can construct a natural Markov chain using $\gvp_{k+1}$s. This is elucidated in the following definition.

\begin{defn}[Sinkhorn Markov chain]\label{prop:mchn}
Let $\gamma_0^{\vep}$ be some arbitrary joint distribution where $p_Y \gamma_0^\vep =e^{-g}$. For $k\ge 1$, consider the family of joint distributions $\gamma_k^\vep$ from \eqref{eq:coupling}. Then, the transition probabilities for the Sinkhorn Markov chain can be defined inductively as follows. For any $k\ge 0$, suppose $(X_k^{\vep},Y_k^{\vep})=(x,y)$. Then sample $X_{k+1}^{\vep}$ from the conditional distribution of $X|Y=y$ under $\gvp_{k+1}$. Now  suppose $X_{k+1}^\vep=x'$. Then, sample $Y^{\vep}_{k+1}$ similarly from the other conditional, i.e., $Y|X=x'$ under $\gvp_{k+1}$. Then $(X_k^{\vep},\ k\ge 0)$ forms a time-inhomogeneous Markov chain which we will call the \emph{Sinkhorn Markov chain}.
\end{defn}

The transition kernel for the Markov chain in \cref{prop:mchn} can be written out in terms of the conditional distributions under $(\gvp_{k+1},\ k\ge 0)$. In particular, note that 
\begin{equation}\label{eq:con1}
p_{Y|X}\gvp_{k+1}(y|x)=\exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}\opV[u_k^{\vep}](y)-\frac{1}{\vep}\opS[u_k^{\vep}](x)-g(y)\right),
\end{equation}
and 
\begin{equation}\label{eq:con2}
p_{X|Y}\gvp_{k+1}(x|y)=\exp\left(\frac{1}{\vep}\langle x,y\rangle-\frac{1}{\vep}\opV[u_k^{\vep}](y)-\frac{1}{\vep}u_k^{\vep}(x)-f(x)\right).
\end{equation}
Then the transition kernel can be written as
\begin{align}\label{eq:tranker}
    \Pr(X^{\vep}_{k+1}\in \,dx|X^{\vep}_k=z)&=\int_{\R^d} p_{Y|X}\gvp_{k}(y|z)p_{X|Y}\gvp_{k+1}(x|y)\,dy.
\end{align}

The following is easy to check. We provide a brief proof below.

\begin{prop}\label{prop:Sinmar}
    The marginals of the \emph{Sinkhorn Markov chain} are distributed according to $\mk{k}$ for $k\ge 1$.
\end{prop}

\begin{proof}
    By \eqref{eq:tranker}, for $k\ge 1$, we get:
    \begin{align*}
        \mathbb{P}(X_{k}^{\vep}\in \,dx)&=\int \mathbb{P}(X_{k}^{\vep}\in \, dx| X_{k-1}^{\vep}=z)p_{X}\gvp_{k-1}(z)\,dz\\ &=\int \int p_{Y|X}\gvp_{k-1}(y|z)p_{X|Y}\gvp_{k}(x|y) p_{X}\gvp_{k-1}(z)\,dy\,dz\\ &=\int p_{X|Y}\gvp_{k}(x|y)\left(\int \gvp_{k-1}(z,y)\,dz\right)\,dy\\ &=\int \exp\left(\frac{1}{\vep}\langle x,y\rangle - \frac{1}{\vep}\opV[u_{k-1}^{\vep}](y)-\frac{1}{\vep}u_{k-1}^{\vep}(x)-f(x)-g(y)\right)\,dy\\ &=\exp\left(\frac{1}{\vep}(u_{k}^{\vep}(x)-u_{k-1}^{\vep}(x))-f(x)\right)=\mk{k}(x).
    \end{align*}
    For the fourth equality, we use \eqref{eq:con2}. The fifth equality uses \eqref{eq:curtaincall}. The rest follows using elementary manipulations of conditional probabilities.

    %Next assume that the conclusion holds for $k=k_0\ge 1$, i.e., we assume that the following holds: 
    %\begin{align}\label{eq:indstep}
     %   \mathbb{P}(X_{k_0}^{\vep}\in dx)=\mk{k_0}(x).
    %\end{align}
    %We aim to show that the same conclusion holds for $k=k_0+1$. To wit, observe that  
    %\begin{align*}
     %   \mathbb{P}(X_{k_0+1}^{\vep}\in \,dx)&=\int \mathbb{P}(X_{k_0+1}^{\vep}\in \, dx| X_{k_0}^{\vep}=z)p_{X}\gvp_{k_0}(z)\,dz\\ &=\int \int \mathbb{P}(X_{k_0+1}^{\vep}\in x| Y_{k_0}^{\vep}=y, X_{k_0}^{\vep}=z)p_{Y|X}\gvp_{k_0}(y|z) p_X \gvp_{k_0}(z)\,dz\,dy\\ &=\int p_{X|Y}\gvp_{k_0+1}(x|y)\left(\int \gvp_{k_0}(z,y)\,dz\right)\,dy\\ &=\int \exp\left(\frac{1}{\vep}\langle x,y\rangle - \frac{1}{\vep}\opV[u_{k_0}^{\vep}](y)-\frac{1}{\vep}u_{k_0}^{\vep}(x)-f(x)-g(y)\right)\,dy\\ &=\exp\left(\frac{1}{\vep}(u_{k_0+1}^{\vep}(x)-u_{k_0}^{\vep}(x))-f(x)\right)=\mk{k_0+1}(x).
   % \end{align*}
\end{proof}

It is natural to ask if the Sinkhorn Markov chain in \cref{prop:mchn} has a diffusive limit as $\vep\rightarrow 0+$. In fact, the drift and the diffusion coefficients in the SDE \eqref{eq:diffSDE} correspond to the leading order terms of the conditional expectation and the conditional variance of the one-step increment of this Markov chain. Hence, it can be conjectured that the Sinkhorn diffusion is indeed the limit of the Sinkhorn Markov chain, under suitably strong assumptions. However, the proof of this convergence appears challenging and is not taken up here. The theorem below proves the weaker statement of one-dimensional marginal convergence, i.e., the convergence of $\rho_k^{\vep}$s according to \cref{prop:Sinmar}. 




\begin{thm}\label{thm:convergence}
    Suppose that Assumption \ref{asn:solcon} holds. Fix $T>0$. For $t\in [0,T]$, recall from \cref{thm:existlin} that $\rho_{t}=\exp(-h_t)=(\nabla w_t)_{\#} e^{-g}$ satisfies the Sinkhorn flow in \eqref{eq:velocity}. Then the following holds:
    
    \begin{equation}\label{eq:step1show}
    \sup_{k\ge 1\ : \ k\vep\le T}\wass_2^2(\rho_{k\vep},\mk{k})\le C_T \vep,
    \end{equation}
    where $C_T$ is a constant depending on $T$ and the constants implicit in Assumption \ref{asn:solcon}. This implies, in particular,  
    $$ \mk{\lfloor T/\vep \rfloor}\to \rho_T, \qquad \mbox{as}\ \vep\to 0$$
    for every fixed $T>0$, in the topology of weak convergence.
\end{thm}

\begin{proof}
{\color{black}    
We set up some notation first. Let $(u_{k\vep})_{k\ge 0}$ and $(w_{k\vep}=u^*_{k\vep})_{k\ge 0}$ denote sequences corresponding to the PMA process \eqref{eq:pma} and the dual PMA process \eqref{eq:dualPMA}, restricted to time points $t=0,\vep,2\vep,\ldots $. Throughout this proof, we will write $C>0$ for a generic constant depending on $f$, $g$, $T$, $d$, but crucially not on $\vep$. Note that this constant may change from one line to another.

To establish \eqref{eq:step1show}, we also define
\[
\tilde{\xi}_{k\vep}:=\left( \nabla w_{k\vep},\mathrm{id}\right)_{\#} e^{-g},
\]
which is the law of $(Y^{w_t},Y)$ where $Y \sim e^{-g}$. Therefore $p_X \tilde{\xi}_{k\vep}=\rho_{k\vep}$. Finally, recall the definition of $\gamma^{\vep}_{k+1}(x,y)$ from \eqref{eq:coupling}.  

\vspace{0.1in}

\emph{Proof of \eqref{eq:step1show}.} In this proof, we will establish the following result:

\begin{align}\label{eq:jointcon}
\wass_2^2(\gamma_{k+1}^{\vep},\tilde{\xi}_{k\vep})\le C\vep.
\end{align}
As the $2$-Wasserstein distance between the marginals is smaller, by \eqref{eq:jointcon}, we have $\wass_2^2(p_X \gamma_{k+1}^{\vep}, \ p_X \tilde{\xi}_{k\vep})\le C\vep$. Further, As $p_X \gvp_{k+1}=\rho_k^{\vep}$ (by \eqref{eq:curtaincall}) and $p_X \tilde{\xi}_{k\vep}=\rho_{k\vep}$ (as argued above), the conclusion in \eqref{eq:step1show} will follow. Therefore, it only remains to show \eqref{eq:jointcon}.

\vspace{0.05in}

\emph{Proof of \eqref{eq:jointcon}.} We will break this proof down into $3$ steps. 

\noindent \emph{Step (a).} We will show that 
\begin{align}\label{eq:stepa}
\wass_2^2(\gamma_{k+1}^{\vep},\tilde{\xi}_{k\vep})\le \E_{\gamma_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2.
\end{align}

\noindent \emph{Step (b).} The right hand side of \eqref{eq:stepa} is technically challenging to work with as it involves $\gvp_{k+1}$, which in turn involves the Sinkhorn potentials $u_k^{\vep}$'s which are not smooth as $\vep\to 0$. To circumvent this, we define a surrogate probability measure replacing $u_k^{\vep}$'s in the right hand side of \eqref{eq:stepa} with $u_{k\vep}$'s from the PMA. To wit, define 
$$
\xi^{\vep}_{k+1}(x,y):=\exp\left(\frac{1}{\vep}\langle x,y\rangle - \frac{1}{\vep}u_{k\vep}(x)-\frac{1}{\vep}\opV[u_{k\vep}](y)-f(x)-g(y)\right),
$$
We will show in step (b) that: 
\begin{align}\label{eq:stepb}
\E_{\gamma_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2\le C\E_{\xi_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2.
\end{align}

\noindent \emph{Step (c).} In the final step, we will leverage the smoothness of $(u_{k\vep})$'s from \cref{asn:solcon} to show that 

\begin{align}\label{eq:stepc}
\sup_{k:\ k\vep\le T} \E_{\xi_{k+1}^{\vep}} \lVert X- Y^{w_{k\vep}}\rVert^2\le C\vep.
\end{align}

Clearly, combining \eqref{eq:stepa}, \eqref{eq:stepb} and \eqref{eq:stepc} establishes \eqref{eq:jointcon}, thereby completing the proof. 

\vspace{0.05in}

\emph{Proof of step (a).} Recall that $p_Y \gvp_{k+1}=e^{-g}$ (from \eqref{eq:curtaincall}) and $p_Y \xi_{k+1}^{\vep}=e^{-g}$ (by definition). With this in mind, construct a coupling $\tilde{\pi}_{k+1}^{\vep}$ between $\gamma_{k+1}^{\vep}$ and $\tilde{\xi}_{k\vep}$ as follows: sample $(X,Y)\sim \gamma_{k+1}^{\vep}$ and let $\tilde{\pi}_{k+1}^{\vep}$ be the law of $(X,Y,Y^{w_{k\vep}},Y)$. By definition of $2$-Wasserstein distance (see \eqref{eq:2wass}), we then have:
\begin{align}\label{eq:jointcon1}
    \wass_2^2(\gamma_{k+1}^{\vep},\tilde{\xi}_{k\vep})\le \E_{\tilde{\pi}_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2=\E_{\gamma_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2.
\end{align}
This establishes step (a).

\vspace{0.05in} 

\emph{Proof of step (b).} For proving step (b), we need the following  preparatory lemma. 

\begin{lmm}\label{lem:erboundmain}
Suppose \cref{asn:solcon} holds. Define $$a_k^{\vep}(x):=\frac{1}{\vep}(u_k^{\vep}-u_{k\vep})(x),\quad b_k^{\vep}(y):=\frac{1}{\vep}(\opV[u_k^{\vep}]-\opV[u_{k\vep}])(y).$$
Then there exists $C>0$ such that 
\begin{align*}
    \sup_{k:\ k\vep\le T} \left(\lVert a_k^{\vep}\rVert_{\infty}+\lVert b_k^{\vep}\rVert_{\infty}\right)\le C.
\end{align*}
\end{lmm}

From the above definitions of $a_k^{\vep}$ and $b_k^{\vep}$, the following relation is immediate:
\begin{align}\label{eq:funrel}
\gamma^{\vep}_{k+1}(x,y)= \xi^{\vep}_{k+1}(x,y) \exp\left(-a_k^\vep(x) - b_k^\vep(y)\right).
\end{align}
An application of \cref{lem:erboundmain} then yields
$$\gamma_{k+1}^{\vep}\le C\xi_{k+1}^{\vep}.$$
This readily implies \eqref{eq:stepb}. We remind the reader here that the constant $C$ is changing from line to line. Therefore, $C$ in the above display is not the same as that in \cref{lem:erboundmain}.

\vspace{0.05in}


\emph{Proof of step (c).} To establish step (c), we need the following lemma. 
\begin{lmm}\label{lem:pmaboundmain}
Suppose \cref{asn:solcon} holds and $\vep \in (0,1)$.  
Then there exists $C>0$ such that 
\begin{align*}
    \sup_{\substack{k:\ k\vep\le T,\\ y\in \R^d}} \left| \opV[u_{k\vep}](y)-w_{k\vep}(y)-\frac{\vep d}{2}\log{(2\pi\vep)}+\vep f(y^{w_{k\vep}})-\frac{\vep}{2}\ldet\left(\frac{\partial y^{w_{k\vep}}}{\partial y\hfill}\right) \right|\le C\vep^2.
\end{align*}
\end{lmm}

The following function (often referred to as the \emph{Bregman divergence function}) will help us simplify notation in the proof of step (c).
\begin{equation}\label{eq:bregdiv}
\mathcal{D}[u_{k\vep}](x|y):=u_{k\vep}(x)+w_{k\vep}(y)-\langle x,y\rangle\ge 0.
\end{equation}

Now the left hand side of \eqref{eq:stepc} (barring the supremum) simplifies as 

\begin{align*}
&\;\;\;\;\E_{\xi_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2\nonumber \\ &=\int \int \lVert x-y^{w_{k\vep}}\rVert^2 \exp\left(\frac{1}{\vep}\langle x,y\rangle - \frac{1}{\vep} u_{k\vep}(x)-\frac{1}{\vep}\opV[u_{k\vep}](y)-f(x)-g(y)\right)\,dx\,dy\nonumber \\ &\le C \int \int \frac{1}{(2\pi\vep)^{d/2}}\sqrt{\mathrm{det}\left(\frac{\partial y\hfill}{\partial y^{w_{k\vep}}}\right)}\lVert x-y^{w_{k\vep}}\rVert^2\times \\ \;\;\;\;&\exp\bigg(-\frac{1}{\vep}\mathcal{D}[u_{k\vep}](x|y)+f(y^{w_{k\vep}})-f(x)-g(y)\bigg)\,dx\,dy
\end{align*}
where the last inequality follows from \cref{lem:pmaboundmain}. 

\noindent By a Taylor expansion of $u_{k\vep}(x)$ around the point $y^{w_{k\vep}}$, coupled with \cref{asn:solcon}, (i), we get:
\begin{equation}\label{eq:estimpf1}
\mathcal{D}[u_{k\vep}](x|y)\ge \frac{A_T}{2}\lVert x-y^{w_{k\vep}}\rVert^2.
\end{equation}

Combining the two displays above with \cref{asn:solcon}, part (ii),  it follows that: 

\begin{align*}
&\;\;\;\;\;\E_{\xi_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2\\ &\le \frac{C B_T^{\frac{d}{2}}}{(2\pi\vep)^{\frac{d}{2}}} \int \int \lVert x-y^{w_{k\vep}}\rVert^2 \exp\left(-\frac{A_T}{2\vep}\lVert x-y^{w_{k\vep}}\rVert^2+f(y^{w_{k\vep}})-f(x)-g(y)\right)\,dx\,dy.
\end{align*}

Next, we focus on the inner integral above (the one with respect to $x$). We drop the $e^{-g(y)}$ term and adjust some constants, all of which are free of $x$ and reproduce the rest below:

\begin{align*}
    &\;\;\;\;\frac{1}{(2\pi\vep A_T)^{\frac{d}{2}}} \int \lVert x-y^{w_{k\vep}}\rVert^2 \exp\left(-\frac{A_T}{2\vep}\lVert x-y^{w_{k\vep}}\rVert^2+f(y^{w_{k\vep}})-f(x)\right)\,dx\\ &\le \frac{1}{(2\pi\vep A_T)^{\frac{d}{2}}} \int \lVert x-y^{w_{k\vep}}\rVert^2 \exp\left(-\frac{A_T}{2\vep}\lVert x-y^{w_{k\vep}}\rVert^2+\lVert x-y^{w_{k\vep}}\rVert_1 \lVert \nabla f\rVert_{\infty}\right)\,dx\\ &=\E\left[\lVert Z_{\vep}-y^{w_{k\vep}}\rVert^2 \exp\left(\lVert Z_{\vep}-y^{w_{k\vep}}\rVert_1 \lVert \nabla f\rVert_{\infty}\right)\right],
\end{align*}
where $Z_{\vep,y}\sim N(y^{w_{k\vep}},\vep A_T^{-1} \mathrm{I}_d)$. 

As $\lVert Z_{\vep,y}-y^{w_{k\vep}}\rVert$ is distributed according to a $\sqrt{\vep A_T^{-1}}\chi_d$ random variable. By a standard $\chi_d$ tail bound (see example \cite[Lemma 1]{laurent2000adaptive}), it follows that:
$$\E\left[\lVert Z_{\vep}-y^{w_{k\vep}}\rVert^2 \exp\left(\lVert Z_{\vep}-y^{w_{k\vep}}\rVert_1 \lVert \nabla f\rVert_{\infty}\right)\right]\le C\vep.$$
By combining the above observations, we then get:
$$\E_{\xi_{k+1}^{\vep}} \lVert X-Y^{w_{k\vep}}\rVert^2\le C\vep \int \exp(-g(y))\,dy=C\vep.$$
As the above bound is uniform over $k$ such that $k\vep\le T$, the conclusion in \eqref{eq:stepc} follows.
}
\end{proof}

\begin{comment}
 Since $\sup_n\mathrm{KL}(\gamma_{k_n}', \xi_{k_n}')< \infty$, it follows that $\sup_n\mathbb{W}_1\left( \gamma_{k_n}', \xi_{k_n}' \right)< \infty$. In particular, $(\gamma_{k_n}',\; n \in \NN)$ is a tight sequence of probability measures. Thus it is relatively compact in weak topology. 

Consider any limit point $\gamma_\infty'$.
Since KL divergence is lower semicontinuous in its arguments in the weak topology, it follows that 
\[
\mathrm{KL}\left( \gamma_\infty', \xi_\infty' \right)\le \liminf_n \mathrm{KL}(\gamma_{k_n}', \xi_{k_n}')< \infty.
\]
Since the $Y$ marginal of $\gamma'_\infty$ must be the limit of the $Y$ marginals of $\gamma'_{k_n}$, it is again $e^{-g}$. Thus 
\begin{enumerate}
\item $\xi_\infty'$ must be supported on the graph of a function, i.e., it must be of the form $(T, \mathrm{id})_{\# e^{-g}}$. 
\item Since the only way the pushforward by two graphs $(\mathrm{id}, T_1)$ and $(\mathrm{id}, T_2)$ of the same measure $e^{-g}$ can have finite KL for the graphs to be the same $e^{-g}$ almost surely.
\end{enumerate}

Thus, necessarily $\gamma_\infty'=\xi_\infty'$ as measures. Hence the marginal law of the $X$ coordinates are also the same. Thus $\lim_{n\rightarrow \infty} \rho^{\vep}_{k_n}= \rho_t$, and we are done. 
\end{comment}

%\begin{remark}
%A similar proof technique would also show convergence of $\rho_k^{\vep}$ to $\rho_{k\vep}$ in the Kullback-Leibler divergence (see \eqref{eq:KLdef}).
%\end{remark}

\noindent In other words, \cref{thm:convergence} shows that the marginals from the Sinkhorn algorithm \eqref{eq:sinkupdt} converge to the marginals of the limiting dynamics governed by the Sinkhorn PDE (see \eqref{eq:velocity}). In light of \cref{sec:mirror}, the above result shows the connection between the Sinkhorn algorithm and the Wasserstein mirror flow of entropy (in continuum) with respect to the mirror $U(\cdot)$ which maps $\rho\mapsto \frac{1}{2}\wass_2^2\left(\rho, e^{-g}\right)$ (see \eqref{eq:mirror}).

\begin{remark}
    Recall that $\gamma_{k+1}^{\vep}$ is the Schr\"{o}dinger Bridge coupling between $\mk{k}$ and $e^{-g}$. From the proof of \cref{thm:convergence} (see in particular \eqref{eq:jointcon}), we actually obtain a quantitative convergence bound on this coupling too. In particular, it holds that $\wass_2^2(\gamma_{k+1}^{\vep},(\nabla w_{k\vep},\mathrm{id})_{\#} e^{-g})\le \vep$ uniformly over $1\le k\le \lfloor T/\vep\rfloor$.
\end{remark}
