{
  "title": "Tuning-free one-bit covariance estimation using data-driven dithering",
  "authors": [
    "Sjoerd Dirksen",
    "Johannes Maly"
  ],
  "submission_date": "2023-07-24T08:41:51+00:00",
  "revised_dates": [
    "2023-08-25T00:07:24+00:00",
    "2023-10-16T00:22:40+00:00",
    "2024-01-15T01:17:38+00:00"
  ],
  "abstract": "We consider covariance estimation of any subgaussian distribution from finitely many i.i.d. samples that are quantized to one bit of information per entry. Recent work has shown that a reliable estimator can be constructed if uniformly distributed dithers on $[-λ,λ]$ are used in the one-bit quantizer. This estimator enjoys near-minimax optimal, non-asymptotic error estimates in the operator and Frobenius norms if $λ$ is chosen proportional to the largest variance of the distribution. However, this quantity is not known a-priori, and in practice $λ$ needs to be carefully tuned to achieve good performance. In this work we resolve this problem by introducing a tuning-free variant of this estimator, which replaces $λ$ by a data-driven quantity. We prove that this estimator satisfies the same non-asymptotic error estimates - up to small (logarithmic) losses and a slightly worse probability estimate. We also show that by using refined data-driven dithers that vary per entry of each sample, one can construct an estimator satisfying the same estimation error bound as the sample covariance of the samples before quantization -- again up logarithmic losses. Our proofs rely on a new version of the Burkholder-Rosenthal inequalities for matrix martingales, which is expected to be of independent interest.",
  "categories": [
    "math.ST",
    "cs.IT"
  ],
  "primary_category": "math.ST",
  "doi": null,
  "journal_ref": null,
  "arxiv_id": "2307.12613",
  "pdf_url": null,
  "comment": null,
  "num_versions": null,
  "size_before_bytes": 4215075,
  "size_after_bytes": 3887661
}